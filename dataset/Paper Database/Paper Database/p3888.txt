IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 20, NO. 10, OCTOBER 2011
A Multilevel Mixture-of-Experts Framework for
Pedestrian Classiﬁcation
Markus Enzweiler and Dariu M. Gavrila
Abstract—Notwithstanding many years of progress, pedestrian
recognition is still a difﬁcult but important problem. We present
a novel multilevel Mixture-of-Experts approach to combine information from multiple features and cues with the objective of
improved pedestrian classiﬁcation. On pose-level, shape cues based
on Chamfer shape matching provide sample-dependent priors
for a certain pedestrian view. On modality-level, we represent
each data sample in terms of image intensity, (dense) depth, and
(dense) ﬂow. On feature-level, we consider histograms of oriented
gradients (HOG) and local binary patterns (LBP). Multilayer
perceptrons (MLP) and linear support vector machines (linSVM)
are used as expert classiﬁers.
Experiments are performed on a unique real-world multimodality dataset captured from a moving vehicle in urban trafﬁc.
This dataset has been made public for research purposes. Our
results show a signiﬁcant performance boost of up to a factor of
42 in reduction of false positives at constant detection rates of our
approach compared to a baseline intensity-only HOG/linSVM
Index Terms—Mixture-of-experts, object detection, pedestrian
classiﬁcation.
I. INTRODUCTION
EDESTRIAN recognition is a key problem for a number
of application domains, such as intelligent vehicles,
surveillance, and robotics. Notwithstanding years of methodical and technical progress, e.g., see , , and , it
is still a difﬁcult task from a machine-vision point of view.
There is a wide range of pedestrian appearance arising from
changing articulated pose, clothing, lighting, and—in the case
of a moving camera in a dynamic environment—ever-changing
backgrounds. Explicit models to solve the problem are not
readily available, so most research has focused on implicit
learning-based representations .
Many interesting pedestrian classiﬁcation approaches have
been proposed; an overview is given in Section II. Most approaches follow a two-step approach involving feature extrac-
Manuscript received October 12, 2010; revised February 03, 2011; accepted
March 23, 2011. Date of publication April 11, 2011; date of current version
September 16, 2011. This work was supported by the Studienstiftung des
deutschen Volkes (German National Academic Foundation). The associate editor coordinating the review of this manuscript and approving it for publication
was Dr. Erhardt Barth.
M. Enzweiler is with the Environment Perception Department, Daimler AG
Group Research & MCG Development, 89081 Ulm, Germany (e-mail: markus.
 ).
D. M. Gavrila is with the Environment Perception Department, Daimler AG
Group Research & MCG Development, 89081 Ulm, Germany, and also with the
Intelligent Autonomous Systems Group, University of Amsterdam, 1098 XH,
Amsterdam, The Netherlands (e-mail: ).
Color versions of one or more of the ﬁgures in this paper are available online
at 
Digital Object Identiﬁer 10.1109/TIP.2011.2142006
tion and pattern classiﬁcation. In recent years, a multitude of
(more or less) different feature sets has been used to discriminate
pedestrians from nonpedestrian images. Most of those features
operate on intensity contrasts in spatially restricted local parts of
an image. As such, they resemble neural structures which exist
in lower level processing stages of the human visual cortex .
In human perception, however, depth and motion are important
additional cues to support object recognition. In particular, the
motion ﬂowﬁeld and surface depth maps seem to be tightly integrated with spatial cues, such as shape, contrasts, or color .
With a few exceptions (see Section II), most spatial features
used in machine vision for object classiﬁcation are based on intensity cues only. If used at all, depth and motion cues merely
provide information about scene geometry or serve as a selection mechanism for regions of interest in a segmentation rather
than a classiﬁcation context , , , .
In this paper, we propose to enrich intensity-based feature
spaces for pedestrian classiﬁcation with features operating on
dense stereo (depth) and dense optical ﬂow (motion). We show
how to combine multifeature/multicue classiﬁers in a principled manner, using a classiﬁer-independent Mixture-of-Experts
framework which does neither suffer from the curse of dimensionality nor impractical training times, given our large high-dimensional dataset.
II. PREVIOUS WORK
Pedestrian classiﬁcation has attracted a signiﬁcant amount of
interest from the research community over the past years. See
 , , , and for recent surveys and performance
studies. In this work, we focus on 2-D approaches which are
suitable for medium resolution pedestrian data (i.e., pedestrian
height between 30 and 80 pixels). We do not consider more detailed perception tasks such as human pose recovery or activity
recognition, e.g., , .
A pedestrian classiﬁer is typically part of an integrated system
involving a preprocessing step to select initial object hypotheses
and a postprocessing step to integrate classiﬁcation results over
time (tracking); see and . The classiﬁer itself is the most
important module. Its performance accounts for the better part
of the overall system performance and the majority of computational resources is spent here.
Most approaches for pedestrian classiﬁcation follow a discriminative scheme by learning discriminative functions (decision boundaries) to separate object classes within a feature
space. Prominent features can be roughly categorized into texture-based and gradient-based.
Nonadaptive texture-based Haar wavelet features have been
popularized by and used by many others , , .
Recently, local binary pattern (LBP) features have also been
1057-7149/$26.00 © 2011 IEEE
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 20, NO. 10, OCTOBER 2011
employed in pedestrian classiﬁcation . The particular structure of local texture features has been optimized in terms of local
receptive ﬁeld (LRF) features , , , , which adapt
to the underlying data during training. Other texture-based features are codebook patches, extracted around interest points in
the image , , and linked via geometric relations.
Gradient-based features have focused on discontinuities in
image brightness. Normalized local histograms of oriented gradients have found wide use in both sparse (SIFT) and dense
representations [histograms of oriented gradients (HOG)] ,
 , , , , – , , , . Spatial variation
and correlation of gradients have been encoded using covariance descriptors enhancing robustness towards brightness variations . However, others have proposed local shape ﬁlters
exploiting characteristic patterns in the spatial conﬁguration of
salient edges , .
Some of the presented spatial ﬁlters have been extended to the
spatio-temporal domain by means of intensity differences over
time , or optical ﬂow .
Regarding pattern classiﬁers, support vector machines
(SVMs) have become very popular in the domain of pedestrian
classiﬁcation, in both linear , , , , , , ,
 , , and nonlinear variants , , . However,
performance boosts resulting from the nonlinear model are
paid for with a signiﬁcant increase in computational costs and
memory. Recent work presented efﬁcient versions of nonlinear
SVMs for a speciﬁc class of kernels . Other popular classiﬁers include neural networks , , , , and
boosted classiﬁers , , – , , , , .
In the past years, many novel feature and classiﬁer combinations were proposed to improve classiﬁcation performance,
along with corresponding experimental studies and benchmarks, e.g., , , , . Orthogonal to such lower level
performance boosts are improvements coming from higher
level methods based on the fusion of multiple classiﬁers.
Several approaches have attempted to break down the complexity of the problem into subparts. One way is to represent
each sample as an ensemble of components which are usually
related to body parts. After detecting the individual body parts,
detection results are fused using statistical models , ,
 , learning or voting schemes , , , , , or
heuristics .
component-based
approaches,
multi-orientation
models are relevant to current work. Here, local pose-speciﬁc
clusters are established, followed by the training of specialized
classiﬁers for each subspace. The ﬁnal decision of the classiﬁer
ensemble involves maximum selection , trajectory-based
data association , shape-based combination , , or a
fusion classiﬁer .
A recent trend in the community involves the combination
of multiple features or modalities, e.g., intensity, depth. and
motion. While some approaches utilize combinations on the
module level , , , , , , others integrate
multiple information sources directly into the pattern classiﬁcation step , , , , , , – , , .
To the best of our knowledge, our work in presented the
ﬁrst use of appearance, motion, and stereo features for pedestrian classiﬁcation. A similar approach was recently presented
in . Some approaches combine features in the intensity domain using a boosted cascade classiﬁer or multiple kernel
learning . One approach combines HOG, covariance, and
edgelet features in the intensity domain into a boosted heterogenous cascade classiﬁer with an explicit optimization with regard to runtime . Others integrate intensity and ﬂow features by boosting , or by concatenating all features into
a single feature vector which is then passed to a single classi-
ﬁer , , . The work in was recently extended to
additionally include depth features . A joint feature space
approach to combine HOG and LBP features was used in .
 presents the integration of HOG features, co-occurrence
features and color frequency descriptors into a very high-dimensional
170 000 dimensions joint feature space in which
classical machine learning approaches are intractable. Hence,
partial least squares is applied to project the features into a subspace with lower dimensionality which facilitates robust classi-
ﬁer learning. Boosting approaches require mapping the multidimensional features to a single dimension, either by applying
projections or treating each dimension as an individual
feature . An alternative is the use of more complex weak
learners that operate in a multidimensional space, e.g., support
vector machines, .
In contrast, , , , and utilize fusion on the classiﬁer level by training a specialized classiﬁer for each cue. The
work in and use a single feature (HOG) in two (intensity
and depth) and three different modalities (intensity, depth, and
motion), respectively. The work in involves a combination
of two features (HOG and LRF) with a single modality (intensity). Finally, the work in presents a classiﬁer-level combination of two features, where each feature operates in a different modality (HOG/intensity and LRF/depth). Classiﬁer fusion is done using fuzzy integration , simple classiﬁer combination rules , or a Mixture-of-Experts framework , ,
 . Our work in , , and provides the foundation for
this paper.
III. OVERVIEW AND CONTRIBUTIONS
Our Mixture-of-Experts framework for pedestrian classiﬁcation combines four modalities (shape, intensity, depth,
and motion) and three features (Chamfer distance, HOG, and
LBP). We follow a multilevel approach by utilizing expert
classiﬁers on pose, modality, and feature levels; see Fig. 1(a).
The local experts are integrated in terms of a probabilistic
pose-speciﬁc model based on fuzzy view-related clustering and
associated sample-dependent cluster priors.
view-related
models, speciﬁc to fuzzy clusters
, are trained in an off-line
step to discriminate between pedestrians and nonpedestrians.
These models consist of sample-dependent cluster priors and
multilevel (multicue/multifeature) expert classiﬁers. In the
online application phase, cluster priors are computed using
shape matching and used to fuse the multilevel expert classi-
ﬁers to a combined decision; see Fig. 1(b). Details are given in
Section IV.
The main contribution of this paper is the aforementioned
pose-speciﬁc multilevel Mixture-of-Experts framework for
pedestrian classiﬁcation, which breaks down the complex classiﬁcation problem into better manageable subproblems. To our
ENZWEILER AND GAVRILA: MULTILEVEL MIXTURE-OF-EXPERTS FRAMEWORK FOR PEDESTRIAN CLASSIFICATION
Fig. 1. Framework overview. (a) Multilevel object representation comprising Mixture-of-Experts on pose, ( ), modality, (
), and feature levels ( ). (b) K
view-related models speciﬁc to fuzzy clusters are used for pedestrian classiﬁcation. The models consist of sample-dependent cluster priors and multicue/feature
discriminative experts which are learned from pedestrian (class ! ) and nonpedestrian (class ! ) samples x.
knowledge, this work represents the ﬁrst integration of shape,
intensity, depth and motion as features into a pattern classiﬁcation framework. We observed that the same pedestrians appear
with a different level of saliency in the gray-level intensity,
depth, and motion images. This motivates our multimodality
fusion approach, to beneﬁt from the strengths of the individual
cues. Our multicue dataset has been made public for evaluation
purposes, see Section V-A.
In this paper, we are not concerned with establishing the best
absolute performance given various state-of-the-art pedestrian
classiﬁers. We refer the reader to recently proposed systems and
benchmark studies, e.g., , , , , , , , ,
 , . Rather, our aim is to demonstrate the relative performance gain resulting from the proposed multilevel approach,
exempliﬁed using state-of-the-art feature sets and classiﬁers in
our experiments. The proposed framework is independent of
the actual feature sets and classiﬁers used. The experiments in
this paper are designed to stimulate further research and provide an accessible baseline—we use publicly available data and
software implementations wherever possible—to which the scientiﬁc community can benchmark additional feature-classiﬁer
combinations.
Our approach has a number of advantages compared to fusion approaches using a joint feature space, e.g., , ,
 . First, our individual expert classiﬁers operate on a local
lower-dimensional feature subspace and are less prone to over-
ﬁtting effects, given an adequate number of training samples.
We do not need to apply dimensionality reduction techniques,
e.g., , to robustly train our classiﬁers. Compared to multifeature boosting approaches, we also do not require techniques to
map the multidimensional features to a single dimension, e.g.,
through projection or selection of 1-D features .
Second, our Mixture-of-Experts framework alleviates practical problems arising from the use of large and high-dimensional datasets in pattern classiﬁcation. Some authors reported
that classical machine learning techniques do not scale up (on
practical terms) to the use of many tens of thousands of high-dimensional training samples, due to excessive memory requirements, e.g., nonlinear SVMs or even linear SVMs , .
In contrast, the local expert classiﬁers in our framework are
trained on a lower dimensional subspace alleviating memory requirements. As a result, more complex classiﬁers and/or a larger
amount of training samples can be used, which results in better
performance.
A third issue is training time, which can be of the order
of weeks on current hardware, particularly for boosting approaches, e.g., , , . In our approach, training times
are usually faster, given the lower dimensionality and inherent
parallelism of training multiple local experts independently at
the same time. Note, that the expert classiﬁers used in our experiments did not require more than one hour for each training
Finally, since our expert classiﬁers are independent from each
other, they are not required to use exactly the same dataset for
training. Given that most recently published datasets include
samples from the intensity domain only , , , our approach could make maximum use of all available samples. For
evaluation purposes, we utilize the same data samples for each
cue/feature in our experiments to eliminate effects arising from
imbalanced data.
This paper goes beyond our earlier work in , , and
 . In , we focus on occlusion handling, whereas the main
contribution of is orientation estimation. In , we address intensity and depth based pedestrian classiﬁcation, but
take neither pose-speciﬁc Mixture-of-Experts nor motion-based
features into account.
The remainder of this paper is structured as follows. In
Section IV, our multilevel Mixture-of-Experts framework is
introduced. Section V presents our dataset and experimental
setup. In Section VI, we experimentally evaluate our approach,
followed by a discussion in Section VII. We conclude in
Section VIII.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 20, NO. 10, OCTOBER 2011
Fig. 2. (a) Average gradient magnitude of all pedestrian training samples for
intensity, depth, and motion (left to right). (b) A difﬁcult-to-recognize (low-contrast) pedestrian in the intensity domain can be very salient in other modalities.
IV. MULTILEVEL MIXTURE-OF-EXPERTS
A. Object Representation
Input to our framework is a training set
of pedestrian
and nonpedestrian
. Each sample
consists of
different modalities
In each modality
, a sample
is represented in
In this work, we consider
different modalities, i.e.,
gray-level image intensity
, dense depth information via
stereo vision
 and dense optical ﬂow
similarly to gray-level intensity images
that both depth and motion cues are represented as images,
where pixel values encode distance from the camera and horizontal optical ﬂow between two temporally aligned images.
Dense stereo provides information for most image areas,
apart from regions which are visible only by one camera (stereo
shadow). Spatial features can be based on either depth
meters) or disparity
(in pixels). Both are inversely proportional, given the camera geometry with focal length
distance between the two cameras
Objects in the scene have similar foreground/background gradients in depth space, irrespective of their location relative to the
camera. In disparity space however, such gradients are larger,
the closer the object is to the camera. To remove this variability,
we derive spatial features from depth instead of disparity.
In case of optical ﬂow, we only consider the horizontal
component of ﬂow vectors, to alleviate effects introduced
from a moving camera with a signiﬁcant amount of changes
in pitch, e.g., a vehicle-mounted camera. Longitudinal camera
motion also induces optical ﬂow. We do not compensate for the
ego-motion of the camera, since we are only interested in local
differences in ﬂow between a pedestrian and the environment.
Besides, robust ego-motion compensation is a rather difﬁcult
task. As a positive side-effect, static pedestrians do not pose a
problem in combination with a moving camera.
A visual inspection of the intensity versus depth and ﬂow images in Figs. 2 and 3 reveals that pedestrians have distinct contours and textures in each modality. Fig. 2(a) shows the average
gradient magnitude of all pedestrian training samples for each
modality. In intensity images, lower-body features (shape and
appearance of legs) are the most signiﬁcant features of a pedestrian (see results of part-based approaches, e.g., ). There is
signiﬁcant texture on the pedestrian due to different clothing.
Fig. 3. Pedestrian and nonpedestrian samples in our dataset. In depth images,
darker colors denote closer distances. Note that the background (large depth
values) has been faded out for visibility. Optical ﬂow images depict the horizontal component of ﬂow vectors. Medium red colors denote close to zero ﬂow,
and darker and brighter colors indicate stronger motion (to the left and to the
right, respectively).
In the depth image, the upper-body area has dominant foreground/background gradients and is particularly characteristic
for a pedestrian. The depth texture on the pedestrian is fairly
uniform, given that areas corresponding to the pedestrian are
approximately in the same distance from the camera. Pedestrian gradients in ﬂow images are particularly strong around
the upper body and torso contours, resulting from motion discontinuities between the (uniformly moving) pedestrian and the
background. Similar to the depth image, the pedestrian upper
body area is fairly homogenous due to uniform pedestrian motion. Legs move nonrigidly and less uniform than the rest of the
pedestrian body. As a result, the lower body area is more blurred
and less signiﬁcant in the average gradient image.
The various salient regions in intensity, depth, and ﬂow
images motivate our use of fusion approaches between
those modalities to beneﬁt from the individual strengths, see
Section IV-C. A characteristic example is shown in Fig. 2(b). A
pedestrian sample which is difﬁcult to classify in the intensity
domain due to low contrast may appear very salient in the depth
and motion modalities. This highlights the complementary
aspect of different modalities.
In our experiments, we consider
features per modality,
that is, HOG features and LBP features . The motivation
for this choice is twofold. First, recent studies have shown that
HOG and LBP features are highly complementary regarding
their sensitivity to noisy background edges which are common
in cluttered backgrounds . Second, despite the vast amount
of features developed in recent years, HOG and LBP are still
among the best features around , , . Detailed parameterization of our feature set is given in Section V-B.
Associated with each sample
is a class label
the pedestrian and
for the nonpedestrian class), as well as a
-dimensional cluster membership vector
deﬁnes the fuzzy membership to a set of
, which relate to the similarity in appearance to
a certain view of a pedestrian. Note that the same also applies
to nonpedestrian training samples, where the image structure
resembles a certain pedestrian view. Our deﬁnition of cluster
membership
is given in Section V-A.
ENZWEILER AND GAVRILA: MULTILEVEL MIXTURE-OF-EXPERTS FRAMEWORK FOR PEDESTRIAN CLASSIFICATION
B. Pedestrian Classiﬁcation
For pedestrian classiﬁcation, our goal is to determine the class
of a previously unseen sample
. We make a Bayesian
decision and assign
to the class with highest posterior probability
We decompose
, the posterior probability that a
given sample is a pedestrian, in terms of the
In this formulation,
represents a sample-dependent cluster membership prior for
. We approximate
using a sample-dependent gating function
, as deﬁned in (15),
in Section IV-D.
represents the cluster-speciﬁc probability that
a given sample
is a pedestrian. Instead of explicitly computing
, we utilize an approximation given by a
set of discriminative models
. The classiﬁer outputs
can be seen as approximation of the cluster-speciﬁc posterior
probabilities
C. Multimodality/Multifeature Expert Classiﬁers
Given our pose-speciﬁc Mixture-of-Experts formulation (4),
we model the pose-speciﬁc expert classiﬁers
of our multimodality dataset (intensity, depth, and ﬂow). We
extend the Mixture-of-Experts formulation by introducing individual classiﬁers for each modality
In this formulation,
denotes a local expert classiﬁer
th fuzzy pose cluster, which is represented in terms of
th modality.
represents a pose- and modality-dependent weight.
Within each modality, we further introduce another level of
expert classiﬁers, in that multiple feature sets
are considered.
Following a similar Mixture-of-Experts principle,
represents a pose-, modality-, and feature-speciﬁc
expert classiﬁer with an associated weight
Plugging (5) and (6) into (4), we approximate
posterior probability that a given sample is a pedestrian, using
our multilevel Mixture-of-Experts model as
As expert classiﬁers
, we use pattern classiﬁers which
are learned on the training set using data from the corresponding
modality/feature only. Given
fuzzy pose clusters,
modalities, and
features, we train
classiﬁers
the full training set
to discriminate between the pedestrian
and the nonpedestrian class. For each training sample
fuzzy cluster membership vector
is used as a sample-dependent weight during training.
In principle, the proposed framework is independent from
the actual discriminative models used . We only require example-dependent weights during training and that the classiﬁer
outputs (decision value) relate to an estimate of posterior probability. For neural networks, example-dependent weights are incorporated using a weighted random sampling step to select the
examples that are presented to the neural network during each
learning iteration. In case of support vector machines, the approach of can be used. In the limit of inﬁnite data, the outputs of many state-of-the-art classiﬁers can be converted to an
estimate of posterior probabilities , . We use this in our
experiments.
We compute
, the weights to the individual expert classiﬁers, by interpreting
[see (10)] as a
dot-product in the
-dimensional space of expert classiﬁer
posterior probabilities. To determine the weights
, we train
a linear support vector machine (linSVM)
in the expert posterior space. With the linSVM bias term constrained to be zero
 , its decision function equals a dot-product
Inserting (11) into (10) then yields
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 20, NO. 10, OCTOBER 2011
D. Sample-Dependent Cluster Priors
Prior probabilities for membership to a certain cluster
an unseen sample
, are introduced in (3). Note, that
this prior is not a ﬁxed prior, but depends on the sample
As such, it represents the gating of the proposed Mixture-of-
Experts architecture.
At this point, information from other cues besides texture (on
which the discriminative models
are based) can be incorporated into our framework in a probabilistic manner. We propose
to model cluster priors using a Bayesian approach as
Cluster conditional-likelihoods
involve the representation of
in terms of a set of features, followed by likelihood estimation. Possible cues include motion-based features,
i.e., optical ﬂow , or shape . Likelihood estimation can
be performed via histogramming on training data or ﬁtting parametric models .
Here, we utilize shape cues to compute priors
the membership of a sample
to a certain cluster
each cluster
, a discrete set of shape templates speciﬁc to
is matched to the sample
. Shape matching involves correlation of the shape templates with a distance-transformed version
denote the residual shape distance between
the best matching shape in cluster
and sample
. By representing
in terms of
and using (14), sample-dependent
shape-based priors for cluster
are approximated as
are assumed equal and cluster-conditionals are
modeled as exponential distributions of
Parameters
of the exponential distributions are learned via
maximum likelihood on the training set.
V. EXPERIMENTAL SETUP
A. Dataset and Evaluation Methodology
The proposed multilevel Mixture-of-Experts framework is
tested in experiments on pedestrian classiﬁcation. We choose
the application of pedestrian classiﬁcation in complex urban
trafﬁc as an experimental testbed, since it is arguably one of
the most challenging problems around. Because we require
multicue (intensity, dense stereo, dense optical ﬂow) training
and test samples, we cannot use most established datasets for
benchmarking, e.g., , , , . The dataset introduced
by includes appearance and binocular image data, however
actual depth maps and optical ﬂow are not provided by the
authors. While depth maps and ﬂow images can be computed
by other authors using this data, it is unclear, to what extent
observed performance differences may result from different
algorithms used to compute depth and motion data. The authors
of , for example, demonstrated that the false positives at
equal detection rate levels could be reduced by a factor of three,
simply by exchanging the method of optical ﬂow estimation.
Moreover, the more sophisticated and visually better ﬂow estimator resulted in worse classiﬁcation performance . Further,
the dataset of lacks realism given our experimental setup
(urban trafﬁc), since it has been captured at walking speeds on
urban sidewalks.
Our experiments involve the recently introduced Daimler
Multi-Cue, Occluded Pedestrian Classiﬁcation Benchmark 
(we do not use the partially occluded pedestrians additionally
present in this dataset) which is publicly available to noncommercial entities for research purposes.1 This dataset is captured
from a moving vehicle in complex urban trafﬁc. We provide
gray-level intensity data, as well as precomputed dense depth
maps and dense optical ﬂow images, to eliminate any effects
arising from differences in the computation of the latter.
Recently, an independently developed approach combining
intensity, motion, and depth was presented in . However, the
dataset used in is only partly publicly available (the training
data is not public).
Performance evaluation of pedestrian classiﬁers can be
performed using a per-image measure (detection context) or a
per-window measure (classiﬁcation context). Dollar et al. 
consider the per-window evaluation for sliding-window detectors ﬂawed, since auxiliary effects, such as grid granularity or
nonmaximum suppression, are not taken into account. They
mention as an additional pitfall the use of incorrectly cropped
samples which skews performance due to boundary artifacts.
We agree with Dollar et al. that per-image evaluation should
be the preferred methodology for the evaluation of (monocular)
sliding-window or interest-point-based detectors , .
Images should be cropped in such a way to avoid boundary
artifacts.
However, we do not consider the per-window evaluation
measure as inherently ﬂawed. Both evaluation setups have
their justiﬁcation, depending on the application context. Most
real-world systems integrate several modules; they do not
follow a brute-force sliding-window detection scheme, but use
a preprocessing step to determine initial pedestrian location
hypotheses for both enhanced performance and computational
efﬁciency, e.g., using background subtraction , shape ,
 , stereo , , , motion , or nonvision sensors,
such as radar or lidar . As a result, the remaining object
hypotheses are not random subwindows, but contain a meaningful structure that resembles pedestrians in some aspect.
Further, the number of hypotheses per image is greatly reduced
(up to a factor of 10 000) compared with dense subwindow
scanning, resulting in a more even ratio between pedestrian and
nonpedestrian samples. In this application context, per-window
evaluation should be the preferred method, since it more closely
resembles the actual system setup.
1[Online].
Available:
 
ENZWEILER AND GAVRILA: MULTILEVEL MIXTURE-OF-EXPERTS FRAMEWORK FOR PEDESTRIAN CLASSIFICATION
TRAINING AND TEST SET STATISTICS
Our training and test samples consist of manually labeled
pedestrian and nonpedestrian bounding boxes in images captured from a vehicle-mounted calibrated stereo camera rig in an
urban environment. For each manually labeled pedestrian, we
create additional samples by geometric jittering. Nonpedestrian
samples result from a pedestrian shape-detection preprocessing
step with a relaxed threshold setting (to not include largely
uniform image patches, such as road surface or sky), as well as
ground-plane constraints and prior knowledge about pedestrian
geometry, i.e., containing a bias towards more ”difﬁcult” patterns, weakly resembling pedestrians in geometry and structure.
Note that this selection strategy has already been performed for
both the provided training and test data, i.e., it is not required
to be implemented to reproduce and compare to the results presented in this paper.
Training and test samples have a resolution of 48
with a 12-pixel border around the pedestrians; there is no arti-
ﬁcial extension of the border (padding, mirroring) in our data.
Dense stereo is computed using the semi-global matching algorithm . To compute dense optical ﬂow, we use the method
of . See Table I and Fig. 3 for an overview of the dataset.
We consider
view-related clusters
, roughly corresponding to similarity in appearance to front, left, back and right
views of pedestrians. We use the approximated cluster prior
probability (see Section IV-D) as cluster membership weights
for training
To compute
, a set of 10 946 shape templates corresponding to clusters
is used according to the methods
outlined in Section IV-D.
B. Feature Extraction and Classiﬁcation
Regarding features for our multicue classiﬁers, we choose
histograms of oriented gradients (HOG) and cell-structured
local binary patterns (LBP) with uniformity constraints ,
 out of many possible feature sets , , . The
motivation for this choice is twofold. First, HOG and LBP
are complementary in the sense that HOGs are gradient-based
whereas LBPs are texture-based features. HOGs are sensitive
to noisy background edges which often occur in cluttered backgrounds. LBPs can ﬁlter out background noise using uniformity
constraints, see . Second, HOG and LBP features are still
among the best performing (and most popular) feature sets
available , , .
We follow and compute histograms of oriented gradients
with nine orientation bins and 8
8 pixel cells, accumulated to
overlapping 16
16 pixel blocks with a spatial shift of eight
pixels. HOG features are computed using the implementation
provided by . To compute cell-structured LBPs, we adopt the
EXPERT WEIGHTS s
FOR FEATURES AND MODALITIES, ESTIMATED BY A
LINEAR SVM ON THE TRAINING SET
terminology and method of and compute L1-sqrt normalized
features, using 8
8 pixel cells and a maximum
number of 0–1 transitions of 2. The resulting feature dimensionality is 1980 for HOG and 4248 for LBP. Note that the same
HOG and LBP feature set is extracted from intensity, dense
stereo and dense ﬂow images.
For classiﬁcation, we employ multilayer perceptrons (MLP)
with one hidden layer consisting of eight neurons with sigmoidal transfer functions, trained stochastically using the
online error back-propagation algorithm. We utilize the FANN
library for MLP training . Compared with the popular
linSVMs, MLPs provide nonlinear decision boundaries which
usually improve performance, see . The training of nonlinear support vector machines was practically infeasible, given
our large datasets.
Expert classiﬁer weights
[see (10) and (11)] are computed using the linear SVM approach given in Section IV-C,
applied to the training set. We utilize the LIBLINEAR library
for linear SVM training . The actual weights for individual
features and modalities are listed in Table II.
We reiterate that the proposed framework is independent from
the actual feature set and discriminative models used. We encourage the scientiﬁc community to present results of other feature-classiﬁer combinations on our multicue data.
VI. EXPERIMENTS
Our experiments are designed to evaluate the different levels
of our proposed Mixture-of-Experts framework [see Fig. 1(a)],
both in isolation and in combination, to quantify the contribution of the individual cues to the overall performance. After presenting the experimental results for pedestrian classiﬁcation in
terms of ROC performance, we analyze the correlation of classiﬁer outputs in different modalities/features to gain further insight into the observed performance.
A. Pose-Level Mixture-of-Experts
In our ﬁrst experiment, we evaluate the beneﬁt of our Mixture-of-Experts architecture on pose-level only. For that, we
compare the proposed pose-speciﬁc mixture architecture to
single “monolithic” classiﬁers trained on the whole dataset
irrespective of view. We do not consider multimodality or
multifeature classiﬁers yet. For this experiment, we utilize
HOG and LBP features separately, operating in the intensity
domain only. Regarding classiﬁers, we compare linear support
vector machines (linSVM) to multilayer perceptrons (MLPs).
Note that the monolithic HOG/linSVM approach corresponds
to the method proposed by Dalal and Triggs . Results are
shown in Fig. 4(a) for HOG and in Fig. 4(b) for LBP features.
Irrespective of the employed feature set, the pose-level mixture classiﬁers perform better than the corresponding monolithic
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 20, NO. 10, OCTOBER 2011
Fig. 4. Pose-level Mixture-of-Experts versus monolithic classiﬁer. (a) HOG features in intensity modality. (b) LBP features in intensity modality.
Fig. 5. Modality-level Mixture-of-Experts. Individual classiﬁcation performance of (a) HOG and (b) LBP features in intensity, depth, and motion modality. Combined classiﬁcation performance of (c) HOG and (d) LBP features in intensity, depth, and motion modality. Note the different scaling on the x-axis.
classiﬁers. The decomposition of the problem into view-related
subparts simpliﬁes the training of the expert classiﬁers, since
a large part of the observable variation in the samples is already accounted for. Classiﬁcation performance and robustness
is increased by a combined decision of the experts. The performance beneﬁt for the pose-level mixture classiﬁer is up to
a factor of two in reduction of false positives at the same detection rate. Further, multilayer perceptrons outperform linear
support vector machines, because of their nonlinearities in decision space. Except for some experiments in Section VI-E, we
utilize pose-level Mixture-of-Experts classiﬁcation throughout
the following experiments.
B. Modality-Level Mixture-of-Experts
In our second experiment, we evaluate the performance
of modality-level classiﬁers, as presented in Section IV-C,
compared with intensity-only classiﬁers. Pose-level mixtures
are also used, that is, the ﬁrst two levels of our framework
[see Fig. 1(a)] are in place in this experiment. Performance is
evaluated for both HOG and LBP features individually. In each
feature-space, we ﬁrst evaluate all modalities separately and
incrementally add depth and motion to the baseline intensity
cue. Results are shown in Fig. 5(a) and (c) for HOG and in
Fig. 5(b) and (d) for LBP features.
ENZWEILER AND GAVRILA: MULTILEVEL MIXTURE-OF-EXPERTS FRAMEWORK FOR PEDESTRIAN CLASSIFICATION
The relative performance of classiﬁers trained on intensity,
depth and motion features only is consistent across the two
different feature spaces, cf. Fig. 5(a) (HOG) versus Fig. 5(b)
(LBP). Classiﬁers in the intensity modality have the best performance, by a large margin. In depth and motion modalities,
performance is similar for both feature sets with depth features
performing better then motion features at higher false positive
rates and worse at lower false positive rates. Note, that these
performance relations are also apparent in the individual expert
classiﬁer weights; see Table II.
Fig. 5(c) and (d) show the effect of incrementally adding
depth and motion to the intensity modality. Here, the best performance is reached when all modalities are taken into account.
However, the observable performance boosts are different for
HOG compared with LBP features. The HOG classiﬁer using
intensity, depth, and motion has approximately a factor of four
less false positives than a comparable HOG classiﬁer using intensity only [see Fig. 5(c)]. From Fig. 5(d) we observe, that in
the case of LBP features, the performance boost resulting from
utilizing all modalities versus intensity-only is approximately
a factor of 12 in reduction of false positives at equal detection
C. Feature-Level Mixture-of-Experts
Similar to analyzing the effect of modality-level Mixture-of-Experts , we now evaluate the effect of feature-level
Mixture-of-Experts. To that extent, we combine pose-level
Mixture-of-Experts with feature-level Mixture-of-Experts and
evaluate the performance of the multifeature approach in all
three modalities, i.e., intensity, depth, motion, individually.
Recalling our framework architecture [see Fig. 1(a)], this
corresponds to having levels 1 (pose) and 3 (features) in place.
Results are given in Fig. 6(a) (intensity), Fig. 6(b) (depth), and
Fig. 6(c) (motion).
In all modalities, one can observe that combining HOG and
LBP improves performance over using both features individually. The largest performance boost coming from the featurelevel Mixture-of-Experts exists in the intensity modality. Here,
the combined HOG+LBP classiﬁer has up to a factor of four
less false positives than the HOG classiﬁer, which in turn outperforms the LBP classiﬁer at higher detection rates. In depth
and motion modalities, the corresponding performance boosts
amount to factors of 2 (motion) and 1.5 (depth) at equal detection rate levels. Compared with the performance improvement obtained by combining different modalities, as shown in
Section VI-B, the effect of feature-level Mixture-of-Experts is
less pronounced, but still signiﬁcant.
D. Multilevel Mixture-of-Experts
We now evaluate the performance of our full multilevel
Mixture-of-Experts framework combining pose-, modality-,
and feature-level expert classiﬁers. As baseline performance,
the monolithic (i.e., no delineation of classiﬁers at pose-level)
HOG/linSVM approach of , as well the best performing
variants from the previous two experiments are utilized:
modality-level Mixture-of-Experts using LBP/MLP in intensity, depth and motion (see Section VI-B) as well as feature-level
Fig. 6. Feature-level Mixture-of-Experts. Individual classiﬁcation performance of HOG, LBP, and HOG+LBP in (a) intensity, (b) depth, and (c) motion
modality. Note the different scaling on the x-axis.
Mixture-of-Experts using HOG+LBP Mixture-of-Experts in
intensity domain only (see Section VI-C).
ROC performance is given in Fig. 7. We observe that our combined multilevel Mixture-of-Experts approach signiﬁcantly outperforms both variants using either modality-level or featurelevel fusion, as well as the state-of-the-art monolithic HOG/
linSVM approach . To quantify performance, Table III lists
the false positive rates of all approaches shown in Fig. 7 using a
detection rate of 90% as a common reference point. We further
indicate the resulting reduction in false positives, in comparison
to the monolithic HOG/linSVM classiﬁer as baseline.
If we combine experts on pose-level with experts on featurelevel (HOG/MLP + LBP/MLP, intensity modality), we achieve
a reduction in false positive of more than a factor of 6 over the
Dalal and Triggs HOG/linSVM approach. The use of pose-level
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 20, NO. 10, OCTOBER 2011
Fig. 7. Performance overview. (a) Monolithic HOG classiﬁer in intensity domain, best feature-level MoE (HOG+LBP, intensity), best modality-level MoE (LBP,
intensity+depth+motion), multilevel MoE (HOG+LBP, intensity+depth+motion). (b) Logarithmic plot of (a), focusing on low false-positive rates.
PERFORMANCE OF APPROACHES IN FIG. 7 USING 90% DETECTION RATE AS A
COMMON REFERENCE POINT
CORRELATION OF CLASSIFIER OUTPUTS IN (A) DIFFERENT MODALITIES AND
(B) DIFFERENT FEATURES
and modality-level experts (LBP/MLP, intensity+depth+motion
modalities) reduces false positives by more than a factor of 13
compared with the HOG/linSVM baseline. Our full multilevel
Mixture-of-Experts approach (HOG/MLP + LBP/MLP, intensity+depth+motion modalities) further boost performance up to
a reduction in false positives by a factor of 42.
The results clearly show the beneﬁt of our integrated
multilevel architecture. Additionally, we observe that the combination of different modalities attributes more to the overall
performance, than the use of multiple features within a single
modality. Given that most recent research has focused on developing yet another feature to be used in the intensity domain,
multicue classiﬁcation approaches seem to be a promising direction for future research in the domain of object classiﬁcation
to boost overall performance.
To gain further insight, we compute the correlation of classi-
ﬁer outputs (decision values) for the individual modality/feature
expert classiﬁers, computed for pedestrian and nonpedestrian
samples individually and then averaged over the two classes, see
Table IV. The correlation analysis shows, that classiﬁer outputs
are far less correlated across different modalities (Table IV-B)
than across different features (Table IV-A). Here, the less correlated two modalities/features are, the larger the beneﬁts obtained
in classiﬁcation performance (see Figs. 5 and 6).
E. Classiﬁer Fusion
In our ﬁnal experiments, we compare our multilevel Mixture-of-Experts fusion approach to other techniques for classiﬁer fusion. First, we analyze fusion approaches involving a
combination of different classiﬁers in other ways than our Mixture-of-Experts framework. Second, we compare our approach
against a single classiﬁer using a joint feature space which consists of all features in all modalities
-normalized and concatenated into a single feature vector . Given our feature setup
as presented in Section V-B, the total dimensionality of the joint
feature space is 18 684. For comparison, the performance of the
Dalal and Triggs HOG/linSVM baseline is also given. Results are shown in Fig. 8(a) for the multiclassiﬁer fusion and in
Fig. 8(b) for the joint space fusion approaches.
The multiclassiﬁer fusion approaches (entitled “Uniform
Sum”, “Product” and “Sugeno Fuzzy Integral”) involve individual classiﬁers for each feature (HOG and LBP) and
modality (intensity, depth and motion). Altogether, there are
six classiﬁers to be combined, using the sum and product of the
individual decision values , as well as a fuzzy integration
using Sugeno integrals . Fuzzy integration involves treating
the individual classiﬁer outputs as a fuzzy set and aggregating
them into a single value using the Sugeno integral. While
those approaches improve performance over the state-of-the-art
Dalal and Triggs HOG/linSVM classiﬁer , our multilevel
Mixture-of-Experts classiﬁer has a much better performance.
This clearly shows the beneﬁt of gating on pose-level [see (4)]
and the learned classiﬁer combination weights in (12).
In terms of joint space approaches, we train both a MLP
and a linSVM in the enlarged 18 684-dimensional joint feature
space (training a nonlinear SVM was not feasible given our
large dataset). While one could expect the MLP to improve
performance over the linSVM, due to the nonlinear decision
boundary, our results paint a different performance picture. The
MLP classiﬁer is outperformed by the linSVM by a signiﬁcant
margin. We attribute this to the so-called “curse of dimensionality,” e.g., , which relates the number of free parameters
in a classiﬁer (as given by feature space dimensionality) to
the amount of available training samples. As a rule of thumb,
the number of training samples should be a factor-of-10 larger
than the number of free parameters to be estimated during
ENZWEILER AND GAVRILA: MULTILEVEL MIXTURE-OF-EXPERTS FRAMEWORK FOR PEDESTRIAN CLASSIFICATION
Fig. 8. Performance of different classiﬁer fusion techniques. (a) Multiclassiﬁer fusion. (b) Joint feature space with single classiﬁers.
training . This rule is severely violated in case of the MLP
in the 18 684-dimensional joint feature space with 149 489 free
parameters and 84 577 training samples. The linSVM can better
cope with the higher dimensionality given its maximum-margin
constraint at the core which is less susceptible to overﬁtting
effects in high-dimensional spaces. Still, our multilevel Mixture-of-Experts framework using MLPs as expert classiﬁers
outperforms the joint space linSVM. We can afford to use
more complex subclassiﬁers in our model, since each MLP is
an expert in a lower dimensional modality/feature subspace,
weighted by the contribution of the shape cues.
VII. DISCUSSION
We obtained a signiﬁcant boost in pedestrian classiﬁcation
performance from the use of multiple modalities and features
in a Mixture-of-Experts setting. Our experiments show that
the largest performance gain stems from the combination of
intensity features with depth and motion features. We expect
the use of additional modalities, e.g., far-infrared (FIR) , to
further increase performance. Multimodality classiﬁers particularly outperform multifeature classiﬁers in a single modality.
However, modalities and features are orthogonal, so that a
combined multimodality/multifeature approach can further
boost performance.
In this work, we did not heavily optimize the feature sets
with regard to the different modalities. Instead, we transferred
general knowledge and experience from the behavior of features and classiﬁers in the intensity domain to the depth and
motion domains. At this point, it is not clear if (and how)
additional modiﬁcation and adaptation of the feature sets to
the different characteristics found in depth and motion data
(see Section IV-A) can further improve performance. While the
HOG/MLP classiﬁer outperforms the LBP/MLP classiﬁer in all
modalities in our experiments, this may not be generally true.
See, for example, , where the relative order of feature/classiﬁer performance reverses with respect to intensity and depth.
Orthogonal to the improvements presented in this paper are
beneﬁts resulting from an increased training set , . In the
intensity domain, feature-classiﬁer combinations respond differently to an increased training set (in both size and dimensionality), e.g., in terms of classiﬁer complexity, discriminative
power, practical feasibility and saturation effects , . It is
currently unknown to what extent similar (or different) effects
are present for features and classiﬁers in other modalities.
Recent work analyzed the dependence of classiﬁcation performance and pedestrian image size (as a proxy for distance to
the camera) in the intensity domain . Results show significant relative performance differences of the evaluated classi-
ﬁers across multiple scales. Similar effects may also be found
in depth and motion features, particularly since depth and motion measurements tend to get noisy at larger distances to the
camera. In case of stereo vision, the range of measurements is
further limited by the camera setup.
Certainly, more research is necessary to fully explore the beneﬁts of multimodality/multifeature classiﬁcation. For that purpose, we provide our multicue dataset not only as a means for
benchmarking but also to stimulate further research on the issues mentioned above.
VIII. CONCLUSION
probabilistic
multilevel
Mixture-of-Experts
view-related
sample-dependent
combination
multicue/multifeature
pedestrian classiﬁers. We use highly complementary Chamfer
distance, HOG, and LBP features that are extracted from
intensity, dense depth and dense ﬂow data. The pose-speciﬁc
Mixture-of-Experts formulation, which divides the complex
pedestrian classiﬁcation problem into better manageable subproblems, is feature- and classiﬁer-independent, practically
feasible and does not suffer from overﬁtting effects in high-dimensional spaces.
Results show a signiﬁcant performance boost of up to a factor
of 42 in reduction of false positives at constant detection rates
over a state-of-the-art intensity-only classiﬁer using HOG features and linear SVM classiﬁcation. The observed performance
improvements stem both from the fuzzy subdivision of our data
in terms of pose and the combination of multiple features and
modalities. In our experiments, we identiﬁed the use of multiple
modalities as the most beneﬁting factor which is conﬁrmed by
a correlation analysis. We make our multicue dataset publicly
available for benchmarking purposes and to stimulate further
research to address open issues with regard to multicue/multifeature classiﬁcation.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 20, NO. 10, OCTOBER 2011
ACKNOWLEDGMENT
The authors would like to thank Prof. C. Schnörr (Image and
Pattern Analysis Group, University of Heidelberg, Germany),
who provided helpful comments and discussions.