IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 43, NO. 2, FEB. 2021
Res2Net: A New Multi-scale Backbone
Architecture
Shang-Hua Gao∗, Ming-Ming Cheng∗, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr
Abstract—Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone
convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent
performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layerwise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like
connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the
range of receptive ﬁelds for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone
CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent
performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and
experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object
detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained
models are available on 
Index Terms—Multi-scale, deep learning.
INTRODUCTION
ISUAL patterns occur at multi-scales in natural scenes as
shown in Fig. 1. First, objects may appear with different
sizes in a single image, e.g., the sofa and cup are of different
sizes. Second, the essential contextual information of an object
may occupy a much larger area than the object itself. For
instance, we need to rely on the big table as context to better
tell whether the small black blob placed on it is a cup or a
pen holder. Third, perceiving information from different scales
is essential for understanding parts as well as objects for tasks
such as ﬁne-grained classiﬁcation and semantic segmentation.
Thus, it is of critical importance to design good features for
multi-scale stimuli for visual cognition tasks, including image
classiﬁcation , object detection , attention prediction
 , target tracking , action recognition , semantic segmentation , salient object detection , , object proposal
 , , skeleton extraction , stereo matching , and
edge detection , .
Unsurprisingly, multi-scale features have been widely used
in both conventional feature design , and deep learning
 , . Obtaining multi-scale representations in vision tasks
requires feature extractors to use a large range of receptive ﬁelds
to describe objects/parts/context at different scales. Convolutional neural networks (CNNs) naturally learn coarse-to-ﬁne
multi-scale features through a stack of convolutional operators.
Such inherent multi-scale feature extraction ability of CNNs
leads to effective representations for solving numerous vision
tasks. How to design a more efﬁcient network architecture is
*Equal contribution
S.H. Gao, M.M. Cheng, K. Zhao, and X.Y Zhang are with the TKLNDST,
College of Computer Science, Nankai University, Tianjin 300350,
M.H. Yang is with UC Merced.
P. Torr is with Oxford University.
M.M. Cheng is the corresponding author ( ).
Fig. 1. Multi-scale representations are essential for various vision
tasks, such as perceiving boundaries, regions, and semantic categories of the target objects. Even for the simplest recognition tasks,
perceiving information from very different scales is essential to understand parts, objects (e.g., sofa, table, and cup in this example),
and their surrounding context (e.g., ‘on the table’ context contributes
to recognizing the black blob).
the key to further improving the performance of CNNs.
In the past few years, several backbone networks, e.g., ,
 , , , , , , , , , have made
signiﬁcant advances in numerous vision tasks with state-of-theart performance. Earlier architectures such as AlexNet and
VGGNet stack convolutional operators, making the datadriven learning of multi-scale features feasible. The efﬁciency
of multi-scale ability was subsequently improved by using conv
layers with different kernel size (e.g., InceptionNets , ,
 ), residual modules (e.g., ResNet ), shortcut connections (e.g., DenseNet ), and hierarchical layer aggregation
(e.g., DLA ). The advances in backbone CNN architectures
have demonstrated a trend towards more effective and efﬁcient
multi-scale representations.
 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 43, NO. 2, FEB. 2021
(a) Bottleneck block
(b) Res2Net module
Fig. 2. Comparison between the bottleneck block and the proposed
Res2Net module (the scale dimension s = 4).
In this work, we propose a simple yet efﬁcient multiscale processing approach. Unlike most existing methods that
enhance the layer-wise multi-scale representation strength of
CNNs, we improve the multi-scale representation ability at a
more granular level. Different from some concurrent works ,
 , that improve the multi-scale ability by utilizing features with different resolutions, the multi-scale of our proposed
method refers to the multiple available receptive ﬁelds at a more
granular level. To achieve this goal, we replace the 3×3 ﬁlters1
of n channels, with a set of smaller ﬁlter groups, each with
w channels (without loss of generality we use n = s × w).
As shown in Fig. 2, these smaller ﬁlter groups are connected
in a hierarchical residual-like style to increase the number
of scales that the output features can represent. Speciﬁcally,
we divide input feature maps into several groups. A group
of ﬁlters ﬁrst extracts features from a group of input feature
maps. Output features of the previous group are then sent to the
next group of ﬁlters along with another group of input feature
maps. This process repeats several times until all input feature
maps are processed. Finally, feature maps from all groups are
concatenated and sent to another group of 1 × 1 ﬁlters to fuse
information altogether. Along with any possible path in which
input features are transformed to output features, the equivalent
receptive ﬁeld increases whenever it passes a 3 × 3 ﬁlter,
resulting in many equivalent feature scales due to combination
The Res2Net strategy exposes a new dimension, namely
scale (the number of feature groups in the Res2Net block), as an
essential factor in addition to existing dimensions of depth ,
width2, and cardinality . We state in Sec. 4.4 that increasing
scale is more effective than increasing other dimensions.
Note that the proposed approach exploits the multi-scale potential at a more granular level, which is orthogonal to existing
methods that utilize layer-wise operations. Thus, the proposed
building block, namely Res2Net module, can be easily plugged
into many existing CNN architectures. Extensive experimental
results show that the Res2Net module can further improve
1. Convolutional operators and ﬁlters are used interchangeably.
2. Width refers to the number of channels in a layer as in .
the performance of state-of-the-art CNNs, e.g., ResNet ,
ResNeXt , and DLA .
RELATED WORK
Backbone Networks
Recent years have witnessed numerous backbone networks
 , , , , , , , , achieving state-ofthe-art performance in various vision tasks with stronger multiscale representations. As designed, CNNs are equipped with
basic multi-scale feature representation ability since the input
information follows a ﬁne-to-coarse fashion. The AlexNet 
stacks ﬁlters sequentially and achieves signiﬁcant performance
gain over traditional methods for visual recognition. However,
due to the limited network depth and kernel size of ﬁlters,
the AlexNet has only a relatively small receptive ﬁeld. The
VGGNet increases the network depth and uses ﬁlters
with smaller kernel size. A deeper structure can expand the
receptive ﬁelds, which is useful for extracting features from a
larger scale. It is more efﬁcient to enlarge the receptive ﬁeld
by stacking more layers than using large kernels. As such, the
VGGNet provides a stronger multi-scale representation model
than AlexNet, with fewer parameters. However, both AlexNet
and VGGNet stack ﬁlters directly, which means each feature
layer has a relatively ﬁxed receptive ﬁeld.
Network in Network (NIN) inserts multi-layer perceptrons as micro-networks into the large network to enhance
model discriminability for local patches within the receptive
ﬁeld. The 1 × 1 convolution introduced in NIN has been a
popular module to fuse features. The GoogLeNet utilizes
parallel ﬁlters with different kernel sizes to enhance the multiscale representation capability. However, such capability is
often limited by the computational constraints due to its limited
parameter efﬁciency. The Inception Nets , stack more
ﬁlters in each path of the parallel paths in the GoogLeNet
to further expand the receptive ﬁeld. On the other hand, the
ResNet introduces short connections to neural networks,
thereby alleviating the gradient vanishing problem while obtaining much deeper network structures. During the feature
extraction procedure, short connections allow different combinations of convolutional operators, resulting in a large number
of equivalent feature scales. Similarly, densely connected layers
in the DenseNet enable the network to process objects in
a very wide range of scales. DPN combines the ResNet
with DenseNet to enable feature re-usage ability of ResNet
and the feature exploration ability of DenseNet. The recently
proposed DLA method combines layers in a tree structure.
The hierarchical tree structure enables the network to obtain
even stronger layer-wise multi-scale representation capability.
Multi-scale Representations for Vision Tasks
Multi-scale feature representations of CNNs are of great importance to a number of vision tasks including object detection , face analysis , , edge detection , semantic
segmentation , salient object detection , , and skeleton detection , boosting the model performance of those
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 43, NO. 2, FEB. 2021
Object detection.
Effective CNN models need to locate objects of different scales
in a scene. Earlier works such as the R-CNN mainly rely on
the backbone network, i.e., VGGNet , to extract features of
multiple scales. He et al. propose an SPP-Net approach that
utilizes spatial pyramid pooling after the backbone network to
enhance the multi-scale ability. The Faster R-CNN method 
further proposes the region proposal networks to generate
bounding boxes with various scales. Based on the Faster R-
CNN, the FPN approach introduces feature pyramid to
extract features with different scales from a single image. The
SSD method utilizes feature maps from different stages to
process visual information at different scales.
Semantic segmentation.
Extracting essential contextual information of objects requires
CNN models to process features at various scales for effective
semantic segmentation. Long et al. propose one of the
earliest methods that enables multi-scale representations of the
fully convolutional network (FCN) for semantic segmentation
task. In DeepLab, Chen et al. , introduces cascaded atrous
convolutional module to expand the receptive ﬁeld further while
preserving spatial resolutions. More recently, global context
information is aggregated from region-based features via the
pyramid pooling scheme in the PSPNet .
Salient object detection.
Precisely locating the salient object regions in an image requires
an understanding of both large-scale context information for
the determination of object saliency, and small-scale features to
localize object boundaries accurately . Early approaches 
utilize handcrafted representations of global contrast or
multi-scale region features . Li et al. propose one of
the earliest methods that enables multi-scale deep features for
salient object detection. Later, multi-context deep learning 
and multi-level convolutional features are proposed for
improving salient object detection. More recently, Hou et al.
 introduce dense short connections among stages to provide
rich multi-scale feature maps at each layer for salient object
detection.
Concurrent Works
Recently, there are some concurrent works aiming at improving
the performance by utilizing the multi-scale features
 , , . Big-Little Net is a multi-branch network
composed of branches with different computational complexity.
Octave Conv decomposes the standard convolution into
two resolutions to process features at different frequencies.
MSNet utilizes a high-resolution network to learn highfrequency residuals by using the up-sampled low-resolution features learned by a low-resolution network. Other than the lowresolution representations in current works, the HRNet ,
 introduces high-resolution representations in the network
and repeatedly performs multi-scale fusions to strengthen highresolution representations. One common operation in , ,
 , , is that they all use pooling or up-sample to resize the feature map to 2n times of the original scale to save
the computational budget while maintaining or even improving
performance. While in the Res2Net block, the hierarchical
residual-like connections within a single residual block module
enable the variation of receptive ﬁelds at a more granular level
to capture details and global features. Experimental results
show that Res2Net module can be integrated with those novel
network designs to further boost the performance.
Res2Net Module
The bottleneck structure shown in Fig. 2(a) is a basic building block in many modern backbone CNNs architectures,
e.g., ResNet , ResNeXt , and DLA . Instead of
extracting features using a group of 3 × 3 ﬁlters as in the
bottleneck block, we seek alternative architectures with stronger
multi-scale feature extraction ability, while maintaining a similar computational load. Speciﬁcally, we replace a group of
3 × 3 ﬁlters with smaller groups of ﬁlters, while connecting
different ﬁlter groups in a hierarchical residual-like style. Since
our proposed neural network module involves residual-like
connections within a single residual block, we name it Res2Net.
Fig. 2 shows the differences between the bottleneck block
and the proposed Res2Net module. After the 1×1 convolution,
we evenly split the feature maps into s feature map subsets,
denoted by xi, where i ∈{1, 2, ..., s}. Each feature subset
xi has the same spatial size but 1/s number of channels
compared with the input feature map. Except for x1, each xi
has a corresponding 3 × 3 convolution, denoted by Ki(). We
denote by yi the output of Ki(). The feature subset xi is added
with the output of Ki−1(), and then fed into Ki(). To reduce
parameters while increasing s, we omit the 3 × 3 convolution
for x1. Thus, yi can be written as:
Ki(xi + yi−1)
Notice that each 3 × 3 convolutional operator Ki() could
potentially receive feature information from all feature splits
{xj, j ≤i}. Each time a feature split xj goes through a 3 × 3
convolutional operator, the output result can have a larger receptive ﬁeld than xj. Due to the combinatorial explosion effect,
the output of the Res2Net module contains a different number
and different combination of receptive ﬁeld sizes/scales.
In the Res2Net module, splits are processed in a multi-scale
fashion, which is conducive to the extraction of both global
and local information. To better fuse information at different
scales, we concatenate all splits and pass them through a 1 × 1
convolution. The split and concatenation strategy can enforce
convolutions to process features more effectively. To reduce the
number of parameters, we omit the convolution for the ﬁrst
split, which can also be regarded as a form of feature reuse.
In this work, we use s as a control parameter of the scale
dimension. Larger s potentially allows features with richer
receptive ﬁeld sizes to be learnt, with negligible computational/memory overheads introduced by concatenation.
Integration with Modern Modules
Numerous neural network modules have been proposed in recent years, including cardinality dimension introduced by Xie et
al. , as well as squeeze and excitation (SE) block presented
by Hu et al. . The proposed Res2Net module introduces
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 43, NO. 2, FEB. 2021
Replace with
group conv
Fig. 3. The Res2Net module can be integrated with the dimension
cardinality (replace conv with group conv) and SE blocks.
the scale dimension that is orthogonal to these improvements.
As shown in Fig. 3, we can easily integrate the cardinality
 and the SE block with the proposed
Res2Net module.
Dimension cardinality.
The dimension cardinality indicates the number of groups
within a ﬁlter . This dimension changes ﬁlters from singlebranch to multi-branch and improves the representation ability
of a CNN model. In our design, we can replace the 3 × 3
convolution with the 3 × 3 group convolution, where c indicates
the number of groups. Experimental comparisons between the
scale dimension and cardinality are presented in Sec. 4.2 and
A SE block adaptively re-calibrates channel-wise feature responses by explicitly modelling inter-dependencies among
channels . Similar to , we add the SE block right
before the residual connections of the Res2Net module. Our
Res2Net module can beneﬁt from the integration of the SE
block, which we have experimentally demonstrated in Sec. 4.2
and Sec. 4.3.
Integrated Models
Since the proposed Res2Net module does not have speciﬁc
requirements of the overall network structure and the multiscale representation ability of the Res2Net module is orthogonal
to the layer-wise feature aggregation models of CNNs, we can
easily integrate the proposed Res2Net module into the state-ofthe-art models, such as ResNet , ResNeXt , DLA 
and Big-Little Net . The corresponding models are referred
to as Res2Net, Res2NeXt, Res2Net-DLA, and bLRes2Net-50,
respectively.
The proposed scale dimension is orthogonal to the cardinality dimension and width dimension of prior work.
Thus, after the scale is set, we adjust the value of cardinality
and width to maintain the overall model complexity similar to
its counterparts. We do not focus on reducing the model size
Top-1 and Top-5 test error on the ImageNet dataset.
top-1 err. (%)
top-5 err. (%)
ResNet-50 
Res2Net-50
InceptionV3 
Res2Net-50-299
ResNeXt-50 
Res2NeXt-50
DLA-60 
Res2Net-DLA-60
DLA-X-60 
Res2NeXt-DLA-60
SENet-50 
SE-Res2Net-50
bLResNet-50 
bLRes2Net-50
Res2Net-v1b-50
Res2Net-v1b-101
Res2Net-200-SSLD 
in this work since it requires more meticulous designs such as
depth-wise separable convolution , model pruning , and
model compression .
For experiments on the ImageNet dataset, we mainly
use the ResNet-50 , ResNeXt-50 , DLA-60 , and
bLResNet-50 as our baseline models. The complexity of
the proposed model is approximately equal to those of the
baseline models, whose number of parameters is around 25M
and the number of FLOPs for an image of 224 × 224 pixels is
around 4.2G for 50-layer networks. For experiments on the
CIFAR dataset, we use the ResNeXt-29, 8c×64w 
as our baseline model. Empirical evaluations and discussions
of the proposed models with respect to model complexity are
presented in Sec. 4.4.
EXPERIMENTS
Implementation Details
We implement the proposed models using the Pytorch framework. For fair comparisons, we use the Pytorch implementation
of ResNet , ResNeXt , DLA as well as bLResNet-
50 , and only replace the original bottleneck block with
the proposed Res2Net module. Similar to prior work, on the
ImageNet dataset , each image is of 224×224 pixels randomly cropped from a re-sized image. We use the same data
argumentation strategy as , . Similar to , we train
the network using SGD with weight decay 0.0001, momentum
0.9, and a mini-batch of 256 on 4 Titan Xp GPUs. The learning
rate is initially set to 0.1 and divided by 10 every 30 epochs.
All models for the ImageNet, including the baseline and
proposed models, are trained for 100 epochs with the same
training and data argumentation strategy. For testing, we use the
same image cropping method as . On the CIFAR dataset, we
use the implementation of ResNeXt-29 . For all tasks, we
use the original implementations of baselines and only replace
the backbone model with the proposed Res2Net.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 43, NO. 2, FEB. 2021
We conduct experiments on the ImageNet dataset , which
contains 1.28 million training images and 50k validation images
from 1000 classes. We construct the models with approximate
50 layers for performance evaluation against the state-of-theart methods. More ablation studies are conducted on the CIFAR
Performance gain.
Table 1 shows the top-1 and top-5 test error on the ImageNet
dataset. For simplicity, all Res2Net models in Table 1 have the
scale s = 4. The Res2Net-50 has an improvement of 1.84% on
top-1 error over the ResNet-50. The Res2NeXt-50 achieves a
0.85% improvement in terms of top-1 error over the ResNeXt-
50. Also, the Res2Net-DLA-60 outperforms the DLA-60 by
1.27% in terms of top-1 error. The Res2NeXt-DLA-60 outperforms the DLA-X-60 by 0.64% in terms of top-1 error. The
SE-Res2Net-50 has an improvement of 1.68% over the SENet-
50. bLRes2Net-50 has an improvement of 0.73% in terms of
top-1 error over the bLResNet-50. The Res2Net module further
enhances the multi-scale ability of bLResNet at a granular level
even bLResNet is designed to utilize features with different
scales as discussed in Sec. 2.3. Note that the ResNet ,
ResNeXt , SE-Net , bLResNet , and DLA are
the state-of-the-art CNN models. Compared with these strong
baselines, models integrated with the Res2Net module still have
consistent performance gains.
We also compare our method against the InceptionV3 
model, which utilizes parallel ﬁlters with different kernel combinations. For fair comparisons, we use the ResNet-50 as
the baseline model and train our model with the input image
size of 299×299 pixels, as used in the InceptionV3 model. The
proposed Res2Net-50-299 outperforms InceptionV3 by 1.14%
on top-1 error. We conclude that the hierarchical residuallike connection of the Res2Net module is more effective than
the parallel ﬁlters of InceptionV3 when processing multi-scale
information. While the combination pattern of ﬁlters in InceptionV3 is dedicatedly designed, the Res2Net module presents a
simple but effective combination pattern.
Going deeper with Res2Net.
Deeper networks have been shown to have stronger representation capability , for vision tasks. To validate our model
with greater depth, we compare the classiﬁcation performance
of the Res2Net and the ResNet, both with 101 layers. As shown
in Table 2, the Res2Net-101 achieves signiﬁcant performance
gains over the ResNet-101 with 1.82% in terms of top-1 error.
Note that the Res2Net-50 has the performance gain of 1.84%
in terms of top-1 error over the ResNet-50. These results show
that the proposed module with additional dimension scale can
be integrated with deeper models to achieve better performance.
We also compare our method with the DenseNet . Compared with the DenseNet-161, the best performing model of the
ofﬁcially provided DenseNet family, the Res2Net-101 has an
improvement of 1.54% in terms of top-1 error.
Effectiveness of scale dimension.
To validate our proposed dimension scale, we experimentally
analyze the effect of different scales. As shown in
Top-1 and Top-5 test error (%) of deeper networks on the ImageNet
top-1 err.
top-5 err.
DenseNet-161 
ResNet-101 
Res2Net-101
Top-1 and Top-5 test error (%) of Res2Net-50 with different scales
on the ImageNet dataset. Parameter w is the width of ﬁlters, and s is
the number of scale, as described in Equation (1).
top-1 err.
top-5 err.
Res2Net-50
( Preserved
complexity)
Res2Net-50
( Increased
complexity)
Res2Net-50-L
the performance increases with the increase of scale. With the
increase of scale, the Res2Net-50 with 14w×8s achieves performance gains over the ResNet-50 with 1.99% in terms of top-1
error. Note that with the preserved complexity, the width of
Ki() decreases with the increase of scale. We further evaluate
the performance gain of increasing scale with increased model
complexity. The Res2Net-50 with 26w×8s achieves signiﬁcant
performance gains over the ResNet-50 with 3.05% in terms
of top-1 error. A Res2Net-50 with 18w×4s also outperforms
the ResNet-50 by 0.93% in terms of top-1 error with only
69% FLOPs. Table 3 shows the Runtime under different scales,
which is the average time to infer the ImageNet validation
set with the size of 224 × 224. Although the feature splits
{yi} need to be computed sequentially due to hierarchical
connections, the extra run-time introduced by Res2Net module
can often be ignored. Since the number of available tensors
in a GPU is limited, there are typically sufﬁcient parallel
computations within a single GPU clock period for the typical
setting of Res2Net, i.e., s = 4.
Stronger representation with ResNet.
To further explore the multi-scale representation ability of
Res2Net, we follow the ResNet v1d to modify Res2Net,
and train the model with data augmentation techniques
i.e., CutMix . The modiﬁed version of Res2Net, namely
Res2Net v1b, greatly improve the classiﬁcation performance on
ImageNet as shown in Table 1. Res2Net v1b further improve
the model performance on downstream tasks. We show the
performance of Res2Net v1b on object detection, instance
segmentation, key-points estimation in Table 5, Table 8, and Table 10, respectively.
The stronger multi-scale representation of Res2Net has been
veriﬁed on many downstream tasks i.e., vectorized road extraction , object detection , weakly supervised semantic
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 43, NO. 2, FEB. 2021
Top-1 test error (%) and model size on the CIFAR-100 dataset.
Parameter c indicates the value of cardinality, and w is the width of
top-1 err.
Wide ResNet 
ResNeXt-29, 8c×64w (base)
ResNeXt-29, 16c×64w 
DenseNet-BC (k = 40) 
Res2NeXt-29, 6c×24w×4s
Res2NeXt-29, 8c×25w×4s
Res2NeXt-29, 6c×24w×6s
ResNeXt-29, 8c×64w-SE 
Res2NeXt-29, 6c×24w×4s-SE
Res2NeXt-29, 8c×25w×4s-SE
Res2NeXt-29, 6c×24w×6s-SE
segmentation , salient object detection , interactive
image segmentation , video recognition , concealed
object detection , and medical segmentation , , .
Semi-supervised knowledge distillation solution can also
be applied to Res2Net, to achieve the 85.13% top.1 acc. on
We also conduct some experiments on the CIFAR-100
dataset , which contains 50k training images and 10k
testing images from 100 classes. The ResNeXt-29, 8c×64w 
is used as the baseline model. We only replace the original
basic block with our proposed Res2Net module while keeping
other conﬁgurations unchanged. Table 4 shows the top-1 test
error and model size on the CIFAR-100 dataset. Experimental
results show that our method surpasses the baseline and other
methods with fewer parameters. Our proposed Res2NeXt-29,
6c×24w×6s outperforms the baseline by 1.11%. Res2NeXt-
29, 6c×24w×4s even outperforms the ResNeXt-29, 16c×64w
with only 35% parameters. We also achieve better performance
with fewer parameters, compared with DenseNet-BC (k =
40). Compared with Res2NeXt-29, 6c×24w×4s, Res2NeXt-
29, 8c×25w×4s achieves a better result with more width and
cardinality, indicating that the dimension scale is orthogonal to
dimension width and cardinality. We also integrate the recently
proposed SE block into our structure. With fewer parameters,
our method still outperforms the ResNeXt-29, 8c×64w-SE baseline.
Scale Variation
Similar to Xie et al. , we evaluate the test performance of
the baseline model by increasing different CNN dimensions,
including scale (Equation (1)), cardinality , and depth .
While increasing model capacity using one dimension, we ﬁx
all other dimensions. A series of networks are trained and
evaluated under these changes. Since has already shown
that increasing cardinality is more effective than increasing
width, we only compare the proposed dimension scale with
cardinality and depth.
Fig. 5 shows the test precision on the CIFAR-100 dataset
with regard to the model size. The depth, cardinality, and scale
Object detection results on the PASCAL VOC07 and COCO
datasets, measured using AP (%) and AP@IoU=0.5 (%). The
Res2Net has similar complexity compared with its counterparts.
AP@IoU=0.5
Res2Net-50
Res2Net-50
Res2Net-v1b-101
Average Precision (AP) and Average Recall (AR) of object detection
with different sizes on the COCO dataset.
Object size
Res2Net-50
Res2Net-50
of the baseline model are 29, 6 and 1, respectively. Experimental results suggest that scale is an effective dimension to
improve model performance, which is consistent with what we
have observed on the ImageNet dataset in Sec. 4.2. Moreover,
increasing scale is more effective than other dimensions, resulting in quicker performance gains. As described in Equation (1)
and Fig. 2, for the case of scale s = 2, we only increase the
model capacity by adding more parameters of 1 × 1 ﬁlters.
Thus, the model performance of s = 2 is slightly worse than
that of increasing cardinality. For s = 3, 4, the combination
effects of our hierarchical residual-like structure produce a rich
set of equivalent scales, resulting in signiﬁcant performance
gains. However, the models with scale 5 and 6 have limited
performance gains, about which we assume that the image in
the CIFAR dataset is too small (32×32) to have many scales.
Class Activation Mapping
To understand the multi-scale ability of the Res2Net, we visualize the class activation mapping (CAM) using Grad-CAM ,
which is commonly used to localize the discriminative regions
for image classiﬁcation. In the visualization examples shown
in Fig. 4, stronger CAM areas are covered with lighter colors.
Compared with ResNet, the Res2Net based CAM results have
more concentrated activation maps on small objects such as
‘baseball’ and ‘penguin’. Both methods have similar activation
maps on the middle size objects, such as ‘ice cream’. Due to
stronger multi-scale ability, the Res2Net has activation maps
that tend to cover the whole object on big objects such as
‘bulbul’, ‘mountain dog’, ‘ballpoint’, and ‘mosque’, while activation maps of ResNet only cover parts of objects. Such ability
of precisely localizing CAM region makes the Res2Net potentially valuable for object region mining in weakly supervised
semantic segmentation tasks .
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 43, NO. 2, FEB. 2021
Res2Net-50
Mountain dog
Fig. 4. Visualization of class activation mapping , using ResNet-50 and Res2Net-50 as backbone networks.
# Params (M)
Test precision (%)
cardinality (-c)
depth (-d)
scale (-s)
Fig. 5. Test precision on the CIFAR-100 dataset with regard to the
model size, by changing cardinality (ResNeXt-29), depth (ResNeXt),
and scale (Res2Net-29).
Object Detection
For object detection task, we validate the Res2Net on the PAS-
CAL VOC07 and MS COCO datasets, using Faster R-
CNN as the baseline method. We use the backbone network
of ResNet-50 vs. Res2Net-50, and follow all other implementation details of for a fair comparison. Table 5 shows the
object detection results. On the PASCAL VOC07 dataset, the
Res2Net-50 based model outperforms its counterparts by 2.3%
on average precision (AP). On the COCO dataset, the Res2Net-
50 based model outperforms its counterparts by 2.6% on AP,
and 2.2% on AP@IoU=0.5.
We further test the AP and average recall (AR) scores for
objects of different sizes as shown in Table 6. Objects are divided into three categories based on the size, according to .
The Res2Net based model has a large margin of improvement
over its counterparts by 0.5%, 2.9%, and 4.9% on AP for small,
medium, and large objects, respectively. The improvement of
AR for small, medium, and large objects are 1.4%, 2.5%, and
3.7%, respectively. Due to the strong multi-scale ability, the
Res2Net based models can cover a large range of receptive
ﬁelds, boosting the performance on objects of different sizes.
Semantic Segmentation
Semantic segmentation requires a strong multi-scale ability of
CNNs to extract essential contextual information of objects. We
thus evaluate the multi-scale ability of Res2Net on the semantic
Performance of semantic segmentation on PASCAL VOC12 val set
using Res2Net-50 with different scales. The Res2Net has similar
complexity compared with its counterparts.
Mean IoU (%)
Res2Net-50
ResNet-101
Res2Net-101
segmentation task using PASCAL VOC12 dataset . We follow the previous work to use the augmented PASCAL VOC12
dataset which contains 10582 training images and 1449
val images. We use the Deeplab v3+ as our segmentation
method. All implementations remain the same with Deeplab
v3+ except that the backbone network is replaced with
ResNet and our proposed Res2Net. The output strides used in
training and evaluation are both 16. As shown in
Res2Net-50 based method outperforms its counterpart by 1.5%
on mean IoU. And Res2Net-101 based method outperforms
its counterpart by 1.2% on mean IoU. Visual comparisons
of semantic segmentation results on challenging examples are
illustrated in Fig. 6. The Res2Net based method tends to
segment all parts of objects regardless of object size.
Instance Segmentation
Instance segmentation is the combination of object detection
and semantic segmentation. It requires not only the correct
detection of objects with various sizes in an image but also the
precise segmentation of each object. As mentioned in Sec. 4.6
and Sec. 4.7, both object detection and semantic segmentation
require a strong multi-scale ability of CNNs. Thus, the multiscale representation is quite beneﬁcial to instance segmentation.
We use the Mask R-CNN as the instance segmentation
method, and replace the backbone network of ResNet-50 with
our proposed Res2Net-50. The performance of instance segmentation on MS COCO dataset is shown in Table 8. The
Res2Net-26w×4s based method outperforms its counterparts
by 1.7% on AP and 2.4% on AP50. The performance gains
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 43, NO. 2, FEB. 2021
ResNet-101
Res2Net-101
Fig. 6. Visualization of semantic segmentation results , using ResNet-101 and Res2Net-101 as backbone networks.
Performance of instance segmentation on the COCO dataset using
Res2Net-50 with different scales. The Res2Net has similar
complexity compared with its counterparts.
AP50 AP75 APS APM APL
Res2Net-50
48w×2s 34.2
26w×4s 35.6
18w×6s 35.7
14w×8s 35.3
Res2Net-v1b-101
on objects with different sizes are also demonstrated. The
improvement of AP for small, medium, and large objects are
0.9%, 1.9%, and 2.8%, respectively. Table 8 also shows the
performance comparisons of Res2Net
under the same complexity with different scales. The performance shows an overall
upward trend with the increase of scale. Note that compared
with the Res2Net-50-48w×2s, the Res2Net-50-26w×4s has an
improvement of 2.8 % on APL, while the Res2Net-50-48w×2s
has the same APL compared with ResNet-50. We assume that
the performance gain on large objects is beneﬁted from the
extra scales. When the scale is relatively larger, the performance
gain is not obvious. The Res2Net module is capable of learning
a suitable range of receptive ﬁelds. The performance gain is
limited when the scale of objects in the image is already covered
by the available receptive ﬁelds in the Res2Net module. With
ﬁxed complexity, the increased scale results in fewer channels
for each receptive ﬁeld, which may reduce the ability to process
features of a particular scale.
Salient Object Detection
Pixel level tasks such as salient object detection also require the
strong multi-scale ability of CNNs to locate both the holistic
objects as well as their region details. Here we use the latest
method DSS as our baseline. For a fair comparison, we
only replace the backbone with ResNet-50 and our proposed
Res2Net-50, while keeping other conﬁgurations unchanged.
Salient object detection results on different datasets, measured
using F-measure and Mean Absolute Error (MAE). The Res2Net has
similar complexity compared with its counterparts.
F-measure↑
Res2Net-50
Res2Net-50
Res2Net-50
Res2Net-50
Following , we train those two models using the MSRA-B
dataset , and evaluate results on ECSSD , PASCAL-
S , HKU-IS , and DUT-OMRON datasets. The
F-measure and Mean Absolute Error (MAE) are used for
evaluation. As shown in Table 9, the Res2Net based model
has a consistent improvement compared with its counterparts
on all datasets. On the DUT-OMRON dataset (containing 5168
images), the Res2Net based model has a 5.2% improvement on
F-measure and a 2.1% improvement on MAE, compared with
ResNet based model. The Res2Net based approach achieves
greatest performance gain on the DUT-OMRON dataset, since
this dataset contains the most signiﬁcant object size variation
compared with the other three datasets. Some visual comparisons of salient object detection results on challenging examples
are illustrated in Fig. 7.
Key-points Estimation
Human parts are of different sizes, which requires the keypoints estimation method to locate human key-points with
different scales. To verify whether the multi-scale representation ability of Res2Net can beneﬁt the task of key-points
estimation, we use the SimpleBaseline as the key-points
estimation method and only replace the backbone with the
proposed Res2Net. All implementations including the training
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 43, NO. 2, FEB. 2021
Res2Net-50
Fig. 7. Examples of salient object detection results, using
ResNet-50 and Res2Net-50 as backbone networks, respectively.
Performance of key-points estimation on the COCO validation set.
The Res2Net has similar complexity compared with its counterparts.
Res2Net-50
ResNet-101
Res2Net-101
Res2Net-v1b-50
Res2Net-v1b-101
and testing strategies remain the same with the SimpleBaseline . We train the model using the COCO key-point
detection dataset , and evaluate the model using the COCO
validation set. Following common settings, we use the same person detectors in SimpleBaseline for evaluation. Table 10
shows the performance of key-points estimation on the COCO
validation set using Res2Net. The Res2Net-50 and Res2Net-101
based models outperform baselines on AP by 3.3% and 3.0%,
respectively. Also, Res2Net based models have considerable
performance gains on human with different scales compared
with baselines.
CONCLUSION AND FUTURE WORK
We present a simple yet efﬁcient block, namely Res2Net, to
further explore the multi-scale ability of CNNs at a more
granular level. The Res2Net exposes a new dimension, namely
“scale”, which is an essential and more effective factor in
addition to existing dimensions of depth, width, and cardinality.
Our Res2Net module can be integrated with existing state-ofthe-art methods with no effort. Image classiﬁcation results on
CIFAR-100 and ImageNet benchmarks suggested that our new
backbone network consistently performs favourably against its
state-of-the-art competitors, including ResNet, ResNeXt, DLA,
Although the superiority of the proposed backbone model
has been demonstrated in the context of several representative
computer vision tasks, including class activation mapping, object detection, and salient object detection, we believe multiscale representation is essential for a much wider range of
application areas. To encourage future works to leverage the
strong multi-scale ability of the Res2Net, the source code is
available on 
ACKNOWLEDGMENTS
This research was supported by NSFC (NO. 61620106008,
61572264),
and Tianjin Natural Science Foundation (17JCJQJC43700,
18ZXZNGX00110).