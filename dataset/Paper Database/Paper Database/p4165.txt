Probabilistic Interpretation of Population
Richard S. Zemel
Peter Dayan
zemeleu.arizona.edu dayaneai.mit.edu
Alexandre Pouget
alexesalk.edu
We present a theoretical framework for population codes which
generalizes naturally to the important case where the population
provides information about a whole probability distribution over
an underlying quantity rather than just a single value. We use
the framework to analyze two existing models, and to suggest and
evaluate a third model for encoding such probability distributions.
Introduction
Population codes, where information is represented in the activities of whole populations of units, are ubiquitous in the brain. There has been substantial work on
how animals should and/or actually do extract information about the underlying
encoded quantity. 5,3,11,9,12 With the exception of Anderson, l this work has concentrated on the case of extracting a single value for this quantity. We study ways
of characterizing the joint activity of a population as coding a whole probability
distribution over the underlying quantity.
Two examples motivate this paper: place cells in the hippocampus of freely moving
rats that fire when the animal is at a particular part of an environment,S and cells in
area MT of monkeys firing to a random moving dot stimulus.7 Treating the activity
of such populations of cells as reporting a single value of their underlying variables
is inadequate if there is (a) insufficient information to be sure (eg if a rat can be
uncertain as to whether it is in place XA or XB then perhaps place cells for both
locations should fire; or (b) if multiple values underlie the input, as in the whole
distribution of moving random dots in the motion display. Our aim is to capture the
computational power of representing a probability distribution over the underlying
parameters.6
RSZ is at University of Arizona, Tucson, AZ 85721; PD is at MIT, Cambridge, MA
02139; AP is at Georgetown University, Washington, DC 20007. This work was funded by
McDonnell-Pew, NIH, AFOSR and startup funds from all three institutions.
Probabilistic Interpretation of Population Codes
In this paper, we provide a general statistical framework for population codes, use
it to understand existing methods for coding probability distributions and also to
generate a novel method. We evaluate the methods on some example tasks.
Population Code Interpretations
The starting point for almost all work on neural population codes is the neurophysiological finding that many neurons respond to particular variable( s) underlying a
stimulus according to a unimodal tuning function such as a Gaussian. This characterizes cells near the sensory periphery and also cells that report the results of
more complex processing, including receiving information from groups of cells that
themselves have these tuning properties (in MT, for instance). Following Zemel
& Hinton's13 analysis, we distinguish two spaces: the explicit space which consists
of the activities r = {rd of the cells in the population, and a (typically low dimensional) implicit space which contains the underlying information X that the
population encodes in which they are tuned. All processing on the basis of the
activities r has to be referred to the implicit space, but it itself plays no explicit
role in determining activities.
Figure 1 illustrates our framework. At the top is the measured activities of a population of cells. There are two key operations. Encoding: What is the relationship
between the activities r of the cells and the underlying quantity in the world X
that is represented? Decoding: What information about the quantity X can be
extracted from the activities? Since neurons are generally noisy, it is often convenient to characterize encoding (operations A and B) in a probabilistic way, by
specifying P[rIX]. The simplest models make a further assumption of conditional independence of the different units given the underlying quantity P[rIX] = I1i P[riIX]
although others characterize the degree of correlation between the units. If the encoding model is true, then a Bayesian decoding model specifies that the information
r carries about X can be characterized precisely as: P[Xlr] ex P[rIX]P[X], where
P[ X] is the prior distribution about X and the constant of proportionality is set
so that Ix P[Xlr]dX = 1. Note that starting with a deterministic quantity X in
the world, encoding in the firing rates r, and decoding it (operation C) results in
a probability distribution over X. This uncertainty arises from the stochasticity
represented by P[rIX]. Given a loss function, we could then go on to extract a
single value from this distribution (operation D).
We attack the common assumption that X is a single value of some variable x, eg
the single position of a rat in an environment, or the single coherent direction of
motion of a set of dots in a direction discrimination task. This does not capture
the subtleties of certain experiments, such as those in which rats can be made to be
uncertain about their position, or in which one direction of motion predominates yet
there are several simultaneous motion directions.7 Here, the natural characterization
of X is actually a whole probability distribution P[xlw] over the value of the variable
x (perhaps plus extra information about the number of dots), where w represents
all the available information. We can now cast two existing classes of proposals for
population codes in terms of this framework.
The Poisson Model
Under the Poisson encoding model, the quantity X encoded is indeed one particular
value which we will call x, and the activities of the individual units are independent,
R. S. Zemel, P. Dayan and A. Pouget
,--_0_ +_ 0 _,_t_f_t_o _ .....
Figure 1: Left: encoding maps X from the world through tuning functions (A) into mean activities (B), leading to Top: observed activities r. We assume complete knowledge of the variables
governing systematic changes to the activities of the cells. Here X is a single value x· in the space
of underlying variables. Right: decoding extracts 1'[Xlr) (C)j a Single value can be picked (D)
from this distribution given a loss function.
with the terms P[rilx] = e-h(x) (h(x)t' jriL The activity ri could, for example, be
the number of spikes the cell emits in a fixed time interval following the stimulus
onset. A typical form for the tuning function h(x) is Gaussian h(x) <X e-(X-Xi)2/20'2
about a preferred value Xi for cell i. The Poisson decoding model is: 3, 11, 9, 12
where K is a constant with respect to x.
Although simple, the Poisson model makes the the assumption criticized above,
that X is just a single value x. We argued for a characterization of the quantity X
in the world that the activities of the cells encode as now P[xlw]. We describe below
a method of encoding that takes exactly this definition of X. However, wouldn't
P[xlr] from Equation 1 be good enough? Not if h(x) are Gaussian, since
logP[xlr] = K' _ ~ (L:i ri) (X _ L:i riXi)2,
completing the square, implying that P[xlr] is Gaussian, and therefore inevitably
unimodal. Worse, the width of this distribution goes down with L:i ri, making it,
in most practical cases, a close approximation to a delta function.
The KDE Model
Anderson1,2 set out to represent whole probability distributions over X rather than
just single values. Activities r represent distribution pr(x) through a linear combination of basis functions tPi(X), ie pr(x) = L:i r~tPi(x) where r~ are normalized
such that pr(x) is a probability distribution. The kernel functions tPi(X) are not
Probabilistic Interpretation of Population Codes
the tuning functions Ji(x) of the cells that would commonly be measured in an
experiment. They need have no neural instantiation; instead, they form part of the
interpretive structure for the population code. If the tPi(X) are probability distributions, and so are positive, then the range of spatial frequencies in P[xlw] that they
can reproduce in pr(x) is likely to be severely limited.
In terms of our framework, the KDE model specifies the method of decoding, and
makes encoding its corollary. Evaluating KDE requires some choice of encoding representing P[xlw] by pr(x) through appropriate r. One way to encode is to use
the Kullback-Leibler divergence as a measure of the discrepancy between P[xlw] and
Ei r~tPi(x) and use the expectation-maximization (EM) algorithm to fit the ira,
treating them as mixing proportions in a mixture mode1.4 This relies on {tPi(X)} being probability distributions themselves. The projection methodl is a one-shot linear
filtering based alternative using the £2 distance. ri are computed as a projection
of P[xlw] onto tuning functions Ji(x) that are calculated from tPj(x).
ri = Ix P[xlw]Ji(x)dx
fi(X) = L Aij1tPj(x)
Aij = Ix tPi (x)tPj (x)dx
Ji(x) are likely to need regularizing, 1 particularly if the tPi(X) overlap substantially.
The Extended Poisson Model
The KDE model is likely to have difficulty capturing in pr(x) probability distributions P[xlw] that include high frequencies, such as delta functions. Conversely, the
standard Poisson model decodes almost any pattern of activities r into something
that rapidly approaches a delta function as the activities increase. Is there any
middle ground?
We extend the standard Poisson encoding model to allow the recorded activities r
to depend on general P[xlw], having Poisson statistics with mean:
(ri) = Ix P[xlw]Ji(x)dx.
This equation is identical to that for the KDE model (Equation 2), except that
variability is built into the Poisson statistics, and decoding is now required to be
the Bayesian inverse of encoding. Note that since ri depends stochastically on
P[xlw], the full Bayesian inverse will specify a distribution P[P[xlw]lr] over possible
distributions. We summarize this by an approximation to its most likely memberwe perform an approximate form of maximum likelihood, not in the value of x, but
in distributions over x. We approximate P[xlw] as a piece-wise constant histogram
which takes the value ¢>j in (xj, Xj+l], and Ji(x) by a piece-wise constant histogram
that take the values Jij in (xj, xj+d. Generally, the maximum a posteriori estimate
for {¢>j} can be shown to be derived by maximizing:
where € is the variance of a smoothness prior. We use a form of EM to maximize
the likelihood and adopt the crude ·approximation of averaging neighboring values
R. S. Zemel, P. Dayan and A. Pouget
Extended Poisson
KDE (Projection)
(r.) = h [I" P(xlw]f.(x)dxj
(r.) = h [R.n •• I" P[xlw]f.(x)dx]
(r.) = h [Rm .. r:J
f;(x) = R.n .. N(x •• u)
f.(x) = L:J Aijl.pj(x)
ri to max. L
A.j = I" .p.(x).pj(x)dx
pr(x) to max. L
pr(x) = L:. ri.p.(x)
pr(x) = L:. r:.p.(x)
ri = I% pr(x)f.(x)dx::::: L:j tPilij
r: = r./ L:J rj
Likelihood
L = log P [{tPi}l{ri}] ::::: L:.r;logf.
L = I" P[xIwJlogpr(x)dx
G = L:. ri log(r;!f.)
E = I" [pr(x) - P[xlwJ] 2 dx
G = I" P[xlw] log ~~X)JdX
Table 1: A summary of the key operations with respect to the framework of the
interpretation methods compared here. hO is a rounding operator to ensure integer
firing rates, and 'l/Ji(X) = N(xi, 0') are the kernel functions for the KDE method.
of ~j on successive iterations. By comparison with the linear decoding of the KDE
method, Equation 4 offers a non-linear way of combining a set of activities {rd to
give a probability distribution pr(x) over the underlying variable x. The computational complexities of Equation 4 are irrelevant, since decoding is only an implicit
operation that the system need never actually perform.
Comparing the Models
We illustrate the various models by showing the faithfulness with which they can
represent two bimodal distributions. We used 0' = 0.3 for the kernel functions
(KDE) and the tuning functions (extended Poisson model) and used 50 units whose
Xi were spaced evenly in the range x = [-10,10]. Table 1 summarizes the three
Figure 2a shows the decoded version of a mixture of two broad Gaussians
1/2N[-2, 1] + 1/2N . Figure 2b shows the same for a mixture of two narrow Gaussians tN[-2, .2] + tN[2, .2]. All the models work well for representing
the broad Gaussians; both forms of the KDE model have difficulty with the narrow Gaussians. The EM version of KDE puts all its weight on the nearest kernel
functions, and so is too broad; the projection version 'rings' in its attempt to represent the narrow components of the distributions. The extended Poisson model
reconstructs with greater fidelity.
Discussion
Informally, we have examined the consequences of the seemingly obvious step of
saying that if a rat, for instance, is uncertain about whether it is at one of two places,
then place cells representing both places could be activated. The complications
Probabilistic Interpretation of Population Codes
O~--~--~--~--~
Figure 2: a) (upper) All three methods provide a good fit to the bimodal Gaussian
distribution when its variance is sufficiently large (7 = 1.0). b) (lower) The KDE
model has difficulty when 7 = 0.2.
come because the structure of the interpretation changes - for instance, one can
no longer think of maximum likelihood methods to extract a single value from the
code directly.
One main fruit of our resulting framework is a method for encoding and decoding
probability distributions that is the natural extension of the (provably inadequate)
standard Poisson model for encoding and decoding single values. Cells have Poisson statistics about a mean determined by the integral of the whole probability
distribution, weighted by the tuning function of the cell. We suggested a particular
decoding model, based on an approximation to maximum likelihood decoding to a
discretized version of the whole probability distribution, and showed that it reconstructs broad, narrow and multimodal distributions more accurately than either the
standard Poisson model or the kernel density model. Stochasticity is built into our
method, since the units are supposed to have Poisson statistics, and it is therefore
also quite robust to noise. The decoding method is not biologically plausible, but
provides a quantitative lower bound to the faithfulness with which a set of activities
can code a distribution.
Stages of processing subsequent to a population code might either extract a single
value from it to control behavior, or integrate it with information represented in
other population codes to form a combined population code. Both operations must
be performed through standard neural operations such as taking non-linear weighted
sums and possibly products of the activities. We are interested in how much information is preserved by such operations, as measured against the non-biological
R. S. Zemel. P. Dayan and A. Pouget
standard of our decoding method. Modeling extraction requires modeling the loss
function - there is some empirical evidence about this from a motion experiment
in which electrical stimulation of MT cells was pitted against input from a moving
stimulus.lO However, much works remains to be done.
Integrating two or more population codes to generate the output in the form of
another population code was stressed by Hinton,6 who noted that it directly relates
to the notion of generalized Hough transforms. We are presently studying how a
system can learn to perform this combination, using the EM-based decoder to generate targets. One special concern for combination is how to understand noise. For
instance, the visual system can be behaviorally extraordinarily sensitive - detecting
just a handful of photons. However, the outputs of real cells at various stages in
the system are apparently quite noisy, with Poisson statistics. If noise is added at
every stage of processing and combination, then the final population code will not
be very faithful to the input. There is much current research on the issue of the
creation and elimination of noise in cortical synapses and neurons.
A last issue that we have not treated here is certainty or magnitude. Hinton's6 idea
of using the sum total activity of a population to code the certainty in the existence
of the quantity they represent is attractive, provided that there is some independent
way of knowing what the scale is for this total. We have used this scaling idea in
both the KDE and the extended Poisson models. In fact, we can go one stage
further, and interpret greater activity still as representing information about the
existence of multiple objects or multiple motions. However, this treatment seems
less appropriate for the place cell system the rat is presumably always certain
that it is somewhere. There it is plausible that the absolute level of activity could
be coding something different, such as the familiarity of a location.
An entire collection of cells is a terrible thing to waste on representing just a single
value of some quantity. Representing a whole probability distribution, at least with
some fidelity, is not more difficult, provided that the interpretation of the encoding
and decoding are clear. We suggest some steps in this direction.