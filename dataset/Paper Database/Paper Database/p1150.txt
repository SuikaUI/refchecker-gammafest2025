KERNEL INDEPENDENT COMPONENT ANALYSIS
Francis R. Bach
Computer Science Division
University of California
Berkeley, CA 94720, USA
 
Michael I. Jordan
Computer Science Division
and Department of Statistics
University of California
Berkeley, CA 94720, USA
 
We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand,
we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of
statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria can be
computed efﬁciently. Minimizing these criteria leads to ﬂexible
and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our
algorithms outperform many of the presently known algorithms.
1. INTRODUCTION
Recent research on kernel methods has yielded important new computational tools for solving large-scale, nonparametric classiﬁcation and regression problems . While some forays have also
been made into unsupervised learning, there is still much unexplored terrain in problems involving large collections of mutually
interacting variables, problems in which Markovian or general
graphical models have excelled. These latter models in fact have
several limitations that invite kernel-based initiatives; in particular,
they are almost entirely based on strong parametric assumptions,
and lack the nonparametric ﬂexibility of the kernel approaches.
Independent component analysis (ICA) is an interesting
unsupervised learning problem in which to explore these issues.
On the one hand, ICA is heavily based on structural assumptions—
viewed as a graphical model it is a directed bipartite graph linking
a set of “source nodes” to a set of “observation nodes,” in which
the lack of edges between the source nodes encodes an assumption
of mutual independence. On the other hand, the ICA problem is
also strongly nonparametric—the distribution of the source variables is left unspeciﬁed. This is difﬁcult to accommodate within
the (current) graphical model formalism, in which all nodes must
be endowed with a probability distribution. It is here that we will
ﬁnd kernel methods to be useful. We will show how kernel methods can be used to deﬁne a “contrast function” that can be used
to estimate the parametric part of the ICA model (the source-toobservation edges), despite the absence of a speciﬁc distribution
on the source nodes. As we will see, compared to current ICA
algorithms, the new kernel-based approach is notable for its robustness.
We refer to our new approach to ICA as “KERNELICA.” It is
important to emphasize at the outset that KERNELICA is not the
“kernelization” of an extant ICA algorithm. Rather, it is a new
approach to ICA based on novel kernel-based measures of dependence. We introduce two such measures. In Section 3, we deﬁne
a kernel-based contrast function in terms of the ﬁrst eigenvalue
of a certain generalized eigenvector problem, and show how this
function relates to probabilistic independence. In Section 4.3, we
introduce an alternative kernel-based contrast function based on
the entire spectrum of the generalized eigenvector problem, and
show how this function can be related to mutual information.
2. BACKGROUND ON ICA
Independent component analysis (ICA) is the problem of recovering a latent random vector x = (x1, . . . , xm)⊤from observations
of m unknown linear functions of that vector. The components of
x are assumed to be mutually independent. Thus, an observation
y = (y1, . . . , ym)⊤is modeled as y = Ax, where x is a latent
random vector with independent components, and where A is an
m×m matrix of parameters. Given N independently, identically
distributed observations of y, we hope to estimate A and thereby
to recover the latent vector x corresponding to any particular y by
solving a linear system.
By specifying distributions for the components xi, one obtains a parametric model that can be estimated via maximum likelihood . Working with W = A−1 as the parameterization, one
readily obtains a gradient or ﬁxed-point algorithm that yields an
estimate ˆ
W and provides estimates of the latent components via
In practical applications, however, one does not generally know
the distributions of the components xi, and it is preferable to view
the ICA model as a semiparametric model in which the distributions of the components of x are left unspeciﬁed . Maximizing the likelihood in the semiparametric ICA model is essentially
equivalent to minimizing the mutual information between the components of the estimate ˆx = ˆ
Wy . Thus it is natural to view
mutual information as a contrast function to be minimized in estimating the ICA model.
Unfortunately, the mutual information for real-valued variables
is difﬁcult to approximate and optimize on the basis of a ﬁnite sample, and much research on ICA has focused on alternative contrast
functions . These have either been derived as expansionbased approximations to the mutual information, or have had a
looser relationship to the mutual information, essentially borrowing its key property of being equal to zero if and only if the arguments to the function are independent. In this paper, we deﬁne
two novel contrast functions. Minimizing them will lead to two
KERNELICA algorithms.
3. MEASURING STATISTICAL DEPENDENCE WITH
In this section, we deﬁne the F-correlation, a measure of statistical
dependence among random variables x1, . . . , xm. For simplicity,
we restrict ourselves initially to the case of two real random variables, x1 and x2, treating the general case of m variables in Section 3.4. (It is also worth noting that the restriction to real random
variables is again for simplicity; a similar measure of dependence
can be deﬁned for any type of data for which Mercer kernels can
be deﬁned).
We assume that we are given a reproducing-kernel Hilbert
space (RKHS) F on R, with kernel K(x, y) and feature map Φ(x).
In this paper, our focus is the Gaussian kernel, K(x, y) =
exp(−(x−y)2/2σ2), which corresponds to an inﬁnite-dimensional
RKHS of smooth functions .
3.1. The F-correlation
Given an RKHS F, we deﬁne the F-correlation as the maximal
correlation between the random variables f1(x1) and f2(x2), where
f1 and f2 range over F:
f1,f2∈F corr(f1(x1), f2(x2))
cov(f1(x1), f2(x2))
(var f1(x1))1/2(var f2(x2))1/2 .
Clearly, if the variables x1 and x2 are independent, then the
F-correlation is equal to zero. Moreover, if the set F is large
enough, the converse is also true. For example, it is well known
that if F contains the Fourier basis (all functions of the form x 7→
eiωx where ω ∈R), then ρF = 0 implies that x1 and x2 are
independent. In , we show that the converse is also true for the
reproducing kernel Hilbert spaces based on Gaussian kernels.
For reasons that will become clear in Section 4.3, it is useful
to work on a logarithmic scale; in particular, we deﬁne our ﬁrst
contrast function as IρF = −1
2 log(1 −ρF). Our converse result
implies that IρF is a valid contrast function; a function that is always nonnegative and equal to zero if and only if the variables x1
and x2 are independent.
The ability to restrict the maximization in Eq. (1) to an RKHS
has an important computational consequence. In particular, we
can exploit the reproducing property, f(x) = ⟨Φ(x), f⟩, to obtain an interpretation of ρF in terms of linear projections. Indeed,
the reproducing property implies that corr(f1(x1), f2(x2)) =
corr (⟨Φ(x1), f1⟩, ⟨Φ(x2), f2⟩) . Consequently, the F-correlation
is the maximal possible correlation between one-dimensional linear projections of Φ(x1) and Φ(x2). This is exactly the deﬁnition
of the ﬁrst canonical correlation between Φ(x1) and Φ(x2).
This interpretation will enable us to derive a computationally efﬁcient algorithm.
3.2. Canonical correlation analysis
Canonical correlation analysis (CCA) is a multivariate statistical
technique similar in spirit to principal component analysis (PCA).
While PCA works with a single random vector and maximizes the
variance of projections of the data, CCA works with a pair of random vectors (or in general with a set of m random vectors) and
maximizes correlation between sets of projections. While PCA
leads to an eigenvector problem, CCA leads to a generalized eigenvector problem. More precisely, given two random vectors, x1
and x2, the ﬁrst canonical correlation between x1 and x2 can be
deﬁned as the maximum possible correlation between the two projections ξ⊤
1 x1 and ξ⊤
2 x2 of x1 and x2:
ξ1,ξ2 corr(ξ⊤
where Cij denotes the covariance matrix cov(xi, xj). By taking
derivatives with respect to ξ1 and ξ2, this problem is easily seen to
reduce to the following generalized eigenvalue problem :
We need to be able to solve this problem in feature space, and thus
we need to consider a “kernelized” version of CCA.
3.3. Estimating the F-correlation
1, . . . , xN
1 } and {x1
2, . . . , xN
2 } denote sets of N empirical
observations of x1 and x2. The observations generate Gram matrices L1 and L2, deﬁned as (Li)ab = K(xa
i). The centered
Gram matrices K1 and K2 are deﬁned as the Gram matrices of the centered (in feature space) data points and are equal to
Ki = PLiP where P = I −1
N 1 is a constant singular matrix (1
is the N ×N matrix composed of ones).
Following the spirit of the derivation of kernel PCA , it
is straightforward to derive a “kernelization” of CCA, which turns
out to involve substituting products of Gram matrices for the covariance matrices in Eq. (3), and maximizing
1 (K1 + NκI/2)2α1)1/2(α⊤
2 (K2 + NκI/2)2α2)1/2 ,
where κ is a small positive regularization parameter. As for CCA
in. Eq. (3), the solution is obtained by solving the following generalized eigenvalue problem (cf. Eq. (3) and (5)):
Since (Ki + κI)2 is necessarily invertible, classical methods can
be invoked to solve the generalized eigenvalue problem in Eq. (6).
Thus kernel CCA reduces to ﬁnding the largest eigenvalue of eKκ =
rκ(K1)rκ(K2)
rκ(K2)rκ(K1)
, with rκ(Ki) = Ki(Ki+κI)−1.
3.4. Generalization to more than two variables
It is straightforward to extend CCA, and its kernelized counterpart, to the case of m variables . The problem becomes that
of ﬁnding the smallest eigenvalue of the generalized eigenvalue
problem Kα = λDα, where K is deﬁned by blocks Kij = KiKj
for i ̸= j and Kii = (Ki + κI)2, and D is block diagonal with
blocks Dii = (Ki + κI)2. We still refer to this eigenvalue as the
F-correlation.1
1See for a detailed explanation of why we use the smallest generalized eigenvalue in our general deﬁnition, and how this accords with our
earlier deﬁnition. In brief, the deﬁnitions are equivalent because of a symmetry property of the eigenvalues for the CCA problem.
It is worth noting that the general version of the F-correlation
that we have deﬁned does not characterize mutual dependence
among m variables, but only characterizes pairwise independence.
Empirically, this does not appear to be a limitation in the ICA setting, as we show in Section 5. However, in situations in which a
measure of mutual independence is required, one can form such a
measure by exploiting the general fact that mutual independence
can be expressed in terms of pairwise mutual information terms involving sets of variables. (Thus, for example, in the three-variable
case we have the expansion I(x, y, z) = I((x, y), z) + I(x, y)).
4. KERNEL INDEPENDENT COMPONENT ANALYSIS
Having deﬁned a contrast function in terms of the solution of a
generalized eigenvalue problem, we now obtain a KERNELICA
algorithm by minimizing this contrast function with respect to the
parameter matrix W.
4.1. Outline of algorithm
Given a set of data vectors y1, y2, . . . , yN, and given a parameter matrix W, we set xi = Wyi, for each i, and thereby form a
set of estimated source vectors {x1, x2, . . . , xN}. The m components of these vectors yield a set of m centered Gram matrices,
K1, K2, . . . , Km. These Gram matrices (which depend on W)
deﬁne the contrast function, C(W) = ˆIρF (K1, . . . , Km), as the
solution to a generalized eigenvalue problem, Kα = λDα, where
K and D are block matrices constructed from the Gram matrices
Ki. The KERNELICA-KCCA algorithm involves minimizing this
function C(W) with respect to W.
4.2. Computational issues
In order to turn this sketch into a practical ICA algorithm, several
computational issues have to be addressed, as we now discuss.
Numerical linear algebra. The F-correlation involves computing the smallest generalized eigenvalue of matrices of size mN.
Thus a naive implementation would scale as O(N 3), a computational complexity whose cubic growth in the number of data points
would be a serious liability in applications to large data sets. However, Gram matrices have a spectrum that tends to show rapid decay, and low-rank approximations of Gram matrices can therefore
often provide sufﬁcient ﬁdelity for the needs of kernel-based algorithms . In , we show theoretically that for a regularization parameter κ that is linear in N, we require low-rank approximations of size M, where M is a constant that is independent of the number N of samples. Since the Gram matrix Ki is
positive semideﬁnite, the low-rank approximation can be found
through incomplete Cholesky decomposition in time O(M 2N),
which gives a M × N matrix Gi such that Ki ≈GiG⊤
perform a singular value decomposition of Gi, in time O(M 2N),
to obtain an N ×M matrix Ui with orthogonal columns (i.e., such
i Ui = I), and an M ×M diagonal matrix Λi such that
i = UiΛiU ⊤
We then have rκ(Ki) = (Ki + κI)−1Ki = UiDiU ⊤
Di is the diagonal matrix obtained from the diagonal matrix Λi by
applying the function λ 7→λ/(λ + κ) to its elements. Finally,
in the two-dimensional case, our problem reduces to ﬁnding the
largest eigenvalue of e
the obvious extension to the m-dimensional case. This problem
can be solved in time linear in N.
Gradient descent on the Stiefel manifold. Since decorrelation implies independence, it is common to enforce decorrelation
of the estimated sources. This is done by whitening the data and
subsequently restricting the minimization to orthogonal matrices
W . The set of orthogonal matrices, which is commonly referred to as the Stiefel manifold, can be equipped with a natural
Riemannian metric, which implies that gradient algorithms can be
used. In our simulations we used steepest descent with line search
along geodesics. The algorithm necessarily converges to a local
minimum of C(W), from any starting point.
The ICA contrast functions have multiple local minima, however, and restarts are generally necessary if we are to ﬁnd the global
optimum. Empirically, the number of restarts that were needed
was found to be small when the number of samples is sufﬁciently
large so as to make the problem well-deﬁned. We have also developed two initialization heuristics that have been found to be
particularly useful in practice for large-scale problems,“one-unit
contrast functions”, and Hermite polynomial kernels. These are
detailed in .
4.3. Kernel generalized variance
The F-correlation is deﬁned as the ﬁrst eigenvalue of the kernelized CCA problem. It is obviously of interest to consider the other
eigenvalues as well. Indeed, there is a classical relationship between the full CCA spectrum and the mutual information of Gaussian variables x1 and x2 : the mutual information I(x1, x2) is
equal to −1
i ). The product Q
i ) is usually
referred to as the generalized variance.
This suggests deﬁning a corresponding quantity for kernelized
CCA. In the case of two variables, we deﬁne the kernel generalized variance (KGV) as the product ˆδF = Q
i ), where
ρi are the (positive) kernel canonical correlations. In the general
case of m variables, we deﬁne ˆδF = det K/ det D. Finally, by
analogy with the mutual information for the Gaussian case, we
also deﬁne a contrast function ˆIδF = −1
2 log ˆδF. It turns out
that ˆIδF (K1, . . . , Km) has as its population counterpart a function IδF (x1, . . . , xm) that is an approximation of the mutual information between the original non-Gaussian variables in the input
space .
5. SIMULATION RESULTS
We have conducted an extensive set of simulation experiments
using data obtained from a variety of source distributions. The
sources that we used (Figure 1, Top) included subgaussian and supergaussian distributions, as well as distributions that are nearly
Gaussian. We studied unimodal, multimodal, symmetric, and nonsymmetric distributions. We also varied the number of components, from 2 to 16, the number of training samples, from 250 to
4000, and studied the robustness of the algorithms to varying numbers of outliers (see for details).
Comparisons were made with three existing ICA algorithms:
the FastICA algorithm , the Jade algorithm , and the extended
Infomax algorithm . All simulations were performed in the situation when the true demixing matrix W0 is known. We measure
the performance of the algorithm in terms of the difference between W and W0, via the standard ICA metric introduced by .
This measure is invariant to permutation and scaling of its arguments, lies between 0 and 100(m −1), and is equal to zero for
perfect demixing.
Fig. 1. (Top) Source density functions. (Bottom) Performance of
ICA algorithms for m = 2. The best performance in each row is
indicated in bold font.
The results in Figure 1 (Bottom) show that the KERNELICA
algorithms are competitive with current algorithms, and are particularly successful at handling asymmetric sources (see, e.g., the
performance for sources j, l and q). In Figure 2 (Top), which reports results for random choices of source distributions, we see
that the KERNELICA algorithms perform well for larger numbers
of components. Finally, in Figure 2 (Bottom), we report the results of an experiment in which we added random outliers to the
source data. We see that our algorithms are particularly resistant
to outliers.
6. CONCLUSIONS
We have presented two novel, kernel-based measures of statistical dependence. These measures can be optimized with respect
to a parameter matrix, yielding new algorithms for ICA. These
algorithms are competitive with current algorithms, and are particularly notable for their resistance to outliers.
Our approach to ICA is more ﬂexible and more demanding
computationally than current algorithms, involving a search in a
reproducing kernel Hilbert space—an inner loop which is not present
in other algorithms. But the problem of measuring (and minimizing) departure from independence over all possible non-Gaussian
source distributions is a difﬁcult one, and we feel that the ﬂexibility provided by our approach is appropriately targeted.
Many other problems at the intersection of graphical models
and nonparametric estimation can also be addressed using these
tools. In particular, in recent work , we have generalized ICA
F-ica Jade Imax
of outliers
Fig. 2. (Top) Performance for larger number of components m.
(Bottom) Performance as a function of the number of outliers.
to a model that no longer requires the sources to be independent,
but requires them only to factorize according to a tree. The departure from a tree distribution can be measured in terms of a sum of
mutual information terms, and approximated using the KGV.
7. REFERENCES
 S. Amari, A. Cichocki, and H. H. Yang.
A new learning
algorithm for blind signal separation. In Adv. in NIPS, 8,
 T. W. Anderson. An Introduction to Multivariate Statistical
Analysis. Wiley & Sons, 1984.
 F. R. Bach and M. I. Jordan. Kernel independent component
analysis. J. of Machine Learning Research, 3:1–48, 2002.
 F. R. Bach and M. I. Jordan.
Tree-dependent component
analysis. In Proc. UAI, 2002.
 A. J. Bell and T. J. Sejnowski. An information-maximization
approach to blind separation and blind deconvolution. Neural
Computation, 7(6):1129–1159, 1995.
 P. J. Bickel, C. A. J. Klaassen, Y. Ritov, and J. A. Wellner. Ef-
ﬁcient and Adaptive Estimation for Semiparametric Models.
Springer-Verlag, 1998.
 J.-F. Cardoso. High-order contrasts for independent component analysis. Neural Computation, 11(1):157–192, 1999.
 A. Hyv¨arinen, J. Karhunen, and E. Oja. Independent Component Analysis. Wiley & Sons, 2001.
 T.-W. Lee, M. Girolami, and T. J. Sejnowski. Independent
component analysis using an extended Infomax algorithm
for mixed sub-gaussian and super-gaussian sources. Neural
Computation, 11(2):417–441, 1999.
 B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT
Press, 2001.