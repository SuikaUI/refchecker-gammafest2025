Machine Learning Testing:
Survey, Landscapes and Horizons
Jie M. Zhang*, Mark Harman, Lei Ma, Yang Liu
Abstract—This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing
(ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components
(e.g., the data, learning program, and framework), testing workﬂow (e.g., test generation and test evaluation), and application scenarios
(e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research
focus, concluding with research challenges and promising research directions in ML testing.
Index Terms—machine learning, software testing, deep neural network,
INTRODUCTION
The prevalent applications of machine learning arouse
natural concerns about trustworthiness. Safety-critical applications such as self-driving systems , and medical
treatments , increase the importance of behaviour relating
to correctness, robustness, privacy, efﬁciency and fairness.
Software testing refers to any activity that aims to detect
the differences between existing and required behaviour .
With the recent rapid rise in interest and activity, testing
has been demonstrated to be an effective way to expose
problems and potentially facilitate to improve the trustworthiness of machine learning systems.
For example, DeepXplore , a differential white-box
testing technique for deep learning, revealed thousands of
incorrect corner case behaviours in autonomous driving
learning systems; Themis , a fairness testing technique
for detecting causal discrimination, detected signiﬁcant ML
model discrimination towards gender, marital status, or race
for as many as 77.2% of the individuals in datasets to which
it was applied.
In fact, some aspects of the testing problem for machine
learning systems are shared with well-known solutions
already widely studied in the software engineering literature. Nevertheless, the statistical nature of machine learning
systems and their ability to make autonomous decisions
raise additional, and challenging, research questions for
software testing , .
Machine learning testing poses challenges that arise
from the fundamentally different nature and construction
of machine learning systems, compared to traditional (relatively more deterministic and less statistically-orientated)
software systems. For instance, a machine learning system
Jie M. Zhang and Mark Harman are with CREST, University College
London, United Kingdom. Mark Harman is also with Facebook London.
E-mail: jie.zhang, 
* Jie M. Zhang is the corresponding author.
Lei Ma is with Kyushu University, Japan.
E-mail: 
Yang Liu is with Nanyang Technological University, Singapore.
E-mail: 
inherently follows a data-driven programming paradigm,
where the decision logic is obtained via a training procedure
from training data under the machine learning algorithm’s
architecture . The model’s behaviour may evolve over
time, in response to the frequent provision of new data .
While this is also true of traditional software systems, the
core underlying behaviour of a traditional system does not
typically change in response to new data, in the way that a
machine learning system can.
Testing machine learning also suffers from a particularly
pernicious instance of the Oracle Problem . Machine learning systems are difﬁcult to test because they are designed
to provide an answer to a question for which no previous
answer exists . As Davis and Weyuker said , for
these kinds of systems ‘There would be no need to write
such programs, if the correct answer were known’. Much
of the literature on testing machine learning systems seeks
to ﬁnd techniques that can tackle the Oracle problem, often
drawing on traditional software testing approaches.
The behaviours of interest for machine learning systems
are also typiﬁed by emergent properties, the effects of which
can only be fully understood by considering the machine
learning system as a whole. This makes testing harder,
because it is less obvious how to break the system into
smaller components that can be tested, as units, in isolation.
From a testing point of view, this emergent behaviour has
a tendency to migrate testing challenges from the unit
level to the integration and system level. For example, low
accuracy/precision of a machine learning model is typically
a composite effect, arising from a combination of the behaviours of different components such as the training data,
the learning program, and even the learning framework/library .
Errors may propagate to become ampliﬁed or suppressed, inhibiting the tester’s ability to decide where the
fault lies. These challenges also apply in more traditional
software systems, where, for example, previous work has
considered failed error propagation , and the subtleties introduced by fault masking , . However,
these problems are far-reaching in machine learning systems, since they arise out of the nature of the machine
learning approach and fundamentally affect all behaviours,
rather than arising as a side effect of traditional data and
control ﬂow .
For these reasons, machine learning systems are thus
sometimes regarded as ‘non-testable’ software. Rising to
these challenges, the literature has seen considerable progress and a notable upturn in interest and activity: Figure 1
shows the cumulative number of publications on the topic
of testing machine learning systems between 2007 and June
2019 (we introduce how we collected these papers in Section 4.2). From this ﬁgure, we can see that 85% of papers
have appeared since 2016, testifying to the emergence of
new software testing domain of interest: machine learning
Number of ML testing publications
Figure 1: Machine Learning Testing Publications (accumulative) during 2007-2019
In this paper, we use the term ‘Machine Learning Testing’ (ML testing) to refer to any activity aimed at detecting differences between existing and required behaviours
of machine learning systems. ML testing is different from
testing approaches that use machine learning or those that
are guided by machine learning, which should be referred
to as ‘machine learning-based testing’. This nomenclature
accords with previous usages in the software engineering
literature. For example, the literature uses the terms ‘statebased testing’ and ‘search-based testing’ , to
refer to testing techniques that make use of concepts of state
and search space, whereas we use the terms ‘GUI testing’
 and ‘unit testing’ to refer to test techniques that
tackle challenges of testing GUIs (Graphical User Interfaces)
and code units.
This paper seeks to provide a comprehensive survey
of ML testing. We draw together the aspects of previous
work that speciﬁcally concern software testing, while simultaneously covering all types of approaches to machine
learning that have hitherto been tackled using testing. The
literature is organised according to four different aspects:
the testing properties (such as correctness, robustness, and
fairness), machine learning components (such as the data,
learning program, and framework), testing workﬂow (e.g.,
test generation, test execution, and test evaluation), and application scenarios (e.g., autonomous driving and machine
translation). Some papers address multiple aspects. For such
papers, we mention them in all the aspects correlated (in
different sections). This ensures that each aspect is complete.
Additionally, we summarise research distribution (e.g.,
categories),
trends, and datasets. We also identify open problems and
challenges for the emerging research community working
at the intersection between techniques for software testing
and problems in machine learning testing. To ensure that
our survey is self-contained, we aimed to include sufﬁcient
material to fully orientate software engineering researchers
who are interested in testing and curious about testing
techniques for machine learning applications. We also seek
to provide machine learning researchers with a complete
survey of software testing solutions for improving the trustworthiness of machine learning systems.
There has been previous work that discussed or surveyed aspects of the literature related to ML testing. Hains et
al. , Ma et al. , and Huang et al. surveyed secure
deep learning, in which the focus was deep learning security
with testing being one of the assurance techniques. Masuda
et al. outlined their collected papers on software quality
for machine learning applications in a short paper. Ishikawa discussed the foundational concepts that might
be used in any and all ML testing approaches. Braiek and
Khomh discussed defect detection in machine learning
data and/or models in their review of 39 papers. As far as
we know, no previous work has provided a comprehensive
survey particularly focused on machine learning testing.
In summary, the paper makes the following contributions:
1) Deﬁnition. The paper deﬁnes Machine Learning Testing (ML testing), overviewing the concepts, testing work-
ﬂow, testing properties, and testing components related to
machine learning testing.
2) Survey. The paper provides a comprehensive survey of 144 machine learning testing papers, across various
publishing areas such as software engineering, artiﬁcial
intelligence, systems and networking, and data mining.
3) Analyses. The paper analyses and reports data on
the research distribution, datasets, and trends that characterise the machine learning testing literature. We observed
a pronounced imbalance in the distribution of research
efforts: among the 144 papers we collected, around 120
of them tackle supervised learning testing, three of them
tackle unsupervised learning testing, and only one paper
tests reinforcement learning. Additionally, most of them (93)
centre on correctness and robustness, but only a few papers
test interpretability, privacy, or efﬁciency.
4) Horizons. The paper identiﬁes challenges, open problems, and promising research directions for ML testing, with
the aim of facilitating and stimulating further research.
Figure 2 depicts the paper structure. More details of the
review schema can be found in Section 4.
PRELIMINARIES OF MACHINE LEARNING
This section reviews the fundamental terminology in machine learning so as to make the survey self-contained.
Machine Learning (ML) is a type of artiﬁcial intelligence technique that makes decisions or predictions from
data , . A machine learning system is typically
Introduction
Organising
Comparison
Directions
Preliminaries of
Definition
Comparison with software testing
Contents of
How to test: MLT workflow
Where to test: MLT components
What to test: MLT properties
(how to test)
properties
(what to test)
Test input generation
Test oracle generation
Test adequacy evaluation
Bug report analysis
Debug and repair
Correctness
Model Relevance
Robustness&Security
Efficiency
Interpretability
Application
components
Data testing
Learning program testing
Framework testing
Autonomous driving
Machine translation
Natural language inference
Machine learning categories
Open-source tools
Machine learning properties
Test prioritisation and reduction
Figure 2: Tree structure of the contents in this paper
composed from following elements or terms. Dataset: A set
of instances for building or evaluating a machine learning
At the top level, the data could be categorised as:
• Training data: the data used to ‘teach’ (train) the algorithm to perform its task.
• Validation data: the data used to tune the hyperparameters of a learning algorithm.
• Test data: the data used to validate machine learning
model behaviour.
Learning program: the code written by developers to build
and validate the machine learning system.
Framework: the library, or platform being used when building a machine learning model, such as Pytorch , Tensor-
Flow , Scikit-learn , Keras , and Caffe .
In the remainder of this section, we give deﬁnitions for
other ML terminology used throughout the paper.
Instance: a piece of data recording the information about an
Feature: a measurable property or characteristic of a phenomenon being observed to describe the instances.
Label: value or category assigned to each data instance.
Test error: the difference ratio between the real conditions
and the predicted conditions.
Generalisation error: the expected difference ratio between
the real conditions and the predicted conditions of any valid
Model: the learned machine learning artefact that encodes
decision or prediction logic which is trained from the training data, the learning program, and frameworks.
There are different types of machine learning. From the
perspective of training data characteristics, machine learning includes:
Supervised learning: a type of machine learning that learns
from training data with labels as learning targets. It is the
most widely used type of machine learning .
Unsupervised learning: a learning methodology that learns
from training data without labels and relies on understanding the data itself.
Reinforcement learning: a type of machine learning where
the data are in the form of sequences of actions, observations, and rewards, and the learner learns how to take actions to interact in a speciﬁc environment so as to maximise
the speciﬁed rewards.
Let X = (x1, ..., xm) be the set of unlabelled training
data. Let Y = (c(x1), ..., c(xm)) be the set of labels corresponding to each piece of training data xi. Let concept
C : X →Y be the mapping from X to Y (the real pattern).
The task of supervised learning is to learn a mapping
pattern, i.e., a model, h based on X and Y so that the learned
model h is similar to its true concept C with a very small
generalisation error. The task of unsupervised learning is to
learn patterns or clusters from the data without knowing
the existence of labels Y.
Reinforcement learning guides and plans with the
learner actively interacting with the environment to achieve
a certain goal. It is usually modelled as a Markov decision
process. Let S be a set of states, A be a series of actions.
Let s and s′ be two states. Let ra(s, s′) be the reward after
transition from s to s′ with action a ∈A. Reinforcement
learning is to learn how to take actions in each step to
maximise the target awards.
Machine learning can be applied to the following typical
tasks 1:
1) Classiﬁcation: to assign a category to each data instance; E.g., image classiﬁcation, handwriting recognition.
2) Regression: to predict a value for each data instance;
E.g., temperature/age/income prediction.
3) Clustering: to partition instances into homogeneous
regions; E.g., pattern recognition, market/image segmentation.
4) Dimension reduction: to reduce the training complexity; E.g., dataset representation, data pre-processing.
5) Control: to control actions to maximise rewards; E.g.,
game playing.
Figure 3 shows the relationship between different categories of machine learning and the ﬁve machine learning
tasks. Among the ﬁve tasks, classiﬁcation and regression
belong to supervised learning; Clustering and dimension
reduction belong to unsupervised learning. Reinforcement
learning is widely adopted to control actions, such as to
control AI-game players to maximise the rewards for a game
In addition, machine learning can be classiﬁed into classic machine learning and deep learning. Algorithms like
Decision Tree , SVM , linear regression , and
Naive Bayes all belong to classic machine learning.
1 These tasks are deﬁned based on the nature of the problems solved
instead of speciﬁc application scenarios such as language modelling.
machine learning
supervised
unsupervised
reinforcement
classification
regression
clustering
Figure 3: Machine learning categories and tasks
Deep learning applies Deep Neural Networks (DNNs)
that uses multiple layers of nonlinear processing units for
feature extraction and transformation. Typical deep learning
algorithms often follow some widely used neural network
structures like Convolutional Neural Networks (CNNs) 
and Recurrent Neural Networks (RNNs) . The scope of
this paper involves both classic machine learning and deep
MACHINE LEARNING TESTING
This section gives a deﬁnition and analyses of ML testing.
It describes the testing workﬂow (how to test), testing
properties (what to test), and testing components (where
A software bug refers to an imperfection in a computer
program that causes a discordance between the existing
and the required conditions . In this paper, we refer the
term ‘bug’ to the differences between existing and required
behaviours of an ML system2.
Deﬁnition 1 (ML Bug). An ML bug refers to any imperfection in a machine learning item that causes a discordance
between the existing and the required conditions.
We deﬁne ML testing as any activity aimed to detect ML
Deﬁnition 2 (ML Testing). Machine Learning Testing (ML
testing) refers to any activity designed to reveal machine
learning bugs.
The deﬁnitions of machine learning bugs and ML testing
indicate three aspects of machine learning: the required
conditions, the machine learning items, and the testing
activities. A machine learning system may have different
types of ‘required conditions’, such as correctness, robustness, and privacy. An ML bug may exist in the data, the
learning program, or the framework. The testing activities
may include test input generation, test oracle identiﬁcation,
test adequacy evaluation, and bug triage. In this survey, we
refer to the above three aspects as testing properties, testing
components, and testing workﬂow, respectively, according
to which we collect and organise the related work.
Note that a test input in ML testing can be much more
diverse in its form than that used in traditional software
2 The existing related papers may use other terms like ‘defect’ or ‘issue’.
This paper uses ‘bug’ as a representative of all such related terms
considering that ‘bug’ has a more general meaning .
testing, because it is not only the code that may contain
bugs, but also the data. When we try to detect bugs in data,
one may even use a training program as a test input to check
some properties required for the data.
ML Testing Workﬂow
ML testing workﬂow is about how to conduct ML testing
with different testing activities. In this section, we ﬁrst
brieﬂy introduce the role of ML testing when building
ML models, then present the key procedures and activities
in ML testing. We introduce more details of the current
research related to each procedure in Section 5.
Role of Testing in ML Development
Figure 4 shows the life cycle of deploying a machine learning system with ML testing activities involved. At the very
beginning, a prototype model is generated based on historical data; before deploying the model online, one needs
to conduct ofﬂine testing, such as cross-validation, to make
sure that the model meets the required conditions. After
deployment, the model makes predictions, yielding new
data that can be analysed via online testing to evaluate how
the model interacts with user behaviours.
There are several reasons that make online testing essential. First, ofﬂine testing usually relies on test data, while test
data usually fails to fully represent future data ; Second,
ofﬂine testing is not able to test some circumstances that
may be problematic in real applied scenarios, such as data
loss and call delays. In addition, ofﬂine testing has no access
to some business metrics such as open rate, reading time,
and click-through rate.
In the following, we present an ML testing workﬂow
adapted from classic software testing workﬂows. Figure 5
shows the workﬂow, including both ofﬂine testing and
online testing.
Ofﬂine Testing
The workﬂow of ofﬂine testing is shown by the top dotted
rectangle of Figure 5. At the very beginning, developers
need to conduct requirement analysis to deﬁne the expectations of the users for the machine learning system under test.
In requirement analysis, speciﬁcations of a machine learning
system are analysed and the whole testing procedure is
planned. After that, test inputs are either sampled from the
collected data or generated based on a speciﬁc purpose. Test
oracles are then identiﬁed or generated (see Section 5.2 for
more details of test oracles in machine learning). When the
tests are ready, they need to be executed for developers to
collect results. The test execution process involves building
a model with the tests (when the tests are training data) or
running a built model against the tests (when the tests are
test data), as well as checking whether the test oracles are
violated. After the process of test execution, developers may
use evaluation metrics to check the quality of tests, i.e., the
ability of the tests to expose ML problems.
The test execution results yield a bug report to help
developers to duplicate, locate, and solve the bug. Those
identiﬁed bugs will be labelled with different severity and
assigned for different developers. Once the bug is debugged
and repaired, regression testing is conducted to make sure
historical
predictions
user request
online data
Figure 4: Role of ML testing in ML system development
the repair solves the reported problem and does not bring
new problems. If no bugs are identiﬁed, the ofﬂine testing
process ends, and the model is deployed.
Online Testing
Ofﬂine testing tests the model with historical data without
in the real application environment. It also lacks the data
collection process of user behaviours. Online testing complements the shortage of ofﬂine testing, and aims to detect
bugs after the model is deployed online.
The workﬂow of online testing is shown by the bottom of
Figure 5. There are different methods of conducting online
testing for different purposes. For example, runtime monitoring keeps checking whether the running ML systems
meet the requirements or violate some desired runtime
properties. Another commonly used scenario is to monitor
user responses, based on which to ﬁnd out whether the new
model is superior to the old model under certain application
contexts. A/B testing is one typical type of such online
testing . It splits customers to compare two versions of
the system (e.g., web pages). When performing A/B testing
on ML systems, the sampled users will be split into two
groups using the new and old ML models separately.
MAB (Multi-Armed Bandit) is another online testing
approach . It ﬁrst conducts A/B testing for a short time
and ﬁnds out the best model, then put more resources on
the chosen model.
ML Testing Components
To build a machine learning model, an ML software developer usually needs to collect data, label the data, design
learning program architecture, and implement the proposed
architecture based on speciﬁc frameworks. The procedure of
machine learning model development requires interaction
with several components such as data, learning program,
and learning framework, while each component may contain bugs.
Figure 6 shows the basic procedure of building an ML
model and the major components involved in the process.
Data are collected and pre-processed for use; the learning
program is the code for running to train the model; the
framework (e.g., Weka, scikit-learn, and TensorFlow) offers
algorithms and other libraries for developers to choose
from, when writing the learning program.
Thus, when conducting ML testing, developers may
need to try to ﬁnd bugs in every component including
the data, the learning program, and the framework. In
particular, error propagation is a more serious problem in
ML development because the components are more closely
bonded with each other than traditional software , which
indicates the importance of testing each of the ML components. We introduce the bug detection in each ML component
Bug Detection in Data. The behaviours of a machine learning system largely depends on data . Bugs in data affect
the quality of the generated model, and can be ampliﬁed
to yield more serious problems over a period a time .
Bug detection in data checks problems such as whether
the data is sufﬁcient for training or test a model (also
called completeness of the data ), whether the data is
representative of future data, whether the data contains a
lot of noise such as biased labels, whether there is skew
between training data and test data , and whether there
is data poisoning or adversary information that may
affect the model’s performance.
Bug Detection in Frameworks. Machine Learning requires a
lot of computations. As shown by Figure 6, ML frameworks
offer algorithms to help write the learning program, and
platforms to help train the machine learning model, making
it easier for developers to build solutions for designing,
training and validating algorithms and models for complex problems. They play a more important role in ML
development than in traditional software development. ML
Framework testing thus checks whether the frameworks of
machine learning have bugs that may lead to problems in
the ﬁnal system .
Bug Detection in Learning Program. A learning program
can be classiﬁed into two parts: the algorithm designed
by the developer or chosen from the framework, and the
actual code that developers write to implement, deploy, or
conﬁgure the algorithm. A bug in the learning program
may arise either because the algorithm is designed, chosen,
or conﬁgured improperly, or because the developers make
typos or errors when implementing the designed algorithm.
ML Testing Properties
Testing properties refer to what to test in ML testing:
for what conditions ML testing needs to guarantee for a
trained model. This section lists some typical properties
that the literature has considered. We classiﬁed them into
basic functional requirements (i.e., correctness and model
relevance) and non-functional requirements (i.e., efﬁciency,
robustness3, fairness, interpretability).
These properties are not strictly independent of each
other when considering the root causes, yet they are different external manifestations of the behaviours of an ML
system and deserve being treated independently in ML
Correctness
Correctness measures the probability that the ML system
under test ‘gets things right’.
Deﬁnition 3 (Correctness). Let D be the distribution of
future unknown data. Let x be a data item belonging
to D. Let h be the machine learning model that we are
3 we adopt the more general understanding from software engineering
community , , and regard robustness as a non-functional requirement.
test generation
training/prediction
identified
bug triage
regression testing
generation
requirements
test input
generation
test oracle
generation
test evaluation
test metric
user split
metric value
collection
deployment
offline testing
online testing
Figure 5: Idealised Workﬂow of ML testing
data collection
data preparation
algorithm selection/design
learning program generation
model training
LEARNING PROGRAM
Figure 6: Components (shown by the grey boxes) involved
in ML model building.
testing. h(x) is the predicted label of x, c(x) is the true
label. The model correctness E(h) is the probability that
h(x) and c(x) are identical:
E(h) = Prx∼D[h(x) = c(x)]
Achieving acceptable correctness is the fundamental requirement of an ML system. The real performance of an
ML system should be evaluated on future data. Since future
data are often not available, the current best practice usually
splits the data into training data and test data (or training
data, validation data, and test data), and uses test data to
simulate future data. This data split approach is called crossvalidation.
Deﬁnition 4 (Empirical Correctness). Let X = (x1, ..., xm)
be the set of unlabelled test data sampled from D.
Let h be the machine learning model under test. Let
Y′ = (h(x1), ..., h(xm)) be the set of predicted labels corresponding to each training item xi. Let Y = (y1, ..., ym)
be the true labels, where each yi ∈Y corresponds to
the label of xi ∈X . The empirical correctness of model
(denoted as ˆE(h)) is:
I(h(xi) = yi)
where I is the indicator function; a predicate returns 1 if p is
true, and returns 0 otherwise.
Model Relevance
A machine learning model comes from the combination
of a machine learning algorithm and the training data. It
is important to ensure that the adopted machine learning
algorithm be not over-complex than just needed . Otherwise, the model may fail to have good performance on
future data, or have very large uncertainty.
The algorithm capacity represents the number of functions that a machine learning model can select (based on
the training data at hand) as a possible solution. It is
usually approximated by VC-dimension or Rademacher
Complexity for classiﬁcation tasks. VC-dimension is the
cardinality of the largest set of points that the algorithm
can shatter. Rademacher Complexity is the cardinality of
the largest set of training data with ﬁxed features that the
algorithm shatters.
We deﬁne the relevance between a machine learning
algorithm capacity and the data distribution as the problem
of model relevance.
Deﬁnition 5 (Model Relevance). Let D be the training
data distribution. Let R(D, A) be the simplest required
capacity of any machine learning algorithm A for D.
R′(D, A′) is the capacity of the machine learning algorithm A′ under test. Model relevance is the difference
between R(D, A) and R′(D, A′).
f = |(R(D, A) −R′(D, A′)|
Model relevance aims to measure how well a machine
learning algorithm ﬁts the data. A low model relevance
is usually caused by overﬁtting, where the model is too
complex for the data, which thereby fails to generalise to
future data or to predict observations robustly.
Of course, R′(D, A), the minimum complexity that is
‘just sufﬁcient’ is hard to determine, and is typically approximate , . We discuss more strategies that could
help alleviate the problem of overﬁtting in Section 6.2.
Robustness
Robustness is deﬁned by the IEEE standard glossary of
software engineering terminology , as: ‘The degree
to which a system or component can function correctly in
the presence of invalid inputs or stressful environmental
conditions’. Adopting a similar spirit to this deﬁnition, we
deﬁne the robustness of ML as follows:
Deﬁnition 6 (Robustness). Let S be a machine learning
system. Let E(S) be the correctness of S. Let δ(S)
be the machine learning system with perturbations on
any machine learning components such as the data, the
learning program, or the framework. The robustness
of a machine learning system is a measurement of the
difference between E(S) and E(δ(S)):
r = E(S) −E(δ(S))
Robustness thus measures the resilience of an ML system’s correctness in the presence of perturbations.
A popular sub-category of robustness is called adversarial
robustness. For adversarial robustness, the perturbations are
designed to be hard to detect. Following the work of Katz
et al. , we classify adversarial robustness into local
adversarial robustness and global adversarial robustness.
Local adversarial robustness is deﬁned as follows.
Deﬁnition 7 (Local Adversarial Robustness). Let x a test
input for an ML model h. Let x′ be another test input
generated via conducting adversarial perturbation on x.
Model h is δ-local robust at input x if for any x′.
∀x′ : ||x −x′||p≤δ →h(x) = h(x′)
||·||p represents p-norm for distance measurement. The
commonly used p cases in machine learning testing are 0, 2,
and ∞. For example, when p = 2, i.e. ||x −x′||2 represents
the Euclidean distance of x and x′. In the case of p = 0,
it calculates the element-wise difference between x and x′.
When p = ∞, it measures the the largest element-wise
distance among all elements of x and x′.
Local adversarial robustness concerns the robustness at
one speciﬁc test input, while global adversarial robustness
measures robustness against all inputs. We deﬁne global
adversarial robustness as follows.
Deﬁnition 8 (Global Adversarial Robustness). Let x a test
input for an ML model h. Let x′ be another test input
generated via conducting adversarial perturbation on x.
Model h is ϵ-global robust if for any x and x′.
∀x, x′ : ||x −x′||p≤δ →h(x) −h(x′) ≤ϵ
The security of an ML system is the system’s resilience
against potential harm, danger, or loss made via manipulating or illegally accessing ML components.
Security and robustness are closely related. An ML system with low robustness may be insecure: if it is less robust
in resisting the perturbations in the data to predict, the
system may more easily fall victim to adversarial attacks;
For example, if it is less robust in resisting training data perturbations, it may also be vulnerable to data poisoning (i.e.,
changes to the predictive behaviour caused by adversarially
modifying the training data).
Nevertheless, low robustness is just one cause of security
vulnerabilities. Except for perturbations attacks, security
issues also include other aspects such as model stealing or
extraction. This survey focuses on the testing techniques
on detecting ML security problems, which narrows the
security scope to robustness-related security. We combine
the introduction of robustness and security in Section 6.3.
Data Privacy
Privacy in machine learning is the ML system’s ability to
preserve private data information. For the formal deﬁnition,
we use the most popular differential privacy taken from the
work of Dwork .
Deﬁnition 9 (ϵ-Differential Privacy). Let A be a randomised
algorithm. Let D1 and D2 be two training data sets that
differ only on one instance. Let S be a subset of the
output set of A. A gives ϵ-differential privacy if
Pr[A(D1) ∈S] ≤exp(ϵ) ∗Pr[A(D2) ∈S]
In other words, ϵ-Differential privacy is a form of ϵcontained bound on output change in responding to single
input change. It provides a way to know whether any one
individual’s data has has a signiﬁcant effect (bounded by ϵ)
on the outcome.
Data privacy has been regulated by law makers,
for example, the EU General Data Protection Regulation (GDPR) and California Consumer Privacy Act
(CCPA) . Current research mainly focuses on how to
present privacy-preserving machine learning, instead of
detecting privacy violations. We discuss privacy-related research opportunities and research directions in Section 10.
The efﬁciency of a machine learning system refers to its
construction or prediction speed. An efﬁciency problem
happens when the system executes slowly or even inﬁnitely
during the construction or the prediction phase.
With the exponential growth of data and complexity of
systems, efﬁciency is an important feature to consider for
model selection and framework selection, sometimes even
more important than accuracy . For example, to deploy
a large model to a mobile device, optimisation, compression, and device-oriented customisation may be performed
to make it feasible for the mobile device execution in a
reasonable time, but accuracy may sacriﬁce to achieve this.
Machine learning is a statistical method and is widely
adopted to make decisions, such as income prediction and
medical treatment prediction. Machine learning tends to
learn what humans teach it (i.e., in form of training data).
However, humans may have bias over cognition, further
affecting the data collected or labelled and the algorithm
designed, leading to bias problems.
The characteristics that are sensitive and need to be
protected against unfairness are called protected characteristics or protected attributes and sensitive attributes. Examples of legally recognised protected classes include race,
colour, sex, religion, national origin, citizenship, age, pregnancy, familial status, disability status, veteran status, and
genetic information.
Fairness is often domain speciﬁc. Regulated domains
include credit, education, employment, housing, and public
accommodation4.
To formulate fairness is the ﬁrst step to solve the fairness
problems and build fair machine learning models. The literature has proposed many deﬁnitions of fairness but no ﬁrm
consensus is reached at this moment. Considering that the
deﬁnitions themselves are the research focus of fairness in
machine learning, we discuss how the literature formulates
and measures different types of fairness in Section 6.5.
Interpretability
Machine learning models are often applied to assist/make
decisions in medical treatment, income prediction, or personal credit assessment. It may be important for humans
to understand the ‘logic’ behind the ﬁnal decisions, so that
they can build trust over the decisions made by ML ,
 , .
The motives and deﬁnitions of interpretability are diverse and still somewhat discordant . Nevertheless, unlike fairness, a mathematical deﬁnition of ML interpretability remains elusive . Referring to the work of Biran and
Cotton as well as the work of Miller , we describe
the interpretability of ML as the degree to which an observer
can understand the cause of a decision made by an ML
Interpretability contains two aspects: transparency (how
the model works) and post hoc explanations (other information that could be derived from the model) . Interpretability is also regarded as a request by regulations like
GDPR , where the user has the legal ‘right to explanation’ to ask for an explanation of an algorithmic decision that
was made about them. A thorough introduction of ML interpretability can be referred to in the book of Christoph .
Software Testing vs. ML Testing
Traditional software testing and ML testing are different
in many aspects. To understand the unique features of
ML testing, we summarise the primary differences between
traditional software testing and ML testing in Table 1.
1) Component to test (where the bug may exist): traditional
software testing detects bugs in the code, while ML testing
detects bugs in the data, the learning program, and the
framework, each of which play an essential role in building
an ML model.
2) Behaviours under test: the behaviours of traditional
software code are usually ﬁxed once the requirement is
ﬁxed, while the behaviours of an ML model may frequently
change as the training data is updated.
3) Test input: the test inputs in traditional software testing
are usually the input data when testing code; in ML testing,
however, the test inputs in may have more diverse forms.
Note that we separate the deﬁnition of ‘test input’ and
‘test data’. In particular, we use ‘test input’ to refer to the
inputs in any form that can be adopted to conduct machine
learning testing; while ‘test data’ specially refers to the
data used to validate ML model behaviour (see more in
Section 2). Thus, test inputs in ML testing could be, but are
4 To prohibit discrimination ‘in a place of public accommodation on the
basis of sexual orientation, gender identity, or gender expression’ .
not limited to, test data. When testing the learning program,
a test case may be a single test instance from the test data or
a toy training set; when testing the data, the test input could
be a learning program.
4) Test oracle: traditional software testing usually assumes
the presence of a test oracle. The output can be veriﬁed
against the expected values by the developer, and thus the
oracle is usually determined beforehand. Machine learning,
however, is used to generate answers based on a set of input
values after being deployed online. The correctness of the
large number of generated answers is typically manually
conﬁrmed. Currently, the identiﬁcation of test oracles remains challenging, because many desired properties are dif-
ﬁcult to formally specify. Even for a concrete domain speciﬁc
problem, the oracle identiﬁcation is still time-consuming
and labour-intensive, because domain-speciﬁc knowledge
is often required. In current practices, companies usually
rely on third-party data labelling companies to get manual
labels, which can be expensive. Metamorphic relations 
are a type of pseudo oracle adopted to automatically mitigate the oracle problem in machine learning testing.
5) Test adequacy criteria: test adequacy criteria are used
to provide quantitative measurement on the degree of
the target software that has been tested. Up to present,
many adequacy criteria are proposed and widely adopted
in industry, e.g., line coverage, branch coverage, dataﬂow
coverage. However, due to fundamental differences of programming paradigm and logic representation format for
machine learning software and traditional software, new
test adequacy criteria are required to take the characteristics
of machine learning software into consideration.
6) False positives in detected bugs: due to the difﬁculty in
obtaining reliable oracles, ML testing tends to yield more
false positives in the reported bugs.
7) Roles of testers: the bugs in ML testing may exist not
only in the learning program, but also in the data or the
algorithm, and thus data scientists or algorithm designers
could also play the role of testers.
PAPER COLLECTION AND REVIEW SCHEMA
This section introduces the scope, the paper collection approach, an initial analysis of the collected papers, and the
organisation of our survey.
Survey Scope
An ML system may include both hardware and software.
The scope of our paper is software testing (as deﬁned in the
introduction) applied to machine learning.
We apply the following inclusion criteria when collecting
papers. If a paper satisﬁes any one or more of the following
criteria, we will include it. When speaking of related ‘aspects
of ML testing’, we refer to the ML properties, ML components, and ML testing procedure introduced in Section 2.
1) The paper introduces/discusses the general idea of ML
testing or one of the related aspects of ML testing.
2) The paper proposes an approach, study, or tool/framework that targets testing one of the ML properties or components.
3) The paper presents a dataset or benchmark especially
designed for the purpose of ML testing.
Table 1: Comparison between Traditional Software Testing and ML Testing
Characteristics
Traditional Testing
ML Testing
Component to test
data and code (learning program, framework)
Behaviour under test
usually ﬁxed
change overtime
Test input
input data
data or code
Test oracle
deﬁned by developers
deﬁned by developers and labelling companies
Adequacy criteria
coverage/mutation score
False positives in bugs
data scientist, algorithm designer, developer
4) The paper introduces a set of measurement criteria that
could be adopted to test one of the ML properties.
Some papers concern traditional validation of ML model
performance such as the introduction of precision, recall,
and cross-validation. We do not include these papers because they have had a long research history and have been
thoroughly and maturely studied. Nevertheless, for completeness, we include the knowledge when introducing the
background to set the context. We do not include the papers
that adopt machine learning techniques for the purpose
of traditional software testing and also those target ML
problems, which do not use testing techniques as a solution.
Some recent papers also target the formal guarantee on
the desired properties of a machine learning system, i.e.,
to formally verify the correctness of the machine learning
systems as well as other properties. Testing and veriﬁcation
of machine learning, as in traditional testing, have their own
advantages and disadvantages. For example, veriﬁcation
usually requires a white-box scenario, but suffers from poor
scalability, while testing may scale, but lacks completeness.
The size of the space of potential behaviours may render
current approaches to veriﬁcation infeasible in general ,
but speciﬁc safety critical properties will clearly beneﬁt
from focused research activity on scalable veriﬁcation, as
well as testing. In this survey, we focus on the machine
learning testing. More details for the literature review of
the veriﬁcation of machine learning systems can be found
in the recent work of Xiang et al. .
Paper Collection Methodology
To collect the papers across different research areas as much
as possible, we started by using exact keyword searching on
popular scientiﬁc databases including Google Scholar, DBLP
and arXiv one by one. The keywords used for searching
are listed below. [ML properties] means the set of ML
testing properties including correctness, model relevance,
robustness, efﬁciency, privacy, fairness, and interpretability.
We used each element in this set plus ‘test’ or ‘bug’ as
the search query. Similarly, [ML components] denotes the
set of ML components including data, learning program/code, and framework/library. Altogether, we conducted
(3 ∗3 + 6 ∗2 + 3 ∗2) ∗3 = 81 searches across the three
repositories before May 15th, 2019.
• machine learning + test|bug|trustworthiness
• deep learning + test|bug|trustworthiness
• neural network + test|bug|trustworthiness
• [ML properties]+ test|bug
• [ML components]+ test|bug
Machine learning techniques have been applied in various domains across different research areas. As a result,
Table 2: Paper Query Results
machine learning test
machine learning bug
machine learning trustworthiness
deep learning test
deep learning bug
deep learning trustworthiness
neural network test
neural network bug
neural network trustworthiness
[ML properties]+test
[ML properties]+bug
[ML components]+test
[ML components]+bug
Author feedback
authors may tend to use very diverse terms. To ensure a
high coverage of ML testing related papers, we therefore
also performed snowballing on each of the related
papers found by keyword searching. We checked the related
work sections in these studies and continue adding the
related work that satisﬁes the inclusion criteria introduced
in Section 4.1, until we reached closure.
To ensure a more comprehensive and accurate survey,
we emailed the authors of the papers that were collected via
query and snowballing, and let them send us other papers
they are aware of which are related with machine learning
testing but have not been included yet. We also asked them
to check whether our description about their work in the
survey was accurate and correct.
Collection Results
Table 2 shows the details of paper collection results. The
papers collected from Google Scholar and arXiv turned out
to be subsets of those from DBLP so we only present the
results of DBLP. Keyword search and snowballing resulted
in 109 papers across six research areas till May 15th, 2019.
We received over 50 replies from all the cited authors
until June 4th, 2019, and added another 35 papers when
dealing with the author feedback. Altogether, we collected
144 papers.
Figure 7 shows the distribution of papers published in
different research venues. Among all the papers, 38.2% papers are published in software engineering venues such as
ICSE, FSE, ASE, ICST, and ISSTA; 6.9% papers are published
in systems and network venues; surprisingly, only 19.4%
of the total papers are published in artiﬁcial intelligence
venues such as AAAI, CVPR, and ICLR. Additionally, 22.9%
of the papers have not yet been published via peer-reviewed
venues (the arXiv part).
Systems and Network
Artificial Intelligence
Data Mining
Software Engineering
Figure 7: Publication Venue Distribution
Paper Organisation
We present the literature review from two aspects: 1) a
literature review of the collected papers, 2) a statistical
analysis of the collected papers, datasets, and tools. The
sections and the corresponding contents are presented in
Table 3: Review Schema
Classiﬁcation
Testing Workﬂow
Test Input Generation
Test Oracle
Test Adequacy
Test Prioritisation and Reduction
Bug Report Analysis
Debug and Repair
Testing Framework and Tools
Testing Properties
Correctness
Model Revelance
Robustness and Security
Interpretability
Testing Components
Bug Detection in Data
Bug Detection in Learning Program
Bug Detection in Framework
Application Scenario
Autonomous Driving
Machine Translation
Natural Language Inference
Summary&Analysis
Research Distribution among Categories
Research Distribution among Properties
Open-source Tool Support
1) Literature Review. The papers in our collection are
organised and presented from four angles. We introduce
the work about different testing workﬂow in Section 5. In
Section 6 we classify the papers based on the ML problems
they target, including functional properties like correctness
and model relevance and non-functional properties like
robustness, fairness, privacy, and interpretability. Section 7
introduces the testing technologies on detecting bugs in
data, learning programs, and ML frameworks, libraries,
or platforms. Section 8 introduces the testing techniques
applied in particular application scenarios such as autonomous driving and machine translation.
The four aspects have different focuses of ML testing,
each of which is a complete organisation of the total collected papers (see more discussion in Section 3.1), as a single
ML testing paper may ﬁt multiple aspects if being viewed
from different angles.
2) Statistical Analysis and Summary. We analyse and
compare the number of research papers on different machine learning categories (supervised/unsupervised/reinforcement learning), machine learning structures (classic/deep learning), testing properties in Section 9. We also
summarise the datasets and tools adopted so far in ML
The four different angles for presentation of related work
as well as the statistical summary, analysis, and comparison,
enable us to observe the research focus, trend, challenges,
opportunities, and directions of ML testing. These results
are presented in Section 10.
ML TESTING WORKFLOW
This section organises ML testing research based on the
testing workﬂow as shown by Figure 5.
ML testing includes ofﬂine testing and online testing.
Albarghouthi and Vinitsky developed a fairness speciﬁcation language that can be used for the development
of run-time monitoring, in detecting fairness issues. Such a
kind of run-time monitoring belongs to the area of online
testing. Nevertheless, current research mainly centres on
ofﬂine testing as introduced below. The procedures that are
not covered based on our paper collection, such as requirement analysis and regression testing and those belonging
to online testing are discussed as research opportunities in
Section 10.
Test Input Generation
We organise the test input generation research based on the
techniques adopted.
Domain-speciﬁc Test Input Synthesis
Test inputs of ML testing can be classiﬁed into two categories: adversarial inputs and natural inputs. Adversarial
inputs are perturbed based on the original inputs. They may
not belong to normal data distribution (i.e., maybe rarely
exist in practice), but could expose robustness or security
ﬂaws. Natural inputs, instead, are those inputs that belong
to the data distribution of a practical application scenario.
Here we introduce the related work that aims to generate
natural inputs via domain-speciﬁc test input synthesis.
DeepXplore proposed a white-box differential testing
technique to generate test inputs for a deep learning system.
Inspired by test coverage in traditional software testing, the
authors proposed neuron coverage to drive test generation
(we discuss different coverage criteria for ML testing in
Section 5.2.3). The test inputs are expected to have high
neuron coverage. Additionally, the inputs need to expose
differences among different DNN models, as well as be like
real-world data as much as possible. The joint optimisation
algorithm iteratively uses a gradient search to ﬁnd a modi-
ﬁed input that satisﬁes all of these goals. The evaluation of
DeepXplore indicates that it covers 34.4% and 33.2% more
neurons than the same number of randomly picked inputs
and adversarial inputs.
To create useful and effective data for autonomous
driving systems, DeepTest performed greedy search
with nine different realistic image transformations: changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain
effect. There are three types of image transformation styles
provided in OpenCV5: linear, afﬁne, and convolutional. The
evaluation of DeepTest uses the Udacity self-driving car
challenge dataset . It detected more than 1,000 erroneous
behaviours on CNNs and RNNs with low false positive
Generative adversarial networks (GANs) are algorithms to generate models that approximate the manifolds and distribution on a given set of data. GAN has
been successfully applied to advanced image transformation (e.g., style transformation, scene transformation) that
look at least superﬁcially authentic to human observers.
Zhang et al. applied GAN to deliver driving scenebased test generation with various weather conditions. They
sampled images from Udacity Challenge dataset and
YouTube videos (snowy or rainy scenes), and fed them into
the UNIT framework7 for training. The trained model takes
the whole Udacity images as the seed inputs and yields
transformed images as generated tests.
Zhou et al. proposed DeepBillboard to generate realworld adversarial billboards that can trigger potential steering errors of autonomous driving systems.
To test audio-based deep learning systems, Du et al. 
designed a set of transformations tailored to audio inputs considering background noise and volume variation.
They ﬁrst abstracted and extracted a probabilistic transition
model from an RNN. Based on this, stateful testing criteria
are deﬁned and used to guide test generation for stateful
machine learning system.
To test the image classiﬁcation platform when classifying biological cell images, Ding et al. built a testing
framework for biological cell classiﬁers. The framework
iteratively generates new images and uses metamorphic
relations for testing. For example, they generate new images
by increasing the number/shape of artiﬁcial mitochondrion
into the biological cell images, which can arouse easy-toidentify changes in the classiﬁcation results.
Rabin et al. discussed the possibilities of testing
code2vec (a code embedding approach ) with semanticpreserving program transformations serving as test inputs.
To test machine translation systems, Sun et al. 
automatically generate test inputs via mutating the words
in translation inputs. In order to generate translation pairs
that ought to yield consistent translations, their approach
conducts word replacement based on word embedding similarities. Manual inspection indicates that the test generation
has a high precision (99%) on generating input pairs with
consistent translations.
5 
6 The examples of detected erroneous behaviours are available at https:
//deeplearningtest.github.io/deepTest/.
7 A recent DNN-based method to perform image-to-image transformation 
Fuzz and Search-based Test Input Generation
Fuzz testing is a traditional automatic testing technique that
generates random data as program inputs to detect crashes,
memory leaks, failed (built-in) assertions, etc, with many
sucessfully application to system security and vulnerability
detection . As another widely used test generation technique, search-based test generation often uses metaheuristic
search techniques to guide the fuzz process for more ef-
ﬁcient and effective test generation , , . These
two techniques have also been proved to be effective in
exploring the input space of ML testing:
Odena et al. presented TensorFuzz. TensorFuzz used a
simple nearest neighbour hill climbing approach to explore
achievable coverage over valid input space for Tensorﬂow
graphs, and to discover numerical errors, disagreements
between neural networks and their quantized versions, and
surfacing undesirable behaviour in RNNs.
DLFuzz, proposed by Guo et al. , is another fuzz test
generation tool based on the implementation of DeepXplore
with nueron coverage as guidance. DLFuzz aims to generate
adversarial examples. The generation process thus does not
require similar functional deep learning systems for crossreferencing check like DeepXplore and TensorFuzz. Rather,
it needs only minimum changes over the original inputs
to ﬁnd those new inputs that improve neural coverage but
have different predictive results from the original inputs.
The preliminary evaluation on MNIST and ImageNet shows
that compared with DeepXplore, DLFuzz is able to generate
135% to 584.62% more inputs with 20.11% less time consumption.
Xie et al. presented a metamorphic transformation
based coverage guided fuzzing technique, DeepHunter,
which leverages both neuron coverage and coverage criteria
presented by DeepGauge . DeepHunter uses a more
ﬁne-grained metamorphic mutation strategy to generate
tests, which demonstrates the advantage in reducing the
false positive rate. It also demonstrates its advantage in
achieving high coverage and bug detection capability.
Wicker et al. proposed feature-guided test generation. They adopted Scale-Invariant Feature Transform
(SIFT) to identify features that represent an image with a
Gaussian mixture model, then transformed the problem of
ﬁnding adversarial examples into a two-player turn-based
stochastic game. They used Monte Carlo Tree Search to
identify those elements of an image most vulnerable as the
means of generating adversarial examples. The experiments
show that their black-box approach is competitive with
some state-of-the-art white-box methods.
Instead of targeting supervised learning, Uesato et
al. proposed to evaluate reinforcement learning with adversarial example generation. The detection of catastrophic
failures is expensive because failures are rare. To alleviate
the consequent cost of ﬁnding such failures, the authors
proposed to use a failure probability predictor to estimate
the probability that the agent fails, which was demonstrated
to be both effective and efﬁcient.
There are also fuzzers for speciﬁc application scenarios
other than image classiﬁcations. Zhou et al. combined
fuzzing and metamorphic testing to test the LiDAR obstacleperception module of real-life self-driving cars, and reported previously unknown software faults. Jha et al. 
investigated how to generate the most effective test cases
(the faults that are most likely to lead to violations of safety
conditions) via modelling the fault injection as a Bayesian
network. The evaluation, based on two production-grade
AV systems from NVIDIA and Baidu, revealed many situations where faults lead to safety violations.
Udeshi and Chattopadhyay generate inputs for text
classiﬁcation tasks and produce a fuzzing approach that
considers the grammar under test as well as the distance
between inputs. Nie et al. and Wang et al. mutated
the sentences in NLI (Natural Language Inference) tasks to
generate test inputs for robustness testing. Chan et al. 
generated adversarial examples for DNC to expose its robustness problems. Udeshi et al. focused much on
individual fairness and generated test inputs that highlight
the discriminatory nature of the model under test. We give
details about these domain-speciﬁc fuzz testing techniques
in Section 8.
Tuncali et al. proposed a framework for testing
autonomous driving systems. In their work they compared
three test generation strategies: random fuzz test generation,
covering array + fuzz test generation, and covering
array + search-based test generation (using Simulated Annealing algorithm ). The results indicated that the test
generation strategy with search-based technique involved
has the best performance in detecting glancing behaviours.
Symbolic Execution Based Test Input Generation
Symbolic execution is a program analysis technique to test
whether certain properties can be violated by the software
under test . Dynamic Symbolic Execution (DSE, also
called concolic testing) is a technique used to automatically generate test inputs that achieve high code coverage.
DSE executes the program under test with random test
inputs and performs symbolic execution in parallel to collect
symbolic constraints obtained from predicates in branch
statements along the execution traces. The conjunction of all
symbolic constraints along a path is called a path condition.
When generating tests, DSE randomly chooses one test
input from the input domain, then uses constraint solving
to reach a target branch condition in the path . DSE
has been found to be accurate and effective, and has been
the primary technique used by some vulnerability discovery
tools .
In ML testing, the model’s performance is decided, not
only by the code, but also by the data, and thus symbolic
execution has two application scenarios: either on the data
or on the code.
Symbolic analysis was applied to generate more effective tests to expose bugs by Ramanathan and Pullum .
They proposed a combination of symbolic and statistical
approaches to efﬁciently ﬁnd test cases. The idea is to
distance-theoretically abstract the data using symbols to
help search for those test inputs where minor changes in the
input will cause the algorithm to fail. The evaluation of the
implementation of a k-means algorithm indicates that the
approach is able to detect subtle errors such as bit-ﬂips. The
examination of false positives may also be a future research
When applying symbolic execution on the machine
learning code, there are many challenges. Gopinath 
listed three such challenges for neural networks in their
paper, which work for other ML modes as well: (1) the
networks have no explicit branching; (2) the networks may
be highly non-linear, with no well-developed solvers for
constraints; and (3) there are scalability issues because the
structures of the ML models are usually very complex and
are beyond the capabilities of current symbolic reasoning
Considering these challenges, Gopinath introduced
DeepCheck. It transforms a Deep Neural Network (DNN)
into a program to enable symbolic execution to ﬁnd pixel
attacks that have the same activation pattern as the original
image. In particular, the activation functions in DNN follow
an IF-Else branch structure, which can be viewed as a path
through the translated program. DeepCheck is able to create
1-pixel and 2-pixel attacks by identifying most of the pixels
or pixel-pairs that the neural network fails to classify the
corresponding modiﬁed images.
Similarly, Agarwal et al. apply LIME , a local
explanation tool that approximates a model with linear
models, decision trees, or falling rule lists, to help get the
path used in symbolic execution. Their evaluation based on
8 open source fairness benchmarks shows that the algorithm
generates 3.72 times more successful test cases than the
random test generation approach THEMIS .
Sun et al. presented DeepConcolic, a dynamic
symbolic execution testing method for DNNs. Concrete execution is used to direct the symbolic analysis to particular
MC/DC criteria’ condition, through concretely evaluating
given properties of the ML models. DeepConcolic explicitly
takes coverage requirements as input. The authors report
that it yields over 10% higher neuron coverage than DeepXplore for the evaluated models.
Synthetic Data to Test Learning Program
Murphy et al. generated data with repeating values,
missing values, or categorical data for testing two ML
ranking applications. Breck et al. used synthetic training
data that adhere to schema constraints to trigger the hidden
assumptions in the code that do not agree with the constraints. Zhang et al. used synthetic data with known
distributions to test overﬁtting. Nakajima and Bui 
also mentioned the possibility of generating simple datasets
with some predictable characteristics that can be adopted as
pseudo oracles.
Test Oracle
Test oracle identiﬁcation is one of the key problems in ML
testing. It is needed in order to enable the judgement of
whether a bug exists. This is the so-called ‘Oracle Problem’ .
In ML testing, the oracle problem is challenging, because many machine learning algorithms are probabilistic
programs. In this section, we list several popular types of
test oracle that have been studied for ML testing, i.e., metamorphic relations, cross-referencing, and model evaluation
Metamorphic Relations as Test Oracles
Metamorphic relations was proposed by Chen et al. 
to ameliorate the test oracle problem in traditional software
testing. A metamorphic relation refers to the relationship
between the software input change and output change during multiple program executions. For example, to test the
implementation of the function sin(x), one may check how
the function output changes when the input is changed from
x to π −x. If sin(x) differs from sin(π −x), this observation
signals an error without needing to examine the speciﬁc values computed by the implementation. sin(x) = sin(π −x)
is thus a metamorphic relation that plays the role of test
oracle (also named ‘pseudo oracle’) to help bug detection.
In ML testing, metamorphic relations are widely studied
to tackle the oracle problem. Many metamorphic relations
are based on transformations of training or test data that are
expected to yield unchanged or certain expected changes
in the predictive output. There are different granularities
of data transformations when studying the corresponding metamorphic relations. Some transformations conduct
coarse-grained changes such as enlarging the dataset or
changing the data order, without changing each single data
instance. We call these transformations ‘Coarse-grained data
transformations’. Some transformations conduct data transformations via smaller changes on each data instance, such
as mutating the attributes, labels, or pixels of images, and
are referred to as ‘ﬁne-grained’ data transformations in this
paper. The related works of each type of transformations are
introduced below.
Coarse-grained Data Transformation. As early as in 2008,
Murphy et al. discuss the properties of machine
learning algorithms that may be adopted as metamorphic
relations. Six transformations of input data are introduced:
additive, multiplicative, permutative, invertive, inclusive,
and exclusive. The changes include adding a constant to
numerical values; multiplying numerical values by a constant; permuting the order of the input data; reversing the
order of the input data; removing a part of the input data;
adding additional data. Their analysis is on MartiRank,
SVM-Light, and PAYL. Although unevaluated in the initial
2008 paper, this work provided a foundation for determining the relationships and transformations that can be used
for conducting metamorphic testing for machine learning.
Ding et al. proposed 11 metamorphic relations
to test deep learning systems. At the dataset level, the
metamorphic relations were also based on training data or
test data transformations that were not supposed to affect
classiﬁcation accuracy, such as adding 10% training images
into each category of the training data set or removing one
category of data from the dataset. The evaluation is based
on a classiﬁcation of biological cell images.
Murphy et al. presented function-level metamorphic relations. The evaluation on 9 machine learning
applications indicated that functional-level properties were
170% more effective than application-level properties.
Fine-grained Data Transformation. In 2009, Xie et al. 
proposed to use metamorphic relations that were speciﬁc to
a certain model to test the implementations of supervised
classiﬁers. The paper presents ﬁve types of metamorphic
relations that enable the prediction of expected changes to
the output (such as changes in classes, labels, attributes)
based on particular changes to the input. Manual analysis
of the implementation of KNN and Naive Bayes from
Weka indicates that not all metamorphic relations
are necessary. The differences in the metamorphic relations
between SVM and neural networks are also discussed in
 . Dwarakanath et al. applied metamorphic relations to image classiﬁcations with SVM and deep learning
systems. The changes on the data include changing the
feature or instance orders, linear scaling of the test features,
normalisation or scaling up the test data, or changing the
convolution operation order of the data. The proposed MRs
are able to ﬁnd 71% of the injected bugs. Sharma and
Wehrheim considered ﬁne-grained data transformations such as changing feature names, renaming feature
values to test fairness. They studied 14 classiﬁers, none of
them were found to be sensitive to feature name shufﬂing.
Zhang et al. proposed Perturbed Model Validation
(PMV) which combines metamorphic relation and data
mutation to detect overﬁtting. PMV mutates the training
data via injecting noise in the training data to create perturbed training datasets, then checks the training accuracy
decrease rate when the noise degree increases. The faster the
training accuracy decreases, the less the machine learning
model overﬁts.
Al-Azani and Hassine studied the metamorphic
relations of Naive Bayes, k-Nearest Neighbour, as well as
their ensemble classiﬁer. It turns out that the metamorphic
relations necessary for Naive Bayes and k-Nearest Neighbour may be not necessary for their ensemble classiﬁer.
Tian et al. and Zhang et al. stated that the
autonomous vehicle steering angle should not change signi-
ﬁcantly or stay the same for the transformed images under
different weather conditions. Ramanagopal et al. used
the classiﬁcation consistency of similar images to serve
as test oracles for testing self-driving cars. The evaluation
indicates a precision of 0.94 when detecting errors in unlabelled data.
Additionally, Xie et al. proposed METTLE, a metamorphic testing approach for unsupervised learning validation. METTLE has six types of different-grained metamorphic relations that are specially designed for unsupervised learners. These metamorphic relations manipulate
instance order, distinctness, density, attributes, or inject outliers of the data. The evaluation was based on synthetic data
generated by Scikit-learn, showing that METTLE is practical and effective in validating unsupervised learners. Nakajima et al. , discussed the possibilities of using
different-grained metamorphic relations to ﬁnd problems in
SVM and neural networks, such as to manipulate instance
order or attribute order and to reverse labels and change
attribute values, or to manipulate the pixels in images.
Metamorphic Relations between Different Datasets. The
consistency relations between/among different datasets can
also be regarded as metamorphic relations that could be
applied to detect data bugs. Kim et al. and Breck et
al. studied the metamorphic relations between training
data and new data. If the training data and new data
have different distributions, the training data may not be
adequate. Breck et al. also studied the metamorphic
relations among different datasets that are close in time:
these datasets are expected to share some characteristics
because it is uncommon to have frequent drastic changes
to the data-generation code.
Frameworks to Apply Metamorphic Relations. Murphy
et al. implemented a framework called Amsterdam
to automate the process of using metamorphic relations to
detect ML bugs. The framework reduces false positives via
setting thresholds when doing result comparison. They also
developed Corduroy , which extended Java Modelling
Language to let developers specify metamorphic properties
and generate test cases for ML testing.
We introduce more related work on domain-speciﬁc
metamorphic relations of testing autonomous driving, Differentiable Neural Computer (DNC) , machine translation systems , , , biological cell classiﬁcation
 , and audio-based deep learning systems in Section 8.
Cross-Referencing as Test Oracles
Cross-Referencing is another type of test oracle for ML testing, including differential Testing and N-version Programming. Differential testing is a traditional software testing
technique that detects bugs by observing whether similar
applications yield different outputs regarding identical inputs , . It is a testing oracle for detecting compiler bugs . According to the study of Nejadgholi and
Yang , 5% to 27% test oracles for deep learning libraries
use differential testing.
Differential testing is closely related with N-version programming : N-version programming aims to generate
multiple functionally-equivalent programs based on one
speciﬁcation, so that the combination of different versions
are more fault-tolerant and robust.
Davis and Weyuker discussed the possibilities of
differential testing for ‘non-testable’ programs. The idea
is that if multiple implementations of an algorithm yield
different outputs on one identical input, then at least one of
the implementation contains a defect. Alebiosu et al. 
evaluated this idea on machine learning, and successfully
found 16 faults from 7 Naive Bayes implementations and 13
faults from 19 k-nearest neighbour implementation.
Pham et al. also adopted cross referencing to test
ML implementations, but focused on the implementation
of deep learning libraries. They proposed CRADLE, the ﬁrst
approach that focuses on ﬁnding and localising bugs in deep
learning software libraries. The evaluation was conducted
on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and
30 pre-trained models. It turned out that CRADLE detects
104 unique inconsistencies and 12 bugs.
DeepXplore and DLFuzz used differential testing
as test oracles to ﬁnd effective test inputs. Those test inputs
causing different behaviours among different algorithms or
models were preferred during test generation.
Most differential testing relies on multiple implementations or versions, while Qin et al. used the behaviours
of ‘mirror’ programs, generated from the training data as
pseudo oracles. A mirror program is a program generated
based on training data, so that the behaviours of the program represent the training data. If the mirror program has
similar behaviours on test data, it is an indication that the
behaviour extracted from the training data suit test data as
Sun et al. applied cross reference in repairing
machine translation systems. Their approach, TransRepair,
compares the outputs (i.e., translations) of different mutated
inputs, and picks the output that shares the most similarity
with others as a superior translation candidate.
Measurement Metrics for Designing Test Oracles
Some work has presented deﬁnitions or statistical measurements of non-functional features of ML systems including
robustness , fairness , , , and interpretability , . These measurements are not direct oracles
for testing, but are essential for testers to understand and
evaluate the property under test, and to provide some actual
statistics that can be compared with the expected ones. For
example, the deﬁnitions of different types of fairness ,
 , (more details are in Section 6.5.1) deﬁne the
conditions an ML system has to satisfy without which the
system is not fair. These deﬁnitions can be adopted directly
to detect fairness violations.
Except for these popular test oracles in ML testing, there
are also some domain-speciﬁc rules that could be applied
to design test oracles. We discussed several domain-speciﬁc
rules that could be adopted as oracles to detect data bugs
in Section 7.1.1. Kang et al. discussed two types of
model assertions under the task of car detection in videos:
ﬂickering assertion to detect the ﬂickering in car bounding
box, and multi-box assertion to detect nested-car bounding.
For example, if a car bounding box contains other boxes,
the multi-box assertion fails. They also proposed some
automatic ﬁx rules to set a new predictive result when a
test assertion fails.
There has also been a discussion about the possibility of
evaluating ML learning curve in lifelong machine learning
as the oracle . An ML system can pass the test oracle if
it can grow and increase its knowledge level over time.
Test Adequacy
Test adequacy evaluation aims to discover whether the
existing tests have a good fault-revealing ability. It provides
an objective conﬁdence measurement on testing activities.
The adequacy criteria can also be adopted to guide test
generation. Popular test adequacy evaluation techniques
in traditional software testing include code coverage and
mutation testing, which are also adopted in ML testing.
Test Coverage
In traditional software testing, code coverage measures the
degree to which the source code of a program is executed by
a test suite . The higher coverage a test suite achieves, it
is more probable that the hidden bugs could be uncovered.
In other words, covering the code fragment is a necessary
condition to detect the defects hidden in the code. It is often
desirable to create test suites to achieve higher coverage.
Unlike traditional software, code coverage is seldom a
demanding criterion for ML testing, since the decision logic
of an ML model is not written manually but rather it is
learned from training data. For example, in the study of
Pei et al. , 100 % traditional code coverage is easy to
achieve with a single randomly chosen test input. Instead,
researchers propose various types of coverage for ML models beyond code coverage.
Neuron coverage. Pei et al. proposed the ﬁrst coverage
criterion, neuron coverage, particularly designed for deep
learning testing. Neuron coverage is calculated as the ratio
of the number of unique neurons activated by all test inputs
and the total number of neurons in a DNN. In particular, a
neuron is activated if its output value is larger than a userspeciﬁed threshold.
Ma et al. extended the concept of neuron coverage.
They ﬁrst proﬁle a DNN based on the training data, so that
they obtain the activation behaviour of each neuron against
the training data. Based on this, they propose more ﬁnegrained criteria, k-multisection neuron coverage, neuron
boundary coverage, and strong neuron activation coverage,
to represent the major functional behaviour and corner
behaviour of a DNN.
MC/DC coverage variants. Sun et al. proposed four
test coverage criteria that are tailored to the distinct features
of DNN inspired by the MC/DC coverage criteria .
MC/DC observes the change of a Boolean variable, while
their proposed criteria observe a sign, value, or distance
change of a neuron, in order to capture the causal changes
in the test inputs. The approach assumes the DNN to be a
fully-connected network, and does not consider the context
of a neuron in its own layer as well as different neuron
combinations within the same layer .
Layer-level coverage. Ma et al. also presented layerlevel coverage criteria, which considers the top hyperactive
neurons and their combinations (or the sequences) to characterise the behaviours of a DNN. The coverage is evaluated
to have better performance together with neuron coverage
based on dataset MNIST and ImageNet. In their followingup work , , they further proposed combinatorial
testing coverage, which checks the combinatorial activation
status of the neurons in each layer via checking the fraction
of neurons activation interaction in a layer. Sekhon and
Fleming deﬁned a coverage criteria that looks for 1) all
pairs of neurons in the same layer having all possible value
combinations, and 2) all pairs of neurons in consecutive
layers having all possible value combinations.
State-level coverage. While aftermentioned criteria, to some
extent, capture the behaviours of feed-forward neural networks, they do not explicitly characterise stateful machine
learning system like recurrent neural network (RNN). The
RNN-based ML approach has achieved notable success
in applications that handle sequential inputs, e.g., speech
audio, natural language, cyber physical control signals. In
order to analyse such stateful ML systems, Du et al. 
proposed the ﬁrst set of testing criteria specialised for RNNbased stateful deep learning systems. They ﬁrst abstracted a
stateful deep learning system as a probabilistic transition
system. Based on the modelling, they proposed criteria
based on the state and traces of the transition system, to
capture the dynamic state transition behaviours.
Limitations of Coverage Criteria. Although there are different types of coverage criteria, most of them focus on DNNs.
Sekhon and Fleming examined the existing testing
methods for DNNs and discussed the limitations of these
Most proposed coverage criteria are based on the structure of a DNN. Li et al. pointed out the limitations
of structural coverage criteria for deep networks caused by
the fundamental differences between neural networks and
human-written programs. Their initial experiments with
natural inputs found no strong correlation between the
number of misclassiﬁed inputs in a test set and its structural
coverage. Due to the black-box nature of a machine learning
system, it is not clear how such criteria directly relate to the
system’s decision logic.
Mutation Testing
In traditional software testing, mutation testing evaluates the fault-revealing ability of a test suite via injecting
faults , . The ratio of detected faults against all
injected faults is called the Mutation Score.
In ML testing, the behaviour of an ML system depends
on, not only the learning code, but also data and model
structure. Ma et al. proposed DeepMutation, which
mutates DNNs at the source level or model level, to make
minor perturbation on the decision boundary of a DNN.
Based on this, a mutation score is deﬁned as the ratio of
test instances of which results are changed against the total
number of instances.
Shen et al. proposed ﬁve mutation operators for
DNNs and evaluated properties of mutation on the MINST
dataset. They pointed out that domain-speciﬁc mutation
operators are needed to enhance mutation analysis.
Compared to structural coverage criteria, mutation testing based criteria are more directly relevant to the decision
boundary of a DNN. For example, an input data that is near
the decision boundary of a DNN, could more easily detect
the inconsistency between a DNN and its mutants.
Surprise Adequacy
Kim et al. introduced surprise adequacy to measure
the coverage of discretised input surprise range for deep
learning systems. They argued that test diversity is more
meaningful when being measured with respect to the training data. A ‘good’ test input should be ‘sufﬁciently but not
overly surprising’ comparing with the training data. Two
measurements of surprise were introduced: one is based
on Keneral Density Estimation (KDE) to approximate the
likelihood of the system having seen a similar input during
training, the other is based on the distance between vectors
representing the neuron activation traces of the given input
and the training data (e.g., Euclidean distance). These criteria can be adopted to detect adversarial examples. Further
investigation is required to determine whether such criteria
enable the behaviour boundaries of ML models to be approximated in terms of surprise. It will also be interesting for
future work to study the relationship between adversarial
examples, natural error samples, and surprise-based criteria.
Rule-based Checking of Test Adequacy
To ensure the functionality of an ML system, there may be
some ‘typical’ rules that are necessary. Breck et al. 
offered 28 test aspects to consider and a scoring system
used by Google. Their focus is to measure how well a given
machine learning system is tested. The 28 test aspects are
classiﬁed into four types: 1) the tests for the ML model itself,
2) the tests for ML infrastructure used to build the model,
3) the tests for ML data used to build the model, and 4)
the tests that check whether the ML system works correctly
over time. Most of them are some must-to-check rules that
could be applied to guide test generation. For example, the
training process should be reproducible; all features should
be beneﬁcial; there should be no other model that is simpler
but better in performance than the current one. Their research indicates that, although ML testing is complex, there
are shared properties to design some basic test cases to test
the fundamental functionality of the ML system.
Test Prioritisation and Reduction
Test input generation in ML has a very large input space
to cover. On the other hand, we need to label every test instance so as to judge predictive accuracy. These two aspects
lead to high test generation costs. Byun et al. used
DNN metrics like cross entropy, surprisal, and Bayesian
uncertainty to prioritise test inputs. They experimentally
showed that these are good indicators of inputs that expose
unacceptable behaviours, which are also useful for retraining.
Generating test inputs is also computationally expensive.
Zhang et al. proposed to reduce costs by identifying those test instances that denote the more effective
adversarial examples. The approach is a test prioritisation
technique that ranks the test instances based on their sensitivity to noise, because the instances that are more sensitive
to noise is more likely to yield adversarial examples.
Li et. al focused on test data reduction in operational DNN testing. They proposed a sampling technique
guided by the neurons in the last hidden layer of a DNN, using a cross-entropy minimisation based distribution approximation technique. The evaluation was conducted on pretrained models with three image datasets: MNIST, Udacity
challenge, and ImageNet. The results show that, compared
to random sampling, their approach samples only half the
test inputs, yet it achieves a similar level of precision.
Ma et al. proposed a set of test selection metrics
based on the notion of model conﬁdence. Test inputs that are
more uncertain to the models are preferred, because they are
more informative and should be used to improve the model
if being included during retraining. The evaluation shows
that their test selection approach has 80% more gain than
random selection.
Bug Report Analysis
Thung et al. were the ﬁrst to study machine learning
bugs via analysing the bug reports of machine learning systems. 500 bug reports from Apache Mahout, Apache Lucene,
and Apache OpenNLP were studied. The explored problems included bug frequency, bug categories, bug severity,
and bug resolution characteristics such as bug-ﬁx time,
effort, and ﬁle number. The results indicated that incorrect
implementation counts for the largest proportion of ML
bugs, i.e., 22.6% of bugs are due to incorrect implementation
of deﬁned algorithms. Implementation bugs are also the
most severe bugs, and take longer to ﬁx. In addition, 15.6%
of bugs are non-functional bugs. 5.6% of bugs are data bugs.
Zhang et al. studied 175 TensorFlow bugs, based
on the bug reports from Github or StackOverﬂow. They
studied the symptoms and root causes of the bugs, the existing challenges to detect the bugs and how these bugs are
handled. They classiﬁed TensorFlow bugs into exceptions
or crashes, low correctness, low efﬁciency, and unknown.
The major causes were found to be in algorithm design and
implementations such as TensorFlow API misuse (18.9%),
unaligned tensor (13.7%), and incorrect model parameter or
structure (21.7%)
Banerjee et al. analysed the bug reports of
autonomous driving systems from 12 autonomous vehicle
manufacturers that drove a cumulative total of 1,116,605
miles in California. They used NLP technologies to classify
the causes of disengagements into 10 types (A disagreement
is a failure that causes the control of the vehicle to switch
from the software to the human driver). The issues in
machine learning systems and decision control account for
the primary cause of 64 % of all disengagements based on
their report analysis.
Debug and Repair
Data Resampling. The generated test inputs introduced in
Section 5.1 only expose ML bugs, but are also studied as
a part of the training data and can improve the model’s
correctness through retraining. For example, DeepXplore
achieves up to 3% improvement in classiﬁcation accuracy
by retraining a deep learning model on generated inputs.
DeepTest improves the model’s accuracy by 46%.
Ma et al. identiﬁed the neurons responsible for
the misclassiﬁcation and call them ‘faulty neurons’. They
resampled training data that inﬂuence such faulty neurons
to help improve model performance.
Debugging Framework Development. Dutta et al. 
proposed Storm, a program transformation framework to
generate smaller programs that can support debugging for
machine learning testing. To ﬁx a bug, developers usually
need to shrink the program under test to write better bug
reports and to facilitate debugging and regression testing.
Storm applies program analysis and probabilistic reasoning
to simplify probabilistic programs, which helps to pinpoint
the issues more easily.
Cai et al. presented tfdbg, a debugger for ML models built on TensorFlow, containing three key components:
1) the Analyzer, which makes the structure and intermediate
state of the runtime graph visible; 2) the NodeStepper,
which enables clients to pause, inspect, or modify at a given
node of the graph; 3) the RunStepper, which enables clients
to take higher level actions between iterations of model
training. Vartak et al. proposed the MISTIQUE system
to capture, store, and query model intermediates to help
the debug. Krishnan and Wu presented PALM, a tool
that explains a complex model with a two-part surrogate
model: a meta-model that partitions the training data and
a set of sub-models that approximate the patterns within
each partition. PALM helps developers ﬁnd out the training
data that impacts prediction the most, and thereby target
the subset of training data that account for the incorrect
predictions to assist debugging.
Fix Understanding. Fixing bugs in many machine learning
systems is difﬁcult because bugs can occur at multiple
points in different components. Nushi et al. proposed
a human-in-the-loop approach that simulates potential ﬁxes
in different components through human computation tasks:
humans were asked to simulate improved component states.
The improvements of the system are recorded and compared, to provide guidance to designers about how they can
best improve the system.
Program Repair. Albarghouthi et al. proposed a
distribution-guided inductive synthesis approach to repair
decision-making programs such as machine learning programs. The purpose is to construct a new program with correct predictive output, but with similar semantics with the
original program. Their approach uses sampled instances
and the predicted outputs to drive program synthesis in
which the program is encoded based on SMT.
General Testing Framework and Tools
There has also been work focusing on providing a testing
tool or framework that helps developers to implement testing activities in a testing workﬂow. There is a test framework
to generate and validate test inputs for security testing .
Dreossi et al. presented a CNN testing framework
that consists of three main modules: an image generator, a
collection of sampling methods, and a suite of visualisation
tools. Tramer et al. proposed a comprehensive testing
tool to help developers to test and debug fairness bugs with
an easily interpretable bug report. Nishi et al. proposed
a testing framework including different evaluation aspects
such as allowability, achievability, robustness, avoidability
and improvability. They also discussed different levels of
ML testing, such as system, software, component, and data
Thomas et al. recently proposed a framework for
designing machine learning algorithms, which simpliﬁes
the regulation of undesired behaviours. The framework
is demonstrated to be suitable for regulating regression,
classiﬁcation, and reinforcement algorithms. It allows one
to learn from (potentially biased) data while guaranteeing
that, with high probability, the model will exhibit no bias
when applied to unseen data. The deﬁnition of bias is userspeciﬁed, allowing for a large family of deﬁnitions. For a
learning algorithm and a training data, the framework will
either returns a model with this guarantee, or a warning that
it fails to ﬁnd such a model with the required guarantee.
ML PROPERTIES TO BE TESTED
ML properties concern the conditions we should care about
for ML testing, and are usually connected with the behaviour of an ML model after training. The poor performance
in a property, however, may be due to bugs in any of the
ML components (see more in Introduction 7).
This section presents the related work of testing both
functional ML properties and non-functional ML properties.
Functional properties include correctness (Section 6.1) and
overﬁtting (Section 6.2). Non-functional properties include
robustness and security (Section 6.3), efﬁciency (Section 6.4),
fairness (Section 6.5).
Correctness
Correctness concerns the fundamental function accuracy
of an ML system. Classic machine learning validation is
the most well-established and widely-used technology for
correctness testing. Typical machine learning validation approaches are cross-validation and bootstrap. The principle
is to isolate test data via data sampling to check whether
the trained model ﬁts new cases. There are several approaches to perform cross-validation. In hold out crossvalidation , the data are split into two parts: one part
becomes the training data and the other part becomes test
data8. In k-fold cross-validation, the data are split into k
equal-sized subsets: one subset used as the test data and
the remaining k −1 as the training data. The process is
then repeated k times, with each of the subsets serving
as the test data. In leave-one-out cross-validation, k-fold
cross-validation is applied, where k is the total number
of data instances. In Bootstrapping, the data are sampled
with replacement , and thus the test data may contain
repeated instances.
There are several widely-adopted correctness measurements such as accuracy, precision, recall, and Area Under
Curve (AUC). There has been work analysing the
disadvantages of each measurement criterion. For example,
accuracy does not distinguish between the types of errors
it makes (False Positive versus False Negatives). Precision
and Recall may be misled when data is unbalanced. An
implication of this work is that we should carefully choose
performance metrics. Chen et al. studied the variability
of both training data and test data when assessing the
correctness of an ML classiﬁer . They derived analytical
expressions for the variance of the estimated performance
and provided an open-source software implemented with
an efﬁcient computation algorithm. They also studied the
performance of different statistical methods when comparing AUC, and found that the F-test has the best performance .
To better capture correctness problems, Qin et al. 
proposed to generate a mirror program from the training
data, and then use the behaviours this mirror program
to serve as a correctness oracle. The mirror program is
expected to have similar behaviours as the test data.
There has been a study of the prevalence of correctness
problems among all the reported ML bugs: Zhang et al. 
studied 175 Tensorﬂow bug reports from StackOverﬂow QA
(Question and Answer) pages and from Github projects.
Among the 175 bugs, 40 of them concern poor correctness.
Additionally, there have been many works on detecting
data bug that may lead to low correctness , ,
 (see more in Section 7.1), test input or test oracle
design , , , , , , , and test
tool design , , (see more in Section 5).
Model Relevance
Model relevance evaluation detects mismatches between
model and data. A poor model relevance is usually associated with overﬁtting or underﬁtting. When a model is too
8 Sometimes a validation set is also needed to help train the model, in
which circumstances the validation set will be isolated from the training
complex for the data, even the noise of training data is ﬁtted
by the model . Overﬁtting can easily happen, especially
when the training data is insufﬁcient, , , .
Cross-validation is traditionally considered to be a useful
way to detect overﬁtting. However, it is not always clear
how much overﬁtting is acceptable and cross-validation
may be unlikely to detect overﬁtting if the test data is
unrepresentative of potential unseen data.
Zhang et al. introduced Perturbed Model Validation
(PMV) to help model selection. PMV injects noise to the
training data, re-trains the model against the perturbed
data, then uses the training accuracy decrease rate to detect
overﬁtting/underﬁtting. The intuition is that an overﬁtted
learner tends to ﬁt noise in the training sample, while an
underﬁtted learner will have low training accuracy regardless the presence of injected noise. Thus, both overﬁtting
and underﬁtting tend to be less insensitive to noise and
exhibit a small accuracy decrease rate against noise degree
on perturbed data. PMV was evaluated on four real-world
datasets (breast cancer, adult, connect-4, and MNIST) and
nine synthetic datasets in the classiﬁcation setting. The results reveal that PMV has better performance and provides a
more recognisable signal for detecting both overﬁtting/underﬁtting compared to 10-fold cross-validation.
An ML system usually gathers new data after deployment, which will be added into the training data to improve
correctness. The test data, however, cannot be guaranteed to
represent the future data. Werpachowski et al. presents
an overﬁtting detection approach via generating adversarial
examples from test data. If the reweighted error estimate on
adversarial examples is sufﬁciently different from that of the
original test set, overﬁtting is detected. They evaluated their
approach on ImageNet and CIFAR-10.
Gossmann et al.
 studied the threat of test data
reuse practice in the medical domain with extensive simulation studies, and found that the repeated reuse of the same
test data will inadvertently result in overﬁtting under all
considered simulation settings.
Kirk mentioned that we could use training time as a
complexity proxy for an ML model; it is better to choose
the algorithm with equal correctness but relatively small
training time.
Ma et al. tried to relieve the overﬁtting problem via
re-sampling the training data. Their approach was found to
improve test accuracy from 75% to 93% on average, based
on an evaluation using three image classiﬁcation datasets.
Robustness and Security
Robustness Measurement Criteria
Unlike correctness or overﬁtting, robustness is a nonfunctional characteristic of a machine learning system. A
natural way to measure robustness is to check the correctness of the system with the existence of noise ; a
robust system should maintain performance in the presence
Moosavi-Dezfooli et al. proposed DeepFool that
computes perturbations (added noise) that ‘fool’ deep networks so as to quantify their robustness. Bastani et al. 
presented three metrics to measure robustness: 1) pointwise
robustness, indicating the minimum input change a classi-
ﬁer fails to be robust; 2) adversarial frequency, indicating
how often changing an input changes a classiﬁer’s results;
3) adversarial severity, indicating the distance between an
input and its nearest adversarial example.
Carlini and Wagner created a set of attacks that
can be used to construct an upper bound on the robustness
of a neural network. Tjeng et al. proposed to use the
distance between a test input and its closest adversarial
example to measure robustness. Ruan et al. provided
global robustness lower and upper bounds based on the
test data to quantify the robustness. Gopinath et al.
proposed DeepSafe, a data-driven approach for assessing
DNN robustness: inputs that are clustered into the same
group should share the same label.
More recently, Mangal et al. proposed the deﬁnition
of probabilistic robustness. Their work used abstract interpretation to approximate the behaviour of a neural network
and to compute an over-approximation of the input regions
where the network may exhibit non-robust behaviour.
Banerjee et al. explored the use of Bayesian Deep
Learning to model the propagation of errors inside deep
neural networks to mathematically model the sensitivity of
neural networks to hardware errors, without performing
extensive fault injection experiments.
Perturbation Targeting Test Data
The existence of adversarial examples allows attacks that
may lead to serious consequences in safety-critical applications such as self-driving cars. There is a whole separate
literature on adversarial example generation that deserves a
survey of its own, and so this paper does not attempt to fully
cover it. Rather, we focus on those promising aspects that
could be fruitful areas for future research at the intersection
of traditional software testing and machine learning.
Carlini and Wagner developed adversarial example generation approaches using distance metrics to
quantify similarity. The approach succeeded in generating
adversarial examples for all images on the recently proposed defensively distilled networks .
Adversarial input generation has been widely adopted
to test the robustness of autonomous driving systems ,
 , , , . There has also been research on generating adversarial inputs for NLI models , (Section 8.3),
malware detection , and Differentiable Neural Computer (DNC) .
Papernot et al. , designed a library to standardise the implementation of adversarial example construction.
They pointed out that standardising adversarial example
generation is important because ‘benchmarks constructed
without a standardised implementation of adversarial example construction are not comparable to each other’: it is
not easy to tell whether a good result is caused by a high
level of robustness or by the differences in the adversarial
example construction procedure.
Other techniques to generate test data that check
the neural network robustness include symbolic execution , , fuzz testing , combinatorial Testing , and abstract interpretation . In Section 5.1,
we cover these test generation techniques in more detail.
Perturbation Targeting the Whole System
Jha et al. presented AVFI, which used application/software fault injection to approximate hardware errors in the
sensors, processors, or memory of the autonomous vehicle
(AV) systems to test the robustness. They also presented
Kayotee , a fault injection-based tool to systematically inject faults into software and hardware components of the autonomous driving systems. Compared with
AVFI, Kayotee is capable of characterising error propagation
and masking using a closed-loop simulation environment,
which is also capable of injecting bit ﬂips directly into GPU
and CPU architectural state. DriveFI , further presented
by Jha et al., is a fault-injection engine that mines situations
and faults that maximally impact AV safety.
Tuncali et al. considered the closed-loop behaviour
of the whole system to support adversarial example generation for autonomous driving systems, not only in image
space, but also in conﬁguration space.
The empirical study of Zhang et al. on Tensorﬂow bugrelated artefacts (from StackOverﬂow QA page and Github)
found that nine out of 175 ML bugs (5.1%) belong to efﬁciency problems. The reasons may either be that efﬁciency
problems rarely occur or that these issues are difﬁcult to
Kirk pointed out that it is possible to use the
efﬁciency of different machine learning algorithms when
training the model to compare their complexity.
Spieker and Gotlieb studied three training data
reduction approaches, the goal of which was to ﬁnd a
smaller subset of the original training data set with similar
characteristics during model training, so that model building speed could be improved for faster machine learning
Fairness is a relatively recently-emerging non-functional
characteristic. According to the work of Barocas and
Selbst , there are the following ﬁve major causes of
unfairness.
1) Skewed sample: once some initial bias happens, such bias
may compound over time.
2) Tainted examples: the data labels are biased because of
biased labelling activities of humans.
3) Limited features: features may be less informative or
reliably collected, misleading the model in building the
connection between the features and the labels.
4) Sample size disparity: if the data from the minority group
and the majority group are highly imbalanced, ML model
may the minority group less well.
5) Proxies: some features are proxies of sensitive attributes
(e.g., neighbourhood in which a person resides), and may
cause bias to the ML model even if sensitive attributes are
Research on fairness focuses on measuring, discovering,
understanding, and coping with the observed differences
regarding different groups or individuals in performance.
Such differences are associated with fairness bugs, which
can offend and even harm users, and cause programmers
and businesses embarrassment, mistrust, loss of revenue,
and even legal violations .
Fairness Deﬁnitions and Measurement Metrics
There are several deﬁnitions of fairness proposed in the
literature, yet no ﬁrm consensus , , , .
Nevertheless, these deﬁnitions can be used as oracles to
detect fairness violations in ML testing.
To help illustrate the formalisation of ML fairness, we
use X to denote a set of individuals, Y to denote the true
label set when making decisions regarding each individual
in X. Let h be the trained machine learning predictive
model. Let A be the set of sensitive attributes, and Z be
the remaining attributes.
1) Fairness Through Unawareness. Fairness Through Unawareness (FTU) means that an algorithm is fair so long
as the protected attributes are not explicitly used in the
decision-making process . It is a relatively low-cost
way to deﬁne and ensure fairness. Nevertheless, sometimes
the non-sensitive attributes in X may contain information
correlated to sensitive attributes that may thereby lead to
discrimination , . Excluding sensitive attributes
may also impact model accuracy and yield less effective
predictive results .
2) Group Fairness. A model under test has group fairness if
groups selected based on sensitive attributes have an equal
probability of decision outcomes. There are several types of
group fairness.
Demographic Parity is a popular group fairness measurement . It is also named Statistical Parity or Independence Parity. It requires that a decision should be independent
of the protected attributes. Let G1 and G2 be the two
groups belonging to X divided by a sensitive attribute
a ∈A. A model h under test satisﬁes demographic parity if
P{h(xi) = 1|xi ∈G1} = P{h(xj) = 1|xj ∈G2}.
Equalised Odds is another group fairness approach proposed by Hardt et al. . A model under test h satisﬁes
Equalised Odds if h is independent of the protected attributes
when a target label Y is ﬁxed as yi: P{h(xi) = 1|xi ∈
G1, Y = yi} = P{h(xj) = 1|xj ∈G2, Y = yi}.
When the target label is set to be positive, Equalised
Odds becomes Equal Opportunity . It requires that the
true positive rate should be the same for all the groups. A
model h satisﬁes Equal Opportunity if h is independent of
the protected attributes when a target class Y is ﬁxed as
being positive: P{h(xi) = 1|xi ∈G1, Y = 1} = P{h(xj) =
1|xj ∈G2, Y = 1}.
3) Counter-factual Fairness. Kusner et al. introduced
Counter-factual Fairness. A model satisﬁes Counter-factual
Fairness if its output remains the same when the protected
attribute is ﬂipped to a counter-factual value, and other
variables are modiﬁed as determined by the assumed causal
model. Let a be a protected attribute, a′ be the counterfactual attribute of a, x′
i be the new input with a changed
into a′. Model h is counter-factually fair if, for any input
xi and protected attribute a: P{h(xi)a = yi|a ∈A, xi ∈
X} = P{h(x′
i)a′ = yi|a ∈A, xi ∈X}. This measurement of
fairness additionally provides a mechanism to interpret the
causes of bias, because the variables other than the protected
attributes are controlled, and thus the differences in h(xi)
i) must be caused by variations in A.
4) Individual Fairness. Dwork et al. proposed a use
task-speciﬁc similarity metric to describe the pairs of individuals that should be regarded as similar. According to
Dwork et al., a model h with individual fairness should
give similar predictive results among similar individuals:
P{h(xi)|xi ∈X} = P{h(xj) = yi|xj ∈X} iff d(xi, xj) < ϵ,
where d is a distance metric for individuals that measures
their similarity, and ϵ is tolerance to such differences.
Analysis and Comparison of Fairness Metrics. Although
there are many existing deﬁnitions of fairness, each has
its advantages and disadvantages. Which fairness is the
most suitable remains controversial. There is thus some
work surveying and analysing the existing fairness metrics,
or investigate and compare their performance based on
experimental results, as introduced below.
Gajane and Pechenizkiy surveyed how fairness
is deﬁned and formalised in the literature. Corbett-Davies
and Goel studied three types of fairness deﬁnitions:
anti-classiﬁcation, classiﬁcation parity, and calibration. They
pointed out the deep statistical limitations of each type with
examples. Verma and Rubin explained and illustrated
the existing most prominent fairness deﬁnitions based on a
common, unifying dataset.
Saxena et al. investigated people’s perceptions of
three of the fairness deﬁnitions. About 200 recruited participants from Amazon’s Mechanical Turk were asked to
choose their preference over three allocation rules on two
individuals having each applied for a loan. The results
demonstrate a clear preference for the way of allocating
resources in proportion to the applicants’ loan repayment
Support for Fairness Improvement. Metevier et al. 
proposed RobinHood, an algorithm that supports multiple user-deﬁned fairness deﬁnitions under the scenario
of ofﬂine contextual bandits9. RobinHood makes use of
concentration inequalities to calculate high-probability
bounds and to search for solutions that satisfy the fairness
requirements. It gives user warnings when the requirements
are violated. The approach is evaluated under three application scenarios: a tutoring system, a loan approval setting,
and the criminal recidivism, all of which demonstrate the
superiority of RobinHood over other algorithms.
Albarghouthi and Vinitsky proposed the concept of
‘fairness-aware programming’, in which fairness is a ﬁrstclass concern. To help developers deﬁne their own fairness
speciﬁcations, they developed a speciﬁcation language. Like
assertions in traditional testing, the fairness speciﬁcations
are developed into the run-time monitoring code to enable
multiple executions to catch violations. A prototype was
implemented in Python.
Agarwal et al. proposed to reduce fairness classi-
ﬁcation into a problem of cost-sensitive classiﬁcation (where
the costs of different types of errors are differentiated).
The application scenario is binary classiﬁcation, with the
underlying classiﬁcation method being treated as a black
9 A contextual bandit is a type of algorithm that learns to take actions
based on rewards such as user click rate .
box. The reductions optimise the trade-off between accuracy
and fairness constraints.
Albarghouthi et al. proposed an approach to repair decision-making programs using distribution-guided
inductive synthesis.
Test Generation Techniques for Fairness Testing
Galhotra et al. , proposed Themis which considers
group fairness using causal analysis . It deﬁnes fairness scores as measurement criteria for fairness and uses
random test generation techniques to evaluate the degree of
discrimination (based on fairness scores). Themis was also
reported to be more efﬁcient on systems that exhibit more
discrimination.
Themis generates tests randomly for group fairness,
while Udeshi et al. proposed Aequitas, focusing on
test generation to uncover discriminatory inputs and those
inputs essential to understand individual fairness. The generation approach ﬁrst randomly samples the input space
to discover the presence of discriminatory inputs, then
searches the neighbourhood of these inputs to ﬁnd more inputs. As well as detecting fairness bugs, Aeqitas also retrains
the machine-learning models and reduce discrimination in
the decisions made by these models.
Agarwal et al. used symbolic execution together
with local explainability to generate test inputs. The key
idea is to use the local explanation, speciﬁcally Local Interpretable Model-agnostic Explanations10 to identify whether
factors that drive decisions include protected attributes.
The evaluation indicates that the approach generates 3.72
times more successful test cases than THEMIS across 12
benchmarks.
Tramer et al. were the ﬁrst to proposed the concept
of ‘fairness bugs’. They consider a statistically signiﬁcant
association between a protected attribute and an algorithmic
output to be a fairness bug, specially named ‘Unwarranted Associations’ in their paper. They proposed the ﬁrst
comprehensive testing tool, aiming to help developers test
and debug fairness bugs with an ‘easily interpretable’ bug
report. The tool is available for various application areas including image classiﬁcation, income prediction, and health
care prediction.
Sharma and Wehrheim sought to identify causes
of unfairness via checking whether the algorithm under
test is sensitive to training data changes. They mutated the
training data in various ways to generate new datasets,
such as changing the order of rows, columns, and shufﬂing
feature names and values. 12 out of 14 classiﬁers were found
to be sensitive to these changes.
Interpretability
Manual Assessment of Interpretability. The existing work
on empirically assessing the interpretability property usually includes humans in the loop. That is, manual assessment is currently the primary approach to evaluate interpretability. Doshi-Velez and Kim gave a taxonomy
evaluation
approaches
interpretability:
10 Local Interpretable Model-agnostic Explanations produces decision
trees corresponding to an input that could provide paths in symbolic
execution 
application-grounded, human-grounded, and functionallygrounded. Application-grounded evaluation involves human experimentation with a real application scenario.
Human-grounded evaluation uses results from human evaluation on simpliﬁed tasks. Functionally-grounded evaluation requires no human experiments but uses a quantitative
metric as a proxy for explanation quality, for example, a
proxy for the explanation of a decision tree model may be
the depth of the tree.
Friedler et al. introduced two types of interpretability: global interpretability means understanding the entirety
of a trained model; local interpretability means understanding the results of a trained model on a speciﬁc input and the
corresponding output. They asked 1000 users to produce
the expected output changes of a model given an input
change, and then recorded accuracy and completion time
over varied models. Decision trees and logistic regression
models were found to be more locally interpretable than
neural networks.
Automatic Assessment of Interpretability. Cheng et al. 
presented a metric to understand the behaviours of an ML
model. The metric measures whether the learner has learned
the object in object identiﬁcation scenario via occluding the
surroundings of the objects.
Christoph proposed to measure interpretability
based on the category of ML algorithms. He claimed that
‘the easiest way to achieve interpretability is to use only
a subset of algorithms that create interpretable models’.
He identiﬁed several models with good interpretability,
including linear regression, logistic regression and decision
tree models.
Zhou et al. deﬁned the concepts of Metamorphic
Relation Patterns (MRPs) and Metamorphic Relation Input
Patterns (MRIPs) that can be adopted to help end users
understand how an ML system works. They conducted
case studies of various systems, including large commercial
websites, Google Maps navigation, Google Maps locationbased search, image analysis for face recognition (including
Facebook, MATLAB, and OpenCV), and the Google video
analysis service Cloud Video Intelligence.
Evaluation of Interpretability Improvement Methods. Machine learning classiﬁers are widely used in many medical
applications, yet the clinical meaning of the predictive outcome is often unclear. Chen et al. investigated several
interpretability-improving methods which transform classi-
ﬁer scores to a probability of disease scale. They showed
that classiﬁer scores on arbitrary scales can be calibrated to
the probability scale without affecting their discrimination
performance.
Ding et al. treat programs as grey boxes, and detect differential privacy violations via statistical tests. For
the detected violations, they generate counter examples to
illustrate these violations as well as to help developers
understand and ﬁx bugs. Bichsel et al.
 proposed to
estimate the ϵ parameter in differential privacy, aiming to
ﬁnd a triple (x, x′, Φ) that witnesses the largest possible
privacy violation, where x and x′ are two test inputs and
Φ is a possible set of outputs.
ML TESTING COMPONENTS
This section organises the work on ML testing by identifying
the component (data, learning program, or framework) for
which ML testing may reveal a bug.
Bug Detection in Data
Data is a ‘component’ to be tested in ML testing, since the
performance of the ML system largely depends on the data.
Furthermore, as pointed out by Breck et al. , it is important to detect data bugs early because predictions from the
trained model are often logged and used to generate further
data. This subsequent generation creates a feedback loop
that may amplify even small data bugs over time.
Nevertheless, data testing is challenging . According to the study of Amershi et al. , the management
and evaluation of data is among the most challenging tasks
when developing an AI application in Microsoft. Breck et
al. mentioned that data generation logic often lacks
visibility in the ML pipeline; the data are often stored in
a raw-value format (e.g., CSV) that strips out semantic
information that can help identify bugs.
Bug Detection in Training Data
Rule-based Data Bug Detection. Hynes et al. proposed data linter– an ML tool, inspired by code linters, to
automatically inspect ML datasets. They considered three
types of data problems: 1) miscoded data, such as mistyping
a number or date as a string; 2) outliers and scaling, such
as uncommon list length; 3) packaging errors, such as duplicate values, empty examples, and other data organisation
Cheng et al. presented a series of metrics to evaluate whether the training data have covered all important
scenarios.
Performance-based Data Bug Detection. To solve the problems in training data, Ma et al. proposed MODE.
MODE identiﬁes the ‘faulty neurons’ in neural networks
that are responsible for the classiﬁcation errors, and tests
the training data via data resampling to analyse whether
the faulty neurons are inﬂuenced. MODE allows to improve
test effectiveness from 75 % to 93 % on average, based on
evaluation using the MNIST, Fashion MNIST, and CIFAR-
10 datasets.
Bug Detection in Test Data
Metzen et al. proposed to augment DNNs with a small
sub-network, specially designed to distinguish genuine data
from data containing adversarial perturbations. Wang et
al. used DNN model mutation to expose adversarial
examples motivated by their observation that adversarial
samples are more sensitive to perturbations . The evaluation was based on the MNIST and CIFAR10 datasets. The
approach detects 96.4 %/90.6 % adversarial samples with
74.1/86.1 mutations for MNIST/CIFAR10.
Adversarial examples in test data raise security risks.
Detecting adversarial examples is thereby similar to bug
detection. Carlini and Wagner surveyed ten proposals that are designed for detecting adversarial examples
and compared their efﬁcacy. They found that detection
approaches rely on loss functions and can thus be bypassed
when constructing new loss functions. They concluded that
adversarial examples are signiﬁcantly harder to detect than
previously appreciated.
The insufﬁciency of the test data may not able to detect
overﬁtting issues, and could also be regarded as a type of
data bugs. The approaches for detecting test data insufﬁciency were discussed in Section 5.3, such as coverage ,
 , and mutation score .
Skew Detection in Training and Test Data
The training instances and the instances that the model
predicts should exhibit consistent features and distributions.
Kim et al. proposed two measurements to evaluate the
skew between training and test data: one is based on Kernel
Density Estimation (KDE) to approximate the likelihood of
the system having seen a similar input during training, the
other is based on the distance between vectors representing
the neuron activation traces of the given input and the
training data (e.g., Euclidean distance).
Breck investigated the skew in training data and
serving data (the data that the ML model predicts after deployment). To detect the skew in features, they do key-join
feature comparison. To quantify the skew in distribution,
they argued that general approaches such as KL divergence
or cosine similarity might not be sufﬁciently intuitive for
produce teams. Instead, they proposed to use the largest
change in probability as a value in the two distributions as
a measurement of their distance.
Frameworks in Detecting Data Bugs
Breck et al. proposed a data validation system for
detecting data bugs. The system applies constraints (e.g.,
type, domain, valency) to ﬁnd bugs in single-batch (within
the training data or new data), and quantiﬁes the distance
between training data and new data. Their system is deployed as an integral part of TFX (an end-to-end machine
learning platform at Google). The deployment in production
provides evidence that the system helps early detection and
debugging of data bugs. They also summarised the type
of data bugs, in which new feature column, unexpected
string values, and missing feature columns are the three
most common.
Krishnan et al. , proposed a model training framework, ActiveClean, that allows for iterative data
cleaning while preserving provable convergence properties.
ActiveClean suggests a sample of data to clean based on the
data’s value to the model and the likelihood that it is ‘dirty’.
The analyst can then apply value transformations and ﬁltering operations to the sample to ‘clean’ the identiﬁed dirty
In 2017, Krishnan et al. presented a system named
BoostClean to detect domain value violations (i.e., when an
attribute value is outside of an allowed domain) in training
data. The tool utilises the available cleaning resources such
as Isolation Forest to improve a model’s performance.
After resolving the problems detected, the tool is able to
improve prediction accuracy by up to 9% in comparison to
the best non-ensemble alternative.
ActiveClean and BoostClean may involve a human in
the loop of testing process. Schelter et al. focus on the
automatic ‘unit’ testing of large-scale datasets. Their system
provides a declarative API that combines common as well
as user-deﬁned quality constraints for data testing. Krishnan and Wu also targeted automatic data cleaning
and proposed AlphaClean. They used a greedy tree search
algorithm to automatically tune the parameters for data
cleaning pipelines. With AlphaClean, the user could focus
on deﬁning cleaning requirements and let the system ﬁnd
the best conﬁguration under the deﬁned requirement. The
evaluation was conducted on three datasets, demonstrating
that AlphaClean ﬁnds solutions of up to 9X better than stateof-the-art parameter tuning methods.
Training data testing is also regarded as a part of a whole
machine learning workﬂow in the work of Baylor et al. .
They developed a machine learning platform that enables
data testing, based on a property description schema that
captures properties such as the features present in the data
and the expected type or valency of each feature.
There are also data cleaning technologies such as statistical or learning approaches from the domain of traditional
database and data warehousing. These approaches are not
specially designed or evaluated for ML, but they can be repurposed for ML testing .
Bug Detection in Learning Program
Bug detection in the learning program checks whether the
algorithm is correctly implemented and conﬁgured, e.g., the
model architecture is designed well, and whether there exist
coding errors.
Unit Tests for ML Learning Program. McClure introduced ML unit testing with TensorFlow built-in testing
functions to help ensure that ‘code will function as expected’
to help build developers’ conﬁdence.
Schaul et al. developed a collection of unit tests
specially designed for stochastic optimisation. The tests are
small-scale, isolated with well-understood difﬁculty. They
could be adopted in the beginning learning stage to test the
learning algorithms to detect bugs as early as possible.
Algorithm Conﬁguration Examination. Sun et al. and
Guo et al. identiﬁed operating systems, language, and
hardware Compatibility issues. Sun et al. studied 329
real bugs from three machine learning frameworks: Scikitlearn, Paddle, and Caffe. Over 22% bugs are found to
be compatibility problems due to incompatible operating
systems, language versions, or conﬂicts with hardware. Guo
et al. investigated deep learning frameworks such as
TensorFlow, Theano, and Torch. They compared the learning accuracy, model size, robustness with different models
classifying dataset MNIST and CIFAR-10.
The study of Zhang et al. indicates that the most
common learning program bug is due to the change of
TensorFlow API when the implementation has not been
updated accordingly. Additionally, 23.9% (38 in 159) of
the bugs from ML projects in their study built based on
TensorFlow arise from problems in the learning program.
Karpov et al. also highlighted testing algorithm
parameters in all neural network testing problems. The
parameters include the number of neurons and their types
based on the neuron layer types, the ways the neurons
interact with each other, the synapse weights, and the
activation functions. However, the work currently remains
unevaluated.
Algorithm Selection Examination. Developers usually have
more than one learning algorithm to choose from. Fu and
Menzies compared deep learning and classic learning
on the task of linking Stack Overﬂow questions, and found
that classic learning algorithms (such as reﬁned SVM) could
achieve similar (and sometimes better) results at a lower
cost. Similarly, the work of Liu et al. found that the
k-Nearest Neighbours (KNN) algorithm achieves similar
results to deep learning for the task of commit message
generation.
Mutant Simulations of Learning Program Faults. Murphy
et al. , used mutants to simulate programming
code errors to investigate whether the proposed metamorphic relations are effective at detecting errors. They
introduced three types of mutation operators: switching
comparison operators, mathematical operators, and off-byone errors for loop variables.
Dolby et al. extended WALA to support static
analysis of the behaviour of tensors in Tensorﬂow learning
programs written in Python. They deﬁned and tracked
tensor types for machine learning, and changed WALA
to produce a dataﬂow graph to abstract possible program
behavours.
Bug Detection in Framework
The current research on framework testing focuses on studying framework bugs (Section 7.3.1) and detecting bugs in
framework implementation (Section 7.3.2).
Study of Framework Bugs
Xiao et al. focused on the security vulnerabilities of
popular deep learning frameworks including Caffe, Tensor-
Flow, and Torch. They examined the code of popular deep
learning frameworks. The dependency of these frameworks
was found to be very complex. Multiple vulnerabilities
were identiﬁed in their implementations. The most common
vulnerabilities are bugs that cause programs to crash, nonterminate, or exhaust memory.
Guo et al. tested deep learning frameworks, including TensorFlow, Theano, and Torch, by comparing their
runtime behaviour, training accuracy, and robustness, under
identical algorithm design and conﬁguration. The results
indicate that runtime training behaviours are different for
each framework, while the prediction accuracies remain
Low Efﬁciency is a problem for ML frameworks, which
may directly lead to inefﬁciency of the models built on them.
Sun et al. found that approximately 10% of reported
framework bugs concern low efﬁciency. These bugs are
usually reported by users. Compared with other types of
bugs, they may take longer for developers to resolve.
Implementation Testing of Frameworks
Many learning algorithms are implemented inside ML
frameworks. Implementation bugs in ML frameworks may
cause neither crashes, errors, nor efﬁciency problems ,
making their detection challenging.
Challenges in Implementation Bug Detection. Thung et
al. studied machine learning bugs in 2012. Their results, regarding 500 bug reports from three machine learning
systems, indicated that approximately 22.6% bugs are due
to incorrect algorithm implementations. Cheng et al. 
injected implementation bugs into classic machine learning code in Weka and observed the performance changes
that resulted. They found that 8% to 40% of the logically
non-equivalent executable mutants (injected implementation bugs) were statistically indistinguishable from their
original versions.
Solutions for Implementation Bug Detection. Some work
has used multiple implementations or differential testing to
detect bugs. For example, Alebiosu et al. found ﬁve
faults in 10 Naive Bayes implementations and four faults
in 20 k-nearest neighbour implementations. Pham et al. 
found 12 bugs in three libraries (i.e., TensorFlow, CNTK, and
Theano), 11 datasets (including ImageNet, MNIST, and KGS
Go game), and 30 pre-trained models (see Section 5.2.2).
However, not every algorithm has multiple implementations. Murphy et al. , were the ﬁrst to discuss the
possibilities of applying metamorphic relations to machine
learning implementations. They listed several transformations of the input data that should ought not to bring
changes in outputs, such as multiplying numerical values
by a constant, permuting or reversing the order of the input
data, and adding additional data. Their case studies found
that their metamorphic relations held on three machine
learning applications.
Xie et al. focused on supervised learning. They
proposed to use more speciﬁc metamorphic relations to
test the implementations of supervised classiﬁers. They
discussed ﬁve types of potential metamorphic relations on
KNN and Naive Bayes on randomly generated data. In
2011, they further evaluated their approach using mutated
machine learning code . Among the 43 injected faults
in Weka (injected by MuJava ), the metamorphic
relations were able to reveal 39. In their work, the test inputs
were randomly generated data.
Dwarakanath et al. applied metamorphic relations
to ﬁnd implementation bugs in image classiﬁcation. For
classic machine learning such as SVM, they conducted
mutations such as changing feature or instance orders, and
linear scaling of the test features. For deep learning models
such as residual networks (which the data features are not
directly available), they proposed to normalise or scale the
test data, or to change the convolution operation order of
the data. These changes were intended to bring no change
to the model performance when there are no implementation bugs. Otherwise, implementation bugs are exposed. To
evaluate, they used MutPy to inject mutants that simulate
implementation bugs, of which the proposed metamorphic
relations are able to ﬁnd 71%.
Study of Frameworks Test Oracles
Nejadgholi and Yang studied the approximated oracles
of four popular deep learning libraries: Tensorﬂow, Theano,
PyTorch, and Keras. 5% to 24% oracles were found to be
approximated oracles with a ﬂexible threshold (in contrast
to certain oracles). 5%-27% of the approximated oracles used
the outputs from other libraries/frameworks. Developers
were also found to modify approximated oracles frequently
due to code evolution.
APPLICATION SCENARIOS
Machine learning has been widely adopted in different
areas. This section introduces such domain-speciﬁc testing
approaches in three typical application domains: autonomous driving, machine translation, and neural language inference.
Autonomous Driving
Testing autonomous vehicles has a comparatively long history. For example, in 2004, Wegener and Bühler compared
different ﬁtness functions when evaluating the tests of
autonomous car parking systems . Testing autonomous
vehicles also has many research opportunities and open
questions, as pointed out and discussed by Woehrle et
al. .
More recently, search-based test generation for AV testing has been successfully applied. Abdessalem et al. ,
 focused on improving the efﬁciency and accuracy of
search-based testing of advanced driver assistance systems
(ADAS) in AVs. Their algorithms use classiﬁcation models
to improve the efﬁciency of the search-based test generation
for critical scenarios. Search algorithms are further used
to reﬁne classiﬁcation models to improve their accuracy.
Abdessalem et al. also proposed FITEST, a multiobjective search algorithm that searches feature interactions
which violate system requirements or lead to failures.
Most of the current autonomous vehicle systems that
have been put into the market are semi-autonomous
vehicles, which require a human driver to serve as a fallback , as was the case with the work of Wegener and
Bühler . An issue that causes the human driver to take
control of the vehicle is called a disengagement.
Banerjee et al. investigated the causes and impacts
of 5,328 disengagements from the data of 12 AV manufacturers for 144 vehicles that drove a cumulative 1,116,605
autonomous miles, 42 (0.8%) of which led to accidents. They
classiﬁed the causes of disengagements into 10 types. 64% of
the disengagements were found to be caused by the bugs in
the machine learning system, among which the behaviours
of image classiﬁcation (e.g., improper detection of trafﬁc
lights, lane markings, holes, and bumps) were the dominant
causes accounting for 44% of all reported disengagements.
The remaining 20% were due to the bugs in the control and
decision framework such as improper motion planning.
Pei et al. used gradient-based differential testing to
generate test inputs to detect potential DNN bugs and
leveraged neuron coverage as a guideline. Tian et al. 
proposed to use a set of image transformation to generate tests, which simulate the potential noise that could be
present in images obtained from a real-world camera. Zhang
et al. proposed DeepRoad, a GAN-based approach to
generate test images for real-world driving scenes. Their
approach is able to support two weather conditions (i.e.,
snowy and rainy). The images were generated with the
pictures from YouTube videos. Zhou et al. proposed
DeepBillboard, which generates real-world adversarial billboards that can trigger potential steering errors of autonomous driving systems. It demonstrates the possibility of generating continuous and realistic physical-world tests for
practical autonomous-driving systems.
Wicker et al. used feature-guided Monte Carlo Tree
Search to identify elements of an image that are most vulnerable to a self-driving system; adversarial examples. Jha et
al. accelerated the process of ﬁnding ‘safety-critical’ issues via analytically modelling the injection of faults into an
AV system as a Bayesian network. The approach trains the
network to identify safety critical faults automatically. The
evaluation was based on two production-grade AV systems
from NVIDIA and Baidu, indicating that the approach can
ﬁnd many situations where faults lead to safety violations.
Uesato et al. aimed to ﬁnd catastrophic failures
in safety-critical agents like autonomous driving in reinforcement learning. They demonstrated the limitations of
traditional random testing, then proposed a predictive adversarial example generation approach to predict failures
and estimate reliable risks. The evaluation on TORCS simulator indicates that the proposed approach is both effective
and efﬁcient with fewer Monte Carlo runs.
To test whether an algorithm can lead to a problematic
model, Dreossi et al. proposed to generate training
data as well as test data. Focusing on Convolutional Neural
Networks (CNN), they build a tool to generate natural
images and visualise the gathered information to detect
blind spots or corner cases under the autonomous driving
scenario. Although there is currently no evaluation, the tool
has been made available11.
Tuncali et al. presented a framework that supports
both system-level testing and the testing of those properties
of an ML component. The framework also supports fuzz test
input generation and search-based testing using approaches
such as Simulated Annealing and Cross-Entropy optimisation.
While many other studies investigated DNN model
testing for research purposes, Zhou et al. combined
fuzzing and metamorphic testing to test LiDAR, which is
an obstacle-perception module of real-life self-driving cars,
and detected real-life fatal bugs.
Jha et al. presented AVFI and Kayotee , which
are fault injection-based tools to systematically inject faults
into autonomous driving systems to assess their safety and
reliability.
O’Kelly et al. proposed a ‘risk-based framework’ for
AV testing to predict the probability of an accident in a base
distribution of trafﬁc behaviour (derived from the public
trafﬁc data collected by the US Department of Transportation). They argued that formally verifying correctness of
an AV system is infeasible due to the challenge of formally
deﬁning “correctness” as well as the white-box requirement.
Traditional testing AVs in a real environment requires prohibitive amounts of time. To tackle these problems, they
view AV testing as rare-event simulation problem, then
evaluate the accident probability to accelerate AV testing.
Machine Translation
Machine translation automatically translates text or speech
from one language to another. The BLEU (BiLingual Evaluation Understudy) score is a widely-adopted measurement criterion to evaluate machine translation quality.
11 
It assesses the correspondence between a machine’s output
and that of a human.
Zhou et al. , used self-deﬁned metamorphic
relations in their tool ‘MT4MT’ to test the translation consistency of machine translation systems. The idea is that
some changes to the input should not affect the overall
structure of the translated output. Their evaluation showed
that Google Translate outperformed Microsoft Translator for
long sentences whereas the latter outperformed the former
for short and simple sentences. They hence suggested that
the quality assessment of machine translations should consider multiple dimensions and multiple types of inputs.
Sun et al. combine mutation testing and metamorphic testing to test and repair the consistency of machine
translation systems. Their approach, TransRepair, enables
automatic test input generation, automatic test oracle generation, as well as automatic translation repair. They ﬁrst
applied mutation on sentence inputs to ﬁnd translation
inconsistency bugs, then used translations of the mutated
sentences to optimise the translation results in a black-box or
grey-box manner. Evaluation demonstrates that TransRepair
ﬁxes 28% and 19% bugs on average for Google Translate and
Transformer.
Compared with existing model retraining approaches,
TransRepair has the following advantages: 1) more effective than data augmentation; 2) source code in dependant
(black box); 3) computationally cheap (avoids space and
time expense of data collection and model retraining); 4)
ﬂexible (enables repair without touching other well-formed
translations).
The work of Zheng et al. , , proposed
two algorithms for detecting two speciﬁc types of machine
translation violations: (1) under-translation, where some
words/phrases from the original text are missing in the
translation, and (2) over-translation, where some words/phrases from the original text are unnecessarily translated
multiple times. The algorithms are based on a statistical
analysis of both the original texts and the translations, to
check whether there are violations of one-to-one mappings
in words/phrases.
Natural Language Inference
A Nature Language Inference (NLI) task judges the inference relationship of a pair of natural language sentences.
For example, the sentence ‘A person is in the room’ could
be inferred from the sentence ‘A girl is in the room’.
Several works have tested the robustness of NLI models.
Nie et al. generated sentence mutants (called ‘rule-based
adversaries’ in the paper) to test whether the existing NLI
models have semantic understanding. Seven state-of-the-art
NLI models (with diverse architectures) were all unable to
recognise simple semantic differences when the word-level
information remains unchanged.
Similarly, Wang et al. mutated the inference target
pair by simply swapping them. The heuristic is that a good
NLI model should report comparable accuracy between the
original and swapped test set for contradictory pairs and
for neutral pairs, but lower accuracy in swapped test set for
entailment pairs (the hypothesis may or may not be true
given a premise).
ANALYSIS OF LITERATURE REVIEW
This section analyses the research distribution among different testing properties and machine learning categories. It
also summarises the datasets (name, description, size, and
usage scenario of each dataset) that have been used in ML
Figure 8 shows several key contributions in the development of ML testing. As early as in 2007, Murphy et al. 
mentioned the idea of testing machine learning applications. They classiﬁed machine learning applications as ‘nontestable’ programs considering the difﬁculty of getting test
oracles. They primarily consider the detection of implementation bugs, described as “to ensure that an application
using the algorithm correctly implements the speciﬁcation
and fulﬁls the users’ expectations”. Afterwards, Murphy
et al. discussed the properties of machine learning
algorithms that may be adopted as metamorphic relations
to detect implementation bugs.
In 2009, Xie et al. also applied metamorphic testing
on supervised learning applications.
Fairness testing was proposed in 2012 by Dwork et
al. ; the problem of interpretability was proposed in
2016 by Burrell .
In 2017, Pei et al. published the ﬁrst white-box testing
paper on deep learning systems. Their work pioneered to
propose coverage criteria for DNN. Enlightened by this paper, a number of machine learning testing techniques have
emerged, such as DeepTest , DeepGauge , Deep-
Concolic , and DeepRoad . A number of software
testing techniques has been applied to ML testing, such as
different testing coverage criteria , , , mutation
testing , combinatorial testing , metamorphic testing , and fuzz testing .
Research Distribution among Machine Learning
Categories
This section introduces and compares the research status of
each machine learning category.
Research Distribution between General Machine
Learning and Deep Learning
To explore research trends in ML testing, we classify the collected papers into two categories: those targeting only deep
learning and those designed for general machine learning
(including deep learning).
Among all 144 papers, 56 papers (38.9%) present testing
techniques that are specially designed for deep learning
alone; the remaining 88 papers cater for general machine
We further investigated the number of papers in each
category for each year, to observe whether there is a trend
of moving from testing general machine learning to deep
learning. Figure 9 shows the results. Before 2017, papers
mostly focus on general machine learning; after 2018, both
general machine learning and deep learning speciﬁc testing
notably arise.
First paper
on ML testing.
Murphy et al.
Metamorphic testing applied
Murphy et al. & Xie et al.
Fairness awareness.
Dwork et al.
DeepFool on
adversarial examples
Moosavi-Dezfooli et al.
Interpretability
awareness.
DeepXplore: the first
white-box DL testing
technique. Pei et al.
DeepTest, DeepGauge,
DeepRoad...
Figure 8: Timeline of ML testing research
Number of publications
general machine learning
deep learning
Figure 9: Commutative trends in general machine learning
and deep learning
Research Distribution among Supervised/Unsupervised/Reinforcement Learning Testing
We further classiﬁed the papers based on the three machine
learning categories: 1) supervised learning testing, 2) unsupervised learning testing, and 3) reinforcement learning
testing. One striking ﬁnding is that almost all the work
we identiﬁed in this survey focused on testing supervised
machine learning. Among the 144 related papers, there are
currently only three papers testing unsupervised machine
learning: Murphy et al. introduced metamorphic relations that work for both supervised and unsupervised
learning algorithms. Ramanathan and Pullum proposed
a combination of symbolic and statistical approaches to
test the k-means clustering algorithm. Xie et al. designed metamorphic relations for unsupervised learning.
We were able to ﬁnd out only one paper that focused on
reinforcement learning testing: Uesato et al. proposed
a predictive adversarial example generation approach to
predict failures and estimate reliable risks in reinforcement
Because of this notable imbalance, we sought to understand whether there was also an imbalance of research
popularity in the machine learning areas. To approximate
the research popularity of each category, we searched terms
‘supervised learning’, ‘unsupervised learning’, and ‘reinforcement learning’ in Google Scholar and Google. Table 4
shows the results of search hits. The last column shows the
Table 4: Search hits and testing distribution of supervised/unsupervised/reinforcement Learning
Scholar hits
Google hits
Testing hits
Supervised
Unsupervised
Reinforcement
number/ratio of papers that touch each machine learning
category in ML testing. For example, 119 out of 144 papers
were observed for supervised learning testing purpose. The
table suggests that testing popularity of different categories
is not related to their overall research popularity. In particular, reinforcement learning has higher search hits than
supervised learning, but we did not observe any related
work that conducts direct reinforcement learning testing.
There may be several reasons for this observation. First,
supervised learning is a widely-known learning scenario
associated with classiﬁcation, regression, and ranking problems . It is natural that researchers would emphasise the
testing of widely-applied, known and familiar techniques
at the beginning. Second, supervised learning usually has
labels in the dataset. It is thereby easier to judge and analyse
test effectiveness.
Nevertheless, many opportunities clearly remain for research in the widely-studied areas of unsupervised learning
and reinforcement learning (we discuss more in Section 10).
Different Learning Tasks
ML involves different tasks such as classiﬁcation, regression,
clustering, and dimension reduction (see more in Section 2).
The research focus on different tasks also appears to exhibit
imbalance, with a large number of papers focusing on
classiﬁcation.
Distribution among
Different Testing
Properties
We counted the number of papers concerning each ML
testing property. Figure 10 shows the results. The properties
in the legend are ranked based on the number of papers
that are specially focused on testing the associated property
(‘general’ refers to those papers discussing or surveying ML
testing generally).
From the ﬁgure, 38.7% of the papers test correctness.
26.8% of the papers focus on robustness and security problems. Fairness testing ranks the third among all the properties, with 12.0% of the papers.
Nevertheless, for model relevance, interpretability testing, efﬁciency testing, and privacy testing, fewer than 6
papers exist for each category in our paper collection.
correctness
robustness&security
model relevance
interpretability
efficiency
Figure 10: Research distribution among different testing
properties
Datasets Used in ML Testing
Tables 5 to
8 show details concerning widely-adopted
datasets used in ML testing research. In each table, the ﬁrst
column shows the name and link of each dataset. The next
three columns give a brief description, the size (the “+”
connects training data and test data if applicable), the testing
problem(s), the usage application scenario of each dataset12.
Table 5 shows the datasets used for image classiﬁcation
tasks. Datasets can be large (e.g., more than 1.4 million
images in ImageNet). The last six rows show the datasets
collected for autonomous driving system testing. Most image datasets are adopted to test correctness, overﬁtting, and
robustness of ML systems.
Table 6 shows datasets related to natural language processing. The contents are usually text, sentences, or text ﬁles,
applied to scenarios like robustness and correctness.
The datasets used to make decisions are introduced in
Table 6. They are usually records with personal information,
and thus are widely adopted to test the fairness of the ML
We also calculate how many datasets an ML testing
paper usually uses in its evaluation (for those papers with
an evaluation). Figure 11 shows the results. Surprisingly,
most papers use only one or two datasets in their evaluation; One reason might be training and testing machine
learning models have high costs. There is one paper with as
many as 600 datasets, but that paper used these datasets to
evaluate data cleaning techniques, which has relatively low
cost .
We also discuss research directions of building dataset
and benchmarks for ML testing in Section 10.
Open-source Tool Support in ML Testing
There are several tools specially designed for ML testing.
Angell et al. presented Themis , an open-source tool
for testing group discrimination14. There is also an ML testing framework for tensorﬂow, named mltest15, for writing
12 These tables do not list datasets adopted in data cleaning evaluation,
because such studies usually involve hundreds of data sets 
14 
15 
Number of papers
Number of data sets
Figure 11: Number of papers with different amounts of
datasets in experiments
simple ML unit tests. Similar to mltest, there is a testing
framework for writing unit tests for pytorch-based ML systems, named torchtest16. Dolby et al. extended WALA
to enable static analysis for machine learning code using
TensorFlow.
Compared to traditional testing, the existing tool support
in ML testing is relatively immature. There remains plenty
of space for tool-support improvement for ML testing.
CHALLENGES AND OPPORTUNITIES
This section discusses the challenges (Section 10.1) and
research opportunities in ML testing (Section 10.2).
Challenges in ML Testing
As this survey reveals, ML testing has experienced rapid
recent growth. Nevertheless, ML testing remains at an early
stage in its development, with many challenges and open
questions lying ahead.
Challenges in Test Input Generation. Although a range
of test input generation techniques have been proposed
(see more in Section 5.1), test input generation remains
challenging because of the large behaviour space of ML
Search-based Software Test generation (SBST) uses
a meta-heuristic optimising search technique, such as a
Genetic Algorithm, to automatically generate test inputs. It
is a test generation technique that has been widely used
in research (and deployment ) for traditional software
testing paradigms. As well as generating test inputs for testing functional properties like program correctness, SBST has
also been used to explore tensions in algorithmic fairness in
requirement analysis. , . SBST has been successfully applied in testing autonomous driving systems ,
 , . There exist many research opportunities in
applying SBST on generating test inputs for testing other
ML systems, since there is a clear apparent ﬁt between SBST
and ML; SBST adaptively searches for test inputs in large
input spaces.
Existing test input generation techniques focus on generating adversarial inputs to test the robustness of an ML
16 
Table 5: Datasets (1/4): Image Classiﬁcation
Description
MNIST 
Images of handwritten digits
60,000+10,000
correctness, overﬁtting, robustness
Fashion MNIST 
MNIST-like dataset of fashion images
correctness, overﬁtting
CIFAR-10 
General images with 10 classes
50,000+10,000
correctness, overﬁtting, robustness
ImageNet 
Visual recognition challenge dataset
14,197,122
correctness, robustness
IRIS ﬂower 
The Iris ﬂowers
overﬁtting
SVHN 
House numbers
73,257+26,032
correctness,robustness
Fruits 360 
Dataset with 65,429 images of 95 fruits
correctness,robustness
Handwritten Letters 
Colour images of Russian letters
correctness,robustness
Balance Scale 
Psychological experimental results
overﬁtting
DSRC 
Wireless communications between vehicles and road
side units
overﬁtting, robustness
Udacity challenge 
Udacity Self-Driving Car Challenge images
101,396+5,614
robustness
challenge 
Dashboard camera
18,659+500,000
robustness
MSCOCO 
Object recognition
correctness
Autopilot-TensorFlow 
Recorded to test the NVIDIA Dave model
robustness
KITTI 
Six different scenes captured by a VW Passat station
wagon equipped with four video cameras
robustness
Table 6: Datasets (2/4): Natural Language Processing
Description
bAbI 
questions and answers for NLP
robustness
Tiny Shakespeare 
Samples from actual Shakespeare
correctness
Dump 
Stack Overﬂow questions and answers
correctness
SNLI 
Stanford Natural Language Inference Corpus
robustness
MultiNLI 
Crowd-sourced collection of sentence pairs annotated
with textual entailment information
robustness
DMV failure reports 
AV failure reports from 12 manufacturers in California13
correctness
system. However, adversarial examples are often criticised
because they do not represent real input data. Thus, an interesting research direction is to how to generate natural test
inputs and how to automatically measure the naturalness of
the generated inputs.
There has been work that tries to generate test inputs to
be as natural as possible under the scenario of autonomous
driving, such as DeepTest , DeepHunter and Deep-
Road , yet the generated images could still suffer from
unnaturalness: sometimes even humans may not recognise
the images generated by these tools. It is both interesting
and challenging to explore whether such kinds of test data
that are meaningless to humans should be adopted/valid in
ML testing.
Challenges on Test Assessment Criteria. There has been a
lot of work exploring how to assess the quality or adequacy
of test data (see more in Section 5.3). However, there is
still a lack of systematic evaluation about how different assessment metrics correlate, or how these assessment metrics
correlate with tests’ fault-revealing ability, a topic that has
been widely studied in traditional software testing .
The relation between test assessment criteria and test suf-
ﬁciency remains unclear. In addition, assessment criteria
may provide a way of interpreting and understanding
behaviours of ML models, which might be an interesting
direction for further exploration.
Challenges Relating to The Oracle Problem. The oracle
problem remains a challenge in ML testing. Metamorphic
relations are effective pseudo oracles but, in most cases,
they need to be deﬁned by human ingenuity . A remaining
challenge is thus to automatically identify and construct
reliable test oracles for ML testing.
Table 7: Datasets (3/4): Records for Decision Making
Description
German Credit 
Descriptions of customers with good and bad credit
Adult 
Census income
Bank Marketing 
Bank client subscription term deposit data
US Executions 
Records of every execution performed in the United
Fraud Detection 
European Credit cards transactions
Admissions
Data 
Graduate school applications to the six largest departments at University of California, Berkeley in 1973
COMPAS 
Score to determine whether to release a defendant
MovieLens Datasets 
People’s preferences for movies
Zestimate 
data about homes and Zillow’s in-house price and
predictions
correctness
FICO scores 
United States credit worthiness
Law school success 
Information concerning law students from 163 law
schools in the United States
Table 8: Datasets (4/4): Others
Description
VirusTotal 
Malicious PDF ﬁles
robustness
Contagio 
Clean and malicious ﬁles
robustness
Drebin 
Applications from different malware families
robustness
Chess 
Chess game data: King+Rook versus King+Pawn on
correctness
Waveform 
CART book’s generated waveform data
correctness
Murphy et al. discussed how ﬂaky tests are likely
to arise in metamorphic testing whenever ﬂoating point
calculations are involved. Flaky test detection is a challenging problem in traditional software testing . It is
perhaps more challenging in ML testing because of the
oracle problem.
Even without ﬂaky tests, pseudo oracles may be inaccurate, leading to many false positives. There is, therefore,
a need to explore how to yield more accurate test oracles
and how to reduce the false positives among the reported
issues. We could even use ML algorithm b to learn to detect
false-positive oracles when testing ML algorithm a.
Challenges in Testing Cost Reduction. In traditional software testing, the cost problem remains a big problem,
yielding many cost reduction techniques such as test selection, test prioritisation, and predicting test execution results.
In ML testing, the cost problem could be more serious,
especially when testing the ML component, because ML
component testing usually requires model retraining or
repeating of the prediction process. It may also require data
generation to explore the enormous mode behaviour space.
A possible research direction for cost reduction is to
represent an ML model as an intermediate state to make
it easier for testing.
We could also apply traditional cost reduction techniques such as test prioritisation or minimisation to reduce
the size of test cases without affecting the test correctness.
More ML solutions are deployed to diverse devices and
platforms (e.g., mobile device, IoT edge device). Due to the
resource limitation of a target device, how to effectively test
ML model on diverse devices as well as the deployment
process would be also a challenge.
Research Opportunities in ML testing
There remain many research opportunities in ML testing.
These are not necessarily research challenges, but may
greatly beneﬁt machine learning developers and users as
well as the whole research community.
Testing More Application Scenarios. Much current research focuses on supervised learning, in particular classiﬁcation problems. More research is needed on problems
associated with testing unsupervised and reinforcement
The testing tasks currently tackled in the literature,
primarily centre on image classiﬁcation. There remain open
exciting testing research opportunities in many other areas,
such as speech recognition, natural language processing and
agent/game play.
Testing More ML Categories and Tasks. We observed
pronounced imbalance regarding the coverage of testing
techniques for different machine learning categories and
tasks, as demonstrated by Table 4. There are both challenges
and research opportunities for testing unsupervised and
reinforcement learning systems.
For instance, transfer learning, a topic gaining much
recent interest, focuses on storing knowledge gained while
solving one problem and applying it to a different but
related problem . Transfer learning testing is also important, yet poorly covered in the existing literature.
Testing Other Properties. From Figure 10, we can see that
most work tests robustness and correctness, while relatively
few papers (less than 3%) study efﬁciency, model relevance,
or interpretability.
Model relevance testing is challenging because the distribution of the future data is often unknown, while the capacity of many models is also unknown and hard to measure.
It might be interesting to conduct empirical studies on the
prevalence of poor model relevance among ML models as
well as on the balance between poor model relevance and
high security risks.
For testing efﬁciency, there is a need to test the efﬁciency
at different levels such as the efﬁciency when switching
among different platforms, machine learning frameworks,
and hardware devices.
For testing property interpretability, existing approaches
rely primarily on manual assessment, which checks whether
humans could understand the logic or predictive results of
an ML model. It will be also interesting to investigate the
automatic assessment of interpretability and the detection
of interpretability violations.
There is a lack of consensus regarding the deﬁnitions and
understanding of fairness and interpretability. There is thus
a need for clearer deﬁnitions, formalisation, and empirical
studies under different contexts.
There has been a discussion that machine learning testing and traditional software testing may have different
requirements in the assurance to be expected for different
properties . Therefore, more work is needed to explore
and identify those properties that are most important for
machine learning systems, and thus deserve more research
and test effort.
Presenting More Testing Benchmarks A large number of
datasets have been adopted in the existing ML testing papers. As Tables 5 to 8 show, these datasets are usually those
adopted for building machine learning systems. As far as
we know, there are very few benchmarks like CleverHans17
that are specially designed for ML testing research purposes,
such as adversarial example construction.
More benchmarks are needed, that are specially designed for ML testing. For example, a repository of machine
learning programs with real bugs would present a good
benchmark for bug-ﬁxing techniques. Such an ML testing
repository, would play a similar (and equally-important)
role to that played by data sets such as Defects4J18 in
traditional software testing.
Covering More Testing Activities. As far we know, requirement analysis for ML systems remains absent in the
ML testing literature. As demonstrated by Finkelstein et
17 
18 
al. , , a good requirements analysis may tackle
many non-functional properties such as fairness.
Existing work is focused on off-line testing. Onlinetesting deserves more research efforts.
According to the work of Amershi et al. , data testing
is especially important. This topic certainly deserves more
research effort. Additionally, there are also many opportunities for regression testing, bug report analysis, and bug
triage in ML testing.
Due to the black-box nature of machine learning algorithms, ML testing results are often more difﬁcult for
developers to understand, compared to traditional software
testing. Visualisation of testing results might be particularly
helpful in ML testing to help developers understand the
bugs and help with the bug localisation and repair.
Mutating Investigation in Machine Learning System.
There have been some studies discussing mutating machine
learning code , , but no work has explored how
to better design mutation operators for machine learning
code so that the mutants could better simulate real-world
machine learning bugs. This is another research opportunity.
CONCLUSION
We provided a comprehensive overview and analysis of
research work on ML testing. The survey presented the
deﬁnitions and current research status of different ML testing properties, testing components, and testing workﬂows.
It also summarised the datasets used for experiments and
the available open-source testing tools/frameworks, and
analysed the research trends, directions, opportunities, and
challenges in ML testing. We hope this survey will help
software engineering and machine learning researchers to
become familiar with the current status and open opportunities of and for of ML testing.
ACKNOWLEDGEMENT
Jie M. Zhang and Mark Harman are supported by the ERC
advanced grant with No. 741278. Lei Ma is suppored by the
JSPS KAKENHI Grant No.19K24348, 19H04086, Qdai-jump
Research Program No.01277, and NVIDIA AI Tech Center
(NVAITC). Yang Liu is supported by Singapore National
Research Foundation with No. NRF2018NCR-NCR005-0001,
National Satellite of Excellence in Trustworthy Software System No. NRF2018NCR-NSOE003-0001, NTU research grant
NGF-2019-06-024.
Before submitting, we sent the paper to those whom we
cited, to check our comments for accuracy and omission.
This also provided one ﬁnal stage in the systematic trawling
of the literature for relevant work. Many thanks to those
members of the community who kindly provided comments
and feedback on earlier drafts of this paper.