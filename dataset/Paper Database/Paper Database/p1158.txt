BIROn - Birkbeck Institutional Research Online
Raman, Natraj and Maybank, Stephen J. Action classification using
a discriminative multilevel HDP-HMM. Neurocomputing 154 , pp. 149-161.
ISSN 0925-2312.
Downloaded from: 
Usage Guidelines:
Please refer to usage guidelines at or alternatively
contact .
Action Classification using a discriminative multilevel
Natraj Raman1 and S.J Maybank2
Department of Computer Science and Information Systems, Birkbeck, University of London
We classify human actions occurring in depth image sequences using features based on skeletal joint
positions. The action classes are represented by a multi-level Hierarchical Dirichlet Process – Hidden
Markov Model (HDP-HMM). The non-parametric HDP-HMM allows the inference of hidden states
automatically from training data. The model parameters of each class are formulated as
transformations from a shared base distribution, thus promoting the use of unlabelled examples
during training and borrowing information across action classes. Further, the parameters are learnt in
a discriminative way. We use a normalized gamma process representation of HDP and margin based
likelihood functions for this purpose. We sample parameters from the complex posterior distribution
induced by our discriminative likelihood function using elliptical slice sampling. Experiments with two
different datasets show that action class models learnt using our technique produce good
classification results.
Keywords: action classification; depth image sequences; HDP-HMM; discriminative classification; slice
1 Introduction
Recognizing actions that occur in videos has applications in diverse areas such as smart surveillance,
search and retrieval of video sequences and human computer interaction. Depth sensors such as
Kinect, with inbuilt human motion capturing techniques, provide estimates of a human skeleton’s 3D
joint positions over time . High level actions can be inferred from these joint positions. However,
robust and accurate inference is still a problem.
Given a sequence of 3D joint positions, a state space model such as a Hidden Markov Model (HMM)
is a natural way to represent an action class. Recall that in an HMM , a sequence of discrete state
variables are linked in a Markov chain by a state transition matrix and each observation in the
sequence is drawn independently from a distribution conditioned on the state. The HMM model
parameters (viz. the state transition matrix and the state specific observation density) corresponding
to each class can be learnt from prototypes belonging to the class. The prediction of a new input’s
class is obtained from the class conditional posterior densities.
In classical parametric HMMs, the number of states must be specified a-priori. In many applications
this number is not known in advance. A typical ad hoc procedure is to carry out training using different
choices for the number of states and then apply a model selection criterion to find the best result.
Instead it is preferable to estimate the correct number of states automatically from data. Further,
when training the HMM models, the focus is on explaining the examples of a particular class rather
than discriminating them from other classes. Often this does not produce good classification results
1 Corresponding author Birkbeck, Malet St, London WC1E7HX, UK. T: +442076316700
2 
Non parametric Bayesian methods such as mixture modelling based on the Dirichlet Process (DP)
estimate the number of mixture components automatically from data. The Hierarchical Dirichlet
Process (HDP), a mixed membership model for groups of data, allows mixture components to be
shared across the groups albeit with group specific mixture proportions . It uses a set of DP priors
– one for each group – with these DP priors linked through a base DP in order to share the mixture
components across groups. The HDP-HMM is a non-parametric variant of the classical HMM that
allows an unbounded set of states with one mixture component corresponding to each state. Each
state (group) in a HDP-HMM thus has state specific transition probabilities (mixture proportions) but
the atoms are shared across the states (groups).
It would be straight forward to use separate HDP-HMMs for each action class and train them
individually. However, this would prohibit sharing training examples across the action classes. To see
the merit of sharing examples, consider that an action is a sequence of poses. It is quite likely that a
set of actions have many similar poses between them with possibly a few poses unique to an action.
In fact, for actions such as ‘stand-up’ and ‘sit-down’ or ‘push’ and ‘pull’, the set of poses may be
identical with only the temporal order of pose sequences differing. What necessarily differentiates
one action from another are the transition probabilities of the poses. If a particular pose is absent
from an action class then there is a low probability of transition to the state for that pose. In our work,
we use a single HDP-HMM to model all the action classes, but with an additional class specific
hierarchical level that accounts for differences in the state transition probabilities among the classes.
Figure 1: Overview of action classification – Training examples contain joint position sequences from
different action classes. The examples from all these action classes are combined in order to infer the
shared pose transitions and pose definitions. P1, P2, P3 and P4 represent the various poses (states)
and each pose is defined through a distribution. The action class specific transitions and definitions
are inferred as transformations of this shared representation. Pose P3 may be absent in the first action
class and hence there is a low probability of transition to it.
Training Examples
Shared Parameters
Class Specific Parameters
As outlined earlier, in a HDP-HMM the mixture components are shared across the hierarchical levels.
It would be more flexible to allow the mixture components of an action class to vary slightly from the
other classes; i.e. we seek a class specific transformation of the shared mixture component
parameters so that we can better discriminate the classes. Note that this is different from using
individually trained HDP-HMM model where the component parameters are unshared among the
classes. We assume the mixture components are distributed as a Gaussian, and use class specific affine
transformation of the Gaussian distribution parameters (mean and covariance) in our work here.
The HDP-HMM based classification approach described above defines a joint distribution of the input
data and class labels to train the classifier. This generative model allows the augmentation of the
labelled training examples with unlabelled examples and thus provides a framework for semisupervised learning. In contrast, a discriminative model uses conditional distribution of the class labels
given the input data to train the classifier. This approach often produces good classification results .
For example, Support Vector Machines (SVMs) use a margin based likelihood that maximizes the
distances of the feature vectors to the classification boundary while minimizing the empirical error
rate on the training set. Inspired by this, we incorporate a margin based term in the likelihood function
used in HDP-HMM training. The inclusion of this discriminative term in the otherwise generative
model, compensates for model mis-specification and leads to better classification results.
Incorporation of a discriminative term into the HDP-HMM model makes the posterior sampling less
straight-forward. The HDP model construction as such has no provision for including an additional
term for the mixing proportions. For the mixture components with Gaussian distribution parameters,
the prior is not any more of the same form as the likelihood and hence is not conjugate. We use a
normalized gamma process formulation of the HDP that allows scaling the mixing proportions of a
DP through a discriminative weighting term. Slice sampling based techniques allow sampling from
any likelihood function, not necessarily a normalized density function. Specifically, we place a Gaussian
prior on the parameters and use Elliptical Slice Sampling to efficiently sample the posterior.
We perform our experiments on two different datasets to classify actions based on the above
discriminative two level HDP-HMM. Both data sets have annotated 3D joint positions estimated from
a depth sensor. We use relative joint positions based on a pre-defined skeleton hierarchy as features.
We also show our results for features obtained by projecting the relative joint positions into three
orthogonal Cartesian planes and employing a histogram based representation.
This paper is organized as follows: in section 2 we review related research and provide relevant
background on HDP-HMM in section 3. We detail our model construction in section 4 and discuss the
discriminative aspect in section 5. Posterior inference is presented in section 6 and the results of the
experiments are shown in section 7. Section 8 is a conclusion.
2 Related Research
 provides a survey of research methods in action analysis and discusses the methodologies used for
recognizing simple actions and high level activities. A review of depth image based algorithms for
action analysis can be found in . provides the state-of-the-art method to extract joint positions
from depth images captured by an infrared sensor.
In , each joint position is associated with a local occupancy feature and the actions are
characterized using a subset of these joint positions called actionlet. A Fourier temporal pyramid is
used to capture the temporal structure and discriminative weights are learnt using a multiple kernel
learning method. uses histograms of 3D joint locations (HOJ3D) as a compact representation for
postures in order to demonstrate view invariant recognition. Linear Discriminant Analysis (LDA) is
applied to the HOJ3D features. The resulting low dimensional features are clustered into visual words
that are modelled by a discrete HMM. In , 3D trajectories of the joint positions are represented
using a 2D trajectory based descriptor called histogram of oriented displacements (HOD) that is scale
and speed invariant. A temporal pyramid is used to model the trajectories over time. The above works
have mainly focused on computing appropriate features from the joint positions in order to classify
the actions. In contrast, our focus in this paper is on a general classification mechanism for observation
sequences in which training examples are shared among different classes and discriminatory model
parameters are learnt. This allows our method to be applied to other sequence classification
There is wide interest in the application of Bayesian non-parametric methods for vision problems. For
example, used a transformed Dirichlet Process to automatically learn the number of parts
composing an object and the number of objects composing a scene. The model used is a single
hierarchical level and the transformation parameters are applied only to the mixture components. In
our work, we use a two-level HDP and extend the transformation to include mixture weights. Further
our model parameters are learnt in a discriminative manner and provides a mechanism for sequence
classification. In , unsupervised activity discovery is achieved using a beta process driven HMM to
segment videos over time. While we use a similar idea of applying non-parametric frameworks for
action recognition, our work differs vastly in the formulation (multi-level gamma process vs beta
process) and application (supervised classification vs unsupervised pattern discovery). In , actions
are segmented and clustered into behaviours using a hierarchical non parametric Bayesian model.
This unsupervised approach uses a switching linear dynamic system to perform hierarchical clustering
which is different from the parameter transformation based model that we use to perform
classification.
Models based on HDP-HMM have been explored before. provides a HDP-HMM based method for
jointly segmenting and classifying actions with new action classes being discovered as they occur. The
model used here is simply the HDP-HMM extended for streaming data by performing batch inferences.
This is in contrast to the multi-level HDP-HMM with discriminatively learnt parameters that we use.
Further, our method allows using unlabelled examples as part of the training procedure. In , a
HDP-HMM based framework is used to detect abnormal activities. However, the HDP-HMM is used
only to compute a feature vector and a one-class SVM is used to determine the decision boundary.
We do not use any additional learning mechanism such as SVM and our method can be extended
seamlessly to semi-supervised learning. applies the sticky HDP-HMM proposed in for error
detection during a robotic assembly task. Our multi-level HDP-HMM that uses the normalized gamma
process formulation is very different from this.
The application of large margin and other discriminative learning approaches for training HMM is
popular in the speech recognition literature. A survey of such approaches can be found in . A
discussion focusing on optimizing the HMM learning procedure for discriminative criteria such as
Minimum Classification Error (MCE) and Maximum Mutual Information (MMI) can be found in . In
 , a margin based approach for supervised topic models that minimized expected margin loss using
Gibbs sampling methods is discussed. More recently, proposes a discriminative multi-scale model
based on SVM for predicting action classes from partially observed videos.
Although HDP-HMM has been used before for solving vision problems, using a discriminative training
method for HDP-HMM has not been explored before. The merits of using margin based learning for
classification is well discussed . The HDP-HMM formulation as such doesn’t provide any
mechanism to learn model parameters discriminatively. Our approach of using a normalized gamma
process formulation and the application of elliptical slice sampling provides a new technique for
discriminative parameter learning in HDP-HMM. To the best of our knowledge, such a model has not
been used before in the literature.
3 Background
In this section we provide relevant background on the classical HMM and it’s non parametric variant
HDP-HMM. For more details see and .
3.1 Bayesian HMM
The classical HMM consists of an observation sequence {𝑥𝑡}𝑡=1
, 𝑥𝑡 ∈ ℝ𝑑 and a corresponding hidden
state sequence {𝑧𝑡}𝑡=1
, 𝑧𝑡 ∈{ 1,2 … 𝐾}. Here 𝐾 is the number of hidden states. The hidden state
sequence follows a first order Markov chain 𝑧𝑡 ⊥𝑧1:𝑡−2 | 𝑧𝑡−1 and the observations are conditionally
independent given the current state i.e. 𝑥𝑡 ⊥𝑧1:𝑡−1, 𝑥1:𝑡−1| 𝑧𝑡.
The probabilities of transitions between states are given by {𝜋𝑗,𝑘}𝑗=0,𝑘=1
where 𝜋𝑗,𝑘=
𝑃(𝑧𝑡= 𝑘|𝑧𝑡−1 = 𝑗) is the probability of transitioning to state 𝑘 given the current state 𝑗 and 𝜋0,𝑘=
𝑃(𝑧1 = 𝑘) is the initial probability of state 𝑘. The observation distribution is parameterized as
𝑃(𝑥𝑡|𝑧𝑡= 𝑘) ~ 𝐹(𝜃𝑘) where 𝜃𝑘 are the natural parameters of the family 𝐹 of distributions. Here we
assume the observations are generated from a mixture of Gaussians, with one Gaussian distribution
corresponding to each state. Hence 𝑃(𝑥𝑡|𝑧𝑡= 𝑘) ~ 𝒩(𝜇𝑘, Σ𝑘) where 𝒩(μ, Σ) is the normal
distribution with mean 𝜇 and covariance Σ.
In this Bayesian approach it is necessary to introduce priors for all the parameters. Let 𝐻 be the prior
for 𝜃 or more specifically for the Gaussian mixtures let the mixture mean have a normal prior
𝜇 ~ 𝒩(𝜇0, Σ0) and let the covariance have an Inverse-Wishart prior Σ ~ 𝐼𝑊(𝜈0, Δ0). We can use a
Dirichlet prior for the state transitions but we must ensure that the transitions out of different states
are coupled. Hence let 𝛽 ~ 𝐷𝑖𝑟(
𝐾) and 𝜋𝑗 ~ 𝐷𝑖𝑟(𝛼𝛽1 … 𝛼𝛽𝐾) where 𝛾, 𝛼 ∈ ℝ+ are the hyper
priors, 𝛽𝑘 is the probability of reaching state 𝑘 and 𝐷𝑖𝑟 is the Dirichlet distribution.
3.2 Stick Breaking Construction of DP
Draws from a Dirichlet Process 𝐺0 ~ 𝐷𝑃(𝛾, 𝐻), where 𝐻 is a base distribution and 𝛾 ∈ ℝ+ a
concentration parameter, are distributions 𝐺0 containing values drawn from 𝐻 with 𝛾 controlling the
variability around 𝐻. The almost sure discreteness of measures drawn from a Dirichlet Process can be
made explicit through the stick breaking construction and we can write
𝐺0 = ∑𝛽𝑘𝛿𝜃𝑘
~ 𝐵𝑒𝑡𝑎(1, 𝛾) 𝜃𝑘 | 𝐻 𝑖𝑖𝑑
where 𝜃𝑘 are the atoms drawn independently from the base distribution and 𝛽𝑘 are the probabilities
that define the mass on the atoms with ∑
= 1. It is common to write the probability measure
𝛽= {𝛽𝑘}𝑘=1
∞ obtained from (1) as 𝛽 ~ 𝐺𝐸𝑀(𝛾).
The DP is a useful nonparametric prior distribution for mixture models. If we interpret the 𝛽𝑘 as a
random probability measure on the set ℤ+, then we can write the generative story for an observation
𝑥𝑛 sampled from a mixture model as
𝛽 | 𝛾 ~ 𝐺𝐸𝑀(𝛾)
𝜃𝑘 | 𝐻 ~ 𝐻
𝑧𝑛 | 𝛽 ~ 𝛽
𝑥𝑛 | 𝑧𝑛, {𝜃𝑘}𝑘=1
∞ ~ 𝐹(𝜃𝑧𝑛)
Here 𝑧𝑛 is a latent variable that indicates the mixture component of the 𝑛𝑡ℎ observation and 𝐹 denotes
the distribution family of the mixture component using 𝜃 as its parameter. Thus the DP can be used
to model a mixture with no upper bounds on the number of components. A component 𝑘 has
parameters 𝜃𝑘 and the probability that an observation is in the 𝑘𝑡ℎ component is 𝛽𝑘.
3.3 Grouped Data and HDP
The HDP is an extension of the DP to model grouped data. Each group is associated with a mixture
model but all the groups share the same mixture components. Hence each group has a separate DP
nonparametric prior and these DPs are linked through a different base DP.
As before, let 𝐺0 be drawn from a Dirichlet Process. Let {𝐺𝑗}𝑗=1
be the set of random distributions
over 𝐽 groups of data. Given the global measure 𝐺0, the set of measures over the 𝐽 groups are
conditionally independent with
𝐺0 | 𝛾, 𝐻 ~ 𝐷𝑃(𝛾, 𝐻)
𝐺𝑗| 𝛼, 𝐺0 ~ 𝐷𝑃(𝛼, 𝐺0)
The global distribution 𝐺0 contains values drawn from the base distribution 𝐻 with 𝛾 controlling the
variability. The 𝑗𝑡ℎ group’s distribution 𝐺𝑗 contains values drawn from 𝐺0 with 𝛼 controlling the
variability. The stick breaking construction of the global measure is same as (1) while the construction
of the group specific measure can be formulated as
𝐺𝑗= ∑𝜋𝑗𝑘𝛿𝜃𝑘
′ ∏(1 −𝜋𝑗𝑙
′ | 𝛼, 𝛽 𝑖𝑖𝑑
~ 𝐵𝑒𝑡𝑎(𝛼𝛽𝑘, 𝛼(1 −∑𝛽𝑙
The HDP is a useful nonparametric prior distribution for a mixture model set. If we interpret
𝜋𝑗= {𝜋𝑗𝑘}𝑘=1
∞ as a random probability measure on the set ℤ+, then we can write the generative story
for an observation 𝑥𝑗𝑛 belonging to the 𝑗𝑡ℎ group as
𝛽 | 𝛾 ~ 𝐺𝐸𝑀(𝛾)
𝜋𝑗 | 𝛼, 𝛽 ~ 𝐷𝑃(𝛼, 𝛽)
𝜃𝑘 | 𝐻 ~ 𝐻
𝑧𝑗𝑛 | 𝜋𝑗 ~ 𝜋𝑗
𝑥𝑗𝑛 | 𝑧𝑗𝑛, {𝜃𝑘}𝑘=1
∞ ~ 𝐹(𝜃𝑧𝑗𝑛)
Here 𝑧𝑗𝑛 is a latent variable that indicates the mixture component of the 𝑗𝑡ℎ group’s 𝑛𝑡ℎ observation.
For a component 𝑘, all the groups share the same parameters 𝜃𝑘 but the 𝑗𝑡ℎ group uses 𝜋𝑗𝑘 proportion
while the 𝑗′𝑡ℎ group uses 𝜋𝑗′𝑘 proportion.
3.4 Non parametric HMM
Figure 2: Graphical representation of a HDP-HMM
As outlined in 3.1, there are 𝐾 hidden states in the parametric HMM and a mixture component
corresponding to each state. Given a state 𝜋𝑗, the 𝑗𝑡ℎ row of the state transition matrix defines the
mixing proportions. In a non-parametric HMM, the number of hidden states 𝐾 is unbounded and the
observations are now generated from an infinite mixture of components. Each state is associated with
a (infinite) mixture model defining varying mixing proportions. In order to ensure that the transitions
out of different states are coupled, the mixture models corresponding to the states must share the
same mixture components.
Thus the non-parametric HMM can be represented using a HDP - a set of (infinite) mixture models,
capturing the state specific mixture proportions, with the mixture models linked through a global DP
that ensures sharing the same mixture components. We can write the generative story for an
observation 𝑥𝑛𝑡 sampled at time 𝑡 from a HDP-HMM that uses Gaussian mixtures as
𝛽 | 𝛾 ~ 𝐺𝐸𝑀(𝛾)
𝜋𝑗 | 𝛼, 𝛽 ~ 𝐷𝑃(𝛼, 𝛽)
𝜇𝑘 | 𝜇0, Σ0 ~ 𝒩(𝜇0, Σ0)
Σ𝑘 | 𝜈0, Δ0 ~ 𝐼𝑊(𝜈0, Δ0)
𝑧𝑛𝑡 | 𝑧𝑛𝑡−1, 𝜋𝑗 ~ 𝜋𝑧𝑛𝑡−1
𝑥𝑛𝑡 | 𝑧𝑛𝑡, {𝜇𝑘, Σ𝑘}𝑘=1
∞ ~ 𝒩(𝜇𝑧𝑛𝑡, Σ𝑧𝑛𝑡)
We are given i.i.d training data 𝑋= {𝑥𝑛}𝑛=1
, 𝑌= {𝑦𝑛}𝑛=1
, where 𝑥𝑛= 𝑥1
𝑛 is an observation
sequence and 𝑦𝑛 ∈{ 1 … 𝐶} its corresponding action class. An observation 𝑥𝑡 ∈ ℝ𝑑 consists of
features extracted from an image sequence at time-step 𝑡. We defer discussion on the features to
section 7. Let the set of all model parameters be 𝜃. Our objective is classification, where given a new
test observation sequence 𝑥̂, we have to predict its corresponding action class 𝑐̂. A suitable prediction
is 𝑐̂ = argmax
𝑝(𝑐 | 𝑥̂, 𝑋, 𝑌). The pdf 𝑝(𝑐 | 𝑥̂, 𝑋, 𝑌) can be written in the form
𝑝(𝑐 | 𝑥̂, 𝑋, 𝑌) = ∫𝑝(𝑐 | 𝑥̂, 𝜃) 𝑝(𝜃 | 𝑋, 𝑌) 𝑑𝜃
The 𝑛𝑡ℎ training example sequence
The class that the 𝑛𝑡ℎ training example belongs to
Observation at time instant 𝑡
Hidden state at time instant 𝑡
Set of all model parameters
Probability of transitioning to state 𝑘
Probability of transitioning to state 𝑘 given state 𝑗
Mean of Gaussian distribution corresponding to component 𝑘
The covariance of Gaussian distribution corresponding to component 𝑘
Hyper-prior for 𝛽
Hyper-prior for 𝜋
Hyper-prior for 𝜇
Hyper-prior for Σ
Two level HDP-HMM
Probability of transitioning to state 𝑘 given state 𝑗 for class 𝑐
Hyper-prior for 𝜑
Transformed HDP-HMM Parameters
Parameter for shifting mean 𝜇𝑘 for class 𝑐
Parameter for scaling covariance Σ𝑘 for class 𝑐
Parameter used for scaling 𝜑𝑗𝑘
𝑐 for class c
Hyper-prior for sampling 𝜌
Hyper-prior for Λ
Hyper-prior for 𝜔
Posterior Inference
Set of model parameters for class 𝑐
Set of model parameters excluding class 𝑐
Set of model parameters shared for all the classes
Upper bound on the number of HMM states
Number of transitions from state 𝑗 to 𝑘 for class 𝑐
Number of transitions from state 𝑗 to 𝑘 for all the classes
Set of observations from class 𝑐 assigned to state 𝑘
Prior controlling importance of discriminative term
Prior controlling the distance between distributions
Table 1: Summary of Notations
4.1 Two level HDP
If we represent each action class by a separate HDP-HMM as outlined in section 3, then 𝜃𝑐=
{𝛽𝑐, 𝜋𝑐, 𝜇1..∞
} are the parameters for class 𝑐 with 𝜃= {𝜃𝑐}𝑐=1
and 𝛾, 𝛼, 𝜇0, Σ0, 𝜈0, Δ0 the hyper
parameters. It would be straight forward to estimate the posterior density of parameters 𝑝(𝜃 | 𝑋, 𝑌)
if each HDP-HMM model is trained separately i.e. we can define a class conditional density 𝑝(𝑥 | 𝑐)
for each class and estimate the posterior from
𝑝(𝜃𝑐 | 𝑋, 𝑌) = 𝑝(𝜃𝑐) ∏𝑝(𝑥𝑛 | 𝜃𝑐)𝑝(𝑐)
However, in this approach we do not make use of training examples from other classes while learning
the parameters of a class. As noted in section 1, many actions contain similar poses and it is useful to
incorporate pose information from other classes during training. Specifically, the inclusion of
additional observations for a similar pose benefits estimation of the Gaussian mixture parameters.
The state transition parameters must continue to be different for each action class since it is these
parameters that necessarily distinguish the actions.
Instead of separate HDP-HMMs, we define a single HDP-HMM for all the action classes albeit with an
extra level that is class specific i.e. in addition to the global distribution 𝐺0 and the state specific
distributions 𝐺𝑗, we now have class specific distributions 𝐺𝑗
𝑐 for every state.
𝐺0 | 𝛾, 𝐻 ~ 𝐷𝑃(𝛾, 𝐻)
𝐺𝑗 | 𝛼, 𝐺0 ~ 𝐷𝑃(𝛼, 𝐺0)
𝑐 | 𝜆, 𝐺𝑗 ~ 𝐷𝑃(𝜆, 𝐺𝑗)
Just as the 𝐺𝑗s are conditionally independent given 𝐺0, the 𝐺𝑗
𝑐s are conditionally independent given
𝐺𝑗. All the classes for a given state share the same subset of atoms but the proportions of these atoms
will differ for each class determined by the concentration parameter 𝜆. The varying atom proportions
induce differences in state transition probabilities between action classes and ensure that
classification can be performed. The stick breaking construction for the additional class specific
measure can be formulated as
𝑐′ ∏(1 −𝜑𝑗𝑙
𝑐′ | 𝜆, 𝜋𝑗 𝑖𝑖𝑑
~ 𝐵𝑒𝑡𝑎(𝜆𝜋𝑗𝑘, 𝜆(1 −∑𝜋𝑗𝑙
Similar to 𝛽 and 𝜋𝑗, if we interpret 𝜑𝑗
∞ as a random probability measure on the set ℤ+,
we can write the generative story for an observation 𝑥𝑡
𝑛 belonging to class 𝑐 sampled at time 𝑡 from
the two level HDP-HMM that uses Gaussian mixtures as
𝛽 | 𝛾 ~ 𝐺𝐸𝑀(𝛾)
𝜋𝑗 | 𝛼, 𝛽 ~ 𝐷𝑃(𝛼, 𝛽)
𝑐| 𝜆, 𝜋𝑗 ~ 𝐷𝑃(𝜆, 𝜋𝑗)
𝜇𝑘 | 𝜇0, Σ0 ~ 𝒩(𝜇0, Σ0)
Σ𝑘 | 𝜈0, Δ0 ~ 𝐼𝑊(𝜈0, Δ0)
𝑛, 𝑦𝑛= 𝑐, {𝜑𝑗
𝑛, {𝜇𝑘, Σ𝑘}𝑘=1
Consequently, for the two level HDP-HMM, the set of all model parameters is 𝜃=
{𝛽, 𝜋, 𝜑1..𝐶, 𝜇1..∞, Σ1..∞} with 𝛾, 𝛼, 𝜇0, Σ0, 𝜈0, Δ0, 𝜆 being the hyper parameters.
4.2 Transformed HDP Parameters
As explained in section 3.3, in a HDP the same atoms are used by the different groups i.e. the
component parameters 𝜃𝑘 remain the same in all 𝐺𝑗 (and 𝐺𝑗
𝑐 in case of an additional level). This is less
flexible than allowing the parameters to vary across the groups. As an example, a squat pose
encountered during the course of an action might mostly look the same across action classes such as
sit up, sit down and pick-up while it may slightly vary for pick-up class. In this case, it would be useful
to capture the deviation from the standard squat pose for this pick-up action class – i.e. we wish to
introduce a transformation of the parameters from its canonical form .
Let 𝜏(𝑢; 𝜙) denote the transformation of a parameter vector 𝑢. In order to express the
transformations through a change of observation coordinates, let us impose the restriction that there
exist a complementary transformation 𝜏′(𝑣; 𝜙) of an observation 𝑣 such that
𝑓(𝑣 | 𝜏(𝑢; 𝜙)) ∝ 𝑓(𝜏′(𝑣; 𝜙) | 𝑢)
where 𝑓 is a density function. The existence of 𝜏′ satisfying (12) is useful during inference. In our work,
we consider the affine transformation of the Gaussian distribution parameters mean 𝜇 and covariance
Σ. Let 𝜌 be a vector and Λ be an invertible matrix. The transforms
𝜏(𝜇, Σ; 𝜌, Λ) = (Λ𝜇+ 𝜌, ΛΣΛ𝑇)
𝜏′(𝑣; 𝜌, Λ) = Λ−1(𝑣− 𝜌)
ensure that the covariance matrix is positive (semi) definite and we have
𝒩(𝑣; Λ𝜇+ 𝜌, ΛΣΛ𝑇) ∝𝒩(Λ−1(𝑣− 𝜌); 𝜇, Σ)
Typically restrictions on Λ would have to be enforced for computational tractability. A useful
simplification is to set Λ equal to the identity matrix. This is equivalent to restricting the
transformations to a translation of the Gaussian mean by 𝜌. We can also restrict Λ to be diagonal, to
account for scaling.
We introduce class specific transformations based on (13) to the Gaussian mixture component
parameters. Let the transformation variable responsible for shifting the mean have a zero mean
normal prior i.e. 𝜌 ~ 𝒩(0, Ω0). We focus only on scale transformations and assume Λ is diagonal.
Effectively the scale transform variable is now a vector and we assign independent log normal prior
for each element i.e. 𝑙𝑜𝑔(Λ𝑗) ~ 𝒩(𝜗0, 𝜎0). An observation 𝑥𝑡
𝑛 belonging to class 𝑐 sampled at time 𝑡
from the two level HDP-HMM that uses Gaussian mixtures with transformed parameters is generated
𝛽 | 𝛾 ~ 𝐺𝐸𝑀(𝛾)
𝜋𝑗 | 𝛼, 𝛽 ~ 𝐷𝑃(𝛼, 𝛽)
𝑐| 𝜆, 𝜋𝑗 ~ 𝐷𝑃(𝜆, 𝜋𝑗)
𝜇𝑘 | 𝜇0, Σ0 ~ 𝒩(𝜇0, Σ0)
Σ𝑘 | 𝜈0, Δ0 ~ 𝐼𝑊(𝜈0, Δ0)
𝑐| Ω0 ~ 𝒩(0, Ω0)
𝑐)| 𝜗0, 𝜎0 ~ 𝒩(𝜗0, 𝜎0)
𝑛, 𝑦𝑛= 𝑐, {𝜑𝑗
𝑛, 𝑦𝑛= 𝑐, {𝜇𝑘, Σ𝑘}𝑘=1
Inclusion of the class specific transforms can be interpreted as an extension of the parameter space.
The global measure is now being drawn from 𝐺0~𝐷𝑃(𝛾, 𝐻𝑠 × 𝐻1 . .×. . 𝐻𝐶), where 𝐻𝑠 is a base
distribution for parameters that are shared across the classes while 𝐻1, … , 𝐻𝐶 are class specific. During
inference, the posterior distributions for the shared parameters do not depend upon the class labels
unlike the class specific parameters. With the augmentation of transform variables, the set of all
model parameters is 𝜃= {𝛽, 𝜋, 𝜑1..𝐶, 𝜇1..∞, Σ1..∞, 𝜌1..∞
1..𝐶, Λ1..∞
1..𝐶} and 𝛾, 𝛼, 𝜇0, Σ0, 𝜈0, Δ0, 𝜆, Ω0, 𝜗0, 𝜎0 are
the hyper parameters.
Figure 3: Graphical representation of a two level HDP-HMM with transformed parameters – the
observations on the left side belong to class ‘1’ while those on the right side belong to class ‘C’
4.3 Chinese Restaurant Process Metaphor
The mixture components generated by DP and its extensions described here can be better understood
using a metaphor . We have a restaurant with unbounded number of tables. A customer entering
the restaurant either selects an unoccupied table with certain probability or selects an occupied table
with a probability proportional to the number of customers already seated at the table. In this DP
metaphor, the tables correspond to mixture components, the dish served at a table to the component
parameters and the customers are observations.
In the HDP analogue, we have multiple restaurants with a single menu i.e. a restaurant franchise. The
tables in the restaurants serve dishes from the shared menu and multiple tables in multiple
restaurants can serve the same dish. A customer entering a given restaurant selects a table in
proportion to the number of customers already seated in that restaurant’s tables but can also select
a new table. Each table is assigned a dish in proportion to the number of tables across the franchise
serving that dish but a new dish can also be ordered. In this HDP metaphor, a restaurant correspond
to a (data) group or in the case of HDP-HMM a state.
In the HDP extended to a second level, each restaurant in the franchise has sections viz. family, kids
and adults section. There is still a single menu across the sections and the restaurants. Given the
customer’s preferred section, the customer entering a given restaurant selects a table in proportion
to the number of customers already seated in the tables of that section of the restaurant. He can also
select a new table in that section. Each table is now assigned a dish in proportion to the number of
tables across the sections, across the franchise serving that dish. In this two-level HDP metaphor, the
sections correspond to the action classes.
In the case of two-level HDP-HMM with transformed parameters, each dish now contains a base part
and a flavouring part. A dish contains flavours for every section viz. spicy flavour for family, bland for
kids and hot for adults. A dish served at a table in a given section (of any restaurant in the franchise)
has its base part seasoned according to that section’s flavour. In this metaphor, the flavours
correspond to the class specific transform parameters while the base part correspond to parameters
shared across the classes.
5 Discriminative Learning
In the two level HDP-HMM with transformed parameters described above, let the model parameters
specific to a class 𝑐 be 𝜃𝑐= {𝜑𝑐, 𝜌1..∞
} and the shared parameters across the classes be 𝜃𝑠=
{𝛽, 𝜋, 𝜇1..∞, Σ1..∞}. Note that 𝜃= 𝜃𝑠 ∪ {𝜃𝑐}𝑐=1
. We will typically apply Gibbs sampling and use a very
similar form to (8) to sample the class specific posterior.
𝑝(𝜃𝑐 | 𝑋, 𝑌, 𝜃𝑠) ∝ 𝑝(𝜃𝑐) ∏𝑝(𝑥𝑛 | 𝜃𝑐, 𝜃𝑠)
The joint distribution over the inputs and labels 𝑝(𝑥, 𝑐 | 𝜃𝑐) is used in this formulation. This type of
learning is intended to best explain the training examples belonging to a class. In the asymptotic limit
of infinite training examples and the distribution specified by the model being identical to the true
distribution of data, it is a very effective way of learning. However, this generative model with its
parameters learnt as above often produces poor classification results. In real world, the specified
model is typically inaccurate and we need to compensate for the model mis-specification.
In contrast, large margin based training used in discriminative learning methods often produces good
classification results. The empirical error rate on the training data is balanced against the
generalization of the test data. The tolerance to mismatch between training and test data is due to
the classifier decision boundary being well separated from the classes – i.e. the decision boundary has
a large margin to the training examples. Since the class conditional data likelihood is used during
prediction in the generative model above, the classifier margin is a function of the model parameters
and adjusting the parameters alters the margins.
There is an implicit assumption in (16) that the parameters of a class are (conditionally) independent
of the parameters of other classes i.e. 𝜃𝑐 ⊥ 𝜃\𝑐 | 𝜃𝑠. Let us relax this assumption and consider a
slightly different formulation.
𝑝(𝜃𝑐 | 𝑋, 𝑌, 𝜃𝑠, 𝜃\𝑐) ∝ 𝑝(𝜃𝑐) ∗ 𝑝(𝜃\𝑐|𝜃𝑐, 𝑋, 𝑌, 𝜃𝑠) ∗∏𝑝(𝑥𝑛 | 𝜃𝑐, 𝜃𝑠)
Here we have made use of the Bayes theorem product rule for 𝑝(𝜃𝑐 | 𝑋, 𝜃\𝑐). The introduction of the
second term 𝑝(𝜃\𝑐 | 𝜃𝑐, 𝑋), referred henceforth as the discriminative term, offers more flexibility. For
example, we can use this term during inference to minimize classification error on the training set and
introduce margin constraints. This discriminative term compensates for the model mis-specification
and improves classification results.
5.1 Scaled HDP and Normalized Gamma Process
The HDP and its stick breaking construction does not provide any mechanism for influencing the pergroup component proportions through additional factors. This makes incorporation of the
discriminant during inference for 𝜑𝑐 tricky. An alternative construction for the last level in the twolevel HDP in (10) is
𝑐 | 𝜆, 𝜋𝑗 𝑖𝑖𝑑
~ 𝐺𝑎𝑚𝑚𝑎(𝜆𝜋𝑗𝑘, 1)
A Dirichlet distributed vector can be generated by independently drawing from a gamma distribution
and normalizing the values. Its infinite extension relates to this normalized gamma process
construction. The representation in (18) as such does not allow using an additional factor. Let each
component be associated with a latent location and let the group specific distribution of the HDP be
formed by scaling the probabilities of an intermediate distribution. More specifically, let us modify
the last level in the two-level HDP described in (9) as
𝑐′ | 𝜆, 𝐺𝑗 ~ 𝐷𝑃(𝜆, 𝐺𝑗)
𝑐′ is an intermediate distribution for the existing parameters and 𝜔𝑗
𝑐 is a scaling factor that
depends on the latent location. Based on this scaled HDP structure, we can make use of the second
variable of the gamma distribution and draw the class specific component proportions as
𝑐 | 𝜆, 𝜋𝑗, 𝜔𝑗
~ 𝐺𝑎𝑚𝑚𝑎(𝜆𝜋𝑗𝑘, 𝑒−𝜔𝑗𝑘
The derivation of (20) follows from the property that if 𝑦 ~ 𝐺𝑎𝑚𝑚𝑎(𝑎, 1) and is scaled by 𝑏> 0 to
produce 𝑧= 𝑏𝑦, then 𝑧 ~ 𝐺𝑎𝑚𝑚𝑎(𝑎, 𝑏−1). We refer the readers to for a detailed discussion on
this construction. In our case, this additional scaling factor allows incorporating the discriminative
term. During inference, we have to draw 𝜔𝑗𝑘
𝑐 in such a way that the posterior 𝜑𝑗𝑘
𝑐 is primed for
classification.
5.2 Elliptical Slice Sampling
We cannot use conjugate priors for the transform parameters 𝜌1..∞
because of the presence of
the discriminative term. Hence there is no closed form solution for posterior inference of these
parameters. In the absence of an analytical update we can resort to a Metropolis step, but it is
necessary to find a proposal distribution. Complex tuning may also be required.
Slice sampling methods provide an alternate solution for sampling from a pdf when the pdf is
known up to a scale factor. The main idea is to sample points uniformly from a region under the true
density curve and then use the sample points based on the horizontal coordinates. Let 𝜙 be a random
variable from which we wish to draw samples and let 𝑓 be a function proportional to the density of 𝜙.
Let 𝜙𝑖 be the current sample. In slice sampling, we first draw an auxiliary variable 𝑢~ 𝕌[0, 𝑓(𝜙𝑖)] that
defines a horizontal slice. We then define a bracket (interval) around the current sample 𝐵(𝜙𝑖) and
draw the new sample 𝜙𝑖+1 ~ { 𝜙′ ∈𝐵(𝜙𝑖) ∶𝑢< 𝑓(𝜙′)}.
The challenge in slice sampling is to define the bracket containing the current value from which the
new value will be drawn. This is especially difficult if 𝜙 takes values in a high dimensional space, as in
our work. If the density function for 𝜙 is a product of a likelihood function and a zero mean Gaussian
prior, then Elliptical Slice sampling provides a better sampling mechanism. Here a full ellipse is
defined passing through the current sample and the brackets are determined by shrinking an angle
Let 𝐿(𝜙) = 𝑝(. |𝜙) be a likelihood function and 𝑝0 = 𝒩(𝜙; 0, Σ) be a multivariate normal prior with
𝑓(𝜙) ∝𝐿(𝜙)𝑝0 the density function from which we wish to draw samples using Elliptical slice
sampling. Similar to slice sampling, an auxiliary variable 𝑢~ 𝕌[0, 𝑓(𝜙𝑖)] is drawn first. We then draw
𝑣 ~ 𝑁(0, Σ) that defines an ellipse centered at the origin. An angle 𝜓 ~ 𝕌[0,2𝜋] determines the
bracket and a new location is computed as 𝜙′ = 𝑣sin 𝜓+ 𝜙𝑖cos 𝜓. If 𝑢< 𝑓(𝜙′), then 𝜙′ is
accepted as the new sample 𝜙𝑖+1; otherwise 𝜓 is shrunk to determine a new location.
Since the angles are shrunk exponentially and the states considered for an update lie within a two
dimensional plane, this technique provides an efficient mechanism for sampling high dimensional
variables. We use Elliptical slice sampling for inferring the transform parameters 𝜌1..∞
density function defined in (17).
6 Posterior Inference
The central computation problem is posterior inference for the parameters. Since it is intractable to
compute the exact posterior, we will resort to Markov Chain Monte Carlo (MCMC) techniques to draw
posterior samples from 𝑝(𝜃 | 𝑋, 𝑌). Recall that we have the shared parameters 𝜃𝑠= {𝛽, 𝜋, 𝜇1..∞, Σ1..∞}
and the class specific parameters 𝜃𝑐= {𝜑𝑐, 𝜌1..∞
} with 𝛾, 𝛼, 𝜇0, Σ0, 𝜈0, Δ0, 𝜆, Ω0, 𝜗0, 𝜎0 as the
hyper parameters. We can apply Gibbs sampling and sample the shared parameters 𝜃𝑠 first and then
given 𝜃𝑠, we can draw samples for each class one by one.
6.1 Truncated Approximation
For sampling the HDP-HMM parameters, one option would be to marginalize over the infinite state
transition distributions 𝜋 and component parameters (𝜇, Σ) and sequentially sample the hidden states
𝑧𝑡. Unfortunately this technique, referred as direct assignment or collapsed sampler, exhibits slow
mixing rates because the HMM states are temporally coupled.
A better technique is to block sample the hidden state sequence 𝑧𝑡 using the standard HMM forwardbackward algorithm . In this uncollapsed sampler the state transition distributions and component
parameters are explicitly instantiated. In order to take account of the fact that there is no upper bound
on the number of states and the corresponding parameters, we can employ slice sampling techniques
 or use truncated approximations . In almost sure truncations, for a given number 𝐿 the
stick breaking construction is discarded for 𝐿+ 1, 𝐿+ 2 … ∞ by setting 𝛽𝐿
′ = 1 in (1). An alternative
technique is to consider a weak limit approximation to DP and set
𝐺𝐸𝑀(𝛾) ≜𝐷𝑖𝑟(𝛾
Here 𝐿 is an upper bound on the number of components and as 𝐿→ ∞, the marginal distribution of
this finite model approaches the DP. We use this weak limit approximation for its computational
efficiency and consequently in (15) we have
𝛽 | 𝛾 ~ 𝐷𝑖𝑟(𝛾
𝜋𝑗 | 𝛼, 𝛽 ~ 𝐷𝑖𝑟(𝛼𝛽1, … 𝛼𝛽𝐿) 𝜑𝑗
𝑐| 𝜆, 𝜋𝑗 ~ 𝐷𝑖𝑟(𝜆𝜋𝑗1, … 𝜆𝜋𝑗𝐿) (22)
Note that this is different from the classical parametric HMM with finite Dirichlet priors. The prior
induced by HDP leads to a subset of 𝐿 possible states with 𝐿 being usually set to a large number. Given
this truncated approximation, the standard forward-backward algorithm can be employed to sample
the hidden state sequences.
6.2 Sampling State Transitions
The sampler is initialized by drawing the initial value of the parameters from their respective priors.
For a training example 𝑥𝑛 whose 𝑦𝑛= 𝑐, given the state transitions {𝜑𝑐 }𝑗=0,𝑘=1
, the component
and the covariances {Λ𝑘
, the hidden state sequence is sampled
𝑛= 𝑘) ∝ 𝜑𝑧𝑡−1
𝑚𝑡+1,𝑡(𝑘) 𝒩(𝑥𝑡
Here 𝑚𝑡,𝑡−1(𝑘) is the HMM backward message that is passed from 𝑧𝑡
𝑛 and is determined
recursively as
𝑚𝑡,𝑡−1(𝑘) = ∑𝜑𝑘𝑗
𝑐 𝑚𝑡+1,𝑡(𝑗) 𝒩(𝑥𝑡
Let 𝑛𝑐∈ℤ𝐿+1×𝐿 be a matrix of counts computed from the sampled hidden state sequences with 𝑛𝑗𝑘
being the number of transitions from states 𝑗 to 𝑘 for class 𝑐. We use the notation 𝑛𝑗𝑘
. to denote the
number of transitions from 𝑗 to 𝑘 for all the classes and 𝑛.𝑘
. to denote the number of transitions to 𝑘.
The scaling factor 𝜔𝑗
𝑐 in (19) is used as the discriminative term and we set it as
Intuitively, the weight for a state 𝑘 will be higher if there are fewer transitions to this state from classes
other than 𝑐. Here 𝜀0 is a prior that controls the importance of the scaling factor and 𝐷 is a sufficiently
large constant to ensure that the scaling factor is positive. We now proceed to sample the posteriors
𝛽 | 𝛾, 𝑚̅ ~ 𝐷𝑖𝑟(𝛾
𝐿+ 𝑚̅.1, … 𝛾
𝜋𝑗 | 𝛼, 𝛽, 𝑛̅ ~ 𝐷𝑖𝑟(𝛼𝛽1 + 𝑛̅𝑗1, … 𝛼𝛽𝐿+ 𝑛̅𝑗𝐿)
𝑐′ | 𝜆, 𝜋𝑗, 𝜔𝑗
𝑐, 𝑛𝑐 ~ 𝐺𝑎𝑚𝑚𝑎(𝜆𝜋𝑗𝑘+ 𝑛𝑗𝑘
Here 𝑚̅, 𝑛̅ are auxiliary count matrices that are sampled from the class specific matrices 𝑛𝑐. In the
Chinese restaurant metaphor, these matrices correspond to the number of tables across the franchise
serving a dish and the number of tables across sections in a restaurant serving a dish. These auxiliary
matrices and the hyper parameters 𝛾, 𝛼, 𝜆 are sampled in the standard way as outlined in .
6.3 Sampling Component Parameters
We sample the shared parameters first and then the class specific parameters. Further we proceed by
sampling posteriors one component at a time. Let the set of observations belonging to class 𝑐 and
assigned to hidden state 𝑘 be 𝒳𝑘
𝑛= 𝑘 ⋀ 𝑦𝑛= 𝑐} with 𝒳𝑘= {𝒳𝑘
. For the mean
and covariance parameters that are shared across the classes, we have conjugate priors and the
posteriors can be computed using the standard closed form updates as
Σ𝑘 | 𝜈0, Δ0, 𝜇𝑘, 𝒳𝑘 ~ 𝐼𝑊(𝜈̅𝑘, 𝜈̅𝑘Δ̅𝑘)
𝜇𝑘 | 𝜇0, Σ0Σ𝑘, 𝒳𝑘 ~ 𝒩(𝜇̅𝑘, Σ̅𝑘)
𝜈̅𝑘= 𝜈0 + |𝒳𝑘|
𝜈̅𝑘Δ̅𝑘= 𝜈0Δ0 + ∑
(𝑥𝑛 − 𝜇𝑘)(𝑥𝑛 − 𝜇𝑘)𝑇
−1 + |𝒳𝑘|Σ𝑘
𝜇̅𝑘= Σ̅𝑘(Σ0
−1𝜇0 + Σ𝑘 ∑
For the transform parameters, we have to sample posterior from (17) after defining the form of
𝑝(𝜃\𝑐 | 𝜃𝑐, 𝑋, 𝜃𝑠). There are several choices for the discriminative term and one option is to set it
based on distance between the distributions of component parameters. If the distribution distances
are large, the parameters are well separated and this will result in a larger margin for the classifier
decision boundary. For the state 𝑘 of class 𝑐 whose transform parameters need to be sampled, we set
𝑝(𝜃\𝑐 | 𝜃𝑐, 𝜃𝑠) = ∏∏exp {−𝜉0max (0, 𝜁0 −𝐷(𝒩( 𝜇̅𝑘
𝑐) || 𝒩( 𝜇̅𝑘′
Here 𝐷(𝑃||𝑄) measures the similarity between two distributions 𝑃 and 𝑄, 𝜁0 is a prior that specifies
the minimum separation distance and 𝜉0 is a constant that controls the overall importance of the
discriminative term. Since we have normal distributions in our case, we can use Hellinger or
Bhattacharya distance as a similarity measure. Intuitively, we compare the distribution of a
component 𝑘 from class 𝑐 that we wish to sample to all the competing classes and their corresponding
components. If the distance is lesser than a pre-specified minimum separation, then the pdf value will
be lower and perhaps the sample is inappropriate. The discriminative term specified in (28) is
computationally simple since it does not involve the training examples and instead uses the sufficient
statistics.
Another option for the discriminative term is to use the likelihood of observations. The idea here is to
ensure that the Gaussian pdf value of an observation from class 𝑐 assigned to a component 𝑘 is larger
than the pdf value of competing classes and their corresponding components.
𝑝(𝜃\𝑐 | 𝜃𝑐, 𝜃𝑠, 𝑋, 𝑌) =
∏exp {−𝜉0max (0, 𝜁0 − (𝒩(𝑥𝑡
If we consider our model as a single component Gaussian instead of a HMM with Gaussian mixtures,
then (29) encourages that the pdf value for the correct label must be greater than the pdf value of
competing classes. The above discriminative term can be treated as an approximation to the empirical
error rate and 𝜁0 offers the flexibility for a soft margin.
By plugging in (28) or (29) into (17), we get the posterior distribution for the transform parameters.
We can sample Λ𝑘
𝑐, 𝜇𝑘 , Σ𝑘 and then 𝜌𝑘
𝑐, 𝜇𝑘 , Σ𝑘. Since the priors for both these variables are
Gaussian distribution, we can use Elliptical slice sampling as specified in section 5.2 for getting the
posterior updates. Note that if we have a non-zero mean as Gaussian prior, we have to perform a shift
to have zero mean.
Input: Training observations with their corresponding class labels and hyper parameters
Output: Samples of posterior parameters
1. Sample the initial values 𝛽, 𝜋, 𝜇1..𝐿, Σ1..𝐿, 𝜑1..𝐶, 𝜌1..𝐿
1..𝐶, Λ1..𝐿
1..𝐶 from their respective hyper
parameters.
2. Sample hidden state sequences 𝑧𝑡
𝑛 using HMM forward backward algorithm as per (23).
3. For all classes, compute the matrix of counts 𝑛𝑐 from the sampled hidden states.
4. For all classes and all states, determine the scaling factor 𝜔𝑗𝑘
𝑐 as per (25).
5. Sample the top level stick breaking weights 𝛽 according to (26) using an auxiliary count matrix.
6. Sample the state specific stick breaking weights 𝜋 for all states according to (26) using an
auxiliary count matrix.
7. Sample the class specific stick breaking weight 𝜑 for all classes and all states according to (26).
8. For all components, sample the shared covariance Σ𝑘 and then the mean 𝜇𝑘 as per (27).
9. For all classes and for all components, use (28) or (29) in (17) and sample the transform
parameters 𝜌𝑘
𝑐 using Elliptical slice sampling.
10. Sample the hyper parameters.
11. Repeat from step (2) to collect more samples.
Figure 4: Posterior Inference Algorithm
7 Experiments
We conduct our experiments on the MSR Action3D and UTKinect-Action datasets. The
datasets contain various actions performed by different subjects. However each action involves only
one individual and there is no human-object interaction. All these datasets use an infrared camera to
capture the depth image sequences. The datasets also contain annotated 3D joint positions of the
subjects. These joint positions were estimated from the depth image sequence as explained in and
may have errors when there are occlusions. We work with these noisy joint positions.
Figure 5: Examples of actions from the MSR-Action3D dataset left: Depth images right: the
corresponding joint positions.
7.1 Joint Position Features
Each depth image contains {𝑃𝑖}𝑖=1
20 ∈(𝑥, 𝑦, 𝑧) joint positions. We perform experiments on two types
of features – one based on a subset of pairwise relative joint positions within a frame and another
based on histogram of gradients that takes into account all combinations of joint positions and
includes adjacent frames. For the first feature type, we determine 19 joint position pairs (𝑃𝑖, 𝑃𝑗). The
pairs are defined based on a pre-defined skeleton hierarchy as outlined in Figure 6. The relative
positions 𝑃𝑖− 𝑃𝑗 are used as features. Hence 𝑥𝑡
𝑛 ∈ ℝ57. By using relative positions as features we
ensure invariance to uniform translation of the body.
Figure 6: Skeleton Hierarchy used for defining joint position pairs . The arrows indicate the parentchild joint pairs with Hip Center as the root joint.
For the second feature type, we compute the relative position of a joint to all the other joints in the
current frame and adjacent frames. Further, we use three 2D values instead of a single 3D value,
representing the projection of a relative position on the orthogonal (𝑥𝑦, 𝑦𝑧, 𝑥𝑧) Cartesian planes i.e.
for a joint 𝑖 the features are
𝑓𝑖(𝑥, 𝑦) = {(𝑃𝑖
𝑦) ∀ 𝑗 ∈𝑃(𝑡−1), 𝑃(𝑡), 𝑃(𝑡+ 1) ⋀ 𝑖≠𝑗}
𝑓𝑖(𝑦, 𝑧) = {(𝑃𝑖
𝑧) ∀ 𝑗 ∈𝑃(𝑡−1), 𝑃(𝑡), 𝑃(𝑡+ 1) ⋀ 𝑖≠𝑗}
𝑓𝑖(𝑥, 𝑧) = {(𝑃𝑖
𝑧) ∀ 𝑗 ∈𝑃(𝑡−1), 𝑃(𝑡), 𝑃(𝑡+ 1) ⋀ 𝑖≠𝑗}
𝑥(𝑡) is the 𝑥 co-ordinate of the 𝑖𝑡ℎ joint position at time 𝑡. We then assign the gradients
𝑓𝑖(𝑥, 𝑦) to a histogram of 8 bins based on the direction with the bin values being the gradient
magnitude. This technique is very similar to the Histogram of Oriented Gradients (HOG) .
Repeating the step for 𝑓𝑖(𝑦, 𝑧) and 𝑓𝑖(𝑥, 𝑧) we now have 24 bins for each joint. Concatenating the bins
Shoulder Center
Shoulder Left/Right
Elbow Left/Right
Wrist Left/Right
Hip Center
Hand Left/Right
Leg Left/Right
Knee Left/Right
Ankle Left/Right
Feet Left/Right
for all the joints, we have a descriptor of length 20*24 for a frame and thus 𝑥𝑡
𝑛 ∈ ℝ480. Finally we
apply Principle Component Analysis (PCA) and use a subset of the Eigen vectors as features.
Figure 7: Comparison of the HOG based descriptor – for two similar poses on the left, the
corresponding descriptor values appear overlapped when compared with the dis-similar poses on the
7.2 UTKinect-Action dataset
We show results from the UTKinect-Action dataset for human actions walk, sit-down, stand-up,
pick-up, carry, throw, pull, wave and clap-hands. All these actions were performed in indoor settings
with each action collected from 10 subjects and repeated twice. For each action, 60% of examples are
used for training and the rest for testing. We use features based on the pairwise relative joint positions
as shown in Figure 6 for this dataset.
Parametric HMM: We first train a classifier, independently for each class, based on classical HMM.
The standard Baum-Welch Expectation Maximization algorithm is used for learning the HMM
parameters. Since the number of states must be specified apriori for parametric HMMs, different
numbers of states for each class are tried during training. In the absence of priors, an additional
clustering step with K-Means is performed to estimate the initial values of transition matrix and the
mean and covariance parameters. During testing, we evaluate a test example against all the classes
and select the class with the largest (log) likelihood as the predicted class. Our observed best
classification accuracy was 58.2%. The summary of classification results for HMM is presented in Table
Number of States Accuracy (%)
Precision (%)
(Average across classes)
Recall (%)
(Average across classes)
Table 2: Classical Parametric HMM classification results
HDP-HMM: We also train a HDP-HMM classifier, independently for each class as before. We specify
an upper bound on the number of states (𝐿= 20) as explained in section 6.1. The number of states is
automatically learnt from the data for HDP-HMM unlike the parametric HMM. In Figure 8 the total
number of states for the different action classes in a sample collected during training is shown. In an
equivalent parametric HMM, we will have to run a tedious and adhoc model selection step individually
for each class since the optimum cardinality of states vary between classes. This advantange of
automatic state inference with HDP-HMM is reflected as an improved classification accuracy of 76.1%.
The results are shown in Table 3.
Figure 8: The number of hidden states being active for different action classes in a sample collected
during training. An active state is one in which at least one observation is assigned to this state.
Precision (%) Recall (%)
clap-hands 50
Table 3: HDP-HMM classification results
Multi-level HDP-HMM with Generative Learning: We evaluate our results on the two-level HDP-HMM
but exclude discriminative criterions. In this method, examples from all the classes are used during
parameter estimation. Thus it allows sharing of parameters across classes and enables semisupervised learning. In order to exclude the discriminative conditions for the state transitions, we
simply set the scaling factor 𝜔𝑗
𝑐 to zero. This is equivalent to sampling 𝜑𝑗𝑘
𝑐 (probability of transitioning
to state 𝑘 given we are in state 𝑗 for a class 𝑐) as per equation (10) instead of (26). Similarly for the
class specific transformation parameters, we set 𝑝(𝜃\𝑐) to be a constant in equation (17) thereby
excluding the discriminative conditions. The classification results are shown in Table 4 and the
accuracy is 77.4%. These results confirm that sharing of parameters across classes doesn’t make the
classification any worse. We interpret the lack of a big increase in accuracy when compared with HDP-
HMM as an indication that there is a need for some additional discriminative condition. In addition,
the smaller number of training examples in this dataset could have been a factor. Nevertheless, this
technique provides a viable way to learn parameters in situations where we can incorporate
unlabelled examples.
Precision (%) Recall (%)
clap-hands 58.3
Table 4: Two-level HDP-HMM with generative learning classification results
Multi-level HDP-HMM with Discriminative Learning: Finally, we evaluate our results on the two-level
HDP-HMM including the discriminative conditions. The confusion matrix is shown in Figure 9. It is
evident from the confusion matrix that the recognition rate is good for most actions. However, there
are a few mis-classifications for actions that involve very similar pose sequences. For example, some
sit-down actions are classified as stand-up, pick-up actions as sit-down and throw actions as claphands. We report an overall classification accuracy of 83.1%.
Figure 9: Confusion matrix for classification results on UTKinect-Action dataset
7.3 MSR-Action3D dataset
We also conduct our experiments on the MSR Action3D dataset. The dataset has 20 actions higharm-wave, horizontal-arm-wave, hammer, hand-catch, forward-punch, high-throw, draw-x, draw-tick,
draw-circle, hand-clap, two-hand-wave, side-boxing, bend, forward-kick, side-kick, jogging, tennisswing, tennis-serve, golf-swing, pickup-throw. The actions were performed by 10 subjects and each
one was repeated two or three times. Since some of the actions overlap, the actions are grouped into
three sets as in for performing classification. As before, we use 60% of examples for training
and the rest for testing. We use features based on HOG descriptor as shown in (30) for this dataset.
Using the multi-level HDP-HMM with discriminative learning, we report an overall classification
accuracy of 81.2%, 78.1% and 90.6% for the three sets respectively. The confusion matrix is shown in
Figure 10.
Figure 10: Confusion matrix for classification results on MSR Action3D dataset – top-left: Actions
horizontal-arm-wave, hammer, forward-punch, high-throw, hand-clap, bend, tennis-serve, pickupthrow, top-right: Actions high-arm-wave, hand-catch, draw-x, draw-tick, draw-circle, two-hand-wave,
forward-kick, side-boxing bottom: Actions high-throw, forward-kick, side-kick, jogging, tennis-swing,
tennis-serve, golf-swing, pickup-throw.
In this dataset as well, the classifier has incorrectly labelled few actions. These mis-classified actions
involve very similar pose sequences. In particular, some bend actions are classified as pickup-throw,
hand-clap actions as tennis-serve and hand-catch actions as arm-wave. The draw-x, draw-tick and
draw-circle actions are challenging to classify and has less labelling accuracy compared to other
Summary: A comparison of the classification results can be seen in Table 5. It is evident that the HDP-
HMM improves classification accuracy significantly when compared with a parametric HMM. The
multi-level HDP-HMM allows sharing the parameters across classes and it doesn’t make the
classification any worse. The introduction of discriminative conditions on the multi-level HDP-HMM
has improved the classification results. Our classifier accuracy in the MSR-Action3D dataset is better
than the other approaches in , and . However our accuracy is less when compared with
 , and for the UT-Kinect dataset.
(Accuracy %)
Parametric HMM
Multi-level HDP-HMM
(Generative Learning)
Multi-level HDP-HMM
(Discriminative
Xia et al 
Devanne et al 
Slama et al 
Table 5: Summary of classification results
Discussion: In our training, we completely exclude 40% of the subjects and use the instances of these
subjects as test examples. This makes classification more difficult than in the alternative arrangement,
in which training samples for all the subjects are included and only specific samples for each subject
are excluded. In , and a Leave-One-Out-Cross-Validation method is used. In a particular
iteration, they use only one observation sequence for testing and the rest of the observation
sequences are used for training. This procedure is repeated to include all the observation sequences
for testing and finally the average accuracy across iterations is reported. In our experiments, we
completely separate the training and test examples and use a more challenging cross subject
evaluation. This tests the variations of actions performed by different subjects in a more realistic
manner. Additionally, our features (relative joint position pairs) are much simpler and generic when
compared with the features used in . Inspite of the limited number of training examples, our
experiments prove the utility of using a multi-level HDP-HMM with discriminative learning for
classification purposes.
8 Conclusion
We have proposed an action classification method based on a multi-level HDP-HMM that shares
training examples across action classes. The non-parametric nature of HDP-HMM allows an
unbounded number of states. The normalized gamma process representation of the HDPs last level
and the usage of elliptical slice sampling has allowed the inference of the posterior parameters in a
discriminative way. Our experiments demonstrate the utility of this approach. We intend to broaden
the discriminative criterions and apply our technique to classify activities involving humans and