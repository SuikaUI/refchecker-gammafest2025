Dual Attention Network for Scene Segmentation
Jing Liu* 1
Haijie Tian
Yongjun Bao
Zhiwei Fang
Hanqing Lu
1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences
2Business Growth BU, JD.com 3 University of Chinese Academy of Sciences
{jun.fu,jliu,zhiwei.fang,luhq}@nlpr.ia.ac.cn,hjtian ,{liyong5,baoyongjun}@jd.com
In this paper, we address the scene segmentation task
by capturing rich contextual dependencies based on the
self-attention mechanism. Unlike previous works that capture contexts by multi-scale feature fusion, we propose a
Dual Attention Network (DANet) to adaptively integrate local features with their global dependencies. Speciﬁcally,
we append two types of attention modules on top of dilated
FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the feature at each
position by a weighted sum of the features at all positions.
Similar features would be related to each other regardless
of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by
integrating associated features among all channel maps.
We sum the outputs of the two attention modules to further
improve feature representation which contributes to more
precise segmentation results. We achieve new state-of-theart segmentation performance on three challenging scene
segmentation datasets, i.e., Cityscapes, PASCAL Context
and COCO Stuff dataset. In particular, a Mean IoU score
of 81.5% on Cityscapes test set is achieved without using
coarse data.1.
1. Introduction
Scene segmentation is a fundamental and challenging
problem, whose goal is to segment and parse a scene image into different image regions associated with semantic
categories including stuff (e.g. sky, road, grass) and discrete objects (e.g. person, car, bicycle). The study of this
*Corresponding Author
1Links can be found at 
Figure 1: The goal of scene segmentation is to recognize
each pixel including stuff, diverse objects.
The various
scales, occlusion and illumination changing of objects/stuff
make it challenging to parsing each pixel.
task can be applied to potential applications, such as automatic driving, robot sensing and image editing. In order to
accomplish the task of scene segmentation effectively, we
need to distinguish some confusing categories and take into
account objects with different appearance. For example, regions of ’ﬁeld’ and ’grass’ are often indistinguishable, and
the objects of ’cars’ may often be affected by scales, occlusion and illumination. Therefore, it is necessary to enhance the discriminative ability of feature representations
for pixel-level recognition.
Recently, state-of-the-art methods based on Fully Convolutional Networks (FCNs) have been proposed to address the above issues. One way is to utilize the multi-scale
context fusion. For example, some works aggregate multi-scale contexts via combining feature maps generated by different dilated convolutions and pooling operations. And some works capture richer global context information by enlarging the kernel size with a decomposed structure or introducing an effective encoding layer
on top of the network. In addition, the encoder-decoder
 
structures are proposed to fuse mid-level and
high-level semantic features. Although the context fusion
helps to capture different scales objects, it can not leverage
the relationship between objects or stuff in a global view,
which is also essential to scene segmentation.
Another type of methods employs recurrent neural networks to exploit long-range dependencies, thus improving
scene segmentation accuracy.
The method based on 2D
LSTM networks is proposed to capture complex spatial dependencies on labels. The work builds a recurrent neural network with directed acyclic graph to capture
the rich contextual dependencies over local features. However, these methods capture the global relationship implicitly with recurrent neural networks, whose effectiveness relies heavily on the learning outcome of the long-term memorization.
To address above problems, we propose a novel framework, called as Dual Attention Network (DANet), for natural scene image segmentation, which is illustrated in Figure.
2. It introduces a self-attention mechanism to capture features dependencies in the spatial and channel dimensions
respectively. Speciﬁcally, we append two parallel attention
modules on top of dilated FCN. One is a position attention
module and the other is a channel attention module. For the
position attention module, we introduce the self-attention
mechanism to capture the spatial dependencies between any
two positions of the feature maps. For the feature at a certain position, it is updated via aggregating features at all
positions with weighted summation, where the weights are
decided by the feature similarities between the corresponding two positions. That is, any two positions with similar
features can contribute mutual improvement regardless of
their distance in spatial dimension. For the channel attention module, we use the similar self-attention mechanism to
capture the channel dependencies between any two channel
maps, and update each channel map with a weighted sum of
all channel maps. Finally, the outputs of these two attention
modules are fused to further enhance the feature representations.
It should be noted that our method is more effective and
ﬂexible than previous methods when dealing with
complex and diverse scenes. Take the street scene in Figure. 1 as an example. First, some ’person’ and ’trafﬁc light’
in the ﬁrst row are inconspicuous or incomplete objects due
to lighting and view. If simple contextual embedding is explored, the context from dominated salient objects (e.g. car,
building) would harm those inconspicuous object labeling.
By contrast, our attention model selectively aggregates the
similar features of inconspicuous objects to highlight their
feature representations and avoid the inﬂuence of salient
objects. Second, the scales of the ’car’ and ’person’ are
diverse, and recognizing such diverse objects requires contextual information at different scales. That is, the features
at different scale should be treated equally to represent the
same semantics. Our model with attention mechanism just
aims to adaptively integrate similar features at any scales
from a global view, and this can solve the above problem to
some extent. Third, we explicitly take spatial and channel
relationships into consideration, so that scene understanding could beneﬁt from long-range dependencies.
Our main contributions can be summarized as follows:
• We propose a novel Dual Attention Network (DANet)
with self-attention mechanism to enhance the discriminant ability of feature representations for scene segmentation.
• A position attention module is proposed to learn the
spatial interdependencies of features and a channel attention module is designed to model channel interdependencies. It signiﬁcantly improves the segmentation
results by modeling rich contextual dependencies over
local features.
• We achieve new state-of-the-art results on three popular benchmarks including Cityscapes dataset , PAS-
CAL Context dataset and COCO Stuff dataset .
2. Related Work
Semantic Segmentation. Fully Convolutional Networks
(FCNs) based methods have made great progress in semantic segmentation. There are several model variants proposed
to enhance contextual aggregation.
First, Deeplabv2 
and Deeplabv3 adopt atrous spatial pyramid pooling
to embed contextual information, which consist of parallel dilated convolutions with different dilated rates. PSP-
Net designs a pyramid pooling module to collect the
effective contextual prior, containing information of different scales. The encoder-decoder structures [?, 6, 8, 9] fuse
mid-level and high-level semantic features to obtain different scale context. Second, learning contextual dependencies over local features also contribute to feature representations. DAG-RNN models directed acyclic graph with
recurrent neural network to capture the rich contextual dependencies. PSANet captures pixel-wise relation by a
convolution layer and relative position information in spatial dimension. The concurrent work OCNet adopts
self-attention mechanism with ASPP to exploit context dependencies. In addition, EncNet introduces a channel
attention mechanism to capture global context.
Self-attention Modules.
Attention modules can model
long-range dependencies and have been widely applied in
many tasks . In particular, the work 
is the ﬁrst to propose the self-attention mechanism to draw
global dependencies of inputs and applies it in machine
translation. Meanwhile, attention modules are increasingly
Channel Attention Module
Position Attention Module
convolution layer
Spatial attention matrix
Channel attention matrix
Matrix multiplication
Element-wise Sum
(HXW)X(HXW)
Channel matrix operation
Spatial matrix operation
Sum fusion
Figure 2: An overview of the Dual Attention Network. (Best viewed in color)
applied in image vision ﬂied. The work introduces selfattention mechanism to learn a better image generator. The
work , which is related to self-attention module, mainly
exploring effectiveness of non-local operation in spacetime
dimension for videos and images.
Different from previous works, we extend the selfattention mechanism in the task of scene segmentation, and
carefully design two types of attention modules to capture
rich contextual relationships for better feature representations with intra-class compactness. Comprehensive empirical results verify the effectiveness of our proposed method.
3. Dual Attention Network
In this section, we ﬁrst present a general framework of
our network and then introduce the two attention modules
which capture long-range contextual information in spatial
and channel dimension respectively. Finally we describe
how to aggregate them together for further reﬁnement.
3.1. Overview
Given a picture of scene segmentation, stuff or objects,
are diverse on scales, lighting, and views. Since convolution
operations would lead to a local receptive ﬁeld, the features
corresponding to the pixels with the same label may have
some differences. These differences introduce intra-class
inconsistency and affect the recognition accuracy. To address this issue, we explore global contextual information
by building associations among features with the attention
mechanism. Our method could adaptively aggregate longrange contextual information, thus improving feature representation for scene segmentation.
As illustrated in Figure. 2, we design two types of attention modules to draw global context over local features
generated by a dilated residual network, thus obtaining better feature representations for pixel-level prediction. We
employ a pretrained residual network with the dilated strategy as the backbone. Noted that we remove the downsampling operations and employ dilated convolutions in the
last two ResNet blocks, thus enlarging the size of the ﬁnal feature map size to 1/8 of the input image. It retains
more details without adding extra parameters. Then the features from the dilated residual network would be fed into
two parallel attention modules. Take the spatial attention
modules in the upper part of the Figure. 2 as an example,
we ﬁrst apply a convolution layer to obtain the features of
dimension reduction. Then we feed the features into the
position attention module and generate new features of spatial long-range contextual information through the following three steps. The ﬁrst step is to generate a spatial attention matrix which models the spatial relationship between
any two pixels of the features. Next, we perform a matrix
multiplication between the attention matrix and the original
features. Third, we perform an element-wise sum operation on the above multiplied resulting matrix and original
features to obtain the ﬁnal representations reﬂecting longrange contexts. Meanwhile, long-range contextual informa-
(HXW)X(HXW)
reshape & transpose
A. Position attention module
softmax CXC
B. Channel attention module
reshape & transpose
Figure 3: The details of Position Attention Module and
Channel Attention Module are illustrated in (A) and (B).
(Best viewed in color)
tion in channel dimension are captured by a channel attention module. The process of capturing the channel relationship is similar to the position attention module except for
the ﬁrst step, in which channel attention matrix is calculated in channel dimension. Finally we aggregate the outputs from the two attention modules to obtain better feature
representations for pixel-level prediction.
3.2. Position Attention Module
Discriminant feature representations are essential for
scene understanding, which could be obtained by capturing
long-range contextual information. However, many works
 suggest that local features generated by traditional
FCNs could lead to misclassiﬁcation of objects and stuff.
In order to model rich contextual relationships over local
features, we introduce a position attention module. The position attention module encodes a wider range of contextual
information into local features, thus enhancing their representation capability. Next, we elaborate the process to adaptively aggregate spatial contexts.
As illustrated in Figure.3(A), given a local feature A ∈
RC×H×W , we ﬁrst feed it into a convolution layers to generate two new feature maps B and C, respectively, where
{B, C} ∈RC×H×W . Then we reshape them to RC×N,
where N = H × W is the number of pixels. After that we
perform a matrix multiplication between the transpose of C
and B, and apply a softmax layer to calculate the spatial
attention map S ∈RN×N:
exp(Bi · Cj)
i=1 exp(Bi · Cj)
where sji measures the ith position’s impact on jth position. The more similar feature representations of the two
position contributes to greater correlation between them.
Meanwhile, we feed feature A into a convolution layer
to generate a new feature map D ∈RC×H×W and reshape
it to RC×N. Then we perform a matrix multiplication between D and the transpose of S and reshape the result to
RC×H×W . Finally, we multiply it by a scale parameter α
and perform a element-wise sum operation with the features
A to obtain the ﬁnal output E ∈RC×H×W as follows:
(sjiDi) + Aj
where α is initialized as 0 and gradually learns to assign
more weight . It can be inferred from Equation 2 that
the resulting feature E at each position is a weighted sum
of the features across all positions and original features.
Therefore, it has a global contextual view and selectively
aggregates contexts according to the spatial attention map.
The similar semantic features achieve mutual gains, thus
imporving intra-class compact and semantic consistency.
3.3. Channel Attention Module
Each channel map of high level features can be regarded as a class-speciﬁc response, and different semantic
responses are associated with each other. By exploiting the
interdependencies between channel maps, we could emphasize interdependent feature maps and improve the feature
representation of speciﬁc semantics. Therefore, we build
a channel attention module to explicitly model interdependencies between channels.
The structure of channel attention module is illustrated in
Figure.3(B). Different from the position attention module,
we directly calculate the channel attention map X ∈RC×C
from the original features A ∈RC×H×W . Speciﬁcally,
we reshape A to RC×N, and then perform a matrix multiplication between A and the transpose of A. Finally, we
apply a softmax layer to obtain the channel attention map
exp(Ai · Aj)
i=1 exp(Ai · Aj)
where xji measures the ith channel’s impact on the jth
channel. In addition, we perform a matrix multiplication
between the transpose of X and A and reshape their result
to RC×H×W . Then we multiply the result by a scale parameter β and perform an element-wise sum operation with
A to obtain the ﬁnal output E ∈RC×H×W :
(xjiAi) + Aj
where β gradually learns a weight from 0. The Equation 4
shows that the ﬁnal feature of each channel is a weighted
sum of the features of all channels and original features,
which models the long-range semantic dependencies between feature maps. It helps to boost feature discriminability.
Noted that we do not employ convolution layers to embed features before computing relationshoips of two channels, since it can maintain relationship between different
channel maps. In addition, different from recent works 
which explores channel relationships by a global pooling or
encoding layer, we exploit spatial information at all corresponding positions to model channel correlations.
3.4. Attention Module Embedding with Networks
In order to take full advantage of long-range contextual
information, we aggregate the features from these two attention modules. Speciﬁcally, we transform the outputs of
two attention modules by a convolution layer and perform
an element-wise sum to accomplish feature fusion. At last
a convolution layer is followed to generate the ﬁnal prediction map. We do not adopt cascading operation because it
needs more GPU memory. Noted that our attention modules are simple and can be directly inserted in the existing
FCN pipeline. They do not increase too many parameters
yet strengthen feature representations effectively.
4. Experiments
To evaluate the proposed method, we carry out comprehensive experiments on Cityscapes dataset , PAS-
CAL VOC2012 , PASCAL Context dataset and
COCO Stuff dataset . Experimental results demonstrate
that DANet achieves state-of-the-art performance on three
In the next subsections, we ﬁrst introduce the
datasets and implementation details, then we perform a series of ablation experiments on Cityscapes dataset. Finally,
we report our results on PASCAL VOC 2012, PASCAL
Context and COCO Stuff.
4.1. Datasets and Implementation Details
Cityscapes The dataset has 5,000 images captured from 50
different cities. Each image has 2048 × 1024 pixels, which
have high quality pixel-level labels of 19 semantic classes.
There are 2,979 images in training set, 500 images in validation set and 1,525 images in test set. We do not use coarse
data in our experiments.
PASCAL VOC 2012 The dataset has 10,582 images for
training, 1,449 images for validation and 1,456 images for
testing, which involves 20 foreground object classes and
one background class.
PASCAL Context The dataset provides detailed semantic
labels for whole scenes, which contains 4,998 images for
training and 5,105 images for testing. Following ,
we evaluate the method on the most frequent 59 classes
along with one background category (60 classes in total).
Without PAM
Groundtruth
Figure 4: Visualization results of position attention module
on Cityscapes val set.
Without CAM
Groundtruth
Figure 5: Visualization results of channel attention module
on Cityscapes val set.
COCO Stuff The dataset contains 9,000 images for training
and 1,000 images for testing. Following , we report
our results on 171 categories including 80 objects and 91
stuff annotated to each pixel.
Implementation Details
We implement our method based on Pytorch. Following
 , we employ a poly learning rate policy where the initial learning rate is multiplied by (1−
total iter)0.9 after each
iteration. The base learning rate is set to 0.01 for Cityscapes
dataset. Momentum and weight decay coefﬁcients are set to
0.9 and 0.0001 respectively. We train our model with Synchronized BN . Batchsize are set to 8 for Cityscapes
and 16 for other datasets.When adopting multi-scale augmentation, we set training time to 180 epochs for COCO
Stuff and 240 epochs for other datasets. Following , we
adopt multi-loss on the end of the network when both two
attention modules are used. For data augmentation, we apply random cropping (cropsize 768) and random left-right
ﬂipping during training in the ablation study for Cityscapes
4.2. Results on Cityscapes Dataset
Ablation Study for Attention Modules
We employ the dual attention modules on top of the dilation network to capture long-range dependencies for better
scene understanding. To verify the performance of attention
Dilated FCN
Dilated FCN
Table 1: Ablation study on Cityscapes val set. PAM represents Position Attention Module, CAM represents Channel
Attention Module.
modules, we conduct experiments with different settings in
As shown in Table 1, the attention modules improve
the performance remarkably.
Compared with the baseline FCN (ResNet-50), employing position attention module yields a result of 75.74% in Mean IoU , which brings
5.71% improvement. Meanwhile, employing channel contextual module individually outperforms the baseline by
4.25%. When we integrate the two attention modules together, the performance further improves to 76.34%. Furthermore, when we adopt a deeper pre-trained network
(ResNet-101), the network with two attention modules signiﬁcantly improves the segmentation performance over the
baseline model by 5.03%. Results show that attention modules bring great beneﬁt to scene segmentation.
The effects of position attention modules can be visualized in Figure.4. Some details and object boundaries are
clearer with the position attention module, such as the ’pole’
in the ﬁrst row and the ’sidewalk’ in the second row. Selective fusion over local features enhance the discrimination
of details. Meanwhile, Figure.5 demonstrate that, with our
channel attention module, some misclassiﬁed category are
now correctly classiﬁed, such as the ’bus’ in the ﬁrst and
third row. The selective integration among channel maps
helps to capture context information. The semantic consistency have been improved obviously.
Ablation Study for Improvement Strategies
Following , we adopt the same strategies to improve performance further. (1) DA: Data augmentation with random
scaling. (2) Multi-Grid: we apply employ a hierarchy of
grids of different sizes (4,8,16) in the last ResNet block. (3)
MS: We average the segmentation probability maps from 8
image scales{0.5 0.75 1 1.25 1.5 1.75 2 2.2} for inference.
Experimental results are shown in Table 2. Data augmentation with random scaling improves the performance
by almost 1.26%, which shows that network beneﬁts from
enriching scale diversity of training data. We adopt Multi-
Multi-Grid
Table 2: Performance comparison between different strategies on Cityscape val set. DANet-101 represents DANet
with BaseNet ResNet-101, DA represents data augmentation with random scaling. Multi-Grid represents employing
multi-grid method, MS represents multi-scale inputs during
inference.
Grid to obtain better feature representations of pretrained
network, which further achieves 1.11% improvements. Finally, segmentation map fusion further improves the performance to 81.50%, which outperforms well-known method
Deeplabv3 (79.30% on Cityscape val set) by 2.20%.
Visualization of Attention Module
For position attention, the overall self-attention map is in
size of (H × W) × (H × W), which means that for each
speciﬁc point in the image, there is an corresponding subattention map whose size is (H × W). In Figure.6, for each
input image, we select two points (marked as #1 and #2)
and show their corresponding sub-attention map in columns
2 and 3 respectively. We observe that the position attention
module could capture clear semantic similarity and longrange relationships. For example, in the ﬁrst row, the red
point #1 are marked on a building and its attention map
(in column 2) highlights most the areas where the buildings
lies on. Moreover, in the sub-attention map, the boundaries
are very clear even though some of them are far away from
the point #1. As for the point #2, its attention map focuses on most positions labeled as ”car”. In the second row,
the same holds for the ’trafﬁc sign’ and ’person’ in global
region, even though the number of corresponding pixels is
less. The third row is for the ’vegetation’ class and ’person’ class. In particular, the point #2 does not respond to
the nearby ’rider’ class, but it does respond to the ’person’
For channel attention, it is hard to give comprehensible
visualization about the attention map directly. Instead, we
show some attended channels to see whether they highlight
clear semantic areas. In Figure.6, we display the eleventh
and fourth attended channels in column 4 and 5. We ﬁnd
that the response of speciﬁc semantic is noticeable after
channel attention module enhances.
For example, 11th
channel map responds to the ’car’ class in all three examples, and 4th channel map is for the ’vegetation’ class,
which beneﬁts for the segmentation of two scene categories.
Groundtruth
Channel map #4
Sub-attention map #1 Sub-attention map #2 Channel map #11
Figure 6: Visualization results of attention modules on Cityscapes val set. For each row, we show an input image, two subattention maps (H ×W) corresponding to the ponits marked in the input image. Meanwhile, we give two channel maps from
the outputs of channel attention module, where the maps are from 4th and 11th channels, respectively. Finally, corresponding
result and groundtruth are provided.
trafﬁc light
trafﬁc sign
vegetation
motorcycle
DeepLab-v2 
ReﬁneNet 
ResNet-38 
PSPNet 
BiSeNet 
PSANet 
DenseASPP 
Table 3: Per-class results on Cityscapes testing set. DANet outperforms existing approaches and achieves 81.5% in Mean
In short, these visualizations further demonstrate the necessity of capturing long-range dependencies for improving
feature representation in scene segmentation.
Comparing with State-of-the-art
We further compare our method with existing methods on
the Cityscapes testing set. Speciﬁcally, we train our DANet-
101 with only ﬁne annotated data and submit our test results to the ofﬁcial evaluation server. Results are shown
in Table 3. DANet outperforms existing approaches with
dominantly advantage. In particular, our model outperforms
PSANet by a large margin with the same backbone
ResNet-101. Moreover, it also surpasses DenseASPP ,
which use more powerful pretrained models than ours.
4.3. Results on PASCAL VOC 2012 Dataset
We carry out experiments on the PASCAL VOC 2012
dataset to further evaluate the effectiveness of our method.
Quantitative results of PASCAL VOC 2012 val set are
Dilated FCN
Table 4: Ablation study on PASCAL VOC 2012 val set.
PAM represents Position Attention Module, CAM represents Channel Attention Module.
shown in Table. 4. Our attention modules improves performance signiﬁcantly, where DANet-50 exceeds the baseline
by 3.3%. When we adopt a deeper network ResNet-101,
the model further achieves a Mean IoU of 80.4%. Following , we employ the PASCAL VOC 2012 trainval
set further ﬁne-tune our best model. The results of PASCAL
VOC2012 on test set is are shown in Table 5.
DeepLab-v2(Res101-COCO) 
Piecewise 
ResNet38 
PSPNet(Res101) 
EncNet (Res101) 
DANet(Res101)
Table 5: Segmentation results on PASCAL VOC 2012 testing set.
FCN-8s 
Piecewise 
DeepLab-v2 (Res101-COCO) 
ReﬁneNet (Res152) 
PSPNet (Res101) 
Ding et al.( Res101) 
EncNet (Res101) 
Dilated FCN(Res50)
DANet (Res50)
DANet (Res101)
Table 6: Segmentation results on PASCAL Context testing
4.4. Results on PASCAL Context Dataset
In this subsection, we carry out experiments on the PAS-
CAL Context dataset to further evaluate the effectiveness of
our method. We adopt the same training and testing settings on PASCAL VOC 2012 dataset. Quantitative results
of PASCAL Context are shown in Table.
The baseline (Dilated FCN-50) yields Mean IoU 44.3%. DANet-50
boosts the performance to 50.1%. Furthermore, with a deep
pretrained network ResNet101, our model results achieve
Mean IoU 52.6%, which outperforms previous methods
by a large margin.
Among previous works, Deeplab-v2
and ReﬁneNet adopt multi-scale feature fusion by different
atrous convolution or different stage of encoder. In addition,
they trained their model with extra COCO data or adopt a
deeper model (ResNet152) to improve their segmentation
results. Different from the previous methods, we introduce
attention modules to capture global dependencies explicitly,
and the proposed method can achieve better performance.
4.5. Results on COCO Stuff Dataset
We also conduct experiments on the COCO Stuff dataset
to verify the generalization of our proposed network. Com-
FCN-8s 
DeepLab-v2(Res101) 
DAG-RNN 
ReﬁneNet (Res101) 
Ding et al.( Res101) 
Dilated FCN (Res50)
DANet (Res50)
DANet (Res101)
Table 7: Segmentation results on COCO Stuff testing set.
parisons with previous state-of-the-art methods are reported
in Table. 7. Results show that our model achieves 39.7%
in Mean IoU, which outperforms these methods by a large
margin. Among the compared methods, DAG-RNN 
utilizes chain-RNNs for 2D images to model rich spatial dependencies, and Ding et al. adopts a gating mechanism
in the decoder stage for improving inconspicuous objects
and background stuff segmentation. our method could capture long-range contextual information more effectively and
learn better feature representation in scene segmentation.
5. Conclusion
In this paper, we have presented a Dual Attention Network (DANet) for scene segmentation, which adaptively
integrates local semantic features using the self-attention
mechanism. Speciﬁcally, we introduce a position attention
module and a channel attention module to capture global
dependencies in the spatial and channel dimensions respectively. The ablation experiments show that dual attention
modules capture long-range contextual information effectively and give more precise segmentation results. Our attention network achieves outstanding performance consistently on four scene segmentation datasets, i.e. Cityscapes,
Pascal VOC 2012, Pascal Context, and COCO Stuff. In
addition, it is important to decrease the computational complexity and enhance the robustness of the model, which will
be studied in future work.
Acknowledgment
This work was supported by Beijing Natural Science
Foundation (4192059) and National Natural Science Foundation of China (61872366, 61472422 and 61872364).