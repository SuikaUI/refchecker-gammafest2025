NetAdapt: Platform-Aware Neural Network
Adaptation for Mobile Applications
Tien-Ju Yang1⋆[0000−0003−4728−0321], Andrew Howard2, Bo Chen2,
Xiao Zhang2, Alec Go2, Mark Sandler2, Vivienne Sze1, and Hartwig Adam2
1 Massachusetts Institute of Technology
2 Google Inc.
{tjy,sze}@mit.edu, {howarda,bochen,andypassion,ago,sandler,hadam}@google.com
Abstract. This work proposes an algorithm, called NetAdapt, that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify
networks based on the number of MACs or weights, optimizing those
indirect metrics may not necessarily reduce the direct metrics, such as
latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics
are evaluated using empirical measurements, so that detailed knowledge
of the platform and toolchain is not required. NetAdapt automatically
and progressively simpliﬁes a pre-trained network until the resource budget is met while maximizing the accuracy. Experiment results show that
NetAdapt achieves better accuracy versus latency trade-oﬀs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simpliﬁcation algorithms. For image classiﬁcation on the
ImageNet dataset, NetAdapt achieves up to a 1.7× speedup in measured
inference latency with equal or higher accuracy on MobileNets (V1&V2).
Introduction
Deep neural networks (DNNs or networks) have become an indispensable component of artiﬁcial intelligence, delivering near or super-human accuracy on common vision tasks such as image classiﬁcation and object detection. However,
DNN-based AI applications are typically too computationally intensive to be
deployed on resource-constrained platforms, such as mobile phones. This hinders the enrichment of a large set of user experiences.
A signiﬁcant amount of recent work on DNN design has focused on improving
the eﬃciency of networks. However, the majority of works are based on optimizing the “indirect metrics”, such as the number of multiply-accumulate operations
(MACs) or the number of weights, as proxies for the resource consumption of
a network. Although these indirect metrics are convenient to compute and integrate into the optimization framework, they may not be good approximations
to the “direct metrics” that matter for the real applications such as latency
⋆This work was done while Tien-Ju Yang was an intern at Google.
 
T.-J. Yang et al.
Network Proposals
Empirical Measurements
Proposal A
Proposal Z
Pretrained
Fig. 1. NetAdapt automatically adapts a pretrained network to a mobile platform
given a resource budget. This algorithm is guided by the direct metrics for resource
consumption. NetAdapt eliminates the requirement of platform-speciﬁc knowledge by
using empirical measurements to evaluate the direct metrics. At each iteration, NetAdapt generates many network proposals and measures the proposals on the target
platform. The measurements are used to guide NetAdapt to generate the next set of
network proposals at the next iteration.
and energy consumption. The relationship between an indirect metric and the
corresponding direct metric can be highly non-linear and platform-dependent as
observed by . In this work, we will also demonstrate empirically that
a network with a fewer number of MACs can be slower when actually running
on mobile devices; speciﬁcally, we will show that a network of 19% less MACs
incurs 29% longer latency in practice (see Table 1).
There are two common approaches to designing eﬃcient network architectures. The ﬁrst is designing a single architecture with no regard to the underlying
platform. It is hard for a single architecture to run optimally on all the platforms
due to the diﬀerent platform characteristics. For example, the fastest architecture on a desktop GPU may not be the fastest one on a mobile CPU with the
same accuracy. Moreover, there is little guarantee that the architecture could
meet the resource budget (e.g., latency) on all platforms of interest. The second
approach is manually crafting architectures for a given target platform based
on the platform’s characteristics. However, this approach requires deep knowledge about the implementation details of the platform, including the toolchains,
the conﬁguration and the hardware architecture, which are generally unavailable
given the proprietary nature of hardware and the high complexity of modern systems. Furthermore, manually designing a diﬀerent architecture for each platform
can be taxing for researchers and engineers.
In this work, we propose a platform-aware algorithm, called NetAdapt, to
address the aforementioned issues and facilitate platform-speciﬁc DNN deploy-
ment. NetAdapt (Fig. 1) incorporates direct metrics in the optimization loop, so
it does not suﬀer from the discrepancy between the indirect and direct metrics.
The direct metrics are evaluated by the empirical measurements taken from the
target platform. This enables the algorithm to support any platform without
detailed knowledge of the platform itself, although such knowledge could still be
incorporated into the algorithm to further improve results. In this paper, we use
latency as the running example of a direct metric and resource to target even
though our algorithm is generalizable to other metrics or a combination of them
(Sec. 4.3).
The network optimization of NetAdapt is carried out in an automatic way to
gradually reduce the resource consumption of a pretrained network while maximizing the accuracy. The optimization runs iteratively until the resource budget
is met. Through this design, NetAdapt can generate not only a network that
meets the budget, but also a family of simpliﬁed networks with diﬀerent tradeoﬀs, which allows dynamic network selection and further study. Finally, instead
of being a black box, NetAdapt is designed to be easy to interpret. For example, through studying the proposed network architectures and the corresponding
empirical measurements, we can understand why a proposal is chosen and this
sheds light on how to improve the platform and network design.
The main contributions of this paper are:
– A framework that uses direct metrics when optimizing a pretrained network
to meet a given resource budget. Empirical measurements are used to evaluate the direct metrics such that no platform-speciﬁc knowledge is required.
– An automated constrained network optimization algorithm that maximizes
accuracy while satisfying the constraints (i.e., the predeﬁned resource budget). The algorithm outperforms the state-of-the-art automatic network simpliﬁcation algorithms by up to 1.7× in terms of reduction in measured inference latency while delivering equal or higher accuracy. Moreover, a family
of simpliﬁed networks with diﬀerent trade-oﬀs will be generated to allow
dynamic network selection and further study.
– Experiments that demonstrate the eﬀectiveness of NetAdapt on diﬀerent
platforms and on real-time-class networks, such as the small MobileNetV1,
which is more diﬃcult to simplify than larger networks.
Related Work
There is a large body of work that aims to simplify DNNs. We refer the readers
to for a comprehensive survey, and summarize the main approaches below.
The most related works are pruning-based methods. aim to remove
individual redundant weights from DNNs. However, most platforms cannot fully
take advantage of unstructured sparse ﬁlters . Hu et al. and Srinivas et
al. focus on removing entire ﬁlters instead of individual weights. The drawback of these methods is the requirement of manually choosing the compression
rate for each layer. MorphNet leverages the sparsifying regularizers to automatically determine the layerwise compression rate. ADC uses reinforcement
T.-J. Yang et al.
learning to learn a policy for choosing the compression rates. The crucial difference between all the aforementioned methods and ours is that they are not
guided by the direct metrics, and thus may lead to sub-optimal performance, as
we see in Sec. 4.3.
Energy-aware pruning uses an energy model and incorporates the
estimated energy numbers into the pruning algorithm. However, this requires designing models to estimate the direct metrics of each target platform, which requires detailed knowledge of the platform including its hardware architecture ,
and the network-to-array mapping used in the toolchain . NetAdapt does not
have this requirement since it can directly use empirical measurements.
DNNs can also be simpliﬁed by approaches that involve directly designing ef-
ﬁcient network architectures, decomposition or quantization. MobileNets 
and ShuﬄeNets provide eﬃcient layer operations and reference architecture
design. Layer-decomposition-based algorithms exploit matrix decomposition to reduce the number of operations. Quantization reduces
the complexity by decreasing the computation accuracy. The proposed algorithm, NetAdapt, is complementary to these methods. For example, NetAdapt
can adapt MobileNets to further push the frontier of eﬃcient networks as shown
in Sec. 4 even though MobileNets are more compact and much harder to simplify
than the other larger networks, such as VGG .
Methodology: NetAdapt
We propose an algorithm, called NetAdapt, that will allow a user to automatically simplify a pretrained network to meet the resource budget of a platform
while maximizing the accuracy. NetAdapt is guided by direct metrics for resource
consumption, and the direct metrics are evaluated by using empirical measurements, thus eliminating the requirement of detailed platform-speciﬁc knowledge.
Problem Formulation
NetAdapt aims to solve the following non-convex constrained problem:
subject to
Resj(Net) ≤Budj, j = 1, . . . , m,
where Net is a simpliﬁed network from the initial pretrained network, Acc(·)
computes the accuracy, Resj(·) evaluates the direct metric for resource consumption of the jth resource, and Budj is the budget of the jth resource and
the constraint on the optimization. The resource can be latency, energy, memory
footprint, etc., or a combination of these metrics.
Based on an idea similar to progressive barrier methods , NetAdapt breaks
this problem into the following series of easier problems and solves it iteratively:
subject to
Resj(Neti) ≤Resj(Neti−1) −∆Ri,j, j = 1, . . . , m,
Algorithm 1: NetAdapt
Input: Pretrained Network: Net0 (with K CONV and FC layers), Resource
Budget: Bud, Resource Reduction Schedule: ∆Ri
Output: Adapted Network Meeting the Resource Budget:
2 Resi = TakeEmpiricalMeasurement(Neti);
3 while Resi > Bud do
Con = Resi - ∆Ri;
for k from 1 to K do
/* TakeEmpiricalMeasurement is also called inside
ChooseNumFilters for choosing the correct number of filters
that satisfies the constraint (i.e., current budget). */
N Filtk, Res Simpk = ChooseNumFilters(Neti, k, Con);
Net Simpk = ChooseWhichFilters(Neti, k, N Filtk);
Net Simpk = ShortTermFineTune(Net Simpk);
Neti+1, Resi+1 = PickHighestAccuracy(Net Simp:, Res Simp:);
i = i + 1;
Net = LongTermFineTune(Neti);
where Neti is the network generated by the ith iteration, and Net0 is the initial
pretrained network. As the number of iterations increases, the constraints (i.e.,
current resource budget Resj(Neti−1)−∆Ri,j) gradually become tighter. ∆Ri,j,
which is larger than zero, indicates how much the constraint tightens for the jth
resource in the ith iteration and can vary from iteration to iteration. This is
referred to as “resource reduction schedule”, which is similar to the concept of
learning rate schedule. The algorithm terminates when Resj(Neti−1) −∆Ri,j
is equal to or smaller than Budj for every resource type. It outputs the ﬁnal
adapted network and can also generate a sequence of simpliﬁed networks (i.e.,
the highest accuracy network from each iteration Net1, ..., Neti) to provide the
eﬃcient frontier of accuracy and resource consumption trade-oﬀs.
Algorithm Overview
For simplicity, we assume that we only need to meet the budget of one resource,
speciﬁcally latency. One method to reduce the latency is to remove ﬁlters from
the convolutional (CONV) or fully-connected (FC) layers. While there are other
ways to reduce latency, we will use this approach to demonstrate NetAdapt.
The NetAdapt algorithm is detailed in pseudo code in Algorithm 1 and in
Fig. 2. Each iteration solves Eq. 2 by reducing the number of ﬁlters in a single
CONV or FC layer (the Choose # of Filters and Choose Which Filters
blocks in Fig. 2). The number of ﬁlters to remove from a layer is guided by
empirical measurements. NetAdapt removes entire ﬁlters instead of individual
weights because most platforms can take advantage of removing entire ﬁlters,
T.-J. Yang et al.
Pretrained Network
# of Filters
Which Filters
Short-Term
# of Filters
Which Filters
Short-Term
Pick Highest
Within Budget
Adapted Network
Fig. 2. This ﬁgure visualizes the algorithm ﬂow of NetAdapt. At each iteration, NetAdapt decreases the resource consumption by simplifying (i.e., removing ﬁlters from)
one layer. In order to maximize accuracy, it tries to simplify each layer individually
and picks the simpliﬁed network that has the highest accuracy. Once the target budget
is met, the chosen network is then ﬁne-tuned again until convergence.
and this strategy allows reducing both ﬁlters and feature maps, which play an
important role in resource consumption . The simpliﬁed network is then
ﬁne-tuned for a short length of time in order to restore some accuracy (the
Short-Term Fine-Tune block).
In each iteration, the previous three steps (highlighted in bold) are applied on
each of the CONV or FC layers individually3. As a result, NetAdapt generates
K (i.e., the number of CONV and FC layers) network proposals in one iteration,
each of which has a single layer modiﬁed from the previous iteration. The network
proposal with the highest accuracy is carried over to the next iteration (the
Pick Highest Accuracy block). Finally, once the target budget is met, the
chosen network is ﬁne-tuned again until convergence (the Long-Term Fine-
Tune block).
Algorithm Details
This section describes the key blocks in the NetAdapt algorithm (Fig. 2).
Choose Number of Filters This step focuses on determining how many
ﬁlters to preserve in a speciﬁc layer based on empirical measurements. NetAdapt
gradually reduces the number of ﬁlters in the target layer and measures the
resource consumption of each of the simpliﬁed networks. The maximum number
3 The algorithm can also be applied to a group of multiple layers as a single unit
(instead of a single layer). For example, in ResNet , we can treat a residual block
as a single unit to speed up the adaptation process.
# Channels
# Channels
6 + 4 = 10 ms
Fig. 3. This ﬁgure illustrates how layer-wise look-up tables are used for fast resource
consumption estimation.
of ﬁlters that can satisfy the current resource constraint will be chosen. Note
that when some ﬁlters are removed from a layer, the associated channels in the
following layers should also be removed. Therefore, the change in the resource
consumption of other layers needs to be factored in.
Choose Which Filters This step chooses which ﬁlters to preserve based on
the architecture from the previous step. There are many methods proposed in
the literature, and we choose the magnitude-based method to keep the algorithm
simple. In this work, the N ﬁlters that have the largest ℓ2-norm magnitude will
be kept, where N is the number of ﬁlters determined by the previous step. More
complex methods can be adopted to increase the accuracy, such as removing the
ﬁlters based on their joint inﬂuence on the feature maps .
Short-/Long-Term Fine-Tune Both the short-term ﬁne-tune and longterm ﬁne-tune steps in NetAdapt involve network-wise end-to-end ﬁne-tuning.
Short-term ﬁne-tune has fewer iterations than long-term ﬁne-tune.
At each iteration of the algorithm, we ﬁne-tune the simpliﬁed networks with
a relatively smaller number of iterations (i.e., short-term) to regain accuracy, in
parallel or in sequence. This step is especially important while adapting small
networks with a large resource reduction because otherwise the accuracy will
drop to zero, which can cause the algorithm to choose the wrong network proposal.
As the algorithm proceeds, the network is continuously trained but does not
converge. Once the ﬁnal adapted network is obtained, we ﬁne-tune the network
with more iterations until convergence (i.e., long-term) as the ﬁnal step.
Fast Resource Consumption Estimation
As mentioned in Sec. 3.3, NetAdapt uses empirical measurements to determine
the number of ﬁlters to keep in a layer given the resource constraint. In theory,
we can measure the resource consumption of each of the simpliﬁed networks
on the ﬂy during adaptation. However, taking measurements can be slow and
diﬃcult to parallelize due to the limited number of available devices. Therefore,
it may be prohibitively expensive and become the computation bottleneck.
T.-J. Yang et al.
Real Latency (ms)
Estimated Latency (ms)
Fig. 4. The comparison between the estimated latency (using layer-wise look-up tables)
and the real latency on a single large core of Google Pixel 1 CPU while adapting the
100% MobileNetV1 with the input resolution of 224 .
We solve this problem by building layer-wise look-up tables with pre-measured
resource consumption of each layer. When executing the algorithm, we look up
the table of each layer, and sum up the layer-wise measurements to estimate
the network-wise resource consumption, which is illustrated in Fig. 3. The reason for not using a network-wise table is that the size of the table will grow
exponentially with the number of layers, which makes it intractable for deep
networks. Moreover, layers with the same shape and feature map size only need
to be measured once, which is common for modern deep networks.
Fig. 4 compares the estimated latency (the sum of layer-wise latency from the
layer-wise look-up tables) and the real latency on a single large core of Google
Pixel 1 CPU while adapting the 100% MobileNetV1 with the input resolution of
224 . The real and estimated latency numbers are highly correlated, and the
diﬀerence between them is suﬃciently small to be used by NetAdapt.
Experiment Results
In this section, we apply the proposed NetAdapt algorithm to MobileNets ,
which are designed for mobile applications, and experiment on the ImageNet
dataset . We did not apply NetAdapt on larger networks like ResNet and
VGG because networks become more diﬃcult to simplify as they become
smaller; these networks are also seldom deployed on mobile platforms. We benchmark NetAdapt against three state-of-the-art network simpliﬁcation methods:
– Multipliers are simple but eﬀective methods for simplifying networks.
Two commonly used multipliers are the width multiplier and the resolution multiplier; they can also be used together. Width multiplier scales the
number of ﬁlters by a percentage across all convolutional (CONV) and fullyconnected (FC) layers, and resolution multiplier scales the resolution of the
input image. We use the notation “50% MobileNetV1 (128)” to denote applying a width multiplier of 50% on MobileNetV1 with the input image
resolution of 128.
– MorphNet is an automatic network simpliﬁcation algorithm based on
sparsifying regularization.
– ADC is an automatic network simpliﬁcation algorithm based on reinforcement learning.
We will show the performance of NetAdapt on the small MobileNetV1 (50%
MobileNetV1 (128)) to demonstrate the eﬀectiveness of NetAdapt on real-timeclass networks, which are much more diﬃcult to simplify than larger networks.
To show the generality of NetAdapt, we will also measure its performance on
the large MobileNetV1 (100% MobileNetV1 (224)) across diﬀerent platforms.
Lastly, we adapt the large MobileNetV2 (100% MobileNetV2 (224)) to push the
frontier of eﬃcient networks.
Detailed Settings for MobileNetV1 Experiments
We perform most of the experiments and study on MobileNetV1 and detail the
settings in this section.
NetAdapt Conﬁguration MobileNetV1 is based on depthwise separable
convolutions, which factorize a m × m standard convolution layer into a m × m
depthwise layer and a 1×1 standard convolution layer called a pointwise layer. In
the experiments, we adapt each depthwise layer with the corresponding pointwise
layer and choose the ﬁlters to keep based on the pointwise layer. When adapting
the small MobileNetV1 (50% MobileNetV1 (128)), the latency reduction (∆Ri,j
in Eq. 2) starts at 0.5 and decays at the rate of 0.96 per iteration. When adapting
other networks, we use the same decay rate but scale the initial latency reduction
proportional to the latency of the initial pretrained network.
Network Training We preserve ten thousand images from the training
set, ten images per class, as the holdout set. The new training set without the
holdout images is used to perform short-term ﬁne-tuning, and the holdout set is
used to pick the highest accuracy network out of the simpliﬁed networks at each
iteration. The whole training set is used for the long-term ﬁne-tuning, which is
performed once in the last step of NetAdapt.
Because the training conﬁguration can have a large impact on the accuracy,
we apply the same training conﬁguration to all the networks unless otherwise
stated to have a fairer comparison. We adopt the same training conﬁguration as
MorphNet (except that the batch size is 128 instead of 96). The learning rate
for the long-term ﬁne-tuning is 0.045 and that for the short-term ﬁne-tuning is
0.0045. This conﬁguration improves ADC network’s top-1 accuracy by 0.3% and
almost all multiplier networks’ top-1 accuracy by up to 3.8%, except for one data
point, whose accuracy is reduced by 0.2%. We use these numbers in the following
analysis. Moreover, all accuracy numbers are reported on the validation set to
show the true performance.
Mobile Inference and Latency Measurement We use Google’s Tensor-
Flow Lite engine for inference on a mobile CPU and Qualcomm’s Snapdragon Neural Processing Engine (SNPE) for inference on a mobile GPU. For
experiments on mobile CPUs, the latency is measured on a single large core of
T.-J. Yang et al.
Top-1 Accuracy
Latency (ms)
Multipliers
1.6x Faster
0.3% Higher Accuracy
1.7x Faster
0.3% Higher Accuracy
Fig. 5. The ﬁgure compares NetAdapt (adapting the small MobileNetV1) with the
multipliers and MorphNet on a mobile CPU of Google Pixel 1.
Google Pixel 1 phone. For experiments on mobile GPUs, the latency is measured
on the mobile GPU of Samsung Galaxy S8 with SNPE’s benchmarking tool. For
each latency number, we report the median of 11 latency measurements.
Comparison with Benchmark Algorithms
Adapting Small MobileNetV1 on a Mobile CPU In this experiment, we
apply NetAdapt to adapt the small MobileNetV1 (50% MobileNetV1 (128)) to
a mobile CPU. It is one of the most compact networks and achieves real-time
performance. It is more challenging to simplify than other larger networks (include the large MobileNetV1). The results are summarized and compared with
the multipliers and MorphNet in Fig. 5. We observe that NetAdapt outperforms the multipliers by up to 1.7× faster with the same or higher accuracy.
For MorphNet, NetAdapt’s result is 1.6× faster with 0.3% higher accuracy.
Adapting Large MobileNetV1 on a Mobile CPU In this experiment, we
apply NetAdapt to adapt the large MobileNetV1 (100% MobileNetV1 (224))
on a mobile CPU. It is the largest MobileNetV1 and achieves the highest accuracy. Because its latency is approximately 8× higher than that of the small
MobileNetV1, we scale the initial latency reduction by 8×. The results are shown
and compared with the multipliers and ADC in Fig. 6. NetAdapt achieves
higher accuracy than the multipliers and ADC while increasing the speed by
1.4× and 1.2×, respectively.
While the training conﬁguration is kept the same when comparing to the
benchmark algorithms discussed above, we also show in Fig. 6 that the accuracy
of the networks adapted using NetAdapt can be further improved with a better
training conﬁguration. After simply adding dropout and label smoothing, the
accuracy can be increased by 1.3%. Further tuning the training conﬁguration
for each adapted network can give higher accuracy numbers, but it is not the
focus of this paper.
Top-1 Accuracy
Latency (ms)
Multipliers
NetAdapt (Better
Training Config.)
1.4x Faster
0.2% Higher Accuracy
1.2x Faster
0.4% Higher Accuracy
Fig. 6. The ﬁgure compares NetAdapt (adapting the large MobileNetV1) with the
multipliers and ADC on a mobile CPU of Google Pixel 1. Moreover, the accuracy
of the adapted networks can be further increased by up to 1.3% through using a better
training conﬁguration (simply adding dropout and label smoothing).
Top-1 Accuracy
Latency (ms)
Multipliers
NetAdapt (Better
Training Config.)
1.2x Faster
0.2% Higher Accuracy
1.1x Faster
0.1% Higher Accuracy
Fig. 7. This ﬁgure compares NetAdapt (adapting the large MobileNetV1) with the
multipliers and ADC on a mobile GPU of Samsung Galaxy S8. Moreover, the
accuracy of the adapted networks can be further increased by up to 1.3% through using
a better training conﬁguration (simply adding dropout and label smoothing).
Adapting Large MobileNetV1 on a Mobile GPU In this experiment, we
apply NetAdapt to adapt the large MobileNetV1 on a mobile GPU to show the
generality of NetAdapt. Fig. 7 shows that NetAdapt outperforms other benchmark algorithms by up to 1.2× speed-up with higher accuracy. Due to the limitation of the SNPE tool, the layerwise latency breakdown only considers the
computation time and does not include the latency of other operations, such as
feature map movement, which can be expensive . This aﬀects the precision
of the look-up tables used for this experiment. Moreover, we observe that there
is an approximate 6.2ms (38% of the latency of the network before applying
NetAdapt) non-reducible latency. These factors cause a smaller improvement on
the mobile GPU compared with the experiments on the mobile CPU. Moreover,
when the better training conﬁguration is applied as previously described, the
accuracy can be further increased by 1.3%.
T.-J. Yang et al.
Top-1 Accuracy (%) # of MACs (×106)
Latency (ms)
25% MobileNetV1 (128) 
MorphNet 
75% MobileNetV1 (224) 
Table 1. The comparison between NetAdapt (adapting the small or large MobileNetV1) and the three benchmark algorithms on image classiﬁcation when targeting
the number of MACs. The latency numbers are measured on a mobile CPU of Google
Pixel 1. We roughly match their accuracy and compare their latency.
Top-1 Accuracy
Latency (ms)
0 Iterations
10k Iterations
40k Iterations
200k Iterations
Fig. 8. The accuracy of diﬀerent shortterm ﬁne-tuning iterations when adapting the small MobileNetV1 (without longterm ﬁne-tuning) on a mobile CPU of
Google Pixel 1. Zero iterations means no
short-term ﬁne-tuning.
Top-1 Accuracy
Latency (ms)
Before LFT
Fig. 9. The comparison between before
ﬁne-tuning
adapting the small MobileNetV1 on a mobile CPU of Google Pixel 1. Although the
short-term ﬁne-tuning preserves the accuracy well, the long-term ﬁne-tuning gives
the extra 3.4% on average (from 1.8% to
Ablation Studies
Impact of Direct Metrics In this experiment, we use the indirect metric (i.e.,
the number of MACs) instead of the direct metric (i.e., the latency) to guide
NetAdapt to investigate the importance of using direct metrics. When computing
the number of MACs, we only consider the CONV and FC layers because batch
normalization layers can be folded into the corresponding CONV layers, and the
other layers are negligibly small. Table 1 shows that NetAdapt outperforms the
benchmark algorithms with lower numbers of MACs and higher accuracy. This
demonstrates the eﬀectiveness of NetAdapt. However, we also observe that the
network with lower numbers of MACs may not necessarily be faster. This shows
the necessity of incorporating direct measurements into the optimization ﬂow.
Impact of Short-Term Fine-Tuning Fig. 8 shows the accuracy of adapting
the small MobileNetV1 with diﬀerent short-term ﬁne-tuning iterations (without
long-term ﬁne-tuning). The accuracy rapidly drops to nearly zero if no shortterm ﬁne-tuning is performed (i.e., zero iterations). In this low accuracy region,
the algorithm picks the best network proposal solely based on noise and hence
Initialization (ms) Decay Rate # of Total Iterations Top-1 Accuracy (%) Latency (ms)
Table 2. The inﬂuence of resource reduction schedules.
10 11 12 13
Number of Filters
Conv2d Layer Index
Multipliers
Fig. 10. NetAdapt and the multipliers generate diﬀerent simpliﬁed networks when
adapting the small MobileNetV1 to match the latency of 25% MobileNetV1 (128).
gives poor performance. After ﬁne-tuning a network for a short amount of time
(ten thousand iterations), the accuracy is always kept above 20%, which allows
the algorithm to make a better decision. Although further increasing the number
of iterations improves the accuracy, we ﬁnd that using forty thousand iterations
leads to a good accuracy versus speed trade-oﬀfor the small MobileNetV1.
Impact of Long-Term Fine-Tuning Fig. 9 illustrates the importance of performing the long-term ﬁne-tuning. Although the short-term ﬁne-tuning preserves
the accuracy well, the long-term ﬁne-tuning can still increase the accuracy by
up to another 4.5% or 3.4% on average. Since the short-term ﬁne-tuning has a
short training time, the training is terminated far before convergence. Therefore,
it is not surprising that the ﬁnal long-term ﬁne-tuning can further increase the
Impact of Resource Reduction Schedules Table 2 shows the impact of
using three diﬀerent resource reduction schedules, which are deﬁned in Sec. 3.1.
Empirically, using a larger resource reduction at each iteration increases the
adaptation speed (i.e., reducing the total number of adaptation iterations) at the
cost of accuracy. With the same number of total iterations, the result suggests
that a smaller initial resource reduction with a slower decay is preferable.
Analysis of Adapted Network Architecture
The network architectures of the adapted small MobileNetV1 by using NetAdapt
and the multipliers are shown and compared in Fig. 10. Both of them have similar
latency as 25% MobileNetV1 (128). There are two interesting observations.
T.-J. Yang et al.
Top-1 Accuracy (%)
Latency (ms)
75% MobileNetV2 (224) 
NetAdapt (Similar Latency)
NetAdapt (Similar Accuracy)
Table 3. The comparison between NetAdapt (adapting the large MobileNetV2 (100%
MobileNetV2 (224))) and the multipliers on a mobile CPU of Google Pixel 1. We
compare the latency at similar accuracy and the accuracy at similar latency.
First, NetAdapt removes more ﬁlters in layers 7 to 10, but fewer in layer 6.
Since the feature map resolution is reduced in layer 6 but not in layers 7 to 10,
we hypothesize that when the feature map resolution is reduced, more ﬁlters are
needed to avoid creating an information bottleneck.
The second observation is that NetAdapt keeps more ﬁlters in layer 13 (i.e.
the last CONV layer). One possible explanation is that the ImageNet dataset
contains one thousand classes, so more feature maps are needed by the last FC
layer to do the correct classiﬁcation.
Adapting Large MobileNetV2 on a Mobile CPU
In this section, we show encouraging early results of applying NetAdapt to MobileNetV2 . MobileNetV2 introduces the inverted residual with linear bottleneck into MobileNetV1 and becomes more eﬃcient. Because MobileNetV2
utilizes residual connections, we only adapt individual inner (expansion) layers
or reduce all bottleneck layers of the same resolution in lockstep. The main diﬀerences between the MobileNetV1 and MobileNetV2 experiment settings are that
each network proposal is short-term ﬁne-tuned with ten thousand iterations, the
initial latency reduction is 1ms, the latency reduction decay is 0.995, the batch
size is 96, and dropout and label smoothing are used. NetAdapt achieves 1.1%
higher accuracy or 1.2× faster speed than the multipliers as shown in Table 3.
Conclusion
In summary, we proposed an automated algorithm, called NetAdapt, to adapt a
pretrained network to a mobile platform given a real resource budget. NetAdapt
can incorporate direct metrics, such as latency and energy, into the optimization
to maximize the adaptation performance based on the characteristics of the
platform. By using empirical measurements, NetAdapt can be applied to any
platform as long as we can measure the desired metrics, without any knowledge
of the underlying implementation of the platform. We demonstrated empirically
that the proposed algorithm can achieve better accuracy versus latency trade-oﬀ
(by up to 1.7× faster with equal or higher accuracy) compared with other stateof-the-art network simpliﬁcation algorithms. In this work, we aimed to highlight
the importance of using direct metrics in the optimization of eﬃcient networks;
we hope that future research eﬀorts will take direct metrics into account in order
to further improve the performance of eﬃcient networks.