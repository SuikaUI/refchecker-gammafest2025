Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2495–2509,
Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics
Addressing Semantic Drift in Question Generation
for Semi-Supervised Question Answering
Shiyue Zhang
Mohit Bansal
UNC Chapel Hill
{shiyue, mbansal}@cs.unc.edu
Text-based Question Generation (QG) aims at
generating natural and relevant questions that
can be answered by a given answer in some
Existing QG models suffer from a
“semantic drift” problem, i.e., the semantics
of the model-generated question drifts away
from the given context and answer. In this paper, we ﬁrst propose two semantics-enhanced
rewards obtained from downstream question
paraphrasing and question answering tasks to
regularize the QG model to generate semantically valid questions. Second, since the traditional evaluation metrics (e.g., BLEU) often
fall short in evaluating the quality of generated questions, we propose a QA-based evaluation method which measures the QG model’s
ability to mimic human annotators in generating QA training data.
Experiments show
that our method achieves the new state-of-theart performance w.r.t. traditional metrics, and
also performs best on our QA-based evaluation metrics. Further, we investigate how to
use our QG model to augment QA datasets
and enable semi-supervised QA. We propose
two ways to generate synthetic QA pairs: generate new questions from existing articles or
collect QA pairs from new articles. We also
propose two empirically effective strategies, a
data ﬁlter and mixing mini-batch training, to
properly use the QG-generated data for QA.
Experiments show that our method improves
over both BiDAF and BERT QA baselines,
even without introducing new articles.1
Introduction
In contrast to the rapid progress shown in Question Answering (QA) tasks , the task of
Question Generation (QG) remains understudied
and challenging. However, as an important dual
1Code and models publicly available at:
github.com/ZhangShiyue/QGforQA
Context: ...during the age of enlightenment, philosophers such as john locke advocated the principle in
their writings, whereas others, such as thomas hobbes,
strongly opposed it. montesquieu was one of the foremost supporters of separating the legislature, the executive, and the judiciary...
Gt: who was an advocate of separation of powers?
Base: who opposed the principle of enlightenment?
Ours: who advocated the principle in the age of enlightenment?
Table 1: An examples of the “semantic drift” issue in
Question Generation (“Gt” is short for “ground truth”).
task to QA, QG can not only be used to augment QA datasets , but can also
be applied in conversation and education systems
 .
Furthermore, given that existing QA models often fall short by doing simple word/phrase matching rather than true comprehension , the task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true
machine comprehension .
Recently, we have seen an increasing interest in
the QG area, with mainly three categories: Textbased QG ,
Knowledge-Base-based QG , and Image-based QG . Our work focuses
on the Text-based QG branch.
Current QG systems follow an attentionbased sequence-to-sequence structure, taking the
paragraph-level context and answer as inputs and
outputting the question. However, we observed
that these QG models often generate questions
that semantically drift away from the given context and answer; we call this the “semantic drift”
problem. As shown in Table 1, the baseline QG
model generates a question that has almost contrary semantics with the ground-truth question,
and the generated phrase “the principle of en-
lightenment” does not make sense given the context. We conjecture that the reason for this “semantic drift” problem is because the QG model
is trained via teacher forcing only, without any
high-level semantic regularization.
Hence, the
learned model behaves more like a question language model with some loose context constraint,
while it is unaware of the strong requirements
that it should be closely grounded by the context and should be answered by the given answer.
Therefore, we propose two semantics-enhanced
rewards to address this drift: QPP and QAP. Here,
QPP refers to Question Paraphrasing Probability,
which is the probability of the generated question
and the ground-truth question being paraphrases;
QAP refers to Question Answering Probability,
which is the probability that the generated question can be correctly answered by the given answer. We regularize the generation with these two
rewards via reinforcement learning. Experiments
show that these two rewards can signiﬁcantly improve the question generation quality separately or
jointly, and achieve the new state-of-the-art performance on the SQuAD QG task.
Next, in terms of QG evaluation, previous
works have mostly adopted popular automatic
evaluation metrics, like BLEU, METEOR, etc.
However, we observe that these metrics often fall
short in properly evaluating the quality of generated questions.
First, they are not always correlated to human judgment about answerability
 . Second, since multiple
questions are valid but only one reference exists in
the dataset, these traditional metrics fail to appropriately score question paraphrases and novel generation (shown in Table 2). Therefore, we introduce a QA-based evaluation method that directly
measures the QG model’s ability to mimic human
annotators in generating QA training data, because
ideally, we hope that the QG model can act like
a human to ask questions. We compare different
QG systems using this evaluation method, which
shows that our semantics-reinforced QG model
performs best. However, this improvement is relatively minor compared to our improvement on
other QG metrics, which indicates improvement
on typical QG metrics does not always lead to better question annotation by QG models for generating QA training set.
Further, we investigate how to use our best
QG system to enrich QA datasets and perform
semi-supervised QA on SQuADv1.1 . Following the back-translation strategy that has been shown to be effective in Machine Translation and Natural Language Navigation , we propose two methods to collect synthetic data.
First, since multiple questions can be asked for one answer while there is
only one human-labeled ground-truth, we make
our QG model generate new questions for existing context-answer pairs in SQuAD training set,
so as to enrich it with paraphrased and other novel
but valid questions.
Second, we use our QG
model to label new context-answer pairs from new
Wikipedia articles. However, directly mixing synthetic QA pairs with ground-truth data will not
lead to improvement. Hence, we introduce two
empirically effective strategies: one is a data ﬁlter
based on QAP (same as the QAP reward) to ﬁlter
out examples that have low probabilities to be correctly answered; the other is a “mixing mini-batch
training” strategy that always regularizes the training signal with the ground-truth data. Experiments
show that our method improves both BiDAF and BERT
 QA baselines by 1.69/1.27
and 1.19/0.56 absolute points on EM/F1, respectively; even without introducing new articles, it
can bring 1.51/1.13 and 0.95/0.13 absolute improvement, respectively.
Related Works
Question Generation
Early QG studies focused
on using rule-based methods to transform statements to questions . Recent works adopted the attention-based sequenceto-sequence neural model 
for QG tasks, taking answer sentence as input and
outputting the question , which proved to be better than rulebased methods.
Since human-labeled questions
are often relevant to a longer context, later works
leveraged information from the whole paragraph
for QG, either by extracting additional information
from the paragraph or by directly taking
the whole paragraph as input . A very recent concurrent work applied the large-scale language model pre-training strategy for QG and
also achieved a new state-of-the-art performance
 . However, the above models
were trained with teacher forcing only. To address
the exposure bias problem, some works applied
reinforcement learning taking evaluation metrics
(e.g., BLEU) as rewards .
Yuan et al. proposed
to use a language model’s perplexity (RPPL) and
a QA model’s accuracy (RQA) as two rewards
but failed to get signiﬁcant improvement. Their
second reward is similar to our QAP reward except that we use QA probability rather than accuracy as the probability distribution is more smooth.
Hosking and Riedel compared a set of different rewards, including RPPL and RQA, and
claimed none of them improved the quality of generated questions. For QG evaluation, even though
some previous works conducted human evaluations, most of them still relied on traditional metrics (e.g., BLEU). However, Nema and Khapra
 pointed out the existing metrics do not correlate with human judgment about answerability,
so they proposed “Q-metrics” that mixed traditional metrics with an “answerability” score. In
our work, we will show QG results on traditional
metrics, Q-metrics, as well as human evaluation,
and also propose a QA-based QG evaluation.
Question Generation for QA
As the dual task
of QA, QG has been often proposed for improving
QA. Some works have directly used QG in QA
models’ pipeline . Some other works
enabled semi-supervised QA with the help of QG.
Tang et al. applied the “dual learning” algorithm to learn QA and QG
jointly with unlabeled texts. Yang et al. 
and Tang et al. followed the GAN paradigm, taking QG as a generator and QA as a discriminator, to utilize unlabeled data. Sachan and Xing proposed a
self-training cycle between QA and QG. However,
these works either reduced the ground-truth data
size or simpliﬁed the span-prediction QA task to
answer sentence selection. Dhingra et al. 
collected 3.2M cloze-style QA pairs to pre-train
a QA model, then ﬁne-tune with the full groundtruth data which improved a BiDAF-QA baseline.
In our paper, we follow the back-translation strategy to generate new QA
pairs by our best QG model to augment SQuAD
training set.
Further, we introduce a data ﬁlter
to remove poorly generated examples and a mixing mini-batch training strategy to more effectively use the synthetic data. Similar methods have
also been applied in some very recent concurrent
works on
SQuADv2.0. The main difference is that we also
propose to generate new questions from existing
articles without introducing new articles.
Question Generation
Base Model
We ﬁrst introduce our base model which mainly
adopts the model architecture from the previous
state-of-the-art .
The differences are that we introduce two linguistic features
(POS & NER), apply deep contextualized word
vectors, and tie the output projection matrix with
the word embedding matrix. Experiments showed
that with these additions, our base model results
surpass the results reported in Zhao et al. 
with signiﬁcant margins. Our base model architecture is shown in the upper box in Figure 1
and described as follow. If we have a paragraph
i=1 and an answer a which is a sub-span
of p, the target of the QG task is to generate a question q = {yj}N
j=1 that can be answered by a based
on the information in p.
The model ﬁrst concatenates four
word representations: word vector, answer tag
embedding, Part-of-Speech (POS) tag embedding,
and Name Entity (NER) tag embedding, i.e., ei =
[wi, ai, pi, ni]. For word vectors, we use the deep
contextualized word vectors from ELMo or BERT . The
answer tag follows the BIO2 tagging scheme.
The output of the embedding layer is
then encoded by a two-layer bi-directional LSTM-
RNNs, resulting in a list of hidden representations
H. At any time step i, the representation hi is the
concatenation of −→
−→h i = −−−−→
LSTM([ei; −→h i−1])
←−h i = ←−−−−
LSTM([ei; ←−h i+1])
2“B”, for “Begin”, tags the start token of the answer span;
“I”, for “Inside”, tags other tokens in the answer span; “O”,
for “Other”, tags other tokens in the paragraph.
Self-attention
A gated self-attention mechanism is applied to H to aggregate the long-term dependency within the paragraph. αi is an attention vector between hi and
each element in H; ui is the self-attention context
vector for hi; hi is then updated to fi using ui; a
soft gate gi decides how much the update is applied. ˆH = [ˆhi]M
i=1 is the output of this layer.
ui = Hαi, αi = softmax(HT W uhi)
fi = tanh(W f[hi; ui])
gi = sigmoid(W g[hi; ui])
ˆhi = gi ∗fi + (1 −gi) ∗hi
The decoder is another two-layer unidirectional LSTM-RNN. An attention mechanism
dynamically aggregates ˆH at each decoding step
to a context vector cj which is then used to update
the decoder state sj.
cj = ˆHαj, αj = softmax( ˆHT W asj)
˜sj = tanh(W c[cj; sj])
sj+1 = LSTM([yj; ˜sj])
The probability of the target word yj is computed
by a maxout neural network.
˜oj = tanh(W o[cj; sj])
oj = [max{˜oj,2k−1, ˜oj,2k}]k
p(yj|y<j) = softmax(W eoj)
In practice, we keep the weight matrix W e the
same as the word embedding matrix and ﬁx it
during training. Furthermore, we apply the same
“maxout pointer” proposed by Zhao et al. 
to enable the model to copy words from input.
Semantics-Reinforced Model
To address the “semantic drift” problem shown in
Table 1, we propose two semantics-enhanced rewards to regularize the generation to focus on generating semantically valid questions.
QPP Reward
To deal with the “exposure bias”
problem, many previous works directly used the
ﬁnal evaluation metrics (e.g., BLEU) as rewards
to train the generation models . However, these metrics sometimes fail to evaluate equally to question paraphrases and thus provide inaccurate rewards. Hence, we propose to use a pre-trained
question paraphrasing classiﬁcation (QPC) model
Environment
(QPP & QAP)
Figure 1: The architecture of our semantics-reinforced
to provide paraphrasing probability as a reward.
Since paraphrasing is more about semantic similarity than superﬁcial word/phrase matching, it
treats question paraphrases more fairly (Example
1 in Table 2).
Therefore, we ﬁrst train a QPC
model with Quora Question Pairs dataset. Next,
we take it as an environment, and the QG model
will interact with it during training to get the probability of the generated question and the groundtruth question being paraphrases as the reward.
QAP Reward
Two observations motivate us to
introduce QAP reward. First, in a paragraph, usually, there are several facts relating to the answer and can be used to ask questions. Neither
the teacher forcing or the QPP reward can favor
this kind of novel generation (Example 2 in Table 2). Second, we ﬁnd semantically-drifted questions are usually unanswerable by the given answer. Therefore, inspired by the dual learning algorithm , we propose to take the
probability that a pre-trained QA model can correctly answer the generated question as a reward,
i.e., p(a∗|qs; p), where a∗is the ground-truth answer and qs is a sampled question. Using this reward, the model can not only gets positive rewards
for novel generation but also be regularized by
the answerability requirement. Note that, this reward is supposed to be carefully used because the
QG model can cheat by greedily copying words
in/near the answer to the generated question. In
this case, even though high QAPs are achieved,
the model loses the question generation ability.
Policy Gradient
To apply these two rewards, we
use the REINFORCE algorithm 
Example 1: Fail to score equally to paraphrases
Context: ...the university ﬁrst offered graduate degrees , in the form of a master
of arts ( ma ) , in the the 1854 – 1855 academic year ...
Gt: in what year was a master of arts course ﬁrst offered ?
Gen1: in what year did the university ﬁrst offer a master of arts ?
Gen2: when did the university begin offering a master of arts ?
Example 2: Fail to score appropriately to novel generation
Context: ...in 1987 , when some students believed that the observer began to
show a conservative bias , a liberal newspaper , common sense was published...
Gt: in what year did the student paper common sense begin publication ?
Gen1: in what year did common sense begin publication ?
Gen2: when did the observer begin to show a conservative bias ?
Table 2: Two examples of where QPP and QAP improve in question quality evaluation.
to learn a generation policy pθ deﬁned by the QG
model parameters θ. We minimize the loss function LRL = −Eqs∼pθ[r(qs)], where qs is a sampled question from the model’s output distribution.
Due to the non-differentiable sampling procedure,
the gradient is approximated using a single sample
with some variance reduction baseline b:
▽θLRL = −(r(qs) −b) ▽θ logpθ(qs)
We follow the effective SCST strategy to take the reward of greedy search
result qg as the baseline, i.e. b = r(qg). However,
only using this objective to train QG will result
in poor readability, so we follow the mixed loss
setting : Lmixed = γLRL +
(1 −γ)LML.
In practice, we ﬁnd the mixing
ratio γ for QAP reward should be lower, i.e., it
needs more regularization from teacher forcing,
so that it can avoid the undesirable cheating issue
mentioned above. Furthermore, we also apply the
multi-reward optimization strategy to train the model with two mixed
losses alternately with an alternate rate n : m, i.e.,
train with Lqpp
mixed for n mini-batches, then train
mixed for m mini-batches, repeat until convergence. n and m are two hyper-parameters.
mixed = γqppLqpp
RL + (1 −γqpp)LML
mixed = γqapLqap
RL + (1 −γqap)LML
Experiments show that these two rewards can signiﬁcantly improve the QG performance separately
or jointly, and we achieve new state-of-the-art QG
performances, see details in Section 6.
QA-Based QG Evaluation
Inspired by the idea that “a perfect QG model can
replace humans to ask questions”, we introduce
a QA-based evaluation method that measures the
quality of a QG model by its ability to mimic human annotators in labeling training data for QA
The evaluation procedure is described
as follows.
First, we sample some unlabeled
Wikipedia paragraphs with pre-extracted answer
spans from HarvestingQA dataset . Second, we make a QG model act as an
“annotator” to annotate a question for each answer
span. Third, we train a QA model using this synthetic QA dataset. Lastly, we use the QA model’s
performance on the original SQuAD development
set as the evaluation for this QG model. The higher
this QA performance is, the better the QG model
mimics a human’s question-asking ability. We believe that this method provides a new angle to evaluate QG model’s quality and also a more reliable
way to choose QG models to conduct data augmentation and semi-supervised QA.
Semi-Supervised Question Answering
Since one of the major goals of developing QG
systems is to generate new QA pairs and augment
QA datasets, we investigate how to use our QG
system to act as a question annotator, collect new
QA pairs, and conduct semi-supervised QA. Figure 2 illustrates the overall procedure of our semisupervised QA approach.
Synthetic Data Generation
To generate synthetic QA pairs, we follow the
effective “back translation” approach proposed
in Neural Machine Translation (NMT) . In NMT, the back translation method
ﬁrst obtains synthetic source sentences by running
a pre-trained target-to-source translation model on
a monolingual dataset of the target language; then,
it combines the synthetic and ground-truth translation pairs to train the desired source-to-target
translation model. Similarly, in the QA scenario,
Model-generated questions
Human-labeled questions
Question answering probability
New or existing paragraphs
Existing paragraphs
when did the observer begin to
show a conservative bias?
.. in 1987, when some students
believed that the observer began to
show a conservative bias, a liberal
newspaper, common sense was
was published …
.. in 1987, when some students
show a conservative bias, a liberal
newspaper, common sense was
was published …
believed that the observer began to
in what year did the student paper
common sense begin publication?
Figure 2: Semi-supervised QA: First, a trained QG model generates questions from new or existing paragraphs
building up a synthetic QA dataset; Second, a data ﬁlter ﬁlters out low-QAP synthetic examples and augment the
rest to human-labeled QA pairs; Lastly, the QA model is trained with the enlarged QA dataset.
the paragraph/answer can be viewed as the “target sentence”, while the question can be taken as
the “source sentence”.
One tricky difference is
that even if the paragraphs can be easily obtained
from Wikipedia, there are no answer span labels.
Therefore, we use two sources to generate questions from, as discussed below.
Generate from Existing Articles
 , each context-answer pair
only has one ground-truth question.
usually, multiple questions can be asked. The diversity lies in question paraphrasing and different facts in the context that can be used to ask
the question. Therefore, without introducing new
Wikipedia articles, we make our QG model generate diverse questions for the existing contextanswer pairs in SQuAD training set by keeping the
all beam search outputs for each example.
Generate from New Articles
To use unlabeled
Wikipedia articles for data augmentation, an automatic answer extractor is indispensable. Some
previous works have proposed methods to detect
key phrases from a paragraph and automatically
extract potential answer spans .
Instead of building up our answer extractor, we
directly take advantage of the released HarvestingQA dataset.
It contains 1.2M synthetic QA
pairs, in which both the answer extractor and
the QG model were proposed by Du and Cardie
 . We use their paragraphs with answer span
labels but generate questions with our QG models,
and only use their questions for comparison.
Synthetic Data Usage
In practice, we ﬁnd that directly mixing the synthetic data with the ground-truth data does not improve QA performance. We conjecture the reason
is that some poor-quality synthetic examples mislead the learning process of the QA model. Therefore, we propose two empirical strategies to better
utilize synthetic data.
QAP Data Filter
In “self-training” literature,
similar issues have been discussed that using
model-labeled examples to train the model will
amplify the model’s error. Later works proposed
co-training or tri-training that uses two or three
models as judges and only keeps examples that all
models agree on . Sachan and Xing also designed question selection oracles based on curriculum learning strategy in their QA-QG self-training
circle. In this paper, we simply design a data ﬁlter based on our QAP measure (same deﬁnition
as the QAP reward) to ﬁlter poor-quality examples. We think if one question-answer pair has a
low QAP, i.e., the probability of the answer given
the question is low, it is likely to be a mismatched
Hence, we ﬁlter synthetic examples with
QAP < ϵ, where ϵ is a hyper-parameter that we
will tune for different synthetic datasets.
Mixing Mini-Batch Training
When conducting semi-supervised learning, we do not want gradients from ground-truth data are overwhelmed by
synthetic data. Previous works proposed to ﬁrst pre-train
the model with synthetic data and then ﬁne-tune
it with ground-truth data. However, we ﬁnd when
the synthetic data size is small (e.g., similar size
as the ground-truth data), catastrophic forgetting
3They actually used the reversed dev-test setup as opposed to the original setup used in Du et al. and Du
and Cardie ).
Thus, we also conducted the reversed dev-test setup and
our QPP&QAP model yields BLEU4/METEOR/ROUGE-
L=20.76/24.20/48.91.
Du and Cardie 
Zhao et al. 3
Our baseline (w. ELMo)
Table 3: The performance of different QG models.
will happen during ﬁne-tuning, leading to similar
results as using ground-truth data only. Thus, we
propose a “mixing mini-batch” training strategy,
where for each mini-batch we combine half minibatch ground-truth data with half mini-batch synthetic data, which keeps the data mixing ratio to
1:1 regardless of what the true data size ratio is. In
this way, we can have the training process generalizable to different amounts of synthetic data and
keep the gradients to be regularized by groundtruth data.
Experiment Setup
For QG, we use the most commonly
used SQuAD QG dataset ﬁrst used by . For QA-based QG evaluation, we obtain
unlabeled paragraph and answer labels from HarvestingQA , and have different QG systems to label questions. For semisupervised QA, we use SQuADv1.1 as our base QA task, and split the original development set in half as our development
and test set respectively. Plus, we make our QG
model generate new questions from both SQuAD
and HarvestingQA. We will sample 10% – 100%
examples from HarvestingQA which are denoted
by H1-10 in our experiments.
Evaluation Metrics
For QG, we ﬁrst adopt 3
traditional metrics (BLEU4/METEOR/ROUGE-
L). Second, we apply the new Q-BLEU1 metric
proposed by .
Moreover, we conduct a pairwise human evaluation
between our baseline and QPP&QAP model on
MTurk. We gave the annotators a paragraph with
an answer bold in context and two questions generated by two models (randomly shufﬂed).
asked them to decide which one is better or nondistinguishable. For both QA-based QG evaluation and semi-supervised QA, we follow the standard evaluation method for SQuAD to use Exact
Our baseline
Table 4: Pairwise human evaluation between our baseline and QPP&QAP multi-reward model.
Du and Cardie
Our baseline
53.20/65.47
55.06/67.83
55.89/68.26
53.40/66.28
56.23/69.23
56.69/69.19
53.12/65.57
57.14/69.39
57.05/70.17
71.16/80.75
71.94/81.26
72.20/81.44
72.02/81.00
72.03/81.38
72.22/81.81
71.48/81.02
72.61/81.46
72.69/82.22
Table 5: The QA-based evaluation results for different
QG systems. The two numbers of each item in this
table are the EM/F1 scores. All results are the performance on our QA test set. “S” is short for “SQuAD”.
Match (EM) and F1.
More details about datasets, evaluation metrics,
human evaluation setup, and model implement details are provided in the Appendix.
Question Generation
First, as shown in Table 3, our baseline QG model obtains a non-trivial improvement
over previous best QG system 
which proves the effectiveness of our newly introduced setups: introduce POS/NER features, use
deep contexturalized word vectors (from ELMo
or BERT), and tie output projection matrix with
non-trainable word embedding matrix.
we apply three evaluation metrics as rewards to
deal with the exposure bias issue and improve
performance.
All the metrics are signiﬁcantly4
(p < 0.001) improved except QPP, which supports
that high traditional evaluation metrics do not always correlate to high semantic similarity.
4The signiﬁcance tests in this paper are conducted following the bootstrap test setup .
Data Size5
Table 6: The effect of QAP-based synthetic data ﬁlter.
We ﬁlter out the synthetic data with QAP < ϵ. All
results are the performance on our QA development set.
Semantics-Reinforced Models
As shown in Table 3, when using QAP and QQP separately,
all metrics are signiﬁcantly (p
0.001) improved over our baseline and all metrics except ROUGE-L are signiﬁcantly (p < 0.05) improved over the models using traditional metrics
as rewards.
After applying multi-reward optimization, our model performs consistently best
on BLEU4/METEOR/ROUGE-L and Q-BLEU1.
Notably, using one of these two rewards will also
improve the other one at the same time, but using both of them achieves a good balance between these two rewards without exploiting either
of them and results in the consistently best performance on other metrics, which is a new state-ofthe-art result. Human Evaluation Results: Table 4 shows the MTurk anonymous human evaluation study, where we do a pairwise comparison between our baseline and QPP&QAP model.
We collected 300 responses in total, 160 of which
voted the QPP&QAP model’s generation is better,
131 of which favors the baseline model, and 9 of
which selected non-distinguishable.
QA-Based Evaluation
As shown in Table 5, we
compare three QG systems using QA-based evaluation on three different amounts of synthetic data
and their corresponding semi-supervised QA setups (without ﬁlter). It can be observed that both
our baseline and our best QG model can significantly improve the synthetic data’s QA performance, which means they can act as better “annotators” than the QG model proposed by Du and
Cardie . However, our best QG model only
has a minor improvement over our baseline model,
which means signiﬁcant improvement over QG
metrics does not guarantee signiﬁcant better question annotation ability.
+ DivBeam10
+ Beam10 + H8
Table 7: The results of our semi-supervised QA method
using a BiDAF-QA model.
New Data Size
Dhingra et al. base
The comparison with the previous semisupervised QA method. All results are the performance
on the full development set of SQuAD, i.e., our QA test
+ development set.
Semi-Supervised Question Answering
Effect of the data ﬁlter
As shown in Table 6,
when using synthetic data only, adding the data
ﬁlter can signiﬁcantly improve QA performance.
In terms of semi-supervised QA, the improvement
is relatively smaller, due to the regularization from
ground-truth data, but still consistent and stable.
Semi-Supervised QA results
Table 7 demonstrates the semi-supervised QA results. Without
introducing new articles, we keep beam search
outputs as additional questions. It can be seen that
using beam search with beam size 10 (+Beam10)
improves the BiDAF-QA baseline by 1.51/1.13
absolute points on the testing set. With introducing new articles, the best performance (+H8) improves the BiDAF-QA baseline by 1.69/1.27 absolute points on the testing set. We also combine
the two best settings (Beam10+H8), but it does not
perform better than using them separately.
We conduct two ablation studies on the development set. First, we compare beam search with
5“Data Size” counts the total number of examples in training set (after ﬁlter). In Table 8, “New Data Size” only counts
# examples generated from articles outside SQuAD.
QA-Eval (H1)
Du and Cardie 
55.11/66.40
Our baseline (w. BERT)
58.63/69.97
60.49/71.81
60.12/71.14
+ QPP & QAP
59.11/70.87
Table 9: The performance of our stronger BERT-QG models.
+ Beam10 + H10
The results of our semi-supervised QA
method using a stronger BERT-QA model.
different beam sizes and diverse beam search , but all of them perform similarly.
Second, increasing the size of synthetic data from
H1 to H10, the performance saturates around H2-
H4. We also observed that when using a big synthetic data, e.g., H10, the model converges even
before all examples were used for training. Based
on these results, we conjecture that there is an upper bound of the effect of synthetic data which
might be limited by the QG quality. To further
improve the performance, more diverse and tricky
questions need to be generated.
To show how
QG models help or limit the QA performance,
we include some synthetic QA examples in Appendix. Finally, we compare our semi-supervised
QA methods with Dhingra et al. . As shown
in Table 8, with no or less new data injection,
our methods achieve larger improvements over a
stronger baseline than their method.
QG and QA Results with BERT
The Bidirectional Encoder Representations from
Transformers (BERT) has recently improved a lot of NLP tasks by substantial
margins. To verify if our improvements still hold
on BERT-based baselines, we propose a BERT-
QG baseline and test our two semantics-enhanced
rewards; further, we conduct our semi-supervised
QA method on a BERT-QA baseline.
Without modifying our QG model’s
architecture, we simply replaced ELMo used
above with BERT. Table 9 shows that our BERT-
QG baseline improves previous ELMo-QG baseline by a large margin; meanwhile, our QPP/QAP
rewards signiﬁcantly improve the stronger QG
baseline and achieve the new state-of-the-art QG
performance w.r.t both traditional metrics and QAbased QG evaluation. One difference is that the
QAP-only model has the overall best performance
instead of the multi-reward model. Note that, we
also obtain the QPP and QAP rewards from BERTbased QPC and QA models, respectively.
Using our QAP-reinforced BERT-
QG model, we apply the same semi-supervised
QA method on a BERT-QA baseline. As shown
in Table 10, though with smaller margins, our
method improves the strong BERT-QA baseline
by 1.19/0.56 absolute points on test set; even without introducing new articles, it obtains 0.95/0.13
absolute gains.
Conclusion
We proposed two semantics-enhanced rewards to
regularize a QG model to generate semantically
valid questions, and introduced a QA-based evaluation method that directly evaluates a QG model’s
ability to mimic human annotators in generating
QA training data. Experiments showed that our
QG model achieves new state-of-the-art performances. Further, we investigated how to use our
QG system to augment QA datasets and conduct
semi-supervised QA via two synthetic data generation methods along with a data ﬁlter and mixing
mini-batch training. Experiments showed that our
approach improves both BiDAF and BERT QA
baselines even without introducing new articles.
Acknowledgments
We thank the reviewers for their helpful comments
and Hao Tan for useful discussions. This work was
supported by DARPA (YFA17-D17AP00022),
NSF-CAREER Award #1846185, ONR Grant
#N00014-18-1-2871, and faculty awards from
Google, Facebook, and Salesforce.
contained in this article are those of the authors
and not of the funding agency.