SCINet: Time Series Modeling and Forecasting with
Sample Convolution and Interaction
Minhao Liu‚àó, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, Qiang Xu‚àó
CUhk REliable Computing (CURE) Lab.
Dept. of Computer Science & Egnineering, The Chinese University of Hong Kong
‚àó{mhliu,qxu}@cse.cuhk.edu.hk
One unique property of time series is that the temporal relations are largely preserved after downsampling into two sub-sequences. By taking advantage of this
property, we propose a novel neural network architecture that conducts sample
convolution and interaction for temporal modeling and forecasting, named SCINet.
SpeciÔ¨Åcally, SCINet is a recursive downsample-convolve-interact architecture. In
each layer, we use multiple convolutional Ô¨Ålters to extract distinct yet valuable
temporal features from the downsampled sub-sequences or features. By combining these rich features aggregated from multiple resolutions, SCINet effectively
models time series with complex temporal dynamics. Experimental results show
that SCINet achieves signiÔ¨Åcant forecasting accuracy improvements over both
existing convolutional models and Transformer-based solutions across various
real-world time series forecasting datasets. Our codes and data are available at
 
Introduction
Time series forecasting (TSF) enables decision-making with the estimated future evolution of metrics or events, thereby playing a crucial role in various scientiÔ¨Åc and engineering Ô¨Åelds such as
healthcare , energy management , trafÔ¨Åc Ô¨Çow , and Ô¨Ånancial investment , to name a
There are mainly three kinds of deep neural networks used for sequence modeling, and they are all
applied for time series forecasting : (i). recurrent neural networks (RNNs) ; (ii). Transformerbased models ; and (iii). temporal convolutional networks (TCN) .
Despite the promising results of TSF methods based on these generic models, they do not consider the
specialty of time series data during modeling. For example, one unique property of time series is that
the temporal relations (e.g., the trend and the seasonal components of the data) are largely preserved
after downsampling into two sub-sequences. Consequently, by recursively downsampling the time
series into sub-sequences, we could obtain a rich set of convolutional Ô¨Ålters to extract dynamic
temporal features at multiple resolutions.
Motivated by the above, in this paper, we propose a novel neural network architecture for time series
modeling and forecasting, named sample convolution and interaction network (SCINet). The main
contributions of this paper are as follows:
‚Ä¢ We propose SCINet, a hierarchical downsample-convolve-interact TSF framework that
effectively models time series with complex temporal dynamics. By iteratively extracting
and exchanging information at multiple temporal resolutions, an effective representation with
enhanced predictability can be learned, as veriÔ¨Åed by its comparatively lower permutation
entropy (PE) .
36th Conference on Neural Information Processing Systems .
 
Conv. Layer
Conv. Layer
Projection
(a) RNN model
(b) Transformer model
(c) TCN model
Figure 1: Existing sequence modeling architectures for time series forecasting.
‚Ä¢ We design the basic building block, SCI-Block, for constructing SCINet, which downsamples
the input data/feature into two sub-sequences, and then extracts features of each subsequence using distinct convolutional Ô¨Ålters. To compensate for the information loss
during the downsampling procedure, we incorporate interactive learning between the two
convolutional features within each SCI-Block.
Extensive experiments on various real-world TSF datasets show that our model consistently outperforms existing TSF approaches by a considerable margin. Moreover, while SCINet does not explicitly
model spatial relations, it achieves competitive forecasting accuracy on spatial-temporal TSF tasks.
Related Work and Motivation
The time series forecasting problem is deÔ¨Åned as: Given a long time series X‚àóand a look-back window
of Ô¨Åxed length T, at timestamp t, time series forecasting is to predict ÀÜXt+1:t+œÑ = {xt+1, ..., xt+œÑ}
based on the past T steps Xt‚àíT +1:t = {xt‚àíT +1, ..., xt}. Here, œÑ is the length of the forecast horizon,
xt ‚ààRd is the value at time step t, and d is the number of variates. For simplicity, in the following
we will omit the subscripts, and use X and ÀÜX to represent the historical data and the forecasted data,
respectively.
Related Work
Traditional time series forecasting methods such as the autoregressive integrated moving average
(ARIMA) model and Holt-Winters seasonal method have theoretical guarantees. However,
they are mainly applicable for univariate forecasting problems, restricting their applications to
complex time series data. With the increasing data availability and computing power in recent years,
it is shown that deep learning-based TSF techniques have the potential to achieve better forecasting
accuracy than conventional approaches .
Earlier RNN-based TSF methods summarize the past information compactly in the internal
memory states that are recursively updated with new inputs at each time step, as shown in Fig. 1(a).
The gradient vanishing/exploding problems and the inefÔ¨Åcient training procedure greatly restrict the
application of RNN-based models.
In recent years, Transformer-based models have taken the place of RNN models in almost all
sequence modeling tasks, thanks to the effectiveness and efÔ¨Åciency of the self-attention mechanisms.
Various Transformer-based TSF methods (see Fig. 1(b)) are proposed in the literature .
These works typically focus on the challenging long-term time series forecasting problem, taking
advantage of their remarkable long sequence modeling capabilities.
Another popular type of TSF model is the so-called temporal convolutional network ,
wherein convolutional Ô¨Ålters are used to capture local temporal features (see Fig. 1(c)). The proposed
SCINet is also constructed based on temporal convolution. However, our method has several key
differences compared with the TCN model based on dilated causal convolution, as discussed in the
following.
Rethinking Dilated Causal Convolution for Time Series Modeling and Forecasting
The local correlation of time series data is reÔ¨Çected in the continuous changes within a time slot, and
convolutional Ô¨Ålters can effectively capture such local features. Consequently, convolutional neural
networks are explored in the literature for time series modeling and forecasting. In particular, dilated
causal convolution (DCS) is the current de facto method used in this respect.
DCS was Ô¨Årst proposed for generating raw audio waveforms in WaveNet . Later, simpliÔ¨Åes
the WaveNet architecture to the so-called temporal convolutional networks (see Fig. 1 (c)). TCN
consists of a stack of causal convolutional layers with exponentially enlarged dilation factors, which
can achieve a large receptive Ô¨Åeld with just a few convolutional layers. Over the years, TCN has been
widely used in all kinds of time series forecasting problems and achieve promising results .
Moreover, convolutional Ô¨Ålters can work seamlessly with graph neural networks (GNNs) to solve
various spatial-temporal TSF problems.
With causal convolutions in the TCN architecture, an output i is convolved only with the ith and
earlier elements in the previous layer. While causality should be kept in forecasting tasks, the potential
‚Äúfuture information leakage" problem exists only when the output and the input have temporal overlaps.
In other words, causal convolutions should be applied only in autoregressive forecasting, wherein the
previous output serves as the input for future prediction. When the predictions are completely based
on the known inputs in the look-back window, there is no need to use causal convolutions. We can
safely apply normal convolutions on the look-back window for forecasting.
More importantly, the dilated architecture in TCN has two inherent limitations:
‚Ä¢ A single convolutional Ô¨Ålter is shared within each layer. Such a uniÔ¨Åed convolutional kernel
tends to extract the average temporal features from the data/features in the previous layer.
However, complex time series may contain substantial temporal dynamics. Hence, it is
essential to extract distinct yet valuable features with a rich set of convolutional Ô¨Ålters.
‚Ä¢ While the Ô¨Ånal layer of the TCN model has the global view of the entire look-back window,
the effective receptive Ô¨Åelds of the intermediate layers (especially those close to the inputs)
are limited, causing temporal relation loss during feature extraction.
The above limitations of the TCN architecture motivate the proposed SCINet design, as detailed in
the following section.
Input: !ùêó!"# (ùêóif ùëò= 1)
#Level = 1
#Level = 2
#Level = L
Output: !ùêó$
Interactive
Concat & Realign
Onput: !ùêó!
#Stack = 1
#Stack = 2
(c) Stacked SCINet
(b) SCINet
(a) SCI-Block
#Stack = K
Figure 2: The overall architecture of Sample Convolution and Interaction Network (SCINet).
SCINet: Sample Convolution and Interaction Network
SCINet adopts an encoder-decoder architecture. The encoder is a hierarchical convolutional network
that captures dynamic temporal dependencies at multiple resolutions with a rich set of convolutional
Ô¨Ålters. As shown in Fig. 2(a), the basic building block, SCI-Block (Section 3.1), downsamples the
input data or feature into two sub-sequences and then processes each sub-sequence with a set of
convolutional Ô¨Ålters to extract distinct yet valuable temporal features from each part. To compensate
for the information loss during downsampling, we incorporate interactive learning between the two
sub-sequences. Our SCINet (Section 3.2) is constructed by arranging multiple SCI-Blocks into a
binary tree structure (Fig. 2(b)). A distinctive advantage of such design is that each SCI-Block has
both local and global views of the entire time series, thereby facilitating the extraction of useful
temporal features. After all the downsample-convolve-interact operations, we realign the extracted
features into a new sequence representation and add it to the original time series for forecasting with
a fully-connected network as the decoder. To facilitate extracting complicated temporal patterns, we
could further stack multiple SCINets and apply intermediate supervision to get a Stacked SCINet
(Section 3.3), as shown in Fig. 2(c).
The SCI-Block (Fig. 2(a)) is the basic module of the SCINet, which decomposes the input feature F
into two sub-features F
even through the operations of Spliting and Interactive-learning.
The Splitting procedure downsamples the original sequence F into two sub-sequences Feven and
Fodd by separating the even and the odd elements, which are of coarser temporal resolution but
preserve most information of the original sequence.
Next, we use different convolutional kernels to extract features from Feven and Fodd. As the
kernels are separate, the extracted features from them would contain distinct yet valuable temporal
relations with enhanced representation capabilities. To compensate for potential information loss with
downsampling, we propose a novel interactive-learning strategy to allow information interchange
between the two sub-sequences by learning afÔ¨Åne transformation parameters from each other. As
shown in Fig. 2 (a), the interactive learning procedure consists of two steps.
First, Feven and Fodd are projected to hidden states with two different 1D convolutional modules œÜ
and œà, respectively, and transformed to the formats of exp and interact to the Feven and Fodd with
the element-wise product (see Eq. (1)). This can be viewed as performing scaling transformation on
Feven and Fodd, where the scaling factors are learned from each other using neural network modules.
Here, ‚äôis the Hadamard product or element-wise production.
odd = Fodd ‚äôexp(œÜ(Feven)),
even = Feven ‚äôexp(œà(Fodd)).
odd ¬± œÅ(Fs
even ¬± Œ∑(Fs
Second, as shown in Eq. (11), the two scaled features Fs
even and Fs
odd are further projected to
another two hidden states with the other two 1D convolutional modules œÅ and Œ∑, and then added to
or subtracted from1 Fs
even and Fs
odd. The Ô¨Ånal outputs of the interactive learning module are two
updated sub-features F
even and F
odd. The default architectures of œÜ, œà, œÅ and Œ∑ are shown in the
Appendix C.
Compared to the dilated convolutions used in the TCN architecture, the proposed downsampleconvolve-interact architecture achieves an even larger receptive Ô¨Åeld at each convolutional layer. More
importantly, unlike TCN that employs a single shared convolutional Ô¨Ålter at each layer, signiÔ¨Åcantly
restricting its feature extraction capabilities, SCI-Block aggregates essential information extracted
from the two downsampled sub-sequences that have both local and global views of the entire time
With the SCI-Blocks presented above, we construct the SCINet by arranging multiple SCI-Blocks
hierarchically and get a tree-structured framework, as shown in Fig. 2 (b).
There are 2l SCI-Blocks at the l-th level, where l = 1, . . . , L is the index of the level, and L is
the total number of levels. Within the k-th SCINet of the stacked SCINet (Section 3.3), the input
time series X (for k =1) or feature vector ÀÜXk‚àí1 ={ÀÜxk‚àí1
, ..., ÀÜxk‚àí1
} (for k >1) is gradually downsampled and processed by SCI-Blocks through different levels, which allows for effective feature
learning of different temporal resolutions. In particular, the information from previous levels will be
gradually accumulated, i.e., the features of the deeper levels would contain extra Ô¨Åner-scale temporal
information transmitted from the shallower levels. In this way, we can capture both short-term and
long-term temporal dependencies in the time series.
After going through L levels of SCI-Blocks, we rearrange the elements in all the sub-features by
reversing the odd-even splitting operation and concatenate them into a new sequence representation.
It is then added to the original time series through a residual connection to generate a new
sequence with enhanced predictability. Finally, a simple fully-connected network is used to decode
the enhanced sequence representation into ÀÜXk ={ÀÜxk
1, ..., ÀÜxk
1The selection of the operators in Eq.(2) affects the parameter initialization of our model and we show its
impact in the Appendix B.3.
Stacked SCINet
When there are sufÔ¨Åcient training samples, we could stack K layers of SCINets to achieve even better
forecasting accuracy (see Fig. 2 (c)), at the cost of a more complex model structure.
SpeciÔ¨Åcally, we apply intermediate supervision on the output of each SCINet using the groundtruth values, to ease the learning of the intermediate temporal features. The output of the k-th
intermediate SCINet, ÀÜXk with length œÑ, is concatenated with part of the input Xt‚àí(T ‚àíœÑ)+1:t to
recover the length to the original input and feeded as input into the (k + 1)-th SCINet, where
k = 1, . . . , K‚àí1, and K is the total number of the SCINets in the stacked structure. The output of
the K-th SCINet, ÀÜXK, is the Ô¨Ånal forecasting results.
Loss Function
To train a stacked SCINet with K (K ‚â•1) SCINets, the loss of the k-th prediction results is calculated
as the L1 loss between the output of the k-th SCINet and the ground-truth horizontal window to be
predicted:
The total loss of the stacked SCINet can be written as:
Complexity Analysis
Thanks to the downsampling procedure, the neurons at each convolutional layer of SCINet have
a larger receptive Ô¨Åeld than those of TCN. More importantly, the set of rich convolutional Ô¨Ålters
in SCINet enable Ô¨Çexible extraction of temporal features from multiple resolutions. Consequently,
SCINet usually does not require downsampling the original sequence to the coarsest level for effective
forecasting. Given the look-back window size T, TCN generally requires ‚åàlog2 T‚åâlayers when the
dilation factor is 2, while the number of layers L in SCINet could be much smaller than log2 T. Our
empirical study shows that the best forecasting accuracy is achieved with L‚â§5 in most cases even
with large T (e.g., 168). As for the number of stacks K, our empirical study also shows that K ‚â§3
would be sufÔ¨Åcient.
Consequently, the computational cost of SCINet is usually on par with that of the TCN architecture.
The worst-case time complexity is O(T log T), much less than that of vanilla Transformer-based
solutions: O(T 2).
Experiments
In this section, we show the quantitative and qualitative comparisons with the state-of-the-art models
for time series forecasting. We also present a comprehensive ablation study to evaluate the effectiveness of different components in SCINet. More details on datasets, evaluation metrics, data
pre-processing, experimental settings, network structures and their hyper-parameters are shown in
the Appendix.
We conduct experiments on 11 popular time series datasets: (1) Electricity Transformer Temperature (ETTh) (2) TrafÔ¨Åc (3) Solar-Energy (4) Electricity (5) Exchange-Rate (6) PeMS (PEMS03,
PEMS04, PEMS07 and PEMS08). A brief description of these datasets is listed in Table 1. All the
experiments on these datasets in this section are conducted under multi-variate TSF setting.
To make a fair comparison, we follow existing experimental settings, and use the same evaluation
metrics as the original publications in each dataset.
Table 1: The overall information of the 11 datasets.
Solar-Energy
Electricity
Exchange-Rate
Granularity
Start time
Multi-step
Multi-step
Single-step
Single-step
Single-step
Single-step
Multi-step
Multi-step
Multi-step
Multi-step
Data partition
Follow 
Training/Validation/Testing: 6/2/2
Training/Validation/Testing: 6/2/2
Results and Analyses
Table 2, 3, 4, 5, 6 provide the main experimental results of SCINet. We observe that SCINet shows
superior performance than other TSF models on various tasks, including short-term, long-term and
spatial-temporal time series forecasting.
Short-term Time Series Forecasting: we evaluate the performance of the SCINet in short-term TSF
tasks with other baseline methods on TrafÔ¨Åc, Solar-Energy, Electricity and Exchange-Rate datasets.
The experimental setting is the same as , which uses the input length of 168 to forecast different
future horizons{3, 6, 12, 24}.
As can be seen in Table 2, the proposed SCINet outperforms existing RNN/TCN-based (LSTNet ,
TPA-LSTM , TCN , TCN‚Ä†) and Transformer-based TSF solutions in most cases,
especially for the Solar-Energy and Exchange-Rate datasets. Note that, TCN‚Ä† denotes a variant of
TCN wherein causal convolutions are replaced by normal convolutions, and improves the original
TCN across all the datasets, which supports our claim in Sec. 2.2. Moreover, we can also observe that
the Transformer-based methods have poor performance in this task. For short-term forecasting, the
recent data points are typically more important for accurate forecasting. However, the permutationinvariant self-attention mechanisms used in Transformer-based methods do not pay much attention to
such critical information. In contrast, the general sequential models (RNN/TCN) can formulate it
easily, showing quite competitive results in short-term forecasting.
Table 2: Short-term forecasting performance comparison on the four datasets. The best results are
shown in bold and second best results are highlighted with underlined blue font. IMP shows the
improvement of SCINet over the best model.
Autoformer 
Informer 
Transformer 
LSTNet 
TPA-LSTM 
Solar-Energy
Electricity
- Autoformer, Informer and Transformer achieved by Autoformer requires pre-prossessed datasets for training.
- N/A denotes no pre-prossessed dataset for training.
- ‚àódenotes re-implementation.
‚Ä† denotes the variant with normal convolutions.
Long-term Time Series Forecasting: many real-world applications also require to predict longterm events. Therefore, we conduct the experiments on Exchange Rate, Electricity ,TrafÔ¨Åc and ETT
datasets to evaluate the performance of SCINet on long-term TSF tasks. In this experiment, we
only compare SCINet with Transformer-based methods , since they are more
popular in recent long-term TSF research.
As can be seen from Table 3, the SCINet achieves state-of-the-art performances in most benchmarks
and prediction length settings. Overall, SCINet yields 39.89% average improvements on MSE among
the above settings. In particular, for Exchange-Rate, compared to previous state-of-the-art results,
SCINet gives average 65% improvements on MSE. We attribute it to that the proposed SCINet can
better capture both short (local temporal dynamics)- and long (trend, seasonality)-term temporal
dependencies to make an accurate prediction in long-term TSF. Besides, compared with the vanilla
Transformer-based methods , the newly-proposed Transformer-based forecasting model
Autoformer achieves the second best performance in all experimental settings and also surpasses
the SCINet in TrafÔ¨Åc(96). This is because, Autoformer incorporates more prior knowledge about
the time series data. It focuses on modelling seasonal patterns and conducts self-attention at the
sub-series level (instead of the raw data), which is much better in extracting long-term temporal
patterns than vanilla Transformer-based methods.
Table 3: Long-term forecasting performance comparison with Transformer-based models.
Autoformer 
‚àóPyraformer 
Informer 
Transformer 
LogTrans 
Reformer 
Electricity
- ‚àódenotes re-implementation.
Table 4: Multivariate time-series forecasting results on the ETT datasets.
LogTrans 
Reformer 
LSTNet 
Informer 
*Pyraformer 
Autoformer 
- ‚àódenotes re-implementation.
Besides, ETT datasets are originally used to evaluate the performance of long-sequence TSF
tasks, which are conducted on two experimental settings, Multivariate Time-series Forecasting and
Univariate Time-series Forecasting. For a fair comparison, we keep all input lengths T the same as
Informer. The results are shown in Table 4 and Table 5, respectively.
Multivariate Time-series Forecasting on ETT: as can be seen from Table 4, compared with RNNbased methods such as LSTMa and LSTnet , Transformer-based methods are
better at capturing the long-term latent patterns in the entire historical data for predicting the future,
leading to lower prediction errors. However, TCN further outperforms such vanilla Transformer-based
methods , because the stacked convolutional layers allow for more effective local-to-global
temporal relation learning for multivariate time series. It is worth noting that SCINet outperforms all
the above models by a large margin. Fig. 3 presents the qualitative results on some randomly selected
sequences of the ETTh1 dataset, which clearly demonstrate the capability of SCINet in obtaining the
trend and seasonality of time series for TSF.
Univariate Time-series Forecasting on ETT: in this experimental setting, we bring several strong
baseline methods for univariate forecasting into comparison, including ARIMA, Prophet ,
DeepAR and N-Beats . In Table 5, we can observe that N-Beats is superior to other
baseline methods in most cases. In fact, N-Beats also takes the unique properties of time series
into consideration and directly learns a trend and a seasonality model using a very deep stack of
fully-connected layers with residuals, which is a departure from the predominant architectures, such
as RNNs, CNNs and Transformers. Nevertheless, the performance of SCINet is still much better than
We attribute the signiÔ¨Åcant performance improvements of SCINet on the ETT datasets to: (i) SCINet
effectively captures temporal dependencies from multiple temporal resolutions; (ii) ETT datasets are
publicly available recently and domain-speciÔ¨Åc solutions tuned speciÔ¨Åcally for these datasets do not
exist yet.
Table 5: Univariate time-series forecasting results on the ETT datasets.
Prophet 
DeepAR 
N-Beats 
Informer 
Autoformer 
Spatial-temporal Time Series Forecasting: besides the general TSF tasks, there is also a large
category of data related to spatial-temporal forecasting. For example, trafÔ¨Åc datasets PeMS 
(PEMS03, PEMS04, PEMS07 and PEMS08) are complicated spatial-temporal time series for public
trafÔ¨Åc network and they have been investigated for decades. Most recent approaches: DCRNN ,
STGCN , ASTGCN , GraphWaveNet , STSGCN , AGCRN , LSGCN and
STFGNN use graph neural networks to capture spatial relations while modeling temporal
dependencies via conventional TCN or LSTM architectures. We follow the same experimental
settings as the above works. As shown in Table 6, these GNN-based methods generally perform
better than pure RNN or TCN-based methods. However, SCINet still achieves better performance
without sophisticated spatial relation modelling, which further proves the superb temporal modeling
capabilities of SCINet.
Table 6: Performance comparison of different approaches on the PeMS datasets.
GraphWaveNet
- dash denotes that the methods do not implement on this dataset.
‚àódenotes re-implementation or re-training.
‚Ä† denotes the variant with normal convolutions.
Predictability estimation: inspired by , we use permutation entropy (PE) to measure the
predictability of the original input and the enhanced representation learnt by SCINet. Time series with
lower PE values are regarded as less complex, thus theoretically easier to predict2. The PE values of
the original time series and the corresponding enhanced representations are shown in Table 7.
Table 7: Permutation entropy comparison before and after SCINet.
Permutation Entropy
Solar-Energy
Electricity
Parameters
m (œÑ = 1)‚àó
Original Input
Enhanced Representation
‚àóm (embedding dimension) and œÑ (time-lag) are two parameters used for calculating PE, and the values are selected following .
As can be observed, the enhanced representations learnt by SCINet indeed have lower PE values
compared with the original inputs, which indicates that it is easier to predict the future from the
enhanced representations using the same forecaster.
2Please note that PE is only a quantitative measurement based on complexity. It would not be proper to say
that a time series with lower PE value will be always easier to predict than a different type of time series with a
higher PE value because the prediction accuracy also depends on many other factors, such as the available data
for training, the trend and seasonality elements of the time series data, as well as the predictive model.
Future Time Steps
Prediction Results
Autoformer
Ground Truth
(a) Sequence 441, Variate 3
Future Time Steps
Prediction Results
Autoformer
Ground Truth
(b) Sequence 1388, Variate 1
Future Time Steps
Prediction Results
Autoformer
Ground Truth
(c) Sequence 2745, Variate 4
Future Time Steps
Prediction Results
Autoformer
Ground Truth
(d) Sequence 2753, Variate 5
Figure 3: The prediction results (Horizon = 48) of SCINet, Autoformer, Informer, and TCN on
randomly-selected sequences from ETTh1 dataset.
Ablation studies
To evaluate the impact of each main component used in SCINet, we experiment on several model
variants on two datasets: ETTh1 and PEMS08.
SCIBlock: we Ô¨Årst set the number of stacks K = 1 and the number of SCINet levels L = 3 .
For the SCI-Block design, to validate the effectiveness of the interactive learning and the distinct
convolution weights for processing the sub-sequences, we experiment on two variants, namely w/o.
InterLearn and WeightShare. The w/o. InterLearn is obtained by removing the interactive-learning
procedure described in Eq. (1) and (11). In this case, the two sub-sequences would be updated using
odd =œÅ(œÜ(Fodd)) and F
even =Œ∑(œà(Feven)). For WeightShare, the modules œÜ, œÅ, œà, and Œ∑ share
the same weight.
The evaluation results in Fig. 4 show that both interactive learning and distinct weights are essential,
as they improve the prediction accuracies of both datasets at various prediction horizons. At the same
time, comparing Fig. 4(a) with Fig. 4(b), we can observe that interactive learning is more effective for
cases with longer look-back window sizes. This is because, intuitively, we can extract more effective
features by exchanging information between the downsampled sub-sequences when there are longer
look-back windows for such interactions.
SCINet: for the design of SCINet with multiple levels of SCI-Blocks, we also experiment on two
variants. The Ô¨Årst variant w/o. ResConn is obtained by removing the residual connection from the
complete SCINet. The other variant w/o. Linear removes the decoder (i.e., the fully-connected
layer) from the complete model. As can be observed in Fig. 4, removing the residual connection
leads to a signiÔ¨Åcant performance drop. Besides the general beneÔ¨Åt in facilitating the model training,
more importantly, the predictability of the original time series is enhanced with the help of residuals.
The fully-connected layer is also critical for prediction accuracy, indicating the effectiveness of
w/o ResCon
w/o Linear
w/o InterLearn
WeightShare
Mean Absolute Error
Mean Absolute Error
(a) ETTh1(H = 720)
(b) PEMS08(H = 12)
w/o ResCon
w/o Linear
w/o InterLearn
WeightShare
Figure 4: Component analysis of SCINet on two datasets. Smaller values are better. See Section 4.3.
the decoder in extracting and fusing the most relevant temporal information according to the given
supervision for prediction.
We also conduct comprehensive ablation studies on the impact of K (number of stacks) and L
(number of levels), and the selection of operator in the interact learning mechanism. These results are
shown in the Appendix B.2 due to space limitation.
Limitations and Future Work
In this paper, we mainly focus on TSF problem for the regular time series collected at even intervals of
time and ordered chronologically. However, in real-world applications, the time series might contain
noisy data, missing data or collected at irregular time intervals, which is referred to as irregular
time series. The proposed SCINet is relatively robust to the noisy data thanks to the progressive
downsampling and interactive learning procedure, but it might be affected by the missing data if
the ratio exceeds a certain threshold, wherein the downsampling-based multi-resolution sequence
representation in SCINet may introduce biases, leading to poor prediction performance. The proposed
downsampling mechanism may also have difÔ¨Åculty handling data collected at irregular intervals. We
plan to take the above issues into consideration in the future development of SCINet.
Moreover, this work focuses on the deterministic time series forecasting problem. Many application
scenarios require probabilistic forecasts, and we plan to revise SCINet to generate such prediction
Finally, while SCINet generates promising results for spatial-temporal time series without explicitly
modeling spatial relations, the forecasting accuracy could be further improved by incorporating
dedicated spatial models. We plan to investigate such solutions in our future work.
Conclusion
In this paper, we propose a novel neural network architecture, sample convolution and interaction
network (SCINet) for time series modeling and forecasting, motivated by the unique properties
of time series data compared to generic sequence data. The proposed SCINet is a hierarchical
downsample-convolve-interact structure with a rich set of convolutional Ô¨Ålters. It iteratively extracts
and exchanges information at different temporal resolutions and learns an effective representation
with enhanced predictability. Extensive experiments on various real-world TSF datasets demonstrate
the superiority of our model over state-of-the-art methods.
Acknowledgments and Disclosure of Funding
This work was supported in part by Alibaba Group Holding Ltd. under Grant No. TA2015393. We
thank the anonymous reviewers for their constructive comments and suggestions.