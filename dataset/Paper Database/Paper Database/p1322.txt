An Active Learning Approach to Knowledge
Transfer for Hyperspectral Data Analysis
Suju Rajan and Joydeep Ghosh, Fellow, IEEE
Department of Electrical and
Computer Engineering
University of Texas at Austin
Austin, Texas 78712
Email: {suju,ghosh}@ece.utexas.edu
Melba M. Crawford, Senior Member, IEEE
School of Civil Engineering
Purdue University
West Lafayette, Indiana 47907
Email: 
Abstract— Obtaining ground truth for classiﬁcation of remotely sensed data is time consuming and expensive. In addition,
a number of factors cause the spectral signatures of the same
class to vary spatially. Therefore, successful adaptation of a
classiﬁer designed from available labeled data to classify new
images acquired over other geographic locations is difﬁcult but
invaluable to the remote sensing community. In this paper we
propose an active learning technique for rapidly updating existing
classiﬁers using very few labeled data points from the new image.
We also show empirically that our updated classiﬁer exhibits
better learning rates than classiﬁers trained via other active
learning and semi-supervised methods.
I. INTRODUCTION
A common application of hyperspectral imaging involves
mapping the spectral signatures in the images to speciﬁc
land cover types. While hyperspectral data are now readily
available, obtaining accurate class labels for each ‘pixel’ is a
non-trivial task involving expensive ﬁeld campaigns and timeconsuming manual interpretation of imagery. Typically, labeled ground truth data are acquired over spatially contiguous
sites that are easily accessible. Such ‘spatially localized’ data
are then used to classify the entire hyperspectral image including those regions from which no labeled data were obtained
 . Implicit in this method of classiﬁcation is the assumption
that the spectral signatures of each land cover type do not
exhibit substantial spatial (or temporal) variations. However,
factors such as soil composition, topographic variations, and
local atmospheric condition alter the spectral characteristics
measured at the sensor, even though they correspond to the
same land cover type, from one region to another. Hence, the
na¨ıve use of a classiﬁer trained on available ground truth data
from one region on data that are from spatially different areas,
without accounting for the variability of the class signatures,
will result in poor classiﬁcation accuracies .
Existing approaches to knowledge transfer typically require
labeled data from the new area for updating existing classiﬁers
 . A pioneering attempt at unsupervised knowledge
transfer for multitemporal remote sensing images was made
in . In this method, classiﬁers trained over one image were
updated using the unlabeled data from the temporally different
image via Expectation-Maximization (EM) techniques . The
only knowledge transferred involved the estimates for the
parameters of the class distributions used to initialize the
EM algorithm. If the spectral signatures between the two
regions vary signiﬁcantly, such na¨ıve transfer of the estimates
might degrade rather than improve the EM process. A possible
solution is to identify classes whose spectral signatures vary
signiﬁcantly between the two regions and use the corresponding labeled data to initialize those class distribution estimates.
However, one then requires the labeled data from the new
region to be able to identify these ‘differing’ classes.
In this paper, we propose an active learning approach for ef-
ﬁciently updating the parameters of the differing classes while
using as few labeled data points from the new area as possible.
The proposed method automatically identiﬁes data points that
change the current belief in the class distributions the most.
Thus, labeled data are required only from those classes that
vary signiﬁcantly, while the existing parameter estimates are
used for the remaining classes. We also empirically show that
using such ‘informative’ data points yields better learning rates
than updating the classiﬁer with randomly chosen data points
from the new area. Our proposed method is also shown to
outperform other active learning strategies as well as semisupervised EM techniques on some hyperspectral datasets.
II. ACTIVE LEARNING
Let X be the set of data points, such that each data point xi
has an associated class label yi drawn from a set of k classes.
Let us assume that there exists a pool D of examples {xi}n
that are to be classiﬁed. In the active learning setting, the
learner is provided with an initial training set drawn from D,
say DL, consisting of pairs of labeled examples, {xi, yi}m
The learner then selects a ˆx from DUL = D\DL, such that
adding (ˆx, ˆy) to DL and retraining the classiﬁer minimizes
the loss function associated with the classiﬁer. Note that the
learner does not have access to the label ˆy prior to committing
to a speciﬁc ˆx. The process of identifying an ˆx and adding it
to DL is repeated for a user-speciﬁed number of iterations.
A. Related Work
A statistical approach to active learning for function approximation problems was ﬁrst proposed by Cohn et al ,
wherein the bias-variance decomposition of the squared error
0-7803-9510-7/06/$20.00 © 2006 IEEE 545
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 16:24 from IEEE Xplore. Restrictions apply.
function was utilized in selecting ˆx. Given an unbiased learner,
the goal is to select a new data point ˆx such that adding it
to DL minimizes the expected variance in the output of the
learner measured over the input space. Unlike classiﬁcation
problems, in this case y ∈ℜd, and the authors present closed
form solutions to compute the expected output variance using
models such as the mixtures of Gaussians and locally weighted
regression.
While MacKay proposed an active learning algorithm that
attempts to increase the expected information gain about a
user-deﬁned variable on adding the new data point (ˆx, ˆy) to
DL , the method of Roy et al , attempts to reduce the
expected user-deﬁned error, measured over the input space.
Given a loss function L, the true probability distribution
Ptrue(y|x), and the probability distribution estimated from
the training set PDL(y|x), the expected loss of the learner
is deﬁned as:
L(Ptrue(y|x), PDL(y|x))P(x)dx
where L is any user-speciﬁed loss function. Active learning
proceeds by selecting a data point such that the expected error
from using the training set D∗
L = DL ∪(ˆx, ˆy) is the least, over
all possible ˆx ∈DUL.
A popular class of active learning algorithms is that of
committee based learners. Of these methods, the ‘Query By
Committee’ (QBC) approach of is a general active learning
algorithm that has theoretical guarantees on the reduction in
prediction error with the number of queries. Given an inﬁnite
stream of unlabeled examples, the QBC selects the data point
on which instances of the Gibbs algorithm, drawn according
to a probability distribution deﬁned over the version space,
disagree. However, the algorithm assumes the existence of a
Gibbs algorithm and noise-free data. A number of variations
to the original QBC algorithm have been proposed, such as
the Query by Bagging and Query by Boosting algorithm 
and the adaptive resampling approach .
Active learning has also been applied in the multi-view
setting . In the multi-view problem, features can be
partitioned into subsets each of which is sufﬁcient for learning
the mapping from the input to the output space. In the Co-
Testing family of algorithms, classiﬁers are constructed for
each view of the data. Provided the views are ‘compatible’ and
‘uncorrelated’ the data points on which the classiﬁers disagree
are likely to be most informative.
B. Proposed Approach
In this paper, we propose a new active learning approach
based on the method proposed by MacKay . While 
deﬁned the ‘interpolant function’ as the variable whose information gain is to be maximized, we try to maximize
the information gain on the posterior probability distribution
deﬁned on the set of data points. Setting D∗
L = DL ∪(ˆx, ˆy)
DL(y|x) and PDL(y|x) as the posterior probability
distributions estimated from D∗
L and DL respectively, it can
be shown that maximizing the expected information gain
between P ∗
DL(y|x) and PDL(y|x) is equivalent to selecting
the data point ˆx from DUL such that the expected Kullback-
Liebler (KL) divergence between P ∗
DL(y|x) and PDL(y|x) is
maximized.
Since the true label of ˆx is initially unknown, we follow
the methodology of and and estimate the expected KL
distance between P ∗
DL(y|x) and PDL(y|x) by ﬁrst selecting an
˜x ∈DUL and assuming ˜y to be its label. Let D∗
UL = DUL\˜x
L = DL ∪(˜x, ˜y). Estimating via sampling, the proposed
KLmax function can be written in terms of (˜x, ˜y) as:
L (˜x, ˜y) =
DL(y|x)||PDL(y|x))
Note that simply assigning a wrong class label to the ˜y for
an ˜x can result in a large value of the corresponding KLmax
Hence, as in and we use the expected KL-distance
DL(y|x) and PDL(y|x), with the expectation estimated
over PDL(y|x), and then select the ˆx which maximizes this
ˆx = argmax
L (˜x, ˜y)PDL(˜y|˜x)
C. Methodology
Let us assume that we have hyperspectral data from two
spatially different areas, Area 1 and 2. Let us also suppose
that for Area 1, there is an adequate amount of labeled data
to build a supervised classiﬁer. To combat the effect of high
dimensionality of the hyperspectral data, the feature space is
reduced by recursively combining highly correlated, adjacent
bands . Since this best-bases feature extraction method
makes use of class-speciﬁc information in determining the set
of adjacent bands that are to be merged, this information can
be exploited in Area 2.
Assuming the class-conditional density functions to be
multivariate Gaussians, a Maximum Likelihood classiﬁer is
trained on the data from Area 1. Prior to learning the classiﬁer
the dimensionality of the feature space is further reduced
by making use of a Fisher-m feature extractor. The bestbases feature extractor, the Fisher-M feature extractor and the
ML classiﬁer from Area 1 are then used to obtain initial
posterior probabilities of the Area 2 data (E-step). While
the labeled data from Area 1 are used to initialize the EM
process, subsequent EM iterations are guided by the posterior
probabilities assigned to the unlabeled Area 2 data. The probabilities thus obtained, are then used to update the parameters
of the Gaussians (M-step). The EM iterations are performed
until the average change in the posterior probabilities between
two iterations is smaller than a speciﬁed threshold. A new
Fisher feature extractor is also computed at each EM iteration,
based on the statistics of the classes at that iteration. The
updated extractor is then used to project the data into the
corresponding Fisher feature space prior to the estimation of
the class conditional pdfs.
Setting PDL(y|x) as the posterior probability of the unlabeled data DUL obtained at the end of the EM iterations, we
0-7803-9510-7/06/$20.00 © 2006 IEEE 546
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 16:24 from IEEE Xplore. Restrictions apply.
need to select the (ˆx, ˆy) from DUL such that the expected KL
divergence between P ∗
DL(y|x) and PDL(y|x) is maximized,
L = DUL ∪(ˆx, ˆy). For reasons of computational ef-
ﬁciency, the (ˆx, ˆy) is selected from a randomly sampled subset
of DUL. A data point ˜x is selected from the subset of DUL
and the label ˜y is assigned to it. This new data point (˜x, ˜y) is
then used to update the existing class parameter estimates, and
a new posterior probability distribution P ∗
DL(y|x) is obtained.
Using Eqn.2 and Eqn.3 the expected value of KLmax
L (˜x, ˜y)
is computed over D∗
UL = DUL\˜x for all possible ˜y. The
data point (ˆx, ˆy) from DUL with the maximum expected KL
divergence is then added to the set of labeled data points,
where ˆy is the true label of ˆx.
For the next iteration of active learning, the EM process is
repeated as before except that we perform constrained EM.
Simply stated, in this technique while both the Area 1 and the
labeled Area 2 data are used to initialize the EM algorithm,
the E-step only updates the posterior probabilities for the
unlabeled Area 2 data while ﬁxing the memberships of the
labeled instances according to the known class assignments.
Note that the posterior probability distributions of the Area
2 data determines PDL(y|x) and guides the active learning
process. Thus, we ensure that we select those ‘informative’
Area 2 data points that change the existing belief in the
distributions of the Area 2 classes the most. Selecting such
data points should result in better learning curves than if the
data are selected at random.
III. EXPERIMENTAL EVALUATION
In this section, we provide empirical evidence that incorporating active learning into the knowledge transfer framework
results in steeper learning rate curves. We present results
showing that our proposed method exhibits better learning
rates than updating existing classiﬁers with data points selected
either at random or via an existing, related active learning
method. We also empirically show results that the active
learning methods offer a signiﬁcant advantage over the more
traditional semi-supervised methods by requiring far fewer
data points to obtain better classiﬁcation accuracies.
A. Data sets
The proposed active learning method was tested on hyperspectral data sets obtained from two sites: NASA’s John F.
Kennedy Space Center (KSC), Florida and the Okavango
Delta, Botswana .
1) Kennedy Space Center (KSC):
The NASA AVIRIS
spectrometer acquired data over the KSC on March 23,
1996. AVIRIS acquires data in 242 bands of 10nm width
from 400-2500nm. The KSC data, collected from an altitude
of approximately 20km, have a spatial resolution of 18m.
Removal of noisy and water absorption bands resulted in 176
candidate features. Discrimination of land cover types for this
environment is difﬁcult due to the similarity of the spectral
signatures for certain vegetation types and the existence of
mixed classes. The 512×614 spatially removed test set (Area
2) is a different subset of the ﬂight line than the 512 × 614
data set from Area 1. While the number of classes in the two
regions differs, we restrict ourselves to those classes that are
present in both regions.
2) Botswana: This 1476×256 pixel study area is located in
the Okavango Delta, Botswana, and has 14 different land cover
types. Data from this region were obtained by the NASA EO-1
satellite for the calibration/validation portion of the mission in
2001. The Hyperion sensor on EO-1 acquires data at 30m
pixel resolution over a 7.7km strip in 242 bands covering
the 400-2500nm portion of the spectrum in 10nm windows.
Uncalibrated and noisy bands that cover water absorption
features were removed resulting in 145 features. The spatially
removed test data for the May 31, 2001 acquisition were
sampled from spatially contiguous clusters of pixels that were
within the same scene, but disjoint from those used for the
training data.
B. Experimental Methodology
In all the data sets, the labeled data (Area 1) were subsampled such that 75% of the data were used for training and
25% as the test set. For both cases, a second test set was also
acquired from the spatially separate region (Area 2).
The ML-EM classiﬁer, for all the methods evaluated, was
modeled using a multivariate Gaussian for each class. The best
bases feature extractor and the Fisher discriminant were used
to reduce the dimensionality of the input data. The number of
best bases was determined by using a validation set from the
Area 1 training data. Constrained EM was used to update the
parameters of the Gaussians as well as the Fisher discriminant
as detailed in Section II-C.
The proposed active learning method was evaluated against
the baseline method of choosing the data points, one at a time,
at random from Area 2 and using constrained EM to update
the estimates of the class parameters from Area 1.
The active learning approach of Roy et al. was also
implemented and evaluated. This method attempts to reduce
the expected error measured over the input space. Using the
log-loss function results in selecting those data points that
cause an increase in the future expected entropy. Following
the notation from Eqns. 2 and 3, the ˆx is selected using the
following equations:
L(˜x, ˜y) =
L(y|x)logPD∗
L(y|x) (4)
The ˆx ∈DUL with the lowest expected error is then selected
for querying and is added to DL.
ˆx = argmin
L(˜x, ˜y)PDL(˜y|˜x)
Finally, for the semi-supervised scenario, small quantities
of labeled data were selected from each class at random. The
knowledge transfer method as proposed in was modiﬁed
to incorporate the labeled data into the EM process. The Area
1 data and the labeled Area 2 data were used to initialize
the Gaussians prior to performing the EM iterations. The
parameters of the Gaussians and the Fisher feature extractors
0-7803-9510-7/06/$20.00 © 2006 IEEE 547
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 16:24 from IEEE Xplore. Restrictions apply.
No. of Samples
Classification Accuracy
Semi−Supervised
Learning Rates for Botswana Area 2
No. of Samples
Classification Accuracy
Semi−supervised
Learning Rates for KSC Area 2
were then updated using the labeled and unlabeled data from
Area 2 via constrained EM.
IV. RESULTS AND DISCUSSION
Figure 1 and 2 show the learning rate curves for the
Botswana and KSC datasets over 140 active learning iterations.
It can be clearly seen that in both cases the proposed active
learning approach yields better classiﬁcation accuracies than
the ‘entropy’ method of Roy et al. . It is interesting to
note that adding a single randomly chosen data point at a
time (Random curve) has the same effect as batch-training the
classiﬁer via the semi-supervised technique (Semi-Supervised
curve). Figure 2 shows that for the KSC dataset the entropybased approach performs worse than the random active learning method, this is because there is a greater disparity in
the spectral signatures of the classes between the two areas.
The proposed method, however, remains unaffected by the
magnitude of spatial separation in the datasets.
V. CONCLUSION
We proposed a new active learning based knowledge transfer approach which seems to be particularly well-suited to the
scenario in which the distributions of the classes show spatial
(or temporal) variations. The principle of selecting those data
points that change the existing belief in class distributions
the most helps in efﬁciently and rapidly updating the existing
classiﬁer for a new, related problem. The proposed method
is empirically shown to be far better than choosing random
points, batch semi-supervised methods, and an entropy-based
active learning method. This study can be expanded when
more hyperspectral data are available, especially to determine
the effectiveness of the active learning based knowledge transfer framework when the spatial/temporal separation of the data
sets is increased systematically.
Acknowledgment: This work was supported by NSF Grant
IIS-0312471.