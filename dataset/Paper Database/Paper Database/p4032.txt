Time-series Classiﬁcation Using Neural
Bag-of-Features
Nikolaos Passalis∗, Avraam Tsantekidis∗, Anastasios Tefas∗,
Juho Kanniainen†, Moncef Gabbouj‡ and Alexandros Iosiﬁdis‡§
∗Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece
{passalis, avraamt}@csd.auth.gr, 
†Laboratory of Industrial and Information Management, Tampere University of Technology, Tampere, Finland
 
‡Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
{moncef.gabbouj, alexandros.iosiﬁdis}@tut.ﬁ
§Department of Engineering, Electrical and Computer Engineering, Aarhus University, Denmark
 
Abstract—Classiﬁcation of time-series data is a challenging
problem with many real-world applications, ranging from identifying medical conditions from electroencephalography (EEG)
measurements to forecasting the stock market. The well known
Bag-of-Features (BoF) model was recently adapted towards timeseries representation. In this work, a neural generalization of the
BoF model, composed of an RBF layer and an accumulation layer,
is proposed as a neural layer that receives the features extracted
from a time-series and gradually builds its representation. The
proposed method can be combined with any other layer or
classiﬁer, such as fully connected layers or feature transformation
layers, to form deep neural networks for time-series classiﬁcation.
The resulting networks are end-to-end differentiable and they can
be trained using regular back-propagation. It is demonstrated,
using two time-series datasets, including a large-scale ﬁnancial
dataset, that the proposed approach can signiﬁcantly increase
the classiﬁcation metrics over other baseline and state-of-the-art
techniques.
I. INTRODUCTION
Classiﬁcation of time-series data is a challenging problem
with many real-word applications, ranging from identifying
medical conditions from electroencephalography (EEG) measurements , to forecasting the stock market . Time-series
data are sequences of observations (usually real-valued numbers) sampled at speciﬁc (uniform or non-uniform) intervals.
Note that the observations can be either one-dimensional,
e.g., the price of a stock, or multi-dimensional, e.g., the
measurements of the multiple electrodes of an EEG device.
A wide range of techniques have been proposed to classify
time-series data – . Some of them rely on a distance
metric, such as the Dynamic Time Wrapping, to measure the
similarity between different time-series and then use simple
machine learning models, such as the k-nn, to classify the
data . Other techniques use recurrent neural networks ,
 , and hidden Markov models , to tackle the problem
of time-series classiﬁcation. In this work we focus on a
recently proposed model for time-series representation and
classiﬁcation, the Bag-of-Features model – .
The Bag-of-Features model (BoF), also known as Bag-of-
Visual Words (BoVW), was originally proposed for image
representations tasks . However, it was established that
it can be used for a wide range of tasks, such as video
 , and time-series representation – . The BoF pipeline
can be summarized as follows. First, multiple features, e.g.,
SIFT descriptors, are extracted from each object, e.g., an
image. This step is called feature extraction. That way, the
feature space is formed where each object is represented as
a set of features. Then, in the dictionary learning step, the
extracted features are used to learn a dictionary/codebook
of representative features (also called codewords). Finally,
the feature quantization and encoding step follows, in which
each feature vector is represented using a codeword from the
learned codebook and a histogram is extracted for each object.
That way, the histogram space is formed where each object is
represented by a constant dimensionality histogram vector.
The BoF model allows for creating constant length representations of time-series regardless their length. This representation captures the dynamics of the time-series behavior
and it can be used with any classiﬁer without having to deal
with the raw data. However, to use the BoF model for timeseries representation a feature extractor must be utilized for extracting feature vectors from each time-series and proceeding
with the aforementioned BoF pipeline. Several options exists
for the feature extraction step. For example, each raw (multidimensional) time-series point can be considered as a feature
vector , short intervals of various lengths can be used to
allow for handling time warping , or handcrafted features
can be extracted from several points of the time-series .
The learned BoF representation also critically relies on
the dictionary learning step. The early BoF approaches (e.g.,
 ) used clustering algorithms, such as k-means, to learn
generic dictionaries that minimize the reconstruction loss of
the quantized features. These dictionaries were not optimized
for a speciﬁc task. However, it was later established that
supervised dictionary learning (e.g., , – ), which
optimizes the dictionary towards a speciﬁc task, performs
signiﬁcantly better.
The contributions of this work are brieﬂy summarized
2017 25th European Signal Processing Conference (EUSIPCO)
ISBN 978-0-9928626-7-1 © EURASIP 2017
bellow. A neural generalization of the BoF model, composed
of an Radial Basis Function (RBF) layer and an accumulation layer, is proposed as a neural layer that receives the
features extracted from a time-series and gradually builds
its representation. This layer, called Neural Bag-of-Features,
can be combined with any other layer or classiﬁer, such as
fully connected layers or feature transformation layers, to
form deep neural networks for time-series classiﬁcation. It
is demonstrated, using two time-series datasets, including a
large-scale ﬁnancial dataset, that the proposed approach can
signiﬁcantly increase the classiﬁcation metrics, as well as,
reduce the size of the extracted representation compared to
the regular BoF model.
The rest of the paper is structured as follows. In Section 2
the related work is introduced and compared to the proposed
Neural BoF model. The proposed method is presented in detail
in Section 3. Next, the Neural BoF is evaluated using two
different datasets (Section 4). Finally, conclusions are drawn
in Section 5.
II. RELATED WORK
This work concerns mainly dictionary learning for the BoF
model, as well as time-series representation and classiﬁcation
using the BoF model.
There is a rich literature on dictionary learning approaches
for the BoF model. In , multiple maximum margin hyperplanes are learned and at the same time the codebooks are
adjusted to maximize the corresponding margins. A simple
method for supervised codebook learning that incorporates
both a traditional MLP layer and a codebook layer is proposed in , while a full neural formulation is presented in
 . In , the optimization aims to minimize the logistic
regression loss, while the representation is optimized using
an LDA-based criterion in . In , multiple dictionaries
with complementary discriminative information are learned by
adjusting the weights used during the clustering process using
the predictions of a classiﬁer. However, these approaches are
oriented towards image classiﬁcation instead of time-series
classiﬁcation.
In , the BoF model is used for time-series representation
and the dictionary is optimized using a discriminative loss
function. In , a similar BoF-based approach is combined
with a retrieval-oriented loss function to optimize the representation towards retrieval. Also, in , time-series segments
of various lengths are used during the feature extraction, which
allows for handling warping, and the output of a Random
Forest classiﬁer is used to alter the codebook. To the best of
our knowledge the proposed method is the ﬁrst that allows for
the end-to-end optimization of a Neural BoF model towards
time-series classiﬁcation.
III. PROPOSED METHOD
The proposed method for classifying time-series data using the Neural BoF model (also abbreviated as N-BoF) is
presented in this Section. First, a feature extractor is used to
extract multiple feature vectors from each time-series. Then,
these features are fed to the Neural BoF layer that extracts
a constant-length representation for each time-series. This
representation is subsequently fed to the used classiﬁer. Endto-end-training is used to learn all the parameters of the
proposed uniﬁed architecture.
Several options exist for the feature extraction step depending on the nature of the time-series. For example, each raw
multi-dimensional point of the time-series can be considered
as a separate feature vector, e.g., the raw multi-channel measurements from an EEG device can be used. On the other hand,
hand-crafted features can be also utilized. For example, if limit
order book data are used, then the feature extraction procedure
proposed in , can be used to extract a 144-dimensional
feature vector from each block of 100 order book records. It
should be also noted that it is straightforward to combine the
proposed approach with trainable recurrent feature extractors,
such as Gated Recurrent Units (GRUs) , to further increase
the ability of the proposed method to capture complex dynamic
phenomena. In this work, all the extracted feature vectors
were normalized to have unit variance and zero mean (z-score
standardization).
After the feature extraction the i-th time-series is represented by a set of Ni feature vectors xij ∈RD (j = 1...Ni),
where D is the dimensionality of the extracted features. Note
that the length of the extracted histogram vector does not
depend on the number of available feature vectors, which
allows the network to handle time-series of any length without
any modiﬁcation.
The BoF model is formulated as a neural layer that is
composed of two sublayers, i.e., an RBF layer that measures
the similarity of the input features to the RBF centers and
an accumulation layer that builds the histogram of the quantized feature vectors. The proposed Neural BoF layer can be
thought as a uniﬁed processing layer that feeds the extracted
representation to a subsequent classiﬁer.
The output of the k-th RBF neuron is deﬁned as:
[φ(xij)]k = exp(−||(xij −vk) ⊙wk||2)
where ⊙is the element-wise multiplication operator. The RBF
neurons behave somewhat like the codewords in the BoF
model, i.e., they are used to measure the similarity of the input
vectors to a set of predeﬁned vectors. Each RBF neuron is also
equipped with a weight vector wk ∈RD that adjusts the width
of its Gaussian function per each dimension. That allows for
better modeling of the input distribution, since the distribution
modeled by each RBF can be independently adjusted. Also,
note that by zeroing some of the input weights the length
of each codeword can be reduced. The number of the RBF
neurons used is denoted by NK. The size of the extracted
representation can be adjusted by using a different number of
RBF neurons (NK) in the Neural BoF layer.
To ensure that the output of each RBF neuron is bounded,
a normalized RBF architecture is used. This normalization is
equivalent to the l1 scaling that is utilized in the BoF model
2017 25th European Signal Processing Conference (EUSIPCO)
ISBN 978-0-9928626-7-1 © EURASIP 2017
that uses soft-assignments . Thus, the output of the RBF
neurons is re-deﬁned as:
[φ(xij)]k =
exp(−||(xij −vk) ⊙wk||2)
m=1 exp(−||(xij −vm) ⊙wm||2)
The output of the RBF neurons is accumulated in the next
layer, compiling the ﬁnal representation of each time-series:
where φ(xij) = ([φ(xij)]1, ..., [φ(xij)]NK)T ∈RNK is the
output vector of the RBF layer. Note that each si has unit l1
norm, deﬁnes a histogram distribution over the RBF neurons
and describes the dynamics of each time-series.
The Neural BoF layer receives the feature vectors of a timeseries and compiles its histogram representation. Then, this
histogram must be fed to a classiﬁer that decides the class of
the time-series. In this work a multilayer perceptron (MLP)
with one hidden layer is used for this purpose, although any
other classiﬁer with differentiable loss function can be used.
Let ti be the label of the i-the training time-series sample
and NC be the number of the different labels. Also, let WH ∈
RNH×NK be the hidden layer weights and WO ∈RNC×NH
be the output layer weights, where NH is the number of
hidden neurons. Then, the hidden layer activations for the
input histogram si of the i-th time-series are computed as
hi = φ(elu)(WHsi + bH) ∈RNH, where φ(elu)(x) is the elu
activation function :
φ(elu)(x) =
αelu(exp(x) −1),
which is applied element-wise and bH ∈RNH is the hidden
layer bias vector. For all the conducted experiments, a hidden
layer with 512 neurons was used. Similarly, the output of the
MLP is calculated as:
yi = φ(softmax)(WOhi + bO) ∈RNC
where each output neuron corresponds to a label (the one-vsall strategy is used), bO ∈RNC is the output layer bias vector
and φ(softmax) is the softmax activation function.
The categorical cross entropy loss is used for training the
[ti]j log([yi]j)
where ti ∈RNC is the target output vector, which depends
on the label (ti) of the input time-series and it is deﬁned as:
[ti]j = 1, if j = ti, or [ti]j = 0, otherwise. All the layers of
the proposed network can be trained using back-propagation
and gradient descent:
∆(WMLP , vk, wk) = −(ηMLP
where the notation WMLP is used to refer to the parameters
of the classiﬁcation layer. Instead of using simple gradient descent, a recently proposed method for stochastic optimization,
the Adam (Adaptive Moment Estimation) algorithm , is
utilized for learning the parameters of the network . Note that
if trainable recurrent feature extractors are used, the gradients
can back-propagate to them, further ﬁne-tuning the feature
extraction process towards the classiﬁcation task at hand. The
learning rates used for the conducted experiments were set
to ηMLP = ηV = 0.001 and ηW = 0.01 and the default
hyper-parameters were used for the Adam algorithm .
The optimization ran for 5000 iterations using batch size of
32 samples (1000 iterations were used for the EEG dataset).
Also, to avoid back-propagating gradients from a randomly
initialized MLP to the Neural BoF layer, the MLP was pretrained for the ﬁrst 500 iterations (100 iterations for the EEG
The centers of the RBF neurons can be either randomly
chosen or initialized using the k-means algorithm over the set
of all feature vectors S = {xij|i = 1...N, j = 1...Ni}. In
this work, the set S is clustered into NK clusters and the
corresponding centroids (codewords) vk ∈RD(k = 1...NK)
are used to initialize the centers of the RBF neurons. The same
approach is used in the BoF model to learn the codebook that
is used to quantize the feature vectors. However, in contrast
to the BoF model, the Neural BoF model uses this process
only to initialize the centers. The RBF weight vectors are also
initialized to 1/g, where g is a small positive number ranging
from 0.1 to 10. The value of g is selected using a validation
set (by splitting the training data into a new training set and
validation set). Both the RBF centers (vk) and the RBF input
weights (wk) are learned using back-propagation.
IV. EXPERIMENTS
We evaluated the proposed Neural BoF model in two
time series problems, those of high-frequency stock market
trading and EEG signal classiﬁcation. Information related to
the datasets and experimental protocols used are provided in
the following, along with experimental results comparing the
proposed model with existing ones.
A. High-frequency stock market trading
In our ﬁrst set of experiments, we employed a new largescale dataset, called FI-2010 in this paper, consisting of limit
order book data collected from 5 Finnish stocks. The dataset is
provided by Nasdaq Nordic , . A limit order in equity
markets is an order to buy (sell) a speciﬁed about of shares
at a speciﬁed bid price (ask price) or better. For each timestep the prices and volumes of limit orders at the 10 best bid
and 10 best ask levels are collected. After collecting data for
10 business days and
pre-processing and cleaning them (e.g., removing unnecessary
event messages) the handcrafted features described in 
were extracted. The total number of the collected limit order
events is 4.5 million, leading to 453,975 144-dimensional
extracted feature vectors 
ISBN 978-0-9928626-7-1 © EURASIP 2017
FI-2010: ANCHORED EVALUATION FOR PREDICTING THE MID-PRICE DIRECTION FOR THE NEXT 10, 50 AND 100 TIMESTEPS.
Predict Target
57.59 ± 7.34
39.26 ± 0.94
51.44 ± 2.53
36.28 ± 2.85
0.1182 ± 0.0246
62.70 ± 6.73
42.28 ± 0.87
61.41 ± 3.68
41.63 ± 1.90
0.1724 ± 0.0212
50.21 ± 5.59
42.56 ± 1.26
49.57 ± 2.28
39.56 ± 2.36
0.1576 ± 0.0254
56.52 ± 8.67
47.20 ± 1.80
58.17 ± 2.61
46.15 ± 4.07
0.2285 ± 0.0419
50.97 ± 5.62
42.89 ± 1.46
47.84 ± 2.08
40.84 ± 2.78
0.1641 ± 0.0300
56.43 ± 6.86
47.27 ± 1.72
54.99 ± 2.19
46.86 ± 2.87
0.2300 ± 0.0338
100 limit orders). Performance evaluation was performed as
follows. For every time-step and given the last 15 feature
vectors, we predict the direction (up, stationary or down) of
the mean mid price, i.e., the average of the highest bid and
the lowest ask prices, after k time steps. The mid prices were
ﬁltered using a moving average ﬁlter with window size 9. The
prediction targets were set for the next 10, 50 and 100 timesteps, which correspond to the next 1, 5 and 10 feature vectors.
For each of them, the threshold for considering the stock
stationary was set to 0.01%, 0.02%, and 0.03% respectively
(as the prediction horizon increases, the changes in the midprice are becoming larger). Finally, an anchored evaluation
setup was used , i.e., the ﬁrst day was used for training
the model and the next for testing, then the ﬁrst two days were
used for training the model and the next for testing, etc. The
mean and the standard deviation of the evaluated metrics are
reported for the 9 anchored evaluation splits.
Note that the FI-2010 dataset is highly unbalanced (most of
the time the mid-price remains stationary). Therefore, during
the training the data from the less frequent classes were fed
to network with an increased to probability to ensure that
each class is equally represented. Also, the accuracy, the
average precision per class (macro precision), the average
recall per class (macro recall), the average F1 per class
(macro F1) , and the Cohen’s κ metric were used
to evaluate the performance of the algorithms. The accuracy
measures the percentage of the predicated labels that exactly
match the ground truth. The precision is deﬁned as the ratio
of the true positives over the sum of the true positives and
the false positives, while the recall as the ratio of the true
positives over the sum of the true positives and the false
negatives. The F1 score is deﬁned as the harmonic mean of the
precision and the recall. The Cohen’s κ is a metric that allows
us to evaluate the agreement between two different sets of
annotations, taking into account the expected agreement when
a random classiﬁer is used. Note that the F1 and the Cohen’s
κ allow for evaluating the performance of algorithms when
unbalanced datasets, like the FI-2010, are used. For all the
evaluated metrics, higher values indicate better performance
of the corresponding aspect of the evaluated method.
In Figure 1, the effect of varying the number of RBF
neurons on the Cohen’s κ metric is evaluated using a validation
set . The proposed Neural BoF model outperforms
the regular BoF model for any number of codewords, allowing
Fig. 1. Effect of varying the number of codewords / RBF neurons (NK) on
the Cohen’s κ metric 
us to use signiﬁcantly smaller representations while increasing
the discriminative abilities of the classiﬁer. In Table 1, the
regular BoF method is compared to the proposed Neural
BoF for different prediction targets. For the regular BoF
method 128 codewords are used, while for the Neural BoF
16 RBF neurons are utilized. Using the proposed Neural BoF
method signiﬁcantly increases all the evaluation metrics, while
decreasing the size of the extracted representation.
B. EEG signal classiﬁcation
In our second experiment, we employed the EEG Database
 , consisting of EEG measurements (64 channels) sampled
at 256Hz for 1 seconds. The data were collected from two
groups, i.e., alcoholics and control. The provided training set
consists of 10 alcoholic subjects and 10 control subjects, while
the test set consists of 10 out-of-sample runs of the same
subjects. More details regarding the data collection protocol
can be found in .
In this dataset, we compare the proposed approach to other
baselines and state-of-the-art methods. The results are shown
in Table II. Only the classiﬁcation accuracy and the κ metric
are reported since the dataset is balanced and the classiﬁcation
problem binary. The proposed method is compared to a simple
MLP with 512 hidden units that receives an input of 256×64.
The method was also compared to a GRU model with 256
hidden units, that is among the state-of-the-art techniques for
2017 25th European Signal Processing Conference (EUSIPCO)
ISBN 978-0-9928626-7-1 © EURASIP 2017
EEG DATABASE EVALUATION
Class. Accuracy
time-series classiﬁcation. For both models the tanh activation
function was used for the hidden layer and they were trained
using the categorical cross-entropy loss. Both the BoF and the
Neural BoF model use NK = 8 codewords. As before, the
proposed Neural BoF model signiﬁcantly outperforms all the
other evaluated methods.
V. CONCLUSION
In this paper the BoF model was generalized and formulated
as a neural layer composed of an RBF layer and an accumulation layer that can be used for classifying time-series data.
The proposed method can be combined with any other shallow
or deep classiﬁer to form powerful time-series classiﬁcation
machines. Two different datasets were used to evaluate the
proposed method. It was demonstrated that the proposed
method can greatly increase the classiﬁcation accuracy, while
reducing the size of the extracted representations compared to
the regular BoF model.
The proposed method can be combined with trainable recurrent feature extractors, such as GRUs , to more accurately
model the dynamics of time-series. Also, two different sets
of RBF neurons can be used to model the short-term and
the long-term behavior of the time-series. Preliminary experiments show that this can further increase the classiﬁcation
metrics, especially for the FI-2010 dataset, where both the
long-term and the short term behavior of the time-series is
important. Finally, unsupervised manifold-based optimization,
e.g,. , can be used to learn representatons optimized toward
exploratory tasks, such as clustering.
ACKNOWLEDGMENT
The research leading to these results has received funding
from the H2020 Project BigDataFinance MSCA-ITN-ETN
675044 ( Training for Big Data in Financial Research and Risk Management. Alexandros Iosiﬁdis
was supported from the Academy of Finland Postdoctoral Research Fellowship (No. 295854). He joined Aarhus University
on August 2017.