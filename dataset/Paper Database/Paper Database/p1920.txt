The Annals of Statistics
2011, Vol. 39, No. 5, 2302â€“2329
DOI: 10.1214/11-AOS894
Â© Institute of Mathematical Statistics, 2011
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES FOR
NOISY LOW-RANK MATRIX COMPLETION
BY VLADIMIR KOLTCHINSKII1, KARIM LOUNICI2 AND
ALEXANDRE B. TSYBAKOV3
Georgia Institute of Technology, Georgia Institute of Technology and CREST
This paper deals with the trace regression model where n entries or linear combinations of entries of an unknown m1 Ã— m2 matrix A0 corrupted by
noise are observed. We propose a new nuclear-norm penalized estimator of
A0 and establish a general sharp oracle inequality for this estimator for arbitrary values of n,m1,m2 under the condition of isometry in expectation. Then
this method is applied to the matrix completion problem. In this case, the estimator admits a simple explicit form, and we prove that it satisï¬es oracle
inequalities with faster rates of convergence than in the previous works. They
are valid, in particular, in the high-dimensional setting m1m2 â‰«n. We show
that the obtained rates are optimal up to logarithmic factors in a minimax
sense and also derive, for any ï¬xed matrix A0, a nonminimax lower bound
on the rate of convergence of our estimator, which coincides with the upper
bound up to a constant factor. Finally, we show that our procedure provides an
exact recovery of the rank of A0 with probability close to 1. We also discuss
the statistical learning setting where there is no underlying model determined
by A0, and the aim is to ï¬nd the best trace regression model approximating the data. As a by-product, we show that, under the restricted eigenvalue
condition, the usual vector Lasso estimator satisï¬es a sharp oracle inequality
(i.e., an oracle inequality with leading constant 1).
1. Introduction.
Assume that we observe n independent random pairs
(Xi,Yi),i = 1,...,n, where Xi are random matrices with dimensions m1 Ã— m2,
and Yi are random variables in R, satisfying the trace regression model
E(Yi|Xi) = tr(XâŠ¤
i = 1,...,n,
where A0 âˆˆRm1Ã—m2 is an unknown matrix, E(Yi|Xi) is the conditional expectation
of Yi given Xi and tr(B) denotes the trace of matrix B. We consider the problem
of estimation of A0 based on the observations (Xi,Yi),i = 1,...,n. Though the
results of this paper are obtained for general n,m1,m2, the main motivation is
Received November 2010; revised March 2011.
1Supported in part by NSF Grants DMS-09-06880 and CCF-0808863.
2Supported in part by NSF Grant DMS-11-06644 and Simons foundation Grant 209842.
3Supported in part by ANR â€œParcimonieâ€ and by PASCAL-2 Network of Excellence.
MSC2010 subject classiï¬cations. Primary 62J99, 62H12; secondary 60B20, 60G15.
Key words and phrases. Matrix completion, low-rank matrix estimation, recovery of the rank, statistical learning, optimal rate of convergence, noncommutative Bernstein inequality, Lasso.
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
in the high-dimensional setting, which corresponds to m1m2 â‰«n, with low-rank
matrices A0.
It will be convenient to write model (1.1) in the form
Yi = tr(XâŠ¤
i A0) + Î¾i,
i = 1,...,n,
where the noise variables Î¾i = Yi âˆ’E(Yi|Xi) are independent and have zero means.
For any matrices A,B âˆˆRm1Ã—m2, we deï¬ne the scalar product
âŸ¨A,BâŸ©= tr(AâŠ¤B)
and the bilinear form
âŸ¨A,BâŸ©L2() = 1
E(âŸ¨A,XiâŸ©âŸ¨B,XiâŸ©).
Here  = 1
i=1 i, where i denotes the distribution of Xi. The corresponding
semi-norm âˆ¥Aâˆ¥L2() is given by
E(âŸ¨A,XiâŸ©2).
EXAMPLE 1 (Matrix completion).
Assume that the design matrices Xi are
i.i.d. uniformly distributed on the set
X = {ej(m1)eâŠ¤
k (m2),1 â‰¤j â‰¤m1,1 â‰¤k â‰¤m2},
where ek(m) are the canonical basis vectors in Rm. The set X forms an orthonormal basis in the space of m1Ã—m2 matrices that will be called the matrix completion
basis. Let also n < m1m2. Then the problem of estimation of A0 coincides with
the problem of matrix completion under uniform sampling at random (USR) as
studied in the nonnoisy case (Î¾i = 0) in , and in the noisy case in .
Considering low-rank matrices A0 is of a particular interest. Clearly, for such Xi
we have the isometry
L2() = Î¼âˆ’2âˆ¥Aâˆ¥2
for all matrices A âˆˆRm1Ã—m2, where Î¼ = âˆšm1m2, and âˆ¥Aâˆ¥2 is the Frobenius norm
of A. However, the restricted isometry property in the usual sense, that is, â€œin
probabilityâ€ (cf., e.g., ) does not hold for matrix completion, since for n <
m1m2 there trivially exists a matrix of rank 1 in the null space of the sampling
One can also consider more general matrix measurement models in which, for
a given orthonormal basis in the space of matrices, a random sample of Fourier
coefï¬cients of the target matrix A0 is observed subject to a random noise. For
more discussion on matrix completion with other types of sampling, see and references therein.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
EXAMPLE 2 (Column masks).
Assume that the design matrices Xi are i.i.d.
replications of a random matrix X, which has only one nonzero column. For instance, let the distribution of X be such that all the columns have equal probability to be nonzero, and the random entries of nonzero column x(j) are such that
(j)) is the identity matrix. Then âˆ¥Aâˆ¥2
L2() = âˆ¥Aâˆ¥2
2/m2,âˆ€A âˆˆRm1Ã—m2, so
that condition (1.4) is satisï¬ed with Î¼ = âˆšm2. More generally, in view of application to multi-task learning (cf. ) one can be interested in considering nonidentically distributed Xi. The model can be then reformulated as a longitudinal regression model, with different distributions of Xi corresponding to different tasks.
EXAMPLE 3 (â€œCompleteâ€ sub-Gaussian design).
Assume that the design matrices Xi are i.i.d. replications of a random matrix X such that âŸ¨A,XâŸ©is a sub-
Gaussian random variable for any A âˆˆRm1Ã—m2. This approach has its roots in
compressed sensing. The two major examples are given by the matrices X whose
entries are either i.i.d. standard Gaussian or Rademacher random variables. In both
cases, we have âˆ¥Aâˆ¥2
L2() = âˆ¥Aâˆ¥2
2,âˆ€A âˆˆRm1Ã—m2, so that condition (1.4) is satis-
ï¬ed with Î¼ = 1. The problem of exact reconstruction of A0 under such a design
in the nonnoisy setting was studied in , whereas estimation of A0 in
the presence of noise is analyzed in , among which treat the
high-dimensional case m1m2 > n.
EXAMPLE 4 (Fixed design).
Assume that all the i are Dirac measures, so
that the design matrices Xi are nonrandom. Then âˆ¥Aâˆ¥2
i=1âŸ¨A,XiâŸ©2,
and we get the problem of trace regression with ï¬xed design; cf. . In particular,
if m1 = m2, and A and Xi are diagonal matrices the trace regression model (1.2)
becomes the usual linear regression model. Accordingly, the rank of A becomes the
number of its nonzero diagonal elements. This observation will allow us to deduce,
as a consequence of our general argument, an oracle inequality for the usual Lasso
in sparse linear regression with ï¬xed design improving in the sense that the
inequality is sharp; cf. Theorem 2 and Section 5.4.
The general oracle inequalities that we will prove in Section 2 can be successfully applied to the above examples. The emphasis in this paper will be on the
matrix completion problem (Example 1), for which the previously obtained results
were suboptimal.
Statistical estimation of low-rank matrices has recently become a very active
ï¬eld with a rapidly growing literature. The most popular methods are based on
penalized empirical risk minimization with nuclear-norm penalty . Estimators with other types of penalization, such as the Schatten-p
norm , the von Neumann entropy , penalization by the rank or some
combined penalties are also discussed.
It is worth pointing out that in many applications, such as in matrix completion,
the distribution  is known, and yet this information has not been exploited since
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
the penalized estimation procedures considered in the literature involve the empirical risk 1
i=1(Yi âˆ’tr(XâŠ¤
i A))2. In this paper we incorporate the knowledge of
 in the construction and we study the following estimator of A0:
Ë†AÎ» = argmin
where A âŠ†Rm1Ã—m2 is a set of matrices,
Ln(A) = âˆ¥Aâˆ¥2
Î» > 0 is a regularization parameter and âˆ¥Aâˆ¥1 is the nuclear norm of A. We will
mainly consider convex sets A. Note that if all Xi are nonrandom, Ë†AÎ» coincides
with the usual matrix Lasso estimator,
Ë†AÎ» = argmin
(Yj âˆ’âŸ¨A,XjâŸ©)2 + Î»âˆ¥Aâˆ¥1
The emphasis in this paper is on the noisy matrix completion setting. Then
the estimator Ë†AÎ» has a particularly simple form; it is obtained from the matrix
i=1 YiXi by soft thresholding of its singular values. One of the main results of this paper is to show that our estimators are rate optimal (up to logarithmic
factors) under the Frobenius error for a simple class of matrices A(r,a) deï¬ned
by two restrictions: the rank of A0 is not larger than given r, and all the entries of
A0 are bounded in absolute value by a constant a. This rather intuitive class has
been ï¬rst considered in . However, the construction of the estimator in 
requires the exact knowledge of rank(A0) and the upper bound on the Frobenius
error obtained in is suboptimal (see the details in Section 3). The recent paper
 obtains suboptimal bounds of â€œslow rateâ€ type for matrix completion while
 focuses on complex-valued Hermitian matrices with nuclear norm equal to 1,
which is motivated by density matrix estimation problem in quantum state tomography. These papers do not address the optimality issue. Optimal rates in noisy
matrix completion are derived in , but on different classes of matrices and
with the empirical prediction error rather than with the Frobenius error. Finally,
 discusses the optimality issue for the Frobenius error on the classes deï¬ned in
terms of a â€œspikiness indexâ€ of A0, which are not related to A(r,a), and suggests
estimators that require prior knowledge about this index.
The main contributions of this paper are the following. In Section 2 we derive
a general oracle inequality for the prediction error âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2(). This oracle
inequality is sharp, that is, with leading constant 1, both in the case of â€œslow rateâ€
(for matrices A0 with small nuclear norm) and in the case of â€œfast rateâ€ (for matrices A0 with small rank). As a particular instance of this general result, in Section 3
we obtain an oracle inequality for the matrix completion problem. In Section 4, we
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
establish minimax lower bounds showing that the rates for matrix completion obtained in Section 3 are optimal up to a logarithmic factor. In Section 5, we brieï¬‚y
discuss some other implications and extensions of our method. Finally, Section 6
is devoted to the control of the stochastic term appearing in the proof of the upper
2. General oracle inequalities.
We recall ï¬rst some basic facts about matrices. Let A âˆˆRm1Ã—m2 be a rectangular matrix, and let r = rank(A) â‰¤min(m1,m2)
denote its rank. The singular value decomposition (SVD) of A has the form
j=1 Ïƒj(A)ujvâŠ¤
j with orthonormal vectors u1,...,ur âˆˆRm1, orthonormal
vectors v1,...,vr âˆˆRm2 and real numbers Ïƒ1(A) â‰¥Â·Â·Â· â‰¥Ïƒr(A) > 0 (the singular
values of A). The pair of linear vector spaces (S1,S2) where S1 is the linear span
of {u1,...,ur}, and S2 is the linear span of {v1,...,vr}, will be called the support
of A. We will denote by SâŠ¥
j the orthogonal complements of Sj, j = 1,2, and by
PS the projector on the linear vector subspace S of Rmj , j = 1,2.
The Schatten-p (quasi-)norm âˆ¥Aâˆ¥p of matrix A is deï¬ned by
min(m1,m2)
for 0 < p < âˆ
âˆ¥Aâˆ¥âˆ= Ïƒ1(A).
Recall the well-known trace duality property,
|tr(AâŠ¤B)| â‰¤âˆ¥Aâˆ¥1âˆ¥Bâˆ¥âˆ
âˆ€A,B âˆˆRm1Ã—m2.
We will also use the fact that the subdifferential of the convex function A â†’âˆ¥Aâˆ¥1
is the following set of matrices:
cf. . Deï¬ne the random matrix
YiXi âˆ’E(YiXi)
We will need the following assumption on the distribution of the matrices Xi.
ASSUMPTION 1.
There exists a constant Î¼ > 0 such that, for all matrices
A âˆˆA âˆ’A := {A1 âˆ’A2 :A1,A2 âˆˆA},
L2() â‰¥Î¼âˆ’2âˆ¥Aâˆ¥2
As discussed in the Introduction, Assumption 1 is satisï¬ed, often with equality
and for A = A âˆ’A = Rm1Ã—m2, in several interesting examples. The next theorem
plays the key role in what follows.
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
THEOREM 1.
Let A âŠ†Rm1Ã—m2 be any set of matrices. If Î» â‰¥2âˆ¥Mâˆ¥âˆ, then
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() â‰¤inf
L2() + 2Î»âˆ¥Aâˆ¥1
If, in addition, A is a convex set, and Assumption 1 is satisï¬ed, then
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() â‰¤inf
Î¼2Î»2 rank(A)
Furthermore, in this case, for all A âˆˆA with support (S1,S2),
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() + (Î» âˆ’2âˆ¥Mâˆ¥âˆ)âˆ¥PSâŠ¥
Î¼2Î»2 rank(A).
It follows from the deï¬nition of the estimator Ë†A that, for all A âˆˆA,
Ln( Ë†AÎ») = âˆ¥Ë†AÎ»âˆ¥2
+ Î»âˆ¥Aâˆ¥1 = Ln(A).
Also, note that
E(YiXi) = 1
E(âŸ¨A0,XiâŸ©Xi)
âŸ¨E(YiXi),AâŸ©= âŸ¨A0,AâŸ©L2().
Therefore, we have
L2() âˆ’2âŸ¨Ë†AÎ»,A0âŸ©L2()
L2() âˆ’2âŸ¨A,A0âŸ©L2() +
YiXi âˆ’E(YiXi)
+ Î»(âˆ¥Aâˆ¥1 âˆ’âˆ¥Ë†AÎ»âˆ¥1),
which implies, due to the trace duality,
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() â‰¤âˆ¥A âˆ’A0âˆ¥2
L2() + 2âˆ¥Ë†AÎ» âˆ’Aâˆ¥1 + Î»(âˆ¥Aâˆ¥1 âˆ’âˆ¥Ë†AÎ»âˆ¥1),
where we set for brevity  = âˆ¥Mâˆ¥âˆ. Under the assumption Î» â‰¥2 this yields
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() â‰¤âˆ¥A âˆ’A0âˆ¥2
L2() + Î»(âˆ¥Ë†AÎ» âˆ’Aâˆ¥1 + âˆ¥Aâˆ¥1 âˆ’âˆ¥Ë†AÎ»âˆ¥1)
L2() + 2Î»âˆ¥Aâˆ¥1,
and the bound (2.3) follows.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
To prove the remaining bounds, note that a necessary condition of extremum in
problem (1.5) implies that there exists Ë†V âˆˆâˆ‚âˆ¥Ë†AÎ»âˆ¥1 such that, for all A âˆˆA,
2âŸ¨Ë†AÎ», Ë†AÎ» âˆ’AâŸ©L2() âˆ’
YiXi, Ë†AÎ» âˆ’A
+ Î»âŸ¨Ë†V , Ë†AÎ» âˆ’AâŸ©â‰¤0.
Indeed, since Ë†AÎ» is a minimizer of Ln(A) in A, there exists a matrix B âˆˆâˆ‚Ln( Ë†AÎ»)
such that âˆ’B belongs to the normal cone of A at the point Ë†AÎ»; cf. , Chapter 4,
Section 2, Corollary 6. It is easy to see that B can be represented as follows:
Rm1Ã—m2âŸ¨Ë†AÎ»,XâŸ©X(dX) âˆ’2
YiXi + Î» Ë†V ,
where Ë†V âˆˆâˆ‚âˆ¥Ë†AÎ»âˆ¥1. The condition that âˆ’B belongs to the normal cone at the point
Ë†AÎ» implies that âŸ¨B, Ë†AÎ» âˆ’AâŸ©â‰¤0, and (2.6) follows.
Consider an arbitrary A âˆˆA of rank r with spectral representation A =
j=1 ÏƒjujvâŠ¤
j and with support (S1,S2). It follows from (2.6) that
2âŸ¨Ë†AÎ» âˆ’A0, Ë†AÎ» âˆ’AâŸ©L2() + Î»âŸ¨Ë†V âˆ’V, Ë†AÎ» âˆ’AâŸ©
â‰¤âˆ’Î»âŸ¨V, Ë†AÎ» âˆ’AâŸ©+ 2âŸ¨M, Ë†AÎ» âˆ’AâŸ©
for an arbitrary V âˆˆâˆ‚âˆ¥Aâˆ¥1. By monotonicity of subdifferentials of convex functions, âŸ¨Ë†V âˆ’V, Ë†AÎ» âˆ’AâŸ©â‰¥0. On the other hand, by (2.1), the following representation holds:
where W is an arbitrary matrix with âˆ¥Wâˆ¥âˆâ‰¤1. It follows from the trace duality
that there exists W with âˆ¥Wâˆ¥âˆâ‰¤1 such that
2 , Ë†AÎ» âˆ’AâŸ©= âŸ¨PSâŠ¥
2 , Ë†AÎ»âŸ©= âŸ¨W,PSâŠ¥
where in the ï¬rst equality we used that A has the support (S1,S2). For this particular choice of W, (2.7) implies that
2âŸ¨Ë†AÎ» âˆ’A0, Ë†AÎ» âˆ’AâŸ©L2() + Î»âˆ¥PSâŠ¥
j , Ë†AÎ» âˆ’A
+ 2âŸ¨M, Ë†AÎ» âˆ’AâŸ©.
Using the identity
2âŸ¨Ë†AÎ» âˆ’A0, Ë†AÎ» âˆ’AâŸ©L2() = âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() + âˆ¥Ë†AÎ» âˆ’Aâˆ¥2
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
and the facts that
j , Ë†AÎ» âˆ’A
j ,PS1( Ë†AÎ» âˆ’A)PS2
we deduce from (2.8) that
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() + âˆ¥Ë†AÎ» âˆ’Aâˆ¥2
L2() + Î»âˆ¥PSâŠ¥
L2() + Î»âˆ¥PS1( Ë†AÎ» âˆ’A)PS2âˆ¥1 + 2âŸ¨M, Ë†AÎ» âˆ’AâŸ©.
To provide an upper bound on 2âŸ¨M, Ë†AÎ» âˆ’AâŸ©we use the following decomposition:
âŸ¨M, Ë†AÎ» âˆ’AâŸ©= âŸ¨PA(M), Ë†AÎ» âˆ’AâŸ©+ âŸ¨PSâŠ¥
2 , Ë†AÎ» âˆ’AâŸ©
= âŸ¨PA(M),PA( Ë†AÎ» âˆ’A)âŸ©+ âŸ¨PSâŠ¥
where PA(M) = M âˆ’PSâŠ¥
2 . This implies, due to the trace duality,
2|âŸ¨M, Ë†AÎ» âˆ’AâŸ©| â‰¤âˆ¥PA( Ë†AÎ» âˆ’A)âˆ¥2 + âˆ¥PSâŠ¥
â‰¤âˆ¥Ë†AÎ» âˆ’Aâˆ¥2 + âˆ¥PSâŠ¥
 = 2âˆ¥PA(M)âˆ¥2,
â‰¤2âˆ¥Mâˆ¥âˆ= 2.
PA(M) = PSâŠ¥
1 MPS2 + PS1M
and rank(PSj ) â‰¤rank(A), j = 1,2, we have
rank(PA(M))âˆ¥Mâˆ¥âˆâ‰¤2
2rank(A) â‰¤
2rank(A)Î».
Due to the fact that
âˆ¥PS1( Ë†AÎ» âˆ’A)PS2âˆ¥1 â‰¤
rank(A)âˆ¥PS1( Ë†AÎ» âˆ’A)PS2âˆ¥2
rank(A)âˆ¥Ë†AÎ» âˆ’Aâˆ¥2
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
and to Assumption 1, it follows from (2.11) and (2.12) that
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() + âˆ¥Ë†AÎ» âˆ’Aâˆ¥2
L2() + Î»âˆ¥PSâŠ¥
rank(A) + 
âˆ¥Ë†AÎ» âˆ’Aâˆ¥L2()
Using the above bounds on  and , we obtain from (2.17) that
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() + âˆ¥Ë†AÎ» âˆ’Aâˆ¥2
L2() + (Î» âˆ’2)âˆ¥PSâŠ¥
rank(A)âˆ¥Ë†AÎ» âˆ’Aâˆ¥L2(),
which implies
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() + (Î» âˆ’2)âˆ¥PSâŠ¥
2Î¼2Î»2 rank(A).
The following immediate corollary of Theorem 1 provides a bound for the
Frobenius error.
COROLLARY 1.
Let A be a convex subset of m1 Ã—m2 matrices containing A0,
and let Assumption 1 be satisï¬ed. If Î» â‰¥2âˆ¥Mâˆ¥âˆ, then
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
2 â‰¤Î»Î¼2 min
Î»Î¼2 rank(A0)
Next, we consider a version of Theorem 1 under weaker assumptions which
are akin to the restricted eigenvalue condition in sparse estimation of vectors. For
simplicity, we will do it only when the domain A of minimization in (1.5) is a
linear subspace of Rm1Ã—m2. Recall that, given A âˆˆA with support (S1,S2), we
PA(B) := B âˆ’PSâŠ¥
A (B) := PSâŠ¥
B âˆˆRm1Ã—m2,
and, for c0 â‰¥0, deï¬ne the following cone of matrices:
CA,c0 := {B âˆˆA:âˆ¥PâŠ¥
A (B)âˆ¥1 â‰¤c0âˆ¥PA(B)âˆ¥1}.
Finally, deï¬ne
Î¼c0(A) := inf
Î¼â€² > 0:âˆ¥PA(B)âˆ¥2 â‰¤Î¼â€²âˆ¥Bâˆ¥L2(),âˆ€B âˆˆCA,c0
Note that Î¼c0(A) is a nondecreasing function of c0. For c0 = +âˆ, the quantity
Î¼âˆ(A) has a simple meaning: it is equal to the norm of the linear transformation
B â†’PA(B) from the space A equipped with the L2()-norm into the space of
all matrices equipped with the Frobenius norm. For c0 = 0, Î¼0(A) is the norm
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
of the same linear transformation restricted to the subspace of A consisting of
all matrices B âˆˆA with PâŠ¥
A (B) = 0. We are more interested in the intermediate
values, c0 âˆˆ(0,+âˆ). In this case, Î¼c0(A) is the â€œnormâ€ of the linear mapping
PA restricted to the cone of matrices B for which PA(B) is the dominant part
A (B) is â€œsmall.â€ Note that the rank of PA(B) is not larger than 2rank(A),
so, when the rank of A is small, the matrices in CA,c0 are approximately â€œlowrank.â€ The quantities of the same ï¬‚avor have been previously used in the literature on Lasso, Dantzig selector and other methods of sparse estimation of vectors.
In these problems, they can be expressed in terms of â€œrestricted eigenvaluesâ€ of
certain Gram matrices; cf. the restricted eigenvalue condition in for the ï¬xed
design case and similar distribution dependent conditions in for the random
design case. Such conditions are also considered in for the matrix case. In
what follows, we use the value c0 = 5 and set Î¼(A) := Î¼5(A).
THEOREM 2.
Let A be a linear subspace of Rm1Ã—m2. If Î» â‰¥3âˆ¥Mâˆ¥âˆ, then
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() â‰¤inf
L2() + Î»2Î¼2(A)rank(A)
Fix A âˆˆA with support (S1,S2). If âŸ¨Ë†AÎ» âˆ’A0, Ë†AÎ» âˆ’AâŸ©L2() â‰¤0, then
we trivially have âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() â‰¤âˆ¥Aâˆ’A0âˆ¥2
L2() in view of (2.9). Thus, assume
that âŸ¨Ë†AÎ» âˆ’A0, Ë†AÎ» âˆ’AâŸ©L2() > 0. In this case, (2.8) and an obvious modiï¬cation
of (2.10) imply
2 âˆ¥1 â‰¤Î»âˆ¥PA( Ë†AÎ» âˆ’A)âˆ¥1 + 2âŸ¨M, Ë†AÎ» âˆ’AâŸ©.
âŸ¨M, Ë†AÎ» âˆ’AâŸ©= âŸ¨M,PA( Ë†AÎ» âˆ’A)âŸ©+ âŸ¨M,PâŠ¥
A ( Ë†AÎ» âˆ’A)âŸ©
âˆ¥PA( Ë†AÎ» âˆ’A)âˆ¥1 + âˆ¥PâŠ¥
A ( Ë†AÎ» âˆ’A)âˆ¥1
By (2.20) and (2.21),
(Î» âˆ’2)âˆ¥PâŠ¥
A ( Ë†AÎ» âˆ’A)âˆ¥1 â‰¤(Î» + 2)âˆ¥PA( Ë†AÎ» âˆ’A)âˆ¥1.
For Î» â‰¥3, this yields
A ( Ë†AÎ» âˆ’A)âˆ¥1 â‰¤5âˆ¥PA( Ë†AÎ» âˆ’A)âˆ¥1,
which implies that Ë†AÎ» âˆ’A âˆˆCA,5, and thus âˆ¥PA( Ë†AÎ» âˆ’A)âˆ¥2 â‰¤Î¼(A)âˆ¥Ë†AÎ» âˆ’
Aâˆ¥L2(). Combining this inequality with (2.11), (2.12), (2.13), (2.14) and using
that Î» â‰¥3, after some algebra we get
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
L2() + âˆ¥Ë†AÎ» âˆ’Aâˆ¥2
L2() + (Î»/3)âˆ¥PSâŠ¥
rank(A)âˆ¥Ë†AÎ» âˆ’Aâˆ¥L2()
L2() + âˆ¥Ë†AÎ» âˆ’Aâˆ¥2
L2() + Î¼2(A)Î»2 rank(A).
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
As a simple example, consider the case when m1 = m2, A is the space of all
diagonal matrices and Xi also belong to A. Then the trace regression model (1.2)
becomes the usual linear regression model. The Schatten p-norms are in this case
equivalent to the â„“p-norms with the operator norm âˆ¥Â· âˆ¥âˆbeing the â„“âˆ-norm and
the rank of matrix A characterizing the sparsity of the corresponding vector. The
problem of minimizing the functional Ln(A) over the space A is a Lasso-type penalized empirical risk minimization. In particular, it coincides with the standard
Lasso if all Xi are nonrandom. Inequalities of Theorem 1 and (2.19) become, in
this case, sparsity oracle inequalities for the Lasso-type estimators. It is noteworthy that these inequalities are sharp (i.e., with leading constant 1), which was not
achieved in the past work. The random matrix M is also diagonal and its norm
âˆ¥Mâˆ¥âˆis just the â„“âˆ-norm of the corresponding random vector, which is the sum
of independent random vectors. Hence, it is easy to provide probabilistic bounds on
âˆ¥Mâˆ¥âˆusing, for instance, the classical Bernstein inequality and the union bound.
We give an example of such an application of Theorem 2 in Section 5.4.
3. Upper bounds for matrix completion.
In this section we consider implications of the general oracle inequalities of Theorem 1 for the model of USR
matrix completion. Thus, we assume that the matrices Xi are i.i.d. uniformly
distributed in the matrix completion basis X , which implies that âˆ¥Aâˆ¥2
(m1m2)âˆ’1âˆ¥Aâˆ¥2
2 for all matrices A âˆˆRm1Ã—m2, and we set Î¼ = âˆšm1m2. The estimator Ë†AÎ» is then deï¬ned by (here and further on we set A = Rm1Ã—m2 in the case
of matrix completion)
Ë†AÎ» = argmin
2 + Î»m1m2âˆ¥Aâˆ¥1),
We can also write Ë†AÎ» explicitly
Ïƒj(X) âˆ’Î»m1m2/2
+uj(X)vj(X)âŠ¤,
where x+ = max{x,0}, Ïƒj(X) are the singular values, and uj(X),vj(X) are the
left and right singular vectors of X = rank(X)
Ïƒj(X)uj(X)vj(X)âŠ¤. Thus, Ë†AÎ» has
a particularly simple form; it is obtained by soft thresholding of singular values in
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
the SVD of X. To see why (3.2) gives the solution of (3.1), note that, in view of
(2.1), the subdifferential of F(A) = âˆ¥Aâˆ’Xâˆ¥2
2 +Î»m1m2âˆ¥Aâˆ¥1 is the set of matrices
2(A âˆ’X) + Î»m1m2
where r,uj,vj,S1,S2 correspond to the SVD of A. Since A â†’F(A) is strictly
convex, the minimizer Ë†AÎ» is unique, and the condition 0 âˆˆâˆ‚F( Ë†AÎ») is necessary
and sufï¬cient characterization of the minimum, where 0 is the zero m1 Ã— m2 matrix. Considering
j : Ïƒj(X)<Î»m1m2/2
uj(X)vj(X)âŠ¤,
it is easy to check that (3.2) satisï¬es this condition.
We will see that the soft thresholding representation (3.2) helps to understand
in an easy way some theoretical properties of Ë†AÎ». However, it may not be always
preferable for computational issues. Indeed, the standard techniques of computation of the SVD can become numerically instable when the dimension is high. On
the other hand, we can always compute Ë†AÎ» from (3.1) using the methods of convex
programming free from this drawback.
In view of Theorem 1, to get the oracle inequalities in a closed form it remains
only to specify the value of regularization parameter Î» such that Î» â‰¥2âˆ¥Mâˆ¥âˆwith
high probability. This requires some assumptions on the distribution of (Xi,Yi),
and the value of Î» will be different under different assumptions. We will consider
only the following two cases of particular interest:
â€¢ Sub-exponential noise and matrices with uniformly bounded entries. There exist
constants Ïƒ,c1 > 0, Î± â‰¥1 and Ëœc such that
i=1,...,nEexp
and maxi,j |a0(i,j)| â‰¤a for some constant a.
â€¢ Statistical learning setting. There exists a constant Î· such that maxi=1,...,n |Yi| â‰¤
Î· almost surely.
In both cases, we obtain the upper bounds for âˆ¥Mâˆ¥âˆ(that we call the stochastic
error) using the noncommutative Bernstein inequalities; cf. Section 6. The resulting values of Î» and the corresponding oracle inequalities are given in the next two
Set m = m1 + m2. In what follows, we will denote by C absolute positive constants, possibly different on different occasions.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
THEOREM 3.
Let Xi be i.i.d. uniformly distributed on X , and the pairs
(Xi,Yi) be i.i.d. Assume that maxi,j |a0(i,j)| â‰¤a for some constant a, and that
condition (3.3) holds. For t > 0, consider the regularization parameter Î» satisfying
Î» â‰¥C(Ïƒ âˆ¨a)max
t + log(m)
(m1 âˆ§m2)n, (t + log(m))log1/Î±(m1 âˆ§m2)
where C > 0 is a large enough constant that can depend only on Î±,c1, Ëœc. Then
with probability at least 1 âˆ’3eâˆ’t we have
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
2 + m1m2 min
m1m2Î»2 rank(A)
for all A âˆˆRm1Ã—m2.
THEOREM 4.
Let Xi be i.i.d. uniformly distributed on X . Assume that
maxi=1,...,n |Yi| â‰¤Î· almost surely for some constant Î·. For t > 0 consider the
regularization parameter Î» satisfying
t + log(m)
(m1 âˆ§m2)n, 2(t + log(m))
Then with probability at least 1 âˆ’eâˆ’t inequality (3.5) holds for all A âˆˆRm1Ã—m2.
Theorems 3 and 4 follow immediately from Theorem 1 and Lemmas 1, 2 and 3
in Section 6 with Î¼ = âˆšm1m2.
Note that the natural choice of t in Theorems 3 and 4 is of the order log(m),
since a larger t leads to slower rate of convergence, and a smaller t does not improve the rate but makes the concentration probability smaller. Note also that, under this choice of t, the second terms under the maxima in (3.4) and (3.6) are negligible for the values of n,m1,m2 such that the term containing rank(A0) in (3.5)
is meaningful. Indeed, if t is of the order log(m), the condition that m1m2Î»2 â‰ª1
necessarily implies n â‰«(m1 âˆ¨m2)log(m). On the other hand, the negligibility of
the second terms under the maxima in (3.4) and (3.6) is approximately equivalent
to n > (m1 âˆ§m2)log1+2/Î±(m) and n > (m1 âˆ§m2)log(m), respectively. Based on
these remarks, we can choose Î» in the form
(m1 âˆ§m2)n,
where câˆ—equals either Ïƒ âˆ¨a or Î· and the constant Câˆ—> 0 is large enough, and we
can state the following corollary that will be further useful for minimax considerations. Deï¬ne Ï„ > 0 by
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
where M = max(m1,m2), and m = m1 + m2.
COROLLARY 2.
Let one of the sets of conditions (i) or (ii) below be satisï¬ed:
(i) The assumptions of Theorem 3 with Î» as in (3.7), n > (m1 âˆ§m2) Ã—
log1+2/Î±(m), câˆ—= Ïƒ âˆ¨a, and a large enough constant Câˆ—> 0 that can depend
only on Î±,c1, Ëœc.
(ii) The assumptions of Theorem 4 with n > 4(m1 âˆ§m2)log(m), Î» as in (3.7),
câˆ—= Î·, and Câˆ—= 4.
Then, with probability at least 1 âˆ’3/(m1 + m2),
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
2 + Ï„ 2 rank(A)
and, in particular,
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
âˆ—log(m)M rank(A0)
where M = max(m1,m2), and m = m1 + m2. Furthermore, with the same probability,
âˆ¥Ë†AÎ» âˆ’A0âˆ¥2
Ï„ 2âˆ’qâˆ¥A0âˆ¥q
(m1m2)q/2 .
Inequalities (3.8) and (3.9) are straightforward in view of Theorems 3
and 4. To prove (3.10) it sufï¬ces to note that, for any Îº > 0, 0 < q â‰¤2,
2 + Îº2 rank(A)
min{Îº2,Ïƒ 2
j (A0)} = Îº2 
â‰¤Îº2âˆ’qâˆ¥A0âˆ¥q
Inequality (3.9) guarantees that the normalized Frobenius error (m1m2)âˆ’1âˆ¥Ë†AÎ»âˆ’
2 of the estimator Ë†AÎ» is small whenever n > C(m1 âˆ¨m2)log(m)rank(A0) with
a large enough C > 0. This quantiï¬es the sample size n necessary for successful
matrix completion from noisy data.
Note that we can choose Î» not necessarily equal but also greater or equal to the
right-hand side of (3.7), or equivalently, Î» = tCâˆ—câˆ—
(m1âˆ§m2)n for any t â‰¥1. Then
the resulting oracle inequalities will remain of the same form with Ï„ 2 multiplied
by the constant t2.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
Keshavan et al. ( , Theorem 1.1), under a sampling scheme different from
ours (sampling without replacement) and sub-Gaussian errors, proposed an estimator Ë†A satisfying, with probability at least 1 âˆ’(m1 âˆ§m2)âˆ’3,
Î² log(n)M rank(A0)
where C > 0 is a constant, and Î² = (m1 âˆ¨m2)/(m1 âˆ§m2) is the aspect ratio.
A drawback is that the construction of Ë†A in requires the exact knowledge of
rank(A0) (although it does not seem to require the knowledge of a). Furthermore,
bound (3.11) is suboptimal for â€œvery rectangularâ€ matrices, that is, when Î² â‰«1.
Candes and Plan provide a coarser bound than (3.11), not guaranteeing a simple consistency when n â†’âˆwhatever are M and rank(A0); see for more
detailed comments on .
4. Lower bounds.
In this section, we prove the minimax lower bounds showing that the rates attained by our estimator are optimal up to logarithmic factors.
The argument here is close to where the lower bounds are obtained on the
Schatten balls. However, we consider different classes that consist of matrices
with uniformly (in m1,m2,n) bounded entries. We cannot apply directly the lower
bounds of Theorem 6 in for USR matrix completion on the Schatten balls because they are achieved on matrices with entries, which are not uniformly bounded
for m1m2 â‰«n.
We will need the following assumption, which is similar in spirit but, in general,
substantially weaker than the usual restricted isometry condition.
ASSUMPTION 2 (Restricted isometry in expectation).
For some 1 â‰¤r â‰¤
min(m1,m2) and some 0 < Î¼ < âˆthat there exists a constant Î´r âˆˆ[0,1) such
(1 âˆ’Î´r)âˆ¥Aâˆ¥2 â‰¤Î¼âˆ¥Aâˆ¥L2() â‰¤(1 + Î´r)âˆ¥Aâˆ¥2
for all matrices A âˆˆRm1Ã—m2 with rank at most r.
For the particular case of ï¬xed Xi (cf. Example 4 in the Introduction), Assumption 2 coincides with the matrix version of scaled restricted isometry with scaling
factor Î¼ .
Inspection of the proof of Theorem 5 shows that it remains valid
if we replace 1 âˆ’Î´r and 1 + Î´r by arbitrary positive constants Î½1 and Î½2 such that
Î½1 â‰¤Î½2. We use the formulation involving Î´r only to ease parallels to the usual
restricted isometry condition.
We will denote by inf Ë†A the inï¬mum over all estimators Ë†A with values in
Rm1Ã—m2. For any integer r â‰¤min(m1,m2) and any a > 0 we consider the class
of matrices
A0 âˆˆRm1Ã—m2 :rank(A0) â‰¤r,max
i,j |a0(i,j)| â‰¤a
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
For any A âˆˆRm1Ã—m2, let PA denote the probability distribution of the observations (X1,Y1,...,Xn,Yn) with E(Yi|Xi) = âŸ¨A,XiâŸ©. We set for brevity M =
max(m1,m2).
THEOREM 5.
Fix a > 0 and an integer 1 â‰¤r â‰¤min(m1,m2). Let Assumption 2 be satisï¬ed with some Î¼ > 0. Assume that Î¼2r â‰¤nmin(m1,m2), and
that conditionally on Xi, the variables Î¾i are Gaussian N(0,Ïƒ 2), Ïƒ 2 > 0, for
i = 1,...,n. Then there exist absolute constants Î² âˆˆ(0,1) and c > 0, such that
L2() > c(1 âˆ’Î´r)2(Ïƒ âˆ§a)2 Mr
Without loss of generality, assume that M = max(m1,m2) = m1 â‰¥
m2. For some constant 0 â‰¤Î³ â‰¤1 we deï¬ne
ËœA = (aij) âˆˆRm1Ã—r :aij âˆˆ
0,Î³ (Ïƒ âˆ§a)
âˆ€1 â‰¤i â‰¤m1,1 â‰¤j â‰¤r
and consider the associated set of block matrices
B(C) = {A = ( ËœA| Â·Â·Â· | ËœA| O ) âˆˆRm1Ã—m2 : ËœA âˆˆC},
where O denotes the m1 Ã—(m2 âˆ’râŒŠm2/râŒ‹) zero matrix, and âŒŠxâŒ‹is the integer part
By construction, any element of B(C) as well as the difference of any two elements of B(C) has rank at most r and the entries of any matrix in B(C) take
values in [0,a]. Thus, B(C) âŠ‚A(r,a). Due to the Varshamovâ€“Gilbert bound (cf.
Lemma 2.9 in ), there exists a subset A0 âŠ‚B(C) with cardinality Card(A0) â‰¥
2rm1/8 +1 containing the zero m1 Ã—m2 matrix 0 and such that, for any two distinct
elements A1 and A2 of A0,
Î³ 2(Ïƒ âˆ§a)2 Î¼2r
16 (Ïƒ âˆ§a)2 Î¼2m1r
In view of Assumption 2, this implies
L2() â‰¥(1 âˆ’Î´r)2 Î³ 2
16 (Ïƒ âˆ§a)2 m1r
Using that, conditionally on Xi, the distributions of Î¾i are Gaussian, we get that,
for any A âˆˆA0, the Kullbackâ€“Leibler divergence K(P0,PA) between P0 and PA
K(P0,PA) =
L2() â‰¤(1 + Î´r)2 Î³ 2
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
From (4.4) we deduce that the condition
Card(A0) âˆ’1
K(P0,PA) â‰¤Î± log
Card(A0) âˆ’1
is satisï¬ed for any Î± > 0 if Î³ > 0 is chosen as a sufï¬ciently small numerical
constant depending on Î±. In view of (4.3) and (4.5), the result now follows by
application of Theorem 2.5 in .
In the USR matrix completion problem we have âˆ¥Aâˆ¥2
L2() = (m1m2)âˆ’1âˆ¥Aâˆ¥2
for all matrices A âˆˆRm1Ã—m2. Thus, the corresponding lower bound follows immediately from the previous theorem with Î´r = 0 and Î¼ = âˆšm1m2.
THEOREM 6.
Fix a > 0 and an integer r such that 1 â‰¤r â‰¤min(m1,m2),
Mr â‰¤n. Let the matrices Xi be i.i.d. uniformly distributed on X , and let, conditionally on Xi, the variables Î¾i be Gaussian N(0,Ïƒ 2), Ïƒ 2 > 0, for i = 1,...,n.
Then there exist absolute constants Î² âˆˆ(0,1) and c > 0, such that
2 > c(Ïƒ âˆ§a)2 Mr
Comparing Theorem 6 with Corollary 2(i) we see that, in the case of Gaussian
errors Î¾i, the rate of convergence of our estimator Ë†AÎ» given in (3.9) is optimal (up
to a logarithmic factor) in a minimax sense on the class of matrices A(r,a).
A similar conclusion can be obtained for the statistical learning setting. Indeed,
assume that the pairs (Xi,Yi) are i.i.d. realizations of a random pair (X,Y) with
distribution PXY belonging to the class
PA0,Î· = {PXY :X âˆ¼0,|Y| â‰¤Î· (a.s.),E(Y|X) = âŸ¨A0,XâŸ©},
where 0 is the uniform distribution on X , 1 â‰¤r â‰¤min(m1,m2) is an integer and
THEOREM 7.
Let n,m1,m2,r be as in Theorem 5. Let (Xi,Yi) be i.i.d. realizations of a random pair (X,Y) with distribution PXY . Then there exist absolute
constants Î² âˆˆ(0,1) and c > 0, such that
rank(A0)â‰¤r
PXY âˆˆPA0,Î·
2 > cÎ·2 Mr
We proceed as in the proof of Theorem 5 with some modiï¬cations.
Assuming that M = max(m1,m2) = m1 â‰¥m2 and 0 â‰¤Î³ â‰¤1/2 we deï¬ne the class
of matrices
ËœA = (aij) âˆˆRm1Ã—r :aij âˆˆ
,âˆ€1 â‰¤i â‰¤m1,1 â‰¤j â‰¤r
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
and take its block extension B(Câ€²). Consider the joint distributions PXY such
that X âˆ¼0 and, conditionally on X, Y = Î· with probability pA0(X) = 1/2 +
âŸ¨A0,XâŸ©/(2Î·) and Y = âˆ’Î· with probability 1 âˆ’pA0(X) = 1/2 âˆ’âŸ¨A0,XâŸ©/(2Î·),
where A0 âˆˆB(Câ€²). It is easy to see that such distributions PXY belong to the class
PA0,Î·, and our assumptions guarantee that 1/4 â‰¤pA0(X) â‰¤3/4, rank(A0) â‰¤r for
all A0 âˆˆB(Câ€²). We will denote the corresponding n-product measure by PA0. For
any A âˆˆB(Câ€²), the Kullbackâ€“Leibler divergence between P0 and PA has the form
K(P0,PA) = nE
p0(X)log p0(X)
log 1 âˆ’p0(X)
Using the inequality âˆ’log(1 + u) â‰¤âˆ’u + u2/2, âˆ€u > âˆ’1, and the fact that 1/4 â‰¤
pA(X) â‰¤3/4, we ï¬nd that the expression under the expectation in (4.8) is bounded
by 2(p0(X) âˆ’pA(X))2. This implies
K(P0,PA) â‰¤n
The remaining arguments are analogous to those in the proof of Theorem 5.
5. Further results and examples.
5.1. Recovery of the rank and speciï¬c lower bound.
A notable property of the
estimator Ë†AÎ» in matrix completion setting is that it has the same rank as the underlying matrix A0 with probability close to 1. As a consequence we can establish a
lower bound for the Frobenius error of Ë†AÎ» with the rates matching up to constants
the upper bounds of Corollary 2.
THEOREM 8.
Let Xi be i.i.d. uniformly distributed on X , and let Î» satisfy
the inequality Î» â‰¥2âˆ¥Mâˆ¥âˆ(as in Theorem 1). Consider the estimator Ë†AÎ»â€² with
Î»â€² = Î»/(1 âˆ’Î´) for some 0 < Î´ < 1. Set Ë†r = rank( Ë†AÎ»â€²). Then
Ë†r â‰¤rank(A0).
If, in addition, minj : Ïƒj(A0)Ì¸=0 Ïƒj(A0) â‰¥Î»â€²m1m2, then
Ë†r â‰¥rank(A0)
âˆ¥Ë†AÎ»â€² âˆ’A0âˆ¥2
4(1 âˆ’Î´)2 rank(A0)(Î»m1m2)2.
Note that X âˆ’A0 = m1m2M. Using standard matrix perturbation argument (cf. , page 203), we get, for all j = 1,...,m1 âˆ§m2,
|Ïƒj(X) âˆ’Ïƒj(A0)| â‰¤Ïƒ1(X âˆ’A0) = m1m2âˆ¥Mâˆ¥âˆâ‰¤Î»m1m2
= (1 âˆ’Î´)Î»â€²m1m2
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
Since, by (3.2), ÏƒË†r(X) > Î»â€²m1m2/2, we ï¬nd that ÏƒË†r(A0) > Î´Î»â€²m1m2/2. This implies (5.1). Now, if Ïƒj(A0) â‰¥Î»â€²m1m2 we get
Ïƒj(X) â‰¥Ïƒj(A0) âˆ’|Ïƒj(X) âˆ’Ïƒj(A0)| â‰¥Î»â€²m1m2 âˆ’(1 âˆ’Î´)Î»â€²m1m2
and thus (5.2) follows.
To prove (5.3), denote by P :Rm1Ã—m2 â†’Rm1Ã—m2 the projector on the linear span of matrices (uj(X)vj(X)âŠ¤,j = 1,...,r), where r = rank(A0). We
have âˆ¥Ë†AÎ»â€² âˆ’A0âˆ¥2 â‰¥âˆ¥P( Ë†AÎ»â€² âˆ’A0)âˆ¥2 â‰¥âˆ¥P( Ë†AÎ»â€² âˆ’X)âˆ¥2 âˆ’âˆ¥P(X âˆ’A0)âˆ¥2. Here
âˆ¥P( Ë†AÎ»â€² âˆ’X)âˆ¥2 = âˆšrÎ»â€²m1m2/2 in view of (3.2) and the fact that Ë†r = r; cf. (5.1)
and (5.2). On the other hand, âˆ¥P(X âˆ’A0)âˆ¥2 â‰¤âˆšrâˆ¥Mâˆ¥âˆm1m2 â‰¤âˆšrÎ»m1m2/2.
This implies
âˆ¥Ë†AÎ»â€² âˆ’A0âˆ¥2 â‰¥
âˆ’(1 âˆ’Î´)Î»â€²m1m2
COROLLARY 3.
Let the assumptions of Corollary 2 be satisï¬ed. Consider the
estimator Ë†AÎ»â€² with
for some 0 < Î´ < 1. Set Ë†r = rank( Ë†AÎ»â€²). Then Ë†r â‰¤rank(A0) with probability at least
1 âˆ’3/(m1 + m2). If, in addition,
j : Ïƒj(A0)Ì¸=0Ïƒj(A0) â‰¥Câˆ—câˆ—
log(m)(m1 âˆ¨m2)
then Ë†r â‰¥rank(A0) and
âˆ¥Ë†AÎ»â€² âˆ’A0âˆ¥2
4(1 âˆ’Î´)2 rank(A0)log(m)(m1 âˆ¨m2)
with the same probability.
We note that the lower bound for Ïƒj(A0) in (5.4) is not excessively high, since
âˆšm1m2 is a â€œtypicalâ€ order of the largest singular value Ïƒ1(A0) for nonlacunary
matrices A0. For example, if all the entries of A0 are equal to some constant a, the
left-hand side of (5.4) is equal to Ïƒ1(A0) = aâˆšm1m2.
5.2. Risk bounds in statistical learning.
The results of the previous sections
can be also extended to the traditional statistical learning setting where (Xi,Yi)
is a sequence of i.i.d. replications of a random pair (X,Y) with X âˆˆRm1Ã—m2 and
Y âˆˆR, and there is no underlying model determined by matrix A0; that is, we
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
do not assume that E(Y|X) = âŸ¨A0,XâŸ©. Then the above oracle inequalities can be
reformulated in terms of the prediction risk
R(A) = E[(Y âˆ’âŸ¨A,XâŸ©)2]
âˆ€A âˆˆRm1Ã—m2.
We illustrate this by an example dealing with USR matrix completion. Speciï¬cally,
Theorem 4 is reformulated in the following way.
THEOREM 9.
Let Xi be i.i.d. uniformly distributed on X . Assume that |Y| â‰¤Î·
almost surely for some constant Î·. For t > 0 consider the regularization parameter
Î» satisfying (3.6). Then with probability at least 1 âˆ’eâˆ’t we have
R( Ë†AÎ») â‰¤R(A) + min
m1m2Î»2 rank(A)
for all A âˆˆRm1Ã—m2. In particular, under the assumptions of Corollary 2(ii),
2Î·2 log(m)M rank(A)
This theorem can be also viewed as a result about the approximate sparsity. We
do not know whether the true underlying model is described by some matrix A0,
but we can guarantee that our estimator is not far from the best approximation
provided by matrices A with small rank or small nuclear norm.
Note that the results of Theorem 9 are uniform over the class of distributions
PÎ· = {PXY :X âˆ¼0,|Y| â‰¤Î· (a.s.)},
where 0 is the uniform distribution on X , and Î· > 0 is a constant. The corresponding lower bound is given in the next theorem.
THEOREM 10.
Let n,m1,m2,r be as in Theorem 5. Let (Xi,Yi) be i.i.d. realizations of a random pair (X,Y) with distribution PXY . Then
R( Ë†A) â‰¥R(A) + cÎ·2 Mr
where Î² âˆˆ(0,1) and c > 0 are absolute constants.
For E(Y|X) = âŸ¨A0,XâŸ©we have R(A) = âˆ¥A âˆ’A0âˆ¥2
L2() + Ïƒ 2 =
(m1m2)âˆ’1âˆ¥A âˆ’A0âˆ¥2
2 + Ïƒ 2, where Ïƒ 2 = E[(Y âˆ’E(Y|X))2]. Thus, using Theorem 7 we get
R( Ë†A) â‰¥R(A) + cÎ·2 Mr
2 > cÎ·2 Mr
Inequalities (5.7) and (5.8) imply minimax rate optimality of Ë†AÎ» up to a logarithmic factor in the statistical learning setting.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
5.3. Risks bounds in spectral norm.
The results of the previous sections on the
Frobenius norm can be extended to the spectral norm. In this subsection we consider the USR matrix completion problem, that is, we assume that the matrices Xi
are i.i.d. uniformly distributed on X , which implies that âˆ¥Aâˆ¥2
2 = (m1m2)âˆ’1âˆ¥Aâˆ¥2
for all matrices A âˆˆRm1Ã—m2.
THEOREM 11.
Let Xi be i.i.d. uniformly distributed on X . Consider the estimator Ë†AÎ» deï¬ned in (3.1). If Î» â‰¥âˆ¥Mâˆ¥âˆ, then
âˆ¥Ë†AÎ» âˆ’A0âˆ¥âˆâ‰¤3
âˆ¥Ë†AÎ» âˆ’A0âˆ¥âˆâ‰¤âˆ¥Ë†AÎ» âˆ’Xâˆ¥âˆ+ m1m2âˆ¥Mâˆ¥âˆ,
where we recall that X = m1m2
i=1 YiXi, E(X) = A0, and M is deï¬ned in (2.2).
In view of (3.2), we clearly have âˆ¥Ë†AÎ» âˆ’Xâˆ¥âˆâ‰¤Î»m1m2/2. The result follows
immediately since âˆ¥Mâˆ¥âˆâ‰¤Î».
As a consequence of the above theorem, we can derive the optimal rate (up a
to logarithmic factor) of USR matrix completion for the spectral norm when the
noise is sub-exponential or in the statistical learning setting.
THEOREM 12.
Let one of the sets of conditions (i) or (ii) in Corollary 2 be
satisï¬ed. Then, with probability at least 1 âˆ’3/(m1 + m2), we have
âˆ¥Ë†AÎ» âˆ’A0âˆ¥âˆâ‰¤CCâˆ—câˆ—
(m1 âˆ¨m2)logm
where C > 0 is an absolute constant.
The proof of this result is immediate by combining Theorem 11 and
Lemmas 1, 2 and 3.
THEOREM 13.
(i) Let the conditions of Theorem 6 be satisï¬ed. Then
âˆ¥Ë†A âˆ’A0âˆ¥âˆ> c(Ïƒ âˆ§a)âˆšm1m2
where Î² âˆˆ(0,1) and c > 0 are absolute constants.
(ii) Let the conditions of Theorem 7 be satisï¬ed. Then
rank(A0)â‰¤r
PXY âˆˆPA0,Î·
âˆ¥Ë†A âˆ’A0âˆ¥âˆ> cÎ·âˆšm1m2
where Î² âˆˆ(0,1) and c > 0 are absolute constants.
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
Note ï¬rst that, in the USR matrix completion problem, Assumption 2
is satisï¬ed with Î´r = 0 and Î¼ = âˆšm1m2.
We prove part (i) of the theorem. Consider the set of matrices A0 introduced in
the proof of Theorem 5. For any two distinct matrices A1,A2 of A0, we have
âˆ¥A1 âˆ’A2âˆ¥âˆâ‰¥
16(Ïƒ âˆ§a)âˆšm1m2
Indeed, if (5.11) does not hold, we get
2 â‰¤rank(A1 âˆ’A2)âˆ¥A1 âˆ’A2âˆ¥2
16(Ïƒ âˆ§a)2m1m2
since rank(A1 âˆ’A2) â‰¤r by construction of A0. This contradicts (4.3).
Next, (4.5) is satisï¬ed for any Î± > 0 if Î³ > 0 is chosen as a sufï¬ciently small
numerical constant depending on Î±.
Combining (5.11) with (4.5) and Theorem 2.5 in gives the result.
The proof of (ii) follows the same arguments.
5.4. Sharp oracle inequalities for the Lasso.
As we already mentioned in Example 4 and in the remark after Theorem 2, one can exploit (2.19) to derive sparsity
oracle inequalities for the usual Lasso. This is detailed in the present subsection. It
is noteworthy that the obtained inequalities are sharp (i.e., with leading constant 1),
which was not achieved in the previous work on the Lasso.
Note that, if m1 = m2 = p and A and Xi are diagonal matrices, then the trace
regression model (1.2) becomes
i = 1,...,n,
where xi,Î²âˆ—âˆˆRp denote the vectors of diagonal elements of Xi,A0, respectively.
Set X = (x1,...,xn)âŠ¤âˆˆRnÃ—p to be the design matrix of this linear regression
model. For a vector z = (z(1),...,z(d)) âˆˆRd, deï¬ne |z|q = (d
j=1 |z(j)|q)1/q for
1 â‰¤q < âˆand |z|âˆ= max1â‰¤jâ‰¤d |z(j)|.
Assume in what follows that xi are ï¬xed and p â‰¥2. Then for A = diag(Î²) we
L2() = nâˆ’1|XÎ²|2
2, where diag(Î²) denotes the diagonal pÃ—p matrix with
the components of Î² on the diagonal. We will assume without loss of generality
that the diagonal elements of the Gram matrix 1
nXâŠ¤X are not larger than 1 (the
general case is obtained from this by simple rescaling).
The estimator Ë†AÎ» deï¬ned in (1.7) becomes the usual Lasso estimator
Ë†Î²Î» = argmin
i Î²)2 + Î»|Î²|1
For a vector Î² âˆˆRp, we set, with a little abuse of notation, Î¼c0(Î²) =
Î¼c0(diag(Î²)), Î¼(Î²) = Î¼5(Î²). Let M(Î²) denote the number of nonzero components of Î².
For simplicity, the result is stated only in the case of Gaussian noise.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
THEOREM 14.
Let Î¾i be i.i.d. Gaussian N(0,Ïƒ 2), and let the diagonal elements of matrix 1
nXâŠ¤X be not larger than 1. Take
where C = 3b
2,b â‰¥1. Then, with probability at least 1 âˆ’
pb2âˆ’1âˆšÏ€ logp, we have
n|X( Ë†Î²Î» âˆ’Î²âˆ—)|2
n|X(Î² âˆ’Î²âˆ—)|2
2 + C2Ïƒ 2 Î¼2(Î²)M(Î²)logp
Combine Theorem 2 and a standard bound on the tail of the Gaussian
distribution, which assures that with probability at least 1 âˆ’
pb2âˆ’1âˆšÏ€ logp,
Given Î² âˆˆRp and J âŠ‚{1,...,p}, denote by Î²J the vector in Rp which has the
same coordinates as Î² on J and zero coordinates on the complement J c of J.
We recall the restricted eigenvalue condition of :
CONDITION RE(s,c0).
For some integer s such that 1 â‰¤s â‰¤p, and a positive
number c0 the following condition holds:
JâŠ†{1,...,p},
uâˆˆRp,uÌ¸=0,
|uJc|1â‰¤c0|uJ |1
We have the following corollary.
COROLLARY 4.
Let the assumptions of Theorem 14 hold, and let condition RE(s,5) be satisï¬ed for some 1 â‰¤s â‰¤p. Then, with probability at least
pb2âˆ’1âˆšÏ€ logp,
n|X( Ë†Î²Î» âˆ’Î²âˆ—)|2
Î²âˆˆRp : M(Î²)â‰¤s
n|X(Î² âˆ’Î²âˆ—)|2
Recall that ej(p) denote the canonical basis vectors of Rp. For any
p Ã— p diagonal matrix A = diag(Î²) with support (S1,S2), S1 = S2 = {ej(p),j âˆˆ
J}, where J âŠ‚{1,...,p} has cardinality |J| â‰¤s, and an arbitrary p Ã— p diagonal
matrix B = diag(u), where u âˆˆRp, we have
âˆ¥PA(B)âˆ¥1 = |uJ|1,
âˆ¥PA(B)âˆ¥2 = |uJ |2,
A (B)âˆ¥1 = |uJ c|1
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
CA,c0 = {diag(u):u âˆˆRp,|uJ c|1 â‰¤c0|uJ |1},
âˆ¥Bâˆ¥L2() = 1
BÌ¸=0: BâˆˆCA,c0
uâˆˆRp,uÌ¸=0,
|uJc|1â‰¤c0|uJ |1
Since Condition RE(s,5) is satisï¬ed, Theorem 14 yields the result.
Oracle inequalities (5.12) and (5.13) extend straightforwardly to
Yi = fi + Î¾i,
i = 1,...,n,
where fi are arbitrary ï¬xed values and not necessarily fi = xâŠ¤
i Î²âˆ—. This setting is
interesting in the context of aggregation. Then x1,...,xn are vectors of values of
some given dictionary of p functions at n given points, and fi are the values of an
unknown regression function at the same points. Under model (5.14), inequalities
(5.12) and (5.13) hold true with the only difference that XÎ²âˆ—should be replaced
by the vector f = (f1,...,fn)âŠ¤. With such a modiï¬cation, (5.13) improves upon
Theorem 6.1 of where the leading constant is greater than 1.
6. Control of the stochastic error.
In this section, we obtain the probability
inequalities for the stochastic error âˆ¥Mâˆ¥âˆ. For brevity, we will write throughout
âˆ¥Â·âˆ¥âˆ= âˆ¥Â·âˆ¥. The following proposition is an immediate consequence of the matrix
version of Bernsteinâ€™s inequality due to (Corollary 9.1 in ).
PROPOSITION 1.
Let Z1,...,Zn be independent random matrices with dimensions m1 Ã— m2 that satisfy E(Zi) = 0 and âˆ¥Ziâˆ¥â‰¤U almost surely for some
constant U and all i = 1,...,n. Deï¬ne
Then, for all t > 0, with probability at least 1 âˆ’eâˆ’t we have
Z1 + Â·Â·Â· + Zn
 â‰¤2max
t + log(m)
,U t + log(m)
where m = m1 + m2.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
Furthermore, it is possible to replace the Lâˆ-bound U on âˆ¥Zâˆ¥in the above
inequality by bounds on the weaker ÏˆÎ±-norms of âˆ¥Zâˆ¥deï¬ned by
= inf{u > 0:Eexp(âˆ¥Zâˆ¥Î±/uÎ±) â‰¤2},
PROPOSITION 2.
Let Z,Z1,...,Zn be i.i.d. random matrices with dimensions m1 Ã— m2 that satisfy E(Z) = 0. Suppose that U(Î±)
< âˆfor some Î± â‰¥1.
Then there exists a constant C > 0 such that, for all t > 0, with probability at least
Z1 + Â·Â·Â· + Zn
 â‰¤C max
t + log(m)
1/Î± t + log(m)
where m = m1 + m2.
This is an easy consequence of Proposition 2 in , which provides an analogous result for Hermitian matrices Z. Its extension to rectangular matrices stated
in Proposition 2 is straightforward via the self-adjoint dilation; cf., for example,
the proof of Corollary 9.1 in .
The next lemma gives a control of the stochastic error for USR matrix completion in the statistical learning setting.
Let Xi be i.i.d. uniformly distributed on X . Assume that
maxi=1,...,n |Yi| â‰¤Î· almost surely for some constant Î·. Then for any t > 0 with
probability at least 1 âˆ’eâˆ’t we have
âˆ¥Mâˆ¥â‰¤2Î· max
t + log(m)
(m1 âˆ§m2)n, 2(t + log(m))
We apply Proposition 1 with Zi = YiXi âˆ’E(YiXi). Recall that here
Xi are i.i.d. with the same distribution as X and Yi are not necessarily i.i.d. Observe
Therefore, âˆ¥Ziâˆ¥â‰¤2Î·, ÏƒZ â‰¤Î·ÏƒX, and the result follows from Proposition 1.
We now consider the USR matrix completion with sub-exponential errors. Recall that in this case we assume that the pairs (Xi,Yi) are i.i.d. We have
YiXi âˆ’E(YiXi)
0 Xi)Xi âˆ’E(tr(AâŠ¤
= 1 + 2.
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
We treat the terms 1 and 2 separately in the two lemmas below.
Let Xi be i.i.d. uniformly distributed on X , and the pairs (Xi,Yi)
be i.i.d. Assume that condition (3.3) holds. Then there exists an absolute constant
C > 0 that can depend only on Î±,c1, Ëœc and such that, for all t > 0, with probability
at least 1 âˆ’2eâˆ’t we have
1 â‰¤CÏƒ max
t + log(m)
(m1 âˆ§m2)n, (t + log(m))log1/Î±(m1 âˆ§m2)
Observe ï¬rst that for ËœX = X âˆ’E(X) we have
Î¾i(Xi âˆ’EXi)
Î¾i(Xi âˆ’EX)
Set Zi = Î¾i(Xi âˆ’EX). These are i.i.d. random matrices having the same distribution as a random matrix Z. It follows from (6.2) that âˆ¥Ziâˆ¥â‰¤2|Î¾i|, and thus
condition (3.3) implies that U(Î±)
â‰¤cÏƒ for some constant c > 0. Furthermore, in
view of (6.4), we have ÏƒZ â‰¤câ€²ÏƒÏƒ ËœX = câ€²Ïƒ/(m1 âˆ§m2)1/2 for some constant câ€² > 0
and ÏƒZ â‰¥c1/2
Ïƒ/(2(m1 âˆ§m2))1/2. Using these remarks we can deduce from Proposition 2 that there exists an absolute constant ËœC > 0 such that for any t > 0 with
probability at least 1 âˆ’eâˆ’t we have
Î¾i(Xi âˆ’EX)
t + log(m)
1/Î± t + log(m)
t + log(m)
(m1 âˆ§m2)n, (t + log(m))log1/Î±(m1 âˆ§m2)
Finally, in view of Condition (3.3) and Bernsteinâ€™s inequality for sub-exponential
noise, we have for any t > 0, with probability at least 1 âˆ’eâˆ’t,
""""" â‰¤CÏƒ max
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
where C > 0 depends only on Ëœc. We complete the proof by using the union bound.
|A0|âˆ—= max
i,j |a0(i,j)|âˆšm1 âˆ¨m2.
Let Xi be i.i.d. random variables uniformly distributed in X . Then,
for all t > 0, with probability at least 1 âˆ’eâˆ’t we have
t + log(m)
i,j |a0(i,j)|t + log(m)
If maxi,j |a0(i,j)| â‰¤a for some a > 0, then with the same probability
2 â‰¤2a max
t + log(m)
(m1 âˆ§m2)n, 2(t + log(m))
We apply Proposition 1 for the random variables Zi = tr(AâŠ¤
0 X)X). Using (6.2) we get âˆ¥Ziâˆ¥â‰¤2maxi,j |a0(i,j)| and
Z â‰¤max{âˆ¥E(âŸ¨A0,XâŸ©2XXâŠ¤)âˆ¥,âˆ¥E(âŸ¨A0,XâŸ©2XâŠ¤X)âˆ¥} â‰¤|A0|2
Thus, (6.6) follows from Proposition 1.