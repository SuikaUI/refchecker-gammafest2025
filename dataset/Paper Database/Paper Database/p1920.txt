The Annals of Statistics
2011, Vol. 39, No. 5, 2302–2329
DOI: 10.1214/11-AOS894
© Institute of Mathematical Statistics, 2011
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES FOR
NOISY LOW-RANK MATRIX COMPLETION
BY VLADIMIR KOLTCHINSKII1, KARIM LOUNICI2 AND
ALEXANDRE B. TSYBAKOV3
Georgia Institute of Technology, Georgia Institute of Technology and CREST
This paper deals with the trace regression model where n entries or linear combinations of entries of an unknown m1 × m2 matrix A0 corrupted by
noise are observed. We propose a new nuclear-norm penalized estimator of
A0 and establish a general sharp oracle inequality for this estimator for arbitrary values of n,m1,m2 under the condition of isometry in expectation. Then
this method is applied to the matrix completion problem. In this case, the estimator admits a simple explicit form, and we prove that it satisﬁes oracle
inequalities with faster rates of convergence than in the previous works. They
are valid, in particular, in the high-dimensional setting m1m2 ≫n. We show
that the obtained rates are optimal up to logarithmic factors in a minimax
sense and also derive, for any ﬁxed matrix A0, a nonminimax lower bound
on the rate of convergence of our estimator, which coincides with the upper
bound up to a constant factor. Finally, we show that our procedure provides an
exact recovery of the rank of A0 with probability close to 1. We also discuss
the statistical learning setting where there is no underlying model determined
by A0, and the aim is to ﬁnd the best trace regression model approximating the data. As a by-product, we show that, under the restricted eigenvalue
condition, the usual vector Lasso estimator satisﬁes a sharp oracle inequality
(i.e., an oracle inequality with leading constant 1).
1. Introduction.
Assume that we observe n independent random pairs
(Xi,Yi),i = 1,...,n, where Xi are random matrices with dimensions m1 × m2,
and Yi are random variables in R, satisfying the trace regression model
E(Yi|Xi) = tr(X⊤
i = 1,...,n,
where A0 ∈Rm1×m2 is an unknown matrix, E(Yi|Xi) is the conditional expectation
of Yi given Xi and tr(B) denotes the trace of matrix B. We consider the problem
of estimation of A0 based on the observations (Xi,Yi),i = 1,...,n. Though the
results of this paper are obtained for general n,m1,m2, the main motivation is
Received November 2010; revised March 2011.
1Supported in part by NSF Grants DMS-09-06880 and CCF-0808863.
2Supported in part by NSF Grant DMS-11-06644 and Simons foundation Grant 209842.
3Supported in part by ANR “Parcimonie” and by PASCAL-2 Network of Excellence.
MSC2010 subject classiﬁcations. Primary 62J99, 62H12; secondary 60B20, 60G15.
Key words and phrases. Matrix completion, low-rank matrix estimation, recovery of the rank, statistical learning, optimal rate of convergence, noncommutative Bernstein inequality, Lasso.
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
in the high-dimensional setting, which corresponds to m1m2 ≫n, with low-rank
matrices A0.
It will be convenient to write model (1.1) in the form
Yi = tr(X⊤
i A0) + ξi,
i = 1,...,n,
where the noise variables ξi = Yi −E(Yi|Xi) are independent and have zero means.
For any matrices A,B ∈Rm1×m2, we deﬁne the scalar product
⟨A,B⟩= tr(A⊤B)
and the bilinear form
⟨A,B⟩L2() = 1
E(⟨A,Xi⟩⟨B,Xi⟩).
Here  = 1
i=1 i, where i denotes the distribution of Xi. The corresponding
semi-norm ∥A∥L2() is given by
E(⟨A,Xi⟩2).
EXAMPLE 1 (Matrix completion).
Assume that the design matrices Xi are
i.i.d. uniformly distributed on the set
X = {ej(m1)e⊤
k (m2),1 ≤j ≤m1,1 ≤k ≤m2},
where ek(m) are the canonical basis vectors in Rm. The set X forms an orthonormal basis in the space of m1×m2 matrices that will be called the matrix completion
basis. Let also n < m1m2. Then the problem of estimation of A0 coincides with
the problem of matrix completion under uniform sampling at random (USR) as
studied in the nonnoisy case (ξi = 0) in , and in the noisy case in .
Considering low-rank matrices A0 is of a particular interest. Clearly, for such Xi
we have the isometry
L2() = μ−2∥A∥2
for all matrices A ∈Rm1×m2, where μ = √m1m2, and ∥A∥2 is the Frobenius norm
of A. However, the restricted isometry property in the usual sense, that is, “in
probability” (cf., e.g., ) does not hold for matrix completion, since for n <
m1m2 there trivially exists a matrix of rank 1 in the null space of the sampling
One can also consider more general matrix measurement models in which, for
a given orthonormal basis in the space of matrices, a random sample of Fourier
coefﬁcients of the target matrix A0 is observed subject to a random noise. For
more discussion on matrix completion with other types of sampling, see and references therein.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
EXAMPLE 2 (Column masks).
Assume that the design matrices Xi are i.i.d.
replications of a random matrix X, which has only one nonzero column. For instance, let the distribution of X be such that all the columns have equal probability to be nonzero, and the random entries of nonzero column x(j) are such that
(j)) is the identity matrix. Then ∥A∥2
L2() = ∥A∥2
2/m2,∀A ∈Rm1×m2, so
that condition (1.4) is satisﬁed with μ = √m2. More generally, in view of application to multi-task learning (cf. ) one can be interested in considering nonidentically distributed Xi. The model can be then reformulated as a longitudinal regression model, with different distributions of Xi corresponding to different tasks.
EXAMPLE 3 (“Complete” sub-Gaussian design).
Assume that the design matrices Xi are i.i.d. replications of a random matrix X such that ⟨A,X⟩is a sub-
Gaussian random variable for any A ∈Rm1×m2. This approach has its roots in
compressed sensing. The two major examples are given by the matrices X whose
entries are either i.i.d. standard Gaussian or Rademacher random variables. In both
cases, we have ∥A∥2
L2() = ∥A∥2
2,∀A ∈Rm1×m2, so that condition (1.4) is satis-
ﬁed with μ = 1. The problem of exact reconstruction of A0 under such a design
in the nonnoisy setting was studied in , whereas estimation of A0 in
the presence of noise is analyzed in , among which treat the
high-dimensional case m1m2 > n.
EXAMPLE 4 (Fixed design).
Assume that all the i are Dirac measures, so
that the design matrices Xi are nonrandom. Then ∥A∥2
i=1⟨A,Xi⟩2,
and we get the problem of trace regression with ﬁxed design; cf. . In particular,
if m1 = m2, and A and Xi are diagonal matrices the trace regression model (1.2)
becomes the usual linear regression model. Accordingly, the rank of A becomes the
number of its nonzero diagonal elements. This observation will allow us to deduce,
as a consequence of our general argument, an oracle inequality for the usual Lasso
in sparse linear regression with ﬁxed design improving in the sense that the
inequality is sharp; cf. Theorem 2 and Section 5.4.
The general oracle inequalities that we will prove in Section 2 can be successfully applied to the above examples. The emphasis in this paper will be on the
matrix completion problem (Example 1), for which the previously obtained results
were suboptimal.
Statistical estimation of low-rank matrices has recently become a very active
ﬁeld with a rapidly growing literature. The most popular methods are based on
penalized empirical risk minimization with nuclear-norm penalty . Estimators with other types of penalization, such as the Schatten-p
norm , the von Neumann entropy , penalization by the rank or some
combined penalties are also discussed.
It is worth pointing out that in many applications, such as in matrix completion,
the distribution  is known, and yet this information has not been exploited since
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
the penalized estimation procedures considered in the literature involve the empirical risk 1
i=1(Yi −tr(X⊤
i A))2. In this paper we incorporate the knowledge of
 in the construction and we study the following estimator of A0:
ˆAλ = argmin
where A ⊆Rm1×m2 is a set of matrices,
Ln(A) = ∥A∥2
λ > 0 is a regularization parameter and ∥A∥1 is the nuclear norm of A. We will
mainly consider convex sets A. Note that if all Xi are nonrandom, ˆAλ coincides
with the usual matrix Lasso estimator,
ˆAλ = argmin
(Yj −⟨A,Xj⟩)2 + λ∥A∥1
The emphasis in this paper is on the noisy matrix completion setting. Then
the estimator ˆAλ has a particularly simple form; it is obtained from the matrix
i=1 YiXi by soft thresholding of its singular values. One of the main results of this paper is to show that our estimators are rate optimal (up to logarithmic
factors) under the Frobenius error for a simple class of matrices A(r,a) deﬁned
by two restrictions: the rank of A0 is not larger than given r, and all the entries of
A0 are bounded in absolute value by a constant a. This rather intuitive class has
been ﬁrst considered in . However, the construction of the estimator in 
requires the exact knowledge of rank(A0) and the upper bound on the Frobenius
error obtained in is suboptimal (see the details in Section 3). The recent paper
 obtains suboptimal bounds of “slow rate” type for matrix completion while
 focuses on complex-valued Hermitian matrices with nuclear norm equal to 1,
which is motivated by density matrix estimation problem in quantum state tomography. These papers do not address the optimality issue. Optimal rates in noisy
matrix completion are derived in , but on different classes of matrices and
with the empirical prediction error rather than with the Frobenius error. Finally,
 discusses the optimality issue for the Frobenius error on the classes deﬁned in
terms of a “spikiness index” of A0, which are not related to A(r,a), and suggests
estimators that require prior knowledge about this index.
The main contributions of this paper are the following. In Section 2 we derive
a general oracle inequality for the prediction error ∥ˆAλ −A0∥2
L2(). This oracle
inequality is sharp, that is, with leading constant 1, both in the case of “slow rate”
(for matrices A0 with small nuclear norm) and in the case of “fast rate” (for matrices A0 with small rank). As a particular instance of this general result, in Section 3
we obtain an oracle inequality for the matrix completion problem. In Section 4, we
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
establish minimax lower bounds showing that the rates for matrix completion obtained in Section 3 are optimal up to a logarithmic factor. In Section 5, we brieﬂy
discuss some other implications and extensions of our method. Finally, Section 6
is devoted to the control of the stochastic term appearing in the proof of the upper
2. General oracle inequalities.
We recall ﬁrst some basic facts about matrices. Let A ∈Rm1×m2 be a rectangular matrix, and let r = rank(A) ≤min(m1,m2)
denote its rank. The singular value decomposition (SVD) of A has the form
j=1 σj(A)ujv⊤
j with orthonormal vectors u1,...,ur ∈Rm1, orthonormal
vectors v1,...,vr ∈Rm2 and real numbers σ1(A) ≥··· ≥σr(A) > 0 (the singular
values of A). The pair of linear vector spaces (S1,S2) where S1 is the linear span
of {u1,...,ur}, and S2 is the linear span of {v1,...,vr}, will be called the support
of A. We will denote by S⊥
j the orthogonal complements of Sj, j = 1,2, and by
PS the projector on the linear vector subspace S of Rmj , j = 1,2.
The Schatten-p (quasi-)norm ∥A∥p of matrix A is deﬁned by
min(m1,m2)
for 0 < p < ∞
∥A∥∞= σ1(A).
Recall the well-known trace duality property,
|tr(A⊤B)| ≤∥A∥1∥B∥∞
∀A,B ∈Rm1×m2.
We will also use the fact that the subdifferential of the convex function A →∥A∥1
is the following set of matrices:
cf. . Deﬁne the random matrix
YiXi −E(YiXi)
We will need the following assumption on the distribution of the matrices Xi.
ASSUMPTION 1.
There exists a constant μ > 0 such that, for all matrices
A ∈A −A := {A1 −A2 :A1,A2 ∈A},
L2() ≥μ−2∥A∥2
As discussed in the Introduction, Assumption 1 is satisﬁed, often with equality
and for A = A −A = Rm1×m2, in several interesting examples. The next theorem
plays the key role in what follows.
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
THEOREM 1.
Let A ⊆Rm1×m2 be any set of matrices. If λ ≥2∥M∥∞, then
∥ˆAλ −A0∥2
L2() ≤inf
L2() + 2λ∥A∥1
If, in addition, A is a convex set, and Assumption 1 is satisﬁed, then
∥ˆAλ −A0∥2
L2() ≤inf
μ2λ2 rank(A)
Furthermore, in this case, for all A ∈A with support (S1,S2),
∥ˆAλ −A0∥2
L2() + (λ −2∥M∥∞)∥PS⊥
μ2λ2 rank(A).
It follows from the deﬁnition of the estimator ˆA that, for all A ∈A,
Ln( ˆAλ) = ∥ˆAλ∥2
+ λ∥A∥1 = Ln(A).
Also, note that
E(YiXi) = 1
E(⟨A0,Xi⟩Xi)
⟨E(YiXi),A⟩= ⟨A0,A⟩L2().
Therefore, we have
L2() −2⟨ˆAλ,A0⟩L2()
L2() −2⟨A,A0⟩L2() +
YiXi −E(YiXi)
+ λ(∥A∥1 −∥ˆAλ∥1),
which implies, due to the trace duality,
∥ˆAλ −A0∥2
L2() ≤∥A −A0∥2
L2() + 2∥ˆAλ −A∥1 + λ(∥A∥1 −∥ˆAλ∥1),
where we set for brevity  = ∥M∥∞. Under the assumption λ ≥2 this yields
∥ˆAλ −A0∥2
L2() ≤∥A −A0∥2
L2() + λ(∥ˆAλ −A∥1 + ∥A∥1 −∥ˆAλ∥1)
L2() + 2λ∥A∥1,
and the bound (2.3) follows.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
To prove the remaining bounds, note that a necessary condition of extremum in
problem (1.5) implies that there exists ˆV ∈∂∥ˆAλ∥1 such that, for all A ∈A,
2⟨ˆAλ, ˆAλ −A⟩L2() −
YiXi, ˆAλ −A
+ λ⟨ˆV , ˆAλ −A⟩≤0.
Indeed, since ˆAλ is a minimizer of Ln(A) in A, there exists a matrix B ∈∂Ln( ˆAλ)
such that −B belongs to the normal cone of A at the point ˆAλ; cf. , Chapter 4,
Section 2, Corollary 6. It is easy to see that B can be represented as follows:
Rm1×m2⟨ˆAλ,X⟩X(dX) −2
YiXi + λ ˆV ,
where ˆV ∈∂∥ˆAλ∥1. The condition that −B belongs to the normal cone at the point
ˆAλ implies that ⟨B, ˆAλ −A⟩≤0, and (2.6) follows.
Consider an arbitrary A ∈A of rank r with spectral representation A =
j=1 σjujv⊤
j and with support (S1,S2). It follows from (2.6) that
2⟨ˆAλ −A0, ˆAλ −A⟩L2() + λ⟨ˆV −V, ˆAλ −A⟩
≤−λ⟨V, ˆAλ −A⟩+ 2⟨M, ˆAλ −A⟩
for an arbitrary V ∈∂∥A∥1. By monotonicity of subdifferentials of convex functions, ⟨ˆV −V, ˆAλ −A⟩≥0. On the other hand, by (2.1), the following representation holds:
where W is an arbitrary matrix with ∥W∥∞≤1. It follows from the trace duality
that there exists W with ∥W∥∞≤1 such that
2 , ˆAλ −A⟩= ⟨PS⊥
2 , ˆAλ⟩= ⟨W,PS⊥
where in the ﬁrst equality we used that A has the support (S1,S2). For this particular choice of W, (2.7) implies that
2⟨ˆAλ −A0, ˆAλ −A⟩L2() + λ∥PS⊥
j , ˆAλ −A
+ 2⟨M, ˆAλ −A⟩.
Using the identity
2⟨ˆAλ −A0, ˆAλ −A⟩L2() = ∥ˆAλ −A0∥2
L2() + ∥ˆAλ −A∥2
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
and the facts that
j , ˆAλ −A
j ,PS1( ˆAλ −A)PS2
we deduce from (2.8) that
∥ˆAλ −A0∥2
L2() + ∥ˆAλ −A∥2
L2() + λ∥PS⊥
L2() + λ∥PS1( ˆAλ −A)PS2∥1 + 2⟨M, ˆAλ −A⟩.
To provide an upper bound on 2⟨M, ˆAλ −A⟩we use the following decomposition:
⟨M, ˆAλ −A⟩= ⟨PA(M), ˆAλ −A⟩+ ⟨PS⊥
2 , ˆAλ −A⟩
= ⟨PA(M),PA( ˆAλ −A)⟩+ ⟨PS⊥
where PA(M) = M −PS⊥
2 . This implies, due to the trace duality,
2|⟨M, ˆAλ −A⟩| ≤∥PA( ˆAλ −A)∥2 + ∥PS⊥
≤∥ˆAλ −A∥2 + ∥PS⊥
 = 2∥PA(M)∥2,
≤2∥M∥∞= 2.
PA(M) = PS⊥
1 MPS2 + PS1M
and rank(PSj ) ≤rank(A), j = 1,2, we have
rank(PA(M))∥M∥∞≤2
2rank(A) ≤
2rank(A)λ.
Due to the fact that
∥PS1( ˆAλ −A)PS2∥1 ≤
rank(A)∥PS1( ˆAλ −A)PS2∥2
rank(A)∥ˆAλ −A∥2
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
and to Assumption 1, it follows from (2.11) and (2.12) that
∥ˆAλ −A0∥2
L2() + ∥ˆAλ −A∥2
L2() + λ∥PS⊥
rank(A) + 
∥ˆAλ −A∥L2()
Using the above bounds on  and , we obtain from (2.17) that
∥ˆAλ −A0∥2
L2() + ∥ˆAλ −A∥2
L2() + (λ −2)∥PS⊥
rank(A)∥ˆAλ −A∥L2(),
which implies
∥ˆAλ −A0∥2
L2() + (λ −2)∥PS⊥
2μ2λ2 rank(A).
The following immediate corollary of Theorem 1 provides a bound for the
Frobenius error.
COROLLARY 1.
Let A be a convex subset of m1 ×m2 matrices containing A0,
and let Assumption 1 be satisﬁed. If λ ≥2∥M∥∞, then
∥ˆAλ −A0∥2
2 ≤λμ2 min
λμ2 rank(A0)
Next, we consider a version of Theorem 1 under weaker assumptions which
are akin to the restricted eigenvalue condition in sparse estimation of vectors. For
simplicity, we will do it only when the domain A of minimization in (1.5) is a
linear subspace of Rm1×m2. Recall that, given A ∈A with support (S1,S2), we
PA(B) := B −PS⊥
A (B) := PS⊥
B ∈Rm1×m2,
and, for c0 ≥0, deﬁne the following cone of matrices:
CA,c0 := {B ∈A:∥P⊥
A (B)∥1 ≤c0∥PA(B)∥1}.
Finally, deﬁne
μc0(A) := inf
μ′ > 0:∥PA(B)∥2 ≤μ′∥B∥L2(),∀B ∈CA,c0
Note that μc0(A) is a nondecreasing function of c0. For c0 = +∞, the quantity
μ∞(A) has a simple meaning: it is equal to the norm of the linear transformation
B →PA(B) from the space A equipped with the L2()-norm into the space of
all matrices equipped with the Frobenius norm. For c0 = 0, μ0(A) is the norm
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
of the same linear transformation restricted to the subspace of A consisting of
all matrices B ∈A with P⊥
A (B) = 0. We are more interested in the intermediate
values, c0 ∈(0,+∞). In this case, μc0(A) is the “norm” of the linear mapping
PA restricted to the cone of matrices B for which PA(B) is the dominant part
A (B) is “small.” Note that the rank of PA(B) is not larger than 2rank(A),
so, when the rank of A is small, the matrices in CA,c0 are approximately “lowrank.” The quantities of the same ﬂavor have been previously used in the literature on Lasso, Dantzig selector and other methods of sparse estimation of vectors.
In these problems, they can be expressed in terms of “restricted eigenvalues” of
certain Gram matrices; cf. the restricted eigenvalue condition in for the ﬁxed
design case and similar distribution dependent conditions in for the random
design case. Such conditions are also considered in for the matrix case. In
what follows, we use the value c0 = 5 and set μ(A) := μ5(A).
THEOREM 2.
Let A be a linear subspace of Rm1×m2. If λ ≥3∥M∥∞, then
∥ˆAλ −A0∥2
L2() ≤inf
L2() + λ2μ2(A)rank(A)
Fix A ∈A with support (S1,S2). If ⟨ˆAλ −A0, ˆAλ −A⟩L2() ≤0, then
we trivially have ∥ˆAλ −A0∥2
L2() ≤∥A−A0∥2
L2() in view of (2.9). Thus, assume
that ⟨ˆAλ −A0, ˆAλ −A⟩L2() > 0. In this case, (2.8) and an obvious modiﬁcation
of (2.10) imply
2 ∥1 ≤λ∥PA( ˆAλ −A)∥1 + 2⟨M, ˆAλ −A⟩.
⟨M, ˆAλ −A⟩= ⟨M,PA( ˆAλ −A)⟩+ ⟨M,P⊥
A ( ˆAλ −A)⟩
∥PA( ˆAλ −A)∥1 + ∥P⊥
A ( ˆAλ −A)∥1
By (2.20) and (2.21),
(λ −2)∥P⊥
A ( ˆAλ −A)∥1 ≤(λ + 2)∥PA( ˆAλ −A)∥1.
For λ ≥3, this yields
A ( ˆAλ −A)∥1 ≤5∥PA( ˆAλ −A)∥1,
which implies that ˆAλ −A ∈CA,5, and thus ∥PA( ˆAλ −A)∥2 ≤μ(A)∥ˆAλ −
A∥L2(). Combining this inequality with (2.11), (2.12), (2.13), (2.14) and using
that λ ≥3, after some algebra we get
∥ˆAλ −A0∥2
L2() + ∥ˆAλ −A∥2
L2() + (λ/3)∥PS⊥
rank(A)∥ˆAλ −A∥L2()
L2() + ∥ˆAλ −A∥2
L2() + μ2(A)λ2 rank(A).
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
As a simple example, consider the case when m1 = m2, A is the space of all
diagonal matrices and Xi also belong to A. Then the trace regression model (1.2)
becomes the usual linear regression model. The Schatten p-norms are in this case
equivalent to the ℓp-norms with the operator norm ∥· ∥∞being the ℓ∞-norm and
the rank of matrix A characterizing the sparsity of the corresponding vector. The
problem of minimizing the functional Ln(A) over the space A is a Lasso-type penalized empirical risk minimization. In particular, it coincides with the standard
Lasso if all Xi are nonrandom. Inequalities of Theorem 1 and (2.19) become, in
this case, sparsity oracle inequalities for the Lasso-type estimators. It is noteworthy that these inequalities are sharp (i.e., with leading constant 1), which was not
achieved in the past work. The random matrix M is also diagonal and its norm
∥M∥∞is just the ℓ∞-norm of the corresponding random vector, which is the sum
of independent random vectors. Hence, it is easy to provide probabilistic bounds on
∥M∥∞using, for instance, the classical Bernstein inequality and the union bound.
We give an example of such an application of Theorem 2 in Section 5.4.
3. Upper bounds for matrix completion.
In this section we consider implications of the general oracle inequalities of Theorem 1 for the model of USR
matrix completion. Thus, we assume that the matrices Xi are i.i.d. uniformly
distributed in the matrix completion basis X , which implies that ∥A∥2
(m1m2)−1∥A∥2
2 for all matrices A ∈Rm1×m2, and we set μ = √m1m2. The estimator ˆAλ is then deﬁned by (here and further on we set A = Rm1×m2 in the case
of matrix completion)
ˆAλ = argmin
2 + λm1m2∥A∥1),
We can also write ˆAλ explicitly
σj(X) −λm1m2/2
+uj(X)vj(X)⊤,
where x+ = max{x,0}, σj(X) are the singular values, and uj(X),vj(X) are the
left and right singular vectors of X = rank(X)
σj(X)uj(X)vj(X)⊤. Thus, ˆAλ has
a particularly simple form; it is obtained by soft thresholding of singular values in
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
the SVD of X. To see why (3.2) gives the solution of (3.1), note that, in view of
(2.1), the subdifferential of F(A) = ∥A−X∥2
2 +λm1m2∥A∥1 is the set of matrices
2(A −X) + λm1m2
where r,uj,vj,S1,S2 correspond to the SVD of A. Since A →F(A) is strictly
convex, the minimizer ˆAλ is unique, and the condition 0 ∈∂F( ˆAλ) is necessary
and sufﬁcient characterization of the minimum, where 0 is the zero m1 × m2 matrix. Considering
j : σj(X)<λm1m2/2
uj(X)vj(X)⊤,
it is easy to check that (3.2) satisﬁes this condition.
We will see that the soft thresholding representation (3.2) helps to understand
in an easy way some theoretical properties of ˆAλ. However, it may not be always
preferable for computational issues. Indeed, the standard techniques of computation of the SVD can become numerically instable when the dimension is high. On
the other hand, we can always compute ˆAλ from (3.1) using the methods of convex
programming free from this drawback.
In view of Theorem 1, to get the oracle inequalities in a closed form it remains
only to specify the value of regularization parameter λ such that λ ≥2∥M∥∞with
high probability. This requires some assumptions on the distribution of (Xi,Yi),
and the value of λ will be different under different assumptions. We will consider
only the following two cases of particular interest:
• Sub-exponential noise and matrices with uniformly bounded entries. There exist
constants σ,c1 > 0, α ≥1 and ˜c such that
i=1,...,nEexp
and maxi,j |a0(i,j)| ≤a for some constant a.
• Statistical learning setting. There exists a constant η such that maxi=1,...,n |Yi| ≤
η almost surely.
In both cases, we obtain the upper bounds for ∥M∥∞(that we call the stochastic
error) using the noncommutative Bernstein inequalities; cf. Section 6. The resulting values of λ and the corresponding oracle inequalities are given in the next two
Set m = m1 + m2. In what follows, we will denote by C absolute positive constants, possibly different on different occasions.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
THEOREM 3.
Let Xi be i.i.d. uniformly distributed on X , and the pairs
(Xi,Yi) be i.i.d. Assume that maxi,j |a0(i,j)| ≤a for some constant a, and that
condition (3.3) holds. For t > 0, consider the regularization parameter λ satisfying
λ ≥C(σ ∨a)max
t + log(m)
(m1 ∧m2)n, (t + log(m))log1/α(m1 ∧m2)
where C > 0 is a large enough constant that can depend only on α,c1, ˜c. Then
with probability at least 1 −3e−t we have
∥ˆAλ −A0∥2
2 + m1m2 min
m1m2λ2 rank(A)
for all A ∈Rm1×m2.
THEOREM 4.
Let Xi be i.i.d. uniformly distributed on X . Assume that
maxi=1,...,n |Yi| ≤η almost surely for some constant η. For t > 0 consider the
regularization parameter λ satisfying
t + log(m)
(m1 ∧m2)n, 2(t + log(m))
Then with probability at least 1 −e−t inequality (3.5) holds for all A ∈Rm1×m2.
Theorems 3 and 4 follow immediately from Theorem 1 and Lemmas 1, 2 and 3
in Section 6 with μ = √m1m2.
Note that the natural choice of t in Theorems 3 and 4 is of the order log(m),
since a larger t leads to slower rate of convergence, and a smaller t does not improve the rate but makes the concentration probability smaller. Note also that, under this choice of t, the second terms under the maxima in (3.4) and (3.6) are negligible for the values of n,m1,m2 such that the term containing rank(A0) in (3.5)
is meaningful. Indeed, if t is of the order log(m), the condition that m1m2λ2 ≪1
necessarily implies n ≫(m1 ∨m2)log(m). On the other hand, the negligibility of
the second terms under the maxima in (3.4) and (3.6) is approximately equivalent
to n > (m1 ∧m2)log1+2/α(m) and n > (m1 ∧m2)log(m), respectively. Based on
these remarks, we can choose λ in the form
(m1 ∧m2)n,
where c∗equals either σ ∨a or η and the constant C∗> 0 is large enough, and we
can state the following corollary that will be further useful for minimax considerations. Deﬁne τ > 0 by
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
where M = max(m1,m2), and m = m1 + m2.
COROLLARY 2.
Let one of the sets of conditions (i) or (ii) below be satisﬁed:
(i) The assumptions of Theorem 3 with λ as in (3.7), n > (m1 ∧m2) ×
log1+2/α(m), c∗= σ ∨a, and a large enough constant C∗> 0 that can depend
only on α,c1, ˜c.
(ii) The assumptions of Theorem 4 with n > 4(m1 ∧m2)log(m), λ as in (3.7),
c∗= η, and C∗= 4.
Then, with probability at least 1 −3/(m1 + m2),
∥ˆAλ −A0∥2
2 + τ 2 rank(A)
and, in particular,
∥ˆAλ −A0∥2
∗log(m)M rank(A0)
where M = max(m1,m2), and m = m1 + m2. Furthermore, with the same probability,
∥ˆAλ −A0∥2
τ 2−q∥A0∥q
(m1m2)q/2 .
Inequalities (3.8) and (3.9) are straightforward in view of Theorems 3
and 4. To prove (3.10) it sufﬁces to note that, for any κ > 0, 0 < q ≤2,
2 + κ2 rank(A)
min{κ2,σ 2
j (A0)} = κ2 
≤κ2−q∥A0∥q
Inequality (3.9) guarantees that the normalized Frobenius error (m1m2)−1∥ˆAλ−
2 of the estimator ˆAλ is small whenever n > C(m1 ∨m2)log(m)rank(A0) with
a large enough C > 0. This quantiﬁes the sample size n necessary for successful
matrix completion from noisy data.
Note that we can choose λ not necessarily equal but also greater or equal to the
right-hand side of (3.7), or equivalently, λ = tC∗c∗
(m1∧m2)n for any t ≥1. Then
the resulting oracle inequalities will remain of the same form with τ 2 multiplied
by the constant t2.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
Keshavan et al. ( , Theorem 1.1), under a sampling scheme different from
ours (sampling without replacement) and sub-Gaussian errors, proposed an estimator ˆA satisfying, with probability at least 1 −(m1 ∧m2)−3,
β log(n)M rank(A0)
where C > 0 is a constant, and β = (m1 ∨m2)/(m1 ∧m2) is the aspect ratio.
A drawback is that the construction of ˆA in requires the exact knowledge of
rank(A0) (although it does not seem to require the knowledge of a). Furthermore,
bound (3.11) is suboptimal for “very rectangular” matrices, that is, when β ≫1.
Candes and Plan provide a coarser bound than (3.11), not guaranteeing a simple consistency when n →∞whatever are M and rank(A0); see for more
detailed comments on .
4. Lower bounds.
In this section, we prove the minimax lower bounds showing that the rates attained by our estimator are optimal up to logarithmic factors.
The argument here is close to where the lower bounds are obtained on the
Schatten balls. However, we consider different classes that consist of matrices
with uniformly (in m1,m2,n) bounded entries. We cannot apply directly the lower
bounds of Theorem 6 in for USR matrix completion on the Schatten balls because they are achieved on matrices with entries, which are not uniformly bounded
for m1m2 ≫n.
We will need the following assumption, which is similar in spirit but, in general,
substantially weaker than the usual restricted isometry condition.
ASSUMPTION 2 (Restricted isometry in expectation).
For some 1 ≤r ≤
min(m1,m2) and some 0 < μ < ∞that there exists a constant δr ∈[0,1) such
(1 −δr)∥A∥2 ≤μ∥A∥L2() ≤(1 + δr)∥A∥2
for all matrices A ∈Rm1×m2 with rank at most r.
For the particular case of ﬁxed Xi (cf. Example 4 in the Introduction), Assumption 2 coincides with the matrix version of scaled restricted isometry with scaling
factor μ .
Inspection of the proof of Theorem 5 shows that it remains valid
if we replace 1 −δr and 1 + δr by arbitrary positive constants ν1 and ν2 such that
ν1 ≤ν2. We use the formulation involving δr only to ease parallels to the usual
restricted isometry condition.
We will denote by inf ˆA the inﬁmum over all estimators ˆA with values in
Rm1×m2. For any integer r ≤min(m1,m2) and any a > 0 we consider the class
of matrices
A0 ∈Rm1×m2 :rank(A0) ≤r,max
i,j |a0(i,j)| ≤a
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
For any A ∈Rm1×m2, let PA denote the probability distribution of the observations (X1,Y1,...,Xn,Yn) with E(Yi|Xi) = ⟨A,Xi⟩. We set for brevity M =
max(m1,m2).
THEOREM 5.
Fix a > 0 and an integer 1 ≤r ≤min(m1,m2). Let Assumption 2 be satisﬁed with some μ > 0. Assume that μ2r ≤nmin(m1,m2), and
that conditionally on Xi, the variables ξi are Gaussian N(0,σ 2), σ 2 > 0, for
i = 1,...,n. Then there exist absolute constants β ∈(0,1) and c > 0, such that
L2() > c(1 −δr)2(σ ∧a)2 Mr
Without loss of generality, assume that M = max(m1,m2) = m1 ≥
m2. For some constant 0 ≤γ ≤1 we deﬁne
˜A = (aij) ∈Rm1×r :aij ∈
0,γ (σ ∧a)
∀1 ≤i ≤m1,1 ≤j ≤r
and consider the associated set of block matrices
B(C) = {A = ( ˜A| ··· | ˜A| O ) ∈Rm1×m2 : ˜A ∈C},
where O denotes the m1 ×(m2 −r⌊m2/r⌋) zero matrix, and ⌊x⌋is the integer part
By construction, any element of B(C) as well as the difference of any two elements of B(C) has rank at most r and the entries of any matrix in B(C) take
values in [0,a]. Thus, B(C) ⊂A(r,a). Due to the Varshamov–Gilbert bound (cf.
Lemma 2.9 in ), there exists a subset A0 ⊂B(C) with cardinality Card(A0) ≥
2rm1/8 +1 containing the zero m1 ×m2 matrix 0 and such that, for any two distinct
elements A1 and A2 of A0,
γ 2(σ ∧a)2 μ2r
16 (σ ∧a)2 μ2m1r
In view of Assumption 2, this implies
L2() ≥(1 −δr)2 γ 2
16 (σ ∧a)2 m1r
Using that, conditionally on Xi, the distributions of ξi are Gaussian, we get that,
for any A ∈A0, the Kullback–Leibler divergence K(P0,PA) between P0 and PA
K(P0,PA) =
L2() ≤(1 + δr)2 γ 2
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
From (4.4) we deduce that the condition
Card(A0) −1
K(P0,PA) ≤α log
Card(A0) −1
is satisﬁed for any α > 0 if γ > 0 is chosen as a sufﬁciently small numerical
constant depending on α. In view of (4.3) and (4.5), the result now follows by
application of Theorem 2.5 in .
In the USR matrix completion problem we have ∥A∥2
L2() = (m1m2)−1∥A∥2
for all matrices A ∈Rm1×m2. Thus, the corresponding lower bound follows immediately from the previous theorem with δr = 0 and μ = √m1m2.
THEOREM 6.
Fix a > 0 and an integer r such that 1 ≤r ≤min(m1,m2),
Mr ≤n. Let the matrices Xi be i.i.d. uniformly distributed on X , and let, conditionally on Xi, the variables ξi be Gaussian N(0,σ 2), σ 2 > 0, for i = 1,...,n.
Then there exist absolute constants β ∈(0,1) and c > 0, such that
2 > c(σ ∧a)2 Mr
Comparing Theorem 6 with Corollary 2(i) we see that, in the case of Gaussian
errors ξi, the rate of convergence of our estimator ˆAλ given in (3.9) is optimal (up
to a logarithmic factor) in a minimax sense on the class of matrices A(r,a).
A similar conclusion can be obtained for the statistical learning setting. Indeed,
assume that the pairs (Xi,Yi) are i.i.d. realizations of a random pair (X,Y) with
distribution PXY belonging to the class
PA0,η = {PXY :X ∼0,|Y| ≤η (a.s.),E(Y|X) = ⟨A0,X⟩},
where 0 is the uniform distribution on X , 1 ≤r ≤min(m1,m2) is an integer and
THEOREM 7.
Let n,m1,m2,r be as in Theorem 5. Let (Xi,Yi) be i.i.d. realizations of a random pair (X,Y) with distribution PXY . Then there exist absolute
constants β ∈(0,1) and c > 0, such that
rank(A0)≤r
PXY ∈PA0,η
2 > cη2 Mr
We proceed as in the proof of Theorem 5 with some modiﬁcations.
Assuming that M = max(m1,m2) = m1 ≥m2 and 0 ≤γ ≤1/2 we deﬁne the class
of matrices
˜A = (aij) ∈Rm1×r :aij ∈
,∀1 ≤i ≤m1,1 ≤j ≤r
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
and take its block extension B(C′). Consider the joint distributions PXY such
that X ∼0 and, conditionally on X, Y = η with probability pA0(X) = 1/2 +
⟨A0,X⟩/(2η) and Y = −η with probability 1 −pA0(X) = 1/2 −⟨A0,X⟩/(2η),
where A0 ∈B(C′). It is easy to see that such distributions PXY belong to the class
PA0,η, and our assumptions guarantee that 1/4 ≤pA0(X) ≤3/4, rank(A0) ≤r for
all A0 ∈B(C′). We will denote the corresponding n-product measure by PA0. For
any A ∈B(C′), the Kullback–Leibler divergence between P0 and PA has the form
K(P0,PA) = nE
p0(X)log p0(X)
log 1 −p0(X)
Using the inequality −log(1 + u) ≤−u + u2/2, ∀u > −1, and the fact that 1/4 ≤
pA(X) ≤3/4, we ﬁnd that the expression under the expectation in (4.8) is bounded
by 2(p0(X) −pA(X))2. This implies
K(P0,PA) ≤n
The remaining arguments are analogous to those in the proof of Theorem 5.
5. Further results and examples.
5.1. Recovery of the rank and speciﬁc lower bound.
A notable property of the
estimator ˆAλ in matrix completion setting is that it has the same rank as the underlying matrix A0 with probability close to 1. As a consequence we can establish a
lower bound for the Frobenius error of ˆAλ with the rates matching up to constants
the upper bounds of Corollary 2.
THEOREM 8.
Let Xi be i.i.d. uniformly distributed on X , and let λ satisfy
the inequality λ ≥2∥M∥∞(as in Theorem 1). Consider the estimator ˆAλ′ with
λ′ = λ/(1 −δ) for some 0 < δ < 1. Set ˆr = rank( ˆAλ′). Then
ˆr ≤rank(A0).
If, in addition, minj : σj(A0)̸=0 σj(A0) ≥λ′m1m2, then
ˆr ≥rank(A0)
∥ˆAλ′ −A0∥2
4(1 −δ)2 rank(A0)(λm1m2)2.
Note that X −A0 = m1m2M. Using standard matrix perturbation argument (cf. , page 203), we get, for all j = 1,...,m1 ∧m2,
|σj(X) −σj(A0)| ≤σ1(X −A0) = m1m2∥M∥∞≤λm1m2
= (1 −δ)λ′m1m2
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
Since, by (3.2), σˆr(X) > λ′m1m2/2, we ﬁnd that σˆr(A0) > δλ′m1m2/2. This implies (5.1). Now, if σj(A0) ≥λ′m1m2 we get
σj(X) ≥σj(A0) −|σj(X) −σj(A0)| ≥λ′m1m2 −(1 −δ)λ′m1m2
and thus (5.2) follows.
To prove (5.3), denote by P :Rm1×m2 →Rm1×m2 the projector on the linear span of matrices (uj(X)vj(X)⊤,j = 1,...,r), where r = rank(A0). We
have ∥ˆAλ′ −A0∥2 ≥∥P( ˆAλ′ −A0)∥2 ≥∥P( ˆAλ′ −X)∥2 −∥P(X −A0)∥2. Here
∥P( ˆAλ′ −X)∥2 = √rλ′m1m2/2 in view of (3.2) and the fact that ˆr = r; cf. (5.1)
and (5.2). On the other hand, ∥P(X −A0)∥2 ≤√r∥M∥∞m1m2 ≤√rλm1m2/2.
This implies
∥ˆAλ′ −A0∥2 ≥
−(1 −δ)λ′m1m2
COROLLARY 3.
Let the assumptions of Corollary 2 be satisﬁed. Consider the
estimator ˆAλ′ with
for some 0 < δ < 1. Set ˆr = rank( ˆAλ′). Then ˆr ≤rank(A0) with probability at least
1 −3/(m1 + m2). If, in addition,
j : σj(A0)̸=0σj(A0) ≥C∗c∗
log(m)(m1 ∨m2)
then ˆr ≥rank(A0) and
∥ˆAλ′ −A0∥2
4(1 −δ)2 rank(A0)log(m)(m1 ∨m2)
with the same probability.
We note that the lower bound for σj(A0) in (5.4) is not excessively high, since
√m1m2 is a “typical” order of the largest singular value σ1(A0) for nonlacunary
matrices A0. For example, if all the entries of A0 are equal to some constant a, the
left-hand side of (5.4) is equal to σ1(A0) = a√m1m2.
5.2. Risk bounds in statistical learning.
The results of the previous sections
can be also extended to the traditional statistical learning setting where (Xi,Yi)
is a sequence of i.i.d. replications of a random pair (X,Y) with X ∈Rm1×m2 and
Y ∈R, and there is no underlying model determined by matrix A0; that is, we
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
do not assume that E(Y|X) = ⟨A0,X⟩. Then the above oracle inequalities can be
reformulated in terms of the prediction risk
R(A) = E[(Y −⟨A,X⟩)2]
∀A ∈Rm1×m2.
We illustrate this by an example dealing with USR matrix completion. Speciﬁcally,
Theorem 4 is reformulated in the following way.
THEOREM 9.
Let Xi be i.i.d. uniformly distributed on X . Assume that |Y| ≤η
almost surely for some constant η. For t > 0 consider the regularization parameter
λ satisfying (3.6). Then with probability at least 1 −e−t we have
R( ˆAλ) ≤R(A) + min
m1m2λ2 rank(A)
for all A ∈Rm1×m2. In particular, under the assumptions of Corollary 2(ii),
2η2 log(m)M rank(A)
This theorem can be also viewed as a result about the approximate sparsity. We
do not know whether the true underlying model is described by some matrix A0,
but we can guarantee that our estimator is not far from the best approximation
provided by matrices A with small rank or small nuclear norm.
Note that the results of Theorem 9 are uniform over the class of distributions
Pη = {PXY :X ∼0,|Y| ≤η (a.s.)},
where 0 is the uniform distribution on X , and η > 0 is a constant. The corresponding lower bound is given in the next theorem.
THEOREM 10.
Let n,m1,m2,r be as in Theorem 5. Let (Xi,Yi) be i.i.d. realizations of a random pair (X,Y) with distribution PXY . Then
R( ˆA) ≥R(A) + cη2 Mr
where β ∈(0,1) and c > 0 are absolute constants.
For E(Y|X) = ⟨A0,X⟩we have R(A) = ∥A −A0∥2
L2() + σ 2 =
(m1m2)−1∥A −A0∥2
2 + σ 2, where σ 2 = E[(Y −E(Y|X))2]. Thus, using Theorem 7 we get
R( ˆA) ≥R(A) + cη2 Mr
2 > cη2 Mr
Inequalities (5.7) and (5.8) imply minimax rate optimality of ˆAλ up to a logarithmic factor in the statistical learning setting.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
5.3. Risks bounds in spectral norm.
The results of the previous sections on the
Frobenius norm can be extended to the spectral norm. In this subsection we consider the USR matrix completion problem, that is, we assume that the matrices Xi
are i.i.d. uniformly distributed on X , which implies that ∥A∥2
2 = (m1m2)−1∥A∥2
for all matrices A ∈Rm1×m2.
THEOREM 11.
Let Xi be i.i.d. uniformly distributed on X . Consider the estimator ˆAλ deﬁned in (3.1). If λ ≥∥M∥∞, then
∥ˆAλ −A0∥∞≤3
∥ˆAλ −A0∥∞≤∥ˆAλ −X∥∞+ m1m2∥M∥∞,
where we recall that X = m1m2
i=1 YiXi, E(X) = A0, and M is deﬁned in (2.2).
In view of (3.2), we clearly have ∥ˆAλ −X∥∞≤λm1m2/2. The result follows
immediately since ∥M∥∞≤λ.
As a consequence of the above theorem, we can derive the optimal rate (up a
to logarithmic factor) of USR matrix completion for the spectral norm when the
noise is sub-exponential or in the statistical learning setting.
THEOREM 12.
Let one of the sets of conditions (i) or (ii) in Corollary 2 be
satisﬁed. Then, with probability at least 1 −3/(m1 + m2), we have
∥ˆAλ −A0∥∞≤CC∗c∗
(m1 ∨m2)logm
where C > 0 is an absolute constant.
The proof of this result is immediate by combining Theorem 11 and
Lemmas 1, 2 and 3.
THEOREM 13.
(i) Let the conditions of Theorem 6 be satisﬁed. Then
∥ˆA −A0∥∞> c(σ ∧a)√m1m2
where β ∈(0,1) and c > 0 are absolute constants.
(ii) Let the conditions of Theorem 7 be satisﬁed. Then
rank(A0)≤r
PXY ∈PA0,η
∥ˆA −A0∥∞> cη√m1m2
where β ∈(0,1) and c > 0 are absolute constants.
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
Note ﬁrst that, in the USR matrix completion problem, Assumption 2
is satisﬁed with δr = 0 and μ = √m1m2.
We prove part (i) of the theorem. Consider the set of matrices A0 introduced in
the proof of Theorem 5. For any two distinct matrices A1,A2 of A0, we have
∥A1 −A2∥∞≥
16(σ ∧a)√m1m2
Indeed, if (5.11) does not hold, we get
2 ≤rank(A1 −A2)∥A1 −A2∥2
16(σ ∧a)2m1m2
since rank(A1 −A2) ≤r by construction of A0. This contradicts (4.3).
Next, (4.5) is satisﬁed for any α > 0 if γ > 0 is chosen as a sufﬁciently small
numerical constant depending on α.
Combining (5.11) with (4.5) and Theorem 2.5 in gives the result.
The proof of (ii) follows the same arguments.
5.4. Sharp oracle inequalities for the Lasso.
As we already mentioned in Example 4 and in the remark after Theorem 2, one can exploit (2.19) to derive sparsity
oracle inequalities for the usual Lasso. This is detailed in the present subsection. It
is noteworthy that the obtained inequalities are sharp (i.e., with leading constant 1),
which was not achieved in the previous work on the Lasso.
Note that, if m1 = m2 = p and A and Xi are diagonal matrices, then the trace
regression model (1.2) becomes
i = 1,...,n,
where xi,β∗∈Rp denote the vectors of diagonal elements of Xi,A0, respectively.
Set X = (x1,...,xn)⊤∈Rn×p to be the design matrix of this linear regression
model. For a vector z = (z(1),...,z(d)) ∈Rd, deﬁne |z|q = (d
j=1 |z(j)|q)1/q for
1 ≤q < ∞and |z|∞= max1≤j≤d |z(j)|.
Assume in what follows that xi are ﬁxed and p ≥2. Then for A = diag(β) we
L2() = n−1|Xβ|2
2, where diag(β) denotes the diagonal p×p matrix with
the components of β on the diagonal. We will assume without loss of generality
that the diagonal elements of the Gram matrix 1
nX⊤X are not larger than 1 (the
general case is obtained from this by simple rescaling).
The estimator ˆAλ deﬁned in (1.7) becomes the usual Lasso estimator
ˆβλ = argmin
i β)2 + λ|β|1
For a vector β ∈Rp, we set, with a little abuse of notation, μc0(β) =
μc0(diag(β)), μ(β) = μ5(β). Let M(β) denote the number of nonzero components of β.
For simplicity, the result is stated only in the case of Gaussian noise.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
THEOREM 14.
Let ξi be i.i.d. Gaussian N(0,σ 2), and let the diagonal elements of matrix 1
nX⊤X be not larger than 1. Take
where C = 3b
2,b ≥1. Then, with probability at least 1 −
pb2−1√π logp, we have
n|X( ˆβλ −β∗)|2
n|X(β −β∗)|2
2 + C2σ 2 μ2(β)M(β)logp
Combine Theorem 2 and a standard bound on the tail of the Gaussian
distribution, which assures that with probability at least 1 −
pb2−1√π logp,
Given β ∈Rp and J ⊂{1,...,p}, denote by βJ the vector in Rp which has the
same coordinates as β on J and zero coordinates on the complement J c of J.
We recall the restricted eigenvalue condition of :
CONDITION RE(s,c0).
For some integer s such that 1 ≤s ≤p, and a positive
number c0 the following condition holds:
J⊆{1,...,p},
u∈Rp,u̸=0,
|uJc|1≤c0|uJ |1
We have the following corollary.
COROLLARY 4.
Let the assumptions of Theorem 14 hold, and let condition RE(s,5) be satisﬁed for some 1 ≤s ≤p. Then, with probability at least
pb2−1√π logp,
n|X( ˆβλ −β∗)|2
β∈Rp : M(β)≤s
n|X(β −β∗)|2
Recall that ej(p) denote the canonical basis vectors of Rp. For any
p × p diagonal matrix A = diag(β) with support (S1,S2), S1 = S2 = {ej(p),j ∈
J}, where J ⊂{1,...,p} has cardinality |J| ≤s, and an arbitrary p × p diagonal
matrix B = diag(u), where u ∈Rp, we have
∥PA(B)∥1 = |uJ|1,
∥PA(B)∥2 = |uJ |2,
A (B)∥1 = |uJ c|1
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
CA,c0 = {diag(u):u ∈Rp,|uJ c|1 ≤c0|uJ |1},
∥B∥L2() = 1
B̸=0: B∈CA,c0
u∈Rp,u̸=0,
|uJc|1≤c0|uJ |1
Since Condition RE(s,5) is satisﬁed, Theorem 14 yields the result.
Oracle inequalities (5.12) and (5.13) extend straightforwardly to
Yi = fi + ξi,
i = 1,...,n,
where fi are arbitrary ﬁxed values and not necessarily fi = x⊤
i β∗. This setting is
interesting in the context of aggregation. Then x1,...,xn are vectors of values of
some given dictionary of p functions at n given points, and fi are the values of an
unknown regression function at the same points. Under model (5.14), inequalities
(5.12) and (5.13) hold true with the only difference that Xβ∗should be replaced
by the vector f = (f1,...,fn)⊤. With such a modiﬁcation, (5.13) improves upon
Theorem 6.1 of where the leading constant is greater than 1.
6. Control of the stochastic error.
In this section, we obtain the probability
inequalities for the stochastic error ∥M∥∞. For brevity, we will write throughout
∥·∥∞= ∥·∥. The following proposition is an immediate consequence of the matrix
version of Bernstein’s inequality due to (Corollary 9.1 in ).
PROPOSITION 1.
Let Z1,...,Zn be independent random matrices with dimensions m1 × m2 that satisfy E(Zi) = 0 and ∥Zi∥≤U almost surely for some
constant U and all i = 1,...,n. Deﬁne
Then, for all t > 0, with probability at least 1 −e−t we have
Z1 + ··· + Zn
 ≤2max
t + log(m)
,U t + log(m)
where m = m1 + m2.
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
Furthermore, it is possible to replace the L∞-bound U on ∥Z∥in the above
inequality by bounds on the weaker ψα-norms of ∥Z∥deﬁned by
= inf{u > 0:Eexp(∥Z∥α/uα) ≤2},
PROPOSITION 2.
Let Z,Z1,...,Zn be i.i.d. random matrices with dimensions m1 × m2 that satisfy E(Z) = 0. Suppose that U(α)
< ∞for some α ≥1.
Then there exists a constant C > 0 such that, for all t > 0, with probability at least
Z1 + ··· + Zn
 ≤C max
t + log(m)
1/α t + log(m)
where m = m1 + m2.
This is an easy consequence of Proposition 2 in , which provides an analogous result for Hermitian matrices Z. Its extension to rectangular matrices stated
in Proposition 2 is straightforward via the self-adjoint dilation; cf., for example,
the proof of Corollary 9.1 in .
The next lemma gives a control of the stochastic error for USR matrix completion in the statistical learning setting.
Let Xi be i.i.d. uniformly distributed on X . Assume that
maxi=1,...,n |Yi| ≤η almost surely for some constant η. Then for any t > 0 with
probability at least 1 −e−t we have
∥M∥≤2η max
t + log(m)
(m1 ∧m2)n, 2(t + log(m))
We apply Proposition 1 with Zi = YiXi −E(YiXi). Recall that here
Xi are i.i.d. with the same distribution as X and Yi are not necessarily i.i.d. Observe
Therefore, ∥Zi∥≤2η, σZ ≤ησX, and the result follows from Proposition 1.
We now consider the USR matrix completion with sub-exponential errors. Recall that in this case we assume that the pairs (Xi,Yi) are i.i.d. We have
YiXi −E(YiXi)
0 Xi)Xi −E(tr(A⊤
= 1 + 2.
NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES
We treat the terms 1 and 2 separately in the two lemmas below.
Let Xi be i.i.d. uniformly distributed on X , and the pairs (Xi,Yi)
be i.i.d. Assume that condition (3.3) holds. Then there exists an absolute constant
C > 0 that can depend only on α,c1, ˜c and such that, for all t > 0, with probability
at least 1 −2e−t we have
1 ≤Cσ max
t + log(m)
(m1 ∧m2)n, (t + log(m))log1/α(m1 ∧m2)
Observe ﬁrst that for ˜X = X −E(X) we have
ξi(Xi −EXi)
ξi(Xi −EX)
Set Zi = ξi(Xi −EX). These are i.i.d. random matrices having the same distribution as a random matrix Z. It follows from (6.2) that ∥Zi∥≤2|ξi|, and thus
condition (3.3) implies that U(α)
≤cσ for some constant c > 0. Furthermore, in
view of (6.4), we have σZ ≤c′σσ ˜X = c′σ/(m1 ∧m2)1/2 for some constant c′ > 0
and σZ ≥c1/2
σ/(2(m1 ∧m2))1/2. Using these remarks we can deduce from Proposition 2 that there exists an absolute constant ˜C > 0 such that for any t > 0 with
probability at least 1 −e−t we have
ξi(Xi −EX)
t + log(m)
1/α t + log(m)
t + log(m)
(m1 ∧m2)n, (t + log(m))log1/α(m1 ∧m2)
Finally, in view of Condition (3.3) and Bernstein’s inequality for sub-exponential
noise, we have for any t > 0, with probability at least 1 −e−t,
""""" ≤Cσ max
V. KOLTCHINSKII, K. LOUNICI AND A. B. TSYBAKOV
where C > 0 depends only on ˜c. We complete the proof by using the union bound.
|A0|∗= max
i,j |a0(i,j)|√m1 ∨m2.
Let Xi be i.i.d. random variables uniformly distributed in X . Then,
for all t > 0, with probability at least 1 −e−t we have
t + log(m)
i,j |a0(i,j)|t + log(m)
If maxi,j |a0(i,j)| ≤a for some a > 0, then with the same probability
2 ≤2a max
t + log(m)
(m1 ∧m2)n, 2(t + log(m))
We apply Proposition 1 for the random variables Zi = tr(A⊤
0 X)X). Using (6.2) we get ∥Zi∥≤2maxi,j |a0(i,j)| and
Z ≤max{∥E(⟨A0,X⟩2XX⊤)∥,∥E(⟨A0,X⟩2X⊤X)∥} ≤|A0|2
Thus, (6.6) follows from Proposition 1.