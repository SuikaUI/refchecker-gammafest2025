Adaptive Locality Preserving Regression
Jie Wen, Zuofeng Zhong, Zheng Zhang, Lunke Fei∗, Zhihui Lai, Runze Chen
Abstract—This paper proposes a novel discriminative regression method, called adaptive locality preserving regression
(ALPR) for classiﬁcation. In particular, ALPR aims to learn
a more ﬂexible and discriminative projection that not only
preserves the intrinsic structure of data, but also possesses the
properties of feature selection and interpretability. To this end,
we introduce a target learning technique to adaptively learn a
more discriminative and ﬂexible target matrix rather than the
pre-deﬁned strict zero-one label matrix for regression. Then a
locality preserving constraint regularized by the adaptive learned
weights is further introduced to guide the projection learning,
which is beneﬁcial to learn a more discriminative projection
and avoid overﬁtting. Moreover, we replace the conventional
‘Frobenius norm’ with the special l2,1 norm to constrain the
projection, which enables the method to adaptively select the
most important features from the original high-dimensional data
for feature extraction. In this way, the negative inﬂuence of
the redundant features and noises residing in the original data
can be greatly eliminated. Besides, the proposed method has
good interpretability for features owning to the row-sparsity
property of the l2,1 norm. Extensive experiments conducted on
the synthetic database with manifold structure and many realworld databases prove the effectiveness of the proposed method.
Index Terms—Linear regression, projection learning, adaptive
locality preserving, supervised graph regularization.
I. INTRODUCTION
EGRESSION analysis focuses on estimating the relationships among the dependent variables and independent
variables, which has aroused much attention in ﬁelds of
machine learning . For supervised classiﬁcation, one of
the major tasks is to learn a proper mapping that precisely
transforms the training data into their labels. To this end,
various regression analysis methods have been proposed over
the past decades, such as the ridge regression , partial
least squares , modiﬁed minimum squared error ,
This work was supported in part by the National Natural Science Foundation
of China under Grant nos. 61702110, 61702117 and 61703169, and in
part by Technology Program of Guangzhou under Grant no. 201804010355.
(Jie Wen and Zuofeng Zhong are co-ﬁrst authors with equal contributions.)
(Corresponding author: Lunke Fei.)
Jie Wen and Lunke Fei are with the School of Computer Science
and Technology, Guangdong University of Technology, Guangzhou 510006,
Guangdong, China. (Email: jiewen ; )
Zuofeng Zhong is with the College of Computer Science and Software
Engineering, Shenzhen University, Shenzhen 518055, Guangdong, China, and
is also with the Institute of Textiles and Clothing, The Hong Kong Polytechnic
University, Hong Kong. (Email: )
Zheng Zhang is with the School of Information Technology & Electrical
Engineering, The University of Queensland, Brisbane, QLD 4072, Australia.
(Email: )
Zhihui Lai is with the College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518055, Guangdong, China. (Email:
lai zhi )
Runze Chen is with the School of Computer Science and Technology,
Harbin Institute of Technology, Shenzhen, Shenzhen 518055, China. (Email:
 )
and least square regularized regression , etc. Besides,
many kernel based regression methods, such as kernel ridge
regression and support vector regression , have also
been proposed for the non-separable cases.
Most of the conventional methods prefer to exploit the predeﬁned zero-one label matrix as the regression target. However, this simple label matrix is not the optimal discriminative
target for supervised classiﬁcation . First, it limits
the ﬂexibility of projection learning because it is too strict.
Second, it cannot push samples of different classes far away
because distances of the correct and incorrect label vectors are
constant (i.e.,
2) in the target space. To solve the problem,
many researchers proposed to learn a more discriminative
regression target rather than used the strict zero-one label
matrix for regression analysis. For example, in , the εdragging technique is introduced to adaptively enlarge the
distance between the correct and incorrect classes. In ,
a sparse error term is also introduced to relax the strict label
matrix, which in turn improves the ﬂexibility of regression
analysis. Zhang et al. proposed a very novel target learning
technique, in which the target matrix is adaptively learned with
large margins between different classes during the regression
and projection learning .
In most cases, these improved regression based methods
can learn a more discriminative projection and obtain a better
performance. However, for data with manifold structure or
noises, these methods will fail. This is mainly because that
(1) these methods only focus on minimizing the regression
errors while ignoring the intrinsic geometric structure of data,
which may destroy the structure of the original data and lead
to overﬁtting; (2) All features including the important features
and noises are treated equally in these methods, which cannot
guarantee these methods to obtain a clean projection.
In recent years, many researchers have also discovered
the ﬁrst issue and have made many efforts to address it.
For instance, the low-rank linear regression (LRLR) tries to
uncover the low-rank structure hidden in the high-dimensional
data for regression . In , a novel inter-class sparse
constraint is introduced, which tries to guarantee the common
structure with respect to each class. Besides, integrating the
manifold learning into the regression framework is a wellreceived approach to avoid overﬁtting . Xue et al.
constructed two graphs according to the label information
and local nearest neighbor information to guide the projection
learning in regression analysis . In , a strict label
based graph is introduced to regularize the projection, which
allows samples of the same class to be pulled together.
Compared with the ﬁrst two mentioned methods, those manifold learning based methods can preserve the local structure
of data better. However, these manifold based methods still
have the following shortcomings: (1) all graphs exploited to
 
guide the projection learning are constructed independently
with the regression in advance, which cannot guarantee the
global optimal projection. (2) These methods are sensitive
to noise. When data contain noises, the constructed graph
will be incorrect. In this case, it is obviously impossible
to learn a discriminative projection with the guiding of the
incorrect graph. (3) These methods cannot preserve the same
nearest neighbor ranks as the original data since they exploit
the same weight, i.e., 1, to regularize all nearest neighbors
while ignoring the differences of similarity degree among these
nearest neighbor pairs. In other words, they cannot preserve
the intrinsic nearest neighbor structure of data.
In this paper, we propose a novel and effective method to
solve the above problems and learn a more discriminative
projection for classiﬁcation. Specially, the proposed method
introduces a novel graph regularization term into the regression
framework, in which the graph is adaptively learned in a supervised style and then in turn guides the projection learning.
In this way, the proposed method has the potential to learn the
global optimal projection that not only can ﬁt the label well,
but also can preserve the intrinsic nearest neighbor structure of
each class. To improve the ﬂexibility of projection learning, the
retargeted learning technique is introduced to our model. Most
importantly, a row-sparsity norm instead of the conventional
‘Frobenius’ norm is introduced to constrain the projection,
which enables the method to reduce the negative inﬂuence
of the redundant features and noises. By artfully integrating
the above terms into one regression framework, the proposed
method is encouraged to perform better. In summary, our work
has the following contributions:
(1) We propose a novel regression framework that integrates
the adaptive locality preserving, feature selection, and discriminative target learning. Compared with the other methods, the
proposed method can learn a more reasonable and discriminative projection and avoid the overﬁtting problem.
(2) We introduce a novel supervised graph learning and
embedding constraint, which can discover the intrinsic local
geometric structures of data and adaptively learn the similarity
weights to nearest neighbor pairs. By exploiting the reliable
local geometric information to regularize the projection, the
proposed method has the potential to preserve the intrinsic
nearest neighbor structure of data.
(3) The proposed method can adaptively select the most
discriminative features for regression by introducing a rowsparsity constraint. This allows the method to reduce the
negative inﬂuence of noise and improves the interpretability
of projection.
This paper is an extended work of our conference paper
 . Compared with our previous version in , (1) we add
more experiments and analyses to prove its effectiveness and
convergence; (2) We give deep analyses to the computational
complexity, convergence, and demonstrate its superior properties through theoretically comparing some related methods;
(3) We analyze the parameter selection in detail; (4) Some
Theorems and propositions are provided for readers to better
understand our paper; (5) A ﬁgure has also been added to
show our method.
The paper is organized as follows: In Section II, some
notations and several related works are brieﬂy described. In
Section III, we mainly present the proposed method and
its optimization processes. Section IV analyzes the proposed
method in depth. Section V conducts several experiments to
prove the effectiveness of the proposed method. Section VI
offers the conclusion of the paper.
II. RELATED WORK
A. Notations
For convenience, some notations used through the paper are
brieﬂy described in this section. In our paper, matrix and vector
are denoted by the uppercase letter (e.g. X) and lowercase
letter (e.g. x), respectively. For a matrix X, we use Xi,j to
denote its ith row and jth column element, and use Xi.: and
X:,j to represent its ith row vector and jth column vector,
respectively. Some typical norms of matrix X ∈Rm×n, such
as the ‘Frobenius norm’ (i.e., ||X||F ), nuclear norm (i.e.,
||X||∗), l1 norm, and l2,1 norm, are deﬁned as: ∥X∥F =
i,j, ∥X∥∗= P
i |δi|, ∥X∥1 =
|Xi,j|, and
i,j, respectively, where δi is the ith
singular value of matrix X . For a vector x with m
elements, its l2 norm is deﬁned as ∥x∥2 =
(xi)2, where
xi denotes its ith element. The trace operation of matrix is
denoted by Tr(·). I denotes the identity matrix. 1 is a column
vector, where all elements are 1. X−1 and XT are the inverse
matrix and transposed matrix of X , respectively.
B. Linear regression and retargeted least square regression
Linear regression (LR) is one of the most popular supervised
classiﬁcation methods in ﬁelds of machine learning. The
objective function of LR is generally formulated as follows
F + λ ∥W∥2
where matrix X ∈Rm×n denotes the training set, where each
column vector represents a sample, m and n denote the feature dimension and number of training samples, respectively.
Y ∈Rn×C is the label matrix, in which the ith row vector
represents the label of the ith sample in the training set X,
C is the class number of the training set. In the conventional
LR, label matrix Y is generally deﬁned as a special zeroone matrix according to the class information of samples as
follows: if the ith sample comes from the jth class, then only
Yi,j = 1, and all the other elements Yi,k = 0, k ̸= j. λ is
a penalty parameter. W is the transformation (or projection)
for label prediction. When the projection W is obtained by
solving (1), the class label of any test sample y ∈Rm×1 can
be predicted via k = arg max
ith element of vector (yT W).
In , Zhang et al. pointed out that using the strict zero-one
label matrix as the regression target is harmful to classiﬁcation and limits the ﬂexibility in the discriminative projection
learning. To address this issue, Zhang et al. proposed the
retargeted least square regression (ReLSR), which seeks to
jointly learn a more discriminative and ﬂexible regression
target and projection in one framework as follows:
F s.t. Ti,li −max
j̸=li Ti,j ≥1 (2)
where li is the true class index of the ith sample.
We can ﬁnd that in ReLSR, the margins of the correct and
incorrect classes are all larger than 1, which encourages it to
increase the separability of data in the target space. Moreover,
the adaptively learned target matrix provides more ﬂexibility
to learn the discriminative projection.
C. Discriminatively regularized least-squares
To pull samples of the same class closer and push samples of
different classes far away as much as possible, Xue et al. 
proposed a graph regularized method, named discriminatively
regularized least-squares (DRLS). DRLS explores the underlying geometric knowledge of the original data to guide the
projection learning of linear regression, in which two graphs,
i.e., intra-class graph W w and inter-class graph W b, are preconstructed and regularized on the projection. The objective
function of DRLS is formulated as follows:
2W T X (ηLw −(1 −η) Lb) XT W (3)
where η (η ∈ ) is a penalty parameter, Lw and Lb are
the Laplacian matrices of graph W w and W b, respectively.
For a non-negative graph S, its Laplacian matrix is deﬁned
as L = D −S+ST
, where matrix D is a diagonal matrix
with the ith diagonal element as Di,i = P
In DLSR, the intra-class graph W w and inter-class graph W b
are respectively constructed as follows according to the nearest
neighbor information and label information of data:
if X:,j ∈Nw (X:,i) or X:,i ∈Nw (X:,j)
if X:,j ∈Nb (X:,i) or X:,i ∈Nb (X:,j)
where Nw (X:,i) denotes the sample set which is composed
of the nearest neighbors with the same class to sample X:,i,
Nb (X:,i) denotes the sample set which is composed of the
nearest neighbors with different classes to sample X:,i.
III. THE PROPOSED METHOD
As we all know, the original data contains much useful
information, such as the given label information and the
underlying geometric information residing in the data .
If we can appropriately utilize these information to guide the
projection learning, then a better classiﬁcation performance
will be obtained. This illustrates that how to well discover and
explore these information is crucial to learn a more compact
and discriminative projection for accurately separating these
samples. Based on this motivation, we propose a simple but effective method, named adaptive locality preserving regression
(ALPR), to learn a more discriminative and compact projection
for classiﬁcation. Fig.1 shows the ﬂowchart of the proposed
A. Model of the proposed method
Generally, if two samples are nearest neighbors in the
original data space, they will be more possible from the same
class. Naturally, we also expect that these nearest neighbor
relationships can be still preserved in the subspace . To this
end, many methods have been proposed, such as the locality
preserving projections and anchorgraph-based locality
preserving projection , etc. Besides, in the branch of LR,
many improved methods have also been proposed by introducing the nearest neighbor information . For instance,
DRLS expect the nearest neighbor samples with the same
class label to be pulled closer by embedding two graphs .
Large amount experiments proved that introducing the nearest
neighbor information has the potential to further improve the
classiﬁcation performance. However, there are two issues in
these conventional LR methods. First, they generally utilize
the pre-constructed graph to guide the projection learning,
which cannot guarantee the global optimal projection for
regression. Second, they ignore the differences of similar
degree among these nearest neighbor samples with the same
class label since all nearest neighbor pairs are regularized with
the same weight (e.g. 1). This will destroy the original nearest
neighbor order in each local space. For example, suppose x2
and x3 are the 1th and 2nd nearest neighbors of sample x1,
respectively. For the conventional graph regularized methods,
their transformed samples, i.e., W T x2 and W T x3 may not
still be the 1th and 2nd nearest neighbors of sample W T x1 in
the discriminant subspace since their regularized weights are
the same. Therefore, using the ﬁxed weight to regularize all
nearest neighbor pairs is not appropriate. A more reasonable
approach is to assign different weights for different nearest
neighbor pairs according to their similar degrees. In other
words, it is better to give a relatively larger weight to the
most nearest neighbor pairs. To this end, we introduce the
following graph regularization term to explore the geometric
information of data:
j,k=1,k̸=j
+∥Y −XT W∥2
j,k = 1, Si
where X ∈Rm×n and Y ∈Rn×C are the given training set
and corresponding label matrix, C is the class number, ni is
the sample number of the ith class. xi
k denote the jth
and kth sample of the ith class, respectively. Accordingly, Si
is the weight to regularize the distance of the corresponding
samples xi
k in the discriminant subspace. λ1 is the
penalty parameter.
introducing
constraint
j,k = 1, Si
j,k ≥0, model (6) treats all classes
equally in preserving their own nearest neighbor structures
and avoids the trivial solution to weight graph S . In
addition, all weights are adaptively learned from the latent
discriminant subspace rather than the complex original space,
Row-sparsity
Projection W
Training set
Target matrix
Supervised local
graph learning
Graph embedding
Regression
Projected samples
Fig. 1: The framework of ALPR. ALPR improves the discriminability of the projection by simultaneously considering the
following three crucial techniques: 1) introducing the retargeted learning technique to improve the ﬂexibility of regression; 2)
imposing the row-sparse constraint on the projection to select the most important features; 3) introducing a novel supervised
graph learning and embedding technique to preserve the intrinsic local structure of data.
which enables the method to capture the intrinsic nearest
neighbor relationships of samples.
As analyzed in the previous section and proved in many
references , exploiting a more ﬂexible target
matrix with large margins between the incorrect and correct
classes is very beneﬁcial to improve the discriminability of
the projection. Owing to the ﬂexibility and simplicity of the
retargeted learning approach , we exploit it to replace the
strict zero-one target matrix and rewrite our model as follows:
W,S,T ∥T −XT W∥2
k,j=1,k̸=j
s.t. Ti,li −max
j̸=li Ti,j ≥1,
j,k = 1, Si
where T ∈Rn×C is the target matrix, li ∈{1, ..., C} is the
true label index of the ith sample.
In most cases, data acquired in the real-world applications
usually have high dimensionality and many redundant features
even noises. These redundant features or noises residing in
the original data are useless even harmful to model training.
To address the issue, we further impose a row-sparsity norm
constraint on the projection and as a result the ﬁnal model is:
F + λ2∥W∥2,1
k,j=1,k̸=j
s.t. Ti,li −max
j̸=li Ti,j ≥1,
jk = 1, Si
where λ2 is also a penalty parameter.
Proposition 1: Introducing the constraint λ2∥W∥2,1 allows
the learned projection W to simultaneously perform feature
selection and feature extraction.
Proof: As presented in Section II, ∥W∥2,1 is deﬁned as
i,j, which is obviously equivalent to
the l1 norm constraint ∥a∥1 =
ai, where ai = ∥Wi,:∥2 =
i,j. According to the theory of sparse representation,
minimizing the optimization problem constrained by the l1
norm will enforce some elements of the corresponding vector
to zero . In other words, some elements of vector a
will be enforced to zero. Accordingly, if element ai is enforced
to zero, we can deduce that all elements corresponding to
the ith row of matrix W will be assigned as zero because
i,1 + . . . + W 2
i,C = 0. This demonstrates that
minimizing problem (8) will adaptively enforce some rows of
the projection W to zero. According to the basic rule of matrix
multiplication, if all elements of the ith row of projection W
are zero, then the corresponding ith feature of any sample will
make no contribution to the generation of the new features
during the linear combination. That is to say, these features
corresponding to the rows of matrix W with all zero values
are not selected during the feature extraction. So we conclude
that introducing the sparse constraint λ2∥W∥2,1 to the model
allows the method to earn the feature selection property. Thus
we complete the proof.
Proposition 1 allows the method to select the important
features for classiﬁcation and eliminate the negative inﬂuence
of noises or redundant features. Besides, imposing the l2,1
norm on the projection can improve its interpretability .
B. Solution to the proposed method
It is obvious that problem (8) has no analytical solution
since it contains three variables, i.e., W, S, T, in one problem.
In this section, we provide an efﬁcient iterative algorithm to
obtain their local optimal solutions as follows.
Step 1: Calculate variable W. For convenience ,we deﬁne
F + λ2∥W∥2,1
k,j=1,k̸=j
Problem (9) can be simpliﬁed as follows:
+ λ2∥W∥2,1 (10)
k,j=1,k̸=j
Then we can obtain the optimal W by setting the derivative
of L(W) with respect to W to zero as follows:
+ λ1SW W + λ2
XXT + λ1SW + λ2
where D ∈Rm×m is a diagonal matrix and its each diagonal
element Di,i = 1
Step 2: Calculate variable S. Fixing variables T and W,
variable S can be obtained by solving the following problem:
k,j=1,k̸=j
j,k = 1, Si
Problem (12) can be further simpliﬁed into the following
subproblems:
Theorem 1 . Given a positive vector b ∈R1×n (∀i
(1 ≤i ≤n), bi > 0), problem
i bi has the
optimal solution as ai = 1
Proof. The detailed proof process is moved to the Appendix
A in the supplementary material.
According to Theorem 1, we can obtain the optimal solution to problem (13):
j −W T xip
Step 3: Calculate variable T. Fixing variables W and S,
the subproblem to variable T is as follows:
Problem (15) is a typical constrained quadratic programming problem .
Theorem 2 . For any given vector g = [g1, g2, . . . , gn],
the optimal solution of problem
i̸=h ti ∥t −g∥2
gi + min (∆−vi, 0) ,
1 + gi −gh,
viΥ(Γ′(vi),0)
Υ(Γ′(vi),0).
Υ (·) is a logic function and has the following deﬁnation
Υ (a, b) =
otherwise . Function Γ′ (x) = 2(x +
min (x −vi, 0)).
Proof: Please refer to Appendix B in supplementary material for the completed proof process.
It is obvious that problem (15) can be decomposed into the
following subproblems
Ti,j≥1 ∥Ti,: −Gi,:∥2
According to Theorem 2, we can fast obtain the optimal
solution Ti,: of problem (17). By computing all rows of T
separately according to (17) and Theorem 2, the optimal
solution T can be ﬁnally obtained.
Algorithm 1 summarizes the completed optimization steps
of our method.
Algorithm 1 : ALPR (solving problem (8))
Input: Data matrix X ∈Rm×n, label matrix Y ∈Rn×C,
class number C, parameters λ1, λ2.
Initialization: Matrix W ∈Rm×C with random values;
T = Y , Graph S is constructed as the k-nearest neighbor
graph with binary values.
while not converged do
1. Update W by using (11);
2. Update all elements of S by using (14);
3. Update all rows of T according to Theorem 2.
Output: W, S, T
C. Classiﬁcation based on ALPR
After obtaining the projection W, LR based methods transform all the original data into the discriminant subspace via
W T X for the subsequent classiﬁcation. It is undoubted that
each projected sample can be viewed as its new representation,
which is obtained by the weighted linear combination of
projection W and all original features of the corresponding
sample. However, this simple weighted combination approach
may magnify the negative inﬂuence of noises or redundant
features residing in the high-dimensional data. Thanks to the
feature selection property of the used l2,1 norm constraint, our
method has the potential to discover the discriminability of
different features. As analyzed in Proposition 1, by imposing
l2,1 norm on the projection, some rows of projection W
corresponding to the unimportant features will be adaptively
assigned very small values even 0. Meanwhile, these rows
also have very small l2 norm values. This illustrates that the
discriminability of each feature can be commonly measured
by the l2 norm of the corresponding row of projection W
 . Thus we can enforce those rows to zero directly to
further eliminate the negative inﬂuence of noises and redundant features. Inspired by this motivation, we provide the
following approach listed in Algorithm 2 to implement the
classiﬁcation, in which a threshold value ρ is set to select
those discriminative features.
Algorithm 2 : Classiﬁcation based on ALPR
Input: Training data X ∈Rm×n and label L ∈Rn,
projection W ∈Rm×c, test sample a ∈Rm, threshold
• Obtain the discriminability pi of each feature by calculating the l2 norm of the corresponding row vector of
• Force all elements of the rows of the projection W that
satisfy pi < ρ to zero.
• Project all training sample and test sample into the
target space via the new projection W and use the
nearest neighbor classify to recognize the test sample.
Output: The label of the test sample.
IV. ANALYSIS OF THE PROPOSED METHOD
A. Computational complexity
In this section, we mainly analyze the computational complexity of the proposed optimization algorithm listed in Algorithm 1. For simplicity, we do not consider the computational complexities of some simple matrix operations, such as
matrix addition, subtraction, multiplication, and element-wise
based matrix division since these operations can be efﬁciently
computed. Overall, there are three main variables, i.e., W, S,
and T, to calculate from Algorithm 1. In Step 1, i.e., updating
variable W, the major computational cost is the matrix inverse
operation which has the computational complexity of O
for an m × m matrix . Thus the total computational
complexity of Step 1 is about O
. For Steps 2 and 3, it is
obvious that these two steps only contain some simple matrix
operations and thus their computational complexities can be
ignored. In summary, the total computational complexity of the
optimization approach listed in Algorithm 1 is about O
where τ is the iteration number.
B. Convergence analysis
From the previous presentation, all variables can be simply
calculated with the closed form solutions in their own subproblems. Meanwhile, we can prove the following proposition.
Proposition 2: For the optimization problem (8), each
subproblem is convex with respect to variables W, S, T, respectively.
Proof: From (9), the subproblem to variable W is a typical
sparse constraint optimization problem. Because the sparse l2,1
norm is a convex function , thus problem (9) is a convex
optimization subproblem. For variable S, it is obvious that
all constraints with respect to variable S are convex, and thus
subproblem (12) is also convex . The subproblem (16)
with respect to variable T is also a typical convex problem,
i.e., convex constraint quadratic programming problem .
Therefore, all subproblems with respect to variables W, S, T
are convex, respectively. Thus we complete the proof.
Based on the proposition 2, we can derive the following
Theorem 3 to the presented Algorithm 1 .
Theorem 3: The optimization approach presented in Algorithm 1 monotonically decreases the objective value of
problem (8).
Proof: Let L (W t, St, T t) be the objective function value
of problem (8) at the tth iteration. At the (t + 1)th iteration,
the optimal solution of W t+1 is ﬁrst calculated by solving the
subproblem (9). Because subproblem (9) is convex, thus we
have the following inequation after this iteration step
 W t+1, St, T t
 W t, St, T t
Similarly, owing to the convex property of subproblems to
variables W and T, we can obtain the following inequations
after the corresponding iterations:
 W t+1, St+1, T t
 W t+1, St, T t
 W t+1, St+1, T t+1
 W t+1, St+1, T t
Combing Eqs. (22), (23) and (24), we can obtain
 W t+1, St+1, T t+1
 W t, St, T t
Therefore, we complete the proof.
Theorem 3 provides some assurances to the convergence
property of the proposed method. Since the objective function
(8) is lower bounded, the proposed method will ﬁnally ﬁnd a
local optimal solution that makes the objective function value
converge . In the subsequent section, we will further prove
its convergence property based on some experiments.
C. Connections to other methods
In this section, we analyze the connections and differences
between the proposed method and some related LR methods,
such as ReLSR , MSRL , DLSR and SLRR ,
(1) Connections to ReLSR and MSRL: The discriminative
regression model of MSRL is as follows:
F + α ∥W∥2
F +β ∥W −AB∥2
W T xi −W T xj
2 Pi,j + σP 2
s.t. Ti,li −max
j̸=li Ti,j ≥1, AT A = I, 0 ≤Pi,j ≤1, P1 = 1
From the models of MSRL in (22) and ReLSR in (2), we
can ﬁnd that the proposed method and MSRL are the two
extensions of ReLSR indeed. By introducing some valuable
constraints to the model, MSRL and the proposed method
can learn a more compact and discriminative projection than
ReLSR. Although there are some similar points between
MSRL and the proposed method, they still have many differences as follows. (i) MSRL exploits an unsupervised graph
regularization term to guide the projection learning. While
in our method, the graph regularization term is supervised.
For MSRL, the unsupervised graph regularization term is
not perfect and has many shortcomings. For example, in
MSRL, samples from different classes may be regarded as the
nearest neighbor samples and connected with large weight,
which is obviously unreasonable. Besides, MSRL is sensitive
to the number of nearest neighbors. Compared with MSRL,
our method does not have the above problems since it can
construct a more reliable graph by introducing the novel
supervised graph learning term. (ii) MSRL imposes ‘Frobenius
norm’ on the projection matrix. In our method, the sparse
l2,1 norm is introduced. Compared with MSRL, the proposed
method has the feature selection property, which is able to
select the most important features for regression and has the
potential to reduce the negative inﬂuence of the redundant
features and noises. (iii) We can ﬁnd that the proposed method
has less penalty parameters than MSRL, which greatly reduces
the complexity of parameter selection. In Fig.2, we have
plotted the ﬁrst 100 rows of the projections learned by ReLSR,
MSRL, and the proposed method on the PIE database. From
Fig.2, it is obvious that some rows of the projection (Fig.2(c))
learned by the proposed method are adaptively enforced to
zero while the other two projections do not show this phenomenon. As analyzed in proposition 1, some features of
original data corresponding to these rows will not be selected
for feature extraction. In Fig.3(a), we have marked these
unselected features with ‘black point’ on the original images.
From Fig.3(a), we can ﬁnd that some similar areas among
different faces are marked while features of the remaining
areas such as eyes, mouth, and nose, etc. are treated as
important features. This also proves that the proposed method
has the potential to select those important features from the
original high-dimensional data. Meanwhile, in Fig.3(b), we
have plotted the weight graph of features, in which the weight
corresponding to the ith feature is calculated as ||Wi,:||2 .
From Fig.3(b), it is easy to discover the importance degrees
of different features in classiﬁcation. This demonstrates that
projection learned by our method has good interpretability for
features. In summary, the proposed method has many superior
properties in comparison with MSRL and ReLSR, which allow
it to perform better than the other methods.
(2) Connections to SLRR and DLSR. Similar to the proposed method, SLRR and DLSR all impose the sparse l2,1
norm constraint on the projection matrix. The objective functions of SLRR and DLSR are respectively formulated as
rank(W )≤s
F + λ∥W∥2,1
Y −B ⊙M −XT W
2,1 + λ∥W∥2,1
where B is a constant matrix and deﬁned as follows
where li is the true class index of sample xi.
From (23), (24), and (8), the proposed method and DLSR
all exploit the target learning technique to enlarge the margins
of samples from different classes, thus can learn a more
discriminative projection than SLRR. Compared with DLSR,
our method not only exploits a more ﬂexible target learning
technique, but also introduces an adaptive graph regularization
term to further improve the compactness of the projection.
These properties encourage the proposed method to learn a
more discriminative projection than DLSR and avoid overﬁtting, and thus can obtain a better performance.
Fig. 2: Projections learned by ReLSR, MSRL, and our method
on the PIE database, in which 20 samples per class are
randomly selected as training samples. Note: all ﬁgures are
shown in the HSV color space; for better comparison, we only
plot the ﬁrst 100 rows of these projections. From the colorbar,
we can infer all element values of these projections. Moreover,
we can clearly see that element values in some rows are 0 or
close to 0 in (c).
V. EXPERIMENTS AND ANALYSIS
In this section, we evaluate the proposed method on the synthetic database and ﬁve real-world databases. Several related
LR based methods, including linear regression classiﬁcation
(LRC) , sparse representation based classiﬁcation (SRC)
 , collaborative representation based classiﬁcation (CRC)
 , support vector machine (SVM)1 , LRLR , lowrank ridge regression (LRRR) , SLRR , discriminative least squares regression (DLSR) , ReLSR ,
1We exploit LibSVM toolbox to implement experiments. LibSVM is
available at cjlin/libsvm/.
(a) Original face images
(b) Weight graph of features
Fig. 3: The original training samples of the PIE database
and the weighted graph of features, in which the ‘black pixel
points’ in (a) denote the rows of projection with all zeros of the
proposed method. Note: weight values in (b) are normalized
to the range of .
DRLS , MSRL , constrained least square regression
(CLSR) , and groupwise retargeted least-squares regression (GReLSR) , are chosen to compare with the proposed
method. Among these methods, LRRR is an extension of
LRLR, which imposes the ‘Frobenius’ norm on the lowrank projection. CLSR and GReLSR can be viewed as the
extensions of ReLSR, which mainly introduce the group based
label relaxation technique to improve the performance. For the
proposed method, the threshold value ρ is set to 0.0001 to all
databases.
A. Experiments and analysis on the synthetic database
Preserving the geometric structure is very important to
discriminant analysis, especially for some databases with manifold structure. Following the experimental settings in ,
we synthesize the typical manifold data, i.e., three-ring data
with two different amplitudes of the third feature, to prove the
effectiveness of the proposed method in dealing with such type
of data. For convenience, we refer to the three-ring data with
amplitude [−20, 20] as Th1, and refer to the other one with
amplitude [−2000, 2000] as Th2. Fig.4 (a) shows the typical
data of the Th1, which contains three features. For Th1 and
Th2, the ﬁrst two features are centrally distributed in the circle
style, while the third feature can be viewed as the noise to
some extent. In our experiments, each synthesized three-ring
data is composed of 3 classes and 1000 samples per class, in
which 500 samples are randomly selected from each class to
form the training set, and the remaining samples are treated
as the test set accordingly.
Table I enumerates the experimental results of different
methods including the nearest neighbor classiﬁer (NC) on
Th1 and Th2. In Fig.4, we plot the projected test samples
and their predicted labels of different methods on Th1, and
show the projection learned by our method on Th1. From the
experimental results in Table I, it is obvious that many methods
except SVM and the proposed method perform worse than
the simplest classiﬁer, i.e., NC, on Th1 and Th2. In addition,
from Fig.4, we can also ﬁnd that only the proposed method
can simultaneously preserve the intrinsic structure well and
obtain the satisfactory classiﬁcation result. Although LRLR,
LRRR, and SLRR can preserve the similar structure as the
original data, they cannot predict these test samples correctly.
Therefore, these experimental results prove the effectiveness
of the proposed method in classifying the data with manifold
structure. Moreover, from Table I, we can also ﬁnd that
with the amplitude of the third feature (noise) increasing,
the classiﬁcation accuracies of almost all methods decrease
dramatically. And it is obvious that the classiﬁcation accuracy
of our method only decreases about 0.06% when the noise
increases. These demonstrate that the proposed method is
superior to the other methods for the classiﬁcation tasks with
noises. Furthermore, from Fig.4 (l), we can ﬁnd that the feature
extraction weights corresponding to the third feature (noise)
of Th1 are adaptively set as 0. This proves that introducing the
row-sparsity norm constraint is valuable, which can effectively
reduce the negative inﬂuence of noises. In summary, the
proposed method is not only suitable to classify the data with
manifold structure, but also robust to noise to some extent.
B. Experiments and analysis on real-world databases
In this subsection, ﬁve benchmark databases listed in Table
II are chosen to evaluate the effectiveness of our method.
(1) COIL100 object database2 : COIL100 database is
one of the most popular benchmarks for object classiﬁcation.
It is composed of 7200 images provided by 100 objects, in
which each object has 72 images with different poses. Fig.5 (a)
shows some example images of the database. Each image used
in the experiments was normalized and resized to 32×32 with
black background in advance. For this database, we randomly
select 10, 15, 20, and 25 samples of each class to form the
training set and treat the remaining samples as the test set,
respectively.
(2) CMU pose, illumination, and expression (PIE)
face database : PIE database is one of the challenging
databases for face recognition since it contains over 40,
000 images with various poses, illumination conditions, and
expressions, etc. In our experiments, we compare different
methods on a subset of PIE which totally contains 11554
images of 68 individuals. Each class has nearly 170 samples
with 5 different poses. Fig.5 (b) shows some typical images
of the database. For computational efﬁciency, each image was
pre-resized to 32 × 32 and then stacked into a vector with
1024 dimensions. For this database, we randomly select 10,
15, 20, and 25 samples per class as the training set and treat the
remaining samples as the test set for experiments, respectively.
(3) Labeled Faces in the Wild (LFW) face database
 : The LFW database is more challenge than the PIE face
database since all images are directly collected from the web
with different poses, backgrounds, expressions, illuminations,
and image acquirement devices, etc. In our experiments, a
subset which contains 1251 cropped face images provided by
86 persons is chosen for comparison . There are about
11-20 samples in each class. Some typical images from the
same class are shown in Fig.5 (c). Similarly, each image was
transformed into a 32 × 32 matrix and then stacked into a
vector. Then 5, 6, 7, and 8 samples are randomly chosen from
2COIL100 database is available at: 
software/softlib/coil-100.php
(a) Test data of Th1
(j) GReLSR
(l) Projection of ALPR
Fig. 4: (a)-(k) are the visualization of classiﬁcation results of different methods on Th1. (l) shows the projection learned by
our method on Th1.
TABLE I: Classiﬁcation accuracies (%) of different methods on the three-ring data. (Note: NC is the abbreviation of the nearest
neighbor classify .
TABLE II: Descriptions of the used real-world databases.
# Sample per class
each class to form the training set and the reaming samples
are regarded as the test set accordingly.
(4) Fifteen Scene Categories (Scene15) database3 :
The Scene15 database is widely chosen to evaluate different
methods for the scene classiﬁcation task. The 4485 images
are naturally collected from 15 common scenes in daily life,
such as street, ofﬁce, store, highway, living room and kitchen,
etc. For each scene, there are about 210-410 natural samples.
Fig. 5 (d) shows some typical images of the database. It
is not suitable to directly evaluate different methods on the
original images because they have many differences in size,
intensity, shape, and background, etc. In our experiments,
we compare different methods on its feature-level database
by following the experimental settings in , in which all
samples are represented by their spatial pyramid features with
3000 dimensions. We refer to the feature-level database as
Scene SPM for convenience. Similarly, 10, 20, 30, and 40
samples of each class are randomly selected to form the
training set and the remaining samples are regarded as the
test set, respectively.
(5) CIFAR-10 database : The CIFAR-10 database is a
popular large-scale image database, which consists of 50000
training images and 10000 test images from 10 classes. The
size of the original color images in the database is 32 × 32.
Some typical images are shown in Fig.5 (e). In our work, we
3The Fifteen Scene Categories database is available at: 
uiuc.edu/ponce grp/data/
ﬁrst exploit k-means based feature extraction method4 to
extract the features of CIFAR-10 database, and then utilize the
principal component analysis (PCA) algorithm to reduce
the feature of each sample to 1000 dimensions to improve the
computational efﬁciency. We refer to the extracted features
of CIFAR-10 as K-means-CIFAR10. On this dataset, several
well-known deep convolutional network based classiﬁcation
methods, including ResNet with 110 layers , simple fast
convolutional (SFC) , deep linear discriminant analysis
(DeepLDA) , and DensetNet , are also compared.
For the ﬁrst four databases, we repeatedly perform different
methods 20 times and report their mean classiﬁcation accuracies for fair comparison. For the CIFAR-10 database, we
implement all methods on the same 50000 training samples
and 10000 test samples. The experimental results of different
methods on the above ﬁve databases are enumerated in Table
III-Table VII. From the experimental results, we can ﬁnd that
our method obtains much better performance than the other
methods in most cases. In addition, the following interesting
points can be obtained according to the experimental results:
(1) We can ﬁnd that DLSR, ReLSR, CLRS, and GReLSR
generally outperform LRLR, LRRR, and SLRR on the above
ﬁve databases, which proves the effectiveness of the εdragging technique and discriminative target learning technique. In other words, learning a more ﬂexible target matrix
with large margins of different classes is beneﬁcial to learn a
more discriminative projection for classiﬁcation.
(2) From these four tables, it is obvious that MSRL and the
proposed method always perform much better than ReLSR.
As analyzed in the previous section, MSRL and the proposed
method are the two extensions of ReLSR, which exploit the
local geometric information of data to guide the projection
4The feature extraction code of k-means is available at: 
edu/∼acoates/papers/kmeans demo.tgz
TABLE III: Mean classiﬁcation accuracies (%) of different methods on the COIL100 database. Note: (1) bold numbers denote
the best results; (2) we directly list the results of MSRL reported in .
TABLE IV: Mean classiﬁcation accuracies (%) of different methods on the PIE database. Note: (1) bold numbers denote the
best results; (2) we directly list the results of MSRL reported in .
TABLE V: Mean classiﬁcation accuracies (%) of different methods on the LFW database. Note: bold numbers denote the best
TABLE VI: Mean classiﬁcation accuracies (%) of different methods on the Scene SPM database. Note: bold numbers denote
the best results.
TABLE VII: Classiﬁcation accuracies (ACC) (%) of different
methods on the K-means-CIFAR10 database. For the four deep
learning based methods, we directly list their reported results.
learning. Therefore, the experimental results prove that preserving the local geometric structure of data during the linear
regression is also signiﬁcant and enables the two methods to
learn a more discriminative projection.
(3) The proposed method and MSRL obtain comparative
good performance on the PIE and Scene-SPM database. While
on the LFW database, the proposed method signiﬁcantly
outperforms MSRL. From Fig.5 (c), we can ﬁnd that images
of the same class also have very large differences in the
LFW database. Thus it is difﬁcult to capture the intrinsic
geometric structure of data especially using the unsupervised
approach. In other words, MSRL cannot ﬁnd the intrinsic
nearest neighbor relationships to guide the projection learning, and thus cannot guarantee the satisfactory performance.
Compared with MSRL, our method overcomes this issue by
exploiting a supervised approach to adaptively capture the
intrinsic similarity relationships among samples of the same
class, which plays a positive guiding role in the projection
learning. Meanwhile, as analyzed in the previous section,
the proposed method has the potential to adaptively select
those important features from data for feature extraction and
effectively reduce the negative inﬂuence of noise, which is also
beneﬁcial to improve the classiﬁcation performance. These
two effective approaches encourage the proposed method to
obtain a better performance than MSRL on the LFW database.
(4) From Table VII, we can obviously ﬁnd that all the deep
convolutional network based methods achieve much better
performance than the conventional methods. This demonstrates
that the deep convolutional network based methods can extract
more discriminative features than the exploited unsupervised
feature extraction method, i.e, k-means. Among all of the
conventional methods, the proposed method still outperforms
all the other methods, which also proves that the proposed
method can learn a more discriminative projection than the
other conventional methods.
C. Parameter sensitivity and selection
Generally, for some methods, selecting the optimal penalty
parameters is crucial to achieve satisfactory performance
on different databases. In this section, we mainly analyze the sensitivity of parameter selection of the proposed
method and then provide a simple strategy to select the
optimal parameters. From (8), we can ﬁnd that the proposed method only contains two penalty parameters, i.e.,
λ1 and λ2, which are regularized on the nearest neighbor
preserving term and the feature selection term, respectively.
To analyze the sensitivity of the classiﬁcation performance
(a) COIL100
(d) Scene15
(e) CIFAR-10
Fig. 5: Typical images of the used real-world databases.
to these two parameters, ﬁrstly, a large candidate range
{10−5, 10−4, 10−3, 10−2, 10−1, 1, 101, 102, 103, 104, 105}
deﬁned for the two penalty parameters. Secondly, we conduct
several experiments to show the relationships of the classiﬁcation accuracies (%) and different values of the two parameters
on the ﬁrst four databases. Fig.6 shows the classiﬁcation
accuracies versus the two parameters. From these ﬁgures, it
is obvious that when parameter λ1 is selected from the range
, and parameter λ2 locates in the proper range,
such as [0.1, 1] on the COIL100 database, [0.1, 1] on the PIE
database, [0.1, 1] on the LFW database, and
Scene SPM database, respectively, the proposed method can
obtain almost constant classiﬁcation accuracy. This demonstrates that the proposed method is insensitive to the selection
of λ1 to some extent.
As far as we know, it is still an open problem to adaptively
select the optimal parameters for different databases. In this
paper, we exploit a simple approach based on the grid search
to ﬁnd the two optimal parameters . According to the
previous analysis, we ﬁrst deﬁne the candidate range
for the two parameters. Then we ﬁx parameter λ1 as 0.1
since the proposed method is insensitive to it, and perform
the proposed method with different values of λ2 selected from
the coarse candidate range. In this way, we can ﬁnd the latent
optimal λ2 from the candidate range. Then we ﬁx λ2 with
the obtained latent optimal value and perform the proposed
again to ﬁnd the optimal value of λ1 from the same candidate
range. Finally, we can obtain the best combination of the two
parameters for experiments.
D. Experiments of convergence study
In this section, we mainly conduct some experiments
to further prove the convergence property of the proposed
optimization approach in Algorithm 1. In Fig. 7, we have
classiﬁcation
accuracies versus the iteration steps on the COIL100, PIE,
LFW, and Scene SPM databases, respectively, in which
the objective function value is directly calculated as obj =
+ λ2∥W∥2,1
according to the objective function (8). From these ﬁgures, it
is obvious that the objective function value is monotonically
decreasing till to the stationary point with the iteration
increasing, which proves the point of Theorem 3. Meanwhile,
we can also ﬁnd that the classiﬁcation accuracy increases
obviously until the objective function value converges to
the stationary point, which demonstrates that the proposed
method can ﬁnally ﬁnd the local optimal solution when the
objective function converges.
VI. CONCLUSION
We proposed an effective linear regression method for
classiﬁcation in this paper. The proposed method improves
the discriminability of projection through three approaches.
Firstly, we adaptively learn a more ﬂexible target matrix with
large margins between the correct and incorrect classes for
regression. Secondly, we replace the conventional ‘Frobenius norm’ with the sparse l2,1 norm to constrain the projection, which enables the proposed method to select the
most important features from the original high-dimensional
data for feature extraction. Thirdly, we introduce a novel
supervised graph regularization term to guide the projection
learning. Compared with the conventional unsupervised graph
learning approach, the supervised approach presented in our
paper is more effective in preserving the intrinsic nearest
neighbor relationships of each class. Most importantly, the
discriminative target learning, intrinsic graph learning, and
projection learning are neatly integrated into one joint learning
framework, which enables the method to obtain the global
optimal projection for classiﬁcation so as to obtain a better
performance. The effectiveness of the proposed method has
been sufﬁciently proved on the synthetic database and many
real-world databases.