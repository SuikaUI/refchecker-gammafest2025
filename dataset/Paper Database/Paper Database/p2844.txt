Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742
Melbourne, Australia, July 15 - 20, 2018. c⃝2018 Association for Computational Linguistics
Coarse-to-Fine Decoding for Neural Semantic Parsing
Li Dong and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
 
 
Semantic parsing aims at mapping natural
language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture
which decomposes the semantic parsing
process into two stages. Given an input utterance, we ﬁrst generate a rough sketch of
its meaning, where low-level information
(such as variable names and arguments) is
glossed over. Then, we ﬁll in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving
competitive results despite the use of relatively simple decoders.
Introduction
Semantic parsing maps natural language utterances onto machine interpretable meaning representations has provided strong impetus to treat semantic parsing as
a sequence-to-sequence problem .
The fact that meaning representations are typically structured objects has prompted efforts to
develop neural architectures which explicitly account for their structure. Examples include tree
decoders , decoders constrained by a
grammar model , or modular
decoders which use syntax to dynamically compose various submodels .
In this work, we propose to decompose the decoding process into two stages. The ﬁrst decoder
focuses on predicting a rough sketch of the meaning representation, which omits low-level details,
such as arguments and variable names. Example
sketches for various meaning representations are
shown in Table 1. Then, a second decoder ﬁlls
in missing details by conditioning on the natural
language input and the sketch itself. Speciﬁcally,
the sketch constrains the generation process and is
encoded into vectors to guide decoding.
We argue that there are at least three advantages
to the proposed approach. Firstly, the decomposition disentangles high-level from low-level semantic information, which enables the decoders
to model meaning at different levels of granularity. As shown in Table 1, sketches are more compact and as a result easier to generate compared to
decoding the entire meaning structure in one go.
Secondly, the model can explicitly share knowledge of coarse structures for the examples that
have the same sketch (i.e., basic meaning), even
though their actual meaning representations are
different (e.g., due to different details). Thirdly,
after generating the sketch, the decoder knows
what the basic meaning of the utterance looks like,
and the model can use it as global context to improve the prediction of the ﬁnal details.
Our framework is ﬂexible and not restricted to
speciﬁc tasks or any particular model. We conduct experiments on four datasets representative of
various semantic parsing tasks ranging from logical form parsing, to code generation, and SQL
query generation.
We adapt our architecture to
these tasks and present several ways to obtain
sketches from their respective meaning representations. Experimental results show that our framework achieves competitive performance compared
Length Example
x : which state has the most rivers running through it?
y : (argmax $0 (state:t $0) (count $1 (and (river:t $1) (loc:t $1 $0))))
a : (argmax#1 state:t@1 (count#1 (and river:t@1 loc:t@2 ) ) )
x : all ﬂights from dallas before 10am
y : (lambda $0 e (and (ﬂight $0) (from $0 dallas:ci) (< (departure time $0) 1000:ti)))
a : (lambda#2 (and ﬂight@1 from@2 (< departure time@1 ? ) ) )
x : if length of bits is lesser than integer 3 or second element of bits is not equal to string ’as’ ,
y : if len(bits) < 3 or bits != ’as’:
a : if len ( NAME ) < NUMBER or NAME [ NUMBER ] != STRING :
Table schema: ∥Pianist∥Conductor∥Record Company∥Year of Recording∥Format∥
x : What record company did conductor Mikhail Snitko record for after 1996?
y : SELECT Record Company WHERE AND (Conductor = Mikhail Snitko)
a : WHERE > AND =
Table 1: Examples of natural language expressions x, their meaning representations y, and meaning
sketches a. The average number of tokens is shown in the second column.
with previous systems, despite employing relatively simple sequence decoders.
Related Work
Various models have been proposed over the years
to learn semantic parsers from natural language
expressions paired with their meaning representations . These systems typically learn lexicalized
mapping rules and scoring models to construct a
meaning representation for a given input.
More recently, neural sequence-to-sequence
models have been applied to semantic parsing with
promising results , eschewing the
need for extensive feature engineering.
ideas have been explored to enhance the performance of these models such as data augmentation ,
transfer learning , sharing parameters for multiple languages or meaning representations , and utilizing user feedback signals .
There are also efforts to develop
structured decoders that make use of the syntax of
meaning representations. Dong and Lapata 
and Alvarez-Melis and Jaakkola develop
models which generate tree structures in a topdown fashion.
Xiao et al. and Krishnamurthy et al. employ the grammar to constrain the decoding process. Cheng et al. 
use a transition system to generate variable-free
queries. Yin and Neubig design a grammar model for the generation of abstract syntax
trees in depth-ﬁrst, left-to-right
order. Rabinovich et al. propose a modular
decoder whose submodels are dynamically composed according to the generated tree structure.
Our own work also aims to model the structure
of meaning representations more faithfully. The
ﬂexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects.
Coarse-to-ﬁne methods have been popular in the
NLP literature, and are perhaps best known for
syntactic parsing . Artzi and Zettlemoyer and Zhang
et al. use coarse lexical entries or macro
grammars to reduce the search space of semantic
parsers. Compared with coarse-to-ﬁne inference
for lexical induction, sketches in our case are abstractions of the ﬁnal meaning representation.
The idea of using sketches as intermediate representations has also been explored in the ﬁeld
of program synthesis . Yaghmazadeh
et al. use SEMPRE to
map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty.
Problem Formulation
Our goal is to learn semantic parsers from instances of natural language expressions paired
with their structured meaning representations.
(departure
Sketch-Guided
Encoder units
Decoder units
Figure 1: We ﬁrst generate the meaning sketch a for natural language input x. Then, a ﬁne meaning
decoder ﬁlls in the missing details (shown in red) of meaning representation y. The coarse structure a is
used to guide and constrain the output decoding.
Let x = x1 · · · x|x| denote a natural language expression, and y = y1 · · · y|y| its meaning representation. We wish to estimate p (y|x), the conditional probability of meaning representation y
given input x. We decompose p (y|x) into a twostage generation process:
p (y|x) = p (y|x, a) p (a|x)
where a = a1 · · · a|a| is an abstract sketch representing the meaning of y.
We defer detailed
description of how sketches are extracted to Section 4. Sufﬁce it to say that the extraction amounts
to stripping off arguments and variable names
in logical forms, schema speciﬁc information in
SQL queries, and substituting tokens with types in
source code (see Table 1).
As shown in Figure 1, we ﬁrst predict sketch a
for input x, and then ﬁll in missing details to generate the ﬁnal meaning representation y by conditioning on both x and a. The sketch is encoded
into vectors which in turn guide and constrain the
decoding of y. We view the input expression x,
the meaning representation y, and its sketch a as
sequences. The generation probabilities are factorized as:
p (at|a<t, x)
p (y|x, a) =
p (yt|y<t, x, a)
where a<t = a1 · · · at−1, and y<t = y1 · · · yt−1.
In the following, we will explain how p (a|x) and
p (y|x, a) are estimated.
Sketch Generation
An encoder is used to encode the natural language
input x into vector representations. Then, a decoder learns to compute p (a|x) and generate the
sketch a conditioned on the encoding vectors.
Input Encoder
Every input word is mapped to
a vector via xt
= Wxo (xt), where Wx
Rn×|Vx| is an embedding matrix, |Vx| is the vocabulary size, and o (xt) a one-hot vector. We use
a bi-directional recurrent neural network with long
short-term memory units as the input encoder. The encoder recursively computes the hidden vectors at
the t-th time step via:
−→e t = fLSTM
−→e t−1, xt
, t = 1, · · · , |x|
←−e t = fLSTM
←−e t+1, xt
, t = |x|, · · · , 1
et = [−→e t, ←−e t]
where [·, ·] denotes vector concatenation, et ∈Rn,
and fLSTM is the LSTM function.
Coarse Meaning Decoder
The decoder’s hidden vector at the t-th time step is computed by
dt = fLSTM (dt−1, at−1), where at−1 ∈Rn is
the embedding of the previously predicted token.
The hidden states of the ﬁrst time step in the decoder are initialized by the concatenated encoding
vectors d0 = [−→e |x|, ←−e 1]. Additionally, we use an
attention mechanism to learn
soft alignments. We compute the attention score
for the current time step t of the decoder, with the
k-th hidden state in the encoder as:
st,k = exp{dt · ek}/Zt
where Zt = |x|
j=1 exp{dt · ej} is a normalization
term. Then we compute p (at|a<t, x) via:
W1dt + W2ed
p (at|a<t, x) = softmaxat
where W1, W2 ∈Rn×n, Wo ∈R|Va|×n, and
bo ∈R|Va| are parameters. Generation terminates
once an end-of-sequence token “</s>” is emitted.
Meaning Representation Generation
Meaning representations are predicted by conditioning on the input x and the generated sketch a.
The model uses the encoder-decoder architecture
to compute p (y|x, a), and decorates the sketch a
with details to generate the ﬁnal output.
Sketch Encoder
As shown in Figure 1, a bidirectional LSTM encoder maps the sketch sequence a into vectors {vk}|a|
k=1 as in Equation (6),
where vk denotes the vector of the k-th time step.
Fine Meaning Decoder
The ﬁnal decoder is
based on recurrent neural networks with an attention mechanism, and shares the input encoder described in Section 3.1. The decoder’s hidden states
t=1 are computed via:
yt−1 is determined by ak
ht = fLSTM (ht−1, it)
where h0 = [−→e |x|, ←−e 1], and yt−1 is the embedding of the previously predicted token. Apart from
using the embeddings of previous tokens, the decoder is also fed with {vk}|a|
k=1. If yt−1 is determined by ak in the sketch (i.e., there is a one-toone alignment between yt−1 and ak), we use the
corresponding token’s vector vk as input to the
next time step.
The sketch constrains the decoding output. If
the output token yt is already in the sketch, we
force yt to conform to the sketch. In some cases,
sketch tokens will indicate what information is
missing (e.g., in Figure 1, token “ﬂight@1” indicates that an argument is missing for the predicate “ﬂight”).
In other cases, sketch tokens
will not reveal the number of missing tokens
(e.g., “STRING” in DJANGO) but the decoder’s
output will indicate whether missing details have
been generated (e.g., if the decoder emits a closing quote token for “STRING”). Moreover, type
information in sketches can be used to constrain
generation. In Table 1, sketch token “NUMBER”
speciﬁes that a numeric token should be emitted.
For the missing details, we use the hidden vector ht to compute p (yt|y<t, x, a), analogously
to Equations (7)–(10).
Training and Inference
The model’s training objective is to maximize the
log likelihood of the generated meaning representations given natural language expressions:
log p (y|x, a) + log p (a|x)
where D represents training pairs.
At test time, the prediction for input x is obtained via ˆa
arg maxa′ p (a′|x) and ˆy
arg maxy′ p (y′|x, ˆa), where a′ and y′ represent
coarse- and ﬁne-grained meaning candidates. Because probabilities p (a|x) and p (y|x, a) are factorized as shown in Equations (2)–(3), we can obtain best results approximately by using greedy
search to generate tokens one by one, rather than
iterating over all candidates.
Semantic Parsing Tasks
In order to show that our framework applies across
domains and meaning representations, we developed models for three tasks, namely parsing natural language to logical form, to Python source
code, and to SQL query. For each of these tasks
we describe the datasets we used, how sketches
were extracted, and specify model details over and
above the architecture presented in Section 3.
Natural Language to Logical Form
For our ﬁrst task we used two benchmark datasets,
namely GEO (880 language queries to a database
of U.S. geography) and ATIS (5, 410 queries to
a ﬂight booking system).
Examples are shown
in Table 1 (see the ﬁrst and second block). We
used standard splits for both datasets: 600 training and 280 test instances for GEO ; 4, 480 training, 480 development, and 450 test examples for ATIS.
Meaning representations in these datasets are based
on λ-calculus .
use brackets to linearize the hierarchical structure.
Algorithm 1 Sketch for GEO and ATIS
Input: t: Tree-structure λ-calculus expression
t.pred: Predicate name, or operator name
Output: a: Meaning sketch
▷(count $0 (< (fare $0) 50:do))→(count#1 (< fare@1 ?))
function SKETCH(t)
if t is leaf then
▷No nonterminal in arguments
return “%s@%d” % (t.pred, len(t.args))
if t.pred is λ operator, or quantiﬁer then ▷e.g., count
Omit variable information deﬁned by t.pred
t.pred ←“%s#%d” % (t.pred, len(variable))
for c ←argument in t.args do
if c is nonterminal then
c ←SKETCH(c)
▷Placeholder for terminal
The ﬁrst element between a pair of brackets is an
operator or predicate name, and any remaining elements are its arguments.
Algorithm 1 shows the pseudocode used to extract sketches from λ-calculus-based meaning representations. We strip off arguments and variable
names in logical forms, while keeping predicates,
operators, and composition information. We use
the symbol “@” to denote the number of missing
arguments in a predicate. For example, we extract “from@2” from the expression “(from $0 dallas:ci)” which indicates that the predicate “from”
has two arguments. We use “?” as a placeholder
in cases where only partial argument information
can be omitted. We also omit variable information deﬁned by the lambda operator and quanti-
ﬁers (e.g., exists, count, and argmax). We use the
symbol “#” to denote the number of omitted tokens. For the example in Figure 1, “lambda $0 e”
is reduced to “lambda#2”.
The meaning representations of these two
datasets are highly compositional, which motivates us to utilize the hierarchical structure of
λ-calculus. A similar idea is also explored in the
tree decoders proposed in Dong and Lapata 
and Yin and Neubig where parent hidden
states are fed to the input gate of the LSTM units.
On the contrary, parent hidden states serve as input
to the softmax classiﬁers of both ﬁne and coarse
meaning decoders.
Parent Feeding
Taking the meaning sketch
“(and ﬂight@1 from@2)” as an example, the parent of “from@2” is “(and”. Let pt denote the parent of the t-th time step in the decoder. Compared
with Equation (10), we use the vector datt
hidden state of its parent dpt to compute the probability p (at|a<t, x) via:
p (at|a<t, x) = softmaxat
t , dpt] + bo
where [·, ·] denotes vector concatenation. The parent feeding is used for both decoding stages.
Natural Language to Source Code
Our second semantic parsing task used DJANGO
 , a dataset built upon the Python
code of the Django library. The dataset contains
lines of code paired with natural language expressions (see the third block in Table 1) and exhibits
a variety of use cases, such as iteration, exception
handling, and string manipulation. The original
split has 16, 000 training, 1, 000 development, and
1, 805 test instances.
We used the built-in lexical scanner of Python1
to tokenize the code and obtain token types.
Sketches were extracted by substituting the original tokens with their token types, except delimiters
(e.g., “[”, and “:”), operators (e.g., “+”, and “*”),
and built-in keywords (e.g., “True”, and “while”).
For instance, the expression “if s[:4].lower() ==
’http’:” becomes “if NAME [ : NUMBER ] . NAME (
) == STRING :”, with details about names, values,
and strings being omitted.
DJANGO is a diverse dataset, spanning various
real-world use cases and as a result models are
often faced with out-of-vocabulary (OOV) tokens
(e.g., variable names, and numbers) that are unseen during training. We handle OOV tokens with
a copying mechanism , which allows
the ﬁne meaning decoder (Section 3.2) to directly
copy tokens from the natural language input.
Copying Mechanism
Recall that we use a softmax classiﬁer to predict the probability distribution p (yt|y<t, x, a) over the pre-deﬁned vocabulary. We also learn a copying gate gt ∈ to
decide whether yt should be copied from the input
or generated from the vocabulary. We compute the
modiﬁed output distribution via:
gt = sigmoid(wg · ht + bg)
˜p (yt|y<t, x, a) = (1 −gt)p (yt|y<t, x, a)
+ 1[yt /∈Vy]gt
1 
where wg ∈Rn and bg ∈R are parameters, and
the indicator function 1[yt /∈Vy] is 1 only if yt is
not in the target vocabulary Vy; the attention score
st,k (see Equation (7)) measures how likely it is to
copy yt from the input word xk.
Natural Language to SQL
The WIKISQL dataset contains 80, 654 examples of questions and SQL
queries distributed across 24, 241 tables from
Wikipedia. The goal is to generate the correct SQL
query for a natural language question and table
schema (i.e., table column names), without using
the content values of tables (see the last block in
Table 1 for an example). The dataset is partitioned
into a training set (70%), a development set (10%),
and a test set (20%). Each table is present in one
split to ensure generalization to unseen tables.
WIKISQL queries follow the format “SELECT
agg op agg col WHERE (cond col cond op
cond) AND ...”, which is a subset of the SQL syntax. SELECT identiﬁes the column that is to be included in the results after applying the aggregation
operator agg op2 to column agg col. WHERE
can have zero or multiple conditions, which means
that column cond col must satisfy the constraints expressed by the operator cond op3 and
the condition value cond.
Sketches for SQL
queries are simply the (sorted) sequences of condition operators cond op in WHERE clauses. For
example, in Table 1, sketch “WHERE > AND =”
has two condition operators, namely “>” and “=”.
The generation of SQL queries differs from our
previous semantic parsing tasks, in that the table schema serves as input in addition to natural language. We therefore modify our input encoder in order to render it table-aware, so to speak.
Furthermore, due to the formulaic nature of the
SQL query, we only use our decoder to generate the WHERE clause (with the help of sketches).
The SELECT clause has a ﬁxed number of slots
(i.e., aggregation operator agg op and column
agg col), which we straightforwardly predict
with softmax classiﬁers (conditioned on the input). We brieﬂy explain how these components
are modeled below.
Table-Aware Input Encoder
Given a table
schema with M columns, we employ the special token “∥” to concatenate its header names
2agg op ∈{empty, COUNT, MIN, MAX, SUM, AVG}.
3cond op ∈{=, <, >}.
|| college
presidents
Input Question
Question-to-Table Attention
LSTM units
Figure 2: Table-aware input encoder (left) and table column encoder (right) used for WIKISQL.
as “∥c1,1 · · · c1,|c1|∥· · · ∥cM,1 · · · cM,|cM|∥”, where
the k-th column (“ck,1 · · · ck,|ck|”) has |ck| words.
As shown in Figure 2, we use bi-directional
LSTMs to encode the whole sequence. Next, for
column ck, the LSTM hidden states at positions
ck,1 and ck,|ck| are concatenated. Finally, the concatenated vectors are used as the encoding vectors
k=1 for table columns.
As mentioned earlier, the meaning representations of questions are dependent on the tables. As
shown in Figure 2, we encode the input question x
into {et}|x|
t=1 using LSTM units.
At each time
step t, we use an attention mechanism towards table column vectors {ck}M
k=1 to obtain the most relevant columns for et. The attention score from et
to ck is computed via ut,k ∝exp{α(et) · α(ck)},
where α(·) is a one-layer neural network, and
Then we compute the context vector ce
k=1 ut,kck to summarize the
relevant columns for et.
We feed the concatenated vectors {[et, ce
t=1 into a bi-directional
LSTM encoder, and use the new encoding vectors
t=1 to replace {et}|x|
t=1 in other model components. We deﬁne the vector representation of
input x as:
˜e = [−→˜e |x|, ←−˜e 1]
analogously to Equations (4)–(6).
SELECT Clause
We feed the question vector ˜e
into a softmax classiﬁer to obtain the aggregation
operator agg op. If agg col is the k-th table
column, its probability is computed via:
σ(x) = w3 · tanh (W4x + b4)
p (agg col = k|x) ∝exp{σ([˜e, ck])}
j=1 p (agg col = j|x) = 1, σ(·) is a
scoring network, and W4 ∈R2n×m, w3, b4 ∈
Rm are parameters.
WHERE < AND =
Sketch-Guided
Classification
Figure 3: Fine meaning decoder of the WHERE
clause used for WIKISQL.
whose details are subsequently decorated by the
ﬁne meaning decoder described in Section 3.2.
As the number of sketches in the training set is
small (35 in total), we model sketch generation as
a classiﬁcation problem. We treat each sketch a
as a category, and use a softmax classiﬁer to compute p (a|x):
p (a|x) = softmaxa (Wa˜e + ba)
where Wa ∈R|Va|×n, ba ∈R|Va| are parameters, and ˜e is the table-aware input representation
deﬁned in Equation (12).
Once the sketch is predicted, we know the condition operators and number of conditions in the
WHERE clause which follows the format “WHERE
(cond op cond col cond) AND ...”. As shown
in Figure 3, our generation task now amounts
to populating the sketch with condition columns
cond col and their values cond.
Let {ht}|y|
t=1 denote the LSTM hidden states
of the ﬁne meaning decoder, and {hatt
the vectors obtained by the attention mechanism as in Equation (9). The condition column
cond colyt is selected from the table’s headers. For the k-th column in the table, we compute p (cond colyt = k|y<t, x, a) as in Equation (14), but use different parameters and compute the score via σ([hatt
t , ck]). If the k-th table
column is selected, we use ck for the input of the
next LSTM unit in the decoder.
Condition values are typically mentioned in the
input questions. These values are often phrases
with multiple tokens (e.g., Mikhail Snitko in Table 1). We therefore propose to select a text span
from input x for each condition value condyt
rather than copying tokens one by one.
xl · · · xr denote the text span from which condyt
is copied. We factorize its probability as:
p (condyt = xl · · · xr|y<t, x, a)
yt|y<t, x, a
yt|y<t, x, a, lL
yt|y<t, x, a
∝exp{σ([hatt
t , ˜el])}
yt|y<t, x, a, lL
∝exp{σ([hatt
t , ˜el, ˜er])}
where lL
yt represents the ﬁrst/last copying
index of condyt is l/r, the probabilities are normalized to 1, and σ(·) is the scoring network de-
ﬁned in Equation (13). Notice that we use different parameters for the scoring networks σ(·).
The copied span is represented by the concatenated vector [˜el, ˜er], which is fed into a one-layer
neural network and then used as the input to the
next LSTM unit in the decoder.
Experiments
We present results on the three semantic parsing
tasks discussed in Section 4. Our implementation
and pretrained models are available at https://
github.com/donglixp/coarse2fine.
Experimental Setup
Preprocessing
For GEO and ATIS, we used the
preprocessed versions provided by Dong and Lapata , where natural language expressions
are lowercased and stemmed with NLTK , and entity mentions are replaced by
numbered markers. We combined predicates and
left brackets that indicate hierarchical structures to
make meaning representations compact. We employed the preprocessed DJANGO data provided
by Yin and Neubig , where input expressions are tokenized by NLTK, and quoted strings
in the input are replaced with place holders. WIK-
ISQL was preprocessed by the script provided
by Zhong et al. , where inputs were lowercased and tokenized by Stanford CoreNLP .
Conﬁguration
hyperparameters
cross-validated on the training set for GEO, and
were validated on the development split for the
other datasets. Dimensions of hidden vectors and
word embeddings were selected from {250, 300}
and {150, 200, 250, 300},
respectively.
dropout rate was selected from {0.3, 0.5}. Label
smoothing was employed
for GEO and ATIS. The smoothing parameter was
set to 0.1. For WIKISQL, the hidden size of σ(·)
ZC07 
UBL 
FUBL 
GUSP++ 
KCAZ13 
DCS+L 
TISP 
SEQ2SEQ 
SEQ2TREE 
ASN 
ASN+SUPATT 
COARSE2FINE
−sketch encoder
+ oracle sketch
Table 2: Accuracies on GEO and ATIS.
and α(·) in Equation (13) was set to 64. Word
embeddings were initialized by GloVe , and were shared by table encoder
and input encoder in Section 4.3. We appended
10-dimensional part-of-speech tag vectors to embeddings of the question words in WIKISQL. The
part-of-speech tags were obtained by the spaCy
toolkit. We used the RMSProp optimizer to train the models. The
learning rate was selected from {0.002, 0.005}.
The batch size was 200 for WIKISQL, and was
64 for other datasets. Early stopping was used to
determine the number of epochs.
Evaluation
We use accuracy as the evaluation
metric, i.e., the percentage of the examples that
are correctly parsed to their gold standard meaning
representations. For WIKISQL, we also execute
generated SQL queries on their corresponding tables, and report the execution accuracy which is
deﬁned as the proportion of correct answers.
Results and Analysis
We compare our model (COARSE2FINE) against
several previously published systems as well as
various baselines. Speciﬁcally, we report results
with a model which decodes meaning representations in one stage (ONESTAGE) without leveraging sketches. We also report the results of several
ablation models, i.e., without a sketch encoder and
without a table-aware input encoder.
Table 2 presents our results on GEO and ATIS.
Overall, we observe that COARSE2FINE outperforms ONESTAGE, which suggests that disentangling high-level from low-level information dur-
Retrieval System
Phrasal SMT
Hierarchical SMT
SEQ2SEQ+UNK replacement
SEQ2TREE+UNK replacement
LPN+COPY 
SNM+COPY 
COARSE2FINE
−sketch encoder
+ oracle sketch
Table 3: DJANGO results. Accuracies in the ﬁrst
and second block are taken from Ling et al. 
and Yin and Neubig .
ing decoding is beneﬁcial. The results also show
that removing the sketch encoder harms performance since the decoder loses access to additional
contextual information. Compared with previous
neural models that utilize syntax or grammatical
information (SEQ2TREE, ASN; the second block
in Table 2), our method performs competitively
despite the use of relatively simple decoders. As
an upper bound, we report model accuracy when
gold meaning sketches are given to the ﬁne meaning decoder (+oracle sketch). As can be seen, predicting the sketch correctly boosts performance.
The oracle results also indicate the accuracy of the
ﬁne meaning decoder.
Table 3 reports results on DJANGO where we
observe similar tendencies. COARSE2FINE outperforms ONESTAGE by a wide margin. It is also
superior to the best reported result in the literature
(SNM+COPY; see the second block in the table).
Again we observe that the sketch encoder is beneﬁcial and that there is an 8.9 point difference in
accuracy between COARSE2FINE and the oracle.
Results on WIKISQL are shown in Table 4. Our
model is superior to ONESTAGE as well as to previous best performing systems. COARSE2FINE’s
accuracies on aggregation agg op and agg col
are 90.2% and 92.0%, respectively, which is comparable to SQLNET .
most gain is obtained by the improved decoder
of the WHERE clause. We also ﬁnd that a tableaware input encoder is critical for doing well
on this task, since the same question might lead
to different SQL queries depending on the table
schemas. Consider the question “how many presidents are graduated from A”. The SQL query
over table “∥President∥College∥” is “SELECT
Aug Ptr Network
SEQ2SQL 
SQLNET 
COARSE2FINE
−sketch encoder
−table-aware input encoder
+ oracle sketch
Table 4: Evaluation results on WIKISQL. Accuracies in the ﬁrst block are taken from Zhong et al.
 and Xu et al. .
COARSE2FINE
Sketch accuracy.
For ONESTAGE,
sketches are extracted from the meaning representations it generates.
COUNT(President) WHERE (College = A)”, but
the query over table “∥College∥Number of Presidents∥” would be “SELECT Number of Presidents
WHERE (College = A)”.
We also examine the predicted sketches themselves in Table 5. We compare sketches generated
by COARSE2FINE against ONESTAGE. The latter
model generates meaning representations without
an intermediate sketch generation stage. Nevertheless, we can extract sketches from the output of
ONESTAGE following the procedures described in
Section 4. Sketches produced by COARSE2FINE
are more accurate across the board. This is not
surprising because our model is trained explicitly
to generate compact meaning sketches. Taken together (Tables 2–4), our results show that better
sketches bring accuracy gains on GEO, ATIS, and
DJANGO. On WIKISQL, the sketches predicted
by COARSE2FINE are marginally better compared
with ONESTAGE. Performance improvements on
this task are mainly due to the ﬁne meaning decoder.
We conjecture that by decomposing decoding into two stages, COARSE2FINE can better
match table columns and extract condition values
without interference from the prediction of condition operators. Moreover, the sketch provides a
canonical order of condition operators, which is
beneﬁcial for the decoding process .
Conclusions
In this paper we presented a coarse-to-ﬁne decoding framework for neural semantic parsing.
We ﬁrst generate meaning sketches which abstract
away from low-level information such as arguments and variable names and then predict missing details in order to obtain full meaning representations. The proposed framework can be easily
adapted to different domains and meaning representations. Experimental results show that coarseto-ﬁne decoding improves performance across
tasks. In the future, we would like to apply the
framework in a weakly supervised setting, i.e., to
learn semantic parsers from question-answer pairs
and to explore alternative ways of deﬁning meaning sketches.
Acknowledgments
We would like to thank
Pengcheng Yin for sharing with us the preprocessed version of the DJANGO dataset. We gratefully acknowledge the ﬁnancial support of the European Research Council (award number 681760;
Dong, Lapata) and the AdeptMind Scholar Fellowship program (Dong).