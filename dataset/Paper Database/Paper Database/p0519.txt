Fine-tuning Convolutional Neural Networks for Biomedical Image
Analysis: Actively and Incrementally*
Zongwei Zhou1, Jae Shin1, Lei Zhang1, Suryakanth Gurudu2, Michael Gotway2, and
Jianming Liang1
1Arizona State University
2Mayo Clinic
Intense interest in applying convolutional neural networks (CNNs) in biomedical image analysis is
wide spread, but its success is impeded by the lack of large annotated datasets in biomedical
imaging. Annotating biomedical images is not only tedious and time consuming, but also
demanding of costly, specialty-oriented knowledge and skills, which are not easily accessible. To
dramatically reduce annotation cost, this paper presents a novel method called AIFT (active,
incremental fine-tuning) to naturally integrate active learning and transfer learning into a single
framework. AIFT starts directly with a pre-trained CNN to seek ‚Äúworthy‚Äù samples from the
unannotated for annotation, and the (fine-tuned) CNN is further fine-tuned continuously by
incorporating newly annotated samples in each iteration to enhance the CNN‚Äôs performance
incrementally. We have evaluated our method in three different biomedical imaging applications,
demonstrating that the cost of annotation can be cut by at least half. This performance is attributed
to the several advantages derived from the advanced active and incremental capability of our AIFT
1. Introduction
Convolutional neural networks (CNNs) have brought about a revolution in computer
vision thanks to large annotated datasets, such as ImageNet and Places . As
evidenced by an IEEE TMI special issue and two forthcoming books , intense
interest in applying CNNs in biomedical image analysis is wide spread, but its success is
impeded by the lack of such large annotated datasets in biomedical imaging. Annotating
biomedical images is not only tedious and time consuming, but also demanding of costly,
specialty-oriented knowledge and skills, which are not easily accessible. Therefore, we seek
to answer this critical question: How to dramatically reduce the cost of annotation when
applying CNNs in biomedical imaging. In doing so, we present a novel method called AIFT
(active, incremental fine-tuning) to naturally integrate active learning and transfer learning
into a single framework. Our AIFT method starts directly with a pre-trained CNN to seek
*This research has been supported partially by NIH under Award Number R01HL128785, by ASU and Mayo Clinic through a Seed
Grant and an Innovation Grant. The content is solely the responsibility of the authors and does not necessarily represent the official
views of NIH.
HHS Public Access
Author manuscript
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in
PMC 2018 October 16.
 
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. 2017 July ; 2017: 4761‚Äì4772. doi:10.1109/
CVPR.2017.506.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
‚Äúsalient‚Äù samples from the unannotated for annotation, and the (fine-tuned) CNN is
continuously fine-tuned by incrementally enlarging the training dataset with newly
annotated samples. We have evaluated our method in three different applications including
colonoscopy frame classification, polyp detection, and pulmonary embolism (PE) detection,
demonstrating that the cost of annotation can be cut by at least half.
This outstanding performance is attributed to a simple yet powerful observation: To boost
the performance of CNNs in biomedical imaging, multiple patches are usually generated
automatically for each candidate through data augmentation; these patches generated from
the same candidate share the same label, and are naturally expected to have similar
predictions by the current CNN before they are expanded into the training dataset. As a
result, their entropy and diversity provide a useful indicator to the ‚Äúpower‚Äù of a candidate in
elevating the performance of the current CNN. However, automatic data augmentation
inevitably generates ‚Äúhard‚Äù samples for some candidates, injecting noisy labels; therefore, to
significantly enhance the robustness of our method, we compute entropy and diversity by
selecting only a portion of the patches of each candidate according to the predictions by the
current CNN.
Several researchers have demonstrated the utility of fine-tuning CNNs for biomedical image
analysis, but they only performed one-time fine-tuning, that is, simply fine-tuning a pretrained CNN once with available training samples involving no active selection processes
(e.g., ). To our knowledge, our proposed method is among the first
to integrate active learning into fine-tuning CNNs in a continuous fashion to make CNNs
more amicable for biomedical image analysis with an aim to cut annotation cost
dramatically. Compared with conventional active learning, our AIFT method offers several
advantages:
Starting with a completely empty labeled dataset, requiring no initial seed
labeled samples (see Alg. 1);
Incrementally improving the learner through continuous fine-tuning rather than
repeatedly re-training (see Sec. 3.1);
Naturally exploiting expected consistency among the patches associated for each
candidate to select samples ‚Äúworthy‚Äù of labeling (see Sec. 3.2);
Automatically handling noisy labels as only a portion (e.g., a quarter) of the
patches in each candidate participate in the selection process (see Sec. 3.3);
Computing entropy and diversity locally on a small number of patches within
each candidate, saving computation time considerably (see Sec. 3.3).
More importantly, our method has the potential to exert important impact on computer-aided
diagnosis (CAD) in biomedical imaging, because the current regulations require that CAD
systems be deployed in a ‚Äúclosed‚Äù environment, in which all CAD results be reviewed and
errors if any be corrected by radiologists; as a result, all false positives are supposed to be
dismissed and all false negatives supplied, an instant on-line feedback that may make CAD
Zhou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2018 October 16.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
systems self-learning and improving possible after deployment given the continuous finetuning capability of our method.
2. Related work
2.1. Transfer learning for medical imaging
Gustavo et al. replaced the fully connected layers of a pre-trained CNN with a new
logistic layer and trained only the appended layer with the labeled data while keeping the
rest of the network the same, yielding promising results for classification of unregistered
multiview mammograms. In , a fine-tuned pre-trained CNN was applied for localizing
standard planes in ultrasound images. Gao et al. fine-tuned all layers of a pre-trained
CNN for automatic classification of interstitial lung diseases. In , Shin et al. used finetuned pre-trained CNNs to automatically map medical images to document-level topics,
document-level sub-topics, and sentence-level topics. In , fine-tuned pre-trained CNNs
were used to automatically retrieve missing or noisy cardiac acquisition plane information
from magnetic resonance imaging and predict the five most common cardiac views. Schlegl
et al. explored unsupervised pre-training of CNNs to inject information from sites or
image classes for which no annotations were available, and showed that such across site pretraining improved classification accuracy compared to random initialization of the model
parameters. Tajbakhsh et al. systematically investigated the capabilities of transfer
learning in several medical imaging applications. However, they all performed one-time
fine-tuning‚Äîsimply fine-tuning a pre-trained CNN just once with available training
samples, involving neither active selection processes nor continuous fine-tuning.
2.2. Integrating active learning with deep learning
The literature of general active learning and deep learning is rich and deep . However, the research aiming to integrate active learning with deep learning is
sparse: Wang and Shang may be the first to incorporate active learning with deep
learning, and based their approach on stacked restricted Boltzmann machines and stacked
autoencoders. A similar idea was reported for hyperspectral image classification . Stark
et al. applied active learning to improve the performance of CNNs for CAPTCHA
recognition, while Al Rahhal et al. exploited deep learning for active electrocardiogram
classification. All these approaches are fundamentally different from our AIFT approach in
that in each iteration they all repeatedly re-trained the learner from scratch while we
continuously fine-tune the (fine-tuned) CNNs in an incremental manner, offering five
advantages as listed in Sec. 1.
3. Proposed method
We present our AIFT method in the context of computer-aided diagnosis (CAD) in
biomedical imaging. A CAD system typically has a candidate generator, which can quickly
produce a set of candidates, among which, some are true positives and some are false
positives. After candidate generation, the task is to train a classifier to eliminate as many
false positives as possible while keeping as many true positives as possible. To train a
Zhou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2018 October 16.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
classifier, each of the candidates must be labeled. We assume that each candidate takes one
of |Y| possible labels. To boost the performance of CNNs for CAD systems, multiple patches
are usually generated automatically for each candidate through data augmentation; these
patches generated from the same candidate inherit the candidate‚Äôs label. In other words, all
labels are acquired at the candidate level. Mathematically, given a set of candidates, = {
1, 2, ‚Ä¶, n}, where n is the number of candidates, and each candidate ùíûi = {xi
is associated with m patches, our AIFT algorithm iteratively selects a set of candidates for
labeling (illustrated in Alg. 1).
Algorithm 1
Active incremental fine-tuning method.
3.1. Continuous fine-tuning
At the beginning, the labeled dataset ‚Ñí is empty; we take a pre-trained CNN (e.g., AlexNet)
and run it on to select b number of candidates for labeling. The newly labeled candidates
will be incorporated into ‚Ñí to continuously fine-tune the CNN incrementally until the
performance is satisfactory. Several researchers have demonstrated that fine-tuning offers
better performance and is more robust than training from scratch. From our experiments, we
have found that continuously fine-tuning the CNN, which has been fine-tuned in the
Zhou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2018 October 16.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
previous iteration, with enlarged datasets converges faster than repeatedly fine-tuning the
original pre-trained CNN. We also found that continuously fine-tuning the CNN with only
newly labeled data demands careful meta-parameter adjustments.
3.2. Active candidate selection
In active learning, the key is to develop a criterion for determining the ‚Äúworthiness‚Äù of a
candidate for annotation. Our criterion is based on an observation: All patches generated
from the same candidate share the same label; they are expected to have similar predictions
by the current CNN. As a result, their entropy and diversity provide a useful indicator to the
‚Äúpower‚Äù of a candidate in elevating the performance of the current CNN. Intuitively, entropy
captures the classification certainty‚Äîhigher uncertainty values denote higher degrees of
information; while diversity indicates the prediction consistency among the patches within a
candidate‚Äîhigher diversity values denote higher degrees of prediction inconsistency among
the patches within a candidate. Therefore, candidates with higher entropy and higher
diversity are expected to contribute more in elevating the current CNN‚Äôs performance.
Formally, assuming the prediction of patch xi
j by the current CNN is pi
j, we define its
entropy as:
j, k log pi
and diversity between patches xi
l of candidate i as:
di(j, l) = ‚àë
Entropy ei
j denotes the information furnished by patch xi
j of candidate i in the unlabeled
pool. Diversity di(j, l), captured by the symmetric Kullback Leibler divergence ,
estimates the amount of information overlap between patches xi
l of candidate i. By
definition, all the entries in ei
j and di(j, l) are non-negative. Further, di(j, j) = 0, ‚àÄj, therefore,
for notational simplicity, we combine ei
j and di(j, l) into a single matrix Ri for each candidate
Ri(j, l) =
Œª2di(j, l) otherwise
where Œª1 and Œª2 are trade-offs between entropy and diversity. We use two parameters for
convenience, so as to easily turn on/off entropy or diversity during experiments.
Zhou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2018 October 16.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
3.3. Handling noisy labels via majority selection
Automatic data augmentation is essential to boost CNN‚Äôs performance, but it inevitably
generates ‚Äúhard‚Äù samples for some candidates as shown in Fig. 1 and Fig. 2(c), injecting
noisy labels; therefore, to significantly enhance the robustness of our method, we compute
entropy and diversity by selecting only a portion of the patches of each candidate according
to the predictions by the current CNN. Specially, for each candidate i we first compute the
average probabilistic prediction of all of its patches:
where m is the number of patches within candidate i, pi
j is the prediction probability of
j. If ai > 0.5, we select the top Œ± percent patches; otherwise, the bottom Œ± percent
patches. Based on the selected patches, we then use Eq. 3 to construct the score matrix Ri of
size Œ±m √ó Œ±m for each candidate i in . Our proposed majority selection method
automatically excludes the patches with noisy labels because of their low confidences. We
should note that the idea of combining entropy and diversity was inspired by , but there is
a fundamental difference because they computed R across the whole unlabeled dataset with
time complexity (m2), which is very computational expensive, while we compute Ri(j, l)
locally on the selected patches within each candidate, saving computation time considerably
with time complexity (Œ±2m2), where Œ± = 1/4 in our experiments.
3.4. An illustration of prediction patterns
Given unlabeled candidates = { 1, 2, ‚Ä¶, n} with ùíûi = {xi
m}, assuming the
prediction of patch xi
j by the current CNN is pi
j, we call the histogram of pi
j for j ‚àà [1,m] the
prediction pattern of candidate i. As shown in Column 1 of Tab. 1, there are seven typical
prediction patterns:
Pattern A: The patches‚Äô predictions are mostly concentrated at 0.5, with a higher
degree of uncertainty. Most active learning algorithms favor this type of
candidate as it is good at reducing the uncertainty.
Pattern B: It is flatter than Pattern A, as the patches‚Äô predictions are spread
widely from 0 to 1, yielding a higher degree of inconsistency. Since all the
patches belonging to a candidate are generated via data argumentation, they (at
least the majority of them) are expected to have similar predictions. This type of
candidate has the potential to contribute significantly to enhancing the current
CNN‚Äôs performance.
Pattern C: The patches‚Äô predictions are clustered at both ends, resulting in a
higher degree of diversity. This type of candidate is most likely associated with
noisy labels at the patch level as illustrated in Fig. 1, and it is the least favorable
in active selection because it may cause confusion in fine-tuning the CNN.
Zhou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2018 October 16.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Patterns D and E: The patches‚Äô predictions are clustered at one end (i.e., 0 or 1)
with a higher degree of certainty. The annotation of these types of candidates at
this stage should be postponed because the current CNN has most likely
predicted them correctly; they would contribute very little to fine-tuning the
current CNN. However, these candidates may evolve into different patterns
worthy of annotation with more fine-tuning.
Patterns F and G: They have higher degrees of certainty in some of the patches‚Äô
predictions and are associated with some outliers in the patches‚Äô predictions.
These types of candidates are valuable because they are capable of smoothly
improving the CNN‚Äôs performance. Though they may not make significant
contributions, they should not cause dramatic harm to the CNN‚Äôs performance.
4. Applications
In this section, we apply our method to three different applications including colonoscopy
frame classification, polyp detection, and pulmonary embolism (PE) detection. Our AIFT
algorithm is implemented in the Caffe framework based on the pre-trained AlexNet
model . In the following, we shall evaluate six variants of AIFT (active incremental finefuning) including Diversity1/4 (using diversity on 1/4 of the patches of each candidate),
Diversity (using diversity on all the patches of each candidate), Entropy1/4, Entropy,
(Entropy+Diversity)1/4, (Entropy+ Diversity), and compare them with IFT Random
(incremental fine-tuning with random candidate selection) and Learning from Scratch in
terms of AUC (area under ROC curve).
4.1. Colonoscopy Frame Classification
Objective quality assessment of colonoscopy procedures is vital to ensure high-quality
colonoscopy. A colonoscopy video typically contains a large number of non-informative
images with poor colon visualization that are not ideal for inspecting the colon or
performing therapeutic actions. The larger the fraction of non-informative images in a video,
the lower the quality of colon visualization, thus the lower the quality of colonoscopy.
Therefore, one way to measure the quality of a colonoscopy procedure is to monitor the
quality of the captured images. Technically, image quality assessment at colonoscopy can be
formulated as an image classification task whereby an input image is labeled as either
informative or non-informative.
For the experiments, 4,000 colonoscopy frames are selected from 6 complete colonoscopy
videos. A trained expert then manually labeled the collected images as informative or noninformative. A gastroenterologist further reviewed the labeled images for corrections. The
labeled frames at the video level are separated into training and test sets, each containing
approximately 2,000 colonoscopy frames. For data augmentation, we extracted 21 patches
from each frame.
In all three applications, our AIFT begins with an empty training dataset and directly uses
AlexNet pre-trained on ImageNet. Fig. 3 shows that at the first step (with 2 labels queried),
Zhou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2018 October 16.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
IFT Random yields the best performance. There are two possible reasons: (1) random
selection gives the samples with the positive/negative ratio compatible with the test dataset;
(2) the pre-trained AlexNet gives poor predictions on our dataset, as it was trained by natural
images instead of biomedical images. Its output probabilities are mostly confused or even
incorrect, yielding poor selection scores. However, AIFT Diversity1/4, Entropy, Entropy1/4
quickly surpass IFT Random after the first fine-tuning, as they select important samples for
fine-tuning, making the training process more efficient than just randomly selecting from the
remaining training dataset. AIFT Entropy and Diversity1/4 with only 4 label queries can
achieve the performance of IFT Random with 18 label queries, and that of Learning from
Scratch with 22 randomly selected frames. Thereby, more than 75% labeling cost could be
saved from IFT Random and 80% from Learning from Scratch.
AIFT Diversity works even poorer than IFT Random because of noisy labels generated
through data augmentation. AIFT Diversity strongly favors frames whose prediction pattern
resembles Pattern C (see Tab. 1). Naturally, it will most likely select an ambiguous frame
such as Fig. 1 and Fig. 2(c), because predictions of its patches are highly diverse. All patches
generated from the same frame inherit the same label as the frame; therefore, at the patch
level, the labels are very noisy for the ambiguous frames. AIFT Entropy, Entropy1/4, and
Diversity1/4 can automatically exclude the noisy label, naturally yielding outstanding
performance. Given the outstanding performance of AIFT Entropy, Entropy1/4, and
Diversity1/4, one may consider combining entropy and diversity, but unfortunately,
combinations do not always give better performance, because finding a nice balance
between entropy and diversity is tricky as shown in our example analysis in Tab. 1 and
supplementary material.
4.2. Polyp Detection
Colonoscopy is the preferred technique for colon cancer screening and prevention. The goal
of colonoscopy is to find and remove colonic polyps‚Äîprecursors to colon cancer‚Äîas shown
in Fig. 4. For polyp detection, our database contains 38 short colonoscopy videos from 38
different patients, and they are separated into the training dataset (21 videos; 11 with polyps
and 10 without polyps) and the testing dataset (17 videos; 8 videos with polyps and 9 videos
without polyps). There are no overlaps between the training dataset and testing dataset at the
patient level. Each colonoscopy frame in the data set comes with a binary ground truth
image. 16300 candidates and 11950 candidates were generated from the training dataset and
testing dataset, respectively.
At each polyp candidate location with the given bounding box, we perform a data
augmentation by a factor f ‚àà {1.0, 1.2, 1.5}. At each scale, we extract patches after the
candidate is translated by 10 percent of the resized bounding box in vertical and horizontal
directions. We further rotate each resulting patch 8 times by mirroring and flipping. The
patches generated by data augmentation belong to the same candidate.
Fig. 5 shows that AIFT (Entropy+Diversity)1/4 and Diversity1/4 reach the peak performance
with 610 label queries, while IFT Random needs 5711 queries, indicating that AIFT can cut
nearly 90% of the annotation cost required by IFT Random. The fast convergence of AIFT
Zhou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2018 October 16.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
(Entropy+Diversity)1/4 and Diversity1/4 is attributed to the majority selection method, which
can efficiently select the informative and representative candidates while excluding those
with noisy labels. When the queried number is about 5000, the AIFT Entropy1/4 reaches its
peak performance. The reason is that the entropy can only measure the informativeness so
the queried sample is very likely to be similar to each other. It needs more queries to select
most of the informative candidates. AIFT Diversity and (Entropy+ Diversity) cannot
perform as well as the counterparts with the majority selection due to noisy labels. Learning
from Scratch never achieves the performance of fine-tuning even if all training samples are
used, which is in agreement with .
To gain further insights, we also monitor the performance of the 8 methods on the remaining
training dataset. Each time after we have fine-tuned the previous CNN, we test it on the
remaining training dataset. We have observed that only 800 candidates are needed to reach
the maximum performance. As is shown in Fig. 6, the candidates selected by our method,
which are only 5% (800/16300) of all the candidates, can represent the remaining dataset,
because in colonoscopy videos consecutive frames are usually similar to each other.
4.3. Pulmonary Embolism Detection
Our experiments are based on the PE candidates generated by the method proposed in 
and the image representation introduced in as shown in Fig. 7. We adopt the 2-channel
representation because it consistently captures PEs in cross-sectional and longitudinal views
of vessels, achieving greater classification accuracy and accelerating CNN training process.
In order to feed the RGB-like patches into CNN, the 2-channel patches are converted to 3channel RGB-like patches by duplicating the second channel. For experiments, we use a
database consisting of 121 CTPA datasets with a total number of 326 PEs. The tobogganing
algorithm is applied to obtain a crude set of PE candidates. 6255 PE candidates are
generated, of which 5568 are false positives and 687 are true positives. To train CNN, we
extract patches of 3 different physical sizes, i.e.,10 mm-, 15 mm-, and 20 mm-wide. Then,
we translate each candidate location along the direction of the affected vessel 3 times, up to
20% of the physical size of each patch. Then, data augmentation for training dataset is
performed by rotating the longitudinal and cross-sectional vessel planes around the vessel
axis, resulting in 5 additional variations for each scale and translation.
Finally, a stratified training dataset with 434 true positive PE candidates and 3406 false
positive PE candidates would be generated for training and incrementally fine-tuning the
CNN and a testing dataset with 253 true positive PE candidates and 2162 false positive PE
candidates. The overall PE probability is calculated by averaging the probabilistic prediction
generated for the patches within PE candidate after data augmentation.
Fig. 8 compares the 8 methods on the testing dataset. The performance of each method
becomes saturated after 2000 labels queried. AIFT (Entropy+Diversity)1/4 and Diversity1/4
converge the fastest among the 8 methods and yields the best overall performance, attributed
to majority selection method proposed in this work. AIFT (Entropy+Diversity)1/4 and
Diversity1/4 with only 1000 labels required can achieve the performance of random selecting
2200 labels fine-tune from AlexNet (IFT Random). Note that even AIFT Diversity reach its
Zhou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2018 October 16.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
peak performance when about 3100 samples queried because PE data set injected little noisy
labels. Since entropy favors the uncertain ambiguous samples, both AIFT Entropy1/4 and
Entropy perform bad at the beginning. IFT Random outperforms at the first few steps as
analysed in Sec. 4.1, but increase slowly overall. Based on this analysis, the cost of
annotation can be cut at least half by the our method.
4.4. Observations on selected patterns
We meticulously monitored the active selection process and examined the selected
candidates, as an example, we include the top 10 candidates selected by the six AIFT
methods at Iteration 3 in colonoscopy frame classification in the supplementary material (see
Fig. 10). From this process, we have observed the following:
Patterns A and B are dominant in the earlier stages of AIFT as the CNN has not
been fine-tuned properly to the target domain.
Patterns C, D and E are dominant in the later stages of AIFT as the CNN has
been largely fine-tuned on the target dataset.
The majority selection‚ÄîAIFT Entropy1/4, Diversity1/4, or (Entropy
+Diversity)1/4‚Äîis effective in excluding Patterns C, D, and E, while AIFT
Entropy (without the majority selection) can handle Patterns C, D, and E
reasonably well.
Patterns B, F, and G generally make good contributions to elevating the current
CNN‚Äôs performance.
AIFT Entropy and Entropy1/4 favor Pattern A because of its higher degree of
uncertainty as shown in Fig. 10.
AIFT Diversity1/4 prefers Pattern B while AIFT Diversity prefers Pattern C (Fig.
10). This is why AIFT Diversity may cause sudden disturbances in the CNN‚Äôs
performance and why AIFT Diversity1/4 should be preferred in general.
Combing entropy and diversity would be highly desirable, but striking a balance
between them is not trivial, because it demands application-specific Œª1 and Œª2
(see Eq. 3) and requires further research.
5. Conclusion, discussion and future work
We have developed an active, incremental fine-tuning method, integrating active learning
with transfer learning, offering several advantages: It starts with a completely empty labeled
dataset, and incrementally improves the CNN‚Äôs performance through continuous fine-tuning
by actively selecting the most informative and representative samples. It also can
automatically handle noisy labels via majority selection and it computes entropy and
diversity locally on a small number of patches within each candidate, saving computation
time considerably. We have evaluated our method in three different biomedical imaging
applications, demonstrating that the cost of annotation can be cut by at least half. This
Zhou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2018 October 16.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
performance is attributed to the advanced active and incremental capability of our AIFT
We based our experiments on the AlexNet architecture because a pre-trained AlexNet model
is available in the Caffe library and its architecture strikes a nice balance in depth: it is deep
enough that we can investigate the impact of AIFT on the performance of pre-trained CNNs,
and it is also shallow enough that we can conduct experiments quickly. Alternatively, deeper
architectures such as VGG, GoogleNet, and Residual network could have been used and
have shown relatively high performance for challenging computer vision tasks. However, the
purpose of this work is not to achieve the highest performance for different biomedical
image tasks but to answer the critical question: How to dramatically reduce the cost of
annotation when applying CNNs in biomedical imaging. The architecture and learning
parameters are reported in the supplementary material.
In the real world, datasets are usually unbalanced. In order to achieve good classification
performance, both classes of samples should be used in training. Fig. 9 shows the positive/
negative label ratio of the samples selected by the six methods in each iteration in
colonoscopy quality application. For random selection, the ratio is nearly the same as whole
training dataset, a reason that IFT Random has stable performance at the cold-start. AIFT
Diversity1/4, Entropy1/4 and Entropy seem capable of keeping the dataset balanced
automatically, a new observation that deserves more investigation in the future.
We choose to select, classify and label samples at the candidate level. Labeling at the patient
level would certainly reduce the cost of annotation more but introduce more severe label
noise; labeling at the patch level would cope with the label noise but impose a much heavier
burden on experts for annotation. We believe that labeling at the candidate level offers a
sensible balance in our three applications.
Finally, in this paper, we use only entropy and diversity as the criteria. In theory, a large
number of active selection methods may be designed, but we have found that there are only
seven fundamental patterns as summarized in the Sec. 3.4. As a result, we could
conveniently focus on comparing the seven patterns rather than the many methods. Multiple
methods may be used to select a particular pattern: for example, entropy, Gaussian distance,
and standard deviation would seek Pattern A, while diversity, variance, and divergence look
for Pattern C. We would not expect significant performance differences among the methods
within each group, resulting in six major selction methods for deep comparisons based on
real-world clinical applications.
Supplementary Material
Refer to Web version on PubMed Central for supplementary material.