Evaluating Compound Critiquing Recommenders:
A Real-User Study
James Reilly
Adaptive Information Cluster
School of Computer Science &
Informatics
UCD Dublin, Ireland
 
Jiyong Zhang
Human Computer Interaction
Group, Swiss Federal Institute
of Technology (EPFL),
Lausanne, Switzerland
 
Lorraine McGinty
Adaptive Information Cluster
School of Computer Science &
Informatics
UCD Dublin, Ireland
 
Human Computer Interaction
Group, Swiss Federal Institute
of Technology (EPFL),
Lausanne, Switzerland
 
Barry Smyth
Adaptive Information Cluster
School of Computer Science &
Informatics
UCD Dublin, Ireland
 
Conversational recommender systems are designed to help
users to more eﬃciently navigate complex product spaces by
alternatively making recommendations and inviting users’
feedback. Compound critiquing techniques provide an eﬃcient way for users to feed back their preferences (in terms
of several simultaneous product attributes) when interfacing with conversational recommender systems. For example, in the laptop domain a user might wish to express
a preference for a laptop that is Cheaper, Lighter, with a
Larger Screen. While recently a number of techniques for
dynamically generating compound critiques have been proposed, to date there has been a lack of direct comparison of
these approaches in a real-user study. In this paper we will
compare two alternative approaches to the dynamic generation of compound critiques based on ideas from data mining and multi-attribute utility theory. We will demonstrate
how both approaches support users to more eﬃciently navigate complex product spaces highlighting, in particular, the
inﬂuence of product complexity and interface strategy on
recommendation performance and user satisfaction.
Categories and Subject Descriptors
H.1.2 [Models and Principles]: User/Machine Systems—
Human factors, Human information processing; H.5.2 [Information Interfaces and Presentation]: User Interfaces—
Evaluation/methodology.
General Terms
Human Factors, Performance, Experimentation.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
EC’07, June 13–16, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-653-0/07/0006 ...$5.00.
user study, compound critiquing, recommender system, recommendation performance, user satisfaction.
INTRODUCTION
Developing eﬀective product recommendation systems is
an important and challenging problem . It is made dif-
ﬁcult for a variety of reasons. Very often users are not familiar with the details of a particular product domain, or
may not fully understand or appreciate the trade-oﬀs that
exist between diﬀerent product features. Many types of recommender systems have been developed to help users locate items of preference, from the very successful single-shot
collaborative systems to the more recent content-based
conversational systems . In this paper we will focus on the
conversational-type systems, which are commonly used to
help users to navigate through complex product-spaces. The
user is guided through a sequence of recommendation cycles
in which one or more products are recommended based on
some evolving model of the user’s requirements. During each
cycle the user is oﬀered the opportunity to provide feedback
in order to steer the recommender in the direction of their
desired product. Unfortunately users rarely provide complete or accurate product speciﬁcations to begin with and
their feedback can be inconsistent and contradictory.
One feature of intelligent user interfaces is an ability to
make decisions that take into account a variety of factors,
some of which may depend on the current situation . Consequently, it is crucial that user interfaces provide appropriate feedback mechanisms for the domain and users in question. Recently researchers have begun to consider the use of
diﬀerent forms of feedback in recommender systems along
a variety of dimensions.
From an interfacing standpoint,
diﬀerent forms of feedback assume diﬀerent degrees of domain expertise and require diﬀerent levels of user eﬀort .
For example, value elicitation, where users indicate a precise feature value — “I want a digital camera with 512MB
of storage”, for example — assumes that users have detailed
domain knowledge and that they are willing to indicate the
precise requirements on a feature by feature basis. In contrast, preference-based feedback asks the user only to indicate a preference for one suggestion over another .
In this paper we are interested in a form of feedback known
as critiquing; see . Critiquing can be viewed as a compromise between the detail provided with value elicitation and
the ease of feedback associated with preference-based methods.
To critique a product a user indicates a directional
change to a speciﬁc feature. For example, a digital camera
shopper might ask for a camera that is more expensive than
the current suggestion; this is a critique over the price feature. More speciﬁcally, in this paper we describe a recent
variation on critiquing known as dynamic critiquing, which
involves the automatic generation of compound critiques at
recommendation time. Compound critiques are collections
of individual feature critiques and allow the user to indicate
a richer form of feedback. For example, the user might indicate that they are interested in a digital camera with a high
resolution and a lower price than the current recommendation by selecting a lower price, higher resolution compound
critique. Importantly, these compound critiques are generated based on an assessment of the characteristics of remaining products as they relate to the current recommendation.
In this paper we compare two alternative approaches: the
Apriori-based approach originally introduced by and a
more recent multi-attribute utility theory based approach
introduced by . We will compare and contrast each critique generation strategy, under diﬀerent data-set and interface conditions, in terms of overall recommendation performance and user satisfaction.
This paper is organized as follows.
The related background work on critiquing is reviewed brieﬂy in Section 2.
Section 3 introduces the two approaches for dynamically
generating compound critiques. In Section 4 we report in
detail the design of the real-user study and the results that
we found. Finally, Section 5 presents the conclusions.
BACKGROUND
Critiquing was ﬁrst introduced as a form of feedback for
recommender interfaces as part of the FindMe recommender
systems , and is perhaps best known for the role it
played in the Entr´ee restaurant recommender. During each
cycle Entr´ee presents users with a ﬁxed set of critiques to
accompany a suggested restaurant case, allowing users to
tweak or critique this case in a variety of directions; for
example, the user may request another restaurant that is
cheaper or more formal, for instance, by critiquing its price
and style features. A similar interface approach was later
adopted by the RentMe and Car Navigator recommenders
from the same research group.
As a form of feedback critiquing has many advantages.
From a user-interface perspective it is relatively easy to incorporate into even the most limited of interfaces. For example, the typical “more” and “less” critiques can be readily
presented as simple icons or links alongside an associated
product feature value and can be chosen by the user with
a simple selection action. Contrast this to value elicitation
approaches where the interface must accommodate text entry for a speciﬁc feature value from a potentially large set
of possibilities, via drop-down list, for example.
In addition, critiquing can be used by users who have only limited
understanding of the product domain e.g. a digital camera
buyer may understand that greater resolution is preferable
but may not be able to specify a concrete target resolution.
While critiquing enjoys a number of signiﬁcant usability
beneﬁts, as indicated above, it can suﬀer from the fact that
the feedback provided by the user is rarely suﬃciently detailed to sharply focus the next recommendation cycle. For
example, by specifying that they are interested in a digital
camera with a greater resolution than the current suggestion
the user is helping the recommender to narrow its search but
this may still lead to a large number of available products
to chose from. Contrast this with the scenario where the
user indicates that they are interested in a 5 megapixel camera, which is likely to reduce the number of product options
much more eﬀectively. The result is that critiquing-based
recommenders can suﬀer from protracted recommendation
sessions, when compared to value elicitation approaches.
The critiques described so far are all examples of, what we
refer to as, unit critiques. That is, they express preferences
over a single feature; Entr´ee’s cheaper critiques a price feature, and more formal critiques a style feature, for example.
This too ultimately limits the ability of the recommender to
narrow its focus, because it is guided by only single-feature
preferences from cycle to cycle. Moreover it encourages the
user to focus on individual features as if they were independent and can result in the user following false-leads.
example, a price-conscious digital camera buyer might be
inclined to critique the price feature until such time as an
acceptable price has been achieved only to ﬁnd that cameras
in this region of the product space do not satisfy their other
requirements (e.g., high resolution). The user will have no
choice but to roll-back some of these price critiques, and will
have wasted considerable eﬀort to little or no avail.
An alternative strategy is to consider the use of what we
call compound critiques . These are critiques that operate over multiple features. This idea of compound critiques
is not novel. In fact the seminal work of Burke et al. 
refers to critiques for manipulating multiple features. For
instance, in the Car Navigator system, an automobile recommender, users are given the option to select a sportier
critique. By clicking on this, a user can increase the horsepower and acceleration features, while allowing for a greater
price. Similarly we might use a high performance compound
critique in a PC recommender to simultaneously increase
processor speed, RAM, hard-disk capacity and price features.
Obviously compound critiques have the potential to improve recommendation eﬃciency because they allow the recommender system to focus on multiple feature constraints
within a single cycle. However, until recently, the usefulness
of compound critiques has been limited by their static nature. The compound critiques have been hard-coded by the
system designer so that the user is presented with a ﬁxed set
of compound critiques in each recommendation cycle. These
compound critiques may, or may not, be relevant depending
on the products that remain at a given point in time. For
instance, in the example above the sportier critique would
continue to be presented as an option to the user despite
the fact that the user may have already seen and declined
all the relevant car options.
DYNAMICALLY GENERATING
COMPOUND CRITIQUES
In this paper we will review and compare two diﬀerent
approaches to the dynamic generation of compound critiques. The ﬁrst approach, which we will call Apriori, uses
a data-mining algorithm to discover patterns in the types
of products remaining, then converts these patterns into
compound critiques. The second approach, MAUT, takes a
utility-based decision theory approach to identify the most
suitable products for users and converts these into a compound critique representation. Prompted by feedback from
peers to both of our research groups, we set out to design
a suitable evaluation platform that could be used to comparatively evaluate these techniques in a realistic product
recommender. Ideally, this exercise would allow us to learn
how to improve and/or look at ways of marrying ideas from
both approaches.
In this paper we summarize our initial
ﬁndings from a ﬁrst real-user trial using this evaluation platform which implements both of the compound critiquing approaches (further described below).
APPROACH 1: APRIORI
One strategy for dynamically generating compound critiques, proposed in , discovers feature patterns that are
common to remaining products on every recommendation
cycle. Essentially, each compound critique describes a set
of products in terms of the feature characteristics they have
in common. For example in the PC domain, a typical compound critique might be for Faster CPU and a Larger Hard-
Disk. By clicking on this the user narrows the focus of the
recommender to only those products that satisfy these feature preferences. The Apriori data-mining algorithm is
used to quickly discover these patterns and convert them
into compound critiques on each recommendation cycle.
The ﬁrst step involves generating critique patterns for each
of the remaining product options in relation to the currently presented example.
Figure 1 shows how a critique
pattern for a sample product p diﬀers from the current recommendation for its individual feature critiques.
For example, the critique pattern shown includes a “<” critique
for Price— we will refer to this as [Price <]—because the
comparison laptop is cheaper than the current recommendation. The next step involves mining compound critiques
by using the Apriori algorithm to identify groups of
recurring unit critiques; we might expect to ﬁnd the cooccurrence of unit critiques like [ProcessorSpeed >] infers
[Price >].
Apriori returns lists of compound critiques of
the form {[ProcessorSpeed >], [Price >]} along with their
support values (i.e., the % of critique patterns for which the
compound critique holds).
Figure 1: Generating a critique pattern.
It is not practical to present large numbers of diﬀerent
compound critiques as user-feedback options in each cycle.
For this reason, a ﬁltering strategy is used to select the k
most useful critiques for presentation based on their support
values. Importantly, compound critiques with low support
values eliminate many more products from consideration if
chosen. More recent work in the area considers compound
critique diversity during the ﬁltering stage, reducing compound critique repetition and better coverage of the product
space .
The ﬁnal step involves constructing a model of user preferences from the critiques speciﬁed so far. Importantly, users
are not always consistent in the feedback they provide, so
the aim of the model is to resolve any preference conﬂicts
that may arise as the session proceeds. Put simply, when
making a recommendation, the system computes a compatibility score for every product (informed by their critiquing
history), and ranks them accordingly. This incremental critiquing approach has been shown to deliver signiﬁcant
beneﬁts in terms of recommendation quality and eﬃciency
in prior evaluations.
APPROACH 2: MAUT
Recently, Zhang and Pu developed an alternative
strategy for generating compound critiques based on the
well-known Multi-Attribute Utility Theory (MAUT) . In
each interaction cycle the system determines a list of products via the user’s preference model, and then generates
compound critiques by comparing them with the current reference product. The system adaptively maintains a model of
the user’s preference model based on user’s critique actions
during the interaction process, and the compound critiques
are determined according to the utilities they gain instead
of the frequency of their occurrences in the data set.
This approach uses the simpliﬁed weighted additive form
to calculate the utility of a product O = ⟨x1, x2, ..., xn⟩as
U(⟨x1, · · · , xn⟩) =
where n is the number of attributes that the products may
have, the weight wi(1 ≤i ≤n) is the importance of the attribute i, and Vi is a value function of the attribute xi which
can be given according to the domain knowledge during the
design time.
The system constructs a preference model which contains
the weights and the preferred values for the product attributes to represent the user’s preferences. At the beginning
of the interaction process, the initial weights are equally set
to 1/n and the initial preferences are stated by the user.
Instead of mining the critiques directly from the data set
based on the Apriori algorithm, the MAUT approach ﬁrst
determines top K (in practice we set K = 5) products with
maximal utilities, and then each of the top K products are
converted into compound critique representation, by comparing them with the current reference product in the same
way as described in the previous section.
When the user selects a compound critique, the corresponding product is assigned as the new reference product,
and the user’s preference model is updated based on this
critique selection.
For each attribute, the attribute value
of the new reference product is assigned as the preference
value, and the weight of each attribute is adaptively adjusted
according to the diﬀerence between the old preference value
and the new preference value. Based on the new reference
product and the updated preference model, the system recommends another set of compound critiques. A more indepth explanation of this approach to generating compound
critiques is contained in .
Table 1: Design of Trial 1 
Dataset: Laptop
(37 users)
(46 users)
REAL-USER EVALUATION
Previous studies have highlighted the eﬀectiveness of dynamic compound critiques over unit-critiques in oﬄine simulations and in real user trials. Apriori-generated compound
critiques have been shown to help deliver signiﬁcant reductions in session-length , and users have also reported
greater satisfaction when using such systems . In a simulated environment, MAUT-generated compound critiques
have shown further improvements in terms recommendation
eﬃciency . However, a direct comparison of these techniques in a real-user evaluation setting is needed to fully
understand their relative pros and cons.
Accordingly, we designed a trial that asks users to compare two systems; one implementing the Apriori approach,
and one implementing the MAUT approach. For this trial
(referred to as Trial 1), we gathered a dataset of 400 laptop
computers. A total of 83 users separately evaluated both
systems by using each system to ﬁnd a laptop that they
would be willing to purchase. The order in which the diﬀerent systems were presented was randomized and at the start
of the trial they were provided with a brief description of the
basic recommender interface to explain the use of unit and
compound critiques and basic system operation.
The results from Trial 1 indicate that the MAUT-based approach
for generating compound critiques had a slight advantage
in terms of recommendation eﬃciency, the applicability of
the compound critiques and overall user satisfaction. The
results from this trial are reported in more detail in .
However, this trial was limited in two important ways.
Firstly, the interface used to present the MAUT-generated
compound critiques was diﬀerent to the interface used to
present the Apriori-generated compound critiques; each conveyed diﬀerent types and amounts of information.
interfaces were selected as they had been used in prior evaluations of the respective approaches and Figures 9 (simpliﬁed) and 10 (detailed) illustrate the diﬀerences between
the two interfaces.
The simpliﬁed interface was used to
display Apriori-generated compound critiques, translating
them into one line of descriptive text.
The MAUT compound critiques were displayed in the more informative detailed interface. Each MAUT compound critique was separated into two parts, highlighting the attributes that will be
improved if the critique is chosen, as well as the compromises
that will have to be made. In addition, the user is given
the opportunity to examine the product that will be recommended on the next cycle if the compound critique is chosen.
We believe that in this trial, the interface for presenting the
compound critiques was having a greater inﬂuence than the
compound critiques themselves on individual users. Hence
it was not possible to attribute the observed performance
Table 2: The datasets used in the oﬄine evaluation
of the dynamic critiquing recommenders.
# Products
# Ordinal Attributes
# Nominal Attributes
diﬀerence to the diﬀerence in critique-generation strategy
since the relative importance of the interface diﬀerences was
The second limitation was that it was performed on one
dataset only – the laptop dataset. In reality, an e-commerce
recommender may be used for many diﬀerent types of products. It maybe reasonable to assume that the results from a
real-user evaluation on one dataset may not be the same on
other datasets. For example, we may ﬁnd that a system employing Apriori-generated critiques performs better on one
dataset, and MAUT-generated critiques perform better on
another. Also, as some of our peers have suggested, asking
users to perform the evaluation on the same dataset twice
with diﬀerent recommenders might bias the results towards
the second system, as users will have become more familiar
with the product domain.
To address the limitations highlighted in Trial 1, we commissioned a second trial (referred to as Trial 2). For this
trial we decided to homogenize the interfaces used by both
techniques by using the detailed interface style for both
the Apriori and MAUT-generated compound critiques. In
this way we can better evaluate the impact of the diﬀerent
critique-generation strategies. In addition, we also used another dataset (containing 103 digital cameras) in order to
thwart a domain learning eﬀect. Table 2 lists the characteristics of the two datasets used in this trial. The attributes
used to describe the digital camera dataset can be seen in
Figure 8, and the attributes for the laptop dataset are shown
in Figure 10.
For Trial 2 we used a within-subjects design. Each participant evaluated the two critiquing-based recommenders in
sequence. In order to avoid any carryover eﬀect, we developed four (2 × 2) experiment conditions. The manipulated
factors are recommenders order (MAUT ﬁrst vs.
ﬁrst) and product dataset order (digital camera ﬁrst vs.
laptop ﬁrst).
Participants were evenly assigned to one of
the four experiment conditions, resulting in a sample size of
roughly 20 subjects per condition cell. Table 3 shows the
details of the user-study design.
This trial was implemented as an online web application
of two stages containing all instructions, interfaces and questionnaires. The wizard-like trial procedure was easy to follow and all user actions were automatically recorded in a
log ﬁle. During the ﬁrst stage, users were instructed to ﬁnd
a product (laptop or camera) they would be willing to purchase if given the opportunity. After making a product selection, they were asked to ﬁll in a post-stage questionnaire
to evaluate their view of the eﬀort involved, their decision
conﬁdence, and their level of trust in the recommender sys-
Table 3: Design of Trial 2 
Interface: Detailed
(19 users)
(23 users)
(22 users)
(19 users)
Table 4: Demographic characteristics of participants
Characteristics
(83 users)
(85 users)
Switzerland
Other Countries
Experience
tem. Next, decision accuracy was estimated by asking each
participant to compare their chosen product to the full list
of products to determine whether or not they preferred another product.
The second stage of the trial was almost
identical, except that this time the users were evaluation a
diﬀerent approach/dataset combination. Finally, after completing both stages, participants were presented with a ﬁnal
questionnaire which asked them to compare both recommender systems. Figures 7 to 10 at the end of this paper,
present some screenshots of the platform we developed for
these real-user trials.
Recommendation Efﬁciency
To be successful, recommender systems must be able to
eﬃciently guide a user through a product-space and, in general, short recommendation sessions are to be preferred.
For this evaluation, we measure the length of a session in
terms of recommendation cycles, i.e. the number of products viewed by users before they accepted the system’s recommendation. For each recommender/dataset combination
we averaged the session-lengths across all users. It is important to remember that any sequencing bias was eliminated by randomizing the presentation order in terms of
critiquing technique and dataset: Sometimes users evaluated the Apriori-based approach ﬁrst and other times they
used the MAUT-based approach ﬁrst. Similarly, sometimes
users operated on the camera dataset ﬁrst and other times
on the laptop dataset ﬁrst.
Figure 2 presents the results of the evaluation on the laptop dataset showing the average number of cycles for Apriori and MAUT based recommenders according to whether
users used the Apriori or the MAUT-based system ﬁrst or
Laptop: Average Session Length
Average session lengths for both approaches on the laptop dataset.
second. The results presented for the Laptop/MAUT combination are consistent with the results from Trial 1, with
users needing between 9.2 and 10.1 cycles to reach their
target product.
However we see that the Apriori system
performs better, with reduced session-lengths of between 6.6
and 7.0 cycles, an improvement over the results reported in
the previous trial, where average session lengths of 8.9 cycles were reported . The reason for this improvement
appears to be the more informative interface that was used
in the current trial and suggests that the Apriori-based approach can lead to reduced session lengths, compared to the
MAUT-based approach, under this more equitable interface
condition.
Despite these beneﬁts enjoyed by the Apriori-based approach on the laptop dataset similar beneﬁts, in terms of reduced session length, were not found for the camera dataset.
The results for this dataset are presented in Figure 3, and
clearly show a beneﬁt for the MAUT-based approach to critique generation, which enjoyed an average session length
of 4.1 cycles, compared to 8.5 cycles for the Apriori-based
approach (signiﬁcantly diﬀerent, p = 0.016).
Dataset complexity is likely to be a factor when it comes
to explaining this diﬀerence in performance. For example,
the increased complexity of the laptop dataset (403 products or 10 attributes) compared to camera dataset (103
products of 8 attributes) suggests that the Apriori approach
may oﬀer improvements over MAUT in more complex product spaces. Overall, both recommenders are quite eﬃcient.
From a database of over 100 digital cameras, both are able
to recommend cameras that users are willing to purchase in
10 cycles or less, on average. The results indicate that both
recommenders are also very scalable. For instance, the laptop database contains over 400 laptop computers and yet
users still ﬁnd suitable laptops in just over 10 cycles. Although the product catalogue size has increased four-fold,
session-lengths have increased by just 30% on average.
Recommendation Accuracy
Session-length is just one performance metric for a conversational recommender system. Recommenders should also
be measured by the quality of the recommendations made to
users over the course of a session . One way to estimate
recommendation quality is to ask users to review their ﬁnal
selection with reference to the full set of products (see ).
Camera: Average Session Length
Average session lengths for both approaches on the camera dataset.
Accordingly the quality or accuracy of the recommender can
be evaluated in terms of percentage of times that the user
chooses to stick with their selected product. If users consistently select a diﬀerent product the recommender is judged
to be not very accurate.
If they usually stick with their
selected product then the recommender is considered to be
The real-world datasets in this trial are relatively large
compared to datasets used in other real-user trials and the
amount of products contained in these datasets presented
us with some interface problems. For example, the laptop
dataset contains over 400 products. Revealing all of these
products to the users at once would lead user confusion.
Also, presenting large numbers of products makes it very
diﬃcult for users to locate the actual product they were
recommended. To deal with this, we designed the interface
to show 20 products at a time while also providing the users
with the facility to sort the products by attribute.
an interface is called ranked −list and had been used as
baseline in earlier research .
The bottom half of the
interface showed the product they originally accepted and
allowed them to select that if they so wished.
Figure 4 presents the average accuracy results for both
approaches on both datasets. Interestingly it appears that
the MAUT approach produces more accurate recommendations. For example, it achieves 68.4% accuracy on the laptop
dataset and 82.5% on the camera dataset. This means that,
on average, 4 out of 5 users didn’t ﬁnd a better camera
when the entire dataset of cameras was revealed to them.
The Apriori approach performed reasonably well, achieving
an accuracy of 57.9% and 64.6% on the camera and laptop
datasets respectively.
The diﬀerence in accuracy between
the two approaches on camera dataset is signiﬁcant (82.5%
vs 57.9%, p = 0.015). However, the diﬀerence in accuracy on
laptop dataset is no signiﬁcant(68.4% vs. 64.6%, p = 0.70).
Thus, despite the fact that users seemed to enjoy shorter
sessions using the Apriori-based approach on the laptop
dataset, they turned out to be selecting less optimal products as a result of these sessions. Users were signiﬁcantly
more likely to stick with their chosen laptop when using the
MAUT-based recommender.
User Experience
In addition to the above performance-based evaluation
Recommendation Accuracy
Average recommendation accuracy of
both approaches on both datasets.
Table 5: Evaluation Questionnaire
I found the compound critiques easy to understand.
I didn’t like this recommender, and I would never
use it again.
I did not ﬁnd the compound critiques informative.
I found the unit-critiques better at searching for
laptops (or digital cameras).
Overall, it required too much eﬀort to ﬁnd my
desired laptop (or digital camera).
The compound critiques were relevant to my preferences.
I am not satisﬁed with the laptop (or digital camera) I settled on.
I would buy the selected laptop (or digital camera), given the opportunity.
I found it easy to ﬁnd my desired laptop (or digital
I would use this recommender in the future to buy
other products.
I did not ﬁnd the compound critiques useful when
searching for laptops (or digital cameras).
we were also interested in understanding the quality of the
user experience aﬀorded by the diﬀerent critique generation
strategies. To test this we designed two questionnaires to
evaluate the response of users to the laptop-based recommender system.
The ﬁrst (post-stage questionnaire) was
presented to the users twice: once after they evaluated the
ﬁrst system and again after they evaluated the second system. This questionnaire asked users about their experience
using the system. After the users had completed both stages
and both questionnaires, they were presented with a ﬁnal
questionnaire that asked them to compare both systems directly to indicate which they preferred.
Post-Stage Questionnaires
Following the evaluation we presented users with a poststudy questionnaire in order to gauge their level of satisfaction with the system. For each of 11 statements (see Table
5). The agreement level ranked from -2 to 2, where -2 is
strongly disagree, and 2 is strongly agree. We were careful
Trial1: Post-Questionnaire Results
Statements
Disagree Agree
Trial2: Post-Questionnaire Results
Statements
Disagree Agree
A comparison of the post-stage questionnaires from Trial 1 and Trial 2 on the laptop
to provide a balanced coverage of both positive and negative
statements so that the answers are not biased by the user’s
expression style. A summary of the responses is shown in
From the results, both systems received positive feedback
from users in terms of their ease of understanding, usability
and interfacing characteristics.
Users were generally satisﬁed with the recommendation results retrieved by both
approaches (see S2 and S7) and found the compound critiques eﬃcient (see S5).
The results generally show that
compound critiquing is a promising approach for providing
recommendation information to users, and most indicated
that they would be willing to use the system to buy laptops
(see S2 and S10).
Some interesting results can be found if we compare the
average ranking level of both systems. In the ﬁrst trial of the
user study, participants indicated on average a higher level
of understanding in MAUT approach (see S1, 1.18 vs. 0.86,
p = 0.006), which shows that compound critiques provided
by the MAUT approach are easier to understand.
on average users ranked the MAUT approach more informative (see S3, −0.59 vs. −0.18, p = 0.009). Moreover,
users are more likely to agree with the statement that the
unit-critiques are better at searching for laptops with Apriori approach than the MAUT approach (see S4, 0.82 vs.
Results From User’s Final Questionnaires
1: Which system did you
2: Which system did you
find more informative?
3: Which system did you
find more useful?
4: Which system had the
better interface?
5: Which system was
better at recommending
laptops you liked?
No Difference
Figure 6: The ﬁnal questionnaire results.
0.41, p = 0.01). In Trial 2 however, these diﬀerences were
no longer signiﬁcant. As we can see, the MAUT approach
acquires similar scores in both trials but now the Apriori
approach scores much better in the second trial when using the same interface as the MAUT approach. This would
seem to support our hypothesis that the compound critique
presentation mechanism has a signiﬁcant role in inﬂuencing
users’ opinions on the compound critiques approaches.
Final Questionnaires
The ﬁnal questionnaire simply asked each user to vote
on which system (Apriori or MAUT) performed better in
terms of various criteria such as overall preference, informativeness, interface etc. The results are presented in Figure
6, showing the original feedback obtained during the earlier
Trial 1 evaluation (which used diﬀerent interface styles
for the Apriori and MAUT approaches) in comparison to
the feedback obtained for the current Trial 2 (in which such
interface diﬀerences were removed). As previously reported
 , users were strongly in favour of the MAUT-based approach. However, the results shown for Trial 2 are consistent with the hypothesis that this preference was largely due
to the more informative interface styles used during Trial 1
by the MAUT-based recommender. In Trial 2, for example,
we see a much more balanced response by users that gives
more or less equal preference to the MAUT and Aprioribased approaches and validate the beneﬁt of the new more
informative interface.
CONCLUSIONS
In this paper two research groups from diﬀerent institutions have come together to carry out a series of comprehensive user studies to evaluate two product recommender systems that diﬀer the way they generate compound critiques.
We developed an online evaluation platform to evaluate both
systems using a mixture of objective criteria (such as the recommendation eﬃciency, recommendation quality/accuracy)
and subjective criteria (such as a user’s perceived satisfaction). Our ﬁndings show that both critique generation approaches are very eﬀective when it comes to helping users to
navigate to suitable products. Both lead to eﬃcient recommendation sessions. The Apriori-based approach appears to
enjoy some advantages when it comes to producing more
eﬃcient sessions in complex product spaces but the MAUTbased approach appears to lead to higher quality recommendations. Overall, users responded equally well to both
systems in terms of the recommendation performance, accuracy and interface style.
ACKNOWLEDGEMENTS
This material is based on works supported by Science
Foundation Ireland under Grant No.
03/IN.3/I361 and
by Swiss National Science Foundation under grant 200020-
111888. We are grateful to the participants of the user studies.