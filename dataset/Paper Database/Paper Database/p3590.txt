Multi-task Self-Supervised Learning for Human Activity Detection
AAQIB SAEED, Eindhoven University of Technology
TANIR OZCELEBI, Eindhoven University of Technology
JOHAN LUKKIEN, Eindhoven University of Technology
Deep learning methods are successfully used in applications pertaining to ubiquitous computing, pervasive intelligence, health,
and well-being. Speciﬁcally, the area of human activity recognition (HAR) is primarily transformed by the convolutional and
recurrent neural networks, thanks to their ability to learn semantic representations directly from raw input. However, in
order to extract generalizable features massive amounts of well-curated data are required, which is a notoriously challenging
task; hindered by privacy issues and annotation costs. Terefore, unsupervised representation learning (i.e., learning without
manually labeling the instances) is of prime importance to leverage the vast amount of unlabeled data produced by smart
devices. In this work, we propose a novel self-supervised technique for feature learning from sensory data that does not require
access to any form of semantic labels, i.e., activity classes. We learn a multi-task temporal convolutional network to recognize
transformations applied on an input signal. By exploiting these transformations, we demonstrate that simple auxiliary tasks
of the binary classiﬁcation result in a strong supervisory signal for extracting useful features for the down-stream task. We
extensively evaluate the proposed approach on several publicly available datasets for smartphone-based HAR in unsupervised,
semi-supervised and transfer learning setings. Our method achieves performance levels superior to or comparable with
fully-supervised networks trained directly with activity labels, and it performs signiﬁcantly beter than unsupervised learning
through autoencoders. Notably, for the semi-supervised case, the self-supervised features substantially boost the detection rate
by ataining a kappa score between 0.7 −0.8 with only 10 labeled examples per class. We get similar impressive performance
even if the features are transferred from a diﬀerent data source. Self-supervision drastically reduces the requirement of
labeled activity data, eﬀectively narrowing the gap between supervised and unsupervised techniques for learning meaningful
representations. While this paper focuses on HAR as the application domain, the proposed approach is general and could be
applied to a wide variety of problems in other areas.
CCS Concepts: •Human-centered computing →Ubiquitous and mobile computing systems and tools; •Computing
methodologies →Machine learning;
Additional Key Words and Phrases: Self-supervised learning, multi-task learning, representation learning, semi-supervised
learning, transfer learning, temporal convolutional neural networks, human activity recognition, deep learning
ACM Reference format:
Aaqib Saeed, Tanir Ozcelebi, and Johan Lukkien. 0. Multi-task Self-Supervised Learning for Human Activity Detection. PACM
Interact. Mob. Wearable Ubiquitous Technol. 0, 0, Article 000 ( 0), 31 pages.
DOI: 00.0000/00000.00000
INTRODUCTION
Over the last years, deep neural networks have been widely adopted for time-series and sensory data processing;
achieving impressive performance in several application areas pertaining to pervasive sensing, ubiquitous
computing, industries, health and well-being . In particular, for smartphone-based human
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that
copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst
page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permited. To copy
otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from
 .
© 0 ACM. 2474-9567/0/0-ART000 $15.00
DOI: 00.0000/00000.00000
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
 
A. Saeed et al.
activity recognition (HAR), 1D convolutional and recurrent neural networks trained on raw labeled signals
signiﬁcantly improve the detection rate over traditional methods . Despite the recent advances
in the ﬁeld of HAR, learning representations from a massive amount of unlabeled data still presents a signiﬁcant
challenge. Obtaining large, well-curated activity recognition datasets is problematic due to a number of issues.
First, smartphone data are privacy sensitive, which makes it hard to collect suﬃcient amounts of user-activity
instances in a real-life seting. Second, the annotation cost and the time it takes to generate a large volume
of labeled instances are prohibitive. Finally, the diversity of devices, types of embedded sensors, variations in
phone-usage, and diﬀerent environments are further roadblocks in producing massive human-labeled data. To
sum up, such expensive and hard to scale process of gathering labeled data generated by smart devices makes it
very diﬃcult to apply supervised learning in this domain directly.
In light of these challenges, we pose the question whether it is possible to learn semantic representations in an
unsupervised way to circumvent the manual annotation of the sensor data with strong labels, e.g., activity classes. In
particular, the goal is to extract features that are on par with those learned with fully-supervised 1 methods. Tere
is an emerging paradigm for feature learning called self-supervised learning that deﬁnes auxiliary (also known
as pretext or surrogate) tasks to solve, where labels are readily extractable from the data without any human
intervention, i.e., self-supervised. Te availability of strong supervisory signals from the surrogate tasks enables
us to leverage objective functions as utilized in a standard supervised learning seting . For instance, the vision
community proposed a considerable number of self-supervised tasks for advancing representation learning2
from static images, videos, and audio (see Section 5). Most prominent among them are: colorization of grayscale
images , predicting image rotations , solving jigsaw puzzles , predicting the direction of video
playback , temporal order veriﬁcation , odd sequence detection , audio-visual correspondence ,
and curiosity-driven agents . Te presented methodology for sensor representation learning takes inspiration
from these methods and takes leverage of signal transformations to extract highly generalizable features for the
down-stream3 task, i.e., HAR.
Our work is motivated by the success of jointly learning to solve multiple self-supervised tasks and
we propose to learn accelerometer representations (i.e., features) by training a temporal convolutional neural
network (CNN) to recognize the transformations applied to the raw input signal. Particularly, we utilize a
set of signal transformations that are applied on each input signal in the datasets, which are then fed
into the convolutional network along with the original data for learning to diﬀerentiate among them. In this
simple formulation, a group of binary classiﬁcation tasks (i.e., to recognize whether a transformation such as
permutation, scaling, and channel shuﬄing was applied on the original signal or not) act as surrogate tasks to
provide a rich supervisory signal to the model. In order to extract highly generalizable features for the end-task
of interest, it is essential to utilize transformations that exploit versatile invariances of the temporal data (further
details are provided in Section 3). To this end, we utilize eight transformations to train a multi-task network for
simultaneously recognizing each of them. Te visual illustration of the proposed approach is given in Figure 1. In
the pre-training phase, the network consisting of a common trunk with a separate head for each task is trained
on self-supervised data, and in the second step, the features learned by the shared layers are utilized by the
HAR model. Importantly, we want to emphasize that in order for the convolutional network to recognize the
transformations, it must learn to understand the core signal characteristics through acquiring knowledge of
underlying diﬀerences in the accelerometer signals for various activity categories. We support this claim through
an extensive evaluation of our method on six publicly available datasets in unsupervised, semi-supervised and
1Te fully-supervised network is the standard deep model that is trained in an end-to-end fashion directly with activity labels without any
pre-training.
2also known as feature learning
3or an end-task
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
Task-specific
Accelerometer
Extracting
Multi-task
Self-Supervised
Small Labeled
Recognition
Classification
Self-Supervised Pre-Training
Activity Detection Model Training/Fine-tuning
Fig. 1. Illustration of the proposed multi-task self-supervised approach for feature learning. We train a temporal convolutional
network for transformation recognition as a pretext task as shown in Step 1. The learned features are utilized by (or transferred
to) the activity recognition model (Step 2) for improved detection rate with a small labeled dataset.
transfer learning setings, where it achieves noticeable improvements in all the cases while not requiring manually
labeled data for feature learning.
Te main contributions of this paper are:
• We propose to utilize self-supervision from large unlabeled data for human activity recognition.
• We design a signal transformation recognition problem as a surrogate task for annotation-free supervision,
which provides a strong training signal to the temporal convolutional network for learning generalizable
• We demonstrate through extensive evaluation that the self-supervised features perform signiﬁcantly beter
in the semi-supervised and transfer learning setings on several publicly available datasets. Moreover, we
show that these features achieve performance that is superior to or comparable with the features learned
via the fully-supervised approach (i.e., trained directly with activity labels).
• We illustrate with SVCCA , saliency mapping , and t-SNE visualizations that the features
extracted via self-supervision are very similar to those learned by the fully-supervised network.
• Our method substantially reduces the labeled data requirement, eﬀectively narrowing the gap between
unsupervised and supervised representation learning.
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
Te paper is organized as follows. Section 2 provides an overview of related paradigms and methodologies as
background information. Section 3 introduces the proposed self-supervised representation learning framework
for HAR. Section 4 presents an evaluation of our framework on publicly available datasets. Section 5 gives an
overview of the related work. Finally, Section 6 concludes the paper and lists future directions for research.
PRELIMINARIES
In this section, we provide a brief overview of multiple learning paradigms, including multi-task, transfer, semisupervised and importantly, representation learning. Tese either beneﬁt or serve as fundamental building blocks
of our self-supervised framework for representation extraction and robust HAR under various setings.
Representation Learning
Representation (feature) learning is concerned with automatically extracting useful information from the data
that can be eﬀectively used for an impending machine learning problem such as classiﬁcation. In the past, most
of the eﬀorts were spent on developing (and manually engineering) feature extraction methods based on domain
expertise to incorporate prior knowledge in the learning process. However, these methods are relatively limited
as they rely on human creativity to come up with novel features and lack the power to capture underlying
explanatory factors in the milieu of low-level sensory input. To overcome these limitations and to automate
the discovery of disentangled features, neural networks based approaches have been widely utilized, such as
autoencoders and their variants . Deep neural networks are composed of multiple (parameterized) non-linear
transformations that are trained through a supervised or unsupervised objective function with the aim of yielding
useful representations. Tese techniques have achieved indisputable empirical success across a broad spectrum
of problems thanks to the increasing dataset sizes and computing power availability.
Nevertheless, representation learning still stands as a fundamental problem in machine intelligence and is an
active area of research (see for a detailed survey).
Multi-task Learning
Te goal of multi-task learning (MTL) is to enhance the learning eﬃciency and accuracy through simultaneously
optimizing multiple objectives based on shared representations and exploiting relations among the tasks . It is
widely utilized in several application domains within machine learning such as natural language processing ,
computer vision , audio sensing , and well-being . In this learning seting, T supervised tasks, each
with a dataset Dt = {xt
i=1 and a separate cost function are made available. Te multi-objective loss is then
generally created through a weighted linear sum of the individual tasks’ losses as:
LAддreдated =
where ψt is the task weight and Lt is a task-speciﬁc loss function. It is important to note that, MTL itself does
not impose any restriction on the loss type of an individual task. Terefore, unsupervised and supervised tasks or
tasks having diﬀerent cost functions can be conveniently combined for learning representations.
Transfer Learning
Transfer learning aims to develop methods for preserving and leveraging previously acquired knowledge to
accelerate the learning of novel tasks. In recent years, it has shown remarkable improvement in performance on
several very challenging problems, especially in areas, where litle-labeled data are available such as in natural
language understanding, object recognition, and activity recognition . In this paradigm, the goal is to
transfer (or reuse) the learned knowledge from a source domain DSRC to a target domain DT RG. More precisely,
consider domains DSRC and DT RG with learning tasks tSRC and tT RG, respectively. Te goal is to help improve
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
the learning of a predictive function f (.) in tT RG using the knowledge extracted from DSCR and tSRC, where
DSRC , DT RG, and/or tSRC , tT RG, meaning that domains or tasks may be diﬀerent. Tis learning formulation
enables to develop a high-quality model under diﬀerent knowledge transfer setings (such as features, instances,
weights) from existing labeled data of some related task or domain. For a detailed review of transfer learning, we
refer an interested reader to .
Semi-supervised Learning
Semi-supervised learning provides a compelling framework for leveraging unlabeled data in cases when labeled
data collection is expensive. It has been repeatedly shown that given enough computational power and supervised
data; deep neural networks can achieve human-level performance on a wide variety of problems . However,
the curation of large-scale datasets is very costly and time-consuming as it either requires crowdsourcing or
domain expertise such as in the case of medical imaging. Likewise, for several practical problems, it is simply
not possible to create a large enough labeled dataset (e.g., due to privacy issues) to learn a model of reasonable
accuracy. Semi-supervised learning algorithms oﬀer a compelling alternative to fully-supervised methods for
jointly learning from few labeled and a large number of unlabeled instances. More speciﬁcally, given a labeled
training set of input-output pairs (X,Y) ∈DL and unlabeled instance set, X ∈DU , the broad aim is to produce a
predictive function fθ(X) making use of not only DL but also the underlying structure in DU , where θ represents
the learnable parameters of the model. For a concise review and realistic evaluation of various deep learning
based semi-supervised techniques, see .
Towards Self-supervision
Deep learning has been increasingly used for end-to-end HAR with far superior performance that can be achieved
through traditional machine learning methods . However, learning from very few labeled data,
i.e. few-shot and semi-supervised learning is still an issue as large labeled datasets are required to train a model
of suﬃcient quality. Similarly, the utilization of previously learned knowledge from related data (or task) to
rapidly solve a comparable problem is not addressed very well by the existing methods (see Section 5 for more
details). In this paper, we explore self-supervised feature learning for HAR that eﬀectively utilizes unlabeled
data. Te exciting ﬁeld of self-supervision is concerned with extracting supervisory signals from data without
requiring any human intervention. Te evolution of feature extraction methods from hand-crafed features
towards self-supervised representations is illustrated in Figure 2. Te input to each of the illustrated approaches
is raw data, which is not shown for the sake of brevity.
Autoencoder
End-to-end Learning
Manual Feature
Extraction
Self-Supervised Learning (Supervision from Data)
End-task Classifier
Feature Transfer or Utilization
Convolutional Network
Auxiliary-task Classifier
Fig. 2. Evolution of feature learning approaches from hand-crafed methods towards task discovery for self-supervision.
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
In this section, we present our self-supervised representation learning framework for HAR. First, we provide an
overview of the methodology. Next, we discuss various learning tasks (i.e. transformation classiﬁcation) and
their beneﬁts for generic features extraction from unlabeled data. Finally, we provide a detailed description of the
network architecture, its implementation, and the optimization process.
Te objective of our work is to learn general-purpose sensor representations based on a temporal convolutional
network in an unsupervised manner. To achieve this goal, we introduce a self-supervised deep network named
Transformation Prediction Network (TPN), which simultaneously learns to solve multiple (signal) transformation
recognition tasks as shown in Figure 1. Speciﬁcally, the proposed multi-task TPN Mθ(.) is trained to produce
estimates of the transformations applied to the raw input signal. We deﬁne a set of |T | distinct transformations
(or tasks) T = {Jt(.)}t ∈T , where Jt(.) is a function that applies a particular signal alteration technique t to
the temporal sequence x ∈R(N,C) to yield a transformed version of the signal Jt(x). Te network Mθ(.) that
has a common trunk and individual head for each task, it takes an input sequence and produces a probability
of the signal being a transformed version of the original, i.e. P(Jt |x) = Mθ(x). Note, that given a set of
unlabeled signals (e.g. of accelerometer), we can automatically construct a self-supervised labeled dataset
D = {{(Jt(xi),True), (xi, False)}t ∈T }m
i=1. Hence, given this set of m training instances, the multi-task selfsupervised training objective that a model must learn to solve is:
i log(Mθ(xt
i )) + (1 −yt
i ) log(1 −Mθ(xt
where Mθ(xt) is the predicted probability of x being a transformed version t and θ are the learnable parameters
of the network. mt represents the number of instances for a task (which can vary but are equal in our case) and
ψt is the loss-weight of task t.
We emphasize that, although the network has a separate layer to diﬀerentiate between original and each of the
T transformations it can be extended in a straight-forward manner to recognize multiple transformations applied
to the same input signal or for multi-label classiﬁcation. In the following subsection, we explain the types of
signal transformations that are used in this work.
Self-supervised Tasks: Signal Transformations
Te aforementioned formulation requires the signal transformations J to deﬁne a multi-task classiﬁcation that
enables the convolutional model to learn disentangled semantic representations useful for down-stream tasks,
e.g. activity detection. We aimed for conceptually simple, yet diverse tasks to possibly cover several invariances
that commonly arise in temporal data . Intuitively, a diverse set of tasks should lead to a broad spectrum of
features, which are more likely to span the feature-space domain needed for a general understanding of the
signal’s characteristics. In this work, we propose to utilize eight straight-forward signal transformations (i.e.
|T | = 8) for the self-supervision of a network. More speciﬁcally, when transformations are applied on an
input signal x, they result in eight variants of x. As mentioned earlier, the temporal convolutional model is then
trained jointly on all the tasks’ data to solve a problem of transformation recognition, which allows the model
to extract high-level abstractions from the raw input sequence. Te transformations utilized in this work are
summarized below:
• Noised: Given sensor readings of a ﬁxed length, a possible transformation is the addition of random
noise (or jiter) in the original signal. Heterogeneity of device sensors, sofware, and other hardware can
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
cause variations (noisy samples) in the produced data. A model that is robust against noise will generalize
beter as it learns features that are invariant to minor corruption in the signal.
• Scaled: A transformation that changes the magnitude of the samples within a window through multiplying with a randomly selected scalar. A model capable of handling scaled signals produces beter
representations as it becomes invariant to amplitude and oﬀset invariances.
• Rotated: Robustness against arbitrary rotations applied on the input signal can achieve sensor-placement
(orientation) invariance. Tis transformation inverts the sample signs (without changing the associated
class-label) as frequently happens if the sensor (or device) is, for example, held upside down.
• Negated: Tis simple transformation is an instance of both scaled (scaling by −1) and rotated transformations. It negates samples within a time window, resulting in a vertical ﬂip or a mirror image of the
input signal.
• Horizontally Flipped: Tis transformation reverses the samples along the time-dimension, resulting in
a complete mirror image of an original signal as if it were evolved in the opposite time direction.
• Permuted: Tis transformation randomly perturbs the events within a temporal window through slicing
and swapping diﬀerent segments of the time-series to generate a new one, hence, facilitating the model
to develop permutation invariance properties.
• Time-Warped: Tis transformation locally stretches or warps a time-series through a smooth distortion
of time intervals between the values (also known as local scaling).
• Channel-Shuﬀled: For a multi-component signal such as a triaxial accelerometer, this transformation
randomly shuﬄes the axial dimensions.
Tere are several beneﬁts of utilizing transformations recognition as auxiliary tasks for feature extraction from
unlabeled data.
Enabling the learning of generic representations: Te primary motivation is that the above-deﬁned
pretext tasks enable the network to capture the core signal characteristics. More speciﬁcally, for the TPN to
successfully recognize if the signal is transformed or not, it must learn to detect high-level semantics, sensor
behavior under diﬀerent device placements, time-shif of the events, varying amplitudes, and robustness against
sensor noise, thus, contributing to solving the ultimate task of HAR.
Task diversiﬁcation and elimination of low-level input artifacts: A clear advantage of using multiple
self-supervised tasks as opposed to a single one is that it will lead to a more diverse set of features that are invariant
to low-level artifacts of the signals. Had we chosen to utilize signal reconstruction, e.g. with autoencoders, this
would learn to compress the input, but due to a weak supervisory signal (as compared to self-supervision), it
may discover trivial features with no practical value for the activity recognition or any other task of interest. We
compare our approach against other methods in section 4.3.
Transferring knowledge: Furthermore, with our approach, the unlabeled sensor data that are produced in
huge quantity can be eﬀectively utilized with no human intervention to pre-train a network that is suitable
for semi-supervised and transfer learning setings. It is particularly of high value for training networks in a
real-world seting, where very litle or no supervision is available to learn a model of suﬃcient quality from
Other beneﬁts: Our self-supervised method has numerous other beneﬁts. It has an equivalent computational
cost to supervised learning but with beter convergence accuracy, making it a suitable candidate for continuous
unsupervised representation learning in-the-wild. Moreover, our technique neither requires a sophisticated
pre-processing (apart from z-normalization) nor needs a specialized architecture (which also requires labeled data)
to exploit invariances. We will show in Section 4.3 through extensive evaluation that the self-supervised models
learn useful representations and dramatically improve performance over other learning strategies. Despite the
simplicity of the proposed scheme, it allows utilizing data collected through a wide variety of devices from a
diverse set of users.
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
Network Architecture and Implementation
We implement the TPN Mθ(.) as a multi-branch temporal convolutional neural network with a common trunk
(shared layers) and a distinct head (private layers) for each task with a separate loss function. Hard parameter
sharing is employed between all the task-speciﬁc layers to encourage strong weight utilization from the trunk.
Figure 3 illustrates the TPN containing three 1D convolutional layers consisting of 32, 64, and 96 feature maps
with kernel sizes of 24, 16 and 8 respectively, and having a stride of 1. Dropout is used afer each of the layers
with a rate of 0.1, and L2 regularization is applied with a rate of 0.0001. Global max pooling is used afer the last
convolution layer to aggregate high-level discriminative features. Moreover, each task-speciﬁc layer is comprised
of a fully-connected layer of 256 hidden units followed by a sigmoidal output layer for binary classiﬁcation. We
use ReLU as non-linearity in all the layers (except the output) and train a network with Adam optimizer 
for a maximum of 30 epochs with a learning rate of 0.0003, unless stated otherwise. Furthermore, the activity
recognition model has a similar architecture to the TPN except for a fully-connected layer that consists of 1024
hidden units followed by a sofmax output layer with units depending on the activity detection task under
consideration. Additionally, during training of this model, we apply early-stopping, if the network fully converges
on the training set to avoid overﬁting.
Te motivation for keeping the TPN architecture simple arises from the fact that we want to show the
performance gain does not come from the number of parameters (or layers) or due to the utilization of other
sophisticated techniques such as batch normalization but the improvement is due to self-supervised pre-training.
Likewise, the choice of multi-task learning seting, where each task has an additional private layer manifests in
leting the model push pretext task-speciﬁc features to the last layers and let the initial layers extract generic
representations that are important for a wide variety of end-tasks. Moreover, our architectural speciﬁcation allows
for a straightforward extension to add other related tasks, if needed, such as input reconstruction. Although, we
do not explore applying multiple transformations to the same sequence or train models for their recognition the
network design is intrinsically capable of performing this multi-label classiﬁcation task.
Our training process is summarized in Algorithm 1. For every instance, we ﬁrst generate transformed versions
of a signal for the self-supervised pre-training of the network. At each training iteration of the TPN model, we
feed the data from all tasks simultaneously, and the overall loss is calculated as a weighted sum of the losses of
diﬀerent tasks. Once pre-training converges, we transfer the weights of convolutional layers from model Mθ to
an activity recognition network Cθ for learning the ﬁnal supervised task. Here, either all the transferred layers
are kept frozen, or the last convolutional layer is ﬁne-tuned depending on the learning paradigm. Figure 3 depicts
this process graphically, where shaded convolutional layers represent frozen weights, while others are either
trained from scratch or optimized further on the end-task. To avoid ambiguity, in the experiment section, we
explicitly mention when the results are from a fully-supervised or self-supervised (including ﬁne-tuned) network.
EVALUATION
In this section, we conduct an extensive evaluation of our approach on several publicly available datasets for
human activity recognition (HAR) in order to determine the quality of learned representations, transferability of
the features, and beneﬁts of this in the low-data regime. Te self-supervised tasks (i.e., transformation predictions)
are utilized for learning rich sensor representations that are suitable for an end-task. We emphasize that achieving
high performance on these surrogate tasks is not our focus.
We consider six publicly available datasets to cover a wide variety of device types, data collection protocols, and
activity recognition tasks performed with smartphones in diﬀerent environments. Some important aspects of the
data are summarized in Table 1. Below, we give brief descriptions of every dataset summarizing its key points.
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
Conv1D + ReLU
Global Max Pooling
FC - 1024 - ReLU
FC - Softmax
Activity Recognition Data
Weights Frozen
Self-Supervised Data
Conv1D + ReLU
Global Max Pooling
FC - Sigmoid
FC - Sigmoid
FC - Sigmoid
FC - 256 - ReLU
FC - 256 - ReLU
FC - 256 - ReLU
Input2 Input1
Fig. 3. Detailed architectural specification of transformation prediction and activity recognition networks. We propose a
framework for self-supervised representation learning from unlabeled sensor data (such as an accelerometer). Various signal
transformations are utilized to establish supervisory tasks, and the network is trained to diﬀerentiate between an original
and transformed version of the input. The three blocks of Conv + ReLU and Dropout layers, which is followed by a Global
Max Pooling are similar across both networks. However, the multi-task model has a separate head for each task. Likewise,
the activity recognizer has an additional densely connected layer. The TPN is pre-trained on self-supervised data, and the
learned weights are transferred (depicted by a dashed arrow) and kept frozen to the lower model, which is then trained to
detect various activities.
HHAR. Te Heterogeneity Human Activity Recognition (HHAR) dataset contains signals from two
sensors (accelerometer and gyroscope) of smartphones and smartwatches for 6 diﬀerent activities, i.e. biking,
siting, standing, walking, stairs-up and stairs-down. Te 9 participants executed a scripted set of activities for 5
minutes to get equal class distribution. Te subjects had 8 smartphones in a tight pouch carried around their
waist and 4 smartwatches, 2 worn on each arm. In total, they used 36 diﬀerent smart devices of 13 models from 4
manufacturers to cover a broad range of devices for sampling rate heterogeneity analysis. Te sampling rate of
signals varied signiﬁcantly across phones with values between 50-200Hz.
UniMiB. Tis dataset contains triaxial accelerometer signals collected from a Samsung Galaxy
Nexus smartphone at 50Hz. Tirty subjects participated in the data collection process forming a diverse sample
of the population with diﬀerent height, weight, age, and gender. Te subject placed the device in her trouser’s
front lef pocket for a partial duration and in the right pocket for the remainder of the experiment. We utilized
the data of 9 activities of daily living (i.e., standing up from siting, standing up from lying, walking, running,
upstairs, jumping, downstairs, lying down from siting, siting) in this paper.
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
Algorithm 1: Multi-task Self-Supervised Learning
Input: Unlabeled instance set DU , labeled dataset DL, task-speciﬁc weights ψ, numbers of epochs EM and EC
Output: Self-supervised network M, activity classiﬁcation model C with n classes
initialize (X,Y)1, . . . , (X,Y)T to hold instance-label pairs for multiple tasks in T
initialize M with parameters θM and C with parameters θC
// Labeled data generation for self-supervision
for each instance x in DU do
for each transformation t ∈T do
Insert (x, False) to (X,Y)t and (Jt(x), True) to (X,Y)t
for each epoch em from 1 to EM do
Randomly sample a mini-batch of m samples for all tasks {(X,Y)1, . . . , (X,Y)T }
Update θM by descending along its gradient
i log(Mθ(xt
i )) + (1 −yt
i ) log(1 −Mθ(xt
i ))) + β ∥θ ∥2 i#
Assign learned parameters from θ1...L
Keep the transferred weights θ1...L
of network C frozen
for each epoch ec from 1 to EC do
Randomly sample a mini-batch of m labeled activity recognition samples DL
Update θC by descending along its gradient ∇θC
k=1 yi,k log(Cθ(xi)) + β(∥θ ∥2)
Gradient-based updates can use any standard gradient-based learning technique. We used Adam in all
our experiments.
Table 1. Summary of datasets used in our evaluation. These datasets are selected based on the diversity of participants,
device types and activity classes. Further details on the pre-processing of each data source and the number of users utilized
are discussed in Section 4.1.
No. of users
No. of activity classes
MotionSense
UCI HAR. Te UCI HAR dataset is obtained from a group of 30 volunteers with a waist-mounted
Samsung Galaxy S2 smartphone. Te accelerometer and gyroscope signals are collected at 50Hz when subjects
performed the following six activities: standing, siting, laying down, walking, downstairs and upstairs.
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
MobiAct. Te MobiAct4 dataset contains signals from a smartphone’s inertial sensors (accelerometer,
gyroscope, and orientation) for 11 diﬀerent activities of daily living and 4 types of falls. It is collected with a
Samsung Galaxy S3 smartphone from 66 participants of diﬀerent gender, age group, and weight through more
than 3200 trials. Te device is placed in a trouser’s pocket freely selected by the subject in any random orientation
to capture everyday usage of the phone. We used the data from 61 participants who have data samples for any of
the following 11 activities: siting, walking, jogging, jumping, stairs up, stairs down, stand to sit, siting on a
chair, sit to stand, car step-in, and car step-out.
WISDM. Te dataset from the Wireless Sensor and Data Mining (WISDM) project was collected in
a controlled study from 29 volunteers, who carried the cell phone in their pockets. Te data were recorded for 6
diﬀerent activities (i.e., sit, stand, walk, jog, ascend stairs, descend stairs) via an app developed for an Android
phone. Te accelerometer signal was acquired every 50ms (sampling rate of 20Hz). We use the data of all the
users available in the raw data ﬁle with user ids ranging from 1 to 36.
MotionSense. Te MotionSense dataset comprises an accelerometer, gyroscope, and altitude data
from 24 participants of varying age, gender, weight, and height. It was collected using an iPhone6s, which is
kept in the user’s front pocket. Te subjects performed 6 diﬀerent activities (i.e., walking, jogging, downstairs,
upstairs, siting, and standing.) in 15 trials under similar environments and conditions. Te study aimed to infer
physical and demographics atributes from time-series data in addition to the detection of activities.
Data Preparation and Assessment Strategy
We applied minimal pre-processing on the accelerometer signals as deep neural networks are very good at
learning abstract representations directly from raw data . We segmented the signals into ﬁxed size windows
that have 400 samples with 50% overlap, for all the datasets under consideration. Te appropriate window size is
a task-speciﬁc parameter and could be tuned or chosen based on prior knowledge for improved performance.
Here, we utilize the same window size based on earlier exploration across datasets and to keep experimental
evaluation impartial towards the eﬀect of this hyper-parameter. Next, we divide each dataset into training and
test sets through randomly selecting 20 −30% of the users for testing and the rest for training and validation;
depending on the dataset size. We used the ceiling function to select number of users, e.g. from HHAR dataset 3
users are used for evaluation out of 9. Te training set users’ data are further divided into 80% for training the
network and 20% for validation and hyper-parameter tuning. Importantly, we also evaluate our models through
user-split based 5-folds cross-validation, wherever it is appropriate. Finally, we normalize the data by applying
z-normalization with summary statistics calculated from the training set. We generate self-supervised data from
an unlabeled training set that is produced as a result of the processing as mentioned earlier. We utilize the data
generation procedure as explained earlier in Section 3.3.
Furthermore, due to the large size of the HHAR dataset and in order to reduce computational load, we randomly
sample 4000 instances from each users’ data to produce transformed signals. Likewise, in the case of UniMiB
because of its relatively small size, we generate 5 times more transformed instances. We evaluate the performance
with Cohen’s kappa, a weighted version of precision, recall and f-score metrics to be robust against inherent
imbalanced nature of the datasets. It is important to highlight that, we use a network architecture with the same
conﬁguration across the datasets to evaluate models’ performance in order to highlight improvement is indeed due to
self-supervision and not due to architectural modiﬁcations.
4second release
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
Qantifying the Qality of Learned Feature Hierarchies. We ﬁrst evaluate our approach to determine the
quality of learned representations versus the model depth (i.e., the layer number from which the features come).
Tis analysis helps in understanding whether the features coming from diﬀerent layers vary in quality concerning
their performance on an end-task and if so, which layer should be utilized for this purpose. To this end, we ﬁrst
pre-train our TPN in a self-supervised manner and learn classiﬁers on top of ConvA, ConvB, and ConvC layers
independently, for several activity recognition datasets. Tese classiﬁers (see Figure 3) are trained in a supervised
way while keeping the learned features ﬁxed during the optimization process. Figure 4 provides kappa values on
test sets averaged across 10-independent runs to be robust against diﬀerences in weight initializations of the
classiﬁers. We observe that for a majority of the datasets the model performance improves with increasing depth
apart from HHAR, where features from ConvB layer results in improved detection rate with a kappa of 0.774
compared to 0.679 of ConvC. It may be because the representation of the last layer starts to become too speciﬁc
on the transformation prediction task or it may also be because we did not utilize the entire dataset for the
self-supervision. To be consistent, in the subsequent experiments we used features from the last convolutional
layer for all the considered datasets. For a new task or recognition problem, we recommend performing a similar
analysis to identify layer/block of the network that gives optimal results on the particular dataset.
MotionSense
Fig. 4. Evaluation of activity classification performance using the features learned based on self-supervision (per layer). We
train an activity classifier on-top of each of the temporal convolution blocks (ConvA, ConvB, and ConvC) that are pre-trained
with self-supervision. The reported results are averaged over 10 independent runs (i.e., training an activity classifier from
scratch). ConvA, ConvB, and ConvC have 32, 64, and 96 feature maps, respectively.
Comparison against Fully-Supervised and Unsupervised Approaches. In this subsection, we assess our
self-supervised representations learned with TPN against other unsupervised and fully-supervised techniques
for feature learning. Table 2 summarizes the results with respect to four evaluation metrics (namely, precision,
recall, f-score, and kappa) for 10-independent runs on the six datasets described earlier. For the Random Init.
entries, we keep the convolutional network layers frozen during optimization and train only a classiﬁer in a
supervised manner. Likewise, for an Autoencoder, we keep the network architecture the same and pre-train
it in an unsupervised way. Aferward, the weights of the encoder are kept frozen, and a classiﬁer is trained
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
on top as usual. Te Self-Supervised entries show the result of the convolutional network pre-trained with our
proposed method, where a classiﬁer is trained on top of the frozen network in a supervised fashion. Furthermore,
Self-Supervised (FT) entries highlight the performance of the network trained with self-supervision but the last
convolution layer, i.e. ConvC is ﬁne-tuned along with a classiﬁer during training on the activity recognition task.
Training an activity classiﬁcation model on top of randomly initialized convolutional layers poorly performs as
expected, which is evidence that the performance improvement is not only because of the activity classiﬁer. Tese
results are followed by a widely used unsupervised learning method, i.e. an autoencoder. Te self-supervised
technique outperforms existing methods and achieves results that are on par with the fully-supervised model.
It is important to note that, for our proposed technique, only the classiﬁer layers are randomly initialized and
trained with activity speciﬁc labels (the rest is transferred from the self-supervised network). We also observe
that ﬁne-tuning the last convolutional layer further improves the classiﬁcation performance of the down-stream
tasks on several datasets such as UniMiB, HHAR, MobiAct, and UCI HAR. Te results show that TPN can learn
highly generalizable representations, thus reducing the performance gap of feature learning with the (end-to-end)
supervised case. For a more rigorous evaluation, we also performed 5-folds (user split based) cross-validation for
every method on all the datasets. Te results are provided in Table 4 of the appendix, which also shows that the
self-supervised method reduces the performance gap with the supervised seting.
Assessment of Individual Self-Supervised Tasks in Contrast with Multiple Tasks. In Figure 5, we show
comparative performance analysis of single self-supervised tasks with each other and importantly with a multitask seting. Tis assessment helps us in understanding whether self-supervised features extracted via jointly
learning to solve multiple tasks are any beter (for activity classiﬁcation) than independently solving individual tasks
and whether multi-task learning helps in learning more useful sensor semantics. To achieve this, we pre-train a TPN
on each of the self-supervised tasks and transfer the weights for learning an activity recognition classiﬁer. We
observe in all the cases that learning representations via solving multiple tasks lead to far beter performance on
the end-task. Tis further highlights that the features learned through various self-supervised tasks have diﬀerent
strengths and weaknesses. Terefore, merging multiple tasks results in an improvement in learning a diverse set
of features. However, we notice that some tasks (such as Channel Shuﬄed, Permuted, and Rotated) consistently
performed beter compared to others across datasets; achieving a kappa score above 0.60 as evaluated on diﬀerent
activity recognition problems. It highlights an important point that there may exist a group of tasks, which are
reasonably suﬃcient to achieve a model of good quality. Furthermore, in Figure 11 of the appendix, we plot the
kappa score achieved by a multi-task TPN on transformation recognition tasks as a function of the number of
training epochs. Tis analysis highlights that task complexity varies greatly from one dataset to another and may
help with the identiﬁcation of trivial auxiliary tasks that may lead to non-generalizable features.
In addition to activity classiﬁcation, for any learning task involving time-series sensor data (e.g., as encountered
in a various Internet of Tings applications), we recommend extracting features through ﬁrst solving individual
tasks and later focusing on the multi-task scenario; discarding low performing tasks or assigning low-weights to
the loss functions of the respective tasks. Another approach could be to auto-tune the task-loss weight by taking
homoscedastic uncertainty of each task into account .
Eﬀectiveness under Semi-Supervised Seting. Our proposed self-supervised feature learning method
atains very high performance on diﬀerent activity recognition datasets. Tis brings up the question, whether
the self-supervised representations can boost performance in the semi-supervised learning seting as well or not. In
particular, can we use this to perform activity detection with very litle labeled data? Intrigued by this, we also
evaluate the eﬀectiveness of our approach to semi-supervised learning. Speciﬁcally, we initially train a TPN on
an entire training set for transformation prediction. Subsequently, we learn a classiﬁer on top of the last layer’s
feature maps with only a subset of the available accelerometer samples and their corresponding activity labels.
For training an activity classiﬁer, we use for each category (class) 2, 5, 10, 20, 50, and 100 examples. Note that,
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
Table 2. Task Generalization: Evaluating self-supervised representations for activity recognition. We compare the proposed
self-supervised method for representation learning with fully-supervised and unsupervised approaches. We use the same
architecture across all the experiments. The self-supervised TPN is trained to recognize transformations applied on the
input signal while the activity classifier is trained on top of these learned features where Self-Supervised (FT) entry provides
results when the last convolution layer is fine-tuned. The Random Init. entries present results when the convolution layers
are randomly initialized and kept frozen during the training of the classifier. The results reported are averaged over 10
independent runs to be robust against variations in the weight initialization and the optimization process.
Random Init.
0.3882±0.0557
0.3101±0.0409
0.2141±0.0404
0.1742±0.0488
Supervised
0.7624±0.0312
0.7353±0.0308
0.7276±0.0297
0.6816±0.0371
Autoencoder
0.7317±0.0451
0.6657±0.0663
0.6585±0.0724
0.5994±0.0784
Self-Supervised
0.7985±0.0155
0.777±0.0199
0.7666±0.0234
0.731±0.0243
Self-Supervised (FT)
0.8218±0.0256
0.797±0.0211
0.7862±0.0187
0.7555±0.025
(b) UniMiB
Random Init.
0.4256±0.0468
0.3546±0.037
0.2775±0.0491
0.2243±0.0474
Supervised
0.8276±0.0148
0.8096±0.0266
0.8097±0.0248
0.7815±0.0299
Autoencoder
0.5922±0.0191
0.5557±0.0232
0.5376±0.0339
0.4824±0.0275
Self-Supervised
0.8133±0.0077
0.7954±0.014
0.7929±0.016
0.7642±0.0162
Self-Supervised (FT)
0.8506±0.007
0.8432±0.0049
0.8425±0.0054
0.8197±0.005
(c) UCI HAR
Random Init.
0.6189±0.0648
0.4392±0.0692
0.3713±0.0952
0.3133±0.0866
Supervised
0.9059±0.0133
0.8998±0.0139
0.8981±0.0148
0.8789±0.0168
Autoencoder
0.8314±0.0590
0.7877±0.1112
0.7772±0.1306
0.7425±0.1359
Self-Supervised
0.9100±0.0081
0.9011±0.0139
0.8987±0.0155
0.8803±0.0169
Self-Supervised (FT)
0.9057±0.0121
0.897±0.0185
0.8946±0.019
0.8754±0.0222
(d) MobiAct
Random Init.
0.4749±0.1528
0.3452±0.1128
0.2813±0.0982
0.1915±0.1017
Supervised
0.908±0.0066
0.895±0.0167
0.8975±0.0133
0.8665±0.0202
Autoencoder
0.7493±0.0328
0.7581±0.0354
0.7293±0.0452
0.6772±0.0517
Self-Supervised
0.9095±0.0035
0.9059±0.0059
0.906±0.0053
0.8795±0.0073
Self-Supervised (FT)
0.9194±0.0057
0.9102±0.0114
0.9117±0.0093
0.8855±0.014
Random Init.
0.5942±0.0599
0.3543±0.077
0.358±0.0837
0.2224±0.0656
Supervised
0.9024±0.0076
0.8657±0.0206
0.8764±0.0168
0.8211±0.0258
Autoencoder
0.6561±0.2775
0.6631±0.1623
0.6358±0.2355
0.5106±0.288
Self-Supervised
0.8894±0.0096
0.8484±0.0269
0.8593±0.0225
0.7986±0.0334
Self-Supervised (FT)
0.8999±0.0111
0.8568±0.0375
0.8686±0.0314
0.8106±0.0466
(f) MotionSense
Random Init.
0.5999±0.0956
0.5029±0.0931
0.4681±0.1105
0.376±0.1176
Supervised
0.9164±0.0053
0.8993±0.0091
0.9027±0.0085
0.8763±0.011
Autoencoder
0.8255±0.0132
0.8116±0.0195
0.8109±0.0169
0.7659±0.0226
Self-Supervised
0.8979±0.0073
0.8856±0.0087
0.8864±0.0083
0.8589±0.0106
Self-Supervised (FT)
0.9153±0.0088
0.8979±0.0092
0.9005±0.0094
0.8744±0.0112
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
Horizontal Flipped
Channel Shuffled
Time Warped
Horizontal Flipped
Channel Shuffled
Time Warped
Horizontal Flipped
Channel Shuffled
Time Warped
Horizontal Flipped
Channel Shuffled
Time Warped
Horizontal Flipped
Channel Shuffled
Time Warped
Horizontal Flipped
Channel Shuffled
Time Warped
MotionSense
Fig. 5. Comparison of individual self-supervised tasks with the multi-task seting. The TPN is pre-trained for solving a
particular task and the activity classifier is trained on-top of the learned features. We report the averaged results of evaluation
metrics for 10 independent runs, where F, K, P, and R refer to F-score, Kappa, Precision and Recall, respectively. We observe
that multi-task learning improves performance in all the cases with tasks such as Channel Shuﬀled, Permuted, and Rotated
consistently performed beter compared to other tasks across datasets.
2-10 samples per class represent a real-world scenario of acquiring a (small) labeled dataset from human users
with minimal interruption to their daily routines, hence, making self-supervision from unlabeled data of great
value. Likewise, we believe, our analysis of learning with very few labeled instances across datasets is the ﬁrst
atempt in quantifying the amount of labeled data required to learn an activity recognizer of decent quality. For
self-supervised models, as earlier, we either kept the weights frozen or only ﬁne-tune the last ConvC layer.
In Figure 6, we plot the average kappa of 10-independent runs as a function of the number of available training
examples. For each run, we randomly sample desired training instances and train a model from scratch. Note
that, we utilize the same instances for evaluating both supervised baseline and our proposed method. Te
fully-supervised baseline (blue curve) shows network performance when a model is trained only with the labeled
data. Te proposed self-supervised pre-training technique, in particular, the version with ﬁne-tuning of the last
ConvC layer, tremendously improved the performance. Te diﬀerence in the performance between supervised
and self-supervised feature learning is signiﬁcant on MotionSense, UCI HAR, MobiAct, and HHAR datasets in
low-data regime (i.e. with 2-10 labeled instances per class). More notably, we observe that pre-training helps
more in a semi-supervised seting when the data are collected from a wide variety of devices; simulating a
real-life seting. Finally, we highlight that a simple convolutional network is used in our experiments to show the
feasibility of self-supervision from unlabeled data. We believe a deeper network trained on a bigger unlabeled
dataset will further improve the quality of learned representations for the semi-supervised seting.
Evaluating Knowledge Transferability. We have shown that representations learned by the self-supervised
TPN consistently achieve the best performance as compared to other unsupervised/supervised techniques and
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
# Training instances per class
Supervised
Ours: Semi-Supervised
Ours: Semi-Supervised (FT)
# Training instances per class
Supervised
Ours: Semi-Supervised
Ours: Semi-Supervised (FT)
# Training instances per class
Supervised
Ours: Semi-Supervised
Ours: Semi-Supervised (FT)
# Training instances per class
Supervised
Ours: Semi-Supervised
Ours: Semi-Supervised (FT)
# Training instances per class
Supervised
Ours: Semi-Supervised
Ours: Semi-Supervised (FT)
# Training instances per class
MotionSense
Supervised
Ours: Semi-Supervised
Ours: Semi-Supervised (FT)
Fig. 6. Generalization of the self-supervised learned features under semi-supervised seting. The TPN is pre-trained on an
entire set of unlabeled data in a self-supervised manner and the activity classifier is trained from scratch on 2, 5, 10, 20, 50,
and 100 labeled instances per class. The blue curve (baseline) depicts the performance when an entire network is trained in a
standard supervised way while the orange curve shows performance when we keep the transferred layers frozen. The green
curve illustrates the kappa score when the last layer is fine-tuned along with the training of a classifier on the available set
of labeled instances. The reported results are averaged over 10 independent runs for each of the evaluated approaches. The
results with weighted f-score are provided in Figure 12 of the Appendix.
also in a semi-supervised seting. As we have utilized the unlabeled data from the same data source for selfsupervised pre-training, a next logical question that arises is can we utilize a diﬀerent (yet similar) data source
for self-supervised representation extraction and gain a performance improvement on a task of interest (also in a
low-data regime)? In Table 3, we assess the performance of our unsupervised learned features across datasets and
tasks by ﬁne-tuning them on HAHR, UniMiB, UCI HAR, WISDM, and MotionSense datasets. For self-supervised
feature learning, we utilized the unlabeled MobiAct dataset as it is collected from a diverse group of users that
performed twelve activities; highest among other considered datasets both in terms of the number of users and
activities. Tis makes MobiAct a suitable candidate to perform transfer learning as it encompasses all the activity
classes in other datasets. Of course, we do not utilize activity labels in MobiAct for self-supervised representation
learning. We begin by pre-training a network on MobiAct dataset and utilize the learned weights for initialization
of an activity recognition model. Moreover, the later model is trained in a fully-supervised manner on an entire
training set of a particular dataset (e.g., UniMiB). In comparison with supervised training of the network (from
scratch), the weights learned through our technique from a diﬀerent and completely unlabeled data source
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
improved the performance in all the cases. On WISDM and HHAR our results are 3 percentage points beter in
terms of kappa score. Similarly, on UniMiB we obtained 4 percentage points improvement over supervised model,
i.e. kappa score increase from 0.781 to 0.821.
Table 3. Task and Dataset Generalization: Qantifying the quality of transferred self-supervised network. We pre-train
a TPN on MobiAct dataset with the proposed self-supervised approach. The classifier is added on the transferred model
and trained in an end-to-end fashion on a particular activity recognition dataset. We chose MobiAct for transfer learning
evaluation because of the large number of users and activity classes it covers. The reported results are averaged over 10
independent runs, where P, R, F, and K refer to Precision, Recall, F-score, and Kappa, respectively.
Supervised (From Scratch)
Transfer (Self-Supervised)
0.7624±0.0312
0.7353±0.0308
0.7276±0.0297
0.6816±0.0371
0.7816±0.0405
0.7617±0.0469
0.7549±0.0452
0.713±0.056
0.8276±0.0148
0.8096±0.0266
0.8097±0.0248
0.7815±0.0299
00.8557±0.0123
0.8444±0.0191
0.8445±0.0185
0.8214±0.0217
0.9059±0.0133
0.8998±0.0139
0.8981±0.0148
0.8789±0.0168
0.9097±0.0129
0.9073±0.0145
0.9065±0.0152
0.8879±0.0175
0.9024±0.0076
0.8657±0.0206
0.8764±0.0168
0.8211±0.0258
0.9058±0.0102
0.8907±0.0113
0.8946±0.0108
0.8517±0.0153
MotionSense
0.9164±0.0053
0.8993±0.0091
0.9027±0.0085
0.8763±0.011
0.9223±0.0081
0.9059±0.0132
0.9096±0.0126
0.8843±0.016
Further, we determine the generalization ability in a low-data regime seting, i.e., when very few labeled
data are atainable from an end-task of interest. We transfer self-supervised learned representations on the
MobiAct dataset as initialization for an activity recognizer. Te network is trained in a supervised manner on the
available labeled instances of a particular dataset. Figure 7 shows average kappa score of 10-independent runs of
a fully-supervised (learned from scratch) and transferred models for 2, 5, 10, 20, 50, and 100 labeled instances. For
each training run, the desired instances are randomly sampled, and for both techniques, the same instances are
used for learning the activity classiﬁer. In the majority of the cases, transfer learning improves the recognition
performance especially when the number of labeled instances per class are very few, i.e. between 2 to 10. In
particular, on HHAR the performance of a model trained with weights transfer is slightly lower in low-data
seting but improves signiﬁcantly as the number of labeled data points increases. We think it may be because of
the complex characteristics of the HHAR dataset as it is particularly collected to show heterogeneity of devices
(and sensors) having varying sampling rates and its impact on the activity recognition performance.
Determining Representational Similarity
Te previous experiments establish the eﬀectiveness of self-supervised sensor representations for activity classiﬁcation that are signiﬁcantly beter than unsupervised and on-par with fully-supervised approaches. Te critical
question that arises is whether the self-supervised representations are similar to those learned via direct supervision,
i.e., with activity labels. Te interpretability of the neural networks and deciphering of the learned representations
have recently gained signiﬁcant atention, especially, for images (see for an excellent review). Here, to
beter understand the similarity of the extracted representation from TPN and the supervised network, we utilize
singular vector canonical correlation analysis (SVCCA) , saliency maps and t-distributed stochastic
neighbor embedding (t-SNE) .
Insights on Representational Similarity with Canonical Correlation. Te SVCCA allows for a comparison of the
learned distributed representations across diﬀerent networks and layers. It does so through identifying optimal
linear relationships between two sets of multidimensional variates (i.e., neuron activation vectors) arising from
an underlying process (i.e., a neural network being trained on a speciﬁc task) . Figure 8 provides a mean
similarity of top 20 SVCCA correlation coeﬃcients for all pairs of layers for a self-supervised (trained to predict
transformations) and a fully-supervised network. We averaged 20 coeﬃcients as SVCCA implicitly assumes that
all CCA vectors are equally crucial for the representations at a speciﬁc layer. However, there is plenty of evidence
that high-performing deep networks do not utilize the entire dimensionality of a layer . Due to this,
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
# Training instances per class
Supervised
# Training instances per class
Supervised
# Training instances per class
Supervised
# Training instances per class
Supervised
# Training instances per class
MotionSense
Supervised
Fig. 7. Assessment of the transferred self-supervised learned features from a diﬀerent but related dataset (MobiAct) under
semi-supervised seting. We evaluate the performance of the self-supervised approach when diﬀerent unlabeled data are
accessible for representation learning but very few labeled instances are available for training a network on the task of
interest. The TPN is pre-trained initially on MobiAct data and the activity classifier is added on-top; later an entire network
is trained in an end-to-end fashion on few labeled instances. The reported results are averaged over 10 independent runs for
each of the evaluated approaches when we randomly sample 2, 5, 10, 20, 50, and 100 for learning an activity classifier. The
results with weighted f-score are provided in Figure 13 of the Appendix.
averaging over all the coeﬃcients underestimates the degree of representational similarity. To apply SVCCA,
we train both the networks as explained earlier and produce activations of each layer. For a layer, where the
number of neurons is larger than the layer in comparison, we randomly sample neuron activation vectors to
have comparable dimensionality. In Figure 8 each grid entry represents a mean SVCCA similarity between two
layers of diﬀerent networks. We observe a high correlation among temporal convolution layers trained with two
diﬀerent methods across all the evaluated datasets. In particular, a strong grid-like structure emerges between
the last layers of the networks, which is because those layers are learned from scratch with activity labeled data
and result in identical representations.
Visualizing Salient Regions. To further understand the predictions produced by both models, we visualize
saliency maps for the highest-scoring class on randomly selected instances from the MotionSense dataset.
Saliency maps highlight which time steps largely aﬀect the output through computing gradient of the loss
function with respect to each input time step. More formally, let x = [x1, . . . ,xN ] be an accelerometer sample
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
MotionSense
Fully-Supervised
Self-Supervised
Fig. 8. CCA similarity between fully-supervised and self-supervised networks. We employ the SVCAA technique to
determine the representational similarity between model layers trained with our proposed approach and standard supervised
seting. Each pane is a matrix of size layers × layers with each entry showing mean similarity (i.e., an average of top-20
correlation coeﬀicients) between the two layers. Note that there is a strong relation between convolutional layers even
though the self-supervised network is pre-trained with unlabeled data; showing that features learned by our approach are
very similar to those learned directly via supervised learning, with activity classes. Likewise, a grid-like structure appears
between the last layers of the networks depicting high similarity as those layers are always (randomly initialized and) trained
with activity labels.
of length N and Cθ(x) be the class probability produced by a network Cθ(.). Te saliency score of each input
element xk indicating its inﬂuence on the prediction is calculated as:
where L is the negative log-likelihood loss of an activity classiﬁcation network for an input example x.
Figure 9 provides a saliency mapping of the same input produced by the two networks for a class with the
highest score. To aid interpretability of the saliency score, we calculate a magnitude of each tri-axial accelerometer
sample, eﬀectively combining all three channels. Te actual input is given in the top-most pane, the magnitudes
with varying color intensity are shown in the botom panes. Te dark color illustrates the regions that contribute
most to the network’s prediction. We observe that the saliency maps of both self-supervised and fully-supervised
networks hint towards similar regions that are crucial for deciding on the class label.
Interestingly, for the Siting class instance both network mainly focus on a smaller region of the input with
slightly more variation in the values. We think it could be because one thing that a network learns is to ﬁnd
periodic variations in the signal (such as peaks and slopes). Hence, it pays more atention even to slightest
ﬂuctuation, but it decides on the Siting label as the signal remains constant (before and afer minor changes)
which is an entirely diﬀerent patern as compared to the instances of other classes. Tis analysis further validates
the point that our self-supervised network learns generalizable features for activity classiﬁcation.
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
Self-Supervised
Fully-Supervised
Downstairs
Self-Supervised
Fully-Supervised
Self-Supervised
Fully-Supervised
Self-Supervised
Fully-Supervised
Self-Supervised
Fully-Supervised
Self-Supervised
Fully-Supervised
Fig. 9. Saliency maps of randomly selected instances from MotionSense dataset. The input signal is illustrated in the
top pane with the magnitude computed from the sample shown in the botom panes for beter interpretability. The strong
colored intensities exhibit the regions that substantially aﬀect the model predictions. The saliency mapping of both networks
focus on similar input areas which shows that the self-supervised representations are useful for the end-task.
Visualization of High-Level Feature Space through t-SNE. t-SNE is a non-linear technique for exploring and
visualizing multi-dimensional data . It approximates a low-dimensional manifold of a high-dimensional
counterpart through minimizing Kullback-Leibler divergence between them with a gradient-based optimization
method. More speciﬁcally, it maps multi-dimensional data onto a lower dimensional space and discovers paterns
in the input through identifying clusters based on the similarity of the data points. Here, the activations from
global max-pooling layers (of both self-supervised and fully-supervised networks) with 96 hidden units are
projected on to a 2D space. Figure 10 provides the t-SNE embeddings showing high semantic relevance of the
learned features for various activity classes. We notice that the self-supervised features largely correspond to
those learned with the labeled activity data. Importantly, the clusters of data points across two feature learning
strategies are similar, e.g. in UCI HAR, the activity classes like Upstairs, Downstairs and Walking are grouped.
Likewise, in HHAR, the data points for Walking, Upstairs, and Downstairs are close-by as opposed to others in the
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
embeddings of both networks. Finally, it is important to note that t-SNE is an unsupervised technique which
does not use class labels; the activity labels are just used for ﬁnal visualization.
Fig. 10. t-SNE visualization of the learned representations. We visualize the features from Global Max Pooling layers of
fully-supervised and self-supervised networks by projecting them on 2D space. The clusters show high correspondence
among the representations across datasets. For instance, in UniMiB embeddings the samples belonging to the same class
are close-by as opposed to those from a diﬀerent class, such as Running and Walking are alongside each other while data
point from SitingDown class are very far. Note that t-SNE embeddings do not use activity labels, they are only used for final
visualizations.
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
RELATED WORK
Deep learning methods have been successfully used in several applications of ubiquitous computing, pervasive
intelligence, health, and well-being and eliminate the need of hand-crafed feature engineering. Convolutional and recurrent neural networks have shown dominant performance in solving numerous
high-level recognition tasks from temporal data such as activity detection and stress recognition . In
particular, CNNs are becoming increasingly popular in sequence (or time-series) modeling due to their ability of
weight sharing, translation invariance, scale separation and localization of ﬁlters in space and time . In
fact, (1D) temporal CNNs are now widely used in the area of HAR (see for a detailed review), but the prior
works are mostly concerned with supervised learning approaches. Te training of deep networks requires a huge
(carefully) curated dataset of labeled instances, which in several domains is infeasible due to required manual
labeling eﬀort or can only be possible on a small-scale in a controlled lab environment. Tis inherent limitation
of the fully-supervised learning paradigm emphasizes the importance of unsupervised learning to leverage a
large amount of unlabeled data for representation learning that can be easily acquired in a real-world seting.
Unsupervised learning has been well-studied in the literature over the past years. Before the era of end-to-end
learning, manual feature design strategies such as those that employ statistical measures have been used
with clustering algorithms to discover a latent group of activities . Although deep learning techniques
have almost entirely replaced hand-crafed feature extraction with directly learning rich features from data,
representation learning still stands as a fundamental problem in machine learning (see for an in-depth
review). Te classical approaches for unsupervised learning include autoencoders , restricted Boltzmann
machines , and convolutional deep belief networks . Another emerging line of research for unsupervised
feature learning (also studied in this work), which has shown promising results and does not require manual
annotations, is self-supervised learning . Tese methods exploit the inherent structure of the data to
acquire a supervisory signal for solving a pretext task with reliable and widely used supervised learning schemes.
Self-supervision has been actively studied recently in the vision domain, and several surrogate tasks have
been proposed for learning representations from static images, videos, sound, and in robotics . For example, in images and videos, spatial and temporal contexts, respectively,
provide forms of rich supervision to learn features. Similarly, colorization of gray-scale images , rotation
classiﬁcation , odd sequence detection , frame order prediction , learning the arrow of time ,
audio-visual correspondence and synchronization are some of the recently explored directions
of self-supervised techniques. Furthermore, multiple such tasks are utilized together in a multi-task learning
seting for solving diverse visual recognition problems . Tese self-supervised learning paradigms have
shown to extract high-level representations that are on par with those acquired through fully-supervised pretraining techniques (e.g., with ImageNet labels) and they tremendously help with transfer and semi-supervised
learning scenarios. Inspired from this research direction, we explore multi-task self-supervision for learning
representations from sensory data through utilizing transformations of the signals.
Some earlier works on time-series analysis have explored transformations to exploit invariances either through
architectural modiﬁcations (to automatically learn task-relevant variations) or less commonly with augmentation
and synthesis. In task-speciﬁc transformations (such as added noise and rotation) are applied to wearable
sensor data to augment and improve the performance of Parkinson’s disease monitoring systems. Saeed et
al. utilized an adversarial autoencoder for class-conditional (multimodal) synthetic data generation for the
behavioral context in a real-life seting. Moreover, Oh et al. focused on learning invariances directly from
clinical time-series data with specialized neural network architecture. Razavian et al. used convolution
layers of varying size ﬁlters to capture diﬀerent resolutions of temporal paterns. Similarly, through additional
pre-processing of the original data Cui et al. used transformed signals as extra channels to the model for
learning multiscale features. To summarize, these works are geared towards learning supervised networks
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
Multi-task Self-Supervised Learning for Human Activity Detection
for speciﬁc tasks through exploiting invariances, but they do not address the topics of semi-supervised and
unsupervised learning.
To the best of our knowledge, the work presented here is the ﬁrst atempt of self-supervision for sensor
representation learning, in particular for HAR. Our work diﬀers from the aforementioned works in several
ways as we learn representations with self-supervision from completely unlabeled data and without using
any specialized architecture. We show that when training a CNN to predict generally known (time-series)
transformations as a surrogate task, the model can learn features that are on a par with a fully-supervised
network and far beter than unsupervised pre-training with an autoencoder. We also demonstrate that the learned
representations from a diﬀerent (but related) unlabeled data source can be successfully transferred to improve
the performance of diverse tasks even in the case of semi-supervised learning. In terms of transfer learning,
our approach also diﬀers signiﬁcantly from some earlier atempts that were concerned with features
transferability from a fully-supervised model learned from inertial measurement units data, as our approach
utilizes widely available smartphones and does not require labeled data. Finally, the proposed technique is
also diﬀerent from previously studied unsupervised pre-training methods such as autoencoders , restricted
Boltzmann machines and sparse coding as we employ an end-to-end (self) supervised learning paradigm
on multiple surrogate tasks to extract features.
CONCLUSIONS AND FUTURE WORK
We present a novel approach for self-supervised sensor representation learning from unlabeled data with a
focus on smartphone-based human activity recognition (HAR). We train a multi-task temporal convolutional
network to recognize potential transformations that may have been applied to the raw input signal. Despite the
simplicity of the proposed self-supervised technique (and the network architecture), we show that it enables
the convolutional model to learn high-level features that are useful for the end-task of HAR. We exhaustively
evaluate our approach under unsupervised learning, semi-supervised learning and transfer learning setings
on several publicly available datasets. Te performance we achieve is consistently superior to or comparable
with fully-supervised methods, and it is signiﬁcantly beter than traditional unsupervised learning methods
such as an autoencoder. Speciﬁcally, our self-supervised framework drastically improved the detection rate
under semi-supervised learning seting, i.e., when very few labeled instances are available for learning. Likewise,
the transferred features learned from a diﬀerent but related unlabeled dataset (MobiAct in our case), further
improves the performance in comparison with merely training a model from scratch. Notably, these transferred
representations even boost the performance of an activity recognizer in semi-supervised learning from a dataset
(or task) of interest. Finally, canonical correlation analysis, saliency mapping, and t-SNE visualizations show that
the representations of the self-supervised network are very similar to those learned by a fully-supervised model
that is trained in an end-to-end fashion with activity labels. We believe that, through utilizing more sophisticated
layers and deep architectures, the presented approach can further reduce the gap between unsupervised and
supervised feature learning.
In this work, we provided the basis for self-supervision of HAR with smartphones through a few labeled data.
In the Internet of Tings era, there are many exciting opportunities for future works in related areas, such as in
industrial manufacturing, electrical grid, smart wearable technologies, and home automation. In particular, we
believe that self-supervision is of immense value for automatically extracting generalizable representations in
domains, where labeled data are challenging to acquire, but unlabeled data are available in vast quantities. We hope
that the presented perspective of self-supervision inspires the development of additional approaches, speciﬁcally
for the selection of appropriate auxiliary tasks (based on domain expertise) that enables the network to learn
useful features to solve a particular problem. Likewise, combining self-supervision with network architecture
search is another crucial area of improvement that will automate the process of optimal model discovery. Another
exciting avenue for future research is evaluating self-supervised representations on an imbalanced activity dataset,
PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 0, No. 0, Article 000. Publication date: 0.
A. Saeed et al.
where, the number of classes are high and collecting a few labeled data points for each activity class is not feasible.
Finally, evaluation in a real-world seting (application deployed on real devices) is of prime importance to further
understand the aspects that need improvement concerning computational, energy and, labeled data requirements.
ACKNOWLEDGMENTS
Tis work is funded by SCOTT (www.scot-project.eu) project. It has received funding from the Electronic
Component Systems for European Leadership Joint Undertaking under grant agreement No 737422. Tis Joint
Undertaking receives support from the European Union’s Horizon 2020 research and innovation programme and
Austria, Spain, Finland, Ireland, Sweden, Germany, Poland, Portugal, Netherlands, Belgium, Norway.
We thank Prof. Peter Baltus for a helpful discussion and anonymous reviewers for their insightful comments and
suggestions. Various icons used in the ﬁgures are created by Anuar Zhumaev, Korokoro, Gregor Cresnar, Becris,
Hea Poh Lin, AdbA Icons, Universal Icons, and Baboon designs from the Noun Project.