A Direct LDA Algorithm for High-Dimensional Data – with
Application to Face Recognition
Hua Yu 1, Jie Yang
Interactive System Labs, Carnegie Mellon University, Pittsburgh, PA 15213
1. Introduction
Linear Discriminant Analysis (LDA) has been successfully used as a dimensionality reduction technique
to many classiﬁcation problems, such as speech recognition, face recognition, and multimedia information
retreival. The objective is to ﬁnd a projection A
that maximizes the ratio of between-class scatter Sb
against within-class scatter Sw (Fisher’s criterion):
However, for a task with very high dimensional data
such as images, the traditional LDA algorithm encounters several diﬃculties. Consider face recognition
for example. A low-deﬁnition face image of size 64 by
64 implies a feature space of 64 × 64 = 4096 dimensions, and therefore scatter matrices of size 4096 ×
4096 = 16M. First, it is computationally challenging
to handle big matrices (such as computing eigenvalues). Second, those matrices are almost always singular, as the number of training images needs to be at
least 16M for them to be non-degenerate.
Due to these diﬃculties, it is commonly believed
that a direct LDA solution for such high-dimensional
data is infeasible. Thus, ironically, before LDA can be
used to reduce dimensionality, another procedure has
to be ﬁrst applied for dimensionality reduction.
In face recognition, many techniques have been
proposed (For a good review, see ). Among them,
the most notable is a two-stage PCA+LDA approach
A = ALDAAPCA
Principal Component Analysis (PCA) is used to
project images from the original image space into a
1 Corresponding author. Email: 
face-subspace, where dimensionality is reduced and
Sw is no longer degenerate, so that LDA can proceed without trouble. A potential problem is that the
PCA criterion may not be compatible with the LDA
criterion, thus the PCA step may discard dimensions
that contain important discriminative information.
Chen et al. have recently proved that the null space
of Sw contains the most discriminative information
 . But, their approach fell short of making use of
any information outside of that null space. In addition, heuristics are needed to extract a small number
of features for image representation, so as to avoid
computational problems associated with large scatter
In this paper, we present a direct, exact LDA algorithm for high dimensional data set. It accepts high
dimensional data (such as raw images) as input, and
optimizes Fisher’s criterion directly, without any feature extraction or dimensionality reduction steps.
2. Direct LDA Solution
At the core of the direct LDA algorithm lies the idea
of simultaneous diagonalization, the same as in the
traditional LDA algorithm. As the name suggests, it
tries to ﬁnd a matrix that simultaneously diagonalizes
both Sw and Sb:
ASwAT = I,
where Λ is a diagonal matrix with diagonal elements
sorted in decreasing order. To reduce dimensionality
to m, we simply pick the top m rows of A, which
corresponds to the largest m diagonal elements in Λ.
Details of the algorithm can be found in .
The key idea of our new algorithm is to discard
the null space of Sb – which contains no useful information – rather than discarding the null space of
 
8 February 2004
Sw, which contains the most discriminative information. This can be achieved by diagonalizing Sb ﬁrst
and then diagonalizing Sw. The traditional procedure
takes the reverse order. While both approaches produce the same result when Sw is not singular, the reversal in order makes a drastic diﬀerence for high dimensional data, where Sw is likely to be singular.
The new algorithm is outlined below. Figure 1 provides a conceptual overview of this algorithm. Computational issues will be discussed shortly after.
(1) Diagonalize Sb: ﬁnd matrix V such that
V T SbV = Λ
where V T V = I. Λ is a diagonal matrix sorted
in decreasing order.
This can be done using the traditional eigenanalysis, i.e. each column of V is an eigenvector
of Sb, and Λ contains all the eigenvalues. As Sb
might be singular, some of the eigenvalues will
be 0 (or close to 0). It is necessary to discard
those eigenvalues and eigenvectors, as projection
directions with a total scatter of 0 don’t carry
any discriminative power at all.
Let Y be the ﬁrst m columns of V (an n × m
matrix, n being the feature space dimensionality), now
Y T SbY = Db > 0
where Db is the m × m principal sub-matrix of
(2) Let Z = Y D
ZT SbZ = I
Thus, Z unitizes Sb, and reduces dimensionality
from n to m.
Diagonalize ZT SwZ by eigen-analysis:
U T ZT SwZU = Dw
where U T U = I. Dw may contain 0s in its diagonal.
Since the objective is to maximize the ratio of
total-scatter against within-class scatter, we can
sort the diagonal elements of Dw and discard
some eigenvalues in the high end, together with
the corresponding eigenvectors. It is important
to keep the dimensions with the smallest eigenvalues, especially 0s. This is exactly the reason
why we started by diagonalizing Sb, rather than
Sw. See Section 2.2 for more discussion.
(3) Let the LDA matrix
A = U T ZT
A diagonalizes both the numerator and the denominator in Fisher’s criterion:
ASwAT = Dw,
(4) For classiﬁcation purpose, notice that A already
diagonalizes Sw, therefore the ﬁnal transformation that spheres the data should be:
2.1. Computational Considerations
Although the scheme above gives an exact solution
for Fisher’s criterion, we haven’t addressed the computational diﬃculty that both scatter matrices are too
big to be held in memory, let alone their eigen-analysis.
Fortunately, the method presented by Turk and
Pentland for the eigenface problem is still applicable. The key observation is that scatter matrices can
be represented in a way that both saves memory, and
facilitates eigen-analysis. For example,
ni(µi −µ)(µi −µ)T = ΦbΦT
Φb = [√n1(µ1 −µ), √n2(µ2 −µ), · · ·]
J is the number of classes, ni is the number of training
images for class i. Thus, instead of storing an n × n
matrix, we need only to store Φb which is n × J. The
eigen-analysis is simpliﬁed by virtue of the following
Lemma 1 For any n × m matrix L, mapping x →
Lx is a one-to-one mapping that maps eigenvectors of
LT L (m × m) onto those of LLT (n × n).
b Φb is an J ×J matrix, eigen-analysis is aﬀordable. In Step 2 of our algorithm, to compute eigenvalues for ZT SwZ, simply notice
(xi −µki)(xi −µki)T = ΦwΦT
Φw = [x1 −µk1, x2 −µk2, · · ·]
nt is the total number of images in the training set.
ZT SwZ = ZT ΦwΦT
We can again use the Lemma 1 to compute eigenvalues.
2.2. Discussions
Null space of Sw The traditional simultaneous diagonalization begins by diagonalizing Sw. If Sw is
Diagonalize
Discard 0s
& Unitize Λ
Diagonalize
Reduce dimensionality
(optional)
Fig. 1. Thumbnail of the Direct LDA Algorithm
not degenerate, it gives the same result as our approach. If Sw is singular, however, the traditional
approach runs into a dilemma: to proceed, it has to
discard those eigenvalues equal to 0; but those discarded eigenvectors are the most important dimensions!
As Chen et al. pointed out , the null space
of Sw 2 carries most of the discriminative information. More precisely, for a projection direction a, if
Swa = 0, and Sba ̸= 0,
aSwaT is maximized. The
intuitive explanation is that, when projected onto
direction a, within-class scatter is 0 but betweenclass scatter is not. Obviously perfect classiﬁcation
can be achieved in this direction.
Diﬀerent from the algorithm proposed in ,
which operates solely in the null space, our algorithm can take advantage of all the information,
both within and outside of Sw’s null space. Our
algorithm can still be used in cases where Sw is
not singular, which is common in tasks like speech
recognition.
Equivalence to PCA+LDA As Fukunaga pointed
out , there are other variants of Fisher’s criterion:
where St = Sb + Sw is the total scatter matrix.
Interestingly, if we use the ﬁrst variant (with
St in the numerator), Step 1 of our algorithm becomes exactly PCA. Discarding St’s eigenvectors
with 0 eigenvalues reduces dimensionality, just
as Belhumeur et al. proposed in their two-stage
PCA+LDA method . If their LDA step handled Sw’s null space properly, the two approaches
would give the same performance. In a sense our
method can be called “uniﬁed PCA+LDA”, since
there is no separate PCA step. It not only leads to
a clean presentation, but also results in an eﬃcient
implementation.
3. Face Recognition Experiments
We tested the direct LDA algorithm on face
images from Olivetti-Oracle Research Lab (ORL,
 The ORL dataset consists of 400 frontal faces: 10 tightly-cropped images of
2 Null space of Sw = {x|Swx = 0, x ∈Rn}.
40 individuals with variations in pose, illumination,
facial expression (open/closed eyes, smiling/not smiling) and facial details (glasses/no glasses). The size
of each image is 92 × 112 pixels, with 256 grey levels
per pixel.
Three sets of experiments are conducted. In all cases
we randomly choose 5 images per person for training,
the other 5 for testing. To reduce variation, each experiment is repeated at least 10 times.
Without dimensionality reduction in Step 2, average recognition accuracy is 90.8%. With dimensionality reduction, where everything outside of Sw’s null
space is discarded, average recognition accuracy becomes 86.6%. This veriﬁes that while Sw’s null space is
important, discriminative information does exist outside of it.
4. Conclusions
In this paper, we proposed a direct LDA algorithm
for high-dimensional data classiﬁcation, with application to face recognition in particular. Since the number
of samples is typically smaller than the dimensionality
of the samples, both Sb and Sw are singular. By modifying the simultaneous diagonalization procedure, we
are able to discard the null space of Sb – which carries
no discriminative information – and to keep the null
space of Sw, which is very important for classiﬁcation.
In addition, computational techniques are introduced
to handle large scatter matrices eﬃciently. The result
is a uniﬁed LDA algorithm that gives an exact solution
to Fisher’s criterion whether or not Sw is singular.