Building a Foundation for Data-Driven, Interpretable,
and Robust Policy Design using the AI Economist
Alexander Trott*,1, Sunil Srinivasa*,1, Douwe van der Wal1, Sebastien Haneuse2,
and Stephan Zheng*,†,1
1Salesforce Research
2Harvard University
August 9, 2021
Optimizing economic and public policy is critical to address socioeconomic issues
and trade-offs, e.g., improving equality, productivity, or wellness, and poses a complex
mechanism design problem. A policy designer needs to consider multiple objectives, policy
levers, and behavioral responses from strategic actors who optimize for their individual
objectives. Moreover, real-world policies should be explainable and robust to simulation-toreality gaps, e.g., due to calibration issues. Existing approaches are often limited to a narrow
set of policy levers or objectives that are hard to measure, do not yield explicit optimal
policies, or do not consider strategic behavior, for example. Hence, it remains challenging to
optimize policy in real-world scenarios. Here we show that the AI Economist framework
enables effective, ﬂexible, and interpretable policy design using two-level reinforcement
learning (RL) and data-driven simulations. We validate our framework on optimizing the
stringency of US state policies and Federal subsidies during a pandemic, e.g., COVID-19,
using a simulation ﬁtted to real data. We ﬁnd that log-linear policies trained using RL
signiﬁcantly improve social welfare, based on both public health and economic outcomes,
compared to past outcomes. Their behavior can be explained, e.g., well-performing policies
respond strongly to changes in recovery and vaccination rates. They are also robust to
calibration errors, e.g., infection rates that are over or underestimated. As of yet, real-world
policymaking has not seen adoption of machine learning methods at large, including RL
and AI-driven simulations. Our results show the potential of AI to guide policy design and
improve social welfare amidst the complexity of the real world.
*: AT, SS, and SZ contributed equally. †: Correspondence to: . AT,
SS, SH, and SZ developed the theoretical framework. DW, SS, and AT cataloged and processed data. SS, AT, and
 
Introduction
Designing policy for real-world problems requires governments (social planners) to consider
multiple policy objectives and levers, and hierarchical networks of strategic actors whose
objectives may not be aligned with the planner’s objectives. Moreover, real-world policies should
be effective, robust to simulation-to-reality gaps, but also explainable, among others. As such,
optimizing policy is a complex problem, akin to mechanism design (1), for which few tractable
and comprehensive solutions exist.
Towards AI policy design for the real world, we extend the AI Economist framework (2, 3)
to learn effective, robust, and interpretable policies. The AI Economist combines multi-agent,
two-level reinforcement learning (RL) – e.g., training both a social planner and economic agents
– in economic simulations grounded in real-world data. This infrastructure was previously used
to design AI income tax policies that improve equality and productivity (2, 3).
We present two contributions. First, we outline desirable properties of a policy design
framework that can be used in the real world and review recent progress in machine learning and
economics towards this goal. Second, we apply this framework to a proof-of-concept use case:
designing US state and federal policy to respond to the COVID-19 pandemic, evaluated within
an integrated pandemic-economic simulation based on real data.
The AI Economist framework ﬁlls an important gap in the space of policy design. Existing
analytical approaches are limited to a narrow set of policy objectives or do not yield explicit
solutions in the sequential setting. Empirical methods lack counterfactual data to estimate
behavioral responses (the Lucas critique). Moreover, existing simulation-based approaches often
do not consider both strategic planners and agents, or interactions between actors (4–6). Such
complex settings include income taxation (tax policy changes the post-tax utility that economic
agents experience) and pandemics (Federal subsidies can lighten the economic burden on citizens
and may incentivize US states to employ more stringent public health policies).
As a case study, we show how our framework can optimize public health and economic
policies for both public health (e.g., reducing COVID-19 deaths) and the economy (e.g., maintaining productivity) during a pandemic. Using real data, we simulate COVID-19, vaccinations,
and unemployment in the United States. We model the stringency level of 51 US state-level
policies, including all 50 US states and Washington D.C., and federal subsidies in the form of
SZ developed the simulator. SS and AT performed experiments. AT, SS, SH, and SZ analyzed experiments. AT and
SZ drafted the manuscript. SS and SH commented on the manuscript. SZ and AT conceived the project. SZ planned
and directed the project. Source code for the economic simulation is available at 
salesforce/ai-economist. More information is available at 
The word “policy” is overloaded. First, it can refer to the decisions made by a social planner (government), as
in “public policy” and akin to a mechanism (1). Second, it may refer to the behavioral policy model of agents, as is
common in the reinforcement learning literature. In this work, “policy” and “planner policy” refer to the social
planner, while “behavioral policy” and “agent policy” refer to the behavior of agents.
Also referred to as “bi-level”.
We explicitly mention “US” in “US state” to disambiguate it from “state” as used in the RL context.
direct payments to citizens. More stringent US state policy (e.g., closing down businesses more)
may temper the spread of the pandemic (7, 8), but may lead to lower productivity owing to
higher unemployment. As a modeling assumption, the policy objective of US states depends
on the level of federal subsidies: higher federal subsidies may incentivize US states to accept
higher unemployment and be more stringent to reduce deaths. However, this may incur a higher
national cost, e.g., through borrowing money. Hence, this presents a hard two-level optimization
problem. Although the underlying public health and economic mechanics are complex, these
correlations and trade-offs are salient in real data.
We ﬁnd that log-linear RL policies achieve signiﬁcantly higher social welfare than baseline
policies executed in simulation. These policies perform well across a wide range of relative
weightings of public health and the economy, and demonstrate how the social planner can align
US state incentives with national social welfare through subsidies. The policies are explainable:
well-performing policies respond strongly to changes in recovery and vaccination rates, for
example. Moreover, these results are robust to a range of calibration errors, e.g., infection rates
that are over or underestimated.
Our work is descriptive, not normative. We show outcomes for a class of stylized social
welfare objectives which follows standard economic modeling. However, we acknowledge that
the objectives of real-world actors typically involves many features. Taken together, our results
motivate building representative and ﬁne-grained simulations and developing machine learning
algorithms to learn AI-driven policies that can recommend policy and improve social welfare in
the real world.
AI-driven Policy Design for the Real World
AI methods can help solve complex policy design problems, as illustrated in our case study on
optimizing pandemic response policies in Section 3. Our AI policy design framework consists
of two pillars: simulations, grounded in real-world data and featuring AI agents, and two-level
RL, as outlined in Figure 1. In this Section, we describe how machine learning can be applied to
economic policy design, aligning concepts from both ﬁelds. We then describe several features
of the AI Economist framework that are desirable for designing policy, and highlight open
challenges. We also outline how to extend our framework to increase realism in the future in the
Discussion.
A Learning Perspective on Economic Policy Design
In this work, we consider a dynamic environment with two actor types: a social planner (denoted
by p) and N economic agents. We index actors with i = 1, . . . , N, p. For example, the social
planner represents a government, while the economic agents represent individuals. In our Case
Study, the social planner represents the federal government, while the economic agents are the
US states.
Extending the AI Economist framework for policy design. The core of the AI
Economist framework is an economic simulation featuring a social planner and economic agents,
together called actors. All actors optimize their (behavioral) policy model using reinforcement
learning; repeatedly observing, deciding, and updating their model using reward feedback. As
agents may strategically respond to the planner’s policy, this poses a two-level (or bi-level)
optimization problem. We extend the framework studied in (2, 3) by adding key desirable
properties for AI policy design for the real world. These include using simulations ﬁtted to
real data, using explainable policy models, and ensuring robust performance under meaningful
simulation parameter perturbations, see Section 2. We use a case study on pandemic response
policy to demonstrate these features, see Section 3. Further extensions and aspects of our
framework are discussed in Section 4.
Formally, the environment can be represented as a Markov Game (9). Episodes iterate for
T > 1 time steps; at each time t, each actor i observes a part of the world state st, can perform
actions ai,t, and receives a scalar reward ri,t. The environment then transitions to the next state
st using environment dynamics T (st+1|st, at). Assuming rationality, each actor optimizes its
behavioral policy πi (ai,t|oi,t) to maximize its total (discounted) future reward P
iri,t. Here,
oi,t is the observation received by i at time t, and 0 < γi ≤1 is a discount factor. A small (or
large) discount factor, γi ≈0 (or γi ≈1), emphasizes long-term (or short-term) rewards.
Following standard economic modeling, each agent experiences a utility ui,t; its reward is
its marginal utility gain ri,t = ui,t −ui,t−1. In effect, an agent optimizes its total (discounted)
utility over the course of an episode. The social planner aims to optimize social welfare
In dynamic economic models, utility is often modeled as a function of the instantaneous consumption at a
single time step. In that case, each step represents a period with meaningful total consumption, e.g., a month or year.
F (s0, . . . , sT; α), parameterized by α, which, informally speaking, represents how well off
society is. Accordingly, the planner’s reward at time t is the marginal gain in social welfare.
A common social welfare choice is the utilitarian objective, in which the planner optimizes
i ui,t, using a discount factor 0 < γp ≤1.
In an economic context, rewards represent incentives and may incentivize agents to cooperate
or compete. As such, the combination of planner and agent reward functions (r1, . . . , rn, rp)
can present signiﬁcant learning challenges and yield complex behaviors during learning and at
equilibrium. For instance, the social planner may attempt to align a-priori misaligned incentives
between the planner and agents, as studied in principal-agent problems (10).
Two-Level Learning.
A special feature of the policy design problem is that it presents a twolevel learning problem: the actions of an actor can explicitly affect the reward that other actors
optimize for. For example, a planner that changes income tax rates changes the post-tax income
that agents receive, which can change (the shape of) their utility (2, 3). As another example,
in this work we consider how planner subsidies (federal level) can affect the social welfare
experienced by agents (US state level). As such, two-level learning problems and generalizations
thereof appear naturally in many economic and machine learning settings (11). However, they
also present a hard optimization challenge and can lead to highly unstable learning behavior.
The key reason is that as one actor changes its policy, the shape of the entire reward function
for other actors may change signiﬁcantly, and cause actors to change their behavior. Hence, our
policy design setting is distinct from games in which deep RL has reached (super)-human skill,
e.g., Go (12) and Starcraft (13), but that have a ﬁxed reward function throughout.
Mechanism Design.
The planner’s policy design problem is an instance of mechanism design
(1) with strategic agents. A mechanism constitutes a set of rewards and environment dynamics
(14), which is akin to the planner’s policy. The mechanism designer’s goal is to ﬁnd a mechanism
that certain desired properties, e.g., it incentivizes agents to reveal their true preferences to the
planner. However, few tractable analytical solutions to the general mechanism design problem
exist, e.g., a full analytical understanding is lacking for auctions with even two items and multiple
bidders (15). As such, our work presents an RL-based solution to ﬁnd optimal mechanisms.
Features of AI-driven Policy Design
The use of AI methods, including RL, has beneﬁts for policy design, but also has idiosyncratic
features that present practical challenges and remain open areas of research. We now present
how these beneﬁts are manifest in the AI Economist framework, how it addresses the challenges
of using AI for policy design, and how it compares to existing policy design methods.
In our work, each time step represents a day, a more ﬁne-grained temporal resolution, and episodes last for about
a year. As such, in our setting, it is more natural to deﬁne rewards such that we accumulate utility over an entire
episode. Here, each episode can be thought of as a single time step at a coarser temporal resolution.
Data-Driven Economic Simulations.
Existing computational approaches to economic modeling include general equilibrium models (16, 17) and inter-generational inter-regional dynamics
(5) that consider macro-economic, aggregate phenomena. However, these approaches typically do not model the behaviors of and interactions between individuals, while their structural
estimation and calibration to real data remains challenging.
On the other hand, agent-based models (18, 19), including agent computational economics
(20), study economies as dynamic systems of individual agents and the resulting emergent
phenomena. However, previous work in these areas largely lack rich behavioral models for
agents that implement strategic behaviors, for example, using multi-agent reinforcement learning.
Our learning-based approach builds on these ideas by modeling strategic behavior at all levels,
e.g., training social planner and economic agents who optimize for their individual or social
objectives. As such, our framework enables a more richer descriptions of collective behavior and
emergent phenomena.
Using Real Data.
Similar to standard economic modeling, our learning approach uses economic simulations that are ﬁtted to real data. Beyond the exposition in this work, we outline
more ways to learn more realistic economic models using micro-level and macro-level data.
First, micro-level data could be used to learn models that mimic human behavior, learning and
adaptation, and utility functions. Second, macro-level data could be used to ﬁt the aggregate
(emergent) behaviors of collections of strategic agents. See the Discussion for more details.
Finding Equilibrium Solutions.
In a game-theoretic sense, a key objective of policy design
is to ﬁnd equilibrium solutions in which the behavior of all actors is optimal. Optimality is not
uniquely deﬁned in the multi-agent setting; many deﬁnitions of equilibria exist. In this work,
we assume the planner’s policy should maximize social welfare and agents should maximize
their utility given the planner’s policy. Such two-level structures (planner and agents) are akin
to Stackelberg games (21). As both the agents and planner are strategic, ﬁnding equilibrium
solutions presents a challenging learning problem.
The AI Economist uses RL to solve the two-level problem, learning both the optimal planner’s
policy and agent behavioral policies using RL. Given the complexity of sequential two-level
optimization, few theoretical guarantees for ﬁnding exact equilibria exist. Moreover, it is hard to
enumerate or classify all equilibria for general-sum, two-level problems. However, we ﬁnd that
two-level RL can ﬁnd well-performing policies that are both robust and explainable. Speciﬁcally,
several intuitive two-level RL strategies have been shown to be effective, including the use of
curricula and training actors separately or jointly in multiple phases (2, 3).
Flexible Policy Objectives and Levers.
In the real world, economic policy needs to be optimized for a wide spectrum of policy objectives, such as productivity, equality, or wellness.
However, economic approaches often optimize policy for total utility and other stylized objectives and constraints (22). Such objectives often enjoy certain theoretical guarantees or analytical
properties, but might be unrealistic and can be hard to measure in the real world. For instance, it
is hard to measure utility, while human agents might not act optimally or display behavior that
optimizes a stylistic utility function.
Similarly, it can be challenging to optimize policy across multiple levers, such as taxes,
subsidies, or business closures.
A key beneﬁt of using RL is that we can use any quantitative objective, including those that
are not analytical or differentiable. For example, the AI Economist optimizes both public health
and economic objectives in the Case Study. This makes our framework highly ﬂexible and easier
to align with real-world use cases.
Robustness.
A critical aspect of economic modeling is uncertainty due to a lack of data, noisy
data, structural misspeciﬁcation, lack of model capacity, or other issues. Speciﬁcally in the
machine learning context, policies learned by RL may be not transfer well to the real world
if they are not robust to errors in the underlying economic simulations and gaps between the
simulation and the real world.
In this work, we demonstrate empirically that two-level RL can learn log-linear policies
that are robust under perturbations of simulation parameters, see the Case Study for details.
However, it is still hard to theoretically guarantee (levels of) robustness in the general setting, and
especially when using complex model classes such as deep neural networks (23). Several machine
learning techniques could be used to improve the robustness of policy models, including data
augmentation (24), domain randomization (25), adversarial methods (26), and reward function
perturbation (27). Furthermore, when using linear policies and linear simulation dynamics,
control theoretic guarantees may exist (28). In all, improving robustness remains a key research
Explainability.
Machine learning models have shown to be highly effective for many prediction and behavioral modeling problems. In particular, deep neural networks can learn rich
strategic behaviors in economic simulations (2, 3). However, to build trust, learned economic
policies should also be explainable or interpretable. Unfortunately, there is no general consensus
deﬁnition of explainability for high-dimensional machine learning models. Rather, explanations
are often domain-speciﬁc (29) and their acceptability is prone to subjectivity.
A form of explainability is feature attribution. For example, the weights of linear models
show how predictions, e.g., policy decisions, depend on (a change in) the input features. Using
this rationale, we show in our Case Study that log-linear policies that achieve high social welfare
respond strongly to (rises in) infection rates.
By extension, local function approximations may provide post-hoc explanations of a complex
model’s behavior on a subset of the data (30).
However, one must guard against over-interpreting explanations, especially in high-dimensional
state or action spaces, and for complex policy models, such as deep neural networks. For instance,
“plausible” explanations may not be robust (31) or generalize to all data or unseen environments.
Simplicity and Policy Constraints.
A heuristic quality of real-world policy is that it should
be “simple” enough to be acceptable. Furthermore, “simple” policies may be more explainable
or robust. For example, it could be desirable that real-world policy should not change too
often or too dramatically, e.g., doubling tax rates. This parallels common economic modeling
choices, such as consumption smoothing (32) and sticky prices (33). A beneﬁt of our RL
approach is that it is relatively straightforward to impose constraints on policy, e.g., through
regularization or manually speciﬁed action masking. We ﬁnd empirically that gradient-based
optimization techniques can effectively ﬁnd optimal policies under such constraints. Moreover,
more sophisticated techniques for constrained RL could be used (34). In contrast, it can be
hard for analytical policy design frameworks to include constraints that are not differentiable or
analytical.
Challenges of RL for Policy Design.
Although RL is a ﬂexible framework that can ﬁnd
well-performing policies, there are several learning challenges that are topics of active research.
One salient challenge is that model-free RL, as used in our work, can be inefﬁcient: it typically
requires many learning samples acquired through interaction with the simulation. Here, “modelfree RL” refers to RL methods that do not use or learn structural models of the simulation
environment (35). Also, multi-agent RL can be challenging because any RL agent experiences
and learns in a non-stationary environment when other RL agents are also learning and changing
their behavior (36). Despite these challenges, we ﬁnd empirically that RL policies can be trained
to perform well in our Case Study within a reasonable amount of time, e.g., 1-2 days, using the
learning strategies detailed hereafter.
Case Study: Optimizing Public Health and the Economy
during Pandemics
We now demonstrate how to apply the AI Economist framework to design and evaluate economic
and public health policy during the COVID-19 pandemic in the United States. We model policy
at the level of US states, which control the stringency of public health measures, as well as the
federal government, which controls direct payment subsidies. Each actor optimizes its own
policy for its own deﬁnition of social welfare, taken here as a weighted combination of health and
economic outcomes (related to deaths and GDP, respectively) within its jurisdiction. This tradeoff between public health and economic outcomes is salient in observable data collected during
the pandemic, although the underlying public health and economic mechanics are complex. The
following sections illustrate the features of our proposed framework via this case study. We
present an overview of how we design the policy simulation and calibrate it with real-world data.
In addition, we analyze the AI policies and outcomes corresponding to a range of possible policy
This trade-off also has been a speciﬁc focus of policymakers and policy measures in public discourse.
Figure 2: Policy design using a simulation of the COVID-19 pandemic and the economy,
and two-level reinforcement learning. We train policy models for a social planner (federal
government) and agents (US states). Each actor takes actions using its policy model, based on
an observation of an integrated pandemic-economic simulation of the United States. US state
agents (federal planner) are rewarded by increases in state-level (federal-level) social welfare.
As a modeling choice, federal subsidies can offset the economic impact for US states, which
may incentivize US states to respond more stringently to the pandemic. As such, this poses a
two-level RL problem in which US states optimize their policy, partially in response to federal
objectives. Finally, we examine the properties of interpretability and robustness by examining
the learned policy weights and analyzing model sensitivity, respectively.
Related Work.
Relatively few works have studied optimal policy design in the intersection
of public health and the economy. To the best of our knowledge, no other work has studied the
interaction between unemployment, COVID-19, and policy interventions. Existing public health
research has focused on many aspects of COVID-19, including augmenting epidemiological
models for COVID-19 (37, 38), and analyzing contact-tracing in agent-based simulations (39),
and evaluating the efﬁcacy of various public health policy interventions (7, 40, 41). An analytical
approach to policy design showed that the elasticity of the fatality rate to the number of infected
is a key determinant of an optimal response policy (8, 42). Moreover, statewide stay-at-home
orders had the strongest causal impact on reducing social interactions (43).
In the economics literature, the effect of pandemic response policies has been studied, such as
the relationship between unemployment insurance and food insecurity (44), while various sources
have tracked government expenditure during the pandemic (45–47). Difference-in-difference
analysis of location trace data ﬁnds that imposing lockdowns leads to lower overall costs to the
economy than staying open (48), under a modiﬁed SIR model. Early US data has also shown the
unequal distribution and unemployment effects of remote work across industries (49).
Simulation Design and Calibration
For our case study, we built a simulation model that captures the impact of policy choices on the
overall economic output and spread of the disease. Our modeling choices are driven by what
can be reasonably calibrated from publicly available health and economic data from 2020 and
2021. Future simulations could be expanded with the availability of more ﬁne-grained data.
This section provides an overview of the simulation design and calibration; concrete details are
provided in the Methods (Sections 6.1-6.5).
Policy Levers
The simulation models the consequences of 51 state-level policies, for all 50
US states and the District of Columbia, and 1 federal policy. Each state-level policy sets a
stringency level (between 0% and 100%), which summarizes the level of restrictions imposed
(e.g. on indoor dining) in order to curb the spread of COVID-19. The stringency level at a
given time reﬂects the number and degree of active restrictions. This deﬁnition follows the
Oxford Government Response Tracker (50). The federal policy periodically sets the daily per
capita subsidy level, or direct payment. Federal subsidies take the form of direct payments to
individuals, varying from $0 to $55 per day per person, in 20 increments.
Epidemiology Model.
We use an augmented SIR-model (51) to model the evolution of the
pandemic, including the effect of vaccines. We only model the outbreak of a single variant of
COVID-19. The standard SIR model emulates how susceptible individuals can become infected
and then recover. By convention, recoveries include deaths. As a simplifying assumption,
only susceptible individuals are vaccinated, recovered individuals cannot get reinfected, and
vaccinated individuals gain full immunity to COVID-19, directly moving from susceptible to
recovered. Within each US state, the infection rate (susceptible-to-infected) is modeled as a
This is motivated by empirical results that vaccines commonly used in the US are more than 95% effective at
preventing serious illness due to and spreading of COVID-19.
linear function of the state’s stringency level. This reﬂects the intuition that imposing stringent
public health measures can temper infection rates.
Economic Model.
For each US state, daily economic output is modeled as the sum of incoming
federal subsidies plus the net production of actively employed individuals. At the federal level,
this output is taken as the sum of the State-level outputs, minus borrowing costs used to fund
the outgoing subsidies. We model unemployment using a time-series model that predicts a
state’s unemployment rate based on its history of stringency level increases and decreases. The
daily productivity per employed individual is calibrated such that yearly GDP at pre-pandemic
employment levels is equal to that of the US in 2019.
Datasets and Calibration.
Data for the daily stringency policies are provided by the Oxford
COVID-19 Government Policy Tracker (50). The date and amount of each set of direct payments
issued through federal policy are taken from information provided by the COVID Money Tracker
project (52). We use the daily cumulative COVID-19 death data provided by the COVID-19
Data Repository at Johns Hopkins University (53) to estimate daily transmission rates. Daily
unemployment rates are estimated based on the monthly unemployment rates reported by the
Bureau of Labor Statistics (54). We ﬁt the disease (unemployment) model to predict each State’s
daily transmission (unemployment) rate given its past stringency levels. We allow State-speciﬁc
parameters for both models, but during ﬁtting we regularize the State-to-State variability to
model common trends across states and to prevent overﬁtting to noisy data.
We calibrated the simulation on data from 2020 only. We ﬁt simulation models on data
through November 2020 and use December 2020 for validation. As such, alignment between
simulated outcomes during 2021 and real-world data from 2021 reﬂects the ability of the
simulation to forecast policy consequences beyond the timeframe used for calibration. See
Section 6.3 in the Methods for additional details regarding datasets and calibration.
Metrics and Objectives.
Each actor (i.e. a US state or the federal government) sets its policy
so as to optimize its social welfare objective, which can be deﬁned differently for different actors.
For each actor i, we assume that its social welfare objective Fi is deﬁned as a weighted sum of a
a public health index Hi and an economic index Ei:
Fi(αi) = αiHi + (1 −αi)Ei.
Here i ∈{1, . . . , 51, p} indexes the and 51 US state- and federal-level actors, and αi ∈ 
parameterizes the priority actor i gives to health versus the economy. With this notation, we
consider the federal actor as the planner.
Each index summarizes the health or economic outcomes experienced during the simulation.
More precisely, Hi (or Ei) is measured as the average daily health (or economic) index for actor
We also account for deaths and active infections when modeling unemployment.
i. The marginal health index ∆Hi,t at time t (each time-step represents a day) measures the
number of new COVID-19 deaths in the jurisdiction of actor i. The marginal economic index
∆Ei,t is a concave function of total economic output (described above) at time t in jurisdiction i.
∆Ei,t = crra
Here, Pi,t denotes the total economic output, which includes any incoming subsidies, and P 0
average pre-pandemic output. The federal government uses
Pi,t −c · ˜T state
so that federal-level economic output accounts for the borrowing cost of funding outgoing
subsidies ˜T state.
Using the CRRA function (see Section 6.4) to deﬁne ∆Ei,t as a concave function of total
economic output is an important modeling choice (albeit a common one in economic modeling).
In particular, it captures the intuition that a decrease in economic output is felt more severely
when that output is already low. Consequently, federal subsidies, which raise the level economic
output for US states, can “soften the blow” of additional unemployment and thereby indirectly
incentivize additional stringency by mitigating the trade-off that States face between Hi and Ei.
This choice therefore imparts a two-level problem structure between the US states and the federal
government, further motivating the application of our policy design framework.
To simplify notation and analysis, we normalize indices so that each minimum-stringency
policy yields Hi = 0 and Ei = 1 and so that each maximum-stringency policy yields Hi = 1
and Ei = 0. As a result, higher index values denote more preferable outcomes.
Finally, we calibrate each αi by setting it to the value ˆαi that maximizes social welfare
Fi(ˆαi) under the real-world stringency policies (see Section 6.5 in the Methods for details).
Calibrating ˆαi in this way facilitates comparison against outcomes simulated using real-world
policies. However, our framework allows us to explore outcomes over a wide space of alternative
α conﬁgurations, which we demonstrate below.
We compare simulated outcomes using real-world and AI policies, trained using the AI Economist
framework with the ˆαi that maximize social welfare under the real-world policies. We use loglinear policies:
π(σi,t = j|oi,t) =
oik,tWkj + bij
Here, π(σi,t = j|oi,t) denotes the (conditional) probability that US state i will select stringency
level σi,t = j at time t, given its observations oi,t. We discretize the stringency level σ into 10
Figure 3: Real-world vs simulation outcome. Left: average US state policy stringency level.
Middle: US unemployment rate. Right: total US deaths. AI policies impose more stringent
policy at the start of the pandemic before tapering off more quickly. This temporarily yields
higher unemployment, but results in fewer total deaths. The simulation ﬁts the data well: the
simulation ﬁts the real data well when executed with the real-world policy.
levels, such that j ∈{1, . . . , 10} indexes each of the possible stringency levels. The normalization Zi,t ensures that π is a proper probability distribution for all i and t: P
j π(σi,t = j|oi,t) = 1.
The terms W and b represent the learnable parameters that are optimized when training the
AI policies. Wkj is the weight matrix, with j indexing stringency levels and k indexing input
features. bij is the bias that US state i has towards stringency level j. The weight matrix W is
shared across US states, whereas the bias terms b are speciﬁc to each state. A similar model is
used for the federal policy. This policy model choice emphasizes explainability and simplicity,
which we explore in more detail hereafter.
Throughout training and analysis, we initialize each simulation episode such that t = 0
corresponds to March 22, 2020. During training, episodes run for T = 540 timesteps; however,
at the time of this analysis, real-world data were only available through the end of April 30, 2021
(t = 404), so we treat this date as the end of our analysis window.
As shown in Figure 3, simulated unemployment and COVID-19 deaths under real-world
policies (dashed red lines) approximate the real-world trends well (dashed gray lines). Compared
to real-world policies, AI policies (blue lines) impose comparatively higher stringency at the
start of the outbreak but reduce stringency more rapidly. Similarly, AI policies result in more
unemployment early on but recover towards pre-pandemic levels more quickly. Overall, however,
unemployment under AI policies is higher on average during the analysis window. Moreover, AI
policies result in considerably fewer COVID-19 deaths in this simulation. Figure 4 illustrates
these trends for several US states. For a full view including all US states, see the Extended Data.
Figure 4: Real-world vs simulation outcome at the state level. In order, from top to bottom:
Stringency level; active COVID-19 case load; cumulative COVID-19 deaths; unemployment;
and daily economic output. Infections, deaths, and unemployment numbers are expressed as
percentages of the state population; economic output is similarly normalized to reﬂect per capita
daily GDP. Note: For the “Real-World” data (gray), infection numbers are estimated based on
available data on COVID-19 deaths (see Methods) and economic output is estimated from our
model given real-world data as inputs.
AI Policies Achieve Higher Social Welfare in Simulation. AI policies achieve
higher social welfare at the federal level and for 33 out of 51 US states when compared to
real-world policies. Top: welfare comparison when using ˆα values given by the default calibration. Middle: welfare comparison with highest tested αp and default ˆα1:N. Bottom: welfare
comparison with highest tested α1:N and default ˆαp. In each plot, welfare is calculated with
the α values used to train AI policies. Bar heights and error bars denote the mean welfare
improvement (as a percentage of welfare under real-world policies) and STE, respectively, across
the 10 random seeds used to train AI policies, for each actor in the simulation.
Improved Social Welfare
Figure 5 shows the percentage change in welfare, for each actor, from AI policies versus from
real-world policies in the simulation. The top subplot shows results when social welfare is
deﬁned using the ˆα values obtained during calibration (see above).
Federal-level welfare is 4.7% higher under AI policies. We identify two features underlying
this improvement. First, the AI stringency policies achieve a more favorable balance between
unemployment and COVID-19 deaths. Second, the AI subsidy policy provides very little direct
payments in order to achieve this balance.
In comparison, the real-world policies include a total of $630B in subsidies (versus just $35B
on average for AI policies). US state-level actors gain welfare via subsidies. As a result, the
welfare beneﬁts from AI policies are less pronounced at the level of US states. Nevertheless, we
observe that welfare improves for 33 of the 51 state-level actors when using AI policies.
As we also demonstrate below, our framework allows us to analyze outcomes under many
possible deﬁnitions of social welfare. To explore the space of welfare conﬁgurations, we
manipulate the ratio of
1−α relative to the default calibration. For example, to explore the case
where US state i cares twice as much about health outcomes, we set αi such that
1−αi = 2 ·
where αi is the health priority parameter used to compute Fi during training and analysis, and ˆαi
is the value obtained during calibration. To simplify notation, we use mi to denote this relative
re-scaling, where, for example, mi = 3 denotes that
1−αi = 3 ·
1−ˆαi, i.e. that US state i cares
three times as much about health outcomes.
Figure 5 (middle) shows welfare improvements using mp = 4. In this case, the federal
government’s increased prioritization on health leads to very large subsidy levels ($4.4T on
average), which considerably improve state-level welfare. These large subsidies induce a shift
towards higher stringency in the state-level policies (and, hence, better health outcomes); hence,
the federal objective (when mp = 4) beneﬁts from this extreme subsidy level as well.
When the increased health priority is applied to the states (m1:N = 4), we again see consistent
social welfare improvements compared to real-world policies (Figure 5, bottom). In this case, the
improvement is driven simply from the capacity of the AI policies to adapt to different objectives,
while the real-world stringency policies are (as expected) suboptimal for this parameterization of
social welfare.
Outcomes under Varying Welfare Objectives
The AI Economist framework can ﬂexibly optimize policy for any quantiﬁable objective. Hence,
we can explore the space of health and economic outcomes by changing the balance between
health and economic policy objectives.
To demonstrate this ﬂexibility, we train AI policies across a range of welfare parameterizations αi at the US state-level and federal-level. We examine different re-scalings mi of the ratio
1−ˆαi, where mi > 1 captures that actor i gives more weight to public health compared to the
default calibration ˆαi (described above). Figure 6 shows the state-level (left) and federal-level
(right) outcomes, in terms of Health and Economic Indices, under various settings for m1:N
(relative health prioritization of each US state) and mp (relative health prioritization of the federal
government).
As expected, changing the policy objective leads the AI policies to select a different trade-off
between public health and the economy. Higher m1:N lead to a higher Health Index at the
expense of the Economic Index. A similar trend is seen at the federal level with increasing mp.
Health and economic indices for different social welfare objectives. Colors
indicate the relative health priority scaling (deﬁned in main text) m1:N of US state policies. Dot
sizes indicate the relative health priority scaling mp of federal policy. Black ‘X’ denotes index
values achieved under the real-world policy. Ceteris paribus, as US states emphasize health more
(higher mi:N, ﬁxed mp), US state-level health indices increase on average, but the economic
index decreases. Similarly, as federal policy emphasizes health more (higher mp), US state and
federal health indices increase, but the federal economic index decreases, reﬂecting the higher
total borrowing cost of subsidies.
In our model, subsidies incentivize a US state to be more stringent by reducing the economic
burden of additional unemployment. For certain settings of m1:N and mp, the federal government
prefers to use this economically costly mechanism to better align states’ incentives with its own
policy objective. As a result, the trade-off between public health and the economy selected at the
federal level depends on mp. Interestingly, however, because subsidies reduce the trade-off US
states face between public health and the economy, higher mp tends to increase both Health and
Economic Indices at the state level. However, this comes at a higher total borrowing cost to fund
subsidies.
Examining Learned Weights of the State-Level Stringency Policy
As discussed in Section 2.2, AI-driven policy design should emphasize explainability and
simplicity when possible. This motivated our choice of using log-linear policies.
Figure 7 illustrates the learned stringency policy weights Wkj and bij (Eq. 4), trained with
health prioritizations ˆαi and averaged over 10 repetitions with random seeds. Examining W
conﬁrms several intuitions: the learned policy is more likely to use higher stringency levels
as susceptible and infected numbers increase. Analogously, they are more likely to use lower
stringency levels as recovered and vaccinated numbers increase. In addition, the biases b show
how stringency level preferences vary across US state.
Importantly, explainable policy model classes facilitate iterating policy design in practice
by identifying potentially undesirable input or policy features. For example, we can see that W
Figure 7: Learned weights of the US state-level policies. Left: weights Wkj for each input
feature (columns indexed by k) and stringency level (rows indexed by j) show how learned US
state policies respond to pandemic conditions. For instance, as infections increase, stringency
levels increase because Wkj > 0 for higher stringency levels and Wkj < 0 for lower stringency
levels. Analogously, as recoveries increase, stringency levels decrease. Right: US state-speciﬁc
biases bij show how US states (columns indexed by i) have varying stringency level preferences.
encodes a strong shift towards lower stringency as the time index grows higher (“time elapsed” in
the Figure). However, timing indicators may be less semantically relevant than epidemiological
or economic metrics. Hence, a practitioner may decide to regularize AI policies to rely less or
not at all on the time index or other input features.
Sensitivity Analysis
A natural concern with using simulation-trained AI policies to recommend real-world policy is
that AI policies may overﬁt to the simulated environment or fail to be robust to simulation-toreality gaps. A sensitivity analysis can test the robustness of AI policies under perturbations of
the simulation parameters. Such perturbations are a proxy for model ﬁtting errors, noisy data,
and other issues that cause model miscalibration and, potentially, model misspeciﬁcation.
To illustrate this, we analyze how social welfare changes under perturbations of the unemployment and COVID transmission-rate model parameters. Figure 8 shows the federal-level
social welfare Fp over a grid of possible perturbations. As expected, social welfare decreases for
stronger unemployment rate and weaker transmission rate responses to changes in stringency
level. This is true both for real-world policies (Fig. 8, left) and AI policies (Fig. 8, middle),
but, for the latter, social welfare is more robust to perturbations. Gains from AI policies remain
Analyzing the impact of potential simulation errors. Left: federal-level social
welfare Fp under real-world policies, across a range of model perturbations affecting how
unemployment (rows) and COVID-19 transmission rates (columns) respond to stringency policy.
Perturbations modulate the scale of the response; for example, with unemployment modulation
> 1, the same increase in stringency yields a larger increase in unemployment (see Methods
Section 6.9 for details). Middle: same, but under AI policies. Right: Fp under AI policies, as a
percentage of Fp under real-world policies. Social welfare improvements persist across a large
range of perturbations.
positive under most perturbation, except when unemployment and transmission-rate responses
are strongly modulated (Fig. 8, right). The stringency and subsidy choices of AI policies differ
under each perturbation setting because these perturbations affect the inputs to the policy networks. This analysis shows that the learned AI policy continues to yield well-performing policy
recommendations across a space of simulation perturbations.
Discussion
Our case study demonstrated that the AI Economist framework has strong potential for real-world
policy design. We discuss some salient aspects and extensions of our framework.
Data Availability Constrains Simulation Model.
All modeling choices are informed by the
availability of data or existing domain knowledge (or lack thereof). For example, we summarize
pandemic response policy using a stringency level, as ﬁne-grained details on the efﬁcacy of
individual policy levers is lacking. Hence, AI policy design can be improved with more and
higher-quality data which would enable more ﬁne-grained design choices, e.g., enable including
more health and economic variables.
Deﬁnition of Social Welfare.
We deﬁned social welfare as to capture the basic trade-off
between public health and the economy. In general, social welfare can include any number
of priorities which are outside the scope of this work. One could add, e.g., keeping ICU
beds available, preventing businesses from failing, or minimizing inequality. Applying the AI
Economist in the real world would require robust consideration of how social welfare is deﬁned
and input from a sufﬁcient set of representative stakeholders.
Retrospective Analysis uses Future Data to Emulate Domain Knowledge.
We apply AI
policies from the start of the COVID-19 pandemic, i.e., March 2020 onward, but we train AI
policies in a simulator calibrated on data from all of 2020 that was gathered under the real-world
policy. Hence, our analysis uses data that would not have been available in the real world,
if hypothetically performed in March 2020. Our use of “future data” emulates the use of 1)
scientiﬁc estimates of unknowns, 2) domain knowledge, and 3) previous experience. Together,
such knowledge could provide forecasts of the pandemic and simulation parameter estimates,
which in turn enable trainin AI policies.
Coarse Level of Data Aggregation.
Policies may affect some social groups differently than
others, but differential data on policy outcomes are not readily available for the COVID-19
pandemic. Hence, it is hard to accurately simulate the effects of real-world diversity with the
current data. Our simulation models a version of the US where unemployment, the pandemic,
and policy impact all people in the same way. Representing diversity is a necessary step if this
technology is eventually used in the real world. However, this requires more robust ﬁne-grained
data than is readily available.
Independency Assumptions across States.
Our simulation uses an independent SIR model
for each region and does not model interactions and cross-over effects between US states, i.e.,
that COVID-19 cases don’t spread across US state lines. This simpliﬁes calibration, while
already demonstrating a good ﬁt with real-world data.
Policy Impact Factors are Assumed Static.
In our simulation, setting a particular stringency
level will always lead to the same transmission rate or unemployment level. In reality, the effect
of policies may depend on past policy choices and the duration of the pandemic in complex ways.
For instance, it may be possible that after a long period of highly stringent policy and subsequent
more relaxed policy, a second period of renewed stringent policy may not be as effective as
public adherence may decrease due to fatigue. The lack of ﬁne-grained data makes it difﬁcult to
model such subtleties. Adding such features could improve the realism of our simulation.
Structural Estimation, Correlation, Causation, and Generalization.
In our model, the
unemployment model and infection rates depend on the stringency level of US state policy only.
This is an intuitive high-level modeling choice which yields strong out-of-sample performance
given historical data. However, there is no (counterfactual) data to ensure this is not a spurious
correlation, nor that other causal factors may yield models that generalize better. In particular,
our model extrapolates the behavior of unemployment and transmission rates (as a function of
policy measures) to pandemic situation that are unseen in the real world. As such, real-world
policy design entails continuous re-calibration and structural estimation.
Modeling Human Behavior using Machine Learning.
We assume that all actors behave
rationally, i.e., optimize policy for a given deﬁnition of social welfare. However, in the real
world actors may not behave optimally or their behavior may not be well explained by standard
objective functions. For example, many human cognitive biases are known, such as recency bias,
ownership bias, behavioral inattention (55), and are well studied in behavioral economics (56).
Hence, rational RL agents may not be sufﬁciently representative of the real world. Moreover,
multi-agent learning algorithms may not always represent how real-world actors learn and adapt
in the presence of others, or make unrealistic assumptions about how much actors know about
the behavior of other actors. As such, it would be fruitful to explore extensions of our framework
that include human-like learning and behaviors. However, it is an open challenge to collect
sufﬁcient micro-level data for these purposes.
This work should be regarded as a proof of concept. There are many aspects of the real world that
our simulation does not capture. We do not endorse using the AI policies learned in this simulation
for actual policymaking. For an extended ethics review, see 
ai-economist-data-driven-interpretable-robust-policy.