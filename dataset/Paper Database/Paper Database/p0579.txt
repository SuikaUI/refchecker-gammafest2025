Unsupervised Cross-lingual Word Embedding
by Multilingual Neural Language Models
Takashi Wada
Nara Institute of Science and Technology,
Nara, Japan
 
Tomoharu Iwata
NTT Communication Science Laboratories,
Kyoto, Japan
 
We propose an unsupervised method to obtain cross-lingual
embeddings without any parallel data or pre-trained word
embeddings. The proposed model, which we call multilingual neural language models, takes sentences of multiple
languages as an input. The proposed model contains bidirectional LSTMs that perform as forward and backward language models, and these networks are shared among all the
languages. The other parameters, i.e. word embeddings and
linear transformation between hidden states and outputs, are
speciﬁc to each language. The shared LSTMs can capture
the common sentence structure among all languages. Accordingly, word embeddings of each language are mapped into
a common latent space, making it possible to measure the
similarity of words across multiple languages. We evaluate
the quality of the cross-lingual word embeddings on a word
alignment task. Our experiments demonstrate that our model
can obtain cross-lingual embeddings of much higher quality
than existing unsupervised models when only a small amount
of monolingual data (i.e. 50k sentences) are available, or the
domains of monolingual data are different across languages.
Introduction
Cross-lingual word representation learning has been recognized as a very important research topic in natural language
processing (NLP). It aims to represent multilingual word
embeddings in a common space, and has been applied to
many multilingual tasks, such as machine translation and bilingual named entity recognition . It also enables
the transfer of knowledge from one language into another
 .
A number of methods have been proposed that obtain
multilingual word embeddings. The key idea is to learn a linear transformation that maps word embedding spaces of different languages. Most of them utilize parallel data such as
parallel corpus and bilingual dictionaries to learn a mapping
 . However, such data are not readily available for many language pairs, especially for lowresource languages.
To tackle this problem, a few unsupervised methods have
been proposed that obtain cross-lingual word embeddings
without any parallel data .
Their methods have opened up the possibility of performing unsupervised neural machine translation . Conneau et al.
 , Zhang et al. propose a model based on adversarial training, and similarly Zhang et al. propose
a model that employs Wasserstein GAN . Surprisingly, these models have outperformed some supervised methods in their experiments.
Recently, however, Søgaard, Ruder, and Vuli´c have
pointed out that the model of Conneau et al. is effective only when the domain of monolingual corpora is
the same across languages and languages to align are linguistically similar. Artetxe, Labaka, and Agirre , on
the other hand, have overcome this problem and proposed a
more robust method that enables to align word embeddings
of distant language pairs such as Finnish and English. However, all of these approaches still have a common signiﬁcant bottleneck: they require a large amount of monolingual
corpora to obtain cross-lingual word embedddings, and such
data are not readily available among minor languages.
In this work, we propose a new unsupervised method that
can obtain cross-lingual embeddings even in a low-resource
setting. We deﬁne our method as multilingual neural language model, that obtains cross-lingual embeddings by capturing a common structure among multiple languages. More
speciﬁcally, our model employs bidirectional LSTM networks that respectively perform as forward and backward language models , and these parameters are shared among multiple languages. The shared
LSTM networks learn a common structure of multiple languages, and the shared network encodes words of different languages into a common space. Our model is signiﬁcantly different from the existing unsupervised methods in
that while they aim to align two pre-trained word embedding spaces, ours jointly learns multilingual word embeddings without any pre-training. Our experiments show that
our model is more stable than the existing methods under a
low-resource condition, where it is difﬁcult to obtain ﬁnegrained monolingual word embeddings.
 
Shared: -, -, EBOS, *EOS
Specific to ℓ: Eℓ, * ℓ
Figure 1: Illustration of our proposed multilingual neural language model. The parameters shared among across
multiple languages are the ones of forward and backward
LSTMs −→f and ←−f , the embedding of <BOS>, EBOS, and
the linear projection for <EOS>, W EOS. On the other hand,
word embeddings, Eℓ, and linear projection W ℓare speciﬁc
to each language ℓ. The shared LSTMs capture a common
structure of multiple languages, and that enables us to map
word embeddings Eℓof multiple languages into a common
We propose a model called multi-lingual neural language
model, which produces cross-lingual word embeddings in an
unsupervised way. Figure 1 brieﬂy illustrates our proposed
model. The model consists of the shared parameters among
multiple languages and the speciﬁc ones to each language.
In what follows, we ﬁrst summerize which parameters are
shared or separate across languages:
• Shared Parameters
f : LSTM networks which perform as forward
and backward language models, independently.
– EBOS: The embedding of <BOS>, an initial input to
the language models.
– W EOS: The linear mapping for <EOS>, which calculates how likely it is that the next word is the end of a
• Separate Parameters
– Eℓ: Word embeddings of language ℓ
– W ℓ: Linear projections of language ℓ, which is used to
calculate the probability distribution of the next word.
The LSTMs −→f and ←−f are shared among multiple languages and capture a common language structure. On the
other hand, the word embedding function Eℓand liner projection W ℓare speciﬁc to each language ℓ. Since different
languages are encoded by the same LSTM functions, similar words across different languages should have a similar
representation so that the shared LSTMs can encode them
effectively. For instance, suppose our model encodes an English sentence “He drives a car.” and its Spanish translation
“El conduce un coche.” In these sentences, each English
word corresponds to each Spanish one in the same order.
Therefore, these equivalent words would have similar representations so that the shared language models can encode
the English and Spanish sentences effectively. Although in
general each language has its different grammar rule, the
shared language models are trained to roughly capture the
common structure such as a common basic word order rule
(e.g. subject-verb-object) among different languages. Sharing <BOS> and <EOS> symbols further helps to obtain
cross-lingual representations, ensuring that the beginning
and end of the hidden states are in the same space regardless of language. In particular, sharing <EOS> symbol indicates that the same linear function predicts how likely it is
that the next word is the end of a sentence. In order for the
forward and backward language models to predict the end
of a sentence with high probability, the words that appear
near the end or beginning of a sentence such as punctuation
marks and conjunctions should have very close representations among different languages.
Network Structure
2, ..., wℓ
N⟩. The forward language model calculates
the probability of upcoming word wℓ
t given the previous
2, ..., wℓ
2, ..., wℓ
2, ..., wℓ
The backward language model is computed similarly given
the backward context:
2, ..., wℓ
t+2, ..., wℓ
The tth hidden states hℓ
t of the forward and backward
LSTMs are calculated based on the previous hidden state
and word embedding,
t = −→f (−→h ℓ
t = ←−f (←−h ℓ
if t = 0 or N+1,
otherwise,
where −→f (·) and ←−f (·) are the standard LSTM functions.
EBOS is the embedding of <BOS>, which is shared among
all the languages. Note that the same word embedding function Eℓis used among the forward and backward language
models. The probability distribution of the upcoming word
t is calculated by the forward and backward models independently based on their current hidden state:
2, ..., wℓ
t−1) = softmax(gℓ(−→h ℓ
t+2, ..., wℓ
N). = softmax(gℓ(←−h ℓ
t) = [W EOS(hℓ
t), W ℓ(hℓ
where [x, y] means the concatenation of x and y. W EOS is
a matrix with the size of (1 × d), where d is the dimension
of the hidden state. This matrix is a mapping function for
<EOS>, and shared among all of the languages. W ℓis a
matrix with the size of (V ℓ× d), where V ℓis the vocabulary
size of language ℓexcluding <EOS>. Therefore, g is a linear transformation with the size of ((V ℓ+ 1) × d). As with
the word embeddings, the same mapping functions are used
among the forward and backward language models.
The largest difference between our model and a standard
language model is that our model shares LSTM networks
among different languages, and the shared LSTMs capture a common structure of multiple languages. Our model
also shares <BOS> and <EOS> among languages, which
encourages word embeddings of multiple languages to be
mapped into a common space.
The proposed model is trained by maximizing the log
likelihood of the forward and backward directions for each
language ℓ:
i,2, ...wℓ
i,t−1; −→θ )
+ log p(wℓ
i,t+2, ...wℓ
i,N i; ←−θ ),
where L and Sℓdenote the number of languages and sentences of language ℓ. −→θ and ←−θ denote the parameters for
the forward and backward LSTMs −→f and ←−f , respectively.
Related Work
Unsupervised Word Mapping
A few unsupervised methods have been proposed that obtain cross-lingual representations in an unsupervised way.
Their goal is to ﬁnd a linear transformation that aligns pretrained word embeddings of multiple languages. For instance, Artetxe, Labaka, and Agirre obtain a linear mapping using a parallel vocabulary of automatically
aligned digits (i.e. 1-1, 2-2, 15-15...). In fact, their method
is weakly supervised because they rely on the aligned information of Arabic numerals across languages. Zhang et
al. and Conneau et al. , on the other hand,
propose fully unsupervised methods that do not make use
of any parallel data. Their methods are based on adversarial training : during the training, a
discriminator is trained to distinguish between the mapped
source and the target embeddings, while the mapping matrix
is trained to fool the discriminator. Conneau et al. further reﬁne the mapping obtained by the adversarial training.
They build a synthetic parallel vocabulary using the mapping, and apply a supervised method given the pseudo parallel data. Zhang et al. employ Wasserstein GAN
and obtain cross-lingual representations by minimizing the
earth-mover’s distance. Artetxe, Labaka, and Agirre 
propose an unsupervised method using a signiﬁcantly different approach from them. It ﬁrst roughly aligns words
across language using structural similarity of word embedding spaces, and reﬁnes the word alignment by repeating
a robust self-learning method until convergence. They have
found that their approach is much more effective than Zhang
et al. and Conneau et al. on realistic scenarios, namely when languages to align are linguistically distant
or training data are non-comparable across language.
The common objective among all these unsupervised
methods is to map word embeddings of multiple languages
into a common space. In their experiments. the word embeddings are pre-trained on a large amount of monolingual data
such as Wikipedia before their methods are applied. Therefore, they haven’t evaluated their method on the condition
when only a small amount of data are available. That condition is very realistic for minor languages, and an unsupervised method can be very useful for these languages. In our
experiments, it turns out that existing approaches do not perform well without enough data, while our proposed method
can align words with as small data as ﬁfty thousand sentences for each language.
Siamese Neural Network
Our model embeds words of multiple languages into a common space by sharing LSTM parameters among the languages. In general, the model architecture of sharing parameters among different domains is called the “Siamese Neural Network” . It is known to be very
effective at representing data of different domains in a common space, and this technique has been employed in many
NLP tasks. For example, Johnson et al. built a neural
machine translation model whose encoder and decoder parameters are shared among multiple languages. They have
observed that sentences of multiple languages are mapped
into a common space, and that has made it possible to perform zero-shot translation. Rudramurthy, Khapra, and Bhattacharyya share LSTM networks of their named entity recognition model across multiple languages, and improve the performance in resource-poor languages. Note that
these models are fully supervised and require parallel data to
obtain cross-lingual representations. Our model, on the other
hand, does not require any parallel or cross-lingual data, and
it acquires cross-lingual word embeddings through ﬁnding a
common language structure in an unsupervised way.
Experiments
We considered two learning scenarios that we deem realistic
for low-resource languages:
1. Only a small amount of monolingual data are available.
2. The domains of monolingual corpora are different across
languages.
For the ﬁrst case, we used the News Crawl 2012 monolingual corpus for every language except for Finnish, for
which we used News Crawl 2014. These data are provided
by WMT20131 and 20172. We randomly extracted 50k sentences in each language, and used them as training data.
1 
translation-task.html
2 
translation-task.html
We also extracted 100k, 150k, 200k, and 250k sentences
and analyzed the impact of the data size. For the second
scenario, we used the Europarl corpus as an
English monolingual corpus, and the News Crawl corpus
for the other languages. We randomly extracted one million sentences from each corpus and used them as training
data. The full vocabulary sizes of the Europarl and News
Crawl corpora in English were 79258 and 265368 respectively, indicating the large difference of the domains. We
did not use any validation data during the training. We tokenized and lowercased these corpora using Moses toolkit3.
We evaluated models in the pairs of {French, German, Spanish, Finnish, Russian, Czech}-English.
Evaluation
In this work, we evaluate our methods on a word alignment task. Given a list of M words in a source language
s [x1, x2, ..., xM] and target language t [y1, y2, ..., yM], the
word alignment task is to ﬁnd one-to-one correspondence
between these words. If a model generates accurate crosslingual word embeddings, it is possible to align words properly by measuring the similarity of the embeddings. In our
experiment, we used the bilingual dictionary data published
by Conneau et al. , and extracted 1,000 unique pairs of
words that are included in the vocabulary of the News Crawl
data of from 50k to 300k sentences. As a measurement of
the word embeddings, we used cross-domain similarity local
scaling (CSLS), which is also used in Conneau et al. 
and Artetxe, Labaka, and Agirre . CSLS can mitigate
the hubness problem in high-dimensional spaces, and can
generally improve matching accuracy. It takes into account
the mean similarity of a source language embedding x to its
K nearest neighbors in a target language:
cos(x, y),
where cos is the cosine similarity and NT (x) denotes the K
closest target embeddings to x. Following their suggestion,
we set K as 10. rR(y) is deﬁned in a similar way for any
target language embedding y. CSLS(x, y) is then calculated
as follows:
CSLS(x, y) = 2cos(x, y) −rT(x) −rS(y).
For each source word xi, we extracted the k target words
that have the highest CSLS scores (k = 1 or 5). However,
since the value of rT(x) does not affect the result of this
evaluation, we omit the score from CSLS in our experiments. We report the precision p@k: how often the correct
translation of a source word xi is included in the k extracted
target words.
As baselines, we compared our model to that of Conneau
et al. and Artetxe, Labaka, and Agirre . Conneau et al. aim to ﬁnd a mapping matrix W based on
3 
mosesDecoder
adversarial training. The discriminator is trained to distinguish the domains (i.e. language) of the embeddings, while
the mapping is trained to fool the discriminator. Then, W is
used to match frequent source and target words, and induce
a bilingual dictionary. Given the pseudo dictionary, a new
mapping matrix W is then trained in the same manner as a
supervised method, which solves the Orthogonal Procrustes
W ∗= arg min
∥WX −Y ∥F = UV T,
V T = SVD(Y XT).
This training can be iterated using the new matrix W to induce a new bilingual dictionary. This method assumes that
the frequent words can serve as reliable anchors to learn a
mapping. Since they suggest normalizing word embeddings
in some language pairs, we evaluated their method with and
without normalization. Artetxe, Labaka, and Agirre 
use a different approach and employ a robust self-learning
method. First, they roughly align words based on the similarity of word emebeddings. Then, they repeat the self-learning
approach, where they alternatively update a mapping function and word alignment.
To implement the baseline methods, we used the code
published by the authors4,5. To obtain monolingual word
embeddings, we used word2vec .
Note that these embeddings were used only for the baselines, but not for ours since our method does not require any
pre-trained embeddings. For a fair comparison, we used the
same monolingual corpus with the same vocabulary size for
the baselines and our model.
Training Settings
We preprocessed monolingual data and generated minibatches for each language. For each iteration, our model alternately read mini-batches of each language, and updated
its parameters every time it read one mini-batch. We trained
our model for 10 epochs with the mini-batch size 64. The
size of word embedding was set as 300, and the size of
LSTM hidden states was also set as 300 for the forward
and backward LSTMs, respectively. Dropout is applied to the hidden state with its rate 0.3. We
used SGD as an optimizer with the learning
rate 1.0. Our parameters, which include word embeddings,
were uniformly initialized in [-0.1, 0.1], and gradient clipping was used with
the clipping value 5.0. We included in the vocabulary the
words that were used at least a certain number of times. For
the News Crawl corpus, we set the threshold as 3, 5, 5, 5
,5, 10, and 20 for 50k, 100k, 150k, 200k, 250k, 300k and
1m sentences. For the Europarl corpus, we set the value as
10. We fed 10000 frequent words into the discriminator in
Conneau et al. .
As a model selection criterion, we employed a similar
strategy used in the baseline. More speciﬁcally, we consid-
4 
5 
Conneau et al. 
Conneau et al. + normalize
Artetxe, Labaka, and Agirre 
Table 1: Word alignment average precisions p@1 and 5 when models are trained on 50k sentences of source and target languages.
Conneau et al. 
Conneau et al. + normalize
Artetxe, Labaka, and Agirre 
Table 2: Word alignment average precisions p@1 and 5 when models are trained on one million sentences extracted from
different domains between source and target languages.
source word (es)
Artetxe, Labaka, and Agirre 
appointment
approximately
actualmente
essentially
candidates
constitutes
commissioned
Table 3: Some examples when Spanish and English words matched correctly by our model using 50k sentences, but not by
Artetxe, Labaka, and Agirre . Each column indicates 1st, 2nd, and 3rd most similar English words to each Spanish word.
English words in bold font are translations of each Spanish word.
ered the 3,000 most frequent source words, and used CSLS
excluding rT(x) to generate a translation for each of them
in a target language. We then computed the average CSLS
scores between these deemed translations, and used them as
a validation metric.
Bilingual Word Embeddings
First, we trained our model and obtained cross-lingual embeddings between two languages for each language pair. We
report our results under the two scenarios that we considerted realistic when dealing with minor languages. In the
ﬁrst scenario, we trained our model on a very small amount
of data, and in the second scenario the model was trained
on a large amount of data extracted from different domains
between source and target languages.
Table 1 illustrates the results of the word alignment task
under the low-resource scenario. RANDOM is the expected
accuracy when words are aligned at random. The result
shows that our model outperformed the baseline methods
signiﬁcantly in all of the language pairs, indicating that ours
is more robust in a low-resource senario. On the other hand,
the baseline methods got poor performance, especially in the
Finnish and English pair. Even though Artetxe, Labaka, and
Agirre report that their method achieves good performance in that language pair, our experiment has demonstrated that it does not perform well without a large amount
Table 2 shows the results when the domains of training
data used to obtain source and target embeddings are different. Our method again outperformed the baselines to a large
extent except for the Spanish-English pair. The poor performance of Conneau et al. in such a setting has also
been observed in Søgaard, Ruder, and Vuli´c , even
though much larger data including Wikipedia were used for
training in their experiments.
Table 3 shows some examples when Spanish and English words were correctly matched by our model, but
not by Artetxe, Labaka, and Agirre under the lowresource scenario. The table lists the three most similar
English words to each Spanish source word. Our method
Figure 2: Comparison of p@1 accuracy of German-English
pair between supervised word mapping method and our
model on 50k sentences. The x axis indicates the number
of pairs of words n (= 0,50,100,150,..., 450, 500) that were
used for the supervised method, but not for ours, to map
word embedding spaces in two languages.
successfully matched similar or semantically related words
to the source words, indicating that our method obtained
good cross-lingual embeddings. For example, to the Spanish source word “casi”, our model aligned its translation
“almost” and also very similar words “approximately” and
“about”. Indeed, many of the aligned target words in our
model have the same part of speech tag as that of the source
word, suggesting that our model captured a common language structure such as rules of word order and roles of
vocabulary by sharing LSTMs. On the other hand, Artetxe,
Labaka, and Agirre could not align words properly,
and there do not seem to exist consistent relations between
the source and extracted words.
Comparison to Supervised Method
To further investigate the effectiveness of our model, we compared our
method to a supervised method under the low-resource setting. The method is a slight modiﬁcation of Conneau et
al. : it is trained using a bilingual dictionary, and
learns a mapping from the source to the target space using iterative Procrustes alignment. We used the code provided by Conneau et al. . Figure 2 compares p@1
accuracy between the supervised method and our model in
the German-English pair. The x-axis denotes the number of
seeds of the bilingual dictionary that were used for the supervised method, but not for ours. The ﬁgure illustrates that our
method achieved a better result than the supervised method
when the number of seeds was less than 400, which is surprising given that our model is fully unsupervised.
Impact of Data Size
We changed the size of the training
data by 50k sentences, and analyzed how the performance
of the baselines and our model changed. Figure 3 illustrates
how the performance changed depending on the data size.
It shows that our method achieved a comparable or better result than the baseline methods in all of the language
pairs when the number of sentences was not more than
Figure 3: Graphs show the change in p@1 accuracy of
each language pair as the size of training data increases.
The x-axis denotes the number of sentences (thousand) in
the monolingual training data of the source and target languages.
100k. In the closely related language pairs such as {French,
German, Spanish}-English, the baselines performed better
when there were enough amount of data. Among the distant languages such as {Finnish, Czech}-English, our model
achieved better results overall, while the baseline methods,
especially Conneau et al. got very poor results.
Quadrilingual Word Embeddings
Our results of the word alignment task have shown that our
model can jointly learn bilingual word embeddings by capturing the common structure of two languages. This success has raised another question: “Is it also possible to learn
a common structure of more than two languages?” To examine this intriguing question, we trained our model that
OURS (BILINGUAL)
OURS (QUADRILINGUAL)
Table 4: Word alignment average precisions p@1 in each language pair when 50k, 100k, and 300k sentences were used for
training. OURS (BILINGUAL) denotes the accuracy of the models that read source and target languages, generating bilingual
word embeddings. OURS (QUADRILINGUAL) denotes the accuracy of one model that reads all four languages, producing
quadrilingual word embeddings.
En (source)
Table 5: Examples of words that were correctly aligned by
OURS (QUADRILINGUAL) among the four languages.
encoded four linguistically similar languages, namely English, French, Spanish, and German, and aimed to capture
the common structure among them. We expect that word
embeddings of the four languages should be mapped into a
common space, generating what we call quadrilingual word
embeddings. Table 4 describes the result of the word alignment when using bilingual and quadrilingual word embeddings of our model. While quadrilingual word embeddings
performed slightly worse than bilingual ones in the Spanish-
English pair, they brought large gains in the German-English
alignment task and achieved comparable performance overall. Our model successfully mapped word embeddings of
the four languages into a common space, making it possible to measure the similarity of words across the multiple
languages.
To investigate whether quadrilingual embeddings were
actually mapped into a common space, we aligned each English word to French, German and Spanish words in the
bilingual dictionary. Table 5 describes the words that were
correctly aligned among the four languages. This result indicates that these equivalent words have very similar representations, and that means our model successfully embedded
these languages into a common space. Figure 4 illustrates
the scatter plot of the embeddings of the most 1,000 frequent words in each corpus of the four languages. It clearly
shows that the word embeddings were clustered based on
their meanings rather than their language. For example, the
prepositions of the four languages (e.g. de (fr, es), of (en),
von (de)) were mapped into the bottom-right area, and determiners (e.g. la (fr, es), the (en), der, die, das (de)) were in
the bottom-left area. Near the area where the embedding of
‘<BOS>’ was mapped, the words from which a new sentence often starts (‘,’ , et (fr), y(es), und (de), and (en)) were
<unk> (en, de. es, fr)
de,del,a(es), de,des(fr), von(de), of(en)
der(de) la (es) the (en) la(fr) die, das(de),
. (en, de, es, fr) ? (en, de, es, fr)
, (en,de,es, fr)
a (es) à (fr) to(en)
que (es), que, qui(fr)
ﬁrst (en) last(en)
Figure 4: Scatter plot of cross-lingual word embeddings
of French, English, German and Spanish obtained by our
model. The embeddings are reduced to 2D using tSNE
 .
Conclusion
In this paper, we proposed a new unsupervised method that
learns cross-lingual embeddings without any parallel data.
Our experiments of a word alignment task in six language
pairs have demonstrated that our model signiﬁcantly outperforms existing unsupervised word translation models in all
the language pairs under a low resource situation. Our model
also achieved better results in ﬁve language pairs when the
domains of monolingual data are different across language.
We also compared our unsupervised method to a supervised
one in the German-English word alignment task, and our
model achieved a better result than the supervised method
that were trained with 350 pairs of words from a bilingual
dictionary. Our model also succeeded in obtaining crosslingual embeddings across four languages, which we call
quadrilingual embeddings. These embeddings enabled us to
align equivalent words among four languages in an unsupervised way. The visualization of the quadrilingual embeddings showed that these embeddings were actually mapped
into a common space, and words with similar meanings had
close representations across different languages.
Potential future work includes extending our approach to
a semi-supervised method that utilizes a bilingual dictionary.
One possible idea is to set an additional loss function in our
model that decreases the distance of embeddings of equivalent words across languages.