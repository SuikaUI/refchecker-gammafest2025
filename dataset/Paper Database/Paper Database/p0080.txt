Patch-based Convolutional Neural Network for Whole Slide
Tissue Image Classification
Le Hou1, Dimitris Samaras1, Tahsin M. Kurc2,4, Yi Gao1,2,3, James E. Davis5, and Joel H.
Saltz1,2,5,6
Le Hou: ; Dimitris Samaras: ; Tahsin M. Kurc:
 ; Yi Gao: ; James E. Davis:
 ; Joel H. Saltz: 
1Dept. of Computer Science, Stony Brook University
2Dept. of Biomedical Informatics, Stony Brook University
3Dept. of Applied Mathematics and Statistics, Stony Brook University
4Oak Ridge National Laboratory
5Dept. of Pathology, Stony Brook Hospital
6Cancer Center, Stony Brook Hospital
Convolutional Neural Networks (CNN) are state-of-the-art models for many image classification
tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel
resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The
differentiation of cancer subtypes is based on cellular-level visual features observed on image
patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image
patches will perform better than or similar to an image-level classifier. The challenge becomes
how to intelligently combine patch-level classification results and model the fact that not all
patches will be discriminative. We propose to train a decision fusion model to aggregate patchlevel predictions given by patch-level CNNs, which to the best of our knowledge has not been
shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method
that automatically locates discriminative patches robustly by utilizing the spatial relationships of
patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma
cases into subtypes. The classification accuracy of our method is similar to the inter-observer
agreement between pathologists. Although it is impossible to train CNNs on WSIs, we
experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patchbased CNN can outperform an image-based CNN.
1. Introduction
Convolutional Neural Networks (CNNs) are currently the state-of-the-art image classifiers
 . However, due to high computational cost, CNNs cannot be applied to very
high resolution images, such as gigapixel Whole Slide Tissue Images (WSI). Classification
of cancer WSIs into grades and subtypes is critical to the study of disease onset and
progression and the development of targeted therapies, because the effects of cancer can be
HHS Public Access
Author manuscript
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in
PMC 2016 October 28.
 
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. 2016 ; 2016: 2424–2433. doi:10.1109/CVPR.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
observed in WSIs at the cellular and sub-cellular levels (Fig. 1). Applying CNN directly for
WSI classification has several drawbacks. First, extensive image downsampling is required
by which most of the discriminative details could be lost. Second, it is possible that a CNN
might only learn from one of the multiple discriminative patterns in an image, resulting in
data inefficiency. Discriminative information is encoded in high resolution image patches.
Therefore, one solution is to train a CNN on high resolution image patches and predict the
label of a WSI based on patch-level predictions.
The ground truth labels of individual patches are unknown, as only the image-level ground
truth label is given. This complicates the classification problem. Because tumors may have a
mixture of structures and texture properties, patch-level labels are not necessarily consistent
with the image-level label. More importantly, when aggregating patch-level labels to an
image-level label, simple decision fusion methods such as voting and max-pooling are not
robust and do not match the decision process followed by pathologists. For example, a
mixed subtype of cancer such as oligoastrocytoma, might have distinct regions of other
cancer subtypes. Therefore, neither voting nor max-pooling could predict the correct WSIlevel label since the patch-level predictions do not match the WSI-level label.
We propose using a patch-level CNN and training a decision fusion model as a two-level
model, shown in Fig. 2. The first-level (patch-level) model is an Expectation Maximization
(EM) based method combined with CNN that outputs patch-level predictions. In particular,
we assume that there is a hidden variable associated with each patch extracted from an
image that indicates whether the patch is discriminative (i.e. the true hidden label of the
patch is the same as the true label of the image). Initially, we consider all patches to be
discriminative. We train a CNN model that outputs the cancer type probability of each input
patch. We apply spatial smoothing to the resulting probability map and select only patches
with higher probability values as discriminative patches. We iterate this process using the
new set of discriminative patches in an EM fashion. In the second-level (image-level),
histograms of patch-level predictions are input into an image-level multiclass logistic
regression or Support Vector Machine (SVM) model that predicts the image-level
Pathology image classification and segmentation is an active research field. Most WSI
classification methods focus on classifying or extracting features on patches . In a pretrained CNN model extracts features on patches which are
then aggregated for WSI classification. As we show here, the heterogeneity of some cancer
subtypes cannot be captured by those generic CNN features. Patch-level supervised
classifiers can learn the heterogeneity of cancer subtypes, if a lot of patch labels are provided
 . However, acquiring such labels in large scale is prohibitive, due to the need for
specialized annotators. As digitization of tissue samples becomes commonplace, one can
envision large scale datasets, that could not be annotated at patch scale. Utilizing unlabeled
patches has led to Multiple Instance Learning (MIL) based WSI classification .
In the MIL paradigm , unlabeled instances belong to labeled bags of instances.
The goal is to predict the label of a new bag and/or the label of each instance. The Standard
Hou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2016 October 28.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Multi-Instance (SMI) assumption states that for a binary classification problem, a bag is
positive iff there exists at least one positive instance in the bag. The probability of a bag
being positive equals to the maximum positive prediction over all of its instances .
Combining MIL with Neural Networks (NN) , the SMI assumption is
modeled by max-pooling. Following this formulation, the Back Propagation for Multi-
Instance Problems (BP-MIP) performs back propagation along the instance with the
maximum response if the bag is positive. This is inefficient because only one instance per
bag is trained in one training iteration on the whole bag.
MIL-based CNNs have been applied to object recognition and semantic segmentation
 in image analysis – the image is the bag and image-windows are the instances .
These methods also follow the SMI assumption. The training error is only propagated
through the object-containing window which is also assumed to be the window that has the
maximum prediction confidence. This is not robust because one significantly misclassified
window might be considered as the object-containing window. Additionally, in WSIs, there
might be multiple windows that contain discriminative information. Hence, recent semantic
image segmentation approaches smooth the output probability (feature) maps of
To predict the image-level label, max-pooling (SMI) and voting (average-pooling) were
applied in . However, it has been shown that in many applications, learning
decision fusion models can significantly improve performance compared to voting . Furthermore, such a learned decision fusion model is based on the Countbased Multiple Instance (CMI) assumption which is the most general MIL assumption .
Our main contributions in this paper are: (1) To the best of our knowledge, we are the first to
combine patch-level CNNs with supervised decision fusion. Aggregating patch-level CNN
predictions for WSI classification significantly outperforms patch-level CNNs with maxpooling or voting. (2) We propose a new EM-based model that identifies discriminative
patches in high resolution images automatically for patch-level CNN training, utilizing the
spatial relationship between patches. (3) Our model achieves multiple state-of-the-art results
classifying WSIs to cancer subtypes on the TCGA dataset. Our results are similar or close to
inter-observer agreement between pathologists. Larger classification improvements are
observed in the harder-to-classify cases. (4) We provide experimental evidence that
combining multiple patch-level classifiers might actually be advantageous compared to
whole image classification.
The rest of this paper is organized as follows. Sec. 2 describes the framework of the EMbased MIL algorithm. Sec. 3 discusses the identification of discriminative patches. Sec. 4
explains the image-level model that predicts the image-level label by aggregating patch-level
predictions. Sec. 5 shows experimental results. The paper concludes in Sec. 6. App. A lists
the cancer subtypes in our experiments.
Hou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2016 October 28.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
2. EM-based method with CNN
An overview of our EM-based method can be found in Fig. 2. We model a high resolution
image as a bag and patches extracted from it as instances. We have a ground truth label for
the whole image but not for the individual patches. We model whether an instance is
discriminative or not as a hidden binary variable.
We denote X = {X1, X2, …, XN} as the dataset containing N bags. Each bag Xi = {Xi, 1,
Xi, 2, …, Xi,Ni} consists of Ni instances, where Xi,j = 〈xi,j, yi〉 is the j-th instance and its
associated label in the i-th bag. Assuming the bags are independent and identically
distributed (i.i.d.), the X and the hidden variables H are generated by the following
generative model:
where the hidden variable H = {H1, H2, …, HN}, Hi = {Hi, 1, Hi, 2, …, Hi,Ni} and Hi,j is the
hidden variable that indicates whether instance xi,j is discriminative for label yi of bag Xi.
We further assume that all Xi,j depends on Hi,j only and are independent with each other
given Hi,j. Thus
We maximize the data likelihood P(X) using EM.
At the initial E step, we set Hi,j = 1 for all i, j. This means that all instances
are considered discriminative.
M step: We update the model parameter θ to maximize the data likelihood
where D is the discriminative patches set. Assuming a uniform generative
model for all non-discriminative instances, the optimization in Eq. 3
simplifies to:
Hou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2016 October 28.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Additionally we assume an uniform distribution over xi,j. Thus Eq. 4
describes a discriminative model (in this paper we use a CNN).
E step: We estimate the hidden variables H. In particular, Hi,j = 1 if and
only if P(Hi,j | X) is above a certain threshold. In the case of image
classification, given the i-th image, P(Hi,j | X) is obtained by applying
Gaussian smoothing on P(yi | xi,j; θ) (Detailed in Sec 3). This smoothing
step utilizes the spatial relationship of P(yi | xi,j; θ) in the image. We then
iterate back to the M step till convergence.
Many MIL algorithms can be interpreted through this formulation. Based on the SMI
assumption, the instance with the maximum P(Hi,j | X) is the discriminative instance for the
positive bag, as in the EM Diverse Density (EM-DD) and the BP-MIP 
algorithms.
3. Discriminative patch selection
Patches xi,j that have P(Hi,j | X) larger than a threshold Ti,j are considered discriminative and
are selected to continue training the CNN. We present in this section the estimation of P(H |
X) and the choice of the threshold.
It is reasonable to assume that P(Hi,j | X) is correlated with P(yi | xi,j; θ), i.e. patches with
lower P(yi | xi,j; θ) tend to have lower probability xi,j to be discriminative. However, a hardto-classify patch, or a patch close to the decision boundary may have low P(yi | xi,j; θ) as
well. These patches are informative and should not be rejected. Therefore, to obtain a more
robust P(Hi,j | X), we apply the following two steps: First, we train two CNNs on two
different scales in parallel. P(yi | xi,j; θ) is the averaged prediction of the two CNNs. Second,
we simply denoise the probability map P(yi | xi,j; θ) of each image with a Gaussian kernel to
compute P(Hi,j | X). This use of spatial relationships yields more robust discriminative patch
identification as shown in the experiments in Sec. 5.
Choosing a thresholding scheme carefully yields significantly better performance than a
simpler thresholding scheme . We obtain the threshold Ti,j for P(Hi,j | X) as follows: We
note Si as the set of P(Hi,j | X) values for all xi,j of the i-th image and Ec as the set of P(Hi,j |
X) values for all xi,j of the c-th class. We introduce the image-level threshold Hi as the P1-th
percentile of Si and the class-level threshold Ri as the P2-th percentile of Ec, where P1 and P2
are predefined. The threshold Ti,j is defined as the minimum value between Hi and Ri. There
are two advantages of our method. First, by using the image-level threshold, there are at least
1 − P1 percent of patches that are considered discriminative for each image. Second, by
using the class-level threshold, the thresholds can be easily adapted to classes with different
prior probabilities.
4. Image-level decision fusion model
We combine the patch-level classifiers of Sec. 3 to predict the image-level label. We input all
patch-level predictions into a multi-class logistic regression or SVM that outputs the imagelevel label. This decision level fusion method is more robust than max-pooling .
Hou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2016 October 28.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Moreover, this method can be thought of as a Count-based Multiple Instance (CMI) learning
method with two-level learning which is a more general MIL assumption than the
Standard Multiple Instance (SMI) assumption.
There are three reasons for combining multiple instances: First, on difficult datasets, we do
not want to assign an image-level prediction simply based on a single patch-level prediction
(as is the case of the SMI assumption ). Second, even though certain patches are not
discriminative individually, their joint appearance might be discriminative. For example, a
WSI of the “mixed” glioma, Oligoastrocytoma (see App. A) should be recognized when two
single glioma subtypes (Oligodendroglioma and Astrocytoma) are jointly present on the
slide possibly on non-overlapping regions. Third, because the patch-level model is never
perfect and probably biased, an image-level decision fusion model may learn to correct the
bias of patch-level decisions.
Because it is unclear at this time whether strongly discriminative features for cancer
subtypes exist at whole slide scale , we fuse patch-level predictions without the spatial
relationship between patches. In particular, the class histogram of the patch-level predictions
is the input to a linear multi-class logistic regression model or an SVM with Radial Basis
Function (RBF) kernel . Because a WSI contains at least hundreds of patches, the class
histogram is very robust to miss-classified patches. To generate the histogram, we sum up all
of the class probabilities given by the patch-level CNN. Moreover, we concatenate
histograms from four CNNs models: CNNs trained at two patch scales for two different
numbers of iterations. We found in practice that using multiple histograms is robust.
5. Experiments
We evaluate our method on two Whole Slide Tissue Images (WSI) classification problems:
classification of glioma and Non-Small-Cell Lung Carcinoma (NSCLC) cases into glioma
and NSCLC subtypes. Glioma is a type of brain cancer that rises from glial cells. It is the
most common malignant brain tumor and the leading cause of cancer-related deaths in
people under age 20 . NSCLC is the most common lung cancer, which is the leading
cause of cancer-related deaths overall . Classifying glioma and NSCLC into their
respective subtypes and grades is crucial to the study of disease onset and progression in
order to provide targeted therapies. The dataset of WSIs used in the experiments part of the
public Cancer Genome Atlas (TCGA) dataset . It contains detailed clinical information
and the Hematoxylin and Eosin (H&E) stained images of various cancers. The typical
resolution of a WSI in this dataset is 100K by 50K pixels. In the rest of this section, we first
describe the algorithm we tested then show the evaluation results on the glioma and NSCLC
classification tasks.
5.1. Patch extraction and segmentation
To train the CNN model, we extract patches of size 500×500 from WSIs (examples in Fig.
3). To capture structures at multiple scales, we extract patches from 20× (0.5 microns per
pixel) and 5× (2.0 microns per pixel) objective magnifications. We discard patches with less
Hou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2016 October 28.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
than 30% tissue sections or have too much blood. We extract around 1000 valid patches per
image per scale. In most cases the patches are non-overlapping given WSI resolution.
To prevent the CNN from overfitting, we perform three kinds of data augmentation in every
iteration. We select a random 400×400 sub-patch from each 500×500 patch. We randomly
rotate and mirror the sub-patch. We randomly adjust the amount of Hematoxylin and eosin
stained on the tissue. This is done by decomposing the RGB color of the tissue into the H&E
color space , followed by multiplying the magnitude of H and E of every pixel by two
i.i.d. Gaussian random variables with expectation equal to one.
5.2. CNN architecture
The architecture of our CNN is shown in Tab. 1. We used the CAFFE tool box for the
CNN implementation. The network was trained on a single NVidia Tesla K40 GPU.
5.3. Experiment setup
The WSIs of 80% of the patients are randomly selected to train the model and the remaining
20% to test. Depending on method, training patches are further divided into i) CNN and ii)
decision fusion model training sets. We separate the data twice and average the results.
Tested algorithms are:
CNN-Vote: CNN followed by voting (average-pooling). We use all patches
extracted from a WSI to train the patch-level CNN. There is no secondlevel model. Instead, the predictions of all patches vote for the final
predicted label of a WSI.
CNN-SMI: CNN followed by max-pooling. Same as CNN-Vote except the
final predicted label of a WSI equals to the predicted label of the patch
with maximum probability over all other patches and classes.
CNN-Fea-SVM: We apply feature fusion instead of decision level fusion.
In particular, we aggregate the outputs of the second fully connected layer
of the CNN on all patches by 3-norm pooling . Then an SVM with
RBF kernel predicts the image-level label.
EM-CNN-Vote/SMI, EM-CNN-Fea-SVM: EM-based method with CNN-
Vote, CNN-SMI, CNN-Fea-SVM respectively. We train the patch-level
EM-CNN on discriminative patches identified by the E-step. Depending
on the dataset, the discriminative threshold P1 for each image ranges from
0.18 to 0.25; the discriminative threshold P2 for each class ranges from
0.05 to 0.28 (details in Sec. 3). In each M-step, we train the CNN on all
the discriminative patches for 2 epochs.
EM-Finetune-CNN-Vote/SMI: Similar to EM-CNN-Vote/SMI except that
instead of training a CNN from scratch, we fine-tune a pretrained 16-layer
CNN model by training it on discriminative patches.
CNN-LR: CNN followed by logistic regression. Same as CNN-Vote
except that we train a second-level multiclass logistic regression to predict
Hou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2016 October 28.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
the image-level label. One tenth of the patches in each image is held out
from the CNN to train the second-level multi-class logistic regression.
CNN-SVM: CNN followed by SVM with RBF kernel instead of logistic
regression.
EM-CNN-LR/SVM: EM-based method with CNN-LR and CNN-SVM
respectively.
EM-CNN-LR w/o spatial smoothing: We do not apply Gaussian
smoothing to estimate P(H | X). Otherwise similar to EM-CNN-LR.
EM-Finetune-CNN-LR/SVM: Similar to EM-CNN-LR/SVM except that
instead of training a CNN from scratch, we fine-tune a pretrained 16-layer
CNN model by training it on discriminative patches.
SMI-CNN-SMI: CNN with max-pooling at both discriminative patch
identification and image-level prediction steps. For the patch-level CNN
training, in each WSI only one patch with the highest confidence is
considered discriminative.
NM-LBP: We extract Nuclear Morphological features and rotation
invariant Local Binary Patterns from all patches. We build a Bag-of-
Words (BoW) feature using k-means followed by SVM with RBF
kernel , as a non-CNN baseline.
Pretrained-CNN-Fea-SVM: Similar to CNN-Fea-SVM. But instead of
training a CNN, we use a pretrained 16-layer CNN model to extract
features from patches. Then we select the top 500 features according to
accuracy on the training set .
Pretrained-CNN-Bow-SVM: We build a BoW model using k-means on
features extracted by the pretrained CNN, followed by SVM .
5.4. WSI of glioma classification
There are WSIs of six subtypes of glioma in the TCGA dataset . The numbers of WSIs
and patients in each class are shown in Tab. 2. All classes are described in App. A.
The results of our experiments are shown in Tab. 3. The confusion matrix is given in Tab. 4.
An experiment showed that the inter-observer agreement of two experienced pathologists on
a similar dataset was approximately 70% and that even after reviewing the cases together,
they agreed only around 80% of the time . Therefore, our accuracy of 77% is similar to
inter-observer agreement.
In the confusion matrix, we note that the classification accuracy between GBM and Low-
Grade Glioma (LGG) is 97% (chance was 51.3%). A fully supervised method achieved 85%
accuracy using a domain specific algorithm trained on ten manually labeled patches per
class . Our method is the first to classify five LGG subtypes automatically, a much more
challenging classification task than the benchmark GBM vs. LGG classification. We achieve
Hou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2016 October 28.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
57.1% LGG-subtype classification accuracy with chance at 36.7%. Most of the confusions
are related to oligoastrocytoma (OA) since it is a mixed glioma that is challenging for
pathologists to agree on, according to a neuropathology study: “Oligoastrocytomas contain
distinct regions of oligodendroglial and astrocytic differentiation… The minimal percentage
of each component required for the diagnosis of a mixed glioma has been debated, resulting
in poor interobserver reproducibility for this group of neoplasms.” .
We compare recognition rates for the OA subtype. The F-score of OA recognition is 0.426,
0.482, and 0.544 using PreCNN-Fea-SVM, CNN-LR, and EM-CNN-LR respectively. We
thus see that the improvement over other methods becomes increasingly more significant
using our proposed method on the harder-to-classify classes.
The discriminative patch (region) segmentation results in Fig. 4 demonstrate the quality of
our EM-based method.
5.5. WSI of NSCLC classification
We use three major subtypes of Non-Small-Cell Lung Carcinoma (NSCLC). Numbers of
WSIs and patients in each class are in Tab. 5. All classes are listed in App. A.
Experimental results are shown in Tab. 6; the confusion matrix is in Tab. 7. When classifying
SCC vs. non-SCC, inter-observer agreement between pulmonary pathology experts and
between community pathologists measured by Cohen’s kappa is κ = 0.64 and κ = 0.41
respectively . We achieved κ = 0.75. When classifying ADC vs. non-ADC, the interobserver agreement between experts and between community pathologists are κ = 0.69 and
κ = 0.46 respectively . We achieved κ = 0.60. Therefore, our results appear close to
inter-observer agreement.
The ADC-mix subtype is hard to classify because it contains visual features of multiple
NSCLC subtypes. The Pretrained CNN-Fea-SVM method achieves an F-score of 0.412
recognizing ADC-mix cases, whereas our proposed method EM-Finetune-CNN-SVM
achieves 0.472. Consistent with the glioma results, our method’s performance advantages
are more pronounced in the hardest cases.
5.6. Rail surface defect severity grade classification
We evaluate our approach beyond classification of pathology images. A CNN cannot be
applied to gigapixel images directly because of computational limitations. Even when the
images are small enough for CNNs, our patch-based method compares favorably to an
image-based CNN if discriminative information is encoded in image patch scale and
dispersed throughout the images.
We classify the severity grade of rail surface defects. Automatic defect grading can obviate
the need for laborious examination and grading of rail surface defects on a regular basis. We
used a dataset of 939 rail surface images with defect severity grades from 0 to 7.
Typical image resolution is 1200×500, as in Fig. 5.
Hou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2016 October 28.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
To support our claim, we tested two additional methods:
CNN-Image: We apply the CNN on image scale directly. In particular, we
train the CNN on 400×400 regions randomly extracted from images in
each iteration. At test time, we apply the CNN on five regions (top left, top
right, bottom left, bottom right, center) and average the predictions.
Pretrained CNN-ImageFea-SVM: We apply a pretrained 16-layer network
 to rail surface images to extract features, and train an SVM on these
The CNN used in this experiment has a similar achitecture to the one described in Tab. 1
with smaller and fewer filters. The size of patches in our patch-based methods is 64 by 64.
We apply 4-fold cross-validation and show the averaged results in Tab. 8. Our patch-based
methods EM-CNN-SVM and EM-CNN-Fea-SVM outperform the conventional image-based
method CNN-Image. Moreover, results using CNN features extracted on patches (Pretrained
CNN-Fea-SVM) are better than results with CNN features extracted on images (Pretrained-
CNN-ImageFea-SVM).
6. Conclusions
We presented a patch-based Convolutional Neural Network (CNN) model with a supervised
decision fusion model that is successful in Whole Slide Tissue Image (WSI) classification.
We proposed an Expectation-Maximization (EM) based method that identifies
discriminative patches automatically for CNN training. With our algorithm, we can classify
subtypes of cancers given WSIs of patients with accuracy similar or close to inter-observer
agreements between pathologists. Furthermore, we experimentally demonstrate using a
comparable non-cancer dataset of smaller images, that the performance of our patch-based
CNN compare favorably to that of an image-based CNN. In the future we will leverage the
non-discriminative patches as part of the data likelihood in the EM formulation. We will
optimize CNN-training so that it scales up to larger scale pathology datasets.
Acknowledgments
This work was supported in part by 1U24CA180924-01A1 from the National Cancer Institute, R01LM011119-01
and R01LM009239, and partially supported by NSF IIS-1161876, IIS-1111047, FRA DTFR5315C00011, the
Subsample project from DIGITEO Institute, France, and a gift from Adobe Corp. We thank Ke Ma for providing
the rail surface dataset.
Appendix A. Description of cancer subtypes
GBM Glioblastoma, ICD-O 9440/3, WHO grade IV. A Whole Slide Image (WSI) is
classified as GBM iff one patch can be classified as GBM with high confidence.
OD Oligodendroglioma, ICD-O 9450/3, WHO grade II.
OA Oligoastrocytoma, ICD-O 9382/3, WHO grade II; Anaplastic oligoastrocytoma, ICD-O
9382/3, WHO grade III. This mixed glioma subtype is hard to classify even by pathologists
Hou et al.
Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Author manuscript; available in PMC 2016 October 28.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
DA Diffuse astrocytoma, ICD-O 9400/3, WHO grade II.
AA Anaplastic astrocytoma, ICD-O 9401/3, WHO grade III.
AO Anaplastic oligodendroglioma, ICD-O 9451/3, WHO grade III.
LGG Low-Grade-Glioma. Include OD, OA, DA, AA, AO.
SCC Squamous cell carcinoma, ICD-O 8070/3.
ADC Adenocarcinoma, ICD-O 8140/3.
ADC-mix ADC with mixed subtypes, ICD-O 8255/3.