Machine Learning, 22, 33-57 
O 1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
Linear Least-Squares Algorithms for Temporal
Difference Learning
STEVEN J. BRADTKE
GTE Data Services, One E Telecorn Pkwy, DC B2H, Temple Terrace, FL 33637
Dept. o] Computer Science, University of Massachusetts, Amherst, MA 01003-4610
bradtke@ gte.com
 
Editor: Leslie Pack Kaelbling
We introduce two new temporal difference (TD) algorithms based on the theory of linear least-
squares function approximation. We define an algorithm we call Least-Squares TD (LS TD) for which we prove
probability-one convergence when it is used with a function approximator linear in the adjustable parameters.
We then define a recursive version of this algorithm, Recursive Least-Squares TD (RLS TD). Although these
new TD algorithms require more computation per time-step than do Sutton's TD(A) algorithms, they are more
efficient in a statistical sense because they extract more information from training experiences. We describe
a simulation experiment showing the substantial improvement in learning rate achieved by RLS TD in an
example Markov prediction problem. To quantify this improvement, we introduce the TD error variance of a
Markov chain, arc,, and experimentally conclude that the convergence rate of a TD algorithm depends linearly
on ~ro. In addition to converging more rapidly, LS TD and RLS TD do not have control parameters, such as
a learning rate parameter, thus eliminating the possibility of achieving poor performance by an unlucky choice
of parameters.
Keywords: Reinforcement learning, Markov Decision Problems, Temporal Difference Methods, Least-Squares
Introduction
The class of temporal difference (TD) algorithms was developed to pro-
vide reinforcement learning systems with an efficient means for learning when the con-
sequences of actions unfold over extended time periods. They allow a system to learn
to predict the total amount of reward expected over time, and they can be used for
other prediction problems as well .
We introduce two new TD algorithms based on the theory of linear
least-squares function approximation. The recursive least-squares function approxima-
tion algorithm is commonly used in adaptive control because it
can converge many times more rapidly than simpler algorithms. Unfortunately, extending
this algorithm to the case of TD learning is not straightforward.
We define an algorithm we call Least-Squares TD (LS TD) for which we prove
probability-one convergence when it is used with a function approximator linear in the
adjustable parameters. To obtain this result, we use the instrumental variable approach
 which provides
a way to handle least-squares estimation with training data that is noisy on both the
input and output observations. We then define a recursive version of this algorithm, Re-
s.J. BRADTKE AND A.G. BARTO
cursive Least-Squares TD (RLS TD). Although these new TD algorithms require more
computation per time step than do Sutton's TD(A) algorithms, they are more efficient
in a statistical sense because they extract more information from training experiences.
We describe a simulation experiment showing the substantial improvement in learning
rate achieved by RLS TD in an example Markov prediction problem. To quantify this
improvement, we introduce the TD error variance of a Markov chain, ~rro, and experi-
mentally conclude that the convergence rate of a TD algorithm depends linearly on art.
In addition to converging more rapidly, LS TD and RLS TD do not have control pa-
rameters, such as a learning rate parameter, thus eliminating the possibility of achieving
poor performance by an unlucky choice of parameters.
We begin in Section 2 with a brief overview of the policy evaluation problem for
Markov decision processes, the class of problems to which TD algorithms apply. After
describing the TD(A) class of algorithms and the existing convergence results in Sec-
tions 3 and 4, we present the least-squares approach in Section 5. Section 6 presents
issues relevant to selecting an algorithm, and Sections 7 and 8 introduce the TD error
variance and use it to quantify the results of a simulation experiment.
Markov Decision Processes
TD(A) algorithms address the policy evaluation problem associated with discrete-time
stochastic optimal control problems referred to as Markov decision processes (MDPs). An
MDP consists of a discrete-time stochastic dynamic system (a controlled Markov chain),
an immediate reward function, R, and a measure of long-term system performance.
Restricting attention to finite-state, finite-action MDP's, we let X and A respectively
denote finite sets of states and actions, and 3 9 denote the state transition probability
function. At time step t, the controller observes the current state, xt, and executes an
action, at, resulting in a transition to state xt+l with probability P(zt, zt+l, at) and the
receipt of an immediate reward rt = R(xt, xt+l, at). A (stationary) policy is a function
# : X --* A giving the controller's action choice for each state.
For each policy # there is a value function, V ~, that assigns to each state a measure of
long-term performance given that the system starts in the given state and the controller
always uses # to select actions. Confining attention to the infinite-horizon discounted
definition of long-term performance, the value function for # is defined as follows:
where 7, 0 _< 7 < 1, is the discount factor and E~ is the expectation given that actions
are selected via #. (In problems in which one can guarantee that there will exist some
finite time T such that rk = 0 for k _> % then one can set "y = 1_) The objective of the
MDP is to find a policy, #*, that is optimal in the sense that V ~* (x) >_ VU(x) for all
x ~ X and for all policies #.
Computing the evaluation function for a given policy is called policy evaluation. This
computation is a component of the policy iteration method for finding an optimal policy,
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 35
and it is sometimes of interest in its own right to solve prediction problems, the perspec-
tive taken by Sutton . The evaluation function of a policy must satisfy the
following consistency condition: for all x E X:
P(x, y, #(x))[R(x, y, #(x)) + 7V"(y)].
This is a set of IX[ linear equations which can be solved for V u using any of a number
of standard direct or iterative methods when the functions R and P are known. The
TD(A) family of algorithms apply to this problem when these functions are not known.
Since our concern in this paper is solely in the problem of evaluating a fixed policy #, we
can omit reference to the policy throughout. We therefore denote the value function V u
simply as V, and we omit the action argument in the functions R and P. Furthermore,
throughout this paper, by a Markov chain we always mean a finite-state Markov chain.
The TD(A) Algorithm
Although any TD(A) algorithm can be used with a lookup-table function representation,
it is most often described in terms of a parameterized function approximator. In this
case, Vt, the approximation of V at time step t, is defined by Vt(x) = f(Ot, ex), for all
x E X, where Ot is a parameter vector at time step t, ex is a feature vector representing
state x, and f is a given real-valued function differentiable with respect to Ot for all ex.
We use the notation V0~ Vt (x) to denote the gradient vector at state x of Vt as a function
Table 1. Notation used in the discussion of the TD(A) learning rule.
states of the Markov chain
the state at time step t
the immediate reward associated with the transition trom state xt to Xt+l; rt =
R(zt, zt+l).
t~he vector of expected immediate rewards;
the vector of starting probabilities
the transpose of the vector or matrix X
the true value function
the feature vector representing state x
the feature vector representing state xt. 49t = fbzt.
the matrix whose x-th row is Cz.
the proportion of time that the Markov chain is expected to spend in state x
the diagonal matrix diag(Tr)
the true value function parameter vector
the estimate of O* at time t
the estimated value of state x using parameter vector 0t
the step-size parameter used to update the value of Ot
the number of transitions from state xt up to time step t.
Using additional notation, summarized in Table 1, the TD(A) learning rule for a dif-
ferentiable parameterized function approximator updates the parameter
S.J. BRADTKE
AND A.G. BARTO
vector Ot as follows:
o~+~ = o~ + ~.(~) [ R~ + ~V~(x~+,)
v~(.~) ] Z ~-kvo~v~(~)
= Ot + c~(:ct)A0t,
[ R, + ~v~(~,+~) - v~(~,) ] Z a~-~Vo, V~(~)
[ R~ + ~y~(x~+~) - y~(x~) ] r~
Notice that A0t depends only on estimates, Vt(xk), made using the latest parameter
values, Or. This is an attempt to separate the effects of changing the parameters from
the effects of moving through the state space. However, when Vt is not linear in the
parameter vector Ot and ~ ¢ 0, the sum Et cannot be formed in an efficient, recursive
manner. Instead, it is necessary to remember the xk and to explicitly compute V0, Vt (xk)
for all k <_ t. This is necessary because if Vt is nonlinear in Or, VotVt(xk) depends on
0t. Thus, Et cannot be defined recursively in terms of Et-1. Because recomputing Et
in this manner at every time step can be expensive, an approximation is usually used.
Assuming that the step-size parameters are small, the difference between Ot and Or-1 is
also small. Then an approximation to Ek can be defined recursively as
Et = ~Et-1 + Vo, Vt(xt).
If V is linear in 0, then (2) can be used to compute Y;t exactly. No assumptions about
the step-size parameters are required, and no approximations are made.
We will be concerned in this paper with function approximators that are linear in the
parameters, that is, functions that can be expressed as follows: Vt(x) = ¢~:0t, where ¢' :g
denotes the transpose of Cz so that ¢'0t is the inner product of qSz and 0t. In this case,
(2) becomes
Bt = ),Et-i + q~t,
so that (1) simplifies to
Et = ~)d-kCk-
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 37
Select 0o.
Set t = 0.
forn= 0to oo {
Choose a start state xt according to the start-state probabilities given by S.
Set An = 0.
while xt is not an absorbing state {
Let the state change from xt to 3gt+l according to the Markov chain transition
probabilities.
Set A,~ = A,~ + AOt, where AOt is given by (3).
Update the parameters at the end of trial number n: 0r~+l = 0,~ + a,~An.
Figure 1. Trial-based TD(A) for absorbing Markov chains. A trial is a sequence of states generated by the
Markov chain, starting with some initial state and ending in an absorbing state. The variable n counts the
number of trials. The variable k counts the number of steps within a trial. The parameter vector 0 is updated
only at the end of a trial.
Previous Convergence Results for TD(A)
Convergence of a TD(A) learning rule depends on the state representation, {¢~}xcx,
and the form of the function approximator. Although TD(A) rules have been used suc-
cessfully with function approximators that are nonlinear in the parameter vector 0, most
notably the use of a multi-layer artificial neural network in Tesauro's backgammon pro-
grams , convergence has only been proven for cases in which the value
function is represented as a lookup table or as a linear function of 0 when the fea-
ture vectors are linearly independent. 1 Sutton and Dayan 
proved parameter convergence in the mean under these conditions, and Dayan and Se-
jnowski proved parameter convergence with probability 1
under these conditions for TD(~) applied to absorbing Markov chains in a trial-based
manner, i.e., with parameter updates only at the end of every trial. A trial is a sequence
of states generated by the Markov chain, starting with some initial state and ending in
an absorbing state. The start state for each trial is chosen according to a probability
distribution S. Figure 1 describes this algorithm. Since parameter updates take place
only at the end of each trial, A0t must be defined somewhat differently from above:
where n is the trial number and t is the time step. The parameter vector 0n is held
constant throughout trial n, and is updated only at the end of each trial.
S.J. BRADTKE AND A.G. BARTO
Less restrictive theorems have been obtained for the TD(0) algorithm by considering
it as a special case of Watkins' Q-learning algorithm. Watkins and
Dayan , Jaakkola, Jordan, and Singh, ,
and Tsitsiklis note that since the TD(0) learning rule is a special case
of Q-learning, their probability-one convergence proofs for Q-learning can be used to
show that on-line use of the TD(0) learning rule (i.e., not trial-based) with a lookup--
table function representation converges to V with probability 1. Bradtke 
extended Tsitsiklis' proof to show that on-line use of TD(0) with a function approximator
that is linear in the parameters and in which the feature vectors are linearly independent
also converges to V with probability 1.
Bradtke also proved probability-one convergence under the same conditions for a
normalized version of TD(0) that he called NTD(0) . Bradtke also defined
the NTD(A) family of learning algorithms, which are normalized versions of TD(A). As
with similar learning algorithms, the size of the input vectors ¢~ can cause instabilities in
TD(A) learning until the step-size parameter, o~, is reduced to a small enough value. But
this can make the convergence rate unacceptably slow. The NTD(A) family of algorithms
addresses this problem. Since we use NTD(A) in the comparative simulations presented
below, we define it here.
The NTD(A) learning rule for a function approximator that is linear in the parameters
0,+1 : 0, +
where e is some small, positive number. If we know that all of the Ct are non-zero, then
we can set e to zero. The normalization does not change the directions of the updates;
it merely bounds their size, reducing the chance for unstable behavior.
A Least-Squares Approach to TD Learning
The algorithms described above require relatively little computation per time step, but
they use information rather inefficiently compared to algorithms based on the least-
squares approach. Although least-squares algorithms require more computation per time
step, they typically require many fewer time steps to achieve a given accuracy than do
the algorithms described above. This section describes a derivation of a TD learning
rule based on least-squares techniques. Table 2 summarizes the notation we use in this
Linear Least-Squares Function approximation
This section reviews the basics of linear least-squares function approximation, including
instrumental variable methods. This background material leads in the next section to a
least-squares TD algorithm. The goal of linear least-squares function approximation is
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 39
Table 2. Notation used throughout this section in the discussion of Least-Squares algorithms.
~ : ~n .___. ~, the linear function to be approximated
wt E ~n, the observed input at time step t
~bt E ~, the observed output at time step t
~Tt E ~, the observed output noise at time step t
£~t = wt + ~t, the noisy input observed at time t
4t E ~n, the input noise at time step t
Cor(x, y) = E {xy'}, the correlation matrix for random variables x and y
Pt E ~n, the instrumental variable observed at time step t
to linearly approximate some function • : ~n
~ ~ given samples of observed inputs
wt E ~n and the corresponding observed outputs ~bt E ~. If the input observations are
not corrupted by noise, then we have the following situation:
= w~O* + ~t,
where 0* is the vector of true (but unknown) parameters and ~Tt is the output observation
Given (5), the least-squares approximation to 0*at time t is the vector Ot that minimizes
the quadratic objective function
Taking the partial derivative of Jt with respect to Ot, setting this equal to zero and solving
for the minimizing 0t gives us the t ~ estimate for 0",
The following lemma, proved in ref. , gives a set of conditions under
which 0t as defined by (6) converges in probability to 0":
LEMMA 1 If the correlation matrix Cor(w, w) is nonsingular and finite, and the output
observation noise 71k is uncorrelated with the input observations wk, then Ot as defined
by (6) converges in probability to 0".
Equation (5) models the situation in which observation errors occur only on the output.
In the more general case, the input observations are also noisy. Instead of being able to
directly observe wt, we can only observe &t = wt + it, where ~t is the input observation
noise vector at time t. This is known as an errors-in-variables situation .
The following equation models the errors-in-variables situation:
- Ct) + 0~
©~0" - <~0" +'qt.
S.J. BRADTKE AND A.G. BARTO
The problem with the errors-in-variables situation is that we cannot use d;t instead of
wt in (6) without violating the conditions of Lemma (1). Substituting ffJ~ directly for wt
in (6) has the effect of introducing noise that is dependent upon the current state. This
introduces a bias, and 0t no longer converges to 0". One way around this problem is
to introduce instrumental variables . An instrumental variable, Pt, is a vector that is correlated with
the true input vectors, wt, but that is uncorrelated with the observation noise, ~t. The
following equation is a modification of (6) that uses the instrumental variables and the
noisy inputs:
The following lemma, proved in ref. , gives a set of conditions under
which the introduction of instrumental variables solves the errors-in-variables problem.
LEMMA 2 If the correlation matrix Cor(p, a;) is nonsingular and finite, the correlation
matrix Cor(p, ~) = O, and the output observation noise rlt is uncorrelated with the
instrumental variables Pt, then Ot as defined by (8) converges in probability to 0".
Algorithm LS TD
Here we show how to use the instrumental variables method to derive an algorithm
we call Least-Squares TD (LS TD), a least-squares version of the TD algorithm. The
TD algorithm used with a linear-in-the-parameters function approximator addresses the
problem of finding a parameter vector, 0", that allows us to compute the value of a state
x as V(z) = Cr 0*
Recall that the value function satisfies the following consistency
condition:
P(x,y)[n(x,y) +~V(y)]
P(x, y)R(x, y) + -y Z
p(x, ~)v(y)
P(~, y)v(y),
where fzis the expected immediate reward for any state transition from state x. We can
rewrite this in the form used in Section 5.1 as
P(x,y)¢'v0*
LINEAR LEAST-SQUARES
ALGORITHMS FOR TEMPORAL
DIFFERENCE
P(x, y)¢y)'0*,
for every state x E X. Now we have the same kind of problem that we considered
in Section 5.1. The scalar output, fx, is the inner product of an input vector, qS~ -
"7 ~-~yeX P(x, y)¢y, and the true parameter vector, 0".
For each time step t, we therefore have the following equation that has the same form
, y)¢y)'0*
where rt is the reward received on the transition from xt to Xt+l. (rt - rt) corresponds
to the noise term r h in (5). The following lemma, proved in Appendix A, establishes
that this noise term has zero mean and is uncorrelated with the input vector wt =
et - "/~yeX P(zt, y)¢y:
LEMMA 3 For any Markov chain, if x and y are states such that P(x,y) > O,
with 'Txy = R(x, y) - ~ and ~o~ = (~ - 7 E~X
P(x, Y)¢y), then E {'7} = 0, and
Cor(w, r~) = O.
Therefore, if we know the state transition probability function, P, the feature vector
Ca, for all x E X, if we can observe the state of the Markov chain at each time step,
and if Cor(w,w) is nonsingular and finite, then by Lemma (1) the algorithm given by
(6) converges in probability to 0".
In general, however, we do not know P, {¢z}zex, or the state of the Markov chain
at each time step. We assume that all that is available to define Ot are et, ¢t+1 and rt.
Instrumental variable methods allow us to solve the problem under these conditions. Let
COt ----- Ct -- ")'¢t+l, and (t = "Y Y:~yeX P(xt, y)¢y - 70t+1. Then we can observe
6t - 76t+1
= (¢t--'~ E P(Xt'Y)¢Y) + (7 E P(xt,?,J)¢y-'~¢t+l)
with cot = et
7 EyEX P(xt, Y)¢y and <t = 7 ~yeX P(xt, y)¢y - "Y~)t+l- We see,
then, that the problem fits the errors-in-variables situation. Specifically, the following
equation in the form of (7) is equivalent to the consistency condition (9):
Tt = ((~t -- ")/¢t+1) '0. -- ("Y ~
P(xt,Y)¢y - "T~St+l) 10. -}- (Tt -- ?wt).
Following Section 5.1, we introduce an instrumental variable, Or, to avoid the asymp-
totic bias introduced by errors-in-variables problems. The following lemma, which is
proved in Appendix A, shows that Pt = et is an instrumental variable because it is
uncorrelated with the input observation noise, {t, defined above:
S.J. BRADTKE
AND A.O. BARTO
Set t = O.
repeat forever {
Set xt to be a start state selected according to the probabilities given by ,..q.
while xt is not an absorbing state {
Let the state change from x~ to xt+l according to the Markov chain
transition probabilities.
Use (1t) to define Or.
Figure 2. Trial-based LS TD for absorbing Markov chains. A trial is a sequence of states that starts at some
start state, follows the Markov chain as it makes transitions, and ends at an absorbing state.
LEMMA 4 For any Markov chain, if (1) x and y are states such that P(x, y) > O; (2)
~xy = "~ ~zEX
P(x, z)O z - ~/¢y. (3) ?]xy = R(x, y) - Tx. and (4) Px = ez, then (l)
Cor(p, rl) = O: and (2) Cor(p, () = O.
Using ez as the instrumental variable, we rewrite (8) to obtain the LS TD algorithm:
[ 7 }--~k=l ek(¢k -- "YCk+l)' ]-a [ Y ~]k=l ¢krk ]-
Figure 2 shows how (8) can be used as part of a trial-based algorithm to find the
value function for an absorbing Markov chain. Figure 3 shows how (8) can be used as
part of an algorithm to find the value function for an ergodic Markov chain. Learning
takes place on-line in both algorithms, with parameter updates after every state tran-
sition. The parameter vector 0t is not well defined when t is small since the matrix
-/~k=l ek(¢k -- "Yek+l)' ] is not invertible.
The LS TD algorithm has some similarity to an algorithm Werbos 
proposed as a linear version of his Heuristic Dynamic Programming . However, Werbos' algorithm is not amen-
able to a recursive formulation, as is LS TD, and does not converge for arbitrary initial
parameter vectors, as does LS TD. See ref. .
It remains to establish conditions under which LS TD converges to 0". According to
Lemma 2, we must establish that Cor(p, w) is finite and nonsingular. We take this up in
the next section.
Convergence of Algorithm LS TD
In this section we consider the asymptotic performance of algorithm LS TD when used
on-line to approximate the value functions of absorbing and ergodic Markov chains.
The following lemma, proved in Appendix A, starts the analysis by expressing 0Ls~v ~
limt-~ 0t, the limiting estimate found by algorithm LS TD for 0", in a convenient form.
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 43
Set t = 0.
Select an arbitrary initial state, xo.
repeat forever {
Let the state change from at to xt+l according to the Markov chain transi-
tion probabilities.
Use (11) to define Or.
Figure 3. LS TD for ergodic Markov chains.
LEMMA 5 For any Markov chain, when (1) Ot is found using algorithm LS TD; (2)
each state :c E X is visited infinitely often; (3) each state z E X is visited in the long
run with probability 1 in Proportion ~r~; and (4) [~srII(I - "/P)O] is invertible, where
is the matrix of whose z-th row is 0~, and II is the diagonal matrix diag(sr), then
with probability 1.
The key to using Lemma 5 lies in the definition of 7rz: the proportion of time that the
Markov chain is expected to spend over the long run in state z. Equivalently, rrz is the
expected proportion of state transitions that take the Markov chain out of state z. For an
ergodic Markov chain, 7rz is the invariant, or steady-state, distribution associated with
the stochastic matrix P . For an absorbing Markov chain, lrx is
the expected number of visits out of state z during one transition sequence from a start
state to a goal state . Since there are no transitions out of a goal
state, lrx = 0 for all goal states. These definitions prepare the way for the following
two theorems. Theorem 1 gives conditions under which LS TD as used in Figure 2 will
cause 0Ls~ to converge with probability 1 to 0 * when applied to an absorbing Markov
chain. Theorem 2 gives conditions under which LS TD as used in Figure 3 will cause
~Ls~ to converge with probability 1 to 0* when applied to an ergodic Markov chain.
THEOREM 1 (CONVERGENCE OF LS TD FOR ABSORBING MARKOV CHAINS)
When using LS TD as described in Figure 2 to estimate the value function for an absorb-
ing Markov chain, if (1) S is such that there are no inaccessible states; (2) R(z, y) = 0
whenever both z, y ~ T, the set of absorbing states; (3) the set of feature vectors repre-
senting the non-absorbing states, { ~ I cc E iV'}, is linearly independent; (4) ckz = O for
all z E T; (5) each ~z is of dimension m = IA;I, and (6) 0 < "7 < 1; then O* is finite
and the asymptotic parameter estimate found by algorithm LS TD, OLs~, converges with
probability 1 to O* as the number of trials (and state transitions) approaches infinity.
Different conditions are required in the absorbing and ergodic chain cases in order to
meet the conditions of Lemma 5. The conditions required in Theorem 1 are generaliza-
S.J. BRADTKE
AND A.G. BARTO
tions of the conditions required for probability 1 convergence of TD(A) for absorbing
Markov chains. The conditions required in Theorem 2 are much less restrictive, though
the discount factor 3' must be less than 1 to ensure that the value function is finite.
2 (CONVERGENCE
When using LS TD as described in Figure 3 to estimate ihe value function for an ergodic
Markov chain, if (1) the set of feature vectors representing the states, {~x I z E X}, is
linearly independent," (2) each ~ is of dimension N = [XI; (3) 0 <_ 3' < 1; then O* is
finite and the asymptotic parameter estimate found by algorithm LS TD, OLs~o, converges
with probability 1 to O* as the number of state transitions approaches infinity.
Theorems 1 and 2 provide convergence assurances for LS TD similar to those provided
by Tsitsiklis and Watkins and Dayan for the
convergence of TD(0) using a lookup-table function approximator.
Proof of Theorem h Condition (1) implies that, with probability 1, as the total number
of state transitions approaches infinity, the number of times each state z C X is visited
approaches infinity. Since this is an absorbing chain, we have with probability 1 that the
states are visited in proportion 7r as the number of trials approaches infinity. Therefore,
by Lemma 5, we know that with probability 1
-- "TP)(I)] -1
assuming that the inverse exists.
Conditions (3), (4), and (5) imply that cI, has rank re, with row 3: of &5 consisting of all
zeros for all a: E T. Condition (1) implies that II has rank re. Row z of II consists of
all zeros, for all x E T. ~5 has the property that if all rows corresponding to absorbing
states are removed, the resulting submatrix is of dimensions (re x re) and has rank re.
Call this submatrix A. I1 has the property that if all rows and columns corresponding to
absorbing states are removed, the resulting submatrix is of dimensions (re x re) and has
rank m. Call this submatrix B. (I - 7P) has the property that if all rows and columns
corresponding to absorbing states are removed, the resulting submatrix is of dimensions
(m × re) and has rank re . Call this submatrix C. It can be
verified directly by performing the multiplications that [~rI(I-
.,/P)~] = [A'BCA].
Therefore, [~'II(I- ~/P)(I)] is of dimensions (m × re) and has rank re.
Thus, it is
invertible.
Now, (9) can be rewritten using matrix notation as
This, together with conditions (2) and (6), implies that 0* is finite. Finally, substituting
(12) into the expression for 0Ls~ gives us
[(~ti~(/ -- ,.yp)~]-i [~'II(I - 'TP)~] O*
Thus, 0Ls-m converges to 0* with probability 1.
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 45
Proof of Theorem 2: Since this is an ergodic chain, as t approaches infinity we have
with probability 1 that the number of times each state z E X is visited approaches
infinity. We also have with probability 1 that the states are visited in the long run in
proportion 7r. Ergodicity implies that ~r~ > 0 for all x E X. Therefore, II is invertible.
Condition (3) implies that (I -~/P) is invertible. Conditions (1) and (2) imply that q) is
invertible. Therefore, by Lemma 5, we know that with probability 1
O~s,o = [~'H(I
.~p)~]-I [~'n~].
Condition (3) together with Equation (12) imply that 0* is finite. And, as above,
substituting (12) into the expression for 0Lsm gives
0Ls~ = [~'Yi(/- "yP)~b] -1 [rb'II(I - 7P)~] O*
Thus, 0Ls~ converges to 0* with probability 1.
Algorithm RLS TD
Algorithm LS TD requires the computation of a matrix inverse at each time step. This
means that LS TD has a computational complexity of O(m3), assuming that the state
representations are of length m. We can use Recursive Least-Squares (RLS) techniques
 to derive a modified
algorithm, Recursive Least-Squares TD (RLS TD), with computational complexity of
O(m2). The following equation set specifies algorithm RLS TD:
et = Rt - (qSt - ")@t+l) t-1
Ot-lCt(~)t
- 7¢t+l)tG-i
1 + (¢t - 7¢t+l)'Ct-lCt
Ot = Ot-1 q- 1 q- ((b t --"7Ot+l)tCt_lfbt et~t"
Notice that (15) is the TD(0) learning rule for function approximators that are linear in
the parameters, except that the scalar step-size parameter has been replaced by a gain
matrix. The user of an RLS algorithm must specify 0o and Co. Ct is the t a sample
estimate of ~Cor(p, cb) -1, where p and © are defined as in Section 5.2. Co 1 is typically
chosen to be a diagonal matrix of the form flI, where/3 is some large positive constant.
This ensures that Go, the initial-guess at the correlation matrix, is approximately 0, but
is invertible and symmetric positive definite.
The convergence of RLS TD requires the same conditions as algorithm LS TD, plus
one more. This is Condition A.1, or equivalently, Condition A.2:
Condition A.I: [Co 1 +
~k=1 pkc% must be non-singular for all times t.
Condition A.2:[1 + ~o~Ct_lpt] 7~ 0 for all times t.
S.J. BRADTKE AND A.G. BARTO
Under the assumption that the conditions A.1 and A.2 are maintained, we have that
If the conditions A. 1 and A.2 are not met at some time to, then all computations made
thereafter will be polluted by the indeterminate or infinite values produced at time to. The
non-recursive algorithm LS TD does not have this problem because the computations
made at any time step do not depend directly on the results of computations made at
earlier time steps.
Dependent or Extraneous Features
The value function for a Markov chain satisfies the equation
When using a function approximator linear in the parameters, this means that the param-
eter vector 0 must satisfy the linear equation
• 0 = [ 1- 7P ]-1 e.
In this section, the rows of i consist only of the feature vectors representing the non-
absorbing states, and V only includes the values for the non-absorbing states. This is
not essential, but it makes the discussion much simpler. Let n = INI be the number of
non-absorbing states in the Markov chain. Matrix ~5 has dimension n × m, where ra is
the length of the feature vectors representing the states.
Now, suppose that rank(1) = m < n.
Dayan shows that in this
case trial-based TD(A) (Figure 1) converges to [if'H(I- 7P)i] -1 [~5'HR] for h = 0.
This is the same result we achieved in Lemma 5, since ra = rank(if) if and only if
[~'H(I - 7P)~] is invertible 2. The proofs of Theorems 1 and 2 show convergence of
0rsvp to [~'II(I - 3,P)~] -1 [ff9'II/~] as a preliminary result. Thus, 0Lsw converges for
both absorbing and ergodic chains as long the assumptions of Lemma (5) are satisfied.
Suppose, on the other hand, that rank(~b) = n < m. This means that the state repre-
sentations are linearly independent but contain extraneous features. Therefore, there are
more adjustable parameters than there are constraints, and an infinite number of param-
eter vectors 0 satisfy (16). The stochastic approximation algorithms TD(A) and NTD(A)
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 47
converge to some 0 that satisfies (16). Which one they find depends on the order in which
the states are visited. LS TD does not converge, since [ 7 ~k=l 4~k(q~k -- 3'qSk+l) ' ] is
not invertible in this case. However, RLS TD converges to some 0 that satisfies (16). In
this case, too, the 0 to which the algorithm converges depends on the order in which the
states are visited.
Choosing an Algorithm
When TD(A) and NTD(A) algorithms are used with function approximators that are linear
in the parameters, they involve O(m) costs at each time step when measured either in
terms of the number of basic computer operations, or in terms of memory requirements,
where m is the length of the feature vectors representing the states. Algorithm LS TD's
costs are O(m 3) in time and O(m 2) in space at each time step, while RLS TD's are
O(m 2) in both time and space. TD(A) and NTD(A) are clearly superior in terms of cost
per time step. However, LS TD and RLS TD are more efficient estimators in the statistical
sense. They extract more information from each additional observation. Therefore, we
would expect LS TD and RLS TD to converge more rapidly than do TD(A) and NTD(A).
The use of LS TD and RLS TD is justified, then, if the increased costs per time step are
offset by increased convergence rate.
The performance of TD(A) is sensitive to a number of interrelated factors that do
not affect the performance of either LS TD or RLS TD. Convergence of TD(A) can be
dramatically slowed by a poor choice of the step-size (a) and trace (A) parameters. The
algorithm can become unstable if a is too large, causing 0t to diverge. TD(A) is also
sensitive to the norms of the feature vectors representing the states. Judicious choice of a
and A can prevent instability, but at the price of decreased learning rate. The performance
of TD(A) is also sensitive to H0o -0* H, the distance between 0* and the initial estimate for
0". NTD(A) is sensitive to these same factors, but normalization reduces the sensitivity.
In contrast, algorithms LS TD and RLS TD are insensitive to all of these factors. Use
of LS TD and RLS TD eliminates the possibility of poor performance due to unlucky
choice of parameters.
The transient behavior of a learning algorithm is also important. TD(A) and NTD(A)
remain stable (assuming that the step-size parameter is small enough) no matter what
sequence of states is visited.
This is not true for LS TD and RLS TD. If Ct -1 =
Co I q- Ek=l PkCJk is ill-conditioned or singular for some time t, then the estimate
Ot can very far from 0". LS TD will recover from this transient event, and is assured
of converging eventually to 0". The version of RLS TD described in Section 5.4 will
not recover if Ct -1 is singular. It may or may not recover from an ill-conditioned C -1,
depending on the machine arithmetic. However, there are well-known techniques for
protecting RLS algorithms from transient instability .
TD(A), NTD(A), and RLS TD have an advantage over LS TD in the case of extraneous
features, as discussed in Section 5.5. TD(A), NTD(A), and RLS TD converge to the
correct value function in this situation, while LS TD does not.
S.J. BRADTKE
AND A.G. BARTO
None of the factors discussed above makes a definitive case for one algorithm over
another in all situations. The choice depends finally on the computational cost structure
imposed on the user of these algorithms.
The TD Error Variance
One of the interesting characteristics of the TD error term,
+Tq~t+10t-1
is that it does not go to zero as Ot converges to 0", except in the trivial case of a
deterministic Markov chain. This is readily verified by inspection of (10). We define the
TD error variance, am, of a Markov chain as follows:
a~ = E {er~(0*) 2}
cr~ is the variance of the TD error term under the assumptions that Ot has converged to 0",
and that the states (and the corresponding TD errors) are sampled on-line by following a
sample path of the Markov chain, a~ is a measure of the noise that cannot be removed
from any of the TD learning rules (TD(A), NTD(A), LS TD, or RLS TD), even after
parameter convergence. It seems reasonable to expect that experimental convergence
rates depend on ~7~.
Experiments
This section describes two experiments designed to demonstrate the advantage in conver-
gence speed that can be gained through using least-squares techniques. Both experiments
compare the performance of NTD(A) with that of RLS TD in the on-line estimation of
the value function of a randomly generated ergodic Markov chain, the first with five states
and the second with fifty states (see Appendix B for the specification of the smaller of
the Markov chains). The conditions of Theorem 2 are satisfied in these experiments,
so that the lengths of the state representation vectors equal five and fifty respectively in
the two experiments. In a preliminary series of experiments, not reported here, NTD(A)
always performed at least as well as TD(A), while showing less sensitivity to the choice
of parameters, such as initial step size. Appendix C describes the algorithm we used
to set the step size parameters for NTD(A). Figures 4, 5, and 6 show the experimental
The x-axis of Figure 4 measures the TD error variance of the test Markov chain, which
was varied over five distinct values from cr~ = 10-1 through ~rro = 10 3 by scaling the
cost function R. The state transition probability function, P, and the state representations,
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 49
, ii ....I
TD error variance
Figure 4. Comparison of RLS TD and NTD(A) on a randomly generated 5 state ergodic Markov chain. The
x-axis measures the TD error variance of the test Markov chain, which was varied over five distinct values from
cra-D = 10-1 through O'TV = 103 by scaling the cost function R. The y-axis measures the average convergence
time over 100 training runs of on-line learning. There was one time step counted for each interaction with
the environment. The parameter vector was considered to have converged when the average of the error
o* ll~ fell below 10 -2 and stayed below this value thereafter. Graph A shows the performance of RLS
TD. Graph B shows the performance of NTD(A) where IJ0o - 0* lie = 1. Graph C shows the performance of
NTD(A) where ]10o
S.J. BRADTKE AND A.G. BARTO
. . i . . . . i . . . t . . . . i . . , i . - . . i . . . i . . . .
TD error variance
Figure 5. Performance of RLS TD on a randomly generated 50 state ergodic Markov chain. The x-axis
measures the TD error variance of the test Markov chain, which was varied over five distinct values from
aro = 10 -] through a-m = 103 by scaling the cost function R. The F-axis measures the average convergence
time over 100 training runs of on-line learning. The parameter vector was considered to have converged when
the average of the error [10t - 0* I[c~ felt below 10 -2 and stayed below this value thereafter.
of state transitions
Figure 6. Learning curves for RLS TD (solid) and NTD(A) (dashed) on a randomly generated 50 state ergodic
Markov chain, for cr~ = ]. The x-axis measures the number of state transitions, or the number of time steps.
The y-axis measures the average parameter error over 100 training runs of on-line learning. The parameter
error was defined as t[0t - 0* I1~.
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 51
if, were left unchanged. The y-axis of Figure 4 measures the average convergence time
over 100 training runs of on-line learning. This was computed as follows. For each of
the 100 training runs, I]Ot - 0*lloo was recorded at each time step (where II" I1~ denotes
the l~, or max, norm). These 100 error curves were averaged to produce the mean error
curve. Finally, the mean error curve was inspected to find the time step t at which the
average error fell below 10 -2 and stayed below 10 -2 for all the simulated times steps
thereafter.
Graph A of Figure 4 shows the performance of RLS TD. The other two graphs show
the performance of NTD(,~) given different initial values for 00. Graph B shows the
performance of NTD()t) when 0o was chosen so that I100- 0"112 = 1 (where I1" 112
denotes the 12, or Euclidean, norm). Graph C shows the performance of NTD(,k) when
0o was chosen so that II0o -
0"112 = 2. One can see that the performance of NTD(,k)
is sensitive to the distance of the initial parameter vector from 0". In contrast, the
performance of RLS TD is not sensitive to this distance (0o for Graph A was the same
as that for Graph B). The performance of NTD(A) is also sensitive to the settings of
four control parameters: ~, C~o, c, and ~-. The parameters c and ~- govern the evolution
of the sequence of step-size parameters (see Appendix C). A search for the best set
of control parameters for NTD(,k) was performed for each experiment in an attempt to
present NTD(~) in the best light. 3 The control parameter e (see Equation 4) was held
constant at 1.0 for all experiments.
Figure 4 shows a number of things. First, RLS TD outperformed NTD(~k) at every level
of cry. RLS TD always converged at least twice as fast as NTD(A), and did much better
than that at lower Levels of cry. Next, we see that, at least for RLS TD, convergence
time is a linear function of a~: increase crrD by a factor of 10, and the convergence
time can be expected to increase by a factor of 10. This relationship is less clear for
NTD(~), although the curves seem to follow the same rule for larger cr~o. It appears that
the effect of the initial distance from 0 to 0", II00 - 0* IIz, is significant when cr~ is small
but becomes less important, and is finally eliminated, as ~r~ increases.
Figures 5 and 6 present the results of repeating the experiment described above for
a randomly generated ergodic Markov chain with fifty states. Each state of this larger
Markov chain has a possible transition to five other states, on average. Figure 5 shows
that the convergence rates for RLS TD follow the same pattern seen in Figure 4: the
convergence time rises at the same rate crvo rises. We attempted tO experimentally test
the convergence times of NTD()~) on this problem as we did on the smaller problem.
However, we were unable to achieve convergence to the criterion (ltOt - 0"11 ~ < 10 -2)
for any value of 0% or any selection of the parameters A, ao, c, and T. Figure 6
compares the learning curves generated by RLS TD and NTD(A) for 0% = 1. The
parameters governing the behavior of NTD(A) were the best we could find. After some
initial transients, RLS TD settles very rapidly toward convergence, while NTD(A) settles
very slowly toward convergence, making almost no progress for tens of thousands of time
steps. These results indicate that the relative advantage of using the RLS TD algorithm
may actually improve as the size of the problem grows, despite the order O(rn 2) cost
required by RLS TD at each time step.
S.J. BRADTKE AND A.G. BARTO
The results shown in Figures 4, 5, and 6 suggest that the use of RLS TD instead
of TD(A) or NTD(A) is easily justified. RLS TD's costs per time step are an order of
m = IX[ more expensive in both time and space than the costs for TD(A) or NTD(A).
However, in the example problems, RLS TD always converged significantly faster than
TD(A) or NTD(A), and was at least an order of m faster for smaller ~y~. RLS TD has the
significant additional advantage that it has no control parameters that have to be adjusted.
In contrast, it required a very extensive search to select settings of the control parameters
a0, c, and ~- of NTD(A) to show this algorithm in a good light.
Conclusion
We presented three new TD learning algorithms, NTD(A), LS TD, and RLS TD, and
we proved probability 1 convergence for these algorithms under appropriate conditions.
These algorithms have a number of advantages over previously proposed TD learning al-
gorithms. NTD(A) is a normalized version of TD(A) used with a linear in-the-parameters
function approximator. The normalization serves to reduce the algorithm's sensitivity to
the choice of control parameters. LS TD is a Least-Squares algorithm for finding the
value function of a Markov chain. Although LS TD is more expensive per time step
than the algorithms TD(A) and NTD(A), it converges more rapidly and has no control
parameters that need to be set, reducing the chances for poor performance. RLS TD is
a recursive version of LS TD.
We also defined the TD error variance of a Markov chain, ~rvo. cr~ is a measure of
the noise that is inherent in any TD learning algorithm, even after the parameters have
converged to 0". Based on our experiments, we conjecture that the convergence rate of a
TD algorithm depends linearly on avo(Figure 4). This relationship is very clear for RLS
TD, but also seems to hold for NTD(A) for larger o-~.
The theorems concerning convergence of LS TD (and RLS TD) can be generalized
in at least two ways. First, the immediate rewards can be random variables instead of
constants. R(x, y) would then designate the expected reward of a transition from state
x to state y. The second change involves the way the states (and state transitions) are
sampled. Throughout this chapter we have assumed that the states are visited along
sample paths of the Markov chain. This need not be the case. All that is necessary is
that there is some limiting distribution, 7r, of the states selected for update, such that
7rz > 0 for all states x.
One of the goals of using a parameterized function approximator (of which the linear-
in-the-parameters approximators considered in this article are the simplest examples) is
to store the value function more compactly than it could be stored in a lookup table.
Function approximators that are linear in the parameters do not achieve this goal if
the feature vectors representing the states are linearly independent, since in this case
they use the same amount of memory as a lookup table. However, we believe that the
results presented here and elsewhere on the performance of "IT) algorithms with function
approximators that are linear in the parameters are first steps toward understanding the
performance of TD algorithms using more compact representations.
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 53
Appendix A
Proofs of Lemmas
In preparation for the proofs of Lemmas 3 and 4, we first examine the sum
~y~X P(x, y)(R(x,y) - rz) for an arbitrary state x:
E P(x,y)(R(x,y)- ~) = E P(x,y)R(x,y)- E P(x,y)f~
Proof of Lemma 3: The result in the preceding paragraph leads directly to a proof that
E {,7} : o:
E{r]} : E{R(x,y)-{z}
: Z ~ Z P(~,y)(R(z,y)-~)
and to a proof that Cor(w, ~7) = 0:
Cor(w,~7) :
P(:~,y)~(R(~,y)- ~)
7rxW x " 0
Proof of Lemma 4: First we consider Cor(p, r/):
Cor(~,,7) : E {p~'}
S.J. BRADTKE AND A.G. BARTO
P(x,y)¢~(R(x,y)-~)'
P(x,y)(R(x,y)-~)'
Now for Cor(p, ~):
Cor(p, ~) = E {pC"}
= ~ ~ ~ P(~,y),~(-~ ~ P(x,z)~z-~)'
~ P(x,y)(,~ ~ P(~,z)¢)-
Proof of Lemma 5: Equation 11 (repeated here) gives us the t~estimate found by algo-
rithm LS TD for 0":
Ot z [l~_l~k(~k --"~Ok+l)tl-1 II~=l~kRk ] "
As t grows we have by condition (2) that the sampled transition probabilities between
each pair of states approaches the true transition probabilities, P, with probability 1. We
also have by condition (3) that each state x E X is visited in the proportion 7r~ with
probability 1. Therefore, given condition (4) we can express the limiting estimate found
by algorithm LS TD, OLsvo, as
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING
Ck(¢k -- 7¢k+1)'
E ~x¢~ E P(x,y)R(x,y)
Appendix B
The Five-State Markov Chain Example
The transition probability function of the five-state Markov chain used in the experiments
appears in matrix form as follows, where the entry in row i, column j is the probability
of a transition from state i to state j (rounded to two decimal places):
0.42 0.13 0.14 0.03 0.28
0.25 0.08 0.16 0.35 0.15
0.08 0.20 0.33 0.17 0.22
0.36 0.05 0.00 0.51 0.07
0.17 0.24 0.19 0.18 0.22
The feature vectors representing the states (rounded to two decimal places) are listed
as the rows of the following matrix ~:
74.29 34.61 73.48 53.29
61.60 48.07 34.68 36.19 82.02
8.51 87.89
41.10 40.13 64.63 92.67 31.09
7.76 79.82 43.78
8.56 61.11
The immediate rewards were specified by the following matrix (which has been rounded
to two decimal places), where the entry in row i, column j determines the immediate
reward for a transition from state i to state j. The matrix was scaled to produce the
different TD error variance values used in the experiments. The relative sizes of the
immediate rewards remained the same.
S.J. BRADTKE AND A.G. BARTO
104.66 29.69
37.49 68.82 ]
75.86 29.24 100.37
0.31 35.99 |
57.68 65.66
56.95 100.44 47.63 |
96.23 14.01
89.77 66.77 |
70.35 23.69
70.70 85.41 J
Appendix C
Selecting Step-Size Parameters
The convergence theorem for NTD(0) requires a separate step-size pa-
rameter, a(x), for each state x, that satisfies the Robbins and Monro criteria
E ak(x) = ~
E ak(x)2 < c~
with probability 1, where ak(x) is the step-size parameter for the k-th visitation of state
x. Instead of a separate step-size parameter for each state, we used a single parameter
at, which we decreased at every time step. For each state x there is a corresponding
subsequence {at}~ that is used to update the value function when x is visited. We
conjecture that if the original sequence {at } satisfies the Robbins and Monro criteria, then
these subsequences also satisfy the criteria, with probability 1. The overall convergence
rate may be decreased by use of a single step-size parameter since each subsequence
will contain fewer large step sizes.
The step-size parameter sequence {at } was generated using the "search then converge"
algorithm described by Darken, Chang, and Moody :
The choice of parameters a0, c, and ~- determines the transition of learning from "search
mode" to "converge mode". Search mode describes the time during which t << w.
Converge mode describes the time during which t >> 7. at is nearly constant in search
mode, while at .w. ~ in converge mode. The ideal choice of step-size parameters moves
Ot as quickly as possible into the vicinity of 0* during search mode, and then settles into
converge mode.
1. If the set of feature vectors is linearly independent, then there exist parameter values such that any real-
valued function of X can be approximated with zero error by a function approximator linear in the
parameters. Using terminology from adaptive control , this situation is said to
satisfy the exact matching condition for arbitrary real-valued functions of X.
LINEAR LEAST-SQUARES ALGORITHMS FOR TEMPORAL DIFFERENCE LEARNING 57
rn can not be hess than rank(b). If m > rank((b), then [,btII(I - 7P)~] is an (rn x m) matrix with
rank less than rn. It is therefore not invertible.
3. The search for the best settings for A, ao, c, and 7 was the limiting factor on the size of the state space
for this experiment.