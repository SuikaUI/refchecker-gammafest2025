LCNN: Lookup-based Convolutional Neural Network
Hessam Bagherinezhad1,2
Mohammad Rastegari2,3
Ali Farhadi1,2,3
1University of Washington
3Allen Institute for AI
{hessam, mohammad, ali}@xnor.ai
Porting state of the art deep learning algorithms to resource constrained compute platforms (e.g. VR, AR, wearables) is extremely challenging. We propose a fast, compact, and accurate model for convolutional neural networks
that enables efﬁcient learning and inference. We introduce
LCNN, a lookup-based convolutional neural network that
encodes convolutions by few lookups to a dictionary that is
trained to cover the space of weights in CNNs. Training
LCNN involves jointly learning a dictionary and a small set
of linear combinations. The size of the dictionary naturally
traces a spectrum of trade-offs between efﬁciency and accuracy. Our experimental results on ImageNet challenge
show that LCNN can offer 3.2× speedup while achieving
55.1% top-1 accuracy using AlexNet architecture.
fastest LCNN offers 37.6× speed up over AlexNet while
maintaining 44.3% top-1 accuracy. LCNN not only offers
dramatic speed ups at inference, but it also enables efﬁcient
training. In this paper, we show the beneﬁts of LCNN in
few-shot learning and few-iteration learning, two crucial
aspects of on-device training of deep learning models.
1. Introduction
In recent years convolutional neural networks (CNN)
have played major roles in improving the state of the art
across a wide range of problems in computer vision, including image classiﬁcation , object detection , segmentation , etc. These models are very expensive in terms of computation and memory. For example, AlexNet has 61M parameters and
performs 1.5B high precision operations to classify a single
image. These numbers are even higher for deeper networks,
e.g.,VGG . The computational burden of learning and
inference for these models is signiﬁcantly higher than what
most compute platforms can afford.
Recent advancements in virtual reality (VR by Oculus)
 , augmented reality (AR by HoloLens) , and smart
wearable devices increase the demand for getting our state
of the art deep learning algorithm on these portable compute
platforms. Porting deep learning methods to these platforms
is challenging mainly due to the gap between what these
platforms can offer and what our deep learning methods require. More efﬁcient approaches to deep neural networks is
the key to this challenge.
Recent work on efﬁcient deep learning have focused on
model compression and reducing the computational precision of operations in neural networks . CNNs suffer from over-parametrization and often encode highly
correlated parameters , resulting in inefﬁcient computation and memory usage . Our key insight is to leverage the correlation between the parameters and represent
the space of parameters by a compact set of weight vectors, called dictionary. In this paper, we introduce LCNN,
a lookup-based convolutional neural network that encodes
convolutions by few lookups to a dictionary that is trained
to cover the space of weights in CNNs. Training LCNN
involves jointly learning a dictionary and a small set of
linear combinations. The size of the dictionary naturally
traces a spectrum of trade-offs between efﬁciency and accuracy. Our experimental results using AlexNet on ImageNet
challenge show that LCNN can offer 3.2× speedup while
achieving 55.1% top-1 accuracy. Our fastest LCNN offers
37.6× speed up over CNN while maintaining 44.3% top-1
accuracy. In the ResNet-18, the most accurate LCNN offers
5× speedup with 62.2% accuracy and the fastest LCNN offers 29.2× speedup with 51.8% accuracy
In addition, LCNN enables efﬁcient training; almost all
the work in efﬁcient deep learning have focused on efﬁcient
inference on resource constrained platforms . Training on these platforms is even more challenging and requires addressing two major problems: i. few-shot learning: the settings of on-device training dictates that there
won’t be enough training examples for new categories. In
fact, most training needs to be done with very few training examples; ii. few-iteration learning: the constraints in
computation and power require the training to be light and
quick. This imposes hard constraints on the number of iterations in training.LCNN offers solutions for both of these
problems in deep on-device training.
 
Few-shot learning, the problem of learning novel categories from few examples (sometimes even one example),
have been extensively studies in machine learning and computer vision . The topic is, however, relatively new for
deep learning , where the main challenge is to avoid
overﬁtting.
The number of parameters are signiﬁcantly
higher than what can be learned from few examples. LCNN,
by virtue of having fewer parameters to learn (only around
7% of parameters of typical networks), offers a simple solution to this challenge. Our dictionary can be learned ofﬂine
from training data where enough training examples per category exists. When facing new categories, all we need to
learn is the set of sparse reconstruction weights. Our experimental evaluations show signiﬁcant gain in few-shot learning; 6.3% in one training example per category.
Few-iteration learning is the problem of getting highest possible accuracy in few iterations that a resource constrained platform can offer. In a typical CNN, training often
involves hundreds of thousands of iterations. This number
is even higher for recent deeper architectures. LCNN offers
a solution: dictionaries in LCNN are architecture agnostic
and can be transferred across architectures or layers. This
allows us to train a dictionary using a shallow network and
transfer it to a deeper one. As before, all we need to learn
are the few reconstruction weights; dictionaries don’t need
to be trained again. Our experimental evaluations on ImageNet challenge show that using LCNN we can train an
18-layer ResNet with a pre-trained dictionary from a 10layer ResNet and achieve 16.2% higher top-1 accuracy on
10K iterations.
In this paper, we 1) introduce LCNN; 2) show state of
the art efﬁcient inference in CNNs using LCNN; 3) demonstrate possibilities of training deep CNNs using as few as
one example per category 4) show results for few iteration
learning .
2. Related Work
A wide range of methods have been proposed to address
efﬁcient training and inference in deep neural networks.
Here, we brieﬂy study these methods under the topics that
are related to our approach.
Weight compression: Several attempts have been made
to reduce the number of parameters of deep neural networks. Most of such methods are based
on compressing the fully connected layers, which contain
most of the weights. These methods do not achieve much
improvement on speed.
In , a small DNN architecture is proposed which is fully connected free and has 50x
fewer parameters in compare to AlexNet . However,
their model is slower than AlexNet. Recently reduced the number of parameters by pruning. All of these
approaches update a pre-trained CNN, whereas we propose
to train a compact structure that enables faster inference.
Low Rank Assumption: Approximating the weights
of convolutional layers with low-rank tensor expansion has
been explored by . They only demonstrated speedup
in the case of large convolutions. uses SVD for tensor
decomposition to reduce the computation in the lower layers on a pre-trained CNN. minimizes the reconstruction error of the nonlinear responses in a CNN, subject to a
low-rank constraint which helps to reduce the complexity of
ﬁlters. Notably, all of these methods are a post processing
on the weights of a trained CNN, and none of them train a
lower rank network from scratch.
Low Precision Networks: A ﬁxed-point implementation of 8-bit integer was compared with 32-bit ﬂoating point
activations in . Several network quantization methods are proposed by . Most recently, binary networks has shown to achieve relatively strong result
on ImageNet . They have trained a network that computes the output with mostly binary operations, except for
the ﬁrst and the last layer. uses the real-valued version
of the weights as a key reference for the binarization process. is an extension of , where both weights and
activations are binarized. retrains a previously trained
neural network with binary weights and binary inputs. Our
approach is orthogonal to this line of work. In fact, any of
these methods can be applied in our model to reduce the
precision.
Sparse convolutions: Recently, several attempts have
been made to sparsify the weights of convolutional layers
 . shows how to reduce the redundancy in
parameters of a CNN using a sparse decomposition. 
proposed a framework to simultaneously speed up the computation and reduce the storage of CNNs. proposed
a Structured Sparsity Learning (SSL) method to regularize
the structures (i.e., ﬁlters, channels, ﬁlter shapes, and layer
depth) of CNNs. Only in a sparse CNN is trained from
scratch which makes it more similar to our approach. However, our method provides a rich set of dictionary that enables implementing convolution with lookup operations.
Few-Shot Learning: The problem of learning novel categories has been studied in . Learning from few
examples per category explored by . proposed a method to learn from one training example per category, known as one-shot learning. Learning without any
training example, zero-shot learning, is studied by .
3. Our Approach
Overview: In a CNN, each convolutional layer consists of
n cubic weight ﬁlters of size m × kw × kh, where m and n
are the number of input and output channels, respectively,
and kw and kh are the width and the height of the ﬁlter.
Therefore, the weights in each convolutional layer is composed of nkwkh vectors of length m. These vectors are
shown to have redundant information . To avoid this re-
Dictionary
 
[ 0.2 , 0.7 , 0.1 ]
 
[ 0.3 , 0.6 , 0.4 ]
Figure 1. This ﬁgure demonstrates the procedure for constructing a weight ﬁlter in LCNN. A vector in the weight ﬁlter (the long colorful
cube in the gray tensor W) is formed by a linear combination of few vectors, which are looked up from the dictionary D. Lookup indices
and their coefﬁcients are stored in tensors I and C.
dundancy, we build a relatively small set of vectors for each
layer, to which we refer as dictionary, and enforce each vector in the weight ﬁlter to be a linear combination of a few
elements from this set. Figure 1 shows an overview of our
model. The gray matrix at the left of the ﬁgure is the dictionary. The dashed lines show how we lookup a few vectors
from the dictionary and linearly combine them to build up
a weight ﬁlter. Using this structure, we devise a fast inference algorithm for CNNs. We then show that the dictionaries provide a strong prior on the visual data and enables
us to learn from few examples. Finally, we show that the
dictionaries can be transferred across different network architectures. This allows us to speedup the training of a deep
network by transferring the dictionaries from a shallower
A convolutional layer in a CNN consists of four parts: 1) the
input tensor X ∈Rm×w×h; where m, w and h are the number of input channels, the width and the height, respectively,
2) a set of n weight ﬁlters, where each ﬁlter is a tensor
W ∈Rm×kw×kh, where kw and kh are the width and the
height of the ﬁlter, 3) a scalar bias term b ∈R for each ﬁlter, and 4) the output tensor Y ∈Rn×w′×h′; where each
channel Y[i,:,:] ∈Rw′×h′ is computed by W ∗X + b. Here
∗denotes the discrete convolution operation1.
For each layer, we deﬁne a matrix D ∈Rk×m as the
shared dictionary of vectors. This is illustrated in ﬁgure 1,
on the left side. This matrix contains k row vectors of length
m. The size of the dictionary, k, might vary for different
layers of the network, but it should always be smaller than
nkwkh, the total number of vectors in all weight ﬁlters of
a layer. Along with the dictionary D, we have a tensor for
1The (:) notation is borrowed from NumPy for selecting all entries in a
dimension.
lookup indices I ∈N s×kw×kh
, and a tensor for lookup coefﬁcients C ∈Rs×kw×kh for each layer. For a pair (r, c),
I[:,r,c] is a vector of length s whose entries are indices of
the rows of the dictionary, which form the linear components of W[:,r,c]. The entries of the vector C[:,r,c] specify
the linear coefﬁcients with which the components should
be combined to make W[:,r,c] (illustrated by a long colorful cube inside the gray cub in Figure 1-right). We set s,
the number of components in a weight ﬁlter vector, to be
a small number. The weight tensor can be constructed as
W[:,r,c] =
C[t,r,c] · D[I[t,r,c],:]
This procedure is illustrated in Figure 1. In LCNN, instead of storing the weight tensors W for convolutional layers, we store D, I and C, the building blocks of the weight
tensors. As a result, we can reduce the number of parameters in a convolutional layer by reducing k, the dictionary
size, and s, the number of components in the linear combinations. In the next section, we will discuss how LCNN
uses this representation to speedup the inference.
Fast Convolution using a Shared Dictionary
A forward pass in a convolutional layer consists of n convolutions between the input X and each of the weight ﬁlters
W. We can write a convolution between an m × kw × kh
weight ﬁlter and the input X as a sum of kwkh separate
(1 × 1)-convolutions:
shiftr,c(X ∗W[:,r,c])
, where shiftr,c is the matrix shift function along rows and
columns with zero padding relative to the ﬁlter size. Now
 
[ 0.2 , 0.7 , 0.1 ]
 
[ 0.3 , 0.6 , 0.4 ]
Figure 2. S is the output of convolving the dictionary with the input tensor. The left side of this ﬁgure illustrates the inference time
forward pass. The convolution between the input and a weight ﬁlter is carried out by lookups over the channels of S and a few linear
combinations. Direct learning of tensors I and C reduces to an intractable discrete optimization. The right side of this ﬁgure shows an
equivalent computation for training based on sparse convolutions. Parameters P can be trained using SGD. The tiny cubes in P denote the
non-zero entries.
we use the LCNN representation of weights (equation 1) to
rewrite each 1 × 1 convolution:
shiftr,c(X ∗(
C[t,r,c] · D[I[t,r,c],:]))
C[t,r,c](X ∗D[I[t,r,c],:]))
Equation 3 suggests that instead of reconstructing the
weight tensor W and convolving with the input, we can
convolve the input with all of the dictionary vectors, and
then compute the output according to I and C. Since the
dictionary D is shared among all weight ﬁlters in a layer,
we can pre-compute the convolution between the input tensor X and all the dictionary vectors. Let S ∈Rk×w×h be
the output of convolving the input X with all of the dictionary vectors D, i.e.,
S[i,:,:] = X ∗D[i,:]
Once the values of S are computed, we can reconstruct the
output of convolution by lookups over the channels of S
according to I, then scale them by the values in C:
C[t,r,c]S[I[t,r,c],:,:])
This is shown in Figure 2 (left). Reducing the size of the
dictionary k lowers the cost of computing S and makes the
forward pass faster. Since S is computed by a dense matrix
multiplication, we are still able to use OpenBlas for
fast matrix multiplication. In addition, by pushing the value
of s to be small, we can reduce the number of lookups and
ﬂoating point operations.
Training LCNN
So far we have discussed how LCNN represents a weight
ﬁlter by linear combinations of a subset of elements in a
shared dictionary. We have also shown that how LCNN
performs convolutions efﬁciently in two stages: 1- Small
convolutions: convolving the input with a set of 1 × 1 ﬁlters (equation 4). 2- Lookup and scale: few lookups over
the channels of a tensor followed by a linear combination
(equation 5) . Now, we explain how one can jointly train
the dictionary and the lookup parameters, I and C. Direct
training of the proposed lookup based convolution leads to a
combinatorial optimization problem, where we need to ﬁnd
the optimal values for the integer tensor I. To get around
this, we reformulate the lookup and scale stage (equation 5)
using a standard convolution with sparsity constraints.
Let T ∈Rk×kw×kh be a one hot tensor, where T[t,r,c] =
1 and all other entries are zero. It is easy to observe that convolving the tensor S with T will result in shiftr,c(S[t,:,:]).
We use this observation to convert the lookup and scale
stage (equation 5) to a standard convolution. Lookups and
scales can be expressed by a convolution between the tensor
S and a sparse tensor P, where P ∈Rk×w×h, and P[:,r,c] is
a s-sparse vector (i.e. it has only s non-zero entries) for all
spatial positions (r, c). Positions of the non-zero entries in
P are determined by the index tensor I and their values are
determined by the coefﬁcient tensor C. Formally, tensor P
can be expressed by I and C:
∃t : It,r,c = j
Note that this conversion is reversible, i.e.,we can create
I and C from the position and the values of the non-zero
entries in P. With this conversion, the lookup and scale
stage (equation 5) becomes:
shift(r,c)(
C[t,r,c]S[I[t,r,c],:,:]) = S ∗P
This is illustrated in Figure 2-right. Now, instead of directly
training I and C, we can train the tensor P with ℓ0-norm
constraints (∥P[:,r,c]∥ℓ0 = s) and then construct I and C
from P. However, ℓ0-norm is a non-continuous function
with zero gradients everywhere. As a workaround, we relax
it to ℓ1-norm. At each iteration of training, to enforce the
sparsity constraint for P[:,r,c], we sort all the entries by their
absolute values and keep the top s entries and zero out the
rest. During training, in addition to the classiﬁcation loss
L we also minimize P
∥P[:,r,c]∥ℓ1 = ∥P∥ℓ1, by adding a
term λ∥P∥ℓ1 to the loss function. The gradient with respect
to the values in P is computed by:
∂(L + λ ∥P∥ℓ1)
∂P + λ sign(P)
∂P is the gradient that is computed through a standard back-propagation. λ is a hyperparameter that adjusts
the trade-off between the CNN loss function and the ℓ1 regularizer. We can also allow s, the sparsity factor, to be different at each spatial position (r, c), and be determined automatically at training time. This can be achieved by applying
a threshold function,
over the values in P during training. We also backpropagate through this threshold function to compute the gradients with respect to P. The derivative of the threshold function is 1 everywhere except at |x| < ϵ, which is 0. Hence,
if any of the entries of P becomes 0 at some iteration, they
stay 0 forever. Using the threshold function, we let each
vector to be a combination of arbitrary vectors. At the end
of the training, the sparsity parameter s at each spatial position (r, c) is determined by the number of non-zero values
in P[:, r, c].
Although the focus of our work is to speedup convolutional layers where most of the computations are, our
lookup based convolution model can also be applied on
fully connected (FC) layers. An FC layer that goes from m
inputs to n outputs can be viewed as a convolutional layer
with input tensor m × 1 × 1 and n weight ﬁlters, each of
size m×1×1. We take the same approach to speedup fully
connected layers.
After training, we convert P to the indices and the coefﬁcients tensors I and C for each layer. At test time, we
follow equation 5 to efﬁciently compute the output of each
convolutional layer.
3.2. Few-shot learning
The shared dictionary in LCNN allows a neural network
to learn from very few training examples on novel categories, which is known as few-shot learning . A good
model for few-shot learning should have two properties:
a) strong priors on the data, and b) few trainable parameters. LCNN has both of these properties. An LCNN trained
on a large dataset of images (e.g. ImageNet ) will have a
rich dictionary D at each convolutional layer. This dictionary provides a powerful prior on visual data. At the time of
ﬁne-tuning for a new set of categories with few training examples, we only update the coefﬁcients in C. This reduces
the number of trainable parameters signiﬁcantly.
In a standard CNN, to use a pre-trained network to classify a set of novel categories, we need to reinitialize the
classiﬁcation layer randomly. This introduces a large number of parameters, on which we don’t have any prior, and
they should be trained solely by a few examples. LCNN,
in contrast, can use the dictionary of the classiﬁcation layer
of the pre-trained model, and therefore only needs to learn
I and C from scratch, which form a much smaller set of
parameters. Furthermore, for all other layers, we only ﬁnetune the coefﬁcients C, i.e.,only update the non-zero entries
of P. Note that the dictionary D is ﬁxed across all layers
during the training with few examples.
3.3. Few-iteration learning
Training very deep neural networks are computationally
expensive and require hundreds of thousands of iterations.
This is mainly due to the complexity of these models. In order to constrain the complexity, we should limit the number
of learnable parameters in the network. LCNN has a suitable setting that allows us to limit the number of learnable
parameters without changing the architecture. This can be
done by transferring the shared dictionaries D from a shallower network to a deeper one.
Not only we can share a dictionary D across layers, but
we can also share it across different network architectures
of different depths. A dictionary D ∈Rm×k can be used
in any convolutional layer with input channel size m in any
CNN architecture. For example, we can train our dictionaries on a shallow CNN and reuse in a deeper CNN with the
same channel size. On the deeper CNN we only need to
train the indices and coefﬁcients tensors I and C.
4. Experiments
We evaluate the accuracy and the efﬁciency of LCNN
under different settings. We ﬁrst evaluate the accuracy and
speedup of our model for the task of object classiﬁcation,
evaluated on the standard image classiﬁcation challenge of
ImageNet, ILSRVC2012 . We then evaluate the accuracy of our model under few-shot setting. We show that
Wen et al. 
XNOR-Net 
LCNN-accurate
Table 1. Comparison of different efﬁcient methods on AlexNet.
The accuracies are classiﬁcation accuracy on the validation set of
ILSVRC2012.
given a set of novel categories with as small as 1 training
example per category, our model is able to learn a classiﬁer
that is both faster and more accurate than the CNN baseline. Finally we show that the dictionaries trained in LCNN
are generalizable and can be transferred to other networks.
This leads to a higher accuracy in small number of iterations
compared to standard CNN.
4.1. Implementation Details
We follow the common way of initializing the convolutional layers by Gaussian distributions introduced in ,
including for the sparse tensor P. We set the threshold in
equation 9 for each layer in such a way that we maintain
the same initial sparsity across all the layers. That is, we
set the threshold of each layer to be ϵ = c · σ, where c
is constant across layers and σ is the standard deviation of
Gaussian initializer for that layer. We use c = 0.01 for
AlexNet and c = 0.001 for ResNet. Similarly, to maintain
the same level of sparsity across layers we need a λ (equation 8) that is proportional to the standard deviation of the
Gaussian initializers. We use λ = λ′ϵ, where λ′ is constant
across layers and ϵ is the threshold value for that layer. We
try λ′ ∈{0.1, 0.2, 0.3} for both AlexNet and ResNet to get
different sparsities in P.
The dictionary size k, the regularizer coefﬁcient λ, and
threshold value ϵ are the three important hyperparameters
for gaining speedup. The larger the dictionary is, the more
accurate (but slower) the model becomes. The size of the
the dictionary for the ﬁrst layer does not need to be very
large as it’s representing a 3-dimensional space. We observed that for the ﬁrst layer, a dictionary size as small as 3
vectors is sufﬁcient for both AlexNet and ResNet. In contrast, fully connected layers of AlexNet are of higher dimensionality and a relatively large dictionary is needed to cover
the input space. We found dictionary sizes 512 and 1024
to be proper for fully connected layers. In AlexNet we use
the same dictionary size across other layers, which we vary
2They have not reported the overall speedup on AlexNet, but only per
layer speedup. 3.1× is the weighted average of their per layer speedups.
3XNOR-Net gets 32× layer-wise speedup on a 32 bit machine. However, since they haven’t binarized the ﬁrst and the last layer (which has
9.64% of the computation), their overall speedup is 8.0×.
XNOR-Net 
LCNN-accurate
Table 2. Comparison of LCNN and XNOR-Net on ResNet-18.
The accuracies are classiﬁcation accuracy on the validation set of
ILSVRC2012.
from 100 to 500 for different experiments. In ResNet, aside
from the very ﬁrst layer, all the other convolutional layers
are grouped into 4 types of ResNet blocks. The dimensionality of input is equal between same ResNet block types,
and is doubled for consecutive different block types. In a
similar way we set the dictionary size for different ResNet
blocks: equal between the same block types, and doubles
for different consecutive block types. We vary the dictionary size of the ﬁrst block from 16 to 128 in different experiments.
4.2. Image Classiﬁcation
In this section we evaluate the efﬁciency and the accuracy of LCNN for the task of image classiﬁcation.
proposed lookup based convolution is general and can be
applied on any CNN architecture. We use AlexNet and
ResNet architectures in our experiments. We use ImageNet challenge ILSVRC2012 to evaluate the accuracy
of our model. We report standard top-1 and top-5 classiﬁcation accuracy on 1K categories of objects in natural scenes.
To evaluate the efﬁciency, we compare the number of ﬂoating point operations as a representation for speedup. The
speed and the accuracy of our model depend on two hyperparameters: 1) k, the dictionary size and 2) λ, which
controls the sparsity of P; i.e.,the average number of dictionary components in the linear combination . One can
Figure 3. Accuracy vs. speedup. By tuning the dictionary size,
LCNN achieves a spectrum of speedups.
top-1 accuracy
# of training examples per category
(a) cats, sofas and bicycles excluded
top-1 accuracy
# of training examples per category
(b) 10 random categories excluded
Figure 4. Comparison between the performance of LCNN and CNN baseline on few-shot learning, for {1, 2, 4} examples per category. In
(a) all cats (7 categories), sofas (1 category) and bicycles (2 categories) are held out for few-shot learning. In (b), 10 random categories are
held out for few-shot learning. We repeat sampling the 10 random categories 5 times to avoid over-ﬁtting to a speciﬁc sampling.
set a trade-off between the accuracy and the efﬁciency of
LCNN by adjusting these two parameters. We compare our
model with several baselines: 1- XNOR-Net , which
reduces the precision of weights and outputs to 1-bit, and
therefore multiplications can be replaced by binary operations. In XNOR-Net, all the layers are binarized except the
ﬁrst and the last layer (in AlexNet, they contain 9.64% of
the computation). 2- Wen et al. , which speeds up the
convolutions by sparsifying the weight ﬁlters.
Table 1 compares the top-1 and top-5 classiﬁcation accuracy of LCNN with baselines on AlexNet architecture. It
shows that with small enough dictionaries and sparse linear
combinations, LCNN offers 37.6× speedup with the accuracy of XNOR-Net. On the other hand, if we set the dictionaries to be large enough, LCNN can be as accurate as
slower models like Wen et al. In LCNN-fast, the dictionary
size of the mid-layer convolutions is 30 and for the fully
connected layers is 512. In LCNN-accurate, the mid-layer
convolutions have a dictionary of size 500 and the size of
dictionary in fully connected layers is 1024. The reguralizer constant (Section 4.1) λ′ for LCNN-fast and LCNNaccurate is 0.3 and 0.1, respectively.
Depending on the dictionary size and λ′, LCNN can
achieve various speedups and accuracies. Figure 3 shows
different accuracies vs. speedups that our model can
achieve. The accuracy is computed by top-1 measure and
the speedup is relative to the original CNN model. It is interesting to see that the trend is nearly linear. The best ﬁtted
line has a slope of −3.08, i.e.,for each one percent accuracy
that we sacriﬁce in top-1, we gain 3.08 more speedup.
We also evaluate the performance of LCNN on ResNet-
18 architecture. ResNet-18 is a compact architecture, which
has 5× fewer parameters in compare to AlexNet while it
achieves 12.7% higher top-1 accuracy.
That makes it a
much more challenging architecture for further compression. Yet we show that we can gain large speedups with
a few points drop in the accuracy.
Table 2 compares
the accuracy of LCNN, XNOR-Net , and the original
model (CNN). LCNN-fast is getting the same accuracy as
XNOR-Net while getting a much larger speedup. Moreover,
LCNN-accurate is getting a much higher accuracy yet maintaining a relatively large speedup. LCNN-fast has dictionaries of size 16, 32, 64, and 128 for different block types.
LCNN-accuracte has larger dictionaries: 128, 256, 512 and
1024 for different block types.
4.3. Few-shot Learning
In this section we evaluate the performance of LCNN on
the task of few-shot learning. To evaluate the performance
of LCNN on this task, we split the categories of ImageNet
challenge ILSVRC2012 into two sets: i) base categories,
a set of 990 categories which we use for pre-training, and
ii) novel categories, a set of 10 categories that we use for
few-shot learning.We do experiments under 1, 2, and 4 samples per category. We take two strategies for splitting the
categories. One is random splitting, where we randomly
split the dataset into 990 and 10 categories. We repeat the
random splitting 5 times and report the average over all. The
other strategy is to hold out all cats (7 categories), bicycles
(2 categories) and sofa (1 category) for few-shot learning,
and use the other 990 categories for pre-training. With this
strategy we make sure that base and novel categories do not
share similar objects, like different breeds of cats. For each
split, we repeat the random sampling of 1, 2, and 4 training images per category 20 times, and get the average over
all. Repeating the random sampling of the few examples is
crucial for any few-shot learning experiment, since a model
can easily overﬁt to a speciﬁc sampling of images.
We compare the performance of CNN and LCNN on
few-shot learning in Figure 4. We ﬁrst train an original
AlexNet and an LCNN AlexNet on all training images of
base categories (990 categories, 1000 images per category).
We then replace the 990-way classiﬁcation layer with a randomly initialized 10-way linear classiﬁer. In CNN, this produces 10×4096 randomly initialized weights, on which we
don’t have any prior. These parameters need to be trained
merely from the few examples. In LCNN, however, we
transfer the dictionary trained in the 990-way classiﬁcation
layer to the new 10-way classiﬁer. This reduces the number
of randomly initialized parameters by at least a factor of 4.
We use AlexNet LCNN-accurate model (same as the one in
Table 1) for few-shot learning. At the time of ﬁne-tuning for
few-shot categories, we keep the dictionaries in all layers
ﬁxed and only ﬁne-tune the sparse P tensor. This reduces
the total number of parameters that need to be ﬁne-tuned by
a factor of 14×. We use different learning rates η and η′ for
the randomly initialized classiﬁcation layer (which needs to
be fully trained) and the previous pre-trained layers (which
only need to be ﬁne-tuned). We tried η′ = η, η′ =
100 and η′ = 0 for both CNN and LCNN, then picked
the best for each conﬁguration.
Figure 4 shows the top-1 accuracies of our model and the
baseline in the two splitting strategies of our few-shot learning experiment. In Figure 4 (a) we are holding out all cat,
sofa, and bicycle categories (10 categories in total) for fewshot learning. LCNN is beating the baseline consistently
in {1, 2, 4} examples per category. Figure 4 (b) shows the
comparison in the random splitting strategy. We repeat randomly splitting the categories into 990 and 10 categories 5
times, and report the average over all. Here LCNN gets a
larger improvement in the top-1 accuracy compared to the
baseline for {1, 2, 4} images per category.
4.4. Few-iteration Learning
In section 3.3 we discussed that the dictionaries in LCNN
can be transferred from a shallower network to a deeper one.
As a result, one can train fewer parameters–only I and C–
in the deeper network with few iterations obtaining a higher
test accuracy compared to a standard CNN. In this experiment we train a ResNet with 1 block of each type, 10 layers
total. We then transfer the dictionaries of each layer to its
corresponding layer of ResNet-18 (with 18 layers). After
transfer, we keep the dictionaries ﬁxed. We show that we
get higher accuracy in small number of iterations compared
to standard CNN. Figure 5 illustrates the learning curves on
top-1 accuracy for both LCNN and standard CNN. The test
accuracy of LCNN is 16.2% higher than CNN at iteration
10K. The solid lines denote the training accuracy and the
dashed lines denote the test accuracy.
top-1 accuracy
# of iterations
LCNN train
Figure 5. LCNN can obtain higher accuracy on few iterations
by transferring the dictionaries D from a shallower architecture.
This ﬁgure illustrates the learning curves on top-1 accuracy for
both LCNN and standard CNN. The accuracy of LCNN is 16.2%
higher than CNN at iteration 10K.
5. Conclusion
With recent advancements in virtual reality, augmented
reality, and smart wearable devices, the need for getting
the state of the art deep learning algorithms onto these resource constrained compute platforms increases. Porting
state of the art deep learning algorithms to resource constrained compute platforms is extremely challenging. We
introduce LCNN, a lookup-based convolutional neural network that encodes convolutions by few lookups to a dictionary that is trained to cover the space of weights in CNNs.
Training LCNN involves jointly learning a dictionary and a
small set of linear combinations. The size of the dictionary
naturally traces a spectrum of trade-offs between efﬁciency
and accuracy.
LCCN enables efﬁcient inference; our experimental results on ImageNet challenge show that LCNN can offer
3.2× speedup while achieving 55.1% top-1 accuracy using AlexNet architecture. Our fastest LCNN offers 37.6×
speed up over AlexNet while maintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups at inference, but it also enables efﬁcient training. On-device training of deep learning methods requires algorithms that can
handle few-shot and few-iteration constrains. LCNN can
simply deal with these problems because our dictionaries
are architecture agnostic and transferable across layers and
architectures, enabling us to only learn few linear combination weights. Our future work involves exploring lowprecision dictionaries as well as compact data structures for
the dictionaries.
Acknowledgments:
This work is in part supported
by ONR N00014-13-1-0720, NSF IIS-1338054, NSF-
1652052, NRI-1637479, Allen Distinguished Investigator
Award, and the Allen Institute for Artiﬁcial Intelligence.