THEORY OF COMPUTING, Volume 8 , pp. 121–164
www.theoryofcomputing.org
RESEARCH SURVEY
The Multiplicative Weights Update Method:
A Meta-Algorithm and Applications
Sanjeev Arora∗
Elad Hazan
Satyen Kale
Received: July 22, 2008; revised: July 2, 2011; published: May 1, 2012.
Algorithms in varied ﬁelds use the idea of maintaining a distribution over a
certain set and use the multiplicative update rule to iteratively change these weights. Their
analyses are usually very similar and rely on an exponential potential function.
In this survey we present a simple meta-algorithm that uniﬁes many of these disparate
algorithms and derives them as simple instantiations of the meta-algorithm. We feel that
since this meta-algorithm and its analysis are so simple, and its applications so broad, it
should be a standard part of algorithms courses, like “divide and conquer.”
ACM Classiﬁcation: G.1.6
AMS Classiﬁcation: 68Q25
Key words and phrases: algorithms, game theory, machine learning
Introduction
The Multiplicative Weights (MW) method is a simple idea which has been repeatedly discovered in ﬁelds
as diverse as Machine Learning, Optimization, and Game Theory. The setting for this algorithm is the
following. A decision maker has a choice of n decisions, and needs to repeatedly make a decision and
obtain an associated payoff. The decision maker’s goal, in the long run, is to achieve a total payoff which
is comparable to the payoff of that ﬁxed decision that maximizes the total payoff with the beneﬁt of
∗This project was supported by David and Lucile Packard Fellowship and NSF grants MSPA-MCS 0528414 and CCR-
2012 Sanjeev Arora, Elad Hazan and Satyen Kale
Licensed under a Creative Commons Attribution License
DOI: 10.4086/toc.2012.v008a006
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
hindsight. While this best decision may not be known a priori, it is still possible to achieve this goal by
maintaining weights on the decisions, and choosing the decisions randomly with probability proportional
to the weights. In each successive round, the weights are updated by multiplying them with factors which
depend on the payoff of the associated decision in that round. Intuitively, this scheme works because it
tends to focus higher weight on higher payoff decisions in the long run.
This idea lies at the core of a variety of algorithms. Some examples include: the Ada Boost algorithm
in machine learning ; algorithms for game playing studied in economics (see references later), the
Plotkin-Shmoys-Tardos algorithm for packing and covering LPs , and its improvements in the case
of ﬂow problems by Young , Garg-K¨onemann , Fleischer and others; methods for
convex optimization like exponentiated gradient (mirror descent), Lagrangian multipliers, and subgradient
methods, Impagliazzo’s proof of the Yao XOR lemma , etc. The analysis of the running time uses a
potential function argument and the ﬁnal running time is proportional to 1/ε2.
It has been clear to several researchers that these results are very similar. For example Khandekar’s
Ph. D. thesis makes this point about the varied applications of this idea to convex optimization.
The purpose of this survey is to clarify that many of these applications are instances of the same, more
general algorithm (although several specialized applications, such as , require additional technical
work). This meta-algorithm is very similar to the “Hedge” algorithm from learning theory . Similar
algorithms have been independently rediscovered in many other ﬁelds; see below. The advantage of
deriving the above algorithms from the same meta-algorithm is that this highlights their commonalities as
well as their differences. To give an example, the algorithms of Garg-K¨onemann were felt to be
quite different from those of Plotkin-Shmoys-Tardos . In our framework, they can be seen as a clever
trick for “width reduction” for the Plotkin-Shmoys-Tardos algorithms (see Section 3.4).
We feel that this meta-algorithm and its analysis are simple and useful enough that they should
be viewed as a basic tool taught to all algorithms students together with divide-and-conquer, dynamic
programming, random sampling, and the like. Note that the multiplicative weights update rule may be
seen as a “constructive” version of LP duality—equivalently, von Neumann’s minimax theorem in game
theory—and it gives a fairly concrete method for competing players to arrive at a solution/equilibrium
(see Section 3.2). This may be an appealing feature in introductory algorithms courses, since the standard
algorithms for LP such as simplex, ellipsoid, or interior point lack such a game-theoretic interpretation.
Furthermore, it is a convenient stepping point to many other topics that rarely get mentioned in algorithms
courses, including online algorithms (see the basic scenario in Section 1.1) and machine learning.
Furthermore our proofs seem easier and cleaner than the entropy-based proofs for the same results in
machine learning (although the proof technique we use here has been used before, see for example Blum’s
survey ).
The current paper is chieﬂy a survey. It introduces the main algorithm, gives a few variants (mostly
having to do with the range in which the payoffs lie), and surveys the most important applications—often
with complete proofs. Note however that this survey does not cover all applications of the technique, as
several of these require considerable additional technical work which is beyond the scope of this paper.
We have provided pointers to some such applications which use the multiplicative weights technique at
their core without going into more details. There are also a few small results that appear to be new, such
as the variant of the Garg-K¨onemann algorithm in Section 3.4 and the lower bound in Section 4.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
Related work.
An algorithm similar in ﬂavor to the Multiplicative Weights algorithm was proposed in
game theory in the early 1950’s . Following Brown , this algorithm was called “Fictitious
Play”: at each step each player observes actions taken by his opponent in previous stages, updates his
beliefs about his opponents’ strategies, and chooses the pure best response against these beliefs. In the
simplest case, the player simply assumes that the opponent is playing from a stationary distribution and
sets his current belief of the opponent’s distribution to be the empirical frequency of the strategies played
by the opponent. This simple idea (which was shown to lead to optimal solutions in the limit in various
cases) led to numerous developments in economics, including Arrow-Debreu General Equilibrium theory
and more recently, evolutionary game theory. Grigoriadis and Khachiyan showed how a randomized
variant of “Fictitious Play” can solve two player zero-sum games efﬁciently. This algorithm is precisely
the multiplicative weights algorithm. It can be viewed as a soft version of ﬁctitious play, when the player
gives higher weight to the strategies which pay off better, and chooses her strategy using these weights
rather than choosing the best response strategy.
In Machine Learning, the earliest form of the multiplicative weights update rule was used by Littlestone in his well-known Winnow algorithm . It is somewhat reminiscent of the older perceptron
learning algorithm of Minsky and Papert . The Winnow algorithm was generalized by Littlestone and
Warmuth in the form of the Weighted Majority algorithm, and later by Freund and Schapire in the
form of the Hedge algorithm . We note that most relevant papers in learning theory use an analysis
that relies on entropy (or its cousin, Kullback-Leibler divergence) calculations. This analysis is closely
related to our analysis, but we use exponential functions instead of the logarithm, or entropy, used in
those papers. The underlying calculation is the same: whereas we repeatedly use the fact that ex ≈1+x
when |x| is small, they use the fact that ln(1 + x) ≈x. We feel that our approach is cleaner (although
the entropy based approach yields somewhat tighter bounds that are useful in some applications, see
Section 2.2).
Other applications of the multiplicative weights algorithm in computational geometry include Clarkson’s algorithm for linear programming with a bounded number of variables in linear time .
Following Clarkson, Br¨onnimann and Goodrich use similar methods to ﬁnd Set Covers for hypergraphs
with small VC dimension .
The weighted majority algorithm as well as more sophisticated versions have been independently
discovered in operations research and statistical decision making in the context of the On-line decision
problem; see the surveys of Cover , Foster and Vohra , and also Blum who includes
applications of weighted majority to machine learning. A notable algorithm, which is different from but
related to our framework, was developed by Hannan in the 1950’s . Kalai and Vempala showed how
to derive efﬁcient algorithms via methods similar to Hannan’s . We show how Hannan’s algorithm
with the appropriate choice of parameters yields the multiplicative update decision rule in Section 3.8.
Within computer science, several researchers have previously noted the close relationships between
multiplicative update algorithms used in different contexts. Young notes the connection between
fast LP algorithms and Raghavan’s method of pessimistic estimators for derandomization of randomized
rounding algorithms; see our Section 3.5. Klivans and Servedio relate boosting algorithms in learning
theory to proofs of Yao’s XOR Lemma; see our Section 3.6. Garg and Khandekar describe a common
framework for convex optimization problems that contains Garg-K¨onemann and Plotkin-Shmoys-Tardos
as subcases.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
To the best of our knowledge our framework is the most general and, arguably, the simplest. We
readily acknowledge the inﬂuence of all previous papers (especially Young and Freund-Schapire )
on the development of our framework. We emphasize again that we do not claim that every algorithm
designed using the multiplicative update idea ﬁts in our framework, just that most do.
Paper organization.
We proceed to deﬁne the illustrative weighted majority algorithm in this section.
In Section 2 we describe the general MW meta-algorithm, followed by numerous and varied applications
in Section 3. In Section 4 we give lower bounds, followed by the more general matrix MW algorithm in
Section 5.
The weighted majority algorithm
Now we brieﬂy illustrate the weighted majority algorithm in a simple and concrete setting, which will
naturally lead to our generalized meta-algorithm. This is known as the Prediction from Expert Advice
Imagine the process of picking good times to invest in a stock. For simplicity, assume that there is a
single stock of interest, and its daily price movement is modeled as a sequence of binary events: up/down.
(Below, this will be generalized to allow non-binary events.) Each morning we try to predict whether the
price will go up or down that day; if our prediction happens to be wrong we lose a dollar that day, and if
it’s correct, we lose nothing.
The stock movements can be arbitrary and even adversarial. To balance out this pessimistic
assumption, we assume that while making our predictions, we are allowed to watch the predictions of n
“experts.” These experts could be arbitrarily correlated, and they may or may not know what they are
talking about. The algorithm’s goal is to limit its cumulative losses (i. e., bad predictions) to roughly the
same as the best of these experts. At ﬁrst sight this seems an impossible goal, since it is not known until
the end of the sequence who the best expert was, whereas the algorithm is required to make predictions
all along.
Indeed, the ﬁrst algorithm one thinks of is to compute each day’s up/down prediction by going with
the majority opinion among the experts that day. But, this algorithm doesn’t work because a majority of
experts may be consistently wrong on every single day.
The weighted majority algorithm corrects the trivial algorithm. It maintains a weighting of the experts.
Initially all have equal weight. As time goes on, some experts are seen as making better predictions than
others, and the algorithm increases their weight proportionately. The algorithm’s prediction of up/down
for each day is computed by going with the opinion of the weighted majority of the experts for that day.
Theorem 1.1. After T steps, let mi(T) be the number of mistakes of expert i and M(T) be the number of
mistakes our algorithm has made. Then we have the following bound for every i:
M(T) ≤2(1+η)mi(T) + 2lnn
In particular, this holds for i which is the best expert, i. e., having the least mi(T).
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
Weighted majority algorithm
Initialization: Fix an η ≤1
2. With each expert i, associate the weight wi(1) := 1.
For t = 1,2,...,T:
1. Make the prediction that is the weighted majority of the experts’ predictions based on the
weights w1(t),...,wn(t). That is, predict “up” or “down” depending on which prediction has a
higher total weight of experts advising it (breaking ties arbitrarily).
2. For every expert i who predicts wrongly, decrease his weight for the next round by multiplying
it by a factor of (1−η):
wi(t+1) = (1−η)wi(t)
(update rule).
When mi(T) ≫(2/η)lnn we see that the number of mistakes made by the algorithm is
bounded from above by roughly 2(1+η)mi(T), i. e., approximately twice the number of mistakes made
by the best expert. This is tight for any deterministic algorithm. However, the factor of 2 can be removed
by substituting the above deterministic algorithm by a randomized algorithm that predicts according to
the majority opinion with probability proportional to its weight. (In other words, if the total weight of
the experts saying “up” is 3/4 then the algorithm predicts “up” with probability 3/4 and “down” with
probability 1/4.) Then the number of mistakes after T steps is a random variable and the claimed upper
bound holds for its expectation (see Section 2 for more details).
Proof. A simple induction shows that wi(t+1) = (1−η)mi(t). Let Φ(t) = ∑i wi(t) (“the potential function”).
Thus Φ(1) = n. Each time we make a mistake, the weighted majority of experts also made a mistake, so at
least half the total weight decreases by a factor 1−η. Thus, the potential function decreases by a factor
of at least (1−η/2):
Φ(t+1) ≤Φ(t)
= Φ(t)(1−η/2).
Thus simple induction gives Φ(T+1) ≤n(1 −η/2)M(T). Finally, since Φ(T+1) ≥wi(T+1) for all i, the
claimed bound follows by comparing the above two expressions and using the fact that
−ln(1−η) ≤η +η2
since η < 1/2.
The beauty of this analysis is that it makes no assumption about the sequence of events: they
could be arbitrarily correlated and could even depend upon our current weighting of the experts. In
this sense, this algorithm delivers more than initially promised, and this lies at the root of why (after
obvious generalization) it can give rise to the diverse algorithms mentioned earlier. In particular, the
scenario where the events are chosen adversarially resembles a zero-sum game, which we consider later
in Section 3.2.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
The Multiplicative Weights algorithm
In the general setting, we have a set of n decisions and in each round, we are required to select one
decision from the set. In each round, each decision incurs a certain cost, determined by nature or an
adversary. All the costs are revealed after we choose our decision, and we incur the cost of the decision
we chose. For example, in the prediction from expert advice problem, each decision corresponds to a
choice of an expert, and the cost of an expert is 1 if the expert makes a mistake, and 0 otherwise.
To motivate the Multiplicative Weights (MW) algorithm, consider the na¨ıve strategy that, in each
iteration, simply picks a decision at random. The expected penalty will be that of the “average” decision.
Suppose now that a few decisions are clearly better in the long run. This is easy to spot as the costs are
revealed over time, and so it is sensible to reward them by increasing their probability of being picked in
the next round (hence the multiplicative weight update rule).
Intuitively, being in complete ignorance about the decisions at the outset, we select them uniformly
at random. This maximum entropy starting rule reﬂects our ignorance. As we learn which ones are the
good decisions and which ones are bad, we lower the entropy to reﬂect our increased knowledge. The
multiplicative weight update is our means of skewing the distribution.
We now set up some notation. Let t = 1,2,...,T denote the current round, and let i be a generic
decision. In each round t, we select a distribution p(t) over the set of decisions, and select a decision i
randomly from it. At this point, the costs of all the decisions are revealed by nature in the form of the
vector m(t) such that decision i incurs cost mi(t). We assume that the costs lie in the range [−1,1]. This is
the only assumption we make on the costs; nature is completely free to choose the cost vector as long
as these bounds are respected, even with full knowledge of the distribution that we choose our decision
The expected cost to the algorithm for sampling a decision i from the distribution p(t) is
Ei∈p(t)[mi(t)] = m(t) ·p(t).
The total expected cost over all rounds is therefore ∑T
t=1 m(t) ·p(t). Just as before, our goal is to design
an algorithm which achieves a total expected cost not too much more than the cost of the best decision
in hindsight, viz. mini ∑T
t=1 mi(t). Consider the following algorithm, which we call the Multiplicative
Weights Algorithm. This algorithm has been studied before as the prod algorithm of Cesa-Bianchi,
Mansour, and Stoltz , and Theorem 2.1 can be seen to follow from Lemma 2 in .
The following theorem—completely analogous to Theorem 1.1—bounds the total expected cost of
the Multiplicative Weights algorithm (given in Figure 1) in terms of the total cost of the best decision:
Theorem 2.1. Assume that all costs mi(t) ∈[−1,1] and η ≤1/2. Then the Multiplicative Weights
algorithm guarantees that after T rounds, for any decision i, we have
m(t) ·p(t) ≤
|mi(t)|+ lnn
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
Multiplicative Weights algorithm
Initialization: Fix an η ≤1
2. With each decision i, associate the weight wi(1) := 1.
For t = 1,2,...,T:
1. Choose decision i with probability proportional to its weight wi(t). I. e., use the distribution
over decisions p(t) = {w1(t)/Φ(t),...,wn(t)/Φ(t)} where Φ(t) = ∑i wi(t).
2. Observe the costs of the decisions m(t).
3. Penalize the costly decisions by updating their weights as follows: for every decision i, set
wi(t+1) = wi(t)(1−ηmi(t))
Figure 1: The Multiplicative Weights algorithm.
Proof. The proof is along the lines of the earlier one, using the potential function Φ(t) = ∑i wi(t):
Φ(t+1) = ∑
wi(t)(1−ηmi(t))
= Φ(t) −ηΦ(t)∑
mi(t)pi(t)
= Φ(t)(1−ηm(t) ·p(t))
≤Φ(t) exp(−ηm(t) ·p(t)).
Here, we used the fact that pi(t) = wi(t)/Φ(t). Thus, by induction, after T rounds, we have
Φ(T+1) ≤Φ(1) exp
m(t) ·p(t)
m(t) ·p(t)
Next we use the following facts, which follow immediately from the convexity of the exponential
(1−η)x ≤(1−ηx)
if x ∈ ,
(1+η)−x ≤(1−ηx)
if x ∈[−1,0].
Since mi(t) ∈[−1,1], we have for every decision i,
Φ(T+1) ≥wi(T+1) = ∏
(1−ηmi(t)) ≥(1−η)∑≥0 mi(t) ·(1+η)−∑<0 mi(t),
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
where the subscripts “≥0” and “< 0” in the summations refer to the rounds t where mi(t) is ≥0 and < 0
respectively. Taking logarithms in equations (2.2) and (2.4) we get:
m(t) ·p(t) ≥∑
mi(t) ln(1−η)−∑
mi(t) ln(1+η).
Negating, rearranging, and scaling by 1/η:
m(t) ·p(t) ≤lnn
mi(t) ln(1+η)
mi(t)(η +η2)+ 1
mi(t)(η −η2)
In the second inequality we used the facts that
ln(1+η) ≥η −η2
for η ≤1/2.
Corollary 2.2. The Multiplicative Weights algorithm also guarantees that after T rounds, for any
distribution p on the decisions,
m(t) ·p(t) ≤
(m(t) +η|m(t)|)·p+ lnn
where |m(t)| is the vector obtained by the taking the coordinate-wise absolute value of m(t).
Proof. This corollary follows immediately from Theorem 2.1, by taking a convex combination of the
inequalities for all decisions i with the distribution p.
Updating with exponential factors: the Hedge algorithm
In our description of the MW algorithm, the update rule uses multiplication by a linear function of the
cost (speciﬁcally, (1−ηmi(t)) for expert i). In several other incarnations of MW algorithm, notably the
Hedge algorithm of Freund and Schapire , an exponential factor is used instead. This update rule is
the following:
wi(t+1) = wi(t) ·exp(−ηmi(t)).
As can be seen from the analysis of the MW algorithm, Hedge is not very different. The bound we obtain
for Hedge is slightly different however. While most of the applications we present in the rest of the paper
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
can be derived using Hedge as well with a little extra calculation, some applications, such as the ones in
Sections 3.3 and 3.5, explicitly need the MW algorithm rather than Hedge to obtain better bounds. Here,
we state the bound obtained for Hedge without proof—the analysis is on the same lines as before. The
only difference is that instead of the inequalities (2.3), we use the inequality
exp(−ηx) ≤1−ηx+η2x2
if |ηx| ≤1.
Theorem 2.3. Assume that all costs mi(t) ∈[−1,1] and η ≤1. Then the Hedge algorithm guarantees
that after T rounds, for any decision i, we have
m(t) ·p(t) ≤
(m(t))2 ·p(t) + lnn
Here, (m(t))2 is the vector obtained by taking coordinate-wise square of m(t).
This guarantee is very similar to the one in Theorem 2.1, with one important difference: the term
multiplying η is a loss which depends on the algorithm’s distribution. In Theorem 2.1, this additional
term depends on the loss of the best decision in hindsight. For some applications the latter guarantee is
stronger (see Section 3.3).
Proof via KL-divergence
In this section, we give an alternative proof of Theorem 2.1 based on the Kullback-Leibler (KL) divergence,
or relative entropy. While this proof is somewhat more complicated, it gives a good insight into why the
MW algorithm works: the reason is that it tends to reduce the KL-divergence to the optimal solution.
Another reason for giving this proof is that it yields a more nuanced form of the MW algorithm that is
useful in some applications (such as the construction of hard-core sets, see Section 3.7). Readers may
skip this section without loss of continuity.
For two distributions p and q on the decision set, the relative entropy between them is
RE(p ∥q) = ∑
where the term pi ln(pi/qi) is deﬁned to be zero if pi = 0 and inﬁnite if pi ̸= 0 , qi = 0.
Consider the following twist on the basic decision-making problem from Section 2. Fix a convex
subset of distributions over decisions, P (note: the basic setting is recovered when P is the set of all
distributions). In each round t, the decision-maker is required to produce a distribution p(t) ∈P. At that
point, the cost vector m(t) is revealed and the decision-maker suffers cost m(t) ·p(t). Since we make the
restriction that p(t) ∈P, we now want to compare the total cost of the decision-maker to the cost of the
best ﬁxed distribution in P. Consider the algorithm in Figure 2.
Note that in the special case when P is the set of all distributions on the decisions, this algorithm is
exactly the basic MW algorithm presented in Figure 1. The relative entropy projection step ensures that
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
Multiplicative Weights Update algorithm with Restricted Distributions
Initialization: Fix a η ≤1
2. Set p(1) to be be an arbitrary distribution in P initialized to 1.
For t = 1,2,...,T:
1. Choose decision i by sampling from p(t).
2. Observe the costs of the decisions m(t).
3. Compute the probability vector ˆp(t+1) using the usual multiplicative update rule: for every
pi(t+1) = pi(t)(1−ηmi(t))/Φ(t)
where Φ(t) is the normalization factor to make ˆp(t+1) a distribution.
4. Set p(t+1) to be the relative entropy projection of ˆp(t) on the set P, i. e.,
p(t+1) = argmin
p∈P RE(p ∥ˆp(t)).
Figure 2: The Multiplicative Weights algorithm with Restricted Distributions.
we always choose a distribution in P. This projection is a convex program since relative entropy is convex
and P is a convex set, and hence can be computed using standard convex programming techniques.
We now prove a bound on the total cost of the algorithm (compare to Corollary 2.2). Note that in
the basic setting when P is the set of all distributions, the bound given below is tighter than the one in
Theorem 2.1.
Theorem 2.4. Assume that all costs mi(t) ∈[−1,1] and η ≤1/2. Then the Multiplicative Weights
algorithm with Restricted Distributions guarantees that after T rounds, for any p ∈P, we have
m(t) ·p(t) ≤
(m(t) +η|m(t)|)·p+ RE(p ∥p(1))
where |m(t)| is the vector obtained by taking the coordinate-wise absolute value of m(t).
Proof. We use the relative entropy between p and p(t), RE(p ∥p(t)) := ∑i pi ln(pi/pi(t)) as a “potential”
function. We have
RE(p ∥ˆpt+1)−RE(p ∥p(t)) = ∑
pi ln pi(t)
(1−ηmi(t))
pimi(t) +ln(1+η)∑
pimi(t) +lnΦ(t)
≤η(m(t) +η|m(t)|)·p+lnΦ(t).
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
The ﬁrst inequality above follows from (2.3), and the second follows from (2.5). Next, we have
lnΦ(t) = ln
pi(t)(1−ηmi(t))
= ln[1−ηm(t) ·p(t)] ≤−ηm(t) ·p(t)
since ln(1−x) ≤−x for x < 1. Thus, we get
RE(p ∥ˆpt+1)−RE(p ∥p(t)) ≤η(m(t) +η|m(t)|)·p−η(m(t) ·p(t)).
This inequality essentially says that if the cost of the algorithm in round t, m(t) ·p(t), is signiﬁcantly larger
than the cost of the comparator, m(t) ·p, then ˆpt+1 moves closer to p (in the relative entropy distance)
than p(t).
Now, projection on the set P using the relative entropy as a distance function is a Bregman projection,
and thus it satisﬁes the following Generalized Pythagorean inequality (see, e. g., ), for any p ∈P:
RE(p ∥p(t+1))+RE(p(t+1) ∥ˆp(t+1)) ≤RE(p ∥ˆp(t+1)).
I. e., the projection step only brings the distribution closer to p. Since relative entropy is always nonnegative, we have RE(p(t+1) ∥ˆp(t+1)) ≥0 and so
RE(p ∥p(t+1))−RE(p ∥p(t)) ≤η(m(t) +η|m(t)|)·p−η(m(t) ·p(t)).
Summing up from t = 1 to T, dividing by η, and simplifying using the fact that RE(p ∥p(T+1)) is
non-negative, we get the stated bound.
Gains instead of losses
There are situations where it makes more sense for the vector m(t) to specify gains for each expert rather
than losses. Now our goal is to get as much total expected payoff as possible in comparison to the total
payoff of the best expert. We can get an algorithm for this case simply by running the Multiplicative
Weights algorithm using the cost vector −m(t).
The resulting algorithm is identical, and the following theorem follows directly from Theorem 2.1 by
simply negating the quantities:
Theorem 2.5. Assume that all gains mi(t) ∈[−1,1] and η ≤1. Then the Multiplicative Weights algorithm
(for gains) guarantees that after T rounds, for any expert i, we have
m(t) ·p(t) ≥
|mi(t)|−lnn
We also have the following immediate corollary, corresponding to Corollary 2.2:
Corollary 2.6. The Multiplicative Weights algorithm also guarantees that after T rounds, for any
distribution p on the decisions,
m(t) ·p(t) ≥
(m(t) −η|m(t)|)·p−lnn
where |m(t)| is the vector obtained by the taking the coordinate-wise absolute value of m(t).
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
Applications
Typically, the Multiplicative Weights method is applied in the following manner. A prototypical example
is to solve a constrained optimization problem. We then let a decision represent each constraint in the
problem, with costs speciﬁed by the points in the domain of interest. For a given point, the cost of a
decision is made proportional to how well the corresponding constraint is satisﬁed on the point. This
might seem counterintuitive, but recall that we reduce a decision’s weight depending on its penalty, and if
a constraint is well satisﬁed on points so far we would like its weight to be smaller, so that the algorithm
focuses on constraints that are poorly satisﬁed.
In many applications (though not all) the choice of point is also under our control. Typically we will
need to generate the maximally adversarial point, i. e., the point that maximizes the expected cost. Then
the overall algorithm consists of two subprocedures: an “oracle” for generating the maximally adversarial
point at each step, and the MW algorithm for updating the weights of the decisions. With this intuition,
we can describe the following applications.
Learning a linear classiﬁer: the Winnow algorithm
To the best of our knowledge, the ﬁrst time multiplicative weight updates were used was in the Winnow
algorithm of Littlestone . This is an algorithmic technique used in machine learning to learn linear
classiﬁers. Equivalently, this can also be seen as solving a linear program.
The setup is as follows. We are given m labeled examples, (a1,ℓ1),(a2,ℓ2),...,(am,ℓm) where
aj ∈Rn are feature vectors, and ℓj ∈{−1,1} are their labels. Our goal is to ﬁnd non-negative weights
such that for any example, the sign of the weighted combination of the features matches its labels, i. e.,
ﬁnd x ∈Rn with xi ≥0 such that for all j = 1,2,...,m, we have sgn(aj ·x) = ℓj. Equivalently, we require
that ℓjaj ·x ≥0 for all j. Without loss of generality, we may assume that the weights sum to 1 so that
they form a distribution, i. e., 1·x = 1, where 1 is the all 1’s vector.
Thus, for notational convenience, if we redeﬁne aj to be ℓjaj, then the problem reduces to ﬁnding a
solution to the following LP:
∀j = 1,2,...,m : aj ·x ≥0,
∀i : xi ≥0.
Note this is a quite general form of LP, and many commonly seen LPs can be reduced to this form.
Now suppose there is a large-margin solution to this problem. I. e., there is an ε > 0 and a distribution
x⋆so that for all j, we have aj ·x⋆≥ε. We now give an algorithm based on MW to solve the LP above.
Deﬁne ρ = max j ∥aj∥∞.
We run the MW algorithm in the gain form (see Section 2.3) with η = ε/(2ρ). The decisions are
given by the n features, and gains are speciﬁed by the m examples. The gain for feature i for example j is
ai j/ρ. Note that these gains lie in the range [−1,1] as required.
In each round t, let x to be the distribution p(t) generated by the MW algorithm. Now, we look for a
misclassiﬁed example, i. e., an example j such that aj ·x < 0. If no such constraint exists, we are done
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
and we can stop. Otherwise, if j is a misclassiﬁed example, then it speciﬁes the gains for round t. Note
that the gain in round t is m(t) ·p(t) = (1/ρ)a j ·x < 0, whereas for the solution x⋆, we have
m(t) ·x⋆= 1
ρ aj ·x⋆≥ε
We keep running the MW algorithm until we ﬁnd a good solution (i. e., one that classiﬁes all examples
correctly).
To get a bound on the number of iterations until we ﬁnd a good solution, we apply Corollary 2.6 with
p = x⋆. Using the trivial bound η|m(t)|·p ≤η, we have
m(t) ·p(t) ≥
(m(t) −η|m(t)|)·p−lnn
2ρ T −2ρ lnn
which implies that T < 4ρ2 ln(n)/ε2. Thus, in at most
4ρ2 ln(n)/ε2
iterations, we ﬁnd a good solution.
Solving zero-sum games approximately
We show how our general algorithm above can be used to approximately solve zero-sum games. (This is a
duplication of the results of Freund and Schapire , who gave the same algorithm but a different proof
of convergence that used KL-divergence. Furthermore, convergence of simple algorithms to zero-sum
game equilibria were studied earlier in .)
Let A be the payoff matrix of a ﬁnite 2-player zero-sum game, with n rows (the number of columns
will play no role). When the row player plays strategy i and the column player plays strategy j, then the
payoff to the column player is A(i, j) := Ai j. We assume that A(i, j) ∈ . If the row player chooses
his strategy i from a distribution p over the rows, then the expected payoff to the column player for
choosing a strategy j is A(p, j) := Ei∈p[A(i, j)]. Thus, the best response for the column player is the
strategy j which maximizes this payoff. Similarly, if the column player chooses his strategy j from a
distribution q over the columns, then the expected payoff he gets if the row player chooses the strategy i is
A(i,q) := Ej∈q[A(i, j)]. Thus, the best response for the row player is the strategy i which minimizes this
payoff. John von Neumann’s min-max theorem says that if each of the players chooses a distribution over
their strategies to optimize their worst case payoff (or payout), then the value they obtain is the same:
A(p, j) = max
where p (resp., q) varies over all distributions over rows (resp., columns). Also, i (resp., j) varies over all
rows (resp., columns). The common value of these two quantities, denoted λ ∗, is known as the value of
Let ε > 0 be an error parameter. We wish to approximately solve the zero-sum game up to additive
error of ε, namely, ﬁnd mixed row and column strategies ˜p and ˜q such that
λ ∗−ε ≤min
i A(i, ˜q),
A(˜p, j) ≤λ ∗+ε .
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
The algorithmic assumption about the game is that given any distribution p on decisions, we have an
efﬁcient way to pick the best response, namely, the pure column strategy j that maximizes A(p, j). This
quantity is at least λ ∗from the deﬁnition above. Call this algorithm the ORACLE.
Theorem 3.1. Given an error parameter ε > 0, there is an algorithm which solves the zero-sum game up
to an additive factor of ε using O(log(n)/ε2) calls to ORACLE, with an additional processing time of
O(n) per call.
Proof. We map our general algorithm from Section 2 to this setting by considering (3.3) as specifying n
linear constraints on the probability vector ˜q: viz., for all rows i, A(i, ˜q) ≥λ ∗−ε. Now, following the
intuition given in the beginning of this section, we make our decisions to correspond to pure strategies
of the row player. Thus a distribution on the decisions corresponds to a mixed row strategy. Costs of
the decisions are speciﬁed by pure strategies of the column player. The cost paid by a decision i when
column player chooses strategy j is A(i, j).
In each round, given a distribution p(t) on the rows, we will choose the column j(t) to be the best
response strategy to p(t) for the column player, by calling ORACLE. Thus, the cost vector m(t) is the
j(t)-th column of the matrix A.
Since all A(i, j) ∈ , we can apply Corollary 2.2 to get that after T rounds, for any distribution on
the rows p, we have
A(p(t), j(t)) ≤(1+η)
A(p, j(t))+ lnn
Dividing by T, and using the fact that A(p, j(t)) ≤1 and that for all t, A(p(t), j(t)) ≥λ ∗, we get
A(p(t), j(t)) ≤1
A(p, j(t))+η + lnn
Setting p = p∗, the optimal row strategy, we have A(p, j) ≤λ ∗for any j. By setting η = ε/2 and
T = ⌈4ln(n)/ε2⌉, we get that
A(p(t), j(t)) ≤1
A(p, j(t))+ε ≤λ ∗+ε .
t=1 A(p(t), j(t)) is an (additive) ε-approximation to λ ∗.
Let ˜t be the round t with the minimum value of A(p(t), j(t)). We have, from (3.5),
A(p(˜t), j(˜t)) ≤1
A(p(t), j(t)) ≤λ ∗+ε .
Since j(˜t) maximizes A(p(˜t), j) over all j, we conclude that p(˜t) is an approximately optimal mixed row
strategy, and thus we can set p∗:= p(˜t).1
1Alternatively, we can set p∗= (1/T)∑t p(t). For let j∗be the optimal column player response to p∗. Then we have
A(p∗, j∗) = 1
A(p(t), j∗) ≤1
A(p(t), j(t)) ≤λ ∗+ε .
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
We set q∗to be the distribution which assigns to column j the probability
|{t : j(t) = j}|
From (3.5), for any row strategy i, by setting p to be concentrated on the pure strategy i, we have
A(i, j(t)) = A(i,q∗)
which shows that q∗is an approximately optimal mixed column strategy.
Plotkin, Shmoys, Tardos framework for packing/covering LPs
Plotkin, Shmoys, and Tardos generalized some known ﬂow algorithms to a framework for approximately solving fractional packing and covering problems, which are a special case of linear programming
formally deﬁned below. Their algorithm is a quantitative version of the classical Lagrangean relaxation
idea, and applies also to general linear programs. Below, we derive the algorithm for general LPs and then
mention the slight modiﬁcation that yields better running time for packing-covering LPs. Also, we note
that we could derive this algorithm as a special case of game solving, but for concreteness we describe it
explicitly.
The basic problem is to check the feasibility of the following convex program:
where A ∈Rm×n is an m×n matrix, x ∈Rn, and P is a convex set in Rn. Intuitively, the set P represents
the “easy” constraints to satisfy, such as non-negativity, and A represents the “hard” constraints to satisfy.
We wish to design an algorithm that, given an error parameter ε > 0, either solves the problem to
an additive error of ε, i. e., ﬁnds an x ∈P such that for all i, Aix ≥bi −ε, or failing that, proves that the
system is infeasible. Here, Ai is the ith row of A.
We assume the existence of an algorithm, called ORACLE, which, given a probability vector p on the
m constraints, solves the following feasibility problem:
p⊤Ax ≥p⊤b.
One way to implement this procedure is by maximizing p⊤Ax over x ∈P. It is reasonable to expect such
an optimization procedure to exist (indeed, such is the case for many applications) since we only need to
check the feasibility of one constraint rather than m. If the feasibility problem (3.6) has a solution x∗,
then the same solution also satisﬁes (3.7) for any probability vector p over the constraints. Thus, if there
is a probability vector p over the constraints such that no x ∈P satisﬁes (3.7), then it is proof that the
original problem is infeasible.
We assume that the ORACLE satisﬁes the following technical condition, which is necessary for
deriving running time bounds.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
Deﬁnition 3.2. An (ℓ,ρ)-bounded ORACLE, for parameters 0 ≤ℓ≤ρ, is an algorithm which given a
probability vector p over the constraints, solves the feasibility problem (3.7). Furthermore, there is a ﬁxed
subset I ⊆[m] of constraints such that whenever the ORACLE manages to ﬁnd a point x ∈P satisfying
(3.7), the following holds:
Aix−bi ∈[−ℓ,ρ],
Aix−bi ∈[−ρ,ℓ].
The value ρ is called the width of the problem.
In previous work, such as , only (ρ,ρ)-bounded ORACLEs are considered. We separate out the
upper and lower bounds in order to obtain tighter guarantees on the running time. The results of can
be recovered simply by setting ℓ= ρ.
Theorem 3.3. Let ε > 0 be a given error parameter. Suppose there exists an (ℓ,ρ)-bounded ORACLE
for the feasibility problem (3.7). Assume that ℓ≥ε/2. Then there is an algorithm which either ﬁnds an x
such that ∀i . Aix ≥bi −ε, or correctly concludes that the system is infeasible. The algorithm makes only
O(ℓρ log(m)/ε2) calls to the ORACLE, with an additional processing time of O(m) per call.
Proof. The condition ℓ≥ε/2 is only technical, and if it is not met we can just redeﬁne ℓto be ε/2. To
map our general framework to this situation, we have a decision representing each of the m constraints.
Costs are determined by points x ∈P. The cost of constraint i for point x is (1/ρ)[Aix−bi] (so that the
costs lie in the range [−1,1]).
In each round t, given a distribution over the decisions (i. e., the constraints) p(t), we run the ORACLE
with p(t). If the ORACLE declares that there is no x ∈P such that p(t)⊤Ax ≥p(t)⊤b, then we stop, because
now p(t) is proof that the problem (3.6) is infeasible.
So let us assume that this doesn’t happen, i. e., in all rounds t, the ORACLE manages to ﬁnd a solution
x(t) such p(t)⊤Ax ≥p(t)⊤b. Since the cost vector to the Multiplicative Weights algorithm is speciﬁed to
be m(t) := (1/ρ)[Ax(t) −b], we conclude that the expected cost in each round is non-negative:
m(t) ·p(t) = 1
ρ [Ax(t) −b]·p(t) = 1
ρ [p(t)⊤Ax−p(t)⊤b] ≥0.
Let i ∈I. Then Theorem 2.1 tells us that after T rounds,
ρ [Aix(t) −bi]+η
ρ |Aix(t) −bi|+ lnm
ρ [Aix(t) −bi]+2η∑
ρ |Aix(t) −bi|+ lnm
ρ [Aix(t) −bi]+ 2ηℓ
Here, the subscript “< 0” refers to the rounds t when Aix(t) −bi < 0. The last inequality follows because
if Aix(t) −bi < 0, then |Aix(t) −bi| ≤ℓ. Dividing by T, multiplying by ρ, and letting ¯x = (1/T)∑T
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
(note that ¯x ∈P since P is a convex set), we get that
0 ≤(1+η)[Ai¯x−bi]+2ηℓ+ ρ ln(m)
Now, if we choose η = ε/(4ℓ) (note that η ≤1/2 since ℓ≥ε/2), and T = ⌈8ℓρ ln(m)/ε2⌉, we get that
0 ≤(1+η)[Ai¯x−bi]+ε
Ai¯x ≥bi −ε .
Reasoning similarly for i /∈I, we get the same inequality. Putting both together, we conclude that ¯x
satisﬁes the feasibility problem (3.6) up to an additive ε factor, as desired.
Concave constraints
The algorithm of Section 3.3 works not just for linear constraints over a convex domain, but also for
concave constraints. Imagine that we have the following feasibility problem:
∀i ∈[m] : fi(x) ≥0
where, as before, P ∈Rn is a convex domain, and for i ∈[m], fi : P →R are concave functions. We wish
to satisfy this system approximately, up to an additive error of ε. Again, we assume the existence of
an ORACLE, which, when given a probability distribution p = ⟨p1, p2,..., pm⟩⊤, solves the following
feasibility problem:
pi fi(x) ≥0.
An ORACLE would be called (ℓ,ρ)-bounded there is a ﬁxed subset of constraints I ⊆[m] such that
whenever it returns a feasible solution x to (3.9), all constraints i ∈I take values in the range [−ℓ,ρ] on
the point x, and all the rest take values in [−ρ,ℓ].
Theorem 3.4. Let ε > 0 be a given error parameter. Suppose there exists an (ℓ,ρ)-bounded ORACLE
for the feasibility problem (3.8). Assume that ℓ≥ε/2. Then there is an algorithm which either solves the
problem up to an additive error of ε, or correctly concludes that the system is infeasible, making only
O(ℓρ log(m)/ε2) calls to the ORACLE, with an additional processing time of O(m) per call.
Proof. Just as before, we have a decision for every constraint, and costs are speciﬁed by points x ∈P.
The cost of constraint i for point x is (1/ρ)fi(x).
Now we run the Multiplicative Weights algorithm with this setup. Again, if at any point the ORACLE
declares that (3.9) is infeasible, we immediately halt and declare the system (3.8) infeasible. So assume
this never happens. Then as before, the expected cost in each round is m(t) · p(t) ≥0. Now, applying
Theorem 2.1 as before, we conclude that for any i ∈I, we have
ρ fi(x(t))+ 2ηℓ
Dividing by T, multiplying by ρ, and letting ¯x = (1/T)∑T
t=1 x(t) (note that ¯x ∈P since P is a convex set),
we get that
0 ≤(1+η)fi(¯x)+2ηℓ+ ρ ln(m)
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
fi(x(t)) ≤fi
by Jensen’s inequality, since all the fi are concave.
Now, if we choose η = ε/4ℓ(note that η ≤1/2 since ℓ≥ε/2), and T = ⌈8ℓρ ln(m)/ε2⌉, we get that
0 ≤(1+η)fi(¯x)+ε
fi(¯x) ≥−ε .
Reasoning similarly for i /∈I, we get the same inequality. Putting both together, we conclude that ¯x
satisﬁes the feasibility problem (3.8) up to an additive ε factor, as desired.
Approximate ORACLEs
The algorithm described in the previous section allows some slack for the implementation of the ORACLE.
This slack is very useful in designing efﬁcient implementations for the ORACLE.
Deﬁne a ε-approximate ORACLE for the feasibility problem (3.6) to be one that solves the feasibility
problem (3.7) up to an additive error of ε. That is, given a probability vector p on the constraints, either it
ﬁnds an x ∈P such that p⊤Ax ≥p⊤b−ε, or it declares correctly that (3.7) is infeasible.
Theorem 3.5. Let ε > 0 be a given error parameter. Suppose there exists an (ℓ,ρ)-bounded ε/3approximate ORACLE for the feasibility problem (3.6). Assume that ℓ≥ε/3. Then there is an algorithm
which either solves the problem up to an additive error of ε, or correctly concludes that the system is
infeasible, making only O(ℓρ log(m)/ε2) calls to the ORACLE, with an additional processing time of
O(m) per call.
Proof. We run the algorithm of the previous section with the given ORACLE, setting η = ε/6ℓ. Now, in
every round, the expected payoff is at least −ε/3ρ. Simplifying as before, we get that after T rounds, we
have, the average point ¯x = (1/T)∑T
t=1 x(t) returned by the ORACLE satisﬁes
3 ≤(1+η)[Ai¯x−bi]+2ηℓ+ ρ ln(m)
Now, if T = ⌈18ℓρ ln(m)/ε2⌉, then we get that for all i, Ai¯x ≥bi −ε, as required.
Fractional Covering Problems
In fractional covering problems, the framework is the same as above, with the crucial difference that the
coefﬁcient matrix A is such that Ax ≥0 for all x ∈P, and b > 0. A ε-approximation solution to this
system is an x ∈P such that Ax ≥(1−ε)b.
We assume without loss of generality (by appropriately scaling the inequalities) that bi = 1 for all
rows, so that now we desire to ﬁnd an x ∈P which satisﬁes the system within an additive ε factor. Since
for all x ∈P, we have Ax ≥0, and since all bi = 1, we conclude that for any i, Aix−bi ≥−1. Thus, we
assume that there is a (1,ρ)-bounded ORACLE for this problem. Now, applying Theorem 3.3, we get the
following theorem.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
Theorem 3.6. Suppose there exists a (1,ρ)-bounded ORACLE for the program Ax ≥1 with x ∈P. Given
an error parameter ε > 0, there is an algorithm which computes a ε-approximate solution to the program,
or correctly concludes that it is infeasible, using O(ρ log(m)/ε2) calls to the ORACLE, plus an additional
processing time of O(m) per call.
Fractional Packing Problems
A fractional packing problem can be written as
where P is a convex domain such that Ax ≥0 for all x ∈P, and b > 0. A ε-approximate solution to this
system is an x ∈P such that Ax ≤(1+ε)b.
Again, we assume that bi = 1 for all i, scaling the constraints if necessary. Now by rewriting this
we cast it in our general framework, and a solution x ∈P which satisﬁes this up to an additive ε is a
ε-approximate solution to the original system. Since for all x ∈P, we have Ax ≥0, and since all bi = 1,
we conclude that for any i, −Aix+bi ≤1. Thus, we assume that there is a (1,ρ)-bounded ORACLE for
this problem. Now, applying Theorem 3.3, we get the following:
Theorem 3.7. Suppose there exists a (1,ρ)-bounded ORACLE for the program −Ax ≥−1 with x ∈P.
Given an error parameter ε > 0, there is an algorithm which computes a ε-approximate solution to the
program, or correctly concludes that it is infeasible, using O(ρ log(m)/ε2) calls to the ORACLE, plus an
additional processing time of O(m) per call.
Approximating multicommodity ﬂow problems
Multicommodity ﬂow problems are represented by packing/covering LPs and thus can be approximately
solved using the PST framework outlined above. The resulting ﬂow algorithm is outlined below together
with a brief analysis. Unfortunately, the algorithm is not polynomial-time because its running time is
bounded by a polynomial function of the edge capacities (as opposed to the logarithm of the capacities,
which is the number of bits needed to represent them). Garg and K¨onemann ﬁxed this problem
with a better algorithm whose running time does not depend upon the edge capacities.
Here we derive the Garg-K¨onemann algorithm using our general framework. This will highlight the
essential new idea, namely, a reweighting of penalties to reduce the width parameter. Note that algorithm
is not quite the same as in (the termination condition is slightly different) and neither is the proof;
the running time bound is the same however.
For illustrative purposes we focus on the maximum multicommodity ﬂow problem. In this problem,
we are given a graph G = (V,E) with capacities ce on edges, and set of k source-sink pairs of nodes.
Let P be the set of all paths between the source-sink pairs. The objective is to maximize the total ﬂow
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
between these pairs. The LP formulation is as follows:
∀p ∈P : fp ≥0.
Here, the variable fp represents the ﬂow on path p.
Before presenting the Garg-K¨onemann idea we ﬁrst present the algorithm one would obtain by
applying our packing-covering framework (Section 3.3) in the obvious way.
First, note that by using binary search we can reduce the optimization problem to feasibility, by
iteratively introducing a new constraint that gives a lower bound on the objective. So assume without
loss of generality that we know the value Fopt of the total ﬂow in the optimum solution. Then we want to
solve the following feasibility problem:
∃?∈P : ∀e ∈E : ∑
f : ∀p ∈P : fp ≥0, ∑
In this form, the feasibility problem given above is a packing LP, thus, we can apply the Multiplicative
Weights algorithm of Section 3.3.4.
As outlined in Section 3.3, the obvious algorithm would maintain at each step t a weight we(t) for
each edge e. The ORACLE can be implemented by ﬁnding the ﬂow in P which minimizes
we(t)/ce .
The optimal ﬂow is supported on a single path, namely, the path p(t) ∈P that has minimum length, when
every edge e ∈E is given length we(t)/ce. Thus in every round we ﬁnd this path p(t) and pass a ﬂow Fopt
on this path. Note that the ﬁnal ﬂow will be an average of the ﬂows in each event, and hence will also
have value Fopt. Costs for the edges are deﬁned as in Section 3.3.
Unfortunately the width parameter is
fp/ce = Fopt/cmin
where cmin is the capacity of the minimum capacity edge in the graph. The algorithm requires T =
ρ ln(n)/ε2 iterations to get an (1−ε)-approximation to the optimal ﬂow. The overall running time is
˜O(FoptTsp/cmin) where Tsp = ˜O(mk) is the time needed to compute k shortest paths. As already mentioned,
this is not polynomial-time since it depends upon 1/cmin rather than the logarithm of this value.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
Now we describe the Garg-K¨onemann modiﬁcation. We continue to maintain weights we(t) for every
edge e, where initially, we(1) = 1 for all e. The costs are determined by ﬂows as before, however, we
consider a larger set of ﬂows, viz.,
P′ := {f : ∀p ∈P : fp ≥0}.
Note that we no longer need to use the value Fopt. Again, in each round we choose a ﬂow f ∈P′ that is
supported on the shortest path p(t) ∈P with edge lengths we/ce.
The main idea behind width reduction is the following: instead of routing the same ﬂow Fopt at each
time step, we route only as much ﬂow as is allowed by the minimum capacity edge on the path. In other
words, at time t we route a ﬂow of value c(t) on path p(t), where c(t) is the minimum capacity of an edge
on the path p(t). The cost incurred by edge e is me(t) = c(t)/ce. (In other words, a cost of 1/ce per unit of
ﬂow passing through e.) The width is therefore automatically upper bounded by 1.
We run the MW algorithm with η = ε/2. The update rule in this setting consists of updating the
weights of all edges in path p(t) and leaving other weights unchanged at that step:
∀e ∈p(t) :
we(t+1) = we(t)
1+η · c(t)
The termination rule for the algorithm is to stop when as soon as for some edge e, the congestion
fe/ce ≥ln(m)/η2, where fe is the total amount of ﬂow routed by the algorithm so far on edge e.
We apply Theorem 2.5. Since we have me(t) ∈ for all edges e and rounds t, we conclude that for any
edge e, we have
m(t) ·p(t) ≥(1−η)
me(t) −ln(m)
We now analyze both sides of this inequality. In round t, for any edge e, we have me(t) = c(t)/ce if e ∈p(t),
and 0 if e /∈p. Thus, we have
me(t) = fe
where fe is the total amount of ﬂow on e at the end of the algorithm, and
m(t) ·p(t) =
∑e∈p(t) c(t)
c(t) · ∑e∈p(t) ·we(t)
Now, suppose the optimum ﬂow assigns f opt
ﬂow to path p, and let Fopt = ∑p f opt
be the total ﬂow.
For any set of edge lengths we/ce, the shortest path p ∈P with these edge lengths satisﬁes
≥∑e we ·∑p′∋e
= ∑p′ f opt
p′ ·∑e∈p′ we
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
The ﬁrst inequality follows because for any edge e, we have ∑p′∋e f opt
p′ ≤ce. The second inequality
follows from the fact that p is the shortest path with edge lengths given by we/ce. Using this bound in
(3.13), we get that
m(t) ·p(t) ≤
where F = ∑T
t=1 c(t) is the total amount of ﬂow passed by the algorithm.
Plugging (3.12) and (3.14) into (3.11), we get that
Fopt ≥(1−η)max
We stop the algorithm as soon as there we have an edge e with congestion fe/ce ≥ln(m)/η2, so when
the algorithm terminates we have
Fopt ≥(1−2η)max
Now, C := maxe { fe/ce} is the maximum congestion of the ﬂow passed by the algorithm. So, the ﬂow
scaled down by C respects all capacities. For this scaled down ﬂow, we have that the total ﬂow is
C ≥(1−2η)Fopt ,
which shows that the scaled-down ﬂow is within (1−2η) = (1−ε) of optimal.
Running time.
In every iteration t of the algorithm, consider the minimum capacity edge e on the
chosen path p(t). It gets congested by the ﬂow of value c(t) = ce sent in that round. Since we stop the
algorithm as soon as the congestion on any edge is at least ln(m)/η2, any given edge can be the minimum
capacity edge on the chosen path at most ⌈ln(m)/η2⌉times in the entire run of the algorithm. Since there
are m edges, the number of iterations is therefore at most m·⌈ln(m)/η2⌉= O(mlog(m)/ε2).
Each iteration involves k shortest path computations. Recall that Tsp is the time needed for this. Thus,
the overall running time is O(Tsp ·mlogm/ε2).
O(logn)-approximation for many NP-hard problems
For many NP-hard problems, typically integer versions of packing-covering problems, one can compute
a O(logn)-approximation by solving the obvious LP relaxation and then using Raghavan-Thompson 
randomized rounding. This yields a randomized algorithm; to obtain a deterministic algorithm, derandomize it using Raghavan’s method of pessimistic estimators.
Young has given an especially clear framework for understanding these algorithms which as a
bonus also yields faster, combinatorial algorithms for approximating integer packing/covering programs.
He observes that one can collapse the three ideas in the algorithm above—LP solving, randomized
rounding, derandomization—into a single algorithm that uses the multiplicative update rule, and does not
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
need to solve the LP relaxation directly. (Young’s paper is titled “Randomized rounding without solving
the linear program.”)
At the root of Young’s algorithm is the observation that the analysis of randomized rounding uses the
Chernoff-Hoeffding bounds. These bounds show that the sum of bounded independent random variables
X1,X2,...,Xn (which should be thought of as the random variables generated in the randomized rounding
algorithm) is sharply concentrated about its mean, and are proved by applying Markov’s inequality to the
variable eη(∑i Xi) for some parameter η. The key observation now is that the aforementioned application
of Markov’s inequality bounds the probability of failure (of the randomized rounding algorithm) away
from 0 in terms of E[eη(∑i Xi)]. Thus, one can treat E[eη(∑i Xi)] as a pessimistic estimator (up to scaling
by a constant) of the failure probability, and derandomization can be achieved by greedily (and hence,
deterministically) choosing the Xi sequentially to decrease this pessimistic estimator. The resulting
algorithm is essentially the MW algorithm: in each round t, the deterministic part of the pessimistic
estimator, viz. eη ∑τ<t Xτ, plays the role of the weight.
Below, we illustrate this idea using the canonical problem in this class, SET COVER. (A similar
analysis works for other problems.) Since we have developed the multiplicative weights framework
already, we do not detail Young’s original intuition involving Chernoff bound arguments and can proceed
directly to the algorithm. In fact, the algorithm can be simpliﬁed so it becomes exactly the classical
greedy algorithm, and we obtain a lnn-approximation, which is best-possible for this problem up to
constant factors (assuming reasonable complexity-theoretic conjectures ).
In the SET COVER problem, we are given a universe of n elements, say U = {1,2,3,...,n} and a
collection C of subsets of U whose union equals U. We are required to pick the minimum number of
sets from C which cover all of U. Let this minimum number be denoted OPT. The Greedy Algorithm
picks subsets iteratively, each time choosing that set which covers the maximum number of uncovered
We analyze the Greedy Algorithm in our setup as follows. Each element of the universe represents a
constraint that the union of sets picked by the algorithm must cover it. Following the guidelines given
in the beginning of Section 3, we cast the problem in our framework by letting decisions correspond to
elements in the universe, and costs determined by sets Cj ∈C. The cost of the constraint corresponding to
element i for a given set Cj is 1 if i ∈Cj and 0 otherwise.
To translate the greedy algorithm to our framework, suppose we run the Multiplicative Weights
Update algorithm with this setup with η = 1. Since the analysis in the proof of Theorem 2.1 technically
requires η ≤1/2, in the following we repeat the same potential function analysis for the current setting
for η = 1. For η = 1, the update rule wi(t+1) = wi(t)(1−ηmi(t)) implies that elements that have been
covered so far have weight 0 while all the rest have weight 1. Thus, the distribution p(t) is simply the
uniform distribution on the uncovered elements until time t. Since cost of an element for a given set is
1 if it is in the set and 0 otherwise, the set that maximizes the expected cost under p(t) is the one that
maximizes the number of uncovered elements. The resulting algorithm is the Greedy Set Cover algorithm.
Since OPT sets cover all the elements, for any distribution p1, p2,..., pn on the elements, one set
must cover at least 1/OPT fraction of elements. This implies that if we choose set C in round t that
maximizes the number of uncovered elements, we have
m(t) ·p(t) = max
pi ≥1/OPT.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
Following the analysis of Theorem 2.1, the change in potential for each round is:
Φ(t+1) = Φ(t)(1−ηm(t) ·p(t)) < Φ(t)e−η/OPT = Φ(t)e−1/OPT .
The strict inequality holds because m(t) · p(t) > 0 as long as there are uncovered elements. Thus, the
potential drops by a factor of e−1/OPT every time.
We run this as long as some element has not yet been covered. We show that T = ⌈lnn⌉OPT iterations
sufﬁce, which implies that we have a ⌈lnn⌉approximation to OPT. We have
Φ(T+1) < Φ(1)e−T/OPT = ne−⌈lnn⌉OPT/OPT = ne−⌈lnn⌉≤1.
Note that with η = 1, Φ(T+1) is exactly the number of elements left uncovered after T iterations. So we
conclude that all elements are covered.
Learning theory and boosting
Boosting —the process of combining several moderately accurate rules-of-thumb into a single
highly accurate prediction rule—is a central idea in Machine Learning today. Freund and Schapire’s
AdaBoost uses the Multiplicative Weights Update Rule and ﬁts in our framework. Here we explain
the main idea using some simplifying assumptions.
Let X be some set (domain) and suppose we are trying to learn an unknown function (concept)
c : X →{0,1} chosen from a concept class C. Given a sequence of training examples (x,c(x)) where x is
generated from a ﬁxed but unknown distribution D on the domain X, the learning algorithm is required to
output a hypothesis h : X →{0,1}. The error of the hypothesis is deﬁned to be Ex∼D[|h(x)−c(x)|].
A strong learning algorithm is one that, for every distribution D, given ε,δ > 0 and access to random
examples drawn from D, outputs with probability at least 1−δ a hypothesis whose error is at most ε.
Furthermore, the running time is required to be polynomial in 1/ε, 1/δ and othe relevant parameters. A
γ-weak learning algorithm, for some given γ > 0, is an algorithm satisfying the same conditions but the
error can be as high as 1/2−γ. Boosting shows that if a γ-weak learning algorithm exists for a concept
class, then a strong learning algorithm exists. (The running time of the algorithm and the number of
samples may depend on γ.)
We prove this result in the so-called boosting by sampling framework, which uses a ﬁxed training
set S of N examples drawn from the distribution D. The goal is to make sure that the ﬁnal hypothesis
erroneously classiﬁes at most ε fraction of this training set. Using VC-dimension theory (see ) one
can then show that if the weak learner produces hypotheses from a class H of bounded VC-dimension,
and if N is chosen large enough (in terms of the error and conﬁdence parameters, and the VC-dimension
of the hypothesis class H), then with probability at least 1−δ over the choice of the sample set, the error
of the hypothesis over the entire domain X (under distribution D) is at most 2ε.
The idea in boosting is to repeatedly run the weak learning algorithm on different distributions deﬁned
on the ﬁxed training set S. The ﬁnal hypothesis has error ε under the uniform distribution on S. We run
the MW algorithm with η = γ for T =
(2/γ2)ln(1/ε)
rounds. The decisions correspond to samples in
S and costs are speciﬁed by a hypothesis generated by the weak learning algorithm, in the following way.
If hypothesis h is generated, the cost for decision point x is 1 or 0 depending on whether h(x) = c(x) or
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
not. In other words, the cost vector m (indexed by x rather than i) is speciﬁed by mx = 1−|h(x)−c(x)|.
Intuitively, we want the weight of an example to increase if the hypothesis labels it incorrectly.
In each iteration, the algorithm presents the current distribution p(t) on the examples to the weak
learning algorithm, and in return obtains a hypothesis h(t) whose error with respect to the distribution p(t)
is not more than 1/2−γ, in other words, the expected cost in each iteration satisﬁes
m(t) ·p(t) ≥1
The algorithm is run for T rounds, where T will be speciﬁed shortly. The ﬁnal hypothesis, hﬁnal, labels
x ∈X according to the majority vote among h(1)(x),h(2)(x),...,h(T)(x).
Let E be the set of x ∈S incorrectly labeled by hﬁnal. The total cost of each x ∈E,
1−|h(t)(x)−c(x)| ≤T
since the majority vote gives an incorrect label for it. Instead of applying Theorem 2.1, we apply
Theorem 2.4 which gives a more nuanced bound. The set P in this theorem is simply the set of all possible
distributions on S. Choosing p to be the uniform distribution on E, we get
m(t) ·p(t) ≤(1+η)∑
m(t) ·p+ RE(p ∥p(1))
2 + log(n/|E|)
Since η = γ and T =
(2/γ2)ln(1/ε)
, the above inequality implies that the fraction of errors, |E|/n, is
at most ε as desired.
Hard-core sets and the XOR Lemma
A boolean function f : X →{0,1}, where X is a ﬁnite domain, is γ-strongly hard, for circuits of size S if
for every circuit C of size at most S,
x [C(x) = f(x)] ≤1
Here x ∈X is drawn uniformly at random, and γ < 1/2 is a parameter. For some parameter ε > 0, it is
ε-weakly hard for circuits of size S if for every circuit C of size at most S, we have
x [C(x) = f(x)] ≤1−ε .
Now given f : {0,1}n →{0,1}, deﬁne f ⊕k : {0,1}nk →{0,1} to be the boolean function obtained
by dividing up the input nk-bit string into k blocks of n bits each in the natural way, applying f to each
block in turn, and taking the XOR of the k outputs. Yao’s XOR Lemma shows that if f is ε-weakly
hard against circuits of size S then f ⊕k is γ +(1−ε)k-strongly hard for circuits of size Sε2γ2/8.
The original proofs were difﬁcult but Impagliazzo suggested a simpler proof that as a byproduct
proves an interesting fact about weakly hard functions: there is a reasonably large subset (at least ε
fraction of X) of inputs on which the function behaves like a strongly hard function, for somewhat smaller
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
circuit size S′. This subset is called a hard-core set. For technical reasons, to prove such a result, it
sufﬁces to exhibit a “smooth” distribution p on X (precise deﬁnition given momentarily), such for any
circuit C of size at most S′, we have
x∼p[C(x) = f(x)] ≤1
An ε-smooth distribution p (the same ε from the weak hardness assumption on f) is one that doesn’t
assign too much probability to any single input: px ≤1/(ε|X|) for any x ∈X. Such a distribution be
decomposed as a convex combination of probability distributions over subsets of size at least ε|X|.
Klivans and Servedio observed that Impagliazzo’s proof is an application of a boosting algorithm.
The argument is as follows. We are given a boolean function f that is ε-weakly hard for circuits of size S.
Assume for the sake of contradiction f is not γ-strongly hard on any smooth distribution for circuits of
some size S′ < S. Then for any smooth distribution, we can ﬁnd a small circuit of size S′ that calculates f
correctly with probability better than 1/2+γ when inputs are drawn from the distribution. Treat this as a
weak learning algorithm, and apply boosting. Boosting combines the small circuits of size S′ found by
the weak learning algorithm into a larger circuit that calculates f correctly with probability at least 1−ε
on the uniform distribution on X, contradicting the fact f is ε-weakly hard, if we ensure that the size of
the larger circuit is smaller than S. This can be done if the S′ is set to O(S/T), where T is the number of
boosting rounds.
With this insight, the problem now boils down to designing boosting algorithms that (a) are able to
deal with smooth distributions on inputs and (b) have a small number of boosting rounds. The lower the
number of boosting rounds, the better circuit size bound we get for showing γ-strong hardness.
The third author has shown how to construct such a boosting algorithm using the MW algorithm
for restricted distributions (see Section 2.2). This boosting algorithm obtains the best known parameters
in hard-core set constructions directly without having to resort to composing two different boosting
algorithms as in . This technique was extended in to obtain uniform constructions of hard-core
sets with the best known parameters.
We describe the boosting algorithm of now. The main observation is that the set of all ε-smooth
distributions is convex. Call this set P. Then, exactly as in Section 3.6, the boosting algorithm simply
runs the MW algorithm, with the only difference being that the distributions it generates are restricted
to be in P using relative entropy projections, as in the algorithm of Section 2.2. We can now apply the
same analysis as in Section 3.6. Following this analysis, let E ⊆X be the set of inputs on which the ﬁnal
majority hypotheses incorrectly computes f. Now we claim that |E| < ε|X|: otherwise, since the uniform
distribution on E is ε-smooth, we obtain a contradiction for T =
(2/γ2)ln(1/ε)
as before. Thus, the
ﬁnal majority hypothesis computes f correctly on at least a 1−ε fraction of inputs from X.
This immediately implies the following hard-core set existence result, which has the best known
parameters to date:
Theorem 3.8. Given a function f : X →{0,1} that is ε-weakly hard for circuits of size S, there is a
subset of X of size at least ε|X| on which f is γ-strongly hard for circuits of size
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
Hannan’s algorithm and multiplicative weights
Perhaps the earliest decision-making algorithm which attains bounds similar to the MW algorithm
is Hannan’s algorithm , dubbed “follow the perturbed leader” by Kalai and Vempala . This
algorithm is given below.
Initialization: Pick random numbers r1,r2,...,rn, one for each decision.
For t = 1,2,...,T:
1. Choose the decision which minimizes the total cost including random initial cost:
i(t) = argmin
where Li(t) = ∑τ<t mi(τ) is the total cost so far for the ith decision.
2. Observe the costs of the decisions m(t).
Figure 3: Hannan’s algorithm.
In this section we show that for a particular choice of random initialization numbers {ri}, the algorithm
above exactly reproduces the multiplicative weights algorithm, or more precisely the Hedge algorithm as
in Section 2.1. This observation is due to Adam Kalai .
Theorem 3.9 ( ). Let u1,...,un be n independent random numbers chosen uniformly from , and
consider the algorithm above with ri = 1
ui . Then for any decision j, we have
∑i e−ηLi(t) .
Proof. By monotonicity of the exponential function, we have:
ηLi(t) +lnln 1
Where wi(t) = e−ηLi(t) and si = ln 1
ui are independent exponentially distributed random variables with
mean 1. The result now follows from Lemma 3.10 below.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
Lemma 3.10. Let w1,...,wn be arbitrary non-negative real numbers, and let s1,...,sn be independent
exponential random variables with mean 1. Then
Proof. The probability density function of si is e−si. Conditioning on the value of sj, for a particular i ̸= j
e−sidsi = e
Since all variables s1,s2,...,sn are independent, we have
∀i ̸= j : wi
Integrating to remove the conditioning on sj, we have
∀i ̸= j : wi
w j e−sjds j =
Online convex optimization
Online convex optimization is a very general framework that can be applied to many of the problems
discussed in the applications section and many more. Here “online” means that the algorithm does not
know the entire input at the start, and the input is presented to it in pieces over many rounds. In this
section we describe the framework and the central role of the multiplicative weights method. For a much
more detailed treatment of online learning techniques see .
In online convex optimization, we move from a discrete decision set to a continuous one. Speciﬁcally,
the set of decisions is a convex, compact set K ⊆Rn. In each round t = 1,2,..., the online algorithm
is required to choose a decision, i. e., point p(t) ∈K. A convex loss function f (t) is presented, and
the decision maker incurs a loss of f (t)(p(t)). The goal of the online algorithm A is to minimize loss
compared to the best ﬁxed ofﬂine strategy. This quantity is called regret in the game theory and machine
learning literature.
f (t)(p(t))−min
The basic decision-making problem described in Section 2 with n discrete decisions is recovered as a
special case of online convex optimization as follows. The convex set K is the n-dimensional simplex
corresponding to the set of all distributions over the n decisions, and the payoff functions f (t) are deﬁned
as f (t)(p(t)) = m(t) ·p(t) given the cost vector m(t). It also generalizes other online learning problems such
as the online portfolio selection problem and online routing (see for more discussion on applications).
Zinkevich gives algorithms for the general online convex optimization problem. More efﬁcient
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
algorithms for online convex optimization based on strong convexity of the loss functions have also been
developed .
We now describe how to use the Multiplicative Weights algorithm to the online convex optimization
problem for the special case where K is the n dimensional simplex of distributions on the coordinates.
The advantage is that this algorithm has much better dependance on the dimension n than Zinkevich’s
algorithm (see for more details). This algorithm has several applications such as in online portfolio
selection . Here is the algorithm. First, deﬁne
∥∇f (t)(p)∥∞,
where ∇f (t)(p) is the (sub)gradient of the function f (t) at point p. The parameter ρ is called the width of
the problem. Then, run the standard Multiplicative Weights algorithm with η =
ln(n)/T and the costs
ρ ∇f (t)(p(t))
where p(t) is the point played in round t. Note that for all t and all i, |mi(t)| ≤1 as required by the MW
algorithm.
Theorem 3.11. After T rounds of applying the Multiplicative Weights algorithm to the online convex
optimization framework, for any p ∈K we have
f (t)(p(t))−
f (t)(p) ≤2ρ
Proof. If f : K →R is a differentiable convex function, then for any two points p,q ∈K we have
f(q) ≥f(p)+∇f(p)·(q−p),
where ∇f(p) is the gradient of f at p. Rearranging we get
f(p)−f(q) ≤∇f(p)·(p−q).
Applying Corollary 2.2, we get that for any p ∈K,
m(t) ·p(t) ≤
(m(t) +η|m(t)|)·p+ lnn
m(t) ·p+η + lnn
since ∥m(t)∥∞≤1. Now we have
[f (t)(p(t))−f (t)(p) ≤
∇f (t)(p(t))·(p(t) −p)
(from (3.16))
ρm(t) ·(p(t) −p)
≤ηρT + ρ lnn
(from (3.17))
Substituting η =
lnn/T completes the proof.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
Other applications
Approximately solving certain semideﬁnite programs
Semideﬁnite programming (SDP) is a special case of convex programming. A semideﬁnite program is
derived from a linear program by imposing the additional constraint that some subset of n2 variables form
an n×n positive semideﬁnite matrix. Since the work of Goemans and Williamson , SDP has become
an important tool in design of approximation algorithms for NP-hard optimization problems. Though this
yields polynomial-time algorithms, their practicality is suspect because solving SDPs is a slow process.
Therefore there is great interest in computing approximate solutions to SDPs, especially the types that
arise in approximation algorithms. Since the SDPs in question are being used to design approximation
algorithms anyway, it is permissible to compute approximate solutions to these SDPs.
Klein and Lu use the PST framework (Section 3.3) to derive a more efﬁcient 0.878-approximation
algorithm for MAX-CUT than the original SDP-based method in Goemans-Williamson . The main
idea in Klein-Lu is to approximately solve the MAX-CUT SDP. However, their idea does work very well
for other SDPs. The main issue is that the width ρ (see 3.3) is too high for certain SDPs of interest.
To be more precise, an SDP feasibility problem is given by:
∀j = 1,2,...,m : A j •X ≥bj ,
Here, we use the notation A • B = ∑ij Ai jBi j to denote the scalar product of two matrices thinking of
them as vectors in Rn2. The set P = {X ∈Rn×n | X ⪰0, Tr(X) ≤1} is the set of all positive semideﬁnite
matrices with trace bounded by 1.
The Plotkin-Shmoys-Tardos framework (see Section 3.3) is suitable for approximating SDPs since
all constraints are linear, and the oracle given in (3.7) can be implemented efﬁciently by an eigenvector
computation. To see this, note that the oracle needs to decide, given a probability distribution p on the
constraints, if there exists an X ∈P such that ∑j pjA j •X ≥∑j pjbj. This can be implemented by solving
the following optimization problem:
It is easily checked that an optimal solution to the above optimization problem is given by the matrix
X = vv⊤where v is unit eigenvector corresponding to the largest eigenvalue of the matrix ∑j pjA j.
The Klein-Lu approach was of limited uses in many cases because it does not do too well when the
additive error ε is required to be small. (They were interested in the MAX-CUT problem, where this
problem does not arise. The reason in a nutshell is that in a graph with m edges, the maximum cut has at
least m/2 edges, so it sufﬁces to compute the optimum to an additive error ε that is a ﬁxed constant.) We
have managed to extend the multiplicative weights framework to many of these settings to design efﬁcient
algorithms for SDP relaxations of many other problems. The main idea is to apply the Multiplicative
Weights framework in a “nested” fashion: one can solve a constrained optimization problem by invoking
the MW algorithm on an subset of constraints (the “outer” constraints) in the manner of Section 3.3,
where the domain is now deﬁned by the rest of the constraints (the “inner” constraints). The oracle can
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
now be implemented by another application of the MW algorithm on the inner constraints. Alternatively,
we can reduce the dependence on the width by using the observation that the Lagrangian relaxation
problem on the inner constraints can be solved by the ellipsoid method. For details of this method, refer
to our paper . For several families of SDPs we obtain the best running time known.
More recently Arora and Kale have designed a new approach for solving SDPs that involves a
variant of the multiplicative update rule at the matrix level; see Section 5 for details.
Yet another approach for approximately solving SDPs is by reducing the SDP into a maximization
problem of a single concave function over the PSD cone. The latter problem can be approximated
efﬁciently via an iterative greedy method. The resulting algorithm is extremely similar to the MW-based
algorithm of , however its analysis is very different, see for more details.
Approximating graph separators
A recent application of the multiplicative weights method is a combinatorial algorithm for approximating
the SPARSEST CUT of graphs . This is a fundamental graph partitioning problem. Given a graph
G = (V,E), the expansion of a cut (S, ¯S) where S ⊆V and ¯S = V \S, is deﬁned to be
|E(S, ¯S)|
min{|S|,| ¯S|} .
Here, E(S, ¯S) is the set of edges with one end point in S and the other in ¯S. The SPARSEST CUT problem
is to ﬁnd the cut in the input graph of minimum expansion. This problem arises as a useful subroutine in
many other algorithms, such as in divide-and-conquer algorithms for optimization problems on graphs,
layout problems, clustering, etc. Furthermore, the expansion of a graph is a very useful way to quantify
the connectivity of a graph and has many important applications in computer science.
The work of Arora, Rao and Vazirani gave the ﬁrst O(√logn) approximation algorithm to the
SPARSEST CUT problem. However, their best algorithm relies on solving an SDP and runs in ˜O(n4.5) time.
They also gave an alternative algorithm based on the notion of expander ﬂows, which are multicommodity
ﬂows in the graph whose demand graph has high expansion. However, their algorithm was based on the
ellipsoid method, and was thus quite inefﬁcient. In the paper , we obtained a much more efﬁcient
algorithm for approximating the SPARSEST CUT problem to an O(√logn) factor in ˜O(n2) time using
the expander ﬂow idea. The algorithm casts the problem of routing an expander ﬂow in the graph as a
linear program, and then checks the feasibility of the linear program using the techniques described in
Section 3.3. The oracle for this purpose is implemented using a variety of techniques: the multicommodity
ﬂow algorithm of Garg and K¨onemann (and its subsequent improvement by Fleischer ),
eigenvalue computations, and graph sparsiﬁcation algorithms of Bencz´ur and Karger based on random
Multiplicative weight algorithms in geometry
The multiplicative weight idea has been used several times in computational geometry. Chazelle 
(p. 6) describes the main idea, which is essentially the connection between derandomization of Chernoff
bound arguments and the exponential potential function noted in Section 3.5.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
The geometric applications consist of derandomizing a natural randomized algorithm by using a
deterministic construction of some kind of small set cover or low-discrepancy set. Formally, the analysis
is similar to our analysis of the Set Cover algorithm in Section 3.5. Clarkson used this idea to give a
deterministic algorithm for Linear Programming . Following Clarkson, Br¨onnimann and Goodrich
use similar methods to ﬁnd Set Covers for hypergraphs with small VC dimension .
The MW algorithm was also used in the context of geometric embeddings of ﬁnite metric spaces,
speciﬁcally, embedding negative-type metrics (i. e., a set of points in Euclidean space such that the
squared Euclidean distance between them also forms a metric) into ℓ1. Such embeddings are important
for approximating the important non-uniform SPARSEST CUT PROBLEM.
The approximation algorithm for SPARSEST CUT in Arora et al. involves a “Structure Theorem.”
This structure theorem was interpreted by Chawla et al. as saying that any n-point negative-type
metric with maximum distance O(1) and average distance Ω(1) can be embedded into ℓ1 such that
average ℓ1 distance is Ω(1/√logn). Then they used the MW algorithm to construct an embedding into
ℓ1 in which every pair of points that have negative-type metric distance Ω(1) have ℓ1 distance that is off
by at most an O(√logn) factor of the original. Using a similar idea for other distances and combining
the resulting embeddings they obtained an embedding of the negative-type metric into ℓ1 in which every
distance distorts by at most a factor O(log3/4 n). Arora et al. gave a more complicated construction to
improve the distortion bound to O(
log(n)loglogn), leading to a O(
log(n)loglogn)-approximation
for non-uniform sparsest cut.
Design of competitive online algorithms
Starting with the work of Alon, Awerbuch, Azar, Buchbinder and Naor , a number of competitive online
algorithms have been developed using an elegant primal-dual approach which involves multiplicative
weight updates. While the analysis of their algorithms seems to be beyond our general framework,
we brieﬂy mention this work without going into many details. We refer the readers to the survey by
Buchbinder and Naor for an extensive discussion of the topic.
Several online problems such as the ski rental problem, caching, load balancing, ad auctions, etc. can
be cast (in their fractional form) as a linear program with non-negative coefﬁcients in the constraints and
cost, where either the constraints or variables arrive online one by one. The online problem is to maintain
a feasible solution at all times with a bounded competitive ratio, i. e., ensuring that the cost of the solution
maintained is bounded in terms of the cost of the optimal solution in each round. The main difﬁculty
comes from the requirement that the solution maintained is monotonic in some sense (for example, the
variables are never allowed to decrease).
Buchbinder and Naor give a algorithm based on the primal-dual method that obtains good
competitive ratios in this scenario. At the heart is a multiplicative weight update rule. Imagine we have a
covering LP, i. e., all constraints are of the form a·x ≥1, where a has non-negative coefﬁcients, and x is
the vector of variables. The cost function has non-negative coefﬁcients as well. Constraints arrive one at
a time in each round and must be satisﬁed by the current solution. The requirement is that no variable can
decrease from round to round.
In every round, the algorithm increases primal variables in the new constraint using multiplicative
updates and the corresponding dual variable additively , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
of Section 3.3). This is done until the constraint gets satisﬁed. Clearly, we maintain a feasible solution
in each round, and the variables never decrease. The analysis goes by bounding the increase in the
primal cost in terms of the dual cost (this is an easy consequence of the multiplicative update, in fact,
the multiplicative update can be derived using this requirement). The competitive ratio is obtained by
showing that the dual solution generated simultaneously, while infeasible, is not far from being feasible,
i. e., scaling down the variables by a certain factor makes it feasible. This gives us a bound on the
competitive ratio via weak duality.
Lower bound
Can our analysis of the Multiplicative Weights algorithm be improved? This section shows that the
answer is no, not only for the MW algorithm itself, but for any algorithm which operates in the online
setting we have described. A similar lower bound was obtained by Klein and Young .
Technically, we prove the following:
Theorem 4.1. For any online decision-making algorithm with n ≥2 decisions, there exists a sequence
of cost vectors m(1),m(2),...,m(T) ∈{0,1}n such that mini ∑t mi(t) = Ω(T), and if the sequence of
distributions over decisions produced by the algorithm are p(1),p(2),...,p(T), then we have
m(t) ·p(t) ≥min
Since in the theorem above we have mini ∑T
t=1 mi(t) = Ω(T), Theorem 2.1 implies that by choosing
the optimal value of η, viz. η = Θ
m(t) ·p(t) ≤min
Hence our analysis is tight up to constants in the additive error term. Moreover, the above lower bound
applies to any algorithm, efﬁcient or not.
Proof. The proof is via the probabilistic method. We construct a distribution over the costs so that the
required bound is obtained in expectation. Interestingly, the distribution is independent of the actual
algorithm used.
We now specify the costs of the decisions. The cost of decision 1 is set to 1/2 for all rounds t. For
any decision i > 1, we construct its cost via the following random process: in each iteration t, choose its
cost to be either one or zero uniformly at random, i. e., mi(t) ∈{0,1} with probability of each outcome
being 1/2.
The expected cost of each decision is 1/2. Hence, the expected cost of the chosen decision is also
1/2 irrespective of the algorithm’s distribution p(t), and hence:
m(t) ·p(t)
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
For every decision i, deﬁne Xi = ∑T
t=1 mi(t). Note that X1 = T/2, and for i > 1, we have Xi ∼B(T,1/2),
where B(T,1/2) is the binomial distribution with T trials and both outcomes equally likely.
We use the following standard concentration lemma (see Proposition 7.3.2 in ):
Lemma 4.2. Let X ∼B(T,1/2) be a binomial random variable with T trials and probability 1/2. Then
for t ∈[0,T/8]:
15e−16t2/T.
We claim that the expectation of the cost of the best decision, viz., E[mini Xi], in our construction is
T/2−Ω(√T logn). Let t = (1/4)
T ln(n−1). We have
15e−16t2/T
≤e−1/15 < 0.95.
The ﬁrst equality above is by the independence of the Xi, and the ﬁrst inequality is by Lemma 4.2. Thus,
with probability at least 1−0.95 = 0.05, mini Xi ≤(T/2)−t. Since mini Xi is always at least T/2, we get
i Xi] ≥0.95· T
T ln(n−1).
It follows that
T ln(n−1).
From (4.1) and (4.2), we have
m(t) ·p(t) −min
T ln(n−1).
t=1 m(t) ·p(t) −mini ∑T
t=1 mi(t) ≤T, by Markov’s inequality we conclude that
m(t) ·p(t) −min
By the Hoeffding bound , for any decision i > 1, we have
≤exp(−T/32).
By the union bound, we have
≤nexp(−T/32) <
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
for large enough T. This implies that there exists a sequence m(1),m(2),...,m(T) for which
mi(t) ≥T/4
m(t) ·p(t) −min
T ln(n−1).
The Matrix Multiplicative Weights algorithm
In the preceding sections, we considered an online decision making problem. We refer to that setting as
the basic or scalar case. Now we brieﬂy consider a different online decision making problem, which is
seemingly quite different from the previous one, but has enough structure that we can obtain an analogous
algorithm for it. We move from cost vectors to cost matrices, and from probability vectors to density
matrices. For this reason, we refer to the current setting as the matrix case. We call the algorithm presented
in this setting the Matrix Multiplicative Weights algorithm. The original motivation for our interest in this
matrix setting is that it leads to a constructive version of SDP duality, just as the standard multiplicative
weights algorithm can be viewed as a constructive version of LP duality. In fact the standard algorithm is
a special subcase of the algorithm in this section, namely, when all the matrices involved are diagonal.
Applications of the matrix multiplicative weights algorithm include solving SDPs , derandomizing
constructions of expander graphs, and obtaining bounds on the sample complexity for a learning problem
in quantum computing. The celebrated result of Jain et al. showing that QIP =PSPACE relied on
the Matrix MW algorithm. Here QIP is the set of all languages which have quantum interactive proofs.
These applications are unfortunately beyond the scope of this survey; please see the third author’s Ph. D.
thesis and Jain et al. for details. The algorithm given here is from a paper of Arora and Kale .
A very similar algorithm was discovered independently slightly earlier by Warmuth and Kuzmin ,
and is based on the even earlier work of Tsuda, R¨atsch, and Warmuth .
We stick with our basic decision-making scenario but decisions now correspond to unit vectors v in
Sn−1, the unit sphere in Rn. As in the basic case, in every round, our task is to pick a decision v ∈Sn−1.
At this point, the costs of all decisions are revealed by nature. These costs are not arbitrary, but they are
correlated in the following way. A cost matrix M ∈Rn×n is revealed, and the cost of a decision v is then
v⊤Mv. We assume that the costs of all decisions lie in [−1,1]. Again, as in the basic case, this is the
only assumption we make on the way nature chooses the costs; indeed, the costs could even be chosen
adversarially. Equivalently, we assume that all the eigenvalues of the matrix M are in the range [−1,1].
This game is repeated over a number of rounds. Let t = 1,2,...,T denote the current round. In each
round t, we select a distribution D(t) over the set of decisions Sn−1, and select a decision v randomly
from it (and use his advised course of action). At this point, the costs of all decisions are revealed by
nature via the cost matrix M(t). The expected cost to the algorithm for choosing the distribution D(t) is
Ev∈D(t)[v⊤M(t)v] = Ev∈D(t)[M(t) •vv⊤] = M(t) •Ev∈D(t)[vv⊤].
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
Recall that we are using the notation A•B = ∑i j Ai jBi j to denote the scalar product of two matrices
thinking of them as vectors in Rn2. Deﬁne the matrix P(t) := Ev∈D(t)[vv⊤]. Note that P(t) is positive
semideﬁnite: this is because it is a convex combination of the elementary positive semideﬁnite matrices
vv⊤. Let Tr(A) denote the trace of a matrix A. Then, Tr(P(t)) = 1, again because for all v, we have
Tr(vv⊤) = ∥v∥2 = 1. A matrix P which is positive semideﬁnite and has trace 1 is called a density matrix.
We will only be interested in the expected cost to the algorithm, and all the information required for
computing the expected cost for a given distribution D over Sn−1 is contained in the associated density
Thus, in each round t, we require our online algorithm to choose a density matrix P(t), rather than a
distribution D(t) over Sn−1. The distribution is implicit in the choice of P(t). The eigendecomposition of
i=1 λiviv⊤
i , where λi is an eigenvalue corresponding to the unit eigenvector vi, gives one such
distribution (a discrete one): vector vi gets probability λi. We then observe the cost matrix M(t) revealed by
nature, and suffer the expected cost M(t) •P(t). After T rounds, the total expected cost is ∑T
t=1 M(t) •P(t),
while the best ﬁxed decision in hindsight corresponds to the unit vector v which minimizes ∑T
t=1 v⊤M(t)v.
Since we minimize this quantity over all unit vectors v, the variational characterization of eigenvalues
implies that this minimum cost is exactly the minimum eigenvalue of ∑T
t=1 M(t), denoted by λn(∑T
t=1 M(t)).
Our goal is to design an online algorithm whose total expected cost over the T rounds is not much more
than the cost of the best decision.
Consider the following generalization of the basic MW algorithm, called the Matrix Multiplicative
Weights algorithm (Matrix MW). It uses the notion of matrix exponential, exp(A) := ∑∞
i=0 Ai/i!. The key
point here is that regardless of the matrix A, its exponential exp(A) is always positive deﬁnite. Note that
in case all matrices involved are diagonal, the Matrix Multiplicative Weights algorithm exactly reduces to
the Hedge algorithm.
Matrix Multiplicative Weights algorithm
Initialization: Fix an η ≤1
2. Initialize the weight matrix W(1) = In.
For t = 1,2,...,T:
1. Use the density matrix P(t) = W(t)
Φ(t) , where Φ(t) = Tr(W(t)).
2. Observe the cost matrix M(t).
3. Update the weight matrix as follows:
W(t+1) = exp(−η∑t
Figure 4: The Matrix Multiplicative Weights algorithm.
The following theorem bounds the total expected cost of the Matrix Multiplicative Weights algorithm
(given in Figure 4) in terms of the cost of the best ﬁxed decision. This theorem is completely analogous
to Theorem 2.3, and in fact, Theorem 2.3 can be obtained directly from this theorem in the case when all
matrices involved are diagonal. We omit the proof of this theorem; again, it is along the lines of the proof
THEORY OF COMPUTING, Volume 8 , pp. 121–164
THE MULTIPLICATIVE WEIGHTS UPDATE METHOD: A META-ALGORITHM AND APPLICATIONS
of Theorem 2.3, and uses Φ(t) as a potential function. The analysis is based on the matrix analogue of
the real number inequality exp(−ηx) ≤1−ηx+η2x2 for |ηx| ≤1: if X is a matrix such that ∥ηX∥≤1,
−exp(−ηX) ⪯I−ηX+η2X2 .
Also, we need an additional inequality from statistical mechanics, the Golden-Thompson inequality , which states that for two matrices A and B, we have
Tr(exp(A+B)) ≤Tr(exp(A)exp(B)).
See for further details.
Theorem 5.1. In the given setup, the Matrix Multiplicative Weights algorithm guarantees that after T
rounds, for any decision v, we have
M(t) •P(t) ≤
(M(t))2 •P(t) + lnn
Applications in solving SDPs
In this section, we mention the application of the Matrix MW algorithm to solving SDPs without going
into any details. We refer the interested reader to for further details and other applications.
Consider the following quite general form of feasibility problem using SDP:
∀j = 1,2,...,m : A j •X ≥0,
Tr(X) = 1,
Note if all A j matrices and X were diagonal, then this exactly reduces to the LP (3.1).
Now suppose, as in Section 3.1, that there exists a large-margin solution, i. e., a density matrix X⋆
such that for all j, we have Aj •X⋆≥ε. Then just as in Section 3.1, we can use the Matrix MW algorithm
to ﬁnd a good solution, i. e., a density matrix X such that for all j we have A j •X ≥0. We now need to
deﬁne ρ = max j ∥Aj∥.
We run the Matrix MW algorithm (in the gain form). In each round, we set X to be the current density
matrix P(t), and check if it is a good solution. If not, and there is a constraint j such that A j •X < 0, then
we set M(t) = (1/ρ)A j. Note that in case all the matrices involved in the SDP are diagonal, then this
algorithm reduces to the Winnow algorithm of Section 3.1.2
The analysis is exactly the same as in Section 3.1. This analysis (using the gain form of Theorem 5.1
corresponding to Theorem 2.5), implies that in at most
4ρ2 lnn/ε2
iterations, we ﬁnd a good solution.
This technique was used in to obtain signiﬁcantly faster algorithms than previously known
for approximating several combinatorial optimization problems such as SPARSEST CUT, BALANCED
SEPARATOR (both in directed and undirected graphs), MIN UNCUT, and MIN 2CNF DELETION. This
also gives the ﬁrst near-linear time algorithm for solving the MAX CUT SDP of to any constant
approximation factor.
2A minor difference being that we get the version of Winnow based on the Hedge algorithm, rather than the MW algorithm.
THEORY OF COMPUTING, Volume 8 , pp. 121–164
SANJEEV ARORA, ELAD HAZAN AND SATYEN KALE
Acknowledgments
This work beneﬁtted greatly from discussions with Avrim Blum, Bernard Chazelle, Russell Impagliazzo,
Satish Rao, Robert E. Schapire, and Manfred K. Warmuth. We also want to thank the editors and the
anonymous referees at Theory of Computing for very carefully reading our manuscript and coming up
with numerous helpful suggestions to improve the presentation. In addition, since a preprint of this paper
was available on the Internet for several years prior to publication, a number of people have read it and
suggested improvements; we cannot list them all but we thank them for their suggestions.