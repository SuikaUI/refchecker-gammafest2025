Journal of Machine Learning Research 5 73-99
Submitted 12/01; Revised 11/02; Published 1/04
Dimensionality Reduction for Supervised Learning with
Reproducing Kernel Hilbert Spaces
Kenji Fukumizu
 
Institute of Statistical Mathematics
4-6-7 Minami-Azabu, Minato-ku, Tokyo 106-8569, Japan
Francis R. Bach
 
Computer Science Division
University of California
Berkeley, CA 94720, USA
Michael I. Jordan
 
Computer Science Division and Department of Statistics
University of California
Berkeley, CA 94720, USA
Editor: Chris Williams
We propose a novel method of dimensionality reduction for supervised learning problems.
Given a regression or classiﬁcation problem in which we wish to predict a response variable
Y from an explanatory variable X, we treat the problem of dimensionality reduction as
that of ﬁnding a low-dimensional “eﬀective subspace” for X which retains the statistical
relationship between X and Y . We show that this problem can be formulated in terms
of conditional independence. To turn this formulation into an optimization problem we
establish a general nonparametric characterization of conditional independence using covariance operators on reproducing kernel Hilbert spaces. This characterization allows us
to derive a contrast function for estimation of the eﬀective subspace. Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method
requires neither assumptions on the marginal distribution of X, nor a parametric model of
the conditional distribution of Y . We present experiments that compare the performance
of the method with conventional methods.
regression, dimensionality reduction, variable selection, feature selection,
kernel methods, conditional independence.
1. Introduction
Many statistical learning problems involve some form of dimensionality reduction, either
explicitly or implicitly. The goal may be one of feature selection, in which we aim to ﬁnd
linear or nonlinear combinations of the original set of variables, or one of variable selection,
in which we wish to select a subset of variables from the original set. The setting may be
unsupervised learning, in which a set of observations of a random vector X are available, or
supervised learning, in which desired responses or labels Y are also available. Developing
methods for dimensionality reduction requires being clear on the goal and the setting, as
methods developed for one combination of goal and setting are not generally appropriate for
c⃝2004 Kenji Fukumizu, Francis R. Bach, and Michael I. Jordan.
Fukumizu, Bach and Jordan
another. There are additional motivations for dimensionality reduction that it is also helpful
to specify, including: providing a simpliﬁed explanation of a phenomenon for a human
(possibly as part of a visualization algorithm), suppressing noise so as to make a better
prediction or decision, or reducing the computational burden. These various motivations
are often complementary.
In this paper we study dimensionality reduction in the setting of supervised learning.
Thus, we consider problems in which our data consist of observations of (X, Y ) pairs, where
X is an m-dimensional explanatory variable and where Y is an ℓ-dimensional response. The
variable Y may be either continuous or discrete. We refer to these problems generically as
“regression” problems, which indicates our focus on the conditional probability density
function pY |X(y | x). In particular, our framework includes discriminative approaches to
classiﬁcation problems, where Y is a discrete label.
We wish to solve a problem of feature selection in which the features are linear combinations of the components of X. In particular, our modeling framework posits that there
is an r-dimensional subspace S ⊂Rm such that
pY |X(y | x) = pY |ΠSX(y | ΠSx),
for all x and y, where ΠS is the orthogonal projection of Rm onto S. The subspace S is
called the eﬀective subspace for regression. Based on a set of observations of (X, Y ) pairs, we
wish to recover a matrix whose columns span the eﬀective subspace. The eﬀective subspace
can help to provide explanation of the statistical relation between X and Y , by isolating
the feature vectors that capture that relation. Also, ﬁnding such a space can suppress noise,
to the extent that the orthogonal direction to the eﬀective subspace is noisy vis-a-vis the
prediction of Y .
We approach the problem as a semiparametric statistical problem; in particular, we make
no assumptions regarding the conditional distribution pY |ΠSX(y | ΠSx), nor do we make
any assumptions regarding the marginal distribution pX(x). That is, we wish to estimate a
ﬁnite-dimensional parameter (a matrix whose columns span the eﬀective subspace), while
treating the distributions pY |ΠSX(y | ΠSx) and pX(x) nonparametrically.
Having found an eﬀective subspace, we may then proceed to build a parametric or
nonparametric regression model on that subspace. Thus our approach is an explicit dimensionality reduction method for supervised learning that does not require any particular form
of regression model, and can be used as a preprocessor for any supervised learner. This
can be compared to the use of methods such as principal components analysis (PCA) in
regression, which also make no assumption regarding the subsequent regression model, but
fail to make use of the response variable Y .
There are a variety of related approaches in the literature, but most of them involve
making speciﬁc assumptions regarding the conditional distribution pY |ΠSX(y | ΠSx), the
marginal distribution pX(x), or both.
For example, classical two-layer neural networks
involve a linear transformation in the ﬁrst “layer,” followed by a speciﬁc nonlinear function
and a second layer . Thus, neural networks can be seen as attempting to
estimate an eﬀective subspace based on speciﬁc assumptions about the regressor pY |ΠSX(y |
ΠSx). Similar comments apply to projection pursuit regression , ACE and additive models , all of which provide a methodology for dimensionality reduction in which an additive
model E[Y | X] = g1(βT
1 X) + · · · + gK(βT
KX) is assumed for the regressor.
Bayesian approaches to variable selection include the Automatic Relevance Determination (ARD) method proposed by Neal for neural networks. Vivarelli and Williams
 have adapted this approach to feature selection in the setting of Gaussian process
regression. These methods also depend on a speciﬁc parametric model, on which a Bayesian
prior distribution is assumed.
Canonical correlation analysis (CCA) and partial least squares are classical multivariate statistical methods that can be used for dimensionality reduction in regression . These methods
are based on a linearity assumption for the regressor, however, and thus are quite strongly
parametric.
The line of research that is closest to our work has its origin in a technique known
as sliced inverse regression . SIR is a semiparametric method for ﬁnding
eﬀective subspaces in regression. The basic idea is that the range of the response variable
Y is partitioned into a set of “slices,” and the sample means of the observations X are
computed within each slice. This can be viewed as a rough approximation to the inverse
regression of X on Y . For univariate Y the method is particularly easy to implement. Noting
that the inverse regression must lie in the eﬀective subspace if the forward regression lies
in such a subspace, principal component analysis is then used on the sample means to ﬁnd
the eﬀective subspace. Li has shown that this approach can ﬁnd eﬀective subspaces,
but only under strong assumptions on the marginal distribution pX(x)—in particular, the
marginal distribution must be elliptically symmetric.
Further developments in the wake of SIR include principal Hessian directions , and sliced average variance estimation . These are all semiparametric methods in that they make no assumptions about
the regressor . However, they again place strong restrictions on the
probability distribution of the explanatory variables.
If these assumptions do not hold,
there is no guarantee of ﬁnding the eﬀective subspace.
There are also related nonparametric approaches that estimate the derivative of the
regressor to achieve dimensionality reduction, based on the fact that the derivative of
the conditional expectation E[y | BT x] with respect to x belongs to the eﬀective subspace . However, nonparametric estimation of derivatives is quite challenging in high-dimensional spaces.
There are also dimensionality reduction methods with a semiparametric ﬂavor in the area
of classiﬁcation, notably the work of Torkkola , who has proposed using nonparametric
estimation of the mutual information between X and Y , and subsequent maximization of
this estimate of mutual information with respect to a matrix representing the eﬀective
In this paper we present a novel semiparametric method for dimensionality reduction
that we refer to as Kernel Dimensionality Reduction (KDR). KDR is based on the estimation and optimization of a particular class of operators on reproducing kernel Hilbert
spaces . Although our use of reproducing kernel Hilbert spaces is related
to their role in algorithms such as the support vector machine and kernel PCA , where the kernel function allows linear op-
Fukumizu, Bach and Jordan
erations in function spaces to be performed in a computationally-eﬃcient manner, our work
diﬀers in that it cannot be viewed as a “kernelization” of an underlying linear algorithm.
Rather, we use reproducing kernel Hilbert spaces to provide characterizations of general
notions of independence, and we use these characterizations to design objective functions
to be optimized. We build on earlier work by Bach and Jordan , who showed how
to use reproducing kernel Hilbert spaces to characterize marginal independence between
pairs of variables, and thereby design an objective function for independent component
analysis. In the current paper, we extend this line of work, showing how to characterize
conditional independence using reproducing kernel Hilbert spaces. We achieve this by expressing conditional independence in terms of covariance operators on reproducing kernel
Hilbert spaces.
How does conditional independence relate to our dimensionality reduction problem?
Recall that our problem is to ﬁnd a projection ΠS of X onto a subspace S such that the
conditional probability of Y given X is equal to the conditional probability of Y given ΠSX.
This is equivalent to ﬁnding a projection ΠS which makes Y and (I −ΠS)X conditionally
independent given ΠSX. Thus we can turn the dimensionality reduction problem into an
optimization problem by expressing it in terms of covariance operators.
In a presence of a ﬁnite sample, we need to estimate the covariance operator so as to
obtain a sample-based objective function that we can optimize. We derive a natural plugin estimate of the covariance operator, and ﬁnd that the resulting estimate is identical to
the kernel generalized variance that has been described earlier by Bach and Jordan 
in the setting of independent component analysis. In that setting, the goal is to measure
departures from independence, and the minimization of the kernel generalized variance can
be viewed as a surrogate for minimizing a certain mutual information. In the dimensionality
reduction setting, on the other hand, the goal is to measure conditional independence, and
minimizing the kernel generalized variance can be viewed as a surrogate for maximizing
a certain mutual information.
Not surprisingly, the derivation that leads to the kernel
generalized variance that we present here is quite diﬀerent from the one presented in the
earlier work on kernel ICA. Moreover, the argument that we present here can be viewed
as providing a rigorous foundation for other, more heuristic, ways in which the kernel
generalized variance has been used, including the model selection algorithms for graphical
models presented by Bach and Jordan .
The paper is organized as follows. In Section 2, we introduce the problem of dimensionality reduction for supervised learning, and describe its relation with conditional independence and mutual information. Section 3 derives the objective function for estimation
of the eﬀective subspace for regression, and describes the KDR method. All of the mathematical details needed for the results in Section 3 are presented in the Appendix, which
also provides a general introduction to covariance operators in reproducing kernel Hilbert
spaces. In Section 4, we present a series of experiments that test the eﬀectiveness of our
method, comparing it with several conventional methods. Section 5 describes an extension
of KDR to the problem of variable selection. Section 6 presents our conclusions.
Kernel Dimensionality Reduction
2. Dimensionality Reduction for Regression
We consider a regression problem, in which Y is an ℓ-dimensional random vector, and X is
an m-dimensional explanatory variable. (Note again that we use “regression” in a generic
sense that includes both continuous and discrete Y ). The probability density function of Y
given X is denoted by pY |X(y | x). Assume that there is an r-dimensional subspace S ⊂Rm
pY |X(y | x) = pY |ΠSX(y | ΠSx),
for all x and y, where ΠS is the orthogonal projection of Rm onto S. The subspace S is
called the eﬀective subspace for regression.
The problem that we treat here is that of ﬁnding the subspace S given an i.i.d. sample
{(X1, Y1), . . . , (Xn, Yn)} from pX and pY |X. The crux of the problem is that we assume no a
priori knowledge of the regressor, and place no assumptions on the conditional probability
As in the simpler setting of principal component analysis, we make the (generally unrealistic) assumption that the dimensionality r is known and ﬁxed. While choosing the
dimensionality is a very important problem, we separate it from the task of ﬁnding the best
subspace of a ﬁxed dimensionality, and mainly focus on the latter problem. In Section 6,
we brieﬂy discuss various approaches to the estimation of the dimensionality.
The notion of eﬀective subspace can be formulated in terms of conditional independence.
Let (B, C) be the m-dimensional orthogonal matrix such that the column vectors of B span
the subspace S, and deﬁne U = BT X and V = CT X. Because (B, C) is an orthogonal
matrix, we have
pX(x) = pU,V (u, v),
pX,Y (x, y) = pU,V,Y (u, v, y),
for the probability density functions. From Equation (2), Equation (1) is equivalent to
pY |U,V (y | u, v) = pY |U(y | u).
This shows that the eﬀective subspace S is the one which makes Y and V conditionally
independent given U (see Figure 1).
Mutual information provides another point of view on the equivalence between conditional independence and the existence of the eﬀective subspace. From Equation (2), it is
straightforward to see that
I(Y, X) = I(Y, U) + EU
I(Y |U, V |U)
where I(Z, W) denotes the mutual information deﬁned by
I(Z, W) :=
pZ,W (z, w) log pZ,W (z, w)
pZ(z)pW (w)dz dw.
Because Equation (1) means I(Y, X) = I(Y, U), the eﬀective subspace S is characterized
as the subspace which retains the mutual information of X and Y by the projection onto
that subspace, or equivalently, which gives I(Y |U, V |U) = 0. This is again the conditional
independence of Y and V given U.
Fukumizu, Bach and Jordan
Figure 1: Graphical representation of dimensionality reduction for regression. The variables
Y and V are conditionally independent given U, where X = (U, V ).
The expression in Equation (3) can be understood in terms of the decomposition of the
mutual information according to a tree-structured graphical model—a quantity that has
been termed the T-mutual information by Bach and Jordan . Considering the tree
Y −U −V in Figure 1(b), we have that the T-mutual information IT is given by
IT = I(Y, U, V ) −I(Y, U) −I(U, V ).
This is equal to the KL-divergence between a probability distribution on (Y, U, V ) and its
projection onto the family of distributions that factor according to the tree; that is, the
set of distributions that verify Y ⊥⊥V | U.
Using Equation (2), we can easily see that
I(Y, U, V ) = I(Y, X) + I(U, V ), and thus we obtain
IT = I(Y, X) −I(Y, U) = EU[I(Y |U, V |U)].
Then, dimensionality reduction for regression can be viewed as the problem of minimizing
the T-mutual information for the ﬁxed tree structure in Figure 1(b).
From Equation (3) and Equation (4), we see that there are two approaches to solve the
problem of dimensionality reduction for regression; one is to maximize the mutual information I(Y, U), and the other is to make Y and V conditionally independent given U. We
choose the latter in deriving our method. Direct evaluation of mutual information is not a
straightforward task in general, because it requires an explicit form for the probability density functions. Although assuming a speciﬁc probability model leads readily to a solution,
it implies a restriction of the range of problems to which the method can be applied, and
does not satisfy our goal of developing a general semiparametric method. Nonparametric
estimation of the probabilities also provides an approach to evaluation of mutual information. However, nonparametric estimation and numerical integration do not give accurate
estimates for high-dimensional variables, and it is a challenge to make a nonparametric
method viable . An alternative semiparametric approach was presented by
Bach and Jordan , who showed that the kernel generalized variance could serve as
a surrogate for the mutual information in the setting of independent component analysis.
Kernel Dimensionality Reduction
Our approach to dimensionality reduction also makes use of the kernel generalized variance. However, we do not take the mutual information as our point of departure, because
we expect to be far from the setting of mutual independence in which Bach and Jordan
 showed that the mutual information and kernel generalized variance are closely related. Instead, we take an entirely diﬀerent path, presenting a rigorous characterization of
conditional independence using reproducing kernels, and showing that this characterization
leads once again to the kernel generalized variance.
3. Kernel Method for Dimensionality Reduction in Regression
In this section we present our kernel-based method for dimensionality reduction. We discuss the basic deﬁnition and properties of cross-covariance operators on reproducing kernel
Hilbert spaces, derive an objective function for characterizing conditional independence using cross-covariance operators, and ﬁnally present a sample-based objective function based
on this characterization.
3.1 Cross-Covariance Operators on Reproducing Kernel Hilbert Spaces
We use cross-covariance operators on reproducing kernel Hilbert spaces to derive an objective function for dimensionality reduction. While cross-covariance operators are generally
deﬁned for random variables in Banach spaces , the
theory is much simpler for reproducing kernel Hilbert spaces. We summarize only basic
mathematical facts in this subsection, and defer the details to the Appendix. Let (H, k) be
a reproducing kernel Hilbert space of functions on a set Ωwith a positive deﬁnite kernel
k : Ω× Ω→R. The inner product of H is denoted by ⟨·, ·⟩H. We consider only real Hilbert
spaces for simplicity. The most important aspect of reproducing kernel Hilbert spaces is
the reproducing property:
⟨f, k(·, x)⟩H = f(x)
for all x ∈Ωand f ∈H.
Throughout this paper we use the Gaussian kernel
k(x1, x2) = exp
−∥x1 −x2∥2/σ2¢
which corresponds to a Hilbert space of smooth functions.
Let (H1, k1) and (H2, k2) be reproducing kernel Hilbert spaces over measurable spaces
(Ω1, B1) and (Ω2, B2), respectively, with k1 and k2 measurable. For a random vector (X, Y )
on Ω1 × Ω2, the cross-covariance operator from H1 to H2 is deﬁned by the relation
⟨g, ΣY Xf⟩H2 = EXY [f(X)g(Y )] −EX[f(X)]EY [g(Y )]
for all f ∈H1 and g ∈H2. Equation (5) implies that the covariance of f(X) and g(Y ) is
given by the action of the linear operator ΣY X and the inner product. (See the Appendix
for a basic exposition of cross-covariance operators.)
Covariance operators provide a useful framework for discussing conditional probability
and conditional independence. As we show in Corollary 3 of the Appendix, the following
Fukumizu, Bach and Jordan
relation holds between the conditional expectation and the cross-covariance operator, given
that ΣXX is invertible:1
EY |X[g(Y ) | X] = Σ−1
for all g ∈H2,
Equation (6) can be understood by analogy to the conditional expectation of Gaussian
random variables. If X and Y are Gaussian random variables, it is well known that the
conditional expectation is given by
EY |X[aT Y | X = x] = xT Σ−1
for an arbitrary vector a, where ΣXX and ΣXY are the variance-covariance matrices in the
ordinary sense.
3.2 Conditional Covariance Operators and Conditional Independence
We derive an objective function for characterizing conditional independence using crosscovariance operators. Suppose we have random variables X and Y on Rm and Rℓ, respectively. The variable X is decomposed into U ∈Rr and V ∈Rm−r so that X = (U, V ).
For the function spaces corresponding to Y , U and V , we consider the reproducing kernel
Hilbert spaces (H1, k1), (H2, k2), and (H3, k3) on Rℓ, Rr, and Rm−r, respectively, each endowed with Gaussian kernels. We deﬁne the conditional covariance operator ΣY Y |U on H1
ΣY Y |U := ΣY Y −ΣY UΣ−1
where ΣY Y , ΣUU, ΣY U are the corresponding covariance operators. As shown by Proposition 5 in the Appendix, the operator ΣY Y |U captures the conditional variance of a random
variable in the following way:
⟨g, ΣY Y |Ug⟩H1 = EU
VarY |U[g(Y ) | U]
where g is an arbitrary function in H1.
As in the case of Equation (7), we can make
an analogy to Gaussian variables. In particular, Equations (8) and (9) can be viewed as
the analogs of the following well-known equality for the conditional variance of Gaussian
variables:
Var[aT Y | U] = aT (ΣY Y −ΣY UΣ−1
It is natural to use minimization of ΣY Y |U as the basis of a method for ﬁnding the
most informative direction U. This intuition is justiﬁed theoretically by Theorem 7 in the
Appendix. That theorem shows that
ΣY Y |U ≥ΣY Y |X
ΣY Y |U −ΣY Y |X = 0
Y ⊥⊥V | U,
1. Even if ΣXX is not invertible, a similar fact holds. See Corollary 3.
Kernel Dimensionality Reduction
where, in Equation (10), the inequality should be understood as the partial order of selfadjoint operators. From these relations, the eﬀective subspace S can be characterized in
terms of the solution to the following minimization problem:
S ΣY Y |U,
subject to U = ΠSX.
In the following section we show how to turn this population-based criterion into a samplebased criterion that can be optimized in the presence of a ﬁnite sample.
3.3 Kernel Generalized Variance for Dimensionality Reduction
To derive a sample-based objective function from Equation (11), we have to estimate the
conditional covariance operator from data, and choose a speciﬁc way to evaluate the size
of self-adjoint operators.
While there are many possibilities for approaching these two
problems, we make a speciﬁc choice, adopting an approach which has been used successfully
for independent component analysis .
We describe a regularized empirical estimate of the cross-covariance operator. Deﬁne
∈H1 by ˜k(i)
= k1(·, Yi)−1
j=1 k1(·, Yj), and ˜k(i)
∈H2 similarly using Ui. By replacing
the expectation with the empirical average, the covariance is estimated by
Let ˆKY be the centralized Gram matrix ,
where (GY )ij = k1(Yi, Yj) is the Gram matrix and 1n = (1, . . . , 1)T is the vector with all
elements equal to 1. The matrix ˆKU is deﬁned similarly using {Ui}n
i=1. Then, it is easy
to see that
ij and ⟨˜k(i)
ij. If we decompose f and g as
ℓ=1 ai˜k(ℓ)
+ f⊥and g = Pn
m=1 bm˜k(m)
+ g⊥, where f ⊥and g⊥are orthogonal to the
linear hull of {˜k(i)
i=1 and {˜k(i)
i=1, respectively, we see that the covariance is approximated
¡ ˆKY ˆKU)lm.
Thus, by restricting the covariance operator ΣY U to the n-dimensional subspaces spanned
i=1 and {k(i)
i=1, we can estimate the operator by
ˆΣY U = ˆKY ˆKU.
In estimating ΣY Y and ΣUU, we use the same regularization technique as Bach and Jordan
 . The empirical conditional covariance matrix ˆΣY Y |U is then deﬁned by
ˆΣY Y |U := ˆΣY Y −ˆΣY U ˆΣ−1
UU ˆΣUY = ( ˆKY + εIn)2 −ˆKY ˆKU( ˆKU + εIn)−2 ˆKU ˆKY ,
where ε > 0 is a regularization constant.
Fukumizu, Bach and Jordan
The size of ˆΣY Y |U in the ordered set of positive deﬁnite matrices can be evaluated by
its determinant. Although there are other choices for measuring the size of ˆΣY Y |U, such
as the trace and the largest eigenvalue, we focus on the determinant in this paper. Using
the Schur decomposition det(A−BC−1BT ) = det
/detC, the determinant of ˆΣY Y |U
can be written as follows:
det ˆΣY Y |U = det ˆΣ[Y U][Y U]
where ˆΣ[Y U][Y U] is deﬁned by
ˆΣ[Y U][Y U] =
µ( ˆKY + εIn)2
( ˆKU + εIn)2
We symmetrize the objective function by dividing by the constant det ˆΣY Y , which yields
the objective function
det ˆΣ[Y U][Y U]
det ˆΣY Y det ˆΣUU
We refer to the problem of minimizing this function with respect to the choice of subspace
S as Kernel Dimensionality Reduction (KDR).
Equation (14) has been termed the “kernel generalized variance” by Bach and Jordan
 , who used it as a contrast function for independent component analysis. In that
setting, the goal is to minimize a mutual information (among a set of recovered “source”
variables), in the attempt to obtain independent components.
Bach and Jordan 
showed that the kernel generalized variance is in fact an approximation of the mutual
information of the recovered sources, when this mutual information is expanded around
the manifold of factorized distributions. In the current setting, on the other hand, our
goal is to maximize the mutual information I(Y, U), and we certainly do not expect to be
near a manifold in which Y and U are independent. Thus the argument for the kernel
generalized variance as an objective function in the ICA setting does not apply here. What
we have provided in the previous section is an entirely distinct argument that shows that the
kernel generalized variance is in fact an appropriate objective function for the dimensionality
reduction problem, and that minimizing the kernel generalized variance in Equation (14)
can be viewed as a surrogate for maximizing the mutual information I(Y, U), while the value
of I(Y, U) may not be explicitly related to the value of the kernel generalized variance.
For the optimization of Equation (14), we use gradient descent with line search in our
experiments.
In a straightforward implementation, the matrix B is updated iteratively
according to
B(t + 1) = B(t) −η ∂log det ˆΣY Y |U
= B(t) −ηTr
where η is optimized through golden section search. The trace term in Equation (15) is
rewritten by
Y Y |U ˆKY
¡ ˆKU + εIn
¡ ˆKU + εIn
¢−2 ˆKU ˆKY
Kernel Dimensionality Reduction
All of these matrices are calculated directly from the deﬁnitions in Equations (12) and (13).
Given that the numerical task that must be solved in KDR is the same as the numerical
task that must be solved in kernel ICA, we can import all of the computational techniques
developed by Bach and Jordan for minimizing kernel generalized variance in the
KDR setting.
In particular, we can exploit incomplete Cholesky decomposition, which
approximates an n × n positive semideﬁnite matrix K by K ≈AAT , where A is an n × ℓ
matrix for ℓ< n. Application of this decomposition reduces the n×n matrices ˆKY and ˆKU
required in Equation (15) to low-rank approximations. This diminishes the computational
cost associated with multiplying and inverting large matrices, especially for a large n. For
an exposition of incomplete Cholesky decomposition, see Bach and Jordan . Another
computational issue, which may arise in minimizing Equation (14), is the problem of local
minima, because the objective function is not a convex function. To cope with this problem,
we make use of an annealing technique, in which the scale parameter σ for the Gaussian
kernel is decreased gradually during the iterations of optimization.
For a larger σ, the
contrast function is smoother with fewer local optima, which makes optimization easier.
The search becomes more accurate as σ is decreased.
4. Experimental Results
We study the eﬀectiveness of the new method through experiments, comparing it with
several conventional methods: SIR, pHd, CCA, and PLS. For the experiments with SIR
and pHd, we use an implementation in R due to Weisberg .
In all of our experiments, we use a ﬁxed value 0.1 for the regularization coeﬃcient ε;
empirically, the performance of the algorithm is robust to small variations in this coeﬃcient.
This coeﬃcient could also be chosen using cross-validation.
4.1 Synthetic Data
The data sets Data I and Data II comprise one-dimensional Y and two-dimensional X =
(X1, X2). One hundred i.i.d. data points are generated by
Y ∼1/(1 + exp(−X1)) + Z,
Y ∼2 exp(−X2
where Z ∼N(0, 0.12), and X = (X1, X2) follows a normal distribution and a normal
mixture with two components for Data I and Data II, respectively. The eﬀective subspace
is spanned by B0 = (1, 0)T in both cases. The data sets are depicted in Figure 2.
Table 1 shows the angles between B0 and the estimated direction. For Data I, all the
methods except PLS yield a good estimate of B0. Data II is surprisingly diﬃcult for the
conventional methods, presumably because the distribution of X is not spherical and the
regressor has a strong nonlinearity. This indicates the weakness of model-based methods;
if data do not ﬁt the model, the obtained result may not be meaningful. The KDR method
succeeds in ﬁnding the correct direction for both data sets.
Data III has 300 samples of 17 dimensional X and one dimensional Y , which are generated by
Y ∼0.9X1 + 0.2
Fukumizu, Bach and Jordan
I: (X1, Y )
I: (X1, X2)
II: (X1, Y )
II: (X1, X2)
Figure 2: Data I and II. One-dimensional Y depends only on X1 in X = (X1, X2).
I: angle (rad.)
II: angle (rad.)
Table 1: Angles between the true and the estimated subspaces for Data I and II.
where Z ∼N(0, 0.012) and X follows a uniform distribution on 17.
The eﬀective
subspace is given by b1 = (1, 0, . . . , 0) and b2 = (0, . . . , 0, 1). We compare the KDR method
with SIR and pHd only—CCA and PLS cannot ﬁnd a two-dimensional subspace, because Y
is one-dimensional. To evaluate the accuracy of the results, we use the multiple correlation
R(b) = max
βT ΣXXβ · bT ΣXXb
which is used in Li . As shown in Table 2, the KDR method outperforms the others
in ﬁnding the weak contribution of the second direction.
Table 2: Correlation coeﬃcients for Data III. SIR(m) indicates SIR with m slices.
Kernel Dimensionality Reduction
training sample
test sample
Heart-disease
Ionosphere
Breast-cancer-Wisconsin
Table 3: Data description for the binary classiﬁcation problem.
4.2 Real Data: Classiﬁcation
In this section we apply the KDR method to classiﬁcation problems. Many conventional
methods of dimensionality reduction for regression are not suitable for classiﬁcation. In
particular, in the case of SIR, the dimensionality of the eﬀective subspace must be less than
the number of classes, because SIR uses the average of X in slices along the variable Y .
Thus, in binary classiﬁcation, only a one-dimensional subspace can be found, because at
most two slices are available. The methods CCA and PLS have a similar limitation on the
dimensionality of the eﬀective subspace; they cannot ﬁnd a subspace of larger dimensionality than that of Y . Thus our focus is the comparison between KDR and pHd, which is
applicable to general binary classiﬁcation problems. Note that Cook and Lee discuss dimensionality reduction methods for binary classiﬁcation, and propose the diﬀerence
of covariance (DOC) method. They compare pHd and DOC theoretically, and show that
these methods are the same in binary classiﬁcation if the population ratio of the classes is
1/2, which is almost the case in our experiments.
In the ﬁrst experiment, we demonstrate the visualization capability of the dimensionality
reduction methods. We use the Wine data set in the UCI machine learning repository to see how the projection onto a low-dimensional space realizes an
eﬀective description of data. The wine data consist of 178 samples with 13 variables and
a label of three classes. We apply the KDR method, CCA, PLS, SIR, and pHd to these
data. Figure 3 shows the projection onto the two-dimensional subspace estimated by each
method. The KDR method separates the data into three classes most completely, while
CCA also shows perfect separation. We can see that the data are nonlinearly separable in
the two-dimensional space. The other methods do not separate the classes completely.
Next we investigate how much information on Y is preserved in the estimated subspace.
After reducing the dimensionality, we use the support vector machine (SVM) method to
build a classiﬁer in the reduced space, and compare its accuracy with an SVM trained
using the full dimensional vector X.2 Although we should theoretically use the (unknown)
optimum classiﬁer to evaluate the extent of the information preserved in the subspace, we
use an SVM classiﬁer whose parameters were chosen by exhaustive grid search. We use the
Heart-disease data set,3 Ionosphere, and Wisconsin-breast-cancer from the UCI repository.
A description of these data is presented in Table 3.
2. In our experiments with the SVM, we used the Matlab Support Vector Toolbox by S. Gunn; see
 
3. We use the Cleveland data set, created by Dr. Robert Detrano of V.A. Medical Center, Long Beach and
Cleveland Clinic Foundation. Although the original data set has ﬁve classes, we use only “no presence”
(0) and “presence” (1-4) for the binary class labels. Samples with missing values are removed in our
experiments.
Fukumizu, Bach and Jordan
Figure 4 shows the classiﬁcation rates for the test set in subspaces of various dimensionality. We can see that KDR yields good separation even in low-dimensional subspaces,
while pHd is much worse in low dimensions. It is noteworthy that in the Ionosphere data set
the classiﬁer in dimensions 5, 10, and 20 outperforms the classiﬁer in the full dimensional
space. This is presumably due to the suppression of noise irrelevant to the prediction of
Y . These results show that the kernel method successfully ﬁnds a subspace which preserves
the class information even when the dimensionality is reduced signiﬁcantly.
5. Extension to Variable Selection
In this section, we describe an extension of the KDR method to the problem of variable
selection. Variable selection is diﬀerent from dimensionality reduction; the former involves
selecting a subset of the explanatory variables {X1, . . . , Xm} in order to obtain a simpliﬁed
prediction of Y from X, while the latter involves ﬁnding linear combinations of the variables.
However, the objective function that we have presented for dimensionality reduction can
be extended straightforwardly to variable selection. In particular, given a ﬁxed number of
variables to be selected, we can compare the KGV for subspaces spanned by combinations of
this number of selected variables. This gives a reasonable way to select variables, because for
a subset W = {Xj1, . . . , Xjr} ⊂{X1, . . . , Xm}, the variables Y and W C are conditionally
independent given W if and only if Y and ΠW cX are conditionally independent given ΠW X,
where ΠW and ΠW C are the orthogonal projections onto the subspaces spanned by W and
W C, respectively. If we try to select r variables from among m explanatory variables, the
total number of evaluations is
is large, we must address the computational cost that arises in comparing
large numbers of subsets. As in most other approaches to variable selection , we propose the use of a greedy algorithm and random search for this
combinatorial aspect of the problem. (In the experiments presented in the current paper,
however, we conﬁne ourselves to small problems in which all combinations are tractably
evaluated).
We apply this kernel-based method of variable selection to the Boston Housing data
 and the Ozone data , which
have been often used as testbed examples for variable selection. Tables 4 and 5 give the
detailed description of the data sets. There are 506 samples in the Boston Housing data, for
which the variable MV, the median value of house prices in a tract, is estimated by using
the 13 other variables. We use the corrected version of the data set given by Gilley and
Pace . In the Ozone data in which there are 330 samples, the variable UPO3 (the
ozone concentration) is to be predicted by 9 other variables.
Table 6 shows the best three sets of four variables that attain the smallest values of the
kernel generalized variance. For the Boston Housing data, RM and LSTAT are included
in all the three of the result sets in Table 6, and PTRATIO and TAX are included in two
of them. This observation agrees well with the analysis using alternating conditional expectation (ACE) by Breiman and Friedman , which gives RM, LSTAT, PTRATIO,
and TAX as the four major contributors. The original motivation in the study was to investigate the inﬂuence of nitrogen oxide concentration (NOX) on the house price . In accordance with the previous studies, our analysis shows a rel-
Kernel Dimensionality Reduction
Figure 3: Wine data. Projections onto the estimated two-dimensional space. The symbols
‘+’, ‘2’, and gray ‘⃝’ represent the three classes.
Fukumizu, Bach and Jordan
(a) Heart-disease
Number of variables
Classification rate (%)
All variables
(b) Ionosphere
Number of variables
Classification rate (%)
All variables
(c) Wisconsin Breast Cancer
Number of variables
Classification rate (%)
All variables
Figure 4: Classiﬁcation accuracy of the SVM for test data after dimensionality reduction.
Kernel Dimensionality Reduction
Description
median value of owner-occupied home
crime rate by town
proportion of town’s residential land zoned for lots
greater than 25,000 square feet
proportion of nonretail business acres per town
Charles River dummy
(= 1 if tract bounds the Charles River, 0 otherwise)
nitrogen oxide concentration in pphm
average number of rooms in owner units
proportion of owner units build prior to 1940
weighted distances to ﬁve employment centers
in the Boston region
index of accessibility to radial highways
full property tax rate ($/$10,000)
pupil-teacher ratio by town school district
black proportion of population
proportion of population that is lower status
Table 4: Boston Housing Data
Description
upland ozone concentration (ppm)
Vandenburg 500 millibar height (m)
himidity (percent)
inversion base height (ft.)
Daggett pressure gradient (mmhg)
inversion base temperature (◦F)
Sandburg Air Force Base temperature (◦C)
visibility (miles)
wind speed (mph)
day of the year
Table 5: Ozone data
atively small contribution of NOX. For the Ozone data, all three of the result sets in the
variable selection method include HMDT, SBTP, and IBHT. The variables IBTP, DGPG,
and VDHT are chosen in one of the sets. This shows a fair accordance with earlier results
by Breiman and Friedman and Li et al. ; the former concludes by ACE that
SBTP, IBHT, DGPG, and VSTY are the most inﬂuential, and the latter selects HMDT,
IBHT, and DGPG using a pHd-based method.
6. Conclusion
We have presented KDR, a new kernel-based approach to dimensionality reduction for regression and classiﬁcation. One of the most notable aspects of this method is its generality—
Fukumizu, Bach and Jordan
Table 6: Variable selection using the proposed kernel method.
we do not impose any strong assumptions on either the conditional or the marginal distribution. This allows the method to be applicable to a wide range of problems, and gives
it a signiﬁcant practical advantage over existing methods such as CCA, PPR, SIR, and
pHd. These methods all impose signiﬁcant restrictions on the conditional probability, the
marginal distribution, or the dimensionality of the eﬀective subspaces.
Our experiments have shown that the KDR method can provide many of the desired
eﬀects of dimensionality reduction: it provides data visualization capabilities, it can successfully select important explanatory variables in regression, and it can yield classiﬁcation
performance that is better than the performance achieved with the full-dimensional covariate space. We have also discussed the extension of the KDR method to variable selection.
Experiments with classical data sets has shown an accordance with the previous results on
these data sets and suggest that further study of this application of KDR is warranted.
The theoretical basis of KDR lies in the nonparametric characterization of conditional
independence that we have presented in this paper. Extending earlier work on the kernelbased characterization of independence in ICA , we have shown
that conditional independence can be characterized in terms of covariance operators on a
reproducing kernel Hilbert space. While our focus has been on the problem of dimensionality
reduction, it is also worth noting that there are many other possible applications of this
characterization. In particular, conditional independence plays an important role in the
structural deﬁnition of probabilistic graphical models, and our results may have applications
to model selection and inference in graphical models.
There are several statistical problems which need to be addressed in further research on
KDR. First, a basic analysis of the statistical consistency of the KDR-based estimator—
the convergence of the estimator to the true subspace when such a space really exists—is
needed. We expect that, to prove consistency, we will need to impose a condition on the
rate of decrease of the regularization coeﬃcient ε as the sample size n goes to inﬁnity.
Second, and most signiﬁcantly, we need rigorous methods for choosing the dimensionality
Kernel Dimensionality Reduction
of the eﬀective subspace. If the goal is that of achieving high predictive performance after
dimensionality reduction, we can use one of many existing methods (e.g., cross-validation,
penalty-based methods) to assess the expected generalization as a function of dimensionality.
Note in particular that by using KDR as a method to select an estimator given a ﬁxed
dimensionality, we have substantially reduced the number of hypotheses being considered,
and expect to ﬁnd ourselves in a regime in which methods such as cross-validation are
likely to be eﬀective. It is also worth noting, however, that the goals of dimensionality
reduction are not always simply that of prediction; in particular, the search for small sets
of explanatory variables will need to be guided by other principles. Finally, asymptotic
analysis may provide useful guidance for selecting the dimensionality; an example of such
an analysis that we believe can be adopted for KDR has been presented by Li for
the SIR method.
Acknowledgments
This work was done while the ﬁrst author was visiting the University of California, Berkeley.
We thank the action editor and reviewers for their valuable comments. In particular, the
proof of Theorem 6 was suggested by one of the reviewers. We also thank Dr. Noboru
Murata of Waseda University and Dr. Motoaki Kawanabe of Fraunhofer, FIRST for their
helpful comments on the early version of this work. We wish to acknowledge support from
JSPS KAKENHI 15700241, ONR MURI N00014-00-1-0637, NSF grant IIS-9988642, and a
grant from Intel Corporation.
Appendix A. Cross-Covariance Operators on Reproducing Kernel
Hilbert Spaces and Independence of Random Variables
In this appendix, we present additional background and detailed proofs for results relating cross-covariance operators to marginal and conditional independence between random
variables.
A.1 Cross-Covariance Operators
While cross-covariance operators are generally deﬁned for random variables on Banach
spaces Vakhania et al. , Baker , they are more easily deﬁned on reproducing
kernel Hilbert spaces (RKHS). In this subsection, we summarize some of the basic mathematical facts used in Sections 3.1 and 3.3.
While we discuss only real Hilbert spaces,
extension to the complex case is straightforward.
Theorem 1 Let (Ω1, B1) and (Ω2, B2) be measurable spaces, and let (H1, k1) and (H2, k2)
be reproducing kernel Hilbert spaces on Ω1 and Ω2, respectively, with k1 and k2 measurable. Suppose we have a random vector (X, Y ) on Ω1 × Ω2 such that EX[k1(X, X)] and
EY [k2(Y, Y )] are ﬁnite. Then, there exists a unique operator ΣY X from H1 to H2 such that
⟨g, ΣY Xf⟩H2 = EXY [f(X)g(Y )] −EX[f(X)]EY [g(Y )]
holds for all f ∈H1 and g ∈H2. This is called the cross-covariance operator.
Fukumizu, Bach and Jordan
Obviously, the operator is unique, if it exists. From Riesz’s representation theorem , the existence of ΣY Xf ∈H2 for
a ﬁxed f can be proved by showing that the right hand side of Equation (16) is a bounded
linear functional on H2. The linearity is obvious, and the boundedness is shown by
¯¯EXY [f(X)g(Y )] −EX[f(X)]EY [g(Y )]
¯¯⟨k1(·, X), f⟩H1⟨k2(·, Y ), g⟩H2
¯¯⟨k1(·, X), f⟩H1
¯¯⟨k2(·, Y ), g⟩H2
∥k1(·, X)∥H1∥f∥H1∥k2(·, Y )∥H2∥g∥H2
∥k1(·, X)∥H1∥f∥H1
∥k2(·, Y )∥H2∥g∥H2
EX[k1(X, X)]1/2EY [k2(Y, Y )]1/2 + EX[k1(X, X)1/2]EY [k2(Y, Y )1/2]
∥f∥H1∥g∥H2
¤1/2∥f∥H1∥g∥H2.
For the second last inequality, ∥k(·, x)∥2
H = k(x, x) is used. The linearity of the map ΣY X
is given by the uniqueness part of Riesz’s representation theorem.
From Equation (17), ΣY X is bounded, and by deﬁnition, we see Σ∗
Y X = ΣXY , where A∗
denotes the adjoint of A. If the two RKHS are the same, the operator ΣXX is called the
covariance operator. A covariance operator ΣXX is bounded, self-adjoint, and trace-class.
In an RKHS, conditional expectations can be expressed by cross-covariance operators,
in a manner analogous to ﬁnite-dimensional Gaussian random variables.
Theorem 2 Let (H1, k1) and (H2, k2) be RKHS on measurable spaces Ω1 and Ω2, respectively, with k1 and k2 measurable, and (X, Y ) be a random vector on Ω1 × Ω2. Assume that
EX[k1(X, X)] and EY [k2(Y, Y )] are ﬁnite, and for all g ∈H2 the conditional expectation
EY |X[g(Y ) | X = ·] is an element of H1. Then, we have for all g ∈H2
ΣXXEY |X[g(Y ) | X = ·] = ΣXY g,
where ΣXX and ΣXY are the covariance and cross-covariance operator.
Proof For any f ∈H1, we have
⟨f, ΣXXEY |X[g(Y ) | X = ·]⟩H1
f(X)EY |X[g(Y ) | X]
−EX[f(X)]EX
EY |X[g(Y ) | X]
= EXY [f(X)g(Y )] −EX[f(X)]EY [g(Y )]
⟨f, ΣXY g⟩H1.
This completes the proof.
Corollary 3 Let ˜Σ−1
XX be the right inverse of ΣXX on (KerΣXX)⊥. Under the same assumptions as Theorem 2, we have
XXΣXY g⟩= ⟨f, EY |X[g(Y ) | X = ·]⟩
for all f ∈(KerΣXX)⊥and g ∈H2. In particular, if KerΣXX = 0, we have
XXΣXY g = EY |X[g(Y ) | X = ·].
Kernel Dimensionality Reduction
Proof Note that the product ˜Σ−1
XXΣXY is well-deﬁned, because RangeΣXY ⊂RangeΣXX =
(KerΣXX)⊥. The ﬁrst inclusion is shown from the expression ΣXY = Σ1/2
Y Y with a
bounded operator V , and the second equation holds for any
self-adjoint operator. Take f = ΣXXh ∈RangeΣXX. Then, Theorem 2 yields
XXΣXY g⟩= ⟨h, ΣXX ˜Σ−1
XXΣXXEY |X[g(Y ) | X = ·]⟩
= ⟨h, ΣXXEY |X[g(Y ) | X = ·]⟩= ⟨f, EY |X[g(Y ) | X = ·]⟩.
This completes the proof.
The assumption EY |X[g(Y ) | X = ·] ∈H1 in Theorem 2 can be simpliﬁed so that it can
be checked without reference to a speciﬁc g.
Proposition 4 Under the condition of Theorem 2, if there exists C > 0 such that
EY |X[k2(y1, Y ) | X = x1]EY |X[k2(y2, Y ) | X = x2] ≤Ck1(x1, x2)k2(y1, y2)
for all x1, x2 ∈Ω1 and y1, y2 ∈Ω2, then for all g ∈H2 the conditional expectation
EY |X[g(Y ) | X = ·] is an element of H1.
Proof See Theorem 2.3.13 in Alpay .
From this proposition, it is obvious that EY |X[g(Y ) | X = ·] ∈H1 holds, if the range of X
and Y are bounded.
For a function f in an RKHS, the expectation of f(X) can be formulated as the inner
product of f and a ﬁxed element. Let (Ω, B) be a measurable space, and (H, k) be an RKHS
on Ωwith k measurable. Note that for a random variable X on Ω, the linear functional
f 7→EX[f(X)] is bounded if EX[k(X, X)] exists. By Riesz’s theorem, there is u ∈H such
that ⟨u, f⟩H = EX[f(X)] for all f ∈H. If we deﬁne EX[k(·, X)] ∈H by this element u, we
formally obtain the equality
⟨EX[k(·, X)], f⟩H = EX[⟨k(·, X), f⟩H],
which looks like the interchangeability of the expectation by X and the inner product.
While the expectation EX[k(·, X)] can be deﬁned, in general, as an integral with respect to
the distribution on H induced by k(·, X), the element EX[k(·, X)] is formally obtained as
above in a reproducing kernel Hilbert space.
A.2 Conditional Covariance Operator and Conditional Independence
We deﬁne the conditional (cross-)covariance operator, and derive its relation with the conditional covariance of random variables. Let (H1, k1), (H2, k2), let (H3, k3) be RKHS on
measurable spaces Ω1, Ω2, and Ω3, respectively, and let (X, Y, Z) be a random vector on
Ω1 × Ω2 × Ω3. The conditional cross-covariance operator of (X, Y ) given Z is deﬁned by
ΣY X|Z := ΣY X −ΣY Z ˜Σ−1
Because KerΣZZ ⊂KerΣY Z from the fact ΣY Z = Σ1/2
Y Y V Σ1/2
ZZ for some bounded operator
V , the operator ΣY ZΣ−1
ZZΣY X can be uniquely deﬁned, even if
Fukumizu, Bach and Jordan
ZZ is not unique. By abuse of notation, we write ΣY ZΣ−1
ZZΣZX, when cross-covariance
operators are discussed.
The conditional cross-covariance operator is related to the conditional covariance of the
random variables.
Proposition 5 Let (H1, k1), (H2, k2), and (H3, k3) be reproducing kernel Hilbert spaces on
measurable spaces Ω1, Ω2, and Ω3, respectively, with ki measurable, and let (X, Y, Z) be
a measurable random vector on Ω1 × Ω2 × Ω3 such that EX[k1(X, X)], EY [k2(Y, Y )], and
EZ[k3(Z, Z)] are ﬁnite. It is assumed that EX|Z[f(X) | Z = ·] and EY |Z[g(Y ) | Z = ·] are
elements of H3 for all f ∈H1 and g ∈H2. Then, for all f ∈H1 and g ∈H2, we have
⟨g, ΣY X|Zf⟩H2 = EXY [f(X)g(Y )] −EZ
EX|Z[f(X) | Z]EY |Z[g(Y ) | Z]
f(X), g(Y ) | Z
From the decomposition ΣY Z = Σ1/2
Y Y V Σ1/2
ZZ, we have ΣZY g ∈(KerΣZZ)⊥. Then,
by Corollary 3, we obtain
⟨g, ΣY Z ˜Σ−1
ZZΣZXf⟩= ⟨ΣZY g, ˜Σ−1
ZZΣZXf⟩= ⟨ΣZY g, EX|Z[f(X) | Z]⟩
g(Y )EX|Z[f(X) | Z]
−EX[f(X)]EY [g(Y )].
From this equation, the theorem is proved by
⟨g, ΣY X|Zf⟩= EXY [f(X)g(Y )] −EX[f(X)]EY [g(Y )]
g(Y )EX|Z[f(X) | Z]
+ EX[f(X)]EY [g(Y )]
= EXY [f(X)g(Y )] −EZ
EX|Z[f(X) | Z]EY |Z[g(Y ) | Z]
The following deﬁnition is needed to state our main theorem. Let (Ω, B) be a measurable
space, let (H, k) be a RKHS over Ωwith k measurable and bounded, and let S be the set
of all the probability measures on (Ω, B). The RKHS H is called probability-determining, if
(f 7→EX∼P [f(X)]) ∈H∗
is one-to-one, where H∗is the dual space of H. From Riesz’s theorem, H is probabilitydetermining if and only if the map
EX∼P [k(·, X)] ∈H
is one-to-one. For Gaussian kernels, the following theorem can be proved by an argument
similar to that used in the proof of Theorem 2 in Bach and Jordan and the uniqueness
of the characteristic function. For completeness, we present another simple proof here.
Theorem 6 For an arbitrary σ > 0, the reproducing kernel Hilbert space H with Gaussian
kernel k(x, y) = exp(−∥x −y∥2/σ2) on Rm is probability-determining.
Kernel Dimensionality Reduction
Suppose P and Q are diﬀerent probabilities on Rm such that EZ∼P [f(Z)] =
EZ∼Q[f(Z)] for all f ∈H. Let y1 and y2 be two diﬀerent vectors in Rm, and Y be a
random variable with probability 1/2 for each of Y = y1 and Y = y2. Deﬁne a random
variable X so that the probability of X given Y = y1 and Y = y2 are P and Q, respectively.
Noting that the marginal distribution of X is (P + Q)/2, we have for all f, g ∈H,
EX,Y [f(X)g(Y )] −EX[f(X)]EY [g(Y )] = EY
EX|Y [f(X)|Y ]g(Y )
−EX[f(X)]EY [g(Y )]
2g(y1)EX|Y [f(X)|Y = y1] + 1
2g(y2)EX|Y [f(X)|Y = y2]
EZ∼P [f(Z)] + EZ∼Q[f(Z)]
¢g(y1) + g(y2)
From Theorem 2 in Bach and Jordan , X and Y must be independent, which contradicts the construction of X and Y .
Recall that for two RKHS H1 and H2 on Ω1 and Ω2, respectively, the direct product
H1 ⊗H2 is the RKHS on Ω1 × Ω2 with the positive deﬁnite kernel k1k2 (see Aronszajn,
Note that if the two RKHS have Gaussian kernels, their direct product is also
a RKHS with Gaussian kernel, and thus probability-determining. The relation between
conditional independence and the conditional covariance operator is given by the following
Theorem 7 Let (H11, k11), (H12, k12), and (H2, k2) be reproducing kernel Hilbert spaces
on measurable spaces Ω11, Ω12, and Ω2, respectively, with continuous and bounded kernels.
Let (X, Y ) = (U, V, Y ) be a random vector on Ω11 × Ω12 × Ω2, where X = (U, V ), and let
H1 = H11 ⊗H12 be the direct product. It is assumed that EY |U[g(Y ) | U = ·] ∈H11 and
EY |X[g(Y ) | X = ·] ∈H1 for all g ∈H2. Then, we have
ΣY Y |U ≥ΣY Y |X,
where the inequality refers to the order of self-adjoint operators, and if further H2 is
probability-determining, the following equivalence holds
ΣY Y |X = ΣY Y |U
Y ⊥⊥V | U.
The right hand side of Equation (21) is equivalent to PY |X = PY |U, where PY |X
and PY |U are the conditional probability of Y given X and given U, respectively. Taking
the expectation of the well-known equality
VarY |U[g(Y ) | U] = EV |U
VarY |U,V [g(Y ) | U, V ]
EY |U,V [g(Y ) | U, V ]
with respect to U, we derive
VarY |U[g(Y ) | U]
VarY |X[g(Y ) | X]
VarV |U[EY |X[g(Y ) | X]]
Since the last term of Equation (22) is nonnegative, we obtain Equation (20) from Proposition 5.
Fukumizu, Bach and Jordan
Equality holds if and only if VarV |U[EY |X[g(Y ) | X]] = 0 for almost every U, which
means EY |X[g(Y ) | X] does not depend on V almost surely. This is equivalent to
EY |X[g(Y ) | X] = EY |U[g(Y ) | U]
for almost every V and U. Because H2 is probability-determining, this means PY |X = PY |U.
A.3 Conditional Cross-Covariance Operator and Conditional Independence
Theorem 7 characterizes conditional independence using the conditional covariance operator. Another formulation is possible with a conditional cross-covariance operator.
Let (Ω1, B1), (Ω2, B2), and (Ω3, B3) be measurable spaces, and let (X, Y, Z) be a random
vector on Ω1 × Ω2 × Ω3 with law PXY Z. We deﬁne a probability measure EZ[PX|Z ⊗PY |Z]
on Ω1 × Ω2 by
EZ[PX|Z ⊗PY |Z](A × B) = EZ
EX|Z[χA|Z] EY |Z[χB | Z]
where χA is the characteristic function of a measurable set A. It is canonically extended to
any product-measurable sets in Ω1 × Ω2.
Theorem 8 Let (Ωi, Bi) (i = 1, 2, 3) be a measurable space, let (Hi, ki) be a RKHS on Ωi
with kernel measurable and bounded, and let (X, Y, Z) be a random vector on Ω1 × Ω2 × Ω3.
It is assumed that EX|Z[f(X) | Z = ·] and EY |Z[g(Y ) | Z = ·] belong to H3 for all f ∈H1
and g ∈H2, and that H1 ⊗H2 is probability-determining. Then, we have
ΣY X|Z = O
PXY = EZ[PX|Z ⊗PY |Z].
Proof The right-to-left direction is trivial from Theorem 5 and the deﬁnition of EZ[PX|Z ⊗
PY |Z]. The left-hand side yields EZ[EX|Z[f(X) | Z]EY |Z[g(Y ) | Z]] = EXY [f(X)g(Y )] for
all f ∈H1 and g ∈H2.
Because H1 ⊗H2 is deﬁned as the completion of all the linear combinations of fi(x)gi(y) for fi ∈H1 and gi ∈H2, we have E(X′,Y ′)∼Q[h(X′, Y ′)] =
EXY [h(X, Y )] for every such linear combination, and thus every h ∈H1 ⊗H2 as a limit,
where Q = EZ[PX|Z ⊗PY |Z].
This implies the right-hand side, because H1 ⊗H2 is
probability-determining.
The right-hand side of Equation (23) is weaker than the conditional independence of X
and Y given Z. However, if Z is a part of X, we obtain conditional independence.
Corollary 9 Let (H11, k11), (H12, k12), and (H2, k2) be reproducing kernel Hilbert spaces
on measurable spaces Ω11, Ω12, and Ω2, respectively, with kernels measurable and bounded.
Let (X, Y ) = (U, V, Y ) be a random vector on Ω11 × Ω12 × Ω2, where X = (U, V ), and
let H1 = H11 ⊗H12 be the direct product. It is assumed that EX|U[f(X) | U = ·] and
EY |U[g(Y ) | U = ·] belong to H11 for all f ∈H1 and g ∈H2, and H1 ⊗H2 is probabilitydetermining. Then, we have
ΣY X|U = O
Y ⊥⊥V | U.
Kernel Dimensionality Reduction
Proof For any measurable sets A ⊂Ω11, B ⊂Ω12, and C ⊂Ω2, we have, in general,
EX|U[χA×B(U, V ) | U]EY |U[χC(Y ) | U]
−EXY [χA×B(U, V )χC(Y )]
EV |U[χB(V ) | U]χA(U)EY |U[χC(Y ) | U]
EV Y |U[χB(V )χC(Y ) | U]χA(U)
PV |U(B | u)PY |U(C | u) −PV Y |U(B × C | u)
From Theorem 8, the left-hand side of Equation (24) is equivalent to EU[PX|U ⊗PY |U] =
PXY , which implies that the last integral in Equation (25) is zero for all A. This means
PV |U(B | u)PY |U(C | u) −PV Y |U(B × C | u) = 0 for almost every u-PU. Thus, Y and V are
conditional independent given U. The converse is trivial.
Note that the left-hand side of Equation (24) is not ΣY V |U but ΣY X|U, which is deﬁned on
the direct product H11 ⊗H12.