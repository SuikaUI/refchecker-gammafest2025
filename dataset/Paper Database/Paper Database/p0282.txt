THIS IS THE AUTHOR’S VERSION OF AN ARTICLE THAT HAS BEEN PUBLISHED IN THIS JOURNAL. CHANGES WERE MADE TO THIS VERSION PRIOR TO PUBLICATION. DOI: 10.1109/MIS.2020.2988525
A Secure Federated Transfer Learning
Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, Qiang Yang, Fellow, IEEE
Abstract—Machine learning relies on the availability of vast amounts of data for training. However, in reality, data are mostly scattered
across different organizations and cannot be easily integrated due to many legal and practical constraints. To address this important
challenge in the ﬁeld of machine learning, we introduce a new technique and framework, known as federated transfer learning (FTL), to
improve statistical modeling under a data federation. FTL allows knowledge to be shared without compromising user privacy and
enables complementary knowledge to be transferred across domains in a data federation, thereby enabling a target-domain party to
build ﬂexible and effective models by leveraging rich labels from a source domain. This framework requires minimal modiﬁcations to the
existing model structure and provides the same level of accuracy as the non-privacy-preserving transfer learning. It is ﬂexible and can
be effectively adapted to various secure multi-party machine learning tasks.
Index Terms—Federated Learning, Transfer Learning, Multi-party Computation, Secret Sharing, Homomorphic Encryption.
INTRODUCTION
ECENT Artiﬁcial Intelligence (AI) achievements have been
depending on the availability of massive amounts of labeled
data. For example, AlphaGo has been trained using a dataset
containing 30 million moves from 160,000 actual games. The
ImageNet dataset has over 14 million images. However, across
various industries, most applications only have access to small or
poor quality datasets. Labeling data is very expensive, especially
in ﬁelds which require human expertise and domain knowledge.
In addition, data needed for a speciﬁc task may not all be stored in
one place. Many organizations may only have unlabeled data, and
some other organizations may have very limited amounts of labels.
It has been increasingly difﬁcult from a legislative perspective for
organizations to combine their data, too. For example, General
Data Protection Regulation (GDPR) , a new bill introduced by
the European Union, contains many terms that protect user privacy
and prohibit organizations from exchanging data without explicit
user approval. How to enable the large number of businesses and
applications that have only small data (few samples and features)
or weak supervision (few labels) to build effective and accurate AI
models while complying with data privacy and security laws is a
difﬁcult challenge.
To address this challenge, Google introduced a federated
learning (FL) system in which a global machine learning model
is updated by a federation of distributed participants while keeping
their data stored locally. Their framework requires all contributors
share the same feature space. On the other hand, secure machine
learning with data partitioned in the feature space has also been
studied . These approaches are only applicable in the context
of data with either common features or common samples under
a federation. In reality, however, the set of such common entities
Yang Liu, Yan Kang and Tianjian Chen are with WeBank, Shenzhen, China.
Chanping Xing is with the Shanghai Jiao Tong University, Shanghai
Qiang Yang is with the Hong Kong University of Science and Technology,
Hong Kong, China.
may be small, making a federation less attractive and leaving the
majority of the non-overlapping data under-utilized.
In this paper, we propose Federated Transfer Learning (FTL)
to address the limitations of existing federated learning approaches. It leverages transfer learning to provide solutions
for the entire sample and feature space under a federation. Our
contributions are as follows:
We formalize the research problem of federated transfer
learning in a privacy-preserving setting to provide solutions for federation problems beyond the scope of existing
federated learning approaches;
We provide an end-to-end solution to the proposed FTL
problem and show that the performance of the proposed
approach in terms of convergence and accuracy is comparable to non-privacy-preserving transfer learning; and
We provide some novel approaches to incorporate additively homomorphic encryption (HE) and secret sharing
using beaver triples into two-party computation (2PC)
with neural networks under the FTL framework such
that only minimal modiﬁcations to the neural network
is required and the accuracy is almost lossless.
RELATED WORK
Recent years have witnessed a surge of studies on encrypted
machine learning. For example, Google introduced a secure aggregation scheme to protect the privacy of aggregated user updates
under their federarted learning framework . CryptoNets 
adapted neural network computations to work with data encrypted
via Homomorphic Encryption (HE). SecureML is a multi-party
computing scheme which uses secret-sharing and Yao’s Garbled
Circuit for encryption and supports collaborative training for linear
regression, logistic regression and neural networks.
Transfer learning aims to build an effective model for an application with a small dataset or limited labels in a target domain by
leveraging knowledge from a different but related source domain.
In recent years, there have been tremendous progress in applying
transfer learning to various ﬁelds such as image classiﬁcation and
Copyright (c) 2020 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
 
THIS IS THE AUTHOR’S VERSION OF AN ARTICLE THAT HAS BEEN PUBLISHED IN THIS JOURNAL. CHANGES WERE MADE TO THIS VERSION PRIOR TO PUBLICATION. DOI: 10.1109/MIS.2020.2988525
sentiment analysis. The performance of transfer learning relies
on how related the domains are. Intuitively, parties in the same
data federation are usually organizations from the same industry.
Therefore, they can beneﬁt more from knowledge propagation. To
the best of our knowledge, FTL is the ﬁrst framework to enable
federated learning to beneﬁt from transfer learning.
PRELIMINARIES AND SECURITY DEFINITION
Consider a source domain dataset DA := {(xA
∈Ra and yA
∈{+1, −1} is the i th label, and a target
domain DB := {xB
j=1 where xB
∈Ra. DA and DB are
separately held by two private parties and cannot be exposed
to each other legally. We assume that there exists a limited
set of co-occurrence samples DAB := {(xA
small set of labels for data from domain B in party A’s dataset:
Dc := {(xB
i=1, where Nc is the number of available target
labels. Without loss of generality, we assume all labels are in party
A, but all the deduction here can be adapted to the case where
labels exist in party B. One can ﬁnd the set of commonly shared
sample IDs in a privacy-preserving setting by masking data IDs
with encryption techniques (e.g., the RSA scheme). We utilize
the RSA Intersection module of the FATE1 framework to align cooccurrence samples of the two parties. Given the above setting, the
objective is for the two parities to build a transfer learning model
to predict labels for the target-domain party accurately without
exposing data to each other.
In this paper, we adopt a security deﬁnition in which all parties
are honest-but-curious. We assume a threat model with a semihonest adversary D who can corrupt at most one of the two data
clients. For a protocol P performing (OA, OB) = P(IA, IB),
where OA and OB are party A’s and party B’s outputs and IA
and IB are their inputs, respectively, P is secure against A if there
exists an inﬁnite number of (I′
B) pairs such that (OA, O′
B). It allows ﬂexible control of information disclosure
compared to complete zero knowledge security.
THE PROPOSED APPROACH
In this section, we introduce our proposed transfer learning model.
Deep neural networks have been widely adopted in transfer
learning to ﬁnd implicit transfer mechanisms. Here, we explore
a general scenario in which hidden representations of A and B
are produced by two neural networks uA
= NetA(x A
= NetB(x B
i ), where uA ∈RNA×d and uB ∈RNB×d,
and d is the dimension of the hidden representation layer. The
neural networks NetA and NetB serve as feature transformation
functions that project the source features of party A and party
B into a common feature subspace in which knowledge can be
transferred between the two parties. Any other symmetric feature
transformation techniques can be applied here to form the common
feature subspace. However, neural networks can help us build an
end-to-end solution to the proposed FTL problem.
To label the target domain, a general approach is to introduce a prediction function ϕ(uB
j ) = ϕ(uA
Without losing much generality, we assume ϕ(uB
j ) is linearly
separable. That is, ϕ(uB
j ) = ΦAG(uB
j ). In our experiment, we
use a translator function, ϕ(uB
j )′, where
1. 
i and G(uB
j )′. We can then write
the training objective using the available labeled set as:
ΘA,ΘB L1 =
where ΘA, ΘB are training parameters of NetA and NetB,
respectively. Let LA and LB be the number of layers for NetA
and NetB, respectively. Then, ΘA = {θA
l=1, ΘB = {θB
l are the training parameters for the lth layer. ℓ1
denotes the loss function. For logistic loss, ℓ1(y, ϕ) = log(1 +
exp(−yϕ)).
In addition, we also aim to minimize the alignment loss
between A and B in order to achieve feature transfer learning
in a federated learning setting:
ΘA,ΘB L2 =
where ℓ2 denotes the alignment loss. Typical alignment losses
can be −uA
i )′ or ||uA
F . For simplicity, we assume
that it can be expressed in the form of ℓ2(uA
i )′, where κ is a constant.
The ﬁnal objective function is:
ΘA,ΘB L = L1 + γL2 + λ
and λ are the weight parameters, and LA
F are the regularization
terms. Now we focus on obtaining the gradients for updating ΘA,
ΘB in back propagation. For i ∈{A, B}, we have:
Under the assumption that A and B are not allowed to expose
their raw data, a privacy-preserving approach needs to be developed to compute equations (3) and (4). Here, we adopt a second
order Taylor approximation for logistic loss:
ℓ1(y, ϕ) ≈ℓ1(y, 0) + 1
2C(y)ϕ + 1
and the gradient is:
where, C(y) = −y, D(y) = y2.
In the following two sections, we will discuss two alternative
constructions of the secure FTL protocol: the ﬁrst one is leveraging
additively homomorphic encryption, and the second one is utilizing the secret sharing based on beaver triples. We carefully design
the FTL protocol such that only minimal information needs to
be encrypted or secretly shared between parties. Besides, the FTL
protocol is designed to be compatible with other homomorphic encryption and secret sharing schemes with minimal modiﬁcations.
FTL USING HOMOMORPHIC ENCRYPTION
Additively Homomorphic Encryption and polynomial approximations have been widely used for privacy-preserving machine learning. Applying equations (5) and (6), and additively homomorphic
THIS IS THE AUTHOR’S VERSION OF AN ARTICLE THAT HAS BEEN PUBLISHED IN THIS JOURNAL. CHANGES WERE MADE TO THIS VERSION PRIOR TO PUBLICATION. DOI: 10.1109/MIS.2020.2988525
encryption (denoted as [[·]]), we obtain the privacy preserved loss
function and the corresponding gradients for the two domains as:
i , 0)]] + 1
i )ΦA[[G(uB
i )ΦA[[(G(uB
i )]](ΦA)′)
i )]] + [[ℓA
i )]] + κuA
3 ]] + [[λ
i )(ΦA)′ΦA]]∂uB
i )ΦA]]∂G(uB
]]) + [[λθB
i )ΦA[[G(uB
]]) + [[λθA
FTL Algorithm - Homomorphic Encryption based
With equations (7), (8) and (9), we now design a federated
algorithm for solving the transfer learning problem. See Figure
1. Let [[·]]A and [[·]]B be homomorphic encryption operators with
public keys from A and B, respectively. A and B initialize and
execute their respective neural networks NetA and NetB locally
to obtain hidden representations uA
i . A then computes
and encrypts components {hk(uA
i )}k=1,2...KA and sends to
B to assist calculations of gradients of NetB. In the current scenario, KA = 3, hA
i ) = {[[ 1
i )(ΦA)′(ΦA)]]A}Nc
i ) = {[[ 1
i )ΦA]]A}Nc
i=1, and hA
i=1 . Similarly, B then computes and encrypts components {hB
i )}k=1,2...KB and sends to A to assist calculations of gradients of NetA and loss L. In the current
scenario, KB = 4, hB
i ) = {[[(G(uB
i ) = {[[G(uB
i ) = {[[κuB
i ) = [[ λ
To prevent A’s and B’s gradients from being exposed, A and B
further mask each gradient with an encrypted random value. They
then send the masked gradients and loss to each other and decrypt
the values locally. A can send termination signals to B once
the loss convergence condition is met. Otherwise, they unmask
the gradients, update the weight parameters with their respective
gradients, and move on to next iteration. Once the model is trained,
FTL can provide predictions for unlabeled data from party B.
Algorithm 1 summaries the prediction process.
Fig. 1. HE-based FTL algorithm workﬂow
Algorithm 1 HE-based FTL: Prediction
Require: Model parameters ΘA, ΘB, {xB
j ←−NetB(ΘB, x B
3: Encrypt {[[G(uB
j )]]B}j∈{1,2,...,NB} and send it to A;
5: Create a random mask mA;
6: Compute [[ϕ(uB
j )]]B = ΦA[[G(uB
j )]]B and send [[ϕ(uB
mA]]B to B;
8: Decrypt ϕ(uB
j ) + mA and send results to A;
10: Compute ϕ(uB
j ) and yB
j and send yB
Security Analysis
Theorem 1. The protocol in the FTL training Algorithm (Figure 1) and Algorithm 1 is secure under our security deﬁnition,
provided that the underlying additively homomorphic encryption
scheme is secure.
Proof. The training protocol in Figure 1 and Algorithm 1 do
not reveal any information, because all A and B learned are the
masked gradients. In each iteration, A and B create new random
masks. The strong randomness and secrecy of the masks secure the
information against the other party . During training, A learns
its own gradients at each step, but this is not enough for A to learn
any information from B based on the impossibility of solving n
equations with more than n unknowns . In other words, there
exist an inﬁnite number of inputs from B that result in the same
gradients A receives. Similarly, B cannot learn any information
from A. Therefore, as long as the encryption scheme is secure,
the proposed protocol is secure. During evaluation, A learns the
predicted result for each sample from B, which is a scalar product,
from which A cannot infer B’s private information. B learns only
the label, from which B cannot infer A’s private information.
THIS IS THE AUTHOR’S VERSION OF AN ARTICLE THAT HAS BEEN PUBLISHED IN THIS JOURNAL. CHANGES WERE MADE TO THIS VERSION PRIOR TO PUBLICATION. DOI: 10.1109/MIS.2020.2988525
At the end of the training process, each party (A or B) only
obtains the model parameters associated with its own features,
and remains oblivious to the data structure of the other party. At
inference time, the two parties need to collaboratively compute
the prediction results. Note that the protocol does not deal with
the situation in which one (or both) of the two parties is (are)
malicious. If A fakes its inputs and submits only one non-zero
input, it might be able to tell the value of uB
i at the position of
that input. It still cannot tell xB
or ΘB, and neither party can
obtain the correct prediction results.
FTL USING SECRET SHARING
Throughout this section, assume that any private value v is shared
between the two parties where A keeps ⟨v⟩A and B keeps
⟨v⟩B such that v = ⟨v⟩A + ⟨v⟩B. To make it possible for the
performance to be comparable with the previous construction,
assume that ℓ1(y, ϕ) and ∂ℓ1
∂ϕ can be approximated by the second
order Taylor expansion following Equations (5) and (6). So L, ∂L
ℓcan be expressed as the following
i , 0) + 1
i )ΦA ∂G(uB
In this case, the whole process can be performed securely if secure matrix addition and multiplication can be constructed. Since
operations with public matrices or adding two private matrices
can simply be done using the shares without any communication,
the remaining operation that requires discussion is secure matrix
multiplication. Beaver’s triples are used to help in the matrix
multiplication.
Secure Matrix Multiplication using Beaver Triples
First, we brieﬂy recall how to perform the matrix multiplication
given that the two parties have already shared a Beaver’s triple.
Suppose that the computation required is to obtain P = MN
where the dimensions of M, N and P are m × n, n × k and
m×k respectively. As assumed, the matrices M and N have been
secretly shared by the two parties where A keeps ⟨M⟩A and ⟨N⟩A
and B keeps ⟨M⟩B and ⟨N⟩B. To assist with the calculation, in
the preprocessing phase, A and B have generated three matrices
D, E, F of dimension m×n, n×k and m×k respectively where
A keeps ⟨D⟩A, ⟨E⟩A and ⟨F⟩A while B keeps ⟨D⟩B, ⟨E⟩B and
⟨F⟩B such that DE = F.
Algorithm 2 Secure Matrix Multiplication
Require: M, N two matrices to be multiplied with dimensions
m × n and n × k respectively and secretly shared between A
and B. Triple (D, E, F = DE) of matrices with dimension
m×n, n×k and m×k respectively secretly shared between
2: ⟨δ⟩A ←−⟨M⟩A −⟨D⟩A, ⟨γ⟩A ←−⟨N⟩A −⟨E⟩A and send
them to B;
4: ⟨δ⟩B ←−⟨M⟩B −⟨D⟩B, ⟨γ⟩B ←−⟨N⟩B −⟨E⟩B and send
them to A;
5: A and B recovers δ = ⟨δ⟩A + ⟨δ⟩B and γ = ⟨γ⟩A + ⟨γ⟩B.
7: ⟨P⟩A ←−⟨M⟩Aγ + δ⟨N⟩A + ⟨F⟩A;
9: ⟨P⟩B ←−⟨M⟩Bγ + δ⟨N⟩B + ⟨F⟩B −δγ;
It is easy to see that ⟨P⟩A + ⟨P⟩B = MN, which is what is
required for this protocol. As can be seen, this method guarantees
efﬁcient online computation in the cost of having ofﬂine phase
where players generated the Beavers triples. So next we discuss
the scheme to generate the triples.
Beaver Triples Generation
In the preprocessing phase, the Beaver’s triples generation protocol uses a sub-protocol to perform the secure matrix multiplication
with the help of a third party, which we will call Carlos. Recall
that having two matrices U and V owned respectively by A and
B, they want to calculate UV securely with the help of Carlos.
In order to do this, Alice and Bob individually generate shares for
U and V respectively. That is, we have U = ⟨U⟩0 + ⟨U⟩1 and
V = ⟨V ⟩0+⟨V ⟩1. Then we have UV = (⟨U⟩0+⟨U⟩1)·(⟨V ⟩0+
⟨V ⟩1) = ⟨U⟩0⟨V ⟩0 + ⟨U⟩0⟨V ⟩1 + ⟨U⟩1⟨V ⟩0 + ⟨U⟩1⟨V ⟩1. So
if Alice sends ⟨U⟩1 to Bob and Bob sends ⟨V ⟩0 to Alice:
⟨U⟩0⟨V ⟩0 can be privately calculated by Alice
⟨U⟩1⟨V ⟩1 can be privately calculated by Bob
⟨U⟩1⟨V ⟩0 can be privately calculated by both Alice and
However, no one can calculate ⟨U⟩0⟨V ⟩1 yet. This is
what Carlos will calculate.
By the use of Algorithm 3, Algorithm 4 generates triple
(D, E, F) such that:
Alice holds ⟨D⟩A, ⟨E⟩A and ⟨F⟩A without learning
anything about (D, E, F), ⟨D⟩B, ⟨E⟩B and ⟨F⟩B.
THIS IS THE AUTHOR’S VERSION OF AN ARTICLE THAT HAS BEEN PUBLISHED IN THIS JOURNAL. CHANGES WERE MADE TO THIS VERSION PRIOR TO PUBLICATION. DOI: 10.1109/MIS.2020.2988525
Algorithm 3 Ofﬂine Secure Matrix Multiplication
Require: U and V, two matrices to be multiplied with dimensions
m × n and n × k respectively; U is owned by A and V is
owned by B.
1: Invite a third party C;
3: Randomly choose ⟨U⟩0 and set ⟨U⟩1 = U −⟨U⟩0;
4: Send ⟨U⟩1 to B and ⟨U⟩0 to C;
6: Randomly choose ⟨V ⟩0 and set ⟨V ⟩1 = V −⟨V ⟩0
7: Send ⟨V ⟩0 to A and ⟨V ⟩1 to C;
9: Compute ˜W = ⟨U⟩0⟨V ⟩1;
10: Randomly choose ⟨˜W⟩A and set ⟨˜W⟩B = ˜W −⟨˜W⟩A;
11: Send ⟨˜W⟩A to A and ⟨˜W⟩B to B;
13: Set ⟨W⟩A = ⟨U⟩0⟨V ⟩0 + ⟨U⟩1⟨V ⟩0 + ⟨˜W⟩A;
15: Set ⟨W⟩B = ⟨U⟩1⟨V ⟩1 + ⟨˜W⟩B;
Bob holds ⟨D⟩B, ⟨E⟩B and ⟨F⟩B without learning anything about (D, E, F), ⟨D⟩A, ⟨E⟩A and ⟨F⟩A.
Algorithm 4 Beaver Triples Generation
Require: The dimensions of the required matrices, m × n, n × k
and m × k;
2: Randomly choose ⟨D⟩A and ⟨E⟩A ;
4: Randomly choose ⟨D⟩B and ⟨E⟩B ;
5: A and B do:
6: Perform Algorithm 3 with U = ⟨D⟩A and V = ⟨E⟩B to
get W = ⟨D⟩A⟨E⟩B such that A holds ⟨W⟩A and B holds
7: Perform Algorithm 3 with the role of A and B reversed, U =
⟨D⟩B and V = ⟨E⟩A to get Z = ⟨D⟩1⟨E⟩0 such that A
holds ⟨Z⟩A and B holds ⟨Z⟩B;
9: Set ⟨F⟩A = ⟨D⟩A⟨E⟩A + ⟨W⟩A + ⟨Z⟩A;
11: Set ⟨F⟩B = ⟨D⟩B⟨E⟩B + ⟨W⟩B + ⟨Z⟩B;
Lastly, during ofﬂine phase, Alice and Bob also requested
Carlos to generate sufﬁcient number of shares for zero matrices
with various dimensions.
FTL Algorithm - Secret Sharing based
Before discussing our FTL protocol that is constructed based on
Beaver triples, we ﬁrst give some notation to simplify Equations
(10), (11) and (12) based on the parties needed to complete the
calculation.
Let LA = PNC
i , 0) + γ PNAB
Let LB = γ PNAB
i=1,··· ,NC ,
i=1,··· ,NC
i=1,··· ,NC and
i )i=1,··· ,NAB.
i=1,··· ,NC , and
i )i=1,··· ,NC.
L = LA + LB +
AB (i)L(B,1)
AB (i)L(B,1)
AB (i)L(B,1)
Let D(B,ℓ)
i )ΦA ∂G(uB
i )ΦA)G(uB
i )(ΦA)(∂G(uB
i=1,··· ,NC ,
i=1,··· ,NAB .
AB (i)D(B,ℓ)
AB (i)L(B,1)
AB (i)D(B,ℓ)
AB (i)D(B,ℓ)
THIS IS THE AUTHOR’S VERSION OF AN ARTICLE THAT HAS BEEN PUBLISHED IN THIS JOURNAL. CHANGES WERE MADE TO THIS VERSION PRIOR TO PUBLICATION. DOI: 10.1109/MIS.2020.2988525
Let D(A,ℓ)
i=1,··· ,NC ,
i=1,··· ,NC
i=1,··· ,NAB .
AB,A,1(i)L(B,1)
A,B L(B,1)
AB,A,2(i)L(B,1)
AB,A,3(i)(uB
To perform the training scheme, both Alice and Bob
ﬁrst initialize and execute their respective neural networks
computes {hA
i )}k=1,··· ,KA.
i )⟩B = hA
i )⟩A. Then Alice
i )⟩B to Bob for k = 1, · · · , KA. Similarly,
Bob computes {hB
i )}k=1,··· ,KB and for each k, he randomly chooses ⟨hB
i )⟩B and sets ⟨hB
i )⟩A = hB
i )⟩B, which is then sent to Alice.
In our scenario, KA = 7, with:
i ) = L(A,1)
i ) = L(A,2)
i ) = L(A,3)
i ) = L(A,4)
i ) = D(A,ℓ)
i ) = D(A,ℓ)
i ) = D(A,ℓ)
and KB = 4 with
i ) = L(B,1)
i ) = L(B,2)
i ) = D(B,ℓ)
i ) = D(B,ℓ)
In addition, Alice privately computes LA and D(A,ℓ)
Bob privately computes LB and D(B,ℓ)
. Algorithm 6 provides
the training protocol for one iteration based on Beaver triples
generated by Algorithm 4.
Algorithm 5 FTL Training: Beaver triples based
Require: Alice holds hA
k , LA and D(A,ℓ)
while Bob holds
k , LB and D(B,ℓ)
. In the ofﬂine phase, they have also
generated sufﬁcient triples with the appropriate dimensions.
We also require a threshold ϵ for termination condition;
1: Calculate LAB with 3NC + NAB inner products of length d
and two real number multiplications. Alice receives ⟨LAB⟩A
and Bob receives ⟨LAB⟩B. Alice sets ⟨L⟩A = LA+⟨LAB⟩A
and Bob sets ⟨L⟩B = LB + ⟨LAB⟩B;
2: Both Alice and Bob publish their shares so they can individually recover L;
3: For each θB
ℓ∈ΘB, calculate DB,ℓ
AB with 3NC + NAB inner
product of vectors of length d and two real number multiplications. Alice receives ⟨D(B,ℓ)
AB ⟩A and sets
AB ⟩A. In the same time, Bob receives ⟨D(B,ℓ)
B = D(B,ℓ)
4: Alice sends
5: Bob recovers
6: Bob updates θB
7: For each θA
ℓ∈ΘA, calculate DA,ℓ
AB with 3NC + NAB inner
product of vectors of length d and two real number multiplications. Alice receives ⟨D(A,ℓ)
AB ⟩A and sets
AB ⟩A. In the same time, Bob receives
AB ⟩B and sets
B = ⟨D(A,ℓ)
8: Bob sends
B to Alice;
9: Alice recovers
10: Alice updates θA
11: Bob updates θB
12: Repeat as long as Lprev −L ≥ϵ;
After the training is completed, we proceed to the prediction
phase. Recall that after the training phase, Alice has the optimal
value for ΘA while Bob has the optimal value for ΘB. Suppose
that now B wants to learn the label for {xB
j }j∈NB. The protocol
can be found in Algorithm 6.
Algorithm 6 FTL Prediction: Beaver triples based
Require: Alice holds the optimal parameter ΘA and Bob holds
the optimal parameter ΘB and unlabeled data {xB
2: Calculate uB
j = NetB(ΘB, xB
3: Calculate G(uB
4: Randomly choose ⟨G(uB
5: Set ⟨G(uB
j )⟩A = G(uB
j ) −⟨G(uB
j )⟩B and send it to A;
7: Calculate ΦA
8: Randomly choose ⟨ΦA⟩A;
9: Set ⟨ΦA⟩B = ΦA −⟨ΦA⟩A and send it to B;
10: Perform secure matrix multiplication from Algorithm 2 so A
receives ⟨ΦAG(uB
j )⟩A and B receives ⟨ΦAG(uB
11: B sends ⟨ΦAG(uB
j )⟩B to A;
12: A recovers ϕ(uB
j ) = ΦAG(uB
j ), calculates yB
j and sends it
THIS IS THE AUTHOR’S VERSION OF AN ARTICLE THAT HAS BEEN PUBLISHED IN THIS JOURNAL. CHANGES WERE MADE TO THIS VERSION PRIOR TO PUBLICATION. DOI: 10.1109/MIS.2020.2988525
Theorem 2. The protocol in Algorithms 3,4,5 and 6 are information theoretically secure against at most one passive adversary.
Proof. Note that in all of these algorithms, the only information
that any party receives regarding any private values is only
the share for an n-out-of-n secret sharing scheme. So by the
property of n-out-of-n secret sharing scheme, no one can learn
any information about the private values they are not supposed to
learn. After the calculation, the same thing can be said since each
party only learns about a share of a secret sharing scheme and
they cannot learn any information regarding values they are not
supposed to learn from there.
Remark 1. Using the argument in we can improve the
efﬁciency in the following manner; for each matrix A, it is always
masked by the same random matrix. This optimization does not
affect the security of the protocol while signiﬁcantly improves the
efﬁciency.
EXPERIMENTAL EVALUATION
In this section, we report experiments conducted on public datasets
including: 1) NUS-WIDE dataset 2) Kaggle’s Default-of-
Credit-Card-Clients (“Default-Credit”) to validate our proposed approach. We study the effectiveness and scalability of
the approach with respect to various key factors, including the
number of overlapping samples, the dimension of hidden common
representations, and the number of features.
The NUS-WIDE dataset consists of 634 low-level features
from Flickr images as well as their associated tags and ground
truth labels. There are in total 81 ground truth labels. We use
the top 1,000 tags as text features and combine all the lowlevel features including color histograms and color correlograms
as image features. We consider solving a one-vs-all classiﬁcation
problem with a data federation formed between party A and party
B, where A has 1000 text tag features and labels, while party B
has 634 low-level image features.
The “Default-Credit” dataset consists of credit card records
including user demographics, history of payments, and bill statements, etc., with users’ default payments as labels. After applying
one-hot encoding to categorical features, we obtain a dataset with
33 features and 30,000 samples. We then split the dataset both in
the feature space and the sample space to simulate a two-party
federation problem. Speciﬁcally, we assign each sample to party
A, party B or both so that there exists a small number of samples
overlapping between A and B. All labels are on the side of party
A. We will examine the scalability (in section 7.3) of the FTL
algorithm by dynamically splitting the feature space.
Impact of Taylor Approximation
We studied the effect of Taylor approximation by monitoring
and comparing the training loss decay and the performance of
prediction. Here, we test the convergence and precision of the
algorithm using the NUS-WIDE data and neural networks with
different levels of depth. In the ﬁrst case, NetA and NetB both
have one auto-encoder layer with 64 neurons, respectively. In
the second case, NetA and NetB both have two auto-encoder
layers with 128 and 64 neurons, respectively. In both cases,
we used 500 training samples, 1,396 overlapping pairs, and set
γ = 0.05, λ = 0.005. We summarize the results in Figures 2(a)
and 2(b). We found that the loss decays at a similar rate when using
Taylor approximation as compared to using the full logistic loss,
and the weighted F1 score of the Taylor approximation approach
is also comparable to the full logistic approach. The loss converges
to a different minima in each of these cases. As we increased the
depth of the neural networks, the convergence and the performance
of the model did not decay.
Most existing secure deep learning frameworks suffer from
accuracy loss when adopting privacy-preserving techniques .
Using only low-degree Taylor approximation, the drop in accuracy
in our approach is much less than the state-of-art secure neural
networks with similarly approximations.
Performance
We tested SS-based FTL (SST), HE-based FTL with Taylor loss
(TLT) and FTL with logistic loss (TLL). For the self-learning
approach, we picked three machine learning models: 1) logistic
regression (LR), 2) SVM, and 3) stacked auto-encoders (SAEs).
The SAEs are of the same structure as the ones we used for transfer
learning, and are connected to a logistic layer for classiﬁcation.
We picked three of the most frequently occurring labels in the
NUS-WIDE dataset, i.e., water, person and sky, to conduct one vs.
others binary classiﬁcation tasks. For each experiment, the number
of overlapping samples we used is half of the total number of
samples in that category. We varied the size of the training sample
set and conducted three tests for each experiment with different
random partitions of the samples. The parameters λ and γ are
optimized via cross-validation.
Figure 2(c) shows the effect of varying the number of overlapping samples on the performance of transfer learning. The overlapping sample pairs are used to bridge the hidden representations
between the two parties. The performance of FTL improves as the
overlap between datasets increases.
The comparison of F-score (mean ± std) among SST, TLT,
TLL and the several other machine learning models is shown
in Table 1. We observe that SST, TLT and TLL yield comparable performance across all tests. This demonstrates that SST
can achieve plain-text level accuracy while TLT can achieve
almost lossless accuracy although Taylor approximation is applied.
The three FTL models outperform baseline self-learning models
signiﬁcantly using only a small set of training samples under
all experimental conditions. In addition, performance improves
as we increased the number of training samples. The results
demonstrated the robustness of FTL.
Scalability
We study the scalability using Default-Credit dataset because it
allows us to conveniently choose features when we do experiments. Speciﬁcally, we study how the training time scales with
the number of overlapping samples, the number of target-domain
features, and the dimension of hidden representations, denote as
d. Based on the algorithmic detail of proposed transfer learning
approach, the communication cost for B sending a message to A
can be calculated by formula CostB−→A = n∗(d2+d)∗ct, where
ct is the size of the message and n is the number of samples sent.
The same cost applies when sending message from A to B.
To speed up the secure FTL algorithm, we preform computeintensive operations in parallel. The logic ﬂow of parallel secure
FTL algorithm includes three stages: parallel encryption, parallel
gradient calculation, and parallel decryption. Detailed logic ﬂow
is shown in Figure 3.
THIS IS THE AUTHOR’S VERSION OF AN ARTICLE THAT HAS BEEN PUBLISHED IN THIS JOURNAL. CHANGES WERE MADE TO THIS VERSION PRIOR TO PUBLICATION. DOI: 10.1109/MIS.2020.2988525
iterations
weighted F1 score
1-layer neurons (64)
loss_Taylor
(a) Learning loss (1-layer)
iterations
weighted F1 score
2-layer neurons (128,64)
loss_Taylor
(b) Learning loss (2-layer)
# overlapping pairs
weighted F1 score
(c) F1 vs. # overlapping pairs
time (second)
#samples = 5
#samples = 10
#samples = 20
(d) time vs. d (HE)
# features
time (second)
#samples = 5
#samples = 10
#samples = 20
(e) time vs. # features (HE)
# overlapping samples
time (second)
(f) time vs. # samples (HE)
time (second)
#samples = 5
#samples = 10
#samples = 20
(g) time vs. d (SS)
# features
time (second)
#samples = 5
#samples = 10
#samples = 20
(h) time vs. # features (SS)
# overlapping samples
time (second)
(i) time vs. # samples (SS)
Fig. 2. Experiment results.
Comparison of weighted F1 scores.
water vs. others 100 0.698 ± 0.011 0.692 ± 0.062 0.691 ± 0.060 0.685 ± 0.020 0.640 ± 0.016 0.677 ± 0.048
water vs. others 200 0.707 ± 0.013 0.702 ± 0.010 0.701 ± 0.007 0.672 ± 0.023 0.643 ± 0.038 0.662 ± 0.010
person vs. others 100 0.703 ± 0.015 0.697 ± 0.010 0.697 ± 0.020 0.694 ± 0.026 0.619 ± 0.050 0.657 ± 0.030
person vs. others 200 0.735 ± 0.004 0.733 ± 0.009 0.735 ± 0.010 0.720 ± 0.004 0.706 ± 0.023 0.707 ± 0.008
sky vs. others
100 0.708 ± 0.015 0.700 ± 0.022 0.713 ± 0.006 0.694 ± 0.016 0.679 ± 0.018 0.667 ± 0.009
sky vs. others
200 0.724 ± 0.014 0.718 ± 0.033 0.718 ± 0.024 0.696 ± 0.026 0.680 ± 0.042 0.684 ± 0.056
On parallel encryption stage, we parallelly encrypt components that will be sent to the other party. On parallel gradient
calculation stage, we parallelly perform operations, including
matrix multiplication and addition, on encrypted components to
calculate encrypted gradients. On parallel decryption stage, we
parallelly decrypt masked loss and gradients. Finally, the two
parties exchange decrypted masked gradients that will be used
to update neural networks. With 20 partitions, the parallel scheme
can boost the secure FTL 100x than sequential scheme.
Figures 2(d), 2(e) and 2(f) illustrate that with parallelism
applied, the running time of HE-based FTL grows approximately
linearly with respect to the size of the hidden representation
dimension, the number of target-domain features, as well as the
number of overlapping samples respectively.
Figures 2(g), 2(h) and 2(i) illustrate how the training time
varies with the three key factors in the SS setting. The communication cost can be simpliﬁed as O(d2) if keeping other factors
constant. As illustrated in Figures 2(g), however, the increasing
rate of the training time is approaching linear rather than O(d2).
We conjecture that this is due to the computational efﬁciency
of SS-based FTL. Besides, as illustrated in Figure 2(h) and 2(i),
respectively, as the feature sizes or overlapping samples increase,
the increasing rate of training time drops.
Further, we compare the scalability of SS-based with that
of HE-based FTL along the axis of the hidden representation
dimension, the number of features, and the number of overlapping
samples, respectively. The results are presented in Table 2. We
notice that SS-based FTL is running much faster than HE-based
FTL. Overall, SS-based FTL speeds up by 1-2 orders of magnitude
compared with HE-based FTL. In addition, as shown in the three
tables, the increasing rate of the training time of SS-based FTL is
much slower than that of HE-based FTL.
CONCLUSIONS AND FUTURE WORK
In this paper we proposed a secure Federated Transfer Learning
(FTL) framework to expand the scope of existing secure fed-
THIS IS THE AUTHOR’S VERSION OF AN ARTICLE THAT HAS BEEN PUBLISHED IN THIS JOURNAL. CHANGES WERE MADE TO THIS VERSION PRIOR TO PUBLICATION. DOI: 10.1109/MIS.2020.2988525
Fig. 3. Logic ﬂow of parallel secure FTL.
Comparison of training time between SS and HE with the increasing
dimension of hidden representation denoted by d, the increasing
number of target-domain features, and the increasing number of
overlapping samples, respectively
HE training time (sec)
SS training time (sec)
# features
HE training time (sec)
SS training time (sec)
HE training time (sec)
SS training time (sec)
erated learning to broader real-world applications. Two secure
approaches, namely, homomorphic encryption (HE) and secret
sharing are proposed in this paper for preserving privacy. The HE
approach is simple, but computationally expensive. The biggest
advantages of the secret sharing approach include (i) there is no
accuracy loss, (ii) computation is much faster than HE approach.
The major drawback of the secret sharing approach is that one
has to ofﬂine generate and store many triplets before online
computation.
We demonstrated that, in contrast to existing secure deep
learning approaches which suffer from accuracy loss, FTL is as
accurate as non-privacy-preserving approaches, and is superior to
non-federated self-learning approaches. The proposed framework
is a general privacy-preserving federated transfer learning solution
that is not restricted to speciﬁc models.
In future research, we will continue improving the efﬁciency
of the FTL framework by using distributed computing techniques
with less expensive computation and communication schemes.