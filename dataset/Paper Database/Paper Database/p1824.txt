How Close is ChatGPT to Human Experts?
Comparison Corpus, Evaluation, and Detection
Biyang Guo1†∗, Xin Zhang2∗, Ziyuan Wang1∗, Minqi Jiang1∗, Jinran Nie3∗
Yuxuan Ding4, Jianwei Yue5, Yupeng Wu6
1AI Lab, School of Information Management and Engineering
Shanghai University of Finance and Economics
2Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen)
3School of Information Science, Beijing Language and Culture University
4School of Electronic Engineering, Xidian University
5School of Computing, Queen’s University, 6Wind Information Co., Ltd
The introduction of ChatGPT2 has garnered widespread attention in both academic
and industrial communities. ChatGPT is able to respond effectively to a wide range
of human questions, providing ﬂuent and comprehensive answers that signiﬁcantly
surpass previous public chatbots in terms of security and usefulness. On one hand,
people are curious about how ChatGPT is able to achieve such strength and how
far it is from human experts. On the other hand, people are starting to worry about
the potential negative impacts that large language models (LLMs) like ChatGPT
could have on society, such as fake news, plagiarism, and social security issues.
In this work, we collected tens of thousands of comparison responses from both
human experts and ChatGPT, with questions ranging from open-domain, ﬁnancial,
medical, legal, and psychological areas. We call the collected dataset the Human
ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the
characteristics of ChatGPT’s responses, the differences and gaps from human
experts, and future directions for LLMs. We conducted comprehensive human
evaluations and linguistic analyses of ChatGPT-generated content compared with
that of humans, where many interesting results are revealed. After that, we conduct
extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore
several key factors that inﬂuence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https:
//github.com/Hello-SimpleAI/chatgpt-comparison-detection.
Introduction
Since its dazzling debut in November 2022, OpenAI’s ChatGPT has gained huge attention and wide
discussion in the natural language processing (NLP) community and many other ﬁelds. According
to OpenAI, ChatGPT is ﬁne-tuned from the GPT-3.5 series with Reinforcement Learning from
Human Feedback (RLHF; ), using nearly the same methods as InstructGPT , but with
slight differences in the data collection setup. The vast amount of knowledge in GPT-3.5 and the
meticulous ﬁne-tuning based on human feedback enable ChatGPT to excel at many challenging NLP
∗Equal Contribution.
†Project Lead. Corresponding to 
+Each author has made unique contributions to the project.
2Launched by OpenAI in November 2022. 
 
tasks, such as translating natural language to code , completing the extremely masked text or
generating stories given user-deﬁned elements and styles , let alone typical NLP tasks like text
classiﬁcation, entity extraction, translation, etc. Furthermore, the carefully collected human-written
demonstrations also make ChatGPT able to admit its mistakes, challenge incorrect premises and
reject even inappropriate requests, as claimed by OpenAI3.
The surprisingly strong capabilities of ChatGPT have raised many interests, as well as concerns:
On the one hand, people are curious about how close is ChatGPT to human experts. Different
from previous LLMs like GPT-3 , which usually fails to properly respond to human queries,
InstructGPT and the stronger ChatGPT have improved greatly in interactions with humans.
Therefore, ChatGPT has great potential to become a daily assistant for general or professional
consulting purposes . From the linguistic or NLP perspectives, we are also interested in
where are the remaining gaps between ChatGPT and humans and what are their implicit linguistic
differences .
On the other hand, people are worried about the potential risks brought by LLMs like ChatGPT.
With the free preview demo of ChatGPT going virus, a large amount of ChatGPT-generated content
crowded into all kinds of UGC (User-Generated Content) platforms, threatening the quality and
reliability of the platforms. For example, Stack Overﬂow, the famous programming questionanswering website, has temporarily banned ChatGPT-generated content4, because it believes "the
average rate of getting correct answers from ChatGPT is too low, the posting of answers created by
ChatGPT is substantially harmful to the site and to users who are asking and looking for correct
answers". Many other applications and activities are facing similar issues, such as online exams 
and medical analysis . Our empirical evaluation of ChatGPT on legal, medical, and ﬁnancial
questions also reveals that potentially harmful or fake information can be generated.
Considering the opaqueness of ChatGPT and the potential social risks associated with model misuse,
we make the following contributions to both the academy and society:
1. To facilitate LLM-related research, especially the study on the comparison between humans
and LLMs, we collect nearly 40K questions and their corresponding answers from human
experts and ChatGPT, covering a wide range of domains (open-domain, computer science,
ﬁnance, medicine, law, and psychology), named as the Human ChatGPT Comparison
Corpus (HC3) dataset. The HC3 dataset is a valuable resource to analyze the linguistic and
stylist characteristics of both humans and ChatGPT, which helps to investigate the future
improvement directions for LLMs;
2. We conduct comprehensive human evaluations as well as linguistic analysis on
human/ChatGPT-generated answers, discovering many interesting patterns exhibited by
humans and ChatGPT. These ﬁndings can help to distinguish whether certain content is
generated by LLMs, and also provide insights about where language models should be
heading in the future;
3. Based on the HC3 dataset and the analysis, we develop several ChatGPT detecting models,
targeting different detection scenarios. These detectors show decent performance in our
held-out test sets. We also conclude several key factors that are essential to the detector’s
effectiveness.
4. We open-source all the collected comparison corpus, evaluations, and detection models, to
facilitate future academic research and online platform regulations on AI-generated content.
Human ChatGPT Comparison Corpus (HC3)
ChatGPT is based on the GPT-3.5 series, which is pre-trained on the super-large corpus, consisting of
web-crawled text, books, and codes, making it able to respond to all kinds of questions. Therefore,
we are curious how will a human (especially an expert) and ChatGPT respond to the same question
respectively. Inspired by , we also want to evaluate whether ChatGPT can keep honest (not
fabricate information or mislead the user), harmless (shouldn’t generate harmful or offensive content),
3 
4 
HC3-English
# Questions
# Human Answers
# ChatGPT Answers
reddit_eli5
ELI5 dataset 
WikiQA dataset 
Crawled Wikipedia (A.1)
Medical Dialog dataset 
FiQA dataset 
HC3-Chinese
# Questions
# Human Answers
# ChatGPT Answers
WebTextQA & BaikeQA 
Crawled BaiduBaike (A.1)
nlpcc_dbqa
NLPCC-DBQA dataset 
Medical Dialog dataset 
ChineseNlpCorpus (A.1)
psychology
from Baidu AI Studio (A.1)
LegalQA dataset (A.1)
Table 1: Meta-information of the HC3 dataset. The English (resp. Chinese) contains 5 (resp. 7) splits.
and how helpful (provide concrete and correct solutions to the user’s question) it is compared to
human experts.
Taking these into account, we decided to collect a comparison corpus that consists of both human and
ChatGPT answers to the same questions. We believe such a comparison corpus can be a valuable and
interesting source to study the nature of the language of both humans and language models.
Human Answers Collection
Inviting human experts to manually write questions and answers is tedious and unaffordable for us
to collect a large amount of data, therefore we construct the comparison dataset mainly from two
• Publicly available question-answering datasets, where answers are given by experts in speciﬁc
domains or the high-voted answers by web users;
• Wiki text. We construct question-answer pairs using the concepts and explanations from wiki
sources like Wikipedia5 and BaiduBaike6.
The split-data source mapping is shown in Table 1, and please refer to Appendix A.1 for further
detailed information.
ChatGPT Answers Collection
Based on the collected human question-answering datasets, we use ChatGPT to generate answers
to these questions. Since the ChatGPT is currently only available through its preview website, we
manually input the questions into the input box, and get the answers, with the aid of some automation
testing tools. Answers by ChatGPT can be inﬂuenced by the chatting history, so we refresh the thread
for each question.
To make the answer more aligned with human answers, we add additional instructions to ChatGPT
for speciﬁc datasets. For example, the human answers from the reddit-eli5 dataset split are under
the context of "Explain like I’m ﬁve", therefore we use this context to instruct ChatGPT by adding
"Explain like I’m ﬁve" at the end of the original question. More detail can be found in the Appendix.
5 
6 
ChatGPT can generate different answers given the same question in different threads, which is
perhaps due to the random sampling in the decoding process. However, we found the differences can
be very small, thereby we only collect one answer for most questions.
Human ChatGPT Comparison Corpus (HC3)
For each question, there can be more than one human/ChatGPT answer, therefore we organize the
comparison data using the following format:
"question ": "Q1",
"human_answers ": ["A1", "A2"],
" chatgpt_answers ": ["B1"]
Overall, we collected 24, 322 questions, 58, 546 human answers and 26, 903 ChatGPT answers for
the English version, and 12, 853 questions, 22, 259 human answers and 17, 522 ChatGPT answers
for the Chinese version. The meta-information of each dataset split is illustrated in Table 1.
Human Evaluation & Summarization
In this section, we invite many volunteer testers and conduct extensive human evaluations from
different aspects. After the human evaluation, we make our collected comparison corpus available
to the volunteers and ask them to manually conclude some characteristics. We then summarize the
feedback from the volunteers combined with our observations.
Human Evaluation
The human evaluation is divided into the Turing test and the Helpfulness Test. The Turing Test
 is a test of a machine’s ability to exhibit intelligent behavior that is indistinguishable from a
human. We invite 17 volunteers, divided into two groups: 8 experts (who are frequent users of
ChatGPT) and 9 amateurs (who have never heard of ChatGPT). This is because people who are
familiar with ChatGPT may have memorized some patterns exhibited by ChatGPT, helping them to
easily distinguish the role.
We designed four types of evaluations, using different query formats or testing groups. We introduce
the speciﬁc evaluation design and results in the following parts:
A. Expert Turing Test, Paired Text (pair-expert)
The pair-expert test is conducted in the expert group. Each tester is required to do a series of
tests, each test containing one question and a pair of answers (one from humans and another from
ChatGPT). The tester needs to determine which answer is generated by ChatGPT.
B. Expert Turing Test, Single Text (single-expert)
The single-expert test is also conducted in the expert group. Each tester is required to do a
series of tests, each test containing one question and a single answer randomly given by humans or
ChatGPT. The tester needs to determine whether the answer is generated by ChatGPT.
C. Amateur Turing Test, Single Text (single-amateur)
The single-amateur test is conducted in the amateur group. Each tester is required to do a series
of tests, each test containing one question and a single answer randomly given by humans or ChatGPT.
The tester needs to determine whether the answer is generated by ChatGPT.
D. Helpfulness Test (helpfulness)
We are also curious about how helpful are the answers from ChatGPT compared with humans’
answers to one question. Note that helpfulness is a very subjective metric, which can be inﬂuenced
by many factors, including emotion, tester personality, personal preference, etc. Therefore, simply
providing more accurate information or a more detailed analysis may not always lead to a more
helpful answer.
The helpfulness test is conducted in the expert group. Each tester is required to do a series of tests,
each containing one question and a pair of answers (one from human and another from ChatGPT).
Human Evaluation (En)
Pair-expert
Single-expert
Single-amateur
Helpfulness
reddit_eli5
Human Evaluation (Zh)
Pair-expert
Single-expert
Single-amateur
Helpfulness
nlpcc_dbqa
psychology
Table 2: Human evaluations of ChatGPT generated answers for both English and Chinese.
Each tester is asked to pretend that the question is proposed by him/herself, and needs to determine
which answer is more helpful to him/her.
We sample around 30 <question, human_answer, chatgpt_answer> triplets from
each split (i.e., reddit_eli5, wikipedia, medical, etc.) as the samples for the human evaluation. We
allocate 2-5 testers for each split and report their average results. For all Turing tests, we report the
proportion that ChatGPT-generated answer is correctly detected by testers. For the helpfulness test,
we report the proportion that ChatGPT-generated answer is considered to be more helpful.
Several conclusions can be drawn from the results shown in Table 2. Comparing the
results of pair-expert and single-expert, we can ﬁnd that it is easier to distinguish ChatGPTgenerated content when providing a comparison pair than only providing a single answer. Comparing the results of single-expert and single-amateur, we can ﬁnd that the accuracy of
experts is much higher than that of amateurs. The helpfulness test gives the proportion of
questions that volunteers think the ChatGPT answer is more helpful to them. Surprisingly, results
show that ChatGPT’s answers are generally considered to be more helpful than humans’ in
more than half of questions, especially for ﬁnance and psychology areas. By checking the speciﬁc
answers in these domains, we ﬁnd that ChatGPT can usually provide more concrete and speciﬁc
suggestions. However, ChatGPT performs poorly in terms of helpfulness for the medical domain
in both English and Chinese. The ChatGPT often gives lengthy answers to medical consulting in
our collected dataset, while human experts may directly give straightforward answers or suggestions,
which may partly explain why volunteers consider human answers to be more helpful in the medical
Human Summarization
After the above evaluations, we open our collected HC3 dataset to the volunteers where they can
freely browse the comparison answers from humans and ChatGPT. All dataset splits are allocated
to different volunteers, and each volunteer is asked to browse at least 100 groups of comparison
data. After that, we ask them to summarize the characteristics of both human answers and ChatGPT
answers. Eventually, we received more than 200 feedbacks, and we summarize these ﬁndings as
Distinctive Patterns of ChatGPT
(a) ChatGPT writes in an organized manner, with clear logic. Without loss of generality,
ChatGPT loves to deﬁne the core concept in the question. Then it will give out detailed
answers step by step and offers a summary at the end, following the deduction and summary
structure;
(b) ChatGPT tends to offer a long and detailed answer. This is the direct product of the
Reinforcement Learning with Human Feedback, i.e. RLHF, and also partly related to the
pattern (a) unless you offer a prompt such as "Explain it to me in one sentence";
(c) ChatGPT shows less bias and harmful information. ChatGPT is neutral on sensitive
topics, barely showing any attitude towards the realm of politics or discriminatory toxic
conversations;
(d) ChatGPT refuses to answer the question out of its knowledge. For instance, ChatGPT
cannot respond to queries that require information after September 2021. Sometimes
ChatGPT also refuses to answer what it believes it doesn’t know. It is also RLHF’s ability to
implicitly and automatically determine which information is within the model’s knowledge
and which is not.
(e) ChatGPT may fabricate facts. When answering a question that requires professional
knowledge from a particular ﬁeld, ChatGPT may fabricate facts in order to give an answer,
though mentions that InstructGPT model has already shown improvements in truthfulness over GPT-3. For example, in legal questions, ChatGPT may invent some non-existent
legal provisions to answer the question. This phenomenon warns us to be extra careful when
using ChatGPT for professional consultations. Additionally, when a user poses a question
that has no existing answer, ChatGPT may also fabricate facts in order to provide a response.
Many of the conclusions mentioned above like (b),(c),(d) are also discussed in by Fu et al.
Major Differences between Human and ChatGPT
(a) ChatGPT’s responses are generally strictly focused on the given question, whereas
humans’ are divergent and easily shift to other topics. In terms of the richness of
content, humans are more divergent in different aspects, while ChatGPT prefers focusing on
the question itself. Humans can answer the hidden meaning under the question based on
their own common sense and knowledge, but the ChatGPT relies on the literal words of the
question at hand;
(b) ChatGPT provides objective answers, while humans prefer subjective expressions.
Generally, ChatGPT generates safer, more balanced, neutral, and informative texts compared
to humans. As a result, ChatGPT is excellent at interpreting terminology and concepts. On
the other hand, human answers are more speciﬁc and include detailed citations from sources
based on legal provisions, books, and papers, especially when providing suggestions for
medical, legal, and technical problems, etc.;
(c) ChatGPT’s answers are typically formal, meanwhile humans’ are more colloquial.
Humans tend to be more succinct with full of oral abbreviations and slang such as "LOL",
"TL;DR", "GOAT" etc. Humans also love to apply humor, irony, metaphors, and examples, whereas ChatGPT never uses antiphrasis. Additionally, human communication often
includes the "Internet meme" as a way to express themselves in a speciﬁc and vivid way;
(d) ChatGPT expresses less emotion in its responses, while human chooses many punctuation and grammar feature in context to convey their feelings. Human uses multiple
exclamation mark(’!’), question mark(’?’), ellipsis(’...’) to express their strong emotion, and
use various brackets(’(’, ’)’, ’[’, ’]’) to explain things. By contrast, ChatGPT likes to use
conjunctions and adverbs to convey a logical ﬂow of thought, such as "In general", "on the
other hand", "Firstly,..., Secondly,..., Finally" and so on.
Overall, these summarised features indicate that ChatGPT has improved notably in questionanswering tasks for a wide range of domains. Compared with humans, we can imagine ChatGPT
as a conservative team of experts. As a "team", it may lack individuality but can have a more
comprehensive and neutral view towards questions.
vocab size
vocab size
reddit_eli5
nlpcc_dbqa
psychology
Table 3: Average answer length, vocabulary size and density comparisons on our corpus.
Linguistic Analysis
In this section, we analyze the linguistic features of both humans’ and ChatGPT’s answers, and try to
ﬁnd some statistical evidence for the characteristics concluded in Section 3.
Vocabulary Features
In this part, we analyze the vocabulary features of our collected corpus. We are interested in how
humans and ChatGPT differ in the choice of words when answering the same set of questions.
Since the number of human/ChatGPT answers is unbalanced, we randomly sample one answer from
humans and one answer from ChatGPT during our statistical process. We calculated the following
features: average length (L), which is the average number of words in each question; vocab size (V ),
the number of unique words used in all answers; we also propose another feature called density (D),
which is calculated by D = 100 × V/(L × N) where N is the number of answers. Density measures
how crowded different words are used in the text. For example, if we write some articles that add up
to 1000 words, but only 100 different words are used, then the density is 100 × 100/1000 = 10. The
higher the density is, the more different words are used in the same length of text.
In Table 3, we report the vocabulary features for both English and Chinese corpus. Looking at both
features of average length and vocab size, we can see that: compared to ChatGPT, human answers
are relatively shorter, but a larger vocabulary is used. This phenomenon is particularly obvious
in the Chinese open_qa split and the medical splits in both languages, where the average length of
ChatGPT is nearly twice longer than that of humans, but the vocab size is signiﬁcantly smaller.
This phenomenon is also reﬂected by the density factor. The word density of humans is greater than
ChatGPT’s in every split, which further reveals that humans use a more diverse vocabulary in
their expressions.
Part-of-Speech & Dependency Analysis
In this part, we compare the occurrences of different part-of-speech (POS) tags and the characteristics
of the dependency relations.
Part-of-Speech
Figure 1 illustrates the comparisons between humans and ChatGPT in terms of POS usage. In
HC3-English, ChatGPT uses more NOUN, VERB, DET, ADJ, AUX, CCONJ and PART words, while using
less ADV and PUNCT words.
Proportion (%)
Part-of-Speech Comparison (En)
Proportion (%)
Part-of-Speech Comparison (Zh)
Figure 1: Part-of-Speech distribution comparison between ChatGPT and human answers. Results are
sorted by POS proportion of human answers. The upper ﬁgure is for the HC3-English dataset and the
lower is for the HC3-Chinese dataset.
A high proportion of nouns (NOUN) often indicates that the text is more argumentative, exhibiting
informativeness and objectivity . Accordingly, adposition (ADP) and adjective (ADJ) words
also tend to appear more frequently . The frequent co-occurrence of conjunctions (CCONJ)
along with nouns, verbs, and adposition words indicates that the structure of the article and the
relationships of cause-and-effect, progression, or contrast are clear. The above are also typical
characteristics in academic papers or ofﬁcial documents . We believe the RLHF training process
has a great inﬂuence on ChatGPT’s writing style, which partly explains the difference in the POS
tags distribution.
Dependency Parsing
Dependency parsing is a technique that analyzes the grammatical structure of a sentence by identifying
the dependencies between its words. We parse the answers in the corpus and compare the proportion
of different dependency relations and their corresponding dependency distances. Figure 2 shows the
comparison between humans and ChatGPT in HC3-English. Due to the limited space, the Chinese
version is placed in the Appendix A.2.
The comparison of dependency relations exhibits similar characteristics to that of POS tags, where
ChatGPT uses more determination, conjunction, and auxiliary relations. In terms of the dependency
distance, ChatGPT has much longer distances for the punct and dep relations, which is perhaps due
to the fact that CharGPT tends to use longer sentences. However, ChatGPT has obviously shorter
conj relations. According to the analysis of POS tags, ChatGPT usually uses more conjunctions than
humans to make the content more logical, this may explain why the conj relations of ChatGPT are
relatively shorter than humans.
Sentiment Analysis
Humans are emotional beings, it is natural that our emotions are reﬂected in our words, to some
extent. ChatGPT is learned on large-scale human-generated text, but it is further ﬁne-tuned with
human instructions. Therefore we are curious how "emotional" ChatGPT is compared with humans.
We use a multilingual sentiment classiﬁcation model7 ﬁne-tuned on Twitter corpus to conduct
sentiment analysis for both English and Chinese comparison data. Note that deep learning-based
models can be greatly inﬂuenced by some indicating words (such as "but" and "sorry" can easily
7 
compoundcc
Proportion (%)
Dependency Relation Comparison (En)
compoundcc
Proportion (%)
Dependency Distance Comparison (En)
Figure 2: Top-30 dependency relations (upper) and corresponding dependency distances (lower)
comparison between human and ChatGPT answers in HC3-English. Results are sorted by relations
proportion of human answers.
Proportion (%)
Sentiment Distribution (English)
(a) Sentiment distribution of HC3-English
Proportion (%)
Sentiment Distribution (Chinese)
(b) Sentiment distribution of the HC3-Chinese
Figure 3: Proportions of three kinds of sentiments (neutral, positive, and negative) in our corpus.
fool the classiﬁer to predict the "negative" label), making the predictions biased . Therefore, the
sentiment given by the classiﬁer is only a reference to the true sentiment behind the text.
Figure 3 shows the comparison of the sentiment distribution of humans and ChatGPT. Several ﬁndings
can be drawn from the results: First, we ﬁnd that the proportion of neutral emotions is the largest for
both humans and ChatGPT, which is in line with our expectations. However, ChatGPT generally
expresses more neutral sentiments than humans. Then, the proportion of negative emotions is
signiﬁcantly higher than that of positive emotions. Notably, humans express signiﬁcantly more
negative emotions than ChatGPT. The proportion of humans’ positive emotions is also slightly
higher than that of ChatGPT. Overall, ChatGPT is less emotional than humans, though it is not
completely emotionless.
Language Model Perplexity
The perplexity (PPL) is commonly used as a metric for evaluating the performance of language
models (LM). It is deﬁned as the exponential of the negative average log-likelihood of the text under
Proportion
(a) English text ppl
Proportion
(b) English sent ppl
1 2 3 4 5 6 7 8
Proportion
(c) Chinese text ppl
1 2 3 4 5 6 7 8
Proportion
(d) Chinese sent ppl
Figure 4: PPL distributions on both English and Chinese data, as well as both text and sentence levels.
the LM. A lower PPL indicates that the language model is more conﬁdent in its predictions, and
is therefore considered to be a better model. The training of LMs is carried out on large-scale text
corpora, it can be considered that it has learned some common language patterns and text structures.
Therefore, we can use PPL to measure how well a text conforms to common characteristics.
We use the open-source GPT-2 small8 (Wenzhong-GPT2-110M9 for Chinese) model to compute the
PPL (both text-level and sentence-level10 PPLs) of the collected texts. The PPL distributions of text
written by humans and text generated by ChatGPT are shown in Figure 4.
It is clearly observed that, regardless of whether it is at the text level or the sentence level, the content
generated by ChatGPT has relatively lower PPLs compared to the text written by humans. ChatGPT
captured common patterns and structures in the text it was trained on, and is very good at reproducing
them. As a result, text generated by ChatGPT have relatively concentrated low PPLs.
Humans have the ability to express themselves in a wide variety of ways, depending on the context,
audience, and purpose of the text they are writing. This can include using creative or imaginative
elements, such as metaphors, similes, and unique word choices, which can make it more difﬁcult for
GPT2 to predict. Therefore, human-written texts have more high-PPL values, and show a long-tailed
distribution, as demonstrated in Figure 4.
ChatGPT Content Detection
AI-generated content (AIGC) is becoming increasingly prevalent on the internet, and it can be
difﬁcult to distinguish it from human-generated content, as shown in our human evaluation (sec 3.1).
Therefore, AIGC detectors are needed to help identify and ﬂag content that has been created by a
machine, to reduce the potential risks to society caused by improper or malicious use of AI models,
and to improve the transparency and accountability of the information that is shared online.
In this section, we conduct several empirical experiments to investigate the ChatGPT content detection
systems. Detecting AI-generated content is a widely studied topic . Based on these , we establish three different types of detection systems, including machine learning-based and
deep learning-based methods, and evaluate them on different granularities and data sources. Detailed
results and discussions are provided.
Detection of machine-generated text has been gaining popularity as text generation models have
advanced in recent years . Here, we implement three representative methods from classic
machine learning and deep learning, i.e, a logistic regression model trained on the GLTR Test-2 
features, a deep classiﬁer for single-text detection and a deep classiﬁer for QA detection. The deep
classiﬁers for both single-text and QA are based on RoBERTa , a strong pre-trained Transformer
 model. In fact, algorithms for OOD detection or anomaly detection can also be applied to
develop ChatGPT content detectors, which we leave for future work.
8 
9 
10For English text, we used NLTK for sentence segmentation (HarvestText for Chinese).
Figure 5: The experiment design for the training and testing of detectors. Different dataset versions
are generated through ﬁltering or splitting.
 studied three tests to compute features of an input text. Their major assumption is that
to generate ﬂuent and natural-looking text, most decoding strategies sample high probabilities tokens
from the head of the distribution. We select the most powerful Test-2 feature, which is the number
of tokens in the Top-10, Top-100, Top-1000, and 1000+ ranks from the LM predicted probability
distributions. And then a logistic regression model is trained to ﬁnish the classiﬁcation.
RoBERTa-sinlge.
A deep classiﬁer based on the pre-trained LM is always a good choice for
this kind of text classiﬁcation problem. It is also investigated in many studies and demo systems
 . Here we ﬁne-tune the RoBERTa model.
RoBERTa-QA.
While most content detectors are developed to classify whether a single piece of
text is AI-generated, we claim that a detector that supports inputting both a question and an answer
can be quite useful, especially for question-answering scenarios. Therefore, we decide to also build
a QA version detector. The RoBERTa model supports a text pair input format, where a separating
token is used to join a question and its corresponding answer.
Implementation Details
For the LM used by GLTR, we use gpt2-small for English, and Wenzhong-GPT2-110M released
by for Chinese, it is the same with sec. 4.4. For RoBERTa-based deep classiﬁers, we use
roberta-base11 and hfl/chinese-roberta-wwm-ext12 checkpoints for English and Chinese,
respectively. All the above models are obtained from huggingface transformers .
We train the logistic regression model by sklearn on the GLTR Test-2 features from trainset, and
search hyper-params following the code of . The RoBERTa-based detectors are trained by the
facilities of transformers. Speciﬁcally, we use the AdamW optimizer, setting batch size to 32 and
learning rate to 5e −5. We ﬁnetune models by 1 epoch for English, and 2 epochs for Chinese.
Experiment Design
The HC3 dataset consists of questions and their corresponding human/ChatGPT answers. We
extracted all the <question, answer> pairs, and assigned label 0 to pairs with human answers and
label 1 to pairs with ChatGPT answers.
Simply using the original answers from humans and ChatGPT to train a binary classiﬁer is the most
straightforward way. However, there might be some issues by doing so:
• First, based on the observations in Section 3, both human answers and ChatGPT answers
may contain some obvious indicating words that may inﬂuence the effectiveness of models;
• Second, users may want to detect whether a single sentence is generated by ChatGPT,
instead of the full text. This can be quite difﬁcult for a classiﬁer that is only trained on full
• Third, taking the corresponding question of the answer into account may help the detector
to make a more accurate judgment, compared with only considering the answer itself. This
11 
12 
can be widely applied to many QA platforms (like Quora, Stack Overﬂow, and Zhihu) to
ﬁnd out which answer below a certain question is generated by AI.
Therefore, we design different groups of experiments to study these key questions:
• How will the indicating words inﬂuence the detector?
• Is it more challenging for the ChatGPT detectors to detect sentence-level content? Is it harder to
train a sentence-level classiﬁer?
• Can the corresponding question help detectors detect the origin of the answer more accurately?
Figure 5 shows how we generate different types of training and testing sets. Speciﬁcally, we use
the collected raw corpus to construct the ﬁrst train-test sets (the "full text (raw)" in the ﬁgure),
which we call the raw-full version. Then we ﬁlter away the indicated words in the text to obtain the
ﬁltered-full version. By splitting the full text into sentences, we obtain the raw-sent version and the
ﬁltered-sent version. We also combine the full text and the sentences into a mixed version, namely
the raw-mix and ﬁltered-mix version. Overall, we have six different versions of training and testing
sets. Evaluating a model’s performance on version B’s testing set which is trained on version A’s
training set can be seen as an out-of-distribution (OOD) generalization evaluation, which is more
challenging since it requires the model to be robust when facing sample style changes.
Following the above experiment design, we conduct comprehensive empirical studies on all kinds of
derived corpus. Table 4 shows the test F1 scores.
GLTR Test-2
Table 4: F1 scores (%) of different models on each testset, average of each language are reported.
Which detector(s) is more useful? ML-based or DL-based? and Why?
According to Table 4, we can derive following conclusions:
Firstly, the robustness of RoBERTa-based-detector is better than GLTR. The F1-scores of
RoBERTa decrease slightly (1.5-2% in English datasets and 2-3% in Chinese datasets) when sentences
are split by comparing the leading diagonal elements in raw→raw and ﬁltered→ﬁltered. In contrast,
the GLTR reduces signiﬁcantly by over 10% in English datasets, and above 15% in Chinese datasets.
Above all, the RoBERTa-based-detector is more robust with anti-interference character. In contrast,
the GLTR reduces signiﬁcantly by over 10% in English datasets, above 15% in Chinese datasets.
Above all, the RoBERTa-based-detector is more robust with anti-interference character.
Secondly, RoBERTa-based-detector is not affected by indicating words.
The F1-scores of
RoBERTa only slightly decreased by 0.03% in English full dataset, and 0.65% in Chinese full dataset,
as seen in the minus of relevant leading diagonal elements in raw→raw versus ﬁltered→ﬁltered. On
the contrary, evaluations based on GLTR decrease by up to 3.1% on Chinese datasets, though tiny
rise on English datasets, indicating that GLTR is sensitive to indicating words, easily inﬂuenced by
the patterns of ChatGPT.
Lastly, RoBERTa-based-detector is effective in handling Out-Of-Distribution scenarios. When
compared to the original model, it demonstrates a signiﬁcant decrease in performance on GLTR’s
OOD test datasets, with a drop of up to 28.8% on English datasets(ﬁltered-full→ﬁltered-full −
ﬁltered-full→ﬁltered-sent) and 45.5% on Chinese datasets(raw-full→raw-full −raw-full→raw-sent).
However, RoBERTa maintains consistent performance with F1-scores varying by no more than 19%.
How will the indicating words inﬂuence the detector?
We ﬁrst collected a bunch of indicating words for both humans and ChatGPT. For example, ChatGPT’s
indicating words (or phrases) include "AI assistant", "I’m sorry to hear that", and "There’re a few
steps...", etc. and humans’ indicating words may include "Hmm", "Nope", "My view is", etc. In the
ﬁltered version, we remove all sentences in the answers that contain the indicating words for both
humans and ChatGPT.
According to Table 4, removing the indicating words helps the models trained on full-text to perform better across different content granularities. For example, the RoBERTa-ﬁlter-full performs
signiﬁcantly better than RoBERTa-raw-full in terms of sentence-level and mix-level evaluations,
improving more than 3% F1 scores on average. However, the ﬁltering may slightly hurt the performances of the models trained on sentences. This may be because the indicating words play a
bigger part in the sentence-level text compared with the full text. Removing the indicating words
may make some sentences literally unable to be distinguished.
Which granularity is more difﬁcult to detect? Full-text or sentence?
Through the extensive experimental results in Table 5, we conclude that detecting ChatGPT generated texts is more difﬁcult in a single sentence than in a full text. This conclusion can be proved
by the following two points: First, our results show that both English and Chinese sentence-based
detectors (i.e., raw-sent and ﬁltered-sent versions) achieve satisfactory results w.r.t. the testing
task of detecting either ChatGPT generated paragraphs or sentences, whereas the opposite is not
true——raw-full and ﬁltered-full are relatively inferior when detecting ChatGPT generated sentences.
In other words, detectors trained on "hard samples" (i.e., sentence corpus) are much easier to solve
simple task (i.e., detecting full corpus), while "simple samples" (i.e., full corpus) may be less useful
for solving more difﬁcult task (i.e., sentence corpus).
Second, we observe that although both full and sentence corpus are provided in the raw-mix and
ﬁltered-mix versions, it is still more difﬁcult for them to detect single sentences generated by ChatGPT.
This is even more obvious for the Chinese corpus, where the F1-score of raw-mix trained on the
Chinese corpus is 94.09% for testing raw sentence answers, compared to that 97.43% for testing raw
full answers. Similar results can be observed for the ﬁltered corpus, where F1-score of ﬁltered-mix is
95.61% for testing ﬁltered sentence answers, compared to its F1-score of 97.66% for testing ﬁltered
full answers. One possible explanation is that the expression pattern of ChatGPT is more obvious
(therefore more easily detected) when paragraphs of text are provided, whereas it is more difﬁcult to
detect generated single sentences.
full-ﬁltered
sent-ﬁltered
mix-ﬁltered
Table 5: F1 scores (%) of RoBERTa models at full & sent & mix mode.
Which corpus is more helpful for model training? Full-text, sentence, or mix of the
We ﬁnd that both English and Chinese RoBERTa-based detectors are more robust when ﬁnegrained corpus data is available in model training. The sentence-based detectors outperform
full-based detectors w.r.t. F1-scores, while the latter can be signiﬁcantly improved when the sentence
corpus is injected in model training, as we observe that mix-based detectors also achieve satisfactory
results. For English corpus, raw-full only achieves 81.89% F1-score for testing sentence answers,
while raw-sent is signiﬁcantly better with 98.43% F1-score, as shown in Table 5. Moreover, the
relatively inferior detection performance can be improved by injecting sentence answers into the
detector, where we ﬁnd that raw-mix can also obtain signiﬁcant improvement (with 98.31% F1-score)
over the detectors trained on only full answers. Similar conclusions can be acquired for the ﬁltered
versions, where both ﬁltered-sent and ﬁltered-mix signiﬁcantly outperform ﬁltered-full version w.r.t.
F1-score, which holds for both English and Chinese corpus.
We indicate that the above conclusions could also hold for other types of detectors like GLTR Test-2
feature-based detectors, as is shown in Table 4. For GLTR Test-2, the average performance of F1score of raw-full and ﬁltered-full is 61.74% and 69.47%, respectively, compared to that of raw-sent
76.26% and ﬁltered-sent 76.41%, where the performance of detectors trained on the mixed corpus is
close to the sentence-based versions.
Taking into account the conclusions of the previous paragraph about the detection difﬁculty between
full and sentence answers, we indicate that the ﬁne-grained corpus is helpful for distinguishing
ChatGPT generated texts, as it additionally provides guidance and hints in model training for
detecting the subtle patterns of ChatGPT hidden in single sentences.
Will a QA-style detector be more effective than a single-text detector?
Table 6 demonstrates the results of both raw-full and ﬁltered-full models across all test datasets.
On English datasets, the QA model’s F1-scores are superior to that of the single model, except for
two full test datasets, where it averages 97.48% F1-scores and surpasses single model by 5.63%.
There exist some differences in Chinese datasets, where the single model outperforms QA in raw-full
train dataset. However, the QA model still yields the best evaluation at 94.22%.
In conclusion, the QA model is generally more effective than the single model and is suitable
for ﬁltered scenarios. And the QA training makes models more robust to the sentence inputs.
Train →raw - full
Train →ﬁltered - full
Table 6: F1 scores (%) of RoBERTa models trained with QA & Single settings.
Which data sources are more difﬁcult for the ChatGPT detectors? and What are the
conditions that make it easier to detect ChatGPT?
As shown in Table 7, the evaluation results based on ﬁltered-full model are separated by various
sources in our HC3 dataset.
On the English datasets, the F1-scores for human answers are slightly higher than those for ChatGPT
without any exceptions, regardless of whether RoBERTa or GLTR is used on full-text test datasets.
However, the F1-scores for ChatGPT are highly inconsistent on transferring test datasets particularly open-qa dataset with varying performance. In terms of data resource, reddit-eli5 and
finance-en has higher values, while wiki-csai poses a challenge for detectors.
On the Chinese datasets, the F1-scores of humans and ChatGPT are comparable with no signiﬁcant
difference. This suggests that the difﬁculty in detecting ChatGPT depends on the data source. It
is observed that open-qa and baike have better performance, whereas the nlpcc-dbqa has
lower performance.
Above all, the evaluations on Chinese dataset show more stability on transferring test dataset compared
to the English datasets. Furthermore, it’s evident that the F1-scores of ChatGPT are lower than those
of human answers, regardless of whether the dataset is English or Chinese. This indicates that
ChatGPT’s detector relies more heavily on In-Distribution models.
reddit_eli5
nlpcc_dbqa
Table 7: Human (F1-hu) and ChatGPT (F1-ch) detection F1 scores (%) w.r.t. different data source,
models are trained on ﬁltered full text, tested on ﬁltered full and sent. On HC3-Chinese, we omitted the
results of medicine and psychology domains, which are similar to ﬁnance and open_qa, respectively.
Conclusion
In this work, we propose the HC3 (Human ChatGPT Comparison Corpus) dataset, which consists
of nearly 40K questions and their corresponding human/ChatGPT answers. Based on the HC3
dataset, we conduct extensive studies including human evaluations, linguistic analysis, and content
detection experiments. The human evaluations and linguistics analysis provide us insights into the
implicit differences between humans and ChatGPT, which motivate our thoughts on LLMs’ future
directions. The ChatGPT content detection experiments illustrate some important conclusions that
can provide beneﬁcial guides to the research and development of AIGC-detection tools. We make
all our data, code, and models publicly available to facilitate related research and applications at
 
Limitations
Despite our comprehensive analysis of ChatGPT, there are still several limitations in the current
paper, which will be considered for improvement in our future work:
1. Despite our efforts in data collection, the amount and range of collected data are still not
enough and the data from different sources are unbalanced, due to limited time and resources.
To make more accurate linguistic analyses and content detection, more data with different
styles, sources, and languages are needed;
2. Currently, all the collected ChatGPT answers are generated without special prompts.
Therefore, the analysis and conclusions in this paper are built upon ChatGPT’s most general
style/state. For example, using special prompts such as "Pretending you are Shakespeare..."
can generate content that bypasses our detectors or make the conclusions in this paper
untenable;
3. ChatGPT (perhaps) is mainly trained on English corpus while less on Chinese. Therefore,
the conclusions drawn from the HC3-Chinese dataset may not always be precise.
Acknowledgments
We would like to thank the volunteers that participated in our human evaluations, many of them are
our good friends and dear family members. We would like to thank Junhui Zhu (BLCU-ICALL)
for the valuable discussions on linguistic analysis. Biyang Guo would like to thank Prof. Hailiang
Huang and Prof. Songqiao Han (AI Lab, SUFE) for providing insightful feedback on the topics and
directions for this project. Xin Zhang would like to thank Yu Zhao (NeXt, NUS and CIC, TJU) for
sharing the OpenAI account. Finally, we thank all team members of this project for their unique
contributions. We together make this possible.