IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
Proposal-free Network for Instance-level
Object Segmentation
Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Jianchao Yang, Liang Lin, Shuicheng Yan Senior
Member, IEEE
Abstract—Instance-level object segmentation is an important yet under-explored task. The few existing studies are almost all
based on region proposal methods to extract candidate segments and then utilize object classiﬁcation to produce ﬁnal results.
Nonetheless, generating accurate region proposals itself is quite challenging. In this work, we propose a Proposal-Free Network
(PFN ) to address the instance-level object segmentation problem, which outputs the instance numbers of different categories
and the pixel-level information on 1) the coordinates of the instance bounding box each pixel belongs to, and 2) the conﬁdences
of different categories for each pixel, based on pixel-to-pixel deep convolutional neural network. All the outputs together, by
using any off-the-shelf clustering method for simple post-processing, can naturally generate the ultimate instance-level object
segmentation results. The whole PFN can be easily trained in an end-to-end way without the requirement of a proposal generation
stage. Extensive evaluations on the challenging PASCAL VOC 2012 semantic segmentation benchmark demonstrate that the
proposed PFN solution well beats the state-of-the-arts for instance-level object segmentation. In particular, the AP r over 20
classes at 0.5 IoU reaches 58.7% by PFN, signiﬁcantly higher than 43.8% and 46.3% by the state-of-the-art algorithms, SDS 
and , respectively.
Index Terms—Instance-level Object Segmentation, Proposal-free, Convolutional Neural Network
INTRODUCTION
Over the past few decades, two of the most popular object
recognition tasks, object detection and semantic segmentation, have received a lot of attention. The goal of object
detection is to accurately predict the semantic category and
the bounding box location for each object instance, which is
a quite coarse localization. Different from object detection,
the semantic segmentation task aims to assign the pixelwise labels for each image but provides no indication of
the object instances, such as the object instance number and
precise semantic region for any particular instance. In this
work, we follow some of the recent works 
and attempt to solve a more challenging task, instancelevel object segmentation, which predicts the segmentation
mask for each instance of each category. We suggest that
the next generation of object recognition should provide a
richer and more detailed parsing for each image by labeling
each object instance with an accurate pixel-wise segmentation mask. This is particularly important for real-world
applications such as image captioning, image retrieval, 3-D
navigation and driver assistance, where describing a scene
with detailed individual instance regions is potentially more
informative than describing roughly with located object
• Xiaodan Liang is with Department of Electrical and Computer Engineering, National University of Singapore, and also with Sun Yatsen University. Yunchao Wei is with Department of Electrical and
Computer Engineering, National University of Singapore, and also
with the Institute of Information Science, Beijing Jiaotong University.
Xiaohui Shen is with Adobe Research. Jianchao Yang is with Snapchat
Research. Liang Lin is with Sun Yat-sen Unviersity. Shuicheng Yan is
with Department of Electrical and Computer Engineering, National
University of Singapore.
detections. However, instance-level object segmentation is
very challenging due to high occlusion, diverse shape
deformation and appearance patterns, obscured boundaries
with respect to other instances and background clutters in
real-world scenes. In addition, the exact instance number
of each category within an image is dramatically different.
Recently, tremendous advances in semantic segmentation and object detection 
have been made relying on deep convolutional neural networks (DCNN) . Some previous works have been
proposed to address instance-level object segmentation.
Unfortunately, none of them has achieved excellent performance in an end-to-end way. In general, these previous
methods take complicated pre-processing such as bottomup region proposal generation or postprocessing such as graphical inference as the requisite.
Speciﬁcally, the recent two approaches, SDS and the
one proposed by Chen et al. , use the region proposal
methods to ﬁrst generate potential region proposals and
then classify on these regions. After classiﬁcation, postprocessing such as non-maximum suppression (NMS) or
Graph-cut inference, is used to reﬁne the regions, eliminate
duplicates and rescore these regions. Note that most region
proposal techniques typically generate thousands of potential regions, and take more than one second
per image. Additionally, these proposal-based approaches
often fail in the presence of strong occlusions. When only
small regions are observed and evaluated without awareness
of the global context, even a highly accurate classiﬁer
can produce many false alarms. Depending on the region
proposal techniques, the common pipelines are often trained
using several independent stages. These separate pipelines
 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
Category-level segmentation Predicted instance locations Instance-level segmentation
Fig. 1. Exemplar instance-level object segmentation results. For each image, the category-level segmentation
results, predicted instance locations for all foreground pixels and instance-level segmentation results are
sequentially shown in each row. Different colors indicate the different object instances for each category. To better
show the predicted instance locations, we plot velocity vectors that start from each pixel to its corresponding
predicted instance center as shown by the arrow. Note that the pixels predicting similar object centers can be
directly collected as one instance region. Best view in color and scale up three times.
rely on independent techniques at each stage and the targets
of the stages are signiﬁcantly different. For example, the
region proposal methods try to maximize region recalls
while the classiﬁcation optimizes for single class accuracy.
In this paper, we propose a simple yet effective Proposal-
Free Network (PFN) for solving the instance-level segmentation task in an end-to-end way. The motivation of
the proposed network is illustrated in Figure 1. The pixels
predicting the same instance locations can be directly clustered into the same object instance region. Moreover, the
object boundaries of the occluded objects can be inferred
by the difference in the predicted instance locations. For
simplicity, we use the term instance locations to denote the
coordinates of the instance bounding box each pixel belongs
to. Inspired by the observation that humans glance at an
image and instantly know what and where the objects are in
the image, we reformulate the instance-level segmentation
task in the proposed network by directly inferring the
regions of object instances from the global image context,
in which the traditional region proposal generation step is
totally disregarded. The proposed PFN framework is shown
in Figure 2. To solve the semantic instance-level object
segmentation task, three sub-tasks are addressed: categorylevel segmentation, instance location prediction for each
pixel, and instance number prediction for each category in
the entire image.
First, the convolutional network is ﬁne tuned based
on the pre-trained VGG classiﬁcation net to predict
the category-level segmentation. In this way, the domainspeciﬁc feature representation on semantic segmentation for
each pixel can be learned.
Second, by ﬁne-tuning on the category-level segmentation network, the instance locations for each pixel as
well as the instance number of each category are simultaneously predicted by the further updated instance-level
segmentation network. In terms of instance locations for
each pixel, six location maps including the coordinates of
the center, the top left corner and the bottom right corner
of the bounding box of each instance, are predicted. The
predicted coordinates can be complementary to each other
and make the algorithm more robust for handling close
or occluded instances. To obtain more precise instance
location prediction for each pixel, multi-scale prediction
streams with individual supervision (i.e. multi-loss) are
appended to jointly encode local details from the early,
ﬁne layers and the global semantic information from the
deep, coarse layers. The feature maps from deep layers
often focus on the global structure, but are insensitive to
local boundaries and spatial displacement. In contrast, the
feature maps from early layers can sense better the local
detailed boundaries. The fusion layer combining multi-scale
predictions is utilized before the ﬁnal prediction layer.
Third, the instance numbers of all categories are described with a real number vector and also regressed
with Euclidean loss in the instance-level segmentation
network. Note that the instance number vector embraces
the category-level information (whether the instance of a
speciﬁc category appears or not) and instance-level information (how many object instances appear for a speciﬁc
category). Thus, the intermediate feature maps from the
category-level segmentation network and the instance-level
feature maps after the fusion layer from the instance-level
segmentation network are concatenated together, which can
be utilized to jointly predict the instance numbers.
In the testing stage, instance numbers and pixel-level
information including category-level conﬁdences and coordinates of the instance bounding box each pixel belongs to, can together help generate the ultimate instancelevel segmentation after clustering. Note that any off-theself clustering method can be used for this simple post-
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
Category-level conﬁdences
Instance locations
Convolutional feature maps
Instance number
Instance-level
segmentation
Clustering
Fig. 2. The proposal-free network overview. Our network predicts the instance numbers of all categories and
the pixel-level information that includes the category-level conﬁdences for each pixel and the coordinates of
the instance bounding box each pixel belongs to. The instance location prediction for each pixel involves the
coordinates of center, top-left corner and bottom-right corner of the object instance that a speciﬁc pixel belongs
to. Any off-the-self clustering method can be utilized to generate ultimate instance-level segmentation results.
processing, and the predicted instance number speciﬁes the
exact cluster number for the corresponding category.
Comprehensive evaluations and comparisons on the PAS-
CAL VOC 2012 segmentation benchmark well demonstrate that the proposed proposal-free network yields results
that signiﬁcantly surpass all previous published methods.
It boosts the current state-of-the-art performance from
46.3% to 58.7%. It should be noted that all previous
works utilize the extra region proposal extraction algorithms
to generate the region candidates and then feed these
candidates into a classiﬁcation network and complex postprocessing steps. Instead, our PFN generates the instancelevel segmentation results in a much simple and more
straightforward way.
RELATED WORK
Deep convolutional neural networks (DCNN) have achieved
great success in object classiﬁcation ,
object detection and object segmentation . In this section, we discuss the
most relevant work on object detection, semantic segmentation and instance-level object segmentation.
Object Detection. Object detection aims to localize and
recognize every object instance with a bounding box. The
detection pipelines generally start from
extracting a set of box proposals from input images and
then identify the objects using classiﬁers or localizers.
The box proposals are extracted either by the hand-crafted
pipelines such as selective search , EdgeBox or
the designed convolutional neural network such as deep
MultiBox or region proposal network . For instance, the region proposal network simultaneously
predicts object bounds and objectiveness scores to generate
a batch of proposals and then uses the Fast R-CNN 
for detection. Different from these prior work, Redmon et
al. ﬁrst proposed a You Only Look Once (YOLO)
pipeline that predicts bounding boxes and class probabilities
directly from full images in one evaluation. Our work shares
some similarities with YOLO, where the region proposal
generation is discarded. However, our PFN is based on the
intuition that the pixels inferring similar instance locations
can be directly collected as a single instance region. The
pixel-wise instance locations and the instance number of
each category are simultaneously optimized in one network.
Finally, the ﬁne-grained segmentation mask of each instance can be produced with our PFN instead of the coarse
outputs depicted by the bounding boxes from YOLO.
Semantic Segmentation. The most recent progress
in object segmentation was achieved by
ﬁne-tuning the pre-trained classiﬁcation network with the
ground-truth category-level masks. For instance, Long et
al. proposed a fully convolutional network for pixelwise labeling. Papandreou et al. utilized the foreground/background segmentation methods to generate segmentation masks, and conditional random ﬁeld inference is
used to reﬁne the segmentation results. Zheng et al. 
formulated the conditional random ﬁelds as recurrent neural
networks for dense semantic prediction. Different from
the category-level prediction by these previous methods,
our PFN targets at predicting the instance-level object
segmentation that provides more powerful and informative
predictions to enable the real-world vision applications.
Note that these previous pipelines using the pixel-wise
cross-entropy loss for semantic segmentation cannot be
directly utilized for instance-level segmentation because
the instance number of each category for different images
signiﬁcantly varies, and the output size of prediction maps
cannot be constrained to a pre-determined number.
Instance-level Object Segmentation. Recently several
approaches which tackle the instance-level object segmentation have emerged. Most of the prior
works utilize the region proposal methods as the requisite.
For example, Hariharan et al. classiﬁed region proposals
using features extracted from both the bounding box and
the region foreground with a jointly trained CNN. Similar
to , Chen et al. proposed to use the category-speciﬁc
reasoning and shape prediction through exemplars to further reﬁne the results after classifying the proposals. 
designed a higher-order loss function to make an optimal
cut in the hierarchical segmentation tree based on the region
features. Other works have resorted to the object detection
task to initialize the instance segmentation and the complex
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
Prediction
location g.t.
Fine-tuning
Category-level
Segmentation
Instance-level
Segmentation
Deep supervision for each scale
number g.t.
fc Prediction
Down-sampling
Prediction
Down-sampling
Down-sampling
Category-level
segmentation g.t.
coordinate map
Fig. 3. The detailed network architecture and parameter setting of PFN. First, the category-level segmentation
network is ﬁne-tuned based on the pre-trained VGG-16 classiﬁcation network. The cross-entropy loss is used
for optimization. Second, the instance-level segmentation network that simultaneously predicts the instance
numbers of all categories and the instance location vector for each pixel is further ﬁne-tuned. The multi-scale
prediction streams (with different resolution and reception ﬁelds) are appended to the intermediate convolutional
layers, and are then fused to generate ﬁnal instance location predictions. During each stream, we incorporate the
corresponding coordinates (i.e. x and y dimension) of each pixel as the feature maps in the second convolutional
layer with 130 = 128 + 2 channels. The regression loss is used during training. To predict instance numbers,
the convolutional feature maps and the instance location maps are concatenated together for inference, and the
Euclidean loss is used. The two losses from two targets are jointly optimized for the whole network training.
post-processing such as integer quadratic program and
probabilistic model to further determine the instance
These prior works based on region proposals are
very complicated due to several pre-processing and postprocessing steps. In addition, combining independent steps
is not an optimal solution because the local and global
context information cannot be incorporated together for
inferring. In contrast, our PFN directly predicts pixelwise instance location maps and uses a simple clustering
technique to generate instance-level segmentation results.
In particular, Zhang et al. predicted depth-ordered
instance labels of the image patch and then combined
predictions into the ﬁnal labeling via the Markov Random
Field inference. However, the instance number to be present
in each image patch is limited to be smaller than 6
(including background), which makes it not scalable for
real-world images with an arbitrary number of possible
object instances. Instead, our network predicts the instance
number in a totally data-driven way by the trained network,
which can be naturally scalable and easily extended to other
instance-level recognition tasks.
PROPOSAL-FREE NETWORK
Figure 3 shows the detailed network architecture of PFN.
The category-level segmentation, instance locations for
each pixel and instance numbers of all categories are taken
as three targets during the PFN training.
Category-level Segmentation Prediction
The proposed PFN is ﬁne-tuned based on the publicly available pre-trained VGG 16-layer classiﬁcation network 
for the dense category-level segmentation task. We utilize
the “DeepLab-CRF-LargeFOV” network structure as the
basic presented in due to its leading accuracy and
competitive efﬁciency. The important convoluational ﬁlters
are shown in the top row of Figure 3, and other intermediate convolutional layers can be found in the published
model ﬁle . The reception ﬁeld of the “DeepLab-CRF-
LargeFOV” architecture is 224 × 224 with zero-padding,
which enables effective prediction of the subsequent instance locations that requires the global image context for
reasoning. For category-level segmentation, the 1000-way
ImageNet classiﬁer in the last layer of VGG-16 is replaced
with C + 1 conﬁdence maps, where C is the category
number. The loss function is the sum of pixel-wise crossentropy in terms of the conﬁdence maps (down-sampled
by 8 compared to the original image). During testing, the
fully-connected conditional random ﬁelds are employed
to generate more smooth and accurate segmentation maps.
This ﬁne-tuned category-level network can generate semantic segmentation masks for the subsequent instancelevel segmentation for each input image. Then the instancelevel network is ﬁne-tuned based on the category-level
network, where the C + 1 category-level predictions are
eliminated. Note that we use two separate stages from opti-
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
mizing category-level segmentation and instance-level segmentation. The intuition is that category-level segmentation
prefers the prediction that is insensitive for different object
instances of a speciﬁc category while instance-level segmentation aims to distinguish between individual instances.
The motivations of two targets are signiﬁcantly different.
Therefore the convolutional feature maps, especially for the
latter convolutional layers, cannot be shared. We verify
the superiority of subsequently ﬁne-tuning two separate
networks for two tasks in the experiment. In addition,
the performance on instance-level segmentation is much
better when ﬁne-tuning the instance-level network based
on the category-level segmentation network compared to
the original VGG-16. This may be because the categorylevel segmentation can provide a better start for parameter
learning where the basic segmentation-aware convolutional
ﬁlters have already been well learned.
Instance-level Segmentation Prediction
The instance-level segmentation network takes an image
with an arbitrary size as the input and outputs the corresponding instance locations for all the pixels and the
instance number of each category.
Pixel-wise Instance Location Prediction. For each image, the instance location vector of each pixel is deﬁned as
the bounding box information of its corresponding object
instance that contains the pixel. The object instance s of a
speciﬁc category can be identiﬁed by its center (cx, cy), the
top-left corner (lx, ly) and the bottom-right corner (rx, ry)
of its surrounding bounding box, as illustrated in Figure 2.
Note that the information may be redundant by using two
corners besides the centers. Incorporating the redundant
information can be treated as the model combination, which
increases the robustness of the algorithm to noises and inaccurate prediction. For each pixel i belonging to the object
instance s, the ground-truth instance location vector is denoted as ti = (cx
where ws and hs are the width and the height of the object
instance s, respectively. With these deﬁnitions, we minimize an objective function to optimize the instance location
which is inspired by the one used for Fast R-CNN . Let
ti denote the predicted location vector and t∗
i the groundtruth location vector for each pixel i, respectively. The loss
function ℓo can be deﬁned as
i ≥1]R(ti −t∗
i ∈{0, 1, 2, . . . , C} is the semantic label for the
pixel i, and C is the category number. R is the robust loss
function (smooth-L1) in . The term [k∗
i ≥1]R(ti −t∗
means the regression loss that is activated only for the
foreground pixels and disabled for background pixels. The
reason of using this ﬁltered loss is that predicting the
instance locations is only possible for foreground pixels
which deﬁnitely belong to a speciﬁc instance.
Following the recent results of , we have also
utilized the multi-scale prediction to increase the instance
location prediction accuracy. As illustrated in Figure 3,
the ﬁve multi-scale prediction streams are attached to the
input image, the output of each of the ﬁrst three max
pooling layers and the last convolutional layer (fc7) in
the category-level segmentation network. For each stream,
two layers (ﬁrst layer: 128 convolutional ﬁlters, second
layer: 130 convolutional ﬁlters) and deep supervision (i.e.
individual loss for each stream) are utilized. The spatial
padding for each convolutional layer is set so that the
spatial resolution of feature maps is preserved. The multiscale predictions from ﬁve streams are accordingly downsampled and then concatenated to generate the fused feature
maps (as the fusing layer in Figure 3). Then the 1 × 1
convolutional ﬁlters are used to generate the ﬁnal pixel-wise
predictions. It should be noted that multi-scale predictions
are with different spatial resolution and inferred under
different reception ﬁelds. In this way, the ﬁne local details
(e.g. boundaries and local consistency) captured by early
layers with higher resolution and the high-level semantic
information from subsequent layers with lower resolution
can jointly contribute to the ﬁnal prediction.
To predict ﬁnal instance location maps in each stream,
we attach their own coordinates of all the pixels as the
additional feature maps in the second convolutional layer
to help generate the instance location predictions. The intuition is that predicting accurate instance locations for each
pixel may be difﬁcult due to various spatial displacements
in each image while the intrinsic offsets between each pixel
position and its corresponding instance locations are much
easier to be learned. By incorporating the spatial coordinate
maps into feature maps, more accurate location predictions
can be obtained, which is also veriﬁed in our experiment.
Consider that the feature maps xv of the v-th convolutional
layer are a three-dimensional array of size hv × wv × dv,
where hv and wv are spatial dimensions and dv is the
channel number. We generate 2 spatial coordinate maps
2] with size hv × wv, where xo
ix,iy,1 and xo
ix,iy,2 at
the spatial position (ix, iy) for each pixel i are set as ix
and iy, respectively. By concatenating the feature maps
xv and the coordinate maps, the combined feature maps
ˆxv = [xv, xo
2] of size hv×wv×(dv+2) can be obtained.
The outputs xv+1
ix,iy at the location (ix, iy) in the next layer
can be computed by
ix,iy = fb({ˆxv
ix+δix,ix+δiy }0≤δix,δiy ≤b),
where b is the kernel size and fb is the convolutional
ﬁlters. In PFN, xv+1 represents the ﬁnal instance location
prediction maps with six channels.
Suppose we have M = 5 multi-scale prediction streams,
and each stream is associated with a regression loss
m(·), m ∈{1, 2, . . . , M}. For each image, the loss for
the ﬁnal prediction maps after fusing is denoted as ℓo
The overall loss function for predicting pixel-wise instance
locations then becomes
Lo(t, t∗) =
fuse(ti, t∗
where t = {ti} and t∗= {t∗
i } represent the predicted
instance locations and ground-truth instance locations of
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
Category-level segmentation:
Instance number prediction:
Category-level segmentation:
person, bicycle, car.
Instance number prediction
person: 4, bicycle: 2.
Predicted category-level
segmentation
Reﬁned category-level
segmentation
Instance-level segmentation
The exemplar segmentation results by reﬁning the category-level segmentation with the predicted
instance numbers. For each image, we show their classiﬁcation results inferred from category-level segmentation
and the predicted instance numbers in the left. In the ﬁrst row, the reﬁning strategy is to convert the inconsistent
predicted labels into background. In the second row, the reﬁning strategy is to convert the wrongly predicted
labels in category-level segmentation to the ones predicted in the instance number vector. Different colors
indicate different object instances. Better viewed in zoomed-in color pdf ﬁle.
all pixels, respectively. The Ωdenotes the number of foreground pixels for each image. Divided by Ω, the resulting
loss Lo can be prevented from being too large, which may
lead to non-convergence.
Instance Number Prediction. Another sub-task of PFN
is to predict the instance numbers of all categories. The
instance numbers of the input image that account for the
object instances of each category naturally contains the
category-level information and instance-level information.
As shown in Figure 3, the feature maps of the last convolutional layer from the previously trained category-level
segmentation network and the instance location predictions
are combined together to form the fused feature maps with
1024 + 6 channels. These fused feature maps are then convolved with 3 × 3 convolutional ﬁlters and down-sampled
with stride 2 to obtain the 128 feature maps. Then the fullyconnected layer with 1024 outputs is performed to generate
the ﬁnal C-dimensional instance number prediction maps.
Given an input image I, we denote the instance number
vector of all C categories as g = [g1, g2, . . . , gC], where
gc, c ∈{1, 2, . . . , C} represents the object instance number
of each category appearing in the image. Let g denote
the predicted instance number vector and g∗represent
the ground-truth instance number vector for each image,
respectively. The loss function Ln is deﬁned as
Ln(g, g∗) = 1
Network Training. To train the whole instance-level
network, the over loss function L for each image is actuated
L(t, t∗, g, g∗) = λLo(t, t∗) + Ln(g, g∗).
The class-balancing parameter λ is empirically set to 10,
which means the bias towards better pixel-wise instance
location prediction. In this way, the instance number predictions and pixel-wise instance location predictions are
jointly optimized in a uniﬁed network. The two different
targets can beneﬁt each other by learning more robust and
discriminative shared convolutional ﬁlters. We borrow the
convolutional ﬁlters except for those of the last prediction
layer in the previously trained category-level network to
initialize the parameters of the instance-level network. We
randomly initialize all newly added layers by drawing
weights from a zero-mean Gaussian distribution with standard deviation 0.01. The network can be trained by backpropagation and stochastic gradient descent (SGD) .
Testing Stage
During testing, we ﬁrst feed the input image I into the
category-level segmentation network to obtain categorylevel segmentation results, and then pass the input image
into the instance-level network to get the instance number
vector g and the pixel-wise instance location predictions t.
Then the clustering based on all the predicted instance
locations t of all the pixels can be performed. We separately
cluster the predicted instance locations for each category,
which can be obtained by ﬁltering the t with the categorylevel segmentation result p, and the predicted instance
numbers of all categories g indicate the expected cluster
numbers used for spectral clustering. The simple normalized spectral clustering is utilized due to its simplicity
and effectiveness. For each category c, the similarity matrix
W is constructed by calculating the similarities between
any pair of pixels that belong to the resulting segmentation
mask pc. Let the spatial coordinate vectors for the pixel i
and j be qi = [ix, iy] and qj = [jx, jy], respectively. The
ti and qi vectors are all normalized by their corresponding
maximum. The Gaussian similarity function wi,j for each
pair (i, j) is computed as
wi,j = exp(−||ti −tj||2/|ti|
) + exp(−||qi −qj||2/|qi|
where |ti| denotes the feature dimension for the vector ti,
which equals 6 (the coordinates of centers, top-left corner
and bottom-right corner), and |qi| indicates the feature
dimension of qi, which equals 2. During clustering, we
simply connect all pixels in the same segmentation mask of
a speciﬁc category with positive similarity because the local
neighboring relationships can be captured by the second
term in Eqn. (6). We simply set σ = 0.5 for all images.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
Category-level segmentation
Instance-level segmentation
w/o size constraints
Instance-level segmentation
Fig. 5. Comparison of segmentation results by constraining the pixel number of each clustered object instance.
To make the clustering results robust to the initialization
of seeds during the k-means step of spectral clustering,
we randomly select the seeds twenty times by balancing
the accuracy and computational cost. Then the clustering
result with maximal average within-cluster similarities for
all clusters is selected as the ﬁnal result.
Note that inconsistent global image category predictions
from instance number vectors and pixel-wise category-level
segmentation are often observed. For example, as illustrated
in the ﬁrst row of Figure 4, the instance number prediction
infers 4 person instances and 2 bicycle instances while the
category-level segmentation indicates three categories (i.e.
person, bicycle, car) appearing in the image. Thus it is
necessary to keep the predicted global image category to
be consistent between the instance number prediction and
the pixel-wise segmentation. Note that the instance number
prediction task is much simpler than pixel-wise semantic
segmentation due to dense pixel-wise optimization targets.
We can thus use instance number prediction to reﬁne the
produced category-level segmentation.
The object category from instance number prediction can
be easily obtained by thresholding the instance number
vector by τ = 0.5, which means, if the predicted instance
number of a speciﬁc category c is larger than τ, the category
c is regarded as the true label. Speciﬁcally, two strategies
are adopted: ﬁrst, if more than one category is predicted to
have at least one instance in the image, any pixels assigned
with all other categories (i.e. the categories with predicted
instance number 0) will be re-labeled as the background,
as illustrated in the ﬁrst row of Figure 4; second, if only
one category is inferred from instance number prediction,
pixels labeled with other object categories (excluding background pixels) in the semantic segmentation mask will be
totally converted into the predicted ones, as illustrated in
the second row of Figure 4. The reﬁned category-level
segmentation masks are used to further generate instancelevel segments.
In addition, the predicted segmentation result is not
perfect due to the noisy background pixels. The instance
locations of pixels belonging to one object have much
higher possibilities to form a cluster while the predictions
of background pixels are often quite random, forming very
small clusters. Therefore, we experimentally discard those
clusters, whose pixel numbers are less than 0.1% of the
pixels in the segmentation mask. Finally, after obtaining
the ﬁnal clustering result for each category, the instancelevel object segmentation result can be easily obtained
by combining all the clustering results of all categories.
Example results after constraining the pixel number of each
clustered instance region are shown in Figure 5.
EXPERIMENTS
Experimental Settings
Dataset and Evaluation Metrics. The proposed PFN is
extensively evaluated on the PASCAL VOC 2012 validation
segmentation benchmark . We compare our method with
two state-of-the-art algorithms: SDS and the method by
Chen et al. . Following two baselines , the segmentation annotations from SBD are used for training
the network, and 1,449 images in the PASCAL VOC 2012
segmentation validation set are used for evaluation. We
cannot report results on PASCAL VOC 2012 segmentation
test set because no instance-level segmentation annotation is
provided. In addition, because VOC 2010 segmentation set
is only a subset of VOC 2012 and no baseline has reported
results on VOC 2010, we only evaluate our algorithm
on VOC 2012 set. For fair comparison with state-of-theart instance level segmentation methods, AP r and AP r
metrics are used following SDS . The AP r metric
measures the average precision under 0.5 IoU overlap with
ground-truth segmentation. Hariharan et al. proposed
to vary IoU scores from 0.1 to 0.9 to show the performance
for different applications. The AP r
vol metric calculates the
mean of AP r under all IoU scores. Note that two baselines
ﬁne-tune the networks based on Alexnet architecture .
For fair comparison, we also report results based on the
Alexnet architecture .
Training Strategies. All networks in our experiments
are trained and tested based on the published DeepLab
code , which is implemented based on the publicly
available Caffe platform on a single NVIDIA GeForce
Titan GPU with 6GB memory. We ﬁrst train the categorylevel segmentation network, which is then used to initialize
our instance-level segmentation network for ﬁne tuning. For
both training stages, we randomly initialize all new layers
by drawing weights from a zero-mean Gaussian distribution
with standard deviation 0.01. We use mini-batch size of 8
images, initial learning rate of 0.001 for pre-trained layers,
and 0.01 for newly added layers in all our experiments. We
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
decrease the learning rate to 1/10 of the previous one after
20 epochs and train the two networks for roughly 60 epochs
one after the other. The momentum and the weight decay
are set as 0.9 and 0.0005, respectively. The same training
setting is utilized for all our compared network variants.
We evaluate the testing time by averaging the running
time for images on the VOC 2012 validation set on
NVIDIA GeForce Titan GPU and Intel Core i7-4930K CPU
@3.40GHZ. Our PFN can rapidly process one 300 × 500
image in about one second. This compares much favorably
to other state-of-the-art approaches, as the current stateof-the-art methods rely on region proposal preprocessing and complex post-processing steps:
about 40 seconds while
 is excepted to be more
expensive than
 because more complex top-down category speciﬁc reasoning and shape prediction are further
employed for inference based on .
Results and Comparisons
In the ﬁrst training stage, we train a category-level segmentation network using the same architecture as “DeepLab-
CRF-LargeFOV” in . By evaluating the pixel-wise
segmentation in terms of pixel intersection-over-union
(IOU) averaged across 21 classes, we archive 67.53%
on category-level segmentation task on the PASCAL VOC
2012 validation set , which is only slightly inferior to
67.64% reported in .
Table 1 and Table 3 present the comparison of the
proposed PFN with two state-of-the-art methods 
using AP r metric at IoU score 0.5 and 0.6 to 0.9,
respectively. We directly use their published results on
PASCAL VOC 2012 validation set for fair comparison.
All results of the state-of-the-art methods were reported
in which re-evaluated the performance of using
VOC 2012 validation set. For fair comparison, we also
report the results of PFN using the Alexnet architecture 
as used in two baselines , i.e. “PFN Alexnet”.
Following the strategy presented in , we convert the
fully connected layers in Alexnet to fully convolutional
layers, and all other settings are the same as those used
in “PFN”. The results of and achieve 43.8% and
46.3% in AP r metric at IoU 0.5. Meanwhile, our “PFN
Alexnet” is signiﬁcantly superior over the two baselines,
i.e. 50.9% vs 43.8% and 46.3% in AP r metric.
Further detailed comparisons in AP r over 20 classes at
IoU scores 0.6 to 0.9 are listed in Table 3. By utilizing
the more powerful VGG-16 network architecture, our PFN
can substantially improve the performance and outperform
these two baselines by over 14.9% for SDS and 12.4%
for Chen et al. . PFN also gives huge boosts in AP r
metrics at 0.6 to 0.9 IoU scores, as reported in Table 3.
For example, when evaluating at 0.9 IoU score where
the localization accuracy for object instances is strictly
required, the two baselines achieve 0.9% for SDS and
 while PFN obtains 15.7%. This veriﬁes
the effectiveness of our PFN although it does not require
extra region proposal extractions as the pre-processing step.
The detailed AP r scores for each class are also listed. In
general, our method shows dramatically higher performance
than the baselines. Especially, in predicting small object
instances (e.g., bird and chair) or object instances with a
lot of occlusion (e.g., table and sofa), our method achieves
a larger gain, e.g. 74.2% vs 60.1% and 61.5% 
for bird, 64.4% vs 26.9% and 33.5% for sofa.
This demonstrates that our network can effectively deal
with the internal boundaries between the object instances
and robustly predict the instance-level masks with various
appearance patterns or occlusion. In Table 2, we also report
vol results of our different architecture variants,
which average all AP r at 0.1 to 0.9 IoU scores. We cannot
compare the AP r
vol with the baselines as they do not
publish these results.
Ablations Studies of Our networks
We further evaluate the effectiveness of our six important components of PFN, including the training strategy,
instance location prediction, network structure, instance
number prediction, testing strategy and upperbounds, respectively. The performance over all the categories by all
variants is reported in Table 1 and Table 2.
Training strategy: Note that our PFN training includes
two stages: the category-level segmentation network and the
instance-level network. To justify the necessity of using two
stages, we evaluate the performance of training a uniﬁed
network that consists of the category-level segmentation,
pixel-wise instance location prediction and instance number
prediction in one learning stage, namely “PFN uniﬁed”.
“PFN uniﬁed” is ﬁne-tuned based on the VGG-16 pretrained model and three losses for three sub-tasks are
optimized in one network. The category-level prediction is
appended in the last convolutional layer within the dashed
blue box in Figure 3, and the loss weight for category-level
segmentation is set as 1. From our experimental results,
“PFN uniﬁed” leads to 9.6% decrease in average AP r and
6.6% decrease in average AP r
vol, compared with “PFN”.
Intuitively, the target of category-level segmentation is to be
robust for individual object instances of the same category
while erasing the instance-level information during optimization. On the contrary, the instance-level network aims
to learn the instance-level information for distinguishing
different object instances with large variance in appearance,
view or scale. This comparison result veriﬁes well that
training with two sequential stages can lead to better global
instance-level segmentation.
Instance location prediction: Recall that PFN predicts
the spatial coordinates (6 dimensions) of the center, topleft corner and bottom-right corner of the bounding box
for each pixel. We also extensively evaluate other ﬁve
kinds of instance location predictions: 1) “PFN offsets (2)”,
which predicts the offsets of each pixel with respect to
the centers of its object instance; 2) “PFN centers (2)”,
where only the coordinates of its object instance center for
each pixel are predicted; 3) “PFN centers, w,h(4)”, which
predicts the centers, width and height of each instance; 4)
“PFN centers, topleft(4)”, where the centers and top-left
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
Comparison of instance-level segmentation performance with several architectural variants of our network and
two state-of-the-arts using AP r metric over 20 classes at 0.5 IoU on the PASCAL VOC 2012 validation set. All
numbers are in %.
58.8 0.5 60.1 34.4 29.5 60.6 40.0 73.6 6.5 52.4 31.7 62.0 49.1 45.6 47.9 22.6 43.5 26.9 66.2 66.1
Chen et al. 
63.6 0.3 61.5 43.9 33.8 67.3 46.9 74.4 8.6 52.3 31.3 63.5 48.8 47.9 48.3 26.3 40.1 33.5 66.7 67.8
Ours (Alexnet)
PFN Alexnet
63.0 15.3 69.8 48.4 23.5 60.2 24.1 82.2 13.9 60.7 41.3 73.5 76.9 69.7 37.6 20.0 41.4 58.8 78.9 58.4
Training Strategy
PFN uniﬁed
72.9 18.1 78.8 55.4 23.2 63.6 17.8 72.1 14.7 64.1 44.5 69.5 71.5 63.3 39.1 9.5 27.9 47.7 72.1 57.0
Location prediction
PFN offsets(2)
73.6 17.2 78.9 55.6 29.4 61.6 31.7 77.8 13.5 59.6 38.4 70.2 67.5 66.8 43.6 9.8 41.1 43.3 75.3 65.2
PFN centers(2)
78.0 15.5 76.4 58.7 25.6 69.3 28.9 88.2 16.6 67.2 47.8 82.3 78.0 71.5 47.0 23.8 48.8 63.8 83.3 72.0
PFN centers,w,h(4)
80.1 15.9 76.6 60.2 25.7 70.9 30.0 87.7 18.3 70.1 50.8 82.5 77.9 71.4 47.4 24.4 48.0 64.0 82.8 72.2
PFN centers,topleft(4)
80.5 15.9 79.2 62.5 27.8 69.4 31.1 86.6 18.8 73.6 50.6 81.6 77.2 71.5 47.5 22.5 47.7 63.6 83.0 72.5
PFN +topright, bottomleft(10) 76.9 15.1 73.9 55.8 26.0 73.7 31.1 92.1 17.6 74.0 48.1 82.0 85.5 71.7 48.8 25.2 57.7 64.5 88.7 72.0
Network structure
PFN w/o multiscale
72.8 16.5 71.9 50.3 25.2 65.9 27.4 90.4 16.3 64.6 48.1 78.9 74.1 72.0 42.4 21.3 46.2 64.4 82.8 72.3
PFN w/o coordinate maps
74.1 17.3 72.8 57.0 27.6 72.8 30.0 92.6 17.7 69.4 48.5 81.7 80.9 72.0 47.8 24.9 50.1 62.7 87.9 72.3
PFN fusing summation
80.6 16.5 78.8 59.4 27.2 71.0 29.9 85.1 18.1 75.4 52.8 80.9 76.5 74.1 47.7 20.8 46.7 66.8 86.0 71.7
Instance number
PFN w/o category-level
72.4 17.6 77.7 55.4 29.4 63.5 32.0 77.8 13.3 61.2 38.5 70.8 69.5 66.1 44.3 13.1 42.6 45.6 71.9 64.5
PFN w/o instance-level
74.0 15.6 72.9 55.5 26.4 72.2 31.3 91.1 19.1 66.9 49.9 82.3 75.0 73.4 47.7 24.2 53.3 64.4 91.0 72.3
PFN separate ﬁnetune
74.1 17.3 74.3 57.2 27.6 72.8 30.2 92.6 18.1 69.7 49.5 81.7 80.9 71.6 47.8 24.9 50.1 62.7 87.9 71.8
Testing strategy
PFN w/o coordinates
72.7 16.8 72.0 51.8 24.0 67.6 27.4 90.4 16.7 64.6 48.1 78.9 74.1 71.5 42.8 22.1 45.2 64.4 82.8 72.3
PFN w/o classify+size
76.2 16.1 72.9 57.9 25.3 69.5 29.4 88.3 15.8 67.1 48.1 82.0 73.8 71.8 46.1 22.3 47.8 62.7 83.7 72.3
PFN w/o size
72.0 17.3 72.8 57.0 27.6 72.8 30.8 92.6 17.7 64.6 48.5 81.7 80.9 73.3 48.8 24.9 50.1 62.7 87.9 72.3
PFN w/o classify
71.0 15.6 72.9 55.3 25.8 70.4 31.4 91.1 17.7 66.9 48.5 82.7 76.4 72.3 47.2 24.2 51.4 64.4 91.0 72.3
Ours (VGG 16)
76.4 15.6 74.2 54.1 26.3 73.8 31.4 92.1 17.4 73.7 48.1 82.2 81.7 72.0 48.4 23.7 57.7 64.4 88.9 72.3
Upperbound
PFN upperbound instnum
81.6 19.0 80.0 58.1 30.0 77.0 33.9 92.9 19.8 82.6 57.2 81.3 83.0 74.4 49.6 21.6 56.2 67.8 91.2 68.9
PFN upperbound instloc
81.8 23.6 84.6 66.4 38.2 75.3 35.3 94.9 24.8 84.2 61.7 83.9 87.2 75.2 55.6 27.3 63.9 69.3 88.9 72.5
corners of each instance are predicted; 5) “PFN +topright,
bottomleft (10)”, which additionally predicts the top-right
corners and bottom-left corners of each instance besides the
ones in “PFN”. The performance is obtained by changing
the channel number in the ﬁnal prediction layer accordingly.
The “PFN offsets (2)” gives dramatically inferior performance to “PFN” (51.0% vs 58.7% in AP r and 47.3% vs
52.3% in AP r
vol). The main reason may be that offsets are
possibly with negative values, which may be difﬁcult to optimize. By only predicting the centers of an object instance,
the resulting AP r of “PFN centers (2)” also shows inferior
performance to “PFN”. The reason for this inferiority may
be that predicting the top-left corners and bottom-right
corners can bring more information about object scales
and spatial layouts. “PFN centers, topleft(4)” and “PFN
centers, w,h(4)” theoretically predict the same information
about instance locations, and also achieve similar results in
AP r, 57.8% vs 58.2%, respectively. It is worth noting that
the 6 dimension predictions of “PFN” capture redundant
information compared to “PFN centers, topleft(4)” and
“PFN centers, w,h(4)”. The superiority of “PFN” over “PFN
centers, topleft(4)” and “PFN centers, w,h(4)” (0.5% and
0.9%, respectively) can be mainly attributed to the effectiveness of model combination. In our primary experiment,
the version that predicts the top-left and bottom-right corner
coordinates achieves unnoticeable performance with “PFN
centers, topleft(4)”, because the information captured by
centers and top-left corners are equal to that by top-left and
bottom-right corners. The combined features used for clustering with additional information can be equally regarded
as multiple model combination, which is widely used for
the object classiﬁcation challenge . Moreover, we test
the performance of introducing two additional points (topright corner and bottom-left corner) as the prediction. Only
a slight improvement (0.3%) by comparing “PFN +topright,
bottomleft (10)” with “PFN” can be observed yet more
parameters and computation memory are required. We thus
only adopt the setting of predicting the center, top-left and
bottom-right corners for all our other experiments.
Network Structure: Extensive evaluations on different
network structures are also performed. First, the effectiveness of multi-scale prediction is veriﬁed. “PFN w/o multiscale” shows the performance of using only one straightforward layer to predict pixel-wise instance locations. The
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
Comparison of instance-level segmentation performance with several architectural variants of our network using
vol metric over 20 classes that averages all AP r performance from 0.1 to 0.9 IoU scores on the PASCAL
VOC 2012 validation set. All numbers are in %.
Ours (Alexnet)
PFN Alexnet
62.2 20.0 64.6 41.0 23.8 56.6 22.4 76.0 15.5 56.2 39.2 68.6 65.4 61.2 35.2 20.2 37.6 52.1 68.9 49.9
Training Strategy
PFN uniﬁed
71.2 22.8 74.1 47.3 24.2 55.1 18.5 69.8 15.4 56.2 40.1 63.7 63.0 56.2 38.1 13.2 31.5 41.6 63.8 47.1
Location prediction
PFN offsets(2)
70.4 23.1 73.4 46.6 31.0 55.3 29.1 74.3 15.6 54.7 35.4 64.9 60.0 57.7 41.3 13.5 42.6 39.6 64.7 53.2
PFN centers(2)
72.5 21.0 69.6 50.0 25.0 63.3 27.2 79.2 17.1 60.8 44.3 74.9 67.8 64.3 41.0 24.3 42.5 55.6 72.8 58.0
PFN centers,w,h(4)
74.4 21.8 69.8 51.3 25.1 64.7 28.2 78.6 18.4 63.3 46.9 74.0 67.6 64.3 41.2 24.7 41.6 55.4 72.6 58.4
PFN centers,topleft(4)
74.3 21.8 72.8 53.0 27.3 63.4 29.0 77.7 18.6 65.7 46.8 73.3 67.1 65.0 41.5 23.2 40.7 54.7 72.8 58.7
PFN +topright, bottomleft(10) 71.3 20.8 66.4 48.5 26.6 65.2 27.2 83.1 17.3 64.6 45.1 74.6 70.8 64.3 41.6 23.4 48.8 56.4 76.1 57.9
Network structure
PFN w/o multiscale
69.6 21.3 66.7 44.1 25.8 58.4 25.3 81.9 17.4 60.6 44.5 72.8 64.7 62.4 39.5 21.6 41.6 57.7 74.2 58.7
PFN w/o coordinate maps
70.5 21.9 65.2 50.0 26.6 67.6 27.8 83.1 17.9 61.4 45.1 73.9 68.8 62.8 41.9 24.1 44.8 56.4 76.5 58.7
PFN fusing summation
74.3 22.7 72.4 50.3 27.0 64.6 27.7 76.2 17.7 66.4 48.6 72.7 66.6 67.1 41.7 21.7 39.8 57.0 75.3 57.8
Instance number
PFN w/o category-level
69.4 22.3 71.9 46.5 31.5 58.0 29.5 74.1 15.8 56.9 35.4 66.1 61.8 57.8 42.1 15.8 43.3 40.9 63.6 52.6
PFN w/o instance-level
69.4 21.1 65.1 48.6 25.6 64.3 27.4 81.9 18.7 61.1 46.0 75.2 67.0 64.0 41.0 22.4 47.6 57.6 77.7 58.8
PFN separate ﬁnetune
70.5 21.9 66.3 50.2 26.6 67.6 27.9 83.1 18.3 61.7 45.8 73.9 68.8 62.4 42.0 24.1 44.8 56.4 76.5 58.1
Testing strategy
PFN w/o coordinates
69.9 21.7 66.7 44.4 25.3 58.9 25.3 81.9 17.9 60.6 44.5 72.8 64.7 63.3 39.5 22.1 41.9 57.7 74.2 58.7
PFN w/o classify+size
70.7 21.3 66.6 49.4 24.6 63.4 27.7 79.3 16.3 60.8 44.5 74.6 64.7 64.6 40.2 22.7 41.7 54.9 73.2 58.4
PFN w/o size
68.8 22.0 65.3 50.0 26.6 67.0 28.5 83.1 17.9 59.3 45.1 74.3 68.8 64.6 42.9 23.7 44.9 56.4 77.5 58.4
PFN w/o classify
67.8 21.1 65.3 48.3 25.7 62.3 27.6 81.9 17.7 60.0 44.9 75.2 67.0 62.9 41.1 22.8 45.0 57.6 77.7 58.8
Ours (VGG 16)
70.8 21.1 66.7 47.6 26.7 65.3 27.5 83.2 17.2 64.5 45.1 74.7 67.9 64.5 41.3 22.1 48.8 56.5 76.2 58.2
Per-class instance-level segmentation results using AP r metric over 20 classes at 0.6 to 0.9 (with a step size of
0.1) IoU scores on the VOC PASCAL 2012 validation set. All numbers are in %.
52.8 19.5 25.7 53.2 33.1 58.1 3.7 43.8 29.8 43.5 30.7 29.3 31.8 17.5 31.4 21.2 57.7 62.7
Chen et al. 57.1 0.1 52.7 24.9 27.8 62.0 36.0 66.8 6.4 45.5 23.3 55.3 33.8 35.8 35.6 20.1 35.2 28.3 59.0 57.6
PFN Alexnet
60.3 9.9 67.4 31.8 18.7 52.9 18.6 75.6 8.1 54.6 36.0 71.3 63.3 65.6 29.3 14.8 31.2 48.5 66.9 47.3
73.2 11.0 70.9 41.3 22.2 66.7 26.0 83.4 10.7 65.0 42.4 78.0 69.2 72.0 38.0 19.0 46.0 51.8 77.9 61.4
32.5 7.2 19.2 47.7 22.8 42.3 1.7 18.9 16.9 20.6 14.4 12.0 15.7 5.0 23.7 15.2 40.5 51.4
Chen et al. 40.8 0.07 40.1 16.2 19.6 56.2 26.5 46.1 2.6 25.2 16.4 36.0 22.1 20.0 22.6 7.7 27.5 19.5 47.7 46.7
PFN Alexnet
56.1 5.0 59.8 25.6 12.7 50.4 15.5 69.3 3.2 42.9 24.5 63.6 58.4 54.4 21.1 7.9 26.2 39.9 59.1 37.0
68.5 5.6 60.4 34.8 14.9 61.4 19.2 78.6 4.2 51.1 28.2 69.6 60.7 60.5 26.5 9.8 35.1 43.9 71.2 45.6
4.5 11.5 32.3 9.0 17.9 0.7
7.9 10.2 12.7 24.3
Chen et al. 10.5
15.7 9.8 11.4 32.7 12.5 34.8 1.1 11.6 9.5 15.3 4.6
3.0 13.9 14.4 27.0 30.4
PFN Alexnet
46.6 1.5 48.0 15.0 8.6 44.3 10.7 58.6 2.0 37.4 15.5 47.8 40.0 34.9 13.5 3.8 20.2 23.4 51.7 30.7
54.6 1.5 49.5 21.0 10.4 50.7 14.2 63.5 2.1 38.3 18.9 51.8 41.2 36.7 16.5 4.2 26.2 25.3 59.6 36.9
Chen et al. 0.6
PFN Alexnet
37.1 0.1 24.6 7.0
3.6 30.4 4.9 40.0 0.6 23.3 2.8 28.9 13.6 7.8
1.1 10.9 12.2 23.8 8.1
43.9 0.1 24.5 7.8
4.1 32.5 6.3 42.0 0.6 25.7 3.2 31.8 13.4 8.1
1.6 14.8 14.3 25.0 8.5
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
performance decreases by 3.5% in AP r compared with
“PFN”. This signiﬁcant inferiority demonstrates the effectiveness of multi-scale fusing that incorporates the local ﬁne
details and global semantic information into predicting the
pixel-wise instance locations.
Note that the spatial coordinates of each pixel are utilized
as the feature maps for predicting the instance locations.
The superiority of using spatial coordinates can be demonstrated by comparing “PFN w/o coordinate maps” with
“PFN”, i.e. 0.7% difference in AP r. The coordinate maps
can help the feature maps be more precise for predicting
absolute spatial layouts of object instances, where the
convolutional ﬁlters can put more focus on learning the
relative spatial offsets.
In the fusing layer for predicting pixel-wise instance
locations, “PFN” utilizes the concatenation operation instead of element-wise summation for multi-scale prediction.
“PFN fusing summation” shows 0.4% decrease in AP r
when compared to “PFN”. The 1×1 convolutional ﬁlters are
utilized to adaptively weigh the contribution of the instance
location prediction of each scale, which is more reasonable
and experimentally effective than simple summation.
Instance Number Prediction: We explore other options
to predict the instance numbers of all categories for each
image. “PFN w/o category-level” only utilizes the instance
location predictions as the feature maps for predicting
instance numbers and the category-level information is
totally ignored. The large gap between “PFN w/o categorylevel” and “PFN” (51.4% vs 58.7%) veriﬁes the importance of using category-level information for predicting
instance numbers. Because the instance lcoation predictions
only capture the instance numbers of all categories and
category-level information is discarded, the exact instance
number for a speciﬁc category thus cannot be inferred.
The importance of incorporating instance-level information
is also veriﬁed by comparing “PFN w/o instance-level”
with “PFN”, 57.9% vs 58.7% in AP r. This shows that the
instance number prediction can beneﬁt from the pixel-wise
instance location prediction, where more ﬁne-grained annotations (pixel-wise instance-level locations) are provided
for learning better feature maps.
We also evaluate the performance of sequentially optimizing the instance locations and the instance number instead of using one uniﬁed network. “PFN separate ﬁnetune” ﬁrst optimizes the network for predicting
pixel-wise instance locations, and then ﬁxes the current
network parameters and only trains the newly added parameters for instance number prediction. The performance
decrease of “PFN separate ﬁnetune” compared to “PFN”
(58.1% vs 58.7% in AP r) shows well the effectiveness of
training one uniﬁed network. The information in the global
aspect from instance numbers can be utilized for predicting
more accurate instance locations.
Testing Strategy: We also test different strategies for
generating ﬁnal instance-level segmentations during testing.
Note that during spectral clustering, the similarity of two
pixels is computed by considering both the prediction
instance locations with 6 dimensions and two spatial coordinates of each pixel. By eliminating the coordinates
in the similarity function, a signiﬁcant decrease in AP r
can be observed by comparing “PFN w/o coordinates”
with “PFN”, 55.3% vs 58.7%. This veriﬁes that the spatial
coordinates can enhance the local neighboring connections
during clustering, which can lead to more reasonable and
meaningful instance-level segmentation results.
Recall that two steps are used for post-processing, including reﬁning the segmentation results with instance number
prediction and constraining the cluster size during clustering. We extensively evaluate the effectiveness of using these
two steps. By comparing 56.4% of “PFN w/o classify +
size” that eliminates these two steps with 58.7% of “PFN”
in AP r, it can be observed that better performance can
be obtained by leveraging the instance number prediction
and constraining the cluster size to reﬁne instance-level
segmentation. After only eliminating the reﬁning strategy
by constraining the cluster size, 0.9% decrease can be observed. It demonstrates that constraining the cluster size can
help reduce the effect of noisy background pixels to some
extent and more robust instance level segmentation results
can be obtained. On the other hand, the incorporation of instance number prediction can help improve the performance
in AP r by 1.3% when comparing “PFN w/o classify”
with “PFN”. In particular, signiﬁcant improvements can be
observed for easily confused categories such as 73.7% vs
66.9% for “cow”, 57.7% vs 51.4% for “sheep” and 81.7%
vs 76.4% for “horse”. This demonstrates the effectiveness
of using instance number prediction for reﬁning the pixelwise segmentation results.
Upperbound: Finally, we also evaluate the limitations
of our current algorithm. First, “PFN upperbound instnum”
reports the performance of using the ground-truth instance
number prediction for clustering and other experimental
settings are kept the same. It can be seen that only
2.6% improvement in AP r is obtained. The errors from
instance number prediction are already small and have
only little effect on the ﬁnal instance-level segmentation.
Second, the upperbound for instance location predictions
is reported in “PFN upperbound instloc” by using the
ground-truth instance locations for each pixel as the features
for clustering. The large gap between 64.7% of “PFN
upperbound instloc” and 58.7% of “PFN” veriﬁes that the
accurate instance location prediction is critical for good
instance-level segmentation. Note that the current categorylevel segmentation only achieves 67.53% of pixel-wise IoU
score, which largely limits the performance of our instancelevel segmentation because we perform the clustering on
the category-level segmentation. A better category-level
segmentation network architecture can deﬁnitely help improve the performance of instance-level segmentation under
our PFN framework.
Visual Illustration
Figure 6 visualizes the predicted instance-level segmentation results with our PFN. Note that we cannot visually
compare with two state-of-the-art methods because they
generate several region proposals for each instance and
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, X 20XX
Instance-level segmentation
Instance-level segmentation
Fig. 7. Illustration of failure cases. Our PFN may fail to
segment object instances with heavy occlusion (in ﬁrst
row) and small instances (in second row).
can only visualize top detection results for each image
as described in their papers. However, our method can
directly produce exact region segments for each object
instance just like the results of category-level segmentation.
Different colors indicate different object instances for the
instance-level segmentation results. The semantic labels of
our instance-level segmentation results are exactly the same
with the ones labeled in category-level segmentation results.
It can be observed that the proposed PFN performs well
in predicting the object instances with heavy occlusion,
large background clutters and complex scenes. For example,
the ﬁrst three rows show the results on images with very
complex background clutters, several objects with heavy
occlusion and diverse appearance patterns. The predicted
instance-level segmentations are highly consistent with the
ground-truth annotations, and the object instances with
heavy occlusion can also be visually distinguished, such as
the ﬁrst images in the ﬁrst and third rows. The fourth row
illustrates some images with very small object instances,
such as birds and potted-plants. The ﬁfth row shows examples where the object instances in one image have high
similarity in appearance and much occlusion with each
other. Other results show more examples of instance-level
images under diverse scenarios and with very challenging
poses, scales, views and occlusion. These visualization
results further demonstrate the effectiveness of the proposed
We also show some failure cases of our PFN in Table 7.
The instances with heavy occlusion and some small object
instances are difﬁcult to identify and segment. In future,
we can make more efforts to tackle these more challenging
instances.
CONCLUSION AND FUTURE WORK
In this paper, we present an effective proposal-free network
for ﬁne-grained instance-level segmentation. Instead of utilizing extra region proposal methods as the requisite, PFN
directly predicts the instance location vector for each pixel
that belongs to a speciﬁc instance and the instance numbers
of all categories. The pixels that predict the same or close
instance locations can be directly regarded as belonging
to the same object instance. During testing, the simple
spectral clustering is performed on the predicted pixelwise instance locations to generate the ﬁnal instance-level
segmentation results, and the predicted instance numbers of
all categories are employed to indicate the cluster number
for each category. Signiﬁcant improvements over the stateof-the-art methods are achieved by PFN on the PASCAL
VOC 2012 segmentation benchmark. Extensive evaluations
of different components of PFN are conducted to validate
the effectiveness of our method. Our PFN, without complicated per-processing and post-processing as requisite, is
much simpler to implement with much lower computational
cost compared with previous state-of-the-arts. In the future,
we plan to extend our framework to the generic multiple
instances in outdoor and indoor scenes, which may have
higher degrees of clutters and occlusion.