Adaptive Learning to Speed-Up Control of Prosthetic Hands: a Few
Things Everybody Should Know
Valentina Gregori1, Arjan Gijsberts1 and Barbara Caputo1
Abstract— A number of studies have proposed to use domain
adaptation to reduce the training efforts needed to control
an upper-limb prosthesis exploiting pre-trained models from
prior subjects. These studies generally reported impressive
reductions in the required number of training samples to
achieve a certain level of accuracy for intact subjects. We
further investigate two popular methods in this ﬁeld to verify
whether this result equally applies to amputees. Our ﬁndings
show instead that this improvement can largely be attributed
to a suboptimal hyperparameter conﬁguration. When hyperparameters are appropriately tuned, the standard approach that
does not exploit prior information performs on par with the
more complicated transfer learning algorithms. Additionally,
earlier studies erroneously assumed that the number of training
samples relates proportionally to the efforts required from the
subject. However, a repetition of a movement is the atomic unit
for subjects and the total number of repetitions should therefore
be used as reliable measure for training efforts. Also when
correcting for this mistake, we do not ﬁnd any performance
increase due to the use of prior models.
I. INTRODUCTION
A majority of upper-limb amputees is interested in prostheses controlled via surface electromyography (sEMG), but
they perceive the difﬁcult control as a great concern . Machine learning has opened a new path to tackle this problem
by allowing the prosthesis to adapt to the myoelectric signals
of a speciﬁc user. Although these methods have been applied
with success in an academic setting (e.g., and references
therein), they require a long and painful training procedure
to learn models with satisfactory performance.
Several studies have proposed to reduce the amount of
required training data by leveraging over previous models
from different subjects . The underlying idea is that
a model for a new target user can be bootstrapped from a
set of prior source models. Though the idea is appealing and
initial studies have shown remarkable improvements, there is
no conclusive evidence that these strategies lead to tangible
beneﬁts in the real world. An obvious limitation in the earlier
studies is that they only considered intact subjects. This is
relevant since myoelectric signals are user-dependent and
this holds in particular for amputees, as the amputation and
subsequent muscular use have a considerable impact on the
quality on the myoelectric signals .
Other limitations relate to the technical and conceptual
execution of the experimental validation. First, the hyperparameters of the algorithms were not optimized for the method
*This work was supported by the Swiss National Science Foundation
Sinergia project #160837 “Megane Pro”
1Department of Computer, Control, and Management Engineering,
University of Rome La Sapienza, via Ariosto 25, 00185 Roma, Italy
 
at hand, but rather chosen based on how well they performed
on average when applied on the data of other subjects.
Individual hyperparameter optimization for the methods and
number of training samples is crucial for the successful
application of machine learning algorithms and omission of
this procedure may skew results. For instance, this tuning
procedure may give an unfair disadvantage to the baseline
that does not use prior information from pre-trained users.
On the conceptual level, the previous studies used the
number of training samples (i.e., windows of the myoelectric signals) as measure for the required training effort. A
consequence of this strategy is that not all available training
samples were used to classify the movements, since subjects
cannot produce individual samples. Instead, a repetition of a
movement consisting of multiple windows is the atomic unit
for subjects. This artiﬁcial reduction of training information
is likely to be disadvantageous for the baseline that only
relies on target training data. In our evaluation, we instead
use the number of movement repetitions as realistic measure
of the training effort. Furthermore, we consider all possible data, subjects and combinations of training repetitions,
thereby removing the possible effects of random selection
from the evaluation.
In this paper, we provide more insight into the beneﬁts of
domain adaptation for prosthetic control by augmenting the
experiments by Patricia et al. with amputated subjects
while also addressing other limitations. This results in three
experimental settings, namely (1) the original experiments
according to the setup common in literature extended with amputated subjects, (2) the same setup with
hyperparameter optimization and ﬁnally (3) a realistic setup
where we also address the conceptual issues. In each setting,
we perform three experiments, where intact and amputated
subjects make up the groups of target and source subjects.
This paper is structured as follows. In Section II we
present the related work on domain adaptation in the context
of myoelectric prosthetics. The algorithms that will be considered in our experiments will then be explained in detail
in Section III. We continue with our experimental setup
in Section IV, after which we will present the results in
Section V. Finally, we conclude the paper in Section VI.
II. RELATED WORK
One of the ﬁrst attempts to classify myoelectric signals
of three volunteers was by Graupe and Cline . In the
following years, studies on prosthetic control led to many advances in the analysis and understanding of sEMG. Castellini
et al. noted that myoelectric signals differ signiﬁcantly
 
from person to person and that models trained for different
subjects are therefore not automatically reusable. However,
they showed that a pre-trained model could be used to
classify samples from similar subjects.
Several studies continued in this direction with different
strategies to build more robust models that take advantage
of past information from source subjects or, in the context
of repeatability, from the target itself. Matsubara et al. 
proposed to separate myoelectric data in user-dependent
and motion-dependent components, and to reuse models by
quickly learning just the user-dependent component for new
subjects. Sensinger et al. presented different methods
based on an appropriate concatenation of target and source
data. Others still approached the problem by searching for
a mapping to project data from different subjects into a
common domain ; a similar strategy was also used
to reduce the recalibration time for a target that attempts to
use the prosthesis on different days .
Other studies proposed to leverage over prior models
from already trained source subjects without requiring direct
access to their data . They tested different types
of so-called Hypothesis Transfer Learning (HTL) algorithms
showing a gain in performance with respect to non-adaptive
baselines. We are particularly interested in the ﬁndings of
Tommasi et al. , Patricia et al. , who worked with a signiﬁcant number of classes and intact subjects from the public
Non-Invasive Adaptive Prosthetics (NinaPro) database .
They report that the number of training samples required to
obtain a given level of performance can be reduced by an
order of magnitude as compared to learning from scratch.
III. ALGORITHMS
We ﬁrst describe the mathematical background by means
of a base learning algorithm in Section III-A, then we
proceed with the domain adaptation methods included in our
evaluations in Section III-B.
A. Background
Let us deﬁne a training dataset D = {xi, yi}N
input samples xi ∈X ⊆Rd and corresponding labels yi ∈
Y = {1, . . . , G}. In the context of myoelectric classiﬁcation,
the inputs are the myoelectric signals and the labels are
the movements chosen from a set of G possible classes.
The goal of a classiﬁcation algorithm is to ﬁnd a function
h(x) that, for any future input vector x, can determine the
corresponding output y. Among the algorithms that construct
such a model, Support Vector Machines (SVMs) are some
of the most popular.
The base of the domain adaptation algorithms described
later on is the Least-Squares Support Vector Machine
(LS-SVM) , a variant of SVM with a squared loss
and equality constraint. It writes the output hypothesis as
h(x) = ⟨w, φ(x)⟩+ b, where w and b are the parameters
of the separating hyperplane between positive and negative
samples. The optimal solution is thus given by
yi = ⟨w, φ(xi)⟩+ b + ξi,
∀i ∈{1, ..., N} ,
where C is a regularization parameter and ξ denotes the
prediction errors. We approach our multi-class classiﬁcation problem via a one-vs-all scheme to discriminate
each class from all others. To obtain a better solution we
mapped the input vectors xi into a higher dimensional
feature space using φ(xi). Usually, this mapping φ(·) is
unknown and we work directly with the kernel function
K(x′, x) = ⟨φ(x′), φ(x)⟩ . In the following, we use a
Radial Basis Function (RBF) kernel
K(x′, x) = e−γ∥x′−x∥2 with γ > 0 .
B. Adaptive Learning
Domain adaptation algorithms construct a classiﬁcation
model for a new target using past experience from the
sources. More speciﬁcally, let us assume that we have K
different sources, where each source is a classiﬁcation model
for the same set of movements. The used HTL algorithms
can then be described as follows.
1) Multi Model Knowledge Transfer: This method aims
to ﬁnd a new separating hyperplane w that is close to a linear
combination of the pre-trained source hypotheses ˆwk .
We solve the optimization problem
βk ˆwk∥2 + C
yi = ⟨w, φ(xi)⟩+ b + ξi .
The vector β = [β1, ..., βK]T with βk ≥0 and ∥β∥2 ≤1
represents the contribution of each source in the target
problem and is obtained by optimizing a convex upper bound
of the leave-one-out misclassiﬁcation loss . A more
general case consists of different weights for different classes
of the same source, such that βk,g is the weight associated to
class g of source k. In this work we used this latter version
of the algorithm.
2) Multi Kernel Adaptive Learning: This algorithm combines source and target information via a linear combination
of kernels . Let us deﬁne
¯w = [w0, w1, ..., wK]
¯φ(x, y) = [φ0(x, y), φ1(x, y), ..., φK(x, y)] ,
respectively as the concatenation of the target and source
hyperplanes and the mapping functions into the corresponding feature spaces. These are both composed of (K + 1)
elements: the ﬁrst refers to the target and the remaining ones
to the sources. The optimization problem becomes
⟨¯w, (¯φ(xi, yi) −¯φ(xi, y))⟩≥1 −ξi, ∀i y ̸= yi .
The element p regulates the level of sparsity in the solution ¯w
and can vary in the range (1, 2]. The solution is obtained via
stochastic gradient descent during T epochs over the shufﬂed
training samples.
IV. EXPERIMENTAL SETUP
The experimental evaluation is subdivided in three settings. The ﬁrst one is modeled after related literature for this
kind of experiments with sEMG data . The second is
identical but adds hyperparameter optimization for the target
models. The third and ﬁnal one is a novel framework in
which we ﬁxed the shortcomings of the previous settings to
make the experiments as realistic as possible. We will refer
to the settings as original, optimized and realistic. In the
following, we ﬁrst explain the used data and classiﬁers, and
subsequently elaborate on the experimental settings.
The data used in our work are from the NinaPro
database1 , the largest publicly available database for
prosthetic movement classiﬁcation with 40 intact subjects
and 11 amputees. Each subject executed 40 movements
for 6 times, such that each repetition was alternated with
a rest posture. While performing the movements, twelve
electrodes acquired sEMG data from the arm of the subject.
The standardized data were used according to the control
scheme by Englehart and Hudgins , where we extracted
features from a sliding window of 200 ms and an increment
of 10 ms. The resulting set of windows was subsequently
split in train and test sets for the classiﬁer; data from
repetitions {1, 3, 4, 6} were dedicated to training while data
from repetitions 2 and 5 were used as test. To reduce the
computational requirements, we subsampled the training data
by a factor of 10 at regular intervals.
B. Classiﬁers
The algorithms used to build the classiﬁcation models
were the two mentioned HTL algorithms together with two
baselines:
• the no transfer model (NoTransfer), which uses an
LS-SVM with RBF kernel trained only on the target
data. This corresponds to learning without the help of
prior knowledge.
• the prior model (Prior), which learns an LS-SVM with
linear kernel on top of the raw predictions of the source
models. This measures the relevance of the source
hypotheses by using them as feature extractors for the
target data.
• Multi Model Knowledge Transfer (MultiKT), as explained in Section III-B.1, which learns a model on the
target data that is close to a weighted combination of
the source hypotheses.
• Multi Kernel Adaptive Learning (MKAL), as explained
in Section III-B.2, which linearly combines an RBF
kernel on the target data with the source predictions.
Parameters p and T were set to 1.04 and 300.
1 
The classiﬁcation models for the sources were based on a
non-linear SVM or LS-SVM with RBF kernel.
C. Settings
For each of the settings, we ran three experiments with
distinct groups of target and source subjects:
• Intact-Intact: intact target subjects exploit prior knowledge of other intact sources;
• Amputees-Amputees: amputated target subjects exploit
previous experience of other amputees;
• Amputees-Intact: amputated target subjects exploit prior
knowledge of intact subjects.
In the ﬁrst and second experiment, each subject toke the role
of target just once, while the remaining subjects were used
as sources. In the third, all of the amputees were once the
target and the set of intact subjects was used only as sources.
1) Original Setting: The purpose of the original and
optimized settings is to investigate the isolated impact of
hyperparameter optimization on the performance of the target
classiﬁers. We therefore replicated, as closely as possible,
the experiments from Patricia et al. with 9 amputees2.
and a random subset of 20 intact subjects from the NinaPro
database. For these subjects we considered 17 movements
plus the rest posture, appropriately subsampled to balance
it with the other movements. The sEMG representation
used in this setting was the average of Mean Absolute
Value (MAV), Variance (VAR) and Waveform Length (WL)
features , as to reduce the dependency on one speciﬁc
type of representation. The details of this and the subsequent
settings are presented schematically in Table I.
The source models were created by training an SVM with
RBF kernel using all training repetitions of the respective
subject. For the target models we trained the classiﬁers on
an increasing number of random samples, from 120 to 2160
in steps of 120, from the training repetitions. The hyperparameters for both the source and target models were chosen
from C, γ ∈{0.01, 0.1, 1, 10, 100, 1000} and kept constant
regardless of the number of training samples. For each
parameter conﬁguration, we evaluated the average balanced
classiﬁcation accuracy of each source subject when tested
on the target subjects. For the target subject and its source
models, we then chose the conﬁguration that maximizes this
average, making sure to exclude the data from the target
subject. The motivation for this procedure is that biased
regularization in MultiKT requires the source and target
models to “live” in the same space.
2) Optimized Setting: Strictly speaking, the above assumption only requires that all the sources have the same
RBF bandwidth γ as the related target. Moreover, MultiKT
can also be interpreted as predicting the difference between
the source predictions and the true labels . In this
alternative interpretation, there is no need for source and
target models to use the same kernel. We therefore tuned the
hyperparameters in the optimized setting for each individual
2We omitted two amputees from the database that had only 10 electrodes
due to insufﬁcient space on their stump.
EXPERIMENTAL SETTINGS.
# Int./Amp.
Source model
Source hyperparameters
Target hyperparameters
9 wrist, 8 ﬁnger,
subsampled rest
avg. MAV/VAR/WL
other subjects
same as source
9 wrist, 8 ﬁnger,
subsampled rest
avg. MAV/VAR/WL
other subjects
balanced accuracy of 5fold
training samples of target
9 wrist, 8 ﬁnger,
over repetitions of source
over train repetitions of
target and for each training set size based on 5-fold cross
validation (CV) on the target training set. Note, however, that
we still use the original method to determine the parameters
for the source models.
3) Realistic Setting: In the ﬁnal setting we extended
the hyperparameter optimization procedure and attempted to
address all issues to make the experiments as realistic as
possible. First, we considered all available movements and
subjects in the NinaPro database3. As sEMG representation
we used marginal Discrete Wavelet Transform (mDWT)
features, which have previously shown excellent performance
in related work on this database .
The main conceptual innovation with respect to the previous settings is that we trained target models on an increasing
number of repetitions. The motivation is that the effort of
the subject during data acquisition is given by the required
number of repetitions of each movements, so we analyze
the accuracy as a function of this atomic unit. Given the
set of training repetitions {1, 3, 4, 6}, we considered all
possible subsets of length between 1 and 4 repetitions. For
all these cases, we optimized the target model using kfold CV, where each fold corresponded to samples belonging to one repetition. In the exceptional case of only a
single training repetition, we instead used 5-fold CV over
the samples. The parameter grid was extended to C ∈
{2−6, 2−4, . . . , 212, 214} and γ ∈{2−20, 2−18, . . . , 2−2, 20}.
Models for the source subjects on the other hand were trained
using all repetitions and the hyperparameters were optimized
speciﬁcally for the individual subject using 6-fold CV, where
the folds again corresponded to the repetitions. The source
models were built with LS-SVM instead of SVM to be more
coherent with the other classiﬁers, which are all derived from
LS-SVM. Due to the much larger number of samples in this
realistic setting4 we further subsampled the data used for
hyperparameter optimization by a factor of 4. For the same
reason we decided to omit MKAL from the analysis.
V. EXPERIMENTS
In this section we ﬁrst investigate the isolated impact of
hyperparameter optimization when applied to the original
setting. Then we verify whether the ﬁndings also apply to
3Data for the ﬁrst amputated subject was omitted, since the acquisition
was interrupted prematurely.
4Each repetition consists of approximately 35000 samples.
the realistic setting described in Section IV-C.3. An in-depth
discussion follows on the explanations of the results.
A. Results
In Figure 1 we report the balanced classiﬁcation accuracy as a function of the number of training samples
averaged over all target subjects. The dotted lines indicate
the results obtained in the original experimental framework
usually employed in literature (see Section IV-C.1). As in
the related studies, MKAL and MultiKT outperform the
baselines NoTransfer and Prior by a signiﬁcant margin for all
training set sizes. This has led to the claim that the adaptive
algorithms can achieve similar performance as NoTransfer
using an order of magnitude less training samples. Since
this improvement is observed whether the target and source
subjects are intact or amputated, it is also assumed that
amputees can equally exploit prior information from intact
as well as other amputated subjects.
When looking at the solid lines in Figure 1, which
show results with hyperparameter optimization, we observe
that the discrepancies between the algorithms disappear.
In other words, the NoTransfer baseline performs just as
well as or even slightly better than the adaptive algorithms.
Furthermore, with hyperparameter optimization all methods
now outperform the results in the original setting. The only
exception to this observation is Prior, which has lower
accuracy in the Amputee-Intact experiment. Contrary to the
earlier statements, this demonstrates that prior models from
intact subjects are not as useful as those from other amputees.
Together with the observation that MKAL and MultiKT
perform nearly identically to NoTransfer, this also allows us
to conclude that rather than transferring from prior models,
the HTL algorithms rely almost exclusively on target data.
Figure 2 shows the standard classiﬁcation accuracy for
the realistic setting described in Section IV-C.3 averaged
over the target subjects and all possible combinations of
a given number of training repetitions. Also in this setting
the hyperparameters were tuned appropriately and the differences among the methods are again negligible. In addition,
we observe signiﬁcantly lower accuracy among amputees
compared to intact subjects, conﬁrming the deterioration of
the myoelectric signals due to amputation and subsequent
lack of muscular use.
NoTransfer
Original Setting
Optimized Setting
Balanced Accuracy
(a) Intact-Intact
(b) Amputee-Amputee
(c) Amputee-Intact
Balanced classiﬁcation accuracy for MultiKT, MKAL, NoTransfer and Prior in the original and hyperparameter optimized settings.
NoTransfer
# Repetitions
(a) Intact-Intact
# Repetitions
(b) Amputee-Amputee
# Repetitions
(c) Amputee-Intact
Standard classiﬁcation accuracy for MultiKT, NoTransfer and Prior in the realistic setting.
B. Discussion
The results clearly show that the improvements usually attributed to prior knowledge can instead be explained by suboptimal hyperparameter optimization. With properly tuned
hyperparameters, the NoTransfer baseline that completely
ignores source information performs as well as the more
complicated domain adaptation methods.
There are multiple explanations for the observed differences in performance in the original setting. First, the
hyperparameters were chosen based on the performance of
an SVM when transferring from the source subjects to the
target subjects. This gives a disadvantage to NoTransfer,
which does not exploit prior knowledge to train the classiﬁer.
Furthermore, this parameter setting is also problematic since
all methods are based on LS-SVM, which uses a different
loss function than SVM. As can be seen in the objective
function in Equation 1, the regularization parameter C is
multiplied with the absolute magnitude of all training losses,
so an optimal setting for SVM does not necessarily work
well for LS-SVM.
A further problem is that the value of C was determined
using the total set of training samples. The same value was
subsequently used when training on much smaller subsets,
leading to a different tradeoff between minimizing training
errors and regularizing the solution. This affects all methods
except MKAL, for which the speciﬁc implementation multiplied the given value of C with the number of training
samples. MKAL therefore effectively used a much larger
value of C (i.e, much less regularization), explaining why
it performed superior to the other methods.
A similar, though slightly more complicated, argument
holds for MultiKT. Recall the formulation of biased regularization in Equation 3; the linear combination of source
hypotheses allows to reduce the effect of the regularization
term by moving the bias in the direction of the optimal
solution. In other words, for the same value of C this allows
to concentrate more on minimizing the training errors on the
target data.
VI. CONCLUSIONS
In this paper, we have tested two popular domain adaptation algorithms that were proposed to reduce the training
time needed to control a prosthesis. We found that the
improvements in earlier studies can in fact be attributed
to suboptimal hyperparameter optimization, which penalized
in particular the NoTransfer reference method. When the
hyperparameters are appropriately tuned on the training data
of the target subject, the previously reported differences
This result also holds when correcting for other technical
and conceptual mistakes in the original experimental framework. The accuracy of the classiﬁcation methods in our
updated setting was evaluated with respect to the number
of repetitions of each movement, which represents the real
effort for the user during the training phase, and for all
subjects in the NinaPro database. Also in this case, we do
not observe any differences between the HTL algorithms and
the NoTransfer baseline.
Intuitively, it should be possible to improve performance
on a speciﬁc task by using prior information from related
tasks. Our ﬁndings show, however, that in the context of
prosthetic control MultiKT and MKAL, which transfer just
source hypotheses rather than source data, do not lead to
improved performance. In future work, we will therefore
continue to investigate how to successfully leverage over
prior information to reduce the training effort for an amputee.
Among the directions we consider are unsupervised domain
adaptation via distribution alignment and subject
invariant data representations using deep learning methods.