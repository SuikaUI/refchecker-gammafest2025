Findings of the Association for Computational Linguistics: EMNLP 2020, pages 172–182
November 16 - 20, 2020. c⃝2020 Association for Computational Linguistics
Few-shot Natural Language Generation for Task-Oriented Dialog
Baolin Peng, Chenguang Zhu, Chunyuan Li
Xiujun Li, Jinchao Li, Michael Zeng, Jianfeng Gao
Microsoft Research, Redmond
{bapeng,chezhu,chunyl,xiul,jincli,nzeng,jfgao}@microsoft.com
As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in
natural language. The success of traditional
template-based or statistical models typically
relies on heavily annotated data, which is infeasible for new domains.
Therefore, it is
pivotal for an NLG system to generalize well
with limited labelled data in real applications.
To this end, we present FEWSHOTWOZ, the
ﬁrst NLG benchmark to simulate the few-shot
learning setting in task-oriented dialog systems. Further, we develop the SC-GPT1 model.
It is pre-trained on a large set of annotated
NLG corpus to acquire the controllable generation ability, and ﬁne-tuned with only a few
domain-speciﬁc labels to adapt to new domains. Experiments on FEWSHOTWOZ and
the large Multi-Domain-WOZ datasets show
that the proposed SC-GPT signiﬁcantly outperforms existing methods, measured by various
automatic metrics and human evaluations.
Introduction
Task-oriented dialog systems are becoming increasingly popular, as they can assist users in various
daily activities such as ticket booking and restaurant reservations. In a typical task-oriented dialog
system, the Natural Language Generation (NLG)
module plays a crucial role: it converts a system
action (e.g., often speciﬁed in a semantic form selected by a dialog policy) into a ﬁnal response in
natural language. Hence, the response should be
adequate to represent semantic dialog actions, and
ﬂuent to engage users’ attention. As the ultimate
interface to interacts with users, NLG plays a signiﬁcant impact on the users’ experience.
Existing methods for NLG can be broadly summarized into two major categories. (i) Template-
1Semantically-Conditioned Generative Pre-Training
based methods require domain experts to handcraft
templates for each domain, and the system ﬁlls in
slot-values afterward . Thus, the produced
responses are often adequate to contain the required
semantic information, but not always ﬂuent and nature, hurting users’ experiences. (ii) Statistical language models such as neural networks learn to generate ﬂuent responses via training from labelled corpus. One canonical model is
semantically conditioned LSTM (SC-LSTM) , which encodes dialog acts with onehot representations and uses it as an extra feature to
inform the sentence generation process. Despite its
good performance on simple domains, it requires
large amounts of domain-speciﬁc annotated data
which is not available for many domains in realworld applications. Even worse, this renders severe
scalability issues when the number of possible combinations of dialog acts grows exponentially with
the number of slots in more complex domains.
We revisit the current research benchmarks for
NLG, and notice that each dialog domain is extensively labelled to favor model training. However,
this is in contrast to the real-world application scenarios, where only very limited amounts of labelled
data are available for new domains. To simulate
such a few-shot learning setting, we have developed a new benchmark dataset, called FEWSHOT-
WOZ, based on the MultiWOZ and Cambridge NLG datasets . FEWSHOTWOZ consists of dialog
utterances from 7 domains. For each domain, we
provide less than 50 labeled utterances for ﬁnetuning. We believe that FEWSHOTWOZ can better
inspire research to address the challenge of learning data-hungry statistical models with very limited
amounts of labelled data in real-world scenarios.
To deal with the challenge of few-shot learning,
we develop the SC-GPT model. SC-GPT is a multi-
Natural Language
Understanding (NLU)
Dialog State
Natural Language
Generation (NLG)
Dialog Policy
Task Oriented Dialog System
Dialog Act
Intent: Confirm
Slot-value pairs:
[ name = Hilton ], [ area = center ]
Let me confirm that
you are searching for Hinton hotel
in the center area
(a) The overall framework of a task-oriented dialog system
(b) Dialog act & Response
Figure 1: Illustration of the NLG module in the overall task-oriented dialog system. (a) The NLG module is
highlighted with glowing black bounding boxes. (b) One example of dialog act (including intent and slot-value
pairs) and its corresponding natural language response.
layer Transformer neural language model, trained
in three steps: (i) Pre-trained on plain text, similar
to GPT-2 (Radford et al.); (ii) Continuously pretrained on large amounts of dialog-act labeled utterances corpora to acquire the ability of controllable
generation; (iii) Fine-tuned for a target domain using very limited amounts of domain labels. Unlike
GPT-2, SC-GPT generates semantically controlled
responses that are conditioned on the given semantic form, similar to SC-LSTM but requiring much
less domain labels to generalize to new domains.
In summary, our key contributions are three-fold:
• A new benchmark FEWSHOTWOZ is introduced to simulate the few-shot adaptation setting where only a handful of training data
from each domain is available.
• We propose a new model SC-GPT. To our
best knowledge, this work is the ﬁrst study
of exploiting state-of-the-art pre-trained language models for NLG in task-oriented dialog
• On the MultiWOZ dataset, SC-GPT creates
a new SOTA, outperforming previous models by 4 points in BLEU. On FEWSHOT-
WOZ, SC-GPT outperforms several strong
baselines such as SC-LSTM and HDSA , showing that SC-GPT adapts to
new domain much more effectively, requiring
much smaller amounts of in-domain labels.
Background
A typical task-oriented spoken dialog system uses
a pipeline architecture, as shown in Figure 1 (a),
where each dialog turn is processed using a fourstep procedure. (i) Transcriptions of user’s input
are ﬁrst passed to the natural language understanding (NLU) module, where the user’s intention and
other key information are extracted. (ii) This information is then formatted as the input to dialog state
tracking (DST), which maintains the current state
of the dialog. (iii) Outputs of DST are passed to
the dialog policy module, which produces a dialog
act based on the facts or entities retrieved from external resources (such as a database or a knowledge
base). (iv) The dialog act emitted by the dialog policy module serves as the input to the NLG, through
which a system response in natural language is generated. In this paper, we focus on the NLG component of task-oriented dialog systems, i.e., how to
produce natural language responses conditioned on
dialog acts.
Speciﬁcally, dialog act A is deﬁned as the combination of intent I and slot-value pairs {(si, vi)}P
, (s1, v1), · · · , (sP , vP )
Slot-value pairs
where P is the number of pairs2, which varies in
different dialog acts.
• Intents are usually used to distinguish different types of system actions. Typical examples
include inform, request, conﬁrm, select etc.
• Slot-value pairs indicate the category and content of the information to express in the utterance, respectively.
The goal of NLG is to translate A into a
natural language response x
[x1, · · · , xT ],
where T is the sequence length.
In Figure 1
(b), we show an example of the dialog act:
confirm (name=Hilton, area=center), and the
2In some literature, dialog act denotes only the type of
system actions, slot-value pairs are deﬁned as meaning representations. Throughout this paper, we follow the usage in
Budzianowski et al. and use dialog acts to indicate
system action and associated slot-value pairs.
Confirm ( name = Hinton , area = center ) [BOS] Let me confirm that you are searching for Hinton hotel in the center area [EOS]
System Response
Dialog Act
[BOS] Let me confirm that you are searching for Hinton hotel in the center area [EOS]
Figure 2: Illustration of SC-GPT. In this example, SC-GPT generates a new word token (e.g., “confirm” or
“center”) by attending the entire dialog act and word tokens on the left within the response.
corresponding natural language response is “Let
me conﬁrm that you are searching for Hilton in the
center area”.
Semantically Conditioned GPT
We tackle this generation problem using conditional neural language models. Given training data
of N samples D = {(An, xn)}N
n=1, our goal is to
build a statistical model parameterized by θ to characterize pθ(x|A). To leverage the sequential structure of response, one may further decompose the
joint probability of x using the chain rule, casting
an auto-regressive generation process as follows:
pθ(xt|x<t, A)
where x<t indicates all tokens before t.
Learning θ is performed via maximizing the loglikelihood (MLE) of the conditional probabilities
in (2) over the entire training dataset:
log pθ(xt,n|x<t,n, An)
In this paper, we employ the Transformers
 to parameterize the conditionals in (2). To enable strong generalization and
controllable ability for the learned model, we propose the following three-stage procedure as the
training recipe.
Massive Plain Language Pre-training.
models trained on massive training corpus usually generalize better to new domains. Inspired
by this, we inherit the GPT-2 architecture (Radford
et al.) as the backbone language model. GPT-2 is
an auto-regressive language model that leverages
12-24 layers of masked, multi-head self-attention
Transformers. GPT-2 is pre-trained on extremely
massive text data OpenWebText (Radford et al.).
It has demonstrated superior performance on characterizing human language data distribution and
knowledge transfer. Given text prompts, GPT-2
can often generate realistic sentences.
Dialog-Act Controlled Pre-training.
the guidance of dialog act in response generation,
we propose to continuously pre-train the GPT-2
model on large amounts of annotated (dialog act,
response) pairs. The pre-training dataset3 includes
annotated training pairs from Schema-Guided Dialog corpus, MultiWOZ corpus, Frame corpus, and
Facebook Multilingual Dialog Corpus. The total
size of the pre-training corpus is around 400k examples.
We ﬁrstly pre-process dialog act A into a sequence of control codes using the following format:
A′ = [ I ( s1
= v1 , · · · sP = vP ) ] (4)
Meanwhile,
the output sequence x′ is preprocessed via appending x with a special start token [BOS] and an end token [EOS]. Finally, the
sequentialized dialog act A′ is concatenated with
its augmented response x′, and then fed into GPT-2.
During training, the prediction loss is only computed for x′, and A′ provides the attended conditions. Since the dialog act represents the semantics
of the generated sentences, we follow the naming
convention of SC-LSTM, and term our model as
Semantically Conditioned Generative Pre-training
(SC-GPT). The overall architecture of SC-GPT is
illustrated in Figure 2.
Fine-tuning.
For a new domain, a dialog act usually contains novel intents or slot-value pairs, and
annotated training samples are often limited. We
3The domains appearing in ﬁne-tuning are excluded.
Statistics
FEWSHOTWOZ
Avg. # Intents
Avg. # Slots
Avg. # Delexicalised DAs in Training
Avg. # Delexicalised DAs in Testing
Overlap Percentage
Avg. # Training Instances
Avg. # Testing Instances
Table 1: Comparison of existing NLG datasets, including E2E NLG , BAGEL , Cambridge NLG and the proposed FEWSHOTWOZ.
Statistics
Restaurant
Attraction
# DAs in training
# DAs in testing
Overlap Percentage
Avg. #DAs per Instance
# Training Instances
# Testing Instances
Table 2: FEWSHOTWOZ statistics over 7 different domains.
ﬁne-tune SC-GPT on limited amounts of domainspeciﬁc labels for adaptation. The ﬁne-tuning follows the same procedure of dialog-act controlled
pre-training, as described above, but uses only a
few dozens of domain labels.
It is worth noticing that the above recipe has
several favorable properties:
• Flexibility. SC-GPT operates on a sequence of
tokens without delexicalization, which means
that SC-GPT does not assume a ﬁxed onehot or tree-structured dialog act representation vectors. Hence, it has great ﬂexibility in
extending to novel dialog acts.
• Controllability. In contrast to GPT-2 that generates natural sentences without high-level semantic guidance, SC-GPT can generate sentences with adequate intent and slot-value information and maintain its ﬂuency.
• Generalizability. SC-GPT is able to generalize signiﬁcantly better than SC-LSTM, due to
the pre-training on massive plain text corpora
and annotated dialog datasets.
Dataset: FEWSHOTWOZ
Revisiting NLG Benchmarks.
The three commonly used NLG datasets in developing and
evaluating task-oriented dialog systems are E2E
NLG BAGEL and RNNLG , as
summarized in Table 1. We observe two issues
from their shared statistics: (i) All the datasets contain a large number of labelled training samples
for each domain, ranging from hundreds to tens of
thousands. However, the cost of labeling is high in
practice, e.g., labeling 50 utterances is 5 hours per
domain. Creating such an extensively annotated
dataset for each new domain is prohibitively expensive. (ii) The percentage of distinct delexicalised
dialog acts between training and testing data is
quite small. For example, the delexicalised dialog
acts in testing is 100% covered by the training set
for the E2E NLG dataset. It renders difﬁculties
in evaluating the model’s generalization ability for
new domains.
FEWSHOTWOZ.
To build a setting for more
pragmatic NLG scenarios, we introduce a new
dataset FEWSHOTWOZ to better reﬂect real application complexity, and encourage the community
to develop algorithms that are capable of generalizing with only a few domain-speciﬁc labels for each
(new) domain. The dataset statistics are shown
in the last column of Table 1. We see that FEW-
SHOTWOZ is different from the other datasets in
three aspects: (i) More domains. FEWSHOTWOZ
contains seven domains in total, which is larger
than any existing NLG datasets. (ii) Less training instances. Importantly, FEWSHOTWOZ has
a much smaller number of training instances per
domain, aiming to evaluate the few-shot learning
ability. (iii) Lower training/testing overlap. FEW-
SHOTWOZ has only 8.82% overlap, signiﬁcantly
smaller than the other datasets, which amount to
more than 90% overlap. The average number of
intents per instance in Attraction/ Taxi/ Train
domain is 2, 1.33, and 2.05, respectively. In contrast, there is only one intent for each example
in the other datasets. The NLG task deﬁned on
FEWSHOTWOZ requires the models to learn to
generalize over new compositions of intents. The
details of FEWSHOTWOZ is shown in Table 2.
Collection Protocols.
We construct FEWSHOT-
WOZ via re-organizing data samples from RNNLG
and MultiWOZ datasets . For each domain in RNNLG, we ﬁrst group
utterances according to their delexicalised dialog
acts, and keep only one utterance as the target sentence. To ensure diversity, we consider three domains from MultiWOZ: Attraction, Taxi, and
Train. Since MultiWOZ is a cross-domain dataset,
the dialog act of an utterance may exist in multiple domains. We choose to keep utterances whose
dialog act appears only in one domain. Similar
delexicalising processing is applied to ensure that
each dialog act has only one target utterance. Finally, to simulate the few-shot learning in practice,
we randomly sample 50 training examples for each
domain, except the Taxi domain, which has 40
Related Work
Pre-trained Models.
Pre-trained language models (PLMs) have substantially advanced the stateof-the-art across a variety of natural language processing (NLP) tasks . PLMs are often trained to predict words
based on their context on massive text data, and the
learned models can be ﬁne-tuned to adapt to various downstream tasks. The closest line of research
to ours are GPT-2 (Radford et al.), CTRL and Grover . GPT-
2 ﬁrst investigated missive Transformer-based autoregressive language models with large-scale text
data for pre-training. After ﬁne-tuning, GPT-2
achieves drastic improvements on several generation tasks. One drawback of GPT-2 is the lack of
high-level semantic controlling ability in language
generation. To alleviate this issue, CTRL was introduced to train the model
based on pre-deﬁned codes such as text style, content description, and task-speciﬁc behavior, meanwhile Grover was proposed
to generate news articles conditioned on authors,
dates etc. Although conceptually similar to our
SC-GPT, CTRL and Grover cannot be readily applied to NLG in task-oriented dialog systems, as
the conditioning codes are quite different. Another controllable generation work for GPT-2 is
PPLM , which provides a
decoding scheme to guide the generation process
using key-words or classiﬁers, without re-training
the model. In this paper, we focus on pre-training
an NLG model conditioned on ﬁner-grained semantic dialog acts, which are more desirable for dialog
Various dialog systems have been developed , including taskoriented dialog systems such as Rasa4, Microsoft
Bot Framework5, and Conversational Learner6, and
chit-chat systems such as XiaoIce (Zhou et al.),
DialoGPT , Meena .
In this paper, we focus on
task-oriented systems, particularly the NLG module. With the blooming of deep learning, neural
sequential models have shown powerful capability
and ﬂexibility in NLG. Extensive efforts have been
made, including new architecture choices such as
RNNs , attention RNNs , SC-LSTM 
and its variants , as well as learning objectives . However, they
all require large amounts of annotated data to reach
satisfactory performance. A more realistic scenario
is to require much less labeling and improve the
sample efﬁciency of models, This is especially important when deploying the models to new domains,
where dialog acts need to be labelled from scratch.
Our paper aims to formally set up such a research
scenario by proposing a new dataset FEWSHOT-
WOZ, and a new model SC-GPT.
4 
5 
6 
Restaurant
Attraction
BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR #
Table 3: Performance of different methods on FEWSHOTWOZ
Informativeness
Naturalness
* p < 0.005, comparison with SC-LSTM
† p < 0.05, comparison with GPT
Table 4: Human evaluation on FEWSHOTWOZ. Statistical signiﬁcance is computed with a two-tailed t-test.
Experiments
In this section, we evaluate the proposed SC-GPT
on the FEWSHOTWOZ and MultiWOZ datasets
to answer two research questions: (i) Is SC-GPT
an effective model for strong generalization and
controllability in dialog response generation? (ii)
Does FEWSHOTWOZ meet the goal of effectively
evaluating the generalization of NLG models in the
few-shot learning setting?
Experimental Setup
Implementation details.
The model was built
upon Huggingface Pytorch Transformer . We use GPT2-Medium with 345M
parameters7 as the initial checkpoint, and byte pair
encodings for the tokenization. Linear rate scheduler with start rate as 5e-
5 was used for both pre-training and ﬁne-tuning.
Adam with weight decay
was used to optimize the parameters.
For pretraining, the model was trained with a mini-batch
of 8 on an 8 Nvidia V100 machine until observing
no signiﬁcant progress on validation loss or up to
20 epochs, whichever is earlier. For ﬁne-tuning
on FEWSHOTWOZ, models were trained on each
domain separately with ﬁve epochs.
 , BLEU scores and the slot error rate (ERR)
are reported. BLEU score evaluates how natu-
7We also experimented using GPT2 with 117M parameters
but observed signiﬁcant poor performance.
ral the generated utterance is compared with human readers.
ERR measures the exact matching of the slot tokens in the candidate utterances.
ERR = (p + q)/M, where M is the total number
of slots in the dialog act, and p, q is the number of
missing and redundant slots in the given realisation.
For each dialog act, we generate ﬁve utterances and
select the top one with the lowest ERR as the ﬁnal
Human evaluation.
We conducted the human
evaluation using Amazon Mechanical Turk to assess subjective quality. We recruit master level
workers (who have good prior approval rates) to
perform a human comparison between generated
responses from two systems (which are randomly
sampled from comparison systems). The workers
are required to judge each utterance from 1 (bad)
to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to
which generated utterance contains all the information speciﬁed in the dialog act. Naturalness
denotes whether the utterance is as natural as a human does. To reduce judgement bias, we distribute
each question to three different workers. Finally,
we collected in total of 5800 judges.
Baselines.
We compare with three baseline methods. (i) SC-LSTM is a canonical model and a strong baseline that uses an additional dialog act vector and a reading gate to
guide the utterance generation. (ii) GPT-2 (Radford et al.) is used to directly ﬁne-tune on the
domain-speciﬁc labels, without pre-training on the
large-scale corpus of (dialog act, response) pairs.
(iii) HDSA is a state-of-the-art
model on MultiWOZ. It leverages dialog act structures to enable transfer in the multi-domain setting,
showing superior performance than SC-LSTM.
FEWSHOTWOZ
Table 3 reports the automatic evaluation performance of different methods on FEWSHOTWOZ.
SC-LSTM fails to learn the generation effectively
SC-LSTM 
HDSA 
Table 5: Performance on MultiWOZ
Table 6: BLEU score of different models on MultiWOZ
using training data of different sizes.
in this few-shot learning setting. The generated
utterances are poor in quality and suffer from inaccurate slot rendering. In addition, GPT-2 performs consistently better than SC-LSTM in all the
domains. It reveals the feasibility of using a pretrained language model for NLG, though only limited annotations are available for ﬁne-tuning. Importantly, SC-GPT performs signiﬁcantly better
than GPT and SC-LSTM in terms of both BLEU
and ERR. In all the domains, SC-GPT reduces the
ERR to a signiﬁcantly lower level, revealing its
strong controllability power. This veriﬁes the importance of pre-training on large annotated dialog
data, as SC-GPT learns how to generate utterances
speciﬁed by the dialog acts accurately.
Table 4 shows the human assessment on FEW-
SHOTWOZ. The results exhibit the same trend
with automatic evaluation. SC-GPT outperforms
GPT-2 and SC-LSTM signiﬁcantly in both metrics,
i.e., SC-GPT can better control the generation to
convey information in the dialog act while maintaining good ﬂuency. Note that the gap between
SC-GPT and human annotation is still large, indicating that the proposed FEWSHOTWOZ exhibits
an under-explored research area, and provides a
large space to encourage future research for improvement.
The results on MultiWOZ are shown in Table 5.
Following Chen et al. , Entity F1 is used to evaluate the entity coverage accuracy (including all slot values, days, numbers,
and reference, etc.). Again, SC-GPT achieves the
best performance on BLEU score. Note that GPT-2
performs similarly with SC-GPT on the full Multi-
Informativeness
Naturalness
* p < 0.005
Table 7: Human evaluation on MultiWOZ. Statistical
signiﬁcance was computed with a two-tailed t-test between SC-GPT and HDSA.
WOZ dataset, this is because MultiWOZ contains
57k utterances, which is large enough for GPT-2
to achieve good performance. The results also con-
ﬁrm that with enough annotated data, conditional
language model formulation performs signiﬁcantly
better than HDSA, a strong competitor that leverages graph/tree-structure information to encode dialog acts.
To study how SC-GPT performs with different
training data sizes. We further conduct experiments
with varying percentages of training data on MultiWOZ, ranging from 0.1% (50 examples) to 50%.
As shown in Table 6, the observations are consistent with FEWSHOTWOZ. SC-GPT performs consistently better than GPT-2, HDSA, and SC-LSTM
for a wide range of dataset sizes, and the improvement is more substantial when the fewer numbers
of in-domain labels are used for ﬁne-tuning.
Table 7 shows the human assessment results on
MultiWOZ. The results are consistent with the automatic evaluation. It is interesting to see that (i)
the gap between the new state-of-the-art method
(i.e., SC-GPT ) and human performance on FEW-
SHOTWOZ (as shown in Table 4) is much larger
than that on MultiWOZ; (ii) the human rating on
the naturalness of SC-GPT is even higher than humans on MultiWOZ, while there is a visible gap on
FEWSHOTWOZ. These results demonstrate that
FEWSHOTWOZ presents a challenging few-shot
learning setting, SG-GPT serves as a simple and
strong baseline in this setting, and the combined
provides a platform for researchers to develop NLG
models that are able to generalize to new domains
and generate semantically conditioned and ﬂuent
responses.
We perform detailed analysis to investigate SG-
GPT’s ﬂexibility, controllability and generalizability. The test set is split into two subsets - seen and
Generated Responses from Different Models
Laptop{inform(name=satellite proteus 84; type=laptop; memory=8 gb; drive=1 tb; weight=2.3 kg)}
the satellite proteus 84 is a laptop with a 1 tb drive , 8 gb memory and weighs in at 2.3 kg
the satellite proteus 84 is a laptop with 8 gb , with a 1 tb drive , and is
for business computing
for business computing
[businesscomputing=true]
[weight=2.3kg]
the satellite proteus 84 laptop has a 1 tb drive , weighs 2.3 kg and is
used for business computing
[businesscomputing=true]
[memory=8 gb]
the satellite proteus 84 is a laptop with 8 gb memory , 1 tb drive , and a weight of 2.3 kg
Restaurant{inform only match(name=marnee thai and thanh long; pricerange=moderate; near=outer sunset; goodformeal=dinner)}
marnee thai and thanh long are the only 2 moderately priced restaurants near outer sunset that are good for dinner
marnee thai and thanh long is is moderate restaurant restaurant and good good good dinner
% [near=outer sunset]
[inform only match]
there is a moderately priced restaurant called marnee thai and thanh long that is near the outer sunset area
[goodformeal=dinner]
[inform only match]
marnee thai and thanh long is the only restaurant that serves moderately priced food near outer sunset and good for dinner
Train{inform(time=50 minutes)}, hotel{request(stars=?; area=?), inform(choice=5)}
travel time is 50 minutes . i found 5 hotels you may like . do you have a star rating request or an area you prefer
there are 5 trains that are 50 minutes . do you have a preferred departure location ?
% [train{inform(choice=5),request(departure=?)}]
[hotel{request(stars=?; area=?), inform(choice=5)}]
that meet your criteria .
do you have a preference on the area or star rating ?
[train{inform(choice=5,
time=50 minutes)}]
[hotel{inform(choice=5)}]
there are 5 hotels that meet your criteria . the trip will last 50 minutes . do you have an area preference or star rating you would like ?
Table 8: Examples of generated utterances from different models, along with its corresponding dialog acts (DAs)
and references. The ﬁrst two examples are sampled from FEWSHOTWOZ and the last one is from MultiWOZ.
Each generated utterance is followed by a brief description explaining the errors (starting with “%”). (Better
viewed in color. wrong , redundant , missing information)
BLEU ↑ERR ↓BLEU ↑ERR ↓
Table 9: Performance of different methods on seen DAs
and unseen DAs in restaurant domain.
unseen. If a dialog act of an example appears in
the training set, the example is marked as seen; otherwise, it is marked as unseen. Table 9 compares
different models on the seen and unseen subsets in
the restaurant domain. SC-GPT yields higher
BLEU and lower ERR, and the improvement is
more signiﬁcant on the unseen set. For example,
SC-GPT reduces ERR to 4.96, an order of magnitude lower than SC-LSTM and only 1/3 of GPT-2.
This demonstrates that SC-GPT generalizes well to
novel dialog acts, and is able to precisely ground
in them to compose ﬂuent responses. This is further conﬁrmed by the quantitative comparison in
Table 8, where we compare the generated utterance
examples of different models. While the baseline
methods prone to over-generate or miss important
slots, SC-GPT can successfully generate ﬂuent natural language utterances that share precise semantic
conditions with the ground-truth references.
We further simulate the process when deploying
SC-GPT for a new domain, using the examples
provided in the RASA dialog toolkit 8. We ﬁrst
ﬁne-tune SC-GPT using a few training examples
(only 16 instances in this new domain), and then
generate utterances based on novel dialog acts that
are unseen in training data. Table 10 shows some
examples of generated utterances with novel dialog
acts. In practice, it is desirable for an NLG system
to deal with an extending domain whose dialog
acts change dynamically. We simulate the setting
by editing the original input dialog acts, such as
inserting or deleting a slot, or substituting a slot
Since SC-LSTM is infeasible in the setting of
an extending domain, we compare SC-GPT with
GPT-2. Results show that SC-GPT produces better utterances than GPT-2. SC-GPT can generate
reasonably good natural language responses with
different combinations of editing operations, showing its high ﬂexibility to generalize to new dialog
acts with very limited training data, and produce
8 
/examples/restaurantbot
Generated Responses from Different Models
Original DA
ack makereservation ( price = moderate ; restaurantcusine = chinese ; location = center ; peoplecount = 2 )
ok making a reservation for restaurant moderate chinese in center for two people?
ack makereservation ( price = moderate ; restaurantcusine = japanese ; location = lincoln square ; peoplecount = 2 )
ok making reservation for moderate japanese restaurant in lincoln square for 2 people?
ok making a reservation for a moderate japanese cuisine restaurant in lincoln square for 2 people?
ack makereservation (
restaurantname = sakura
; price = moderate ; restaurantcusine =
; location =
lincoln sqare ; peoplecount = 2 )
ok making a reservation for 2 people sakura moderate japanese restaurant located lincoln sqare for 10 minutes?
ok making a reservation for sakura, moderate japanese cuisine in lincoln square for 2 people?
ack makereservation ( price = expensive ; restaurantcusine = japanese ; location = center ; peoplecount = 2 )
ok making reservation for japanese cuisine expensive?
ok making reservation for expensive japanese cuisine?
Generated Responses from Different Models
Original DA
ack makereservation ( childrenfriendly = true )
make reservation for a children friendly restaurant ?
ack makereservation ( restaurantname = sakura
; childrenfriendly = true ; price = moderate )
make reservation at sakura child friendly restaurant and make table moderate price?
make reservation for restaurant sakura moderate price and children friendly restaurant?
Table 10: Examples of generated utterances with novel dialog acts. SC-GPT produces better utterances than
GPT-2 for with edited dialog acts. Since both models produce similar responses to references for the original dialog act, the results are not shown here. (Better viewed in color.
insert a slot , substitute a slot value ,
delete a slot ).
controllable responses.
Conclusion
In this paper, we have made two major contributions towards developing a more pragmatic NLG
module for task-oriented dialog systems: (i) A new
benchmark FEWSHOTWOZ is introduced to simulate the few-shot learning scenarios with scarce
labelled data in real-world applications. (ii) A new
model SC-GPT is proposed to endow the NLG
module with strong semantically controlling and
generalization ability. Empirical results on both
FEWSHOTWOZ and MultiWOZ show that SC-
GPT achieves the best overall performance in both
automatic and human evaluations.
There are two interesting directions for future
work. The ﬁrst is to design mechanisms to generate more interpersonal responses which are proven
to help improve user experiences . The other is to generalize the generative pre-training idea to all four modules in
the dialog system pipeline for end-to-end training. Since these four modules process information in order, one may organize their input/output
as segments, and pre-train a segment-level autoregressive model.