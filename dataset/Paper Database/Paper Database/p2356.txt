Ensemble learning for data stream analysis: a survey
Bartosz Krawczyka,∗, Leandro L. Minkub, Jo˜ao Gamad, Jerzy Stefanowskid,
Micha l Wo´zniake
aDepartment of Computer Science, Virginia Commonwealth University, Richmond, VA
23284, USA
bDepartment of Computer Science, University of Leicester, Leicester, UK
cLaboratory of Artiﬁcial Intelligence and Decision Support, University of Porto, Porto,
dInstitute of Computing Science, Pozna´n University of Technology,
60-965 Pozna´n, Poland
eDepartment of Systems and Computer Networks, Wroc law University of Science and
Technology, Wyb. Wyspia´nskiego 27, 50-370 Wroc law, Poland
In many applications of information systems learning algorithms have to
act in dynamic environments where data are collected in the form of transient
data streams. Compared to static data mining, processing streams imposes
new computational requirements for algorithms to incrementally process incoming examples while using limited memory and time. Furthermore, due to
the non-stationary characteristics of streaming data, prediction models are
often also required to adapt to concept drifts. Out of several new proposed
stream algorithms, ensembles play an important role, in particular for nonstationary environments. This paper surveys research on ensembles for data
stream classiﬁcation as well as regression tasks. Besides presenting a comprehensive spectrum of ensemble approaches for data streams, we also discuss
advanced learning concepts such as imbalanced data streams, novelty detection, active and semi-supervised learning, complex data representations and
structured outputs. The paper concludes with a discussion of open research
problems and lines of future research.
∗Corresponding author
Email addresses: (Bartosz Krawczyk),
 (Leandro L. Minku), (Jo˜ao
Gama), (Jerzy Stefanowski),
 (Micha l Wo´zniak)
 
February 1, 2017
Ensemble learning, Data streams, Concept drift, Online
learning, Non-Stationary Environments
1. Introduction
The analysis of huge volumes of data is recently the focus of intense research, because such methods could give a competitive advantage for a given
company. For contemporary enterprises, the possibility of making appropriate business decisions on the basis of knowledge hidden in stored data is one
of the critical success factors. Similar interests in exploring new types of data
are present in many other areas of human activity.
In many of these applications, one should also take into consideration that
data usually comes continuously in the form of data streams. Representative
examples include network analysis, ﬁnancial data prediction, traﬃc control,
sensor measurement processing, ubiquitous computing, GPS and mobile device tracking, user’s click log mining, sentiment analysis, and many others
 .
Data streams pose new challenges for machine learning and data mining
as the traditional methods have been designed for static datasets and are not
capable of eﬃciently analyzing fast growing amounts of data and taking into
consideration characteristics such as:
• Limited computational resources as memory and time, as well as tight
needs to make predictions in reasonable time.
• The phenomenon called concept drift, i.e., changes in distribution of
data which occur in the stream over time. This could dramatically
deteriorate performance of the used model.
• Data may come so quickly in some applications that labeling all items
may be delayed or sometimes even impossible.
Out of several tasks studied in data streams , supervised classiﬁcation
has received the most research attention. It is often applied to solve many
real life problems such as discovering client preference changes, spam ﬁltering, fraud detection, and medical diagnosis to enumerate only a few. The
aforementioned speed, size and evolving nature of data streams pose the need
for developing new algorithmic solutions. In particular, classiﬁers dedicated
to data streams have to present adaptation abilities, because the distribution
of the data in motion can change. To tackle these challenges, several new
algorithms, such as VFDT , specialized sliding windows, sampling methods, drift detectors and adaptive ensembles have been introduced in the last
In our opinion, ensemble methods are one of the most promising research
directions . An ensemble, also called a multiple classiﬁer or committee,
is a set of individual component classiﬁers whose predictions are combined
to predict new incoming instances. Ensembles have been shown to be an ef-
ﬁcient way of improving predictive accuracy or/and decomposing a complex,
diﬃcult learning problem into easier sub-problems.
The main motivation for using classiﬁer ensembles is the no free lunch
theorem formulated by Wolpert . According to it, there is not a single
classiﬁer that is appropriate for all the tasks, since each algorithm has its own
domain of competence. Usually, we have a pool of classiﬁers at our disposal
to solve a given problem. Turner showed that averaging outputs of
an inﬁnite number of unbiased and independent classiﬁers may lead to the
same response as the optimal Bayes classiﬁer . Ho underlined that
a decision combination function must receive useful representation of each
individual decision. Speciﬁcally, they considered several methods based on
decision ranks, such as Borda count.
We also have to mention another of Ho’s work , who distinguished
two main approaches to design a classiﬁer ensemble:
• Coverage optimization focuses on the generation of a set of mutually
complementary classiﬁers, which may be combined to achieve optimal
accuracy using a ﬁxed decision combination function.
• Decision optimization concentrates on designing and training an appropriate decision combination function, while a set of individual models
is given in advance .
Other important issues that have be taken into consideration when building classiﬁer ensembles are the following:
• Proposing interconnections among individual classiﬁers in the ensemble.
• Selecting a pool of diverse and complementary individual classiﬁers for
the ensemble.
classifier #1
classifier #2
classifier #n
combination
Figure 1: A diagram of the classiﬁer ensemble.
• Proposing a combination rule, responsible for the ﬁnal decision of the
ensemble, which should exploit the strengths of the component classi-
The general diagram of a classiﬁer ensemble is depicted in Figure 1.
The selection of classiﬁers for the ensemble is a key factor.
ensemble includes mutually complementary individual classiﬁers which are
characterized by high diversity and accuracy . It is generally agreed
that not only the accuracy, but also the diversity of the classiﬁers is a key
ingredient for increasing the ensemble’s accuracy . Classiﬁers must be
selected to obtain positive results from their combination. Sharkley et al.
 proposed four levels of diversity based on the majority vote rule, coincident error, and the possibility of at least one correct answer of ensemble
members. Brown et al. reﬂected that it is inappropriate for the case
where diversity of an ensemble is diﬀerent in various subspaces of the feature
space. For comprehensive reviews on ensemble methods developed for static
datasets see, e.g., .
Classiﬁer ensembles are an attractive approach to construct data stream
classiﬁers, because they facilitate adaptation to changes in the data distribution. Their adaptation could be done by changing the line-up of the ensemble,
e.g., by adding components classiﬁers trained on the most recent data and/or
removing the outdated classiﬁers, or by retraining the ensemble components.
There are several interesting books or surveys on the data stream analysis
and classiﬁcation, but most of them focus on general methods of data stream
analysis, not dedicating too much space to ensemble approaches , and some have been written several years ago .
Therefore, there is still a gap in this literature with respect to present the
development in learning ensembles from data streams. This survey aims to
ﬁll this gap.
It is also worth mentioning the work , where data stream mining
challenges have been discussed. We will discuss open research problems and
lines of future research in the speciﬁc area of ensemble approaches for data
We will pay the most attention to classiﬁer ensembles, given that most
existing literature is in this area. However, we will also discuss research on regression (or prediction model) ensembles. Furthermore, we will review recent
ensemble approaches dedicated to various more complex data representations
in streams.
This survey is organized as follows. Section 2 focuses on the main characteristics of data streams and methods dedicated to their analysis, as well as
on the type of data streams and drift detection methods. Section 3 presents
methods for evaluating classiﬁers over streaming data. In section 4, a comprehensive survey on ensemble techniques for classiﬁcation and regression
problems is presented. Section 5 enumerates advanced problems for data
stream mining, such as imbalanced data, novelty detection, one-class classi-
ﬁcation, and active learning, as well as focuses on non-standard and complex
data representations or class structures. The ﬁnal section draws open challenges in this ﬁeld for future research.
2. Data Stream Characteristics
In this section we will provide a general overview of the data stream
domain, discussing diﬀerent types of streaming data, learning frameworks
used for its analysis, and the issue of changes in the data stream distribution,
known as concept drift.
2.1. General Issues
A data stream is a potentially unbounded, ordered sequence of data items
which arrive over time. The time intervals between the arrival of each data
item may vary. These data items can be simple attribute-value pairs like
relational database tuples, or more complex structures such as graphs.
The main diﬀerences between data streams and conventional static datasets
include :
• data items in the stream appear sequentially over time,
• there is no control over the order in which data items arrive and the
processing system should be ready to react at any time,
• the size of the data may be huge (streams are possibly of inﬁnite
length); it is usually impossible to store all the data from the data
stream in memory,
• usually only one scan of items from a data stream is possible; when
the item is processed it is discarded or sometimes stored if necessary,
or aggregated statistics or synopses are calculated,
• the data items arrival rate is rapid (relatively high with respect to the
processing power of the system),
• data streams are susceptible to change (data distributions generating
examples may change on the ﬂy),
• the data labeling may be very costly (or even impossible in some cases),
and may not be immediate.
These data stream characteristics pose the need for other algorithms than
ones previously developed for batch learning, where data are stored in ﬁnite,
persistent data repositories. Typical batch learning algorithms are not capable of fulﬁlling all of the data stream requirements such as constraints of
memory usage, restricted processing time, and one scan of incoming examples
 . Note that some algorithms, like Na¨ıve Bayes, instance based learning or
neural networks are naturally incremental ones. However, simple incremental learning is typically insuﬃcient, as it does not meet tight computational
demands and does not tackle evolving nature of data sources .
Constraints on memory and time have resulted in the development of
diﬀerent kinds of windowing techniques, sampling (e.g. reservoir sampling)
and other summarization approaches. However, the distribution in the data
source generating the stream data items may change over time. Thus, in case
of non-stationary data streams, data from the past can become irrelevant
or even harmful for the current situation, deteriorating predictions of the
classiﬁers. Data management approaches can play the role of a forgetting
mechanism where old data instances are discarded.
2.2. Types of Data Streams and Learning Frameworks
If a completely supervised learning framework is considered, it is assumed
that after some time the true target output value yt of the example is available. Thus, data stream S is a sequence of labeled examples zt = (xt, yt) for
t = 1, 2, . . . , T. Usually, x is a vector of attribute values, and y is either a
discrete class label (y ∈{K1, . . . , Kl}) for classiﬁcation problems or numeric
output (independent) values for regression problems. The general task is to
learn from the past data (a training set of examples) the relationship between
the set of attributes and the target output. In the case of classiﬁcation, this
relationship corresponds to discovered classiﬁcation knowledge and it is often
used as classiﬁer C to determine the class label for the new coming example
xt′. In the case of regression, the learned model is used to predict a numeric
value. Note that the classiﬁer or the regression model is supposed to provide
its prediction at any time based on what it has learned from the data items
{z1, z2, . . . , zt} seen so far. This prediction ˆyt and true target value yt can
be used by the learning algorithm as additional learning information.
As most of the current research on data stream ensembles concerns classi-
ﬁcation, we will present the remaining of this section using the classiﬁcation
terminology. However, nearly all of these issues are also valid for regression
The majority of proposed algorithms for learning stream classiﬁers follow
the supervised framework (i.e.
with a complete and immediate access to
class labels for all processed examples). However, in some applications the
assumption of a complete labeling of learning examples may be unrealistic
or impractical, as the class labels of newly coming examples in data streams
are not immediately available. For instance, in the ﬁnancial fraud detection,
information on fraud transactions is usually known after a long delay (e.g.
when an account holder receives the monthly report ), while for a credit
approval problem the true label is often available after 2-3 years. Moreover,
the acquiring of labels from experts is costly and needs substantial eﬀorts
 . Therefore some researchers consider other frameworks such as:
• learning with delayed labeling when an access to true class labels is
available much later than it is expected; the classiﬁer may adapt to the
stream earlier without knowing it ,
• semi-supervised learning where labels are not available for all incoming
examples; They are provided in limited portions from time to time;
block #k+l
update model using
example #k
classifier
update model block
of data #k
classifier
Figure 2: Diﬀerence between incremental and block base classiﬁer updating.
alternatively, the system employs an active learning technique, which
selects unlabeled examples for acquiring their labels ,
• unsupervised framework or learning from initially labeled examples;
An initial classiﬁer is learned from a limited number of labeled training examples, and then it processes the upcoming stream of unlabeled
examples without any access to their labels .
We will come to these issues in Section 5.3.
Examples from the data stream are provided either online, i.e., instance
by instance, or in the form of data chunks (portions, blocks). In the ﬁrst
approach, algorithms process single examples appearing one by one in consecutive moments in time, while in the other approach, examples are available
only in larger sets called data blocks (or data chunks) S = B1 ∪B2 ∪. . .∪Bn.
Blocks are usually of equal size and the construction, evaluation, or updating
of classiﬁers is done when all examples from a new block are available. This
distinction may be connected with supervised or semi-supervised frameworks.
For instance, in some problems data items are more naturally accumulated
for some time and labeled in blocks while an access to class labels in an online
setup is more demanding. Moreover, these types of processing examples also
inﬂuence the evaluation of classiﬁers. Both discussed modes are depicted in
2.3. Stationary and Non-Stationary (Drifting) Data Streams
Two basic models of data streams are considered: stationary, where examples are drawn from a ﬁxed, albeit unknown, probability distribution, and
non-stationary, where data can evolve over time. In the second case, target
concepts (classes of examples) and / or attribute distributions change. In
other words, the concept from which the data stream is generated shifts after a minimum stability period . This phenomenon is called concept drift,
a.k.a, covariant shift. Concept drifts are reﬂected in the incoming instances
and deteriorate the accuracy of classiﬁers / regression models learned from
past training instances. Typical real life streams aﬀected by concept drift
could include :
• computer or telecommunication systems, where attackers look for new
ways of overcoming security systems,
• traﬃc monitoring, where traﬃc patterns may change over time,
• Weather predictions, where climate changes and natural anomalies may
inﬂuence the forecast,
• system following personal interests, like personal advertisement, where
users may change their preferences, and
• medical decision aiding, where disease progression may be inﬂuenced
and changed in response to applied drugs or natural resistance of the
Other examples of real life concept drifts include spam categorization, object positioning, industrial monitoring systems, ﬁnancial fraud detection, and
robotics; and they are reviewed in the recent survey .
Concept drift can be deﬁned from the perspective of hidden data contexts, which are unknown to the learning algorithm. Zliobaite also calls it an
unforeseen change as the change is unexpected with respect to the current
domain knowledge or previous learning examples . However, a more
probabilistic view on this matter is usually presented, e.g. .
In each point in time t, every example is generated by a source with a joint
probability distribution P t(x, y). Concepts in data are stable or stationary
if all examples are generated by the same distribution. If, for two distinct
points in time t and t + ∆, there exits x such that P t(x, y) ̸= P t+∆(x, y),
then concept drift has occurred.
Diﬀerent components of P t(x, y) may change . In particular, when
concept drift occurs, either one or both of the following changes:
• prior probabilities of classes P(y),
• class conditional probabilities P(x|y).
As a result, posterior probabilities of the classes P(y|x) may (or may not)
Based on the cause and eﬀect of these changes, two types of drift are
distinguished: real drift and virtual drift.
A real drift is deﬁned as a change in P(y|x). It is worth noting that such
changes can occur with or without changes in P(x). Therefore, they may
or may not be visible from the data distribution without knowing the true
class labels. Such a distinction is crucial, as some methods attempt to detect
concept drifts using solely input attribute values. Real drift has also been
referred to as concept shift and conditional change .
A virtual drift is usually deﬁned as a change in the attribute-value P(x),
or class distributions P(y) that does not aﬀect decision boundaries. In some
work virtual drift is deﬁned as a change that does not aﬀect the posterior
probabilities, but it is hard to imagine that P(x) is changed without changing
P(y)P(x|y)
in real world applications.
However, the source and
therefore the interpretation of such changes diﬀers among authors. Widmer
and Kubat attributed virtual drift to incomplete data representation
rather than to true changes in concepts. Tsymbal on the other hand
deﬁned virtual drift as changes in the data distribution that do not modify
the decision boundary, while Delany described it as a drift that does not
aﬀect the target concept. Furthermore, virtual drifts have also been called
temporary drifts, sampling shifts or feature changes .
Most current research on learning classiﬁers from evolving streams concentrates on real drifts. However, it is worth mentioning that even if the true
class boundaries do not change in virtual drifts, this type of drift may still
result in the learnt class boundaries to become inadequate. Therefore, techniques for handling real drifts may still work for certain types of virtual drifts.
If posterior probabilities do not change, it is worthless to rebuild the model,
because the decision boundaries are still the same. Virtual drift detection is
also important, because even though it does not eﬀect the decision boundaries of the classiﬁer, its wrong interpretation (i.e., detecting and classifying
as real drift) could provide wrong decision about classiﬁer retraining.
Apart from diﬀerences in the cause and eﬀect of concept changes, researchers distinguish between several ways of how such changes occur. Concept drifts can be further characterized, for example, by their permanence,
severity, predictability, and frequency.
The reader is also referred to the
recent paper by Hyde et al.
 , which is the ﬁrst attempt to provide
incremental
reccouring
Figure 3: Type of drifts.
the more formal framework for comparing diﬀerent types of drifts and their
main properties. These authors also proposed a new, quite comprehensive
taxonomy of concept drift types.
The most popular categorizations include sudden (abrupt) and gradual
drifts . The ﬁrst type of drift occurs when, at a moment in time t, the
source distribution in St is suddenly replaced by a diﬀerent distribution in
St+1. Gradual drifts are not so radical and are connected with a slower rate
of changes, which can be noticed while observing a data stream for a longer
period of time. Additionally, some authors distinguish two types of gradual
drift . The ﬁrst type of gradual drift refers to the transition phase where
the probability of sampling from the ﬁrst distribution P j decreases while the
probability of getting examples from the next distribution P j+1 increases.
The other type, called incremental (stepwise) drift, consists of a sequence of
small (i.e., not severe) changes. As each change is small, the drift may be
noticed only after a long period of time, even if each small change occurs
In some domains, situations when previous concepts reappear after some
time are separately treated and analyzed as recurrent drifts. This re-occurrence
of drifts could be cyclic (concepts reoccur in a speciﬁc order) or not .
Moreover, data streams may contain blips (rare events/outliers) and noise,
but these are not considered as concept drifts and data stream classiﬁers
should be robust to them. The diﬀerences among the drifts are depicted in
Some other drift characteristics are also considered in the literature. Typically, real concept drift concerns changes for all examples but it could be
also a sub-concept change where drift is limited to a subspace of a domain –
see discussions on the drift severity in . Moreover, in real life situations,
performance
restoration time
max. performance deterioration
Figure 4: The idea of the model restoration time.
concept drifts may be a complex combination of many types of basic drifts.
For more information on these and other changes in underlying data distributions, the reader is referred to . These studies,
and more application oriented papers, such as , demonstrate that the
problem of concept drift has also been recognized and addressed in multiple
application areas. This shows the strong requirement for streaming classiﬁers
to be capable of predicting, detecting, and adapting to concept drifts.
2.4. Drift Detection Methods
Concept drift detectors are methods, which on the basis of information
about classiﬁer’s performance or the incoming data items themselves, can
signal that data stream distributions are changing. Such signals usually trigger updating / retraining of the model, or substituting the outdated model
by the new one. Our aim is on the one hand to reduce the maximum performance deterioration and on the other hand to minimize so-called restoration
time (see Figure 4).
The detectors may return not only signals about drift detection, but also
warning signals, which are usually treated as a moment when a change is
suspected and a new training set representing the new concept should start
being gathered. The idea of drift detection is presented in Figure 5.
Drift detection is not a trivial task, because on the one hand we require
suﬃciently fast drift detection to quickly replace outdated model and to
Figure 5: The idea of drift detection based on tracking classiﬁer errors.
reduce the restoration time. On the other hand we do not want too many
false alarms . Therefore, to assess a concept drift detector’s performance,
the following metrics are usually considered:
• number of true positive drift detections,
• number of false alarms, i.e., false positive drift detections,
• drift detection delay, i.e., time between real drift appearance and its
detection.
One diﬃculty arises because there is typically a trade-oﬀbetween diﬀerent
metrics. For instance, a drift detector can typically be tuned to decrease
the detection delay, but this may lead to a higher number of false alarms.
In view of that, Alippi et al. have recently used the following procedure
to evaluate their drift detection method when using artiﬁcial data streams.
They generates a stream that contains enough instances after a drift so that
drifts are always detected by all drift detection methods being evaluated.
They then plotted the number of false alarms versus the drift detection delay
for all drift detectors, using several diﬀerent parameter conﬁgurations. This
lead to a curve that resembles the Receiver Operating Characteristics curve,
but used to evaluate drift detection methods rather than classiﬁers.
In a few papers aggregated measures, which take into consideration the
aforementioned metrics, are also proposed. It is worth mentioning the work
of Pesaranghader and Victor , where the acceptable delay length was
deﬁned to determine how far the detected drift could be from the true location of drift, for being considered as a true positive. A recent experimental
framework for the drift detection evaluation can be found in .
The authors of propose to categorize the drift detectors into the
following four main groups:
1. Detectors based on Statistical Process Control.
2. Detectors based on the sequential analysis.
3. Methods monitoring distributions of two diﬀerent time windows.
4. Contextual approaches.
In the next paragraphs, we brieﬂy describe a few drift detection methods.
DDM (Drift Detection Method) is the most well known representative
of the ﬁrst category. It estimates classiﬁer error (and its standard deviation),
which (assuming the convergence of the classiﬁer training method) has to decrease as more training examples are received . If the classiﬁer error is
increasing with the number of training examples, then this suggests a concept
drift, and the current model should be rebuilt. More technically, DDM generates a warning signal if the estimated error plus twice its deviation reaches
a warning level.
If the warning level is reached, new incoming examples
are remembered in a special window. If afterwards the error falls below the
warning threshold, this warning is treated as a false alarm and this special
window is dropped. However, it the error increases with time and reaches the
drift level, the current classiﬁer is discarded and a new one is learned from
the recent labeled examples stored in the window. Note that this detection
idea may be also used to estimate time interval between the warning and
drift detection, where shorter times indicate a higher rate of changes.
EDDM (Early Drift Detection Method) is a modiﬁcation of DDM to improve the detection of gradual drifts . The same idea of warning and drift
levels is realized with a new proposal of comparing distances of error rates.
Yet another detector ECDD employs the idea of observing changes in the
exponentially weighted moving average .
The sequential probability ratio tests, such as the Wald test, are the basis
for detectors belonging to the second category. The cumulative sum approach
(CUSUM) detects a change of a given parameter value of a probability
distribution and indicates when the change is signiﬁcant. As the parameter
the expected value of the classiﬁcation error could be considered, which may
be estimated on the basis of labels of incoming examples from data stream.
A comprehensive analysis of the relationship between CUSUM’s parameters
and its performance was presented in .
PageHinkley is modiﬁcation of the CUSUM algorithm, where the cumulative diﬀerence between observed classiﬁer error and its average is taken into
consideration .
Yet other drift detectors based on non-parametric estimation of classiﬁer
error employing Hoeﬀding’s and McDiarmid’s inequalities were proposed in
ADWIN is the best known representative of methods comparing two sliding windows. In this algorithm a window of incoming examples grows
until identifying a change in the average value inside the window. When the
algorithm succeeds at ﬁnding two distinct sub-windows, their split point is
considered as an indication of concept drift.
Besides the use of parametric tests for concept drift detection, some nonparametric tests have also been investigated, such as the computational intelligence cumulative sum test and the intersection of conﬁdence intervalsbased change detection test .
Alippi presents an interesting comparison of diﬀerent triggering mechanisms for concept drift detection . It is worth noting that drift detectors
frequently rely on continuous access to class labels, which usually cannot be
granted from the practical point of view. Therefore, during constructing the
concept drift detectors we have to take into consideration the cost of data
labeling, which is usually passed over. A very interesting way to design detectors is to employ the active learning paradigm or unlabeled examples
Unsupervised detection of virtual concept drift is most often performed
with statistical tests , which check whether a current data portion comes
from the same distribution as the reference data. Obviously, not all statistical
tests are suited for this task, e.g., two-sample parametric tests such as a T2
statistic assume a speciﬁc distribution, which might not be a correct
approach in the real data case. Also, the distributions may not be similar
to any standard distribution, what moreover suggests non-parametric tests
for the task of unsupervised concept drift detection. Examples of such tests
include :
• CNF Density Estimation test introduced in , describes the data
by vectors of binary features, assigned by discretizing attributes into
sets of bins. Then, it creates a set of Boolean attributes, which covers
all of the examples in the reference dataset, meaning that each true
feature in attribute set is the same as in at least one of the vectors
describing the data points in the reference set. Next, another set of
data is drawn from the same distribution as the data in the reference
set, represented as binary vectors, and compared to the attribute set by
applying a Matt-Whitney test. If the diﬀerence is insigniﬁcant, all data
is considered to come from the same distribution, otherwise a diﬀerence
in distributions is detected.
• The multivariate version of the Wald-Wolfowitz test constructs
a complete graph, with examples as vertices and distances between
them as edges. This graph is then transformed into a forest and a test
statistic is computed basing on the amount of trees.
Furthermore, non-parametric univariate statistical tests are often used
for detecting concept drift in data distribution :
• Two-sample Kolmogorov-Smirnov test,
• Wilcoxon rank sum test,
• Two-sample t-test.
Unfortunately, it is easy to show that without access to class labels the
real drift could be undetected if they are not associated to changes in
As yet not so many papers deal with combined drift detectors. Bifet et
al. proposed the simple combination rules based on the appearance of
drift once ignoring signals about warning level.
It is worth mentioning Drift Detection Ensemble , where a small
ensemble of detectors is used to make a decision about the drift and Selective
Detector Ensemble based on a selective detector ensemble to detect both
abrupt and gradual drifts. Some experimental studies showed that simple
detector ensembles do not perform better than simple drift detection methods
3. Evaluation in Data Stream Analysis
Proper evaluation of classiﬁers or regression models is a key issue in machine learning. Many evaluation measures, techniques for their experimental
estimation and approaches to compare algorithms have already been proposed for static data scenarios. A comprehensive review is presented in .
In the context of data stream mining, especially in non-stationary environments, new solutions are needed. While evaluating predictive ability,
it is necessary to consider both incremental processing as well as evolving
data characteristics and the classiﬁer reactions to changes. New classes may
appear, feature space changes and decision rules lose relevance over time.
Moreover, one should take into account computational aspects such as processing time, recovery of the model after the change, and memory usage.
Fast updating of a learning model and gradual recovery is often more reasonable than gathering data for a longer period of time and trying to rebuild
the model in a single time consuming step. Instead of examining point or
average prediction measures of the classiﬁer, one is usually more interested
in tracking its working characteristics over the course of stream progression.
The authors of several papers often present graphical plots for a given
dataset presenting the algorithms’ functioning in terms of the chosen evaluation measure, such as e.g. training time, testing time, memory usage, and
classiﬁcation accuracy over time. By presenting the measures calculated after
each data chunk or single example on the y-axis and the number of processed
training examples on the x-axis, one can examine the dynamics of a given
classiﬁer, in particular, its reactions to concept drift. Such plots also nicely
support a comparative analysis of several algorithms.
Additionally, one must also consider the availability of information regarding the true target values of incoming examples. The majority of current measures and evaluation techniques assume immediate or not too much
delayed access to these labels.
However, in some real life problems, this
assumption is unrealistic.
It is also worth mentioning that a thorough evaluation of predictive models in non-stationary environments typically requires the use of not only real
world data streams, but also data streams with artiﬁcially generated concept
drifts. Real world data streams enable us to evaluate how helpful a predictive model is in real world situations. However, they usually do not allow
us to know when exactly a drift occurs, or even if there are really drifts.
This makes it diﬃcult to provide an in depth understanding of the behaviour
of predictive models or drift detection methods. Data streams with artiﬁcially induced drifts enable a more detailed analysis. Therefore, both real
world data streams and data streams with artiﬁcially induced drifts are important when evaluating predictive models and concept drift detectors in
non-stationary environments.
The comparison of algorithms proposed in the literature is not an easy
task, as authors do not always follow the same recommendations, experimental evaluation procedures and / or datasets. Below, we discuss the most
popular evaluation measures and then their experimental estimation procedures.
3.1. Evaluation Measures
The predictive ability of classiﬁers or regression models is usually evaluated with the same measure as proposed for static, non-online learning which
are also the least computationally demanding ones. Below we list the most
popular ones:
• Accuracy : the proportion of all correct predictions to the total number of examples, or its corresponding measure classiﬁcation error,
are the most commonly used for classiﬁcation.
• Mean square error or absolute error is a typical measure for regression.
• Sensitivity of the class of interest (also called Recall or True Positive
Rate) is accuracy of a given class.
• G-Mean : the geometric mean of sensitivity and speciﬁcity is often
applied on class-imbalanced data streams to avoid the bias of the overall
• Kappa Statistic : K = p0−pc
1−pc , where p0 is accuracy of the classiﬁer and
pc is the probability of a random classiﬁer making a correct prediction.
• Generalized Kappa Statistics such as Kappa M proposed in ,
which should be more appropriate than the standard Kappa Statistics
for dealing with imbalanced data streams.
Furthermore, in the case of static data the area under the Receiver Operating Characteristics curve, or simply AUC, is a popular measure for evaluating classiﬁers both on balanced and imbalanced class distributions .
However, in order to calculate AUC one needs to sort scores of the classiﬁers on a given dataset and iterate through each example. This means
that the traditional version of AUC cannot be directly computed on large
data streams. The current use of AUC for data streams has been limited
only to estimations on periodical holdout sets or entire streams of a limited length . A quite recent study introduces an eﬃcient algorithm
for calculating Prequential AUC, suitable for assessing classiﬁers on evolving
data streams. Its statistical properties and comparison against simpler point
measures, such as G-mean or Kappa statistics, has been examined in .
When analyzing the performance of classiﬁers dedicated to drifted data,
we should also take into consideration their adaptation abilities, i.e., evaluating the maximum performance deterioration and restoration time, as mentioned in Section 2.4.
Apart from the predictive accuracy or error, the following performance
metrics should be monitored and taken into account during properly executed
evaluation of streaming algorithms:
• Memory consumption: it is necessary to monitor not only the average memory requirements of each algorithm, but also their change over
time with respect to actions being taken.
• Update time: here one is interested in the amount of time that an
algorithm requires to update its structure and accommodate new data
from the stream. In an ideal situation, the update time should be lower
than the arrival time of a new example (or chunk of data).
• Decision time: amount of time that a model needs to make a decision
regarding new instances from the stream. This phase usually comes before the updating procedure takes place. So, any decision latency may
result in creating a bottleneck in the stream processing. This is especially crucial for algorithms that cannot update and make predictions
regarding new instances at the same time.
Nevertheless, in order to calculate reaction times and other adaptability
measures, usually a human expert needs to determine moments when a drift
starts and when a classiﬁer recovers from it. Alternately, such evaluations
are carried out with synthetic data generators.
More complex measures have also been proposed to evaluate other properties of algorithms. Shaker and H¨ullermeier proposed a complete framework for evaluating the recovery rate of the algorithm once a change has
occurred in the stream. They consider not only how well the model reduced
its error in the new decision space, but also what was the time necessary
to achieve this. Zliobaite et al. introduced the notion of cost-sensitive
update in order to evaluate the potential gain from the cost (understood as
time and computational resources) put into adapting the model to the current change. The authors argue that this allows to check if the actual update
of the model was a worthwhile investment. Hassani et al. proposed a
new measure for evaluating clustering algorithms for drifting data streams,
with special attention being paid to the behavior of micro-clusters.
3.2. Estimation Techniques
In the context of static and batch learning the most often used scenario for
estimating prediction measures is cross validation. However, in the context of
online learning with computationally strict requirements and concept drifts,
it is not directly applicable. Other techniques are considered. Two main
approaches are used depending whether the stream is stationary or not, as
shown below.
• Holdout evaluation: In this case two sub-sets of data are need: the
training dataset (to learn the model) and the independent holdout set
to test it. It is arranged that, at any given moment of time when we
want to conduct model evaluation, we have at our disposal a holdout
set not previously used by our model. By testing the learning model on
such a continuously updated set (it must be changed after each usage
to ensure that it represents the current concept well), we obtain an
unbiased estimator of the model error. When conducted in a given
time or instance interval, it allows us to monitor the progress of the
• Prequential evaluation is a sequential analysis where the sample size is not ﬁxed in advance. Instead, data are evaluated as they
are collected. Predictive sequential evaluation, or prequential, also referred to as interleave train and test, follows the online learning protocol. Whenever an example is observed, the current model makes a
prediction; when the system receives feedback from the environment,
we can compute the loss function.
Prequential measures can be calculated only for selected instances, thus
allowing to accommodate the assumption of limited label availability.
On the other hand, simply calculating a cumulative measure over the
entire stream may lead to strongly biased results. One may easily imagine a situation in which the overall cumulative evaluation is strongly
inﬂuenced by a certain time period, when, e.g., access to training data
was limited, the decision problem was much more simple, or drift was
not present. Thus, to make the error estimation more robust to such
cases, a proper forgetting mechanism must be implemented – sliding
windows or fading factors.
With this, an emphasis is put on error
calculation from the most recent examples. Indeed the term prequential (combination of words predictive and sequential) stems from online
learning and is used in the literature to denote algorithms that base
their functioning only on the most recent data. Prequential accuracy
 is popularly used with supervised learning, but also a prequential
version of AUC metric was proposed by Brzezinski and Stefanowski
 , being suitable for streams with skewed distributions. This issue
was also addressed by Bifet and Frank , who also proposed a prequential modiﬁcation of kappa statistic suitable for streams.
A more elaborated approach to evaluate and compare algorithms in streaming scenarios have been introduced recently. Shaker and H¨ullermeier 
proposed an approach, called recovery analysis, which uses synthetic datasets
to calculate classiﬁer reaction times.
The authors proposed to divide a
dataset with a single drift into two sets without drifts. Afterwards, they
propose to plot the accuracy of the tested classiﬁer on each of these datasets
separately. The combination of these two plots is called the optimal performance curve and serves as a reference that can be compared with the
accuracy plot of the classiﬁer on the original dataset. Zliobaite proposed
to use modify a real stream by controlled permutations to better study the
reaction of classiﬁers to drifts . Recently Bifet at al. considered a prequential and parallel evaluation strategy inspired by cross-validation, which
switches new incoming examples between copies of classiﬁers – some of them
use it for updating while others for testing .
Statistical tests have gained a signiﬁcant popularity in the machine learning community . In the area of data streams there were few a approaches
to using these tools . However, they usually concentrated on applying
standard tests over the averaged results or by using sliding window technique.
One may be critical to such approaches, as they either try to transform a
dynamic problem into a static one, or take under consideration only local
characteristics. So far, there has been no uniﬁed statistical testing frame-
work proposed for data streams that would seem fully appropriate.
4. Ensemble Learning from Data Streams
This section discusses supervised data stream ensemble learning approaches
for classiﬁcation and regression problems. To organize the subjects discussed
in this survey and to oﬀer a navigation tool for the reader, we summarize
the proposed taxonomy of ensemble learning approaches for data streams in
Figure 6. Content presented there will be discussed in detail in Sections 4
and 5, with in-depth presentation of advances in the respective areas. Here,
we would like to explain a disproportion in the subcategories between supervised learning in classiﬁcation and regression problems.
Theoretically,
the same taxonomy used for the classiﬁcation ensembles could be used for
the regression ones. However, as there are still very few methods developed
in this area, we have opted for not proposing a separate taxonomy for the
streaming regression ensembles yet.
4.1. Supervised Learning for Classiﬁcation Problems
Ensembles are the most often studied new classiﬁers in the data stream
community, see e.g. lists of methods in . The proposed stream classiﬁers can be categorized with respect to diﬀerent points of view. The most
common categorizations are the following:
• stationary vs. non-stationary stream classiﬁers,
• active vs. passive approaches,
• chunk based vs. on-line learning modes,
• distinguishing diﬀerent techniques for updating component classiﬁers
and aggregating their predictions.
Approaches for stationary environments do not contain any mechanism
to accelerate adaptation when concept drift occurs.
Approaches for nonstationary environments are approaches speciﬁcally designed to tackle potential concept drifts.
When studying approaches to tackle concept drift, researchers usually
distinguish between active vs. passive (also called trigger vs. adaptive) approaches, see e.g. a discussion in . Active algorithms use special
techniques to detect concept drift which trigger changes or adaptations in
Ensemble learning from data streams
Supervised learning
for classiﬁcation
Chunk-based ensembles
for stationary streams
Online ensembles
for stationary streams
Chunk-based ensembles
for non-stationary streams
1. typical
2. alternative
Online ensembles
for non-stationary streams
2. passive
Supervised learning
for regression
Advanced issues
Imbalanced classiﬁcation
Novelty detection and
one-class classiﬁcation
Active and
semi-supervised
Complex data and
structured outputs
Figure 6: The taxonomy of ensemble learning methods for data streams discussed thorough
this survey.
classiﬁers (e.g., rebuilding it from the recent examples) – see the discussion
in earlier Section 2.4. Passive approaches do not contain any drift detector
and continuously update the classiﬁer every time that a new data item is
presented (regardless whether real drift is present in the data stream or not).
The majority of current ensembles follow a passive schema of adaptation,
while triggers are usually used mainly with single online classiﬁers. A few
rare cases of integrating them with ensembles, such as ACE , BWE 
or DDD , will be further discussed.
Then, with respect to the way of processing examples, the classiﬁers can
be categorized into chunk-based approaches and online learning approaches.
Chunk-based approaches process incoming data in chunks, where each chunk
contains a ﬁxed number of training examples. The learning algorithm may
iterate over the training examples in each chunk several times. It allows to
exploit batch algorithms to learn component classiﬁers. Online learning approaches, on the other hand, process each training examples separately, upon
arrival. This type of approach is intended for applications with strict time
and memory constraints, or applications where we cannot aﬀord processing
each training example more than once, e.g., applications where the amount
of incoming data is very large.
It is worth noting that the above categorization does not mean that chunkbased approaches must be used only for situations where new training examples arrive in chunks. They can also be used to learn training examples that
arrive separately, because each new training example can be stored in a buﬀer
until the size of this buﬀer reaches the size of the chunk. Then, chunk-based
approaches may process all these examples stored in the buﬀer. Similarly,
this categorization does not mean that online learning approaches must be
used only for situations where new training examples arrive separately, oneby-one. Online learning approaches can process each training example of a
chunk separately. They can be used for applications where training examples
arrive in chunks.
Finally, considering diﬀerent strategies for re-constructing ensemble component classiﬁers and aggregating their predictions, one can recall Kuncheva’s
categorization , where she has distinguished the following four basic
strategies:
• Dynamic combiners – component classiﬁers are learnt in advance and
are not further updated; the ensemble adapts by changing the combination phase (usually by tuning the classiﬁer weights inside the voting
rule, e.g., the level of contribution to the ﬁnal decision is directly proportional to the relevance ). The drawback of this approach
is that all contexts must be available in advance; emergence of new
unknown contexts may result in a lack of experts.
• Updating training data – recent training examples are used to onlineupdate component classiﬁers (e.g. in on-line bagging or its further
generalizations ).
• Updating ensemble members – updating online or retraining in batch
mode (using chunks) .
• Structural changes of the ensemble – replacing the worst performing
classiﬁers in the ensemble and adding a new component, e.g., individual
classiﬁers are evaluated dynamically and the worst one is replaced by
a new one trained on the most recent data 
In this paper, the main criterion used to categorize classiﬁcation ensemble approaches is the data processing method, i.e., whether examples are
processed in chunks or one-by-one. Then, as the second criterion we use information on whether the approaches are designed to deal with stationary or
non-stationary data streams. We consider these two criteria ﬁrst because approaches within each of these categories tackle diﬀerent types of data stream
applications. Within each of these categories, we will then use further criteria
to distinguish among existing approaches.
Section 4.1.1 presents chunk-based ensemble approaches for stationary
environments, section 4.1.2 presents online learning approaches for stationary environments, section 4.1.3 presents chunk-based ensemble approaches
for non-stationary environments, and section 4.1.4 presents online learning
approaches for non-stationary environments.
4.1.1. Chunk-Based Ensembles for Stationary Streams
Chunk-based ensembles for stationary data streams are not so well developed as online versions and did not receive so signiﬁcant attention from the
research community. They are also related to the issue of batch processing
of larger sets of data, and often do not explicitly refer to this as stream mining. This section reviews the most popular methods in this area. They are
summarized in Table 1.
Learn++ is one of the most well recognized approaches to stationary
streams . This ensemble constructs new neural network models on each
Table 1: Chunk-based ensembles for stationary data streams.
Description
Learn++ 
Incremental neural network ensemble
Ada.Boost RAN-LTM 
Combination of AdaBoost.M1 and RAN-LTM classiﬁer
Growing NCL 
Incremental version of the Negative Correlation Learning
Bagging++ 
Training classiﬁers with Bagging from incoming chunks of data
incoming chunk of data, and then combines their outputs using majority
voting. This allows to accommodate new incoming instances into the ensemble. This approach however retains all previously learned classiﬁers, thus
being ineﬃcient for handling massive datasets as the size of the ensemble
continuously grows.
Kidera et al. proposed a combination of AdaBoost.M1 and Resource
Allocating Network with Long-Term Memory, a stable neural network classiﬁer for incremental learning. They used a predetermined number of base
classiﬁers for the entire stream processing and incrementally updated them
with new chunks. They suppressed the forgetting factor in these classiﬁers
in order to allow an eﬃcient weight approximation for weighted voting combination. This however limits the usability of this approach for potentially
unbounded streams.
Minku et al. introduced an incremental version of Negative Correlation Learning that aimed at co-training an ensemble of mutually diverse
and individually accurate neural networks. At the same time their proposed
learning scheme allowed to maintain a trade-oﬀbetween the forgetting rate
and adapting to new incoming data. Two models were discussed: ﬁxed size
and growing size, diﬀering in their approach to maintaining the ensemble
set-up. Experimental results showed that the ﬁxed size approach has better
generalization ability, while the growing size may easily overcome the impact
of too strong forgetting.
Bagging++ was developed as an improvement over Learn++ by
utilizing Bagging to construct new models from incoming chunks of data.
Additionally, the ensemble consisted of heterogeneous classiﬁers selected from
a set of four diﬀerent base classiﬁers. Authors showed that their approach
gives comparable results to Learn++ and Negative Correlation Learning,
while being signiﬁcantly faster.
4.1.2. Online Ensembles for Stationary Streams
Online ensembles for stationary data streams have gained signiﬁcantly
more attention than their chunk-based counterparts. This was caused by
a general popularity of online learning and its application to various reallife scenarios, not only limited to streaming data. Let us review the most
representative proposals in this area. They are summarized in Table 2.
Table 2: Online ensembles for stationary data streams.
Description
Bagging-based
OzaBag 
Online Bagging
Ensemble of adaptive-size Hoeﬀding trees
LevBag 
Leveraging Bagging with increased resampling and output detection codes
ORF 
Online Random Forest
Online Mondrian Forest
Boosting-based
OzaBoost 
Online Boosting
Ultra fast forest of binary trees
Hoeﬀding Option Trees
EOS-ELM 
Ensemble of online extreme learning machines
Oza and Russel introduced Online Bagging, which alleviates the
limitations of standard Bagging of requiring the entire training set available
beforehand for learning. They assumed that, in online learning, each new incoming instance may be replicated zero, one or many times during the update
process of each base classiﬁer. Thus each classiﬁer in the ensemble is updated
with k copies of the newly arrived instance. The value of k is selected on the
basis of Poisson distribution, where k ∼Poisson(1). This comes from the
fact that for potentially unbounded data streams the binominal distribution
of k in standard Bagging tends to this speciﬁc Poisson distribution. Theoretical foundations of this approach were further developed by Lee and Clyde
 . They proposed a Bayesian Online Bagging that was equivalent to the
batch Bayesian version. By combining it with a lossless learning algorithm,
they obtained a lossless online bagging approach.
Bifet et al. introduced two modiﬁcations of Oza’s algorithm called Adaptive-
Size Hoeﬀding Trees (ASHT) and Leveraging Bagging , which aim
at adding more randomization to the input and output of the base classi-
ﬁers. ASHT synchronously grows trees of diﬀerent sizes, whereas Leveraging
Bagging increases resampling from Poisson(1) to Poisson(λ) (where λ is a
user-deﬁned parameter) and uses output detection codes .
Another online ensemble developed by Oza and Russel is Online Boosting . This ensemble maintains a ﬁxed size set of classiﬁers trained on
the examples received so far. Each new example is used to update each of
the classiﬁers in a sequential manner. Examples misclassiﬁed by the former
classiﬁers in the sequence have their weights updated so as to be emphasized
by the latter classiﬁers. This is done in the following way. For each new incoming example, one initially assigns the highest possible weight λ = 1 to it.
The ﬁrst classiﬁer in the pool is updated with this example k = Poisson(λ)
times. After the update, this classiﬁer is used to predict this example, and
the weighted overall fraction ϵ of examples that it misclassiﬁed is updated.
If the example is correctly classiﬁes the example, the example’s weight λ is
multiplied by
2(1−ϵ). If this classiﬁer misclassiﬁed the example, we multiply
the weight associated to this example by 1
2ϵ. This procedure is then repeated
for the next classiﬁer in the pool, but using the new weight λ.
Several researchers developed ensembles based on a combination of decision trees. Hoeﬀding Option Trees (HOT) can be seen as an extension of
Kirkby’s Option Tree . It allows each training example to update a set
of option nodes rather than just a single leaf. It provides a compact structure
that works like a set of weighted classiﬁers, and just like regular Hoeﬀding
Trees, they are built in an incremental way – for a more detailed algorithm
refer to its description in .
Ultra Fast Forest of Trees, developed by Gama and Medas , uses an
ensemble of Hoeﬀding trees for online learning. Their split criterion is applicable only to binary classiﬁcation tasks. To handle multi-class problems,
a binary decomposition is applied.
A binary tree is constructed for each
possible pair of classes. When a new instance arrives, each classiﬁer is updated only if the true class label for this instance is used by the binary base
classiﬁer.
Ensemble of Online Extreme Learning Machines was developed by
Lan et al. It is a simple combination of online randomized neural networks,
where initial diversity of the pool is achieved by a randomized training procedure. Base models are combined using averaging of individual outputs.
Each base model is updated with the incoming instances, but no discussion
of veriﬁcation of how the diversity in the ensemble is maintained during the
course of stream processing was given.
Some other researchers focused their work on proposing online versions
of the popular Random Forest algorithm . They introduced online
Random Trees that generate test functions and thresholds at random and
select the best one according to a quality measure.
Their online update
methodology is based on the idea of generating a new tree having only one
root node with a set of randomly selected tests. Two statistics are calculated
online: minimum number of instances before split and minimum gain to be
achieved. When a split occurs statistics regarding the instances that will
fall into left and right node splits are propagated into children nodes, thus
they start already with the knowledge of their parent node. Although the
authors acknowledge the existence of the Hoeﬀding bound, they argue that
using online updated gain is closer to the real idea behind decision trees.
Additionally, a forgetting mechanism via temporal knowledge weighting is
applied to reduce the inﬂuence of old instances. This is realized as pruning
random trees, where a classiﬁer is discarded from the ensemble based on its
out-of-bag error and the time its age (time spend in the ensemble).
This idea was further developed by Lakshminarayanan et al. into online
Mondrian Forest algorithm . They used Mondrian processes for their
tree induction scheme, which are a family of random binary partitions. As
they were originally introduced as inﬁnite structures, the authors modiﬁed
them into ﬁnite Mondrian trees. The main diﬀerences between this approach
and standard decision trees are the independence of splits from class labels,
usage of split time at every node, introduction of parameter controlling dynamically the number of nodes and that the slit is bounded by the training
data and is not generalized over the entire feature space. The ensemble is
constructed identically as in standard Random Forest, but another diﬀerence lies in online update procedure. Mondrian trees can accommodate new
instances by creating a new split that will be on higher level of tree hierarchy than existing ones, extending the existing split, or splitting the existing
leaf into children nodes. Please note that standard online Random Forest
can only update their structure using the third of mentioned methods. This
makes Mondrian Forests much more adaptable to streaming data, allowing
for more in-depth modiﬁcations in ensemble structure. The authors report
that their method outperforms existing online Random Forests, achieves accuracy similar to batch versions and is at least an order of magnitude faster
than reference ensembles.
4.1.3. Chunk-Based Ensembles for Non-Stationary Streams
Chunk-based approaches for non-stationary environments usually adapt
to concept drifts by creating new component (a.k.a. base) classiﬁers from
new chunks (blocks or batches) of training examples. In general, component
classiﬁers of the ensemble are constructed from chunks which correspond to
diﬀerent parts of the stream.
Therefore, the ensemble may represent a
mixture of diﬀerent distributions (concepts) that have been present in the
data stream. Learning a new component from the most recent chunk is also
a natural way of adaptating to drifts . Additionally, some chunk-based
ensembles maintain an additional buﬀer for storing old classiﬁers that can
be reused when needed, oﬀering a potential to handle recurring concepts.
Learning component classiﬁers from complete chunks enables applying
standard, batch learning algorithms. Forgetting of old classiﬁcation knowledge can be done by eliminating too poorly performing components. This
oﬀers a way to limit the amount of memory required to store the ensemble,
even though it impedes the ensemble of recovering deleted classiﬁers if and
when their corresponding concept reoccurs.
Most of the chunk-based ensembles periodically evaluate their components with the newest chunk.
The results of this evaluation are used to
update weights associated to each component classiﬁer. These weights can
be used to emphasise the classiﬁers that best reﬂect the most recent data distribution when making an ensemble prediction, or to decide which unhelpful
classiﬁers should be discarded.
One of the main features to distinguish between diﬀerent chunk-based ensembles for non-stationary environments is whether or not they always create
new classiﬁers for each new chunk of data in order to deal with concept drift.
So, we discuss these approaches under this perspective below.
algorithms are summarized in Table 3.
Typical Chunk-based Approaches.
Typically, chunk-based ensembles are constructed according to the following schema:
1. For each new chunk Bi ∈S, evaluate component classiﬁers Cj in the
ensemble with respect to a given evaluation measure Q(Cj);
2. Learn a new candidate classiﬁer Cc using Bi;
3. Add Cc to the ensemble if the ensemble size is not exceeded; otherwise
replace one of the existing components of the ensemble.
Table 3: Chunk-based ensembles for non-stationary data streams.
Description
Typical approaches
Streaming Ensemble Algorithm
Accuracy Weighted Ensemble
Aboost 
Adaptive, fast and light Boosting
Learn++.NSE 
Learn++ for non-stationary environments
Alternative approaches
Boosting-like method using knowledge-based sampling
Accuracy Updated Ensemble
Weighted Aging Ensemble
Batch Weighted Ensemble
Ensemble tracking for recurring concepts
Each of these approaches implements a diﬀerent strategy to restrict the ensemble size and to weight diﬀerent classiﬁers in the ensemble.
As a new classiﬁer is always created to learn each new data chunk, the size
of the chunk plays a particularly important role. A too large chunk size would
result in slow adaptation to drifts. On the other hand, a too small chunk size
would not be enough to learn an entire stable concept well, would increase
computational costs, and may result in poor classiﬁcation performance .
One of the earliest well known approaches in this category is the Streaming Ensemble Algorithm (SEA), proposed by Street and Kim . This
approach creates a new classiﬁer to learn each new chunk of training data.
If the maximum ensemble size has not been reached yet, this new classiﬁer
is simply added to the ensemble. Otherwise, the quality of the new classiﬁer
is ﬁrst evaluated based on the next incoming training chunk. Then, the new
classiﬁer replaces an existing classiﬁer whose quality is worse than the quality of the new classiﬁer on this training chunk. One of the key features for
the success of this approach is its quality measure. It favours the classiﬁers
which correctly classify examples that are nearly undecided by the ensemble.
In this way, this approach can avoid overﬁtting and maintain diversity. The
predictions given by the ensemble are based on the majority voting. This
approach has been shown to recover faster from concept drift than single
classiﬁers. One of its potential problems is that old classiﬁers can outweigh
the new classiﬁer, potentially slowing down adaptation to new concepts. How
fast the ensemble can recover from drifts depends not only on the chunk size,
but also on the ensemble size.
A similar way of restructuring an ensemble was proposed by Wang et al.
as the algorithm called Accuracy Weighted Ensemble (AWE) .
key idea of AWE is to assign weights to each classiﬁer of the ensemble based
on their prediction error on the newest training chunk. A special variant of
the mean square error (which allows to deal with probabilities of a component
classiﬁer predictions) is used for that purpose. The assumption made by this
approach is that the newest training chunk is likely to represent the current
test examples better. Classiﬁers that have equal or worse performance than a
random classiﬁer (in terms of their mean square errors) are discarded. Pruning can also be applied to maintain only the K classiﬁers with the highest
weights. In this way, it is possible to remove classiﬁers that would hinder
the predictions and include new classiﬁers that can learn the new concepts.
For cost-sensitive applications, it is also possible to use instance-based dynamic ensemble pruning . This approach was shown to be successful in
achieving better accuracy than single classiﬁers when the ensemble size becomes large enough (i.e., after enough data chunks are received).
as noticed in , the AWE’s pruning strategy may sometimes delete too
many component classiﬁers in the case of certain sudden drifts and decrease
too much of AWE’s classiﬁcation accuracy. Another problem concerns the
evaluation of the new candidate classiﬁer – it requires k-fold cross-validation
inside the latest chunk, which increases computational time.
Chu and Zaniolo proposed a chunk-based approach inspired by the
boosting framework. When a training chunk is received, the ensemble error is
calculated. After that, a mechanism based on statistical tests is used to detect
concept drifts. If a concept drift is detected, all the classiﬁers composing
the ensemble are deleted. After the concept drift detection mechanism is
applied (and the possible deletion of ensemble members), a new classiﬁer is
created to learn the training chunk. The training examples of the chunk
are associated to weights determined in an AdaBoost way based on the the
ensemble error.
If the ensemble error on the current chunk is e and the
example i is misclassiﬁed, then this example’s weight is set to wi = (1−e)/e.
If the example was correctly classiﬁed, its weight is maintained as 1. If the
inclusion of the new classiﬁer makes the ensemble exceed the maximum size
M, the oldest ensemble member is eliminated.
The classiﬁcation is done
by averaging the probabilities predicted by the classiﬁers and selecting the
class with the highest probability. This approach was shown to be able to
improve predictive performance in comparison to previous approaches such
as SEA and Wang et al.’s in the presence of concept drift. A
potential problem of this approach is that it resets the whole ensemble upon
drift detection. This strategy can be sensitive to false alarms (false positive
drift detections) and is unable to deal with recurring concepts.
Another approach inspired by the boosting framework is Elwell and Polikar’s generalization of Learn++ for Non-Stationary Environments (called
Learn++.NSE) . This approach also sets the weights of the training examples from a new data chunk based on the ensemble error on this chunk.
If an example i is misclassiﬁed, its weight is set to wi = 1/e. Otherwise,
it is set to 1. One of the main diﬀerences between this approach and Chu
and Zaniolo’s is that it does not use a concept drift detection mechanism. Instead, reaction to drifts is based on weights associated to each base
classiﬁer. These weights are higher when the corresponding base classiﬁer is
able to correctly classify examples that were missclassiﬁed by the ensemble.
Weights are lower if the corresponding base classiﬁer misclassiﬁes examples
that were correctly classiﬁed by the ensemble. Weights are also set to give
more importance to the misclassiﬁcations on more recent data chunks, which
are believed to represent the current concept better. The predictions given
by the ensemble are based on weighted majority voting.
Therefore, base
classiﬁers that were poorly performing for some period of time can be automatically re-emphasised through their weights once they become useful.
The fact that base classiﬁers are not deleted can help dealing with recurrent
drifts. However, as the ensemble size is unlimited and a new base classiﬁer is
added for every new data chunk, the number of base classiﬁers may become
Alternative Chunk-Based Approaches.
Chunk-based ensembles are typically quite sensitive to a proper tuning of
the size of the data chunk. In particular, a too large chunk size may delay
reaction to drifts, while a too small chunk size may lead to poorly performing
base classiﬁers. Moreover, learning every new data chunk may introduce a
learning overhead that could be unnecessary when existing classiﬁers are
considered good enough for the current concept. Some researchers proposed
approaches that deviate from the typical chunk-based learning schema in an
attempt to overcome some of these issues. We discuss some representative
approaches in this section.
Scholz and Klinkenberg’s approach decides, for each new training chunk, whether to train a new classiﬁer or update the newest existing
classiﬁer with it. This decision is based on the accuracy resulting from training the most recent classiﬁer with the new chunk in comparison with the
accuracy obtained by training a new classiﬁer on the new chunk. Only the
best between these two classiﬁers is kept.
This strategy may reduce the
problem of creating poor base classiﬁers due to small chunk sizes, because
existing classiﬁers can be trained with more than one chunk. Besides assigning weights to the examples within a training chunk in a boosting-like style,
each classiﬁer itself also has a weight, which is assigned depending on its
performance on the new training chunk. These weights are not only used to
speed up reaction to concept drifts, but also to prune unhelpful classiﬁers.
This approach has been shown to perform well in comparison to previous
approaches such as adaptive window size and batch selection .
However, it did not perform so well when the drift consisted of an abrupt
concept drift quickly followed by a change back to the previous concept.
Deckert proposed an ensemble approach that uses a concept drift
detection method to decide whether a new classiﬁer should be created to
learn a new data chunk, or whether the new data chunk should be discarded
without further training.
Another alternative chunk-based approach is the Accuracy Updated Ensemble (AUE) . In this ensemble, all component classiﬁers are incrementally updated with a portion of the examples from the new chunk.
This may help reducing the problems associated to creating poor base classi-
ﬁers due to small chunk sizes. Another novelty includes weighting classiﬁers
with non-linear error functions, which better promotes more accurate components. Moreover, the newest candidate classiﬁer always receives the highest
weight, as it should reﬂect the most recent data distribution better. AUE also
contains other techniques for improving pruning of ensembles and achieving
better computational costs. The experimental studies showed that AUE
constructed with Hoeﬀding Trees obtained higher classiﬁcation accuracy than
other chunk ensembles in scenarios with various types of drifts as well as in
stable streams.
Yet another approach to rebuilding a chunk-based ensemble was presented
by Wozniak et al. Weighted Aging Ensemble (WAE) modiﬁes the classiﬁer
ensemble line-up on the basis of their diversity. The ensemble prediction is
made according to the weighted majority voting, where the weight of a given
classiﬁer depends on its accuracy and time spent inside an ensemble .
A number of approaches have been discussed in the literature to speciﬁcally tackle recurring concepts in data streams. Ramamurthy and Bhatnagar
 proposed an ensemble tracking approach that tries to deal with recurring concepts explicitly. It maintains a global set of classiﬁers representing
diﬀerent concepts. Whenever a new training chunk is available, the error of
each classiﬁer on it is determined. MaxMSE is deﬁned as the classiﬁcation
error of a classiﬁer that predicts randomly. If at least one classiﬁer has error lower than a pre-deﬁned value τ, or if the error of the weighted ensemble
formed by all classiﬁers with error lower than AcceptanceFactor∗MaxMSE
is lower than τ, no new classiﬁer is created. This reduces the overhead associated to learning every new data chunk. If neither a single classiﬁer nor the
above mentioned ensemble have error lower than τ, a new classiﬁer is created
and trained with the new data chunk, which is assumed to represent a new
concept. One of the problems of this approach is that it has no strategy to
limit the size of the global set of classiﬁers.
Another approach for storing the special deﬁnitions of previous concepts
has been considered by Katakis et al.
in their ensemble with conceptual
clusters calculated and compared for each data chunk . Jackowski 
described an evolutionary approach for selecting and weighting classiﬁers
for the ensemble in the presence of recurrent drifts, while Sobolewski and
Wozniak used the idea of the recurring concepts to generate a pool of artiﬁcial
models and select the best ﬁtted in the case of concept drift .
4.1.4. Online Ensembles for Non-Stationary Streams
Online ensembles learn each incoming training example separately, rather
than in chunks, and then discard it. By doing so, these approaches are able
to learn the data stream in one pass, potentially being faster and requiring
less memory than chunk-based approaches. These approaches also avoid the
need for selecting an appropriate chunk size. This may reduce the problems
associated with poor base models resulting from small chunk sizes, even
though these approaches would still normally have other parameters aﬀecting
the speed of reaction to drifts (e.g., parameters related to sliding windows
and fading factors).
One of the main features to distinguish between diﬀerent online ensemble
learning approaches for non-stationary environments is the use of concept
drift detection methods. So, they are divided into passive or active categories.
Presented algorithms are summarized in Table 4.
Passive Approaches.
Passive approaches are approaches which do not use explicit concept drift
Table 4: Online ensembles for non-stationary data streams.
Description
Passive approaches
Dynamic Weighted Majority
AddExp 
Addictive expert ensembles for classiﬁcation
Horse racing ensembles
Concept Drift Committee
Online Accuracy Updated Ensemble
Ensemble of classiﬁers using overlapping windows
ADACC 
Anticipative Dynamic Adaptation to Concept Changes
Active approaches
Adaptive Classiﬁers-Ensemble
Todi 
Two Online Classiﬁers For Learning And Detecting Concept Drift
Diversity for Dealing with Drifts
ADWINBagging 
Online Bagging with ADWIN drift detector
detection methods. Diﬀerent passive online ensembles have diﬀerent strategies to assign weights to classiﬁers, as well as to decide when to add or
remove classiﬁers from the ensemble in order to react to potential concept
drifts. Most of these approaches present mechanisms to continuously adapt
to concept drifts that may occur in the stream.
How fast adaptation is
achieved and how sensitive this adaptation is to noise usually depends on
parameters.
One of the most well known approaches under this category is Dynamic
Weighted Majority (DWM) , proposed by Kolter and Maloof. In this
approach, each classiﬁer has a weight that is reduced by a multiplicative constant β (0 ≤β < 1) when it makes a wrong prediction, similar to Littlestone
and Warmuth’s Weighted Majority Algorithm . This allows the ensemble to emphasize the classiﬁers that are likely to be most accurate at a given
point in time. All classiﬁers are incrementally trained on the incoming training examples. In addition, in order to accelerate reaction to concept drift,
it is possible to add a new classiﬁer or remove existing classiﬁers. New classiﬁers are added when the ensemble misclassiﬁes a given training example.
They can learn potentially new concepts from scratch, avoiding the need for
existing classiﬁers to forget their old knowledge when there is concept drift.
Classiﬁers whose weights are too low are classiﬁers that have been unhelpful
for a long period of time. They can be deleted to avoid the ensemble becom-
ing too large. The weight updates and the addition and removal of classiﬁers
are performed only at every p time steps, where p is a pre-deﬁned value.
Larger values of p are likely to be more robust against noise. However, too
large p values can result in slow adaptation to concept drift. At every p training examples, the weights of all ensemble members are also normalized, so
that the new member to be included does not dominate the decision-making
of all the others. DWM has demonstrated to achieve good performance in
the presence of concept drifts , usually achieving similar performance
to an approach with perfect forgetting.
However, it may not perform so
well as Littlestone and Warmuth’s Weighted Majority Algorithm under
stationary conditions.
Addictive Expert Ensembles (AddExp) is a method similar to DWM
 . The main motivation for this method is the fact that it allows the
deﬁnition of mistake and loss bounds. In this method, the parameter p is
eliminated, so that weight updates happen whenever a base classiﬁer misclassiﬁes a new training example. A new classiﬁer is always added when the
prediction of the ensemble as a whole is wrong. When combined with a strategy to prune the oldest classiﬁers once a maximum pre-deﬁned ensemble size
if reached, the bounds are deﬁned in the same way as when no pruning of
classiﬁers is performed. However, eliminating the oldest classiﬁers may not
be a good strategy to deal with non-stationary environments, as old classi-
ﬁers may still be very useful. The alternative strategy of pruning the lowest
weight classiﬁers is more practical, but oﬀers no theoretical guarantees.
Other approaches to combine online classiﬁers are also considered in
Hedge β or Winnow algorithm .
Kuncheva called them “horse racing” ensembles . For instance, Hedge β works in a similar way to the
Weighted Majority Algorithm, but instead of using an aggregating rule it selects one component classiﬁer based on the probability distribution obtained
by normalized weights to represent the ﬁnal ensemble prediction. Winnow
also follows the main schema of Weighted Majority Algorithm, but uses different updating and calculating weights ideas.
Another example of passive online learning ensemble approach for nonstationary environments is Stanley’s Concept Drift Committee (CDC) .
As with DWM and AddExp, all classiﬁers that compose the ensemble are
trained on the incoming training examples. Instead of multiplying the weights
of the classiﬁers by a constant β upon misclassiﬁcations, CDC uses weights
that are proportional to the classiﬁer’s accuracy on the last n training examples. A new classiﬁer is added whenever a new training example becomes
available, rather than only when the ensemble misclassiﬁes the current training example. When a maximum pre-deﬁned ensemble size is reached, a new
classiﬁer is added only if an existing one can be eliminated. A classiﬁer can
be deleted if its weight is below a pre-deﬁned threshold t and its age (number of time steps since its creation) is higher than a pre-deﬁned maturity
age. Imature classiﬁers do not contribute to the ensemble’s prediction. This
gives them a chance to learn the concept without hindering the ensemble’s
generalization. This approach was shown to achieve comparable or better
performance than previous approaches such as FLORA4 and instancebased learning 3 (IB3) in the presence of concept drifts, but sometimes
presented worse performance than FLORA4 before the drifts.
Yet another idea has been used in Online Accuracy Updated Ensemble
(OAUE) . It inherits some positive solutions coming from its hybrid preceder AUE, like incremental updating of component classiﬁers and learning
new classiﬁers at some time steps. However, to more eﬃciently process incoming single examples and weight component classiﬁers, the new proposal of
a cost-eﬀective function was introduced. It achieves a good trade-oﬀbetween
predictive accuracy, memory usage and processing time.
The WWH algorithm from Yoshida et al. builds diﬀerent component classiﬁers on overlapping windows to select the best learning examples
and aggregates component predictions similarly to the Weighted Majority
Algorithm. Therefore, WWH can be seen as a combination of an instance
selection windowing technique with an adaptive ensemble.
Quite recently, Jaber proposed the Anticipative Dynamic Adaptation to
Concept Changes (ADACC) ensemble, which attempts to optimize control
over the online classiﬁers by recognizing concepts in incoming examples .
Active Approaches.
Even though active online ensemble approaches are not so common as
passive ones, there are a few approaches in this category. One of the advantages of using explicit drift detection methods is the possibility to inform
practitioners of the existence of concept drifts. The use of concept drift detectors can also help approaches to swiftly react to concept drifts once they
are discovered. However, if concept drift detectors fail to detect drifts, these
approaches will be unable to react to drifts. Concept drift detectors may also
present false alarms, i.e., false positive drift detections. Therefore, it is important for active ensemble approaches to implement mechanisms to achieve
robustness against false alarms.
An example of active online ensemble is the Adaptive Classiﬁers-Ensemble
(ACE) . This approach uses both an online classiﬁer to learn new training examples and batch classiﬁers trained on old examples stored in a buﬀer.
The batch classiﬁers are used not only to make predictions, but also to detect
concept drifts. ACE considers that there is a concept drift if the accuracy
of the most accurate batch classiﬁer over the last W examples is outside the
conﬁdence interval formed by its accuracy over the W examples preceding
the last W examples. Whenever a concept drift is detected or the maximum
number of training examples to be stored in the buﬀer is attained, a new
batch classiﬁer is trained with the stored examples and both the online classiﬁer and the buﬀer are reset. A pruning method is used to limit the number
of batch classiﬁers used. This pruning method removes older classiﬁers ﬁrst,
unless they present the highest predictive accuracy over a long period of time.
In that way, the approach can use old knowledge when there are recurring
concepts. The classiﬁcation is done by weighted majority vote. The weight
is based on the accuracy on the most recent W training examples, and it
is zero if this accuracy is equal to or lower than the lower endpoint of the
accuracy conﬁdence interval. As the size of the buﬀer of stored examples is
independent of the size of the sliding window W, ACE can respond to sudden changes even if the buﬀer is large. However, determining the size W of
the sliding window may not be easy. ACE also requires storage of examples
in an incremental way to create the batch classiﬁers, but this issue can be
easily overcome by replacing the buﬀer by an online learning algorithm. A
comparative experiment of ACE against other ensembles has been presented
Two Online Classiﬁers For Learning And Detecting Concept Drift (Todi)
 uses two online classiﬁers to detect concept drift. One of them (H0) is
rebuilt every time a drift is detected. The other one (H1) is not rebuilt when
a drift is detected, but can be replaced by the current H0 if a detected drift
is conﬁrmed. Todi detects concept drift by performing a statistical test of
equal proportions to compare H0’s accuracies on the most recent W training
examples and on all the training examples presented so far excluding the last
W training examples. After the detection of a concept drift, a statistical test
of equal proportions with signiﬁcance level β is done to compare the number
of correctly classiﬁed training examples by H0 and H1 since the beginning of
the training of H0. If statistical signiﬁcant diﬀerence is detected, this means
that H0 was successful to handle concept drift, and the drift is conﬁrmed.
H0 then replaces H1 and is rebuilt. The classiﬁcation is done by selecting
the output of the most accurate classiﬁer considering the W most recent
training examples. This strategy makes the approach more robust to false
alarms than approaches that reset the learning system upon drift detection
 . However, no strategy is adopted to accelerate the learning of a new
concept, as the new concept has to be learnt from scratch.
Another example of active online ensemble learning approach in this category is Diversity for Dealing with Drifts (DDD) . DDD is based on the
observation that very highly diverse ensembles (whose base classiﬁers produce
very diﬀerent predictions from each other) are likely to have poor predictive
performance under stationary conditions, but may become useful when there
are concept drifts. So, in the mode prior to drift detection, DDD maintains
both a low diversity ensemble and a high diversity ensemble. The low diversity ensemble is used for learning and for making predictions. The high
diversity ensemble is used for learning and is only activated for predictions
upon drift detection. This is because this ensemble is unlikely to perform
well under stationary conditions. Concept drifts can be detected by using
existing methods from the literature. Once a concept drift is detected, the
approach shifts to the mode after drift detection, where it activates both the
low and high diversity ensembles and creates new low and high diversity ensembles to start learning the new concept from scratch. The prediction given
by DDD is then set to the weighted majority vote of the predictions given
by its ensembles, except for the new high diversity ensemble. The weight of
each ensemble is proportional to its prequential accuracy since drift detection. This approach manages to achieve robustness to diﬀerent types of drift
and to false alarms, because the diﬀerent ensembles are most adequate for
diﬀerent situations. However, the use of more than one ensemble can make
this approach heavier for applications with very strict time constraints.
Modiﬁcations of the architecture of tree ensembles with drift detectors
have also been considered by Bifet at al. . The ADWIN change detector
has been used to reset ensemble members when their predictive accuracy
degrades signiﬁcantly. This makes it possible to better deal with evolving
data streams. The same ADWIN method may also be integrated with online
bagging ensemble – see ADWINBagging .
4.2. Supervised Learning for Regression Problems
Regression analysis is a technique for estimating a functional relationship
between a numeric dependent variable and a set of independent variables. It
has been widely studied in statistics, pattern recognition, machine learning
and data mining. Many ensemble methods can be found in the literature
for solving classiﬁcation tasks on streams, but only a few exist for regression
tasks. Discussed algorithms are summarized in Table 5.
Table 5: Ensembles for regression from data streams.
Description
OzaBag 
Online Bagging for regression
OzaBoost 
Online Boosting for regression
AddExp 
Addictive expert ensembles for regression
ILLSA 
Incremental local learning soft sensing algorithm
eFIMT-DD 
Ensembles of any-time model trees
AMRules 
Ensemble of randomized adaptive model rules
iSOUP-Tree-MTR 
Ensembles of global and local trees
Dynamic cross-company learning
Dycom 
Dynamic cross-company mapped model learning
LGPC 
Lazy Gaussian Process committee
Online weighted ensemble of regressor models
DOER 
Dynamic and on-line ensemble regression
Oza and Russel’s online bagging algorithm for stationary data streams
 described in section 4.1.2 is an example of method that is inherently
applicable both to classiﬁcation and regression.
Kolter and Maloof’s Addictive Expert Ensembles (AddExp) for nonstationary data streams also contains another version for continuous dependent variables . As in the AddExp for classiﬁcation problems, a weight
is associated to each base learner. For classiﬁcation, AddExp makes predictions by using weighted majority vote, while for regression, weighted average
In the version for classiﬁcation, the weight associated to a base
classiﬁer is multiplied by β, 0 ≤β < 1, whenever it misclassiﬁes a training
example. In the version for regression, the weight of a base learner is always
multiplied by β|ˆy−y|, where ˆy is the prediction given by the base learner is y
is the actual value of the dependent variable.
Kadlec and Gabrys developed an incremental local learning soft sensing
algorithm (ILLSA) , operating in two phases. During the initial phase a
number of base models is being trained, each using diﬀerent concepts (subsets) of the training data.
During the online data stream mining phase,
weights assigned to models are recalculated instance-by-instance using their
proposed Bayesian framework working on output posterior probabilities.
The most in depth study on learning ensembles of model trees from data
streams appears in . These research include two diﬀerent methods
for online learning of tree-based ensembles for regression from data streams.
Both methods are implemented on the top of single model trees induced
using the FIMT-DD algorithm (a special incremental algorithm for learning
any-time model trees from evolving data streams).
Then, the ensembles
of model trees are induced by the online bagging algorithm and consist of
model trees learned with the original FIMT-DD algorithm and a randomized
version named R-FIMT-DD. Authors explore the idea of randomizing the
learning process through diversiﬁcation of the input space and the search
trajectory and examine the validity of the statistical reasoning behind the
idea for aggregating multiple predictions.
It is expected that this would
bring the resulting model closer to the optimal or best hypothesis, instead
of relying only on the success of a greedy search strategy in a constrained
hypothesis space. The authors also perform a comparison with respect to
the improvements that an option tree brings to the learning process.
In , the authors observe that the use of options acts as a kind of
backtrack past selection decisions. Their empirical comparison has shown
that the best tree found within the option tree has a better accuracy (on most
of the problems) than the single tree learned by FIMT-DD. The increased
predictive performance and stability comes at the cost of a small increase of
the processing time per example and a controllable increase in the allocation
of memory.
The increase in the computational complexity is due to the
increased number of internal nodes being evaluated at any given point in time.
The option tree incurs an additional increase in computational complexity
when computing the aggregate of the multiple predictions for a single testing
example, as it has to examine all of the options on the path from the root to
the corresponding leaf node.
Adaptive Model Rules is the ﬁrst streaming rule learning algorithm
for regression problems. It extends AMRules algorithm by using random
rules from data streams. Several sets of rules are being generated. Each rule
set is associated with a set of Natt attributes. These attributes are selected
randomly from the full set of attributes of the dataset. The new algorithm
improves the performance of the previous version.
Osojnik et al. investigated ensembles of local and global trees for
multi-target regression from data streams.
Authors argued that predicting all target at once is more beneﬁcial to mining streams than using local
models. A novel global method was proposed, named incremental Structured
Output Prediction Tree for Multi-target Regression (iSOUP-Tree-MTR). For
improving the predictive power, the authors used it as a base learner for Oza’s
Online Bagging.
An approach called Dynamic Cross-company Learning (DCL) has
been proposed to perform transfer learning for data streams in non-stationary
environments. The approach aims at making predictions in the context of
a given target company or organization. A data stream containing training
examples from this company or organization is available, but produces few
examples over time. This can happen, for example, when it is expensive
to collect labeled examples in the context of a given company. Therefore,
this approach maintains not only a base learner to learn such examples, but
also other base learners to learn examples obtained from other companies or
organizations. A weight is associated to each base learner. This weight is
multiplied by β, 0 ≤β < 1, whenever this base learner is not the one that
provided the best prediction to a new target company / organization training
example. So, these weights can be used to emphasize the base learners that
currently best reﬂect the present concept of the target company / organization. The prediction given by the ensemble is the weighted average of the
predictions given by the base learners.
Another approach called Dynamic Cross-company Mapped Model Learning (Dycom) extends DCL to learn linear functions to map the base
learners created with data from other companies or organizations to the current concept of the target company or organization. These mapping functions
are trained based on a simple algorithm that uses training examples from the
target company / organization data stream and the predictions given to these
examples by the base learners representing other companies / organizations.
This algorithm operates in an online manner and gives more importance to
more recent training examples, so that the mapping functions represent the
current concept of the companies / organizations. It is expected to enable a
reduction in the number of training examples required from the target company while keeping a similar predictive performance to DCL. This is because
it can beneﬁt from all base learners by mapping them to the concept of the
target company, rather than beneﬁting only from base learners that currently
best represent the concept of the target company.
Xiao and Eckert proposed an approximation of Gaussian processes
for online regression tasks. They combined several base models, each being
initialized with random parameters. Each incoming instance is used to update a selected subset of base models that are being chosen using a reedy
subset selection, realizing an optimization of a submodular function. The
authors showed that their method displays favorable results in terms of error
reduction and computational complexity, however used only methods based
on Gaussian processes as a reference.
On-line Weighted Ensemble (OWE) of regressor models was discussed by
Soares and Araujo . It was designed to handle various types of concept
drift, including recurrent ones. The ensemble model is based on a sliding
window that allows to incorporate new samples and remove redundant ones.
A boosting-like solution is used for weight calculation of ensemble models, by
measuring their error on the current window. Additionally, contribution of
old windows can be taken into consideration during weight calculation, thus
allowing for switching between recurring and non-recurring environments.
Finally, OWE can expand its structure by adding new model when the ensemble error is increasing and pruning models characterized by highest loss
of accuracy.
This concept was further developed by the same authors in their dynamic
and on-line ensemble regression (DOER) . Here, the selection and pruning of models within the ensemble is being done dynamically, instance after
instance, to oﬀer improved adaptation capabilities. Additional novelty lies in
ability of each base model to update its parameters during the stream mining
procedure.
An evolutionary-based ensemble that can adapt the competence areas
and weights assigned to base models for regression tasks was also discussed
by Jackowski in .
5. Advanced Issues in Data Stream Analysis
The previous sections have discussed typical representations of examples
and output values (as attribute - value pairs) and learning problems which
are the commonly encountered in data stream analysis. However, in several
new studied problems one can meet more complex representations or learning
issues. We will now discuss ensemble solutions to these problems, including
learning from imbalanced data, novelty detection, lack of counterexamples,
active learning and non-standard data structures.
5.1. Imbalanced Classiﬁcation
Non-stationary data streams may be aﬀected by additional data complexity factors besides concept drifts and computational requirements. In
particular, it concerns class imbalance, i.e., situations when one of the target classes is represented by much less instances than other classes. Class
imbalance is an obstacle even for learning from static data, as classiﬁers are
biased toward the majority classes and tend to misclassify minority class examples. Dealing with unequal cardinalities of diﬀerent classes is one of the
contemporary challenges in batch learning from static data. It has been more
studied in this static framework and many new algorithms have already been
introduced, for their comprehensive review see the recent monograph or
surveys .
Out of these new solutions ensembles are one of the most promising directions. However, class imbalance has still received less attention in nonstationary learning .
Note that imbalanced data streams may not be
characterized only by an approximately ﬁxed class imbalance ratio over time.
The relationships between classes may also be no longer permanent in evolving imbalanced streams.
A more complex scenario is possible where the
imbalance ratio and the notion of a minority class may change over time.
It becomes even more complex when multi-class problems are being considered . Below we discuss main ensemble-based proposals for mining
imbalanced evolving streams. They are summarized in Table 6.
Table 6: Ensembles for imbalanced data streams.
Description
Chunk-based approaches
Ensemble with majority class sampling
Selectively recursive approach for sampling minority class
SERA with k-NN for chunk similarity analysis
Boundary deﬁnition ensemble
Learn++.CDC 
Learn++ with concept drift and SMOTE
Online approaches
Ensemble of online cost-sensitive neural networks
ESOS-ELM 
Ensemble of subset online sequential extreme learning machines
Oversampling-based online Bagging
Undersampling-based online Bagging
MOOB 
Multi-class oversampling-based online Bagging
MUOB 
Multi-class undersampling-based online Bagging
Many of these proposals adapt an idea of re-sampling the data in incoming
data to obtain more balanced class distributions.
In general re-sampling
methods transform the distribution of examples in the original data towards
more balanced classes.
Undersampling removes some examples from the
majority classes while oversampling adds minority class examples (either by
random replicating or generating synthetic new ones).
The ﬁrst proposal by Gao et al.
 was an ensemble approach that
divided examples from the incoming data chunk into positive (the minority
class) and negative (other classes) subsets. To build a new base classiﬁer
one takes all positive instances gathered so far and randomly selects a subset
of the negative instances of the new data chunk. The size of this subset
is calculated basing on a parameter referring to the class distribution ratio. Then, this new classiﬁer is added to the ensemble. Predictions of base
classiﬁers are combined using a simple voting technique. In order to accommodate this idea for a potentially inﬁnite stream authors propose to sample
examples from only a limited number of the most recent chunks, using either ﬁxed (each chunk contributes equally) or fading (the more recent chunks
contribute more instances) strategy. However, as all positive examples are
used to learn each classiﬁer, this method is limited to situations with a stable
deﬁnition of the minority class.
Selectively recursive approach (SERA) is another ensemble method
proposed by Chen and He that extends the Gao et al. concept by using
selective sampling of the minority class. Mahalanobis distance is used to
select a subset of most relevant minority instances (from the previous chunks)
for the current chunk of the stream and combine them with bagging method
applied on examples from the majority class. This approach alleviates the
drawbacks of the previous method regarding drifts on minority class, but at
the same time makes SERA very sensitive to proper selection of the number
of minority samples taken under consideration.
Chen and He proposed yet another ensemble, called REA , which
changes SERA properties by adopting the k-nearest neighbor principle to
estimate similarity between previous minority examples with ones in the
most recent chunk. The predictions of base classiﬁers are weighted on the
basis of their classiﬁcation of the recent chunk.
Lichtenwalter and Chawla proposed weighted ensembles in which
both classiﬁed minority and majority instances are being propagated between
chunks. This allows to better capture the potentially changing boundary between classes. A combination of information gain and Hellinger’s distance
(a skew-insensitive metric) is used to measure similarities between two data
chunks and thus to implicitly check if a concept drift has taken place. This
information is then used to weight ensemble members during the combination
of their predictions, with a linear function being inverse of the actual closeness of chunks. The authors acknowledge the potential limitations of this
approach (like small diﬀerences in weights or reduced variance) but leave a
more precise examination of diﬀerent combination functions for future studies.
Ditzler and Polikar proposed an extension of their Learn++ ensemble
for incremental learning from imbalanced data. This combines their previous approach to learning in non-stationary scenarios with idea of bagging,
where undersampling is performed in each bag. Classiﬁers are weighted based
on their performance on both minority and majority classes, thus preventing signiﬁcant loss of accuracy on negative cases. However, one must point
out that this approach assumes well-deﬁned minority class and cannot handle dynamically changing properties of classes. The authors also studied a
variant called Learn++.CDC (Concept Drift with SMOTE), which employs
oversampling of the minority class.
Ghazikhani et al. introduced an ensemble of online neural networks
to handle drifting and imbalanced streams. They embedded a cost-sensitive
learning into the process of neural network training in order to tackle the
skewed class distribution.
A number of cost-sensitive neural networks is
trained at the beginning of the stream using diﬀerent initial random weights.
Then, the ensemble is updated with new instances without set-up modiﬁcations. A cost matrix is predeﬁned, with penalty for errors on minority class
being twice the remaining costs. The usage of the ﬁxed cost matrix limits
the adaptability to evolving streams. Classiﬁers are combined using weighted
voting, and individual weights are calculated with a modiﬁed Winnow strategy.
An ensemble of online sequential extreme learning machine (ESOS-ELM)
was developed by Mirza et al. . It maintains randomized neural networks
that are trained on balanced subsets of stream. Short and long term memories
were implemented to store the ensemble and the progress of the stream. Two
diﬀerent learning schemes were proposed for moderate and high imbalance
ratios (the diﬀerence being the way of processing majority class instances).
However, the algorithm replicates the limitations of some of the previous
methods, assuming no drift on the minority class taking place.
Another approach to imbalanced and drifting streams was proposed by
Wang et al. . These authors are the only researchers which currently
consider also dynamic changes of class cardinalities. They proposed a num-
ber of online bagging-based solutions that are able to cope with dynamically
changing imbalance ratio and switching of class properties (e.g. majority
becoming minority over time). They considered a dedicated concept drift
detector for imbalanced streams, whose output directly inﬂuences the oversampling or undersampling ratios, allowing to accommodate evolving data
skewness. A further modiﬁcation, called WEOB, uses a combination of both
under and oversampling in order to choose the better strategy for the current state of the stream. An adaptive weighting combination scheme was
proposed to accommodate this hybrid solution, where the weights of the
sampling strategies are either computed as their G-mean values or are binary
(meaning only one of them will be used at a time). A multi-class extension
of this method was discussed in , where concepts of multi-minority and
multi-majority classes are used to model complex relations among classes.
Finally, recently some researchers have started to discuss the need for
new evaluation measures to address complexity of imbalanced data streams,
see , e.g., .
5.2. Novelty Detection and One-class Classiﬁcation
Due to the evolving nature of data streams the learning algorithm has to
be prepared to handle new, unseen data that do not follow the previously
seen distributions. Such examples may be caused by noise in the stream or
may actually originate from a novel concept that started emerging. Such a
novelty may be caused by some abnormality (like zero-day-attach in networks
or anomaly in the system) or may be a new instance from a concept that was
previously not seen. In the latter case a completely new class may appear
in the decision space, existing classes may merge or one of the classes may
star to disappear. This may happen in the context of two possible scenarios:
binary and multi-class.
In the former case we may treat it as a task of
recognizing a target (correct) concept and a set of potential outliers ,
while in the latter we must deal at the same time with a recognition problem
among a number of classes and detection of possible new emerging classes
 . For the binary case we often must face the fact that it is diﬃcult or
even impossible to gather suﬃcient representatives of the novel class, or that
they may not even form a class. Therefore, one-class classiﬁcation (known
as learning in the absence of counterexamples) is being utilized as it allows
to model the target concept without making any assumptions regarding the
properties of the novelty observations to appear.
Let us discuss now main ensemble-based methods suitable for these scenarios. They are summarized in Table 7.
Table 7: Ensembles for novelty detection and one-class classiﬁcation.
Description
OCLS 
One-class learning and summarization ensemble
UOCL 
Extended ensemble for one-class learning and summarization
IncOCBagg 
Incremental one-class Bagging
One-class ensemble based on prototypes
Learn++.NC 
Learn++ ensemble for novel class detection
ECSMiner 
Ensemble for novelty detection with time constraints
Ensemble for novelty detection and drifting feature space
AnyNovel 
Two-step clustering ensemble for novelty detection
CBCE 
Class-based ensemble for class evolution
Class-based micro classiﬁer ensemble
Stream Classiﬁer and novel and recurring class detector
Zhu et al. proposed an one-class ensemble approach to mining data
streams with a concept summarization approach by providing labels not for
single instances but for chunks of instances. They introduced a vague oneclass learning module, based on one-class Support Vector Machines. Each
base classiﬁer utilized weights assigned to instances from given chunk, re-
ﬂecting their level of relevance (in the discussed application the relevance
was based on user’s interests in given information).
This was done in a
two-step procedure, utilizing local and global weighting.
Local weighting
calculated instance weight values using examples in the given data chunk.
Global weighting was used to calculate a weight value for both positive and
unlabeled instances in given chunk, utilizing information coming from classi-
ﬁers trained on previous data chunks. This weight information was directly
embedded in the process of classiﬁer training. A weighted classiﬁer combination scheme was used to make a ﬁnal ensemble decision, where the weights of
each classiﬁer were calculated as an agreement measure between it and the
most recent classiﬁer in the pool. One must notice that this approach used
static one-class classiﬁers and thus adaptability was achieved only by adding
new members to the ensemble.
This idea was further developed by Liu et al. . They also proposed a
chunk-based ensemble of one-class classiﬁers for simultaneous learning from
uncertain data streams and concept summarization. They proposed a diﬀer-
ent scheme for calculating instance weights by using a local kernel-density
approach. It allowed to generate a bound score for each example based on its
local nearest neighbors in a kernel feature space. Thus, instance weight was
calculated only once and directly embedded in the process of one-class Support Vector Machine training. A combination of classiﬁers was done using a
weighted aggregation, where a weight for each base classiﬁer was determined
by its mean square error. Similar to the previous work, classiﬁers used here
were static ones.
An ensemble of adaptive one-class classiﬁers for drifting data streams
was proposed by Krawczyk and Wo´zniak . Here, classiﬁers were trained
with the usage of Bagging. The set-up of the ensemble remains unchanged
during the stream processing, but base classiﬁers are updated with random
subsets of examples from incoming data chunks. As a base classiﬁer they
used an incremental weighted one-class Support Vector Machine . It
incorporates new examples by re-weighting support vectors and adding /
removing them according to the stream progress.
New instances can be
weighted according to two diﬀerent strategies (highest priority to newest
examples or weights based on the distance from the hypersphere center).
The forgetting mechanism was implemented as a gradual decrease of weights
assigned to vectors, realized as a time-dependent function (the longer time
given instance spent in the stream, the higher the forgetting ratio). This
approach allowed the method to adapt to concept drift without a need for
an external drift detector, as old concepts were gradually removed from the
ensemble memory. Additionally, a parallel implementation was proposed in
order to achieve a computational speed-up. However, authors focused their
works only with chunk-based processing of data streams.
Czarnowski and Jedrzejowicz proposed yet another chunk-based ensemble of one-class classiﬁers for handling binary and multi-class data streams.
Here a single one-class classiﬁers (decision tree) was responsible for tackling a
single class. Each class-based data chunk utilized for training classiﬁers consisted of class prototypes and information about whether the class predictions
of these instances, carried-out at earlier steps, has been correct. When a new
chunk of data becomes available, an instance selection algorithm is applied to
select the most valuable examples. Classiﬁers are combined using a weighted
voting scheme.
Muhlbaier et al. introduced an extension of Learn++ for the cases
with novel class appearance in streams. The main change over the previous
version of the ensemble is an extension of the classiﬁer combination phase.
A dynamically weighted consult and vote was proposed, where individual
classiﬁers interchange their information regarding novel instances and select
the most competent ones by assigning them highest weights. This allows to
prevent cases when a new classiﬁer trained with a novel class is outvoted by
older ones who did not have access to new instances. However, this solution
is suitable only to scenarios in which classes emerge in a transient manner.
Masud et al.
 introduced an ensemble classiﬁer for simultaneous
classiﬁcation and novelty detection in drifting data streams with embedded
time constraints. It worked under an assumption that each example must be
evaluated within a given time window not to create a bottleneck for rapidly
incoming instances. This is of crucial importance to the novelty detection
module that is usually characterized by the highest computational complexity in the entire classiﬁcation system. Additionally, authors took into account
the possible delay with which a true class label may become available to the
system. These two constraints allowed to create a computationally eﬃcient
ensemble for high-speed and evolving data streams. As a base component authors proposed Enhanced Classiﬁer for Data Streams with novel class Miner
(ECSMiner), an ensemble system with three buﬀers: for potentially novel
instances, for instances waiting for class labels, and for labeled instances to
be used in training new classiﬁers.
In their follow-up work Masud et al.
 proposed a new ensemble
method that take into account not only concept drift and novel class appearance, but also the possibility of evolving feature space. They assumed
that new features may appear over time, which is being justiﬁed by speciﬁc
domain-based applications (e.g., new phrases in text stream mining). Each
model in the ensemble was built using feature space homogenization using
lossless conversion, to avoid diﬀerences between training and testing sets.
However, there are several diﬀerent modiﬁcations of their methods in this
work. The outlier detection module has been enhanced with an adaptive
threshold for changing deﬁnitions of novel instances. The novelty detection
module was constructed with the usage of Gini coeﬃcient to simultaneously
measure the diﬀerence among new instance and existing classes, as well as its
similarity to other novel instances stored in a buﬀer. Finally, the proposed
classiﬁcation system allowed for detecting multiple novel classes at the same
time using a graph analysis.
Abdallah et al. proposed an adaptive ensemble approach for multiclass novelty detection. The proposed method was based on a two-step cluster
formation. Firstly a supervised learning method was applied to divide the
initial data into class-based clusters. Then, an unsupervised learning was
applied to detect sub-concepts within each cluster and thus to create more
local models. Authors showed that their algorithm can eﬃciently distinguish
between actual novel concept appearance, drift present in one of the existing
sub-concepts or singular outliers appearance. This was done by deﬁning novel
concept as residing outside all existing cluster-based models and consistently
moving away from all existing concepts. A forgetting mechanism was implemented to detect concepts that no longer appear in the incoming stream and
mark them as irrelevant. To evaluate the model within the stream progress,
authors proposed an active learning strategy to reduce labeling costs.
Sun et al.
 introduced Class-Based ensemble for Class Evolution
(CBCE). They considered three possible scenarios: class emergence, disappearance and re-occurrence. CBCE constructs its ensemble by storing in a
memory an online classiﬁer for every single class that has appeared during
the course of data stream processing. This is done via one-vs-all binary decomposition. Additionally, a dynamic undersampling technique to deal with
class imbalance is applied to each base classiﬁer to counter the evolving disproportions between instances in classes. However, CBCE requires its base
classiﬁers to provide predictions in the form of a score, which limits the number of possible models to be used. When a novel class emerges, then its prior
probability is being estimated and a new classiﬁer is being trained. Classi-
ﬁers may be deactivated when a concept disappears and reactivated when its
re-occurrence has been detected.
Two other ensemble-based approaches to novel class detection were proposed by Al-Khateeb et al. , namely Class Based Micro Classiﬁer Ensemble (CLAM) and Stream Classiﬁer And Novel and Recurring class detector (SCARN). CLAM uses an ensemble of micro-classiﬁers, where each base
micro-classiﬁer has been trained using only positive instances from a given
class. This is done via a clustering approach. When a new instance becomes
available, the ensemble of micro-classiﬁers decides whether this is instance
belongs to any of existing classes or it is a novel one. After a certain number of instances has been tagged as representatives of a novel concept, a
new classiﬁer is trained on them and added to the ensemble. The novelty
detection is conducted using a proposed neighborhood-based distance score.
SCARN approach uses two ensemble models: primary ensemble and auxiliary ensemble. The primary ensemble is responsible for distinction between
known classes and potential outliers. If the outlier has been detected by the
primary ensemble, it is then delegated to the auxiliary ensemble. Its role is
to decide whether this is a reoccurring concept from previously known class
or a completely new case.
5.3. Active and Semi-supervised Learning
Fast availability of information about true target value (class) of incoming
examples is another issue which should be taken into account. As mentioned
in Section 3 most of used frameworks assume immediate or not too much
delayed access to target values. In some situations it is possible to obtain
true example state at minimal or no cost. An example would be weather
prognosis, where our prediction will be evaluated in future. This is however
connected with the problem of label latency - even if we will have access to
such an information it does not become available right after the arrival of a
new instance. However, in many practical situation this assumption may not
be realistic, mainly due to potentially high speed of incoming examples and
costs of human labeling. Note that while cooperating with human experts one
has to take into account their limited abilities, responsiveness, and threshold
on amount of data labeled in a certain amount of time. When all examples
cannot be quickly labeled, it may be still possible to obtain true target values
for a limited number of these examples at reasonable costs – see a discussion
in subsection 2.2. This can be exploited with active learning or semisupervised (including self-labeling) learning .
Active learning techniques must take into account the possible drifts in
data and adapt their sampling rules to it . One cannot use standard
static uncertainty-based methods, as they are not robust to situations where
drift occurs in a region of high classiﬁer certainty. In recent years, one could
see an increased number of studies dealing with this problem that propose
various mechanisms for adaptive active learning over non-stationary streams
 . Ensemble-inspired approaches have been already applied
to select examples in static, non-stream data frameworks. However, existing
work on using ensemble-based approaches for active learning in data stream
mining is scarce and this direction seems worthwhile for future exploitation.
We present the ensemble solutions for active and semi-supervised learning
over data streams below. Discussed algorithms are summarized in Table 8.
It is worth mentioning one of the key concepts of active learning called
Query by Committee , where active learning sampling is controlled by
an ensemble of classiﬁers.
The most popular methods from this domain
include Query by Bagging and Query by Boosting . They have been
proven to oﬀer increased stability and improved instance selection for labeling
Table 8: Active and semi-supervised ensembles.
Description
Optimal Weight Classiﬁer Ensemble with active learning
ReaSC 
Ensemble of semi-supervised micro-clusters
Semi-supervised ensemble integrating classiﬁers and clusters
COMPOSE 
Ensemble for initially labeled data streams
SPASC 
Ensemble of semi-supervised clustering algorithms
compared to queries based on a single classiﬁer decision. However, work on
using ensemble-based approaches for active learning in data stream mining
is scarce and this direction also seems worthwhile for future exploitation.
Zhu et al. proposed to use active learning for controlling the adaptation progress of an ensemble over drifting data streams. Authors argued
that variance of an ensemble has a direct relationship with its error rate and
thus one should select such instances for labeling that contribute towards
the minimization of the variance. Authors used bias-variance decomposition
of ensemble error as a basis for their minimum-variance instance selection
method. Additionally, these authors derived an optimal weight calculation
scheme for combining components. These two elements work in an active
learning loop – weights from the previous iteration are used to guide the
active learning procedure, after which a set of labeled examples is used for
the weight update step.
Masud et al. proposed an approach where micro-clusters are generated using semi-supervised clustering and a combination of these models are
used to handle unlabeled data incoming from the stream. A label propagation technique is used to assign each micro-cluster to a class. Then, inductive
label propagation is used to classify a new instance. New micro-clusters can
be added to an ensemble with the progress and changes in the stream. Additionally, an ensemble pruning technique is utilized, deleting any micro-cluster
with accuracy dropping below the given threshold (70%).
Learning with delayed labels has often been studied with a mechanism
to propagate available labels through the next steps when only unlabeled
data is available. For instance, Zhang et al. considered a hybrid ensemble
integrating classiﬁers and clusters, where labeled example are used to learn
classiﬁers while clusters are formed from unlabeled data . New incoming
instance receives a label resulting from voting both classiﬁers and clusters.
Another interesting statistical approach to represent each class in the stream
by a mixture of sub-population was considered by Krempl and Hofer .
However this approach is restricted to track only limited gradual drifts in
unlabeled data.
COMPOSE (COMPacted Object Sample Extraction) ensemble was
proposed for streams where labeled instances are available only during the
initial training of classiﬁers.
After this phase, all incoming instances are
assumed to be non-labeled.
COMPOSE works in three steps.
First, initial labels are combined with new unlabeled data to train a semi-supervised
classiﬁer and use it to label these instances. Then, each class gets assigned
a geometric descriptor to construct an enclosing boundary and provide the
current distribution of this class. Finally, instances called core supports are
extracted to serve as class representatives. This allows to track concept drift
in a semi-supervised manner and adapt models accordingly.
Hosseini et al. proposed an ensemble of semi-supervised clustering algorithms, where each class is described by a single model. Each new incoming
chunk obtains a pre-deﬁned number of labeled instances, which are used to
update classiﬁers in the ensemble. Chunks are assigned based on a semisupervised Bayesian approach. Authors claim that their approach is able to
automatically recognize recurrent concepts within the data stream.
5.4. Complex Data Representations and Structured Outputs
Non-standard data and class structures have gained increasing attention
in recent years from the machine learning community. Due to the advent of
big data and the necessity to mine unstructured, heterogeneous and complex
information, we require learning methods that can eﬃciently accommodate
such instances. Although most of the current research concerns static, nonstreaming frameworks, some research has been undertaken in the case of data
streams. The most important streaming ensemble solutions are discussed
below and are summarized in Table 9.
Multi-label and multi-instance learning is still a largely unexplored area in
data stream mining. In case of multi-label algorithm a proper experimental
and evaluation framework was proposed by Read et al. , but there is
not an abundance of work that follow it, especially from the ensemble point
of view. Qu et al. proposed a dynamic classiﬁer ensemble for multilabel data streams, where a binary relevance scheme was extended by using
feature weighting and keeping a subset of the most recent classiﬁers in the
pool, instead of all possible pairwise combinations. Classiﬁers are weighted
dynamically for each incoming example from the stream.
Table 9: Ensembles for streaming complex data representations.
Description
Multi-label data streams
Dynamic ensemble with improved binary relevance
Multiple-window ensemble for multi-label streams
MLDE 
Multi-voting dynamic ensemble with clustering
FCM-BR 
Binary relevance with fuzzy confusion matrix
Multi-instance data streams
MILTrack 
Multi-instance online Boosting
OMILBoost 
Online Boosting based on image patches
Semi-WMIL 
Semi-supervised ensemble of weak online classiﬁers
Other data structures
AdaTreeMiner 
XML stream mining using closed tree algorithms
Ensemble of maximal frequent subtrees for each class
gSLU 
Ensemble based framework to partition graph streams
gEboost 
Boosting for imbalanced and noisy graph streams
Xiouﬁs et al. introduced an ensemble using a binary relevance model
and maintaining two separate windows – one for positive and one for negative examples. An eﬃcient implementation of k-NN classiﬁer is used due
to its natural incremental nature, while each base classiﬁer is trained on an
undersampled label set to tackle possible label imbalance.
The problems related with labeling costs for multi-label data streams
were discussed by Wang et al. . A theoretical loss function for their
proposed ensemble classiﬁer and an active learning function to select examples minimizing this function were derived. This allowed for using less labeled
instances for training and detecting concept drift on the basis of labeling the
most uncertain examples.
Multi-Label Dynamic Ensemble (MLDE) was developed in . It used
adaptive cluster-based classiﬁers that were combined by a voting method
utilizing two separate weights based on accuracy on the given dara chunk
and similarity among chunks.
Trajdos and Kurzynski proposed a stream-based extension of binary
relevance model utilizing a fuzzy confusion matrix to correct the decisions of
base classiﬁers in the ensemble. The correction model was updated as the
stream progressed, thus adapting to its current state. However, no explicit
drift detection technique was used.
Multi-instance learning is an even less exploited area in the stream mining
context. Most work in this domain concentrates on image analysis applications and is used in online video processing. However, one may see a video
as a stream of images. Babenko et al. proposed a modiﬁcation of online
boosting for learning from bags of examples. They assumed that once a bag
is labeled as a positive one, then all examples within are also positive and
hence used for training. However, this drawback was reduced by choosing
weak classiﬁers on the basis of a bag likelihood loss function. The ensemble
could be updated with new models with the progress of the stream similar
to standard online Boosting. A similar approach was proposed by Qi et al.
 , using however a diﬀerent classiﬁer selection approach based on selecting
correct image patch around the labeled target. Wang et al. proposed a
semi-supervised ensemble of weak online classiﬁers for object tracking. The
ﬁnal ensemble was constructed by selecting weak classiﬁers obtained by maximizing the log-likelihood function but minimizing the inconsistency function.
Mining XML data is well-studied in static scenarios. However, modern
computing environments require online and eﬃcient document processing
within time and memory constraints. Bifet and Gavald`a proposed compression of XML trees into vectors that are possible for processing by standard classiﬁers, creating closed frequent pattern data structures. These are
later feed into a number of stream classiﬁers based on variants of Bagging
and Boosting for online analysis. However, the main contribution of the paper lied in new data structures, whereas their ensembles were standard ones
from the literature.
Brzezinski and Piernik developed XML Stream Classiﬁer (XSC) ensemble. It creates a set of maximal frequent subtrees for each class independently. Label prediction is done using association between new documents
incoming from the stream and the closest maximal frequent subtree (and
thus the class to which it is associated). The base classiﬁers are updated in
sequential manner, but as each class has its own classiﬁer the update rates or
size of the update chunks may vary. This makes XCS suitable for processing
imbalance and locally drifting data streams.
Streams of graphs are also a frequent challenge for learning algorithms,
as they become more and more prevalent with the constant growth of social
networks. Pan et al. proposed an ensemble approach for mining graph
streams, where a stream is partitioned into a number of chunks, each of
which contains both labeled and unlabeled graphs. A minimum-redundancy
feature selection is applied independently in each chunk to reduce its di-
mensionality. A sliding window solution with instance weighting is used to
accommodate the possibility of drift presence and forget outdated examples.
Each chunk serves as a training set to build a classiﬁer, and then form them
into an ensemble. Nearly the same authors have recently extended this idea
by proposing a Boosting approach called gEboost for imbalanced and noisy
graph streams . It maintains the graph partitioning approach (including
a special feature selection from subgraphs), but for each chunk a Boosting
classiﬁer was constructed and learned with a variant of margin maximization.
Instance weighting was incorporated directly into this scheme to put more
emphasis on the most diﬃcult examples for the imbalance problem.
6. Future Research Directions
In this paper, we have discussed the challenging issues of learning ensembles from data streams. We have considered both classiﬁcation and regression
ensembles, even though classiﬁer ensembles are typically the most often applied approaches in data stream analysis.
In the ﬁrst sections of the paper, we have presented characteristics which
distinguish data streams from the standard static data repositories. New requirements to using computationally eﬀective algorithms, which should usually also be able to adapt to concept drift in non-stationary data streams,
have been discussed. Diﬀerent types of concept drift, their characteristics,
and methods for their detection in diﬀerent stream scenarios have been reviewed. Moreover, diﬃculties in evaluating stream classiﬁers in presence of
concept drift have been shown. The main part of our paper includes a detailed survey of ensembles, which are categorized with respect to diﬀerent
criteria (stationary or not data, chunk or online processing modes, passive or
active reactions to drifts). Furthermore, we have extended this study to more
complex stream situations such as class-imbalanced learning, novelty detection, active and semi-supervised learning, and dealing with more complex
data structures.
Despite many interesting developments in the ﬁeld of mining data streams,
there is still a number of open research problems and challenges awaiting to
be properly addressed. We brieﬂy present our views on potential directions
that seem worthwhile to be further explored below:
• Better handling delayed information and extending current
techniques within semi-supervised learning: these approaches
are still limited to few ensemble proposals and deﬁnitely need more
attention. In particular, in fast evolving streams, the relationship between attributes and target values may be only locally valid due to
concept drift . Many of the discussed approaches employ a kind
of transfer learning, where predictions from models learned from labeled examples are transferred to next unlabeled portions of the data.
In general, they are more useful for limited gradual drifts, while more
complex scenarios are still open problems. Developing new approaches
to deal with delayed information, including ensembles, that would work
in the presence of diﬀerent types of drift is a non-trivial research task.
It would be particularly useful for many real life automated systems,
where an interaction with human experts is quite limited.
delayed information may not refer to target values only, but may concern also incomplete attribute descriptions. The problem of incomplete
data is more intensively studied in static, oﬀ-line data mining, where
diﬀerent imputation techniques have been developed. In the streaming
context, there is not too much research on such techniques or other
approaches which could learn classiﬁers with omitting such incomplete
descriptions and then update the classiﬁer structure.
• New frameworks for evaluating data stream classiﬁers: several
interesting issues on evaluating classiﬁers have been studied for static,
oﬀ-line data. For a comprehensive overview, we refer the reader to .
Although new measures have been recently introduced,
the nature of complex evolving data streams still poses requirements
for novel theoretical and algorithmic solutions.
This is particularly
needed for more complex stream scenarios with veriﬁcation latency,
changing class imbalance, censored even data streams , multiple
data streams , and changes of misclassiﬁcation costs .
researchers have considered many diﬀerent kinds of measures (e.g. predictive performance, time or memory costs, reaction time and many
others), a multi-criteria analysis may be more appropriate than aggregating several measures into a single coeﬃcient .
Another open
issue is rethinking frameworks for testing stream algorithms. Tuning
parameters of streaming ensembles is more diﬃcult than in the static
case, where special validation sets or internal cross-validation are usually employed. Their equivalents for evolving streams are yet to be invented. How to acess ground truth in unsupervised streams also needs
to be elaborated. Finally, statistical analysis of signiﬁcance of diﬀerence between several algorithms with respect to time changes should
be developed, similarly to recent recommendations to use appropriate
non-parametric tests for static oﬄine setup.
• Benchmark datasets: the number of real-world publicly available
datasets for testing stream classiﬁers is still too small. It limits comparative studies of diﬀerent streaming algorithms. Moreover, some popular
data used in the literature is questioned to represent suﬃciently real
drifts, see e.g. discussions on electricity data . This is a more dif-
ﬁcult situation compared to the state of available static datasets such
as the UCI Machine Learning Repository.
• Dedicated diversity measures for data stream classiﬁer ensembles: recall that ensemble diversity is one of the important characteristics of ensembles in the standard, static data context .
As discussed in Section 1, several researchers studied the relationship
between high ensemble predictive performance and the diversity of its
components. Others used specialized diversity measures to visually analyzing ensemble classiﬁcation accuracy. These measures have
also been used to tune the combination rule for aggregating component
classiﬁer predictions or to prune too large pool of components inside
the ensemble. However, such research is not much visible in case of
streaming ensembles. On the one hand, one can say that as component classiﬁers are learnt from diﬀerent parts of the stream, they are
already diﬀerent and diverse ones. On the other hand, our literature
survey shows that only few authors directly consider promoting diversity while constructing an ensemble or rebuilding them in the moment
of detecting drifts, see e.g. DDD ensemble or other generalizations
of online bagging such as . However, nearly nobody directly measures the diversity of component classiﬁers in streams. Rare studies are
based on taking into consideration the diversity measures developed for
static, oﬀ-line solutions. The most recent study provides a wider
experimental study of using six of the most popular diversity measures
 , where a few online and chunk-based ensembles were evaluated in
several scenarios of drifts. The ﬁrst observation from these experiments
is that diversity of ensembles is rather low. Some diversity measures,
e.g., κ inter-agreement measure, change values over the stream with re-
lation to occurring drifts – it is more visible for sudden changes rather
than for gradual drifts. So, these results may indicate further research
lines on combining selected diversity measures, perhaps also with more
typical drift detectors to better monitor changes in the evolving stream
and to more precisely identify moments of drifts. This could also lead to
new solutions for monitoring changes in unlabeled streams. Nevertheless, more research on new diversity measures specialized for evolving
stream should be undertaken.
• Dealing with multiple streams and more complex representations: nearly all streaming ensembles have been proposed to processing
a single stream only. However, some applications, see e.g. studies on
internet messages or censored data in the variant of survival analysis
 , may provide several parallel streams. In such multiple streams,
the same data events (objects identiﬁed in the data sources) may appear in diﬀerent time moments in each stream and may have diﬀerent
descriptions. This poses several interesting and new challenges, e.g.,
how to aggregate the information about the same event available in
diﬀerent streams, how to predict the moment of an event appearing in
one of the streams, given knowledge on other streams, and whether to
develop a new ensemble dedicated to work over such multiple streams.
These aspects should be particularly important in the context of integrating diﬀerent (also heterogeneous) data repositories in Big Data
Analysis . Note that data streams are becoming more and more
complex in some new applications, such as social media or electronic
health records, which require to deal with many heterogeneous data
representations at the same moment. Such mixed representations include both structured, semi-structured and completely unstructured
data ﬁelds, quite often referring to static images, video sequences, or
other signals. To fully comprehend the dynamic and phenomenon of
these data sources, we need to ﬁnd interactions among such complex
and varying data.
As ensembles naturally integrate diverse models,
they seem to be a highly promising solution for this challenge.
• Considering more complex class distributions in imbalanced
streams: working with class-imbalanced and evolving streams is still
in early stages. Among very few existing ensemble proposals, most researchers consider the simplest problem of the imbalanced class ratio,
without changes of imbalance ratio over time. Note that in the
static data framework, other data diﬃculty factors such as decomposition of the minority class into rare sub-concepts, overlapping with
other classes, and presence of very rare minority cases in the majority
class regions are also considered as more inﬂuential than the global imbalance between classes. Considering them in drifting scenarios, where
sub-concepts or rare cases appear over time and overlapping regions
change, is an open research problem. Similar new challenges may refer
to studying changing multiple minority classes . Finally, new evaluation measures and more rigorous evaluation procedures are needed
for evaluating algorithms in such complex imbalanced streams – see a
discussion in .
• More studies on the nature of some drift types: although a lot
of research has been done on adaptating ensembles to diﬀerent concept
drifts, several more detailed characteristics of drifts have not yet been
consistently examined in literature. In particular, gradual drifts are
more diﬃcult to be detected and tracked than sudden changes or reoccurring concepts. The current drift detectors work better with sudden
drifts, while the identiﬁcation of characteristic moments of developing
gradual or incremental drifts in real streams are still not suﬃciently
developed. Furthermore, a more formal deﬁnition of diﬀerent kinds of
gradual drifts should be proposed. The authors of showed that
the progress of changes inside gradual drifts may be realized in many
diﬀerent ways and needs more specialized solutions. The work of 
also considers diﬀerent types of gradual drifts, besides considering that
drifts may occur in a sequence of several abrupt and non-severe drifts.
The paper postulates that the idea of the so called limited gradual drift is used rather in an intuitive way in most work. Although
the work of has attempted to provide more formal deﬁnitions of
drift characteristics and introduces a new taxonomy of diﬀerent types
of drift, more research should be undertaken to better understand the
nature of some drifts, how they develop in real streams, how to measure
drift magnitude (e.g. small, medium or high), and which forms of drift
could be better handled by speciﬁc categories of ensembles.
• Considering background knowledge or context while classifying data streams: some researchers argue for including more addi-
tional information than basic descriptions of instances when constructing predictions from streams. One of the options is to add background
knowledge into drift adaptation techniques . For instance, taking
into account seasonal eﬀects while analyzing the electricity benchmark
data set nicely illustrates the usefulness of this postulate . Another
possibility is classifying data streams taking context into consideration,
i.e., usually Markov chains are used to analyze the data stream when
there are inter-dependencies between the successive labels, e.g., medical diagnosis – the state of the patient depends not only on the recent
observation but also his/her history is taken into consideration. The
same in the case of character recognition, when we know that the text
is, e.g., written in English, where we can recognize the current letter
on the basis of its characteristic, but also take into consideration what
was the previous letter (some combinations are not possible and some of
them are almost impossible). There are several studies on classiﬁcation
with context, e.g., .
• Self-tuning ensembles: most online and chunk-based approaches use
models with parameters being either individually tuned or using some
preset values – ﬁxed for the complete analysis process. However, with
the changes within the stream the previously set parameters may no
longer be the suﬃciently good (especially in case of parameter-sensitive
methods, like support vector machines or neural networks). Therefore,
proposing a new methodology for self-tuning streaming ensemble systems may lead to improved predictive power. Additionally, tuning parameters for single classiﬁers should take into account that they are
components within the ensemble. Thus, more global update methods
that can lead to obtain more complementary models seems to be worth
exploring.
• Ensemble pruning: although many ensembles for data streams apply
pruning procedures, they are usually based on prediction performance
or time that the model has spent within the ensemble. However, as
data stream mining is a complex task, these factors may not be suf-
ﬁcient to capture the full dynamics of changes. More advanced pruning techniques could also exploit a multiple criteria analysis, including
not only current predictive ability, but also computational eﬃciency
of base models, memory usage or other resources, current diversity of
the ensemble, available information on class labels, etc. At the same
time, these pruning techniques should impose minimal computational
overhead. Such compound, yet lightweight approaches, should lead to
maintaining better ensemble setup and improve adaptation abilities to
various types of changes.
• Other requirements to processing Big Data and privacy issues:
when dealing with massive data streams, algorithms should be able to
handle not only changing data, but also big volumes of instances arriving rapidly. At the same time, an ensemble for such data must still
work under strict time and memory constraints. This can be handled
in two ways – by proposing algorithms with improved scalability or
by using special performance computing environments, like SPARK,
Hadoop or GPU clusters. Although some attempts to extend the most
often used software, like MOA, have already been undertaken, there
is still a need for eﬃcient implementations of existing methods within
these specialized frameworks for Big Data, as well as developing new
solutions natively for them. Another aspect of analyzing Big Data concerns the requirements for privacy protection, especially in complex
systems where streams are a sub-part of a more complex analytical
workﬂow . Here, often not only no information can be leaked outside, but also the teams participating within the analysis may not be
willing to directly share their data. It raises the need for data stream
ensemble algorithms able to work in such scenarios without the possibility of reverse-engineering the underlying data from their decisions
and models.
Acknowledgments.
This work was supported by the Polish National Science Center under the
grants no. DEC-2013/09/B/ST6/02264 and no. DEC-2013/11/B/ST6/00963.
J. Gama acknowledges project MAESTRA - ICT-750 2013-612944.