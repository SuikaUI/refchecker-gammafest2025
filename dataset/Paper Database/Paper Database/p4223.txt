Automatic Multi-organ Segmentation on Abdominal
CT with Dense V-networks
Eli Gibson∗†, Francesco Giganti§¶, Yipeng Hu∗†, Ester Bonmati∗†, Steve Bandula∥, Kurinchi Gurusamy¶,
Brian Davidson¶, Stephen P. Pereira∗∗, Matthew J. Clarkson†∗, and Dean C. Barratt∗†
∗UCL Centre for Medical Image Computing, Department of Medical Physics & Biomedical Engineering,
University College London, UK
§Department of Radiology, University College Hospital Trust, UK
¶Division of Surgery and Interventional Science, University College London, UK
∥UCL Centre for Medical Imaging, University College London, UK
∗∗Institute for Liver and Digestive Health, University College London, UK
†Wellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, UK
Abstract—Automatic segmentation of abdominal anatomy on
computed tomography (CT) images can support diagnosis, treatment planning and treatment delivery workﬂows. Segmentation
methods using statistical models and multi-atlas label fusion
(MALF) require inter-subject image registrations which are
challenging for abdominal images, but alternative methods without registration have not yet achieved higher accuracy for
most abdominal organs. We present a registration-free deeplearning-based segmentation algorithm for eight organs that are
relevant for navigation in endoscopic pancreatic and biliary
procedures, including the pancreas, the GI tract (esophagus,
stomach, duodenum) and surrounding organs (liver, spleen, left
kidney, gallbladder). We directly compared the segmentation
accuracy of the proposed method to existing deep learning and
MALF methods in a cross-validation on a multi-centre data
set with 90 subjects. The proposed method yielded signiﬁcantly
higher Dice scores for all organs and lower mean absolute
distances for most organs, including Dice scores of 0.78 vs. 0.71,
0.74 and 0.74 for the pancreas, 0.90 vs 0.85, 0.87 and 0.83 for
the stomach and 0.76 vs 0.68, 0.69 and 0.66 for the esophagus.
We conclude that deep-learning-based segmentation represents a
registration-free method for multi-organ abdominal CT segmentation whose accuracy can surpass current methods, potentially
supporting image-guided navigation in gastrointestinal endoscopy
procedures.
Index Terms—Abdominal CT, Segmentation, Deep learning,
Pancreas, Gastrointestinal tract, Stomach, Duodenum, Esophagus, Liver, Spleen, Kidney, Gallbladder
I. INTRODUCTION
EGMENTATION of organs in abdominal images can
support clinical workﬂows in multiple domains, including
diagnostic interventions, treatment planning and treatment
delivery. Organ segmentation is a crucially important step
for computer-assisted diagnostic and biomarker measurement
systems . Segmentations of treatment volumes and organsat-risk are also central to planning radiation therapies .
More generally, segmentation-based patient-speciﬁc anatomical models can support surgical planning and delivery via
intra-operative image-guidance systems .
Corresponding author: E. Gibson (email: ). Copyright
(c) 2017 IEEE. Personal use of this material is permitted. However, permission
to use this material for any other purposes must be obtained from the IEEE
by sending a request to .
In endoscopic pancreatobiliary procedures, an endoscope
is inserted orally and navigated through the gastrointestinal
tract to speciﬁc positions on the stomach or duodenal wall to
allow pancreatobiliary imaging and intervention. Due to the
small endoscopic ﬁeld of view and the lack of visual orientation cues, this navigation task is challenging, particularly for
novice endoscopists . Image-guidance showing registered
anatomical models would provide orientation and targeting
cues that are outside of the endoscopic ﬁeld of view or
challenging to see on endoscopic images. To support targeting
and navigation, segmentations of multiple organs are needed:
the pancreas, gastrointestinal organs (esophagus, stomach and
duodenum), and nearby organs used as navigational landmarks
(liver, gallbladder, spleen and left kidney).
Manual segmentation of 3D abdominal images is laborintensive and impractical for most clinical workﬂows, motivating (semi-)automated segmentation tools . Research into
such tools has focused on computed tomography (CT), due to
its clinical prevalence, and on three methodologies: statistical
models (SM) , , multi-atlas label fusion (MALF) –
 and registration-free methods – . SM and MALF,
reviewed in more detail in Section I-A1, rely on establishing
anatomic correspondences between images from different subjects, a task that remains challenging due to high inter-subject
variability in organ shape and appearance as well as soft tissue
deformation . Registration-free methods trade registration
challenges for the challenges of constructing variability- and
deformation-invariant features (”hand-tuned” or learnt) that
characterize anatomy in an unregistered training data set.
Despite the claimed advantage of this approach, registrationfree methods have achieved less accurate multi-organ segmentations than the registration-based approaches .
Recent advances in machine learning, computational power
and data availability, however, have enabled the training of
more complex registration-free methods, including deep fully
convolutional networks (FCNs), promising increased segmentation accuracy . FCNs, discussed in detail in Section I-A2,
are particularly well-suited to multi-organ abdominal segmentation because they require neither explicit anatomical correspondences nor hand-tuned image features. In multi-organ ab-
This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.
The final version of record is available at
 
Copyright (c) 2018 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
dominal segmentation, they have been used alone or with
pre- or post-processing, such as level sets and MALF ,
demonstrating their potential value. However, these pipelines
still have not achieved higher accuracies than the most accurate
registration-based methods for most organs .
This study presents the dense V-network FCN (DenseVNet)
and its application to multi-organ segmentation on abdominal
CT, yielding higher accuracies than three existing methods.
The contributions of this work are four-fold:
1) The DenseVNet segmentation network is presented,
which enables high-resolution activation maps through
memory-efﬁcient dropout and feature reuse.
2) A batch-wise spatial dropout scheme is proposed, which
lowers the memory and computational costs of dropout.
3) The accuracy of DenseVNet for multi-organ segmentation from abdominal CT is evaluated using a crossvalidation over 90 manually segmented images from
multiple centres. The results indicate that higher segmentation accuracy can be achieved than a state-of-theart MALF method and two existing FCNs.
4) The parts of DenseNet critical for accuracy are identiﬁed
by comparing the accuracies of network variants.
This builds on our preliminary work , with an improved
network architecture, a larger data set, and more extensive
comparisons with other algorithms and network variants.
A. Related work
1) Common multi-organ segmentation methodologies: Statistical models , involve co-registering images in a
training data set to estimate anatomical correspondences, constructing a statistical model of the distribution of shapes 
and/or appearances of corresponding anatomy in the
training data, and ﬁtting the resulting model to new images to
generate segmentations. Multi-atlas label fusion methods –
 register images in a training data set to each new image
and combine propagated reference segmentations to generate
new segmentations. Statistical models and multi-atlas methods
are limited by image registration accuracy. This registration,
while extensively studied, remains challenging . The size,
shape, appearance, and relative positions of abdominal organs
vary considerably between patients due to natural variability,
disease status and previous treatments and within each patient
due to soft tissue deformation. To avoid challenging registrations, registration-free methods train a voxel-wise classiﬁer
on unregistered images. Some methods have relied on handcrafted organ-speciﬁc image features , , but many
recent approaches involve training classiﬁers on selected (but
typically organ-agnostic) image features , . Registration challenges notwithstanding, MALF has yielded more
accurate multi-organ abdominal CT segmentations than registration free methods for most organs . However, recent
advances in registration-free methods may change this.
2) FCNs for segmentation:
FCNs are compositions of
simple image-to-image functions with trainable parameters,
including convolution with linear kernels and voxelwise nonlinearities. FCNs are efﬁcient architectures for deep-learningbased tasks that require image outputs like segmentation.
FCNs have recently been applied to segmentation of volumetric images in medical image analysis , , – 
where such images are common. Segmentation of volumetric
images face particular challenges, mainly due to the need to
process large volumetric images under memory constraints.
One strategy to constrain the memory usage is to process
smaller images: small patches of a larger image or lower resolution images. Image-patch segmentations consider various
patch types – single 2D slices, slabs of adjacent 2D slices
or smaller cropped regions – and orientations – single axisaligned slices, multiple slices from multiple axes, or oblique
slices. These methods gain memory efﬁciency but lose spatial
context. In contrast, Milletari et al. and C¸ ic¸ek et al. 
used 3D representations of the entire image by downsampling
the image sequentially so that most image features are only
represented at low resolution. Our previous work used 3D
representations with fewer, but higher-resolution, features by
using dense blocks , stacks of convolutional units in which
the input of each layer comprises the outputs of all preceding
stack layers, compensating for using fewer features.
Another strategy to constrain the memory usage is to limit
the network depth. However, this affects the FCN receptive
ﬁeld (i.e. the size of the image region affecting each output
voxel), which grows linearly with the network depth. Larger
convolutional kernels mitigate this by increasing the linear
growth rate; however, this can result in a very high parameter
count (which grows as the cube of kernel size in 3D). Sequential downsampling, mentioned above, also mitigates this effect,
as the receptive ﬁeld grows exponentially with the number of
downsampling stages. Dilated convolutions , used in our
previous work , instead use large, but sparse kernels to
give exponential receptive ﬁeld size with few parameters.
Multi-organ segmentation poses additional challenges. First,
more information must be propagated through the network,
exacerbating the aforementioned memory challenges. The relative weighting of the losses for different organs (with high volume imbalance) can have unpredictable effects on convergence
and ﬁnal errors; using the Dice coefﬁcient is common but
remains poorly characterized. Imposing shape and topological constraints between speciﬁed organs also remains
challenging. Despite these challenges, deep learning has been
used in multi-organ abdominal CT segmentation alone ,
 or as part of a larger segmentation pipeline , .
Zhou et al. segmented 19 abdominal organs on 2D
slices in axial, sagittal and coronal views and combined the
results using majority-voting label fusion. Roth et al. 
segmented 7 organs using a two-stage hierarchical pipeline
based on 3D UNet . Hu et al. segmented 4 organs
using a 3D FCN to generate organ probability maps as
features for a level-set-based segmentation. Larsson et al. 
used MALF to identify a region of interest for each organ
and a 3D FCN with hand-tuned input features to complete
the segmentation. Compared to registration-based methods in
a recent segmentation challenge , these methods were
substantially more accurate (>2% Dice score improvement)
for gallbladder, achieved parity (within 2% Dice score) for the
liver, left kidney, right adrenal gland and aorta, but have lower
accuracy for the pancreas, gastrointestinal tract 2018 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
stomach) and other organs (spleen, right kidney, vena cava,
portal/splenic vein, and left adrenal gland).
Ninety abdominal CT images and corresponding reference
standard segmentations of the spleen, left kidney, gallbladder,
esophagus, liver, stomach, pancreas and duodenum were used
for this study. The CT images and some of the segmentations
were drawn from two publicly available data sets: fortythree subjects from the Cancer Imaging Archive Pancreas-CT
data set , , with pancreas segmentations and 47
subjects from the ‘Beyond the Cranial Vault’ (BTCV) segmentation challenge with segmentations of all organs except
duodenum. The remaining reference standard segmentations
were performed at our centre. The completed segmentations
and subject identiﬁers have been made publicly available
(DOI: 
A. Image data
The Pancreas-CT data set comprises abdominal CT acquired at the National Institutes of Health Clinical Center
from pre-nephrectomy healthy kidney donors or patients with
neither major abdominal pathologies nor pancreatic cancer
lesions . The BTCV data set comprises abdominal CT
acquired at the Vanderbilt University Medical Center from
metastatic liver cancer patients or post-operative ventral hernia
patients . Images had voxel sizes from 0.6–0.9 mm in the
anterior-posterior (AP) and left-right (LR) axes and 0.5–5.0
mm in the inferior-superior (IS) axis. Images were manually
cropped to the rib-cage and abdominal cavity transversely, to
the superior extent of the liver or spleen and the inferior extent
of the liver or kidneys, resulting in ﬁelds of view ranging from
172–318mm AP, 246–367mm LR and 138–283mm IS.
B. Reference standard segmentations
Segmentations from the Pancreas-CT and BTCV datasets
were used where available. An imaging research fellow (E.G.),
under the supervision of a board-certiﬁed radiologist with 8
years of experience in gastrointestinal CT and MRI image
interpretation (F.G.), interactively segmented the unsegmented
organs on both data sets and edited the segmented organs to
ensure a consistent segmentation protocol, using Matlab 2015b
and ITK-SNAP 3.2 ( 
III. METHODS
This study compares our proposed algorithm to multiple
automated segmentation algorithms in two experiments. First,
to evaluate the improvements to the state of the art in segmentation accuracy due to our algorithm, we compare three distinct
algorithms detailed below: the multi-atlas-label-fusion-based
DEEDS+JLF , , the deep-learning-based VoxRes-
Net , and the proposed deep-learning-based DenseVNet.
Second, to clarify the architectural factors contributing to
these improvements, we compare variations of the proposed
DenseVNet architecture.
TABLE OF SYMBOLS
logit segmentation from V-network
logit spatial prior
L′, L′′, L′′
logit and probabilistic segmentation and l-th channel
l-th channel on reference standard segmentation
stochastic binary masks for dropout
convolution kernel
c(X, W, s, γ)
convolutional unit
rectiﬁed linear non-linearity
channel-wise batch normalization
¯o(X, BI, BO)
batch-wise spatial dropout
dense feature stack
bilinear upsampling
Operator parameters (operator: parameters)
stride, scale parameter
f: m, a, di, nf
# layers, kernel size,
i-th layer dilation rate, # features in each unit
Other notation†
approximate probability of keeping each channel
voxel coordinates
† Symbols used within one paragraph are omitted for brevity.
A. Proposed algorithm: Dense V-network segmentation
segmentation
fullyconvolutional neural network based on convolutional
units composed as shown in Figure 1. The architecture design
can be understood in terms of 5 key features described below:
batch-wise spatial dropout, dense feature stacks, V-network
downsampling and upsampling, dilated convolutions, and an
explicit spatial prior. For clarity and precision, each of these
will be described conceptually and speciﬁed mathematically.
The supplementary material, available in the multimedia tab
online, has guidance for implementing the network.
Each convolutional unit comprised three functions: (1) a
3D convolution with a learned kernel, (2) a batch normalization to facilitate robust gradient propagation, and (3) a
rectiﬁed linear unit (ReLU) non-linearity to represent nonlinear functions. Speciﬁcally, convolutional units are denoted,
c(X, W, s, γ)x,y,z = r(b((X ∗W)sx,sy,sz, γ))
is a convolutional kernel; batch normalization
b(X, γ) transforms the mean of each channel to 0 and the
variance to a learned per-channel scale parameter γ, and the
rectiﬁed linear unit r(X) = max(0, X) induces non-linearity.
For computational and memory efﬁciency, we introduce our
new batch-wise spatial dropout. In regular spatial dropout ,
to regularize the network, random channels are dropped (i.e.
set to zero) with an independent speciﬁed probability,
ˆXi = ˆo(c( ˆXi−1, W, s, γ), BO)
where ˆo(X, BO) sets channels masked by stochastic binary
mask BO to zero, and ˆXi−1 = ˆo(Xi−1, BI) is the previous
unit’s output after dropout with mask BI. Standard implementations calculate and store the dropped activations that do
not affect subsequent layers. Our proposed batch-wise spatial
dropout avoids computing these activations by modifying the
convolution kernels instead of the activation maps, denoted
¯Xi = c( ¯Xi−1, ¯o(W, BI, BO), s, ¯γ))
This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.
The final version of record is available at
 
Copyright (c) 2018 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
where ¯o(W, BI, BO) is a new kernel without input and output
channels masked by BI and BO, ¯Xi−1 is the output of the previous unit after batch-wise spatial dropout, and ¯γ is the scale
parameter of undropped channels. Note that ¯Xi is identical to
the undropped channels of ˆXi but does not compute or store
the dropped channels, and that subsequent convolutions are
unaffected if their kernel is similarly modiﬁed. To realize the
efﬁciency gains, two further changes are made. First, the same
channels are dropped for all images in each mini-batch, so that
the same convolution kernels can be used for the whole minibatch. Second, the distribution of dropped channels is changed
to limit the maximum memory usage. In spatial dropout,
the probability distribution of keeping k out of n channels
is a binomial distribution p(K = k) =
pk(1 −p)n−k;
although the expected value E[K = k] = pn, the maximum
value (corresponding to the maximum memory usage) is n.
Instead the proposed batch-wise spatial dropout drops channels
using dependent Bernoulli distributions, such that a ﬁxed
number of channels ⌈pn⌉are kept. Segmentation inference can
use all features by scaling the convolutional unit outputs by
nf/⌈nfp⌉; this requires more memory per subject than training, as all nf feature maps are generated. Alternatively, Monte-
Carlo inference can be used (increasing the computation
cost but lowering the memory usage) by inferring multiple
segmentation samples using dropout, and combining them.
Both of these approaches are evaluated in the experiments
below. An implementation of batch-wise spatial dropout is
available in the NiftyNet platform1.
Dense feature stacks, adapted from the dense block deﬁned
by Huang et al. , are a sequence of composite functions
where the input of each function is the concatenated output of
all preceding functions. In contrast to Huang’s dense block,
our composite function use our batch-wise spatial dropout
for regularization, and do not use 1 × 1 bottleneck layers.
Speciﬁcally, the output of an m-layer dense feature stack
fm(X0) = [X0; X1; ...; Xm] where
Xi = ˆfi([X0; X1; ...; Xi−1])
ˆfi(X) = c(X, ¯o(Wa,nf ,di, BI
i ), 1, ¯γ))
where [A; B] denotes channel-wise concatenation; Wa,nf ,di is
an a×a×a convolution kernel (a = 3) with nf output channels
(4, 8 and 16 for high, medium and low resolution dense feature
stacks) and dilation rate di (d2 = 3, d3 = 9, di/∈{2,3} = 1);
1 ; ...; BO
i−1] selects all previously computed
channels, BO
selects all channels from X0 and otherwise
is sampled stochastically such that ⌈pnf⌉channels are
selected (p = 0.5). This structure has several advantages.
First, like residual networks , the feature stacks inherently
encode identity functions, as the ﬁnal output channels include
the inputs. Second, they combine multiple network depths
within a single network allowing both effective propagation of gradients through the network (every kernel weight
lies in a shallow sub-graph of depth 1) and representation
of complex functions (every kernel weight lies in multiple
deeper sub-graphs with depths 2 to m). Finally, when memory
1niftynet.layer.channel_sparse_convolution.Channel
SparseConvolutionalLayer in the code repositories.
constraints limit the number of activation maps, information
from earlier layers is stored only once in memory, but accessed
by later layers. Memory-efﬁcient dense blocks , where a
careful implementation of feature concatenation avoids storing
multiple copies of feature maps, can achieve O(m) memory
usage. The improvements of batch-wise spatial dropout can be
combined with those of memory-efﬁcient dense blocks by only
allocating shared memory storage for the number of computed
activation maps, which is ﬁxed for our dependent Bernoulli
distributions.
A V-network architecture comprises downsampling and
upsampling subnetworks, with skip connections to propagate higher resolution information to the ﬁnal segmentation. Previous V-networks , , typically use shallow
strided-convolution downsampling units followed by shallow
transpose-convolutional upsampling units with additive or
concatenating skip connections within each resolution. DenseVNet differs in several ways: the downsampling subnetwork
is a sequence of three dense feature stacks connected by downsampling strided convolutions; each skip connection is a single
convolution of the corresponding dense feature stack output,
and the upsampling network comprises bilinear upsampling
to the ﬁnal segmentation resolution. Memory efﬁciencies of
dense feature stacks and batch-wise spatial dropout enable
deeper networks at higher resolutions, which is advantageous
for segmentation of smaller structures. The bilinear upsampling of skip connections to the segmentation resolution (723)
limits artifacts induced by transpose convolution . The Vnetwork generates a logit label prediction L with 9 classes.
Dilated convolutions use sparse convolution kernels to represent functions with large receptive ﬁelds but few trainable parameters. Speciﬁcally, a dilated kernel Wa,k,d is a
(d(a −1) + 1)3 kernel with a trainable parameter every
d elements in each dimension and 0 elsewhere. For the ith convolutional layer of a FCN, the relative resolution is
j=1 1/sj, and the receptive ﬁeld size, expressed recursively,
is ri = ri−1 + di(ai −1) Qi−1
j=1 sj, where di, si and ai are the
dilation rate, stride and kernel size (before dilation) of layer
i. Because both resolution and receptive ﬁeld size depend on
si, sequential downsampling can generate either local highresolution features in early layers or global low-resolution
features after the downsampling layers. In contrast, by increasing di exponentially with si = 1, dilated convolutions can
generate high-resolution features with exponentially growing
receptive ﬁelds in the early layers. This allows more complex
functions of these features to be computed in later layers. The
high-resolution large-receptive-ﬁeld features in lower layers
may help the segmentation of small structures (e.g. esophagus)
whose location can be inferred from large structures nearby.
Finally, we use an explicit spatial prior introduced in our
previous work . Medical images are frequently acquired in
standard anatomically aligned views with relatively consistent
organ positions and orientations, motivating spatial segmentation priors. Spatial priors can be encoded implicitly, due
to boundary effects of convolution or by providing image
coordinates as an input channel . Our previous work 
introduced an explicit spatial prior. The spatial prior P is
This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.
The final version of record is available at
 
Copyright (c) 2018 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
a low-resolution 3D map of trainable parameters, which is
bilinearly upsampled to the segmentation resolution and added
to the outputs of the V-network (i.e. L′ = u(P) + L).
Conceptually, this could represent the posterior log-probability
L′ = logp(L|x, I) of the class label L at voxel x given image I
as the sum of a log-likelihood L = logp(I|x, L) generated by
the V-network and a prior log-probability u(P) = logp(L|x)
generated by the spatial prior. However, the spatial prior
parameters are trained as part of the end-to-end gradient-based
optimization and may not represent the true prior probability.
1) Implementation details:
The loss function was the
weighted sum of an L2 regularisation loss with labelsmoothed probabilistic Dice scores for each organ l
averaged across subjects in each minibatch,
pDicel(L′′
l , 0.9) · Rl
||Rl||2 + ||min(L′′
l , 0.9)||2
where vectors L′′
l = softmax(L′)l and Rl are the algorithm’s
probabilistic segmentation and the binary reference standard
segmentation for organ l for each subject, respectively. To
further mitigate the extreme class imbalance (e.g. esophagus
averaged 0.09% of the image and liver averaged 11.7%), Dice
score hinge losses heavily penalizing Dice scores below 0.01
and 0.10 were introduced after warm up periods of 25 and 100
iterations, respectively. The loss function at iteration i was
loss(L′′, i) =
d(pDicel(L′′
l , Rl), i)
d(l, i) = l + 100h(l, i, 0.01, 25) + 10h(l, i, 0.1, 100)
h(l, i, v, t) = sigmoid(6(i −t)/t)(max(0, v −l)/v)4
where w ∈W are kernel values, l is the Dice loss, v is the
hinge loss threshold, and t is the delay in iterations.
The network was trained using the Adam optimizer with
ϵ = 0.001 and mini-batch size 10 for 5000 iterations (i.e. 625
epochs). Training each instance of the network took approximately 6 hours using Titan X Pascal or P100 GPUs (NVIDIA
Corporation, Los Alamitos, CA). A Tensorﬂow implementation of a trained DenseVNet network is available in the
NiftyNet platform model zoo ( zoo).
The cropped region of interest, ranging from 209–471
voxels (172–367mm) transversely and 32–450 voxels (138–
283mm) in the IS axis, was resampled to a 1443-voxel volume.
During training, for data augmentation, afﬁne perturbations
were applied yielding skewed subregions 0% to 10% smaller
in each dimension. For the baseline DenseVNet used in the
algorithm comparison, we used Monte Carlo inference using
the mode of 30 723 segmentation samples (chosen heuristically
apriori), taking approximately 8–15 seconds per image. In
post-processing, the 723 segmentation labels were resampled
to the original cropped region at the original image resolution
in Matlab using curvature ﬂow smoothing with 2 iterations (chosen visually a priori to avoid quantization artifacts).
Then, for each organ, the union of all connected components
comprising >10% (chosen ad hoc, a priori) of the segmented
organ volume was taken as the ﬁnal mask.
DETAILED PARAMETERS FOR DENSEVNET ARCHITECTURE.
Kernel Stride Subunits
B. Evaluation metrics and statistical methods
We compared the accuracy of segmentation algorithms
using a 9-fold cross-validation over 90 subjects. For each test
image in each fold, we compared each organ segmentation to
the reference standard segmentation using three metrics:
• Dice coefﬁcient – 2|A ∩B|/(|A| + |B|)
• symmetric mean boundary distance – (D(A, B) +
D(B, A))/2, and
• symmetric 95% Hausdorff distance – (P95(D(A, B)) +
P95(D(B, A)))/2,
where A and B are the algorithm and reference segmentations,
D(A, B) is the set of distances from boundary pixels of A,
ΩA, to the nearest boundary pixel in ΩB (i.e. D(A, B) =
x∈ΩB ||x −y|| |y ∈ΩA}), and P95(D) is the 95-th percentile
of D. The Dice coefﬁcient measures the relative volumetric
overlap between segmentations. The mean boundary and 95%
Hausdorff distances reﬂect the agreement between segmentation boundaries, with the latter being more sensitive to
localized disagreements.
In each analysis, we compared the accuracy of the proposed
algorithm to each comparator using a sign test for correlated
data , which is insensitive to the skewed distribution
of accuracy differences observed in our data, and accounts
for the correlation between values within each fold due to
the shared training set. We used Benjamini–Hochberg falsediscovery-rate multiple-comparison correction (α = 0.05) for
pairwise tests. This correction was performed separately for
the primary analysis comparing algorithms and the secondary
analysis comparing architecture variants. In several subjects,
one or more organs were not present in the images due to
prior surgeries; these organs (7 gallbladders, 1 left kidney and
1 esophagus) were excluded from the aggregate descriptive
statistics and statistical comparisons above as the measures
used are not meaningful in this scenario. In these cases, we
reported the segmented volume (ideally 0) for these organs
(Supplementary material Table II, available in the multimedia
tab online).
C. Primary analysis: algorithm comparison
We compared the segmentation accuracy of our algorithm
to those of two existing algorithms: the deep-learning-based
This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.
The final version of record is available at
 
Copyright (c) 2018 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
Convolutional
downsampling
Convolution
upsampling
feature stack
Activation
Fig. 1. DenseVNet network architecture. First, 723 feature maps are computed using a strided convolution. Second, a cascade of dense feature stacks and
strided convolutions generate activation maps at three resolutions. Third, a convolution unit is applied at each resolution reducing the number of features.
Fourth, after bilinear upsampling back to 723, the maps are concatenated and a ﬁnal convolution generates the likelihood logits. Finally, these are added to
the upsampled spatial prior to generate the segmentation logit. Parameters for individual components are given in Table II.
VoxResNet method and the MALF-based DEEDS+JLF
method , , described below.
1) Comparison algorithm 1: deep-learning-based VNet:
To evaluate our speciﬁc architecture, we compare it to a
baseline V-network architecture for 3D image segmentation,
VNet , which has been widely used and adapted, and
has a publicly available implementation ( 
faustomilletari/VNet). Because the original VNet was designed for binary segmentation, we modiﬁed the loss gradient
∂Dice/∂L′′
l,xyz to support multiple labels and missing organs,
from −1l(2R1,xyzU1 −4L′′
1,xyzI1)/U 2
1 to −(2Rl,xyzUl −
l,xyzIl)/(U 2
l + ϵ), where Ul and Il are the volumes of
the union and intersection of Rl and argmaxi(L′′
respectively, and ϵ = 0.01 is a constant for numerical stability when there are missing organs. VNet uses a parametric
rectiﬁed linear unit activation function and does not use batchnormalization. The downsampling subnetwork comprises a
residual unit (i.e. one or more convolutional units in series added back to the input) at 5 resolutions with strided
convolution for downsampling. The upsampling subnetwork
concatenates features upsampled using transpose convolution
with ’skip’ features directly from downsampling subnetwork
and applies a residual unit. We trained VNet using Caffe and
our adaptation of the reference implementation and applied
the same post-processing as for DenseVNet.
2) Comparison algorithm 2: deep-learning-based VoxRes-
Net: To further evaluate our speciﬁc architecture, we compare
it to an existing off-the-shelf FCN for multi-class segmentation, VoxResNet . VoxResNet has been evaluated for segmentation of multiple tissue types in the brain, but was adapted
to multi-organ segmentation by adding output channels.
architecture
VNet, using batch normalization and rectiﬁed-linear-unit nonlinearities, and combining all upsampled features together
instead of incrementally at each resolution. The downsampling
subnetwork includes a combination of convolutional units
and residual network units . The upsampling subnetwork
comprises, for each of 4 resolutions, a transpose convolution to
upsample the network to the segmentation resolution followed
by a convolutional unit; these upsampled logits are summed to
yield the segmentation. We trained VoxResNet using the same
loss function and optimization protocol and applied the same
post-processing as for DenseVNet.
3) Comparison algorithm 3: MALF-based DEEDS+JLF:
To evaluate our proposed method relative to the current
state of the art, we compare its segmentation accuracy to
an existing MALF-based method , , abbreviated as
DEEDS+JLF. First, atlas images from the training data were
registered to the input image using dense displacement sampling (DEEDS) . Then, transformed reference labels were
combined using joint label fusion (JLF) . DEEDS minimizes the self-similarity context metric under an afﬁne transformation then under free-form deformation with diffusionbased regularization using a discretised search space. DEEDS
was shown to yield the highest registration accuracies in a
direct comparison of 6 publicly available algorithms . Joint
label fusion computes the weighted average of the transformed
labels, where the weights are a function of the image similarity
between the atlas images and target image compensating for
correlations between atlas images. A combination of DEEDS
and JLF achieved the highest accuracies in the BTCV segmentation challenge . DEEDS and JLF computations were
performed using the publicly available deedsRegSSC (http:
//www.mpheinrich.de/software.html) and PICSL Multi-Atlas
Segmentation Tool ( malf)
implementations, respectively, using published abdominal CT
registration parameters for DEEDS and default parameters
for JLF. Segmentations were post-processed as for DenseVNet.
D. Secondary analysis: architecture features
To isolate and quantify the contribution of each element of
the proposed architecture, we conducted a series of ‘ablation’
experiments, wherein we altered key concepts underlying the
architecture: the dense feature stacks, V-network structure,
hinge loss, dilated convolutions, spatial prior, batch-wise spatial dropout, Monte Carlo inference, and receptive ﬁeld size.
To evaluate the dense feature stacks, we compared DenseVNet to variants with regular inter-layer connections:
NoDenseLow, where dense feature stacks were replaced with
regular convolutional unit stacks 2018 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
its immediate predecessor) with the same number of channels
as DenseVNet, but lower trainable parameter count (due to
connectivity); and NoDenseHigh, with regular convolutional
unit stacks but more channels (12, 24 and 48) to match the
parameter count of DenseVNet.
Evaluating the V-network structure is challenging, as it
induces several properties: representations at multiple scales,
increased layer count, larger receptive ﬁeld size, higher numbers of channels, and more learnable parameters. Because
these properties interact, it is not feasible to manipulate
them independently while holding the others constant. Instead,
we evaluate these factors together by comparing our fourresolution network to 3 alternatives: ShallowV, with three
resolutions (1443, 723, and 363); NoVLow, with only two
resolutions (1443 and 723) and correspondingly low trainable
parameter count; and NoVHigh, with two resolutions but more
channels per convolution (nf = 35 and 75 features in the skip
connection) to match the parameter count of DenseVNet.
To evaluate the hinge loss, dilated convolutions, explicit
spatial prior, batch-wise spatial dropout, we compared our
network to four corresponding alternative networks:
d(pDicel(L′′
l , Rl), i)
pDicel(L′′
l , Rl), abbreviated as NoHinge.
• dilated convolutions: a network with each dilated convolution replaced with a standard 3 × 3 × 3 convolution,
abbreviated as NoDil.
• the explicit spatial prior: a network with the spatial prior
layer omitted, abbreviated as NoPrior.
• batch-wise spatial dropout: a network using standard
Bernoulli distributed channel-wise spatial dropout 
within the dense feature stacks with probability p = 0.5,
abbreviated as NoBSDO.
To evaluate Monte Carlo inference, the trained DenseVNet
was used, but inference was performed with no dropout, using
all features; these results are abbreviated as Deterministic.
To further evaluate the impact of receptive ﬁeld size, we
compared our network to one with the ShallowV architecture
without dilated convolutions, abbreviated as NoDilShallowV.
In these experiments, the batch size was reduced from 10 to
8, so that DenseVNet and the less memory efﬁcient NoBSDO
could be directly compared using the same batch size.
IV. RESULTS
A. Primary analysis: algorithm comparison
For the algorithm comparison, the medians of the segmentation evaluation metrics for each organ are reported in
Table III and whisker plots are shown in Figure 3, respectively.
Representative segmentations from the 25th, 50th and 75th
percentiles of Dice scores shown in Figure 2.
Excluding the duodenum (discussed below), the proposed
network yielded higher Dice scores than VNet, VoxResNet
and DEEDS+JLF (all statistically signiﬁcant) and lower mean
boundary distances (all statistically signiﬁcant except for pancreas segmentations with DEEDS+JLF). The 95% Hausdorff
distance had higher variability, likely due to the poor robustness of 95% quantiles; consequently, while the proposed
MEDIAN SEGMENTATION METRICS FROM THE ALGORITHM COMPARISON
(WHISKER PLOTS IN FIGURE 3). BOLDFACE DENOTES STATISTICALLY
SIGNIFICANT FINDINGS WHERE THE MEDIAN METRIC DIFFERED FROM
DENSEVNET (CONFIDENCE INTERVALS ARE GIVEN IN TABLE VI).
L. Kid. Gallb. Esoph.
Stom. Panc. Duod.
Dice coefﬁcient (%)
DEEDS+JLF 0.89
DenseVNet 0.96
Mean boundary distance (mm)
DEEDS+JLF 2.1
95% Hausdorff distance (mm)
DEEDS+JLF 5.1
method had lower observed distances in 20/21 comparisons,
the difference only reached statistical signiﬁcance in 15/21.
The duodenum segmentations were the least accurate for all
algorithms and all metrics. DenseVNet had higher Dice scores
than other algorithms, and lower mean boundary distance than
VNet, but no other comparisons were statistically signiﬁcant.
Although partially attributable to higher variability due to high
interpatient anatomical variation and poor image contrast with
distal intestines, the observed differences in median accuracy
metrics are generally smaller than for other organs.
B. Secondary analysis: architecture features
For the evaluation of architecture features, the medians of
the segmentation evaluation metrics for each organ are reported in Table IV. The accuracy differences when eliminating
dilated convolutions and the explicit spatial prior were not
statistically signiﬁcant in any comparisons. Eliminating batchwise spatial dropout or one downsampling unit of the Vstructure yielded a small loss in accuracy by all metrics for
most organs, but differences for the gallbladder or esophagus,
the smallest organs (0.17% and 0.09% of voxels on average),
were not signiﬁcant. Eliminating the dense connections or
eliminating the V-network entirely yielded substantially less
accurate segmentations for all comparisons.
V. DISCUSSION
In this paper, we present a registration-free deep-learningbased algorithm to segment multiple organs from abdominal
CT. To facilitate intraprocedural navigation in endoscopic
pancreatobiliary interventions, eight organs were segmented:
the pancreas, three organs from the gastrointestinal tract
(esophagus, stomach and duodenum) where the endoscope
is navigated, and four nearby organs for use as navigational
landmarks (liver, gallbladder, spleen and left kidney).
Clinically acceptable segmentation accuracies have yet to
be deﬁned for guiding abdominal interventions, and depend
on the intervention and guidance system. However, accuracy
improvements for the pancreas 2018 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
25th percentile
75th percentile
Fig. 2. Top: Posterior view of segmentations from three patients with Dice scores closest to the 25th, 50th, and 75th percentile: pancreas (cyan), esophagus
(purple), stomach (yellow), duodenum (pink), liver (orange), spleen (red), left kidney (brown) and gallbladder (green). Bottom: Segmentations overlaid on CT.
targets) and the gastrointestinal tract (where the endoscope is
navigated) should be prioritized over navigational landmarks
as an endoscope can be oriented without precise boundaries.
A direct comparison of our proposed method with two
existing deep-learning based methods and a recent MALFbased method demonstrated improved overlap for all eight
organs and improved boundary accuracy for seven of the
eight. The largest improvements were for the smallest organs
(gallbladder and esophagus). This may be due in part to the
challenges in registering small organs for MALF, which is
avoided with registration-free methods and in part due to the
deeper high-resolution features in DenseVNet. Segmentation
accuracy for the duodenum, which has been less well studied,
was much lower than other organs, but interestingly was
very similar across all four algorithms. FCN methods were
also found to be faster (<1s for deterministic and 8–15s for
Monte Carlo inference). DEEDS+JLF took 60s per pairwise
registrations and 78 minutes to fuse labels from 80 registered
atlases. Although these times would not be a limiting factor
in a clinical workﬂow for fully-automated segmentation, the
deep learning methods are fast enough to use for more accurate
semi-automatic segmentations.
Many previous studies have proposed methods for multiorgan segmentation of abdominal CT. Some of the required organs (pancreas, liver, kidney and spleen) are included in many
of these studies; however, gastrointestinal tract segmentation
This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.
The final version of record is available at
 
Copyright (c) 2018 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
MEDIAN SEGMENTATION METRICS FOR THE EVALUATION OF
ARCHITECTURE FEATURES. BOLDFACE DENOTES STATISTICALLY
SIGNIFICANT FINDINGS WHERE THE MEDIAN METRIC FOR THE VARIANT
DIFFERED FROM DENSEVNET.
Param. Spl. L. Kid. Gallb. Esoph. Liv. Stom. Panc. Duod.
Dice coefﬁcient (%)
NoDenseLow
NoDenseHigh 865k 0.78
NoDilShallowV 277k 0.95
Deterministic
Mean boundary distance (mm)
NoDenseLow
NoDenseHigh 865k
NoDilShallowV 277k
Deterministic
95% Hausdorff distance (mm)
NoDenseLow
NoDenseHigh 865k 17.3
NoDilShallowV 277k
Deterministic
has received much less attention. Only one other study has
reported duodenal segmentation (pooled with the stomach as
a single segmentation). Comparing segmentation accuracy to
previous literature is challenging; quantitative metrics are not
directly comparable due to differences in data set sizes, imaging protocols and quality and reference segmentation protocols
and quality. This notwithstanding, Table V shows mean Dice
scores for the segmented organs from many reported methods.
In most previous work and our proposed method, liver, spleen
and kidney segmentations have consistently yielded higher
Dice scores than other anatomy. On these organs, many
segmentations yield segmentations with Dice scores within
0.02 of the highest reported scores (ours for spleen, Hu et
al. for left kidney and liver). It is unlikely that these
differences are statistically or meaningfully different, given the
data variability. Segmentation accuracies for the other organs
have been more variable. Notably, Cerrolaza et al. reported
some of the highest accuracies for gallbladder, stomach and
pancreas segmentations, but much lower accuracies for organ
segmentations that are commonly very accurate: spleen, left
kidney and liver. Our method yielded the highest mean Dice
scores for spleen, esophagus, stomach and pancreas, and mean
Dice scores within 0.02 of the highest for liver and left
kidney. However, the mean Dice score for the gallbladder
were lower than several other methods with a highly skewed
distribution (the median Dice score was 0.84, but 13/84 had
scores less than 0.50). This may be due to 6 patients in our
data set without gallbladders; while gallbladder segmentations
for these patients were excluded from the scores, they were
included in the training data and may have affected training.
MEAN DICE SCORES FOR PREVIOUS ABDOMINAL CT MULTI-ORGAN
SEGMENTATION METHODS. DIFFERENT DATA SETS AND SEGMENTATION
OF UNLISTED ORGANS PRECLUDE DIRECT COMPARISONS.
Mean Dice scores %; higher is better
Spl. L.Kid. Gall.Esoph. Liv. Stom. Panc. Duod.
Cerrolaza 
Heinrich MA
0.92 0.60 0.69
Kechichian 
Larsson 
0.91 0.62 0.66
0.88††0.90††
Shimizu 
0.91† 0.88† 0.77† 0.37† 0.94† 0.55† 0.53†
Suzuki 
0.88† 0.65† 0.25†
0.85† 0.07† 0.46†
0.84 0.27 0.43
0.92† 0.91† 0.65† 0.43† 0.95† 0.60†‡ 0.62† 0.60†‡
Zografos 
0.92† 0.92†
0.90 0.60 0.64
0.89 0.69 0.65
0.90 0.64 0.65
0.93 0.73 0.71
MA=multi-atlas label fusion; SM=statistical shape/appearance model;
RF=registration-free; DL=deep learning. + denotes processing with
methods commonly used in registration-free methods.
† estimated from mean Jaccard index; †† estimated from mean precision
and recall. ‡ Stomach and duodenum were segmented as one class.
The architecture experiments identiﬁed two features that
were critical to the segmentation accuracy: the dense connections and the multi-scale V-network structure signiﬁcantly improved the performance. First, the dense connections yielded
the largest improvement. The relatively small difference between NoDenseHigh and NoDenseLow suggests that this improvement is not due to the additional trainable parameters, but
rather due to the connectivity structure that supports gradient
propagation and feature reuse. Second, the four-resolution
DenseVNet with the multi-scale V-network structure outperformed the two-resolution NoVLow network without it. Some
of the differences can be attributed to having fewer trainable
parameters, as NoVHigh recovered 38–82% of the difference
depending on the organ; however, NoVHigh still substantially
underperformed DenseVNet. DenseVNet yielded signiﬁcant
but very small improvements over the three-resolution ShallowV for most organs, suggesting that even minimal multiscale structure delivers the vast majority of the beneﬁt. Although one purported advantage of multi-scale networks is
that the receptive ﬁeld of the network is larger, ShallowV,
where the receptive ﬁeld covers 50–100% of the image, and
NoDilShallowV, where the receptive ﬁeld covers 5–50% of the
This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.
The final version of record is available at
 
Copyright (c) 2018 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing .
image, yielded very similar performance, suggesting that the
entire image is not necessary for accurate segmentation.
To avoid biased performance estimates due to selecting the
best of multiple variants for baseline comparisons, the network
architecture for algorithm comparisons was speciﬁed a priori.
It included the following features where the observed accuracy differences did not reach statistical signiﬁcance: dilated
convolutions, the explicit spatial prior, and the hinge loss. This
suggests that these features may be omitted from DenseVNet
with minimal change in segmentation performance. Additionally, our experiments in which these features were omitted give
the following insights into information propagation within the
architecture. The dilated convolutions increase the receptive
ﬁeld of the features early in the network, particularly the high
resolution features; the receptive ﬁeld of the high resolution
features spans half the image in the network with dilated
convolutions, while it spans less than a quarter of the image in
the network without them. The small performance difference
without dilated convolutions suggests that high resolution
global information is either incorporated late in the network, or
is unnecessary to achieve the observed segmentation accuracy.
Our spatial prior, which encoded the voxel-wise label probability, is suitable for coarsely aligned images (via standardized
acquisition or pre-processing). Our results suggest that this
simple spatial information can be implicitly encoded in learned
features when the prior is excluded, perhaps using features
from nearby organs or image-boundary artifacts. However,
forms of spatial prior that encode global dependencies, such as
topological or shape-representation loss functions,
may still improve performance, and merit investigation. Two
features were introduced to reduce memory costs without
affecting performance: batch-wise spatial dropout and Monte
Carlo inference. Differences from these additions did not reach
statistical signiﬁcance for 46/48 comparisons. Our ﬁndings
suggest that batch-wise spatial dropout should be used in DenseVNet implementations and that the inference method can be
chosen based on memory or computation time constraints.
While our experiments focused on the network architecture,
the data pipeline also included pre- and post-processing. First,
to constrain memory, images were cropped manually to the
abdominal cavity. Altering the cropping protocol for the test
data sufﬁciently (i.e. beyond the variability generated by data
augmentation) can impact segmentation accuracy. Speciﬁcally,
when the cropped region was adjusted outward by 5% in
each axis (outside the data augmentation variability), the Dice
scores decreased by 0.03 ± 0.06 (mean±SD); however, when
the cropped regions on the test data were adjusted inward by
the same 5% (within the data augmentation variability), the
Dice scores changed by 0.00 ± 0.04. This suggests that data
augmentation variability should cover the anticipated cropping
variability. Second, the data were post-processed to ﬁlter out
small false positive regions and upsample the segmentations
smoothly. In post hoc comparisons, removing the connected
component ﬁlter or using bilinear upsampling instead of our
smoothing upsampling scheme generally had minimal impact
on the segmentation performance, with median Dice, MAD
and Hausdorff distance scores changing by less than 0.005,
0.1 mm and 1.1 mm, respectively.
Our conclusions are qualiﬁed by some limitations. Authors
were not blinded to the manual segmentations during algorithm development; although the cross-validation was only run
after algorithm development was complete, design decisions
may have been inﬂuenced by data observations. Algorithm parameters for the FCN methods were not extensively optimized
for this application; the reported performance of DenseVNet,
VNet and VoxResNet methods may underestimate their potential performance. The statistical comparison accounts for
correlation within each fold, but not between folds due to
partial overlap of the training data; an independent evaluation
on a disjoint data set would be valuable. The evaluation metrics
measure segmentation ﬁdelity with the manual reference, and
not the clinical utility of the resulting segmentations for aiding
endoscopic navigation; future work will evaluate whether the
proposed algorithm is accurate enough to provide a 3D patientspeciﬁc anatomical model to aid endoscopic navigation.
In conclusion, the proposed deep-learning-based DenseVNet can segment the pancreas, esophagus, stomach, liver,
spleen, gallbladder, left kidney and duodenum more accurately
than previous methods using deep learning or multi-atlas label
fusion. The densely linked layers and a shallow V-network
architecture are critical to the segmentation accuracy of this
network. The use of dilated convolutions was not necessary,
suggesting that global high-resolution non-linear features are
not critical for abdominal CT organ segmentation. The use of
an explicit spatial prior was also not necessary, suggesting that
convolutional neural networks are implicitly encoding spatial
priors, despite their purported translational invariance. The
automatically generated segmentations of abdominal anatomy
have the potential to support image-guided navigation in
pancreatobiliary endoscopy procedures.
ACKNOWLEDGEMENTS
This publication presents independent research supported by
Cancer Research UK (Multidisciplinary C28070/A19985) and
the National Institute for Health Research UCL/UCL Hospitals
Biomedical Research Centre.