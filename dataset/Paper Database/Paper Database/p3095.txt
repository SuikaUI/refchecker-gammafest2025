Parallel Tempering: Theory, Applications, and New
Perspectives
David J. Earla,b and Michael W. Deema
a Departments of Bioengineering and Physics & Astronomy, Rice University,
6100 Main Street MS142, Houston, Texas 77005 USA.
E-mail: 
b Rudolf Peierls Centre for Theoretical Physics, Oxford University,
1 Keble Road, Oxford OX1 3NP United Kingdom.
E-mail: 
November 26, 2024
To appear in Physical Chemistry Chemical Physics
We review the history of the parallel tempering simulation method. From its origins in data analysis,
the parallel tempering method has become a standard workhorse of physiochemical simulations. We
discuss the theory behind the method and its various generalizations. We mention a selected set of
the many applications that have become possible with the introduction of parallel tempering and we
suggest several promising avenues for future research.
Introduction
The origins of the parallel tempering, or replica exchange, simulation technique can be traced to a
1986 paper by Swendsen and Wang.1 In this paper, a method of replica Monte Carlo was introduced
in which replicas of a system of interest are simulated at a series of temperatures. Replicas at adjacent
temperatures undergo a partial exchange of conﬁguration information. The more familiar form of
parallel tempering with complete exchange of conﬁguration information was formulated by Geyer
in 1991.2 Initially, applications of the new method were limited to problems in statistical physics.
However, following Hansmann’s use of the method in Monte Carlo simulations of a biomolecule,3
Falcioni and Deem’s use of parallel tempering for X-ray structure determination,4 and Okamoto and
co-worker’s formulation of a molecular dynamics version of parallel tempering,5 the use of parallel
tempering in ﬁelds spanning physics, chemistry, biology, engineering and materials science rapidly
increased.
The general idea of parallel tempering is to simulate M replicas of the original system of interest,
each replica typically in the canonical ensemble, and usually each replica at a different temperature.
The high temperature systems are generally able to sample large volumes of phase space, whereas
low temperature systems, whilst having precise sampling in a local region of phase space, may become trapped in local energy minima during the timescale of a typical computer simulation. Parallel
tempering achieves good sampling by allowing the systems at different temperatures to exchange
complete conﬁgurations. Thus, the inclusion of higher temperature systems ensures that the lower
temperature systems can access a representative set of low-temperature regions of phase space. This
concept is illustrated in Figure 1.
Simulation of M replicas, rather than one, requires on the order of M times more computational effort.
This ‘extra expense’ of parallel tempering is one of the reasons for the initially slow adoption of the
method. Eventually, it became clear that a parallel tempering simulation is more than 1/M times
more efﬁcient than a standard, single-temperature Monte Carlo simulation. This increased efﬁciency
derives from allowing the lower temperature systems to sample regions of phase space that they
would not have been able to access had regular sampling been conducted for a single-temperature
simulation that was M times as long. While not essential to the method, it is also the case that
parallel tempering can make efﬁcient use of large CPU clusters, where different replicas can be run
in parallel. An additional beneﬁt of the parallel tempering method is the generation of results for a
range of temperatures, which may also be of interest to the investigator. It is now widely appreciated
that parallel tempering is a useful and powerful computational method.
One of the debated issues in parallel tempering regards the details of the exchange, or swapping, of
conﬁgurations between replicas. Pertinent questions include how many different replicas and at what
temperatures to use, and how frequently swaps should be attempted, and the relative computational
effort to expend on the different replicas. Another emerging issue is how to swap only part of the
system, so as to overcome the growth as
N of the number replicas required to simulate a system of
size N. We address these points of controversy in this review.
The widespread use of parallel tempering in the simulation ﬁeld has led to the emergence of a number
of new issues. It has also become clear that temperature may not always be the best parameter to
temper, and parallel tempering can be conducted with order parameters other than temperature, such
as pair potentials or chemical potentials. Of interest is how to choose the order parameter whose
swapping will give the most efﬁcient equilibration. It has become clear that multi-dimensional parallel
tempering is possible. That is, swapping between a number of parameters in the same simulation, in
a multi-dimensional space of order parameters, is feasible and sometimes advised. The improvement
in sampling resulting from the use of parallel tempering has revealed deﬁciencies in some of the
most popular force ﬁelds used for atomistic simulations, and it would seem that the use of parallel
tempering will be essential in tests of new and improved force ﬁelds.
Parallel tempering can be combined with most other simulation methods, as the exchanges, if done
correctly, maintain the detailed balance or balance condition of the underlying simulation. Thus, there
is almost an unlimited scope for the utilization of the method in computer simulation. This leads to
intriguing possibilities, such as combining parallel tempering with quantum methods.
2.1 Theory of Monte Carlo Parallel Tempering.
In a typical parallel tempering simulation we have M replicas, each in the canonical ensemble, and
each at a different temperature, Ti. In general T1 < T2 < ... < TM, and T1 is normally the temperature
of the system of interest. Since the replicas do not interact energetically, the partition function of this
larger ensemble is given by
i exp[−βiU(rN
where qi =
j=1(2πmjkBTi)3/2 comes from integrating out the momenta, mj is the mass of atom j,
i speciﬁes the positions of the N particles in system i, βi = 1/(kBTi) is the reciprocal temperature,
and U is the potential energy, or the part of the Hamiltonian that does not involve the momenta. If
the probability of performing a swap move is equal for all conditions, exchanges between ensembles
i and j are accepted with the probability
A = min{1, exp
+(βi −βj)(U(rN
Swaps are normally attempted between systems with adjacent temperatures, j = i + 1.
Parallel tempering is an exact method in statistical mechanics, in that it satisﬁes the detailed balance
or balance condition,6 depending on the implementation. This is an important advantage of parallel tempering over simulated annealing, as ensemble averages cannot be deﬁned in the latter method.
Parallel tempering is complementary to any set of Monte Carlo moves for a system at a single temperature, and such single-system moves are performed between each attempted swap. To satisfy detailed
balance, the swap moves must be performed with a certain probability, although performing the swaps
after a ﬁxed number of single-temperature Monte Carlo moves satisﬁes the sufﬁcient condition of balance.6 A typical sequence of swaps and single-temperature Monte Carlo moves is shown in Figure 2.
Kofke conducted an analysis of the average acceptance rate, ⟨A⟩, of exchange trials and argued that
this quantity should be related to the entropy difference between phases.7–9 For systems assumed to
have Gaussian energy distributions, typical of many systems that are studied using computer simulation, see Figure 3, he found the average acceptance ratio, ⟨A⟩, to be given by
(1 + (βj/βi)2)1/2
where Cv is the heat capacity at constant volume, which is assumed to be constant in the temperature
range between βi and βj. Simply put, the acceptance rate for the trials depends on the likelihood
that the system sampling the higher temperature happens to be in a region of phase space that is
important at the lower temperature. This theoretical analysis of the acceptance rates becomes useful
when considering the optimal choice of temperatures for a parallel tempering simulation (see Section
2.2 Theory of Molecular Dynamics Parallel Tempering.
In Monte Carlo implementations of parallel tempering, we need only consider the positions of the
particles in the simulation. In molecular dynamics, we must also take into account the momenta of all
the particles in the system. Sugita and Okamoto proposed a parallel tempering molecular dynamics
method in which after an exchange, the new momenta for replica i, p(i)′, should be determined as
where p(i) are the old momenta for replica i, and Told and Tnew are the temperatures of the replica
before and after the swap, respectively.5 This procedure ensures the average kinetic energy remains
equal to 3
2NkBT. The acceptance criterion for an exchange remains the same as for the MC implementation (Eqn. 2) and satisﬁes detailed balance.
When doing parallel tempering molecular dynamics, one must take care in the interpretation of the
results. A parallel tempering exchange is an ‘unphysical’ move, and so one cannot draw conclusions
about dynamics. That is, when using parallel tempering molecular dynamics, one is only really doing
a form of sampling and not ‘true’ molecular dynamics.
2.3 Optimal Choice of Temperatures.
How one chooses both the number of replicas employed in a parallel tempering simulation and the
temperatures of the replicas are questions of great importance. One wishes to achieve the best possible sampling with the minimum amount of computational effort. The highest temperature must be
sufﬁciently high so as to ensure that no replicas become trapped in local energy minima, while the
number of replicas used must be large enough to ensure swapping occurs between all adjacent replicas. Several suggestions for the number and temperature of the replicas have been offered. It is clear
from Figure 3 and Eqn. 2 that the energy histograms must overlap for swaps to be accepted. Sugita et
al. and Kofke have proposed that the acceptance probability could be made uniform across all of the
different replicas, in an attempt to ensure that each replica spends the same amount of simulation time
at each temperature.5,7,8 Kofke showed that a geometric progression of temperatures ( Ti
Tj = constant)
for systems in which Cv is constant across the temperatures results in equal acceptance ratios. Sanbonmatsu et al. suggested that a target acceptance ratio, Atarget, can be obtained iteratively by solving
Atarget = exp[∆β∆E] ,
where ∆E is the difference in the average energies of the systems in adjacent temperatures.10 Iterative
methods for adjusting the temperatures of the different systems to ensure that acceptance ratios stay
within acceptable bounds had previously been proposed and utilized by Falcioni11 and Schug et al.12 in
adaptive temperature control schemes. Rathore et al.13 extended these approaches to suggest a scheme
for the optimal allocation of temperatures to replicas that is also based on iteratively altering system
temperatures. In their scheme, the lowest temperature is ﬁxed, and the other system temperatures are
determined by iteratively solving
for each of the temperatures, Tj, where σm = [σ(Tj)+σ(Ti)]/2 is the average deviation of the energies
in the two systems. One can choose the target value to achieve a desired acceptance ratio.
Rathore et al. also consider the optimal acceptance ratio and number of replicas in parallel tempering
simulations.13 For the case studies used in their work, they found that an acceptance ratio of 20%
yielded the best possible performance. That is, adding more replicas once the high and low temperatures are ﬁxed and the acceptance ratio of 20% had been achieved resulted in no increase in the
performance of the simulation.
Recently, Kone and Kofke have provided an analysis of the selection of temperature intervals in systems where Cv is assumed to be piecewise constant across each temperature interval.14 They argue
that although this may not always be the case, the assumption is reasonable and does not require an
iterative scheme that can consume valuable CPU time and which violates detailed balance. Their
analysis is based on maximising the mean square displacement, σ2, of a system as it performs the
random walk over temperatures. The value of σ2 is proportional to the number of accepted swaps and
(ln(βj/βi))2. By maximizing σ2 with respect to the acceptance probability, they found that an acceptance probability of 23% is optimal. This value is strikingly similar to the empirically determined
20% of Rathore et al. Kone and Kofke suggest “tuning” the temperature intervals to achieve the 23%
acceptance probability during the initial equilibration of a simulation. This approach appears to be an
efﬁcient method to select temperature intervals in parallel tempering simulations that mix efﬁciently.
A similar scheme for choosing the temperatures has recently been proposed by Katzgraber et al.,
which uses an adaptive feedback-optimized algorithm to minimize round-trip times between the lowest and highest temperatures.15 This approach more directly characterizes the mixing between the high
and low temperature systems. In complex cases, where there are subtle bottlenecks in the probability
of exchange of conﬁgurations, the round-trip time is likely to better characterize the overall efﬁciency
of parallel tempering than is the average acceptance probability. The approach of Katzgraber et al. is
a promising one for such complex cases.
A related issue is how much simulation effort should be expended on each replica. For example, it
would seem that the low temperature replicas would beneﬁt from additional simulation effort, as the
correlation times at lower temperature are longer. This issue is untouched in the literature.
Since the width of the energy histograms increases as
N, but the average energy increases as N,
the number of replicas increases as
N, where N is is the system size.4 One, therefore, would like a
method where only part of the conﬁgurational degrees of freedom are exchanged. Interestingly, this
issue was solved in Swendsen and Wang’s 1986 paper for spin systems,1 but it has not been solved in
an exact, efﬁcient way for atomistic systems. The main difﬁculty seems to be in deﬁning a piece of a
system that can be exchanged without the penalty of a large surface energy.
2.4 Parallel Tempering with Alternative Parameters and Sampling Methods.
The general idea of parallel tempering is not limited to exchanges or swaps between systems at different temperatures. Investigators have developed a number of methods based on swapping alternative
parameters in order to minimize barriers that inhibit correct sampling. Additionally, parallel tempering can be combined with a large number of alternative sampling methods, and its use has led to a
great improvement in the sampling of many existing computational methods.
Fukunishi et al. developed a Hamiltonian parallel tempering method that they applied to biomolecular
systems.16 In this approach, only part of the interaction energy between particles is scaled between the
different replicas. In their work, they conduct case studies using two different implementations of their
approach. In the ﬁrst they scale hydrophobic interactions between replicas. In the second, they scale
the van der Waals interactions between replicas by introducing a cut-off in the interaction, effectively
allowing chains of atoms to pass through each other. The acceptance probability in Hamiltonian
parallel tempering for a swap between replicas i and j is given by
A = min{1, exp [−β ([Hi(X′) + Hj(X)] −[Hi(X) + Hj(X′)])]} ,
where Hi(X) is the Hamiltonian of conﬁguration X in replica i, and conﬁgurations X and X′ are the
conﬁgurations in replicas i and j, respectively, prior to the swap attempt.
Parallel tempering using multiple swapping variables was ﬁrst proposed and developed by Yan and
de Pablo.17,18 Instead of considering a one-dimensional array of replicas at different temperatures,
they suggested using an n-dimensional array, where each dimension represented a parameter that
varied between replicas. Swaps both within and between dimensions were allowed in their scheme.
In their ﬁrst work they conducted parallel tempering between different temperatures and chemical
potentials in the grand canonical ensemble, but the scheme they proposed was general. They showed
that extensions of parallel tempering to multiple dimensions are limited only by the imagination of
the investigator in choosing the variables to swap and the available computational resources. Sugita
et al. utilized multdimensional exchanges in molecular dynamics studies.19
de Pablo and co-workers also implemented parallel tempering in the multicanonical ensemble.20 In
the multicanonical ensemble, the probability distribution is no longer Boltzmann, but becomes
p(rN) = (const)e−βU(rN)w(rN)
The weight factors, w(rN), are chosen so as to lower the barriers in the system. de Pablo and coworkers derived multicanonical weights by an iterative process using a Boltzmann inversion of histograms. Another way to write Eq. (8) is to use instead of the Hamiltonian U, the weighted Hamiltonian U + ξ(U) when attempting swap moves, where ξ(U) is an umbrella potential. By using a multicanonical ensemble, de Pablo and co-workers were able to reduce the number of replicas required in
their simulation, due to a broader overlap of thermodynamic-property histograms. In general, when
combined with a multicanonical simulation, a short parallel tempering run can be performed, and the
multicanonical weight factors can be determined by using histogram reweighting. These weights can
then be used in the multicanonical part of the calculation.21 Parallel tempering can be combined with
a multicanonical simulation. That is, in the multicanonical simulation, a number of replicas, each in
the multicanonical ensemble but each with different multicanonical weight factors covering different
energy ranges, may be employed.21 It should be noted that far fewer replicas are needed in this method
than in typical parallel tempering because the energy ranges covered in a multicanonical simulation
are far wider than in a canonical simulation. The weight factors utilized in these methods may then
be iteratively improved during the equilibration period as the simulation proceeds, using histogram
reweighting techniques.
In free energy perturbation calculations, a parameter λ is introduced. One wishes to compute the free
energy difference to go from an initial (λ = 0) state and a ﬁnal (λ = 1) state. For parallel tempering
with free energy perturbation one can consider M replicas, each with a different λ parameter, where
each replica has a slightly different Hamiltonian
Uλ = Uλ=0 + λ(Uλ=1 −Uλ=0) .
Swaps may be attempted between replicas using the Hamiltonian acceptance criterion (Eq. 7), and the
free energy difference between two lambda parameters can be determined as in regular free energy
calculations. Of course, one may utilize a number of different temperature replicas for each value
of λ in a multidimensional approach. Use of parallel tempering in multicanonical simulations, free
energy calculations, and umbrella sampling is growing.19–25
One of the most fruitful combinations of parallel tempering with existing sampling techniques has
been with density of states methods based on Wang-Landau sampling.26 Density of states methods
are similar to multicanonical ones in that the weight factor is the reciprocal of the density of states.
However, in density of states methods a random walk in energy space is conducted, and a running
estimate of the inverse of the density of states as a function of the energy is performed. Alternatively
the conﬁgurational temperature is collected as a function of the energy and the density of states determined by integrating the inverse temperature.27 Other sets of conjugate variables can also proﬁtably
be used.28 These methods effectively circumvent the tedious and time consuming process of calculating weight factors in multicanonical simulations. de Pablo and co-workers have proposed extended
ensemble density of states methods where overlapping windows or replicas of different energy or
reaction/transition coordinate values are utilized.29 Conﬁgurational swaps between windows are attempted at regular intervals to prevent the simulations in the parallel replicas from becoming stuck in
non-representative regions of phase space. A combination of density of states methods and parallel
tempering has successfully been used to study protein folding30–32 and solid-liquid equilibria.33
Vlugt and Smit applied parallel tempering to the transition path sampling method.34 They showed
that parallel tempering conducted between different temperatures and between different regions along
transition paths is able to overcome the problem of multiple saddle points on a free energy surface.
Parallel tempering transition path sampling can provide for more accurate estimates of transition rates
between stable states than single-temperature Monte Carlo transition path sampling.
Parallel tempering has been combined with a number of other computational methods, and in almost
all cases its use has resulted in better sampling and an increase in the accuracy of the computational
method. Prominent examples include parallel tempering with cavity bias to study the phase diagram
of Lennard-Jones ﬂuids,35 with analytical rebridging for the simulation of cyclic peptides,36 and with
the wormhole algorithm to explore the phase behavior of random copolymer melts.37
Very recently an extension to parallel tempering, known as Virtual-Move Parallel Tempering, has
been proposed by Coluzza and Frenkel.38 In their scheme they include information about all possible
parallel tempering moves between all replicas in the system, rather than just between adjacent replicas,
when accumulating statistical averages. This approach is essentially a parallel tempering version of
the “waste recyling” Monte Carlo method of Frenkel39 and has been shown to improve statistical
averaging by upto a factor of 20.
2.5 Non-Boltzmann Distributions.
Since their introduction in the late 1980s, Tsallis statistics have become increasingly important in
statistical mechanics.40 Due to their power-law, rather than Boltzmann, properties, Tsallis statistics
generally lead to smaller energy barriers. Therefore, optimization with Tsallis, rather than Boltzmann,
statistics can be very useful in energy minimization problems. Whitﬁeld et al. have developed a
version of the parallel tempering algorithm that is based upon Tsallis statistics.41 This method has
been used, for example, for fast conformational searches of peptide molecules.42
Applications
3.1 Polymers.
Simulations of polymeric systems are notoriously difﬁcult due to chain tangling, the high density
of the systems of interest, and the large system sizes required to accurately model high molecular
weight species. The ﬁrst application of parallel tempering to polymeric systems was by Yan and de
Pablo to high molecular weight species.18 Bunker and Dunweg43 were the ﬁrst to utilize excluded volume parallel tempering, where different replicas have different core potentials. They studied polymer
melts for polymer chain lengths ranging from 60 to 200 monomers. Their method created a thermodynamic path from the full excluded volume system to an ideal gas of random walks and increased
the efﬁciency of all their simulations. Bedrov and Smith44 studied fully atomistic polymer melts of
1,4-polybutadiene at a range of temperatures, performing parallel tempering swaps isobarically. They
showed that their parallel tempering approach provided a substantial improvement in equilibration
and sampling of conformational phase space when compared to regular MD simulations. See Figure 4. Theodorou and co-workers studied cis-1,4 polyisoprene melts using parallel tempering and
once again found that use of parallel tempering resulted in far quicker equilibration over a range of
temperatures.45 More recently, Banaszak et al. have utilized hyperparallel tempering in an osmotic
ensemble to study the solubility of ethylene in low-density polyethylene.46 Using their novel method
they were able to examine the effect of both polyethylene chain length and branching on the solubility
of ethylene.
3.2 Proteins.
Biological systems, particularly proteins, are computationally challenging because they have particularly rugged energy landscapes that are difﬁcult for regular Monte Carlo and molecular dynamics
techniques to traverse. Hansmann was the ﬁrst to apply parallel tempering to biological molecules in a
Monte Carlo based study of the simple 7-amino acid Met-enkephalin peptide.3 Hansmann showed that
parallel tempering based simulations could overcome the “simulation slowdown” problem and were
more efﬁcient than regular canonical Monte Carlo simulations. The application of parallel tempering
to biological problems, however, did not take-off until Sugita and Okamoto’s work that introduced the
use of molecular dynamics parallel tempering.5 They applied their approach to Met-enkephalin and
demonstrated that their parallel tempering based method did not get trapped in local energy minima,
unlike regular microcanonical molecular dynamics simulations of the same molecule.
Following demonstration of the power of parallel tempering for molecular systems, its use in the biological simulation community rapidly expanded. Parallel tempering has been used to determine folding free energy contour maps for a number of proteins, revealing details about folding mechanisms
and intermediate state structures47–51 and has facilitated the simulation of membrane proteins.52–55
Parallel tempering has proved to be particularly powerful when applied to NMR structure reﬁnement
and in the interpretation of data from NMR,56–60 circular dichroism,61 IR spectra,62 and electric deﬂection data63 of proteins and peptides. For models of globular proteins and oligomeric peptides, parallel
tempering has been used to study previously unexplored regions of phase diagrams and to sample aggregate transitions.64,65 In the study of sucrose solutions near the glass transition temperature, parallel
tempering simulations showed a much better ﬁt to experimental data than did conventional NPT MC
results.66 Other interesting work using parallel tempering includes studies of the thermodynamics of
ﬁbril formation using an intermediate resolution protein model67 and of the hypervariable regions of
an antibody domain where the chief interactions governing conformational equilibria in these systems
were determined.68
With this increased sampling ability of parallel tempering has come the realization that current force
ﬁelds for biological simulation are lacking in some respects. Parallel tempering simulations of solvated biological molecules have also revealed deﬁciencies in popular implicit solvent models.51,69,70
As parallel tempering can also be used with explicit solvent models, the differences between the treatments can be determined, and in the future such simulations could be used to improve implicit solvent
Brooks and co-workers have developed a multiscale modeling toolkit that can interface with the popular CHARMM and AMBER molecular simulation codes.71 Parallel tempering is implemented in the
toolkit to allow enhanced sampling and is used to study the ab initio folding of peptides from ﬁrst
principles. Parallel tempering has clearly become the method of choice in ab initio protein folding as
evidenced by the the work of Skolnick and co-workers,72 Garcia and Sanbonmatsu,10,73,74 and Yang
When examining the biological and chemical literature of parallel tempering, it is apparent that the
vast majority of work is based on molecular dynamics, rather than Monte Carlo. As one is not doing
‘true’ MD when using parallel tempering, there is no reason why Monte Carlo methodologies cannot
be implemented more frequently in the biological and chemical communities. Indeed, we expect this
to be a promising avenue for future research.
3.3 Solid state.
Crystal structure solution provided one of the ﬁrst mainstream atomistic simulation examples of the
power of parallel tempering. Falcioni and Deem used parallel tempering in a biased MC scheme to
determine the structures of zeolites from powder diffraction data.76 For complicated zeolite structures
containing more than eight unique tetrahedral atoms, simulated annealing is unable to solve the crystal
structures. However, parallel tempering simulations were shown to be able to solve the structures of
all complex zeolites, including the most complicated zeolite structure, ZSM-5, which contains twelve
unique tetrahedral atoms. ZefsaII has since been successfully used to solve the structures of at least
a dozen newly synthesized zeolites and is freely downloadable on the web. A similar approach to
crystal structure determination from powder diffraction data has been implemented by Favre-Nicolin
et al.,77 and this method has been successful in solving several structures.78–80
A seminal simulation study of the rate of crystal nucleation by Auer and Frenkel utilized the parallel
tempering method by allowing swaps between ‘windows’ at different points along the reaction coordinate from the liquid to solid state.81 This work introduced, for the ﬁrst time, the ability to calculate
nucleation rates from ﬁrst principles.
Other examples of solid-state parallel tempering simulations include the computation of sodium ion
distributions in zeolites,82 studying the ﬁnite temperature behavior of C60 clusters,83 the simulation of
Si surfaces,84,85 and the explanation of the titration behavior of MbCO over a range of pH values.86
3.4 Spin glass.
Spin glasses have provided a severe test of the effectiveness of parallel tempering.87 In the Parisi
solution of the inﬁnite range Edwards-Anderson model, widely believed by many but not all physicists
to apply to ﬁnite-range spin glasses as well, there is a ﬁnite energy for excitations above the ground
state, and the boundary of these excitations has a space-ﬁlling structure. Initial simulations for the
Edwards-Anderson model conﬁrmed the ﬁnite excitation energy.88 Initial suggestions for a fractal
surface88 were ruled out by later simulations.89 For the vector spin glass model, the excitation energy
was again found to be ﬁnite.90 Initial suggestions of a fractal surface were also largely ruled out in
later simulations.91
3.5 Quantum.
Quantum level systems, whilst being far more computationally demanding than classical systems,
may beneﬁt from the improved sampling provided by parallel tempering. So far, the main application
of parallel tempering at the quantum level has been in studies of phase transitions and in the location of energy minima in complex systems. Parallel tempering is ideal for these studies, as dynamics
are not of interest. Okamoto and co-workers conducted parallel tempering based ab initio correlated
electronic structure calculations.92 In their studies of Li clusters, they demonstrated that parallel tempering could be successfully applied to systems described with a high level of detail. Sengupta et al.
combined quantum Monte Carlo with parallel tempering to study the phase diagram of a 1-D Hubbard
model.93 Quantum parallel tempering was found to signiﬁcantly reduce “sticking” effects, where the
simulation gets stuck in the incorrect phase close to the phase boundary.
Shin et al. have studied quantum phase transitions of water clusters,94 where the rotational modes can
be highly quantum. Parallel tempering allowed for efﬁcient conformational sampling. They remark
that “combining Car-Parrinello approach with replica exchange [parallel tempering] and path integral
molecular dynamics can provide an ideal methodology for studying quantum behavior of clusters.”
Although the suggested approach is highly computationally expensive, it may become increasingly
feasible in future years. Parallel tempering has also been successfully employed in a study of the
ﬁnite temperature optical spectroscopy of CaArn clusters95 and in quantum path integral simulations
of the solid-liquid phase diagrams of Ne13 and (para-H2)13 clusters.96
3.6 General Optimization Problems.
Parallel tempering has been successfully used in a number of general optimization problems. Habeck
et al. developed a sampling algorithm for the exploration of probability densities that arise in Bayesian
data analysis.97 Their approach utilized Tsallis statistics, and the effectiveness of parallel tempering
was demonstrated by interpreting experimental NMR data for a folded protein. In image analysis,
parallel tempering has been shown to lead to an improvement by a factor of two for both success rate
and mean position error when compared to simulated annealing approaches.98 Parallel tempering has
also been utilized to locate the global minima of complex and rugged potential energy surfaces that
arise in atomistic models of receptor-ligand docking99 and in risk analysis.100
Conclusion
In this review we have given an overview of the history of parallel tempering. We have described
the basic theory and many of the extensions to the original method. Several examples in a variety
of physiochemical arenas have been discussed. Highlighted technical aspects to sort out include best
allocations to cluster computers,101 determination of the optimal amount of simulation effort to expend
on each replica, and partial swapping of partial conﬁguration information for atomistic systems.
A number of potential new areas for application of parallel tempering occur to us. One rather large
one is the application of parallel tempering, rather than simulated annealing,102 to X-ray single-crystal
structure solution. A related issue is the prediction of polymorphs for crystals of small, organic drug
molecules. Also related is use of parallel tempering in rational drug design—most current approaches
use grid searching, traditional Monte Carlo, or at best simulated annealing.103 Another physical application where enhanced sampling might be of use is in ﬁeld theories for polymeric systems with nontrivial phase structure.104 Also possible would be the complementary inclusion in ab initio molecular
dynamics, if sampling only is desired. Even experimental applications could be possible in materials
discovery105 or laboratory protein evolution.106