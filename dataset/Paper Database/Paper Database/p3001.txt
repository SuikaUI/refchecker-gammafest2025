Deep Tracking: Seeing Beyond Seeing
Using Recurrent Neural Networks
Peter Ondr´uˇska and Ingmar Posner
Mobile Robotics Group, University of Oxford, United Kingdom
{ondruska, ingmar}@robots.ox.ac.uk
This paper presents to the best of our knowledge the
ﬁrst end-to-end object tracking approach which directly
maps from raw sensor input to object tracks in sensor
space without requiring any feature engineering or system identiﬁcation in the form of plant or sensor models.
Speciﬁcally, our system accepts a stream of raw sensor
data at one end and, in real-time, produces an estimate
of the entire environment state at the output including
even occluded objects. We achieve this by framing the
problem as a deep learning task and exploit sequence
models in the form of recurrent neural networks to learn
a mapping from sensor measurements to object tracks.
In particular, we propose a learning method based on a
form of input dropout which allows learning in an unsupervised manner, only based on raw, occluded sensor data without access to ground-truth annotations. We
demonstrate our approach using a synthetic dataset designed to mimic the task of tracking objects in 2D laser
data – as commonly encountered in robotics applications – and show that it learns to track many dynamic
objects despite occlusions and the presence of sensor
Introduction
As we demand more from our robots the need arises for them
to operate in increasingly complex, dynamic environments
where scenes – and objects of interest – are often only partially observable. However, successful decision making typically requires complete situational awareness. Commonly
this problem is tackled by a processing pipeline which uses
separate stages of object detection and tracking, both of
which require considerable hand-engineering. Classical approaches to object tracking in highly dynamic environments
require the speciﬁcation of plant and observation models as
well as robust data association.
In recent years neural networks and deep learning approaches have revolutionised how we think about classiﬁcation and detection in a number of domains . The often unreasonable effectiveness of such approaches is commonly attributed to both an ability to learn
Copyright c⃝2016, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
What the robot sees
True world state y
Recurrent neural network
Robot sensor
Figure 1: Often a robot’s sensors provide only partial observations of the surrounding environment. In this work we
leverage a recurrent neural network to effectively reveal occluded parts of a scene by learning to track objects from raw
sensor data – thereby effectively reversing the sensing process.
relevant representations directly from raw data as well as
a vastly increased capacity for function approximation afforded by the depth of the networks .
In this work we propose a framework for deep tracking, which effectively provides an off-the-shelf solution for
learning the dynamics of complex environments directly
from raw sensor data and mapping it to an intuitive representation of a complete and unoccluded scene around the
robot as illustrated in Figure 1.
To achieve this we leverage Recurrent Neural Networks
(RNNs), which were recently demonstrated to be able to
effectively handle sequence data . In particular we consider a complete yet uninterpretable hidden state of the world and then
train the network end-to-end to update its belief of this state
using sequence of partial observations and map it back into
an interpretable unoccluded scene. This gives the network a
freedom to optimally deﬁne the content of the hidden state
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)
and the considered operations without the need of any handengineering.
We demonstrate that such a system can be trained in an
entirely unsupervised manner only based on raw sensor data
and without the need for supervisor annotation. To the best
of our knowledge this is the ﬁrst time when such a system
is demonstrated and we believe our work will provide a new
paradigm for an end-to-end tracking. In particular, our contributions can be summarized as follows:
• A novel, end-to-end way of ﬁltering partial observations
of dynamic scenes using recurrent neural networks to provide a full, unoccluded scene estimation.
• A new method of unsupervised training based on a novel
form of dropout encouraging the network to correctly predict objects in unobserved spaces.
• A compelling demonstration and attached video 1 of
the method in a synthesised scenario of robot tracking
the state of the surrounding environment populated by
many moving objects. The network directly accepts unprocessed raw sensor inputs and learns to predict positions of all objects in real time even through complete
sensor occlusions and noise 2.
Deep Tracking
Formally, our goal is to predict the current, un-occluded
scene around the robot given the sequence of sensor observations, i.e. to model P(yt|x1:t) where yt ∈RN is a discretetime stochastic process with unknown dynamics modelling
the scene around the robot containing other both static and
dynamic objects and xt ⊆yt are the sensor measurements of
directly visible scene parts. We use encoding xt = {vt, rt}
t} = {1, yi
t} if i-th element of yt is observed
and {0, 0} otherwise. Moreover, as the scene changes the
robot can observe different parts of the scene at different
In general, this problem can not be solved effectively if
the process yt is purely random and unpredictable as in such
case there is no way to determine the unobserved elements
of yt. In practice, however, the sequences yt and xt exhibit
structural and temporal regularities which can be exploited
for effective prediction. For example, objects tend to follow
certain motion patterns and have certain appearance. This
allows the behaviour of the objects to be estimated when
they are visible such that later this knowledge can be used
for prediction when they become occluded.
Deep tracking leverages recurrent neural networks to effectively model P(yt|x1:t). In the remainder of this section
we ﬁrst describe the probabilistic model assumed to underlie
the sensing process before detailing how RNNs can be used
for tracking.
Like many approaches to object tracking, our model is inspired by Bayesian ﬁltering . Note, however, the
1Video is available at: 
2The source code of our experiments is available at:
 people/peter-ondruska/
Figure 2: The assumed graphical model of the generative process. World dynamics are modelled by the hidden
Markov process ht with appearance yt which is partially observed by sensor measurements xt.
process yt does not satisfy Markov property: Even knowledge of yt at any particular time t provides only partial information about the state of the world such as object positions,
but does not contain all necessary information such as their
speed or acceleration required for prediction. The latter can
be obtained only by relating subsequent measurements. The
methods such as the Hidden Markov Model 
are therefore not directly applicable.
To handle this problem we assume the generative model
in Figure 2 such that alongside yt there exists another underlying Markov process, ht, which completely captures the
state of the world with the joint probability density
P(y1:N, x1:N, h1:N) =
P(xt|yt)P(yt|ht)P(ht|ht−1),
• P(ht|ht−1) denotes the hidden state transition probability
capturing the dynamics of the world;
• P(yt|ht) is modelling the instantaneous unoccluded sensor space;
• and P(xt|yt) describes the actual sensing process.
The task of estimating P(yt|x1:t) can now be framed in the
context of recursive Bayesian estimation of
the belief Bt which, at any point in time t, corresponds to a
distribution Bel(ht) = P(ht|x1:t). This belief can be computed recursively for the considered model as
P(ht|ht−1)Bel(ht−1)
P(xt|yt)P(yt|ht)Bel−(ht)
where Bel−(ht) denotes belief prediction one time step into
the future and Bel(ht) denotes the corrected belief after the
latest measurement has become available. P(yt|x1:t) in turn
can then be expressed simply using this belief,
P(yt|x1:t)
P(yt|ht)Bel(ht).
Raw occluded
sensor input
neural network
Figure 3: An illustration of the ﬁltering process using a recurrent neural network.
Filtering Using Recurrent Neural Network
Computation of P(yt|x1:t) is carried easily by iteratively updating the current belief Bt
Bt = F(Bt−1, yt)
deﬁned by Equations 2,3 and simultaneously providing prediction
P(yt|x1:t) = P(yt|Bt)
deﬁned by equation 4. Moreover, instead of yt the same
method can be used to predict any state in the future
P(yt+n|x1:t) by providing empty observations of the form
x(t+1):(t+n) = ∅.
For realistic applications this approach is, however, limited by the need for the suitable belief state representation as
well as the explicit knowledge of the distributions in Equation 1 modelling the generative process.
The key idea of our solution is to avoid the need to specify this knowledge and instead use a highly expressive neural networks, governed by weights WF and WP , to model
both F(Bt−1, xt) and P(yt|Bt) allowing to learn their function directly form the data. Speciﬁcally, we assume Bt can
be approximated and represented as a vector, Bt ∈RM. As
illustrated in Figure 3, the function F is then simply a recurrent mapping from RM × R2N →RM corresponding
to an implementation by a Feed-Forward Recurrent Neural
Network where the Bt acts as the
network’s memory passed from one time step to the next.
Importantly, we do not impose any restrictions on the content or function of this hidden representation. Provided both
networks for F(Bt, xt) and P(yt|Bt) are differentiable they
can be trained together end-to-end as a single recurrent network with a cost function directly corresponding to the data
likelihood provided in Equation 4. This allows the neural
networks to adapt to each other and to learn an optimal hidden state representation for Bt together with procedures for
belief updates using partial observations xt and to use this
information to decode the ﬁnal scene state yt. Training of
the network is detailed in the next section while the concrete
architecture used for our experiment is discussed as a part of
Experimental Results.
The network can be trained both in a supervised mode, i.e.
with both known y1:N, x1:N as well as in an unsupervised
mode where only x1:N is known. A common method of supervised training is to minimize the negative log-likelihood
of the true scene state given the sensor input
log P(yt|x1:t)
This objective can be optimised by the gradient descent with
partial derivatives given by backpropagation through time
 
∂F(Bt, xt+1)
−∂log P(yt|Bt)
∂F(Bt−1, xt)
∂log P(yt|Bt)
However, a major disadvantage of the supervised training
is the requirement of the ground-truth scene state yt containing parts not visible by the robot sensor. One way of obtaining this information is to place additional sensors in the
environment which jointly provide information about every
part of the scene. Such an approach, however, can be impractical, costly or even impossible. In the following part we describe a training method which only requires raw, occluded
sensor observations alone. This dramatically increases the
practicality of the method as the only necessary learning information is a long enough record of x1:N as obtained by the
sensor in the ﬁeld.
Unsupervised Training
The aim of unsupervised training is to learn F(Bt−1, xt) and
P(yt|Bt) using only x1:t. This might seem impossible without knowledge of yt as there would be no way to correct the
network predictions. However, some values of yt as xt ⊆yt
are known in the directly observed sensor measurements. A
naive approach would be to train the network using the objective from Equation 7 of predicting the known values of yt
and not back-propagating from the unobserved ones. Such
an approach would fail, however, as the network would only
learn to copy the observed elements of xt to the output and
never predict objects in occlusion.
Instead, and crucially, we propose to train the network not
to predict the current state P(yt|x1:t) but a state in the future P(yt+n|x1:t). In order for the network to correctly predict the visible part of the object after several time steps the
network must learn how to represent and simulate object dynamics during the time when the object is not seen using F
and then convert this information to predict P(yt+n|Bt+n).
This form of learning can be implemented easily by considering the cost function from Equation 7 of predicting visible elements of yt:t+n and dropping-out all xt:t+n by setting
them to 0 marking the input unobserved. This is similar to
standard node dropout to prevent network overﬁtting. Here, we are dropping out the observations
both spatially across the entire scene and temporarily across
multiple time steps in order to prevent the network to overﬁt
to the task of simply copying over the elements. This forces
the network to learn the correct patterns.
An interesting observation demonstrated in the next section is that even though the network is in theory trained to
predict only the future observations xt+n, in order to achieve
this the network learns to predict yt+n. We hypothesise this
is due to the tendency of the regularized network to ﬁnd the
simplest explanation for observations where it is easier to
model P(yt|Bt) than P(xt|Bt) =
yt P(xt|yt)P(yt|Bt).
Experimental Results
In this section we demonstrate the effectiveness of our approach in estimating the full scene state in a simulated scenario representing a robot equipped with a 2D laser scanner surrounded by many dynamic objects. Our inspiration
here is a situation of a robot in a middle of a busy street surrounded by a crowd of moving people. Objects are modelled
as circles independently moving with constant velocity in a
random direction but never colliding with each other or with
the robot. The number of objects present in the scene is varied over time between two and 12 as new objects randomly
appear and later disappear at a distance from the robot.
Sensor Input
At each time step we simulated the sensor input as commonly provided by a planar laser scanner. We divided the
scene yt around the robot using a 2D grid of 50×50 pixels
with the robot placed towards the bottom centre. To model
sensor observations xt we ray-traced the visibility of each
pixel and assigned values xi
t} indicating pixel visibility (by the robot) and presence of an obstacle if the pixel
is visible. An example of the input yt and corresponding observation xt is depicted in Figure 3. Note that, as in the case
of a real sensor, only the surface of the object as seen from
particular viewpoint is ever observed. This input was then
fed to the network as a stream of 50×50 2-channel binary
images for ﬁltering.
Neural Network
To jointly model F(Bt−1, xt) and P(yt|Bt) we used a small
feed-forward recurrent network as illustrated in Figure 4.
This architecture has four layers and uses convolutional operations followed by a sigmoid nonlinearity as the basic information processing step at every layer. The network has in
total 11k parameters and its hyperparameters such as number of channels in each layer and size of the kernels were
set by cross-validation. The belief state Bt is represented by
the 3rd layer of size 50×50×16 which is kept between time
The mapping F(Bt−1, xt) is composed of the stage of
input-preprocessing (the Encoder) followed by a stage of
hidden state propagation (the Belief tracker). The aim of the
Encoder is to analyse the sensor measurements, to perform
operations such as detecting objects directly visible to the
7x7 Kernel
5x5 Kernel
7x7 Kernel
5x5 Kernel
Figure 4: The 4-layer recurrent network architecture used
for the experiment. Directly visible objects are detected in
Encoder and fed into Belief tracker to update belief Bt used
for scene deocclusion in Decoder.
sensor and to convert this information into a 50×50×8 embedding as input to the Belief tracker. This information is
concatenated with the previous belief state Bt−1 and in Belief tracker combined into a new belief state Bt.
Finally, P(yt|Bt) is modelled by a Bernoulli distribution
by interpreting the network ﬁnal layer output as a probabilistic occupancy grid pt of the corresponding pixels being part
of an object giving rise to the total joint probability
P(yt|pt) =
with logarithm corresponding to the binary cross-entropy.
One pass through the network takes 10ms on a standard laptop drawing the method suitable for real-time data ﬁltering.
We generated 10,000 sequences of length 200 time steps and
trained the network for a total of 50,000 iterations using
stochastic gradient descent with learning rate 0.9. The initial belief Bt was modelled as a free parameter being jointly
optimised with the network weights WF , WP .
Both supervised (i.e. when the groundtruth of yt is
known) and un-supervised training were attempted with almost identical results. Figure 5 illustrates the unsupervised
training progress. Initially, the network produces random
output, then it gradually learns to predict the correct shape of
the visible objects and ﬁnally it learns to track their position
even through complete occlusion. As illustrated in the attached video, the fully trained network is able to conﬁdently
and correctly start tracking an object immediately after it
has seen even only a part of it and is then able to conﬁdently
keep predicting its position even through long periods of occlusion.
Finally, Figure 6 shows example activations of layers
from different parts of the network. It can be seen that the
Encoder module learned to produce in one of its layers a
spike at the center of visible objects. Particularly interesting
is the learned structure of the belief state. Here, the network
true positive
false positive
false negative
true negative (visible)
true negative (occluded)
Network output
Network input
Training cost
Figure 5: The unsupervised training progress. From left to right, the network ﬁrst learns to complete objects and then it learns
to track the objects through occlusions. Some situations can not be predicted correctly such as the object in occlusion on the
fourth frame not seen before. Note, the ground-truth for comparison was used only for evaluation, but not during the training.
learned to represent hypotheses of objects having different
motion patterns with different patterns of unit activations
and to track this pattern from frame to frame. None of this
behaviour is hard-coded and it is a pure result of the network
adaptation to the task.
Additionally we performed experiments changing the
shape of objects from circles to squares and also simulating
sensor noise on 1% of all pixel observations3. In all cases
the network correctly learned and predicted the world state,
suggesting the approach and the learning procedure is able
to perform well in a variety of scenarios. We however expect more complex cases will require networks having more
intermediate layers or using different units such as LSTM
 .
3See the attached video.
Related Works
Our work concerns modelling partially-observable stochastic processes with a particular focus on the application
of tracking objects. This problem is commonly solved by
Bayesian ﬁltering which gave rise to tracking methods for a
variety of domains . Commonly, in these approaches the state representation is designed by hand and the prediction and correction operations
are made tractable under a number of assumption on model
distributions or via sampling based methods. The Kalman
ﬁlter , for example, assumes a multivariate
normal distribution to represent the belief over the latent
state leading to the well-known prediction/update equations
but limiting its expressive power. In contrast, the Particle ﬁlter foregoes any assumptions on belief distributions and employs a sample-based approach. In addition, where multiple objects are to be tracked,
correct data association is crucial for the tracking process to
Belief tracker
Figure 6: Examples of the ﬁlter activations from different parts of the network (one ﬁlter per layer). The Encoder adapts to
produce a spike at the position of directly visible objects. Interesting is the structure of the belief Bt. As highlighted by circles
in 2nd row, it adapted to assign different activation patterns to objects having different motion patterns (best viewed in colour).
In contrast to using such hand-designed pipelines we provide a novel end-to-end trainable solution allowing to automatically learn an appropriate belief state representation as
well as the corresponding predict and update operations for
an environment containing multiple objects with different
appearance and potentially very complex behaviour. While
we are not the ﬁrst to leverage the expressive power of neural
networks in the context of tracking, prior art in this domain
primarily concerned only the detection part of the tracking
pipeline , .
A full neural-network pipeline requires the ability to learn
and simulate the dynamics of the underlying system state in
the absence of measurements such as when the tracked object becomes occluded. Modelling complex dynamic scenes
however requires modelling distributions of exponentiallylarge state representations such as real-valued vectors having thousands of elements. A pivotal work to model highdimensional sequences was based on using Temporal Restricted Boltzman Machines .
This generative model allows to model the joint distribution of P(y, x). The downside of using an RBM, however,
is the need of sampling, making the inference and learning
computationally expensive. In our work, we assume an underlying generative model but, in the spirit of Conditional
Random Fields we
directly model only P(y|x). Instead of RBM our architecture features Feed-forward Recurrent Neural Networks
 making the inference
and weight gradients computation exact using a standard
back-propagation learning procedure . Moreover, unlike undirected models,
a feed-forward network allows the freedom to straightforwardly apply a wide variety of network architectures such as
fully-connected layers and LSTM to design the network. Our used architecture is
similar to recent Encoder-Recurrent-Decoder and similarly to features convolutions for spatial processing.
Conclusions
In this work we presented Deep Tracking, an end-to-end approach using recurrent neural networks to map directly from
raw sensor data to an interpretable yet hidden sensor space,
and employ it to predict the unoccluded state of the entire
scene in a simulated 2D sensing application. The method
avoids any hand-crafting of plant or sensor models and instead learns the corresponding models directly from raw,
occluded sensor data. The approach was demonstrated on
a synthetic dataset where it achieved highly faithful reconstructions of the underlying world model.
As future work our aim is to evaluate Deep Tracking on
real data gathered by robots in a variety of situations such as
pedestrianised areas or in the context of autonomous driving in the presence of other trafﬁc participant. In both situa-
tions knowledge of the likely unoccluded scene is a pivotal
requirement for robust robot decision making. We further intend to extend our approach to different modalities such as
3D point-cloud data and depth cameras.