UC San Diego
Recent Work
Tests of Conditional Predictive Ability
 
Giacomini, Raffaella
White, Halbert
Publication Date
2003-06-01
eScholarship.org
Powered by the California Digital Library
University of California
Tests of conditional predictive ability
Raﬀaella Giacomi ni and Hal b ert White∗
University of California, San Di ego
Thi s ve rs i on: Apr i l 2 00 3
We argue that the current framework for predictive ability testing is not
necessarily useful for real-time forecast selection, i.e., for assessing which of two competing
forecasting methods will perform better in the future. We propose an alternative framework
for out-of-sample comparison of predictive ability which delivers more practically relevant conclusions. Our approach is based on inference about conditional expectations of forecasts and
forecast errors rather than the unconditional expectations that are the focus of the existing
literature. We capture important determinants of forecast performance that are neglected in
the existing literature by evaluating what we call the forecasting method (the model and the
parameter estimation procedure), rather than just the forecasting model. Compared to previous approaches, our tests are valid under more general data assumptions (heterogeneity rather
than stationarity) and estimation metho ds, and they c an handle comparison of b oth nested and
non-nested models, which is not currently possible. To illustrate the usefulness of the proposed
tests, we compare the forecast performance of three leading parameter-reduction methods for
macroeconomic forecasting using a large number of predictors: a sequential model selection approach, the “diﬀusion indexes” approach of Sto ck and Watson , and the u se of Baye si an
shrinkage estimators.
∗Discussions with Clive Granger, Graham Elliott and Andrew Patton were essential to the paper. We also wish to
thank L utz K ilian for insightful su gg estions a nd Fa rshid Vahid and semin ar participants at UCSD , N uﬃ eld C olle ge ,
LSE, University of Exeter, University of Warwick, University of Manchester, Cass Business School, North Carolina
State University, Boston College, Texas A&M, University of Chicago GSB, the International Finance Division of the
Federal Reserve Board and the 2002 EC2 conference in Bologna, Italy for helpful comments. The computations in
the paper were carried out in the UCSD Experimental and Computational Laboratory, for which we thank Vince
Craw ford.
C orresp onding author: R aﬀ aella G ia comini, D epartm ent o f E conomics, U nive rsity of C alifornia, San
Diego, 9500 Gilman Dr., La Jolla, CA 92093-0508, U.S.A. Web page: E-mail:
 .
JEL: C12, C22, C52, C53
Keywords: Forecast Evaluation, Asymptotic Inference, Parameter-Reduction Methods
Introduction
Forecasting is central to economic decision-making. Government institutions and regulatory authorities often base policy decisions on forecasts of major economic variables, and ﬁrms rely on
forecasting for inventory management and production planning decisions. A problem that economic forecasters often face is how to select the best forecasting method from a set of two (or
more) alternatives. The econometric answer to this problem is to develop tests for comparing the
predictive ability of two alternative forecast methods, given the forecaster’s loss function. The
literature on forecast comparison has witnessed a renaissance in recent years, and a number of authors have proposed econometric techniques for forecast comparison under general loss functions,
known as out-of-sample predictive ability testing. This literature was initiated by Diebold and
Mariano and further formalized by West , West and McCracken , McCracken
 , Clark and McCracken , Corradi, Swanson and Olivetti , Chao, Corradi and
Swanson , among others, and it represents a generalization of several existing evaluation techniques which typically restricted attention to a particular loss function .
In this paper, we argue that the current framework for out-of-sample predictive ability testing
 is not necessarily
appropriate for real-time forecast selection, i.e., for assessing which of two competing forecasting
methods will give better forecasts in the future.
We propose an alternative approach to outof-sample predictive ability testing that delivers inferences that are more relevant to economic
forecasters. Our tests can be applied to multi-step point, interval, probability or density forecasting,
and they can be viewed as a generalization of the tests of West since they are applicable in
all cases in which his tests are applicable and in many more besides.
From a methodological point of view, the main idea of the paper is to view the problem of
forecast evaluation as a problem in inference about conditional expectations of forecasts and forecast
errors rather than the unconditional expectations that are the focus of the approach of West .
An important distinction between our approach and the existing literature is that we consider
the object of the evaluation to be what we call the “forecasting method”, which includes not only
the forecast model but also a number of choices that must be made by the forecaster at the time of
the prediction, such as which estimation procedure to choose and which data to use for estimation.
The current approach to forecast evaluation focuses instead solely on the forecast model. The reason
for evaluating the forecasting method and not just the model is that all elements of the method
can aﬀect future forecast performance: a good model can produce bad forecasts if its parameters
are not precisely estimated or if they change over time. The fact that we consider the forecasting
method rather than the model implies that our tests can lead to a diﬀerent conclusion than West’s
 tests. Suppose for example that one of the two models is correctly speciﬁed but has a large
number of parameters, while the competitor is a simpler, misspeciﬁed model. West’s test
will tend to choose the large model, while our tests may choose the forecasting method that uses
the small model, especially if we are in the presence of high estimation uncertainty.
Our approach is applicable in many situations where the tests of West are not valid.
One such case is when the data are heterogeneous, in the form of time-varying underlying processes
for the series of interest. As emphasized by Clements and Hendry , this is a more
realistic assumption for economic forecasting contexts than the assumption of stationarity that
is typically made in the literature.
The assumption of heterogeneity also aﬀects the approach
to estimation. In this context, instead of considering a recursive forecasting scheme, where the
estimation window expands over time, it makes sense to consider a rolling window forecast procedure
where the forecasts are based on a moving window of the data which discards old observations. The
size of the estimation window can itself be time-varying, as in the procedure suggested by Pesaran
and Timmermann . A fundamental diﬀerence with the existing literature is that here we
consider the estimation window to be a component of the forecasting method under evaluation.
In the existing literature, instead, the sample split between estimation and evaluation samples is
arbitrary and typically there is little guidance on how to choose it in practice.
Another situation where the tests of West are not applicable is in comparing forecasts
based on nested models. This is an important comparison because many models considered for
forecasting are naturally derived as generalizations of existing models and it is often of interest to
test if a larger, more sophisticated model can outperform a simple, nested benchmark model. Our
framework permits a uniﬁed treatment of nested and non-nested models.
Finally, the current framework for predictive ability testing is not valid when the forecasts
are obtained by using estimation methods such as Bayesian estimation, semi-parametric, or nonparametric estimation. Our framework, instead, can accommodate such estimation procedures and
can thus be used to compare the impact on forecast performance of using diﬀerent estimation
techniques, a question that cannot be answered within the current framework.
A ﬁnal, practical advantage of our tests is that they are easily computed using standard regression packages, whereas the existing tests can be quite diﬃcult to compute or have limiting
distributions that are context-speciﬁc .
To illustrate the usefulness of the conditional predictive ability tests, we consider the problem
of macroeconomic forecasting using a large number of predictors and compare forecasts of eight
macroeconomic variables obtained by employing leading methods for parameter reduction: a sequential procedure which is a simpliﬁed version of the general-to-speciﬁc model selection approach
implemented by Hoover and Perez , the “diﬀusion indexes” approach of Stock and Watson
 and the use of Bayesian shrinkage estimators . We use the data set of Stock
and Watson , including monthly U.S. data on 146 macroeconomic variables and evaluate 1-,
6- and 12-month ahead forecasts of four measures of real activity and four price indexes obtained
using the diﬀerent forecasting methods. The general conclusion is that for the price indexes the
forecast performance of the three methods is indistinguishable from that of a univariate autoregression. For the real variables, instead, Bayesian shrinkage appears to be the preferred method.
Finally, the sequential model selection approach performs poorly for most variables and forecast
horizons, and it is often outperformed by the naive autoregressive and random walk benchmarks.
Unconditional and conditional approaches to predictive ability
To illustrate the diﬀerences between conditional and unconditional out-of-sample predictive ability
testing, suppose we are interested in comparing the accuracy of two competing forecasting models
ft(β1) and gt(β2) for the conditional mean of the variable of interest Yt+1, given a squared error loss
function. The dependence of the forecasts on parameters β1 and β2 indicates that in this example
the forecasting models are parametric. The approach of West consists of testing the null
hypothesis of equal accuracy of the two forecasts formulated as
H0 : E[(Yt+1 −ft(β∗
1))2 −(Yt+1 −gt(β∗
2))2] = 0,
2 are population values of the parameters (i.e., probability limits of the parameter
estimates). The null hypothesis (1) can be interpreted as saying that the two forecast models are
equally accurate on average. If the null hypothesis is rejected, one would choose the model yielding
the lower loss. Notice that a test of the null hypothesis (1) will tend to choose a forecast based on
a correctly speciﬁed model (i.e., the test will choose the correctly speciﬁed model asymptotically
with probability 1 −α, where α is the level of the test).1 A focus on the null hypothesis (1) is thus
justiﬁable if one is interested in establishing which of two models better approximates the datagenerating process. However, even a model that well approximates the data-generating process
may forecast poorly, for example in the case that its parameters are imprecisely estimated. If the
question is which model will give better forecasts in the future, therefore, it is not clear that (1) is
in fact the appropriate null hypothesis.
The central idea of this paper is to test a null hypothesis diﬀerent than (1), where the expectation
is conditional on the information set Ft available at time t and the losses depend on the parameter
1To see why, suppose forecast 1 is based on a correctly speciﬁed model, which implies that ft(β∗
1) is the true
conditional mean of Yt+1. Also assume for simplicity that the two forecasts are based on the same information set.
Since the conditional mean is the optimal forecast for a squared error loss function, ft(β∗
1) minimizes the expected
loss: E[(Yt+1 −ft(β∗
1))2] < E[(Yt+1 −f 0
t)2] for any other forecast f 0
t, and thus in particular for gt(β∗
estimates at time t, ˆβ1t and ˆβ2t, rather than on their probability limits:
H0 : E[(Yt+1 −ft(ˆβ1t))2 −(Yt+1 −gt(ˆβ2t))2|Ft] = 0 almost surely, t = 1, 2, ...
We call a test of the hypothesis (2) a test of equal conditional predictive ability. The motivation for
conducting inference about a conditional, rather than an unconditional, expectation is that it more
closely represents the real-time problem of a forecaster. In particular, we can view this hypothesis
as saying that the forecaster cannot predict which of the two forecasts will be more accurate, given
what is known today.
Further motivation for expressing the null hypothesis in terms of time-t parameter estimates
rather than probability limits is that they are more relevant for the forecaster: since the population
parameters are not known and must be estimated, it is the actual future loss that is of interest to
the forecaster, rather than that based on some population value that is only attained in the limit.
As a result, whereas the unconditional tests restrict attention to the forecast model, the conditional
approach allows evaluation of the forecasting method, which includes the model, the estimation
procedure and the possible choice of estimation window. Viewed this way, it appears obvious that
considering the forecasting method as a whole is appropriate, as each of its components can have
a potential impact on future forecast performance.
In the following subsections, we outline in detail the directions along which the conditional
testing framework represents a more realistic environment for forecast evaluation and discuss how
it directly accounts for diﬀerent determinants of forecast performance that are neglected by the
unconditional framework.
Heterogeneity of economic data
One of the conclusions of Clements and Hendry is that the main explanation for systematic forecast failure in economics is a non-constant underlying process generating the series to
be forecast. It is thus of fundamental importance to develop evaluation techniques that take into
account the possibly heterogeneous nature of economic variables. In this paper, we therefore work
with the assumption that the data generating process is heterogeneous rather than stationary.2 In
our view, this is a realistic and practical assumption for economic forecasting contexts and more
plausible than the perhaps idealistic assumption of stationarity typically made in the unconditional
predictive ability literature. Speciﬁc sources for heterogeneity in the series that economists forecast
are several. First, even if the underlying economic processes were stationary, heterogeneity in the
observed time series can arise from changes in the measurement process. This source of heterogeneity is one that macroeconomic variables are particularly sensitive to; among other things: the
2The type of non-stationarity we consider here is that induced by a distribution that changes over time. We also
assume short memory, thus ruling out non-stationarity due to the presence of unit roots.
deﬁnition of the measured variables may change from time to time; which entities are measured
in constructing the variables measured changes; budgets for the economic, demographic, and statistical agencies measuring the processes of interest change, leading to the possibility of greater
or lesser care in producing the oﬃcial numbers on strict time schedules; and directors and other
key personnel of these agencies regularly join and leave, leading to intentional or unintentional
variations in the processes and procedures that produce the oﬃcial time series. Heterogeneity in
even one of these sources would produce heterogeneity in the observed series. These sources of
heterogeneity are plausibly less a concern for non-aggregated time series, such as the prices of welldeﬁned commodities, as in ﬁnancial economics. Nevertheless, the underlying economic processes
themselves are comprised of a variety of forces that operate as further sources of heterogeneity,
aﬀecting either the nature of the commodity itself, or the way the commodity is traded. With
regard to the latter, the laws and regulations governing trade change, and the technologies used by
buyers and sellers of the commodities change. (The onward march of both computing and software
technology is an obvious example.) With regard to the former, the laws and regulations governing
for example the behavior of ﬁrms represented by equity assets change, as do market conditions
and technologies used by such ﬁrms. Taken together, these factors make it plausible in our view
that the relations between variables of interest this month and next relevant for forecasting are
somewhat diﬀerent now than they were last year, let alone ﬁve, ten, or twenty years ago, and are
not plausibly identical, as stationarity would require.
If heterogeneity is accepted as an accurate description of economic time series, appropriate
methods for model-based forecasting and forecast evaluation need to be applied. In general, it
seems appropriate in a time-varying environment to consider estimators with ﬁnite memory, rather
than basing forecasts on an expanding window of data. An example is the practice of specifying
and estimating forecast models over a rolling window of the data, as a way to accommodate a data
generating process that varies slowly over time .
The size of the estimation window may itself be time-varying, as was recently suggested by Pesaran
and Timmermann , who propose a recursive procedure which detects breaks in real time and
then uses an appropriate subset of the data for estimation. Further, the estimators can usefully
incorporate time weights which may assign decreasing importance to observations from the more
distant past. The use of these methods in the production of forecasts has important implications
for the evaluation procedure. The approach to out-of-sample testing in the unconditional predictive
ability framework is to arbitrarily split the data into an estimation and an evaluation sample, and
to obtain the asymptotic distribution of the test statistic under the assumption that both the insample and the out-of-sample sizes diverge to inﬁnity. The choice of sample split is thus a ﬁnite
sample artiﬁce. In contrast, in our conditional framework the size of the estimation window and
the possible time weighting are treated as choice variables of the forecast method, and as such
they can be evaluated along with the forecast model and the estimation procedure as parts of the
forecasting method under analysis.
Estimation uncertainty
As emphasized by Clements and Hendry and Ericsson , parameter estimation
uncertainty is an important determinant of forecast performance. Our conditional tests directly
account for the eﬀects of estimation uncertainty on forecast performance by expressing the null
hypothesis in terms of parameter estimates and by considering ﬁnite window estimation, which leads
to asymptotically non-vanishing estimation uncertainty. In contrast, the unconditional framework
does not take into account diﬀering model complexities, unless explicitly incorporated into the
accuracy measure (e.g., AIC or BIC); further, the presence of probability limits in the unconditional
null hypothesis (1) means that diﬀerent estimators that converge to the same limit will lead to the
same conclusion. As a result, the unconditional tests are not able to detect superior forecasting
performance which is due to reduced estimation uncertainty. For example, consider the case of
comparing the accuracy of nested models in the unconditional framework. If the smaller model
is correctly speciﬁed, the forecast errors from the two models calculated at the probability limits
of the parameters are identical, and the null hypothesis (1) is automatically satisﬁed (this would
hold for any loss function). In other words, one would conclude that the two models yield equally
accurate forecasts, regardless of the number of excess parameters contained in the larger model. A
test of this sort may thus lead to misleading conclusions if the goal is real-time forecast selection.
Out-of-sample versus in-sample testing
The literature on forecast evaluation has long argued that out-of-sample, rather than in-sample,
testing is the “true” test of a forecast model. As Granger observes, the potentially
large number of parameters compared with the relative scarcity of macroeconomic data “leads to
worries that a model presented for consideration is the result of considerable speciﬁcation searching
..., data mining, or data snooping (in which data are used several times). Such a model might well
appear to ﬁt the data, in sample, rather well but will often not perform satisfactorily out-of-sample”.
This is related to the problem of overﬁtting: a good ﬁt may result from explaining not only the
stable relationships that are useful for forecasting but also possible accidental relationships that are
speciﬁc to the sample. Out-of-sample evaluation of forecast performance, on the contrary, simulates
a real-time forecast scenario where the quality of a forecasting method is directly measured against
the actual data. The unreliability of in-sample testing is particularly evident if the data in the
sample are generated by a time-varying process, whereas out-of-sample testing can incorporate
this heterogeneity through recursive speciﬁcation and estimation of the model. Our conditional
testing framework is fully congruent with this motivation for out-of-sample testing: it is valid
under heterogeneity of the data-generating process, and it is based on the forecasts and forecast
errors actually observed, rather than viewing them as estimates of some population quantities. In
the unconditional predictive ability framework, on the other hand, it is less clear why one should
use out-of-sample rather than in-sample testing if the goal is to test hypotheses about population
parameters under the assumption of stationarity. This point is well made by Inoue and Kilian
 , who argue that if the goal of the testing procedure is to assess population predictability
(corresponding to testing a null hypothesis of equal unconditional predictive ability of two nested
models, e.g., as in (1)), the use of out-of-sample testing involves an unnecessary loss of information,
whereas the in-sample test of the same hypothesis utilizes all the information available and thus
leads to power gains in ﬁnite samples.
Practical advantages of the conditional tests
In addition to the methodological considerations just articulated, there are also signiﬁcant practical
advantages to the approach advocated here. The main beneﬁt of our approach is that it allows a
uniﬁed treatment of nested and non-nested models, while the existing testing framework of West
 is only valid under non-nestedness. Unconditional tests of predictive ability for nested models
have been proposed by Clark and McCracken , among others, but they lack the general
applicability of West ’s results, as the test statistics have complicated limiting distributions
that are context-speciﬁc. As discussed in section 3.2, the diﬀerent treatment of nested and nonnested models in the unconditional framework is due to the fact that the asymptotic distribution
of the test statistic relies on convergence of the parameter estimates to their probability limits, and
this limiting behavior diﬀers in the two cases. The fact that we don’t rely on such convergence
in the conditional approach instead makes it possible to consider nested and non-nested models
in the same framework. A second advantage of the conditional tests is that they do not impose
restrictions on the estimation procedure utilized to produce the forecasts, while the approach of
West rules out, e.g., Bayesian, semi-parametric, and non-parametric estimation. Finally,
our tests are simple to compute due to the imposition of a particular time dependence structure
under the null hypothesis (e.g., martingale diﬀerence sequences for the one-step-ahead forecasts),
which leads to a computationally simple expression for the asymptotic variance estimator.
Description of the environment
Consider a stochastic process W ≡{Wt : Ω−→Rs+1, s ∈N, t = 1, . . . , T} deﬁned on a complete
probability space (Ω, F, P). We partition the observed vector Wt as Wt ≡(Yt, X0
t)0, where Yt : Ω→
R is the variable of interest and Xt : Ω→Rs is a vector of predictor variables, and we deﬁne
Ft = σ(W 0
1, ..., W 0
t+1)0 . We adopt the standard convention of
denoting random variables by upper case letters and realizations by lower case letters.
We focus for simplicity on univariate forecasts.
Consider a situation where two alternative
models are used to forecast the variable of interest τ steps ahead, Yt+τ. The forecasts are formulated at time t and are based on the information set Ft. Denote the two forecasts by ˆfm,t ≡
f(wt, wt−1, ..., wt−m+1; ˆβm,t) and ˆgm,t ≡g(wt, wt−1, ..., wt−m+1; ˆβm,t), where f and g are measurable
functions. The subscripts indicate that the forecast formulated at time t is a measurable function
of a sample of at most size m, consisting of the m most recent observations. Recall that we do not
restrict attention to point forecasting. Our framework accommodates evaluation of point, interval,
probability, and density forecasts. If the forecasts are based on parametric models, the parameter
estimates from the two models are collected in the k × 1 vector ˆβm,t. Otherwise, ˆβm,t represents
whatever semi-parametric or non-parametric estimators are used in constructing forecasts. The
estimator ˆβm,t can be further selected to minimize a weighted loss function over the estimation
period, where smaller weights are typically assigned to observations from the more distant past.
For example, for a linear model Yt = Xtβ + ut and a quadratic loss function, we can consider the
family of weighted least squares estimators ˆβm,t = minβ
s=t−m+1(ys−xsβ)2wm,s, where {wm,s} is
a sequence of weights assigned to the observations in the estimation sample that can be selected by
the user (for example one may assign exponentially decreasing weights to the observations further
away from t).
We emphasize that the estimators may be parametric, semi-parametric or non-parametric. The
only requirement here is that m (the maximum estimation window size) must be ﬁnite. All of the
elements listed above - the model, the estimation procedure, the size of the estimation window and
the estimation weight function - are treated as dimensions of choice by the user and are part of
what we call the “forecasting method” under evaluation.
The evaluation is performed in a simulated out-of-sample fashion. Let T be the size of the sample available. Since the data indexed 1, ..., m are used for estimation of the ﬁrst set of parameters,
the ﬁrst τ−step ahead forecasts are formulated at time m and compared to the realization ym+τ.
The second set of forecasts is produced by moving the estimation window forward one step and
estimating the parameters on data indexed 2, ..., m + 1. These forecasts are compared to the realization ym+1+τ. The procedure is thus iterated and the last forecasts are generated at time T −τ,
by estimating the parameters on data indexed T −τ −m + 1, ..., T −τ, and they are compared to
yT. This rolling window procedure yields a sequence of n ≡T −τ −m + 1 forecasts and relative
forecast errors.
The sequence of out-of-sample forecasts thus produced is evaluated by selecting a loss function
Lt+τ(Yt+τ, ˆfm,t), which depends on the forecasts and on the realizations of the variable. This loss
function is either an economically meaningful criterion such as utility or proﬁts or a statistical measure of accuracy. The following
are some examples of statistical loss functions that have been considered in the forecast evaluation
literature. Examples of appropriate loss functions for the evaluation of quantile, probability, and
density forecasts are also discussed in Diebold and Lopez , Lopez , Giacomini and
Komunjer and Giacomini . For simplicity, let ft ≡ˆfm,t and consider τ = 1.
1. Squared error loss function: Lt+1(Yt+1, ft) = (Yt+1 −ft)2.
2. Absolute error loss function: Lt+1(Yt+1, ft) = |Yt+1 −ft|.
3. Asymmetric linear cost function of order α (also known as the lin-lin or “tick function”):
Lt+1(Yt+1, ft) = (α −1(Yt+1 −ft < 0))(Yt+1 −ft), for α ∈(0, 1).
4. Linex loss function: Lt+1(Yt+1, ft) = exp(a(Yt+1 −ft)) −a(Yt+1 −ft) −1, a ∈R.
5. Direction-of-change loss function: Lt+1(Yt+1, ft) = 1{sign(Yt+1 −Yt) 6= sign(ft −Yt)}.
6. Predictive log-likelihood: Lt+1(Yt+1, ft) = log ft(Yt+1), where ft is in this case the density
forecast of Yt+1
One-step conditional predictive ability test
For a given loss function, we write the null hypothesis of equal conditional predictive ability of
forecasts f and g as
E[Lt+τ(Yt+τ, ˆfm,t) −Lt+τ(Yt+τ, ˆgm,t)|Ft]
E[∆Lm,t+τ|Ft] = 0 almost surely t = 1, 2, ... .
Due to certain computational issues, we consider separately the cases of one-step and multi-step
forecast horizons.
Null hypothesis
When τ = 1, the null hypothesis claims that the out-of-sample sequence {∆Lm,t+1, Ft} is a martingale diﬀerence sequence (mds). In this case, the conditional moment restriction (3) is equivalent
to stating that E[ht∆Lm,t+1] = 0, for all Ft−measurable functions ht. Let us restrict attention
to a given subset of such functions, which we collectively denote by the q × 1 Ft−measurable
vector ht and follow Stinchcombe and White by referring to this as the “test function”.
For a given choice of test function ht, we construct a test exploiting the consequence of the mds
property that H0,h : E[ht∆Lm,t+1] = 0. The standard unconditional approach to predictive ability
testing corresponds to testing the hypothesis H0,h with ht = 1 and with the parameter estimate
ˆβm,t replaced with its probability limit β∗.
For ﬁxed m, standard asymptotic normality arguments suggest using a Wald-type test statistic
of the form
n,m = n(n−1
ht∆Lm,t+1)0 ˆΩ−1
ht∆Lm,t+1) = n ¯Z0
where ¯Zm,n ≡n−1 PT−1
t=m Zm,t+1, Zm,t+1 ≡ht∆Lm,t+1 and ˆΩn ≡n−1 PT−1
t=m Zm,t+1Zm,t+10 is a q ×q
matrix consistently estimating the variance of Zm,t+1.
A level α test can be conducted by rejecting the null hypothesis of equal conditional predictive
ability whenever T h
q,1−α, where χ2
q,1−α is the (1 −α)−quantile of a χ2
q distribution. The
asymptotic justiﬁcation for the test is provided in the following theorem, which characterizes the
behavior of the test statistic (4) under the null hypothesis.
Theorem 1 (Conditional predictive accuracy test) For forecast horizon τ = 1, maximum
estimation window size m < ∞and q × 1 test function sequence {ht} suppose:
(i) {Wt}, {ht} are mixing sequences with φ of size −r/(2r −1), r ≥1 or α of size −r/(r −1),
(ii) E|Zm,t+1,i|2(r+δ) < ∆< ∞for some δ > 0, i = 1, ..., q and for all t;
(iii) Ωn ≡n−1 PT−1
t=m E[Zm,t+1Z0
m,t+1] is uniformly positive deﬁnite.
Then, under H0 in (3), T h
q as n →∞.
Comments: 1. Assumption (i) is mild, allowing the data to be characterized by considerable
heterogeneity as well as dependence. This is in contrast with the existing literature, which typically
assumes stationarity of the loss diﬀerences.
2. The asymptotic distribution is obtained for the number of out-of-sample observations going
to inﬁnity, whereas the estimation sample size m remains ﬁnite.
This leads to asymptotically
non-vanishing estimation uncertainty. In contrast, in the unconditional framework of West ,
both the in-sample and the out-of-sample sizes grow, causing estimation uncertainty to vanish
asymptotically.
A result of letting both m and n grow is that the choice of how to split the
available sample into in-sample and out-of-sample portions is arbitrary, while in our framework the
choice of estimation window (up to some maximum m) is part of the forecasting method under
evaluation. Also notice that the requirement of ﬁnite estimation window rules out the use of a
recursive forecasting scheme, which utilizes an expanding estimation window.
3. Assumption (iii), imposing positive deﬁniteness of the asymptotic variance of the test statistic, is related to a similar requirement made in the existing literature about predictive ability testing
 , but it diﬀers in a fundamental way. In that literature, the size
of the estimation window is assumed to grow at the same rate or faster than the out-of-sample size,
which means that the asymptotic variance of the test statistic is computed at the probability limits
of the parameters. Because of the focus on this limiting behavior, the asymptotic variance matrix
may be singular when the forecasts are based on nested models. In contrast, in the conditional
framework the size of the estimation window remains ﬁnite as the prediction sample size n grows to
inﬁnity, which prevents the parameter estimates from reaching their probability limits. This makes
our tests applicable to both nested and non-nested models.
4. The test statistic for conditional predictive ability test is straightforward to compute. A
further simplifying feature is the fact that the null hypothesis imposes a particular time dependence
structure (in this case that of a martingale diﬀerence sequence), which implies that the asymptotic
variance can be consistently estimated by the sample variance.
The following results provide computationally convenient ways to obtain the test statistic for
the conditional predictive ability test using standard regression packages.
Corollary 2 Under the assumptions of Theorem 1, the test statistic T h
n,m can be alternatively
computed as nR2, where R2 is the uncentered squared multiple correlation coeﬃcient for the artiﬁcial
regression of the constant unity on the 1 × q vector (ht∆Lm,t+1)0 for t = m, ..., T −1.
Corollary 3 Let assumptions (i), (iii) and (iv) of Theorem 1 hold and further assume
(ii)0 E|∆Lm,t+1|2(r+δ1) < ∆1 < ∞and E|hti|2(r+δ2) < ∆2 < ∞for some δ1, δ2 > 0, i = 1, ..., q
and for all t;
(v) E[(∆Lm,t+1)2|Ft] = σ2 for all t and some σ2 > 0.
Then the conditional predictive ability test can be alternatively based on the test statistic nR2,
where R2 is the uncentered squared multiple correlation coeﬃcient for the artiﬁcial regression of
∆Lm,t+1 on the 1 × q vector h0
t, for t = m, ..., T −1. A level α test can be conducted by rejecting
the null hypothesis H0 of equal conditional predictive ability whenever nR2 > χ2
q,1−α, where χ2
is the (1 −α)−quantile of a χ2
q distribution.
If the conditional homoskedasticity assumption (v) can be reasonably expected to hold in a
given application, the true distribution of the regression-based test statistic in Corollary 3 may be
better approximated by its asymptotic distribution than the statistic of Corollary 2, and it might
thus deliver better inference.
Alternative hypothesis
We now analyze the behavior of the test statistic T h
n,m under a form of global alternative to the
null hypothesis H0. Because we do not impose the requirement of identical distribution, we must
exercise care in specifying the global alternative in this context. In fact, we will be able to obtain
tests consistent against
HA,h : E[ ¯Z0
m,n]E[ ¯Zm,n] ≥δ > 0 for all n suﬃciently large.
The following theorem characterizes the behavior of T h
n,m under the global alternative HA,h.
Theorem 4 Given Assumptions (i), (ii) and (iii) of Theorem 1, under HA,h in (5) for any constant
c ∈R, P[T h
n,m > c] →1 as n →∞.
Notice that H0 and HA,h are not necessarily exhaustive. For a given test function sequence
{ht}, it may in fact happen that E[ ¯Z0
m,n0]E[ ¯Zm,n0] = 0 for some sequence {n0}, without {∆Lm,t+1}
being an mds. The resulting test may thus have no power against alternatives for which ∆Lm,t+1 is
uncorrelated with the chosen test function (and thus E[ ¯Z0
m,n0]E[ ¯Zm,n0] = 0) but it is correlated with
some element of the information set Ft that is not contained in ht. In other words, the properties of
the test will depend on the chosen test function. The ﬂexibility in the choice of test function is both
a shortcoming and an advantage of our testing framework. On the one hand, for any given selection
of ht the test may have no power against possibly important alternatives. On the other hand, one
is left free to choose which test function is more relevant in any situation and thus focus power in
that speciﬁc direction. Further, using methods developed recently in the statistics literature, one
may be able to identify with some conﬁdence which elements of ht are responsible for rejection
of the null hypothesis using the notion of False Discovery Rate for multiple comparison testing
procedures .
In practice, the test function is chosen by the researcher to embed elements of the information
set Ft that are believed to have potential explanatory power for the future diﬀerence in predictive
ability. Examples are, e.g., indicators of past relative performance or other variables that may help
distinguish between the forecast performance of the two methods, such as business cycle indicators
that may capture possible asymmetries in relative performance during booms and recessions. When
choosing the number of elements for the test function ht, it is important to keep in mind that the
properties of the test will be altered if one either includes too few or too many elements. If ht
leaves out elements of the information set Ft that are correlated with ∆Lm,t+1, the test may have
little or no power against the alternative for which ∆Lm,t+1 is not mds. As a consequence, the
test would incorrectly “accept” a false null hypothesis.
On the other hand, the inclusion of a
number of elements that are either uncorrelated or weakly correlated with ∆Lm,t+1 will in some
sense dilute the signiﬁcance of the truly important elements and thus erode the power of the test.
A possible way to confront this diﬃculty is to apply the approaches advocated by Bierens 
or Stinchcombe and White , which deliver consistent tests.
Multi-step conditional predictive ability test
For a forecast horizon τ > 1, the null hypothesis (3) of equal conditional predictive ability of
forecasts f and g implies in particular that for all Ft−measurable test functions ht the sequence
{ht∆Lm,t+τ} is “ﬁnitely correlated”, so that cov(ht∆Lm,t+τ, ht−j∆Lt+τ−j(ˆβm,t−j)) = 0 for all
j ≥τ. Similarly to the previous section, we are able to exploit this simplifying feature in the
derivation of the test statistic. Using reasoning that mirrors the development of the test for the
one-step horizon, we construct a test of
H0,τ : E[∆Lm,t+τ|Ft] = 0
against the global alternative
HA,h,τ : E[ ¯Z0
m,n]E[ ¯Zm,n] ≥δ > 0 for all n suﬃciently large,
where ht is a q × 1 Ft−measurable test function and ¯Zm,n ≡n−1 PT−τ
t=m Zm,t+τ, Zm,t+τ ≡
ht∆Lm,t+τ. For a ﬁxed maximum estimation window length m, the test is based on the statistic
n,m,τ = n(n−1
ht∆Lm,t+τ)0 ˜Ω−1
ht∆Lm,t+τ) = n ¯Z0
where ˜Ωn ≡n−1 PT−τ
t=m Zm,t+τZ0
m,t+τ + n−1 Pτ−1
t=m+j[Zm,t+τZ0
m,t+τ−j + Zm,t+τ−jZ0
with wn,j a weight function such that wn,j →1 as n →∞for each j = 1, ..., τ −1 .
A level α test rejects the null hypothesis of equal conditional predictive ability whenever T h
q,1−α, where χ2
q,1−α is the (1 −α)−quantile of a χ2
q distribution. The following result is the
equivalent of Theorems 1 and 4 for the multi-step forecast horizon case.
Theorem 5 (Multi-step conditional predictive accuracy test) For given forecast horizon τ >
1, maximum estimation window size m < ∞and a q × 1 test function sequence {ht} suppose:
(i) {Wt}, {ht} are mixing sequences with φ of size −r/(2r −2), r ≥2 or α of size −r/(r −2),
(ii) E|Zm,t+1,i|r+δ < ∆< ∞for some δ > 0, i = 1, ..., q and for all t;
(iii) Ωn ≡n−1 PT−τ
t=m E[Zm,t+τZ0
m,t+τ]+n−1 Pτ−1
t=m+j(E[Zm,t+τZ0
m,t+τ−j ]+E[Zm,t+τ−jZ0
is uniformly positive deﬁnite.
Then, (a) under H0,τ in (6), T h
q as n →∞and (b) under HA,h,τ in (7), for any
constant c ∈R, P[T h
n,m,τ > c] →1 as n →∞.
A decision rule for forecast selection
If the null hypothesis of equal conditional predictive ability of forecast methods f and g is rejected,
this raises the possibility that one might be able to select at time T a best forecasting method for
time T + τ. Rejection of the null hypothesis is caused by the fact that the test functions {ht} have
predictive power for the loss diﬀerences {∆Lm,t+τ} over the out-of-sample period. This suggests
that the test function at time T, hT, can be used to predict which forecast method will yield lower
loss at time T + τ, resulting, for example, in the following decision rule:
• Let ˆαn denote the coeﬃcient obtained by regressing ∆Lm,t+τ = Lt+τ(Yt+τ, ˆfm,t)−Lt+τ(Yt+τ, ˆgm,t)
on ht over the out-of-sample period t = m, ..., T −τ. Then choose g if h0
T ˆαn > c and choose
T ˆαn < c, where c is a user-speciﬁed threshold.
In general, the plot of the predicted loss diﬀerences over the out-of-sample period {h0
contains useful information for assessing the relative performance of f and g. For example, one
could consider the indicator In,c = n−1 PT−τ
tˆαn > c}, where 1{A} is the indicator variable
taking the value 1 if A is true and 0 otherwise. In,c represents the proportion of times that the
above decision rule would have chosen forecast method g over the out-of-sample period. In the
empirical application in section 5, we utilize the indicator In,c with c = 0 to summarize the relative
performance of the forecast methods under analysis.
Monte Carlo evidence
In this section, we investigate the size and power properties of our conditional predictive ability
test in ﬁnite samples of the sizes typically available in macroeconomic forecasting applications.
For simplicity, we restrict attention to a squared error loss function and to the one-step forecast
Size properties
In order to construct a series of data and forecasts that satisfy the null hypothesis, we exploit the
following result.
Proposition 6 E[(Yt+1 −ˆfm,t)2 −(Yt+1 −ˆgm,t)2|Ft] = 0 if and only if either ˆfm,t = ˆgm,t a.s. or
E[Yt+1|Ft] = ( ˆfm,t + ˆgm,t)/2.
We can thus generate data under the null hypothesis
H0 : E[(Yt+1 −ˆfm,t)2 −(Yt+1 −ˆgm,t)2|Ft] = E[∆Lm,t+1|Ft] = 0
by ﬁrst constructing forecasts { ˆfm,t, ˆgm,t} and then letting Yt+1 = ( ˆfm,t + ˆgm,t)/2 + εt+1, where
εt+1 ~ i.i.d. N(0, σ2). One of the important features of our testing framework is its ability to handle heterogeneous data. To create data that exhibits interesting behavior we consider an actual
macroeconomic time series {Wt}, which corresponds to one of the measures of inﬂation that we
consider in the empirical application. Speciﬁcally, we let Wt be the second (log) diﬀerence of the
monthly U.S. Consumer Price Index measured over the period 1959:1-1998:12, for a total sample
size T = 468. We construct the forecasts ˆfm,t and ˆgm,t by a rolling window procedure; the ﬁrst
forecast is simply the unconditional mean of the estimation sample, while the second is the forecast
implied by an AR(1) model for Wt:
(Wt + ... + Wt−m+1)/m
ˆαm,t + ˆβm,tWt.
We consider a range of values for the size of the estimation sample m and for the variance of the
disturbances σ2 : m = (36, 60, 120, 240, 360) and σ2 = (.1, 1, 3). For each pair (m, σ2) we generate
10, 000 Monte Carlo replications of the time series {Yt+1, ˆfm,t, ˆgm,t} and compute the proportion of
rejections of the null hypothesis (9) at the 10% nominal level. The test function is ht = (1, ∆Lm,t)0.
The results are collected in Table 1.
[TABLE 1 HERE]
From the analysis of Table 1, the test appears to be reasonably well-sized, with a mild tendency
to under-reject. The size properties of the test are seemingly unaﬀected by varying the length of
the estimation window and the error variances.
Power properties
We investigate the power of the CPA test against serially correlated alternatives. In particular, we
consider the following alternative hypothesis
Ha,ρ : E[∆Lm,t+1|Ft] = ρ∆Lm,t,
which occurs when E[Yt+1|Ft] = ( ˆfm,t + ˆgm,t)/2 −ρ∆Lm,t/(2( ˆfm,t −ˆgm,t)). We consider a number
of diﬀerent values for the AR coeﬃcient ρ, ranging from ρ = 0.05 to ρ = 0.5, at increments of
0.05. For a given ρ, we generate data under the alternative hypothesis (11) by ﬁrst constructing
the forecasts { ˆfm,t, ˆgm,t} as in (10) and then letting Yt+1 = ( ˆfm,t + ˆgm,t)/2 −ρ∆Lm,t/(2( ˆfm,t −
ˆgm,t)) + εt+1, where εt+1 ~ i.i.d. N(0, 1) and the initial value ∆Lm,m is drawn from a standard
normal distribution. For each parameterization, we generate 10, 000 Monte Carlo replications of
the time series {Yt+1, ˆfm,t, ˆgm,t} and compute the proportion of rejections of the null hypothesis
(9) at the 10% nominal level.3 Figure 1 plots the power curves for m = (60, 120, 240).
3We drop the ﬁrst 100 observations of the generated time series {Yt+1, ˆfm,t, ˆgm,t} to reduce the dependence on
the initial observation, which leaves us with a total sample size T = 368.
[FIGURE 1 HERE]
The test displays good power properties. For example, more than 50% of the time the test is
able to detect the presence of moderately low serial correlation (i.e., an AR coeﬃcient between 0.15
and 0.2). As expected, the power of the test increases as the size of the out-of-sample evaluation
data set increases.
Application: comparing parameter-reduction methods in macroeconomic forecasting
A problem that often arises in macroeconomic forecasting is the selection of a manageable subset of
predictors from a large number of potentially useful variables. In this situation, one key determinant
of the resulting forecast performance is the trade-oﬀbetween the information content of each
series and the estimation uncertainty introduced. The goal of our application is to analyze and
compare the forecast performance of several parameter-reduction schemes that have been considered
in the literature to overcome this so-called “curse of dimensionality”.
We will consider three
leading methods; a sequential model-selection approach based on a simpliﬁed general-to-speciﬁc
modelling strategy , the “diﬀusion indexes” approach of Stock
and Watson and the use of Bayesian shrinkage estimation . We also compare each method to benchmark forecasts. The existing framework for
comparison of predictive ability is not appropriate for addressing these issues, since it does not
easily accommodate, for example, Bayesian estimation or the presence of estimated regressors.
Further, some of the comparisons are between nested models, in which case the existing techniques
are not readily applicable. In contrast, our conditional predictive ability approach is naturally well
suited for comparison of nested models and for detecting diﬀerences in predictive ability arising
from use of diﬀerent modelling and estimation techniques.
We consider the “balanced panel” subset of the data set of Stock and Watson (henceforth
SW), including 146 monthly economic time series measured over the period 1959:1-1998:12. We
use the diﬀerent parameter reduction methods to construct 1-, 6- and 12- month-ahead forecasts
for eight U.S. macroeconomic variables: four measures of aggregate real activity and four price
indexes. The ﬁrst group includes the components of the Index of Coincident Economic Indicators
maintained by the Conference Board: total industrial production; real personal income less transfers; real manufacturing and trade sales and number of employees on nonagricultural payrolls. The
price indexes are: consumer price index; consumer price index less food; personal consumption
expenditure implicit price deﬂator and producer price index.4 We refer the reader to SW for a
complete description of the data.
4These variables coincide with the variables forecasted by SW, with the exception of the consumer price index
Parameter-reduction methods
Following SW, our approach to multistep-ahead forecasting is to consider forecast models that
project the τ−step ahead variable Y τ
t+τ onto predictor variables measured at time t. Both the
dependent variable and the predictors are transformations of the original data that render these
variables I(0). In particular, the real variables are modeled as being I(1) in logarithms and the
price indexes as I(2) in logarithms. If RAWt is the original datum at time t, this implies that Y τ
is generated as
Real variables
t+τ = (1200/τ) log(RAWt+τ/RAWt)
Price indexes
t+τ = (1200/τ) log(RAWt+τ/RAWt) −1200 log(RAWt/RAWt−1).
For ease of notation, we denote the one-step ahead variable Y 1
t+1 as Yt+1.We consider the following
forecasting methods.
Sequential model selection
This method considers the entire set of 145 predictors, together with lags of the dependent variable
and performs a sequential selection search on each estimation sample that retains only variables
that are statistically signiﬁcant. The subset of signiﬁcant variables is then used for forecasting.
The initial model speciﬁcation is
t+τ = α + β0Xt + γ(L)Yt + εt+τ.
where Xt indicates the vector containing the 145 predictors and γ(L) is an autoregressive polynomial
of order 6. We overcome the problem of multicollinearity in the original Xt matrix by removing
the groups of variables whose correlation is greater than .98 and replacing them with an average
of all the highly correlated variables. After this procedure, the new matrix X∗
t contains a total of
130 regressors. Our sequential modeling approach begins by estimating the full model and then
applies a series of sequential tests until a more parsimonious restriction is found that conveys most
of the information contained in the initial model.5
We apply a simpliﬁed version of the search
algorithm described by Hoover and Perez , which consists of reducing the number of
regressors in the model by performing a sequence of stability tests, residual autocorrelation tests
and t−and F−tests of signiﬁcance of the regressor’s coeﬃcients. The simpliﬁcation adopted here
considers only one reduction path rather than multiple paths and performs only a subset of the
less food which replaces the consumer price index less food and energy series considered by SW (not included in the
data set available to the authors).
5See Hoover and Perez, and the ensuing discussion for relevant references, a thorough description of the
methodology, and an account of the heated debate about the merits and shortcomings of the so-called LSE approach
to econometric modeling.
sequential tests in Hoover and Perez . As suggested by these authors, we use a signiﬁcance
level α = 0.01 for all the tests, which should encourage parsimony of the ﬁnal model. A complete
description of the particular algorithm that we utilize is contained in Appendix B.
Diﬀusion indexes
This is a new method proposed by SW. The forecasts are constructed using a two-step procedure.
First, the method of principal components is used to estimate the factors Ft from the predictors
Xt. Second, the forecasting model is constructed as
t+τ = α + β0 ˆFt + γ(L)Yt + εt+τ,
where both the number of factors k retained in ˆFt and the order p of γ(L) are selected by BIC,
with 1 ≤k ≤12 and 0 ≤p ≤6.
Bayesian shrinkage estimation
We consider the full model (13) and apply Bayesian estimation of its coeﬃcients using the Litterman
 prior. The Litterman prior, when applied to variables expressed in diﬀerences, shrinks all
coeﬃcients in (13) towards zero, except that for the intercept term a diﬀuse prior is used. Formally,
the variance-covariance matrix V for the prior distribution of θ ≡(α, β0, γ0)0 is diagonal, with
α ∼N(0, 108), βi ∼N(0, (w · λ · ˆσy/ˆσxi)2), i = 1, ..., k and γj ∼N(0, (λ/j))2), j = 1, ..., p. There
are two hyperparameters that must be selected a priori: λ and w. The parameter λ is the prior
standard deviation of the ﬁrst autoregressive coeﬃcient (that is, the coeﬃcient of Yt). The prior
standard deviation of the subsequent lags of Yt is further divided by the lag length to reﬂect an
increasing conﬁdence in the prior mean for longer lags. The parameter w is a number between zero
and one that reﬂects the belief that the predictors collected in Xt are less useful for forecasting than
lagged values of the dependent variable. Further, the prior standard deviation of βi is multiplied
by the ratio of the sample standard deviations of the dependent variable and of the ith regressor
ˆσy/ˆσxi, to eliminate the eﬀects of diﬀerences in scale. The Bayesian estimate of θ is then given by
θB = (X0X + ˆσ2V −1)−1(X0Y τ),
where X is the m×151 matrix (m is the size of the estimation sample) with rows (X0
t, Yt, Yt−1, ..., Yt−5),
Y τ is the m × 1 vector with rows Y τ
t+τ and ˆσ is the estimated standard error of the residuals in a
univariate autoregression for Y τ
t+τ. As suggested by Litterman , we set w = 0.2 and λ = 0.2.6
6The results were generally robust to a number of diﬀerent choices for w and λ.
Benchmarks
In addition to the three methods above, we consider two benchmarks. The ﬁrst is a forecasting
method based on an autoregressive (AR) model
t+τ = α + γ(L)Yt + εt+τ,
where the lag order p of the lag polynomial γ(L) is selected by BIC with 0 ≤p ≤6. The second
benchmark is based on a random walk hypothesis for the levels of the variable; this amounts to
specifying the following forecast equation for the variable in diﬀerences:
t+τ = α + εt+τ.
Real-time forecasting experiment
The ﬁve methods described above are used to simulate real-time forecasting. The available sample
has size T = 468, and we choose a maximum estimation window m = 150+τ, which is the minimal
length that allows us to estimate and test the full model in the sequential model selection approach.
For comparability, we apply the same transformations to the original series as those documented in
Appendix B of SW. The ﬁrst estimation sample we consider ranges from 1960:1 through 1972:6+τ
(the ﬁrst 12 data were used as initial observations). The data in this sample are ﬁrst screened for
outliers, which we replace with the unconditional mean of the corresponding variable. We then
standardize the regressors, estimate the diﬀusion indexes and select the autoregressive lag lengths
and number of diﬀusion indexes by BIC. Finally, we run the regressions (13), (14), (16), (17) and
apply the Bayesian shrinkage method for t =1960:1,...,1972:6. We use the values of the regressors
at time t =1972:6 + τ to generate a set of forecasts for Y τ
1972:6+2τ. We then move the estimation
window forward one period and repeat all of the above steps (outlier detection, standardization,
speciﬁcation, estimation and so forth) on data from 1960:2 through 1972:7 + τ. This generates the
set of forecasts for Y τ
1972:7+2τ. The ﬁnal forecasts are produced at t =1998:12 −τ for the variable
Results of the conditional predictive ability tests
We apply the conditional predictive ability test of Theorem 1 to evaluate the accuracy of the
diﬀerent forecast methods.
We take the series of 1-, 6- and 12-month-ahead forecast errors e
calculated above for each of the ﬁve models and conduct a number of pairwise tests using absolute
error and squared error loss functions: L1(e) = |e| and L2(e) = e2. For τ = 1, 6 and 12, the null
hypotheses of equal conditional predictive ability for the two loss functions are given by
E[|Yt+τ −ˆfm,t| −|Yt+τ −ˆgm,t| |Ft] ≡E[∆L1t+τ|Ft] = 0 and
E[(Yt+τ −ˆfm,t)2 −(Yt+τ −ˆgm,t)2|Ft] ≡E[∆L2t+τ|Ft] = 0.
The hypothesis test Hi
0 makes use of test function: ht = (1, ∆Lit)0, i = 1, 2. As discussed in section
3.4, in case of rejection of the null hypothesis of equal conditional predictive ability, we consider
the sequence of predicted loss diﬀerences over the out-of-sample period {h0
t=m, to establish
which method would have been selected at each point in time by the decision rule described in
that section. To illustrate, Figure 2 plots the sequence of predicted absolute error loss diﬀerences
for 1-month ahead forecasts of industrial production, for the two comparisons sequential model
selection versus AR and Bayesian shrinkage versus AR.
[FIGURE 2 HERE]
The predicted loss for the sequential method is greater than the predicted loss for the AR
for the vast majority of the sample dates, while the predicted loss for the Bayesian shrinkage is
always smaller than that of the AR. Further, notice that the predicted loss diﬀerences for the
pair sequential-AR are several orders of magnitude higher and more volatile than those for the
pair Bayesian shrinkage-AR. Provided the test rejects the null hypothesis of equal conditional
predictive ability, these considerations lead to the conclusion that Bayesian shrinkage would have
been invariably a better method than the AR for forecasting one-month ahead industrial production
over the years 1972-1998.
The results of the test for all pairwise comparisons, loss functions, and forecast horizons are
contained in Tables 2-5. Tables 2 and 3 present the results for the real variables forecasts, whereas
Tables 4 and 5 consider the price indexes forecasts. The entries in each table are the p-values of the
tests of equal conditional predictive ability of the two methods. The number within parentheses
below each entry is the indicator In,c discussed in section 3.4 (for c = 0) which represents the
proportion of times the method in the column would have been preferred to the method in the
row over the out-of-sample period using the decision rule described in that section. To facilitate
interpretation of the tables, we use a plus sign to indicate rejection of the null hypothesis of equal
conditional predictive ability of the two methods at the 10% level and to signal that the method
in the column would have been chosen more often than the method in the row (as suggested by
an entry In,c > .5). Similarly, a minus sign denotes rejection of the null hypothesis at the 10%
level and it indicates that the method in the column would have been chosen more often than the
method in the row (i.e., In,c < .5).
[TABLES 2 - 5 HERE]
A sharp result that emerges from Tables 2-5 is that the sequential model selection method is
characterized by the worst performance across all forecast horizons, especially for the real variables.
In the majority of cases, it is outperformed by every other method, including the naive random walk
forecast. The likely explanation for this is the tendency of the method to select over-parameterized
models (cases with 40 or more predictors in the ﬁnal model were not uncommon), in spite of the
use of a small conﬁdence level for the sequential tests. Further, performing a new sequential search
on each of the rolling estimation windows means that we typically select a diﬀerent model at each
iteration, in spite of the fact that consecutive windows only diﬀer by two observations. This suggests
that improvements on the performance of the sequential method may be obtained by updating the
model less frequently than every month.
A second general observation is that the information contained in the predictors seems to be
less useful for forecasting price indexes than real variables. For the price indexes, there are only
a few cases where the AR benchmark is outperformed (by the diﬀusion index method). In the
majority of cases, the parameter-reduction methods, while outperforming the naive random walk
forecasts, are indistinguishable from the AR benchmark. Further, the Bayesian shrinkage method
is outperformed by the diﬀusion indexes and by the AR method mainly at the 6- and 12-month
forecast horizons.
The Bayesian shrinkage and the diﬀusion indexes methods appear to fare better for forecasting
real variables. Bayesian shrinkage, in particular, outperforms the AR in 11 of the 12 comparisons,
while the diﬀusion indexes method outperforms the AR in 7 of the 12 comparisons. It is interesting
to note that for the majority of variables and forecast horizons the AR forecasts are not distinguishable from the random walk forecasts. This suggests that the predictors do contain useful
information for forecasting real variables beyond what can be captured by the variable’s own lags.
Bayesian shrinkage emerges in this case as the best method for reducing the estimation uncertainty
of the system, while still conveying its information content.
Conclusion
We propose a general framework for out-of-sample predictive ability testing which, as we argue,
represents a more realistic setting for economic forecasting than the existing framework, exempliﬁed
by West . We start from the premise that the forecaster not only cares about whether two
competing forecasts do equally well on average, but also whether she can predict which forecast will
do better tomorrow. We implement this diﬀerent focus by conducting inference about conditional,
rather than unconditional moments of forecasts and forecast errors. Recognizing that even a good
model may produce bad forecasts due to estimation uncertainty or model instability, we make the
object of evaluation the entire forecasting method (including the model, the estimation procedure
and the size of the estimation window), whereas the existing literature concentrates solely on the
model. In so doing, we are also able to handle more general data assumptions (heterogeneity rather
than stationarity) and estimation methods, as well as providing a uniﬁed framework for comparing
forecasts based on nested or non-nested models, which was not previously available.
One useful application of the conditional predictive ability tests is in evaluating diﬀerent methods for model selection and parameter estimation. We considered in particular the case of macroeconomic forecasting with a large number of predictors and compared the forecast performance
of diﬀerent parameter-reduction methods: a sequential model selection approach, the “diﬀusion
indexes” approach of Stock and Watson and the use of Bayesian shrinkage estimation. Using the data set of Stock and Watson , including monthly U.S. data on a large number of
macroeconomic variables, we generated 1-, 6- and 12-month ahead forecasts of four measures of
real activity and four price indexes using the diﬀerent forecast methods. The conditional predictive
ability tests led to the conclusion that the sequential model selection was the worst performing
method, probably due to its tendency to select large models. A second general result was that the
information contained in the predictors appeared to be less useful for forecasting price indexes than
real variables. For the price indexes, the performance of the various methods was mostly indistinguishable from the one of a simple autoregression. For the real variables, instead, we found that
the predictors did contain useful information beyond what is contained in the variable’s own lags.
For these variables Bayesian shrinkage seemed to be the best method for reducing the estimation
uncertainty of the system. We emphasize that the results of the empirical application are speciﬁc
to the situation where the number of parameters is very large relative to the sample size and thus
one should be careful in generalizing our conclusions to other situations. The fact that shrinkage
estimation methods work best in such an environment should come as no surprise. Likewise, it
could be argued that the sequential model selection approach was originally conceived for the case
where there are enough observations per parameter to make the results of the sequential tests
credible. Viewed in this light, our experiments may be unduly hard on the sequential methodology.
Much work remains to be done. A reﬁnement that we are currently exploring is to consider
a richer set of decision rules for selecting the best forecasting method or for optimally combining
the information embedded in each method once the null hypothesis of equal conditional predictive
ability is rejected. A further natural generalization of the tests proposed in the paper is to consider
multiple comparisons, for example by adapting the approach of White to our conditional
framework. Finally, it may be possible to obtain asymptotic reﬁnements of the tests presented
here by using bootstrap resampling techniques, for example by establishing whether the results of
Andrews can be extended to the case of heterogeneous data.
Appendix A. Proofs
Proof of Theorem 1.
Under the null hypothesis H0 in (3), {Zm,t+1, Ft} is an mds, and we can
apply an mds central limit theorem (CLT) to show that
as n →∞, from which it follows that T h
q as n →∞. The mds CLT we use requires
conditions such that the sample variance ˆΩn is a consistent estimator of Ωn = var(√n ¯Zm,n), i.e.,
such that ˆΩn −Ωn
p→0. Write Zm,t+1Z0
m,t+1 = f(ht, Wt+1, ..., Wt−m), where f(·) is a measurable
Since {Wt} and {ht} are mixing from (i), and f is a function of only a ﬁnite number of leads and lags of Wt and ht, it follows from Lemma 2.1 of White and Domowitz 
that {Zm,t+1Z0
m,t+1} is also mixing of the same size as Wt. To apply a law of large numbers
(LLN) to Zm,t+1Z0
m,t+1, we further need to ensure that each of its elements has absolute r + δ moment bounded uniformly in t. By the Cauchy-Schwarz inequality and (ii), E|Zm,t+1,iZm,t+1,j|r+δ ≤
m,t+1,i|r+δ]1/2[E|Z2
m,t+1,j|r+δ]1/2 < ∆1/2∆1/2 < ∞, i, j = 1, ..., q and for all t. That ˆΩn−Ωn
then follows from McLeish ’s LLN as in Corollary 3.48 of White . Ωn is ﬁnite by (ii)
and it is uniformly positive deﬁnite by (iii).
We apply the Cramér-Wold device and show that for all λ ∈Rq, λ0λ = 1, λ0Ω−1/2
N(0, 1), which implies that Ω−1/2
d→N(0, I). Consider
√n ¯Zm,n = n−1/2
and write λ0Ω−1/2
Zm,t+1 = Pq
i=1 ˜λiZm,t+1,i. The variable ˜λiZm,t+1,i is measurable with respect to
Ft, and the linearity of conditional expectations implies that
Zm,t+1|Ft] =
˜λiE[Zm,t+1,i|Ft] = 0,
given (3). Hence {λ0Ω−1/2
Zm,t+1, Ft} is an mds. The asymptotic variance is ¯σ2
n = var(λ0Ω−1/2
√n ¯Zm,n) =
var(√n ¯Zm,n)Ω−1/2
λ = 1 for all n suﬃciently large. We have that
m,t+1Ω−1/2
λ−1 = λ0Ω−1/2
λ = g(ˆΩn)−g(Ωn)
since ˆΩn −Ωn
p→0 and by Proposition 2.30 of White . Further, by Minkowski’s inequality,
Zm,t+1|2+δ = E|
˜λiZm,t+1,i|2+δ ≤[
˜λi(E|Zm,t+1,i|2+δ)1/(2+δ)]2+δ < ∞,
the last inequality following from (ii). Hence, the sequence {λ0Ω−1/2
Zm,t+1, Ft} satisﬁes the conditions of Corollary 5.26 of White (CLT for mds), which implies that λ0Ω−1/2
N(0, 1). By the Cramér-Wold device , Ω−1/2
N(0, I), from which (19) follows by consistency of ˆΩn for Ωn.
Proof of Corollary 2.
The (constant unadjusted) R2 for the regression of the constant unity
on the variables Z0
m,t+1 ≡(ht ∆Lm,t+1)0 can be written as R2 = ι0Zm[Z0
mι/ι0ι, where ι is
an n × 1 vector of ones and Zm is the n × q matrix with rows Z0
m,t+1. Since ˆΩn = Z0
mZm/n, it thus
follows that nR2 = n(ι0Zm/n)ˆΩ−1
mι/n) = T h
Proof of Corollary 3.
The (constant unadjusted) R2 for the regression of ∆Lm,t+1 on h0
can be written as R2 = ∆L0h[h0h]−1h0∆L/∆L0∆L, where ∆L is the n × 1 vector with elements
∆Lm,t+1 and h is the n × q matrix with rows h0
t. We thus have nR2 = n ¯Z0
m,n(ˆσnVn)−1 ¯Zm,n, where
ˆσn = ∆L0∆L/n and Vn = h0h/n. We will show that ˆσnVn −Ωn
p→0, which implies that the two
statistics T h
m,n and nR2 are asymptotically equivalent and thus the conditional predictive ability
test can be alternatively based on the statistic nR2. By the law of iterated expectations
E[ht(∆Lm,t+1)2ht0] = n−1
E[htE[(∆Lm,t+1)2|Ft]]ht0] = σ2E[h0h/n],
where the last equality follows from assumption (v). Given assumptions (i) and (ii)0, the sequences
t} and {(∆Lm,t+1)2} satisfy a LLN and it thus follows that Vn −E[h0h/n]
p→0 and ˆσn −σ2 =
ˆσn−E[ˆσn]
p→0, where the last equality is implied by (v). Hence, ˆσnVn−Ωn = ˆσnVn−σ2E[h0h/n]
0, and the proof is complete.
Proof of Theorem 4.
Given Assumption (i), it follows from Lemma 2.1 of White and
Domowitz that {Zm,t+1} is mixing of the same size as Wt, since it is a function of only a ﬁnite
number of leads and lags of Wt and ht. Further, each element of Zm,t+1 is bounded uniformly in t by
(ii). McLeish ’s LLN then implies that ¯Zm,n−E[ ¯Zm,n]
0. By deﬁnition, under HA,h there exists ε > 0 such that E[ ¯Z0
m,n]E[ ¯Zm,n] > 2ε for all n suﬃciently
large. We then have that
m,n ¯Zm,n > ε] ≥P[ ¯Z0
m,n ¯Zm,n−E[ ¯Z0
m,n]E[ ¯Zm,n] > −ε] ≥P[| ¯Z0
m,n ¯Zm,n−E[ ¯Z0
m,n]E[ ¯Zm,n]| < ε] →1.
By arguments identical to those used in the proof of Theorem 1, {Zm,t+1Z0
m,t+1} is mixing of the
same size as Wt by (i) and each of its elements is bounded uniformly in t by (ii). McLeish ’s
LLN then implies that ˆΩn −Ωn
p→0, with Ωn uniformly positive deﬁnite by (iii). The conditions
of Theorem 8.13 of White are then satisﬁed, and the theorem implies that for any constant
c ∈R, P[T h
n,m > c] →1 as n →∞.
Proof of Theorem 5.
(a) Under the null hypothesis H0 in (3), we show that
as n →∞, from which (a) follows. First, we apply the Cramér-Wold device and show that for
all λ ∈Rq, λ0λ = 1, λ0Ω−1/2
d→N(0, 1), where Ωn = var(√n ¯Zm,n), using the fact that
E[Zm,t+τ|Ft] = 0. The asymptotic variance Ωn is ﬁnite by (ii) and it is uniformly positive deﬁnite
by (iii), Write λ0Ω−1/2
√n ¯Zm,n = n−1/2 PT−τ
t=m λ0Ω−1/2
Zm,t+τ and consider the scalar sequence
Zm,t+τ}. We verify that the sequence satisﬁes the conditions of the Wooldridge and White
 CLT for mixing processes. For each t, λ0Ω−1/2
Zm,t+τ = f(ht, Wt+τ, ..., Wt−m), where f(·) is
a measurable function. Since { Wt} and {ht} are mixing from (i), and f is a function of only a ﬁnite
number of leads and lags of Wt and ht, it follows from Lemma 2.1 of White and Domowitz 
that {λ0Ω−1/2
Zm,t+τ} is also mixing of the same size as Wt. Further, ¯σ2
n = var(λ0Ω−1/2
√n ¯Zm,n) =
var(√n ¯Zm,n)Ω−1/2
λ = 1 > 0 for all n suﬃciently large. Finally, by Minkowski’s inequality,
Zm,t+τ|2+δ = E|
˜λiZm,t+τi|2+δ ≤[
˜λi(E|Zm,t+τi|2+δ)1/(2+δ)]2+δ < ∞,
the last inequality following from (ii). Hence, the sequence {λ0Ω−1/2
Zm,t+τ} satisﬁes the conditions of Corollary 3.1 of Wooldridge and White , which implies that λ0Ω−1/2
N(0, 1). By the Cramér-Wold device , we then conclude that
d→N(0, I). It remains to show that ˜Ωn −Ωn
p→0, from which (21) follows. We have
m,t+τ −E(Zm,t+τZ0
m,t+τ−j −E(Zm,t+τZ0
+Zm,t+τ−jZ0
m,t+τ −E(Zm,t+τ−jZ0
For j = 0, ..., τ −1, {Zm,t+τZ0
m,t+τ−j} is mixing of the same size as Wt and each of its elements is
bounded uniformly in t by (ii). Applying McLeish ’s LLN 
and using the fact that wn,j →1 for n →∞, it follows that n−1wn,j
t=m+j[Zm,t+τZ0
E(Zm,t+τZ0
p→0 for each j = 0, ..., τ −1 (with wn,0 ≡1), which in turn implies that
p→0 and the proof is complete.
(b) Given Assumption (i), it follows from Lemma 2.1 of White and Domowitz that
{Zm,t+τ} is mixing of the same size as Wt, since it is a function of only a ﬁnite number of leads
and lags of Wt and ht. Further, each element of Zm,t+τ is bounded uniformly in t by (ii). McLeish
 ’s LLN then implies that ¯Zm,n −E[ ¯Zm,n]
deﬁnition, under HA,h,τ there exists ε > 0 such that E[ ¯Z0
m,n]E[ ¯Zm,n] > 2ε for all n suﬃciently
large. We then have that
m,n ¯Zm,n > ε] ≥P[ ¯Z0
m,n ¯Zm,n−E[ ¯Z0
m,n]E[ ¯Zm,n] > −ε] ≥P[| ¯Z0
m,n ¯Zm,n−E[ ¯Z0
m,n]E[ ¯Zm,n]| < ε] →1.
By arguments identical to those used in part (a) - which for this particular result did not necessitate
the time dependence structure imposed under the null hypothesis - it follows that ˜Ωn −Ωn
with Ωn uniformly positive deﬁnite by (iii). Theorem 8.13 of White then implies that for
any constant c ∈R, P[T h
n,m,τ > c] →1 as n →∞.
Proof of Proposition 6.
We have E[(Yt+1 −ˆfm,t)2 −(Yt+1 −ˆgm,t)2|Ft] = E[−2Yt+1( ˆfm,t −
ˆgm,t) + ˆf2
m,t|Ft] = −2( ˆfm,t −ˆgm,t)E[Yt+1|Ft] + ˆf2
m,t = ( ˆfm,t −ˆgm,t)(−2E[Yt+1|Ft] +
ˆfm,t + ˆgm,t) which is zero (a.s.) if and only if either one of the two factors is zero (a.s.).
Appendix B. Sequential model selection algorithm
The following is our modiﬁcation of the search algorithm described by Hoover and Perez . All the tests are conducted for a signiﬁcance level α = 0.01 and use heteroskedasticity
and autocorrelation consistent standard errors and covariance matrices .
1. Estimate the full model on the available sample and run the following tests:
a. Autocorrelation of residuals up to sixth order .
b. Stability test .
If the full model fails any one of the tests (i.e., autocorrelation and/or structural breaks are
detected), do not use this test in the following steps.
2. Eliminate the variable of the general speciﬁcation that has the lowest t−statistic and reestimate the model, which becomes the current model.
3. On each current model, perform the two tests in step 1 together with
c. F-test of the hypothesis that the coeﬃcients of the variables in the full model that are not
included in the current model are jointly insigniﬁcant. If the hypothesis cannot be rejected,
the current model can be considered a valid restriction of the full model (in this case we say
that the model passes the test).
4. If the current model passes all three tests, eliminate the variable with the next lowest
t−statistic and perform the tests on the new current model.
If this model fails any one
of the tests, restore the last variable eliminated and remove the variable with the next lowest
t−statistic. Continue in this fashion until the current model passes all the tests and either
all the variables are signiﬁcant or the elimination of any residual insigniﬁcant variable would
lead to failing one of the tests.
5. Estimate the ﬁnal model from step 4.
5.1. If all remaining variables are signiﬁcant terminate the algorithm.
5.2. If there are remaining insigniﬁcant variables, remove all of them and perform the three
tests on the restricted model.
a. If the restricted model passes all the tests and all the variables are signiﬁcant, terminate the algorithm.
b. If the restricted model fails any of the tests, restore the block of eliminated insigniﬁcant
variables and terminate the algorithm.
c. If the restricted model passes all the tests but there are some remaining insigniﬁcant
variables, go back to step 5.2.