Adversarial Personalized Ranking for Recommendation∗
Xiangnan He
National University of Singapore
 
Zhankui He
Fudan University
 
Chengdu University of Information Technology
 
Tat-Seng Chua
National University of Singapore
 
Item recommendation is a personalized ranking task. To this end,
many recommender systems optimize models with pairwise ranking objectives, such as the Bayesian Personalized Ranking (BPR).
Using matrix Factorization (MF) — the most widely used model in
recommendation — as a demonstration, we show that optimizing
it with BPR leads to a recommender model that is not robust. In
particular, we find that the resultant model is highly vulnerable to
adversarial perturbations on its model parameters, which implies
the possibly large error in generalization.
To enhance the robustness of a recommender model and thus
improve its generalization performance, we propose a new optimization framework, namely Adversarial Personalized Ranking (APR).
In short, our APR enhances the pairwise ranking method BPR by
performing adversarial training. It can be interpreted as playing
a minimax game, where the minimization of the BPR objective
function meanwhile defends an adversary, which adds adversarial
perturbations on model parameters to maximize the BPR objective
function. To illustrate how it works, we implement APR on MF
by adding adversarial perturbations on the embedding vectors of
users and items. Extensive experiments on three public real-world
datasets demonstrate the effectiveness of APR — by optimizing
MF with APR, it outperforms BPR with a relative improvement
of 11.2% on average and achieves state-of-the-art performance for
item recommendation. Our implementation is available at: https:
//github.com/hexiangnan/adversarial_personalized_ranking.
CCS CONCEPTS
• Information systems →Recommender systems; Information retrieval; Retrieval models and ranking;
∗This work is done when during the internship of Zhankui He and Xiaoyu Du at
National University of Singapore. NExT research is supported by the National Research
Foundation, Prime Minister’s Office, Singapore under its IRC@SG Funding Initiative.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
SIGIR’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07...$15.00
 
Personalized Ranking, Pairwise Learning, Adversarial Training,
Matrix Factorization, Item Recommendation
ACM Reference Format:
Xiangnan He, Zhankui He, Xiaoyu Du, and Tat-Seng Chua. 2018. Adversarial
Personalized Ranking for Recommendation. In SIGIR ’18: 41st International
ACM SIGIR Conference on Research and Development in Information Retrieval,
July 8-12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages.
 
INTRODUCTION
Recent advances on adversarial machine learning show that
many state-of-the-art classifiers are actually very fragile and vulnerable to adversarial examples, which are formed by applying small
but intentional perturbations to input examples from the dataset.
A typical example can be found in Figure 1 of , which demonstrates that by adding small adversarial perturbations to an image
of panda, a well-trained classier misclassified the image as a gibbon
with a high confidence, whereas the effect of perturbations can
hardly be perceived by human. This points to an inherent limitation of training a model on static labeled data only. To address the
limitation and improve model generalization, researchers then developed adversarial training methods that train a model to correctly
classify the dynamically generated adversarial examples .
While the inspiring progress of adversarial machine learning
mainly concentrated on the computer vision domain where the
adversarial examples can be intuitively understood, to date, there
is no study about such adversarial phenomenon in the field of information retrieval (IR). Although the core task in IR is ranking, we
point out that many learning to rank (L2R) methods are essentially
trained by optimizing a classification function, such as the pairwise
L2R method Bayesian Personalized Ranking (BPR) in recommendation , among others . This means that it is very likely that
the underlying IR models also lack robustness and are vulnerable
to certain kinds of “adversarial examples”. In this work, we aim to
fill the research gap by exploring adversarial learning methods on
item recommendation, an active and fundamental research topic in
IR that concerns personalized ranking.
Nevertheless, directly grafting the way of generating adversarial
examples from the image domain is infeasible, since the inputs of
recommender models are mostly discrete features (i.e., user ID, item
ID, and other categorical variables). Clearly, it is meaningless to
apply noises to discrete features, which may change their semantics.
To address this issue, we consider exploring the robustness of a
recommender model at a deeper level — at the level of its intrinsic
 
model parameters rather than the extrinsic inputs. Using the matrix
factorization (MF) model trained with BPR as a demonstration (we term this instantiation as MF-BPR), we investigate its
robustness to perturbations on embedding parameters. Note that
MF-BPR is a highly competitive approach for item recommendation
and has been used in many papers as the state-of-the-art baseline
up until recently . We found that MF-BPR is not robust and
is vulnerable to adversarial perturbations on the parameters. This
sheds light on the weakness of training with BPR, and motivates
us to develop adversarial learning methods that can result in better
and more robust recommender models.
As the main contribution of this work, we propose a novel Adversarial Personalized Ranking (APR) method to learn recommender
models. With BPR as the building block, we introduce an additional
objective function in APR to quantify the loss of a model under
perturbations on its parameters. The formulation of APR can be
seen as playing a minimax game, where the perturbations are optimized towards maximizing the BPR loss, and the model is trained to
minimize both the BPR loss and the additional loss with adversarial
perturbations. With a differentiable recommender model, the whole
framework of APR can be optimized with the standard stochastic
gradient descent. To demonstrate how it works, we derive the APR
solver for MF and term the method as Adversarial Matrix Factorization (AMF). We conduct extensive experiments on three public
datasets constructed from Yelp, Pinterest and Gowalla that represent various item recommendation scenarios. Both quantitative
and qualitative analysis justify the effectiveness and rationality of
adversarial training for personalized ranking. Specifically, our AMF
outperforms MF-BPR with a significant improvement of 11% on
average in NDCG and hit ratio. It also outperforms the recently
proposed neural recommender models and IRGAN , and
achieves state-of-the-art performance for item recommendation.
PRELIMINARIES
First the matrix factorization model for recommendation is described. Next the pairwise learning method Bayesian Personalized
Ranking is shortly recapitulated. The novel contribution of this section is to empirically demonstrate that the MF model optimized by
BPR (a.k.a. MF-BPR) is not robust and is vulnerable to adversarial
perturbations on its parameters.
Matrix Factorization
MF has been recognized as the basic yet most effective model in
recommendation since several years . Being a germ of
representation learning, MF represents each user and item as an
embedding vector. The core idea of MF is to estimate a user’s preference on an item as the inner product between their embedding
vectors. Formally, let u denote a user and i denote an item, then
the predictive model of MF is formulated as: ˆyui(Θ) = pTu qi, where
pu ∈RK and qi ∈RK denote the embedding vector for user u
and item i, respectively, and K is the size of embedding vector also
called as embedding size. Θ denotes the model parameters of MF,
which is consisted of all user embedding and item embedding vectors, i.e., Θ = {{pu}u ∈U, {qi }i ∈I}, where U and I denote the set
of all users and items, respectively. We use P and Q to denote the
embedding matrix P = {pu}u ∈U, Q = {qi }i ∈I for short.
Bayesian Personalized Ranking
BPR is a pairwise L2R method and has been widely used to optimize
recommender models towards personalized ranking . Targeting
at learning from implicit feedback, it assumes that observed interactions should be ranked higher than the unobserved ones. To this
end, BPR maximizes the margin between an observed interaction
and its unobserved counterparts. This is fundamentally different
from pointwise methods that optimize each model prediction
towards a predefined groundtruth. Formally, the objective function
(to be minimized) of BPR is
LBPR(D|Θ) =
−lnσ(ˆyui(Θ) −ˆyuj(Θ)) + λΘ||Θ||2,
where σ(·) is the sigmoid function, λΘ are model specific regularization parameters to prevent overfitting, and D denotes the set
of pairwise training instances D := {(u,i, j)|i ∈I+
u ∧j ∈I \ I+
u denotes the set of items that u has interacted with before,
and I denotes the whole item set. Since the number of training
instances in BPR is very huge, the optimization of BPR is usually
done by performing stochastic gradient descent (SGD). After obtaining parameters, we can get the personalized ranked list for a
user u based on the value of ˆyui(Θ) over all items.
Owing to its rationality and ease of optimization, BPR has been
used in a wide variety of scenarios and plays an important role in optimizing recommender models. It is worth noting
that the behavior of BPR can be interpreted as a classifier — given
a triplet (u,i, j), it determines whether (u,i) should have a higher
score than (u, j). Under this interpretation, a positive instance of
(u,i, j) means that ˆyui should be larger than ˆyuj as much as possible
to get the correct label of +1; and vice versa, a negative instance
can be seen as having a label of 0.
MF-BPR is Vulnerable to Adversarial Noises
Inspired by the findings of adversarial examples in image classification , we are particularly interested in exploring whether
the similar phenomenon exists for BPR, since it can also be seen as a
classification method with triplet (u,i, j) as the input. Distinct from
the image domain where adding small noises to an input image
shall not change its visual content, the input to BPR is discrete ID
features and changing an ID feature will change the semantics of
the input. For example, if we change an input (u,i, j) to (u′,i, j)
by corrupting the user ID, the semantics of the triplet becomes
totally different and the label may change. As such, existing methods that generate adversarial examples for an image classifier are
inappropriate for BPR.
Since it is irrational to add noises in the input layer, we instead
consider exploring the robustness of BPR at a deeper level — the
parameters of the underlying recommender model. It is natural to
assume that a robust model should be rather insensitive to small
perturbations on its parameters; that is, only when large perturbations are enforced, the model behavior should be changed dramatically. To benchmark the perturbations needed, we use random
perturbations as the baseline. If we can find a way to perturb the
models parameters more effectively than random perturbations,
i.e., resulting in a much worse recommendation performance, it
NDCG (testing)
Adversarial Noise
Random Noise
(a) Testing NDCG vs. ϵ
Accuracy (training)
Adversarial Noise
Random Noise
(b) Training Accuracy vs. ϵ
NDCG (testing)
Adversarial Noise
Random Noise
(c) Testing NDCG vs. ϵ
Accuracy (training)
Adversarial Noise
Random Noise
(d) Training Accuracy vs. ϵ
Figure 1: Impact of applying adversarial noises and random noises to the parameters of MF-BPR on Pinterest and Gowalla.
means that the model is not that robust and is vulnerable to certain
perturbations.
Settings. Considering the dominant role of MF in recommendation, we choose MF as the recommender model and optimize it with
BPR. We first train MF-BPR until convergence using SGD. We then
compare the effect of adding random perturbations and adversarial
perturbations to the embeddings of MF. For adversarial perturbations, we define it as the perturbations that aim to maximize the
objective function of BPR:
∆adv = arg
∆, ||∆||≤ϵ LBPR(D| ˆΘ + ∆),
where ϵ controls the magnitude of adversarial perturbations, || · ||
denotes the L2 norm, and ˆΘ is a constant set denoting the current
model parameters. As MF is a bilinear model and BPR objective
function involves nonlinear operations, it is intractable to get exact maximization with respect to ∆. Inspired by the fast gradient
method proposed in Goodfellow et al. , we approximate the objective function by linearizing it round ∆. With this approximation
and the max-norm constraint, we can obtain the optimal ∆as:
∆adv = ϵ Γ
Γ = ∂LBPR(D| ˆΘ + ∆)
As the number of training instances in D is huge, we sample one
unobserved item j to pair with an observed interaction (u,i). We
then perform experiments on this reduced set of examples D′ to
verify the effect of adversarial perturbations.
Results. Figure 1 shows the impact of applying adversarial and
random perturbations to MF-BPR with different settings1 ofϵ on our
Pinterest and Gowalla datasets (details see Section 4.1). Specifically,
we show the performance evaluated by NDCG@100 on the holdout
testing set (Figure 1(a,c)) and the classification accuracy on the
reduced training set D′ (Figure 1(b,d)). The setting of ϵ = 0 means
no perturbations are used, indicating the performance of the welltrained MF-BPR. We have two main observations.
• First, both datasets show that adding adversarial noises leads to
a more significant performance drop than adding random noises.
For example on Gowalla, when ϵ is set to 0.4, applying random
perturbations decreases the testing NDCG by 1.6%, which is a
very minor impact on recommendation; in contrast, applying
adversarial perturbations decreases NDCG significantly by 21.2%
— 13 times larger than that of random perturbations.
• Second, even though the adversarial perturbations are derived
based on partial training instances D′ only, it has a significant
1Note that we enforce the max-norm constraint of ϵ on each embedding vector in P
and Q, rather than the whole matrix.
adverse effect on the recommendation performance. For example
on Gowalla, whenϵ is set to 1, NDCG decreases by 55.4%, whereas
the training accuracy decreases by 5.1% only. Similar finding
applies to the Pinterest dataset, where the drop of testing NDCG
and training accuracy at ϵ = 2 are 57.8% and 10.1%, respectively.
Our results indicate that MF-BPR is relatively robust to random
noises, but it is rather vulnerable to certain perturbations that are
purposefully designed. If a recommender model is robust and can
predict user preference well, how can it be confused so much by
perturbations at a small scale? The existence of such effective adversarial perturbations implies that the model learns a complicated
function that overfits the training data and does not generalize well.
This motivates us to develop new training methods for personalized
ranking, which can lead to robust recommender models that are
insensitive to such adversarial perturbations.
PROPOSED METHODS
In this section, we first present APR, an adversarial learning framework for personalized ranking. We then derive a generic solver
for APR based on SGD. Lastly, we present the AMF method, an
instantiation of APR that uses MF as the recommender model.
Adversarial Personalized Ranking
Our target is to design a new objective function such that by optimizing it, the recommender model is both suitable for personalized
ranking and robust to adversarial perturbations. Due to the rationality of the BPR pairwise objective in personalized ranking, we
choose it as the building block. To enhance the robustness, we
enforce the model to perform well even when the adversarial perturbations (defined in Equation (2)) are presented. To achieve this,
we additionally optimize the model to minimize the BPR objective
function with the perturbed parameters. Formally, we define the
objective function of adversarial personalized ranking as follows:
LAPR(D|Θ) = LBPR(D|Θ) + λLBPR(D|Θ + ∆adv),
∆adv = arg
∆, ||∆||≤ϵ LBPR(D| ˆΘ + ∆),
where ∆denotes the perturbations on model parameters, ϵ ≥0
controls the magnitude of the perturbations, and ˆΘ denotes the
current model parameters. In this formulation, the adversarial term
LBPR(D|Θ + ∆adv) can be seen as regularizing the model by stabilizing the classification function in BPR. As such, we also call
it as adversarial regularizer and use λ to control its strength. As
the intermediate variable ∆maximizes the objective function to be
minimized by Θ, the training process of APR can be expressed as
playing a minimax game:
Θ∗, ∆∗= arg min
∆, ||∆||≤ϵ LBPR(D|Θ) + λLBPR(D|Θ + ∆),
where the learning algorithm for model parameters Θ is the minimizing player, and the procedure for getting perturbations ∆acts as
the maximizing player, which aims to identify the worst-case perturbations against the current model. The two players alternately
play the game until convergence. Since the focus of APR is to get
a good recommender model, in practice we can determine when
to stop the adversarial training by tracking how does the model
perform on a validation set.
We can see that, similar to BPR, our formulation of APR leads to
a general learning framework which is model independent. As long
as the underlying model ˆyui(Θ) is differentiable, it can be learned
under our APR framework using backpropagation and gradientbased optimization algorithms. There are two hyper-parameters —
ϵ and λ — to be specified in APR in addition to the ones in BPR. In
what follows, we propose a generic solution for APR based on SGD.
A Generic SGD Solver for APR
Two optimization strategies are most widely used in recommendation — coordinate descent (CD) and stochastic gradient descent
(SGD). A typical example of CD is alternating least squares ,
which iterates through model parameters and updates one parameter at a time. Note that CD is mostly used to optimize the pointwise
regression loss on linear models . When the optimization target
involves nonlinearities, SGD becomes the default choice due to its
ease in deriving the update strategy . Since APR involves
nonlinear function in its objective function and it has a huge number of training instances (same as BPR), we optimize APR with SGD,
which is easier to implement and is more efficient than CD.
The idea of SGD is to randomly draw a training instance and
update model parameters with respect to the single instance only.
So we consider how to optimize model parameters with respect to
a randomly sampled instance (u,i, j).
Step 1. Constructing Adversarial Perturbations. Given a
training instance (u,i, j), the problem of constructing adversarial
perturbations ∆adv can be formulated as maximizing
ladv((u,i, j)|∆) = −λ lnσ(ˆyui( ˆΘ + ∆) −ˆyuj( ˆΘ + ∆)).
Here ˆΘ is a constant set denoting current model parameters. As
such, the L2 regularizer for Θ is dropped since it is irrelevant to
∆. However, for many models of interest such as the bilinear MF
and multi-layer neural networks , it is difficult to get the
exact optimal solution of ∆adv. Thus, we employ the fast gradient
method proposed in Goodfellow et al. , a common choice in
adversarial training . The idea is to approximate the
objective function around ∆as a linear function. To maximize the
approximated linear function, we only need to move towards the
gradient direction of the objective function with respect to ∆, which
can be derived as2:
∂ladv((u,i, j)|∆)
= −λ(1 −σ(ˆyuij( ˆΘ + ∆))) ∂ˆyuij( ˆΘ + ∆)
2Note the used derivative rules are: ∂ln x
x , and ∂σ (x)
= σ(x)(1 −σ(x)).
Algorithm 1: SGD learning algorithm for APR.
Input: Training data D, adversarial noise level ϵ, adversarial
regularizer λ, L2 regularizer λΘ, learning rate η;
Output: Model parameters Θ;
1 Initialize Θ from BPR ;
2 while Stopping criteria is not met do
Randomly draw (u,i, j) from D ;
// Constructing adversarial perturbations
∆adv ←Equation (8) ;
// Updating model parameters
Θ ←Equation (11) ;
7 return Θ
where ˆyuij(x) = ˆyui(x) −ˆyuj(x) for short. With the max-norm
constraint ||∆|| ≤ϵ, we have the solution for ∆adv as:
∆adv = ϵ Γ
Γ = ∂ladv((u,i, j)|∆)
Step 2. Learning Model Parameters. We now consider how to
learn model parameters Θ. The local objective function to minimize
for a training instance (u,i, j) is as follows:
lAPR((u,i, j)|Θ) = −lnσ(ˆyui(Θ) −ˆyuj(Θ)) + λΘ||Θ||2
−λ lnσ(ˆyui(Θ + ∆adv) −ˆyuj(Θ + ∆adv)).
In this problem, ∆adv is a constant obtained from Equation (8). The
derivative of the objective function with respect to Θ is as follows:
∂lAPR((u,i, j)|Θ)
= −(1 −σ(ˆyuij(Θ))) ∂ˆyuij(Θ)
−λ(1 −σ(ˆyuij(Θ + ∆adv))) ∂ˆyuij(Θ + ∆adv)
Then we can obtain the SGD update rule for Θ:
Θ = Θ −η ∂lAPR((u,i, j)|Θ)
where η denotes the learning rate.
To summarize the SGD solver for APR, we give the training process
in Algorithm 1. In each training step (line 3-5), we first randomly
draw a instance (u,i, j). We then execute the update rule for adversarial perturbations and model parameters in sequential order.
Initialization. It is worth mentioning that the model parameters
Θ are initialized by optimizing BPR (line 1), rather than randomly
initialized. This is because the adversarial perturbations are only
reasonable and necessary to add when the model parameters start
to overfit the data. When the model is underfitting, normal training
process is sufficient to get better parameters. Besides pre-training
with BPR, another feasible strategy is to dynamically adjust ϵ that
controls the level of perturbations during training. For example, it
is possible to learn ϵ based on a holdout validation set. We leave
this exploration as future work, since we find that the current pretraining strategy with a constant ϵ works quite well.
Adversarial Matrix Factorization
To demonstrate how the APR works, we now provide a specific
recommender solution based on MF, a basic yet very effective model
in recommendation. The solution is simple and straightforward
— we first train MF with BPR, and then further optimize it under
our APR framework. We term the method as Adversarial Matrix
Factorization (AMF). Figure 2 illustrates our AMF method. Since
the parameters of MF are embedding vectors for users and items,
we apply adversarial perturbations on the embedding vector. Given
a (u,i) pair, the predictive model with perturbations is defined as:
ˆyui(Θ + ∆) = (pu + ∆u)T (qi + ∆i),
where ∆u ∈RK and ∆i ∈RK denote the perturbation vector for
user u and item i, respectively. Note that the max-norm constraint
||∆|| ≤ϵ is enforced on the level of perturbation vector. To apply
Algorithm 1 in AMF, we simply need to materialize Equation (8)
and (11). For Equation (8), we give the key derivatives as:
∂ˆyuij( ˆΘ + ∆)
= qi + ∆i −qj −∆j,
∂ˆyuij( ˆΘ + ∆)
= pu + ∆u,
∂ˆyuij( ˆΘ + ∆)
= −pu −∆u .
To implement Equation (11), we give the key derivatives as follows:
∂ˆyuij(Θ + ∆adv)
= qi + ∆i −qj −∆j,
∂ˆyuij(Θ + ∆adv)
= pu + ∆u,
∂ˆyuij(Θ + ∆adv)
= −pu −∆u.
Mini-batch Training for AMF. Modern computing units
such as CPU and GPU usually provide speedups for matrix-wise
float operations. To leverage such speedups in learning complex
models, a common strategy is to perform SGD in a mini-batch manner, i.e., updating model parameters on a set of training instances
rather than one instance only. In fact, many machine learning methods implemented in modern tools such as TensorFlow and Theano
apply mini-batch optimizers. Since AMF plays a minimax game
and has two coupled procedures, there are several ways to perform
mini-batch training. Below we detail how we perform mini-batch
training for AMF.
First, given the mini-batch size S, we randomly draw S training
instances from D and term the mini-batch as D′. We then construct
adversarial perturbations by maximizing the adversarial regularizer
over the mini-batch:
Ladv(D′|∆) =
(u,i,j)∈D′
ladv((u,i, j)|∆),
where ladv((u,i, j)|∆) has been defined in Equation (6). For each
user and item3 that occurred in D′, we compute its perturbed vector
by enforcing the max-norm constraint on ∂Ladv (D′|∆)
3Note that the item includes both positive item i and negative item j. It is possible
that a positive i occurs in another instance as a negative item, and vice versa. This
needs to be taken into account to avoid mistake.
-ln σ(ŷui - ŷuj )
Embeddings &
Perturbations
Predictions
Figure 2: Illustration of our AMF method. The perturbations
∆are enforced on each embedding vector of user and item.
Next, we update the model parameters based on the mini-batch
D′. The APR objective function over the mini-batch is given as:
LAPR(D′|Θ) =
(u,i,j)∈D′
lAPR((u,i, j)|Θ),
where lAPR((u,i, j)|Θ) has been defined in Equation (10). Similarly,
for each user and item that occurred in D′, we perform a SGD update as Θ = Θ−η ∂LAPR(D′|Θ)
. We iterate the above two steps until
AMF reaches a convergence state or the validation performance
starts to degrade.
EXPERIMENTS
As the key contribution of this work is to develop a new adversarial
learning method APR for personalized ranking, we aim to answer
the following research questions via experiments.
RQ1 How is the effect of adversarial learning? Can AMF improve
over MF-BPR by performing adversarial learning?
RQ2 How does AMF perform compared with state-of-the-art item
recommendation methods?
RQ3 How do the hyper-parameters ϵ and λ affect the performance
and how to choose optimal values?
Next, we first describe the experimental settings. We then report
results by answering the above research questions in turn.
Experimental Settings
Datasets. We experiment with three publicly available
datasets. Table 1 summarizes the statistics of the datasets (after
all pre-processing steps). These three million-size scale datasets
represent different item recommendation scenarios for business,
image, and location check-in.
Table 1: Statistics of the experimented datasets.
Interaction#
1. Yelp4. This is the Yelp Challenge data of user ratings on businesses. We use the filtered subset created by for evaluating
item recommendation. We find that a user may rate an item multiple times at different timestamps. Since a recommender system
4Downloaded from: 
Figure 3: Training curves of MF-BPR and AMF on Yelp.
typically aims to recommend items that a user did not consume
before, we further merge repetitive ratings to the earliest one. This
can also avoid a testing interaction appearing in the training set.
2. Pinterest5. This implicit feedback dataset was originally constructed by for content-based image recommendation. We use
the filtered subset created by for evaluating collaborative recommendation on images. Since no repetitive interactions are found,
we use the downloaded dataset as it is.
3. Gowalla6. This is the check-in dataset constructed by for
item recommendation. Each interaction represents a user’s check-in
behavior on a venue in Gowalla, a location-based social network.
Same as the setting of Yelp, we merge repetitive check-ins to the
earliest check-in. We then filter out items that have less than 10
interactions and users that have less than 2 interactions.
Evaluation Protocol. We employ the standard leave-oneout protocol, which has been widely used in item recommendation
evaluation . Specifically, for each user in Yelp and Gowalla,
we hold out the latest interaction as the testing set and train a
model on the remaining interactions. As the Pinterest data has no
timestamp information, we randomly hold out an interaction for
each user to form the testing set.
After a model is trained, we generate the personalized ranking
list for a user by ranking all items that are not interacted by the user
in the training set. To study the performance of top-K recommendation, we truncate the ranking list at position K; the default setting
of K is 100 without special mention. We then evaluate the ranking
list using Hit Ratio (HR) and Normalized Discounted Cumulative
Gain (NDCG). HR is a recall-based metric, measuring whether the
testing item is in the top-K list. While NDCG is position-sensitive,
which assigns higher score to hits at higher positions. For both
metrics, larger values indicate better performance. We report the
average score for all users, and perform one-sample paired t-test to
judge the statistical significance where necessary.
Baselines. We compare with the following methods:
- ItemPop. This method ranks items based on their popularity,
evidenced by the number of interactions in the training set. This
is a non-personalized method to benchmark the performance of
personalized recommendation.
- MF-BPR . This method optimizes MF with the BPR objective function. It is a highly competitive approach for item recommendation. We tuned the learning rate and the coefficient for L2
regularization.
5Downloaded from: 
6Downloaded from: 
Figure 4: Training curves of MF-BPR and AMF on Pinterest.
Figure 5: Training curves of MF-BPR and AMF on Gowalla.
- CDAE . This method extends the Denoising Auto-Encoder
for item recommendation. It has been shown to be able to generalize
several latent factor models. We used the original implementation
released by the authors7, and tuned the hyperparameters in the
same way as reported in their paper, including the loss function,
corruption level, L2 regularization and learning rate.
- NeuMF . Neural Matrix Factorization is the state-of-the-art
item recommendation method. It combines MF and multi-layer
perceptrons (MLP) to learn the user-item interaction function. As
suggested in the paper, we pre-trained the model with MF, and
tuned the depth and L2 regularizer for the hidden layers.
- IRGAN . This method combines two types of models via
adversarial training, a generative model that generates items for
a user and a discriminative model that determines whether the
instance is from real data or generated. We used the implementation released by the authors8. We followed the setting of the paper
that pre-trains the generator with LambdaFM . We tuned the
learning rate and number of epochs for generator and discriminator
separately, which we found to have a large impact on its performance. Further tuning of the sampling temperature did not improve
the results, so we used their default settings.
This set of baselines stands for the state-of-the-art performance for
the item recommendation task. In particular, CDAE and NeuMF
are the recently proposed neural recommender models which have
shown significant improvements over conventional shallow methods like MF and FISM . IRGAN takes advantage of generative
adversarial networks and shows good performance on several
IR tasks including recommendation in their paper.
Implementation and Parameter Settings. Our implementation is based on TensorFlow, which is available at: 
com/hexiangnan/adversarial_personalized_ranking. To tune the
hyper-parameters, we randomly holdout one interaction for each
user from the training interactions as the validation set, and we
7 
8 
Embedding Size
Embedding Size
Embedding Size
Figure 6: Performance comparison of HR between MF-BPR and AMF with respect to different embedding sizes.
choose the optimal hyperparameters based on NDCG@100. For a
fair comparison, all models are set with an embedding size of 64
and optimized using the mini-batch Adagrad with a batch size
of 512; moreover, the learning rate is tuned in [0.005, 0.01, 0.05]. For
AMF, we tuneϵ in [0.001, 0.005, 0.01, ..., 1, 5] and λ in [0.001, 0, 01, ..., 1000].
With MF-BPR as pre-training, AMF achieves good performance
when ϵ = 0.5 and λ = 1 on all datasets. As such, without special
mention, we report the performance of AMF on this specific setting.
Effect of Adversarial Learning (RQ1)
To validate the effect of adversarial learning, we first train MF with
BPR for 1, 000 epochs (mostly converged), where each epoch is
defined as training the number of instances the same as the size as
the training set. We then continue training MF with APR, i.e., our
proposed AMF method; as a comparison, we further train MF with
BPR to be consistent with APR.
1. Training Process. Figure 3 to 5 show the performance of MF
and AMF evaluated per 20 epochs on the three datasets. We can see
that all figures show the same trend — after 1, 000 epochs, further
training MF with APR leads to a significant improvement, whereas
further training MF with BPR has little improvements. For example,
on Yelp (Figure 3) the best HR and NDCG of MF-BPR are 0.1721
and 0.0420, respectively, which are improved to 0.1881 and 0.0470
by training with APR. This roughly 10% relative improvement is
very remarkable in recommendation, especially considering that
the underlying recommender model remains the same and we only
change the way of training it.
On Gowalla (Figure 5), the improvements are even larger — 13.5%
and 16.8% in terms of HR and NDCG, respectively. On Pinterest (Figure 4), we notice that HR and NDCG of MF exhibit different trends,
where after 1, 000 epochs HR starts to decrease while NDCG keeps
increasing. This is understandable, since HR and NDCG measure
different aspects of a ranking list — NDCG is position-sensitive by
assigning higher rewards to hits at higher positions while HR is not.
Moreover, this points to the strength of BPR in ranking top items,
owing to its pairwise objective. This observation is consistent with
 ’s finding in evaluating top-K recommendation.
2. Improvements vs. Model Size. Furthermore, we investigate
whether the advantages of adversarial learning apply to models
of different sizes. Figure 6 show the performance of MF-BPR and
AMF with respect to different embedding sizes. Note that we show
the results of HR only due to space limitation, and the figures of
NDCG admit the same findings. First, we can see a clear trend that
the performance of both methods increase with a larger embedding
size. This indicates that a larger model is beneficial to top-K recommendation due to the increased modeling capability. Second, we
observe that AMF demonstrates consistent improvements over MF
on models of all embedding sizes. Notably, AMF with an embedding
size of 32 even performs better than MF with a larger embedding
size of 64 on all datasets. This further verifies the positive effect of
adversarial learning in our APR method.
Lastly, it is worth noting that the improvements of AMF are
less significant when the embedding size is small, compared to the
setting of large embedding size. This implies that when a model is
small and has limited capability, its robustness is not a serious issue.
While for large models that are easy to overfit the training data, it
is crucial to increase a model’s robustness by learning with adversarial perturbations, which in turn can increase its generalization
performance. We believe that this insight is particularly useful for
the recommendation task, which typically involves a large space
of input features (e.g., user ID, item ID, and other attributed and
contextual variables). Given such a large feature space, even a shallow embedding model like Factorization Machine will have a
large number of parameters, not to mention the more expressive
deep neural networks such as Neural Factorization Machine 
and Deep Crossing . This work introduces adversarial learning
to address the ranking task, providing a new means to increase the
generalization ability of large models and having the potential to
improve a wide range of models.
Table 2: The impact of applying adversarial perturbations
to the MF model trained by BPR and APR, respectively. The
number shows the relative decrease in NDCG.
3. Robustness of AMF. We retrospect our motivating example in
Section 2.3 to investigate the robustness of a model trained by APR.
Table 2 shows the impact of applying adversarial perturbations to
the MF model trained by BPR and APR, respectively.
We can see that by training MF with APR, the model becomes less
sensitive to adversarial perturbations compared to that trained with
BPR. For example, on Gowalla, adding adversarial perturbations at
a noise level of 0.5 to MF-BPR decreases NDCG by 26.3%, while the
number is only 2.9% for AMF. These results verify that our AMF is
rather robust to adversarial perturbations, an important property
that indicates good generalization ability of a model.
Table 3: Top-K recommendation performance at K = 50 and K = 100. The best result of each setting is highlighted in bold font.
∗indicates that the improvement of the best result is statistically significant for p < 0.01 compared against all other methods.
The last column “RI” indicates the relative improvement of AMF over the corresponding baseline on average.
Yelp, NDCG
Pinterest, HR
Pinterest, NDCG
Gowalla, HR
Gowalla, NDCG
IRGAN 
NeuMF 
Embedding Norm
MF(L2_reg=0)
MF(L2_reg=0.0001)
Embedding Norm
MF(L2_reg=0)
MF(L2_reg=0.0001)
Figure 7: The norm of embedding matrices of MF and AMF
at each training epoch on Yelp and Gowalla.
4. Adversarial Regularization vs. L2 Regularization. The reason that APR improves over BPR is because of the adversarial
regularizer. To be clear about its effect on parameter learning, we
perform some micro-level analysis on model parameters. Figure 7
shows the norm of embedding matrices (i.e., ||P||2 + ||Q||2) of MF
and AMF in each epoch on Yelp and Gowalla. As a comparison, we
also show the effect of L2 regularization, a popular technique in
recommendation to prevent overfitting.
Interestingly, we find that adding adversarial regularization increases the embedding norm. This is reasonable, since we constrain
the adversarial perturbations in APR to have a fixed norm, thus
increasing the embedding norm is helpful to reduce the impact of
perturbations. Nevertheless, simply increasing the norm by scaling
up the parameters is a trivial solution, which will not improve a
model’s generalization performance. This provides evidence that
our proposed learning algorithm indeed updates parameters in a
rather meaningful way towards enhancing the model’s robustness.
In contrast, adding L2 regularization decreases the embedding norm
to combat overfitting. Based on these, we conclude that adversarial
regularization improves a model’s generalization in a different but
more effective way from the conventional L2 regularization.
Performance Comparison (RQ2)
We now compare our AMF with baselines. Table 3 shows the results of top-K recommendation with K setting to 50 and 100. Note
that we do not report results at smaller K, because our protocol
ranks all items which makes the results at smaller K exhibit large
variances. More importantly, evaluating at a larger K is more instructive for practitioners9. From Table 3, we have the following
key observations:
9Practical recommender systems typically have two stages , 1) candidate selection
that selects hundreds of items that might be of interest to a user, and 2) ranking
that re-ranks the candidates to show top a few results. The first stage typically relies
on collaborative filtering (CF) with the objective of a high recall. Thus, it is more
instructive to evaluate CF with a large K of hundreds, rather than a small number.
1. Our AMF achieves the best results in most cases. The only
exception is on Yelp, where IRGAN outperforms AMF by a small
margin in NDCG@50 and is on par with AMF in NDCG@100.
For the other cases, AMF outperforms other comparing methods
statistically significantly with a p-value of smaller than 0.01. This
signifies that AMF achieves the state-of-the-art performance for
item recommendation.
2. Specifically, compared to NeuMF — a recently proposed and
very expressive deep learning model, AMF exhibits an average
improvement of 2.9%. This is very remarkable, since AMF uses the
shallow MF model that has much fewer parameters, which also
implies the potential of improving conventional shallow methods
with a better training algorithm.
3. Moreover, as compared to IRGAN, which also applies adversarial learning on MF but in a different way, AMF betters it by 5.9% on
average. This further verifies the effectiveness of our APR method.
It is worth mentioning that APR is more efficient and much easier
to train than IRGAN, which needs to be carefully tuned to avoid
mode collapse, while APR only requires an initialization from BPR.
4. Among the baselines, NeuMF performs the best, which verifies the advantage of nonlinear neural networks in learning the
user-item interaction function. Another neural recommender model
CDAE performs weaker, which only shows significant improvements over MF-BPR on the Gowalla dataset. IRGAN manages to
outperform MF-BPR in most cases, which can be attributed to its
improved training process, since the underlying model is also MF.
Lastly, all personalized methods outperform ItemPop by a large
margin, which indicates the necessity of personalization in recommendation task. This is not a new finding and has been verified by
many previous works .
Hyper-parameter Studies (RQ3)
Our APR method introduces two additional hyper-parameters ϵ
and λ to control the noise level and the strength of adversarial
regularizer, respectively. Here we show how do the two hyperparameters impact the performance and also shed lights on how
to set them. Due to space limitation, we show the results on the
Pinterest and Gowalla datasets only, and the results on the Yelp
dataset show exactly the same trend.
First, we fix λ to the default value of 1 and vary ϵ. As can be
seen from Figure 8, the optimal value is around 0.5. When ϵ is too
small (e.g., less than 0.1), AMF behaves similarly to MF-BPR and
has only minor improvements. This further verifies the positive
effect of increasing the robustness of a model to perturbations on
Figure 8: Performance of AMF with respect to different values of ϵ on Pinterest and Gowalla (λ is set to 1).
its parameters. Moreover, when ϵ is too large (e.g., larger than 1),
the performance drops dramatically. This indicates that too large
perturbations will destroy the learning process of model parameters.
As such, our suggested setting of ϵ is 0.5 for AMF when it has been
pre-trained with BPR.
Figure 9: Performance of AMF with respect to different values of λ on Pinterest and Gowalla (ϵ is set to 0.5).
Second, we fix ϵ to 0.5 and vary λ. Figure 9 shows the results.
We can see that when λ is smaller than 1, increasing λ leads to
gradual improvements. When λ is larger than 1, further increasing
it neither improves nor decreases the performance up until a large
value of 1, 000. This means that AMF is rather insensitive when
λ is sufficiently large to reflect the adversarial effect. As such, we
suggest to set λ to 1 (or a larger value such as 10) for AMF.
RELATED WORK
Item Recommendation
Due to the abundance of user feedback such ratings and purchases
that can directly reflect a user’s preference, research on item recommendation have mainly focused on mining the feedback data,
known as collaborative filtering (CF). Among the various CF methods, matrix factorization (MF) , a special type of latent factor
models, is a basic yet most effective recommender model. Popularized by the Netflix Challenge, early works on CF have largely
focused on explicit ratings . These works formulated the
recommendation task as a regression problem to predict the rating
score. Later on, some research found that a good CF model in rating
prediction may not necessarily perform well in top-K recommendation , and called on recommendation research to focus more
on the ranking task.
Along another line, research on CF has gradually shifted from
explicit ratings to one-class implicit feedback . Rendle et
al. first argued that item recommendation is a personalized
ranking task, and such that, the optimization should be tailored
for ranking rather than regression. They then proposed a pairwise
learning method BPR, which optimizes a model based on the relative preference of a user over pairs of items. Later on, BPR has
been used to optimize a wide range of models ,
being a dominant technique in recommendation. Recently, Ding
et al. improved BPR with a better negative sampler by additionally leveraging view data in E-commerce. Our proposed APR
directly enhances BPR by adversarial training, having the potential
to improve all existing recommender systems based on BPR.
From the perspective of models, there are many recent efforts developing non-linear neural network models for CF to take advantage of deep learning. In particular, He et
al. argued the limitation of fixed interaction function (i.e., inner
product) in MF, and proposed a neural collaborative filtering (NCF)
framework that learns the interaction function from data. They then
designed a NCF model named NeuMF, which unifies the strength of
MF and MLP in learning the interaction function. Later on, the NCF
framework was extended to incorporate the neighborhoods 
and attributes of users and items, to model contexts for POI
recommendation , to model image/video content features for
multi-media recommendation , to model aspects in textual reviews , to recommend items for a group of users , and so on.
In addition to the feed-forward NCF framework, recurrent neural
networks have also been developed to handle the temporal signal
in session-aware recommendation .
Adversarial Learning
This work is inspired by the recent developments of adversarial
machine learning techniques . Briefly speaking, it
was found that normal supervised training process makes a classier
vulnerable to adversarial examples , which revealed the potential issue of an unstable model in generalization. To address
the issue, researchers then proposed adversarial training methods
which augment the training process by dynamically generating adversarial examples . Learning over these adversarial examples
can be seen as a way to regularize the training process. Recently,
the idea of adversarial training has been extended to learn adaptive
dropout for hidden layers in deep neural networks .
Existing work on the emerging field of adversarial learning was
largely focused on the domain of image classification. There are
very few studies on adversarial learning for ranking — the core task
in IR. The work that is most relevant with ours is IRGAN , which
also employs adversarial learning, more precisely the GAN framework , to address the matching problem. Our APR methodology
is fundamentally different from IRGAN, which aims to unify the
strength of generative and discriminative models. Specifically, in
the pairwise formulation of IRGAN, the generator approximates
the relevance distribution to generate document (item) pairs given
a query (user), and the discriminator tries to distinguish whether
the document pairs are from real data or generated. Unfortunately,
it is intuitively difficult to understand why IRGAN-pairwise can
improve personalized ranking in item recommendation (in fact,
both the original paper and their released codes only have IRGANpointwise for the recommendation task).
It is worth noting that in the literature of recommender systems,
the concept of robustness usually refers to the degree that an algorithm can resist the profile injection attack, i.e., the attack that tries
to manipulate the recommendation by inserting user profiles .
This line of research is orthogonal to our work, since we consider
improving a recommender model by making it resistant to adversarial perturbations on its parameters. Through this way, we can get a
more robust and stable predictive function, and in turn improving
its generalization performance. To the best of our knowledge, this
has never been explored before in the domain of IR.
CONCLUSION AND FUTURE WORK
This work contributes a new learning method for optimizing recommender models. We show that a model optimized by BPR, a
dominant pairwise learning method in recommendation, is vulnerable to adversarial perturbations on its parameters. This implies
the possible weakness of a model optimized with BPR in generalization. Towards the goal of learning more robust models for
personalized ranking, we propose to perform adversarial training
on BPR, namely, Adversarial Personalized Ranking. We develop a
generic learning algorithm for APR based on SGD, and employ the
algorithm to optimize MF. In our evaluation, we perform extensive
analysis to demonstrate the highly positive effect of adversarial
learning for personalized ranking.
In future, we plan to extend our APR method to other recommender models. First, we are interested in exploring more generic
feature-based models like Neural Factorization Machines and
Deep Crossing that can support a wide range of recommendation scenarios, such as cold-start, context-aware, session-based
recommendation and so on. Second, we will employ APR on the
recently developed neural CF models such as NeuMF and
neighbor-based NCF to further advance the performance of
item recommendation. The challenge here is how to properly employ adversarial training on deep hidden layers, since this work
addresses the embedding layer of shallow MF model only. Lastly,
it is worth mentioning that our APR represents a generic methodology to improve pairwise learning by using adversarial training.
Pairwise learning is not specific to recommendation, and it has
been widely applied to many other IR tasks, such as text retrieval,
web search, question answering, knowledge graph completion, to
name a few. We will work on extending the impact of APR to these
fields beyond recommendation.
Acknowledgments. This work is supported by NExT, by the National Research Foundation Singapore under its AI Singapore Programme, Linksure Network Holding Pte Ltd and the Asia Big Data
Association , and by the National
Natural Science Foundation of China under Grant No.: 61702300.