A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional
Neural Networks for Sentence Classiﬁcation
Dept. of Computer Science
University of Texas at Austin
 
Byron C. Wallace
University of Texas at Austin
 
Convolutional Neural Networks (CNNs)
have recently achieved remarkably strong
performance on the practically important task of sentence classiﬁcation . However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the ﬁlter region size, regularization parameters,
and so on. It is currently unknown how
sensitive model performance is to changes
in these conﬁgurations for the task of sentence classiﬁcation.
We thus conduct a
sensitivity analysis of one-layer CNNs to
explore the effect of architecture components on model performance; our aim
is to distinguish between important and
comparatively inconsequential design decisions for sentence classiﬁcation.
focus on one-layer CNNs (to the exclusion of more complex models) due to their
comparative simplicity and strong empirical performance, which makes it a modern
standard baseline method akin to Support
Vector Machine (SVMs) and logistic regression. We derive practical advice from
our extensive empirical results for those
interested in getting the most out of CNNs
for sentence classiﬁcation in real world
Introduction
Convolutional Neural Networks (CNNs) have recently been shown to achieve impressive results
on the practically important task of sentence categorization . CNNs can capitalize on distributed representations of words by ﬁrst converting the tokens
comprising each sentence into a vector, forming a
matrix to be used as input (e.g., see Fig. 1). The
models need not be complex to realize strong results: Kim , for example, proposed a simple
one-layer CNN that achieved state-of-the-art (or
comparable) results across several datasets. The
very strong results achieved with this comparatively simple CNN architecture suggest that it may
serve as a drop-in replacement for well-established
baseline models, such as SVM 
or logistic regression. While more complex deep
learning models for text classiﬁcation will undoubtedly continue to be developed, those deploying such technologies in practice will likely be attracted to simpler variants, which afford fast training and prediction times.
Unfortunately, a downside to CNN-based models – even simple ones – is that they require practitioners to specify the exact model architecture to
be used and to set the accompanying hyperparameters. To the uninitiated, making such decisions
can seem like something of a black art because
there are many free parameters in the model. This
is especially true when compared to, e.g., SVM
and logistic regression. Furthermore, in practice
exploring the space of possible conﬁgurations for
this model is extremely expensive, for two reasons: (1) training these models is relatively slow,
even using GPUs.
For example, on the SST-1
dataset , it takes about 1 hour
to run 10-fold cross validation, using a similar
conﬁguration to that described in .1
(2) The space of possible model architectures and
hyperparameter settings is vast. Indeed, the simple
CNN architecture we consider requires, at a minimum, specifying: input word vector representations; ﬁlter region size(s); the number of feature
maps; the activation function(s); the pooling strategy; and regularization terms (dropout/l2).
1All experiments run with Theano on an NVIDIA K20
 
In practice, tuning all of these parameters
is simply not feasible, especially because parameter estimation is computationally intensive.
Emerging research has begun to explore hyperparameter optimization methods, including random
search , and Bayesian optimization . However, these sophisticated search methods still require knowing which hyperparameters
are worth exploring to begin with (and reasonable
ranges for each). Furthermore, we believe it will
be some time before Bayesian optimization methods are integrated into deployed, real-world systems.
In this work our aim is to identify empirically
the settings that practitioners should expend effort
tuning, and those that are either inconsequential
with respect to performance or that seem to have
a ‘best’ setting independent of the speciﬁc dataset,
and provide a reasonable range for each hyperparameter. We take inspiration from previous empirical analyses of neural models due to Coates et al.
 and Breuel , which investigated factors in unsupervised feature learning and hyperparameter settings for Stochastic Gradient Descent
(SGD), respectively. Here we report the results
of a large number of experiments exploring different conﬁgurations of CNNs run over nine sentence
classiﬁcation datasets. Most previous work in this
area reports only mean accuracies calculated via
cross-validation. But there is substantial variance
in the performance of CNNs, even on the same
folds and with model conﬁguration held constant.
Therefore, in our experiments we perform replications of cross-validation and report accuracy/Area
Under Curve (AUC) score means and ranges over
For those interested in only the punchlines,
we summarize our empirical ﬁndings and provide
practical guidance based on these in Section 5.
Background and Preliminaries
Deep and neural learning methods are now well
established in machine learning . They have been especially
successful for image and speech processing tasks.
More recently, such methods have begun to overtake traditional sparse, linear models for NLP
 .
Recently, word embeddings have been exploited for sentence classiﬁcation using CNN architectures.
Kalchbrenner proposed a
CNN architecture with multiple convolution layers, positing latent, dense and low-dimensional
word vectors (initialized to random values) as inputs. Kim deﬁned a one-layer CNN architecture that performed comparably. This model
uses pre-trained word vectors as inputs, which
may be treated as static or non-static. In the former approach, word vectors are treated as ﬁxed
inputs, while in the latter they are ‘tuned’ for
a speciﬁc task.
Elsewhere, Johnson and Zhang
 proposed a similar model, but swapped in
high dimensional ‘one-hot’ vector representations
of words as CNN inputs. Their focus was on classiﬁcation of longer texts, rather than sentences (but
of course the model can be used for sentence classiﬁcation).
The relative simplicity of Kim’s architecture –
which is largely the same as that proposed by
Johnson and Zhang , modulo the word vectors – coupled with observed strong empirical performance makes this a strong contender to supplant existing text classiﬁcation baselines such as
SVM and logistic regression. But in practice one
is faced with making several model architecture
decisions and setting various hyperparameters. At
present, very little empirical data is available to
guide such decisions; addressing this gap is our
CNN Architecture
We begin with a tokenized sentence which we
then convert to a sentence matrix, the rows of
which are word vector representations of each token. These might be, e.g., outputs from trained
word2vec or GloVe models. We denote the dimensionality of the word vectors by d.
length of a given sentence is s, then the dimensionality of the sentence matrix is s × d.2 Following
Collobert and Weston , we can then effectively treat the sentence matrix as an ‘image’, and
perform convolution on it via linear ﬁlters. In text
applications there is inherent sequential structure
to the data. Because rows represent discrete symbols (namely, words), it is reasonable to use ﬁlters with widths equal to the dimensionality of the
2We use the same zero-padding strategy as in (Kim,
word vectors (i.e., d). Thus we can simply vary
the ‘height’ of the ﬁlter, i.e., the number of adjacent rows considered jointly. We will refer to the
height of the ﬁlter as the region size of the ﬁlter.
Suppose that there is a ﬁlter parameterized by
the weight matrix w with region size h; w will
contain h · d parameters to be estimated. We denote the sentence matrix by A ∈Rs×d, and use
A[i : j] to represent the sub-matrix of A from row
i to row j. The output sequence o ∈Rs−h+1 of
the convolution operator is obtained by repeatedly
applying the ﬁlter on sub-matrices of A:
oi = w · A[i : i + h −1],
where i = 1 . . . s −h + 1, and · is the dot product between the sub-matrix and the ﬁlter (a sum
over element-wise multiplications). We add a bias
term b ∈R and an activation function f to each
oi, inducing the feature map c ∈Rs−h+1 for this
ci = f(oi + b).
One may use multiple ﬁlters for the same region size to learn complementary features from
the same regions. One may also specify multiple kinds of ﬁlters with different region sizes (i.e.,
‘heights’).
The dimensionality of the feature map generated by each ﬁlter will vary as a function of the
sentence length and the ﬁlter region size. A pooling function is thus applied to each feature map to
induce a ﬁxed-length vector. A common strategy
is 1-max pooling , which
extracts a scalar from each feature map. Together,
the outputs generated from each ﬁlter map can be
concatenated into a ﬁxed-length, ‘top-level’ feature vector, which is then fed through a softmax
function to generate the ﬁnal classiﬁcation. At this
softmax layer, one may apply ‘dropout’ as a means of regularization. This entails randomly setting values in the weight vector
to 0. One may also impose an l2 norm constraint,
i.e., linearly scale the l2 norm of the vector to a
pre-speciﬁed threshold when it exceeds this. Fig.
1 provides a schematic illustrating the model architecture just described.
A reasonable training objective to be minimized
is the categorical cross-entropy loss. The parameters to be estimated include the weight vector(s)
of the ﬁlter(s), the bias term in the activation function, and the weight vector of the softmax function. In the ‘non-static’ approach, one also tunes
the word vectors. Optimization is performed using SGD and back-propagation .
Brieﬂy, these are summarized as follows.
MR: sentence polarity dataset from . (2) SST-1: Stanford Sentiment Treebank . To make input representations consistent across tasks, we only train
and test on sentences, in contrast to the use in
 , wherein models were trained on both
phrases and sentences. (3) SST-2: Derived from
SST-1, but pared to only two classes. We again
only train and test models on sentences, excluding
phrases. (4) Subj: Subjectivity dataset .
(5) TREC: Question classiﬁcation
dataset . (6) CR: Customer
review dataset . (7) MPQA:
Opinion polarity dataset . Additionally, we use (8) Opi:
Opinosis Dataset,
which comprises sentences extracted from user reviews on a given topic, e.g. “sound quality of ipod
nano”. There are 51 such topics and each topic
contains approximately 100 sentences . (9) Irony : this
contains 16,006 sentences from reddit labeled as
ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make
classes sizes equal.3 For this dataset we report the
Area Under Curve (AUC), rather than accuracy,
because it is imbalanced.
Baseline Models
To provide a point of reference for the CNN results, we ﬁrst report the performance achieved using SVM for sentence classiﬁcation. As a baseline, we used a linear kernel SVM exploiting uniand bi-gram features.4
We then used averaged
word vectors (from Google word2vec5 or GloVe6)
calculated over the words comprising the sentence
as features and used an RBF-kernel SVM as the
classiﬁer operating in this dense feature space. We
3Empirically,
under-sampling
outperformed
oversampling in mitigating imbalance, see also Wallace .
4For this we used scikit-learn .
5 
6 
region size
6 univariate
concatenated
together to form a
single feature
Sentence matrix
3 region sizes: (2,3,4)
2 ﬁlters for each region
totally 6 ﬁlters
convolution
activation function
softmax function
regularization
in this layer
Figure 1: Illustration of a CNN architecture for sentence classiﬁcation. We depict three ﬁlter region sizes:
2, 3 and 4, each of which has 2 ﬁlters. Filters perform convolutions on the sentence matrix and generate
(variable-length) feature maps; 1-max pooling is performed over each map, i.e., the largest number from
each feature map is recorded. Thus a univariate feature vector is generated from all six maps, and these
6 features are concatenated to form a feature vector for the penultimate layer. The ﬁnal softmax layer
then receives this feature vector as input and uses it to classify the sentence; here we assume binary
classiﬁcation and hence depict two possible output states.
also experimented with combining the uni-gram,
bi-gram and word vector features with a linear kernel SVM. We kept only the most frequent 30k ngrams for all datasets, and tuned hyperparameters
via nested cross-fold validation, optimizing for accuracy (AUC for Irony). For consistency, we used
the same pre-processing steps for the data as described in previous work . We report
means from 10-folds over all datasets in Table 1.7
Notably, even naively incorporating word2vec embeddings into feature vectors usually improves results.
7Note that parameter estimation for SVM via QP is deterministic, thus we do not replicate the cross validation here.
Baseline Conﬁguration
We ﬁrst consider the performance of a baseline
CNN conﬁguration. Speciﬁcally, we start with the
architectural decisions and hyperparameters used
in previous work and described in
To contextualize the variance in performance attributable to various architecture decisions and hyperparameter settings, it is critical
to assess the variance due strictly to the parameter estimation procedure. Most prior work, unfortunately, has not reported such variance, despite
a highly stochastic learning procedure. This variance is attributable to estimation via SGD, random
dropout, and random weight parameter initialization. Holding all variables (including the folds)
Accuracy (AUC for Irony) achieved
by SVM with different feature sets. bowSVM:
uni- and bi-gram features.
wvSVM: a naive
word2vec-based representation, i.e., the average
(300-dimensional) word vector for each sentence.
bowwvSVM: concatenates bow vectors with the
average word2vec representations.
constant, we show that the mean performance calculated via 10-fold cross validation (CV) exhibits
relatively high variance over repeated runs. We
replicated CV experiments 100 times for each
dataset, so that each replication was a 10-fold CV,
wherein the folds were ﬁxed. We recorded the average performance for each replication and report
the mean, minimum and maximum average accuracy (or AUC) values observed over 100 replications of CV (that is, we report means and ranges
of averages calculated over 10-fold CV). This provides a sense of the variance we might observe
without any changes to the model. We did this for
both static and non-static methods. For all experiments, we used the same preprocessing steps for
the data as in . For SGD, we used the
ADADELTA update rule , and set
the minibatch size to 50. We randomly selected
10% of the training data as the validation set for
early stopping.
Fig. 2 provides density plots of the mean accuracy of 10-fold CV over the 100 replications
for both methods on all datasets. For presentation clarity, in this ﬁgure we exclude the SST-1,
Opi and Irony datasets, because performance was
substantially lower on these (results can be found
in the tables).
Note that we pre-processed/split
datasets differently than in some of the original
work to ensure consistency for our present analysis; thus results may not be directly comparable
to prior work. We emphasize that our aim here is
not to improve on the state-of-the-art, but rather
to explore the sensitivity of CNNs with respect to
design decisions.
Having established a baseline performance for
CNNs, we now consider the effect of different ar-
Description
input word vectors
Google word2vec
ﬁlter region size
feature maps
activation function
1-max pooling
dropout rate
l2 norm constraint
Table 2: Baseline conﬁguration. ‘feature maps’
refers to the number of feature maps for each ﬁlter
region size. ‘ReLU’ refers to rectiﬁed linear unit
 , a commonly used activation
function in CNNs.
Non-static word2vec
Static word2vec
Figure 2: Density curve of accuracy using static
and non-static word2vec-CNN
chitecture decisions and hyperparameter settings.
To this end, we hold all other settings constant (as
per Table 2) and vary only the component of interest. For every conﬁguration that we consider,
we replicate the experiment 10 times, where each
replication again constitutes a run of 10-fold CV.8
We again report average CV means and associated ranges achieved over the replicated CV runs.
We performed experiments using both ‘static’ and
‘non-static’ word vectors.
The latter uniformly
outperformed the former, and so here we report
results only for the ‘non-static’ variant.
Effect of input word vectors
A nice property of sentence classiﬁcation models
that start with distributed representations of words
as inputs is the ﬂexibility such architectures afford
to swap in different pre-trained word vectors during model initialization. Therefore, we ﬁrst explore the sensitivity of CNNs for sentence classi-
ﬁcation with respect to the input representations
Speciﬁcally, we replaced word2vec with
GloVe representations. Google word2vec uses a
local context window model trained on 100 billion
8Running 100 replications for every conﬁguration that we
consider was not computationally feasible.
Non-static word2vec-CNN
Non-static GloVe-CNN
Non-static GloVe+word2vec CNN
81.24 (80.69, 81.56)
81.03 (80.68,81.48)
81.02 (80.75,81.32)
47.08 (46.42,48.01)
45.65 (45.09,45.94)
45.98 (45.49,46.65)
85.49 (85.03, 85.90)
85.22 (85.04,85.48)
85.45 (85.03,85.82)
93.20 (92.97, 93.45)
93.64 (93.51,93.77)
93.66 (93.39,93.87)
91.54 (91.15, 91.92)
90.38 (90.19,90.59)
91.37 (91.13,91.62)
83.92 (82.95, 84.56)
84.33 (84.00,84.67)
84.65 (84.21,84.96)
89.32 (88.84, 89.73)
89.57 (89.31,89.78)
89.55 (89.22,89.88)
64.93 (64.23,65.58)
65.68 (65.29,66.19)
65.65 (65.15,65.98)
67.07 (65.60,69.00)
67.20 (66.45,67.96)
67.11 (66.66,68.50)
Performance using non-static word2vec-CNN, non-static GloVe-CNN, and non-static
GloVe+word2vec CNN, respectively. Each cell reports the mean (min, max) of summary performance
measures calculated over multiple runs of 10-fold cross-validation. We will use this format for all tables
involving replications
words from Google News ,
while GloVe is a model based on global wordword co-occurrence statistics . We used a GloVe model trained on a corpus of 840 billion tokens of web data. For both
word2vec and GloVe we induce 300-dimensional
word vectors. We report results achieved using
GloVe representations in Table 3. Here we only
report non-static GloVe results (which again uniformely outperformed the static variant).
experimented
concatenating
word2vec and GloVe representations, thus creating 600-dimensional word vectors to be used
as input to the CNN. Pre-trained vectors may
not always be available for speciﬁc words (either
in word2vec or GloVe, or both); in such cases,
we randomly initialized the corresponding subvectors. Results are reported in the ﬁnal column
of Table 3.
The relative performance achieved using GloVe
versus word2vec depends on the dataset, and, unfortunately, simply concatenating these representations does necessarily seem helpful. Practically,
our results suggest experimenting with different
pre-trained word vectors for new tasks.
We also experimented with using long, sparse
one-hot vectors as input word representations, in
the spirit of Johnson and Zhang . In this
strategy, each word is encoded as a one-hot vector, with dimensionality equal to the vocabulary
size. Though this representation combined with
one-layer CNN achieves good results on document classiﬁcation, it is still unknown whether
this is useful for sentence classiﬁcation. We keep
the other settings the same as in the basic con-
ﬁguration, and the one-hot vector is ﬁxed during
training. Compared to using embeddings as input to the CNN, we found the one-hot approach
to perform poorly for sentence classiﬁcation tasks.
We believe that one-hot CNN may not be suitable for sentence classiﬁcation when one has a
small to modestly sized training dataset, likely
due to sparsity: the sentences are perhaps too
brief to provide enough information for this highdimensional encoding. Alternative one-hot architectures might be more appropriate for this scenario. For example, Johnson and Zhang propose a semi-supervised CNN
variant which ﬁrst learns embeddings of small text
regions from unlabeled data, and then integrates
them into a supervised CNN. We emphasize that
if training data is plentiful, learning embeddings
from scratch may indeed be best.
Effect of ﬁlter region size
Region size
77.85 (77.47,77.97)
80.48 (80.26,80.65)
81.13 (80.96,81.32)
81.65 (81.45,81.85)
81.43 (81.28,81.75)
81.26 (81.01,81.43)
81.06 (80.87,81.30)
80.91 (80.73,81.10)
80.91 (80.72,81.05)
Table 4: Effect of single ﬁlter region size. Due to
space constraints, we report results for only one
dataset here, but these are generally illustrative.
We ﬁrst explore the effect of ﬁlter region size
when using only one region size, and we set the
number of feature maps for this region size to 100
(as in the baseline conﬁguration). We consider region sizes of 1, 3, 5, 7, 10, 15, 20, 25 and 30, and
record the means and ranges over 10 replications
of 10-fold CV for each. We report results in Table 10 and Fig. 3. Because we are only interested
in the trend of the accuracy as we alter the region
size (rather than the absolute performance on each
Filter region size
Change in accuracy (%)
Figure 3: Effect of the region size (using only
Number of feature maps for each filter region size (log-scale)
Change in accuracy (%)
Figure 4: Effect of the number of feature maps.
task), we show only the percent change in accuracy (AUC for Irony) from an arbitrary baseline
point (here, a region size of 3).
From the results, one can see that each dataset
has its own optimal ﬁlter region size. Practically,
this suggests performing a coarse grid search over
a range of region sizes; the ﬁgure here suggests
that a reasonable range for sentence classiﬁcation
might be from 1 to 10. However, for datasets comprising longer sentences, such as CR (maximum
sentence length is 105, whereas it ranges from 36-
56 on the other sentiment datasets used here), the
optimal region size may be larger.
We also explored the effect of combining different ﬁlter region sizes, while keeping the number of feature maps for each region size ﬁxed at
100. We found that combining several ﬁlters with
region sizes close to the optimal single region size
can improve performance, but adding region sizes
far from the optimal range may hurt performance.
For example, when using a single ﬁlter size, one
can observe that the optimal single region size for
the MR dataset is 7. We therefore combined several different ﬁlter region sizes close to this optimal range, and compared this to approaches that
Multiple region size
Accuracy (%)
81.65 (81.45,81.85)
81.24 (80.69, 81.56)
81.28 (81.07,81.56)
81.57 (81.31,81.80)
81.69 (81.27,81.93)
(10,11,12)
81.52 (81.27,81.87)
(11,12,13)
81.53 (81.35,81.76)
81.43 (81.10,81.61)
81.62 (81.38,81.72)
81.63 (81.33,82.08)
81.73 (81.33,81.94)
Table 5: Effect of ﬁlter region size with several
region sizes on the MR dataset.
use region sizes outside of this range. From Table 5, one can see that using (5,6,7),and (7,8,9)
and (6,7,8,9) – sets near the best single region size
– produce the best results. The difference is especially pronounced when comparing to the baseline setting of (3,4,5). Note that even only using
a single good ﬁlter region size (here, 7) results in
better performance than combining different sizes
(3,4,5). The best performing strategy is to simply use many feature maps (here, 400) all with region size equal to 7, i.e., the single best region size.
However, we note that in some cases (e.g., for the
TREC dataset), using multiple different, but nearoptimal, region sizes performs best.
We provide another illustrative empirical result
using several region sizes on the TREC dataset in
Table 6. From the performance of single region
size, we see that the best single ﬁlter region sizes
for TREC are 3 and 5, so we explore the region
size around these values, and compare this to using multiple region sizes far away from these ‘optimal’ values.
Multiple region size
Accuracy (%)
91.21 (90.88,91.52)
91.20 (90.96,91.43)
91.48 (90.96,91.70)
91.56 (91.24,91.81)
91.48 (91.17,91.68)
90.79 (90.57,91.26)
(14,15,16)
90.23 (89.81,90.51)
91.57 (91.25,91.94)
91.42 (91.11,91.65)
91.32 (90.53,91.55)
Table 6: Effect of ﬁlter region size with several
region sizes using non-static word2vec-CNN on
TREC dataset
Here we see that (3,3,3) and (3,3,3,3) perform
worse than (2,3,4) and (3,4,5). However, the result
still shows that a combination of region sizes near
the optimal single best region size outperforms using multiple region sizes far from the optimal single region size. Furthermore, we again see that a
single good region size (3) outperforms combining several suboptimal region sizes: (7,8,9) and
(14,15,16).
In light of these observations, we believe it advisable to ﬁrst perform a coarse line-search over a
single ﬁlter region size to ﬁnd the ‘best’ size for
the dataset under consideration, and then explore
the combination of several region sizes nearby this
single best size, including combining both different region sizes and copies of the optimal sizes.
Effect of number of feature maps for
each ﬁlter region size
We again hold other conﬁgurations constant, and
thus have three ﬁlter region sizes: 3, 4 and 5. We
change only the number of feature maps for each
of these relative to the baseline of 100; we consider values ∈{10, 50, 100, 200, 400, 600, 1000,
2000}. We report results in Fig. 4.
The ‘best’ number of feature maps for each ﬁlter region size depends on the dataset. However,
it would seem that increasing the number of maps
beyond 600 yields at best very marginal returns,
and often hurts performance (likely due to over-
ﬁtting). Another salient practical point is that it
takes a longer time to train the model when the
number of feature maps is increased. In practice,
the evidence here suggests perhaps searching over
a range of 100 to 600.
Note that this range is
only provided as a possible standard trick when
one is faced with a new similar sentence classiﬁcation problem; it is of course possible that in some
cases more than 600 feature maps will be bene-
ﬁcial, but the evidence here suggests expending
the effort to explore this is probably not worth it.
In practice, one should consider whether the best
observed value falls near the border of the range
searched over; if so, it is probably worth exploring beyond that border, as suggested in (Bengio,
Effect of activation function
We consider seven different activation functions in
the convolution layer, including: ReLU (as per the
baseline conﬁguration), hyperbolic tangent (tanh),
Sigmoid function , SoftPlus
function , Cube function , and tanh cube function . We use ‘Iden’ to denote the identity function, which means not using any activation function.
We report results achieved using
different activation functions in non-static CNN in
For 8 out of 9 datasets, the best activation function is one of Iden, ReLU and tanh. The SoftPlus
function outperformedd these on only one dataset
(MPQA). Sigmoid, Cube, and tanh cube all consistently performed worse than alternative activation functions. Thus we do not report results for
these here. The performance of the tanh function
may be due to its zero centering property (compared to Sigmoid).
ReLU has the merits of a
non-saturating form compared to Sigmoid, and it
has been observed to accelerate the convergence
of SGD . One interesting result is that not applying any activation function (Iden) sometimes helps. This indicates that on
some datasets, a linear transformation is enough to
capture the correlation between the word embedding and the output label. However, if there are
multiple hidden layers, Iden may be less suitable
than non-linear activation functions. Practically,
with respect to the choice of the activation function in one-layer CNNs, our results suggest experimenting with ReLU and tanh, and perhaps also
Effect of pooling strategy
We next investigated the effect of the pooling strategy and the pooling region size. We ﬁxed the ﬁlter
region sizes and the number of feature maps as in
the baseline conﬁguration, thus changing only the
pooling strategy or pooling region size.
In the baseline conﬁguration, we performed 1max pooling globally over feature maps, inducing
a feature vector of length 1 for each ﬁlter. However, pooling may also be performed over small
equal sized local regions rather than over the entire feature map . Each small
local region on the feature map will generate a single number from pooling, and these numbers can
be concatenated to form a feature vector for one
feature map. The following step is the same as 1max pooling: we concatenate all the feature vectors together to form a single feature vector for the
classiﬁcation layer. We experimented with local
region sizes of 3, 10, 20, and 30, and found that
1-max pooling outperformed all local max pooling
81.28 (81.07, 81.52)
80.58 (80.17, 81.12)
81.30 (81.09, 81.52)
81.16 (80.81, 83.38)
47.02 (46.31, 47.73)
46.95 (46.43, 47.45)
46.73 (46.24,47.18)
47.13 (46.39, 47.56)
85.43 (85.10, 85.85)
84.61 (84.19, 84.94)
85.26 (85.11, 85.45)
85.31 (85.93, 85.66)
93.15 (92.93, 93.34)
92.43 (92.21, 92.61)
93.11 (92.92, 93.22)
93.13 (92.93, 93.23)
91.18 (90.91, 91.47)
91.05 (90.82, 91.29)
91.11 (90.82, 91.34)
91.54 (91.17, 91.84)
84.28 (83.90, 85.11)
83.67 (83.16, 84.26)
84.55 (84.21, 84.69)
83.83 (83.18, 84.21)
89.48 (89.16, 89.84)
89.62 (89.45, 89.77)
89.57 (89.31, 89.88)
89.35 (88.88, 89.58)
65.69 (65.16,66.40)
64.77 (64.25,65.28)
65.32 (64.78,66.09)
65.02 (64.53,65.45)
67.62 (67.18,68.27)
66.20 (65.38,67.20)
66.77 (65.90,67.47)
66.46 (65.99,67.17)
Table 7: Performance of different activation functions
conﬁgurations. This result held across all datasets.
We also considered a k-max pooling strategy
similar to , in which the
maximum k values are extracted from the entire
feature map, and the relative order of these values
is preserved. We explored the k ∈{5, 10, 15, 20},
and again found 1-max pooling fared best, consistently outperforming k-max pooling.
Next, we considered taking an average, rather
than the max, over regions .
We held the rest of architecture constant. We experimented with local average pooling region sizes
{3, 10, 20, 30}.
We found that average pooling uniformly performed (much) worse than max
pooling, at least on the CR and TREC datasets.
Due to the substantially worse performance and
very slow running time observed under average
pooling, we did not complete experiments on all
Our analysis of pooling strategies shows that 1max pooling consistently performs better than alternative strategies for the task of sentence classiﬁcation. This may be because the location of
predictive contexts does not matter, and certain
n-grams in the sentence can be more predictive
on their own than the entire sentence considered
Effect of regularization
Two common regularization strategies for CNNs
are dropout and l2 norm constraints; we explore
the effect of these here. ‘Dropout’ is applied to the
input to the penultimate layer. We experimented
with varying the dropout rate from 0.0 to 0.9, ﬁxing the l2 norm constraint to 3, as per the baseline
conﬁguration. The results for non-static CNN are
shown in in Fig. 5, with 0.5 designated as the baseline. We also report the accuracy achieved when
we remove both dropout and the l2 norm constraint (i.e., when no regularization is performed),
denoted by ‘None’.
Separately, we considered the effect of the
Dropout rate
Change in accuracy (%)
Figure 5: Effect of dropout rate. The accuracy
when the dropout rate is 0.9 on the Opi dataset
is about 10% worse than baseline, and thus is not
visible on the ﬁgure at this point.
l2 norm imposed on the weight vectors that
parametrize the softmax function. Recall that the
l2 norm of a weight vector is linearly scaled to
a constraint c when it exceeds this threshold, so
a smaller c implies stronger regularization. (Like
dropout, this strategy is applied only to the penultimate layer.) We show the relative effect of varying
c on non-static CNN in Figure 6, where we have
ﬁxed the dropout rate to 0.5; 3 is the baseline here
(again, arbitrarily).
l2 norm constraint on weight vectors
Change in accuracy (%)
Effect of the l2 norm constraint on
weight vectors.
From Figures 5 and 6, one can see that non-zero
dropout rates can help (though very little) at some
points from 0.1 to 0.5, depending on datasets. But
imposing an l2 norm constraint generally does not
improve performance much (except on Opi), and
even adversely effects performance on at least one
dataset (CR).
We then also explored dropout rate effect when
increasing the number of feature maps. We increase the number of feature maps for each ﬁlter
size from 100 to 500, and set max l2 norm constraint as 3. The effect of dropout rate is shown
in Fig. 7. We see that the effect of dropout rate
Dropout rate when feature map is 500
Change in accuracy (%)
Figure 7: Effect of dropout rate when using 500
feature maps.
is almost the same as when the number of feature
maps is 100, and it does not help much. But we
observe that for the dataset SST-1, dropout rate actually helps when it is 0.7. Referring to Fig. 4, we
can see that when the number of feature maps is
larger than 100, it hurts the performance possibly
due to overﬁtting, so it is reasonable that in this
case dropout would mitigate this effect.
We also experimented with applying dropout
only to the convolution layer, but still setting the
max norm constraint on the classiﬁcation layer to
3, keeping all other settings exactly the same. This
means we randomly set elements of the sentence
matrix to 0 during training with probability p, and
then multiplied p with the sentence matrix at test
time. The effect of dropout rate on the convolution layer is shown in Fig. 8. Again we see that
dropout on the convolution layer helps little, and
large dropout rate dramatically hurts performance.
To summarize, contrary to some of the existing
literature e , we found that
dropout had little beneﬁcial effect on CNN performance. We attribute this observation to the fact
Dropout rate
Change in accuracy (%)
Figure 8: Effect of dropout rate on the convolution
layer (The accuracy when the dropout rate is 0.9
on the Opi dataset is not visible on the ﬁgure at
this point, as in Fig. 5)
that one-layer CNN has a smaller number parameters than multi-layer deep learning models. Another possible explanation is that using word embeddings helps to prevent overﬁtting (compared
to bag of words based encodings). However, we
are not advocating completely foregoing regularization. Practically, we suggest setting the dropout
rate to a small value (0.0-0.5) and using a relatively large max norm constraint, while increasing
the number of feature maps to see whether more
features might help. When further increasing the
number of feature maps seems to degrade performance, it is probably worth increasing the dropout
Conclusions
We have conducted an extensive experimental
analysis of CNNs for sentence classiﬁcation. We
conclude here by summarizing our main ﬁndings
and deriving from these practical guidance for researchers and practitioners looking to use and deploy CNNs in real-world sentence classiﬁcation
scenarios.
Summary of Main Empirical Findings
• Prior work has tended to report only the mean
performance on datasets achieved by models.
But this overlooks variance due solely to the
stochastic inference procedure used. This can
be substantial: holding everything constant
(including the folds), so that variance is due
exclusively to the stochastic inference procedure, we ﬁnd that mean accuracy (calculated
via 10 fold cross-validation) has a range of
up to 1.5 points. And the range over the AUC
achieved on the irony dataset is even greater
– up to 3.4 points (see Table 3). More replication should be performed in future work, and
ranges/variances should be reported, to prevent potentially spurious conclusions regarding relative model performance.
• We ﬁnd that, even when tuning them to the
task at hand, the choice of input word vector
representation (e.g., between word2vec and
GloVe) has an impact on performance, however different representations perform better
for different tasks. At least for sentence classiﬁcation, both seem to perform better than
using one-hot vectors directly. We note, however, that: (1) this may not be the case if
one has a sufﬁciently large amount of training data, and, (2) the recent semi-supervised
CNN model proposed by Johnson and Zhang
 may improve performance, as compared to the simpler version
of the model considered here ).
• The ﬁlter region size can have a large effect
on performance, and should be tuned.
• The number of feature maps can also play
an important role in the performance, and increasing the number of feature maps will increase the training time of the model.
• 1-max pooling uniformly outperforms other
pooling strategies.
• Regularization has relatively little effect on
the performance of the model.
Speciﬁc advice to practitioners
Drawing upon our empirical results, we provide
the following guidance regarding CNN architecture and hyperparameters for practitioners looking
to deploy CNNs for sentence classiﬁcation tasks.
• Consider starting with the basic conﬁguration described in Table 2 and using non-static
word2vec or GloVe rather than one-hot vectors.
However, if the training dataset size
is very large, it may be worthwhile to explore using one-hot vectors.
Alternatively,
if one has access to a large set of unlabeled
in-domain data, 
might also be an option.
• Line-search over the single ﬁlter region size
to ﬁnd the ‘best’ single region size. A reasonable range might be 1∼10. However, for
datasets with very long sentences like CR, it
may be worth exploring larger ﬁlter region
sizes. Once this ‘best’ region size is identiﬁed, it may be worth exploring combining
multiple ﬁlters using regions sizes near this
single best size, given that empirically multiple ‘good’ region sizes always outperformed
using only the single best region size.
• Alter the number of feature maps for each ﬁlter region size from 100 to 600, and when this
is being explored, use a small dropout rate
(0.0-0.5) and a large max norm constraint.
Note that increasing the number of feature
maps will increase the running time, so there
is a trade-off to consider.
Also pay attention whether the best value found is near the
border of the range . If the
best value is near 600, it may be worth trying
larger values.
• Consider different activation functions if possible: ReLU and tanh are the best overall candidates. And it might also be worth trying
no activation function at all for our one-layer
• Use 1-max pooling; it does not seem necessary to expend resources evaluating alternative strategies.
• Regarding regularization: When increasing
the number of feature maps begins to reduce
performance, try imposing stronger regularization, e.g., a dropout out rate larger than
• When assessing the performance of a model
(or a particular conﬁguration thereof), it is
imperative to consider variance. Therefore,
replications of the cross-fold validation procedure should be performed and variances
and ranges should be considered.
Of course, the above suggestions are applicable
only to datasets comprising sentences with similar properties to the those considered in this work.
And there may be examples that run counter to our
ﬁndings here. Nonetheless, we believe these suggestions are likely to provide a reasonable starting point for researchers or practitioners looking
to apply a simple one-layer CNN to real world
sentence classiﬁcation tasks. We emphasize that
we selected this simple one-layer CNN in light of
observed strong empirical performance, which positions it as a new standard baseline model akin to
bag-of-words SVM and logistic regression. This
approach should thus be considered prior to implementation of more sophisticated models.
We have attempted here to provide practical,
empirically informed guidance to help data science practitioners ﬁnd the best conﬁguration for
this simple model. We recognize that manual and
grid search over hyperparameters is sub-optimal,
and note that our suggestions here may also inform hyperparameter ranges to explore in random
search or Bayesian optimization frameworks.
Acknowledgments
This work was supported in part by the Army Research Ofﬁce (grant W911NF-14-1-0442) and by
The Foundation for Science and Technology, Portugal .
This work was also made possible by the support
of the Texas Advanced Computer Center (TACC)
at UT Austin.
We thank Tong Zhang and Rie Johnson for helpful feedback.