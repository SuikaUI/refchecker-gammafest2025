Feature Generating Networks for Zero-Shot Learning
Yongqin Xian1
Tobias Lorenz1
Bernt Schiele1
Zeynep Akata1,2
1Max Planck Institute for Informatics
2Amsterdam Machine Learning Lab
Saarland Informatics Campus
University of Amsterdam
Suffering from the extreme training data imbalance between seen and unseen classes, most of existing state-of-theart approaches fail to achieve satisfactory results for the
challenging generalized zero-shot learning task. To circumvent the need for labeled examples of unseen classes, we
propose a novel generative adversarial network (GAN) that
synthesizes CNN features conditioned on class-level semantic information, offering a shortcut directly from a semantic
descriptor of a class to a class-conditional feature distribution. Our proposed approach, pairing a Wasserstein GAN
with a classiﬁcation loss, is able to generate sufﬁciently discriminative CNN features to train softmax classiﬁers or any
multimodal embedding method. Our experimental results
demonstrate a signiﬁcant boost in accuracy over the state of
the art on ﬁve challenging datasets – CUB, FLO, SUN, AWA
and ImageNet – in both the zero-shot learning and generalized zero-shot learning settings.
1. Introduction
Deep learning has allowed to push performance considerably across a wide range of computer vision and machine
learning tasks. However, almost always, deep learning requires large amounts of training data which we are lacking
in many practical scenarios, e.g. it is impractical to annotate all the concepts that surround us, and have enough of
those annotated samples to train a deep network. Therefore, training data generation has become a hot research
topic . Generative Adversarial Networks are particularly appealing as they allow generating realistic and sharp images conditioned, for instance, on
object categories . However, they do not yet generate images of sufﬁcient quality to train deep learning architectures as demonstrated by our experimental results.
In this work, we are focusing on arguably the most extreme case of lacking data, namely zero-shot learning , where the task is to learn to classify when no labeled
examples of certain classes are available during training.
CNN feature
Head color: brown
Belly color: yellow
Bill shape: pointy
Figure 1: CNN features can be extracted from: 1) real images, however in zero-shot learning we do not have access
to any real images of unseen classes, 2) synthetic images,
however they are not accurate enough to improve image
classiﬁcation performance. We tackle both of these problems and propose a novel attribute conditional feature generating adversarial network formulation, i.e. f-CLSWGAN,
to generate CNN features of unseen classes.
We argue that this scenario is a great testbed for evaluating the robustness and generalization of generative models.
In particular, if the generator learns discriminative visual
data with enough variation, the generated data should be
useful for supervised learning. Hence, one contribution of
our paper is a comparison of various existing GAN-models
and another competing generative model, i.e. GMMN, for
visual feature generation. In particular, we look into both
zero-shot learning (ZSL) where the test time search space is
restricted to unseen class labels and generalized zero-shot
learning (GZSL) for being a more realistic scenario as at test
time the classiﬁer has to decide between both seen and unseen class labels. In this context, we propose a novel GANmethod – namely f-CLSWGAN that generates features instead of images and is trained with a novel loss improving
over alternative GAN-models.
We summarize our contributions as follows. (1) We propose a novel conditional generative model f-CLSWGAN
that synthesizes CNN features of unseen classes by optimizing the Wasserstein distance regularized by a classiﬁcation
loss. (2) Across ﬁve datasets with varying granularity and
sizes, we consistently improve upon the state of the art in
both the ZSL and GZSL settings. We demonstrate a prac-
 
tical application for adversarial training and propose GZSL
as a proxy task to evaluate the performance of generative
models. (3) Our model is generalizable to different deep
CNN features, e.g. extracted from GoogleNet or ResNet,
and may use different class-level auxiliary information, e.g.
sentence, attribute, and word2vec embeddings.
2. Related work
In this section we review some recent relevant literature
on Generative Adversarial Networks, Zero-Shot Learning
(ZSL) and Generalized Zero-Shot (GZSL) Learning.
Generative Adversarial Network. GAN was originally proposed as a means of learning a generative model
which captures an arbitrary data distribution, such as images, from a particular domain. The input to a generator
network is a “noise” vector z drawn from a latent distribution, such as a multivariate Gaussian. DCGAN extends GAN by leveraging deep convolution neural networks
and providing best practices for GAN training. improves DCGAN by factorizing the image generation process into style and structure networks, InfoGAN extends GAN by additionally maximizing the mutual information between interpretable latent variables and the generator distribution. GAN has also been extended to a conditional GAN by feeding the class label , sentence descriptions , into both the generator and discriminator. The theory of GAN is recently investigated in
 , where they show that the Jenson-Shannon divergence optimized by the original GAN leads to instability
issues. To cure the unstable training issues of GANs, 
proposes Wasserstein-GAN (WGAN), which optimizes an
efﬁcient approximation of the Wasserstein distance. While
WGAN attains better theoretical properties than the original
GAN, it still suffers from vanishing and exploding gradient
problems due to weight clipping to enforce the 1-Lipschitz
constraint on the discriminator. Hence, proposes an
improved version of WGAN enforcing the Lipschitz constraint through gradient penalty.
Although those papers
have demonstrated realistic looking images, they have not
applied this idea to image feature generation.
In this paper, we empirically show that images generated
by the state-of-the-art GAN are not ready to be used as
training data for learning a classiﬁer. Hence, we propose a
novel GAN architecture to directly generate CNN features
that can be used to train a discriminative classiﬁer for zeroshot learning. Combining the powerful WGAN loss
and a classiﬁcation loss which enforces the generated features to be discriminative, our proposed GAN architecture
improves the original GAN by a large margin and has
an edge over WGAN thanks to our regularizer.
ZSL and GZSL. In the zero-shot learning setting, the set
of classes seen during training and evaluated during test are
disjoint . As supervised learning methods can not be employed for this task, proposed to
solve it by solving related sub-problems. learn
unseen classes as a mixture of seen class proportions, and
 learn a compatibility between images and classes. On the other hand, instead of using only labeled data, leverage unlabeled data from unseen classes in the transductive setting.
While zero-shot learning has attracted a lot of attention,
there has been little work in the more realistic generalized zero-shot learning setting, where both seen and unseen classes appear at test time.
In this paper, we propose to tackle generalized zero-shot
learning by generating CNN features for unseen classes via
a novel GAN model. Our work is different from because they generate additional examples for data-starved
classes from feature vectors alone, which is unimodal and
do not generalize to unseen classes. Our work is closer
to in which they generate features via GMMN .
Hence, we directly compare with them on the latest zeroshot learning benchmark and show that WGAN 
coupled with our proposed classiﬁcation loss can further
improve GMMN in feature generation on most datasets for
both ZSL and GZSL tasks.
3. Feature Generation & Classiﬁcation in ZSL
Existing ZSL models only see labeled data from seen
classes during training biasing the predictions to seen
classes. The main insight of our proposed model is that
by feeding additional synthetic CNN features of unseen
classes, the learned classiﬁer will also explore the embedding space of unseen classes. Hence, the key to our approach is the ability to generate semantically rich CNN feature distributions conditioned on a class speciﬁc semantic
vector e.g. attributes, without access to any images of that
class. This alleviates the imbalance between seen and unseen classes, as there is no limit to the number of synthetic
CNN features that our model can generate. It also allows to
directly train a discriminative classiﬁer, i.e. Softmax classi-
ﬁer, even for unseen classes.
We begin by deﬁning the problem of our interest. Let
S = {(x, y, c(y))|x ∈X, y ∈Ys, c(y) ∈C} where
S stands for the training data of seen classes, x ∈Rdx
is the CNN features, y denotes the class label in Ys =
{y1, . . . , yK} consisting of K discrete seen classes, and
c(y) ∈Rdc is the class embedding, e.g. attributes, of class
y that models the semantic relationship between classes.
In addition, we have a disjoint class label set Yu
{u1, . . . , uL} of unseen classes, whose class embedding set
U = {(u, c(u))|u ∈Yu, c(u) ∈C} is available but images
and image features are missing. Given S and U, the task of
ZSL is to learn a classiﬁer fzsl : X →Yu and in GZSL we
learn a classiﬁer fgzsl : X →Ys ∪Yu.
3.1. Feature Generation
In this section, we begin our discussion with Generative
Adversarial Networks (GAN) for it being the basis of
our model. GAN consists of a generative network G and
a discriminative network D that compete in a two player
minimax game. In the context of generating image pixels,
D tries to accurately distinguish real images from generated
images, while G tries to fool the discriminator by generating images that are mistakable for real. Following , we
extend GAN to conditional GAN by including a conditional
variable to both G and D. In the following we give the details of the conditional GAN variants that we develop. Our
novelty lies in that we develop three conditional GAN variants, i.e. f-GAN, f-WGAN and f-CLSWGAN, to generate
image features rather than image pixels. It is worth noting
that our models are only trained with seen class data S but
can also generate image features of unseen classes.
f-GAN. Given the train data S of seen classes, we aim to
learn a conditional generator G : Z × C →X, which takes
random Gaussian noise z ∈Z ⊂Rdz and class embedding
c(y) ∈C as its inputs, and outputs a CNN image feature ˜x ∈
X of class y. Once the generator G learns to generate CNN
features of real images, i.e. x, conditioned on the seen class
embedding c(y) ∈Ys, it can also generate ˜x of any unseen
class u via its class embedding c(u). Our feature generator
f-GAN is learned by optimizing the following objective,
D LGAN =E[log D(x, c(y))]+
E[log (1 −D(˜x, c(y)))],
with ˜x = G(z, c(y)). The discriminator D : X × C →
 is a multi-layer perceptron with a sigmoid function
as the last layer. While D tries to maximize the loss, G
tries to minimizes it. Although GAN has been shown to
capture complex data distributions, e.g. pixel images, they
are notoriously difﬁcult to train .
f-WGAN. We extend the improved WGAN to a conditional WGAN by integrating the class embedding c(y) to
both the generator and the discriminator. The loss is,
LW GAN =E[D(x, c(y))] −E[D(˜x, c(y))]−
λE[(||∇ˆxD(ˆx, c(y))||2 −1)2],
where ˜x = G(z, c(y)), ˆx = αx + (1 −α)˜x with α ∼
U(0, 1), and λ is the penalty coefﬁcient. In contrast to the
GAN, the discriminative network here is deﬁned as D : X ×
C →R, which eliminates the sigmoid layer and outputs a
real value. The log in Equation 1 is also removed since we
are not optimizing the log likelihood. Instead, the ﬁrst two
terms in Equation 2 approximate the Wasserstein distance,
and the third term is the gradient penalty which enforces
the gradient of D to have unit norm along the straight line
z ~ N(0, 1)
Head color: brown
Belly color: yellow
Bill shape: pointy
discriminator
Head color: brown
Belly color: yellow
Bill shape: pointy
Our f-CLSWGAN: we propose to minimize
the classiﬁcation loss over the generated features and the
Wasserstein distance with gradient penalty.
between pairs of real and generated points. Again, we solve
a minmax optimization problem,
f-CLSWGAN. f-WGAN does not guarantee that the generated CNN features are well suited for training a discriminative classiﬁer, which is our goal. We conjecture that this
issue could be alleviated by encouraging the generator to
construct features that can be correctly classiﬁed by a discriminative classiﬁer trained on the input data. To this end,
we propose to minimize the classiﬁcation loss over the generated features in our novel f-CLSWGAN formulation. We
use the negative log likelihood,
LCLS = −E˜x∼p˜x[log P(y|˜x; θ)],
where ˜x = G(z, c(y)), y is the class label of ˜x, P(y|˜x; θ)
denotes the probability of ˜x being predicted with its true
class label y. The conditional probability is computed by a
linear softmax classiﬁer parameterized by θ, which is pretrained on the real features of seen classes. The classiﬁcation loss can be thought of as a regularizer enforcing the
generator to construct discriminative features. Our full objective then becomes,
D LW GAN + βLCLS
where β is a hyperparameter weighting the classiﬁer.
3.2. Classiﬁcation
Given c(u) of any unseen class u ∈Yu, by resampling
the noise z and then recomputing ˜x = G(z, c(u)), arbitrarily many visual CNN features ˜x can be synthesized. After
repeating this feature generation process for every unseen
class, we obtain a synthetic training set ˜U = {(˜x, u, c(u))}.
We then learn a classiﬁer by training either a multimodal
embedding model or a softmax classiﬁer. Our generated
features allow to train those methods on the combinations
of real seen class data S and generated unseen class data ˜U.
Multimodal Embedding.
Many zero-shot learning
approaches, e.g.
ALE , DEVISE , SJE , ES-
ZSL and LATEM , learn a multimodal embedding
between the image feature space X and the class embedding
space C using seen classes data S. With our generated features, those methods can be trained with seen classes data S
together with unseen classes data ˜U to learn a more robust
classiﬁer. The embedding model F(x, c(y); W), parameterized by W, measures the compatibility score between any
image feature x and class embedding c(y) pair. Given a
query image feature x, the classiﬁer searches for the class
embedding with the highest compatibility via:
f(x) = argmax
F(x, c(y); W),
where in ZSL, y ∈Yu and in GZSL, y ∈Ys ∪Yu.
Softmax. The standard softmax classiﬁer minimizes the
negative log likelihood loss,
log P(y|x; θ),
where θ ∈Rdx×N is the weight matrix of a fully connected layer which maps the image feature x to N unnormalized probabilities with N being the number of classes,
and P(y|x; θ) =
Depending on the task,
T = ˜U if it is ZSL and T = S ∪˜U if it is GZSL. The
prediction function is:
f(x) = arg max
P(y|x; θ),
where in ZSL, y ∈Yu and in GZSL, y ∈Ys ∪Yu.
4. Experiments
First we detail our experimental protocol, then we
present (1) our results comparing our framework with the
state of the art for GZSL and ZSL tasks on four challenging
datasets, (2) our analysis of f-xGAN 1 under different conditions, (3) our large-scale experiments on ImageNet and
(4) our comparison of image and image feature generation.
Datasets. Caltech-UCSD-Birds 200-2011 (CUB) , Oxford Flowers (FLO) and SUN Attribute (SUN) 
are all ﬁne-grained datasets.
CUB contains 11,788 images from 200 different types of birds annotated with 312
attributes.
FLO dataset 8189 images from 102 different
types of ﬂowers without attribute annotations. However, for
both CUB and FLO we use the ﬁne-grained visual descriptions collected by . SUN contains 14,340 images from
717 scenes annotated with 102 attributes. Finally, Animals
with Attributes (AWA) is a coarse-grained dataset with
1We denote our f-GAN, f-WGAN, f-CLSWGAN as f-xGAN
|Ys| + |Yu|
Table 1: CUB, SUN, FLO, AWA datasets, in terms of number of attributes per class (att), sentences (stc), number
of classes in training + validation (Ys) and test classes (Yu).
30,475 images, 50 classes and 85 attributes. Statistics of the
datasets are presented in Table 1. We use the zero-shot splits
proposed by for AWA, CUB and SUN insuring that
none of the training classes are present in ImageNet 2.
For FLO, we use the standard split provided by .
Features. As real CNN features, we extract 2048-dim toplayer pooling units of the 101-layered ResNet from the
entire image. We do not do any image pre-processing such
as cropping or use any other data augmentation techniques.
ResNet is pre-trained on ImageNet 1K and not ﬁne-tuned.
As synthetic CNN features, we generate 2048-dim CNN
features using our f-xGAN model. As the class embedding, unless it is stated otherwise, we use per-class attributes
for AWA (85-dim), CUB (312-dim) and SUN (102-dim).
Furthermore, for CUB and Flowers, we extract 1024-dim
character-based CNN-RNN features from ﬁne-grained
visual descriptions (10 sentences per image). None of the
Yu sentences are seen during training the CNN-RNN. We
build per-class sentences by averaging the CNN-RNN features that belong to the same class.
Evaluation Protocol. At test time, in the ZSL setting, the
aim is to assign an unseen class label, i.e. Yu to the test
image and in GZSL setting, the search space includes both
seen or unseen classes, i.e. Ys ∪Yu. We use the uniﬁed
evaluation protocol proposed in . In the ZSL setting,
the average accuracy is computed independently for each
class before dividing their cumulative sum by the number of
classes; i.e., we measure average per-class top-1 accuracy
(T1). In the GZSL setting, we compute average per-class
top-1 accuracy on seen classes (Ys) denoted as s, average
per-class top-1 accuracy on unseen classes (Yu) denoted as
u and their harmonic mean, i.e. H = 2 ∗(s ∗u)/(s + u).
Implementation details. In all f-xGAN models, both the
generator and the discriminator are MLP with LeakyReLU
activation. The generator consists of a single hidden layer
with 4096 hidden units. Its output layer is ReLU because
we aim to learn the top max-pooling units of ResNet-101.
While the discriminator of f-GAN has one hidden layer
with 1024 hidden units in order to stabilize the GAN training, the discriminators of f-WGAN and f-CLSWGAN have
2as ImageNet is used for pre-training the ResNet 
Zero-Shot Learning
Generalized Zero-Shot Learning
CUB FLO SUN AWA
DEVISE none
LATEM 
69.9 53.6 39.2
ESZSL 
f-CLSWGAN 61.5 71.2 62.1
43.7 57.7 49.7 59.0 73.8 65.6 42.6 36.6 39.4 57.9 61.4 59.6
Table 2: ZSL measuring per-class average Top-1 accuracy (T1) on Yu and GZSL measuring u = T1 on Yu, s = T1 on Ys,
H = harmonic mean (FG=feature generator, none: no access to generated CNN features, hence softmax is not applicable).
f-CLSWGAN signiﬁcantly boosts both the ZSL and GZSL accuracy of all classiﬁcation models on all four datasets.
one hidden layer with 4096 hidden units as WGAN 
does not have instability issues thus a stronger discriminator can be applied here. We do not apply batch normalization our empirical evaluation showed a signiﬁcant degradation of the accuracy when batch normalization is used. The
noise z is drawn from a unit Gaussian with the same dimensionality as the class embedding. We use λ = 10 as
suggested in and β = 0.01 across all the datasets.
4.1. Comparing with State-of-the-Art
In a ﬁrst set of experiments, we evaluate our f-xGAN
features in both the ZSL and GZSL settings on four challenging datasets: CUB, FLO, SUN and AWA. Unless it is
stated otherwise, we use att for CUB, SUN, AWA and
stc for FLO (as att are not available). We compare the
effect of our feature generating f-xGAN to 6 recent stateof-the-art methods .
ZSL with f-CLSWGAN. We ﬁrst provide ZSL results with
our f-CLSWGAN in Table 2 (left).
Here, the test-time
search space is restricted to unseen classes Yu. First, our
f-CLSWGAN in all cases improves the state of the art that is
obtained without feature generation. The overall accuracy
improvement on CUB is from 54.9% to 61.5%, on FLO
from 53.4% to 71.2%, on SUN from 58.1% to 62.1% and
on AWA from 65.6% to 69.9%, i.e. all quite signiﬁcant.
Another observation is that feature generation is applicable
to all the multimodal embedding models and softmax.
These results demonstrate that indeed our f-CLSWGAN
generates generalizable and strong visual features of previously unseen classes.
GZSL with f-CLSWGAN. Our main interest is GZSL
where the test time search space contains both seen and unseen classes, Ys ∪Yu, and at test time the images come
both from seen and unseen classes. Therefore, we evaluate both seen and unseen class accuracy, i.e. s and u, as
well as their harmonic mean (H). The GZSL results with
f-CLSWGAN in Table 2 (right) demonstrate that for all
datasets our f-xGAN signiﬁcantly improves the H-measure
over the state-of-the-art. On CUB, f-CLSWGAN obtains
49.7% in H measure, signiﬁcantly improving the state of
the art (34.4%), on FLO it achieves 65.6% (vs. 21.9%), on
SUN it reaches 39.4% (vs. 26.3%), and on AWA it achieves
59.6% (vs. 27.5%). The accuracy boost can be attributed to
the strength of the f-CLSWGAN generator learning to imitate CNN features of unseen classes although not having
seen any real CNN features of these classes before.
We also observe that without feature generation on all
models the seen class accuracy is signiﬁcantly higher than
unseen class accuracy, which indicates that many samples
are incorrectly assigned to one of the seen classes. Feature generation through f-CLSWGAN ﬁnds a balance between seen and unseen class accuracies by improving the
unseen class accuracy while maintaining the accuracy on
seen classes. Furthermore, we would like to emphasize that
the simple softmax classiﬁer beats all the models and is
now applicable to GZSL thanks to our CNN feature generation. This shows the true potential and generalizability of
feature generation to various tasks.
ZSL and GZSL with f-xGAN. The generative model is an
important component of our framework. Here, we evalu-
LATEM ESZSL
Classification Model
Top-1 Acc. (in %)
LATEM ESZSL
Classification Model
Top-1 Acc. (in %)
(a) Zero-Shot Learning
LATEM ESZSL
Classification Model
Harmonic Mean (in %)
LATEM ESZSL
Classification Model
Harmonic Mean (in %)
(b) Generalized Zero-Shot Learning
Figure 3: Comparing f-xGAN versions with f-GMMN as well as comparing multimodal embedding methods with softmax.
Top-1 Acc. (in %)
Top-1 Acc. (in %)
Figure 4: Measuring the seen class accuracy of the classi-
ﬁer trained on generated features of seen classes w.r.t. the
training epochs (with softmax).
ate all versions of our f-xGAN and f-GMMN for it being
a strong alternative. We show ZSL and GZSL results of
all classiﬁcation models in Figure 3. We selected CUB and
FLO for them being ﬁne-grained datasets, however we provide full numerical results and plots in the supplementary
which shows that our observations hold across datasets. Our
ﬁrst observation is that for both ZSL and GZSL settings all
generative models improve in all cases over “none” with
no access to the synthetic CNN features. This applies to
the GZSL setting and the difference between “none” and
f-xGAN is strikingly signiﬁcant. Our second observation
is that our novel f-CLSWGAN model is the best performing
generative model in almost all cases for both datasets. Our
ﬁnal observation is that although f-WGAN rarely performs
lower than f-GMMN, e.g. ESZL on FLO, our f-CLSWGAN
which uses a classiﬁcation loss in the generator recovers
from it and achieves the best result among all these generative models. We conclude from these experiments that generating CNN features to support the classiﬁer when there is
missing data is a technique that is ﬂexible and strong.
We notice that recently has shown great performance on the old splits of AWA and CUB datasets. We
compare our method with using the same evaluation
protocol as our paper, i.e same data splits and evaluation
metrics. On AWA, in ZSL task, the comparison is 66.1%
vs 69.9% (ours) and in GZSL task, it is 41.4% vs 59.6%
(ours). On CUB, in ZSL task, the comparison is 50.1% vs
61.5% (ours) and in GZSL task it is 29.2% vs 49.7% (ours).
# of generated features per class
Top-1 Acc. (in %)
# of generated features per class
Top-1 Acc. (in %)
Figure 5: Increasing the number of generated f-xGAN features wrt unseen class accuracy (with softmax) in ZSL.
4.2. Analyzing f-xGAN Under Different Conditions
In this section, we analyze f-xGAN in terms of stability, generalization, CNN architecture used to extract real
CNN features and the effect of class embeddings on two
ﬁne-grained datasets, namely CUB and FLO.
Stability and Generalization. We ﬁrst analyze how well
different generative models ﬁt the seen class data used
for training. Instead of using Parzen window-based loglikelihood that is unstable, we train a softmax classiﬁer
with generated features of seen classes and report the classi-
ﬁcation accuracy on a held-out test set. Figure 4 shows the
classiﬁcation accuracy w.r.t the number of training epochs.
On both datasets, we observe a stable training trend. On
FLO, compared to the supervised classiﬁcation accuracy
obtained with real images, i.e. the upper bound marked with
dashed line, f-GAN remains quite weak even after convergence, which indicates that f-GAN has underﬁtting issues.
A strong alternative is f-GMMN leads to a signiﬁcant accuracy boost while our f-WGAN and f-CLSWGAN improve
over f-GMMN and almost reach the supervised upper bound.
After having established that our f-xGAN leads to a
stable training performance and generating highly descriptive features, we evaluate the generalization ability of the
f-xGAN generator to unseen classes. Using the pre-trained
model, we generate CNN features of unseen classes. We
then train a softmax classiﬁer using these synthetic CNN
features of unseen classes with real CNN features of seen
ResNet-101
Table 3: GZSL results with GoogLeNet vs ResNet-101 features on CUB (CNN: Deep Feature Encoder Network, FG:
Feature Generator, u = T1 on Yu, s = T1 on Ys, H = harmonic mean, “none”= no generated features).
Attribute (att)
Sentence (stc)
Table 4: GZSL results with conditioning f-xGAN with stc
and att on CUB (C: Class embedding, FG: Feature Generator, u = T1 on Yu, s = T1 on Ys, H = harmonic mean,
“none”= no generated features).
classes. On the GZSL task, Figure 5 shows that increasing
the number of generated features of unseen classes from 1
to 100 leads to a signiﬁcant boost of accuracy, e.g. 28.2%
to 56.5% on CUB and 37.9% to 66.5% on FLO. As in the
case for generating seen class features, here the ordering is
f-GAN < f-WGAN < f-GMMN < f-CLSWGAN on CUB
and f-GAN < f-GMMN < f-WGAN < f-CLSWGAN on
FLO. With these results, we argue that if the generative
model can generalize well to previously unseen data distributions, e.g. perform well on GZSL task, they have practical use in a wide range of real-world applications. Hence,
we propose to quantitatively evaluate the performance of
generative models on the GZSL task.
Effect of CNN Architectures. The aim of this study is
to determine the effect of the deep CNN encoder that provides real features to our f-xGAN discriminator. In Table 3,
we ﬁrst observe that with GoogLeNet features, the results
are lower compared to the ones obtained with ResNet features. This indicates that ResNet features are stronger than
GoogLeNet, which is expected. Besides, most importantly,
with both CNN architectures we observe that our f-xGAN
outperforms the “none” by a large margin.
Speciﬁcally,
the accuracy increases from 25.8% to 36.9% for GoogleNet
features and 34.4% to 49.7% for ResNet features. Those results are encouraging as they demonstrate that our f-xGAN
is not limited to learning the distribution of ResNet-101 features, but also able to learn other feature distributions.
Effect of Class Embeddings. The conditioning variable,
3H M500 M1K M5K L500 L1K L5K
Top-1 Acc. (in %)
3H M500 M1K M5K L500 L1K L5K
Top-1 Acc. (in %)
Figure 6: ZSL and GZSL results on ImageNet (ZSL: T1
on Yu, GZSL: T1 on Yu). The splits, ResNet features and
Word2Vec are provided by . “Ours” = feature generator: f-CLSWGAN, classiﬁer: softmax.
class embedding, is an important component of our
f-xGAN. Therefore, we evaluate two different class embeddings, per-class attributes (att) and per-class sentences
(stc) on CUB as this is the only dataset that has both.
In Table 4, we ﬁrst observe that f-CLSWGAN features generated with att not only lead to a signiﬁcantly higher result
(49.7% vs 34.4%), s and u are much more balanced (57.7%
and 43.7% vs. 62.8% and 23.7%) compared to the state-ofthe-art, i.e. “none”. This is because generated CNN features help us explore the space of unseen classes whereas
the state of the art learns to project images closer to seen
class embeddings.
Finally, f-CLSWGAN features generated with per-class
stc signiﬁcantly improve results over att, achieving
54.0% in H measure, and also leads to a notable u of 50.3%
without hurting s (58.3%). This is due to the fact that stc
leads to high quality features reﬂecting the highly descriptive semantic content language entails and it shows that
our f-CLSWGAN is able to learn higher quality CNN features given a higher quality conditioning signal.
4.3. Large-Scale Experiments
Our large-scale experiments follow the same zero-shot
data splits of and serve two purposes.
show the generalizability of our approach by conducting
ZSL and GZSL experiments on ImageNet for it being the largest-scale single-label image dataset, i.e. with
21K classes and 14M images. Second, as ImageNet does
not contain att, we use as a (weak) conditioning signal
Word2Vec to generate f-CLSWGAN features. Figure 6
shows that softmax as a classiﬁer obtains the state-of-theart of ZSL and GZSL on ImageNet, signiﬁcantly improving
over ALE . These results show that our f-CLSWGAN
is able to generate high quality CNN features also with
Word2Vec as the class embedding.
For ZSL, for instance, with the 2H split “Ours” almost
doubles the performance of ALE (5.38% to 10.00%) and
in one of the extreme cases, e.g. with L1K split, the accuracy improves from 2.85% to 3.62%. For GZSL the same
Generated Data
Image (with )
CNN feature (Ours) 50.3 58.3 54.0 59.0 73.8 65.6
Table 5: Summary Table (u = T1 on Yu, s = T1 accuracy on
Ys, H = harmonic mean, class embedding = stc). “none”:
ALE with no generated features.
observations hold, i.e. the gap between ALE and “Ours” is
2.18 vs 4.38 with 2H split and 1.21 vs 2.50 with L1K split.
Note that, reports the highest results with SYNC 
and “Ours” improves over SYNC as well, e.g. 9.26% vs
10.00% with 2H and 3.23% vs 3.56% with L1K. With these
results we emphasize that with a supervision as weak as a
Word2Vec signal, our model is able to generate CNN features of unseen classes and operate at the ImageNet scale.
This does not only hold for the ZSL setting which discards
all the seen classes from the test-time search space assuming that the evaluated images will belong to one of the unseen classes. It also holds for the GZSL setting where no
such assumption has been made. Our model generalizes to
previously unseen classes even when the seen classes are included in the search space which is the most realistic setting
for image classiﬁcation.
4.4. Feature vs Image Generation
As our main goal is solving the GZSL task which suffers from the lack of visual training examples, one naturally thinks that image generation serves the same purpose.
Therefore, here we compare generating images and image
features for the task of GZSL. We use the StackGAN 
to generate 256 × 256 images conditioned on sentences.
In Table 5, we compare GZSL results obtained with
“none”, i.e. with an ALE model trained on real images
of seen classes, Image, i.e. image features extracted from
256 × 256 synthetic images generated by StackGAN 
and CNN feature, i.e. generated by our f-CLSWGAN.
Between “none” and “Image”, we observe that generating images of unseen classes improves the performance
i.e. harmonic mean on FLO (49.0% for “Image” vs 21.9%
for “none”), but hurts the performance on CUB (31.9% for
“Image” vs 45.1% for “none”). This is because generating birds is a much harder task than generating ﬂowers.
Upon visual inspection, we have observed that although
many images have an accurate visual appearance as birds
or ﬂowers, they lack the necessary discriminative details
to be classiﬁed correctly and the generated images are not
class-consistent. On the other hand, generating CNN features leads to a signiﬁcant boost of accuracy, e.g. 54.0%
on CUB and 65.6% on FLO which is clearly higher than
having no generation, i.e. “none”, and image generation.
We argue that image feature generation has the following
advantages. First, the number of generated image features
is limitless. Second, the image feature generation learns
from compact invariant representations obtained by a deep
network trained on a large-scale dataset such as ImageNet,
therefore the feature generative network can be quite shallow and hence computationally efﬁcient. Third, generated
CNN features are highly discriminative, i.e. they lead to a
signiﬁcant boost in performance of both ZSL and GZSL. Finally, image feature generation is a much easier task as the
generated data is much lower dimensional than high quality
images necessary for discrimination.
5. Conclusion
In this work, we propose f-xGAN, a learning framework for feature generation followed by classiﬁcation, to
tackle the generalized zero-shot learning task. Our f-xGAN
model adapts the conditional GAN architecture that is frequently used for generating image pixels to generate CNN
features. In f-CLSWGAN, we improve WGAN by adding
a classiﬁcation loss on top of the generator, enforcing it to
generate features that are better suited for classiﬁcation. In
our experiments, we have shown that generating features of
unseen classes allows us to effectively use softmax classi-
ﬁers for the GZSL task.
Our framework is generalizable as it can be integrated
to various deep CNN architectures, i.e.
GoogleNet and
ResNet as a pair of the most widely used architectures. It
can also be deployed with various classiﬁers, e.g. ALE,
SJE, DEVISE, LATEM, ESZSL that constitute the state of
the art for ZSL but also the GZSL accuracy improvements
obtained with softmax is important as it is a simple classiﬁer
that could not be used for GZSL before this work. Moreover, our features can be generated via different sources of
class embeddings, e.g. Sentence, Attribute, Word2vec, and
applied to different datasets, i.e. CUB, FLO, SUN, AWA
being ﬁne and coarse-grained ZSL datasets and ImageNet
being a truly large-scale dataset.
Finally, based on the success of our framework, we motivated the use of GZSL tasks as an auxiliary method for evaluation of the expressive power of generative models in addition to manual inspection of generated image pixels which
is tedious and prone to errors. For instance, WGAN 
has been proposed and accepted as an improvement over
GAN . This claim is supported with evaluations based
on manual inspection of the images and the inception score.
Our observations in Figure 3 and in Figure 5 support this
and follow the same ordering of the models, i.e. WGAN
improves over GAN in ZSL and GZSL tasks. Hence, while
not being the primary focus of this paper, we strongly argue, that ZSL and GZSL are suited well as a testbed for
comparing generative models.