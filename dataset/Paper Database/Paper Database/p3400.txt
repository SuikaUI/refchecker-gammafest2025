Received March 6, 2019, accepted March 12, 2019, date of publication March 18, 2019, date of current version April 5, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2905633
A Deep Learning Method With Filter Based
Feature Engineering for Wireless Intrusion
Detection System
SYDNEY MAMBWE KASONGO
AND YANXIA SUN
Department of Electrical and Electronic Engineering Science, University of Johannesburg, Johannesburg 2006, South Africa
Corresponding author: Sydney Mambwe Kasongo ( )
This work was supported in part by the South African National Research Foundation under Grant 112108 and Grant 112142, in part by the
South African National Research Foundation Incentive Grant under 95687, in part by the Eskom Tertiary Education Support Programme
Grant, and in part by the Research Grant from URC of the University of Johannesburg.
ABSTRACT In recent years, the increased use of wireless networks for the transmission of large volumes of
information has generated a myriad of security threats and privacy concerns; consequently, there has been the
development of a number of preventive and protective measures including intrusion detection systems (IDS).
Intrusion detection mechanisms play a pivotal role in securing computer and network systems; however, for
various IDS, the performance remains a major issue. Moreover, the accuracy of existing methodologies for
IDS using machine learning is heavily affected when the feature space grows. In this paper, we propose a
IDS based on deep learning using feed forward deep neural networks (FFDNNs) coupled with a ﬁlter-based
feature selection algorithm. The FFDNN-IDS is evaluated using the well-known NSL-knowledge discovery
and data mining (NSL-KDD) dataset and it is compared to the following existing machine learning methods:
support vectors machines, decision tree, K-Nearest Neighbor, and Naïve Bayes. The experimental results
prove that the FFDNN-IDS achieves an increase in accuracy in comparison to other methods.
INDEX TERMS Deep learning, feature extraction, intrusion detection, machine learning, wireless networks.
I. INTRODUCTION
Computer networks and wireless networks in particular are
subjects to a myriad of security threats and attacks. The
security challenges that have to be solved originate from the
open nature, the ﬂexibility and the mobility of the wireless
communication medium , . In an effort to secure these
networks, various preventive and protective mechanisms such
as intrusion detection systems (IDS) were developed . Primarily, IDS can be classiﬁed as: host based intrusion detection systems (HIDS) and network based intrusion detection
systems (NIDS) . Furthermore, both HIDS and NIDS can
be categorized into: signature-based IDS, anomaly-based IDS
and hybrid IDS , . An Anomaly based IDS analyses the
network under normal circumstances and ﬂags any deviation
as an intrusion. A signature-based IDS relies on a predeﬁned
database of known intrusions to pinpoint an intrusion. In this
case, a manual update of the database is performed by the
system administrators.
The associate editor coordinating the review of this manuscript and
approving it for publication was Shagufta Henna.
In terms of performance, an IDS is considered effective
or accurate in detecting intrusions when it concurrently
achieves low false alarm rates and high classiﬁcation
accuracy ; therefore, decreasing the law false alarm rate
as well as increasing the detection accuracy of an IDS
should be one of the crucial tasks when designing an IDS.
In this paper, the terms wireless intrusion detection system (WIDS) and intrusion detection system (IDS) will be
used interchangeably.
In a bid to build efﬁcient IDS systems, Machine Learning (ML) approaches are used to identify various types of
attacks. ML is the scientiﬁc study of procedures, algorithms
and statistical models used by computer systems to solve
complex problems and it is considered a subset Artiﬁcial
Intelligence (AI) citeb8. Since the issue of intrusion detection
is a classiﬁcation problem, it can be modeled using ML
techniques. It has been proven that developing IDS using ML
methods can produce high levels of accuracy citeb5; however,
citeb9 showed that the most accurate and effective IDS has
not been discovered and that each IDS solution presents its
own advantages and handicaps under various conditions.
VOLUME 7, 2019
2019 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See for more information.
S. M. Kasongo, Y. Sun: Deep Learning Method With Filter-Based Feature Engineering
The most popular ML approaches to intrusion detection
include K-Nearest-Neighbors (KNN)citeb10, Decision Tree
(DT)citeb11, Support Vector Machines (SVM)citeb12, Random Forest (RF) citeb13, Naive Bayes (NB) citeb14 and
Multi-Layered Perceptions (MLP) associated with all Deep
Learning (DL) Methodologies citeb15, b16, b17. An IDS
generally treats large amount of data that causes ML techniques such as the ones in citeb10,b11,b12, b13,b14 to perform poorly; therefore is imperative to devise appropriate
strategies and classiﬁcation approaches to overcome the issue
of under-performance. This paper focuses on DL to try to
improve on the shortcomings of existing systems.
DL was ﬁrst proposed by Professor Hinton and it is
an advanced sub-ﬁeld of ML that simpliﬁes the modeling
of various complex concepts and relationships using multiple levels of representation . DL has achieved a great
amount of success in ﬁelds such as language identiﬁcation,
image processing and pharmaceutical research – .
This has prompted researchers to explore the application of DL theory to the intrusion detection classiﬁcation
The major characteristic that distinguishes DL from traditional ML methods is the improved performance of DL as
the amount of data increases. DL algorithms are not well
suited for problems involving small volumes of data because
these algorithms require a considerable amount of data to be
capable of learning more efﬁciently . Although DL can
handle a high throughput in terms of data, the questions of
accuracy improvement and lowering of false-positive alarm
rate still remain due to the ever-growing size of datasets
used for IDS research. Moreover, as the datasets dilate in
terms of volume; there is also an expansion of the input
space and attack classiﬁcation dimension. Consequently,
instances of misclassiﬁcation are prevalent, which in turn
trigger an increase in false positive alarm rate and impacts
negatively the overall system performance. Therefore, it is
crucial to implement solutions that are capable of selecting
only the needed features to perform an optimal classiﬁcation
operation.
Feature engineering (FE) have become a key topic in many
ML research domains – . As part of FE, the feature
selection algorithms fall into the following the categories:
ﬁlter model, wrapper model and hybrid model. The ﬁlter
model bases itself on the intrinsic nature of the data and it
is independent of the classiﬁer used. The wrapper method
evaluates the performance of the classiﬁcation algorithm used
on a candidate feature subset, whereas the hybrid method is
a combination the wrapper and ﬁlter algorithms . The
methodology proposed in this paper focuses on a ﬁlter-based
approach as the two latter techniques are computationally
expensive .
The major contributions of this paper are outlined as
• A Feature Extraction Unit (FEU) is introduced. By using
ﬁlter-based algorithms, the FEU generates optimal subsets of features with minimum redundancy.
• We scrutinize the performance of the following existing classiﬁcation algorithms applied to IDS without the
FEU by using the NSL-KDD dataset: k-nearest neighbor (KNN), support vector machine (SVM), Decision
Tree (DT), Random Forest (RF) and Naive Bayes (NB).
Moreover, we study the performance of those algorithms
coupled with the FEU.
• A feed-forward deep neural network (FFDNN) is introduced. We study its performance using the FEU and
the NSL-KDD dataset. After the comparison to KNN,
SVM, DT, RF and ND, the FEU-FFDNN proves to be
very appropriate for intrusion detection systems. Furthermore, Experimental results demonstrate that depth
and the number of neurons (nodes) used for an FFDDN
classiﬁer have a direct impact on its accuracy.
The rest of this paper is organized as follow: Section II
of the paper provides a background on wireless networks.
Section III gives an account of similar research with a focus
on ML based IDS as well as various methods for features
selection. Section IV details a background on traditional
machine learning classiﬁers that are also explored in this
work. Section V of this document provides an architecture
of the proposed method for wireless intrusion detection.
Section VI details the experimental setup used in this research
as well as the tools used to design, implement, evaluate and
test the following classiﬁers: SVM, DT, RF, NB, KNN and
FFDNN, and the results are discussed. Section VII concludes
the paper.
II. BACKGROUND: WIRELESS NETWORKS
In recent years, the growth of wireless networks has been
very predominant over wired ones. Wireless communication
is attractive because it does not require any wired additional infrastructure for the communication media. Today,
the most popular form of wireless networks are Wireless
Local Area networks (WLANs). WLANs form part of the
IEEE 802.11 family and are intensively used as an effective
alternative to wired communication in various areas such as
industrial communication and in building communication.
A myriad of security mechanisms including Wired Equivalent
Protection (WEP) and WiFi Protected Access (WAP, WAP2)
have been mainly used to secure and protect WLANs; however, they have shown many ﬂaws when it comes to threats
such as Denial of Service (DoS) attacks, network discovery
attacks, brute force attacks, etc , , . In order to
reinforce WLANs security against those vulnerabilities, IDSs
are generally implemented. In this research, we focus on
an IDS for WLANs using DL approach. Furthermore, since
wired and wireless IDS systems research go hand in hand,
this work reviews strategies used both in wired and wireless
IDS research using ML and DL.
III. RELATED WORK
This section provides an account of previous studies on feature selection methods in general as well as intrusion detection systems using ML and DL techniques.
VOLUME 7, 2019
S. M. Kasongo, Y. Sun: Deep Learning Method With Filter-Based Feature Engineering
The research conducted in presented a deep learning based intrusion detection system that made use of
non-symmetric deep auto-encoder (NDAE) for feature learning and a classiﬁcation methodology using stacked NDAEs.
An NDAE is an auto-encoder made of non-symmetrical multiple hidden layers. In simple terms, it is a deep neural network composed of many non-symmetrical hidden layers. The
evaluation of the IDS scheme was made using two datasets:
the KDDCup 99 and the NSL-KDD. The performance of
the multiclass classiﬁcation experiment yielded an accuracy
of 85.42% over the NSL-KDD dataset and an accuracy of
97.85% on the KDDCup 99 dataset.
In , the researchers gave an account of a multi-objective
algorithm for feature selection labeled MOMI. This approach
is centered on Mutual Information (MI) and considers the features redundancy and relevancy during the feature evaluation
and selection process. The experiments carried out to evaluate
MOMI’s performance were conducted using the WEKA
tool with three separate datasets. Two classiﬁers, namely
Naive Bayes (NB) and support vector machine (SVM) were
used. The results of this research suggested that MOMI
was able to select only the features needed for the best
performance.
Chakraborty and Pal presented a feature selection (FS)
algorithm using a multilayer percetron (MLP) framework
with a controlled redundancy (CoR). This approach is
labelled as FSMLP-CoR. An MLP is a neural network with
an input layer, multiple hidden layers and an output layer 
and it is generally used for approximation, classiﬁcation,
regression, and prediction in many domains – . In this
case, an MLP was used to identify and drop those features
that are not relevant in resolving the problem at hand. The
FSMLP-CoR was tested using 23 datasets and the results
led researchers to conclude that it was effective in selecting
important features.
In , an ant colony optimization (ACO) technique was
applied for feature selection on the KDDCup 99 dataset for
intrusion detection. The KDD Cup 99 dataset has 41 features.
ACO was inspired by how ants use pheromones in a bid to
remember their path. ACO has different variations. In this
research, the authors used the ant colony system (ACS) with
two level pheromones update. The proposed solution was
evaluated using the binary SVM classiﬁer library in WEKA
(LibSVM) . The results revealed that a higher accuracy is
obtained with an optimal feature subset of 14 inputs.
The research in proposed a wrapper based feature
selection algorithm for intrusion detection using the genetic
algorithm (GA) as an heuristic search method and Logistic
Regression (LR) as the evaluating learning algorithm. The
whole approach is labeled as GA-LR. GA originates from
the natural selection process and it is under the category
of evolutionary based algorithms . GA has the following building blocks: an initial population, a ﬁtness function,
a genetic operator (variation, crossover and selection) and a
stopping criterion. The experiments conducted to evaluate the
GA-LR were done using the KDD Cup 99 Dataset and the
UNSW-B15 Dataset. Decision Tree classiﬁers were applied
to candidates feature subsets and the results suggested that
GA-LR is an efﬁcient method.
Wang et al. took a different direction in terms of the
feature engineering approach by using a feature augmentation (FA) algorithm rather than a feature reduction one.
The classiﬁer used in this research was the SVM and the
FA algorithm used was the logarithm marginal density ratio
transformation. The goal was to obtain newly improved features that would ultimately lead to a higher performance in
detection accuracy. The evaluation of the proposed scheme
was conducted using the NSL-KDD dataset and the outcomes
from the empirical experiments suggested the FA coupled
with the SVM yielded a robust and improved overall performance in intrusion detection capacity.
In , an intrusion detection system (IDS) was designed
and modelled based on DL using Recurrent Neural Networks
(RNNs). RNNs are neural networks whereby the hidden
layers act as the information storage units. The benchmark dataset used in this research was the NSL-KDD. The
RNN-IDS was compared to the following commonly used
classiﬁcation methods: J.48, Random Forest and SVM. The
accuracy (AC) was mainly used as the performance indicator during the experiments and the results suggested that
RNN-IDS presented an improved accuracy of intrusion detection compared to traditional machine learning classiﬁcation
methods. These results reinforced the assumption that DL
based intrusion detection systems are superior to classic ML
algorithms. In the binary classiﬁcation scheme, a model with
80 hidden nodes, a learning rate of 0.1 achieved an accuracy of 83.28% whereas in the multiclass classiﬁcation using
5 classes, a model with 80 hidden neurons and learning rate
of 0.5 got an accuracy of 81.29%.
The approach proposed in used a deep learning
approach to intrusion detection for IEEE 802.11 wireless
networks using stacked auto encoders (SAE). A SAE is a
neural network created by stacking together multiple layers
of sparse auto encoder. The experiments undertook in this
research were made using the Aegean Wireless Intrusion
Dataset (AWID) that is comprised of 155 attributes with the
last attribute representing the class that can take the following values: injection, ﬂooding, impersonation and normal.
According to Thing , this was the ﬁrst work that proposed
a deep learning approach applied to IEEE 802.11 networks
for classiﬁcation. The overall accuracy achieved in this work
was 98.6688%.
Ding and Wang investigated the use of DL for intrusion detection technology using the KDDCup 99 Dataset.
The architecture used for the neural network model consisted of 5 hidden layers of 10-20-20-40-64 dense feed forward (fully connected layers). The activation function used
in this research was the ReLU (Rectiﬁed Linear Unit) and
the back-propagation method for training this model was
the Adam optimizer (Ad-op). The Ad-op was used in a bid
to increase the training speed and to prevent overﬁtting.
Although this research yielded some advancements, it equally
VOLUME 7, 2019
S. M. Kasongo, Y. Sun: Deep Learning Method With Filter-Based Feature Engineering
showed no signiﬁcant improvement in detecting rare attacks
types (U2R and R2L) present in the dataset.
In , an ML approach to detect ﬂooding Denial of
Service (DoS) in IEEE 802.11 networks was proposed. The
dataset used in this research was generated by the authors in
a computer laboratory. The setup was made of 40 computers
in which seven were designated as attackers to lunch the
ﬂooding DoS and each of the legitimate node was connected
to any of the available ﬁve Access Points (APs). The obtained
dataset was segmented in the following two portions: 66%
for ML training and 34% for ML testing. Using the WEKA
tool , six classiﬁcations ML learning algorithms were
applied consecutively, namely: SVM, Naive Bayes, Naive
Bayes Net, Ripple-DOwn Rule Learner (RIDOR), Alternating Decision Tree and Adaptive Boosting (AdaBoost). The
empirical results based on the accuracy and the recall numbers suggested that AdaBoost was more efﬁcient than the
other algorithms.
In , a performance comparison of SVM, Extreme
Learning Machine (ELM) and Random Forest (RF) for intrusion detection was investigated using the NSL-KDD as the
benchmark dataset. Each of the ML algorithms used in this
investigation was evaluated using the following performance
metrics: Accuracy, Precision and Recall. The outcome of the
experiments showed that ELM outperformed RF and SVM;
consequently, the authors concluded that ELM is a viable
option when designing and implementing intrusion detection
IV. BACKGROUND ON TRADITIONAL MACHINE
LEARNING CLASSIFIERS
A. SUPPORT VECTOR MACHINE
Support Vector Machines (SVM) is one of the most popular ML techniques applied to Big Data and used in ML
research. SVM is a supervised machine learning method
that is used to classify different categories of data. SVM is
able to solve the complexity of both linear and non-linear
problems. SVM works by generating a hyperplane or several
hyperplanes within a high-dimensional space to separate data
and the ones that optimally split the data per class type are
selected as the best .
B. K-NEAREST NEIGHBOR
K-Nearest Neighbor (KNN) is another ML method used to
classify data. The KNN algorithm bases itself on the standard
Euclidean distance between instances in a space and can
be deﬁned as follow : let x and y instances in space P,
the distance between x and y, d(x, y), is given the following
expression:
where n represents the total number of instances. The KNN
method classiﬁes an instance x0 within a space by calculating the Euclidean distance between x0 and k closet samples
within the training set and x0 takes the label of k most similar
neighbors .
C. NAIVE BAYES
Naive Bayes (NB) classiﬁers are simple classiﬁcation algorithms based on Bayes’ Theorem . Given a dataset, an NB
classiﬁer assumes a ‘‘naive’’ independence between the features. Let X an instance with n features to be classiﬁed represented by the vector X = (x1, . . . , xn). In order to ﬁgure out
the class Ck for X, NB does the following:
p(Ck|X) = p(X|Ck)p(Ck)
And the class for X is assigned using the following
expression:
y = argmax
k∈{1,...,K}
where y is the predicted label.
D. DECISION TREE AND RANDOM FOREST
Decision Tree (DT) algorithm is widely used in data mining
and ML. Given a dataset with labeled instances (training),
DT algorithm generates a predictive model in a shape of a
tree capable of predicting the class of unknown records .
A DT has three main components: a root node, internal nodes
and category nodes. The classiﬁcation processes happens in
a top-down manner and an optimal decision is reached when
the correct category of leaf node is found. A Random Forest
classiﬁer on the other hand applies multiple DTs on a given
dataset for classiﬁcation.
V. PROPOSED METHOD FOR WIRELESS
INTRUSION DETECTION
A. FEED FORWARD DEEP NEURAL NETWORKS
Deep neural networks (DNNs) are widely used in ML and
DL to solve complex problems. The most basic element of
a DNN is an artiﬁcial neuron (AN) which is inspired from
biological neurons within the human brain. An AN computes
and forwards the sum of information received at its input side.
Due the the non-linearity of real life problems and in a bid
to enhance learnability and approximation, each AN applies
an activation function before generating an output . This
activation function can be a Sigmoid, σ =
1+e−t ; a Rectiﬁed
Linear Unit (ReLU): f (y) = max(0, y); or an hyperbolic
tangent shown in expression (4).
tanh(y) = 1 −e−2y
The above-mentioned activation functions have advantages
and drawbacks; moreover, their optimal performance is problem speciﬁc. Traditionally, artiﬁcial neural networks (ANNs)
have an input layer, one to three hidden layers and an output
layer as shown in Fig. 1; whereas DNNs may contain three
to tens or hundreds of hidden layers . There is no general
rule for determining whether an ANN is deep or not. For the
VOLUME 7, 2019
S. M. Kasongo, Y. Sun: Deep Learning Method With Filter-Based Feature Engineering
FIGURE 1. Feed forward neural network architecture.
sake of our research, we will consider a DNN to be a neural
network with two or more hidden layers. In a Feed Forward
DNN, the ﬂow of information goes in one direction only:
from the input layers via the hidden layers to the output layers.
Neurons within the same layer do not communicate. Each AN
in the current layer is fully connected to all neurons in the next
layer as depicted in Fig.1.
FIGURE 2. Proposed FFDNN architecture.
The block diagram in Fig. 2 presents the architecture of
the proposed Feed Forward Deep Neural Network (FFDNN)
IDS. In this architecture, the ﬁrst step consists of the separation of raw data. It is crucial to split the main training
set between two main sets: the reduced training set and the
evaluation set. The evaluation dataset is used to validate
the training process. The test set has a totally different data
distribution from the training and evaluation (validation) sets.
The second step involves a feature transformation process and
a two-way normalization process of the raw data as well as a
feature extraction (selection) procedure based on Information
Gain. It is important to transform and to normalize the data
because most of the features within a dataset come in different
formats that can be numerical or nonnumberical. The last
processes of the architecture are the models training and
testing using the FFDNN and the FEU-FFDNN. Since the
training and the validation data originate from the same data
distribution, it is important to ensure that during the training
process, the selected model doesn’t train on the validation
data because training the model on previously seen data may
cause the ﬁnal model to perform poorly. The next sections
explain in detail the role of each of the components in Fig. 2.
B. DATASET
In the proposed research, the NSL-Knowledge Discovery and
Data mining (NSL-KDD) which is an improved version of
the KDDCup 99 is used to train, evaluate and test the
designed system shown in Fig 2. The NSL-KDD is considered a benchmark dataset in IDS research and it is used
for both wired and wireless systems , , , .
The NSL-KDD comprises one class label categorized in the
following major groups: Normal, Probe, Denial of Service
(DoS), User to Root (U2R) and Remote to User (R2L).
Furthermore, the NSL-KDD is made of 41 features of which
three are nonenumeric and 38 are numeric as depicted
in Table 1.
The NSL-KDD comes with two set of data: the training
set (KDDTrain+ full) and the test sets (KDDTest+ full and
KDDTest-21). In this research, we use the KDDTrain+ and
the KDDTest+. KDDTrain+ is divided into two partitions:
the KDDTrain+75, which is 75 % of the KDDTrain+ and
it will be used for training, the KDDTEvaluation that is
25 % the KDDTrain+ and it will be used for evaluation after
the training process. Table 2 provides a breakdown of the
components in each dataset.
C. FEATURE ENGINEERING
In a dataset, features may take different forms such as numeric
and nonnumeric. DNN models can only process numeric
TABLE 1. NSL-KDD Features List.
TABLE 2. Datasets breakdown.
VOLUME 7, 2019
S. M. Kasongo, Y. Sun: Deep Learning Method With Filter-Based Feature Engineering
values; therefore it is crucial to transform all nonnumeric or
symbolic features into a numerical counterpart. Within the
NSL-KDD, f 2 ‘protocol_type’, f 3 ‘service’ and f 4 ‘ﬂag’
are symbolic features. We apply a mapping process in Scikit
Learn whereby all symbols are mapped to a unique
numerical value. Moreover, it is important to transform and
normalize features as they may have an uneven distribution.
For instance, taking a look at f 5 which represents ‘src_bytes’
in Table 1: f 5 has values such as 12983 and values like
20; consequently, normalization is required to keep values
within the same range for optimal processing. In this research,
we apply a two-step normalization procedure. We ﬁrst apply a
logarithmic normalization shown in expression (5) to all the
features so that we keep them within acceptable range and
secondly, we linearly cap the values to be in this range 
using equation in (6).
xnormalized = log(xi + 1)
xnormalized = (b −a)
xi −min(xi)
max(xi) −min(xi)
where b = 5 and a = 0.
After the two step normalization process, the Feature
Extraction Unit (FEU) has the role to rank the features using
an algorithm based on Information Gain (IG) which originates from Information Theory . We will compute the IG
of each feature with respect to the class. Unlike the standard
correlation algorithms such as Pearson Linear Correlation
Coefﬁcient that is only capable of establishing linear
correlations between features, IG is capable of discovering
nonlinear connection as well. In information theory, the measure of uncertainty of a variable X is called entropy, H(X),
and it is calculated as follow:
P(x)log2(x)
And the conditional entropy of two random variables X and
Y is determined using the following expression:
H(X|Y) = −
P(x|y)log2(P(x|y))
where P is the probability. IG is derived from the expressions
in (7) and (8) as follow:
IG(X|Y) = H(X) −H(X|Y)
Therefore, a given feature Y possesses a stronger correlation to feature X than feature V if IG(X|Y) > IG(V|Y).
D. ALGORITHMS FOR FEATURE ENGINEERING
Given a feature vector F(f1, . . . , fn) with 1 < n < T, where
T is the total number of features and C the class label in
the dataset, the Transform Features module in Fig 2. applies
Algorithm 1 as follow:
After the execution of Algorithm 1, we obtained a transformed feature vector, Ftransformed(f t
1 , . . . , f t
n ), that is fed into
Algorithm 2 to generate a vector, Franked, with features that
are ranked by IG with respect to C.
Algorithm 1 Normalization Algorithm
Input: F(f1, . . . , fn), 1 < n < T
Output: Ftransformed(f t
1 , . . . , f t
for i from 1 to n do
if (fi a symbolic feature) then
apply sckikit learn mapping
Step 1 normalize using log(fi + 1)
Step 2 normalize using (b −a)
fi−min(fi)
max(fi)−min(fi)
Step 1 normalize using log(fi + 1)
Step 2 normalize using (b −a)
fi−min(fi)
max(fi)−min(fi)
Algorithm 2 Features IG Ranking Algorithm
Input: Ftransformed(f t
1 , . . . , f t
Output: Franked
for i from 1 to n do
compute IG: IGi(fi|C) = H(fi) −H(fi|C)
if (IGi >= IGtreshold) then
load IGi into Franked
E. ALGORITHM FOR TRAINING FFDNNs
Training feed forward neural networks consists of the following three major steps:
1) Forward propagation.
2) Back-propagation of the computed error.
3) Updating the weights and biases.
The algorithm used to train the FFDNNs is explained
in Algorithm 3. Given a set of m training sample
{(x1, y1), . . . , (xm, ym)} and η the learning rate. We train
FFDNNs presented in this research using the back propagation algorithm backed by a stochastic gradient descent (SDG)
for the weights and biases update. Additionally, the cost
function used to calculate the difference between the target
and the obtained output is shown in this expression:
C(W, b; x, y) = 1
2∥y−output∥2
VI. EXPERIMENTAL SETTING, RESULTS AND
DISCUSSIONS
For the purpose of this research, we have used a Python based
library, Scikit-Learn which is widely used in machine
learning and deep learning research. Our simulations were
executed on an ASUS laptop with the following speciﬁcations: Intel Core i-3-3217U CPU @1.80GHz and 4.00G of
RAM. The metrics used to evaluate the performance of the
FFDNNs presented in this research are the accuracy in (11),
the precision in (12) and the recall in (13). These indicators
are derived from the confusion matrix shown in Table 3 and
they are deﬁned as follow:
• True positive (TP): Intrusions that are successfully
detected by the proposed IDS.
VOLUME 7, 2019
S. M. Kasongo, Y. Sun: Deep Learning Method With Filter-Based Feature Engineering
Algorithm 3 Forward and Back-Propagation Algorithm
Input: W, b
Output: updated W, b
1: Forward propagate xi through layers l = L2, L3, . . . Lnl,
(nl is the subscript of the last layer) using zl+1
W lal + bl and al+1 = f (zl+1) with f , a rectiﬁed linear
unit (ReLU) of this form f (z) = max(0, z)
2: Compute the error term ξ for each output unit i as follow:
2∥y−output∥2 = −(yi−anl
i ).f ′(znl
3: For each hidden units in l = nl −1, nl −2, . . . , 2,
compute the following for each node i in l:
4: Calculate the required partial derivatives with respect to
weights and biases for each training example as follow:
C(W, b; x, y) = al
C(W, b; x, y) = ξl+1
5: Update the weight and biases as follow:
TABLE 3. Confusion matrix.
• False positive (FP): Normal / non-intrusive behaviour
that is wrongly classiﬁed as intrusive by the IDS.
• True Negative (TN): Normal / non-intrusive behaviour
that is successfully labelled as normal/non-intrusive by
• False Negative (FN): Intrusions that are missed by the
IDS, and classiﬁed as normal / non-intrusive.
Accuracy =
TP + TN + FP + FN
Precision =
The experiments were carried out in multiple phases using
the NSL-KDD dataset explained in section V. The NSL-KDD
has the following classes: Normal, DoS, Probe, U2R and
R2L. For binary classiﬁcation, we map the DoS, Probe, U2R
and R2L classes to one class called ‘‘attack’’ and for multiclass classiﬁcation, we use the dataset with its original ﬁve
major classes. In this research, the following rule applies: a
classiﬁer performs better than another one when it yields a
higher accuracy on previously unseen data that can be found
in the KDDTest+ set.
A. PHASE 1: BINARY CLASSIFICATION WITH 41 FEATURES
This phase uses all 41 features for binary classiﬁcation.
We only apply Algorithm 1 to transform the inputs. In order
to select the best FFDNN, we ran models with 41 units at
the input layer, two nodes at the output layer and the following hidden nodes numbers: 30, 40, 60, 80 and 150. These
numbers were selected by trial and error method. Moreover,
we were also varying the number hidden layers as well as the
learning rate. The details are presented in Table 4. For better
performance analysis and for the purpose of comparison,
we also perform classiﬁcation using the following classiﬁer:
SVM, KNN, RF, DT and NB. The obtained results suggested
that for binary classiﬁcation, a model with a learning rate of
0.05, 30 neurons spread over 3 hidden layers got an accuracy
of 99.69% on the KDDEvaluation set and 86.76% on the
KDDTest+. Fig. 3 shows a comparison of this model with
other classiﬁcation algorithms. The Random Forest classiﬁer
with an accuracy of 85.18% for the KDDTest+ came into second position after the FFDNN model and the SVM classiﬁer
produced an accuracy of 84.41% on the same test set.
TABLE 4. Accuracy during training of FFDNN - binary classification.
B. PHASE 2: MULTICLASS CLASSIFICATION
WITH 41 FEATURES
We conducted multiclass classiﬁcation in this phase by using
ﬁve classes of the NSL-KDD dataset with all 41 features.
As described in Table 5, the FFDNN model with 60 nodes
spread through three hidden layers with a learning rate
of 0.05 got an accuracy of 86.62% which is a much better
performance compared to other FFDNN models settings.
In order to put this experiment in perspective, we also
VOLUME 7, 2019
S. M. Kasongo, Y. Sun: Deep Learning Method With Filter-Based Feature Engineering
FIGURE 3. Binary classification accuracy comparison.
TABLE 5. Accuracy during training of FFDNN - multiclass classification.
conducted a multiclass classiﬁcation using SVM, KNN, RF,
DT and NB classiﬁers. As depicted in Fig. 4, the comparison
shows that FFDNN outperformed all other classiﬁer on the
test data; however, the RF classiﬁer performed relatively well
with an accuracy of 86.35% on test data and the SVM model
got an accuracy of 83.83%.
C. PHASE 3: FEATURE EXTRACTION
We applied Algorithm 1 and Algorithm 2 in the FEU to the
KDDTrain+ Full dataset in order to extract a reduced vector
of features. The goal in this step was to select the features
with enough information gain (IG) with respect to the class.
The ﬁltering process generated the features in Table 6 which
represents 21 features.
In the next two phases, we repeat the experiments in
Phase 1 and Phase 2; however, in these instances, a reduced
feature vector Franked of 21 ranked features is used.
FIGURE 4. Multiclass classification accuracy comparison.
TABLE 6. Ranked Features.
D. PHASE 4: BINARY CLASSIFICATION WITH A REDUCED
FEATURES VECTOR
Table 7 shows multiple FEU-FFDNN models that all have
21 inputs and two outputs. The best performing model with
30 hidden nodes, a hidden layer size of 3 and a learning
rate of 0.05 got an accuracy of 99.37% over the evaluation
set and 87.74% on the test data. This is an improvement
over the best model using 41 inputs in Phase 1. Additionally,
Fig. 5 shows an accuracy comparison between SVM, KNN,
RF, DT, NB and FEU-FFDNN classiﬁer for better contrasting. We noticed that the FEU-FFDNN outperformed other
VOLUME 7, 2019
S. M. Kasongo, Y. Sun: Deep Learning Method With Filter-Based Feature Engineering
TABLE 7. Accuracy during training of FEU-FFDNN - Binary Classification.
FIGURE 5. Binary classification accuracy comparison with reduced
features set.
E. PHASE 5: MULTICLASS CLASSIFICATION WITH A
REDUCED FEATURES VECTOR
In this stage of the experiments, we ran several FEU-FFDNN
models and we used all classes groups present in the
NSL-KDD dataset. The model that performed the best has
150 neurons, a learning rate of 0.05 and it yielded an accuracy of 99.54% on the validation data and 86.19% on the
test data. In comparison to the results in Phase 2 of this
research, this model needed more neurons as the feature
vector dimension was reduced by the ﬁltering process. Additionally, Fig. 6 shows a comparison of this model to existing
ML models and the results showed that the FEU-FFDNN
based model outperformed all other existing models.
Moreover, for the best performing model (150 neurons,
three hidden layers, learning rate = 0.02), we plotted the
precision and recall curve over the test dataset as seen
TABLE 8. Accuracy during training of FEU-FFDNN - multiclass
classification.
FIGURE 6. Multiclass classification accuracy comparison with reduced
features set.
in Fig. 7 where class 0 = ‘normal’, class 1 = ‘R2L’, class 2 =
‘U2R’, class 3 = ‘Probe’ and class 4 = ‘DoS’. This curve
gave us more details on how our model performed for different classes.
F. DISCUSSIONS
Our research explores in detail the application of FFDNNs
to wireless intrusion detection using the NSL-KDD dataset.
Experiments were carried out for both binary and multiclass
classiﬁcation. In the ﬁrst two phases of the experimental
process, the training and testing of the models were done
using the entire feature vector. The results suggested that for
both phases, FFDNNs outperformed other ML models. For
binary classiﬁcation, FFDDNNs required less neurons than
for multiclass classiﬁcation. In Phase 1, only 30 nodes spread
VOLUME 7, 2019
S. M. Kasongo, Y. Sun: Deep Learning Method With Filter-Based Feature Engineering
FIGURE 7. Precision-Recall curve.
over three hidden layers were needed for the generalization
of our model; however, 60 neurons in three hidden layers
were necessary for better approximation in the multiclass
problem. Although the depth (number of hidden layers) was
not affected, we can derive that the more attacks classes we
have, the more neurons are needed to solve the complexity of
the intrusion detection classiﬁcation problem.
In Phase 3, a feature transformation and extraction procedure was executed based on IG and a feature vector with
21 ranked features was generated.
In Phase 4 and Phase 5, the experiment carried out in
Phase 1 and Phase 2 were repeated respectively using a
feature vector with a reduced dimension obtained from Phase
3. The results achieved in Phase 4 showed that with the
same number of neurons as well as the same learning rate,
the accuracy of the FEU-FFDNN model increased from
86.76% to 87.74% on the KDDTest+. For multiclass classiﬁcation using the FEU in Phase 5, we obtained an overall
accuracy of 86.19% with a depth of three hidden layers and
150 neurons. Here as well, the FEU-FDNN outperformed
other methods as revealed in Fig.6. Moreover, we studied the
intrinsic details of the classiﬁcation in Phase 5 by plotting
a Precision-Recall curve as depicted in Fig. 7. Based on the
curve area values, Class 1 and class 2 were the classes with the
most misclassiﬁcations instances because they do not appear
often in both the training and test datasets.
Additionally, in comparison to other deep learning based
methodologies using 41 features for multiclass classiﬁcation such as stacked non-symmetric auto-encoders (S-NDAE)
used in that got 85.42% and recurrent neural networks (RNN) used in that achieved an overall accuracy
of 81.29%; the FFDNN in our research produced an accuracy
of 86.62% on the test set, which is superior to the S-NDAE
and RNN models.
VII. CONCLUSION
This paper presented the design, implementation and testing
of a DL based intrusion detection system using FFDNNs.
A literature review of ML and DL methods was conducted
and it was found that the most efﬁcient approach to intrusion
detection has yet to be found. The FFDNN models used in
this research were coupled to a FEU using IG in a bid to
reduce the input dimension while increasing the accuracy of
the classiﬁer. The dataset used in this work is the NSL-KDD.
For the binary and the multiclass classiﬁcations problems,
the FFDNNs models both with a full and a FEU-reduced
feature space achieved a performance that is superior to SVM,
RF, NB, DT and KNN. In future work, we aim at ﬁnding
a strategy to increase the detection rates of R2L and U2R
attacks in the NSL-KDD dataset. Moreover, we will apply
the FEU and the FFDDNs to the AWID dataset in order to
investigate further the superiority of DL based methods for
IDS over other ML approaches.
VIII. ACKNOWLEDGMENT
This research is partially supported by the South African
National Research Foundation (Nos: 112108, 112142);
SouthAfrican National Research Foundation Incentive Grant
(No.95687); Eskom Tertiary Education Support Programme
Grant; Research grant from URC of University of Johannes-