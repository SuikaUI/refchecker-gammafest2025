Hybrid Heterogeneous Transfer Learning through Deep Learning
Joey Tianyi Zhou†, Sinno Jialin Pan‡, Ivor W. Tsang§, and Yan Yan]
†Nanyang Technological University, Singapore
‡Institute for Infocomm Research, Singapore
§University of Technology, Sydney, Australia
]The University of Queensland, Australia
† , ‡ , § , ] 
Most previous heterogeneous transfer learning methods
learn a cross-domain feature mapping between heterogeneous feature spaces based on a few cross-domain
instance-correspondences, and these corresponding instances are assumed to be representative in the source
and target domains respectively. However, in many realworld scenarios, this assumption may not hold. As a
result, the constructed feature mapping may not be
precise due to the bias issue of the correspondences
in the target or (and) source domain(s). In this case,
a classiﬁer trained on the labeled transformed-sourcedomain data may not be useful for the target domain. In
this paper, we present a new transfer learning framework called Hybrid Heterogeneous Transfer Learning (HHTL), which allows the corresponding instances
across domains to be biased in either the source or target domain. Speciﬁcally, we propose a deep learning
approach to learn a feature mapping between crossdomain heterogeneous features as well as a better feature representation for mapped data to reduce the bias
issue caused by the cross-domain correspondences. Extensive experiments on several multilingual sentiment
classiﬁcation tasks verify the effectiveness of our proposed approach compared with some baseline methods.
Introduction
Transfer learning or domain adaptation is a new machine
learning paradigm, which aims to transfer knowledge extracted from an auxiliary domain, i.e., a source domain,
where sufﬁcient labeled data are available, to solve learning
problems in a new domain, i.e., a target domain, with little or
no additional human supervision . Recently, more and more attention has been shifted from transferring knowledge across homogeneous domains to transferring knowledge across heterogeneous domains where the
source and target domains may have heterogeneous types of
features .
Different from homogeneous transfer learning, which assumes that the source and target domain data are represented in the same feature space of the same dimensionality , and thus the domain difference is only caused
Copyright c⃝2014, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
by bias in feature or data distributions, heterogeneous transfer learning allows the source and target domains to be represented in different feature spaces. There are many realworld applications where heterogeneous transfer learning is
crucial. For instance, many Natural Language Processing
(NLP) tasks, such as named entity recognition, coreference
resolution, etc., highly rely on a lot of annotated corpora
and linguistic or semantic knowledge bases to build precise
classiﬁers. For English, annotated corpora and knowledge
bases are widely available, while for other languages, such
as Thai, Vietnamese, etc., there are few resources. In this
case, heterogeneous transfer learning is desirable to transfer
knowledge extracted from rich English resources to solve
NLP tasks in other languages whose resources are poor.
Most existing approaches to heterogeneous transfer learning aim to learn a feature mapping across heterogeneous feature spaces based on some cross-domain correspondences
constructed either by labels or a translator . With the learned
feature mapping, instances can be mapped from the target
domain to the source domain or the other way round. In this
way, source domain labeled data can be used to learn an accurate classiﬁer for the target domain. There is a common
assumption behind these methods that the selection of the
instance-correspondences to learn the feature mapping is unbiased. In other words, the cross-domain corresponding instances are assumed to be representative in the source and
target domains respectively. However, in many real-world
scenarios, this assumption may not hold, which means that
the selected corresponding instances may be biased and able
to represent neither the source nor target domain data.
Taking cross-language document classiﬁcation as a motivating example, which is illustrated in Figure 1(a), the
objective is to learn a text classiﬁer on documents in one
language (e.g., German) only with a set of annotated English documents. To apply heterogeneous transfer learning methods to solve this task, one can simply construct
German-English document-correspondences by translating
some German documents into English by Google translator.
However, the wordbook of the translated English documents
may be quite different from that of the original English documents. For instance, the German word “betonen” is translated into the English word “emphasize” by Google translator. However, in an original English document, its corre-
Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence
target domain
source domain
gl¨ucklich
(a) Case I: target corresponding instances are not biased
wohlklingend
euphonious
target domain
source domain
missbrauchen
(b) Case II: target corresponding instances are biased
Figure 1: Hybrid Heterogeneous Transfer Learning.
sponding word is “highlight” or “stress”. This is referred to
as the “bias” issue in word-distribution between the translated documents and the original ones in the target domain.
In this case, a feature mapping learned on such correspondences may not be effective.
As a second example as shown in Figure 1(b), we consider
a multilingual sentiment classiﬁcation task, where our task
is to automatically classify overall sentiment polarities of
songs reviews in German given labeled books reviews in English, and some unlabeled pairs of songs reviews in German
and their English translations. Though one can make use of
the German-English review-correspondences to learn a feature mapping from English to German, the labeled books
reviews in English after transformation may not be useful
to learn an accurate sentiment classiﬁer for songs reviews in
German. The reason is that, opinion and topic words used
on different types of products can be very different . In the context of this example, the “bias” between the
transformed source domain data and the target domain data
is caused by the difference of product domains.
Motivated by the above observations, we propose a new
heterogeneous transfer learning framework called “hybrid
heterogeneous transfer learning” (HHTL) to ensure knowledge transfer across heterogeneous domains to be effective
even though the cross-domain correspondences are biased.
Speciﬁcally, the HHTL framework consists of two main
components: 1) learning a heterogeneous feature map between the source domain labeled data and the target unlabeled domain, and 2) discovering a latent representation
to reduce the distribution bias between the transformed target domain unlabeled data and source domain labeled data.
Thereafter, standard classiﬁcation methods can be applied
on the source domain labeled data with the latent representation to build a target classiﬁer effectively. We present a deep
learning method to simultaneously learn a feature transformation from the target domain to the source domain, and two
mappings from lower-level features to higher-level features
in the source and target domains respectively.
Note that the proposed HHTL framework is different from
multi-view learning , where fully
correspondences between two views of data are required,
and corresponding labels of the correspondences are assumed to be available in general. In HHTL, no label information of the cross-domain instance-correspondences is required. Moreover, in HHTL, labeled data is only assumed to
be available in the source domain, while the goal is to learn
a classiﬁer to be used in the target domain.
Related Work
Homogeneous transfer learning aims to improve models’
generalization ability across different domains of the same
feature space. Learning a good feature representation for different domain data is crucial to homogeneous transfer learning. For instance, Pan et al. proposed a dimensionality reduction method, namely transfer component Analysis (TCA), to learn a low-dimensional space where the distance in distributions between a source domain and a target domain can be reduced. Raina et al. proposed
a self-taught learning framework based on sparse coding
to learn high-level features for transfer learning. Recently,
deep learning techniques have been proposed for transfer
learning. Glorot, Bordes, and Bengio applied stack
denoised autoencoder (SDA) to learn hidden feature representations for cross-domain sentiment classiﬁcation. In a
follow-up work, Chen et al. proposed a variation of SDA for
transfer learning, namely marginalized SDA (mSDA), which
has been shown to be more effective and efﬁcient.
Heterogeneous transfer learning aims to transfer knowledge across different feature spaces. Most methods for heterogeneous transfer learning aim to learn a common feature
representation based on some correspondences between domains such that both source and target domain data can be
represented by homogeneous features. Speciﬁcally, one can
learn two different feature mappings to transform the source
domain and target domain data to a latent feature space respectively .
Alternatively, one can learn an asymmetric transformation
to map data from one domain to another domain . Moreover,
there are many methods proposed to learn the mappings by incorporating
instance correspondences between domains. Deep learning
techniques have also been proposed to heterogeneous transfer learning , where knowledge is transferred from text to image based on a lot of text-image correspondences through deep learning. Our proposed framework
can be considered as a more general case of heterogeneous
transfer learning, where the bias of the correspondences between the source and target domains is considered.
A Deep Learning Approach
Problem Formulation
Given a set of target domain unlabeled data DT ={xTi}n1
a set of source domain labeled data DS = {(xSi, ySi)}n2
and an additional set of pairs of source and target domain unlabeled data, namely parallel data, DC = {(x(c)
where xSi or x(c)
Si is in RdS⇥1, and xTi or x(c)
Ti is in RdT ⇥1.
Our objective is to learn weight matrices WS and WT to
project the source domain labeled data and the target domain unlabeled data to hidden representations, WSXS and
WT XT , respectively, and a feature mapping G to map data
from the target domain feature space to the source domain
feature space,1 such that the difference between the original source domain data and the transferred target domain
data is small. With the learned weight matrices WS, WT
and the feature mapping G, one can train a classiﬁer f from
{(WSXSi, ySi)}n1
i=1, and make prediction on a target domain unlabeled data x⇤
T by applying f(G(WT X⇤
As discussed in the two examples of cross language/lingual text mining, on one hand, the selection of corresponding instances in the source domain is always biased
in general. This is because the corresponding instances in
the source domain are not randomly selected based on the
source domain data distribution but based on the selection
of the corresponding instances in the target domain. On the
other hand, the selection of the corresponding instances in
the target domain can be either: 1) unbiased, e.g., in the example of cross language document classiﬁcation, one can
apply Google translator on a set of randomly selected documents in German to construct correspondences; or 2) bias,
e.g., in the example of cross lingual sentiment classiﬁcation,
a set of reviews in German to be used to construct correspondences are about books while our target is to learn a sentiment classiﬁer on music reviews. In the following sections,
we will address these two problems through the proposed
hybrid heterogeneous transfer learning framework.
High-Level Homogeneous Feature Learning
Inspired by the motivations and promising results of
self-taught learning and deep learning in domain adaptation , to address the bias issue in either the
source domain only or both the source and target domains
caused by instance shift or feature mismatch, we propose to
apply Stacked Denoised Autoencoder (SDA) on the source domain data, i.e., the source domain labeled data and the source-domain corresponding unlabeled
data2, and the target domain data, i.e., the target domain
unlabeled data and the target-domain corresponding unlabeled data, to learn high-level representations. Speciﬁcally,
SDA ﬁrstly randomly sets some values of source domain
1Alternatively, one can learn a feature mapping G> to map data
from the source domain to the target domain.
2In practice, if there is an additional set of unlabeled data in the
source domain, one can use it as well for high-level feature learning. However, in this paper, for simplicity in description, we do not
assume that additional source domain unlabeled data is available.
features to be 0, which is referred to as a “corruption” of
source domain data. In total, one can obtain m different corruptions. After that SDA tries to learn high-level features
by reconstructing these m corruptions. For example, German word “betonen” is translated to“emphasize” by using
Google Translator. However in human writings, one may use
the words “proud” and “like” instead. SDA aims to reconstruct the machine translated word “emphasize” by using the
words “proud” and “like”. Therefore, the learned high-level
features have capability to reduce data or feature bias.
In particular, we adopt a recently proposed method named
Marginalized Stacked Denoised Autoencoder (mSDA) for
high-level feature learning on homogeneous features. mSDA
is an extension of SDA, which simpliﬁes the reconstruction
from two-level encoder and decoder to a single mapping.
The reasons why we use mSDA are two folds: 1) the effectiveness of mSDA has been shown in homogeneous domain adaptation problems , and 2) compared to the standard SDA method, mSDA has proven to be
much more efﬁcient. For simplicity in presentation, following the notations used in , we absorb a
constant feature into the feature vector as xS = [x>
or xT = [x>
T 1]>, and incorporate a bias term b within
the weight matrix as W = [W b]. We further denote
>]> the union source domain data, and
>]> the union target domain data.
Firstly, for the source domain data, we apply mSDA on
XS to learn a weight matrix WS 2 R(dS+1)⇥(dS+1) by
minimizing the squared reconstruction loss as follows,
"""XS −WSX
denotes the i-th corrupted version of XS. The
solution to (1) depends on how the original features are corrupted which can be explicitly expressed as follows,
with Q = eXS eX>
S and P = bXS eX>
where bXS = [XS XS · · · XS] denotes the m-times repeated version of XS, and eXS is the corrupted version of
bXS. In general, to alleviate bias in estimation, a large number of m over the training data with random corruptions are
required, which is computationally expensive. To address
this issue, mSDA introduces a corruption probability p to
model inﬁnite corruptions, i.e., m −! 1. Deﬁne a feature
vector q = [1−p, · · · , 1−p, 1]> 2 RdS+1, where qi represents the probability of a feature indexed by i “surviving”
after the corruption. Thus, we can obtain the expectation of
(1), and its solution can be written analytically as
WS = E[P]E[Q]−1,
where E[P]ij = Sijqj, S = XSX
if i 6= j,
otherwise.
After WS is learned, the nonlinearity of features is injected
through the nonlinear encoder function h(·) that is learned
together with the reconstruction weights WS, mSDA applies a nonlinear squashing-function, e.g., the hyperbolic
tangent function tanh(·), on the outputs of mSDA, HS =
tanh(WSXS), to generate nonlinear features.
Feature Learning for Source and Target Domains
The above process can be recursively done by replacing XS
with HS to obtain a series of weight matrices {Wk
each layer of feature learning. Similarly, for the union target
domain data XT , we can recursively apply mSDA to learn a
series of reconstruction weights {Wk
T }’s and generate highlevel nonlinear features for each layer of feature learning,
similarly. Note that when there is no data or feature bias in
the target domain, one can simply set WT to be the identity
matrix of the dimensionality dT + 1, and replace tanh(·) by
the identical function, respectively.
Heterogeneous Feature Mapping
So far, in a speciﬁc layer k of feature learning, we have
learned a pair of reconstruction weights WS,k and WT,k,
and higher-level feature representations HS,k and HT,k for
the union source and target domain data respectively. By
denoting H
T,k the higher-level feature representations of the cross-domain corresponding instances in the
source and target domains respectively, we now introduce
how to learn a feature mapping across heterogeneous features H
Speciﬁcally, in layer k, we aim to learn a feature transformation Gk 2 R(dS+1)⇥(dT +1), where a bias term is incorporated within the transformation, by minimizing the following objective,
kHS,k −GHT,kk2
F + λkGkk2
where λ > 0 is a parameter of the regularization term on
Gk, which controls the tradeoff between the alignment of
heterogeneous features and the complexity of Gk. It can be
shown that the optimization problem (5) has a closed form
solution which can be written as follows,
Gk = (HS,kH>
T,k)(HT,kH>
T,k + λI)−1,
where I is the identity matrix of the dimensionality dT + 1.
Prediction by Stacking Layers
By deﬁning the total number of layers K of deep learning,
one can recursively apply mSDAs and the heterogeneous
feature mapping learning algorithm on the source and target domain data in each layer to generate different levels
of features and feature transformations between heterogeneous features. After learning high-level features and feature
mappings, for each source domain labeled instance xSi, by
denoting hSi,k its corresponding higher-level feature representation in the k-th layer, we can deﬁne a new feature vector zSi by augmenting its original features and high-level
features of all layers as zSi = [h>
Si,1 · · · h>
Si,K]>, where
hSi,1 = xSi. We then apply a standard classiﬁcation algorithm on {zSi, ySi}’s to train a target classiﬁer f. For making a prediction on a target domain instance x⇤
T , we ﬁrst
generate its higher-level feature representations {h⇤
T,1 = xT , of each layer by using the weight matrices {WT,k}K
k=1, where WT,1 = I, and do the feature augmentation, zT = [(G1h⇤
T,1)> · · · (GKh⇤
T,K)>]>. Finally,
we apply the learned classiﬁer f on zT to make prediction
f(zT ). The reason that we augment different layers of features for both training and testing is because we aim to incorporate additional high-level features to alleviate the bias for
both two domains without losing original feature information. The overall algorithm is summarized in Algorithm 1.
Algorithm 1 Hybrid Heterogeneous Transfer Learning.
Input: target domain unlabeled data DT = {xTi}n1
source domain labeled data DS = {(xSi, ySi)}n2
i=1, crossdomain parallel data, DC = {(x(c)
i=1, a feature
corruption probability p in mSDA, a trade-off parameter
λ, and the number of layers K.
Initializations: XS = [XS X(c)
S ], XT = [XT X(c)
HS,1 = XS, HT,1 = XT , and learn G1 by solving
S,1 −G1H(c)
F + λkG1k2
for k = 2, ..., K do
1: Apply mSDA on HS,k−1 and HT,k−1:
{WS,k, HS,k} = mSDA(HS,k−1, p),
{WT,k, HT,k} = mSDA(HT,k−1, p).
Note: if there is no data or feature bias in the target
domain, simply set WT,k = I, and HT,k = XT .
2: Learn heterogeneous feature mapping Gk:
S,k −GkH(c)
F + λkGkk2
Do feature augmentation on source domain labeled data
S,1 · · · H>
and train a classiﬁer f with {ZS, YS}.
Output: f, {Gk}K
k=1, {WS,k}K
k=2, and {WT,k}K
Experiments
In experiments, we verify the proposed framework on several cross-language classiﬁcation tasks in terms of classiﬁcation accuracy, impact of layers and parameter sensitivity.
Experiment Setting
Dataset The cross-language sentiment dataset comprises of Amazon product reviews of
three product categories: books, DVDs and music. These reviews are written in four languages: English (EN), German
(GE), French (FR), and Japanese (JP). For each language,
the reviews are split into a train ﬁle and a test ﬁle, including
2,000 reviews per categories. We use the English reviews in
the train ﬁle as the source domain labeled data, non-English
(each of the other 3 languages) reviews in a train ﬁle as target
domain unlabeled data. Moreover, we apply Google translator on the non-English reviews in a test ﬁle to construct
cross-domain (English v.s. non-English) unlabeled parallel
data. The performance of all methods are evaluated on the
target domain unlabeled data.
Baselines In our heterogeneous transfer learning setting, no
labeled data is provided for the target domain. Most HTL
methods which require target domain labeled data cannot be
used as baselines. Therefore, we only compare the proposed
HHTL framework with the following baselines:
• SVM-SC: We ﬁrst train a classiﬁer on the source domain
labeled data and then predict on the source domain parallel data. By using the correspondence, the predicted labels
for source parallel data can be transferred into target parallel data. Next, we train a model on the target parallel
data with predicted labels to make predictions on the target domain test data. We name this baseline as the SVM-
Source-Correspondence transfer (SVM-SC).
• CL-KCCA: We apply Cross-Lingual Kernel Canonical
Component Analysis (CL-KCCA) on the unlabeled paralle data
to learn two projections for the source and target languages, and then train a monolingual classiﬁer with the
projected source domain labeled data.
• HeMap: We apply heterogeneous Spectral Mapping
(HeMap) to learn mappings to project
two domain data onto a common feature subspace. However, HeMap does not take the instance correspondence
information into consideration.
• mSDA-CCA:
Multimodal
Learning to learn a shared feature representation for “multimodal” domains. In experiments, for
the fair comparison, instead of using RBM for high-level
feature learning, we adopt mSDA and conduct CCA on
the correspondences between domains in same layers.
For all experiments, we employ the linear support vector machine (SVM) with default parameter settings. We use the cross-validation to adjust
the model parameters. Speciﬁcally, we choose λ from
{0.01, 0.1, 1, 10, 100} for HHTL, choose corruption probability p from {0.5, 0.6, 0.7, 0.8, 0.9} for mSDA from, and
ﬁx the number of layers used in mSDA to be 3. We tune the
parameter for CL-KCCA (see (5) in ), mSDA-CCA, and the parameter β for HeMap (See (1) in ) from
{0.01, 0.1, 1, 10, 100}. In this paper, we employ the deep
learning network to learn better high-level feature representations. Due to the cost of memory and computation, we only
study 3 layers. And due to the limit of space, we only report the results of 1 and 3 layers, denoted by HHTL(1) and
HHTL(3), respectively. For the mSDA-CCA, we only report
the results with a 3-layer structure.
Performance Comparison
We evaluate the performance of proposed methods under
two learning settings: 1) learning with unbiased target correspondence instances, and 2) learning with biased target correspondence instances.
Table 1: Learning with unbiased target correspondence instances: comparison results in terms of testing accuracy (%).
Learning with unbiased target correspondence instances
In this setting, we consider a data bias in the source domain.
During the training process, all the original English reviews
that consist of 3 categories are used as the source domain
labeled data, and non-English reviews are considered as target domain data. We randomly choose 2,000 non-English
reviews of all the categories, and translate them to English
to form the parallel data. The remaining non-English reviews
form the unlabeled target data. The averaged results in terms
of accuracy on the reviews of each non-English language
over 10 repetitions are reported in Table 1.
From Table 1, we can observe that the proposed HHTL
method with 3 layers outperforms all the other baselines signiﬁcantly. The reason is that HHTL beneﬁts a lot from learning the high-level features which can alleviate the data bias
between the translated and original reviews in the source
domain. Besides, the performance of CL-KCCA and SVM-
SC is much better than HeMap. The inferior performance
of HeMap is caused by the fact that HeMap discards the
valuable corresponding information in training. Moreover,
mSDA-CCA performs slightly better than CL-KCCA, and
much better than all the other baselines because of the powerful representations learned by the deep structure. However,
it still performs worse than HHTL because the representations learned by mSDA-CCA are based on the biased correspondences. From the results, we can also see that the performance of all the methods in the Japanese domain is much
worse than that in the other two language domains. It is due
to the fact that Japanese is more different from English compared to German and French. French and German all come
from the similar family of languages in the West. Thus they
share many similar words with English, while Japanese belongs to the family of Eastern languages. Therefore, it is
more difﬁcult to transfer knowledge from the English domain to the Japanese domain.
Learning with biased target correspondence instances
In this setting, we focus on cross-language cross-category
learning between English and the other 3 languages. For
the comprehensive comparisons, we construct 18 crosslanguage cross-category sentiment classiﬁcation tasks as
follows: EN-B-FR-D, EN-B-FR-M, EN-B-GE-D, EN-B-
GE-M, EN-B-JP-D, EN-B-JP-M, EN-D-FR-B, EN-D-FR-
M, EN-D-GE-B, EN-D-GE-M, EN-D-JP-B, EN-D-JP-M,
EN-M-FR-B, EN-M-FR-D, EN-M-GE-B, EN-M-GE-D,
EN-B-JP-B, EN-B-JP-D. For example, the task EN-B-FR-
D uses all the Books reviews in French in the test ﬁle and its
English translations as the parallel data, the DVD reviews in
French as the target language test data, and original English
Books reviews as the source domain labeled data.
The results are summarized in the Table 2. This setting
is more challenging than the previous one due to larger
data bias after feature transformation. Therefore, the performance of all target languages is dropped compared to that
7UDGHRII3DUDPHWHUλ
(a) Accuracy v.s. λ
&RUUXSWLRQ3UREDELOLW\S
(b) Accuracy v.s. p
7KHFRUUHVSRQGHQFHVL]HQF
(c) Accuracy v.s. nc on GE
Figure 2: Parameter Analysis.
Table 2: Learning with biased target correspondence instances: comparison results in terms of testing accuracy (%).
shown in Table 1. Our proposed method HHTL still performs much more stable and better than the other 3 baselines
except for a few tasks. This is because that our proposed
method can largely reduce the data bias by generating the
more powerful and higher-level features for both the source
and target domains, which can lead to better cross-language
mappings in each layer.
Impact of Layers in the HHTL Structure
As shown in Table 2, the more layers are used, the better performance is achieved. SVM-SC manifests comparable results with HHTL using 1 layer, where the learned high-level
features are not sufﬁciently useful. However, with increasing number of layers, HHTL can enhance the cross-language
classiﬁcation performance by generating more useful and
higher-level features that alleviate the data bias.
Parameter Sensitivity Study
The proposed method has two parameters to tune: 1) the
tradeoff parameter λ, and 2) the feature corruption probability p. Besides these, the number of correspondences nc
from the parallel data may also affect the performance of
cross-language classiﬁcation. In this section, we conduct a
series of experiments to study the sensitivity issues of the
parameters, λ, p and nc.
In the ﬁrst experiment, we aim to analyze how performance changes with varying values of λ in the range of
[0.001, 0.01, 1, 10, 100] with p = 0.7 and nc = 50. From
Figure 2(a), we can observe that the performance is stable
when λ is no more than 10. Besides, we set λ = 0.01,
nc = 50 and vary p in the range of [0.5, 0.6, 0.7, 0.8, 0.9].
The results of all the target languages are illustrated in Figure 2(b). We can see that corruption probability cannot be
either too large or too small to achieve good performance.
On one hand, if we corrupt many features, the original feature information will be discarded a lot, resulting in failure
of discovering powerful hidden features in higher layers by
reconstructions. On the other hand, if we only corrupt a few
features, the high-level hidden features recovered from the
original features always tends to be similar to the original
ones. In this case, we cannot learn informative hidden features for knowledge transfer.
experiment,
( ) to the overall
performance of HHTL. The results for German are reported
in Figure 2(c).3 All the methods which use the unlabeled
parallel data consistently outperform HeMap, which discards the correspondence information. Nevertheless, HTLL
performs the best and achieve more stable improvement
with increasing size of parallel data. Different from HTLL,
the accuracies of CL-KCCA and SVM-SC almost stop
increasing when the size of parallel data is larger than 750.
The reason is that the impact of domain distribution mismatch hinders the improvement of the overall performance
even though with more correspondences. mSDA-CCA
performs even worse than SVM-SC and CL-KCCA when
the number of correspondences is smaller than 750. This is
because that the multimodal deep learning method requires
sufﬁcient correspondence data between two modalities to
learn reliable feature representations. Though mSDA-CCA
outperforms other baselines when nc is larger than 750,
it still performs much worse than HHTL. All the results
demonstrate the effectiveness and robustness of HHTL for
cross-language sentiment classiﬁcation.
Conclusions
In this paper, we have proposed a Hybrid Heterogeneous
Transfer Learning (HHTL) framework to transfer knowledge across different feature spaces and simultaneously correct the data bias on the transformed feature space. Based
on the framework, we proposed a deep learning approach.
Extensive experiments demonstrated the superiority of the
proposed method on several multilingual text mining tasks.
3Due to the limit of space, we only report the results on German.
However, we observe similar results on the other languages.
Acknowledgments
This research was in part supported by the Australian Research Council Future Fellowship FT130100746.