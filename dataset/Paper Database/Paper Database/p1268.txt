Towards a Rigorous Evaluation of XAI Methods on Time Series
Udo Schlegel
University of Konstanz
Konstanz, Germany
 
Hiba Arnout
Siemens CT & TU Munich
Munich, Germany
 
Mennatallah El-Assady
University of Konstanz
Konstanz, Germany
 
Daniela Oelke
Siemens CT
Munich, Germany
 
Daniel A. Keim
University of Konstanz
Konstanz, Germany
 
Explainable Artiﬁcial Intelligence (XAI) methods are
typically deployed to explain and debug black-box machine
learning models. However, most proposed XAI methods are
black-boxes themselves and designed for images. Thus, they
rely on visual interpretability to evaluate and prove explanations. In this work, we apply XAI methods previously used
in the image and text-domain on time series. We present a
methodology to test and evaluate various XAI methods on
time series by introducing new veriﬁcation techniques to incorporate the temporal dimension. We further conduct preliminary experiments to assess the quality of selected XAI
method explanations with various veriﬁcation methods on a
range of datasets and inspecting quality metrics on it. We
demonstrate that in our initial experiments, SHAP works
robust for all models, but others like DeepLIFT, LRP, and
Saliency Maps work better with speciﬁc architectures.
1. Introduction
Due to state-of-the-art performance of Deep Learning
(DL) in many domains ranging from autonomous driving to speech assistance and the developing democratization of it, interpretability and explainability of such
complex models captured more and more interest. Agencies such as DARPA introduced the explainable AI (XAI)
initiative to promote the research around interpretable
Machine Learning (ML) to foster trust into models. Laws
like the EU General Data Protection Regulation got ratiﬁed to force companies to be able to explain the decisions
of algorithms to support fairness and privacy and mitigate
trust issues of users and costumers. The desiderata of ML
systems (fairness, privacy, reliability, trust building ) led
to a new selection process for models .
on the task either interpretable models, such as decision
trees , or new XAI methods, e.g., local interpretable
model-agnostic explanations (LIME) , on top of trained
complex models, for instance, Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN), are incorporated to guarantee the interpretability demands .
Due to these new methods for interpretability on a level
above a model, we introduce a few new deﬁnitions. In the
following, we refer, e.g., LIME as XAI method. Explainers
are deﬁned as an XAI method used on top of a model to get
an XAI explanation of the decision making.
Many prominent XAI methods are tailored onto certain
input types such as images, e.g., Saliency Maps , or text,
e.g., layer-wise relevance propagation (LRP) . They often beneﬁt of their domain to explain with certain aspects,
such as a heatmap on the input , as they can be used as
an overlay by building an abstract feature importance .
However, for instance, videos (sequences of images) and
audio have another temporal dimension which is currently
omitted by XAI methods.
Only limited consideration is
taken into account for sequence or temporal data, e.g., on
XAI method evaluation on natural language processing .
There is currently only limited work about XAI on time
series data such as interpretable decision tress , calculating prototypes and using attention mechanisms .
Dividing the hard task of video classiﬁer explanation into
time series and image tasks is not possible as there is no
good time series solution. However, due to sensors getting cheaper and cheaper, more time-oriented data besides
video and audio is generated, and thus, it is important ﬁrst to
test already prominent XAI methods and discover new ones.
Analyzing time series further enables to automate more actions, e.g., heartbeat anomaly detection , solve new tasks,
e.g., predictive maintenance , and predict stock, e.g.,
stock market forecasting .
 
Konstanzer Online-Publikations-System (KOPS)
URL: 
Erschienen in: 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). - Piscataway,
NJ : IEEE, 2019. - S. 4321-4325. - ISSN 2473-9936. - eISSN 2473-9944. - ISBN 978-1-72815-023-9
 
To debug and optimize time series prediction models in
diverse tasks, not only understanding is essential but also
that the XAI explanation is correct itself . Evaluating
and verifying these explanations is a difﬁcult task due to raw
time series being large and hardly interpretable even by domain experts themselves, and so an evaluation by raw data
and explanation inspection is not feasible. Due to this lack
of connectable domain knowledge, a quantiﬁable approach
is necessary to verify explanations. Notably, in computer
vision exists some work about the evaluation of explanations (e.g., set relevant pixels to zero ), which is
also possible to use on time series. However, these methods omit temporal dependencies by assuming feature independence or only local (short-term) dependency and thus
are only limited veriﬁable on time-oriented data. Hence,
adapted or novel variants of previous methods are needed
to evaluate explanations on time series.
In this work, we show the practical use of various XAI
methods on time series and present the ﬁrst evaluation of
selected methods on a variety of real-world benchmark
datasets. Further, we introduce two sequence veriﬁcation
methods and a methodology to evaluate and check XAI
explanations on time series automatically. In preliminary
experiments, we show the results of our veriﬁcation techniques for the selected XAI techniques and their results.
2. Time Series Explanations
XAI methods have their main application ﬁeld in computer vision due to the state-of-the-art success of black-box
DL models in object recognition and detection and the
visual interpretability of the input . However, a need
for explainability is desired in other domains to either understand the decision making or to improve the models’ performance by debugging failures. Thus, the domain of time
series prediction has a high demand for XAI methods.
A classiﬁcation dataset with univariate time series data
D consists of n samples with classes c1, c2, c3, ..., ck from
a label (multiple classes) with k different classes. A sample
t of D consists of m time points t = (t0, t1, t2, ..., tm).
E.g., an anomaly detection dataset has only two classes
(anomaly, e.g., c2, and normal, e.g., c1). In the following,
the generally considered explanation of most XAI methods is the local feature importance. Time points get converted to features to introduce a workaround to use XAI
methods on time series. The local feature importance produces a relevance ri for each time point ti. Afterward a
tuple (ti, ri) can be build or more general for the time series vector t = (t0, t1, t2, ..., tm) a relevance vector can be
generated as r = (r0, r1, r2, ..., rm).
A model m trained on a subset X from D with labels Y
can be formalized to m(x) = y with x ∈X and y ∈Y . The
model m learns based on the provided data X, Y to predict
an unseen subset Xnew. In the case of time series, x is a
sample like t = (t0, t1, t2, ..., tm) with m time points. If
then an XAI method xai is incorporated to explain the decisions of such a model, another layer on top of it is created.
An explanation can then be formalized as xai(x, m) = exp
with exp being the resulting explanation. With time series,
the explanation exp is a relevance r = (r0, r1, r2, ..., rm)
vector for m time points.
Similar to the saliency masks on images, a heatmap can
be created based on the relevance produced by XAI methods. It is possible to create a visualization with this heatmap
enriching a line plot of the original time series. Together
with domain knowledge, an expert can inspect the produced
explanation visualizations to verify the result qualitatively.
Figure 1. shows an example of relevance heatmaps on time
series. However, as these heatmaps are hard to interpret and
a signiﬁcant challenge to scale to large datasets or long time
series, automated veriﬁcation needs to be applied.
3. Evaluating Time Series Explanations
There are various options on how to evaluate and verify
XAI explanations automatically. In computer vision, a common method consists of a perturbation analysis . This
analysis method substitutes a few pixels (e.g., exchange to
zero) of an image according to their importance (most or
least relevant pixels). However, because, e.g., a zero could
be an indicator for an anomaly in a time series task, the
methodology of evaluation of XAI methods for time series
needs specialized heuristics. We present two novel methods suited explicitly for time series by taking the sequence
property of the time-oriented data into account.
3.1. Perturbation on time series
At ﬁrst, a perturbation analysis presents preliminary
comparison baselines. The evaluation is based on the assumption that if relevant features (time points) get changed,
the performance of an accurate model should decrease massively. If random time points of the data get changed, the
performance should either stagnate or decrease.
Perturbation Analysis – The assumption follows the time
series t = (t0, t1, t2, ..., tm) and the relevance produced by
the XAI method as r = (r0, r1, r2, ..., rm) to get a worse result of the quality metric qm for the classiﬁer if combined.
A time point ti gets gets changed if ri is larger than a certain threshold e, e.g. the 90th percentile of r. Due to XAI
methods have problems with some time-series samples, the
threshold leads to only changing a small number of time
points. In the case of time series, the time point ti is set to
zero or the inverse (maxti −ti) and leads to the new time
series samples tzero and tinverse.
Perturbation Veriﬁcation – To verify the assumption, a
random relevance rr = (r0, r1, r2, ..., rm) is used for the
same procedure.
The number of changed time points,
amount of ri larger than the threshold e, is the same as in
Figure 1. Relevance Heatmaps on an exemplary time series of the FordA dataset using a ResNet paper model. XAI methods shown with
their relevance heatmaps are Saliency Maps, LRP, DeepLIFT, LIME, and SHAP. The blue rectangles display controversial parts of the time
series for the XAI methods with red marking high importance for the classiﬁcation which are, e.g., set to zero by veriﬁcation methods.
the case before to set the same prerequisites for the classi-
ﬁer. This technique creates new time series like the perturbation analysis such as tzero
and tinverse
. The assumption
to verify the model and the XAI method with the random
relevance method follows the schema that the quality metric qm shows e.g. qm(t) ≥qm(tzero
) > qm(tzero) for a
model that maximizes qm.
3.2. Sequence Evaluation
To verify that the model and the XAI method also includes time series features such as slopes or minima, we
present two novel sequence-dependent methods. If the assumptions of the perturbation analysis hold, there is still a
lack of evaluation of trends or patterns in the time series.
E.g., for the classiﬁcation, a decrease to zero could be signiﬁcant, but the perturbation sets the zero to the max as it
is essential for the model and so the classiﬁcation should
get worse. However, if a model learns the general pattern
and generalizes good enough to overcome this change, the
testing is useless. Thus to take the inter-dependency of time
points into account, a closer look onto the time points itself is crucial. We propose two new techniques to test and
evaluate XAI methods incorporating this hypothesis.
Swap Time Points – The ﬁrst additional method again takes
the time series t = (t0, t1, t2, ..., tm) and the relevance for
it r = (r0, r1, r2, ..., rm). However, it takes the time points
with the relevance over the threshold as the starting point
for further changes of the time series.
So, ri > e describes the start point to extract the sub-sequence tsub =
(ti, ti+1, ..., ti+ns) with length ns. The sub-sequence then
gets reversed to tsub = (ti+ns, ..., ti+1, ti) and inserted
back into the time series. Further, in another experiment,
the sub-sequence gets set to zero to test the method. Also,
like in the perturbation analysis, the same procedure is done
with a random time point positions to verify the time points
relevance again.
Mean Time Points – Same as the ﬁrst additional method,
the second one also takes into account the time series
(t0, t1, t2, ..., tm) and the relevance for it r
(r0, r1, r2, ..., rm).
Also, it takes the time points with
the relevance over the threshold as the starting point for
further changes of the time series.
However, instead of
swapping the time points, the mean µ of the sub-sequence
tsub = (ti, ti+1, ..., ti+ns) is taken to exchange the whole
sub-sequence to tsub = (µtsub, µtsub, ..., µtsub) and inserted
back into the time series. Further, in another experiment,
the sub-sequence gets set to zero to test the method. Also,
like in the perturbation analysis, the same procedure is done
with a random time point positions to verify the time points
relevance again.
3.3. Methodology
The methodology to verify an XAI method is conducted
in three stages (model training and evaluation, model explanation creation, explanation evaluation, and veriﬁcation).
1. In the ﬁrst step, a model learns the training data. Afterward, the trained model predicts the test data and a
quality measure (e.g., accuracy) calculates the performance of the result.
2. In the next step, a selected XAI method creates explanations for every sample of the test data. Based on the
time point relevance by the explanations, the test data
gets changed by the evaluation and veriﬁcation methods mentioned before.
3. Then, in the last step, each of these newly created test
sets gets predicted by the model, and the quality measure is calculated for the comparison.
If the XAI method produces correct explanations, the assumptions qm(t) ≥qm(tc
r) > qm(tc) with qm as the
quality measure, t the original time series, tc
r the random
changed, and tc the relevant changed time series, holds.
Table 1. Results table with the averaged changed accuracy of the different models over all datasets. Change to test accuracy is calculated
by normalizing the base accuracy to the one from the changed data.
4. Discussion
The discussion divides into three parts.
At ﬁrst, the
datasets and employed models are addressed to help to
reproduce the experiments. Afterward, the selected XAI
methods are introduced in short, giving an overview. Lastly,
we discuss the preliminary evaluation results.
4.1. Datasets & Models
Nine datasets of the UCR Time Series Classiﬁcation Archive and a ECG hearbeat dataset are
included in a real-world focused preliminary experiment.
These ten datasets, namely FordA, FordB, ElectricDevices, MelbournePedestrian, ChlorineConcentration,
Earthquakes, NonInvasiveFetalECGThorax1, NonInvasive-
FetalECGThorax2, Strawberry , and Physionet’s MIT-
BIH Arrhythmia , consist of two different tasks, binary
and multi-class prediction.
Primarily, binary classiﬁcation, e.g., for anomaly detection, is a critical use case for
time-series predictions to tackle applications like predictive
maintenance or heartbeat categorization.
During the experiments, two different architectures
(CNN and RNN) are used as baseline models.
If available, the architecture provided by the dataset paper is also
incorporated. The considered CNN consists of a 1D convolution layer with kernel and channel size of three. Afterward, a dense layer with 100 neurons learns the classiﬁcation for a speciﬁc problem. The considered RNN consists
of an LSTM layer with 100 neurons and again a dense layer
with 100 neurons for the classiﬁer. Both networks train each
dataset individually for 50 epochs. The paper models consist of ResNet-based architectures and also 50 epochs.
4.2. XAI methods
The experiment is conducted with the ﬁve most prominent XAI methods (LIME , LRP , DeepLIFT ,
Saliency Maps , SHAP ).
LIME employs a socalled surrogate model to explain the decision of an ML
model. Thru sampling data points around an example to be
explained, it learns a linear model to extract local feature
importance for the prediction of the more complex model.
By propagating the gradients through the network, Saliency
Maps and DeepLIFT build a heatmap as feature importance.
LRP propagates a relevance score backward through the underlying model to specify feature importance. SHAP employs shapely values and game theory to ﬁnd the best ﬁtting
feature to gain the most for the prediction.
4.3. Results
Our preliminary results,
see Table 1.,
DeepLIFT and LRP have the largest overall quality metric decrease in CNNS for the perturbation and sequence
analysis, which shows the working local feature importance. Saliency Maps and SHAP outperform the others in
RNNs by showing quality metric decreases, which is somewhat unexpected but shows a need for further exploration
of RNNs with XAI methods. In more advanced ResNet
architectures, SHAP produces the best results. However,
also DeepLIFT and LRP show good results, which again
shows the practical local feature importance. LIME shows
terrible results in all cases, most likely due to large dimensionality and the employed linear classiﬁer. Further, the results show the desiderata for the sequence veriﬁcation methods as the random perturbation of time points has a signiﬁcant quality metric decrease. Our proposed sequence
veriﬁcation methods present more clearly that the assumption qm(t) ≥qm(tmean
random) > qm(tmean) with qm as the
quality measure, t the time series, tmean
random and tmean the
changed time series holds.
5. Conclusion and Future Work
Our methodology and veriﬁcation methods show that
XAI methods, proposed for images and text, work on time
series data by specifying a relevance to time points. The
methods also demonstrate that the models take the temporal aspect into account in some cases. In our experiment,
we ﬁnd that SHAP works robust for all models, but others like DeepLIFT, LRP, and Saliency Maps work better for
speciﬁc architectures. LIME performs worst most likely because of the large dimensionality by converting time to features. However, we also conclude that a demand is given
to introduce more suitable XAI methods on time series to
guarantee a better human understanding in the process of
XAI. As seen by the hard to interpret visual saliency masks
(heatmaps) on time series, a need for a more abstract representation is necessary and increases the importance for
more sophisticated visual XAI methods on time series.