This is the accepted manuscript made available via CHORUS. The article has been
published as:
Finding Density Functionals with Machine Learning
John C. Snyder, Matthias Rupp, Katja Hansen, Klaus-Robert Müller, and Kieron Burke
Phys. Rev. Lett. 108, 253002 — Published 19 June 2012
DOI: 10.1103/PhysRevLett.108.253002
REVIEW COPY
NOT FOR DISTRIBUTION
Finding Density Functionals with Machine Learning
John C. Snyder,1 Matthias Rupp,2, 3 Katja Hansen,2 Klaus-Robert M¨uller,2, 4 and Kieron Burke1
1Departments of Chemistry and of Physics, University of California, Irvine, CA 92697, USA
2Machine Learning Group, Technical University of Berlin, 10587 Berlin, Germany
3Institute of Pharmaceutical Sciences, ETH Zurich, 8093 Z¨urich, Switzerland
4Department of Brain and Cognitive Engineering,
Korea University, Anam-dong, Seongbuk-gu, Seoul 136-713, Korea
Machine learning is used to approximate density functionals. For the model problem of the kinetic
energy of non-interacting fermions in 1d, mean absolute errors below 1 kcal/mol on test densities
similar to the training set are reached with fewer than 100 training densities. A predictor identiﬁes
if a test density is within the interpolation region. Via principal component analysis, a projected
functional derivative ﬁnds highly accurate self-consistent densities. Challenges for application of our
method to real electronic structure problems are discussed.
PACS numbers: 31.15.E-, 31.15.X-, 02.60.Gf, 89.20.Ff
Each year, more than 10,000 papers report solutions
to electronic structure problems using Kohn-Sham (KS)
density functional theory (DFT) . All approximate
the exchange-correlation (XC) energy as a functional of
the electronic spin densities. The quality of the results
depends crucially on these density functional approximations. For example, present approximations often fail for
strongly correlated systems, rendering the methodology
useless for some of the most interesting problems.
Thus, there is a never-ending search for improved XC
approximations. The original local density approximation (LDA) of Kohn and Sham is uniquely deﬁned by
the properties of the uniform gas, and has been argued
to be a universal limit of all systems . But the reﬁnements that have proven useful in chemistry and materials are not, and diﬀer both in their derivations and
Traditionally, physicists favor a non-empirical
approach, deriving approximations from quantum mechanics and avoiding ﬁtting to speciﬁc ﬁnite systems .
Such non-empirical functionals can be considered controlled extrapolations that work well across a broad range
of systems and properties, bridging the divide between
molecules and solids. Chemists typically use a few 
or several dozen parameters to improve accuracy on a
limited class of molecules. Empirical functionals are limited interpolations that are more accurate for the molecular systems they are ﬁtted to, but often fail for solids.
Passionate debates are fueled by this cultural divide .
Machine learning (ML) is a powerful tool for ﬁnding
patterns in high-dimensional data.
ML employs algorithms by which the computer learns from empirical data
via induction, and has been very successful in many applications .
In ML, intuition is used to choose
the basic mechanism and representation of the data, but
not directly applied to the details of the model. Mean
errors can be systematically decreased with increasing
number of inputs. In contrast, human-designed empirical approximations employ standard forms derived from
general principles, ﬁtting the parameters to training sets.
These explore only an inﬁnitesimal fraction of all possible
functionals and use relatively few data points.
DFT is useful for electronic structure because the underlying many-body Hamiltonian is simple, while accurate solution of the Schr¨odinger equation is very demanding. All electrons Coulomb repel one-another and have
spin 1/2, which makes the Hohenberg-Kohn theorem 
possible. But real electronic structure problems are further limited to only those one-body potentials due to
Coulomb attraction to the nuclei. ML is a natural tool
for taking maximum advantage of this simplicity. For ML
to be useful, a pattern must exist, but one that evades
human intuition. Furthermore, most present approximations begin from LDA , and fail miserably when LDA is
a poor starting point. An ML-produced functional suffers no such bias, and so should be most useful where
present approximations fail, if it has good examples to
Here, we adapt ML to a prototype density functional
problem: non-interacting spinless fermions conﬁned to
a 1d box, subject to a smooth potential. We deﬁne key
technical concepts needed to apply ML to DFT problems.
FIG. 1. (color online). Comparison of a projected (see within)
functional derivative of our MLA with the exact curve.
The accuracy we achieve in approximating the kinetic energy (KE) of this system is far beyond the capabilities of
any present approximations and is even suﬃcient to produce highly accurate self-consistent densities. Our ML
approximation (MLA) achieves chemical accuracy using
many more inputs, but requires far less insight into the
underlying physics.
We illustrate the accuracy of our MLA with Fig. 1, in
which the functional was constructed from 100 densities
on a dense grid. This success opens up a new approach to
functional approximation, entirely distinct from previous
approaches: Our MLA contains ∼105 empirical numbers
and satisﬁes none of the standard exact conditions.
The prototype DFT problem we consider is N noninteracting spinless fermions conﬁned to a 1d box, 0 ≤
x ≤1, with hard walls. For continuous potentials v(x),
we solve the Schr¨odinger equation numerically with the
lowest N orbitals occupied, ﬁnding the KE and the electronic density n(x), the sum of the squares of the occupied orbitals. Our aim is to construct an MLA for the
KE T [n] that bypasses the need to solve the Schr¨odinger
equation—a 1d analog of orbital-free DFT .
(3d) orbital-free DFT, the local approximation, as used
in Thomas-Fermi theory, is typically accurate to within
10%, and the addition of the leading gradient correction
reduces the error to about 1% . Even this small an error in the total KE is too large to give accurate chemical
properties.)
First, we specify a class of potentials from which we
generate densities, which are then discretized on a uniform grid of G points. We use a linear combination of 3
Gaussian dips with diﬀerent depths, widths, and centers:
ai exp(−(x −bi)2/ , we ﬁnd, for N up to 4 electrons, the KE Tj,N and
density nj,N ∈RG on the grid using Numerov’s method
 . For G = 500, the error in Tj,N due to discretization
is less than 1.5 × 10−7. We take 1000 densities as a test
set, and choose M others for training. The variation in
this dataset for N = 1 is illustrated in Fig. 2.
Kernel ridge regression (KRR) is a non-linear version
of regression with regularization to prevent overﬁtting
 . For KRR, our MLA takes the form
T ML(n) = ¯T
αjk(nj, n),
where αj are weights to be determined, nj are training
densities and k is the kernel, which measures similarity
between densities. Here ¯T is the mean KE of the training set, inserted for convenience. We choose a Gaussian
FIG. 2. (color online). The shaded region shows the extent of
variation of n(x) within our dataset for N = 1. Exact (red)
and a self-consistent (black, dashed) density for potential of
kernel, common in ML:
k(n, n′) = exp(−∥n −n′∥2/(2σ2)),
where the hyperparameter σ is called the length scale.
The weights are found by minimizing the cost function
j + λ∥α∥2,
where ∆Tj = T ML
−Tj and α = (α1, . . . , αM).
second term is a regularizer that penalizes large weights
to prevent overﬁtting. The hyperparameter λ controls
regularization strength. Minimizing C(α) gives
α = (K + λI)−1T ,
where K is the kernel matrix, with elements Kij =
k(ni, nj), and I is the identity matrix. Then σ and λ are
determined through 10-fold cross-validation: The training set is partitioned into 10 bins of equal size. For each
bin, the functional is trained on the remaining samples
and σ and λ are optimized by minimizing the mean absolute error (MAE) on the bin. The partitioning is repeated
up to 40 times and the hyperparameters are chosen as the
median over all bins.
Table I gives the performance of T ML (Eq. 2) trained
on M N-electron densities and evaluated on the corresponding test set.
The mean KE of the test set for
N = 1 is 5.40 Hartree (3390 kcal/mol).
To contrast,
the LDA in 1d is T loc[n] = π2 R
dx n3(x)/6 and the von
Weizs¨acker functional is T W[n] =
dx n′(x)2/(8n(x)).
For N = 1, the MAE of T loc on the test set is 217
kcal/mol and the modiﬁed gradient expansion approximation , T MGEA[n] = T loc[n] −c T W[n], has a MAE
of 160 kcal/mol, where c = 0.0543 has been chosen to
minimize the error (the gradient correction is not as beneﬁcial in 1d as in 3d).
For T ML, both the mean and
TABLE I. Parameters and errors (mean absolute, std. dev.,
and max abs. in kcal/mol) as a function of electron number N
and number of training densities M. Brackets represent errors
on self-consistent densities with m = 30 and ℓ= 5. The αj
are on the order of 106 and both positive and negative .
†Training set includes nj,N, for j = 1, . . . , 100, N = 1, . . . , 4.
maximum absolute errors improve as N or M increases
(the system becomes more uniform as N →∞ ). At
M = 80, we have already achieved “chemical accuracy,”
i.e., a MAE below 1 kcal/mol. At M = 200, no error is
above 1 kcal/mol. Simultaneously incorporating diﬀerent N into the training set has little eﬀect on the overall
performance, and we stop at N = 4 merely for convenience. Note that our accuracy is so high that energy
diﬀerences due to very subtle density changes are accurately captured by our approximation.
With such unheard of accuracy, it is tempting to declare “mission accomplished,” but this would be premature. A KE functional that predicts only the energy is
useless in practice, since orbital-free DFT uses functional
derivatives in self-consistent procedures to ﬁnd the density within a given approximation, via
δn(x) = µ −v(x),
where µ is adjusted to produce the required particle number. The (discretized) functional derivative of T ML is
∆x∇nT ML(n) =
j(nj −n)k(nj, n),
j = αj/(σ2∆x). This oscillates wildly relative to
the exact curve (Fig. 3), typical behavior that does not
improve with increasing M. No ﬁnite interpolation can
accurately reproduce all details of a functional derivative,
and this behavior probably worsens when more varied
densities are treated.
We overcome this problem using principal component
analysis (PCA). The space of all densities is contained
FIG. 3. (color online). Functional derivative of T ML, evaluated on the density of Fig. 2.
in RG, but only a few directions in this space are relevant. For a given density n, ﬁnd the m training densities (nj1, . . . , njm) closest to n. Construct the covariance matrix of directions from n to each training density
C = X⊤X/m, where X = (nj1 −n, . . . , njm −n)⊤.
Diagonalizing C ∈RG×G gives eigenvalues λj and eigenvectors xj which we list in decreasing order.
with larger λj are directions with substantial variation
in the dataset. Those with λj below a cutoﬀare irrelevant . In these extraneous dimensions, there is too
little variation within the dataset, producing noise in the
model functional derivative. By projecting onto the subspace spanned by the relevant dimensions, we eliminate
this noise. This projection is given by Pm,ℓ(n) = V ⊤V ,
where V = (x1, . . . , xℓ)⊤and ℓis the number of relevant
eigenvectors. In Fig 1, with m = 30 and ℓ= 5, the projected functional derivatives are in excellent agreement.
The ultimate test for a density functional is to produce
a self-consistent density that minimizes the total energy
and check its error. This error will be several times larger
than that of the functional evaluated on the exact density.
For example, T loc on particles in 1d ﬂat boxes always
gives 4 times larger error. To ﬁnd a minimizing density,
perform a gradient descent search restricted to the local
PCA subspace: Starting from a guess n(0), take a small
step in the opposite direction of the projected functional
derivative of the total energy in each iteration j:
n(j+1) = n(j) −ǫPm,ℓ(n(j))(v+∇nT ML(n(j))/∆x), (8)
where ǫ is a small number and v is the discretized potential. The search is unstable if ℓis too large, inaccurate if
ℓis too small, and relatively insensitive to m .
The performance of T ML in ﬁnding self-consistent densities is given in Table I. Errors are an order of magnitude
larger than that of T ML on the exact densities. We do
not ﬁnd a unique density, but instead a set of similar densities depending on the initial guess (e.g. Fig. 2). The
density with lowest total energy does not have the smallest error. Although the search does not produce a unique
minimum, it produces a range of similar but valid approximate densities, each with a small error. Even with an
order of magnitude larger error, we still reach chemical
accuracy, now on self-consistent densities.
No existing
KE approximation comes close to this performance.
What are the limitations of this approach? ML is a
balanced interpolation on known data, and should be
unreliable for densities far from the training set.
demonstrate this, we generate a new dataset of 5000
densities with N = 1 for an expanded parameter range:
0.1 < a < 20, 0.2 < b < 0.8 and 0.01 < c < 0.3. The
predictive variance (borrowed from Gaussian process regression )
V[T ML(n)] = k(n, n) −k(n)⊤(K + λI)−1k(n),
where k(n) = (k(n1, n), . . . , k(nM, n)), is a measure of
the uncertainty in the prediction T ML(n) due to sparseness of training densities around n. In Fig. 4, we plot the
error ∆T as a function of log(V[T ML(n)]), for both the
test set and the new dataset, showing a clear correlation.
From the inset, we expect our MLA to deliver chemical
accuracy for log(V[T ML(n)]) < −24.
FIG. 4. (color online). The correlation between MLA error
and predictive variance for N = 1, M = 100.
Each point
represents a density in the test set (blue) or new dataset (red).
The vertical line denotes the transition between interpolation
and extrapolation.
Does ML allow for human intuition? In fact, the more
prior knowledge we insert into the MLA, the higher the
accuracy we can achieve. Writing T = T W + Tθ, where
Tθ ≥0 , we repeat our calculations to ﬁnd an MLA
for Tθ. For N = 1 we get almost zero error, and a factor
of 2-4 reduction of error otherwise. Thus, intuition about
the functional can be built in to improve results.
The primary interest in KS DFT is XC for molecules
and solids. We have far less information about this than
in the prototype studied here. For small molecules and
simple solids, direct solutions of the Schr¨odinger equation yield highly accurate values of EXC. Imagine a sequence of models, beginning with atoms, diatomics, etc.,
in which such accurate results are used as training data
for an MLA. In the case of XC, key issues are how accurate a functional can be attained with a ﬁnite number of
data, and what fraction of the density space it is accurate
A more immediate target is the non-interacting KE in
KS DFT calculations. An accurate approximation would
allow ﬁnding densities and energies without solving the
KS equations, greatly increasing the speed of large calculations . The key diﬀerences with our prototype is
the three-dimensional nature, the Coulomb singularities,
and the variation with nuclear positions. For this problem, ﬁnding self-consistent densities is crucial, and hence
our focus here. But in the 3d case, every KS calculation ever run, including every iteration in a self-consistent
loop, generates training data—a density, KE, KS potential and functional derivative. The space of all systems
of practical interest, including both solids and molecules,
is vast, but can be approached in small steps, including
always training on ‘nearby’ densities.
Continuing the discussion of the KE functional, our
demo has been (purposely) limited to a very simple class
of potentials.
But unlike traditional ﬁtting to limited
approximate forms of a functional, there is no reason apriori to expect our method to scale poorly with the complexity of the one-body potential. In ML, the problem is
reduced to approximating a functional by a scalar function of a high-dimensional domain (500 here). The dif-
ﬁculty depends on how smooth this functional is, which
determines how many training densities we need to interpolate accurately. We estimate the eﬀective dimensionality, or RDE , of our data at about 12. We anticipate
this to increase by a modest factor when dealing with
electrons of diﬀering character (e.g. d and f electrons),
but not exponentially, for the weakly correlated systems
for which present XC functionals are useful. Moreover,
statistical learning theory shows that the error
of regression estimators (i.e. our method) scales asymptotically as 1/M with the number of training data M
for faithful models and as 1/
M for unfaithful ones. As
is customary in ML, none of these questions will be answered until the full problem has been attempted. Preliminary model calculations for bond dissociation, where
most present approximations fail due to their local nature, show only a mild increase in the need for training
data .
Two last points: The ﬁrst is that this type of empiricism is qualitatively distinct from that present in the literature . The choices we made are those customary in
ML, and require no intuition about the physical nature of
the problem. Second, the approximation is expressed in
terms of about 105 numbers, and only the projected functional derivative is accurate. We have no simple way of
comparing such approximations to those presently popular. For example, for N = 1 in the prototype, the exact
functional is T W. How is this related to our MLA, and
how does our MLA account for this exact limit?
The authors thank the Institute for Pure and Applied
Mathematics at UCLA for hospitality and acknowledge
NSF CHE-1112442 (JS, KB), EU PASCAL2 and DFG
MU 987/4-2 (MR, KH, KRM), EU Marie Curie IEF
273039 and NRF Korea R31-10008 (MR).
 P. Hohenberg and W. Kohn, Phys. Rev. B 136, 864
 W. Kohn and L. J. Sham, Phys. Rev. A 140, 1133 .
 P. Elliott, D. Lee, A. Cangi, and K. Burke, Phys. Rev.
Lett. 100, 256406 .
 P. J. Stephens, F. J. Devlin, C. F. Chabalowski, and M.
J. Frisch, J. Phys. Chem. 98, 11623 .
 J. P. Perdew, K. Burke, and M. Ernzerhof, Phys. Rev.
Lett. 77, 3865 .
 J. P. Perdew and A. Ruzsinszky, Int. J. Quant. Chem.
110, 2801 .
 A. D. Becke, Phys. Rev. A 38, 3098 .
 C. Lee, W. Yang, and R. G. Parr, Phys. Rev. B 37, 785
 Y. Zhao and D. Truhlar, Theor. Chem. Accounts 120,
215 .
 K. Burke, J. Chem. Phys. 136, 150901 .
 K.-R. M¨uller, S. Mika, G. R¨atsch, K. Tsuda, and B.
Sch¨olkopf, IEEE Trans. Neural Network 12, 181 .
 O. Ivanciuc, in Reviews in Computational Chemistry,
edited by K. Lipkowitz and T. Cundari , Vol. 23, p. 291.
 M. Rupp, A. Tkatchenko, K.-R. M¨uller, and O. A. von
Lilienfeld, Phys. Rev. Lett. 108, 058301 .
 V. Karasiev, R. Jones, S. Trickey, and F. Harris, in New
Developments in Quantum Chemistry, edited by J. Paz
and A. Hern´andez (Research Signpost, Kerala, in press).
 R. M. Dreizler and E. K. U. Gross, Density Functional
Theory: An Approach to the Quantum Many-Body Problem .
 See e.g. E. Hairer, P. Nørsett, P. Syvert Paul and G.
Wanner, Solving ordinary diﬀerential equations I: Nonstiﬀproblems .
 T. Hastie, R. Tibshirani, and J. Friedman, The Elements
of Statistical Learning. Data Mining, Inference, and Prediction, 2nd ed. .
 See Supplemental Material at [URL will be inserted by
publisher] for information necessary to construct the
MLA functional and more detail on the PCA projections
and self-consistent densities.
 D. Lee, L. A. Constantin, J. P. Perdew, and K. Burke, J.
Chem. Phys. 130, 034107 .
 C. Rasmussen and C. Williams, Gaussian Processes for
Machine Learning .
 M. Braun, J. Buhmann, and K.-R. M¨uller, Journal of
Machine Learning Research 9, 1875 .
 K.-R. M¨uller, N. Murata, M. Finke, K. Schulten, and S.
Amari, Neural Computation 8, 1085 .
 J. C. Snyder, M. Rupp, K. Hansen, K.-R. M¨uller, and K.
Burke, in prep .