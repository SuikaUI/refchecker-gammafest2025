Findings of the Association for Computational Linguistics: ACL 2022, pages 2912 - 2924
May 22-27, 2022 c⃝2022 Association for Computational Linguistics
Controllable Natural Language Generation with Contrastive Preﬁxes
Jing Qian1, Li Dong2, Yelong Shen2, Furu Wei2, Weizhu Chen2
1University of California, Santa Barbara
2Microsoft Corporation
 
{lidong1,yeshe,fuwei,wzchen}@microsoft.com
To guide the generation of large pretrained
language models (LM), previous work has
focused on directly ﬁne-tuning the language
model or utilizing an attribute discriminator.
In this work, we propose a novel lightweight
framework for controllable GPT2 generation, which utilizes a set
of small attribute-speciﬁc vectors, called pre-
ﬁxes , to steer natural language generation. Different from Li and Liang
 , where each preﬁx is trained independently, we take the relationship among preﬁxes
into consideration and train multiple preﬁxes
simultaneously, as illustrated in Figure 1. We
propose a novel supervised method and also an
unsupervised method to train the preﬁxes for
single-aspect control while the combination of
these two methods can achieve multi-aspect
control. Experimental results on both singleaspect and multi-aspect control show that our
methods can guide generation towards the desired attributes while keeping high linguistic
Introduction
The goal of controllable Natural Language Generation (NLG) is to guide generation towards the desired attributes in the concerned aspects of the text.
For example, the aspect can be topic or sentiment,
and sentiment may have two attributes: positive and
negative. Previous work has focused on directly
ﬁne-tuning the existing models or using
a discriminator to guide generation .
CTRL achieves controllability at the expense of training a large conditional
LM. GeDi also trains conditional LMs but uses them as discriminators to
guide generation, introducing additional 345M parameters. Besides, GeDi focuses on single-aspect
control, ignoring the need for multi-aspect control.
Figure 1: A comparison of preﬁx-tuning (top) and our framework (bottom) on sentiment
control. The solid arrows show the training process,
while the dashed ones show the inference (generation)
process. In our proposed framework, the training can
be supervised, semi-supervised, or unsupervised.
PPLM guides generation
by iteratively updating the LM’s hidden activations.
However, this decoding strategy is extremely computationally intensive, resulting in a slow generation speed .
Preﬁx-tuning proposes to
optimize a preﬁx, which is a small continuous taskspeciﬁc vector, as a lightweight alternative to ﬁnetuning an NLG task, such as table-to-text generation or summarization. Inspired by Li and Liang
 , we propose to use preﬁxes, a set of small
continuous attribute-speciﬁc vectors, to steer NLG.
Compared with using an attribute model or a generative discriminator , using learned preﬁxes to achieve controllability has the following beneﬁts. First, it introduces fewer additional parameters (~0.2%-2% of
GPT2 parameters in our experiments). Second, using preﬁxes keeps the inference speed comparable
to that of the original GPT2 model.
In a general sense, preﬁx-tuning can be considered as controlling the genera-
tion of language models. Preﬁx-tuning views each
preﬁx as an independent control task thus trains
each preﬁx separately (top in Figure 1). However,
one aspect of controllability in NLG involves multiple attributes, which might have a relationship
with each other. For example, the sentiment aspect
usually has two attributes: positive and negative,
which are in opposition to each other. We think
that this opposite relationship can be helpful to improve the controllability of a preﬁx. Therefore, we
propose a novel supervised method and a novel unsupervised one in our framework, which takes the
relationship among preﬁxes into consideration and
trains multiple preﬁxes simultaneously with novel
training objectives, as illustrated in Figure 1.
Experimental results on the single-aspect control
tasks (sentiment control, detoxiﬁcation, and topic
control) show that our proposed methods can guide
generation towards the target attribute while keeping high linguistic quality, even when only several
dozen labeled examples are available. In addition
to single-aspect control, multi-aspect control can
be achieved by combining the proposed supervised
method with the unsupervised method in our framework. Experimental results on the sentiment and
topic control show that the preﬁxes trained with our
method can successfully control these two aspects
simultaneously.
Our main contributions are as follows:
• We propose a novel framework that utilizes pre-
ﬁxes with frozen LMs as a lightweight alternative
for controllable GPT2 generation.
• We propose a supervised method and an unsupervised method with novel objectives for preﬁx
training, where the relationship among preﬁxes
are considered and multiple preﬁxes are trained
simultaneously.
• This work provides a uniﬁed perspective for
single-aspect control and multi-aspect control.
Experimental results show that our methods can
effectively guide generation in both single-aspect
control and multi-aspect control.
Related Work
Ficler and Goldberg control the stylistic
aspects of the generated text with a conditioned
RNN (Recurrent Neural Network) LM. Holtzman
et al. compose a committee of discriminators to guide an RNN generator towards the generations with the desired linguistic quality. Hu et al.
 aim at controlling the sentiment and tense
of the generated text by combining variational autoencoders (VAE) and attribute discriminators.
More recently, with the advent of Transformers and large pretrained language models, such as
GPT2, an extensive body of work has focused on
controlling the generation of these Transformerbased models. Keskar et al. train a 1.63
billion-parameter conditional transformer LM from
scratch with 55 attribute control codes to guide generation. However, this method is expensive and
lacks ﬂexibility since the control codes are ﬁxed.
Dathathri et al. address these limitations by
developing a plug-and-play model which leverages
an attribute discriminator to perturb the LM’s hidden activations. However, updating gradients at the
token level results in slow inference. Instead of updating the hidden activations, Krause et al. ;
Yang and Klein ; Lin and Riedl introduce generative discriminators to re-weight the
next token distributions on the ﬂy during inference,
thus improving the inference speed.
Our work is mostly related to Yu et al. ; Li
and Liang . Yu et al. use a pretrained
LM followed by an attribute alignment function to
encode the tokens of the target attributes and the
resulting hidden states are used to control generation. Different from their work, we do not take
the tokens of the target attributes as input. Instead,
we directly train a set of parameters, which acts
as the prepended hidden states of GPT2, to control generation. Avoiding using attribute tokens
can circumvent the problems when it is difﬁcult to
describe the desired attribute with only one word.
Besides, Yu et al. focus on attributes disentanglement, which is not a focus in our work, so our
training methods are different. Preﬁx-tuning can, in a general sense, be viewed
as controlling the generation of LMs, where the LM
is controlled to depict a speciﬁc NLG task, while
in this work, the LM is controlled to carry speciﬁc
attributes in a generation. Besides, our proposed
methods for preﬁx training are different from Li
and Liang , as stated in Section 1.
Our method uses preﬁxes to guide GPT2 generation, where a preﬁx is a continuous attributespeciﬁc vector prepended to the activations of
GPT2. Preﬁxes are free parameters denoted as
Hθ. Different from Li and Liang , where
each preﬁx is trained independently, we consider
the relationship among attributes and train multiple preﬁxes simultaneously, so Hθ is of dimension
N × M × D, where N is the number of preﬁxes.
In single-aspect control, N equals the number of
attributes in the concerned aspect. M is the length
of a preﬁx. D = 2 × L × E is the dimension
of the activation in GPT2, where L is the number
of transformer layers, E is the hidden size, and
2 indicates one key vector and one value vector.
Following Li and Liang , we reparametrize
Hθ[i, j, :] = WiH′
θ[i, j, :] by a smaller parameter
θ) composed with a large matrix (Wi). After the
training ﬁnishes, only Hθ needs to be saved for generation while W and H′
θ can be discarded. Since
the GPT2 parameters are kept frozen during training, they do not need to be saved either. Figure 2
shows an example of the generation process under
the control of a trained preﬁx. The preﬁxes can be
trained in a supervised, semi-supervised, or unsupervised way. Since the semi-supervised method is
a combination of the supervised and the unsupervised method, we introduce the supervised and the
unsupervised method in this section. For clarity,
we introduce these methods under the single-aspect
control setting.
Supervised Method
Suppose the concerned aspect has the attribute set
Y , each training example is a pair of (x, y) where
x is the input text and y ∈Y is the attribute label
of x. Note that the attribute label also indicates the
ground truth index of the preﬁx in Hθ, so y also
refers to the preﬁx index in the following description. As mentioned in Section 1, we introduce an
additional discriminative loss to train multiple pre-
ﬁxes simultaneously. Therefore, the training loss
Lsup is a weighted sum of the language model loss
LLM and the discriminative loss Ld:
Lsup = ω1LLM + ω2Ld
log p(xt|x<t, y)
p(y)p(x|y)
y′∈Y p(y′)p(x|y′)
The computation of log p(xt|x<t, y) is parameterized as log pθ,γ(xt|x<t, Hθ[y, :, :]), where γ is
the set of ﬁxed GPT2 parameters, and θ represents learnable preﬁx parameters. log p(x|y) =
t log p(xt|x<t, y), so the parameterization of
Figure 2: An illustration of the GPT2 generation process unfolded through time, controlled by a positive
sentiment preﬁx H1 = Hθ[1, :, :]. “The book” is the
given prompt. “is good” is the generated completion.
log p(x|y) is the sum of log pθ,γ(xt|x<t, Hθ[y, :, :])
Note that each preﬁx can be trained independently using LLM alone, which would be the same
as preﬁx-tuning . Intuitively,
preﬁxes trained by LLM are infused with the information of what is encouraged to generate. However,
we observe that in controllable NLG, it is helpful to
also infuse a preﬁx with the information of what is
discouraged to generate. Given a training example
(x, y), the preﬁx Hθ[y, :, :] should be optimized towards generating x, while the other preﬁxes should
be discouraged to generate x. To achieve this goal,
all the preﬁxes in Hθ should be trained simultaneously. Therefore, the discriminative loss Ld is
introduced. As in equation 3, optimizing Ld improves the attribute alignment p(y|x) by increasing
p(x|y) and lowering p(x|¯y), ¯y ∈Y \{y} at the
same time. We assume uniform prior, so p(y) and
p(y′) can be canceled out in Equation 3. Figure 3
illustrates the training process with two preﬁxes.
Unsupervised Method
In the unsupervised setting, we assume the attribute
set Y of the concerned aspect is known. The training example consists of input text x only. The
attribute label y is no longer available and thus the
index of the preﬁx associated with x is unknown. In
other words, the index of the preﬁx corresponding
to x is a latent variable z, whose posterior distribution follows a categorical distribution. Inspired by
VQ-VAE , we consider
the preﬁxes as discrete latent representations. We
take the backbone model in the above supervised
method as the decoder and introduce an encoder
to parameterize the categorical distribution q(z|x).
According to q(z|x), a preﬁx index z is selected
and the preﬁx Hθ[z, :, :] is then fed into the decoder
Figure 3: An illustration of the supervised training method on sentiment control. H0 is the preﬁx of negative
sentiment. H1 is the preﬁx of positive sentiment. Note that training without Ld is equivalent to Li and Liang
 , where H0 and H1 are trained separately. The GPT2 is pretrained, and its parameters are frozen.
Figure 4: An illustration of the unsupervised training method. Hθ denotes the 2 preﬁxes. z is the latent variable
indicating the index of the preﬁx corresponding to the input text x. ¯z is the latent variable indicating the index of
the opposite preﬁx. ⊗is matrix multiplication. LKL is not shown in this ﬁgure for clarity.
to reconstruct the input text x. Since the selection
process of the preﬁxes is non-differentiable, we use
Gumbel-Softmax (GS) relaxation following Sønderby et al.
 ; Ramesh et al. . Formally, q(z|x) is
computed as follows:
q(z|x) = GS(−∥Enc(x) −Hθ∥2, τ)
where τ is the temperature of Gumbel-Softmax,
and Enc is the encoder function. We use a pretrained GPT-2 model followed by a linear layer as
the encoder. To train the preﬁxes, the loss function
is a weighted sum of the three loss terms:
Luns = ω1LLM + ω2LKL + ω3Lc
log p(xt|x<t, z)
LKL = KL[q(z|x)||p(z)]
where LLM is the language model loss.
Similar as that in the supervised method, the computation of log p(xt|x<t, z) is parameterized as
log pθ,γ(xt|x<t, Hθ[z, :, :]). LKL is the Kullback-
Leibler divergence, where we assume the prior p(z)
to be uniform. Note that these two terms constitute
the loss function of VAE. Optimizing these two
loss terms improves the evidence lower bound of
log p(x). Similar to the intuition behind Ld in the
supervised method, if the ground truth preﬁx for
x is Hθ[y, :, :], then the other preﬁxes should be
discouraged to generate x. However, Ld requires
the ground truth label y for computation. Instead,
we introduce an unsupervised contrastive loss Lc.
Lc = max(m −∥p(z|x) −p(¯z|x)∥2, 0)2
where m is a pre-set margin and ¯z is another latent
variable indicating the index of the opposite preﬁx
of x. q(¯z|x) is computed as follows:
q(¯z|x) = GS(∥Enc(x) −Hθ∥2, τ)
Lc is aimed at increasing the attribute alignment
by pushing p(z|x) away from p(¯z|x) by a margin.
The computation of p(z|x) is as follows:
p(z)p(x|z)
z′∈Y p(z′)p(x|z′)
We assume uniform prior, so p(z) and p(z′)
can be canceled out. Similar as the parameterization of log p(x|y) in the supervised method,
the parameterization of log p(x|z) is the sum of
log pθ,γ(xt|x<t, Hθ[z, :, :]) over t.
The training
process is illustrated in Figure 4.
Experiments
We experiment with three tasks: sentiment control,
detoxiﬁcation, and topic control. We compare our
method to GPT2, PPLM, and GeDi. We focus on
English text in all the experiments and we experiment with GPT2-medium (345M parameters) for
all the methods. We use the original implementation of PPLM and GeDi released by Dathathri et al.
 and Krause et al. , and the hyperparameters are set to the reported value in the original
paper. The detailed hyperparameters in each task
are listed in appendix A. For the GPT2 model, we
do experiments under two settings. First, the GPT2
model generates completions of each prompt in
the evaluation dataset, which is denoted as GPT2medium. Second, GPT2-medium + prompt engineering prepends a guiding sentence to each testing prompt and then generates completions of each
augmented prompt. We evaluate the linguistic quality and attribute alignment of the generation. The
linguistic quality is evaluated using the perplexity
calculated by GPT2-large (774M parameters).
To evaluate the robustness of our supervised
method with the size of the training dataset, we experiment with the following three different settings:
1) using the complete training dataset; 2) using
1,000 examples per attribute for training; 3) using
24 examples per attribute for training. We evaluate
our unsupervised method on the sentiment control task and the detoxiﬁcation task, which are binary tasks. Note that different from the supervised
method, our unsupervised method does not use any
attribute labels, so the order of the attributes in the
trained preﬁxes is undetermined. After the preﬁxes
ﬁnish training using the unsupervised method, we
manually check the order of the attributes.
Single-Aspect Control
Sentiment Control
Same as GeDi, we use IMDb
movie reviews to train our
model. The number of preﬁxes is 2. Note that
GeDi only uses 11.25k examples from the dataset
for training.
To be a fair comparison, we randomly sample 11.25k examples from the dataset
to train our model.
To evaluate the sentiment
alignment of the generated text, we ﬁnetune a
RoBERTa classiﬁer using the
Yelp Review dataset . The
prompts used for evaluation are the same as those
in the PPLM experiment .
For each of the 15 prompts, 45 completions are generated. In the GPT2-medium + prompt engineering
setting, we prepend each prompt with the guiding
sentence “This is a negative review:” for negative sentiment control, and similarly, we prepend
each prompt with “This is a positive review:” for
positive sentiment control.
Detoxiﬁcation
We use Jigsaw Toxic Comment
Classiﬁcation Challenge Dataset1 to train our
The number of preﬁxes is 2.
Perspective API2 is used for toxicity evaluation.
The testing prompts are collected from RealToxicityPrompts . We use the
prompts categorized as “challenging” in the dataset.
We further ﬁlter out the prompts with toxicity larger
than 0.5, scored by Perspective. The resulted evaluation dataset consists of 203 prompts. For each
of these prompts, 20 completions are generated. In
the GPT2-medium + prompt engineering setting,
we prepend each prompt with the guiding sentence
“This is a non-toxic comment:”.
Topic Control
We experiment with the AGNews
dataset and DBPedia dataset .
The number of preﬁxes is 4 and 14, respectively.
The prompts used for evaluation are the same as
those in the PPLM experiment . For each of the 20 prompts, 45 completions
are generated. Same as that in GeDi, we split each
of the original training datasets in half. One half is
used to train preﬁxes, while the other half is used
to train a RoBERTa topic classiﬁer for topic relevance evaluation. In the GPT2-medium + prompt
engineering setting, the guiding sentence follows
1 
2 
the template “The following is about [TOPIC]”.
We do not compare with PPLM in the topic control task since PPLM uses a bag-of-words attribute
model to do topic control, where the 7 predeﬁned
topics are different from the topics in the AGNews
dataset or the DBPedia dataset.
All the experiments are conducted on NVIDIA
Tesla V100 GPUs. The detailed hyper-parameters
for each experiment are listed in appendix A.
In the unsupervised setting, GPT2-medium +
prompt engineering shows controllability on sentiment control (Table 1) and topic control (Table 3).
However, this method does not work on the detoxi-
ﬁcation task (Table 2). Our unsupervised method
signiﬁcantly lowers the toxicity on the detoxiﬁcation task and the ablation study shows that the contrastive loss Lc is crucial. On the sentiment control
task, our unsupervised method does not achieve
good attribute alignment when the target sentiment
is negative, but it performs well when the target
sentiment is positive. One possible reason is that
compared with the differences between toxic and
normal sentences, the difference between positive
sentiment and negative sentiment is more subtle,
so it is more challenging for the GPT2 encoder in
our unsupervised model to accurately separate the
unlabeled data into two sentiments. As a result, the
encoder’s implicit criterion to categorize the input
text may not be exactly the sentiment, which is also
the reason that after removing the contrastive loss
Lc in the unsupervised loss function, the attribute
relevance on the negative sentiment is higher while
that on the positive sentiment is lower.
In the supervised setting with full data, our supervised method consistently achieves better controllability than PPLM while maintaining the linguistic
quality of the generations (Table 1, 2). Although
GeDi achieves a high attribute alignment score on
the three tasks, it severely sacriﬁces the linguistic
quality, as indicated by the high perplexity. In the
few-shot setting, where the number of labeled training examples is reduced to 1000 or 24 examples per
attribute, our supervised method can still maintain
good controllability on the three tasks, showing the
robustness of our method to the size of the training
Ablation study shows the importance of the discriminative loss Ld in our supervised method. As
mentioned in section 3, training without Ld is
equivalent to preﬁx-tuning. Comparing the results
of Ours−Ld and GPT2-medium show that directly
using preﬁx-tuning can achieve controllability on
the sentiment or the topic. However, it is less effective on detoxiﬁcation. The reason is that different
from topic control or sentiment control, detoxiﬁcation requires the model to avoid generating some
words or phrases according to the context, which
can not be achieved by preﬁx-tuning. Ld ﬁlls this
gap by increasing p(x|y) and lowering p(x|¯y) at
the same time. Therefore, incorporating Ld is of
critical importance to the detoxiﬁcation task. In
the DBPedia topic control task, adding Ld also
achieves a large improvement on attribute alignment. The number of attributes in this task is much
larger than that in the other tasks, so incorporating
Ld can effectively push the preﬁxes to capture the
unique features of each topic.
We compare the average inference speed of our
methods with the baselines (Table 5). The inference speed of PPLM is several dozen times slower
than that of the original GPT2 model. GeDi’s inference speed is much faster than that of PPLM. The
inference speed of our method is the closest to that
of the original GPT2.
Human Evaluation
Besides automatic evaluation, we also conduct human evaluations on Amazon Mechanical Turk to
compare the performance of the baselines and our
methods. In each task, workers are presented with
a prompt along with the completions generated
by different methods. Workers are instructed to
answer two questions:“Which one has the best
linguistic quality?” and “The target attribute is
[ATT]. Which one aligns best with the target attribute?”. [ATT] is the control attribute used when
generating the completions. In order to evaluate the
linguistic quality and the attribute alignment separately, the workers are instructed not to consider the
control aspect or the factual errors when answering
the ﬁrst question and not to consider the linguistic
quality when answering the second question. The
user interface provided to the workers is shown
in the appendix (Figure 5). We conduct human
evaluations on the results of the sentiment control
experiment and those of the AGNews topic control
experiment separately. 100 tasks are randomly sampled from the results of each control experiment.
Each task is assigned to 3 different Mechanical
Turk workers and the annotations are aggregated by
majority voting. To ensure data quality, we restrict
the workers to be in Canada or United States with
Att. Rel. %↑
Att. Rel. %↑
Unsupervised training
GPT2-medium
+ prompt engineering
Supervised training (few-shot learning)
Ours (24 samples)
Ours (1k samples)
Supervised training (using full data)
−Ld (preﬁx-tuning)
Table 1: Results on sentiment control. “PPL.”: perplexity scores.
“Att. Rel.”: attribute relevance. “−Lc / −Ld”: ablating loss terms
as described in Eq. 8 and Eq. 3. Ours −Ld is equivalent to preﬁxtuning .
Unsupervised training
GPT2-medium
+ prompt engineering
Supervised training (few-shot learning)
Ours (24 samples)
Ours (1k samples)
Supervised training (using full data)
−Ld (preﬁx-tuning)
Results on detoxiﬁcation.
“Tox.”: toxicity. “−Lc / −Ld”: ablating
loss terms as in Eq. 8 and Eq. 3. Ours−
Ld is equivalent to preﬁx-tuning .
PPL.↓Att. Rel. %↑PPL.↓Att. Rel. %↑
Unsupervised training
GPT2-medium
+ prompt engineering
Supervised training (few-shot learning)
Ours (24 samples)
Ours (1k samples)
Supervised training (using full data)
−Ld (preﬁx-tuning)
Table 3: Results on topic control. “−Ld”: ablating loss terms as
described in Eq. 3. Ours −Ld is equivalent to preﬁx-tuning.
Att.↑Lin.↑Att.↑Lin.↑
GPT2 + prompt
engineering
Table 4: Human evaluation on sentiment
control and AGNews topic control. The
values in the table are the ratio of each
method selected in the attribute alignment
(Att.) questions and the linguistic quality
(Lin.) questions separately.
Time Cost (second)↓
GPT2-medium
Table 5: The average time for generating a completion.
a HIT approval rate higher than 95%. In total, 81
workers participated in the human evaluation. For
the sentiment control task, we compare the results
of GPT2-medium + prompt engineering, PPLM,
GeDi, and our supervised method (with full training dataset). For the AGNews topic control task,
PPLM is not evaluated as explained above. The
results are shown in Table 4. The inter-annotator
agreement on the sentiment task and the AGNews
task is 0.39 and 0.30 in Fleiss’ κ, respectively. Appendix B lists other details of the human evaluation.
In the sentiment control task, the result of human evaluation on linguistic quality is generally
consistent with the result of automatic evaluation.
However, different from the result of the automatic evaluation, annotators are more inclined to
select Ours and GPT2 + prompt engineering when
evaluating attribute alignment. Although the annotators are instructed not to consider linguistic
quality when evaluating sentiment alignment, they
tend to select the one with better linguistic quality
when multiple completions exhibits equally good
attribute alignment. In the AGNews topic control
task, the result of human evaluation on attribute
alignment is generally consistent with the result of
automatic evaluation. However, in more than half
of the linguistic quality questions, the annotators
select Ours, although GPT2-medium + prompt engineering achieves lower perplexity than Ours. On
inspection, we ﬁnd that GPT2-medium + prompt
Senti. Rel. %↑
Topic Rel. %↑
Senti. Rel. %↑
Topic Rel. %↑
GPT2-medium
+ prompt engineering
Ours (concatenation)
Ours (semi-supervised)
Table 6: Experimental results of the multi-aspect control task. “PPL.”: perplexity scores. “Senti. Rel.”: sentiment
relevance. “Topic Rel.”: topic relevance. “−Ld / −Lenc”: ablating loss terms as described in Eq. 3 and Eq. 12.
engineering in this task exhibits a more severe repetition problem compared to that in the sentiment
control task. This inconsistency shows the limitation of using automatic evaluations, as alluded to
in Welbl et al. .
Both human evaluation and automatic evaluation
show that the linguistic quality of GeDi is inferior
to that of the other methods. One possible reason
is the length of the prompt. In the original experiment in Krause et al. , each prompt is at least
150 characters for sentiment control evaluation and
at least 30 characters for topic control evaluation.
However, we use the prompts as in Dathathri et al.
 , where the average prompt length is 11.8
characters for sentiment control evaluation and 14.5
characters for topic control evaluation. The generated examples are shown in the appendix (Table 7).
Multi-Aspect Control
Our method can also be applied to multi-aspect
control. Directly applying our supervised method
to multi-aspect control requires training examples
with multi-aspect labels. However, such datasets
are usually not readily available since most of the
datasets are labeled for a single task. Although
multi-aspect labeled examples are limited, we have
training examples with single-aspect labels from
multiple aspects, which can be utilized to achieve
multi-aspect control. One method is to train a set
of preﬁxes for each aspect separately using our
supervised method and then concatenate the pre-
ﬁxes from different aspects for generation. This
method is denoted as Ours (concatenation) in the
result table. Another method is to train the pre-
ﬁxes of multiple aspects simultaneously by considering each single-aspect labeled example as partially labeled. We use a semi-supervised method for
training, which is a combination of our supervised
method and unsupervised method in Section 3. The
model structure is the same as in the unsupervised
method (Figure 4). The loss function is as follows:
L = ω1LLM + ω2Ld + ω3Lenc
Lenc = −log q(zsup = y|x)
q(z|x) = σ(−∥Enc(x) −Hθ∥2)
where the latent variable z is the concatenation of
the latent variable of each aspect, including both the
supervised aspects and the unsupervised ones z =
[zsup; zuns]. Lenc is used to train the encoder. It is
introduced because the partially labeled examples
imply the ground truth indexes of the preﬁxes in the
labeled aspect, providing supervision for both the
preﬁx and the encoder. σ is the softmax function.
We experiment with controlling the following
two aspects simultaneously: sentiment and topic.
We use the binary sentiment dataset from Amazon
review and the DBPedia topic
dataset. The prompts used for evaluation are the
same as those in the topic control experiment. For
each of the 20 prompts, 45 completions are generated. In the GPT2-medium + prompt engineering
setting, the guiding sentence follows the template
“This is a [SENTIMENT] review on [TOPIC]:”. In
Ours (concatenation), the sentiment preﬁxes and
the topic preﬁxes are trained separately using our
supervised method and then concatenated as multiaspect preﬁxes. In Ours (semi-supervised), we
reuse the preﬁxes trained in the single-aspect control tasks to initialize Hθ. All the experiments are
conducted on NVIDIA Tesla V100 GPUs. The
hyper-parameters are listed in appendix A.
Experimental results on multi-aspect control (Table 6) show that simply concatenating the preﬁxes
trained for single-aspect control can effectively control the sentiment and topic simultaneously, and our
experiments show that the order of the preﬁxes does
not impact the result. On the other hand, training
using the combination of our supervised and unsupervised methods can further improve the attribute
alignment without sacriﬁcing too much linguistic
quality. Same as the observations stated in Section 4.1.2, removing the discriminative loss Ld will
signiﬁcantly degrade the attribute relevance, especially the topic relevance. Removing the encoder
loss Lenc may achieve higher overall attribute relevance at the cost of linguistic quality, indicated
by a higher perplexity. We present the generated
examples in the appendix (Table 7).
Conclusion
We propose a novel framework for controllable
GPT2 generation with frozen LMs, which utilizes
contrastive preﬁxes to guide generation. Experimental results show that our framework can not
only successfully guide generation from a single
aspect but also achieve promising results on multiaspect control tasks. Besides the control tasks we
experimented with, our proposed framework can
be freely applied to other desired attributes.
Ethical Considerations
With our controlling methods, it is not one hundred
percent guaranteed that the generations will have
the desired attributes, but the probability for the
generations to exhibit the desired attributes will
increase. When applied to detoxiﬁcation, although
the probability of toxicity degeneration will decrease, the controlled language model may still
produce unsafe text. We would like to clarify that
the offensive language generated by the language
model controlled with our methods does not represent any opinion of the authors.
Besides, our proposed methods control the highlevel attributes of the generation, such as toxicity,
topic, or sentiment, but there is no guarantee of
factual accuracy for the generation, which is a wellknown problem in NLG models. Our controlling
methods may not be used for factual accuracy controlling. While reducing hallucination is not the
focus of this work, knowledge-grounded generation
techniques can be used to alleviate this problem.