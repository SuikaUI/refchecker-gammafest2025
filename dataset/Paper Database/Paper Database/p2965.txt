Towards Explainable Artiﬁcial Intelligence
Wojciech Samek1 and Klaus-Robert M¨uller2,3,4
1 Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany
 
2 Technische Universit¨at Berlin, 10587 Berlin, Germany
3 Korea University, Anam-dong, Seongbuk-gu, Seoul 02841, Korea
4 Max Planck Institute for Informatics, Saarbr¨ucken 66123, Germany
 
Abstract. In recent years, machine learning (ML) has become a key
enabling technology for the sciences and industry. Especially through
improvements in methodology, the availability of large databases and increased computational power, today’s ML algorithms are able to achieve
excellent performance (at times even exceeding the human level) on an
increasing number of complex tasks. Deep learning models are at the
forefront of this development. However, due to their nested non-linear
structure, these powerful models have been generally considered “black
boxes”, not providing any information about what exactly makes them
arrive at their predictions. Since in many applications, e.g., in the medical domain, such lack of transparency may be not acceptable, the development of methods for visualizing, explaining and interpreting deep
learning models has recently attracted increasing attention. This introductory paper presents recent developments and applications in this ﬁeld
and makes a plea for a wider use of explainable learning algorithms in
Keywords: Explainable Artiﬁcial Intelligence · Model Transparency ·
Deep Learning · Neural Networks · Interpretability
Introduction
Today’s artiﬁcial intelligence (AI) systems based on machine learning excel in
many ﬁelds. They not only outperform humans in complex visual tasks 
or strategic games , but also became an indispensable part of our every
day lives, e.g., as intelligent cell phone cameras which can recognize and track
faces , as online services which can analyze and translate written texts 
or as consumer devices which can understand speech and generate human-like
answers . Moreover, machine learning and artiﬁcial intelligence have become
authenticated
publication
 In: W. Samek et al. (Eds.)
Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture
Notes in Computer Science, vol. 11700, pp. 5-22. Springer, Cham .
W. Samek and K.-R. M¨uller
indispensable tools in the sciences for tasks such as prediction, simulation or exploration . These immense successes of AI systems mainly became
possible through improvements in deep learning methodology , the availability of large databases and computational gains obtained with powerful
GPU cards .
Despite the revolutionary character of this technology, challenges still exist
which slow down or even hinder the prevailance of AI in some applications.
Examplar challenges are (1) the large complexity and high energy demands of
current deep learning models , which hinder their deployment in resource
restricted environments and devices, (2) the lack of robustness to adversarial
attacks , which pose a severe security risk in application such as autonomous
driving5, and (3) the lack of transparency and explainability , which
reduces the trust in and the veriﬁability of the decisions made by an AI system.
This paper focuses on the last challenge. It presents recent developments in
the ﬁeld of explainable artiﬁcial intelligence and aims to foster awareness for the
advantages–and at times–also for the necessity of transparent decision making
in practice. The historic second Go match between Lee Sedol and AlphaGo 
nicely demonstrates the power of today’s AI technology, and hints at its enormous potential for generating new knowledge from data when being accessible
for human interpretation. In this match AlphaGo played a move, which was
classiﬁed as “not a human move” by a renowned Go expert, but which was the
deciding move for AlphaGo to win the game. AlphaGo did not explain the move,
but the later play unveiled the intention behind its decision. With explainable AI
it may be possible to also identify such novel patterns and strategies in domains
like health, drug development or material sciences, moreover, the explanations
will ideally let us comprehend the reasoning of the system and understand why
the system has decided e.g. to classify a patient in a speciﬁc manner or associate certain properties with a new drug or material. This opens up innumerable
possibilities for future research and may lead to new scientiﬁc insights.
The remainder of the paper is organized as follows. Section 2 discusses the
need for transparency and trust in AI. Section 3 comments on the diﬀerent types
of explanations and their respective information content and use in practice.
Recent techniques of explainable AI are brieﬂy summarized in Section 4, including methods which rely on simple surrogate functions, frame explanation as an
optimization problem, access the model’s gradient or make use of the model’s
internal structure. The question of how to objectively evaluate the quality of
explanations is addressed in Section 5. The paper concludes in Section 6 with a
discussion on general challenges in the ﬁeld of explainable AI.
5 The authors of showed that deep models can be easily fooled by physical-world
attacks. For instance, by putting speciﬁc stickers on a stop sign one can achieve that
the stop sign is not recognized by the system anymore.
1. Towards Explainable Artiﬁcial Intelligence
Need for Transparency and Trust in AI
Black box AI systems have spread to many of today’s applications. For machine
learning models used, e.g., in consumer electronics or online translation services,
transparency and explainability are not a key requirement as long as the overall
performance of these systems is good enough. But even if these systems fail,
e.g., the cell phone camera does not recognize a person or the translation service
produces grammatically wrong sentences, the consequences are rather unspectacular. Thus, the requirements for transparency and trust are rather low for
these types of AI systems. In safety critical applications the situation is very
diﬀerent. Here, the intransparency of ML techniques may be a limiting or even
disqualifying factor. Especially if single wrong decisions can result in danger to
life and health of humans (e.g., autonomous driving, medical domain) or signiﬁcant monetary losses (e.g., algorithmic trading), relying on a data-driven system
whose reasoning is incomprehensible may not be an option. This intransparency
is one reason why the adoption of machine learning to domains such as health is
more cautious than the usage of these models in the consumer, e-commerce or
entertainment industry.
In the following we discuss why the ability to explain the decision making of
an AI system helps to establish trust and is of utmost importance, not only in
medical or safety critical applications. We refer the reader to for a discussion
of the challenges of transparency.
Explanations Help to Find “Clever Hans” Predictors
Clever Hans was a horse that could supposedly count and that was considered a
scientiﬁc sensation in the years around 1900. As it turned out later, Hans did not
master the math but in about 90 percent of the cases, he was able to derive the
correct answer from the questioner’s reaction. Analogous behaviours have been
recently observed in state-of-the-art AI systems . Also here the algorithms
have learned to use some spurious correlates in the training and test data and
similarly to Hans predict right for the ‘wrong’ reason.
For instance, the authors of showed that the winning method of the
PASCAL VOC competition was often not detecting the object of interest,
but was utilizing correlations or context in the data to correctly classify an image. It recognized boats by the presence of water and trains by the presence
of rails in the image, moreover, it recognized horses by the presence of a copyright watermark6. The occurrence of the copyright tags in horse images is a
clear artifact in the dataset, which had gone unnoticed to the organizers and
participants of the challenge for many years. It can be assumed that nobody
has systematically checked the thousands images in the dataset for this kind
of artifacts (but even if someone did, such artifacts may be easily overlooked).
Many other examples of “Clever Hans” predictors have been described in the
6 The PASCAL VOC images have been automatically crawled from ﬂickr and especially the horse images were very often copyrighted with a watermark.
W. Samek and K.-R. M¨uller
literature. For instance, show that current deep neural networks are distinguishing the classes “Wolf” and “Husky” mainly by the presence of snow in
the image. The authors of demonstrate that deep models overﬁt to padding
artifacts when classifying airplanes, whereas show that a model which was
trained to distinguish between 1000 categories, has not learned dumbbells as
an independent concept, but associates a dumbbell with the arm which lifts it.
Such “Clever Hans” predictors perform well on their respective test sets, but will
certainly fail if deployed to the real-world, where sailing boats may lie on a boat
trailer, both wolves and huskies can be found in non-snow regions and horses do
not have a copyright sign on them. However, if the AI system is a black box, it
is very diﬃcult to unmask such predictors. Explainability helps to detect these
types of biases in the model or the data, moreover, it helps to understand the
weaknesses of the AI system (even if it is not a “Clever Hans” predictor). In the
extreme case, explanations allow to detect the classiﬁer’s misbehaviour (e.g., the
focus on the copyright tag) from a single test image7. Since understanding the
weaknesses of a system is the ﬁrst step towards improving it, explanations are
likely to become integral part of the training and validation process of future AI
Explanations Foster Trust and Veriﬁability
The ability to verify decisions of an AI system is very important to foster trust,
both in situations where the AI system has a supportive role (e.g., medical diagnosis) and in situations where it practically takes the decisions (e.g., autonomous
driving). In the former case, explanations provide extra information, which, e.g.,
help the medical expert to gain a comprehensive picture of the patient in order
to take the best therapy decision. Similarly to a radiologist, who writes a detailed
report explaining his ﬁndings, a supportive AI system should in detail explain
its decisions rather than only providing the diagnosis to the medical expert. In
cases where the AI system itself is deciding, it is even more critical to be able to
comprehend the reasoning of the system in order to verify that it is not behaving like Clever Hans, but solves the problem in a robust and safe manner. Such
veriﬁcations are required to build the necessary trust in every new technology.
There is also a social dimension of explanations. Explaining the rationale
behind one’s decisions is an important part of human interactions . Explanations help to build trust in a relationship between humans, and should therefore
be also part of human-machine interactions . Explanations are not only an inevitable part of human learning and education (e.g., teacher explains solution to
student), but also foster the acceptance of diﬃcult decisions and are important
for informed consent (e.g., doctor explaining therapy to patient). Thus, even if
not providing additional information for verifying the decision, e.g., because the
patient may have no medical knowledge, receiving explanations usually make us
feel better as it integrates us into the decision-making process. An AI system
which interacts with humans should therefore be explainable.
7 Traditional methods to evaluate classiﬁer performance require large test datasets.
1. Towards Explainable Artiﬁcial Intelligence
Explanations are a Prerequisite for New Insights
AI systems have the potential to discover patterns in data, which are not accessible to the human expert. In the case of the Go game, these patterns can be
new playing strategies . In the case of scientiﬁc data, they can be unknown
associations between genes and diseases , chemical compounds and material
properties or brain activations and cognitive states . In the sciences,
identifying these patterns, i.e., explaining and interpreting what features the AI
system uses for predicting, is often more important than the prediction itself, because it unveils information about the biological, chemical or neural mechanisms
and may lead to new scientiﬁc insights.
This necessity to explain and interpret the results has led to a strong dominance of linear models in scientiﬁc communities in the past (e.g. ). Linear
models are intrinsically interpretable and thus easily allow to extract the learned
patterns. Only recently, it became possible to apply more powerful models such
as deep neural networks without sacriﬁcing interpretability. These explainable
non-linear models have already attracted attention in domains such as neuroscience , health , autonomous driving , drug design 
and physics and it can be expected that they will play a pivotal role in
future scientiﬁc research.
Explanations are Part of the Legislation
The inﬁltration of AI systems into our daily lives poses a new challenge for the
legislation. Legal and ethical questions regarding the responsibility of AI systems
and their level of autonomy have recently received increased attention .
But also anti-discrimination and fairness aspects have been widely discussed in
the context of AI . The EU’s General Data Protection Regulation (GDPR)
has even added the right to explanation to the policy in Articles 13, 14 and 22,
highlighting the importance of human-understandable interpretations derived
from machine decisions. For instance, if a person is being rejected for a loan by
the AI system of a bank, in principle, he or she has the right to know why the
system has decided in this way, e.g., in order to make sure that the decision is
compatible with the anti-discrimination law or other regulations. Although it is
not yet clear how these legal requirements will be implemented in practice, one
can be sure that transparency aspects will gain in importance as AI decisions
will more and more aﬀect our daily lives.
Diﬀerent Facets of an Explanation
Recently proposed explanation techniques provide valuable information about
the learned representations and the decision-making of an AI system. These
explanations may diﬀer in their information content, their recipient and their
purpose. In the following we describe the diﬀerent types of explanations and
comment on their usefulness in practice.
W. Samek and K.-R. M¨uller
Diﬀerent recipients may require explanations with diﬀerent level of detail and
with diﬀerent information content. For instance, for users of AI technology it may
be suﬃcient to obtain coarse explanations, which are easy to interpret, whereas
AI researchers and developers would certainly prefer explanations, which give
them deeper insights into the functioning of the model.
In the case of image classiﬁcation such simple explanations could coarsely
highlight image regions, which are regarded most relevant for the model. Several
preprocessing steps, e.g., smoothing, ﬁltering or contrast normalization, could be
applied to further improve the visualization quality. Although discarding some
information, such coarse explanations could help the ordinary user to foster
trust in AI technology. On the other hand AI researchers and developers, who
aim to improve the model, may require all the available information, including
negative evidence, about the AI’s decision in the highest resolution (e.g., pixelwise explanations), because only this complete information gives detailed insights
into the (mal)functioning of the model.
One can easily identify further groups of recipients, which are interested in
diﬀerent types of explanations. For instance, when applying AI to the medical
domain these groups could be patients, doctors and institutions. An AI system
which analyzes patient data could provide simple explanations to the patients,
e.g., indicating too high blood sugar, while providing more elaborate explanations to the medical personal, e.g., unusual relation between diﬀerent blood
parameters. Furthermore, institutions such as hospitals or the FDA might be
less interested in understanding the AI’s decisions for individual patients, but
would rather prefer to obtain global or aggregated explanations, i.e., patterns
which the AI system has learned after analyzing many patients.
Information Content
Diﬀerent types of explanation provide insights into diﬀerent aspects of the model,
ranging from information about the learned representations to the identiﬁcation
of distinct prediction strategies and the assessment of overall model behaviour.
Depending on the recipient of the explanations and his or her intent, it may be
advantageous to focus on one particular type of explanation. In the following we
brieﬂy describe four diﬀerent types of explanations.
1. Explaining learned representations: This type of explanation aims to
foster the understanding of the learned representations, e.g., neurons of
a deep neural network. Recent work investigates the role of single
neurons or group of neurons in encoding certain concepts. Other methods
 aim to interpret what the model has learned by building prototypes that are representative of the abstract learned concept. These methods, e.g., explain what the model has learned about the category “car” by
generating a prototypical image of a car. Building such a prototype can
be formulated within the activation maximization framework and has been
1. Towards Explainable Artiﬁcial Intelligence
shown to be an eﬀective tool for studying the internal representation of a
deep neural network.
2. Explaining individual predictions: Other types of explanations provide
information about individual predictions, e.g., heatmaps visualizing which
pixels have been most relevant for the model to arrive at its decision 
or heatmaps highlighting the most sensitive parts of an input . Such
explanations help to verify the predictions and establish trust in the correct
functioning on the system. Layer-wise Relevance Propagation (LRP) 
provides a general framework for explaining individual predictions, i.e., it is
applicable to various ML models, including neural networks , LSTMs ,
Fisher Vector classiﬁers and Support Vector Machines . Section 4
gives an overview over recently proposed methods for computing individual
explanations.
3. Explaining model behaviour: This type of explanations go beyond the
analysis of individual predictions towards a more general understanding of
model behaviour, e.g., identiﬁcation of distinct prediction strategies. The
spectral relevance analysis (SpRAy) approach of computes such meta
explanations by clustering individual heatmaps. Each cluster then represents
a particular prediction strategy learned by the model. For instance, the authors of identify four clusters when classifying “horse” images with the
Fisher Vector classiﬁer trained on the PASCAL VOC 2007 dataset ,
namely (1) detect the horse and rider, 2) detect a copyright tag in portrait
oriented images, 3) detect wooden hurdles and other contextual elements of
horseback riding, and 4) detect a copyright tag in landscape oriented images.
Such explanations are useful for obtaining a global overview over the learned
strategies and detecting “Clever Hans” predictors .
4. Explaining with representative examples: Another class of methods
interpret classiﬁers by identifying representative training examples .
This type of explanations can be useful for obtaining a better understanding
of the training dataset and how it inﬂuences the model. Furthermore, these
representative examples can potentially help to identify biases in the data
and make the model more robust to variations of the training dataset.
Besides the recipient and information content it is also important to consider the
purpose of an explanation. Here we can distinguish two aspects, namely (1) the
intent of the explanation method (what speciﬁc question does the explanation
answer) and (2) our intent (what do we want to use the explanation for).
Explanations are relative and it makes a huge diﬀerence whether their intent
is to explain the prediction as is (even if it is incorrect), whether they aim to
visualize what the model “thinks” about a speciﬁc class (e.g., the true class) or
whether they explain the prediction relative to another alternative (“why is this
image classiﬁed as car and not as truck”). Methods such as LRP allow to answer
all these diﬀerent questions, moreover, they also allow to adjust the amount of
positive and negative evidence in the explanations, i.e., visualize what speaks
W. Samek and K.-R. M¨uller
for (positive evidence) and against (negative evidence) the prediction. Such ﬁnegrained explanations foster the understanding of the classiﬁer and the problem
Furthermore, there may be diﬀerent goals for using the explanations beyond
visualization and veriﬁcation of the prediction. For instance, explanations can
be potentially used to improve the model, e.g., by regularization . Also since
explanations provide information about the (relevant parts of the) model, they
can be potentially used for model compression and pruning. Many other uses
(certiﬁcation of the model, legal use) of explanations can be thought of, but the
details remain future work.
Methods of Explainable AI
This section gives an overview over diﬀerent approaches to explainable AI, starting with techniques which are model-agnostic and rely on a simple surrogate
function to explain the predictions. Then, we discuss methods which compute
explanations by testing the model’s response to local perturbations (e.g., by utilizing gradient information or by optimization). Subsequently, we present very
eﬃcient propagation-based explanation techniques which leverage the model’s
internal structure. Finally, we consider methods which go beyond individual explanations towards a meta-explanation of model behaviour.
This section is not meant to be a complete survey of explanation methods,
but it rather summarizes the most important developments in this ﬁeld. Some
approaches to explainable AI, e.g., methods which ﬁnd inﬂuencial examples ,
are not discussed in this section.
Explaining with Surrogates
Simple classiﬁers such as linear models or shallow decision trees are intrinsically
interpretable, so that explaining its predictions becomes a trivial task. Complex
classiﬁers such as deep neural networks or recurrent models on the other hand
contain several layers of non-linear transformations, which largely complicates
the task of ﬁnding what exactly makes them arrive at their predictions.
One approach to explain the predictions of complex models is to locally
approximate them with a simple surrogate function, which is interpretable. A
popular technique falling into this category is Local Interpretable Model-agnostic
Explanations (LIME) . This method samples in the neighborhood of the
input of interest, evaluates the neural network at these points, and tries to ﬁt
the surrogate function such that it approximates the function of interest. If the
input domain of the surrogate function is human-interpretable, then LIME can
even explain decisions of a model which uses non-interpretable features. Since
LIME is model agnostic, it can be applied to any classiﬁer, even without knowing
its internals, e.g., architecture or weights of a neural network classiﬁer. One
major drawback of LIME is its high computational complexity, e.g., for state-ofthe-art models such as GoogleNet it requires several minutes for computing the
explanation of a single prediction .
1. Towards Explainable Artiﬁcial Intelligence
Similar to LIME which builds a model for locally approximating the function
of interest, the SmoothGrad method samples the neighborhood of the input
to approximate the gradient. Also SmoothGrad does not leverage the internals
of the model, however, it needs access to the gradients. Thus, it can also be
regarded as a gradient-based explanation method.
Explaining with Local Perturbations
Another class of methods construct explanations by analyzing the model’s response to local changes. This includes methods which utilize the gradient information as well as perturbation- and optimization-based approaches.
Explanation methods relying on the gradient of the function of interest 
have a long history in machine learning. One example is the so-called Sensitivity
Analysis (SA) . Although being widely used as explanation methods,
SA technically explains the change in prediction instead of the prediction itself.
Furthermore, SA has been shown to suﬀer from fundamental problems such as
gradient shattering and explanation discontinuities, and is therefore considered
suboptimal for explanation of today’s AI models . Variants of Sensitivity
Analysis exist which tackle some of these problems by locally averaging the
gradients or integrating them along a speciﬁc path .
Perturbation-based explanation methods explicitly test the model’s
response to more general local perturbations. While the occlusion method of 
measures the importance of input dimensions by masking parts of the input, the
Prediction Diﬀerence Analysis (PDA) approach of uses conditional sampling
within the pixel neighborhood of an analyzed feature to eﬀectively remove information. Both methods are model-agnostic, i.e., can be applied to any classiﬁer,
but are computationally not very eﬃcient, because the function of interest (e.g.,
neural network) needs to be evaluated for all perturbations.
The meaningful perturbation method of is another model-agnostic
technique to explaining with local perturbations. It regards explanation as a
meta prediction task and applies optimization to synthesize the maximally informative explanations. The idea to formulate explanation as an optimization
problem is also used by other methods. For instance, the methods 
aim to interpret what the model has learned by building prototypes that are
representative of the learned concept. These prototypes are computed within
the activation maximization framework by searching for an input pattern that
produces a maximum desired model response. Conceptually, activation maximization is similar to the meaningful perturbation approach of . While
the latter ﬁnds a minimum perturbation of the data that makes f(x) low, activation maximization ﬁnds a minimum perturbation of the gray image that makes
f(x) high. The costs of optimization can make these methods computationally
very demanding.
W. Samek and K.-R. M¨uller
Propagation-Based Approaches (Leveraging Structure)
Propagation-based approaches to explanation are not oblivious to the model
which they explain, but rather integrate the internal structure of the model into
the explanation process.
Layer-wise Relevance Propagation (LRP) is a propagation-based explanation framework, which is applicable to general neural network structures,
including deep neural networks , LSTMs , and Fisher Vector classiﬁers
 . LRP explains individual decisions of a model by propagating the prediction
from the output to the input using local redistribution rules. The propagation
process can be theoretically embedded in the deep Taylor decomposition framework . More recently, LRP was extended to a wider set of machine learning
models, e.g., in clustering or anomaly detection , by ﬁrst transforming
the model into a neural network (‘neuralization’) and then applying LRP to
explain its predictions. The leveraging of the model structure together with the
use of appropriate (theoretically-motivated) propagation rules, enables LRP to
deliver good explanations at very low computational cost (one forward and one
backward pass). Furthermore, the generality of the LRP framework allows also to
express other recently proposed explanation techniques, e.g., . Since LRP
does not rely on gradients, it does not suﬀer from problems such as gradient
shattering and explanation discontinuities .
Other popular explanation methods leveraging the model’s internal structure
are Deconvolution and Guided Backprogagation . In contrast to LRP,
these methods do not explain the prediction in the sense “how much did the
input feature contribute to the prediction”, but rather identify patterns in input
space, that relate to the analyzed network output.
Many other explanation methods have been proposed in the literature which
fall into the “leveraging structure” category. Some of these methods use heuristics to guide the redistribution process , others incorporate an optimization
step into the propagation process . The iNNvestigate toolbox provides
an eﬃcient implementation for many of these propagation-based explanation
Meta-Explanations
Finally, individual explanations can be aggregated and analyzed to identify general patterns of classiﬁer behavior. A recently proposed method, spectral relevance analysis (SpRAy) , computes such meta explanations by clustering
individual heatmaps. This approach allows to investigate the predictions strategies of the classiﬁer on the whole dataset in a (semi-)automated manner and to
systematically ﬁnd weak points in models or training datasets.
Another type of meta-explanation aims to better understand the learned representations and to provide interpretations in terms of human-friendly concepts.
For instance, the network dissection approach of evaluates the semantics
of hidden units, i.e., quantify what concepts these neurons encode. Other recent
work provides explanations in terms of user-deﬁned concepts and tests to
which degree these concepts are important for the prediction.
1. Towards Explainable Artiﬁcial Intelligence
Evaluating Quality of Explanations
The objective assessment of the quality of explanations is an active ﬁeld of research. Many eﬀorts have been made to deﬁne quality measures for heatmaps
which explain individual predictions of an AI model. This section gives an
overview over the proposed approaches.
A popular measure for heatmap quality is based on perturbation analysis
 . The assumption of this evaluation metric is that the perturbation of
relevant (according to the heatmap) input variables should lead to a steeper
decline of the prediction score than the perturbation of input dimensions which
are of lesser importance. Thus, the average decline of the prediction score after
several rounds of perturbation (starting from the most relevant input variables)
deﬁnes an objective measure of heatmap quality. If the explanation identiﬁes the
truly relevant input variables, then the decline should be large. The authors of
 recommend to use untargeted perturbations (e.g., uniform noise) to allow fair
comparison of diﬀerent explanation methods. Although being very popular, it is
clear that perturbation analysis can not be the only criterion to evaluate explanation quality, because one could easily design explanations techniques which
would directly optimize this criterion. Examples are occlusion methods which
were used in , however, they have been shown to be inferior (according to
other quality criteria) to explanation techniques such as LRP .
Other studies use the ‘pointing game” to evaluate the quality of a
heatmap. The goal of this game is to evaluate the discriminativeness of the
explanations for localizing target objects, i.e., it is compared if the most relevant point of the heatmap lies on the object of designated category. Thus, these
measures assume that the AI model will focus most attention on the object of
interest when classifying it, therefore this should be reﬂected in the explanation.
However, this assumption may not always be true, e.g., “Clever Hans” predictors may rather focus on context than of the object itself, irrespectively of
the explanation method used. Thus, their explanations would be evaluated as
poor quality according to this measure although they truly visualize the model’s
prediction strategy.
Task speciﬁc evaluation schemes have also been proposed in the literature.
For example, use the subject-verb agreement task to evaluate explanations
of a NLP model. Here the model predicts a verb’s number and the explanations verify if the most relevant word is indeed the correct subject or a noun
with the predicted number. Other approaches to evaluation rely on human judgment . Such evaluation schemes relatively quickly become impractical if
evaluating a larger number of explanations.
A recent study proposes to objectively evaluate explanation for sequential
data using ground truth information in a toy task. The idea of this evaluation
metric is to add or subtract two numbers within an input sequence and measure
the correlation between the relevances assigned to the elements of the sequence
and the two input numbers. If the model is able to accurately perform the addition and subtraction task, then it must focus on these two numbers (other
W. Samek and K.-R. M¨uller
numbers in the sequence are random) and this must be reﬂected in the explanation.
An alternative and indirect way to evaluate the quality of explanations is to
use them for solving other tasks. The authors of build document-level representations from word-level explanations. The performance of these documentlevel representations (e.g., in a classiﬁcation task) reﬂect the quality of the wordlevel explanations. Another work uses explanation for reinforcement learning.
Many other functionally-grounded evaluations could be conceived such as
using explanations for compressing or pruning the neural network or training
student models in a teacher-student scenario.
Lastly, another promising approach to evaluate explanations is based on the
fulﬁllment of a certain axioms . Axioms are properties of an explanation that are considered to be necessary and should therefore be fulﬁlled.
Proposed axioms include relevance conservation , explanation continuity ,
sensitivity and implementation invariance . In contrast to the other quality measures discussed in this section, the fulﬁllment or non-fulﬁllment of certain
axioms can be often shown analytically, i.e., does not require empirical evaluations.
Challenges and Open Questions
Although signiﬁcant progress has been made in the ﬁeld of explainable AI in the
last years, challenges still exist both on the methods and theory side as well as
regarding the way explanations are used in practice. Researchers have already
started working on some of these challenges, e.g., the objective evaluation of
explanation quality or the use of explanations beyond visualization. Other open
questions, especially those concerning the theory, are more fundamental and
more time will be required to give satisfactory answers to them.
Explanation methods allow us to gain insights into the functioning of the
AI model. Yet, these methods are still limited in several ways. First, heatmaps
computed with today’s explanation methods visualize “ﬁrst-order” information,
i.e., they show which input features have been identiﬁed as being relevant for the
prediction. However, the relation between these features, e.g., whether they are
important on their own or only whether they occur together, remains unclear.
Understanding these relations is important in many applications, e.g., in the
neurosciences such higher-order explanations could help us to identify groups of
brain regions which act together when solving a speciﬁc task (brain networks)
rather than just identifying important single voxels.
Another limitation is the low abstraction level of explanations. Heatmaps
show that particular pixels are important without relating these relevance values
to more abstract concepts such as the objects or the scene displayed in the
image. Humans need to interpret the explanations to make sense them and
to understand the model’s behaviour. This interpretation step can be diﬃcult
and erroneous. Meta-explanations which aggregate evidence from these low-level
heatmaps and explain the model’s behaviour on a more abstract, more human
1. Towards Explainable Artiﬁcial Intelligence
understandable level, are desirable. Recently, ﬁrst approaches to aggregate lowlevel explanations and quantify the semantics of neural representations 
have been proposed. The construction of more advanced meta-explanations is a
rewarding topic for future research.
Since the recipient of explanations is ultimately the human user, the use
of explanations in human-machine interaction is an important future research
topic. Some works (e.g., ) have already started to investigate human factors in
explainable AI. Constructing explanations with the right user focus, i.e., asking
the right questions in the right way, is a prerequisite to successful human-machine
interaction. However, the optimization of explanations for optimal human usage
is still a challenge which needs further study.
A theory of explainable AI, with a formal and universally agreed deﬁnition of
what explanations are, is lacking. Some works made a ﬁrst step towards this goal
by developing mathematically well-founded explanation methods. For instance,
the authors of approach the explanation problem by integrating it into
the theoretical framework of Taylor decomposition. The axiomatic approaches
 constitute another promising direction towards the goal of developing
a general theory of explainable AI.
Finally, the use of explanations beyond visualization is a wide open
challenge. Future work will show how to integrate explanations into a larger
optimization process in order to, e.g., improve the model’s performance or
reduce its complexity.
Acknowledgements. This work was supported by the German Ministry
for Education and Research as Berlin Big Data Centre (01IS14013A), Berlin
Center for Machine Learning (01IS18037I) and TraMeExCo (01IS18056A).
Partial funding by DFG is acknowledged .
This work was also supported by the Institute for Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea
government .