Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715–1725,
Berlin, Germany, August 7-12, 2016. c⃝2016 Association for Computational Linguistics
Neural Machine Translation of Rare Words with Subword Units
Rico Sennrich and Barry Haddow and Alexandra Birch
School of Informatics, University of Edinburgh
{rico.sennrich,a.birch}@ed.ac.uk, 
Neural machine translation (NMT) models typically operate with a ﬁxed vocabulary, but translation is an open-vocabulary
Previous work addresses the
translation of out-of-vocabulary words by
backing off to a dictionary.
In this paper, we introduce a simpler and more effective approach, making the NMT model
capable of open-vocabulary translation by
encoding rare and unknown words as sequences of subword units. This is based on
the intuition that various word classes are
translatable via smaller units than words,
for instance names (via character copying
or transliteration), compounds (via compositional translation), and cognates and
loanwords (via phonological and morphological transformations). We discuss the
suitability of different word segmentation
techniques, including simple character ngram models and a segmentation based on
the byte pair encoding compression algorithm, and empirically show that subword
models improve over a back-off dictionary
baseline for the WMT 15 translation tasks
English→German and English→Russian
by up to 1.1 and 1.3 BLEU, respectively.
Introduction
Neural machine translation has recently shown
impressive results . We note that such
techniques make assumptions that often do not
hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and
target words because of variance in the degree of
morphological synthesis between languages, like
in our introductory compounding example. Also,
word-level models are unable to translate or generate unseen words. Copying unknown words into
the target text, as done by , is a reasonable strategy for names,
but morphological changes and transliteration is
often required, especially if alphabets differ.
We investigate NMT models that operate on the
level of subword units. Our main goal is to model
open-vocabulary translation in the NMT network
itself, without requiring a back-off model for rare
words. In addition to making the translation process simpler, we also ﬁnd that the subword models
achieve better accuracy for the translation of rare
words than large-vocabulary models and back-off
dictionaries, and are able to productively generate
new words that were not seen at training time. Our
analysis shows that the neural networks are able to
learn compounding and transliteration from subword representations.
This paper has two main contributions:
• We show that open-vocabulary neural ma-
chine translation is possible by encoding
(rare) words via subword units. We ﬁnd our
architecture simpler and more effective than
using large vocabularies and back-off dictionaries .
• We adapt byte pair encoding (BPE) , a compression algorithm, to the task
of word segmentation. BPE allows for the
representation of an open vocabulary through
a ﬁxed-size vocabulary of variable-length
character sequences, making it a very suitable word segmentation strategy for neural
network models.
Neural Machine Translation
We follow the neural machine translation architecture by Bahdanau et al. , which we will
brieﬂy summarize here. However, we note that our
approach is not speciﬁc to this architecture.
The neural machine translation system is implemented as an encoder-decoder network with recurrent neural networks.
The encoder is a bidirectional neural network
with gated recurrent units 
that reads an input sequence x = (x1, ..., xm)
and calculates a forward sequence of hidden
states (−→h 1, ..., −→h m), and a backward sequence
(←−h 1, ..., ←−h m). The hidden states −→h j and ←−h j are
concatenated to obtain the annotation vector hj.
The decoder is a recurrent neural network that
predicts a target sequence y = (y1, ..., yn). Each
word yi is predicted based on a recurrent hidden
state si, the previously predicted word yi−1, and
a context vector ci. ci is computed as a weighted
sum of the annotations hj. The weight of each
annotation hj is computed through an alignment
model αij, which models the probability that yi is
aligned to xj. The alignment model is a singlelayer feedforward neural network that is learned
jointly with the rest of the network through backpropagation.
A detailed description can be found in . Training is performed on a
parallel corpus with stochastic gradient descent.
For translation, a beam search with small beam
size is employed.
Subword Translation
The main motivation behind this paper is that
the translation of some words is transparent in
that they are translatable by a competent translator even if they are novel to him or her, based
on a translation of known subword units such as
morphemes or phonemes. Word categories whose
translation is potentially transparent include:
• named entities. Between languages that share
an alphabet, names can often be copied from
source to target text. Transcription or transliteration may be required, especially if the alphabets or syllabaries differ. Example:
Barack Obama (English; German)
Áàðàê Îáàìà (Russian)
バラク・オバマ(ba-ra-ku o-ba-ma) (Japanese)
• cognates and loanwords. Cognates and loanwords with a common origin can differ in
regular ways between languages, so that
character-level translation rules are sufﬁcient
 . Example:
claustrophobia (English)
Klaustrophobie (German)
Êëàóñòðîôîáèÿ (Klaustrofobiâ) (Russian)
• morphologically complex words. Words containing multiple morphemes, for instance
formed via compounding, afﬁxation, or in-
ﬂection, may be translatable by translating
the morphemes separately. Example:
solar system (English)
Sonnensystem (Sonne + System) (German)
Naprendszer (Nap + Rendszer) (Hungarian)
In an analysis of 100 rare tokens (not among
the 50 000 most frequent types) in our German
training data1, the majority of tokens are potentially translatable from English through smaller
We ﬁnd 56 compounds,
6 loanwords with a common origin (emancipate→emanzipieren), 5 cases of transparent afﬁxation (sweetish ‘sweet’ + ‘-ish’ →süßlich ‘süß’ +
‘-lich’), 1 number and 1 computer language identiﬁer.
Our hypothesis is that a segmentation of rare
words into appropriate subword units is sufﬁcient to allow for the neural translation network
to learn transparent translations, and to generalize this knowledge to translate and produce unseen
words.2 We provide empirical support for this hy-
1Primarily parliamentary proceedings and web crawl data.
2Not every segmentation we produce is transparent.
While we expect no performance beneﬁt from opaque segmentations, i.e. segmentations where the units cannot be
translated independently, our NMT models show robustness
towards oversplitting.
pothesis in Sections 4 and 5. First, we discuss different subword representations.
Related Work
For Statistical Machine Translation (SMT), the
translation of unknown words has been the subject
of intensive research.
A large proportion of unknown words are
names, which can just be copied into the target text if both languages share an alphabet. If
alphabets differ, transliteration is required . Character-based translation has
also been investigated with phrase-based models,
which proved especially successful for closely related languages .
The segmentation of morphologically complex
words such as compounds is widely used for SMT,
and various algorithms for morpheme segmentation have been investigated .
Segmentation algorithms commonly used for phrase-based SMT
tend to be conservative in their splitting decisions,
whereas we aim for an aggressive segmentation
that allows for open-vocabulary translation with a
compact network vocabulary, and without having
to resort to back-off dictionaries.
The best choice of subword units may be taskspeciﬁc. For speech recognition, phone-level language models have been used . Mikolov et al. investigate subword
language models, and propose to use syllables.
For multilingual segmentation tasks, multilingual
algorithms have been proposed . We ﬁnd these intriguing, but inapplicable at test time.
Various techniques have been proposed to produce ﬁxed-length continuous word vectors based
on characters or morphemes . An effort to apply such techniques
to NMT, parallel to ours, has found no signiﬁcant
improvement over word-based approaches . One technical difference from our
work is that the attention mechanism still operates on the level of words in the model by Ling
et al. , and that the representation of each
word is ﬁxed-length. We expect that the attention
mechanism beneﬁts from our variable-length representation: the network can learn to place attention on different subword units at each step. Recall our introductory example Abwasserbehandlungsanlange, for which a subword segmentation
avoids the information bottleneck of a ﬁxed-length
representation.
Neural machine translation differs from phrasebased methods in that there are strong incentives to
minimize the vocabulary size of neural models to
increase time and space efﬁciency, and to allow for
translation without back-off models. At the same
time, we also want a compact representation of the
text itself, since an increase in text length reduces
efﬁciency and increases the distances over which
neural models need to pass information.
A simple method to manipulate the trade-off between vocabulary size and text size is to use shortlists of unsegmented words, using subword units
only for rare words. As an alternative, we propose a segmentation algorithm based on byte pair
encoding (BPE), which lets us learn a vocabulary
that provides a good compression rate of the text.
Byte Pair Encoding (BPE)
Byte Pair Encoding (BPE) is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. We adapt this
algorithm for word segmentation. Instead of merging frequent pairs of bytes, we merge characters or
character sequences.
Firstly, we initialize the symbol vocabulary with
the character vocabulary, and represent each word
as a sequence of characters, plus a special end-ofword symbol ‘·’, which allows us to restore the
original tokenization after translation. We iteratively count all symbol pairs and replace each occurrence of the most frequent pair (‘A’, ‘B’) with
a new symbol ‘AB’. Each merge operation produces a new symbol which represents a character n-gram. Frequent character n-grams (or whole
words) are eventually merged into a single symbol, thus BPE requires no shortlist. The ﬁnal symbol vocabulary size is equal to the size of the initial
vocabulary, plus the number of merge operations
– the latter is the only hyperparameter of the algorithm.
For efﬁciency, we do not consider pairs that
cross word boundaries. The algorithm can thus be
run on the dictionary extracted from a text, with
each word being weighted by its frequency.
minimal Python implementation is shown in Al-
Algorithm 1 Learn BPE operations
import re, collections
def get_stats(vocab):
pairs = collections.defaultdict(int)
for word, freq in vocab.items():
symbols = word.split()
for i in range(len(symbols)-1):
pairs[symbols[i],symbols[i+1]] += freq
return pairs
def merge_vocab(pair, v_in):
v_out = {}
bigram = re.escape(' '.join(pair))
p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
for word in v_in:
w_out = p.sub(''.join(pair), word)
v_out[w_out] = v_in[word]
return v_out
vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,
'n e w e s t </w>':6, 'w i d e s t </w>':3}
num_merges = 10
for i in range(num_merges):
pairs = get_stats(vocab)
best = max(pairs, key=pairs.get)
vocab = merge_vocab(best, vocab)
print(best)
Figure 1: BPE merge operations learned from dictionary {‘low’, ‘lowest’, ‘newer’, ‘wider’}.
gorithm 1. In practice, we increase efﬁciency by
indexing all pairs, and updating data structures incrementally.
The main difference to other compression algorithms, such as Huffman encoding, which have
been proposed to produce a variable-length encoding of words for NMT , is that our symbol sequences are still interpretable as subword units, and that the network
can generalize to translate and produce new words
(unseen at training time) on the basis of these subword units.
Figure 1 shows a toy example of learned BPE
operations. At test time, we ﬁrst split words into
sequences of characters, then apply the learned operations to merge the characters into larger, known
This is applicable to any word, and
allows for open-vocabulary networks with ﬁxed
symbol vocabularies.3 In our example, the OOV
‘lower’ would be segmented into ‘low er·’.
3The only symbols that will be unknown at test time are
unknown characters, or symbols of which all occurrences
in the training text have been merged into larger symbols,
like ‘safeguar’, which has all occurrences in our training text
merged into ‘safeguard’. We observed no such symbols at
test time, but the issue could be easily solved by recursively
reversing speciﬁc merges until all symbols are known.
We evaluate two methods of applying BPE:
learning two independent encodings, one for the
source, one for the target vocabulary, or learning
the encoding on the union of the two vocabularies (which we call joint BPE).4 The former has the
advantage of being more compact in terms of text
and vocabulary size, and having stronger guarantees that each subword unit has been seen in the
training text of the respective language, whereas
the latter improves consistency between the source
and the target segmentation. If we apply BPE independently, the same name may be segmented
differently in the two languages, which makes it
harder for the neural models to learn a mapping
between the subword units. To increase the consistency between English and Russian segmentation despite the differing alphabets, we transliterate the Russian vocabulary into Latin characters
with ISO-9 to learn the joint BPE encoding, then
transliterate the BPE merge operations back into
Cyrillic to apply them to the Russian training text.5
Evaluation
We aim to answer the following empirical questions:
• Can we improve the translation of rare and
unseen words in neural machine translation
by representing them via subword units?
• Which segmentation into subword units performs best in terms of vocabulary size, text
size, and translation quality?
We perform experiments on data from the
shared translation task of WMT 2015.
English→German, our training set consists of 4.2
million sentence pairs, or approximately 100 million tokens. For English→Russian, the training set
consists of 2.6 million sentence pairs, or approximately 50 million tokens. We tokenize and truecase the data with the scripts provided in Moses
 . We use newstest2013 as development set, and report results on newstest2014
and newstest2015.
We report results with BLEU (mteval-v13a.pl),
and CHRF3 , a character n-gram
F3 score which was found to correlate well with
4In practice, we simply concatenate the source and target
side of the training set to learn joint BPE.
5Since the Russian training text also contains words that
use the Latin alphabet, we also apply the Latin BPE operations.
human judgments, especially for translations out
of English .
main claim is concerned with the translation of
rare and unseen words, we report separate statistics for these. We measure these through unigram
F1, which we calculate as the harmonic mean of
clipped unigram precision and recall.6
We perform all experiments with Groundhog7
 . We generally follow settings by previous work .
All networks have a hidden
layer size of 1000, and an embedding layer size
of 620. Following Jean et al. , we only keep
a shortlist of τ = 30000 words in memory.
During training, we use Adadelta ,
a minibatch size of 80, and reshufﬂe the training set between epochs. We train a network for
approximately 7 days, then take the last 4 saved
models (models being saved every 12 hours), and
continue training each with a ﬁxed embedding
layer ) for 12
hours. We perform two independent training runs
for each models, once with cut-off for gradient
clipping of 5.0, once with
a cut-off of 1.0 – the latter produced better single
models for most settings. We report results of the
system that performed best on our development set
(newstest2013), and of an ensemble of all 8 models.
We use a beam size of 12 for beam search,
with probabilities normalized by sentence length.
We use a bilingual dictionary based on fast-align
 . For our baseline, this serves
as back-off dictionary for rare words. We also use
the dictionary to speed up translation for all experiments, only performing the softmax over a ﬁltered list of candidate translations , we use K = 30000; K′ = 10).
Subword statistics
Apart from translation quality, which we will verify empirically, our main objective is to represent
an open vocabulary through a compact ﬁxed-size
subword vocabulary, and allow for efﬁcient training and decoding.8
Statistics for different segmentations of the Ger-
6Clipped unigram precision is essentially 1-gram BLEU
without brevity penalty.
7github.com/sebastien-j/LV_groundhog
8The time complexity of encoder-decoder architectures is
at least linear to sequence length, and oversplitting harms ef-
man side of the parallel data are shown in Table
1. A simple baseline is the segmentation of words
into character n-grams.9 Character n-grams allow
for different trade-offs between sequence length
(# tokens) and vocabulary size (# types), depending on the choice of n. The increase in sequence
length is substantial; one way to reduce sequence
length is to leave a shortlist of the k most frequent
word types unsegmented. Only the unigram representation is truly open-vocabulary. However, the
unigram representation performed poorly in preliminary experiments, and we report translation results with a bigram representation, which is empirically better, but unable to produce some tokens in
the test set with the training set vocabulary.
We report statistics for several word segmentation techniques that have proven useful in previous
SMT research, including frequency-based compound splitting , rulebased hyphenation , and Morfessor
 . We ﬁnd that they only
moderately reduce vocabulary size, and do not
solve the unknown word problem, and we thus ﬁnd
them unsuitable for our goal of open-vocabulary
translation without back-off dictionary.
BPE meets our goal of being open-vocabulary,
and the learned merge operations can be applied
to the test set to obtain a segmentation with no
unknown symbols.10
Its main difference from
the character-level model is that the more compact representation of BPE allows for shorter sequences, and that the attention model operates
on variable-length units.11
Table 1 shows BPE
with 59 500 merge operations, and joint BPE with
89 500 operations.
In practice, we did not include infrequent subword units in the NMT network vocabulary, since
there is noise in the subword symbol sets, e.g.
because of characters from foreign alphabets.
Hence, our network vocabularies in Table 2 are
typically slightly smaller than the number of types
in Table 1.
9Our character n-grams do not cross word boundaries. We
mark whether a subword is word-ﬁnal or not with a special
character, which allows us to restore the original tokenization.
10Joint BPE can produce segments that are unknown because they only occur in the English training text, but these
are rare (0.05% of test tokens).
11We highlighted the limitations of word-level attention in
section 3.1. At the other end of the spectrum, the character
level is suboptimal for alignment .
vocabulary
unigram F1 (%)
segmentation shortlist
target single ens-8 single ens-8
syntax-based 
- 59.1 46.0
- 300 000 500 000
48.9 56.7 20.4
- 300 000 500 000
52.4 58.1 36.8
char-bigram
53.5 58.4 40.5
53.9 58.4 40.9
BPE-J90k BPE (joint)
54.1 58.5 41.8
Table 2: English→German translation performance (BLEU, CHRF3 and unigram F1) on newstest2015.
Ens-8: ensemble of 8 models. Best NMT system in bold. Unigram F1 (with ensembles) is computed for
all words (n = 44085), rare words (not among top 50 000 in training set; n = 2900), and OOVs (not in
training set; n = 1168).
segmentation
characters
character bigrams
character trigrams
compound splitting△
morfessor*
hyphenation⋄
BPE (joint)
character bigrams
(shortlist: 50 000)
Table 1: Corpus statistics for German training
corpus with different word segmentation techniques.
#UNK: number of unknown tokens in
newstest2013. △: ; *:
 ; ⋄: .
Translation experiments
English→German translation results are shown in
Table 2; English→Russian results in Table 3.
Our baseline WDict is a word-level model with
a back-off dictionary. It differs from WUnk in that
the latter uses no back-off dictionary, and just represents out-of-vocabulary words as UNK12. The
back-off dictionary improves unigram F1 for rare
and unseen words, although the improvement is
smaller for English→Russian, since the back-off
dictionary is incapable of transliterating names.
All subword systems operate without a back-off
dictionary. We ﬁrst focus on unigram F1, where
all systems improve over the baseline, especially
for rare words (36.8%→41.8% for EN→DE;
26.5%→29.7% for EN→RU). For OOVs, the
baseline strategy of copying unknown words
works well for English→German. However, when
alphabets differ, like in English→Russian, the
subword models do much better.
12We use UNK for words that are outside the model vocabulary, and OOV for those that do not occur in the training
Unigram F1 scores indicate that learning the
BPE symbols on the vocabulary union (BPE-
J90k) is more effective than learning them separately (BPE-60k), and more effective than using
character bigrams with a shortlist of 50 000 unsegmented words (C2-50k), but all reported subword
segmentations are viable choices and outperform
the back-off dictionary baseline.
Our subword representations cause big improvements in the translation of rare and unseen
words, but these only constitute 9-11% of the test
sets. Since rare words tend to carry central information in a sentence, we suspect that BLEU
and CHRF3 underestimate their effect on translation quality. Still, we also see improvements over
the baseline in total unigram F1, as well as BLEU
and CHRF3, and the subword ensembles outperform the WDict baseline by 0.3–1.3 BLEU and
0.6–2 CHRF3. There is some inconsistency between BLEU and CHRF3, which we attribute to the
fact that BLEU has a precision bias, and CHRF3 a
recall bias.
For English→German, we observe the best
BLEU score of 25.3 with C2-50k, but the best
CHRF3 score of 54.1 with BPE-J90k. For comparison to the (to our knowledge) best non-neural
MT system on this data set, we report syntaxbased SMT results .
We observe that our best systems outperform the
syntax-based system in terms of BLEU, but not
in terms of CHRF3. Regarding other neural systems, Luong et al. report a BLEU score of
25.9 on newstest2015, but we note that they use an
ensemble of 8 independently trained models, and
also report strong improvements from applying
dropout, which we did not use. We are conﬁdent
that our improvements to the translation of rare
words are orthogonal to improvements achievable
through other improvements in the network archi-
tecture, training algorithm, or better ensembles.
For English→Russian, the state of the art is
the phrase-based system by Haddow et al. .
It outperforms our WDict baseline by 1.5 BLEU.
The subword models are a step towards closing
this gap, and BPE-J90k yields an improvement of
1.3 BLEU, and 2.0 CHRF3, over WDict.
As a further comment on our translation results,
we want to emphasize that performance variability is still an open problem with NMT. On our development set, we observe differences of up to 1
BLEU between different models. For single systems, we report the results of the model that performs best on dev (out of 8), which has a stabilizing effect, but how to control for randomness
deserves further attention in future research.
Unigram accuracy
Our main claims are that the translation of rare and
unknown words is poor in word-level NMT models, and that subword models improve the translation of these word types. To further illustrate
the effect of different subword segmentations on
the translation of rare and unseen words, we plot
target-side words sorted by their frequency in the
training set.13 To analyze the effect of vocabulary
size, we also include the system C2-3/500k, which
is a system with the same vocabulary size as the
WDict baseline, and character bigrams to represent unseen words.
Figure 2 shows results for the English–German
ensemble systems on newstest2015.
F1 of all systems tends to decrease for lowerfrequency words. The baseline system has a spike
in F1 for OOVs, i.e. words that do not occur in
the training text. This is because a high proportion of OOVs are names, for which a copy from
the source to the target text is a good strategy for
English→German.
The systems with a target vocabulary of 500 000
words mostly differ in how well they translate
words with rank > 500 000. A back-off dictionary
is an obvious improvement over producing UNK,
but the subword system C2-3/500k achieves better
performance. Note that all OOVs that the backoff dictionary produces are words that are copied
from the source, usually names, while the subword
13We perform binning of words with the same training set
frequency, and apply bezier smoothing to the graph.
systems can productively form new words such as
compounds.
For the 50 000 most frequent words, the representation is the same for all neural networks, and
all neural networks achieve comparable unigram
F1 for this category. For the interval between frequency rank 50 000 and 500 000, the comparison
between C2-3/500k and C2-50k unveils an interesting difference. The two systems only differ in
the size of the shortlist, with C2-3/500k representing words in this interval as single units, and C2-
50k via subword units. We ﬁnd that the performance of C2-3/500k degrades heavily up to frequency rank 500 000, at which point the model
switches to a subword representation and performance recovers. The performance of C2-50k remains more stable. We attribute this to the fact
that subword units are less sparse than words. In
our training set, the frequency rank 50 000 corresponds to a frequency of 60 in the training data;
the frequency rank 500 000 to a frequency of 2.
Because subword representations are less sparse,
reducing the size of the network vocabulary, and
representing more words via subword units, can
lead to better performance.
The F1 numbers hide some qualitative differences between systems.
For English→German,
WDict produces few OOVs (26.5% recall), but
with high precision (60.6%) , whereas the subword
systems achieve higher recall, but lower precision.
We note that the character bigram model C2-50k
produces the most OOV words, and achieves relatively low precision of 29.1% for this category.
However, it outperforms the back-off dictionary
in recall (33.0%). BPE-60k, which suffers from
transliteration (or copy) errors due to segmentation inconsistencies, obtains a slightly better precision (32.4%), but a worse recall (26.6%). In contrast to BPE-60k, the joint BPE encoding of BPE-
J90k improves both precision (38.6%) and recall
For English→Russian, unknown names can
only rarely be copied, and usually require transliteration.
Consequently, the WDict baseline performs more poorly for OOVs (9.2% precision;
5.2% recall), and the subword models improve
both precision and recall (21.9% precision and
15.6% recall for BPE-J90k). The full unigram F1
plot is shown in Figure 3.
vocabulary
unigram F1 (%)
segmentation
target single ens-8 single ens-8
phrase-based 
- 56.0 31.3
- 300 000 500 000
49.9 54.2 25.2
- 300 000 500 000
51.0 54.8 26.5
char-bigram
51.6 55.2 27.8
52.7 55.3 29.7
BPE-J90k BPE (joint)
90 000 100 000
53.0 55.8 29.7
Table 3: English→Russian translation performance (BLEU, CHRF3 and unigram F1) on newstest2015.
Ens-8: ensemble of 8 models. Best NMT system in bold. Unigram F1 (with ensembles) is computed for
all words (n = 55654), rare words (not among top 50 000 in training set; n = 5442), and OOVs (not in
training set; n = 851).
training set frequency rank
unigram F1
C2-300/500k
Figure 2: English→German unigram F1 on newstest2015 plotted by training set frequency rank
for different NMT systems.
training set frequency rank
unigram F1
Figure 3: English→Russian unigram F1 on newstest2015 plotted by training set frequency rank
for different NMT systems.
Manual Analysis
Table 4 shows two translation examples for
the translation direction English→German, Table 5 for English→Russian.
The baseline system fails for all of the examples, either by deleting content (health), or by copying source words
that should be translated or transliterated.
subword translations of health research institutes show that the subword systems are capable of learning translations when oversplitting (research→Fo|rs|ch|un|g), or when the segmentation
does not match morpheme boundaries: the segmentation Forschungs|instituten would be linguistically more plausible, and simpler to align to the
English research institutes, than the segmentation
Forsch|ungsinstitu|ten in the BPE-60k system, but
still, a correct translation is produced. If the systems have failed to learn a translation due to data
sparseness, like for asinine, which should be translated as dumm, we see translations that are wrong,
but could be plausible for (partial) loanwords (asinine Situation→Asinin-Situation).
The English→Russian examples show that
the subword systems are capable of transliteration.
However, transliteration errors do occur,
either due to ambiguous transliterations, or because of non-consistent segmentations between
source and target text which make it hard for
the system to learn a transliteration mapping.
Note that the BPE-60k system encodes Mirzayeva inconsistently for the two language pairs
(Mirz|ayeva→Ìèð|çà|åâà Mir|za|eva). This example is still translated correctly, but we observe
spurious insertions and deletions of characters in
the BPE-60k system. An example is the transliteration of rakﬁsk, where a ï is inserted and a ê
is deleted.
We trace this error back to translation pairs in the training data with inconsistent
segmentations, such as (p|rak|ri|ti→ïðà|êðèò|è
health research institutes
Gesundheitsforschungsinstitute
Forschungsinstitute
Fo|rs|ch|un|gs|in|st|it|ut|io|ne|n
Gesundheits|forsch|ungsinstitu|ten
Gesundheits|forsch|ungsin|stitute
asinine situation
dumme Situation
asinine situation →UNK →asinine
as|in|in|e situation →As|in|en|si|tu|at|io|n
as|in|ine situation →A|in|line-|Situation
as|in|ine situation →As|in|in-|Situation
Table 4: English→German translation example.
“|” marks subword boundaries.
Ìèðçàåâà (Mirzaeva)
→UNK →Mirzayeva
Mi|rz|ay|ev|a →Ìè|ðç|àå|âà (Mi|rz|ae|va)
Mirz|ayeva
→Ìèð|çà|åâà (Mir|za|eva)
Mir|za|yeva →Ìèð|çà|åâà (Mir|za|eva)
ðàêôèñêà (rakﬁska)
→UNK →rakﬁsk
ra|kf|is|k →ðà|êô|èñ|ê (ra|kf|is|k)
rak|f|isk →ïðà|ô|èñê (pra|f|isk)
rak|f|isk →ðàê|ô|èñêà (rak|f|iska)
Table 5: English→Russian translation examples.
“|” marks subword boundaries.
(pra|krit|i)), from which the translation (rak→ïðà)
is erroneously learned. The segmentation of the
joint BPE system (BPE-J90k) is more consistent
(pra|krit|i→ïðà|êðèò|è (pra|krit|i)).
Conclusion
The main contribution of this paper is that we
show that neural machine translation systems are
capable of open-vocabulary translation by representing rare and unseen words as a sequence of
subword units.14 This is both simpler and more
effective than using a back-off translation model.
We introduce a variant of byte pair encoding for
word segmentation, which is capable of encoding open vocabularies with a compact symbol vocabulary of variable-length subword units.
show performance gains over the baseline with
both BPE segmentation, and a simple character bigram segmentation.
Our analysis shows that not only out-ofvocabulary words, but also rare in-vocabulary
words are translated poorly by our baseline NMT
segmentation
algorithms
is available at 
subword-nmt.
system, and that reducing the vocabulary size
of subword models can actually improve performance. In this work, our choice of vocabulary size
is somewhat arbitrary, and mainly motivated by
comparison to prior work. One avenue of future
research is to learn the optimal vocabulary size for
a translation task, which we expect to depend on
the language pair and amount of training data, automatically. We also believe there is further potential in bilingually informed segmentation algorithms to create more alignable subword units, although the segmentation algorithm cannot rely on
the target text at runtime.
While the relative effectiveness will depend on
language-speciﬁc factors such as vocabulary size,
we believe that subword segmentations are suitable for most language pairs, eliminating the need
for large NMT vocabularies or back-off models.
Acknowledgments
We thank Maja Popovi´c for her implementation of CHRF, with which we veriﬁed our reimplementation.
The research presented in this
publication was conducted in cooperation with
Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland. This project received
funding from the European Union’s Horizon 2020
research and innovation programme under grant
agreement 645452 (QT21).