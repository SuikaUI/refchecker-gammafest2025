Activity Recognition and Monitoring Using Multiple Sensors on Different Body
Uwe Maurer1, Asim Smailagic2, Daniel P. Siewiorek2, Michael Deisher3,
2School of Computer Science, Carnegie Mellon University, Pittsburgh
1Computer Science Department, Technische Universität München, Germany
3 Intel, Hillsboro, OR
The design of an activity recognition and monitoring system based on the eWatch, multi-sensor platform worn on
different body positions, is presented in this paper. The system identiﬁes the user’s activity in realtime using multiple
sensors and records the classiﬁcation results during a day.
We compare multiple time domain feature sets and sampling rates, and analyze the tradeoff between recognition
accuracy and computational complexity. The classiﬁcation
accuracy on different body positions used for wearing electronic devices was evaluated.
1. Introduction
The primary goal of this paper is to study the effectiveness of activity classiﬁers in a multi-sensor system as we
vary the wearing positions of the sensors. The eWatch is
used as a multi sensor platform for wearable context-aware
computing. Previous feature extraction studies examining
accelerometer data have shown that it is a viable input for
detecting user states when it is worn on the wrist . Motivated by other possible sensor platform locations, especially
with mobile communication devices such as a cell phone or
PDA, we designed a study to investigate the dependency of
the eWatch classiﬁcation accuracy on given different body
positions. We investigate wearing the eWatch in the following locations: the belt, shirt pocket, trouser pocket, backpack, and necklace. The results of the study would help us
decide on the best position to place such a sensor platform,
and understand the nature of the trade-off between wearing
position and classiﬁcation performance.
In , the authors used multiple accelerometers worn on
a person’s body to recognize their physical activity. Sensor
data from multiple body positions was combined for classifying the activities.
In , a low power sensor hardware system is presented, including accelerometer, light sensor, microphone,
and wireless communication. Based on this hardware, a de-
Figure 1. eWatch sensing platform
sign method for a context recognition system is proposed.
It evaluates multiple feature sets and makes the tradeoff between power consumption and recognition accuracy. A system that classiﬁes household activities in realtime with a
focus on low power consumption is presented in .
In , a system using an armband based sensor array
and unsupervised machine learning algorithms was able to
determine a meaningful user context model.
In Section 2, we describe the sensor platform and Section 3 explains the experimental design. Section 4 describes
the activity recognition method. Section 5 presents the results of the data analysis, and Section 6 addresses the performance of our on-board activity classiﬁer.
2. Sensing Platform
Our sensor platform, the eWatch (Figure 1), is based on
the Philips LPC2106 ARM7 TDMI microcontroller, with
128kB of internal ﬂash memory and 64kB of RAM .
The LPC2106 is a 32bit processor running at up to 60Mhz.
eWatch contains four sensors: a dual axes accelerometer,
light, temperature sensor and microphone. Sensor data can
be stored in a 1MB external ﬂash memory.
3. Experiment Design
In our study we focussed on six primary activities: sitting, standing, walking, ascending stairs, descending stairs
Proceedings of the International Workshop on Wearable and Implantable Body Sensor Networks (BSN’06)
0-7695-2547-4/06 $20.00 © 2006 IEEE
Report Documentation Page
Form Approved
OMB No. 0704-0188
Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and
maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information,
including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington
VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it
does not display a currently valid OMB control number.
1. REPORT DATE
2. REPORT TYPE
3. DATES COVERED
00-00-2006 to 00-00-2006
4. TITLE AND SUBTITLE
Activity Recognition and Monitoring Using Multiple Sensors on Different
Body Positions
5a. CONTRACT NUMBER
5b. GRANT NUMBER
5c. PROGRAM ELEMENT NUMBER
6. AUTHOR(S)
5d. PROJECT NUMBER
5e. TASK NUMBER
5f. WORK UNIT NUMBER
7. PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES)
Carnegie Mellon University,School of Computer Science,5000 Forbes
Ave,Pittsburgh,PA,15213
8. PERFORMING ORGANIZATION
REPORT NUMBER
9. SPONSORING/MONITORING AGENCY NAME(S) AND ADDRESS(ES)
10. SPONSOR/MONITOR’S ACRONYM(S)
11. SPONSOR/MONITOR’S REPORT
12. DISTRIBUTION/AVAILABILITY STATEMENT
Approved for public release; distribution unlimited
13. SUPPLEMENTARY NOTES
14. ABSTRACT
The design of an activity recognition and monitoring system based on the eWatch, multi-sensor platform
worn on different body positions, is presented in this paper. The system identifies the user?s activity in
realtime using multiple sensors and records the classification results during a day. We compare multiple
time domain feature sets and sampling rates, and analyze the tradeoff between recognition accuracy and
computational complexity. The classification accuracy on different body positions used for wearing
electronic devices was evaluated.
15. SUBJECT TERMS
16. SECURITY CLASSIFICATION OF:
17. LIMITATION OF
Report (SAR)
18. NUMBER
19a. NAME OF
RESPONSIBLE PERSON
unclassified
b. ABSTRACT
unclassified
c. THIS PAGE
unclassified
Standard Form 298 (Rev. 8-98)
Prescribed by ANSI Std Z39-18
and running.
Body positions that are normally used for
wearing electronic devices, such as cell phones or PDAs,
were studied.
We placed our sensor hardware on the
left wrist, belt, necklace, in the right trouser pocket, shirt
pocket, and bag. The subjects wore six eWatch devices located at these body positions during the study. The devices
recorded sensor data from the accelerometer and light sensor into their ﬂash memory. The user was asked to perform
tasks that consist of the activities, such as working on the
computer or walking to another building. The lead experimenter annotated the current activity and instructed the subjects on how to proceed. The annotations were done using
an application running on an extra eWatch worn by the lead
experimenter.
Six subjects participated in the study, each subject performed the given tasks in 45 to 50 minutes. In total we
collected over 290 minutes of sensor data.
Sensor setup
eWatch recorded both axes of the accelerometer and the light sensor. All sensors values were
recorded with a frequency of 50Hz and with 8bit resolution.
The accelerometer was calibrated so that both axes operate
in a range of ±2g. Evaluation of the recorded data was done
with Matlab and the WEKA software .
4. Activity recognition method
The sensor values recorded from the accelerometers and
the light sensor are split into short time windows. These
windows are then transformed into the feature space by calculating several feature functions over the individual windows.
Features from both accelerometer axes (X & Y),
the light sensor, and a combined value of both accelerometer signals were calculated. To reduce the dependency on
the orientation, both X and Y values were combined calculating the squared length of the acceleration vector. The
classiﬁcation accuracy with individual sensors as well as
with multiple sensor combined was investigated.
Only time domain features were considered to avoid the
costly computation that is required to transform the signal
into the frequency domain. Table 1 shows the list of features that were considered. The functions to calculate these
features were implemented on the eWatch and the required
number of clock cycles per function was measured. Each
function was executed 2000 times with different recorded
sensor inputs, and then the average value was computed.
The execution time was calculated based on the measured
clock cycles and the CPU frequency at 59MHz. Table 1
shows the measured clock cycles and execution time using
a four second window sampled at 20Hz (80 samples).
Figure 2 depicts the feature space after a transformation
with Linear Discriminant Analysis (LDA). It shows that the
descending
Figure 2. Feature space after LDA transformation
standing, sitting and running activities form seperate clusters, while walking, ascending and descending stairs are
closer together since these activities are very similar.
Feature Subsets
To reduce the time and energy required
to calculate the feature vector, several subsets of the complete feature space were evaluated. Some features are irrelevant or redundant and do not provide information to signiﬁcantly improve the classiﬁcation accuracy. Therefore a subset of the available features can be selected to decrease the
computation time without signiﬁcantly decreasing recognition accuracy.
The Correlation based Feature Selection (CFS) method
from the WEKA toolkit was used to ﬁnd feature sets con-
Features / Function
Empirical Mean
Root Mean Square
Standard Deviation
Mean Absolute Deviation
Cumulative Histogram (256 bins)
n’th Percentile (n = 5, 10, . . . , 95)
Interquartile Range
Zero Crossing Rate
Mean Crossing Rate
Sq. Length of X,Y (x2 + y2)
Decision Tree classiﬁer (18 nodes)
Table 1. List of time domain features and the
average clock cycles and time to calculate
them on the eWatch running at 59MHz
Proceedings of the International Workshop on Wearable and Implantable Body Sensor Networks (BSN’06)
0-7695-2547-4/06 $20.00 © 2006 IEEE
Sample frequency (Hz)
Classification accuracy (%)
(a) Features from the accelerometer’s
Sample frequency (Hz)
Classification accuracy (%)
(b) Features from the accelerometer’s Y-Axis
Sample frequency (Hz)
Classification accuracy (%)
(c) Features from the light sensor
Sample frequency (Hz)
Classification accuracy (%)
(d) Features from the x2 + y2 value
of the accelerometers
Figure 3. Recognition accuracy with different feature sets
taining features that are highly correlated within the particular class but are uncorrelated with each other. Table 2 shows
the feature sets that were compared.
Classiﬁcaton method
We evaluated and compared several classiﬁcation methods, namely Decision Trees (C4.5 algorithm), k-Nearest Neighbor (k-NN), Naive-Bayes and the
Bayes Net classiﬁer. Decision Trees and Naive-Bayes were
found to achieve high recognition accuracy with acceptable
computational complexity. Decision Trees were used for
activity classiﬁcation in and . It was shown in that
the discretized version of Naive-Bayes can outperform the
Decision Tree classiﬁer for general classiﬁcation problems.
Finally the Decision Tree classiﬁer was chosen as it provides a good balance between accuracy and computational
complexity. For all further experiments this classiﬁer with
a 5-fold cross validation was used.
Sampling frequency
During the user study the sensors
were sampled with a frequency of 50Hz and later downsampled to lower frequencies. To maintain some of the high
frequency components information and to reduce the computational complexity signiﬁcantly, no low pass ﬁlter was
used for downsampling the data. Figure 3 shows the recognition accuracy for different sample rates from 1 to 30Hz
for the different body positions. The recognition accuracy
was deﬁned as the percentage of correctly classiﬁed feature
vectors averaged for all six activities. The recognition accuracy increases with higher sampling rates, and with the
accelerometer features the accuracy then stabilizes between
15 to 20Hz, and is only improved marginally with higher
sampling rates. The accuracy with the light sensor only is
lower and it stabilizes beginning with 7Hz. In Figure 3(c)
the results from the belt and pocket position is not shown
because the light sensor did not provide any useful classiﬁcation information at these positions.
descending
Classification accuracy (%)
Figure 4. Recognition accuracy for the activities at different body locations
5. Data Analysis - eWatch Classiﬁcation Performance for Various Wearing Positions
We calculated the classiﬁcation accuracy for every activity on each of the six different body positions. The data
from all subjects was combined to train a general classiﬁer
that is not speciﬁc to a person.
Table 2 shows the feature sets and the classiﬁcation results for the different body positions. The features are calculated from a 20Hz signal.
Figure 4 shows the recognition accuracy for the individual activities at different body locations. For the classiﬁcation the reduced feature set F6 was used. The data indicate
that any of the six positions are good for detecting walking,
standing, sitting and running. Ascending and descending
the stairs is difﬁcult to distinguish from walking in all positions, since the classiﬁer was trained for multiple persons.
The wrist performs best because the feature set was optimized for the wrist position.
6. Onboard Activity Classiﬁer
Based on these results we implemented a decision tree
classiﬁer that runs on the eWatch. The feature set F6 was
used to build the decision tree.
The sensor sampling is
interrupt-based, and triggers the sampling of the sensors at
Proceedings of the International Workshop on Wearable and Implantable Body Sensor Networks (BSN’06)
0-7695-2547-4/06 $20.00 © 2006 IEEE
CPU Cycles
Time in μs
Classiﬁcation Accuracy for Body Position
All features, all sensors
All features from accelerometer X
All features from accelerometer Y
All features from light
All features from accelerometer XY (x2 + y2)
prcy(3), rmsxy, prcy(20), prcy(97),
rmslight, madx, meany, prcy(10)
prcy(3), iqry, prcy(10), prcy(97), madx
rmsxy, qrtx, rmsx, madxy, meanxy
Table 2. Feature sub sets and classiﬁcation accuracy for body positions
descending
Time in minutes
Figure 5. Activity classiﬁcation recorded over
100 minutes
20Hz. The sensor value is stored in a buffer with the size of
the sliding window. The activity is classiﬁed every 0.5 seconds based on the sensor data from the 4 second buffer. The
classiﬁcation results are stored into ﬂash memory and are
downloaded to a computer later for further processing and
analysis. They can also be transferred in realtime over the
Bluetooth connection. In order to save energy, the system
remains idle between servicing interrupts.
A subject wore the eWatch with the built-in activity classiﬁer on the wrist during the day. The system classiﬁed the
activity in realtime and recorded the classiﬁcation results
to ﬂash memory. Figure 5 shows 100 minutes of activity
classiﬁcation, as the user walked to a restaurant, sat down,
ate lunch, went back to the ofﬁce and sat down to continue working. The classiﬁcation results match well with
the actual activities; eating lunch was partially interpreted
as walking or running activity due to arm movements.
7. Conclusions and Future Work
The activity recognition and monitoring system that can
identify and record the user’s activity in realtime using multiple sensors is presented. We compared multiple feature
sets and sampling rates to ﬁnd an optimized classiﬁcation
method, and showed how well they perform on different
body locations that are commonly used for wearing electronic devices.
We will extend our activity classiﬁer to other activites
and investigate how the activity classiﬁcation can support
the recognition of the user’s location.
7.1. Acknowledgments
This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under
Contract No. NBCHD030010, the National Science Foundation under Grant Nos. 0205266 and 0203448, a grant
from Intel Corporation, and PA Infrastructure (PITA).