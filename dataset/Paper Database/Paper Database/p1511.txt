Geodesic Flow Kernel for Unsupervised Domain Adaptation
Boqing Gong, Yuan Shi, Fei Sha
Dept. of Computer Science
U. of Southern California
{boqinggo, yuanshi, feisha}@usc.edu
Kristen Grauman
Dept. of Computer Science
U. of Texas at Austin
 
In real-world applications of visual recognition, many
factors—such as pose, illumination, or image quality—can
cause a signiﬁcant mismatch between the source domain
on which classiﬁers are trained and the target domain to
which those classiﬁers are applied. As such, the classiﬁers
often perform poorly on the target domain. Domain adaptation techniques aim to correct the mismatch. Existing approaches have concentrated on learning feature representations that are invariant across domains, and they often do
not directly exploit low-dimensional structures that are intrinsic to many vision datasets. In this paper, we propose
a new kernel-based method that takes advantage of such
structures. Our geodesic ﬂow kernel models domain shift
by integrating an inﬁnite number of subspaces that characterize changes in geometric and statistical properties from
the source to the target domain. Our approach is computationally advantageous, automatically inferring important
algorithmic parameters without requiring extensive crossvalidation or labeled data from either domain.
introduce a metric that reliably measures the adaptability
between a pair of source and target domains. For a given
target domain and several source domains, the metric can
be used to automatically select the optimal source domain
to adapt and avoid less desirable ones. Empirical studies
on standard datasets demonstrate the advantages of our approach over competing methods.
1. Introduction
Imagine that we are to deploy an Android application
to recognize objects in images captured with mobile phone
cameras. Can we train classiﬁers with Flickr photos, as they
have already been collected and annotated, and hope the
classiﬁers still work well on mobile camera images?
Our intuition says no. We suspect that the strong distinction between Flickr and mobile phone images will cripple those classiﬁers.
Indeed, a stream of studies have
shown that when image classiﬁers are evaluated outside
of their training datasets, the performance degrades signiﬁcantly . Beyond image recognition, mismatched
training and testing conditions are also abundant: in other
computer vision tasks , speech and language
processing , and others.
All these pattern recognition tasks involve two distinct
types of datasets, one from a source domain and the other
from a target domain. The source domain contains a large
amount of labeled data such that a classiﬁer can be reliably
built. The target domain refers broadly to a dataset that
is assumed to have different characteristics from the source
domain. The main objective is to adapt classiﬁers trained
on the source domain to the target domain to attain good
performance there. Note that we assume the set of possible
labels are the same across domains.
Techniques for addressing this challenge have been investigated under the names of domain adaptation, covariate
shift, and transfer learning. There are two settings: unsupervised domain adaptation where the target domain is
completely unlabeled, and semi-supervised domain adaptation where the target domain contains a small amount of
labeled data. Often the labeled target data alone is insufﬁcient to construct a good classiﬁer. Thus, how to effectively
leverage unlabeled target data is key to domain adaptation.
A very fruitful line of work has been focusing on deriving new feature representations to facilitate domain adaptation, where labeled target data is not needed . The objective is to identify a new feature space
such that the source domain and the target domain manifest shared characteristics. Intuitively, if they were indistinguishable, a classiﬁer constructed for the source domain
would work also for the target domain.
Deﬁning and quantifying shared characteristics entails
careful examination of our intuition on what type of representations facilitate adaptation. For example, in the part-ofspeech (POS) task of tagging words into different syntactic
categories , the idea is to extract shared patterns from
auxiliary classiﬁcation tasks that predict “pivot features”,
frequent words that are themselves indicative of those categories. While sensible for language processing tasks, typi-
To appear, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
Φ(t), 0 ≤t ≤ 1
Figure 1. Main idea of our geodesic ﬂow kernel-based approach
for domain adaptation (Best viewed in color). We embed source
and target datasets in a Grassmann manifold. We then construct
a geodesic ﬂow between the two points and integrate an inﬁnite number of subspaces along the ﬂow Φ(t). Concretely, raw
features are projected into these subspaces to form an inﬁnitedimensional feature vector z∞∈H∞. Inner products between
these feature vectors deﬁne a kernel function that can be computed over the original feature space in closed-form. The kernel
encapsulates incremental changes between subspaces that underly
the difference and commonness between the two domains. The
learning algorithms thus use this kernel to derive low-dimensional
representations that are invariant to the domains.
cal histogram based features of low-level visual descriptors
do not enjoy having pivot “visual words” — in general, no
single feature dimension from a particular histogram bin is
discriminative enough to differentiate visual categories.
On the other hand, many visual data are assumed to lie in
low-dimensional subspaces. Given data from two domains,
how can we exploit the subspaces in these datasets, which
can be telltale in revealing the underlying difference and
commonness between the domains?
Moreover, given multiple source domains and a target
domain, how can we select which source domain to pair
with the target domain? This is an especially important
problem to address in order to apply domain adaptation to
real-world problems. For instance, in the context of object recognition, we can choose from multiple datasets as
our source domain: ImageNet, Caltech-101/256, PASCAL
VOC, etc. It is much more cost-effective to be able to select
one (or a limited few) that are likely to adapt well to the
target domain, instead of trying each one of them.
To address the ﬁrst challenge, we propose a kernel-based
method for domain adaptation. The proposed geodesic ﬂow
kernel is computed over the subspaces of the source and the
target domains. It integrates an inﬁnite number of subspaces
that lie on the geodesic ﬂow from the source subspace to
the target one. The ﬂow represents incremental changes
in geometric and statistical properties between the two domains. Being mindful of all these changes, our learning
algorithm extracts those subspace directions that are truly
domain-invariant. Fig. 1 sketches the main idea.
To address the second challenge, we introduce a metric
called Rank of Domain (ROD) that can be used to rank a list
of source domains based on how suitable they are to domain
adaptation. The metric integrates two pieces of information:
how much the subspaces of the source and the target domains overlap, and how similarly the target and source data
are distributed in the subspaces. In our experiments, ROD
correlates well with adaptation performance.
We demonstrate the effectiveness of the proposed approaches on benchmark tasks of object recognition. The
proposed methods outperform signiﬁcantly state-of-the-art
methods for domain adaptation. Additionally, as a novel application of these methods, we investigate the dataset bias
problem, recently studied in . Through their analysis,
the authors identiﬁed a few datasets of high “market value”,
suggesting that they are less biased, and more representative
of real-world objects. We re-examine these datasets with a
new perspective: are such high-valued datasets indeed useful in improving a target domain’s performance? Our analysis suggests it would be beneﬁcial to also consider “ease
of adaptability” in assessing the value of datasets.
Contributions. To summarize, our main contributions are:
i) a kernel-based domain adaptation method that exploits
intrinsic low-dimensional structures in the datasets (section 3.3); the method is easy to implement, with no parameters to cross-validate (sections 3.4 and 4.4); ii) a metric that
can predict which source domain is better suited for adaptation to a target domain, without using labeled target data
(sections 3.5 and 4.5); iii) empirical studies validating the
advantages of our approaches over existing approaches on
benchmark datasets (section 4.2 and 4.3); iv) a new perspective from re-examining cross-dataset generalization using
domain adaptation (section 4.6).
2. Related Work
Domain adaptation has been extensively studied in many
areas, including in statistics and machine learning , speech and language processing , and more
recently computer vision .
Of particular relevance to our work is the idea of learning
new feature representations that are domain-invariant, thus
enabling transferring classiﬁers from the source domain to
the target domain . Those approaches are
especially appealing to unsupervised domain adaptation as
they do not require labeled target data. Other methods for
unsupervised domain adaptation have been explored, for example, with transductive SVMs or iteratively relabeling
(the target domain) . Note that the latter approach depends very much on tuning several parameters, which requires extensive computation of training many SVMs.
Gopalan et al’s work is the closest to ours in spirit .
They have also explored the idea of using geodesic ﬂows to
derive intermediate subspaces that interpolate between the
source and target domains. A crucial difference of that work
from ours is that they sample a ﬁnite number of subspaces
and stack these subspaces into a very high-dimensional projection matrix. Our kernel method is both conceptually and
computationally simpler and eliminates the need to tune
many parameters needed in Gopalan et al’s approach. We
will return to the comparison after we describe both approaches in section 3.
3. Proposed Approach
The main idea behind our approach is to explicitly construct an inﬁnite-dimensional feature space H∞that assembles information on the source domain DS, on the target
domain DT , and on “phantom” domains interpolating between those two — the nature of the interpolation will be
made more precise later. Inner products in H∞give rise to
a kernel function that can be computed efﬁciently in closedform. Thus, this geodesic ﬂow kernel (GFK) can be readily
used to construct any kernelized classiﬁers.
We start by reviewing basic notions of Grassmann manifolds; the subspaces of the data from the source and target
domains are represented as two points on one such manifold. We then discuss a previous approach where multiple
subspaces are sampled from the manifold to derive new feature representations. Then in section 3.3, we describe our
approach in detail and contrast to the previous one.
The dimensionality of the subspaces is an important parameter. In section 3.4, we present a subspace disagreement measure (SDM) for selecting this parameter automatically without cross-validation. Finally, in section 3.5, we
describe a Rank of Domain (ROD) metric that computes
compatibility between two domains for adaptation.
3.1. Background
In statistical modeling, we often assume data can be embedded in a low-dimensional linear subspace. For example,
principal component analysis (PCA) identiﬁes the subspace
where the variances of the embedded data are maximized.
Most of the time, it is both sufﬁcient and convenient to refer
to a subspace with its basis P ∈RD×d, where D is the dimensionality of the data and d is the dimensionality of the
subspace. For PCA, the basis is then the top d eigenvectors of the data’s covariance matrix. The collection of all
d-dimensional subspaces form the Grassmannian G(d, D),
a smooth Riemannian manifold on which we can deﬁne geometric, differential, and probabilistic structures.
As an intuitive example of how Grassmannians can help
us to attack the problem of domain adaptation, imagine that
we compute the subspaces of the datasets for the DS and
DT domains and map them to two points on a Grassmannian. Intuitively, if these two points are close by, then the
two domains could be similar to each other, for example,
their features may be similarly distributed. Thus, a DStrained classiﬁer is likely to work well on DT .
However, what if these two domains are far apart on the
manifold? We brieﬂy describe an earlier work by Gopalan
et al . Our method extends and improves upon it.
3.2. Subspaces by sampling geodesic ﬂow (SGF)
Consider two datasets of “Cars” with large differences
in poses are placed far apart on the manifold. The key idea
is to use intermediate subspaces to learn domain-invariant
features to adapt . Speciﬁcally, the intermediate subspaces would capture statistics of car images under poses
interpolated between the source and the target domain. Being informed of all these different subspaces from the same
category, the learning algorithms might be able to extract
features that are less sensitive to variations in pose.
Concretely, the approach of sampling geodesic ﬂow
(SGF) consists of the following steps: i) construct a
geodesic ﬂow curve connecting the source and target domains on the Grassmannian; ii) sample a ﬁxed number of
subspaces from this curve; iii) project original feature vectors into these subspaces and concatenate them into feature super-vectors; iv) reduce dimensionality of the supervectors; v) use the resulting representations as new feature
vectors to construct classiﬁers.
Despite its encouraging results, the SGF approach has
several limitations. It is not clear how to choose the best
sampling strategy.
A few important parameters need to
be tuned: the number of subspaces to sample, the dimensionality of the subspaces, and how to cope with the highdimensionality of the new representations. Critically, crossvalidating all these “tweaking knobs” is impractical in typical settings for domain adaptation, where there is little or
no labeled target data.
In the following, we show how these limitations can be
addressed in a simple kernel-based framework.
3.3. Our approach: geodesic ﬂow kernel (GFK)
Our approach consists of the following steps: i) determine the optimal dimensionality of the subspaces to embed
domains; ii) construct the geodesic curve; iii) compute the
geodesic ﬂow kernel; iv) use the kernel to construct a classiﬁer with labeled data. We defer describing step i) to the
next section and focus on steps ii) and iii).
For step ii), we state only the main computational steps.
The detailed derivation can be found in and its references. We also omit step iv) for brevity, as it is the same as
constructing any other kernel-based classiﬁer.
Construct geodesic ﬂow Let PS, PT ∈RD×d denote the
two sets of basis of the subspaces for the source and target
domains. Let RS ∈RD×(D−d) denote the orthogonal complement to PS, namely RT
SPS = 0. Using the canonical
Euclidean metric for the Riemannian manifold, the geodesic
ﬂow is parameterized as Φ : t ∈ →Φ(t) ∈G(d, D)
under the constraints Φ(0) = PS and Φ(1) = PT . For
Φ(t) = PSU1Γ(t) −RSU2Σ(t),
where U1 ∈Rd×d and U2 ∈R(D−d)×d are orthonormal
matrices. They are given by the following pair of SVDs,
SPT = U1ΓV T,
SPT = −U2ΣV T .
Γ and Σ are d×d diagonal matrices. The diagonal elements
are cos θi and sin θi for i = 1, 2, . . . , d. Particularly, θi are
called the principal angles between PS and PT :
0 ≤θ1 ≤θ2 ≤· · · ≤θd ≤π/2
They measure the degree that subspaces “overlap”. Moreover, Γ(t) and Σ(t) are diagonal matrices whose elements
are cos(tθi) and sin(tθi) respectively.
Compute geodesic ﬂow kernel (GFK) The geodesic ﬂow
parameterizes how the source domain smoothly changes
to the target domain.
Consider the subspace Φ(t) for a
t ∈(0, 1) and compute Φ(t)Tx, ie, the projection of a feature vector x into this subspace. If x is from the source
domain and t is close to 1, then the projection will appear
more likely coming from the target domain and conversely
for t close to 0. Thus, using the projection to build a classi-
ﬁer would result in a model using a set of features that are
characteristic of both domains. Hence, this classiﬁer would
likely perform well on the target domain.
Which (or which set of) t should we use then? Our answer is surprising at the ﬁrst glance: all of them! Intuitively,
by expanding the original features with projections into all
subspaces, we force a measurement of similarity (as we will
be using inner products to construct classiﬁers) that is robust to any variation that leans either toward the source or
towards the target or in between. In other words, the net
effect is a representation that is insensitive to idiosyncrasies
in either domain. Computationally, however, we cannot use
this representation explicitly. Nevertheless, we next show
that there is no need to actually compute, store and manipulate inﬁnitely many projections.
For two original D-dimensional feature vectors xi and
xj, we compute their projections into Φ(t) for a continuous t from 0 to 1 and concatenate all the projections into
inﬁnite-dimensional feature vectors z∞
j . The inner
product between them deﬁnes our geodesic-ﬂow kernel,
(Φ(t)Txi)T(Φ(t)Txj) dt = xT
where G ∈RD×D is a positive semideﬁnite matrix. This is
precisely the “kernel trick”, where a kernel function induces
inner products between inﬁnite-dimensional features.
The matrix G can be computed in a closed-form from
previously deﬁned matrices:
G = [PSU1 RSU2]
where Λ1 to Λ3 are diagonal matrices, whose diagonal elements are
λ1i = 1+sin(2θi)
, λ2i = cos(2θi) −1
, λ3i = 1−sin(2θi)
Detailed derivations are given in the Supplementary.
Our approach is both conceptually and computationally
simpler when compared to the previous SGF approach. In
particular, we do not need to tune any parameters — the
only free parameter is the dimensionality of the subspaces
d, which we show below how to automatically infer.
3.4. Subspace disagreement measure (SDM)
For unsupervised domain adaptation, we must be able
to select the optimal d automatically, with unlabeled data
only. We address this challenge by proposing a subspace
disagreement measure (SDM).
To compute SDM, we ﬁrst compute the PCA subspaces
of the two datasets, PCAS and PCAT .
We also combine the datasets into one dataset and compute its subspace
PCAS+T . Intuitively, if the two datasets are similar, then
all three subspaces should not be too far away from each
other on the Grassmannian. The SDM captures this notion
and is deﬁned in terms of the principal angles (cf. eq. (3)),
D(d) = 0.5 [sin αd + sin βd]
where αd denotes the d-th principal angle between the
PCAS and PCAS+T and βd between PCAT and PCAS+T .
sin αd or sin βd is called the minimum correlation distance .
Note that D(d) is at most 1. A small value indicates
that both αd and βd are small, thus PCAS and PCAT are
aligned (at the d-th dimension). At its maximum value of
1, the two subspaces have orthogonal directions (i.e., αd =
βd = π/2). In this case, domain adaptation will become
difﬁcult as variances captured in one subspace would not be
able to transfer to the other subspace.
To identify the optimal d, we adopt a greedy strategy:
d∗= min{d|D(d) = 1}.
Intuitively, the optimal d∗should be as high as possible (to
preserve variances in the source domain for the purpose of
building good classiﬁers) but should not be so high that the
two subspaces start to have orthogonal directions.
3.5. Rank of domain (ROD)
Imagine we need to build a classiﬁer for a target domain
for object recognition. We have several datasets, Caltech-
101, PASCAL VOC, and ImageNet to choose from as the
source domain. Without actually running our domain adaptation algorithms and building classiﬁers, is it possible to
determine which dataset(s) would give us the best performance on the target domain?
To answer this question, we introduce a Rank of Domain
(ROD) metric that integrates two sets of information: geometrically, the alignment between subspaces, and statistically, KL divergences between data distributions once they
are projected into the subspaces.
We sketch the main idea in the following; the detailed
derivation is described in the Supplementary. Given a pair
of domains, computing ROD involves 3 steps: i) determine
the optimal dimensionality d∗for the subspaces (as in section 3.4); ii) at each dimension i ≤d∗, approximate the data
distributions of the two domains with two one-dimensional
Gaussians and then compute the symmetrized KL divergences between them; iii) compute the KL-divergence
weighted average of principal angles, namely,
R(S, T ) = 1
θi [KL(Si∥Ti) + KL(Ti∥Si)] .
Si and Ti are the two above-mentioned Gaussian distributions; they are estimated from data projected onto the principal vectors (associated with the i-th principal angle).
A pair of domains with smaller values of R(S, T ) are
more likely to adapt well: the two domains are both geometrically well-aligned (small principal angles) and similarly
distributed (small KL divergences). Empirically, when we
use the metric to rank various datasets as source domains,
we ﬁnd the ranking correlates well with their relative performance improvements on the target domain.
4. Experiments
We evaluate our methods in the context of object recognition. We ﬁrst compare our geodesic-ﬂow kernel method
to baselines and other domain adaptation methods .
We then report results that validate our automatic procedure
of selecting the optimal dimensionality of subspaces (section 3.4). Next we report results to demonstrate our Rank of
Domain (ROD) metric predicts well which source domain is
more suitable for domain adaptation. At last, we re-examine
the dataset bias problem, recently studied in , from the
perspective of “ease of adaptability”.
4.1. Setup
Our experiments use the three datasets which were studied in : Amazon (images downloaded from online merchants), Webcam (low-resolution images by a web camera),
and DSLR (high-resolution images by a digital SLR camera).
Additionally, to validate the proposed methods on
a wide range of datasets, we added Caltech-256 as a
fourth dataset. We regard each dataset as a domain.
We extracted 10 classes common to all four datasets:
TOURING-BIKE,
CALCULATOR,
Caltech-256 Amazon
DSLR Webcam
Figure 2. Example images from the MONITOR category in Caltech-
256, Amazon, DSLR, and Webcam. Caltech and Amazon images
are mostly from online merchants, while DSLR and Webcam images are from ofﬁces. (Best viewed in color.)
COMPUTER-KEYBOARD,
LAPTOP-101,
COMPUTER-MONITOR,
COMPUTER-MOUSE,
AND VIDEO-PROJECTOR.
There are 8 to 151
samples per category per domain, and 2533 images in total.
Fig. 2 highlights the differences among these domains with
example images from the category of MONITOR.
We report in the main text our results on the 10 common
classes. Moreover, we report in the Supplementary our results on 31 categories common to Amazon, Webcam and
DSLR, to compare directly to published results .
Our results on either the 10 or 31 common classes demonstrate the same trend that the proposed methods signiﬁcantly outperform existing approaches.
We follow similar feature extraction and experiment protocols used in previous work. Brieﬂy, we use SURF features
 and encode the images with 800-bin histograms with the
codebook trained from a subset of Amazon images. The
histograms are normalized ﬁrst and then z-scored to have
zero mean and unit standard deviation in each dimension.
For each pair of source and target domains, we conduct experiments in 20 random trials. In each trial, we randomly
sample labeled data in the source domain as training examples, and unlabeled data in the target domain as testing
examples. In semi-supervised domain adaptation, we also
sample a small number of images from the target domain
to augment the training set. More details on how data are
split are given in the Supplementary. We report averaged
accuracies on target domains as well as standard errors.
1-nearest neighbor is used as our classiﬁer as it does not
require cross-validating parameters. For our algorithms, the
dimensionality of subspaces are selected according to the
criterion in section 3.4. For methods we compare to, we use
what is recommended in the published work.
4.2. Results on unsupervised adaptation
Our baseline is OrigFeat, where we use original features, ie., without learning a new representation for adaptation. Other types of baselines are reported in the Suppl.
For our methods, we use two types of subspaces for the
Table 1. Recognition accuracies on target domains with unsupervised adaptation (C: Caltech, A: Amazon, W: Webcam, and D: DSLR)
GFK(PCA, PCA)
GFK(PLS, PCA)
Table 2. Recognition accuracies on target domains with semi-supervised adaptation (C: Caltech, A: Amazon, W: Webcam, and D: DSLR)
Metric 
GFK(PCA, PCA)
GFK(PLS, PCA)
GFK(PLS, PLS)
source data: PCA which is the PCA subspace and PLS
which is the Partial Least Squares (PLS) subspace. PLS
is similar to PCA except it takes label information into consideration, and thus can be seen as a form of supervised
dimension reduction . For the target domains, we use
only PCA as there is no label. Thus, there are two variants of our kernel-based method: GFK(PCA, PCA) and
GFK(PLS, PCA).
We also implement the method described in section 3.2 . We refer to it as SGF. As the authors of this
method suggest, we use the PCA subspaces for both domains. We also use the parameter settings reported in .
Table 1 summarizes the classiﬁcation accuracies as well
as standard errors of all the above methods for different pairings of the source and target domains. We report 8 pairings;
the rest are reported in the Supplementary. The best group
(differences up to one standard error) in each column are in
bold font and the second best group (differences up to one
standard error) are in italics and underlined.
All domain adaptation methods improve accuracy over
the baseline OrigFeat. Further, our GFK based methods
in general outperform SGF. Moreover, GFK(PLS, PCA)
performs the best. Two key factors may contribute to the
superiority of our method: i) the kernel integrates all the
subspaces along the ﬂow, and is hence able to model better the domain shift between the source and the target; ii)
this method uses a discriminative subspace (by PLS) in the
source domain to incorporate the label information. This
has the beneﬁt of avoiding projection directions that contain noise and very little useful discriminative information,
albeit making source and target domains look similar. PCA,
on the other hand, does not always yield subspaces that contain discriminative information. Consequently all the improvements by our GFK(PLS, PCA) over SGF are statistically signiﬁcant, with margins more than one standard error.
For a given target domain, there is a preferred source
domain which leads to the best performance, either using
OrigFeat or any of the domain adaptation methods. For
example, for the domain Webcam, the source domain DSLR
Dimensionality
Amazon −−> Caltech
Semisupervised
Unsupervised
Dimensionality
Webcam −−> DSLR
Semisupervised
Unsupervised
Figure 3. Selecting the optimal dimensionality d∗with SDM (sec.
3.4); selected d∗(where the arrows point to) leads to the best adaptation performance. (Best viewed in color)
is better than the domain Amazon. This might be attributed
to the similarity in DSLR and Webcam, illustrated in Fig. 2.
We analyze this in detail in section 4.5.
4.3. Results on semi-supervised adaptation
In semi-supervised adaptation, our algorithms have access to a small labeled set of target data. Therefore, we
also compare to GFK(PLS, PLS), and the metric learning
based method Metric which uses the correspondence
between source and target labeled data to learn a Mahalanobis metric to map data into a new feature space.
Table 2 shows the results of all methods. Our GFK(PLS,
PCA) is still the best, followed by GFK(PCA, PCA). Note
that though GFK(PLS, PLS) incorporates discriminative
information from both domains, it does not perform as well
as GFK(PLS, PCA). This is probably due to the lack of
enough labeled data in the target domains to give a reliable
estimate of PLS subspaces. The Metric method does not
perform well either, probably due to the same reason.
As in Table 1, for a given target domain, there is a “pal”
source domain that improves the performance the most.
Moreover, this pal is the same as the one in the setting of
unsupervised domain adaptation. Thus, we believe that this
“pal” relationship is intrinsic to datasets; in section 4.5, we
will analyze them with our ROD metric.
4.4. Selecting the optimal dimensionality
Being able to choose the optimal dimensionality for the
subspaces is an important property of our methods. Fig. 3
Table 3. Cross-dataset generalization with and without domain adaptation among domains with high and low “market values” 
No domain adaptation
Using domain adaptation
Mean Targets
Mean Targets
Improvement
Caltech101
Table 4. ROD values between 4 domains. Lower values signify
stronger adaptability of the corresponding source domain.
shows that the subspace disagreement measure (SDM) described in section 3.4 correlates well with recognition accuracies on the target domains. In the plots, the horizontal
axis is the proposed dimensionality (in log scale) and the
right vertical axis reports accuracies on both unsupervised
domain adaptation and semi-supervised domain adaptation.
The left vertical axis reports the values of SDM.
The plots reveal two conﬂicting forces at play. As the dimensionality increases, SDM—as a proxy to difference in
geometric structures—quickly rises and eventually reaches
its maximum value of 1. Beyond that point, adaptation becomes difﬁcult as the subspaces have orthogonal directions.
However, before the maximum value is reached, the geometric difference is countered by the increase in variances
— a small dimensionality would capture very little variances in the source domain data and would result in poor
accuracies on both domains. The tradeoff occurs at where
the geometric difference is just being maximized, justifying
our dimensionality selection criterion in eq. (8).
4.5. Characterizing datasets with ROD
Given a target domain and several choices of datasets
as source domains, identifying which one is the best to be
adapted not only has practical utility but also provides new
insights about how datasets are related to each other: ease
of adaptation functions as a barometer, indicating whether
two datasets are similar both geometrically and statistically,
and piercing through each dataset’s own idiosyncrasies.
To this end, we examine whether the Rank of Domain
(ROD) metric described in section 3.5 correlates with our
empirical ﬁndings in Table 1 and 2. We compute ROD using
PCA subspaces and report the values among the 4 domains
in Table 4. In general, ROD correlates well with recognition accuracies on the target domains and can reliably identify the best source domains to adapt. For example, when
Caltech is the target domain (the ﬁrst column), Amazon has
the smallest value and Amazon indeed leads to better classiﬁcation accuracies on Caltech than DSLR or Webcam.
If we group Caltech and Amazon into a meta-category
“Online” and DSLR and Webcam into another metacategory “Ofﬁce”, the distributions of ROD values with respect to the categories suggest that the domains with the
same meta-category have stronger similarity than domain
pairs crossing categories (such as Caltech and DSLR). Thus
ROD can also be used as a measure to partition datasets
into clusters, where datasets in the same cluster share latent properties that might be of surprise to their users — the
presence of such properties is probably not by design.
4.6. Easy to adapt: a new perspective on datasets?
Torralba and Efros study the sources of dataset bias in
several popular ones for object recognition . To quantify the quality of each dataset, they devise a “market value”
metric. Datasets with higher values are more diverse, and
therefore are likely to reﬂect better the richness of realworld objects. In particular, they point out that PASCAL
VOC 2007 and ImageNet have high values.
Building on their ﬁndings, we turn the table around and
investigate: how valuable are these datasets in improving a
target domain’s performance?
Table 3 summarizes our preliminary results on a subset
of datasets used in ; PASCAL VOC 2007 , ImageNet , and Caltech-101 . The recognition tasks are
to recognize the category PERSON and CAR. The crossdataset generalization results are shown on the left side of
the table, without using adaptation techniques (as in );
and the adaptation results using our kernel-based method
are on the right side of the table.
The rows are the source domain datasets and the columns
are the target domains.
The “Drop” columns report the
percentages of drop in recognition accuracies between the
source and the averaged accuracy on target domains, ie, the
“Mean Targets” columns. The rightmost “Improvement”
column is the percentage of improvement on target domains
due to the use of domain adaptation. Clearly, domain adaptation noticeably improves recognition accuracies on the
target domains. Caltech-101 is the exception where the improvement is marginal (47% vs. 46%). This corroborates
the low “market value” assigned to this dataset in .
PASCAL VOC 2007 has the smallest drop without domain adaptation so it would appear to be a better dataset
than the other two. Once we have applied domain adaptation, we observe a negative drop — ie, the performance on
the target domains is better than on the source domain itself!
However, its improvement is not as high as ImageNet’s.
Our conjecture is that the data in PASCAL VOC 2007
can be partitioned into two parts: one part is especially
“hard” to be adapted to other domains and the other part
is relatively “easy”. The reverse of the performance drop
suggests that the “easy” portion can be harvested by domain adaptation techniques. However, the beneﬁt is limited
due to the “hard” part. On the other end, for ImageNet, a
larger portion of its data is perhaps amenable to adaptation.
Hence, it attains a bigger improvement after adaptation.
In short, while PASCAL VOC 2007 and ImageNet are
assigned the same “market value” in , their usefulness
to building object recognition systems that can be applied
to other domains needs to be carefully examined in the context of adaptation. It might be beneﬁcial to incorporate the
notion of “ease of adaptability” in the process of evaluating
datasets — a concept worth further exploring and reﬁning.
5. Conclusion
We propose a kernel-based technique for domain adaptation. The techniquesembed datasets into Grassmann manifolds and constructing geodesic ﬂows between them to
model domain shift. The propose methods integrate an in-
ﬁnite number of subspaces to learn new feature representations that are robust to change in domains. On standard
benchmark tasks of object recognition, our methods consistently outperform other competing algorithms.
For future work, we plan to exploit latent structures beyond linear subspaces for domain adaptation.
Acknowledgements
This work is partially supported by NSF IIS#1065243
and CSSG (B. G., Y. S. and F. S.), and ONR ATL (K. G.).