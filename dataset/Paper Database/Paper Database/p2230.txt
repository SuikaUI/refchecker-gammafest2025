Perceptual Losses for Real-Time Style Transfer
and Super-Resolution
Justin Johnson, Alexandre Alahi, Li Fei-Fei
{jcjohns, alahi, feifeili}@cs.stanford.edu
Department of Computer Science, Stanford University
Abstract. We consider image transformation problems, where an input
image is transformed into an output image. Recent methods for such
problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel
work has shown that high-quality images can be generated by deﬁning
and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the beneﬁts of both approaches, and propose the use of perceptual loss functions for training
feed-forward networks for image transformation tasks. We show results
on image style transfer, where a feed-forward network is trained to solve
the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment
with single-image super-resolution, where replacing a per-pixel loss with
a perceptual loss gives visually pleasing results.
Keywords: Style transfer, super-resolution, deep learning
Introduction
Many classic problems can be framed as image transformation tasks, where a
system receives some input image and transforms it into an output image. Examples from image processing include denoising, super-resolution, and colorization,
where the input is a degraded image (noisy, low-resolution, or grayscale) and the
output is a high-quality color image. Examples from computer vision include semantic segmentation and depth estimation, where the input is a color image and
the output image encodes semantic or geometric information about the scene.
One approach for solving image transformation tasks is to train a feedforward convolutional neural network in a supervised manner, using a per-pixel
loss function to measure the diﬀerence between output and ground-truth images.
This approach has been used for example by Dong et al for super-resolution ,
by Cheng et al for colorization , by Long et al for segmentation , and by
Eigen et al for depth and surface normal prediction . Such approaches are
eﬃcient at test-time, requiring only a forward pass through the trained network.
However, the per-pixel losses used by these methods do not capture perceptual
diﬀerences between output and ground-truth images. For example, consider two
 
Johnson et al
Gatys et al 
Ground Truth
SRCNN 
Perceptual loss
Fig. 1. Example results for style transfer (top) and ×4 super-resolution (bottom). For
style transfer, we achieve similar results as Gatys et al but are three orders of
magnitude faster. For super-resolution our method trained with a perceptual loss is
able to better reconstruct ﬁne details compared to methods trained with per-pixel loss.
identical images oﬀset from each other by one pixel; despite their perceptual
similarity they would be very diﬀerent as measured by per-pixel losses.
In parallel, recent work has shown that high-quality images can be generated
using perceptual loss functions based not on diﬀerences between pixels but instead on diﬀerences between high-level image feature representations extracted
from pretrained convolutional neural networks. Images are generated by minimizing a loss function. This strategy has been applied to feature inversion by
Mahendran et al, to feature visualization by Simonyan et al and Yosinski et
al , and to texture synthesis and style transfer by Gatys et al . These
approaches produce high-quality images, but are slow since inference requires
solving an optimization problem.
In this paper we combine the beneﬁts of these two approaches. We train feedforward transformation networks for image transformation tasks, but rather than
using per-pixel loss functions depending only on low-level pixel information, we
train our networks using perceptual loss functions that depend on high-level
features from a pretrained loss network. During training, perceptual losses measure image similarities more robustly than per-pixel losses, and at test-time the
transformation networks run in real-time.
We experiment on two tasks: style transfer and single-image super-resolution.
Both are inherently ill-posed; for style transfer there is no single correct output,
and for super-resolution there are many high-resolution images that could have
generated the same low-resolution input. Success in either task requires semantic
reasoning about the input image. For style transfer the output must be semantically similar to the input despite drastic changes in color and texture; for superresolution ﬁne details must be inferred from visually ambiguous low-resolution
inputs. In principle a high-capacity neural network trained for either task could
implicitly learn to reason about the relevant semantics; however in practice we
Perceptual Losses for Real-Time Style Transfer and Super-Resolution
need not learn from scratch: the use of perceptual loss functions allows the transfer of semantic knowledge from the loss network to the transformation network.
For style transfer our feed-forward networks are trained to solve the optimization problem from ; our results are similar to both qualitatively and
as measured by objective function value, but are three orders of magnitude faster
to generate. For super-resolution we show that replacing the per-pixel loss with
a perceptual loss gives visually pleasing results for ×4 and ×8 super-resolution.
Related Work
Feed-forward image transformation. In recent years, a wide variety of feedforward image transformation tasks have been solved by training deep convolutional neural networks with per-pixel loss functions.
Semantic segmentation methods produce dense scene labels
by running a network in a fully-convolutional manner over an input image, training with a per-pixel classiﬁcation loss. moves beyond per-pixel losses by
framing CRF inference as a recurrent layer trained jointly with the rest of the
network. The architecture of our transformation networks are inspired by and
 , which use in-network downsampling to reduce the spatial extent of feature
maps followed by in-network upsampling to produce the ﬁnal output image.
Recent methods for depth and surface normal estimation are
similar in that they transform a color input image into a geometrically meaningful output image using a feed-forward convolutional network trained with perpixel regression or classiﬁcation losses. Some methods move beyond
per-pixel losses by penalizing image gradients or using a CRF loss layer 
to enforce local consistency in the output image. In a feed-forward model is
trained using a per-pixel loss to transform grayscale images to color.
Perceptual optimization. A number of recent papers have used optimization to generate images where the objective is perceptual, depending on highlevel features extracted from a convolutional network. Images can be generated to
maximize class prediction scores or individual features in order to understand the functions encoded in trained networks. Similar optimization techniques
can also be used to generate high-conﬁdence fooling images .
Mahendran and Vedaldi invert features from convolutional networks by
minimizing a feature reconstruction loss in order to understand the image information retained by diﬀerent network layers; similar methods had previously
been used to invert local binary descriptors and HOG features .
The work of Dosovitskiy and Brox is particularly relevant to ours, as they
train a feed-forward neural network to invert convolutional features, quickly
approximating a solution to the optimization problem posed by . However,
their feed-forward network is trained with a per-pixel reconstruction loss, while
our networks directly optimize the feature reconstruction loss of .
Style Transfer. Gatys et al perform artistic style transfer, combining
the content of one image with the style of another by jointly minimizing the
feature reconstruction loss of and a style reconstruction loss also based on
Johnson et al
Image Transform Net
Style Target
Content Target
Loss Network (VGG-16)
Fig. 2. System overview. We train an image transformation network to transform input
images into output images. We use a loss network pretrained for image classiﬁcation
to deﬁne perceptual loss functions that measure perceptual diﬀerences in content and
style between images. The loss network remains ﬁxed during the training process.
features extracted from a pretrained convolutional network; a similar method
had previously been used for texture synthesis . Their method produces highquality results, but is computationally expensive since each step of the optimization problem requires a forward and backward pass through the pretrained
network. To overcome this computational burden, we train a feed-forward network to quickly approximate solutions to their optimization problem.
Image super-resolution. Image super-resolution is a classic problem for
which a wide variety of techniques have been developed. Yang et al provide
an exhaustive evaluation of the prevailing techniques prior to the widespread
adoption of convolutional neural networks. They group super-resolution techniques into prediction-based methods (bilinear, bicubic, Lanczos, ), edgebased methods , statistical methods , patch-based methods [25,30,31,32,3
and sparse dictionary methods . Recently achieved excellent performance on single-image super-resolution using a three-layer convolutional neural
network trained with a per-pixel Euclidean loss. Other recent state-of-the-art
methods include .
As shown in Figure 2, our system consists of two components: an image transformation network fW and a loss network φ that is used to deﬁne several loss
functions ℓ1, . . . , ℓk. The image transformation network is a deep residual convolutional neural network parameterized by weights W; it transforms input images
x into output images ˆy via the mapping ˆy = fW (x). Each loss function computes
a scalar value ℓi(ˆy, yi) measuring the diﬀerence between the output image ˆy and
a target image yi. The image transformation network is trained using stochastic
gradient descent to minimize a weighted combination of loss functions:
W ∗= arg min
λiℓi(fW (x), yi)
Perceptual Losses for Real-Time Style Transfer and Super-Resolution
To address the shortcomings of per-pixel losses and allow our loss functions
to better measure perceptual and semantic diﬀerences between images, we draw
inspiration from recent work that generates images via optimization .
The key insight of these methods is that convolutional neural networks pretrained for image classiﬁcation have already learned to encode the perceptual
and semantic information we would like to measure in our loss functions. We
therefore make use of a network φ which as been pretrained for image classi-
ﬁcation as a ﬁxed loss network in order to deﬁne our loss functions. Our deep
convolutional transformation network is then trained using loss functions that
are also deep convolutional networks.
The loss network φ is used to deﬁne a feature reconstruction loss ℓφ
a style reconstruction loss ℓφ
style that measure diﬀerences in content and style
between images. For each input image x we have a content target yc and a style
target ys. For style transfer, the content target yc is the input image x and the
output image ˆy should combine the content of x = yc with the style of ys; we
train one network per style target. For single-image super-resolution, the input
image x is a low-resolution input, the content target yc is the ground-truth highresolution image, and the style reconstruction loss is not used; we train one
network per super-resolution factor.
Image Transformation Networks
Our image transformation networks roughly follow the architectural guidelines
set forth by Radford et al . We do not use any pooling layers, instead using
strided and fractionally strided convolutions for in-network downsampling and
upsampling. Our network body consists of ﬁve residual blocks using the architecture of . All non-residual convolutional layers are followed by spatial
batch normalization and ReLU nonlinearities with the exception of the output layer, which instead uses a scaled tanh to ensure that the output image has
pixels in the range . Other than the ﬁrst and last layers which use 9 × 9
kernels, all convolutional layers use 3 × 3 kernels. The exact architectures of all
our networks can be found in the supplementary material.
Inputs and Outputs. For style transfer the input and output are both color
images of shape 3 × 256 × 256. For super-resolution with an upsampling factor
of f, the output is a high-resolution image patch of shape 3 × 288 × 288 and
the input is a low-resolution patch of shape 3 × 288/f × 288/f. Since the image
transformation networks are fully-convolutional, at test-time they can be applied
to images of any resolution.
Downsampling and Upsampling. For super-resolution with an upsampling
factor of f, we use several residual blocks followed by log2 f convolutional layers
with stride 1/2. This is diﬀerent from who use bicubic interpolation to upsample the low-resolution input before passing it to the network. Rather than
relying on a ﬁxed upsampling function, fractionally-strided convolution allows
the upsampling function to be learned jointly with the rest of the network.
Johnson et al
Fig. 3. Similar to , we use optimization to ﬁnd an image ˆy that minimizes the
feature reconstruction loss ℓφ,j
feat(ˆy, y) for several layers j from the pretrained VGG-16
loss network φ. As we reconstruct from higher layers, image content and overall spatial
structure are preserved, but color, texture, and exact shape are not.
For style transfer our networks use two stride-2 convolutions to downsample
the input followed by several residual blocks and then two convolutional layers
with stride 1/2 to upsample. Although the input and output have the same size,
there are several beneﬁts to networks that downsample and then upsample.
The ﬁrst is computational. With a naive implementation, a 3×3 convolution
with C ﬁlters on an input of size C × H × W requires 9HWC2 multiply-adds,
which is the same cost as a 3 × 3 convolution with DC ﬁlters on an input of
shape DC × H/D × W/D. After downsampling, we can therefore use a larger
network for the same computational cost.
The second beneﬁt has to do with eﬀective receptive ﬁeld sizes. High-quality
style transfer requires changing large parts of the image in a coherent way;
therefore it is advantageous for each pixel in the output to have a large eﬀective
receptive ﬁeld in the input. Without downsampling, each additional 3×3 convolutional layer increases the eﬀective receptive ﬁeld size by 2. After downsampling
by a factor of D, each 3×3 convolution instead increases eﬀective receptive ﬁeld
size by 2D, giving larger eﬀective receptive ﬁelds with the same number of layers.
Residual Connections. He et al use residual connections to train very deep
networks for image classiﬁcation. They argue that residual connections make it
easy for the network to learn the identify function; this is an appealing property
for image transformation networks, since in most cases the output image should
share structure with the input image. The body of our network thus consists of
several residual blocks, each of which contains two 3 × 3 convolutional layers.
We use the residual block design of , shown in the supplementary material.
Perceptual Loss Functions
We deﬁne two perceptual loss functions that measure high-level perceptual and
semantic diﬀerences between images. They make use of a loss network φ pretrained for image classiﬁcation, meaning that these perceptual loss functions are
themselves deep convolutional neural networks. In all our experiments φ is the
16-layer VGG network pretrained on the ImageNet dataset .
Perceptual Losses for Real-Time Style Transfer and Super-Resolution
Fig. 4. Similar to , we use optimization to ﬁnd an image ˆy that minimizes the style
reconstruction loss ℓφ,j
style(ˆy, y) for several layers j from the pretrained VGG-16 loss
network φ. The images ˆy preserve stylistic features but not spatial structure.
Feature Reconstruction Loss. Rather than encouraging the pixels of the
output image ˆy = fW (x) to exactly match the pixels of the target image y, we
instead encourage them to have similar feature representations as computed by
the loss network φ. Let φj(x) be the activations of the jth layer of the network
φ when processing the image x; if j is a convolutional layer then φj(x) will be
a feature map of shape Cj × Hj × Wj. The feature reconstruction loss is the
(squared, normalized) Euclidean distance between feature representations:
feat(ˆy, y) =
∥φj(ˆy) −φj(y)∥2
As demonstrated in and reproduced in Figure 3, ﬁnding an image ˆy that
minimizes the feature reconstruction loss for early layers tends to produce images
that are visually indistinguishable from y. As we reconstruct from higher layers,
image content and overall spatial structure are preserved but color, texture,
and exact shape are not. Using a feature reconstruction loss for training our
image transformation networks encourages the output image ˆy to be perceptually
similar to the target image y, but does not force them to match exactly.
Style Reconstruction Loss. The feature reconstruction loss penalizes the output image ˆy when it deviates in content from the target y. We also wish to
penalize diﬀerences in style: colors, textures, common patterns, etc. To achieve
this eﬀect, Gatys et al propose the following style reconstruction loss.
As above, let φj(x) be the activations at the jth layer of the network φ for
the input x, which is a feature map of shape Cj × Hj × Wj. Deﬁne the Gram
j (x) to be the Cj × Cj matrix whose elements are given by
j (x)c,c′ =
φj(x)h,w,cφj(x)h,w,c′.
If we interpret φj(x) as giving Cj-dimensional features for each point on a
Hj × Wj grid, then Gφ
j (x) is proportional to the uncentered covariance of the
Johnson et al
Cj-dimensional features, treating each grid location as an independent sample.
It thus captures information about which features tend to activate together. The
Gram matrix can be computed eﬃciently by reshaping φj(x) into a matrix ψ of
shape Cj × HjWj; then Gφ
j (x) = ψψT /CjHjWj.
The style reconstruction loss is then the squared Frobenius norm of the difference between the Gram matrices of the output and target images:
style(ˆy, y) = ∥Gφ
j (ˆy) −Gφ
The style reconstruction loss is well-deﬁned even when ˆy and y have diﬀerent
sizes, since their Gram matrices will both have the same shape.
As demonstrated in and reproduced in Figure 5, generating an image ˆy
that minimizes the style reconstruction loss preserves stylistic features from the
target image, but does not preserve its spatial structure. Reconstructing from
higher layers transfers larger-scale structure from the target image.
To perform style reconstruction from a set of layers J rather than a single
layer j, we deﬁne ℓφ,J
style(ˆy, y) to be the sum of losses for each layer j ∈J.
Simple Loss Functions
In addition to the perceptual losses deﬁned above, we also deﬁne two simple loss
functions that depend only on low-level pixel information.
Pixel Loss. The pixel loss is the (normalized) Euclidean distance between the
output image ˆy and the target y. If both have shape C × H × W, then the pixel
loss is deﬁned as ℓpixel(ˆy, y) = ∥ˆy −y∥2
2/CHW. This can only be used when
when we have a ground-truth target y that the network is expected to match.
Total Variation Regularization. To encourage spatial smoothness in the
output image ˆy, we follow prior work on feature inversion and superresolution and make use of total variation regularizer ℓT V (ˆy).
Experiments
We perform experiments on two image transformation tasks: style transfer and
single-image super-resolution. Prior work on style transfer has used optimization
to generate images; our feed-forward networks give similar qualitative results but
are up to three orders of magnitude faster. Prior work on single-image superresolution with convolutional neural networks has used a per-pixel loss; we show
encouraging qualitative results by using a perceptual loss instead.
Style Transfer
The goal of style transfer is to generate an image ˆy that combines the content of
a target content image yc with the the style of a target style image ys. We train
one image transformation network per style target for several hand-picked style
targets and compare our results with the baseline approach of Gatys et al .
Perceptual Losses for Real-Time Style Transfer and Super-Resolution
Fig. 5. Our style transfer networks and minimize the same objective. We compare
their objective values on 50 images; dashed lines and error bars show standard deviations. Our networks are trained on 256 × 256 images but generalize to larger images.
Baseline. As a baseline, we reimplement the method of Gatys et al . Given
style and content targets ys and yc and layers j and J at which to perform
feature and style reconstruction, an image ˆy is generated by solving the problem
ˆy = arg min
feat(y, yc) + λsℓφ,J
style(y, ys) + λT V ℓT V (y)
where λc, λs, and λT V are scalars, y is initialized with white noise, and optimization is performed using L-BFGS. We ﬁnd that unconstrained optimization
of Equation 5 typically results in images whose pixels fall outside the range
 . For a more fair comparison with our method whose output is constrained
to this range, for the baseline we minimize Equation 5 using projected L-BFGS
by clipping the image y to the range at each iteration. In most cases optimization converges to satisfactory results within 500 iterations. This method
is slow because each L-BFGS iteration requires a forward and backward pass
through the VGG-16 loss network φ.
Training Details. Our style transfer networks are trained on the Microsoft
COCO dataset . We resize each of the 80k training images to 256 × 256 and
train our networks with a batch size of 4 for 40,000 iterations, giving roughly
two epochs over the training data. We use Adam with a learning rate of
1 × 10−3. The output images are regularized with total variation regularization
with a strength of between 1 × 10−6 and 1 × 10−4, chosen via cross-validation
per style target. We do not use weight decay or dropout, as the model does
not overﬁt within two epochs. For all style transfer experiments we compute
feature reconstruction loss at layer relu2_2 and style reconstruction loss at
layers relu1_2, relu2_2, relu3_3, and relu4_3 of the VGG-16 loss network φ.
Our implementation uses Torch and cuDNN ; training takes roughly 4
hours on a single GTX Titan X GPU.
Qualitative Results. In Figure 6 we show qualitative examples comparing
our results with those of the baseline method for a variety of style and content
images. In all cases the hyperparameters λc, λs, and λT V are exactly the same
between the two methods; all content images are taken from the MS-COCO 2014
validation set. Overall our results are qualitatively similar to the baseline.
Although our models are trained with 256 × 256 images, they can be applied
in a fully-convolutional manner to images of any size at test-time. In Figure 7
we show examples of style transfer using our models on 512 × 512 images.
Johnson et al
The Starry Night,
Vincent van Gogh,
Pablo Picasso,
Composition VII,
Kandinsky, 1913
The Great Wave oﬀ
Kanagawa, Hokusai,
The Simpsons
Fig. 6. Example results of style transfer using our image transformation networks. Our
results are qualitatively similar to Gatys et al but are much faster to generate (see
Table 1). All generated images are 256 × 256 pixels.
Perceptual Losses for Real-Time Style Transfer and Super-Resolution
Fig. 7. Example results for style transfer on 512 × 512 images. The model is applied
in in a fully-convolutional manner to high-resolution images at test-time. The style
images are the same as Figure 6.
In these results it is clear that the trained style transfer network is aware of
the semantic content of images. For example in the beach image in Figure 7 the
people are clearly recognizable in the transformed image but the background is
warped beyond recognition; similarly in the cat image, the cat’s face is clear in
the transformed image, but its body is not. One explanation is that the VGG-16
loss network has features which are selective for people and animals since these
objects are present in the classiﬁcation dataset on which it was trained. Our
style transfer networks are trained to preserve VGG-16 features, and in doing so
they learn to preserve people and animals more than background objects.
Quantitative Results. The baseline and our method both minimize Equation 5. The baseline performs explicit optimization over the output image, while
our method is trained to ﬁnd a solution for any content image yc in a single
forward pass. We may therefore quantitatively compare the two methods by
measuring the degree to which they successfully minimize Equation 5.
We run our method and the baseline on 50 images from the MS-COCO
validation set, using The Muse by Pablo Picasso as a style image. For the baseline
we record the value of the objective function at each iteration of optimization,
and for our method we record the value of Equation 5 for each image; we also
compute the value of Equation 5 when y is equal to the content image yc. Results
are shown in Figure 5. We see that the content image yc achieves a very high
loss, and that our method achieves a loss comparable to between 50 and 100
iterations of explicit optimization.
Although our networks are trained to minimize Equation 5 for 256 × 256
images, they are also successful at minimizing the objective when applied to
larger images. We repeat the same quantitative evaluation for 50 images at
512 × 512 and 1024 × 1024; results are shown in Figure 5. We see that even at
higher resolutions our model achieves a loss comparable to 50 to 100 iterations
of the baseline method.
Johnson et al
Gatys et al 
Image Size
15.86s 0.015s 212x 636x 1060x
10.97 32.91s
205x 615x 1026x
1024 × 1024 42.89 128.66s 214.44s 0.21s
208x 625x 1042x
Table 1. Speed (in seconds) for our style transfer network vs the optimization-based
baseline for varying numbers of iterations and image resolutions. Our method gives
similar qualitative results (see Figure 6) but is faster than a single optimization step
of the baseline method. Both methods are benchmarked on a GTX Titan X GPU.
Speed. In Table 1 we compare the runtime of our method and the baseline
for several image sizes; for the baseline we report times for varying numbers
of optimization iterations. Across all image sizes, we see that the runtime of
our method is approximately twice the speed of a single iteration of the baseline
method. Compared to 500 iterations of the baseline method, our method is three
orders of magnitude faster. Our method processes images of size 512 × 512 at 20
FPS, making it feasible to run style transfer in real-time or on video.
Single-Image Super-Resolution
In single-image super-resolution, the task is to generate a high-resolution output image from a low-resolution input. This is an inherently ill-posed problem, since for each low-resolution image there exist multiple high-resolution images that could have generated it. The ambiguity becomes more extreme as the
super-resolution factor grows; for large factors (×4, ×8), ﬁne details of the highresolution image may have little or no evidence in its low-resolution version.
To overcome this problem, we train super-resolution networks not with the
per-pixel loss typically used but instead with a feature reconstruction loss
(see Section 3) to allow transfer of semantic knowledge from the pretrained
loss network to the super-resolution network. We focus on ×4 and ×8 superresolution since larger factors require more semantic reasoning about the input.
The traditional metrics used to evaluate super-resolution are PSNR and
SSIM , both of which have been found to correlate poorly with human assessment of visual quality . PSNR and SSIM rely only on lowlevel diﬀerences between pixels and operate under the assumption of additive
Gaussian noise, which may be invalid for super-resolution. In addition, PSNR is
equivalent to the per-pixel loss ℓpixel, so as measured by PSNR a model trained
to minimize per-pixel loss should always outperform a model trained to minimize feature reconstruction loss. We therefore emphasize that the goal of these
experiments is not to achieve state-of-the-art PSNR or SSIM results, but instead
to showcase the qualitative diﬀerence between models trained with per-pixel and
feature reconstruction losses.
Model Details. We train models to perform ×4 and ×8 super-resolution by
minimizing feature reconstruction loss at layer relu2_2 from the VGG-16 loss
network φ. We train with 288×288 patches from 10k images from the MS-COCO
training set, and prepare low-resolution inputs by blurring with a Gaussian kernel
of width σ = 1.0 and downsampling with bicubic interpolation. We train with
Perceptual Losses for Real-Time Style Transfer and Super-Resolution
Ground Truth
This image
31.78 / 0.8577
28.43 / 0.8114
Ours (ℓpixel)
31.47 / 0.8573
28.40 / 0.8205
SRCNN 
32.99 / 0.8784
30.48 / 0.8628
Ours (ℓfeat)
29.24 / 0.7841
27.09 / 0.7680
Ground Truth
This Image
Set14 mean
BSD100 mean
21.69 / 0.5840
25.99 / 0.7301
25.96 / 0.682
Ours (ℓpixel)
21.66 / 0.5881
25.75 / 0.6994
25.91 / 0.6680
SRCNN 
22.53 / 0.6524
27.49 / 0.7503
26.90 / 0.7101
Ours (ℓfeat)
21.04 / 0.6116
24.99 / 0.6731
24.95 / 63.17
Fig. 8. Results for ×4 super-resolution on images from Set5 (top) and Set14 (bottom).
We report PSNR / SSIM for each example and the mean for each dataset. More results
are shown in the supplementary material.
a batch size of 4 for 200k iterations using Adam with a learning rate of
1×10−3 without weight decay or dropout. As a post-processing step, we perform
histogram matching between our network output and the low-resolution input.
Baselines. As a baseline model we use SRCNN for its state-of-the-art performance. SRCNN is a three-layer convolutional network trained to minimize
per-pixel loss on 33 × 33 patches from the ILSVRC 2013 detection dataset. SR-
CNN is not trained for ×8 super-resolution, so we can only evaluate it on ×4.
SRCNN is trained for more than 109 iterations, which is not computationally feasible for our models. To account for diﬀerences between SRCNN and our
model in data, training, and architecture, we train image transformation networks for ×4 and ×8 super-resolution using ℓpixel; these networks use identical
data, architecture, and training as the networks trained to minimize ℓfeat.
Evaluation. We evaluate all models on the standard Set5 , Set14 , and
BSD100 datasets. We report PSNR and SSIM , computing both only on
the Y channel after converting to the YCbCr colorspace, following .
Results. We show results for ×4 super-resolution in Figure 8. Compared to the
other methods, our model trained for feature reconstruction does a very good
job at reconstructing sharp edges and ﬁne details, such as the eyelashes in the
Johnson et al
Ground Truth
This image
Set14 mean
BSD100 mean
22.75 / 0.5946
23.80 / 0.6455
22.37 / 0.5518
22.11 / 0.5322
Ours (ℓpixel)
23.42 / 0.6168
24.77 / 0.6864
23.02 / 0.5787
22.54 / 0.5526
Ours (ℓfeat)
21.90 / 0.6083
23.26 / 0.7058
21.64 / 0.5837
21.35 / 0.5474
Fig. 9. Super-resolution results with scale factor ×8 on an image from the BSD100
dataset. We report PSNR / SSIM for the example image and the mean for each dataset.
More results are shown in the supplementary material.
ﬁrst image and the individual elements of the hat in the second image. The
feature reconstruction loss gives rise to a slight cross-hatch pattern visible under
magniﬁcation, which harms its PSNR and SSIM compared to baseline methods.
Results for ×8 super-resolution are shown in Figure 9. Again we see that our
ℓfeat model does a good job at edges and ﬁne details compared to other models,
such as the horse’s legs and hooves. The ℓfeat model does not sharpen edges
indiscriminately; compared to the ℓpixel model, the ℓfeat model sharpens the
boundary edges of the horse and rider but the background trees remain diﬀuse,
suggesting that the ℓfeat model may be more aware of image semantics.
Since our ℓpixel and our ℓfeat models share the same architecture, data, and
training procedure, all diﬀerences between them are due to the diﬀerence between
the ℓpixel and ℓfeat losses. The ℓpixel loss gives fewer visual artifacts and higher
PSNR values but the ℓfeat loss does a better job at reconstructing ﬁne details,
leading to pleasing visual results.
Conclusion
In this paper we have combined the beneﬁts of feed-forward image transformation tasks and optimization-based methods for image generation by training
feed-forward transformation networks with perceptual loss functions. We have
applied this method to style transfer where we achieve comparable performance
and drastically improved speed compared to existing methods, and to singleimage super-resolution where we show that training with a perceptual loss allows
the model to better reconstruct ﬁne details and edges.
In future work we hope to explore the use of perceptual loss functions for
other image transformation tasks, such as colorization and semantic segmentation. We also plan to investigate the use of diﬀerent loss networks to see whether
for example loss networks trained on diﬀerent tasks or datasets can impart image
transformation networks with diﬀerent types of semantic knowledge.
Perceptual Losses for Real-Time Style Transfer and Super-Resolution