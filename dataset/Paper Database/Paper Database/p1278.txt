CornerNet: Detecting Objects as Paired Keypoints
Hei Law · Jia Deng
Abstract We propose CornerNet, a new approach to
object detection where we detect an object bounding
box as a pair of keypoints, the top-left corner and the
bottom-right corner, using a single convolution neural
network. By detecting objects as paired keypoints, we
eliminate the need for designing a set of anchor boxes
commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network
better localize corners. Experiments show that Corner-
Net achieves a 42.2% AP on MS COCO, outperforming
all existing one-stage detectors.
Keywords Object Detection
1 Introduction
Object detectors based on convolutional neural networks (ConvNets) have achieved
state-of-the-art results on various challenging benchmarks . A common component of state-of-the-art
approaches is anchor boxes , which are boxes of various sizes and aspect ratios that serve as detection candidates. Anchor boxes
are extensively used in one-stage detectors , which can achieve results highly competitive with two-stage detectors while being
Princeton University, Princeton, NJ, USA
E-mail: 
Princeton Universtiy, Princeton, NJ, USA
more eﬃcient. One-stage detectors place anchor boxes
densely over an image and generate ﬁnal box predictions by scoring anchor boxes and reﬁning their coordinates through regression.
But the use of anchor boxes has two drawbacks.
First, we typically need a very large set of anchor boxes,
e.g. more than 40k in DSSD and more
than 100k in RetinaNet . This is because the detector is trained to classify whether each
anchor box suﬃciently overlaps with a ground truth
box, and a large number of anchor boxes is needed to
ensure suﬃcient overlap with most ground truth boxes.
As a result, only a tiny fraction of anchor boxes will
overlap with ground truth; this creates a huge imbalance between positive and negative anchor boxes and
slows down training .
Second, the use of anchor boxes introduces many
hyperparameters and design choices. These include how
many boxes, what sizes, and what aspect ratios. Such
choices have largely been made via ad-hoc heuristics,
and can become even more complicated when combined
with multiscale architectures where a single network
makes separate predictions at multiple resolutions, with
each scale using diﬀerent features and its own set of anchor boxes , who detect and group
keypoints in the context of multiperson human-pose estimation. Fig. 1 illustrates the overall pipeline of our
Another novel component of CornerNet is corner
pooling, a new type of pooling layer that helps a convolutional network better localize corners of bounding
boxes. A corner of a bounding box is often outside the
object—consider the case of a circle as well as the examples in Fig. 2. In such cases a corner cannot be localized based on local evidence. Instead, to determine
whether there is a top-left corner at a pixel location,
we need to look horizontally towards the right for the
topmost boundary of the object, and look vertically towards the bottom for the leftmost boundary. This motivates our corner pooling layer: it takes in two feature
maps; at each pixel location it max-pools all feature
vectors to the right from the ﬁrst feature map, maxpools all feature vectors directly below from the second
feature map, and then adds the two pooled results together. An example is shown in Fig. 3.
We hypothesize two reasons why detecting corners
would work better than bounding box centers or proposals. First, the center of a box can be harder to localize because it depends on all 4 sides of the object,
whereas locating a corner depends on 2 sides and is thus
easier, and even more so with corner pooling, which encodes some explicit prior knowledge about the deﬁnition of corners. Second, corners provide a more eﬃcient
way of densely discretizing the space of boxes: we just
need O(wh) corners to represent O(w2h2) possible anchor boxes.
We demonstrate the eﬀectiveness of CornerNet on
MS COCO . CornerNet achieves a
42.2% AP, outperforming all existing one-stage detectors. In addition, through ablation studies we show that
corner pooling is critical to the superior performance of
CornerNet. Code is available at 
princeton-vl/CornerNet.
2 Related Works
2.1 Two-stage object detectors
Two-stage approach was ﬁrst introduced and popularized by R-CNN . Two-stage detectors generate a sparse set of regions of interest (RoIs)
and classify each of them by a network. R-CNN generates RoIs using a low level vision algorithm . Each region is
then extracted from the image and processed by a ConvNet independently, which creates lots of redundant
computations. Later, SPP and Fast-
RCNN improve R-CNN by designing
a special pooling layer that pools each region from feature maps instead. However, both still rely on separate
proposal algorithms and cannot be trained end-to-end.
Faster-RCNN does away low level
proposal algorithms by introducing a region proposal
network (RPN), which generates proposals from a set of
CornerNet: Detecting Objects as Paired Keypoints
Fig. 2 Often there is no local evidence to determine the location of a bounding box corner. We address this issue by proposing
a new type of pooling layer.
feature maps
top-left corner pooling
Fig. 3 Corner pooling: for each channel, we take the maximum values (red dots) in two directions (red lines), each from a
separate feature map, and add the two maximums together (blue dot).
pre-determined candidate boxes, usually known as anchor boxes. This not only makes the detectors more eﬃcient but also allows the detectors to be trained end-toend. R-FCN further improves the eﬃciency of Faster-RCNN by replacing the fully connected
sub-detection network with a fully convolutional subdetection network. Other works focus on incorporating
sub-category information , generating object proposals at multiple scales with more contextual information , selecting better features , improving speed , cascade procedure and better training procedure and
SSD have popularized the one-stage
approach, which removes the RoI pooling step and detects objects in a single network. One-stage detectors
are usually more computationally eﬃcient than twostage detectors while maintaining competitive performance on diﬀerent challenging benchmarks.
SSD places anchor boxes densely over feature maps
from multiple scales, directly classiﬁes and reﬁnes each
anchor box. YOLO predicts bounding box coordinates
directly from an image, and is later improved in YOLO9000 by switching to anchor boxes.
DSSD and RON 
adopt networks similar to the hourglass network , enabling them to combine low-level and
high-level features via skip connections to predict bounding boxes more accurately. However, these one-stage
detectors are still outperformed by the two-stage detectors until the introduction of RetinaNet . In , the authors suggest that
the dense anchor boxes create a huge imbalance between positive and negative anchor boxes during training. This imbalance causes the training to be ineﬃcient
and hence the performance to be suboptimal. They propose a new loss, Focal Loss, to dynamically adjust the
weights of each anchor box and show that their onestage detector can outperform the two-stage detectors.
ReﬁneDet proposes to ﬁlter the an-
Hei Law, Jia Deng
chor boxes to reduce the number of negative boxes, and
to coarsely adjust the anchor boxes.
DeNet is a
two-stage detector which generates RoIs without using
anchor boxes. It ﬁrst determines how likely each location belongs to either the top-left, top-right, bottomleft or bottom-right corner of a bounding box. It then
generates RoIs by enumerating all possible corner combinations, and follows the standard two-stage approach
to classify each RoI. Our approach is very diﬀerent from
DeNet. First, DeNet does not identify if two corners
are from the same objects and relies on a sub-detection
network to reject poor RoIs. In contrast, our approach
is a one-stage approach which detects and groups the
corners using a single ConvNet. Second, DeNet selects
features at manually determined locations relative to
a region for classiﬁcation, while our approach does not
require any feature selection step. Third, we introduce
corner pooling, a novel type of layer to enhance corner
detection.
Point Linking Network (PLN) 
is an one-stage detector without anchor boxes. It ﬁrst
predicts the locations of the four corners and the center
of a bounding box. Then, at each corner location, it predicts how likely each pixel location in the image is the
center. Similarly, at the center location, it predicts how
likely each pixel location belongs to either the top-left,
top-right, bottom-left or bottom-right corner. It combines the predictions from each corner and center pair
to generate a bounding box. Finally, it merges the four
bounding boxes to give a bounding box. CornerNet is
very diﬀerent from PLN. First, CornerNet groups the
corners by predicting embedding vectors, while PLN
groups the corner and center by predicting pixel locations. Second, CornerNet uses corner pooling to better
localize the corners.
Our approach is inspired by Newell et al. on
Associative Embedding in the context of multi-person
pose estimation. Newell et al. propose an approach that
detects and groups human joints in a single network. In
their approach each detected human joint has an embedding vector. The joints are grouped based on the
distances between their embeddings. To the best of our
knowledge, we are the ﬁrst to formulate the task of
object detection as a task of detecting and grouping
corners with embeddings. Another novelty of ours is
the corner pooling layers that help better localize the
corners. We also signiﬁcantly modify the hourglass architecture and add our novel variant of focal loss to help better train the network.
3 CornerNet
3.1 Overview
In CornerNet, we detect an object as a pair of keypoints—
the top-left corner and bottom-right corner of the bounding box. A convolutional network predicts two sets of
heatmaps to represent the locations of corners of different object categories, one set for the top-left corners
and the other for the bottom-right corners. The network
also predicts an embedding vector for each detected corner such that the distance between
the embeddings of two corners from the same object
is small. To produce tighter bounding boxes, the network also predicts oﬀsets to slightly adjust the locations
of the corners. With the predicted heatmaps, embeddings and oﬀsets, we apply a simple post-processing
algorithm to obtain the ﬁnal bounding boxes.
Fig. 4 provides an overview of CornerNet. We use
the hourglass network as the backbone network of CornerNet. The hourglass network is
followed by two prediction modules. One module is for
the top-left corners, while the other one is for the bottomright corners. Each module has its own corner pooling
module to pool features from the hourglass network before predicting the heatmaps, embeddings and oﬀsets.
Unlike many other object detectors, we do not use features from diﬀerent scales to detect objects of diﬀerent
sizes. We only apply both modules to the output of the
hourglass network.
3.2 Detecting Corners
We predict two sets of heatmaps, one for top-left corners
and one for bottom-right corners. Each set of heatmaps
has C channels, where C is the number of categories,
and is of size H × W. There is no background channel.
Each channel is a binary mask indicating the locations
of the corners for a class.
For each corner, there is one ground-truth positive
location, and all other locations are negative. During
training, instead of equally penalizing negative locations, we reduce the penalty given to negative locations
within a radius of the positive location. This is because
a pair of false corner detections, if they are close to
their respective ground truth locations, can still produce a box that suﬃciently overlaps the ground-truth
box (Fig. 5). We determine the radius by the size of an
object by ensuring that a pair of points within the radius would generate a bounding box with at least t IoU
with the ground-truth annotation (we set t to 0.3 in all
experiments). Given the radius, the amount of penalty
reduction is given by an unnormalized 2D Gaussian,
CornerNet: Detecting Objects as Paired Keypoints
Hourglass Network
Embeddings
Corner Pooling
Top-left Corners
Bottom-right corners
Prediction Module
Prediction Module
Prediction Module
Fig. 4 Overview of CornerNet. The backbone network is followed by two prediction modules, one for the top-left corners and
the other for the bottom-right corners. Using the predictions from both modules, we locate and group the corners.
Fig. 5 “Ground-truth” heatmaps for training. Boxes (green
dotted rectangles) whose corners are within the radii of the
positive locations (orange circles) still have large overlaps
with the ground-truth annotations (red solid rectangles).
2σ2 , whose center is at the positive location and
whose σ is 1/3 of the radius.
Let pcij be the score at location (i, j) for class c
in the predicted heatmaps, and let ycij be the “groundtruth” heatmap augmented with the unnormalized Gaussians. We design a variant of focal loss :
(1 −pcij)α log (pcij)
if ycij = 1
(1 −ycij)β (pcij)α log (1 −pcij) otherwise
where N is the number of objects in an image, and
α and β are the hyper-parameters which control the
contribution of each point (we set α to 2 and β to 4 in
all experiments). With the Gaussian bumps encoded in
ycij, the (1 −ycij) term reduces the penalty around the
ground truth locations.
Many networks 
involve downsampling layers to gather global information and to reduce memory usage. When they are applied to an image fully convolutionally, the size of the
output is usually smaller than the image. Hence, a location (x, y) in the image is mapped to the location
in the heatmaps, where n is the downsampling factor. When we remap the locations from the
heatmaps to the input image, some precision may be
lost, which can greatly aﬀect the IoU of small bounding
boxes with their ground truths. To address this issue we
predict location oﬀsets to slightly adjust the corner locations before remapping them to the input resolution.
where ok is the oﬀset, xk and yk are the x and y coordinate for corner k. In particular, we predict one set of
oﬀsets shared by the top-left corners of all categories,
and another set shared by the bottom-right corners. For
training, we apply the smooth L1 Loss 
at ground-truth corner locations:
SmoothL1Loss (ok, ˆok)
3.3 Grouping Corners
Multiple objects may appear in an image, and thus multiple top-left and bottom-right corners may be detected.
We need to determine if a pair of the top-left corner and
bottom-right corner is from the same bounding box.
Our approach is inspired by the Associative Embedding method proposed by Newell et al. for the
task of multi-person pose estimation. Newell et al. detect all human joints and generate an embedding for
each detected joint. They group the joints based on the
distances between the embeddings.
The idea of associative embedding is also applicable
to our task. The network predicts an embedding vector
for each detected corner such that if a top-left corner
and a bottom-right corner belong to the same bounding box, the distance between their embeddings should
Hei Law, Jia Deng
be small. We can then group the corners based on the
distances between the embeddings of the top-left and
bottom-right corners. The actual values of the embeddings are unimportant. Only the distances between the
embeddings are used to group the corners.
We follow Newell et al. and use embeddings
of 1 dimension. Let etk be the embedding for the top-left
corner of object k and ebk for the bottom-right corner.
As in Newell and Deng , we use the “pull” loss to
train the network to group the corners and the “push”
loss to separate the corners:
(etk −ek)2 + (ebk −ek)2i
max (0, ∆−|ek −ej|) ,
where ek is the average of etk and ebk and we set ∆
to be 1 in all our experiments. Similar to the oﬀset
loss, we only apply the losses at the ground-truth corner
3.4 Corner Pooling
As shown in Fig. 2, there is often no local visual evidence for the presence of corners. To determine if a
pixel is a top-left corner, we need to look horizontally
towards the right for the topmost boundary of an object and vertically towards the bottom for the leftmost
boundary. We thus propose corner pooling to better localize the corners by encoding explicit prior knowledge.
Suppose we want to determine if a pixel at location
(i, j) is a top-left corner. Let ft and fl be the feature
maps that are the inputs to the top-left corner pooling
layer, and let ftij and flij be the vectors at location
(i, j) in ft and fl respectively. With H × W feature
maps, the corner pooling layer ﬁrst max-pools all feature vectors between (i, j) and (i, H) in ft to a feature
vector tij, and max-pools all feature vectors between
(i, j) and (W, j) in fl to a feature vector lij. Finally,
it adds tij and lij together. This computation can be
expressed by the following equations:
 ftij, t(i+1)j
 flij, li(j+1)
where we apply an elementwise max operation. Both
tij and lij can be computed eﬃciently by dynamic programming as shown Fig. 8.
We deﬁne bottom-right corner pooling layer in a
similar way. It max-pools all feature vectors between
(0, j) and (i, j), and all feature vectors between (i, 0)
and (i, j) before adding the pooled results. The corner
pooling layers are used in the prediction modules to
predict heatmaps, embeddings and oﬀsets.
The architecture of the prediction module is shown
in Fig. 7. The ﬁrst part of the module is a modiﬁed
version of the residual block . In this
modiﬁed residual block, we replace the ﬁrst 3 × 3 convolution module with a corner pooling module, which
ﬁrst processes the features from the backbone network
by two 3 × 3 convolution modules 1 with 128 channels
and then applies a corner pooling layer. Following the
design of a residual block, we then feed the pooled features into a 3×3 Conv-BN layer with 256 channels and
add back the projection shortcut. The modiﬁed residual
block is followed by a 3×3 convolution module with 256
channels, and 3 Conv-ReLU-Conv layers to produce the
heatmaps, embeddings and oﬀsets.
3.5 Hourglass Network
CornerNet uses the hourglass network as its backbone network. The hourglass network
was ﬁrst introduced for the human pose estimation task.
It is a fully convolutional neural network that consists
of one or more hourglass modules. An hourglass module
ﬁrst downsamples the input features by a series of convolution and max pooling layers. It then upsamples the
features back to the original resolution by a series of upsampling and convolution layers. Since details are lost
in the max pooling layers, skip layers are added to bring
back the details to the upsampled features. The hourglass module captures both global and local features
in a single uniﬁed structure. When multiple hourglass
modules are stacked in the network, the hourglass modules can reprocess the features to capture higher-level of
information. These properties make the hourglass network an ideal choice for object detection as well. In fact,
many current detectors already
adopted networks similar to the hourglass network.
Our hourglass network consists of two hourglasses,
and we make some modiﬁcations to the architecture
of the hourglass module. Instead of using max pool-
1 Unless otherwise speciﬁed, our convolution module consists of a convolution layer, a BN layer and a ReLU layer
CornerNet: Detecting Objects as Paired Keypoints
Fig. 6 The top-left corner pooling layer can be implemented very eﬃciently. We scan from right to left for the horizontal
max-pooling and from bottom to top for the vertical max-pooling. We then add two max-pooled feature maps.
Top-left Corner Pooling
Embeddings
3x3 Conv-ReLU
1x1 Conv-BN
3x3 Conv-BN
3x3 Conv-BN-ReLU
Top-left Corner Pooling Module
Fig. 7 The prediction module starts with a modiﬁed residual block, in which we replace the ﬁrst convolution module with
our corner pooling module. The modiﬁed residual block is then followed by a convolution module. We have multiple branches
for predicting the heatmaps, embeddings and oﬀsets.
ing, we simply use stride 2 to reduce feature resolution. We reduce feature resolutions 5 times and increase the number of feature channels along the way
(256, 384, 384, 384, 512). When we upsample the features,
we apply 2 residual modules followed by a nearest neighbor upsampling. Every skip connection also consists of
2 residual modules. There are 4 residual modules with
512 channels in the middle of an hourglass module. Before the hourglass modules, we reduce the image resolution by 4 times using a 7 × 7 convolution module
with stride 2 and 128 channels followed by a residual
block with stride 2 and 256 channels.
Following , we also add intermediate supervision in training. However, we do not add
back the intermediate predictions to the network as we
ﬁnd that this hurts the performance of the network. We
apply a 1 × 1 Conv-BN module to both the input and
output of the ﬁrst hourglass module. We then merge
them by element-wise addition followed by a ReLU and
a residual block with 256 channels, which is then used as
the input to the second hourglass module. The depth of
the hourglass network is 104. Unlike many other stateof-the-art detectors, we only use the features from the
last layer of the whole network to make predictions.
4 Experiments
4.1 Training Details
We implement CornerNet in PyTorch . The network is randomly initialized under the
default setting of PyTorch with no pretraining on any
external dataset. As we apply focal loss, we follow to set the biases in the convolution layers
that predict the corner heatmaps. During training, we
set the input resolution of the network to 511 × 511,
which leads to an output resolution of 128 × 128. To
reduce overﬁtting, we adopt standard data augmentation techniques including random horizontal ﬂipping,
random scaling, random cropping and random color
jittering, which includes adjusting the brightness, saturation and contrast of an image. Finally, we apply
PCA to the input image.
We use Adam to optimize
the full training loss:
L = Ldet + αLpull + βLpush + γLoﬀ
where α, β and γ are the weights for the pull, push and
oﬀset loss respectively. We set both α and β to 0.1 and
Hei Law, Jia Deng
Table 1 Ablation on corner pooling on MS COCO validation.
w/o corner pooling
w/ corner pooling
improvement
Table 2 Reducing the penalty given to the negative locations near positive locations helps signiﬁcantly improve the performance of the network
w/o reducing penalty
ﬁxed radius
object-dependent radius
Table 3 Corner pooling consistently improves the network performance on detecting corners in diﬀerent image quadrants,
showing that corner pooling is eﬀective and stable over both small and large areas.
mAP w/o pooling
mAP w/ pooling
improvement
Top-Left Corners
Top-Left Quad.
Bottom-Right Quad.
Bottom-Right Corners
Top-Left Quad.
Bottom-Right Quad.
γ to 1. We ﬁnd that 1 or larger values of α and β lead
to poor performance. We use a batch size of 49 and
train the network on 10 Titan X (PASCAL) GPUs (4
images on the master GPU, 5 images per GPU for the
rest of the GPUs). To conserve GPU resources, in our
ablation experiments, we train the networks for 250k
iterations with a learning rate of 2.5 × 10−4. When we
compare our results with other detectors, we train the
networks for an extra 250k iterations and reduce the
learning rate to 2.5 × 10−5 for the last 50k iterations.
4.2 Testing Details
During testing, we use a simple post-processing algorithm to generate bounding boxes from the heatmaps,
embeddings and oﬀsets. We ﬁrst apply non-maximal
suppression (NMS) by using a 3×3 max pooling layer on
the corner heatmaps. Then we pick the top 100 top-left
and top 100 bottom-right corners from the heatmaps.
The corner locations are adjusted by the corresponding oﬀsets. We calculate the L1 distances between the
embeddings of the top-left and bottom-right corners.
Pairs that have distances greater than 0.5 or contain
corners from diﬀerent categories are rejected. The average scores of the top-left and bottom-right corners are
used as the detection scores.
Instead of resizing an image to a ﬁxed size, we maintain the original resolution of the image and pad it with
zeros before feeding it to CornerNet. Both the original
and ﬂipped images are used for testing. We combine the
detections from the original and ﬂipped images, and apply soft-nms to suppress redundant
detections. Only the top 100 detections are reported.
The average inference time is 244ms per image on a
Titan X (PASCAL) GPU.
4.3 MS COCO
We evaluate CornerNet on the very challenging MS
COCO dataset . MS COCO contains
80k images for training, 40k for validation and 20k for
testing. All images in the training set and 35k images in
the validation set are used for training. The remaining
5k images in validation set are used for hyper-parameter
searching and ablation study. All results on the test set
are submitted to an external server for evaluation. To
provide fair comparisons with other detectors, we report our main results on the test-dev set. MS COCO
uses average precisions (APs) at diﬀerent IoUs and APs
for diﬀerent object sizes as the main evaluation metrics.
CornerNet: Detecting Objects as Paired Keypoints
w/o corner pooling
w/ corner pooling
Fig. 8 Qualitative examples showing corner pooling helps better localize the corners.
Table 4 The hourglass network is crucial to the performance of CornerNet.
FPN (w/ ResNet-101) + Corners
Hourglass + Anchors
Hourglass + Corners
4.4 Ablation Study
4.4.1 Corner Pooling
Corner pooling is a key component of CornerNet. To
understand its contribution to performance, we train
another network without corner pooling but with the
same number of parameters.
Tab. 1 shows that adding corner pooling gives signiﬁcant improvement: 2.0% on AP, 2.1% on AP50 and
2.1% on AP75. We also see that corner pooling is especially helpful for medium and large objects, improving their APs by 2.4% and 3.6% respectively. This is
expected because the topmost, bottommost, leftmost,
rightmost boundaries of medium and large objects are
likely to be further away from the corner locations.
Fig. 8 shows four qualitative examples with and without corner pooling.
4.4.2 Stability of Corner Pooling over Larger Area
Corner pooling pools over diﬀerent sizes of area in different quadrants of an image. For example, the top-left
corner pooling pools over larger areas both horizontally
and vertically in the upper-left quadrant of an image,
compared to the lower-right quadrant. Therefore, the
location of a corner may aﬀect the stability of the corner pooling.
We evaluate the performance of our network on detecting both the top-left and bottom-right corners in
diﬀerent quadrants of an image. Detecting corners can
be seen as a binary classiﬁcation task i.e. the groundtruth location of a corner is positive, and any location
outside of a small radius of the corner is negative. We
measure the performance using mAPs over all categories on the MS COCO validation set.
Tab. 3 shows that without corner pooling, the topleft corner mAPs of upper-left and lower-right quadrant are 66.1% and 60.8% respectively. Top-left corner pooling improves the mAPs by 3.1% (to 69.2%)
and 2.7% (to 63.5%) respectively. Similarly, bottomright corner pooling improves the bottom-right corner
mAPs of upper-left quadrant by 2.8% (from 53.4% to
56.2%), and lower-right quadrant by 2.6% (from 65.0%
to 67.6%). Corner pooling gives similar improvement to
corners at diﬀerent quadrants, show that corner pooling
is eﬀective and stable over both small and large areas.
4.4.3 Reducing Penalty to Negative Locations
We reduce the penalty given to negative locations around
a positive location, within a radius determined by the
size of the object (Sec. 3.2). To understand how this
helps train CornerNet, we train one network with no
penalty reduction and another network with a ﬁxed radius of 2.5. We compare them with CornerNet on the
validation set.
Tab. 2 shows that a ﬁxed radius improves AP over
the baseline by 2.7%, APm by 1.5% and APl by 5.3%.
Object-dependent radius further improves the AP by
Hei Law, Jia Deng
Table 5 CornerNet performs much better at high IoUs than other state-of-the-art detectors.
RetinaNet 
Cascade R-CNN 
Cascade R-CNN + IoU Net 
Table 6 Error analysis. We replace the predicted heatmaps and oﬀsets with the ground-truth values. Using the ground-truth
heatmaps alone improves the AP from 38.4% to 73.1%, suggesting that the main bottleneck of CornerNet is detecting corners.
w/ gt heatmaps
w/ gt heatmaps + oﬀsets
Fig. 9 Qualitative example showing errors in predicting corners and embeddings. The ﬁrst row shows images where CornerNet
mistakenly combines boundary evidence from diﬀerent objects. The second row shows images where CornerNet predicts similar
embeddings for corners from diﬀerent objects.
2.8%, APm by 2.0% and APl by 5.8%. In addition,
we see that the penalty reduction especially beneﬁts
medium and large objects.
4.4.4 Hourglass Network
CornerNet uses the hourglass network as its backbone network. Since the hourglass network is not commonly used in other state-of-the-art detectors, we perform an experiment to study the contribution of the hourglass network in CornerNet. We train
a CornerNet in which we replace the hourglass network
with FPN (w/ ResNet-101) , which is
more commonly used in state-of-the-art object detectors. We only use the ﬁnal output of FPN for predictions. Meanwhile, we train an anchor box based detector which uses the hourglass network as its backbone.
Each hourglass module predicts anchor boxes at multiple resolutions by using features at multiple scales during upsampling stage. We follow the anchor box design
in RetinaNet and add intermediate
supervisions during training. In both experiments, we
initialize the networks from scratch and follow the same
training procedure as we train CornerNet (Sec. 4.1).
Tab. 4 shows that CornerNet with hourglass network outperforms CornerNet with FPN by 8.2% AP,
and the anchor box based detector with hourglass network by 5.5% AP. The results suggest that the choice of
the backbone network is important and the hourglass
network is crucial to the performance of CornerNet.
CornerNet: Detecting Objects as Paired Keypoints
Table 7 CornerNet versus others on MS COCO test-dev. CornerNet outperforms all one-stage detectors and achieves results
competitive to two-stage detectors
Two-stage detectors
DeNet 
ResNet-101
CoupleNet 
ResNet-101
Faster R-CNN by G-RMI 
Inception-ResNet-v2 
Faster R-CNN+++ 
ResNet-101
Faster R-CNN w/ FPN 
ResNet-101
Faster R-CNN w/ TDM 
Inception-ResNet-v2
D-FCN 
Aligned-Inception-ResNet
Regionlets 
ResNet-101
Mask R-CNN 
ResNeXt-101
Soft-NMS 
Aligned-Inception-ResNet
LH R-CNN 
ResNet-101
Fitness-NMS 
ResNet-101
Cascade R-CNN 
ResNet-101
D-RFCN + SNIP 
DPN-98 
One-stage detectors
YOLOv2 
DarkNet-19
DSOD300 
DS/64-192-48-1
GRP-DSOD320 
DS/64-192-48-1
SSD513 
ResNet-101
DSSD513 
ResNet-101
ReﬁneDet512 (single scale) 
ResNet-101
RetinaNet800 
ResNet-101
ReﬁneDet512 (multi scale) 
ResNet-101
CornerNet511 (single scale)
Hourglass-104
CornerNet511 (multi scale)
Hourglass-104
Fig. 10 Example bounding box predictions overlaid on predicted heatmaps of corners.
4.4.5 Quality of the Bounding Boxes
A good detector should predict high quality bounding boxes that cover objects tightly. To understand the
quality of the bounding boxes predicted by CornerNet,
we evaluate the performance of CornerNet at multiple IoU thresholds, and compare the results with other
state-of-the-art detectors, including RetinaNet , Cascade R-CNN 
and IoU-Net .
Tab. 5 shows that CornerNet achieves a much higher
AP at 0.9 IoU than other detectors, outperforming Cascade R-CNN + IoU-Net by 3.9%, Cascade R-CNN by
7.6% and RetinaNet 2 by 7.3%. This suggests that Cor-
 
master/MODEL_ZOO.md
nerNet is able to generate bounding boxes of higher
quality compared to other state-of-the-art detectors.
4.4.6 Error Analysis
CornerNet simultaneously outputs heatmaps, oﬀsets,
and embeddings, all of which aﬀect detection performance. An object will be missed if either corner is
missed; precise oﬀsets are needed to generate tight bounding boxes; incorrect embeddings will result in many
false bounding boxes. To understand how each part contributes to the ﬁnal error, we perform an error analysis
by replacing the predicted heatmaps and oﬀsets with
the ground-truth values and evaluting performance on
the validation set.
Tab. 6 shows that using the ground-truth corner
heatmaps alone improves the AP from 38.4% to 73.1%.
Hei Law, Jia Deng
Fig. 11 Qualitative examples on MS COCO.
APs, APm and APl also increase by 42.3%, 40.7% and
30.0% respectively. If we replace the predicted oﬀsets
with the ground-truth oﬀsets, the AP further increases
by 13.0% to 86.1%. This suggests that although there
is still ample room for improvement in both detecting
and grouping corners, the main bottleneck is detecting
corners. Fig. 9 shows some qualitative examples where
the corner locations or embeddings are incorrect.
4.5 Comparisons with state-of-the-art detectors
We compare CornerNet with other state-of-the-art detectors on MS COCO test-dev (Tab. 7). With multiscale evaluation, CornerNet achieves an AP of 42.2%,
the state of the art among existing one-stage methods
and competitive with two-stage methods.
5 Conclusion
We have presented CornerNet, a new approach to object detection that detects bounding boxes as pairs of
corners. We evaluate CornerNet on MS COCO and
demonstrate competitive results.
CornerNet: Detecting Objects as Paired Keypoints
Acknowledgements This work is partially supported by
a grant from Toyota Research Institute and a DARPA grant
FA8750-18-2-0019. This article solely reﬂects the opinions and
conclusions of its authors.