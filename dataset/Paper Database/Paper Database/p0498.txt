AUTHOR(S):
Publisher citation:
OpenAIR citation:
Publisher copyright statement:
OpenAIR takedown statement:
This publication is made
________ open access.
This is the ______________________ version of an article originally published by ____________________________
__________________________________________________________________________________________
(ISSN _________; eISSN __________).
This publication is distributed under a CC ____________ license.
____________________________________________________
Section 6 of the “Repository policy for OpenAIR @ RGU” (available from provides guidance on the criteria under which RGU will
consider withdrawing material from OpenAIR. If you believe that this item is subject to any of these criteria, or for
any other reason should not be held on OpenAIR, then please contact with the details of
the item and the nature of your complaint.
HUSSEIN, A., GABER, M.M., ELYAN, E. and JAYNE, C.
Imitation learning: a survey of learning methods.
HUSSEIN, A., GABER, M.M., ELYAN, E. and JAYNE, C. 2017 Imitation learning: a survey of
learning methods. ACM computing surveys [online], 50(2), article 21. Available from:
 
HUSSEIN, A., GABER, M.M., ELYAN, E. and JAYNE, C. 2017 Imitation learning: a survey of
learning methods. ACM computing surveys, 50(2), article 21. Held on OpenAIR [online].
Available from: 
AUTHOR ACCEPTED
ACM computing surveys
©ACM, 2017. This is the author's version of the work. It is posted here by permission of ACM for your
personal use. Not for redistribution. The definitive version was published in ACM computing surveys, 50(2)
2017 
 
OpenAIR at RGU
Digitally signed by OpenAIR at RGU
DN: cn=OpenAIR at RGU, o=Robert Gordon
University, ou=Library,
email= , c=GB
Date: 2018.02.15 10:44:38 Z
Imitation Learning: A Survey of Learning Methods
Ahmed Hussein, School of Computing Science and Digital Media, Robert Gordon University
Mohamed Medhat Gaber, School of Computing and Digital Technology, Birmingham City University
Eyad Elyan, School of Computing Science and Digital Media, Robert Gordon University
Chrisina Jayne, School of Computing Science and Digital Media, Robert Gordon University
Imitation learning techniques aim to mimic human behavior in a given task. An agent (a learning machine)
is trained to perform a task from demonstrations by learning a mapping between observations and actions.
The idea of teaching by imitation has been around for many years, however, the ﬁeld is gaining attention
recently due to advances in computing and sensing as well as rising demand for intelligent applications. The
paradigm of learning by imitation is gaining popularity because it facilitates teaching complex tasks with
minimal expert knowledge of the tasks. Generic imitation learning methods could potentially reduce the
problem of teaching a task to that of providing demonstrations; without the need for explicit programming
or designing reward functions speciﬁc to the task. Modern sensors are able to collect and transmit high
volumes of data rapidly, and processors with high computational power allow fast processing that maps the
sensory data to actions in a timely manner. This opens the door for many potential AI applications that
require real-time perception and reaction such as humanoid robots, self-driving vehicles, human computer
interaction and computer games to name a few. However, specialized algorithms are needed to effectively
and robustly learn models as learning by imitation poses its own set of challenges. In this paper, we survey imitation learning methods and present design options in different steps of the learning process. We
introduce a background and motivation for the ﬁeld as well as highlight challenges speciﬁc to the imitation
problem. Methods for designing and evaluating imitation learning tasks are categorized and reviewed. Special attention is given to learning methods in robotics and games as these domains are the most popular in
the literature and provide a wide array of problems and methodologies. We extensively discuss combining
imitation learning approaches using different sources and methods, as well as incorporating other motion
learning methods to enhance imitation. We also discuss the potential impact on industry, present major
applications and highlight current and future research directions.
CCS Concepts: rGeneral and reference →Surveys and overviews; rComputing methodologies →
Learning paradigms; Learning settings; Machine learning approaches; Cognitive robotics; Control
methods; Distributed artiﬁcial intelligence; Computer vision;
General Terms: Design, Algorithms
Additional Key Words and Phrases: Imitation learning, learning from demonstrations, intelligent agents,
learning from experience, self-improvement, feature representations, robotics, deep learning, reinforcement
ACM Reference Format:
Ahmed Hussein, Mohamed M. Gaber, Eyad Elyan, and Chrisina Jayne, 2016. Imitation Learning: A Survey
of Learning Methods. ACM Comput. Surv. V, N, Article A (January YYYY), 35 pages.
DOI: 
Author’s addresses: A. Hussein, E. Elyan, and Chrisina Jayne School of Computing Science and Digital
Media, Robert Gordon University, Riverside East, Garthdee Road, Aberdeen AB10 7GJ, United Kingdom
M. M. Gaber, School of Computing and Digital Technology, Birmingham City University, 15 Bartholomew
Row, Birmingham B5 5JU, United Kingdom
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned
by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from .
c⃝YYYY ACM. 0360-0300/YYYY/01-ARTA $15.00
DOI: 
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
1. INTRODUCTION
In recent years, the demand for intelligent agents capable of mimicking human behavior has grown substantially. Advancement in robotics and communication technology
have given rise to many potential applications that need artiﬁcial intelligence that can
not only make intelligent decisions, but is able to perform motor actions realistically
in a variety of situations. Many future directions in technology rely on the ability of
artiﬁcial intelligence agents to behave as a human would when presented with the
same situation. Examples of such ﬁelds are self-driving vehicles, assistive robots and
human computer interaction. For the latter especially, opportunities for new applications are growing due to recent interest in consumer virtual reality and motion capture
systems1. In these applications and many robotics tasks, we are faced with the problem of executing an action given the current state of the agent and its surroundings.
The number of possible scenarios in a complex application is too large to cover by explicit programming and so a successful agent must be able to handle unseen scenarios.
While such a task may be formulated as an optimization problem, it has become widely
accepted that having prior knowledge provided by an expert is more effective and ef-
ﬁcient than searching for a solution from scratch [Schaal 1999] [Schaal et al. 1997]
[Bakker and Kuniyoshi 1996] [Billard et al. 2008]. In addition, optimization through
trial and error requires reward functions that are designed speciﬁcally for each task.
One can imagine that even for simple tasks, the number of possible sequences of actions an agent can take grows exponentially. Deﬁning rewards for such problems is
difﬁcult, and in many cases unknown.
One of the more natural and intuitive ways of imparting knowledge by an expert
is to provide demonstrations for the desired behavior that the learner is required to
emulate. It is much easier for the human teacher to transfer their knowledge through
demonstration than to articulate it in a way that the learner will understand [Raza
et al. 2012]. This paper reviews the methods used to teach artiﬁcial agents to perform
complex sequences of actions through imitation.
Imitation learning is an interdisciplinary ﬁeld of research. Existing surveys focus
on different challenges and perspectives of tackling this problem. Early surveys review the history of imitation learning and early attempts to learn from demonstration [Schaal 1999] [Schaal et al. 2003]. In [Billard et al. 2008] learning approaches
are categorized as engineering oriented and biologically oriented methods, [Ijspeert
et al. 2013] focus on learning methods from the viewpoint of dynamical systems, while
[Argall et al. 2009] address different challenges in the process of imitation such as
acquiring demonstrations, physical and sensory issues as well as learning techniques.
However, due to recent advancements in the ﬁeld and a surge in potential applications,
it is important at this time to conduct a survey that focuses on the computational methods used to learn from demonstrated behavior. More speciﬁcally, we review artiﬁcial
intelligence methods which are used to learn policies that solve problems according
to human demonstrations. By focusing on learning methods, this survey addresses
learning for any intelligent agent, whether it manifests itself as a physical robot or
a software agent (such as games, simulations, planning, etc. ). The reviewed literature addresses various applications, however, many of the methods used are generic
and can be applied to general motion learning tasks. The learning process is categorized into: creating feature representations, direct imitation and indirect learning. The
methods and sources of learning for each process are reviewed as well as evaluation
metrics and applications suitable for these methods.
1In the last two years the virtual reality market has attracted major technology companies and billions
of dollars in investment and is still rapidly growing. 
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
Imitation learning refers to an agent’s acquisition of skills or behaviors by observing
a teacher demonstrating a given task. With inspiration and basis stemmed in neuroscience, imitation learning is an important part of machine intelligence and human
computer interaction, and has from an early point been viewed as an integral part in
the future of robotics [Schaal 1999]. Another popular paradigm is learning through
trial and error; however, providing good examples to learn from expedites the process
of ﬁnding a suitable action model and prevents the agent from falling into local minima. Moreover, a learner could very well arrive on its own at a suitable solution, i.e one
that achieves a certain quantiﬁable goal, but which differs signiﬁcantly from the way
a human would approach the task. It is sometimes important for the learner’s actions
to be believable and appear natural. This is necessary in many robotic domains as well
as human computer interaction where the performance of the learner is only as good
as a human observer’s perception of it. It is therefore favorable to teach a learner the
desired behavior from a set of collected instances. However, it is often the case that
direct imitation of the expert’s motion doesn’t sufﬁce due to variations in the task such
as the positions of objects or inadequate demonstrations. Therefore imitation learning
techniques need to be able to learn a policy from the given demonstrations that can
generalize to unseen scenarios. As such the agent learns to perform the task rather
than deterministically copying the teacher.
The ﬁeld of imitation learning draws its importance from its relevance to a variety
of applications such as human computer interaction and assistive robots. It is being
used to teach robots of varying skeletons and degrees of freedom (DOF) to perform
an array of different tasks. Some examples are navigational problems, which typically
employ vehicle-like robots, with relatively lower degrees of freedom. These include
ﬂying vehicles [Sammut et al. 2014] [Abbeel et al. 2007] [Ng et al. 2006], or ground
vehicles [Silver et al. 2008] [Ratliff et al. 2007a] [Chernova and Veloso 2007a] [Ollis
et al. 2007]. Other applications focus on robots with higher degrees of freedom such as
humanoid robots [Mataric 2000a] [Asfour et al. 2008][Calinon and Billard 2007a] or
robotic arms [Kober and Peters 2010][Kober and Peters 2009b][M¨ulling et al. 2013].
High DOF humanoid robots can learn discrete actions such as standing up, and cyclic
tasks such as walking [Berger et al. 2008]. Although the majority of applications target
robotics, imitation learning applies to simulations [Berger et al. 2008] [Argall et al.
2007] and is even used in computer games [Thurau et al. 2004a] [Gorman 2009] [Ross
and Bagnell 2010].
Imitation learning works by extracting information about the behavior of the teacher
and the surrounding environment including any manipulated objects, and learning
a mapping between the situation and demonstrated behavior. Traditional machine
learning algorithms do not scale to high dimensional agents with high degrees of freedom [Kober and Peters 2010]. Specialized algorithms are therefore needed to create
adequate representations and predictions to be able to emulate motor functions in humans.
Similar to traditional supervised learning where examples represent pairs of features and labels, in imitation learning the examples demonstrate pairs of states and
actions. Where the state represents the current pose of the agent, including the position and velocities of relevant joints and the status of a target object if one exists (such
as position, velocity, geometric information, etc.). Therefore, Markov decision processes
(MDPs) lend themselves naturally to imitation learning problems and are commonly
used to represent expert demonstrations. The Markov property dictates that the next
state is only dependent on the previous state and action, which alleviates the need
to include earlier states in the state representation [Kober et al. 2013]. A typical imitation learning work ﬂow starts by acquiring sample demonstrations from an expert
which are then encoded as state-action pairs. These examples are then used to train
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
a policy. However, learning a direct mapping between state and action is often not
enough to achieve the required behavior. This can happen due to a number of issues
such as errors in acquiring the demonstrations, variance in the skeletons of the teacher
and learner (correspondence problem) or insufﬁcient demonstrations. Moreover, the
task performed by the learner may slightly vary from the demonstrated task due to
changes in the environment, obstacles or targets. Therefore, imitation learning frequently involves another step that requires the learner to perform the learned action
and re-optimize the learned policy according to its performance of the task. This selfimprovement can be achieved with respect to a quantiﬁable reward or learned from
examples. Many of these approaches fall under the wide umbrella of reinforcement
Figure 1 shows a workﬂow of an imitation learning process. The process starts by
capturing actions to learn from, this can be achieved via different sensing methods.
The data from the sensors is then processed to extract features that describe the state
and surroundings of the performer. The features are used to learn a policy to mimic
the demonstrated behavior. Finally the policy can be enhanced by allowing the agent
to act out the policy and reﬁne it based on its performance. This step may or may not
require the input of a teacher. It might be intuitive to think of policy-reﬁnement as
a post learning step, but in many cases it occurs in conjunction with learning from
demonstrations.
Imitation learning ﬂowchart
Imitation learning applications face a number of diverse challenges due to their
interdisciplinary nature:
— Starting with the process of acquiring demonstrations, whether capturing data from
sensors on the learner or the teacher, or using visual information, the captured signals are prone to noise and sensor errors. Similar problems arise during execution,
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
when the agent is sensing the environment. Noisy or unreliable sensing will result
in erroneous behavior even if the model is adequately trained. [Argall et al. 2009]
survey the different methods of gathering demonstrations and the challenges in each
— Another issue concerning demonstration is the correspondence problem [Dautenhahn and Nehaniv 2002]. Correspondence is the matching of the learner’s capabilities, skeleton and degrees of freedom to that of the teacher. Any discrepancies in the
size or structure between the teacher and learner need to be compensated for during training. Often in this case a learner can learn the shape of the movement from
demonstrations, then reﬁne the model through trial and error to achieve its goal.
— A related challenge is the problem of observability where the kinematics of the
teacher are not known to learner [Schaal et al. 2003]. If the demonstrations are not
provided by a designated teacher, the learner may not be aware of the capabilities
and possible actions of the teacher; it can only observe the effects of the teacher’s
actions and attempt to replicate them using its own kinematics.
— The learning process also faces practical problems as traditional machine learning
techniques do not scale well to high degrees of freedom [Kober and Peters 2010].
Due to the real-time nature of many imitation learning applications, the learning
algorithms are restricted by computing power and memory limitations; especially in
robotic applications that require on-board computers to perform the real-time processing.
— Moreover, complex behaviors can often be viewed as a trajectory of dependent micro
actions which violates the independent and identically distributed (i.i.d.) assumption
adopted in most machine learning practices. The learned policy must be able to adapt
its behavior based on previous actions and make corrections if necessary.
— The policy must also be able to adapt to variations in the task and the surrounding environment. The complex nature of imitation learning applications dictate that
agents must be able to reliably perform the task even under circumstances that vary
from the training demonstration.
— Tasks that entail human-robot interaction pose a new set of challenges. Naturally
safety is a chief concern in such applications [De Santis et al. 2008] [Ikemoto et al.
2012], and measures need to be taken to prevent injury of the human partners and
insure their safety. Moreover, other challenges concern the mechanics of the robot,
such as its ability to react to the humans’ force and adapt to their actions.
In the next section we formally present the problem of imitation learning and discuss
different ways to formulate the problem. Section 3 describes the different methods for
creating feature representations. Section 4 reviews direct imitation methods for learning from demonstrations. Section 5 surveys indirect learning techniques and presents
the different approaches to improve learned policies through optimizing reward functions and teachers’ behaviors. The paradigms for improving direct imitation through
indirect learning are also discussed. Section 6 reviews the use of imitation learning in
multi-agent scenarios. Section 7 describes different evaluation approaches and Section
8 shows the potential applications to utilize imitation learning. Finally, we present a
conclusion and discuss future directions in Section 9.
2. PROBLEM FORMULATION
In this section we formalize the problem of imitation learning and introduce some
preliminaries and deﬁnitions.
DEFINITION 1. The process of imitation learning is one by which an agent uses instances of performed actions to learn a policy that solves a given task.
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
DEFINITION 2. An agent is deﬁned as an entity that autonomously interacts within
an environment towards achieving or optimizing a goal [Russell and Norvig 2003].
An agent can be thought of as a software robot; it receives information from the environment by sensing or communication and acts upon the environment using a set of
actuators.
DEFINITION 3. A policy is a function that maps a state (a description of the agent,
such as pose, positions and velocities of various parts of the skeleton, and its relevant
surrounding) to an action. It is what the agent uses to decide which action to execute
when presented with a situation.
Policies can be learned from demonstration or experience. The demonstrations may
come from a designated teacher or another agent; the experience may be the agent’s
own or another’s. The difference between the two types of training instances is that
demonstrations provide the optimal action to a given state, and so the agent learns to
reproduce this behavior in similar situations. This makes demonstrations suited for
direct policy learning such as supervised learning methods. While experience shows
the performed action, which may not be optimal, but also provides the reward (or cost)
of performing that action given the current state, and so the agent learns to act in a
manner that maximizes its overall reward. Therefore reinforcement learning is mainly
used to learn from experience. More formally, demonstrations and experiences can be
deﬁned as follows.
DEFINITION 4. A demonstration is presented as a pair of input and output (x, y).
Where x is a vector of features describing the state at that instant and y is the action
performed by the demonstrator.
DEFINITION 5. An experience is presented as a tuple (s, a, r, s′) where s is the state,
a is the action taken at state s, r is the reward received for performing action a and s′ is
the new state resulting from that action.
It is clear from this formulation that learning from demonstration doesn’t require
the learner to know the cost function optimized by the teacher. It can simply optimize
the error of deviating from the demonstrated output such as the least square error in
supervised learning. More formally, from a set of demonstrations D = (xi, yi) an agent
learns a policy π such that:
u(t) = π(x(t), t, α)
Where u is the predicted action, x is the feature vector, t is the time and α is the set
of policy parameters that are changed through learning. While the time parameter t
is used to specify an instance of input and output, it is also input to the policy π as a
separate parameter.
DEFINITION 6. A policy that uses t in learning the parameters of the policy is called
a non-stationary policy i.e the
policy takes into consideration at what stage of the task the agent is currently acting.
DEFINITION 7. A stationary policy (autonomous) neglects the time parameter and
learns one policy for all steps in an action sequence.
One advantage of stationary policies is the ability to learn tasks where the horizon
(the time limit for actions) is large or unknown [Ross and Bagnell 2010]. While non
stationary policies are more naturally situated to learn motor trajectories i.e actions
that occur over a period of time and are comprised of multiple motor primitive executed sequentially. However, these policies are difﬁcult to adapt to unseen scenarios
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
and changes in the parameters of the task [Schaal et al. 2003]. Moreover, this failure to
adapt to new scenarios, at one point in the trajectory, can result in compounded errors
as the agent continues to perform the remainder of action. In light of these drawbacks,
methods for learning trajectories using stationary policies are motivated. An example
is the use of structured predictions [Ross et al. 2010] where the training demonstrations are aggregated with labeled instances at different time steps in the trajectory –
so time is encoded in the state. Alternatively, [Ijspeert et al. 2002a] learns attractor
landscapes from the demonstrations, creating a stationary policy that is attracted to
the demonstrated trajectory. This avoids compounded errors as the current state is
considered by the policy before executing each state of the trajectory.
Learning from experience is commonly formulated as a Markov decision process
(MDP). MDPs lend themselves naturally to motor actions, as they represent a stateaction network and are therefore suitable for reinforcement learning. In addition the
Markov property dictates that the next state is only dependent on the previous state
and action, regardless of earlier states. This timeless property promotes stationary
policies. There are different methods to learn from experience through reinforcement
learning that are out of the scope of this paper. For a survey and formal deﬁnitions of
reinforcement learning methods for intelligent agents, the reader is referred to [Kober
et al. 2013]. Note that both learning paradigms are similarly formulated with the exception of the cost function; the feature vector x(t) corresponds to s, u(t) corresponds
to a and x(t + 1) corresponds to the resulting state s′. It is therefore not uncommon
(especially in more recent research) to combine learning from demonstrations and experience to perform a task.
We now consider the predicted action u(t) in equation 1.
DEFINITION 8. An action u(t) can often represent a vector rather than a single
value. This means that the action is comprised of more than one decision executed simultaneously; such as pressing multiple buttons on a controller or moving multiple
joints in a robot.
Actions can also represent different levels of motor control: low level actions, motor
primitives and high level macro actions [Argall et al. 2009].
DEFINITION 9. Low level actions are those that execute simple commands such as
move forward and turn in navigation tasks, or jump and shoot in games.
These low level actions can be directly predicted using a supervised classiﬁer. Low
level actions also extend to regression when the predicted actions have continuous
values rather than a discrete set of actions (see learning motion).
DEFINITION 10. Motor primitives are simple building blocks that are executed in
sequence to perform complex motions. An action is broken down into basic unit actions
(often concerning one degree of freedom or actuator) that can be used to make up any
action that needs to be performed in the given problem.
These primitives are then learned by the policy. In addition to being useful in building complex actions from a discrete set of primitives, motor primitives can represent a
desired move in state space, since they can be used to reach any possible state. As in
MDPs described above, the transition from one state to another state based on which
action is taken is easily tracked when using motor primitives. In this case the output
of the policy in equation 1 can represent the change in the current state [Schaal et al.
2003] as follows:
˙x(t) = π(x(t), t, α)
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
DEFINITION 11. High level macro actions are decisions that determine the immediate plan of the agent. It is then broken down to lower level action sequences. Examples
of high level decisions are grasp object or perform forehand.
For a thorough formalization of learning from demonstrations, we refer the reader
to [Schaal et al. 2003].
3. FEATURE REPRESENTATIONS
Before learning a policy it is important to represent the observable state in a form that
is adequate and efﬁcient for training. This representation is called a feature vector.
A feature may include information about the learner, its environment, manipulable
objects and other agents in the experiment. Training features need to be adequate,
meaning that they convey enough information to form a solid policy to solve the task.
It is also important that the features can be extracted and processed efﬁciently with
respect to the time and computational restriction of imitation learning applications.
When capturing data, the important question is: what to imitate? In most real applications, the environment is often too complicated to be represented in its totality, because it usually has an abundance of irrelevant or redundant information. It is therefore necessary to consider which aspects of the demonstrations we want to present to
the learner.
3.1. Handling Demonstrations
Even before feature extraction stages, dealing with demonstrations poses a number
of challenges. A major issue is the correspondence problem introduced in section 1.
Creating correspondence mappings between teacher and learner can be computationally intensive, but some methods attempt to create such correspondence in real-time.
In [Jin et al. 2016] a projection of the teacher’s behavior to the agent’s motion space
is developed online by sparsely sampling corresponding features. Neural network can
also be utilized to improve the response time of inverse kinematics (IK) based methods
[Hwang et al. 2016] and ﬁnd trajectories appropriate for the learner’s motion space
based on the desired state of end-effectors. However, IK methods place no further restrictions on the agent’s behavior as long as the end effector is in the demonstrated
position [Mohammad and Nishida 2013]. This poses a problem for trajectory imitation applications such as gesture imitation. To alleviate this limitation [Mohammad
and Nishida 2013] propose a close-form solution to the correspondence problem based
on optimizing external (mapping between observed demonstrations and expected behavior of learner) and learner mapping (mapping between state of the learner and its
observed behavior). While most approaches store learned behaviors after mapping to
the learner’s motion space, in [Bandera 2010] human gestures are captured, identi-
ﬁed and learned in the motion space of the teacher. While that requires learning a
model for the teacher’s motion, it allows perception to be totally independent of the
learner’s constraints and facilitates human motion recognition. The learned motions
are ﬁnally translated to the robot’s motion space before execution. A different approach
is to address correspondence in reward space where corresponding reward proﬁles for
the demonstrators and the robot are built [Gonz´alez-Fierro et al. 2013]. The difference
between them is optimized with respect to the robots internal constraints to ensure
the feasibility of the developed trajectory. This enables learning from demonstrations
by multiple human teachers with different physical characteristics.
Another challenge concerning demonstrations is incomplete or inaccurate demonstrations. Statistical models can deal with sensor error or inaccurate demonstrations,
however, incomplete demonstrations can result in suboptimal behavior [Khansari-
Zadeh and Billard 2011]. In [Khansari-Zadeh and Billard 2011] it is noted that a robot
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
provided only with demonstrations starting from the right of the target, will start
by moving to the familiar position if initialized in a different position. In [Kim et al.
2013] a method for learning from limited and inaccurate data is proposed, where the
demonstrations are used to constraint a reinforcement learning algorithm that learns
from trial and error. Combining learning from demonstrations and experience is extensively investigated in subsection 5.1. As an alternative way to cope with the lack of
good demonstrations, [Grollman and Billard 2012] learn a policy from failed demonstrations where the agent is trained to avoid repeating unsuccessful behavior.
Demonstrations need not be provided by a dedicated teacher but can instead be
observed from another agent. In this case an important question is what to imitate
from the continuous behavior being demonstrated. In [Mohammad and Nishida 2012]
learning from unplanned demonstrations is addressed by segmenting actions from the
demonstrations and estimating the signiﬁcance of the behavior in these segments to
the investigated task according to 3 criteria: (1) Change detection is used to discover
signiﬁcant regions of the demonstrations, (2) constrained motif discovery identiﬁes recurring behaviors and (3) change-causality explores the causality between the demonstrated behavior and changes in the environment. Similarly, in [Tan 2015] acquired
recordings of a human hand are segmented into basic behaviors before extracting relevant features and learning behavior generation. The feature extraction and behavior
generation are performed with respect to 3 attributes: (1) Preconditions, which are environmental condition required for the task. (2) Internal constraints, which are characteristics of the agent that restrict its behavior. (3) Post results, which represent the
consequences of a behavior.
Regardless of the source of the signal, captured data may be represented in different
ways. We categorize representations as: raw features, designed or engineered features,
and extracted features. Figure 2 Shows the relations between different feature representations.
Features relation diagram
Extracting binary features from an image
3.2. Raw Features
Raw data captured from the source can be used directly for training. If the raw features
convey adequate information and are of an appropriate number of dimensions, they
can be suitable for learning with no further processing. This way no computational
overhead is spent in calculating features.
In [Ross and Bagnell 2010] the agent learns to play 2 video games: Mario Bros, a
classic 2D platformer, and Super TuX Kart, a 3D kart racing game. In both cases,
the input is a screenshot of the game at the current frame with the number of pixels
reduced. In Super TuX Kart a lower dimensional version of the image is directly input
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
to the classiﬁer without extracting any designed features. The image is down-sampled
to 24 × 18 pixels to avoid the complications that come with high dimensional feature
vectors. An image of that size with three color channels yields a 1,296 feature vector.
3.3. Manually Designed Features
These are features that are extracted using specially designed functions. These methods incorporate expert knowledge about the data and application to determine what
useful information can be derived by processing the raw data. [Torrey et al. 2005] extract features from the absolute positions of players on a soccer ﬁeld. More meaningful
information such as relative distance from a player, distance and angle to the goal, and
length and angles of player triangles are calculated. The continuous features are discretized into overlapping tiles. This transformation of features from a numeric domain
to binary tiles is reported to signiﬁcantly improve learning.
Manually designed features are popular with learning from visual data. They play
an important part in computer vision methods that are used to teach machines by
demonstration. Computer vision approaches are popular from an early point in teaching robots to behave from demonstrations [Schaal 1999]. These approaches rely on
detecting objects of interest and tracking their movement to capture the demonstrated
actions in a form that can be used for learning. In [Demiris and Dearden 2005] a
computer vision system is employed to create representations that are used to train
a bayesian belief network. Training samples are generated by detecting and tracking
objects in the scene; which is performed by clustering regions of the image according
to their position and movement properties. [Billard and Matari´c 2001] imitate human
movement by tracking relevant points on the human body. A computer vision system
is used to detect human arm movement and track motion based on markers worn by
the performer. The system only learns when motion is detected by observing change
in the marker positions. In a recent study [Hwang et al. 2016], humanoid robots are
trained to imitate human motion from visual observation. Demonstrations are captured using a stereo-vision system to create a 3D image sequence. The demonstrator’s
body is isolated from the sequence and a set of predetermined points on the upper and
lower body are identiﬁed. Subsequently the extracted features are used to estimate the
demonstrator’s posture along the trajectory. A variation of inverse-kinematics that employs neural networks is used to reproduce the key posture features in the humanoid
For the Mario bros game in [Ross and Bagnell 2010], the source signal is the screenshot at the current frame. The image is divided into 22 × 22 equally sized cells. For
each cell, 14 binary features (the value of each feature can be 0 or 1) are generated;
each signifying whether or not the cell contains a certain object such as enemies, obstacles and/or power-ups. As such, each cell can contain between 0 and 14 of the predeﬁned objects. A demonstration is made up of the last 4 frames (so as to contain
information about the direction in which objects are moving) as well as the last 4 chosen actions. [Ortega et al. 2013] use a similar grid to represent the environment, but
add more numerical features and features describing the state of the character.
Figure 3 illustrates dividing an image to sub-patches and extracting binary features.
3.4. Extracted Features
Feature extraction techniques automatically process raw data to extract the features
used for learning. The most relevant information is extracted and mapped to a different domain usually of a lower dimensionality. When dealing with high DOF robots,
describing the posture of the robot using the raw values of the joints can be ineffective due to the high number of dimensions. This is more pronounced if the robot only
uses a limited number of joints to perform the action rendering most of the features
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
irrelevant. This issue also applies to visual information. If the agent observes its surroundings using visual sensors, it is provided with high dimensional data in the form
of pixels per frame. However, at any given point, most of the pixels in the captured
frame would probably be irrelevant to the agent or contain redundant information.
Principal component analysis (PCA) can be used to project the captured data onto orthogonal axes and represent the state of the agent in lower dimensions. This technique
has been widely used with high DOF robots [Ikemoto et al. 2012] [Berger et al. 2008]
[Vogt et al. 2014] [Calinon and Billard 2007b]. In [Curran et al. 2015], PCA is used to
extract features in a Mario game. Data describing the state of the playable character,
dangers, rewards and obstacles is projected onto as few as 4 dimensions. It is worth
mentioning that the three types of features (raw, designed and extracted) were used in
the literature to provide representations for the same Mario task.
Deep learning approaches [Bengio 2009] can also be used to extract features without
expert knowledge of the data. These approaches ﬁnd success in automatically learning
features from high dimensional data; especially when no established sets of features
are available. In a recent study [Mnih et al. 2015], Deep Q Learning (DQN) is used to
learn features from high dimensional images. The aim of this technique is to enable
a generic model to learn a variety of complex problems automatically. The method is
tested on 49 Atari games, each with different environments, goals and actions. Therefore, it is beneﬁcial to be able to extract features automatically from the captured
signals (in this case screenshots of the Atari games at each frame) rather than manually design speciﬁc features for each problem. A low resolution (84 × 84) version of
the colored frames is used as input to a deep convolutional neural network (CNN) that
is coupled with Q based reinforcement learning to automatically learn a variety of different problems through trial and error. The results in many cases surpass other AI
agents and in some cases are comparable to human performance. Similarly, [Koutn´ık
et al. 2013] use deep neural networks to learn from video streams in a car racing game.
Note that these examples utilize deep neural networks with reinforcement learning,
without employing a teacher or optimal demonstrations. However the feature extraction techniques can be used to learn from demonstrations or experience alike. Since the
success of DQN, several variations of deep reinforcement learning have emerged that
utilize actor-critic methods [Mnih et al. 2016] [Lillicrap et al. 2015] which allow for potential combinations with learning from demonstrations. In [Guo et al. 2014] learning
from demonstrations is applied on the same Atari benchmark [Bellemare et al. 2012].
A supervised network is used to train a policy using samples from a high performing
but non real-time agent. This approach is reported to outperform agents that learn
from scratch through reinforcement learning. In [Levine et al. 2015] deep learning is
used to train a robot to perform a number of object manipulation tasks using guided
policy search (see section on reinforcement learning).
Automatically extracted features have the advantage of minimizing the task speciﬁc
knowledge required for training agents. Which allows the creation of more generic
learning approaches that can be used to learn a variety of tasks directly from demonstrations with minimal tweaking. Learning with extracted features is gaining popularity due to recent advancements in deep learning. The success of deep learning methods in a variety of applications [Ciresan et al. 2012] [Krizhevsky et al. 2012] promotes
learning from raw data without designing what to learn from the demonstrations. That
being said, deep learning can also be used to extract higher level features from manually selected features. This approach allows for the extraction of complex features
while limiting computation by manually selecting relevant information from the raw
data. In recent attempts to teach an agent to play the board game ‘Go’ [Clark and
Storkey 2015] [Silver et al. 2016], the board is divided into a 19 × 19 grid. Each cell in
the grid consists of a feature vector describing the state of the game in this partition of
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
the board. This state representation is input into a deep convolutional neural network
that extracts higher level features and maps the learned features to actions.
4. LEARNING MOTION
We now address the different methods for learning a policy from demonstrations. After
considering what to learn, this process is concerned with the question how to learn?
The most straight forward way to learn a policy from demonstrations is direct imitation. That is to learn a supervised model from the demonstration, where the action
provided by the expert acts as the label for a given instance. The model is then capable of predicting the appropriate action when presented with a situation. Supervised
learning methods are categorized into classiﬁcation and regression.
4.1. Classiﬁcation
Classiﬁcation is a popular task in machine learning where observations are automatically categorized into a ﬁnite set of classes. A classiﬁer h(x) is used to predict the class
y to which an independent observation x belongs; where y ∈Y , Y = {y1, y2 . . . yp} is
a ﬁnite set of classes, and x = {x1, x2 . . . xm} is a vector of m features. In supervised
classiﬁcation, h(x) is trained using a dataset of n labeled samples (x(i), y(i)), where
x(i) ∈X, y(i) ∈Y and i = 1, 2 . . . n.
Classiﬁcation approaches are used when the learner’s actions can be categorized into
discrete classes [Argall et al. 2009]. This is suitable for applications where the action
can be viewed as a decision such as navigation [Chernova and Veloso 2007b] and ﬂight
simulators [Sammut et al. 2014]. In [Chernova and Veloso 2007b], a Gaussian mixture models (GMM) is trained to predict navigational decisions. Meta classiﬁers are
used in [Ross and Bagnell 2010] to learn a policy to play computer games. The base
classiﬁer used in this paper is a neural network. In The Kart racing game the analog
joystick commands are discretized into 15 buckets, reducing the problem to a 15 class
classiﬁcation problem. So the neural network used had 15 output nodes. The Mario
Bros game uses a discrete controller. Actions are selected by pressing one or more of
4 buttons. So in the neural network, the action for a frame is represented by 4 output nodes. This enables the classiﬁer to choose multiple classes for the same instance.
Although the results are promising, it is argued that using an Inverse Optimal Control (IOC) technique [Ratliff et al. 2007b] as the base classiﬁer might be beneﬁcial. In
[Ross et al. 2010] the experiments are repeated this time using regression (see regression section) to learn the analog input in Super Tux Kart. For Mario Bros, 4 Support
Vector Machine (SVM) classiﬁers replace the neural network to predict the value of
each of the 4 binary classes. Classiﬁcation can also be used to make decisions that
entail lower level actions. In [Raza et al. 2012] high level decision are predicted by
the classiﬁer in a multi-agent soccer simulation. Decisions such as ‘Approach ball’ and
‘Dribble towards goal’ can then be deterministically executed using lower level actions.
An empirical study is conducted to evaluate which classiﬁers are best suited for the imitation learning task. Four different classiﬁers are compared with respect to accuracy
and learning time. The results show that a number of classiﬁers can perform predictions with comparable accuracy, however, the learning time relative to the number of
demonstrations can vary greatly [Raza et al. 2012]. Recurrent neural networks (RNN)
are used in [Rahmatizadeh et al. 2016] to learn trajectories for object manipulation
from demonstrations. RNNs incorporates memory of past actions when considering
the next action. Storing memory enables the network to learn corrective behavior such
as recovery from failure given that the teacher demonstrates such a scenario.
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
4.2. Regression
Regression methods are used to learn actions in a continuous space. Unlike classiﬁcation, regression methods map the input state to a numeric output that represents
an action. Thus they are suitable for low level motor actions rather than higher level
decisions. Especially when actions are represented continuous values, such as input
from a joystick [Ross et al. 2010]. The regressor I(x) maps an independent sample x
to a continuous value y rather than a set of classes. Where y ∈R, the set of real numbers. Similarly the regressor is trained using a set of labeled samples (x(i), y(i)), where
y(i) ∈R and i = 1, 2 . . . n.
A commonly used technique is locally weighted regression (LWR). LWR is suitable
for learning trajectories, as these motions are made up of sequences of continuous values. Examples of such motions are batting tasks [Kober and Peters 2009c] [Ijspeert
et al. 2002b] (where the agent is required to execute a motion trajectory in order to
pass by a point and hit a target); and walking [Nakanishi et al. 2004] where the agent
needs to produce a trajectory that results in smooth stable motion. A more comprehensive application is table tennis. [M¨ulling et al. 2013] use Linear Bayesian Regression
to teach a robot arm to play a continuous game of table tennis. The agent is required
to move with precision in a continuous 3D space in different situations, such as when
hitting the ball, recovering position after a hit and preparing for the opponent’s next
move. Another paradigm commonly used for regression is artiﬁcial neural networks
(ANN). Neural networks differ from other regression techniques in that they are demanding in training time and training samples. Neural network approaches are often
inspired by biology and neuroscience studies, and attempt to emulate the learning and
imitation process in humans and animals [Billard et al. 2008]. The use of regression
with a dynamic system of motor primitives has produced a number of applications
for learning discrete and rhythmic motions [Ijspeert et al. 2002a] [Schaal et al. 2007]
[Kober et al. 2008], though most approaches focus on direct imitation without further optimization [Kober and Peters 2009a]. In such applications, a dynamic system
represents a single degree of freedom (DOF) as each DOF has a different goal and
constraints [Schaal et al. 2007].
Dynamic systems can be combined with probabilistic machine learning methods to
reap the beneﬁts of both approaches. This allows the extraction of patterns that are
important to a given task and generalization to different scenarios while maintaining the ability to adapt and correct movement trajectories in real time [Calinon et al.
2012]. In [Calinon et al. 2012] the estimation of dynamical systems’ parameters is
represented as a Gaussian mixture regression (GMR) problem. This approach has an
advantage over LWR based approaches as it allows learning of the activation functions along with the motor actions. The proposed method is used to learn time-based
and time-invariant movement. In [Rozo et al. 2015] a similar GMM based method is
used in a task-parametrized framework which allows shaping the robot’s motion as
a function of the task parameters. Human demonstrations are encoded to reﬂect parameters that are relevant to the task at hand and identify the position, velocity and
force constraints of the task. This encoding allows the framework to derive the state in
which the robot should be, and optimize the movement of the robot accordingly. This
approach is used in a Human Robot Collaboration (HRC) context and aims to optimize
human intervention as well as robot effort. Deep learning is combined with dynamical
systems in [Chen et al. 2015]. Dynamic movement primitives (DMP) are embedded
into autoencoders that learn representations of movement from demonstrated data.
Autoencoders non-linearly map features to a lower dimensional latent space in the
hidden layer. However, in this approach, the hidden units are constrained to DMPs to
limit the hidden layer into representing the dynamics of the system.
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
In both classiﬁcation and regression methods, a design decision can be made regarding the learning models resources. Lazy learners such as kNN and LWR do not
need training but need to retain all training samples when performing predictions.
On the other hand, trained models such as ANN and SVM require training time, but
once a model is created the training samples are no longer needed and only the model
is stored, which saves memory. These models can also result in very short prediction
4.3. Hierarchical Models
Rather than using a single model to reproduce human behavior, a hierarchical model
can be employed that breaks down the learned actions into a number of phases. A classiﬁcation model can be used to decide which action or sub-action, from a set of possible
actions, should be performed at a given time. A different model is then used to deﬁne
the details of the selected action, where each possible low-level action has a designated
model. [Bentivegna et al. 2004] use a hierarchical approach for imitation learning on
two different problems. The ﬁrst is Air Hockey which is played against an opponent,
and the objective is to shoot a puck into the opponent’s goal while protecting your own.
The second game is marble maze; the maze can be tilted around different axis to move
the ball towards the end of the maze. Each task has a set of low level actions called motor primitives that make up the playing possibilities for the agent (e.g., Straight shot,
Defend Goal, and Roll ball to corner). In the ﬁrst stage, a nearest neighbor classiﬁer is
used to select the action to be performed. By observing the state of the game the classiﬁer searches for the most similar instances in the demonstrations provided by the
human expert, and retrieves the primitive selected by the human at that point. The
next step is to deﬁne the goal of the selected action, for example the velocity of the ball
or the position of the puck when the primitive is completed. The goal is then used in a
regression model to ﬁnd the parameters of the action that would optimize the desired
goal. The goal is derived from the k nearest neighbor demonstrations found in the previous step. The goals in those demonstrations are input in a local weighted regression
model to perform the primitive. In a similar fashion, [Chernova and Veloso 2008] use a
classiﬁer to make decisions in a sorting task consisting of the following macro actions
(wait, sort left, sort right and pass). Each macro action entails temporal motor actions
such as picking up a ball, moving and placing the ball.
Example of hierarchical learning of actions
Table I shows a list of methods used for direct imitation in the literature. Along with
the year, the domain in which imitation learning was used, and whether additional
learning methods where used to improve learning from demonstrations. Popular applications in robotics are given their own category, such as navigation or batting (which
are applications where a robot limb moves to make contact with an object, such as table tennis). More diverse or generic tasks are listed as robotics. The table shows that
robotics and games are popular domains for imitation learning. They cover a wide variety of applications where an intelligent agent acts in the real world and in simulated
environments respectively. Robotics is an attractive domain for AI research due to the
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
huge potential in applications that can take advantage of sensing and motor control in
the physical world. While video games can be attractive because they alleviate many
challenges such as data capturing and sensor error; and thus allow development of
new complex learning methods in a controlled and easily reproducible environment.
Moreover, games have built-in scoring measures that can facilitate evaluation and
even designing the learning process in reinforcement learning approaches.
5. INDIRECT LEARNING
In this section, we discuss indirect ways to learn policies that can complement or replace direct imitation. The policy can be reﬁned from demonstrations, experience or
observation to be more accurate or to be more general and robust against unseen circumstances.
It is often the case that direct imitation on its own is not adequate to reproduce
robust, human-like behavior in intelligent agents. This limitation can be attributed
to two main factors: (1) errors in demonstration, and (2) poor generalization. Due to
limitations of data acquisition techniques, such as correspondence problem, sensor error and physical inﬂuences in kinesthetic demonstrations [Argall et al. 2009], direct
imitation can lead to inaccurate or unstable performance, especially in tasks that require precise motion in continuous space such as reaching or batting. For example, in
[Berger et al. 2008] a robot attempting to walk by directly mimicking the demonstrations would fall because the demonstrations do not accurately take into consideration
the physical properties involved in the task such as the robot’s weight and center of
mass. However, reﬁnement of the policy through trial and error would take these factors into account and produce a stable motion.
While generalization is an important issue in all machine learning practices, a special case of generalization is highlighted in imitation learning applications. It is common that human demonstrations are provided as sequence of actions. The dependence
of each action on the previous part of the sequence violates the ‘iid’ assumption of training samples that is integral to generalization in supervised learning [Ross and Bagnell 2010]. Moreover, since human experts provide only correct examples, the learner
is unequipped to handle errors in the trajectory. If the learner deviates from the optimal performance at any point in the trajectory (which is expected in any machine
learning task), it would be presented with an unseen situation that the model is not
trained to accommodate for. A clear example is provided in [Togelius et al. 2007] where
supervised learning was used to learn a policy to drive a car. Given that human demonstrations contained only ‘good driving’ with no crashes or close calls, when error occurs
and the car deviates from demonstrated trajectories, the learner does not know how to
5.1. Reinforcement Learning
Reinforcement learning (RL) learns a policy to solve a problem via trial and error.
DEFINITION 12. In RL, an agent is modeled as a Markov Decision Process (MDP)
that learns to navigate in a state space. A ﬁnite MDP consists of a tuple (S, A, T, R),
where S is a ﬁnite set of states, A is the set of possible actions, T is the set of state
transition probabilities and R is a reward function. TPsa contain a set of probabilities
where Psa is the probability of arriving at state s given action a and where a ∈A and
s ∈S. The reward function R(sk, ak, sk+1) returns an immediate reward for taking an
action in a given state and ending up in a new state ak →sk+1 where k is the time step.
This reward is discounted over time by a discount factor γ ∈[0, 1) and the goal of the
agent is to maximize the expected discounted reward at each time step.
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
Table I. Direct Learning Methods.
Learning Method
Self-improvement
[Lin 1992]
navigation
Artiﬁcial Neural Networks (ANN)
[Pook and Ballard 1993]
object manipulation
Hidden Markov Model
Neighbor (KNN)
[Mataric 2000b]
[Billard and Matari´c 2001]
[Ijspeert et al. 2002b]
Regression (LWR)
[Geisler 2002]
video game
Naive Bayes (NB), Decision Tree (DT), ANN
[Oztop and Arbib 2002]
object manipulation
[Nicolescu and Mataric 2003]
object manipulation
graph based method
[Dixon and Khosla 2004]
navigation
[Ude et al. 2004]
optimization
[Nakanishi et al. 2004]
[Bentivegna et al. 2004]
[Thurau et al. 2004b]
bayesian methods
[Aler et al. 2005]
soccer simulation
[Torrey et al. 2005]
soccer simulation
Rule based learning
[Saunders et al. 2006]
navigation, object manipulation
[Chernova and Veloso 2007b]
navigation
Model (GMM)
[Guenter et al. 2007]
object manipulation
[Togelius et al. 2007]
games/driving
[Schaal et al. 2007]
[Calinon and Billard 2007b]
object manipulation
[Berger et al. 2008]
direct recording
[Asfour et al. 2008]
object manipulation
[Coates et al. 2008]
aerial vehicle
Expectation
Maximization (EM)
[Mayer et al. 2008]
[Kober and Peters 2009c]
[Munoz et al. 2009]
games/driving
[Cardamone et al. 2009]
games/driving
[Ross et al. 2010]
Machine (SVM)
[Mu˜noz et al. 2010]
games/driving
[Ross and Bagnell 2010]
[Geng et al. 2011]
robot grasping
[Ikemoto et al. 2012]
assistive robots
[Judah et al. 2012]
benchmark tasks
linear logistic regression
[Vlachos 2012]
structured datasets
passiveaggressive algorithm
[Raza et al. 2012]
soccer simulation
ANN, NB, DT, PART
[M¨ulling et al. 2013]
Regression
[Ortega et al. 2013]
[Niekum et al. 2013]
[Rozo et al. 2013]
[Vogt et al. 2014]
[Droniou et al. 2014]
[Brys et al. 2015b]
benchmark tasks
Rule based learning
[Levine et al. 2015]
object manipulation
[Silver et al. 2016]
board game
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
RL starts off with a random policy and modiﬁes its parameters based on rewards
gained from executing this policy. Reinforcement learning can be used on its own to
learn a policy for a variety of robotic applications. However, if a policy is learned from
demonstration, reinforcement learning can be applied to ﬁne tune the parameters. Providing positive or negative examples to train a policy helps reinforcement learning by
reducing the search space available [Billard et al. 2008]. Enhancing the policy using
RL is sometimes necessary if there are physical discrepancies between the teacher and
the learner or to alleviate errors in acquiring the demonstrations. RL can also be useful to train the policy for unseen situations that are not covered in the demonstrations.
Applying reinforcement learning to the learned policy instead of a random one can signiﬁcantly speed up the RL process and avoids the risk of the policy from converging
into a local minimum. Moreover RL can ﬁnd a policy to perform a task that does not
look normal to the human observer. In applications where the learner interacts with a
human, it is important for the user to intuitively recognize the agent’s actions. This is
common in cases where robots are introduced into established environments (such as
homes and ofﬁces) to interact with untrained human users [Calinon and Billard 2008].
By applying Reinforcement learning to a policy learned from human demonstrations
the problem of unfamiliar behavior can be avoided. In imitation learning methods, reinforcement learning is often combined with learning from demonstrations to improve
a learned policy when the ﬁtness of the performed task can be evaluated.
In early research, teaching using demonstrations of successful actions was used to
improve and speed up reinforcement learning. In [Lin 1992], reinforcement learning is
used to learn a policy to play a game in a 2D dynamic environment. Different methods
for enhancing the RL policy are examined. The results demonstrate that teaching the
learner with demonstrations improves its score and helps prevent the learner from
falling in local minima. It is also noted that the improvement from teaching increases
with the difﬁculty of the task. Solutions to simple problems can be easily inferred
without requiring demonstrations from an expert. But as the complexity of the task
increases the advantage of learning from demonstrations becomes more signiﬁcant,
and even necessary for successful learning in more difﬁcult tasks [Lin 1991].
In [Guenter et al. 2007] Gaussian mixed regression (GMR) is used to train a robot
on an object grasping task. Since unseen scenarios such as obstacles and the variable
location of the object are expected in this application, reinforcement learning is used
to explore new ways to perform the task. The trained system is a dynamic system
that performs damping on the imitated trajectories. This allows the robot to smoothly
achieve its target and prevents reinforcement learning from resulting in oscillations.
Using damping in dynamic systems is a common approach when combining imitation
learning and reinforcement learning [Kober and Peters 2010][Kober et al. 2013].
An impressive application of imitation and reinforcement learning is training an
agent to play the board game ‘Go’ that rivals human experts [Silver et al. 2016]. A
deep convolutional neural network is trained using past games. Then reinforcement
learning is used to reﬁne the weights of the network and improve the policy.
A different approach to combine learning from demonstrations with reinforcement
learning is employed in [Brys et al. 2015a]. Rather than using the demonstrations to
train an initial policy, they are used to derive prior knowledge for reward shaping [Ng
et al. 1999]. A reward function is used to encourage sub-achievements in the task,
such as milestones reached in the demonstrations. This reward function is combined
with the primary reward function to supply the agent with the cost of its actions. This
paradigm of using expert demonstrations to derive a reward function is similar to
inverse reinforcement learning approaches [Abbeel and Ng 2004].
Policy search methods are a subset of reinforcement learning that lend themselves
naturally to robotic applications as they scale to high dimensional MDPs [Kober et al.
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
2013]. Therefore policy search methods are a good ﬁt to integrate with imitation learning methods. A policy gradient method is used in [Kohl and Stone 2004] to improve an
existing policy that can be created through supervised learning or explicit programming. A similar approach [Peters and Schaal 2008] is used within a dynamic system
that was previously used for supervised learning from demonstrations [Ijspeert et al.
2002b]. This led to a series of work that utilizes the dynamic system in [Ijspeert et al.
2002b] to learn from demonstrations and subsequently use reinforcement learning for
self-improvement [Kober and Peters 2010] [Kober and Peters 2014] [Buchli et al. 2011]
[Pastor et al. 2011]. This framework is used to teach robotic arms a number of applications such as ball in cup, ball paddling [Kober and Peters 2010] [Kober and Peters
2009c] and playing table tennis [M¨ulling et al. 2013]. Rather than using reinforcement
learning to reﬁne a policy trained from demonstrations, demonstrations can be used
to guide the policy search. In [Levine and Koltun 2013] differential dynamic programming is used to generate guiding samples from human demonstrations. These guiding
samples help the policy search explore high reward regions of the policy space.
Recurrent neural networks are incorporated into guided policy search in [Zhang
et al. 2016] to facilitate dealing with partially observed problems. Past memories are
augmented to the state space and are considered when predicting the next action. A
supervised approach uses demonstrated trajectories to decide which memories to store
while reinforcement learning is used to optimize the policy including the memory state
A different way to utilize reinforcement learning in imitation learning is to use RL
to provide demonstrations for direct imitation. This approach does not need a human
teacher as the policy is learned from scratch using trial and error and then used to
generate demonstrations for training. One reason for generating demonstrations and
training a supervised model rather than using the RL policy directly is that the RL
method does not act in real-time [Guo et al. 2014]. Another situation is when the RL
policy is learned in a controlled environment. In [Levine et al. 2015] reinforcement
learning is used to learn a variety of robotic tasks in a controlled environment. Information such as the position of target objects is available during this phase. A deep
convolutional neural network is then trained using demonstrations from the RL policy.
The neural network learns to map visual input to actions and thus learns to perform
the tasks without the information needed in the RL phase. This mimics human demonstrations as humans utilize expert knowledge – that is not incorporated in the training
process – to provide demonstrations.
For a comprehensive survey of reinforcement learning in robotics, the reader is referred to [Kober et al. 2013]
5.2. Optimization
Optimization approaches can also be used to ﬁnd a solution to a given problem.
DEFINITION 13. Given a cost function f : A →R that reﬂects the performance of an
agent, where A is a set of input parameters and R is the set of real numbers, optimization
methods aim to ﬁnd the input parameters x0 that minimize the cost function. Such that
f(x0) ≤f(x) ∀x ∈A
Similar to reinforcement learning, optimization techniques can be used to ﬁnd solutions to problems by starting with a random solution and iteratively improving to
optimize the ﬁtness function. Evolutionary algorithms (EA) are popular optimization
methods that have extensively been used to ﬁnd motor trajectories for robotic tasks
[Nolﬁand Floreano 2000]. EAs are used to generate motion trajectories for high and
low DOF robots [Rokbani et al. 2012] [Min et al. 2005]. Popular swarm intelligence
methods such as Particle Swarm Optimization (PSO) [Zhang et al. 2015] and Ant
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
Colony Optimization (ACO) [Zhang et al. 2010] are used to generate trajectories for unmanned vehicle navigation. These techniques simulate the behavior of living creatures
to ﬁnd and optimal global solution in the search space. As is the case with reinforcement learning, evolutionary algorithms can be integrated with imitation learning to
improve trajectories learned by demonstration or to speed up the optimization process.
In [Berger et al. 2008] a genetic algorithm (GA) is used to optimize demonstrated
motion trajectories. The trajectories are used as a starting population for the genetic
algorithm. The recorded trajectories are encoded as chromosomes constituted of genes
representing the motor primitives. The GA searches for the chromosome that optimizes
a ﬁtness function that evaluates the success of the task. Projecting the motor trajectories to lower dimension illustrates the signiﬁcant change between the optimized motion and the one learned directly from kinesthetic manipulation [Berger et al. 2008].
Similarly in [Aler et al. 2005] evolutionary algorithms are used after training agents
in a soccer simulation. A possible solution (chromosome) is represented as a set of
if-then rules. The rules are ﬁnite due to the ﬁnite permutations of observations and
actions. A weighted function of the number of goals and other performance measures
is used to evaluate the ﬁtness of a solution. Although the evolutionary algorithm had
a small population size and did not employ crossover, it showed promising results over
the rules learned from demonstrations.
[Togelius et al. 2007] also used evolutionary algorithms to optimize multiple objectives in a racing game. The algorithms evolve an optimized solution (controller) from
an initial population of driving trajectories. Evaluation of the evolved controllers found
that they stay faithful to the driving style of the players they are modeled after. This
is true for quantitative measures such as speed and progress, and for subjective observations such as driving in the center of the road.
[Ortega et al. 2013] treat the weights of a neural network as the genome to be optimized. The initial population is provided by training the network with demonstrated
samples to initialize the weights. The demonstrations are also used to create a ﬁtness value corresponding to the mean squared error distance from the desired outputs
(human actions).
In [Sun et al. 2008] Particle Swarm Optimization (PSO) is used to ﬁnd the optimal
path for an Unmanned Aerial Vehicle (UAV) by ﬁnding the best control points on a
B-spline curve. The initial points that serve as the initial PSO particles are provided
by skeletonization. A social variation of PSO is introduced in [Cheng and Jin 2015], inspired by animals learning in nature from observing their peers. Each particle starts
with a random solution and a ﬁtness function is used to evaluate each solution. Then
imitator particles (all except the one with the best ﬁtness) modify their behavior by
observing demonstrator particles (better performing particles). As in nature an imitator can learn from multiple demonstrators and a demonstrator can be used to teach
more than one imitator. Interactive Evolutionary algorithms (IEA) [Gruau and Quatramaran 1997] employ a different paradigm. Rather than use human input to start an
initial population of solutions and the optimize them, IEA uses human input to judge
the ﬁtness of the solutions. To avoid the user evaluating too many potential solutions,
a model is trained on supervised examples to estimate the human user’s evaluation.
In [Bongard and Hornby 2013] ﬁtness based search is combined with Preference-based
Policy Learning (PPL) to learn robot navigation. The user evaluations from PPL guide
the search away from local minima while the ﬁtness based search searches for a solution. In similar spirit [Lin et al. 2011] train a robot to imitate human arm movement. The difference in degrees of freedom (DOF) between the human demonstrator
and the robot obstructs using the demonstrations as an initial population. However,
rather than use human input to subjectively evaluate a solution, the similarity of the
robot movement to human demonstrations is quantitatively evaluated. A sequence-
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
independent joint representation for the demonstrator and the learner is used to form
a ﬁtness function. PSO is used to ﬁnd the joint angles to optimize this similarity measure. A different method of integrating demonstrations is proposed in [El-Hussieny
et al. 2015]. Inspired by Inverse Reinforcement Learning (see section on apprenticeship learning), an Inverse Linear Quadratic Regulator(ILQR) framework is used to
learn cost function optimized by the human demonstrator. PSO is then employed to
ﬁnd a solution for the learned function instead of gradient methods.
5.3. Transfer Learning
Transfer learning is a machine learning paradigm where knowledge of a task or a
domain is used to enhance learning of another task.
DEFINITION 14. Given a source Domain Ds and task Ts, transfer learning is deﬁned
as improving the learning of a target task Tt in domain Dt using knowledge of Ds and
Ts; where Ds ̸= Dt or Ts ̸= Tt. A domain D = {χ, P(X)} is deﬁned as a feature space χ
and a marginal probability distribution P(X), Where X = {x1, ...xn} ∈χ. The condition
Ds ̸= Dt holds if χs ̸= χt or Ps(X) ̸= Pt(X) [Pan and Yang 2010].
A learner can acquire various forms of knowledge about a task from another agent
such as useful feature representations or parameters for the learning model. Transfer
learning is relevant to imitation learning and robotic applications because acquiring
samples is difﬁcult and costly. Utilizing knowledge of a task that we already invested
to learn can be efﬁcient and effective.
A policy learned in one task can be used to advice (train) a learner on another task
that carries some similarities. In [Torrey et al. 2005] this approach is implemented on
two robocup soccer simulator tasks, the ﬁrst is to keep the ball from the other team,
and second to score a goal. It is obvious that skills learned to perform the ﬁrst task
can be of use in the later. In this case advice is formulated as a rule concerning the
state and one or more action. To create advice the policy for the ﬁrst task is learned
using reinforcement learning. The learned policy is then mapped by a user (to avoid
discrepancies in state or action spaces) into the form of advice that is used to initiate
the policy for the second task. After receiving advice the learner continues to reﬁne the
policy through reinforcement learning and can modify or ignore the given advice if it
proves through experience to be inaccurate or irrelevant.
Often in transfer learning, human input is needed to map the knowledge from one
domain to another, however, in some cases the mapping procedure can be automated
[Torrey and Shavlik 2009]. For example, in [Kuhlmann and Stone 2007] a mapping
function for general game playing is presented. The function automatically maps between different domains to learn from previous experience. The agent is able to identify
previously played games relevant to the current task. The agent may have played the
same game before or a similar one and is able to select an appropriate source task to
learn from without it being explicitly designated. Experiments show that the transfer
learning approach speeds up the process of learning the game via reinforcement learning (compared to learning from scratch) and achieves a better performance after the
learning iterations are complete. The results also suggest that the advantage of using
transfer learning is correlated with the number of training instances transferred from
the source tasks. Even if the agent encounter negative transfer [Pan and Yang 2010]
for example from overﬁtting to the source task, it can recover by learning through experience and rectifying its model in the current task to converge in appropriate time
[Kuhlmann and Stone 2007].
Brys et al [Brys et al. 2015b] combine reward shaping and transfer learning to learn
a variety of benchmark tasks. Since reward shaping relies on prior knowledge to inﬂuence the reward function, transfer learning can take advantage of a policy learned for
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
one task and perform reward shaping for a similar task. In [Brys et al. 2015b] transfer
learning is applied from a simple version of the problem to a more complex one (e.g 2D
to 3D mountain car and a Mario game without enemies to a game with enemies).
5.4. Apprenticeship Learning
In many artiﬁcial intelligence applications such as games or complex robotic tasks,
the success of an action is hard to quantify. In that case the demonstrated samples can
be used as a template for the desired performance. In [Abbeel and Ng 2004], apprenticeship learning (or inverse reinforcement learning) is proposed to improve a learned
policy when no clear reward function is available; such as the task of driving. In such
applications the aim is to mimic the behavior of the human teachers under the assumption that the teacher is optimizing an unknown function.
DEFINITION 15. Inverse reinforcement learning (IRL) uses the training samples to
learn the reward function being optimized by the expert and use it to improve the trained
Thus, IRL obtains performance similar to that of the expert. With no reward function the agent is modeled as an MDP/R (S,A,T). Instead the policy is modeled after
feature expectations µE derived from expert’s demonstrations. Given n trajectories
1 , . . . }m
i=1 the empirical estimate for the feature expectation of the expert’s policy µE = µ(ΠE) is denoted as:
Where γ is a discount factor and φ(s(i)
t ) is the feature vector at time t of demonstration i. The goal of the RL algorithm is to ﬁnd a policy ¯π such that ||µ(¯π) −µE||2 ≤ϵ
where µ(¯π) is the expectation of the policy [Abbeel and Ng 2004].
[Ziebart et al. 2008] employ a maximum entropy approach to IRL to alleviate ambiguity. Ambiguity arises in IRL tasks since many reward functions can be optimized by
the same policy. This poses a problem when learning the reward function, especially
when presented with imperfect demonstrations. The proposed method is demonstrated
on a task of learning driver route choices where the demonstrations may be suboptimal and non-deterministic. This approach is extended to a deep-learning framework
in [Wulfmeier et al. 2015]. Maximum entropy objective functions enable straightforward learning of the network weights, and thus the use of deep networks trained with
stochastic gradient descent [Wulfmeier et al. 2015]. The deep architecture is further
extended to learn the features via Convolution layers instead of using pre-extracted
features. This is an important step in the route to automate the learning process. One
of the main challenges in reinforcement learning through trial and error is the requirement of human knowledge in designing the feature representations and reward
functions [Kober et al. 2013]. By using deep learning to automatically learn feature
representations and using IRL to infer reward functions from demonstrations, the
need for human input and design is minimized. The inverse reinforcement learning
paradigm provides an advantage over other forms of learning from demonstrations in
that the cost function of the task is decoupled from the environment. Since the objective of the demonstrations is learned rather than demonstrations themselves, the
demonstrator and learner do not need to have the exact skeleton or surroundings,
thus alleviating challenges such as the correspondence problem. Therefore, it is easier
to provide demonstrations that are generic and not tailor-made for a speciﬁc robot or
environment.
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
In addition, IRL can be employed rather than traditional RL even if a reward function exists (given that demonstrations are available). For example, in [Lee et al. 2014]
apprenticeship learning is used to derive a reward function from expert demonstrations in a Mario game. While the goals in a game such as Mario can be pre-deﬁned
(such as score from killing enemies and collecting coins or the time to complete the
level), it is not known how an expert user prioritizes these goals. So in an effort to
mimic human behavior, a reward function extracted from demonstrations is favored to
a manually designed reward function.
5.5. Active Learning
Active learning is a paradigm where the model is able to query an expert for the optimal response to a given state, and use these active samples to improve its policy.
DEFINITION 16. A classiﬁer h(x) is trained on a labeled dataset DK(x(i), y(i)) and
used to predict the labels of an unlabelled dataset DU(x(i)). A subset DC(x(i)) ⊂DU is
chosen by the learner to query the expert for the correct labels y∗(i). The active samples
DC(x(i), y∗(i)) are used to train h(x) with the goal of minimizing n : the number of
samples in DC.
Active learning is a useful method to adapt the model to situations that were not
covered in the original training samples. Since imitation learning involves mimicking
the full trajectory of a motion, an error may occur at any step of the execution. Creating
passive training sets that can avoid this problem is very difﬁcult.
One approach to decide when to query the expert is using conﬁdence estimations to
identify parts of the learned model that need improvement. When performing learned
actions, the conﬁdence in this prediction is estimated and the learner can decide to
request new demonstrations to improve this area of the application or to use the current policy if the conﬁdence is sufﬁcient. Alternating between executing the policy and
updating it with new samples, the learner gradually gains conﬁdence and obtains a
generalized policy that after some time does not need to request more updates. Con-
ﬁdence based policy improvement is used in [Chernova and Veloso 2007b] to learn
navigation and in [Chernova and Veloso 2008] for a macro sorting task.
In [Judah et al. 2012] active learning is introduced to enable the agent to query
expert at any step in the trajectory, given all the past steps. This problem is reduced
to iid active learning and is argued to signiﬁcantly decrease the number of required
demonstrations.
[Ikemoto et al. 2012] propose active learning in human-robot cooperative tasks. The
human and robot physically interact to achieve a common goal in an asymmetric task
(i.e the human and the robot have different roles). Active learning occurs between
rounds of interaction and the human provides feedback to the robot via a graphical
user interface (GUI). The feedback is recorded and is added to a database of training
samples that is used to train the Gaussian mixture model that controls the actions
of the robot. The physical interaction between the human and robot results in mutually dependent behavior. So with each iteration of interaction, the coupled actions of
the two parties converge into a smoother motion trajectory. Qualitative analysis of the
experiments show that if the human adapts to the robots actions, the interaction between them can be improved; and that the interaction is more signiﬁcantly improved
if the robot in turn adapts to the human’s action with every round of interaction.
In [Calinon and Billard 2007b] the teacher initiates the corrections rather than the
learner sending a query. The teacher observes the learner’s behavior and kinesthetically corrects the position of the robot’s joints while it performs the task. The learner
tracks its assisted motion through its sensors and uses these trajectories to reﬁne the
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
model which is learned incrementally to allow for additional demonstrations at any
5.6. Structured Predictions
In a similar spirit, DAGGER [Ross et al. 2010] employs sample aggregation to generalize for unseen situations. However, the approach is fundamentally different. DAGGER
formulates the imitation learning problem as a structured prediction problem inspired
by [Daum´e Iii et al. 2009], an action is regarded as a sequence of dependent predictions. Since each action is dependent on the previous state, an error leads to unseen
state from which the learner cannot recover, leading to compounded errors. DAGGER
shows that it is both necessary and sufﬁcient to aggregate samples that cover initial
learning errors. Therefore, an iterative approach is proposed that uses an optimal policy to correct each step of the actions predicted using the current policy, thus creating
new modiﬁed samples that are used to update the policy. As the algorithm iterates, the
utilization of the optimal policy diminishes until only the learned policy is used as the
ﬁnal model.
[Le et al. 2016] propose an algorithm called SIMILE that mitigates the limitations
of [Ross et al. 2010] and [Daum´e Iii et al. 2009] by producing a stationary policy that
doesn’t require data aggregation. SIMILE alleviates the need for an expert to provide
the action at every stage of the trajectory by providing ”virtual expert feedback” that
controls the smoothness of the corrected trajectory and converges to the expert’s actions.
Considering past actions in the learning process is an important point in imitation
learning as many applications rely on performing trajectories of dependent motion
primitives. A generic method of incorporating memory in learning is using recurrent
neural networks (RNN) [Droniou et al. 2014]. RNNs create a feedback loop among the
hidden layers in order to consider the network’s previous outputs and are therefore
well suited for tasks with structured trajectories [Mayer et al. 2008].
learning methods from different sources
To conclude this section, Figure 5 shows a Venn diagram outlining the sources
of data employed by different learning methods. An agent can learn from dedicated
teacher demonstrations, observing other agent’s actions or through trial and error. Active learning needs a dedicated oracle that can be queried for demonstrations. While
other methods that utilize demonstrations can acquire them from a dedicated expert
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
or by observing the required behavior from other agents. RL and optimization methods learn through trial and error and do not make use of demonstrations. Transfer
learning uses experience from old tasks, or knowledge from other agents to learn a
new policy. Apprenticeship learning uses demonstrations from an expert or observation to learn a reward function. A policy that optimizes the reward function can then
be learned through experience.
6. MULTI-AGENT IMITATION
Although creating autonomous multi-agents have been thoroughly investigated
through reinforcement learning [Shoham et al. 2003] [Busoniu et al. 2008], it is not
as extensively explored in imitation learning. Despite the lack of research, imitation
learning and multi-agent applications can be a good ﬁt. Learning from demonstrations can be improved in multi-agent environments as knowledge can be transferred
between agents of similar objectives. On the other hand, imitation learning can be
beneﬁcial in tasks where agents need to interact in a manner that is realistic from a
human’s perspective. Following we present methods that incorporate imitation learning in multiple agents.
In [Price and Boutilier 1999] implicit imitation is used to improve a learner’s RL
model. A multi-agent setting enables an agent to learn by observing other agents performing a similar task using similar actions to those possessed by the agent. The mentor agent provides demonstrations by performing the task to optimize its objectives, so
there is no need for a designated teacher; and the actions of an agent are unknown to
other agents. A learner can observe the state changes resulting from the actions of a
mentor agent and accordingly reﬁne its model. This premise is useful for real applications where multiple agents act in the same environment. However, this work assumes
that the agents are non-interacting, i.e., the consequences of one agent’s actions are independent of other agents. Implicit imitation is closely related to transfer learning as
a learner acquires the knowledge learned previously by a different learner. This application corresponds to a transductive transfer learning setting where the task is the
same but the domains of the mentor and learner are different [Pan and Yang 2010]. An
interesting aspect of this approach is that an agent can learn different skills from different mentors. In one experiment two mentors are acting to achieve different goals;
the learner uses observations from both agents to learn a more difﬁcult task. Note
however, that the tasks were designed so that a combination of the policies used by the
mentors form an optimal policy for the learner’s problem. [Price and Boutilier 1999].
This can be considered as an example of hierarchical transfer learning, where learning solutions to multiple problems can help achieve more complex tasks [Torrey and
Shavlik 2009]. However, in this case the knowledge of the tasks is learned through
observations and imitation.
Multi-agent settings add complexity to the imitation problem in a number of ways.
The learner’s state space can be signiﬁcantly expanded to include the status of other
agents, as observing the actions of other agents affects its decision. The reward function is also affected if multiple agents compete or collaborate towards the same goal.
The complexity of reward functions can increase even if the agents do not share the
same goal. In a competitive setting, in addition to maximizing its own reward, an agent
aims to minimize its opponents reward. In a cooperative setting, the total reward of the
team might be taken into consideration. These new complexities can be incorporated
in the learning process at different levels. For example, [Price and Boutilier 1999] exploit the presence of multiple agents to learn from observation, however, robots are
non-interacting and act independently of each other. So state space and reward function are not affected. In ‘keep away’ in robot soccer, the robots collaborate as a team
to keep ball from other team. However, the whole team share a Q policy, because they
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
all perform the same task. When a new action is performed the shared Q policy of all
players is updated and the same reward is applied to the entire team. While in [Raza
et al. 2012] agents learn different roles that complement each other to maximize the
common goal. In a defensive task, one agent tries to recover the ball from the attacking
opponents while the other falls back to act as a goal keeper. The roles are interchangeable – so each agent learns both skills as well as when to assume one of the two roles.
In [DAmbrosio and Stanley 2013] a team of agents is treated as a pattern of policies
rather than individual agents. That is a pattern that connects agents in a team that
perform complementary roles. This method enables a team of agents to be scaled signiﬁcantly after training without requiring retraining of different agents for similar
Multi-agent learning from demonstration can however introduce new challenges in
terms of acquiring demonstrations. In [Chernova and Veloso 2008] where active learning is employed in a multi-robot environment, the human expert is required to interact simultaneously with multiple robots. A system is then developed to divide the
expert’s time among the learners based on their need, by attracting the expert’s attention through audio visual cues in order to query information.
7. EVALUATION
Imitation learning is a relatively young ﬁeld, and evaluation of novel algorithms is
challenging due to the lack of standard evaluation criteria and benchmark tasks. It
is common that experiments are conducted on specialized robots or simulators. This
makes comparison to other approaches difﬁcult for two reasons, ﬁrstly, because the
designed algorithms are often specialized for a speciﬁc task and setup, and secondly,
because reproducing the results of an experiment needs special hardware or software
that may not be available to other researchers.
One track of imitation learning uses structured prediction datasets as a benchmark
for comparison with other techniques [Ross et al. 2010] [He et al. 2012] [Judah et al.
2012]. However, structured prediction tasks do not always correspond to real robotic
applications. The dependency of a state on previous states can be limited. Furthermore, simple hamming loss is usually employed, i.e., if a mistake is made in step t
the best action for t + 1 remains unchanged. This makes such benchmarks less suitable for testing robustness to errors and the ability to recover. Although not widely
used to compare different techniques, some tools are readily available to perform and
evaluate imitation learning such as video games [Thurau et al. 2004a] [Gorman 2009]
[Mnih et al. 2015] [Geisler 2002] [Ross and Bagnell 2010], AI simulators [Raza et al.
2012] [Aler et al. 2005] and reinforcement learning benchmarks [Judah et al. 2012]
[Randlov and Alstrom 1998]. Methods for evaluating imitation learning applications
can be quantitative or qualitative.
7.1. Quantitative Evaluation
7.1.1. Error Calculation. These are tasks where the score of an executed policy can be directly calculated. For instance a ball in cup task ultimately depends on the ball falling
into the cup [Kober and Peters 2009b]. The loss in the performance is the distance of
the ball from the cup. Another example is quantifying progress in a Mario game as the
distance traveled from left to right [Ross and Bagnell 2010]. In cyclic tasks where the
agent is required to maintain an action, time can be inherently used as a score. For
example, in Keepaway[Torrey et al. 2005], a robocup soccer simulator task, one team is
required to keep the ball from the other team. The game ends if the ball is intercepted
or goes out of bounds. Therefore, the duration that the ball is kept until the game ends
reﬂects the performance of the keeping team. Another measure for evaluating cyclic
actions is the amount of training needed to obtain a stable performance [Kober and Pe-
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
ters 2009b]. The error could also be calculated as deviation from the teacher’s actions.
This is possible if the motor primitives are the same for the teacher and test samples
can be reproduced [Schaal et al. 2003]. However, these methods need to account for the
fact that small errors could substantially change the action trajectory.
7.1.2. Quantitative Scores. In many applications the error cannot be explicitly calculated. However, some criteria can be used to quantify the quality of an executed policy. The performance is usually quantiﬁed by counting the number of desired (or unwanted) statuses achieved during the execution of a policy over a sequence. For example, in a driving simulator an undesired action is hitting an object. Thus, the number
of collisions could be used as a criterion for quantifying a driving task [Abbeel and Ng
7.2. Qualitative Evaluation
In some applications it is as important for the action to appear natural and believable
as it is to achieve the target. It is difﬁcult to quantify the believability of a performance,
even by comparing it to recorded human performances. In this case it is common for
the performance to be subjectively judged by human inspection. As with any subjective
analysis it is advisable to use more than one observer and regulate their decisions as
comparable scores. [Gorman 2009] use a believability index to assess agents in a multiplayer game. The index is calculated from ratings on a scale of 1-5 of how-human
like the performance appears. The ratings are performed by a number of independent
observers.
[Ortega et al. 2013] conducted a qualitative study in the form of an online Turing
test. The users are presented with a number of pairs of play-through footage of a Mario
game. For each pair, the user has to decide which footage belonged to a human player,
and which belonged to an AI controller. A similar evaluation is used in [Lee et al. 2014]
for the same task. In [Ortega et al. 2013] the AI controllers where distinguishable
from human players. However, agents that are designed to imitate human players
where more believable than agents designed to perform well in the game. In addition
to the Turing test, a qualitative evaluation is performed to test how closely the trained
agents resemble human players. One drawback with this approach is that you can only
compare an agent’s behavior to one human demonstration, as human demonstrations
can vary signiﬁcantly among themselves.
It is clear that quantitative and qualitative analysis focus on different aspects of
the agent’s performance; and thus have different requirements. Table II summarizes
points that help decide whether quantitative or qualitative evaluation is more appropriate for a given task.
Table II. When to use quantitative and qualitative evaluation.
Quantitative
Qualitative
Effectiveness in the task is more important
than believability
How the agent acts is more important than
what it achieves
Distance from the desired goal could be calculated
Subjects are available to perform subjective
Scoring criteria could be designed for the task
Subjective analysis could be formalized(e.g via
questionnaire)
Deviation from the optimal action could be calculated
A Turing test could be conducted (the performer is hidden from the observers)
In addition to evaluating the quality of an agent at performing a task, it is important for researchers to evaluate the value of the various self-improvement methods
available. In transfer learning, three measures are used to quantify the advantage
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
of using prior knowledge form other tasks [Torrey and Shavlik 2009]. The ﬁrst is to
compare a model that has learned only from previous tasks to an untrained one. This
measure demonstrates if the skills learned in the source task are of any use in the target task, without any subsequent learning. The second measure compares the time to
completely learn the target task from scratch against a learner initialized using transfer learning. This is useful to ﬁnd out if transfer learning can speed up the learning
process for the target task. And ﬁnally to evaluate the performance of the initialized
agent after subsequently learning from the target task against one that learned the
target task from scratch. Comparison of the ﬁnal performance represents the ability
of transfer learning to improve over a fully trained learner. A graph plotting the performance of an agent learning from scratch against one that uses transfer learning
over time can illustrate all three measures [Torrey and Shavlik 2009] [Kuhlmann and
Stone 2007]
7.3. Evaluating the Teaching Process
While most research focus on evaluating the performance of the learner, [Calinon and
Billard 2007b] raise the issue of evaluating the teaching process as part of evaluating an imitation learning system. A number of benchmarks are proposed to evaluate
different teaching methods and their educational value. The paper also discusses recommendations for demonstrating techniques that provide better training examples.
8. APPLICATIONS
Advances in imitation learning open the door for a variety of potential applications.
Though commercialization and ﬁnished products of such applications may not be realized yet, we present some of the directions taken in the literature and their potential
applications.
Autonomous vehicles have been a popular concept in AI from its early days. And
recently self-driving cars have been gaining a lot of attention from car manufacturers and tech companies alike. The advancement in sensors and onboard computers in
modern cars have rekindled the interest in producing driverless vehicles commercially.
Early research in imitation learning focused on this problem, proposing a method for
learning to ﬂy an aircraft from demonstrations provided via remote control [Sammut
et al. 2014] and self-driving road vehicles [Pomerleau 1995]. Since then, many researchers have directed imitation learning research to navigational or driving tasks
[Abbeel and Ng 2004] [Chernova and Veloso 2007b] [Dixon and Khosla 2004] [Saunders et al. 2006] [Ross and Bagnell 2010] [Munoz et al. 2009] [Cardamone et al. 2009],
not only low level control car route selection and planning [Ziebart et al. 2008]
Assistive robotics aims to provide intelligent robots that can help elderly or recovering individuals in their day to day activities. Generalization is necessary in most
applications as the human partner is usually untrained, so the robot must be able
to behave robustly in unseen situations. The Human-robot interaction is not limited
to physical assistance. Socially assistive robots can offer help with sociological and
mental problems [Feil-Seifer and Mataric 2005] [Bemelmans et al. 2012] [Tapus et al.
2009]. For robots to be effective in such a social context, their behavior must be humanlike to be intuitively recognized by the human partner. Therefore, imitation learning
has the potential to be an integral part in assistive robots. The same argument can be
made for teaching infants using interactive robots. [Ikemoto et al. 2012] incorporates
interaction in the training process to account for the changing behavior of the human
partner. The robot, therefore, can adapt to the human’s reactions, as the human naturally makes similar adaptations.
Electronic games is a multi-billion dollar industry, and one in which realism and
immersion are important factors. There is an ever growing demand for believable arti-
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
A. Hussein et al.
ﬁcial intelligence to enhance immersion in games [Hingston 2012]. Similar to Humanrobot interaction, the possibilities are too great to consider with explicit programming
[Geisler 2002], especially in modern games where the player has more freedom and
control and the environments are becoming increasingly complex. A number of studies
investigate the effectiveness of imitation learning in video games such as First Person Shooters [Geisler 2002] [Gorman 2009] [Thurau et al. 2004a], platformers [Ortega
et al. 2013] [Ross and Bagnell 2010] [Ross et al. 2010] and racing[Ross and Bagnell
2010] [Ross et al. 2010] [Munoz et al. 2009] . These examples are applied to relatively
simple problems, with limited action options and few features to consider from the
environment; however, they show promising performances from imitation agents that
could be extended to more complex games in the future. For instance, the study in [Gorman 2009] showed that not only are imitating agents subjectively believable, but that
they also outperform other hand crafted agents. Electronic games provide a friendly
platform for advancing imitation learning as they do not require additional systems
such as sensors and hardware; and the cost of faults is low compared to applications
where faults might endanger people or property. They can also be used as a testbed for
artiﬁcial general intelligence [Schaul et al. 2011].
Humanoid robots is one of the domains most associated with artiﬁcial intelligence.
It is one of the most relatable domains, because many of its potential applications are
quite obvious. The premise of humanoid robots is to replace some of the workload that
humans do. These problems range from speciﬁc tasks such as physical chores and
housework, to generic problem solving. Since most of the required tasks are already
performed by humans, humanoid robot applications are inherently suitable for the
concept of learning by imitation. Humanoid robots can learn to perform actions that
only utilize part of their skeleton [Billard and Matari´c 2001] [Ijspeert et al. 2002b]
[Vogt et al. 2014] or the entire body [Berger et al. 2008] [Calinon and Billard 2007b]
[Ude et al. 2004]. Since humanoid robots are commonly used to interact with humans
in a social context, it is important not only to learn how to move but also how to direct
its attention to important occurrences. In [Shon et al. 2005] a robot learns where to look
by imitating a teacher’s gaze and learns to estimate saliency based on the teacher’s
preferences.
Another ﬁeld of robotic applications is automation.It is different from the aforementioned domains in that automation is more relevant to industrial tasks while the
other domains aim to create AI for personal products and services. However, automation can still be useful in domestic applications [Saunders et al. 2006]. While automation is not a new concept, learning by imitation introduces generalization and adaptability to automated tasks [Nicolescu and Mataric 2003]. This means that the robot
can act robustly in unseen situations such as the introduction of obstacles or changes
in the shape or position of relevant objects. Generalization diminishes the need for
supervision and human intervention between small tasks. Imitation learning research
done in this direction focus on object manipulation, such as sorting and assembly tasks
[Saunders et al. 2006] [Pook and Ballard 1993] [Oztop and Arbib 2002] [Calinon and
Billard 2007b] [Nicolescu and Mataric 2003]. Another example of automation is medical procedures. In [Mayer et al. 2008] a robot is trained to automate part of a surgical
procedure from surgeons’ demonstrations.
9. CONCLUSION AND FUTURE DIRECTIONS
In this survey, we review and discuss the methods and techniques used in the literature to learn by imitation. A key challenge in imitation is generalization to unseen
scenarios. Due to the dynamic nature of imitation learning applications, demonstration
can only cover a subset of possible states, therefore direct imitation learns from a state
distribution that is different to the distribution that faces the agent when performing
ACM Computing Surveys, Vol. V, No. N, Article A, Publication date: January YYYY.
Imitation Learning: A Survey of Learning Methods
the task. It is therefore essential to perform indirect learning that allows the agent
to perform its current policy and reﬁne it based on feedback from the environment or
the teacher. Optimization approaches that search directly in solution space face similar limitations as they optimize for speciﬁc situations. A more adaptable approach is
searching in policy space such as using evolutionary algorithms to ﬁnd weights for
neural networks. Indirect learning also addresses another important challenge which
is imperfect demonstrations. This may arise from the capturing process or due to discrepancies between the teacher and learner or their respective environments. These
challenges motivate a trend of generalizing the learning process. Creating methods
that are speciﬁc to a particular setup obstructs extending these techniques to different
tasks. It also makes it difﬁcult to replicate and compare different techniques. Although
imitation learning methods have developed substantially in recent years and demonstrated success in a variety of problems, the above mentioned challenges, among others, present many open research points. We now highlight key directions for future
— General feature representations can be created by automatically extracting features to represent the current state. Eliminating the need for engineered features
brings us a step closer to the goal of a generic task independent learning process.
While recent utilization of deep learning have produced success stories in this area,
such techniques need to be examined in more realistic dynamic environments.
— General task learning follows a similar motivation to learning feature representations. The aim is to create a learning process with minimal expert knowledge of the
task. The end goal is to have the ability to train intelligent agents solely by demonstrating a task. Since direct imitation is often no sufﬁcient for robust imitation, apprenticeship learning might play an important role as a self-improvement method
that does not require human intervention or knowledge of the task apart from the
provided demonstrations.
— Benchmarking can greatly beneﬁt any research topic as it allows comparing different techniques and identifying challenges more clearly. Although some benchmarking tools are available, more realistic tasks are needed to serve as standard imitation
learning problems.
— Multi-agent imitation poses a complex learning situation that requires agents to
learn cooperative or competitive strategies. Most of the current research focuses on
single agents learning from a single teacher, although most real life applications will
require the agent to interact with a number of humans or other agents in a shared
environment [Rozo Casta˜neda et al. 2013].
— Agent memory is another area that is scarcely addressed. Memory allows the agent
to perceive its actions in context and predict a likely outcome. [Pastor et al. 2013]
highlight the need for such awareness and propose Associative Skill Memory (ASM)
as a possible extension to learning from demonstrations and experience.