The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)
Knowledge Distillation with
Adversarial Samples Supporting Decision Boundary
Byeongho Heo,1∗Minsik Lee,2∗Sangdoo Yun,3 Jin Young Choi1
{bhheo, jychoi}@snu.ac.kr, , 
1Department of ECE, ASRI, Seoul National University, Korea
2Division of EE, Hanyang University, Korea
3Clova AI Research, NAVER Corp, Korea
Many recent works on knowledge distillation have provided
ways to transfer the knowledge of a trained network for improving the learning process of a new one, but ﬁnding a good
technique for knowledge distillation is still an open problem.
In this paper, we provide a new perspective based on a decision boundary, which is one of the most important component
of a classiﬁer. The generalization performance of a classiﬁer
is closely related to the adequacy of its decision boundary,
so a good classiﬁer bears a good decision boundary. Therefore, transferring information closely related to the decision
boundary can be a good attempt for knowledge distillation.
To realize this goal, we utilize an adversarial attack to discover samples supporting a decision boundary. Based on this
idea, to transfer more accurate information about the decision boundary, the proposed algorithm trains a student classi-
ﬁer based on the adversarial samples supporting the decision
boundary. Experiments show that the proposed method indeed improves knowledge distillation and achieves the stateof-the-arts performance.
Introduction
Knowledge distillation is a method to enhance the training of a new network based on an existing, already trained
network. In a teacher-student framework, the existing network is considered as a teacher and the new network becomes a student. Hinton, Vinyals, and Dean , a pioneer in knowledge distillation, proposed a loss minimizing the cross-entropy between the outputs of the student and
the teacher, which referred to as the knowledge distillation
loss (KD loss). Due to the KD loss, the student network is
trained to be a better classiﬁer than the network trained without knowledge distillation. Although the goals of the knowledge distillation are diverse, recent studies focus on improving a small student network using a large network as a teacher using a large teacher
network. These studies aim to create a small network with
the speed of a small network and the performance of a large
network. This paper, too, focuses on knowledge distillation
in the respect of enhancing the performance of a small network using a large network.
∗Authors contributed equally.
Copyright c⃝2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Many of recent studies are focusing on manipulating the
KD loss for various purposes. Romero et al. and
Zagoruyko and Komodakis proposed new distillation losses to transfer the hidden layer response of the network and used it with the KD loss. Chen et al. and
Wang and Lan designed new distillation losses for
other applications based on the KD loss. In contrast to these
existing approaches that concentrate on how to manipulate
various parts of a network in order to improve the effect of
knowledge distillation, in this paper, we investigate informative samples for an effective knowledge transfer. In general,
samples near the decision boundary of a classiﬁer have a
larger impact on the performance than those far apart from
it . Therefore, if we can generate
samples close to the decision boundary, the knowledge of
a teacher network would be transferred more effectively by
utilizing those samples.
To obtain the informative samples close to the decision boundary, we utilize an adversarial attack . An adversarial attack is a technique to tamper with the result of a classiﬁer by adding a small perturbation to an input image. Although an adversarial attack is
not particularly aimed at ﬁnding a decision boundary, they
are closely related to each other . An
adversarial attack tries to ﬁnd a small modiﬁcation that can
change the class of a sample, i.e., it tries to move the sample beyond a nearby decision boundary. Inspired by this fact,
we propose to perform knowledge distillation with the help
of an adversarial attack. To get samples beneﬁcial to knowledge distillation, we modify an attack scheme to search an
adversarial sample supporting a decision boundary. The resulting sample is referred to as the boundary supporting sample (BSS) in this paper. A new loss function using BSSs is
suggested for knowledge distillation that transfers decision
boundary to a student classiﬁer. In order to verify whether
the proposed method actually transfers the decision boundaries, we also propose two similarity metrics that compares
the decision boundaries of two classiﬁers and use these metrics to examine the decision boundaries of a teacher and a
The proposed method is veriﬁed through experiments.
First, we show that the use of BSSs could improve the
knowledge distillation scheme of Hinton, Vinyals, and
Adversarial attack
Proposed distillation with BSS
Trained with BSS
Trained with one-hot labels
Proposed method
Classifier
Classifier
Classifier
Classifier
Classifier
Figure 1: The concept of knowledge distillation using samples close to the decision boundary. The dots in the ﬁgure represent
the training sample and the circle around a dot represents the distance to the nearest decision boundary. The samples close to
the decision boundary enable more accurate knowledge transfer.
Dean in an image classiﬁcation problem. After this,
we perform more experiments to examine the generalization
performance of the proposed method, of which the result indicates that the proposed method has better generalization
performance, and as a result, it can provide good results
with less training samples. Finally, we analyze the proposed
method with various experiments.
Related Works
Many studies have been conducted for knowledge distillation since Hinton, Vinyals, and Dean proposed the
ﬁrst knowledge distillation method based on class probability. Romero et al. used the hidden layer response of
a teacher network as a hint for a student network to improve
knowledge distillation. Zagoruyko and Komodakis 
found the area of activated neurons in a teacher network
and transferred the activated area to a student network. In
the case of Yim et al. , the channel response of a
teacher network was transferred for knowledge distillation.
Xu, Hsu, and Huang proposed a knowledge distillation method based on the framework of generative adversarial network . Some studies extended knowledge distillation to computer vision
applications. Knowledge distillation has been studied in various directions, but most of the studies are focused on manipulating the hidden layer response of a network or changing the loss appropriately for the purpose. As far as we know,
the proposed method is the ﬁrst method to improve knowledge distillation by changing the samples used for training.
In the meantime, Szegedy et al. found that a classiﬁer based on a neural network could be fooled easily by
a small noise. This work gave rise to a new research topic
in neural networks called an adversarial attack, which is
about ﬁnding a noise that can deceive a neural network.
Moosavi-Dezfooli, Fawzi, and Frossard proposed a
method to optimize a classiﬁer based on a linear approximation to ﬁnd the closest adversarial example. Goodfellow,
Shlens, and Szegedy proposed the adversarial training which trains a classiﬁer with adversarial samples in order to make the network robust to an adversarial attack. Cao
and Gong found that an adversarial sample was located near the decision boundary, and used this property to
defense an adversarial attack. There have also been some
works that connect an adversarial attack to another research
topic. Papernot et al. found that a network trained
by knowledge distillation is robust to adversarial attacks.
The relationship between an adversarial attack and a decision boundary was used to prevent an adversarial attack in
Cao and Gong . Knowledge distillation was also used
to prevent an adversarial attacks in Papernot et al. .
Our study is closely related to these approaches, except that
we take an opposite direction to them: We use adversarial attacks to ﬁnd decision boundary to enhance knowledge distillation, which is a novel approach that has not been attempted
Adversarial attack for knowledge distillation
An adversarial attack is to change a sample in a class into
an adversarial sample in another class for a given classiﬁer.
In our paper, the given sample for the adversarial attack is
referred to as the base sample. In this section, we present
an idea to utilize the adversarial attack in knowledge distillation. The idea is about using adversarial samples near a
decision boundary to transfer the knowledge related with the
boundary. In the following sections, we ﬁrst explain the deﬁnition of boundary supporting sample (BSS) and its beneﬁts
in knowledge distillation, and then we provide an iterative
procedure to ﬁnd BSSs.
Beneﬁts of BSSs in knowledge distillation
It is wellknown that the generalization performance of a classi-
ﬁer highly depends on how well the classiﬁer learns the
true decision boundary between the actual class distributions . This indicates that if a classiﬁer yields a good performance then it
probably has good decision boundary that is close to the
true one. We can analyze knowledge distillation in this respect. The knowledge distillation approaches attempt to resolve the generalization issue with help of the trained network with high-performance, i.e., a teacher, by transferring
its knowledge to the classiﬁer we are to train, i.e., the student . If we
train the student without knowledge distillation, then its performance may not as good as the teacher, which indicates
that a decision boundary of the teacher is likely better than
that of the student, as shown in Figure 1. On the other hand,
knowledge distillation can enhance the performance of the
student, which suggests that the decision boundary of the
student is getting improved by knowledge distillation.
However, existing works do not explicitly address that the
information about the decision boundary is transferred by
knowledge distillation. In our paper, inspired by this motivation, we utilize adversarial samples obtained from the training samples to transfer the glimpse of a more accurate decision boundary. A boundary supporting sample (BSS) is de-
ﬁned in this respect, it is an adversarial sample that lies near
the decision boundary of a teacher classiﬁer. Since BSSs
are labeled samples near decision boundary as depicted in
the second picture of Figure 1, they contain the information
about the decision boundary. Hence, using BSSs in knowledge distillation can provide a more accurate transfer of decision boundary information. An BSS in our work is obtained by a gradient descent method based on a classiﬁcation score functions, and it contains information about both
the distance and the path direction from the base sample to
the decision boundary. In conclusion, BSSs could be bene-
ﬁcial to improve the decision boundary, and hence the generalization performance, of a student classiﬁer in knowledge
distillation.
Iterative Scheme to ﬁnd a BSS
For a given sample, as
shown in Figure 2, there exist many BSSs over all classes
except the base class that contains the base sample. To ﬁnd a
BSS, we deﬁne a loss function based on classiﬁcation scores
produced by a classiﬁer. Then, we search a BSS in the gradient direction of the loss function based on the method in
 with a modiﬁed update rule.
Given a sample vector x in a base class, its corresponding adversarial samples are calculated based on an iterative
procedure. First, a sample is initialized to xk
0 = x, and then
it is iteratively updated to a target class k, k =, 1, 2, · · · K.
Here, the adversarial sample after the i-th iteration is denoted by xk
i . Assume that the classiﬁer f produces classiﬁcation scores for all classes, where the class of a sample is
determined by the class having the maximum score. Then,
let fb(x) and fk(x) be the classiﬁcation scores for the base
class and the target class k, respectively.
The goal of the adversarial attack is to decrease the score
for the base class while increasing the score for the target
class. To this end, the loss function for the attack to the target
class k is deﬁned by
Lk(x) = fb(x) −fk(x).
This loss becomes zero at a point on the decision boundary,
positive at a point within the base class, and negative at an
adversarial point within the target class. To ﬁnd an adversarial sample, we move the sample to the direction minimizing
Adversarial
Perturbation
Adversarial
Adversarial
Adversarial
Adversarial
Figure 2: Iterative scheme to ﬁnd BSSs for a base sample
the loss by the iterative scheme in (2), until the loss becomes
where ∇Lk(xk
i ) refers to the gradient of Lk is abnormally large due to a small
gradient. To prevent this, we introduce a learning rate η
which is used together with the loss function to control the
step size. Note that the loss is large at the initial point and
small near the decision boundary. In addition, ϵ derives the
sample to cross the decision boundary as shown in the following.
If we derive the ﬁrst-order Taylor series approximation of
i+1) at xk
i and substitute (2) to remove (xk
then we have
i+1) ≈Lk(xk
Let us assume that we have chosen a small enough η so that
2 < 1. Then, if the sample approaches a decision boundary, Lk(xk
i ) becomes small. In this case, without
the last term in (3) that exists due to the introduction of ϵ in
(2), the loss converges to zero but does not become negative
which means the sample does not cross the decision boundary. By introducing ϵ in (2), the loss can become negative
due to the second term in (3).
To lead the adversarial sample to a location near the decision boundary, we establish the stop conditions. The iteration stops if one of the following conditions occurs:
i+1) < 0 and Lk(xk
There exists any ¯k such that
i+1) > max(fb(xk
i+1), fk(xk
i + 1 ≥Imax,
where Imax is a predeﬁned number of maximum iterations.
Condition (a) means that the adversarial sample crosses the
decision boundary. If (a) is satisﬁed, then the attack is successful and the resulting sample is regarded as an BSS. On
the other hand, conditions (b) and (c) are about failure cases
and we discard the sample if one of them is satisﬁed. Condition (b) means that the sample has stepped into a class that
is not the target. This case occurs when there exists a nontarget class between the base class and the target class. Condition (c) happens if the decision boundary is too far from
the base sample.
Knowledge distillation using BSS
As mentioned in the previous section, BSSs of a teacher are
beneﬁcial for improving the generalization performance of
a student classiﬁer. In this section, we present a method to
enhance knowledge distillation by transferring information
on decision boundary more precisely using BSSs.
Loss function for BSS distillation
From a training batch,
our distillation scheme uses a set of base sample pairs
{(xn, cn) | n = 1, · · · , N} where cn denotes the class index
of xn. A set of BSSs is denoted by {˚xk
n | n = 1, · · · , N; k =
1, · · · , K}. Let the teacher and the student classiﬁers be denoted by ft and fs respectively. For a sample xn, the class
probability vectors are denoted by qt
n = σ (ft(xn)) and
n = σ (fs(xn)), where σ(·) refers to the softmax function. The desired class label for x is denoted by a one-hot
label vector ytrue of which the element is either one for
the ground-truth class or zero for the other classes. The proposed loss function to train the student classiﬁer combines
three losses; a classiﬁcation loss Lcls, the knowledge distillation loss LKD in , and
an boundary supporting loss LBS:
L(n) = Lcls(n) + αLKD(n) + β
nLBS(n, k).
If we deﬁne the entropy function as J(a, b) = −aT log b,
where a and b are column vectors and log is the elementwise logarithm, each loss is deﬁned by
Lcls(n) = J(ytrue
, σ (fs(xn))),
LKD(n) = J(σ
LBS(n, k) = J . pk
n in the third term of (4) is the probability of class k being selected as the target class, which
is introduced to sample target classes stochastically during
training. The deﬁnition of pk
n can be found in (10). The linearly decaying weights are used for α and β, following the
common practice in existing knowledge distillation techniques.
Note that the Lcls transfers direct answers (one-hot labels)
for the training samples, whereas LKD transfers probabilistic labels . In contrast, the
boundary supporting loss LBS is introduced to transfer the
information about the decision boundary directly.
Miscellaneous issues on using BSSs
Base sample selection for boundary supporting loss.
reduce the computation, we select N base samples out of
Nbatch training samples according to a speciﬁc rule explained below and apply the boundary supporting loss only
to the selected samples.
The base samples for generating the adversarial samples
are selected from the training batch B = {(xn, cn) | n =
1, 2, · · · , Nbatch}. A training sample pair (xn, cn) is selected as the base sample for an adversarial attack if the class
cn has the highest probability for both the teacher and the
student classiﬁers. That is, considering the probability vectors qt
n, the base sample set is determined by
C = {(xn,cn) | argmax
n,c) ≡cn, (xn, cn) ∈B}
n,c (o = t, s) is the cth element of qo
n. If the size of
C is smaller than a predeﬁned N, all the samples in C is used
for the boundary supporting loss. If the size of C is larger
than N, we select N samples that have the highest distance
between qt
Large dn means that the probability vector qt
n of the teacher
and the probability vector qs
n of the student are largely different from each other at the base sample position xn. Since
the reduction of dn matches the goal of knowledge distillation, it is reasonable to choose a base sample with large
Target class sampling.
A BSS can target all classes except the base class. In the learning process, one of the classes
is selected as the target class according to the following criteria and an BSS is generated by adversarial attacking to the
selected target class. For the base sample xn, the probability pk
n to sample the class k is deﬁned based on the class
probability qt
n of the teacher as follows:
n,k/(1 −qt
otherwise.
This is motivated from that it is important to precisely
transfer the knowledge on the decision boundary between
two classes that are hard to discriminate from each other.
n,k̸=cn having high value means that the class k is hard to
discriminate from the base class cn for the base sample xn.
Therefore, the target class is sampled with priority given to
the class with a high qt
n,(·) for xn.
Metrics for similarity of decision boundaries
To verify whether the proposed method actually transfers decision boundaries in knowledge distillation, we need some
metrics. Here, we propose two metrics based on BSSs to
measure the similarity between the decision boundaries of
two classiﬁers (i.e., teacher and student classiﬁers in knowledge distillation). These metrics are used to evaluate the performance of knowledge distillation or analyze the beneﬁts of
BSSs in knowledge distillation.
Table 1: Comparison on CIFAR-10 dataset
FITNET 
FSP 
FSP 
Given the nth base sample xn, the perturbation vector to
attack the target class k for the teacher classiﬁer is obtained
Likewise, ¯xk,s
denotes the perturbation vector for the student classiﬁer. Using a set of perturbation vector pairs
n ) | n = 1, 2, · · · , N; k = 1, · · · , K}, the similarity between the two decision boundaries is deﬁned by two
metrics: The Magnitude Similarity (MagSim) in (12) and the
Angle similarity (AngSim) in (13):
min , ImageNet
32×32 and Tiny-
ImageNet datasets using a variety of residual networks .
Performance on image classiﬁcation
The performance of the proposed method is veriﬁed by image classiﬁcation on the CIFAR-10, ImageNet 32×32, and
TinyImageNet datasets. We trained the student classiﬁers in
seven different ways. The ﬁrst method is denoted as ‘original’, which uses only the classiﬁcation loss for training. The
second method is denoted as ‘Hinton’. Using the classiﬁcation loss and the KD loss, ‘Hinton’ was implemented in
the same way as in Hinton et al. . The next three methods are the latest knowledge distillation methods implemented with the KD loss. The ‘FIT-
NET’ transfers the response of the intermediate layer. The method denoted as ‘AT’ is transferring the spatial attention of the teacher classiﬁer to the student classiﬁer . The ‘FSP’
simpliﬁes the layer response of the teacher into a channelwise correlation matrix, which is used as the medium of
knowledge transfer . Since the three methods use the KD loss, they are labeled together with ‘+ Hinton’. The last two methods are; the proposed method which
is denoted as ‘proposed’ and the proposed method implemented together with the ‘FSP’ method which is denoted
as ‘FSP+proposed’. The performance of all classiﬁers was
measured in terms of accuracy.
The CIFAR-10 is a classiﬁcation dataset with
10 classes and 32x32 resolution, consisting of 50k training
images and 10k test images. ResNet26 with 92.55% accuracy is used as a teacher classiﬁer. Meanwhile, ResNet8,
ResNet14 and ResNet20 are used as student classiﬁers.
All classiﬁers were trained over 80 epochs. ‘FITNET’
and ‘FSP’, which require two-stage learning scheme, were
learned over 80 epochs after using 40 epochs for initialization. Table 1 shows the result. The proposed method shows
improved performance compared to Hinton. Also, the performance improvement of the proposed method is better
than the existing state-of-the-arts. This conﬁrms that the additional samples of the proposed method is useful for the
knowledge distillation.
32×32downsampled version of the ImageNet dataset This dataset is a classiﬁcation dataset
with 1000 classes, consisting of 1,281k training images
and 50k validation images. ResNet32 with 48.04% top-1
accuracy and 73.22% top-5 accuracy is used as a teacher
classiﬁer and ResNet8 is used as a student classiﬁer. All
classiﬁers were trained over 40 epochs. ‘FITNET’ and
‘FSP’ spent 4 epochs for initialization. Table 2 shows the
result. The proposed method shows better performance
than Hinton method and shows comparable performance
with other state-of-the-arts. When combined with ‘FSP’,
the proposed method shows better performance for top-5
TinyImageNet.
TinyImageNet is a subset of the ImageNet
dataset with 64×64 resolution. It contains 100k training images and 10k test images in 200 classes. ResNet42 is used
for the teacher classiﬁer, which has 56.10% top-1 accuracy
Table 2: Comparison on ImageNet 32×32
FITNET 
FSP 
FSP 
Table 3: Comparison on TinyImageNet
FITNET 
FSP 
FSP 
and 78.71% top-5 accuracy. ResNet10 is selected for the student classiﬁer. Classiﬁers were trained for 80 epochs. ‘FIT-
NET’ and ‘FSP’ spent 10 epochs for initialization. The result is described in Table 3. The proposed method shows
better results than Hinton and ‘AT’. Although the proposed
method shows lower top-1 accuracy than ‘FITNET’ and
‘FSP’ which require additional learning steps, it has higher
top-5 accuracy than other state-of-the-arts. Also, the proposed method shows performance superior to those of other
algorithms when combined with ‘FSP’.
Generalization of the classiﬁer
The proposed method improves the generalization performance of a student classiﬁer using the samples supporting the decision boundary obtained from a teacher classiﬁer. Through an experiment, we veriﬁed that the generalization performance of the student classiﬁer actually increases by the proposed method. In order to measure the
generalization performance, the experiment was repeated
while reducing the number of training samples from 100%
to 20%. The CIFAR-10 dataset was used in this experiment,
and ResNet26 trained on the whole dataset was used as the
teacher classiﬁer while ResNet14 was used as the student
classiﬁer. All methods were trained on the same training data
for fairness.
Figure 4 shows the performance improvement from the
original method for the size of the dataset. Here, we can see
that the performance improvement of the other methods does
not change much regardless of the size of data. On the other
hand, the proposed method shows bigger performance improvement for less training data. In a situation where it is
difﬁcult to achieve generalization due to insufﬁcient data,
the proposed method shows a large performance improvement, which means that the proposed method improves the
generalization of the student classiﬁer.
Analysis with similarity measure
We conducted an experiment to analyze the effect of the proposed method on the decision boundary of the network. The
experiment is to measure the similarity metrics (MagSim,
AngSim) of trained student and teacher. Two similarity metrics reﬂect the similarity of decision boundaries. Thus, high
similarity metrics mean that decision boundary is transferred
by knowledge distillation. Experiment was performed in
a CIFAR-10 test set. ResNet 8 and Resnet 14 were used.
‘Original’, ‘Hinton’, and the proposed method were tested.
The experimental results are shown in Figure 3. Compared
with ‘original’, the Hinton method mainly increases MagSim
and AngSim changes are small. In other words, the Hinton
method transfers only the distance to the decision boundary
and does not consider the direction. On the other hand, the
proposed method increases both MagSim and AngSim when
compared to ’original’. Therefore, the proposed method
transfers both distance and direction of decision boundary.
The experiment show that the proposed method transfers the
decision boundary more accurately and explains the reason
for the high performance in the previous experiments.
Self-comparison
We conducted self-comparisons to analyze the effects of
a boundary supporting sample and miscellaneous issues.
The experiments were performed on the CIFAR-10 dataset.
ResNet26 was used as the teacher classiﬁer, and ResNet8
was the student classiﬁer. A boundary supporting sample
(BSS) is an adversarial sample that especially designed to
reﬂect the information about a decision boundary. Therefore, a BSS is more suitable for knowledge distillation than
other types of adversarial samples. To verify this, we tested
different kinds of adversarial attacks for knowledge distillation. Experiments were performed on ﬁve kinds of adversarial samples. The results are described in Table 4. The
‘Baseline’ shows the performance of knowledge distillation
without the proposed boundary supporting loss. The ‘Random noise’ uses a randomly generated noise instead of a
gradient-based adversarial sample. The method denoted as
‘L2 minimize’ presents the performance of the proposed
method with adversarial samples calculated based on the L2
minimization of (1). The ‘FGSM’ and ‘DeepFool’ use other well-known attack methods for
the proposed method. The ‘Proposed’ is the proposed distillation method with BSS. The result shows that all adversarial
samples including random noise are beneﬁcial to knowledge
distillation. Random noise shows the smallest performance
Table 4: Comparison of knowledge distillation using various types of adversarial samples.
Random noise
L2 minimize
FGSM 
DeepFool 
ResNet8 – CIFAR 10
ResNet14 – CIFAR 10
Figure 3: Evaluation of proposed method for decision boundary similarities (MagSim, AngSim).
Accuracy improvement
Percentage of used training samples in CIFAR-10
Figure 4: Generalization of the classiﬁer. The smaller the
number of training samples are, the larger improvement the
proposed method shows.
improvement. The gradient-based methods except the proposed method show similar performance improvement. On
the other hand, the proposed method using BSS shows the
greatest improvement, showing that a BSS is more suitable
for knowledge distillation.
Experiment on miscellaneous issues is presented in Table 5. ’Proposed’ shows the performance of the proposed
method using base sample selection and target class sampling. ’All selection’ shows performance when all samples
are used as base samples, and ’Random selection’ shows
performance when base samples are randomly selected without the proposed scheme. Two results show that the proposed base sample selection not only reduces computation
but also contributes to performance. The performance when
the target class is selected randomly without the proposed
sampling is shown in ’Random target class’. The result implies that the proposed sampling according to the class probability is reasonable and effective method.
Implementation details
All the experiments were performed using residual networks . The channel sizes of ResNet were
set to 16, 32, and 64 for CIFAR-10, and 32, 64, and 128
for ImageNet 32×32. For TinyImageNet, ResNet with four
block was used with channel size of 16, 32, 64, and 128.
Table 5: Self-comparison on base sample selection and target class selection.
All sample
target class
We used random crop and random horizontal ﬂip for data
augmentation and normalized an input image based on the
mean and the variance of the dataset. The temperatures of
the KD loss and the adversarial loss were ﬁxed to 3 in all
experiments. The parameter α in (4) was initialized to 4 and
linearly decreased to 1 at the end of training. The β in (4)
was set to 2 initially and linearly decreased to 0 at the 75%
of the whole training procedure, based on our empirical observations: When β was not zero at the ﬁnal training stage,
the performance was degraded. The learning process was
performed with 256 batch size, with a learning rate which
started at 0.1 and decreased to 0.01 at half of the maximum
epoch and to 0.001 in 3/4 of the maximum epoch. The momentum used in the study was 0.9 and the weight decay was
0.0001. η = 0.3 was used for the adversarial attack in the
proposed method and the maximum number of iteration was
set to 10 for knowledge distillation. For the boundary supporting loss, Nadv = 64 was selected among 256 batch samples.
Conclusion
In this paper, we have investigated informative samples for
efﬁcient knowledge transfer. The adversarial attack method
was modiﬁed to ﬁnd a boundary supporting sample (BSS)
supporting a decision boundary. Based on the BSS, we
proposed a knowledge distillation method to transfer more
accurate information about the decision boundary. Experiments have shown that the proposed method improves the
performance of knowledge distillation. Also, it was shown
that the proposed method has stronger generalization performance and so it is more effective in situations with fewer
training samples. Designing a knowledge distillation method
in terms of sample manipulation is a new direction that has
not been attempted in the past studies. It is also a new approach to utilize an adversarial attack to ﬁnd and transfer
the information about the decision boundary. Therefore, this
work can be useful for future research on knowledge distillation and on the application of an adversarial attack.
Acknowledgement
Next-Generation
Program through NRF funded by Ministry of S&ICT
[2017M3C4A7077582]
[No.B0101-15-0552,
Predictive
Intelligence Technology].