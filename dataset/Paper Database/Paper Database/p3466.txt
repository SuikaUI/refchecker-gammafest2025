CityPersons: A Diverse Dataset for Pedestrian Detection
Shanshan Zhang, Rodrigo Benenson and Bernt Schiele
Max Planck Institute for Informatics
Saarbrücken, Germany
 
Convnets have enabled signiﬁcant progress in pedestrian
detection recently, but there are still open questions regarding suitable architectures and training data. We revisit CNN
design and point out key adaptations, enabling plain FasterRCNN to obtain state-of-the-art results on the Caltech
To achieve further improvement from more and better
data, we introduce CityPersons, a new set of person
annotations on top of the Cityscapes dataset.
The diversity of CityPersons allows us for the ﬁrst time to
train one single CNN model that generalizes well over multiple benchmarks. Moreover, with additional training with
CityPersons, we obtain top results using FasterRCNN
on Caltech, improving especially for more difﬁcult cases
(heavy occlusion and small scale) and providing higher localization quality.
1. Introduction
Pedestrian detection is a popular topic in computer vision community, with wide applications in surveillance, driving assistance, mobile robotics, etc. During the last decade, several benchmarks have been created for this task
 . These benchmarks have enabled great progress
in this area .
While existing benchmarks have enabled progress, it is
unclear how well this progress translate in open world performance. We think it is time to give emphasis not only to
intra-dataset performance, but also across-datasets.
Lately, a wave of convolutional neural network (convnet) variants have taken the Caltech benchmark top ranks
 . Many of these are custom architectures
derived from the FasterRCNN general object
We show here that a properly adapted Faster-
RCNN can match the detection quality of such custom architectures. However since convnets are high capacity models, it is unclear if such model will beneﬁt from more data.
To move forward the ﬁeld of pedestrian detection, we
introduce “CityPersons”, a new set of annotations on top
Figure 1: The diversity of the newly introduced CityPersons
annotations allows to train one convnet model that generalizes well over multiple benchmarks.
of Cityscapes . These are high quality annotations, that
provide a rich diverse dataset, and enable new experiments
both for training better models, and as new test benchmark.
In summary, our main contributions are:
1. We introduce CityPersons, a new set of high quality
bounding box annotations for pedestrian detection on the
Cityscapes dataset (train, validation, and test sets).
train/val. annotations will be public, and an online benchmark will be setup.
2. We report new state-of-art results for FasterRCNN on
Caltech and KITTI dataset, thanks to properly adapting the
model for pedestrian detection and using CityPersons
pre-training. We show in particular improved results for
more difﬁcult detection cases (small and occluded), and
overall higher localization precision.
3. Using CityPersons, we obtain the best reported acrossdataset generalization results for pedestrian detection.
4. We show preliminary results exploiting the additional
Cityscapes annotations. Using semantic labelling as additional supervision, we obtain promising improvements for
detecting small persons.
 
Section 1.1 covers the related work, section 2 discusses how
to adapt FasterRCNN for best detection quality, section 3
describes our annotation process, some statistics of the new
data and baseline experiments. Finally, section 4 explores
different ways to use CityPersons to improve person detection quality.
1.1. Related work
In this paper, we investigate convnets, datasets and semantic labels for pedestrian detection, so we discuss related
works for these three aspects.
pedestrian
detection.
Convolutional
neural networks (convnets) have achieved great success in
classiﬁcation and detection on the ImageNet , Pascal,
and MS COCO datasets . FasterRCNN has
become the de-facto standard detector architecture. Many
variants work try to extend it , but few improve
results with a simpler architecture. A notable exception is
SSD , which obtains comparable results with a simpler
architecture.
Initial attempts to apply convnets for pedestrian detection, used existing detectors (mainly decision forests over
hand-crafted features ) outputs and re-scored them
with a convnet classiﬁer (plus bounding box regression)
 . Better results are shown when using the reverse conﬁguration: detections resulted from a convnet are
re-scored with decision forests classiﬁer (trained over convnet features) . Recently good results are presented
by customized pure convnet architectures such as MS-CNN
 and SA-FastRCNN .
In this paper we show that a properly adapted plain FasterRCNN matches state-of-the-art detection quality without
needing add-ons.
Pedestrian datasets.
In the last decade several datasets
have been created for pedestrian detection training and
evaluation. INRIA , ETH , TudBrussels , and
Daimler represent early efforts to collect pedestrian
datasets. These datasets have been superseded by larger
and richer datasets such as the popular Caltech-USA 
and KITTI . Both datasets were recorded by driving
through large cities and provide annotated frames on video
sequences.
Despite the large number of frames, both datasets suffer
from low-density. With an average of ∼1 person per image, occlusions cases are severely under-represented. Another weakness of both dataset, is that each was recorded
in a single city. Thus the diversity in pedestrian and background appearances is limited.
Building upon the strengths of the Cityscapes data , our
new annotations provide high quality bounding boxes, with
larger portions of occluded persons, and the diversity of 27
different cities. Such diversity enables models trained on
CityPersons to better generalize to other test sets.
Semantic labels for pedestrian detection.
In section 4.3
we will explore using the semantic labels from Cityscapes
to train a pedestrian detector with better context modelling.
The idea of using semantic labels to improve detections is
at least a decade old , and two recent incarnations are
 . We will use the semantic probability maps computed from a semantic labeller network as additional input
channels (next to RGB channels) for the pedestrian detection convnet (see section 4.3).
2. A convnet for pedestrian detection
Before delving into our new annotations (in section 3),
we ﬁrst build a strong reference detector, as a tool for
our experiments in sections 3.4 and 4. We aim at ﬁnding
a straightforward architecture that provides good performance on the Caltech-USA dataset .
Training, testing (MRO, MRN).
We train our Caltech
models using the improved 10× annotations from ,
which are of higher quality than the original annotations
(less false positives, higher recall, improved ignore regions,
and better aligned bounding boxes). For evaluation we follow the standard Caltech evaluation ; log miss-rate (MR)
is averaged over the FPPI (false positives per image) range
of [10−2, 100] FPPI.
Following , we evaluate both
on the “original annotations” (MRO) and new annotations
(MRN); and indicate speciﬁcally which test set is being
used each time. Unless otherwise speciﬁed, the evaluation
is done on the “reasonable” setup .
FasterRCNN.
The FasterRCNN detector obtains competitive performance on general object detection. After retraining with default parameters it will under-perform on
the pedestrian detection task (as reported in ).
reason why vanilla FasterRCNN underperforms on the Caltech dataset is that it fails to handle small scale objects
(50 ∼70 pixels), which are dominant on this dataset. To
better handle small persons, we propose ﬁve modiﬁcations
(Mi) that bring the MRO (miss-rate) from 20.98 down to
10.27 (lower is better, see table 1). As of writing, the best
reported results on Caltech is 9.6 MRo, and our plain FasterRCNN ranks third with less than a point difference. We
train FasterRCNN with VGG16 convolutional layers, initialized via ImageNet classiﬁcation pre-training .
M1 Quantized RPN scales. The default scales of the RPN
(region proposal network in FasterRCNN) are sparse
([0.5, 1, 2]) and assume a uniform distribution of object
scales. However, when we look at the training data on Caltech, we ﬁnd much more small scale people than large ones.
Detector aspect
FasterRCNN-vanilla
+ quantized rpn scales
+ input up-scaling
+ Adam solver
+ ignore region handling
+ ﬁner feature stride
FasterRCNN-ours
Table 1: Step by step improvements on Caltech from vanilla
FasterRCNN to our adapted version, we gain 10.71 MR
points in total.
Our intuition is to let the network generate more proposals
for small sizes, so as to better handle them. We split the full
scale range in 10 quantile bins (equal amount of samples
per bin), and use the resulting 11 endpoints as RPN scales
to generate proposals.
M2 Input up-scaling. Simply up-sampling the input images by 2x, provides a signiﬁcant gain of 3.74 MRO percent
points (pp). We attribute this to a better match with the ImageNet pre-training appearance distribution. Using larger
up-sampling factors does not show further improvement.
M3 Finer feature stride. Most pedestrians in Caltech have
height×width = 80×40. The default VGG16 has a feature
stride of 16 pixels. Having such a coarse stride compared to
the object width reduces the chances of having a high score
over persons, and forces the network to handle large displacement relative to the object appearance. Removing the
fourth max-pooling layer from VGG16 reduces the stride to
8 pixels; helping the detector to handle small objects.
M4 Ignore region handling. The
FasterRCNN
code does not cope with ignore regions (areas where the
annotator cannot tell if a person is present or absent, and
person groups where individuals cannot be told apart).
Simply treating these regions as background introduces
confusing samples, and has a negative impact on the
detector quality. By ensuring that during training the RPN
proposals avoid sampling the ignore regions, we observe a
1.33 MR pp improvement.
M5 Solver. Switching from the standard Caffe SGD solver
to the Adam solver , provides a consistent gain in our
experiments.
We show the step-by-step improvements in table 1.
and M2 are key, while each of the other modiﬁcations add
about ∼1 MR pp. All together these modiﬁcations adapt
the vanilla FasterRCNN to the task of pedestrian detection.
Other architectures.
We also explored other architectures such as SSD or MS-CNN but, even after adaptations, we did not manage to obtain improved results.
Amongst all the variants reaching ∼10% MR our Faster-
RCNN is the simplest.
Conclusion.
Once properly adapted, FasterRCNN obtains competitive performance for pedestrian detection on
the Caltech dataset. This is the model we will use in all
following experiments.
In section 3 we introduce a new dataset that will enable
further improvements of detection performance.
3. CityPersons dataset
The Cityscapes dataset was created for the task of semantic segmentation in urban street scenes. It consists of a
large and diverse set of stereo video sequences recorded in
streets from different cities in Germany and neighbouring
countries. Fine pixel-level annotations of 30 visual classes
are provided for 5 000 images from 27 cities. The ﬁne annotations include instance labels for persons and vehicles.
Additionally 20 000 images from 23 other cities are annotated with coarse semantic labels, without instance labels.
In this paper, we present the CityPersons dataset, built
upon the Cityscapes data to provide a new dataset of interest
for the pedestrian detection community. For each frame
in the 5 000 ﬁne-annotations subset, we have created high
quality bounding box annotations for pedestrians (section
3.1). In section 3.2 we contrast CityPersons with previous datasets regarding: volume, diversity and occlusion.
In section 4 we show how to use this new data to improve
results on other datasets.
3.1. Bounding box annotations
The Cityscapes dataset already provides instance level
segments for each human. These segments indicate the visible parts of humans. Simply using bounding boxes of these
segments would raise three issues. I1) The box aspect ratio
would be irregular, persons walking have varying width. It
has been proposed to thus normalize aspect ratio for pedestrian annotations. I2) Even after normalizing aspect ratio,
the boxes would not align amongst each other. They will
be off in the horizontal axis due to being normalized based
on the segment centre rather the object centre. They will
be off in the vertical axis due to variable level of occlusion
for each person. It has been shown that pedestrian detectors
beneﬁt from well aligned training samples , and conversely, training with misaligned samples will hamper results. I3) Existing datasets (INRIA, Caltech, KITTI) have
deﬁned bounding boxes covering the full object extent, not
just the visible area. In order to train compatible, high quality models, we need to have annotations that align well the
full extent of the persons bodies (“amodal bounding box”
Fine-grained categories.
In the Cityscapes dataset, humans are labelled as either person or rider. In this paper, we
provide further ﬁne-grained labels for persons. Based on
the postures, we group all humans into four categories: pedestrian (walking, running or standing up), rider (riding bicycles or motorbikes), sitting person, and other person (with
unusual postures, e.g. stretching).
Annotation protocol.
For pedestrians and riders (cyclists, motorists), we follow the same protocol as used in
 , where the full body is annotated by drawing a line
from the top of the head to the middle of two feet, and the
bounding box is generated using a ﬁxed aspect ratio (0.41).
This protocol has been shown to provide accurate alignments. The visible bounding box for each instance is the
tightest one fully covering the segment mask, and can be
generated automatically from the segment. See an illustration in ﬁgure 2. The occlusion ratio can then be computed
as area(BB−vis)
area(BB−full).
As of other categories of persons, i.e. sitting and other
persons, there is no uniform alignment to apply, so we only
provide the segment bounding box for each of them without
full body annotations.
Apart from real persons, we also ask the annotators to
search over the whole image for areas containing fake humans, for instance, people on posters, statue, mannequin,
people’s reﬂection in mirror or window, etc., and mark them
as ignore regions.
Annotation tool.
Since we already have the segment
mask for each instance, we can do the annotations in a more
efﬁcient way than from scratch. To this end, we develop a
new annotation tool to avoid searching for persons over the
images by exploiting the available instance segments. This
tool pops out one person segment at a time and asks the
annotator to recognize the ﬁne-grained category ﬁrst and
then do the full body annotation for pedestrians and riders.
Thanks to the high-quality of segmentation annotations, using such a tool also reduces the risk of missing persons,
especially at crowded scenes. But the ignore region annotations have to be done by searching over the whole images.
3.2. Statistics
We show the number of bounding box annotations provided by us in table 2. In a total of 5 000 images,
we have ~35k person and ~13k ignore region annotations.
And we notice the density of persons are consistent across
train/validation/test subsets. Please note we use the same
split as Cityscapes.
Diversity.
We compare the diversity of Caltech, KITTI
and CityPersons in table 3. Since KITTI test set annota-
(b) Segmentation mask (c) Bounding box anno.
Figure 2: Illustration of bounding box annotations for pedestrians. For each person, the top of the head and middle
of the feet is drawn by the annotator. An aligned bounding
box is automatically generated using the ﬁxed aspect ratio
(0.41). The bounding box covering the segmentation mask
is used to estimate the visible part.
# ignore regions
Statistics of bounding box annotations on
CityPersons dataset.
tions are not publicly available, we only consider the training subset for a fair comparison.
The CityPersons training subset was recorded across
18 different cities, three different seasons, and various
weather conditions. While the Caltech and KITTI datasets
are only recorded in one city at one season each.
In terms of density, we have on average ~7 persons per
image. This number is much higher than that on the Caltech and KITTI datasets, where each image only contains
~1 person on average.
Also, the number of identical persons is another important evidence of diversity. On our CityPersons dataset, the number of identical persons amounts up to ∼
20 000. In contrast, the Caltech and KITTI dataset only
contains ~ 1 300 and ~ 6 000 unique pedestrians respectively. Note KITTI and CityPersons frames are sampled
very sparsely, so each person is considered as unique.
CityPersons also provides ﬁne-grained labels for persons. As shown in ﬁgure 3, pedestrians are the majority
(83%). Although riders and sitting persons only occupy
10% and 5% respectively, the absolute numbers are still
considerable, as we have a large pool of ~35k persons.
Occlusion.
The Cityscapes data was collected by driving through the centre of some highly populated cities,
e.g. Frankfurt and Hamburg. We notice that on some images, there are ~100 people walking on the street, highly
occluded by each other. Such a high occlusion is rarely
CityPersons
# person/image
# unique person
Comparison of diversity on different datasets
(training subset only).
pedestrian
Figure 3: Fine-grained person categories on CityPersons.
% Pedestrian
Occlusion ratio
CityPersons
Heavy occlusion
Reasonable
Comparison of occlusion distributions on
CityPersons and Caltech datasets. CityPersons contains more occlusions in the reasonable subset than Caltech.
28.5%(0.0)
27.4%(0.2)
Figure 5: Top 9 of quantized 11 occlusion patterns of pedestrians on CityPersons dataset. Two numbers on top indicate percentage and average occlusion ratio of samples
clustered into each pattern.
seen in previous datasets.
In ﬁgure 4, we compare the
distribution of pedestrians at different occlusion levels for
Caltech and CityPersons.
We notice that on Caltech
there are more than 60% fully visible pedestrians, while
on CityPersons there are less than 30%. This indicates
we have two times more occlusions than Caltech, which
makes CityPersons a more interesting ground for occlusion handling. Moreover, on the reasonable subset (<=0.35
occlusion) the community typically use, Caltech is dominated by fully visible pedestrians, while CityPersons has
more occlusion cases.
In order to understand which kinds of occlusions we have
on CityPersons, we quantize all persons into 11 patterns
and show the top 9 of them in ﬁgure 5 (the last two patterns are not shown as they are of less than 1% and thus
noisy). For visualization, we resize each full body bounding box to a ﬁxed size, and then overlay the segmentation
mask. For each pattern, the bright area shows the visible
part and the two numbers on top indicate the percentage and
average occlusion ratio of corresponding pattern. The ﬁrst
two patterns (55.9%) roughly cover the “reasonable” subset; the third and fourth patterns correspond to occlusions
from either left or right side. Apart from that, we still have
about 30% pedestrians distributed in various patterns, some
of which have a very high occlusion ratio (>0.9). Such distributed occlusion patterns increase the diversity of the data
and hence makes the dataset a more challenging test base.
3.3. Benchmarking
With the publication of this paper, we will create a
website for CityPersons dataset, where train/validation
annotations can be downloaded, and an online evaluation
server is available to compute numbers over the held-out
test annotations.1
We follow the same evaluation protocol as used for Caltech , by allowing evaluation on different subsets. In this
paper, MR stands for log-average miss rate on the “reasonable” setup (scale [50, ∞], occlusion ratio [0, 0.35]) unless
otherwise speciﬁed. While evaluating pedestrian detection
performance, cyclists/sitting persons/other persons/ignore
regions are not considered, which means detections matching with those areas are not counted as mistakes.
3.4. Baseline experiments
To understand the difﬁculties of pedestrian detection on
the CityPersons dataset, we train and evaluate three different detectors. ACF and Checkerboards are representatives from the Integral Channel Features detector
(ICF) family, while FasterRCNN acts as the state-ofthe-art detector. We set up the FasterRCNN detector by following the practices we learned from Caltech experiments
(section 2). Since CityPersons images are ~7 times larger
than Caltech, we are only able to use an upsampling factor
of 1.3 to ﬁt in 12GB of GPU memory.
We re-train each detector using the CityPersons train-
1As a subset of the Cityscapes dataset, CityPersons annotations
and benchmark will be available on the Cityscapes website. The evaluation
server is being setup and the metrics will change.
Checkerboards
Faster RCNN
CityPersons
Figure 6: Comparison of baseline detectors on Caltech test
and CityPersons val.
set (reasonable).
Numbers are
MRN on Caltech and MR on CityPersons (lower is better). Ranking of methods between two datasets is stable.
For all methods, CityPersons is more difﬁcult to solve
than Caltech.
ing set and then evaluate on the validation set. Note that
all the CityPersons numbers reported in this paper are on
the validation set. Consistent with the reasonable evaluation
protocol, we only use the reasonable subset of pedestrians
for training; cyclists/sitting persons/other persons/ignore regions are avoided for negative sampling.
In ﬁgure 6, we show the comparison of the above three
detectors on CityPersons and Caltech. FasterRCNN outperforms ICF detectors by a large margin, which indicates
the adaptation of FasterRCNN on Caltech is also transferable to CityPersons. Moreover, we ﬁnd the ranking of
three detectors on CityPersons is consistent with that on
Caltech, but the performance on CityPersons dataset is
lower for all three detectors. This comparison shows that
CityPersons is a more challenging dataset, thus more interesting for future research in this area.
To understand the impact of having a larger amount of
training data, we show how performance grows as training
data increases in ﬁgure 7. We can see performance keeps
improving with more data. Therefore, it is of great importance to provide CNNs with a large amount of data.
Considering the trade off between speed and quality, we
use an alternative model of our FasterRCNN by switching
off input image upsampling for the analysis experiments
shown in ﬁgure 7 and section 4.3. This model is about 2x
faster at both training and test time, but only drops the performance by ~2 pp (from 13% MR to 15% MR).
Conclusion.
The CityPersons dataset can serve as a
large and diverse database for training a powerful model,
as well as a more challenging test base for future research
on pedestrian detection.
Proportion of training data
Figure 7: Quality as function of training volume. Faster-
RCNN model trained/evaluated on CityPersons train/val.
set (MR: lower is better).
4. Improve quality using CityPersons
Having the CityPersons dataset at hand, we now proceed to illustrate three different ways it enables to improve pedestrian detection results (§4.1, §4.2, §4.3). As we
will see, CityPersons is particularly effective at improving results for small scale pedestrians, occluded ones, and
providing higher localization accuracy.
4.1. Generalization across datasets
Commonly, a detector is trained on the training set of
the target benchmark. As such, one needs to train multiple
detectors for different benchmarks. Ideally, one would wish
to train one detector that is able to perform well on multiple
benchmarks. Since the CityPersons dataset is large and
diverse, we wonder if it can allow us to train a detector with
good generalization capabilities.
To see how well CityPersons data generalizes across
different datasets, we train models on Caltech, KITTI and
CityPersons datasets, and then apply each of them on
six different test sets: Caltech, KITTI, CityPersons, IN-
RIA, ETH and Tud-Brussels. For KITTI, we split the public training data into training and validation subsets (2:1)
by random sampling. Table 4 shows comparisons of two
detectors: ACF and FasterRCNN .
We observe:
(1) Overall, when trained with the same data Faster-
RCNN generalizes better across datasets than ACF. (Note
that FasterRCNN beneﬁts from ImageNet pre-training,
while ACF does not.)
(2) For both detectors, the mean MR across test sets is
signiﬁcantly better for models trained with CityPersons
training data. CityPersons generalizes better than Caltech and KITTI.
These experiments conﬁrm the generalization ability of
CityPersons dataset, that we attribute to the size and diversity of the Cityscapes data, and to the quality of the
bounding boxes annotations.
CityPersons
CityPersons
Tud-Brussels
CityPersons
CityPersons
Tud-Brussels
(b) FasterRCNN
Table 4: Generalization ability of two different methods,
trained and tested over different datasets. All numbers are
MR on reasonable subset. Bold indicates the best results
obtained via generalization across datasets (different train
and test).
4.2. Better pre-training improves quality
In table 4, we ﬁnd the CityPersons data acts as very
good source of training data for different datasets, assuming
we are blind to the target domain. Furthermore, when we
have some training data from the target domain, we show
CityPersons data can be also used as effective external
training data, which helps to further boost performance.
First, we consider Caltech as the target domain, and compare the quality of two models. One is trained on Caltech
data only; and the other is ﬁrst trained on CityPersons,
and then ﬁnetuned on Caltech (CityPersons→Caltech).
From table 5, we can see the additional training with
CityPersons data improves the performance in the following three aspects.
CityPersons data improves overall performance.
When evaluated on the reasonable setup, the
CityPersons→Caltech model obtains ~1 pp gain.
(2) CityPersons data improves more for harder cases,
smaller scale, heavy occlusion.
We notice the gap
for heavy occlusion is large (~9 pp), due to more occluded
training samples on the CityPersons dataset.
trend is also found for smaller scale persons ( ).
(3) CityPersons data helps to produce better-aligned
CityPersons
Reasonable
Heavy occl.
Reasonable
Reasonable
[50, ∞] 0.75
Table 5: Gains from additional CityPersons training at
different evaluation setups on Caltech test set. MRO and
MRN indicate numbers evaluated on original and new annotations . CityPersons pre-training helps more for
more difﬁcult cases. See also table 6.
CityPersons
Reasonable
Reasonable
[50, ∞] 0.75
Table 6: Gains from additional CityPersons training at
different evaluation setups on KITTI validation set.
numbers are MR (see §2). Here also, CityPersons pretraining helps more for more difﬁcult cases. See also table
detections. The Caltech new annotations are well aligned,
thus a good test base for alignment quality of detections.
When we increase the IoU threshold for matching from
0.50 to 0.75, the gain from CityPersons data also grows
from 1 pp to 5 pp. This gap indicates the high quality of
CityPersons annotations are beneﬁcial to produce betteraligned detections.
Compared with other state-of-the-art detectors, our best
model using CityPersons for pre-training obtains 5.1%
MRN at IoU 0.50 evaluation, outperforming previous best
reported results (7.3% MRN) by 2.2 pp (ﬁgure 8a); this gap
becomes even larger (~ 20 pp) when we use a stricter IoU
of 0.75 (ﬁgure 8b). From the comparison, our FasterRCNN
detector obtains state-of-the-art results on Caltech, and improves the localization quality signiﬁcantly.
When we consider KITTI as the target domain, we
also see improvements brought by additional training with
CityPersons data. As shown in table 6, the gain on reasonable evaluation setup is 2.5 pp, while for smaller scale,
the gap becomes more impressive (10.7 pp). The 4.1 pp
gap at IoU 0.75 again veriﬁes CityPersons data helps to
produce better aligned detections.
4.3. Exploiting Cityscapes semantic labels
In this subsection, we explore how much improvement
can be obtained for pedestrian detection by leveraging the
false positives per image
16.4% FasterRCNN-vanilla-Caltech
16.0% FasterRCNN-ours-CityPersons
9.2% CompACT-Deep
8.1% MS-CNN
7.3% RPN+BF
5.8% FasterRCNN-ours-Caltech
5.1% FasterRCNN-ours-CityPersons Caltech
(a) IoU=0.50
false positives per image
59.9% RPN+BF
59.8% CompACT-Deep
57.4% FasterRCNN-vanilla-Caltech
52.7% MS-CNN
36.7% FasterRCNN-ours-CityPersons
30.6% FasterRCNN-ours-Caltech
25.8% FasterRCNN-ours-CityPersons Caltech
(b) IoU=0.75
Figure 8: Comparison of state-of-the-art results on the Caltech test set (reasonable subset), MRN.
(a) Original image
(b) Semantic map
Figure 9: Example of semantic map generated by an FCN-
8s model trained on Cityscapes coarse annotations.
semantic labels available on the Cityscapes dataset.
We use an FCN-8s model trained on Cityscapes
coarse annotations to predict semantic labels. Note we cannot involve ﬁne-annotation images in this semantic labelling
training, otherwise our following detection training will suffer from overﬁtting. Although this model is only trained
on coarse annotations, we can see the semantic segmentation mask provides a reasonable structure for the whole
scene (ﬁgure 9). Then we concatenate semantic channels
Scale range
+ Semantic
Table 7: Improvements from semantic channels in different scale ranges. Numbers are MR on the CityPersons
val. set. Albeit there is small overall gain, adding semantic
channels helps for the difﬁcult case of small persons.
with RGB channels and feed them altogether into convnets,
letting convnets to ﬁgure out the hidden complementarity.
For the reasonable evaluation setup, we get an overall
improvement of ~0.6 pp from semantic channels. When
we look at the ﬁne-grained improvements for different scale
ranges, we ﬁnd that semantic channels help more for small
persons, which is a hard case for our task (table 7).
As a preliminary trial, we already get some improvements from semantic labels, which encourage us to explore
more effective ways of using semantic information.
5. Summary
In this paper, we ﬁrst show that a properly adapted
FasterRCNN can achieve state-of-the-art performance on
Caltech. Aiming for further improvement from more and
better data, we propose a new diverse dataset namely
CityPersons by providing bounding box annotations for
persons on top of Cityscapes dataset. CityPersons shows
high contrast to previous datasets as it consists of images
recorded across 27 cities, 3 seasons, various weather conditions and more common crowds.
Serving as training data, CityPersons shows strong
generalization ability from across dataset experiments.
Our FasterRCNN model trained on CityPersons obtains
reasonable performance over six different benchmarks.
Moreover, it further improves the detection performance
with additional ﬁnetuning on the target data, especially for
harder cases (small scale and heavy occlusion), and also enhance the localization quality.
On the other hand, CityPersons can also be used as a
new test benchmark as there are more challenges, e.g. more
occlusions and diverse environments. We will create a website for this benchmark and only allows for online evaluations by holding out the test set annotations.
Other than bounding box annotations for persons, there
are additional information to leverage on CityPersons,
for instance, ﬁne semantic segmentations, other modalities of data (stereo, GPS), and un-annotated neighbouring
frames. Our preliminary results of using semantic labels
show promising complementarity. These rich data will motivate more efforts to solve the problem of pedestrian detection.