Received January 1, 2019, accepted January 24, 2019, date of publication February 1, 2019, date of current version February 22, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2896880
Speech Recognition Using Deep Neural Networks:
A Systematic Review
ALI BOU NASSIF
1, ISMAIL SHAHIN
1, IMTINAN ATTILI1,
MOHAMMAD AZZEH2, AND KHALED SHAALAN
1Department of Electrical and Computer Engineering, University of Sharjah, Sharjah 27272, United Arab Emirates
2Department of Software Engineering, Applied Science Private University, Amman 163, Jordan
3Faculty of Engineering and IT, The British University in Dubai, Dubai 345015, United Arab Emirates
Corresponding author: Ali Bou Nassif ( )
This work was supported by the University of Sharjah through the Competitive Research Project ‘‘Emotion Recognition in each of
Stressful and Emotional Talking Environments Using Artiﬁcial Models’’ under Grant 1602040348-P. The work of M. Azzeh was supported
by the Applied Science Private University, Amman, Jordan.
ABSTRACT Over the past decades, a tremendous amount of research has been done on the use of machine
learning for speech processing applications, especially speech recognition. However, in the past few years,
research has focused on utilizing deep learning for speech-related applications. This new area of machine
learning has yielded far better results when compared to others in a variety of applications including speech,
and thus became a very attractive area of research. This paper provides a thorough examination of the
different studies that have been conducted since 2006, when deep learning ﬁrst arose as a new area of
machine learning, for speech applications. A thorough statistical analysis is provided in this review which
was conducted by extracting speciﬁc information from 174 papers published between the years 2006 and
2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus
to new research topics.
INDEX TERMS Speech recognition, deep neural network, systematic review.
I. INTRODUCTION
Since the last decade, deep learning has arisen as a new attractive area of machine learning, and ever since has been examined and utilized in a range of different research topics .
Deep learning consists of a multiple of machine learning
algorithms fed with inputs in the form of multiple layered
models. These models are usually neural networks consisting
of different levels of non-linear operations. The machine
learning algorithms attempt to learn from these deep neural
networks by extracting speciﬁc features and information .
Prior to 2006, searching deep architecture inputs was not a
predictable straight forward task; however, the development
of deep learning algorithms helped resolve this issue and
simpliﬁed the process of searching the parameter space of
deep architectures . Deep learning models can also operate
as a greedy layerwise unsupervised pre-training. This means
that it will learn hierarchy from extracted features from each
layer at a time. Feature learning is achieved by training each
The associate editor coordinating the review of this manuscript and
approving it for publication was Malik Jahan Khan.
layer with an unsupervised learning algorithm, which takes
the features extracted from the previous layer and uses it as
an input for the next layer. Thus, feature learning will attempt
to learn the transformation of the previously learned features
at each new layer. Each iteration feature learning adds one
layer of weights to a deep neural network. The resulted layers
with learned weights can eventually be loaded to initialize a
deep supervised predictor , . Using deep architectures
has proven to be more efﬁcient in representing non-linear
functions in comparison to shallower architectures. Studies
have shown that fewer parameters are required to represent a
certain non-linear function in a deep architecture in comparison with the large number of parameters needed to represent
the same function in a shallower architecture. This shows that
deeper architectures are more efﬁcient from a statistical point
of view , .
Deep learning algorithms have been mostly used to further
enhance the capabilities of computers so that it understands
what humans can do, which includes speech recognition.
Speech in particular, being the main method of communication among human beings, received much interest for the
VOLUME 7, 2019
2019 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See for more information.
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
past ﬁve decades right from the introduction of artiﬁcial
intelligence , . Therefore, it is only natural that one
of the early applications of deep learning was speech, and
up to this day a huge number of research papers have been
 
applications speciﬁcally speech recognition , , , .
The conventional speech recognition systems are based on
representing speech signals using Gaussian Mixture Models
(GMMs) that are based on hidden Markov models (HMMs).
This is due to the fact that a speech signal can be considered
as a piecewise stationary signal or in other terms a short
time stationary signal. In this short time scale, the speech
signal can be approximated as a stationary process, thus it
can be thought of as a Markov model for many stochastic processes. Each HMM uses a mixture of Gaussian to
model a spectral representation of the sound wave. This type
of systems is considered simple in design and practical in
use. However, they are considered statistically inefﬁcient for
modeling non-linear or near non-linear functions , .
Opposite to HMMs, neural networks permit discriminative
training in a much efﬁcient manner. However, it works better
for short time signals such as isolated words, when it comes to
continuous speech signals it is rarely successful. This is due
to its inability to model temporal dependencies for continuous signals. Thus, one solution is using neural networks as
a pre-processing e.g. feature transformation, dimensionality
reduction for the HMM based recognition . There are
many examples that prove that using deep neural networks
yield better results than classical models. In 2012, Microsoft
released the newest version of their Microsoft Audio Video
Indexing Service (MAVIS) which is a speech system based on
deep learning. Their ﬁnal results clearly showed that the word
error rate (WER) reduced on four major benchmarks by 30%
compared to the state-of-the-art models based on Gaussian
mixtures .
This systematic literature review (SLR) follows Kitchenham and Charters guidelines and was focusing on identifying the different research papers that have been published
from 2006 to 2018 in the area of deep neural networks
in speech-related applications. Such applications include:
automatic speech recognition, emotional speech recognition,
speaker identiﬁcation and speech enhancement among others. The identiﬁed number of papers was originally 230;
however, after applying the inclusion/exclusion criteria, only
174 papers were included in the study. The research questions
were answered by extracting proper information from these
174 papers and then forming a statistical representation using
tables and ﬁgures. The results presented are intended to show
the trend of research done in this area over the past years as
well as bring focus to new interesting research topics.
This paper summarizes the related work in Section 2,
whereas information regarding the background such as
speech recognition and deep neural networks is presented in
Section 3. Section 4 summarizes the methodology used to
conduct this review. Section 5 demonstrates the results, where
Section 6 concludes the paper.
II. RELATED WORK
Some surveys have been conducted in the area of speech
recognition. For instance, Morgan conducted a review in
the area of speech recognition assisted with discriminatively
trained feed-forward networks. The main focus of the review
was to shed the light on papers that employ multiple layers of processing prior to the hidden Markov model based
decoding of word sequences. Throughout the paper, some of
the methods that incorporate multiple layers of computation
for the purpose of either providing large gains for noisy
speech in small vocabulary tasks or signiﬁcant gains for
high Signal-to-Noise Ratio (SNR) speech on large vocabulary
tasks were described. Moreover, a detailed description was
provided about the methods with structures that incorporate
a large number of layers (the depth) and multiple streams
using Multilayer Perceptrons (MLPs) with a large number
of hidden layers. This review paper eventually concluded
that even though the deep processing structures are capable
of providing improvements in this genre, choice of features
and the structure with which they are incorporated, including
layer width, can also be signiﬁcant factors.
Hinton et al. , presents an overview on the use of deep
neural networks that incorporate many number of hidden
layers that are trained using some of the new techniques.
The overview summarizes the ﬁndings of four different
research groups that collaborated to reveal the advantage of
a feed-forward neural network that has quite a few frames
of coefﬁcients as an input and produces subsequent probabilities over HMM states as an output. This technique was
studied as an alternative to using the traditional HMMs
and GMMs for acoustic modeling in speech recognition.
The collected results have shown that deep neural networks
that incorporate many hidden layers and are trained by
new techniques outperform GMMs - HMMs on a variety
of speech recognition benchmarks, by sometimes a large
Deng et al. presented an overview summary on the
papers that were part of the session at ICASSP- 2013, entitled
‘‘New Types of Deep Neural Network Learning for Speech
Recognition and Related Applications,’’ which was organized by the authors. In addition to that, the paper presented
the history of the development of the deep neural networks
for acoustic models for speech recognition. The overview
summary focused on the different ways that can be utilized
to improve deep learning, which was classiﬁed into ﬁve
different categories: enhanced types of network architecture
and activation functions, enhanced optimization methods,
enhanced ways of determining the deep neural networks
parameters and ﬁnally enhanced ways of leveraging a number
of languages at the same time. The overview revealed the
rapid continues progress in the acoustic models that use deep
neural networks which can be seen on several fronts when
compared to those based on GMMs. The paper also revealed
that these acoustic models can also be applicable and enhance
performance in other signal processing applications, and not
only speech recognition.
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
Deng et al. conducted a summary on the work done
by Microsoft since the year 2009 in the area of speech using
deep learning. The paper focused on more recent advances
which helped shed some light on the different capabilities
as well as limitations of deep learning in the area of speech
recognition. This was done by providing samples of the
recent experiments carried by Microsoft for advancing speech
related applications through the use of deep learning methods. Speech related applications included features extraction,
modeling language, acoustic models, understanding speech
as well as dialogue estimation. Experimental results have
shown clearly that the speech spectrogram features are more
advanced to MFCC with deep neural networks compared to
the traditional practice using GMMs - HMMs. This paper also
shows that improvements should be done on the architecture
of deep neural networks in order to improve further the
features of acoustic measurements
Li et al. presented the basics of the state of the art
solutions for automatic spoken language recognition for both,
computational and phonological perspectives. Huge progress
was achieved in recent years in the area of spoken language
recognition which was mostly directed by breakthroughs
in relevant signal processing areas such as pattern recognition and cognitive science. Several main aspects relevant to
language recognition was discussed such as language characterization, modeling methods, as well as system development techniques. Findings clearly indicate that even though
this area has hugely developed in the past years, it is still
far from the perfect, especially when it comes to language
characterization. In addition, this paper provides an overview
on the current research trends and future directions which
was carried using the language recognition evaluation (LRE)
which is developed by the National Institute of Standards and
Technology (NIST).
Li et al. provided an overview on modern noise robust
techniques for automatic speech recognition developed over
the past three decades. More emphasis was given on the
techniques that have proven successful over the years and
are likely to maintain and further expand in their applicability
in the future. The examined techniques were categorized and
evaluated using ﬁve different criteria, which are: using former
knowledge about the acoustic environment distortion, model
domain processing versus feature domain processing, using
speciﬁc environment distortion models, uncertainty processing versus predetermined processing and ﬁnally using acoustic models trained by the same model adaptation process used
in the testing stage. This study helps the reader differentiate
between the different noise-robust techniques, as well as provides a comprehensive insight on the performance complex
tradeoffs that should be taken into account when selecting
between the available techniques.
This systematic review is different from the above as we
are presenting a comprehensive study on the use of deep
neural networks in the area of speech recognition. We ﬁrst
provided an overview on machine learning, its categories and
deﬁnitions, with emphasis on deep learning which is our main
concern in the paper. We then provided an overview on speech
recognition as well, its different applications, features and
types. This provides the reader with a suitable comprehensive
theoretical background in order to fully grasp the topic presented which is the use of deep neural networks in the area of
speech. Moreover, in order to carry the systematic literature
review, 174 papers were used which were published on the
span of 12 years between 2006 and 2018. The information
extracted from the above papers includes the following:
1) The different types of speech identiﬁed.
2) The types of database used to train and test the
algorithm.
3) The different languages used to train and test the
algorithm.
4) The type of environment (noisy, emotional or neutral)
each study used.
5) The different types of features extracted from speech.
6) The type of publication (journal, conference or
workshop).
7) The speciﬁc name of the conference or journal that
published the paper.
8) The distribution of papers over the years.
This extracted information helped identify research patterns over the past decade in the use of deep neural networks
in the area of speech. The developed statistics throughout our
study helped shed light on research gaps and shortcomings,
as well as the past and future direction of studies in this area.
This will hopefully help future researchers in identifying new
research topics in this area as well as try to ﬁnd suitable
solutions to amend the gaps and shortcomings in the already
existing research.
III. BACKGROUND
A. SPEECH SIGNALS
Speech signals can provide us with different kinds of information. Such kinds of information are:
• Speech recognition, which gives information about the
content of speech signals.
• Speaker recognition that carries information about the
speaker identity.
• Emotion recognition, which delivers information about
the speaker’s emotional state.
• Health recognition, which offers information on the
patient’s health status.
• Language recognition, that yields information of the
spoken language.
• Accent recognition, which produces information about
the speaker accent.
• Age recognition that supplies information about the
speaker age.
• Gender recognition, which carries information about the
speaker gender.
Automatic speech recognition is the capability of a
machine or computer to recognize the content of words
and phrases in an uttered language and transform them to
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
a machine-understandable format. Speech recognition can
be used in many applications. Such applications appear
in: dictating computers instead of typing, spaceships when
the extremities are busy, helping handicapped people, smart
homes, and many others.
Automatic speaker recognition can be deﬁned as the process of recognizing the unknown speaker on the basis of
the information embedded in his/her speech signal using
machine (computer). Speaker recognition is divided into
two parts: speaker identiﬁcation and speaker veriﬁcation
(authentication). The process of determining to which of the
registered speaker a given utterance corresponds to is termed
as the speaker identiﬁcation part. This part can be used in
public facilities or for the media. These cases comprise, but
not limited to, district or other government institutions, calls
to radio stations, insurance agencies, or documented conversations , . Speaker veriﬁcation part is described
as the procedure of admitting or discarding the claimed
speaker identity. The applications of this part comprise the
use of voice as a focal factor to authorize the claimed
speaker identity. Business relations using a telephone network, dataset access facilities, security control for private
information areas, remote access to computers, and intelligent health care systems are some of application areas of
this branch , . Emotion cue-based speaker recognition becomes one of the research ﬁelds for human-machine
interaction or affective computing that has recently earned
accelerating attentions due to the broad diversity of applications that proﬁt from this up-to-date technology . The
main inspiration arises from the demand to mature a humanmachine (computer) interface that is more adaptive to a user’s
identity. The major role of the intelligent human-machine
interaction is to enable computers with the capability of
affective computing so that computers can recognize the
user for distinct applications. The low speaker recognition
performance in emotional talking environments is considered
as one of the most challenging issues of this ﬁeld – .
Emotion recognition by machine can be deﬁned as the
task of recognizing the unknown emotion based on information inserted in speech signals. Emotion recognition ﬁeld is
divided into emotion identiﬁcation and emotion veriﬁcation
branches. In the ﬁrst branch, the unknown emotion is identiﬁed as the emotion whose model best matches the input
speech signal. In the second branch, the goal is to determine
whether a given emotion belongs to a speciﬁc known emotion or to some other unknown emotions. The applications of
emotion recognition clearly appear in – perceiving
the speaker emotional status in telephone call center conversations and supplying feedback to an operator for observing
purposes, categorizing voice mail messages based on emotions articulated by callers, and recognizing the mistrusted
individuals who produced emotional voice (e.g. happiness)
in emotional talking environments.
Automatic health recognition is deﬁned as using the
patient’s voice to provide information on the patient’s health
status. Automatic health recognition can be used in intelligent
health care systems , . These systems can be utilized in hospitals which include computerized health categorization and evaluation methods . These systems can
also be used in the pathological voice assessment (functional dysphonic voices) . The dysphonic voice can be
hoarse or extremely breathy, harsh, or rough. In addition,
automatic health recognition systems can be used in the
diagnosis of Parkinson’s disease. In Massachusetts Institute
of Technology (MIT), a team led by Max Little conducted
some experiments and tests to analyze and evaluate the voice
characteristics of patients who had been detected with Parkinson’s disease. They found that they could establish a tool to
detect such a disease in the individuals’ speech patterns.
Language recognition is the problem of determining which
natural language given a speech content is in. One of the
immense challenges of language recognition systems is to
differentiate between closely correlated languages. Similar
languages such as Serbian and Croatian or Indonesian and
Malay show signiﬁcant lexical and structural overlap. Hence,
the discrimination between each such two languages become
challenging for language recognition systems. The applications of automatic language recognition evidently appear in
spoken language translation , multilingual speech recognition , and spoken document retrieval .
The task of accent recognition is the recognition of a
speaker’s regional accent, within a predetermined language,
given the acoustic signal alone. The problem of accent recognition has been considered as more challenging than that of
language recognition due to the greater similarity between
accents of the same language . Accent recognition has
wide range of applications in our daily life. Accent recognition helps in Automatic Speech Recognition (ASR) since
speakers with diverse accents pronounce some words differently, constantly varying particular phones. Accent recognition also allows us to conclude the speaker’s regional origin
and ethnicity and consequently to adjust features used in
speaker recognition to regional origin.
Age recognition by voice is the process of estimating the
speaker’s age (e.g. child, young, adult, senior, etc.) using
his/her uttered speech signals. Automatic age recognition can
be used in security applications, age-restriction applications,
and others .
Automatic gender recognition is the process of recognizing
whether the speaker is a male or female. Generally, automatic
gender recognition results in a great accuracy without much
effort because the results of such type of recognition is binary
(either male or female) , . Automatic gender recognition clearly appears in the applications of call centers of
some conservative societies, where automatic dialog systems
with the ability of recognizing genders are favored over those
without such ability.
B. MACHINE LEARNING
Recently data has become very easily obtained through
numerous numbers of open sources. Extracting knowledge
from data is considered the real challenge. With the use of
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
FIGURE 1. Different stages of machine learning.
computers and smart software that can perform numerous
computations and calculations in seconds, the process of
analyzing data has become easier. Moreover, learning from
obtained data is also essential as adapting with new inputs
is a highly important process that ensures the continuous
development of any smart system. For that reason, a lot of
attention has been given on the ﬁeld of machine learning over
the past years.
Machine learning is deﬁned as the ﬁeld of study that
provides computers with the ability to learn from input data
without being explicitly programmed to do so. The learning
process is done iteratively from analyzed data and new input
data. This iterative aspect allows computers to identify hidden
insights and repeated patterns and use these ﬁndings to adapt
when exposed to a new data . The different types of
data used in this learning process can vary from observations and examples to instructions and direct experience .
The gained knowledge will help in producing reliable and
repeated results. Thus, we can describe machine learning as
a method that learns from past experiences and uses gained
knowledge to do better in the future , . Figure 1 illustrates the main stages of machine learning.
Machine learning emphasizes on automatically learning
and adapting when exposed to data without the need of human
intervention. As mentioned earlier, the past affects the future
thus machine learning is viewed as programming by example.
In order to solve a certain task, rather than explicitly programming the computer to perform that task, we let it come
up with its own program based on provided examples from
which the computer learns . Using techniques based on
data mining and statistical analysis, machine learning allows
computers to emulate human learning behavior, reasoning
and decision making. Improving machine learning is highly
important since without it there will be no hope of one day
reaching artiﬁcial intelligence. The reason for that is based
on the fact that no system can be described as intelligent if it
does not have the ability to learn and adapt .
Machine learning has gained signiﬁcant attention specially in recent years and can be found in several applications such as spam ﬁlters, web search, credit scoring, fraud
detection, pricing prediction, ad placement, drug design,
healthcare, transportation and many other applications .
Five main techniques of machine learning exist which are:
supervised learning, unsupervised learning, semi-supervised
learning, reinforcement learning and ﬁnally deep learning.
Figure 2 shows the different machine learning types and
algorithms that will be mentioned in details later.
In general, supervised learning includes training datasets at
which data is presented in pairs, an input and its corresponding correct output. An example for that is the actual price of
electronic devices provided the features associated with each
device by which the price is directly affected. This process
helps to train the algorithm and thus develop a model capable
of predicting the price of new input devices that were not
included in the training dataset . On the other hand, unsupervised leaning attempts to ﬁnd common points between
the inputs in the dataset. This process can be described
as clustering of inputs that are greatly correlated under
one general label based on their statistical properties .
Semi-supervised learning is a combination of the two previously described types as the algorithm is trained using a
dataset that contains both labeled and unlabeled input points.
This type is used mainly to enhance the performance of the
algorithm through the use of both types of inputs . Reinforcement learning depends on the trial and error process to
uncover the set of actions that maximizes a cumulative reward
metric, which is used to make the algorithm understand
whether it’s going in the right direction or not . Lastly
is deep learning, this type attempts to model the abstractions
found in the dataset using a graph with multiple processing
layers. These layers aim to mimic the neural network found
within our brains . A detailed description of all ﬁve types
of machine learning is provided in the subsections below.
1) SUPERVISED LEARNING
This type of machine learning is based on using labeled
data to train the learning algorithm. The data is described
as labeled since it consists of pairs, an input that can be
represented by a vector and its corresponding desired output
which can be described as a supervisory signal, , .
The learning mechanism is described as supervised since the
correct output is known and the learning algorithm attempts
to iteratively predict this output and is corrected to reduce the
variation gap between its predicted and the actual output .
Analyzing the training data allows the supervised learning
algorithm to produce a function that is called a classiﬁer
function if the output was discrete, and a regression function
if the output was continuous . The learning algorithm
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
FIGURE 2. Different machine learning types and algorithms.
FIGURE 3. Different stages of the supervised learning.
generalizes detected patterns and features from the training
data to a new input data in a reasonable way and thus the
produced function predicts the output corresponding to any
provided input. Figure 3 illustrates the different stages of the
supervised machine learning method.
Regression algorithms (continuous output) and classiﬁcation algorithms (discrete output) are considered as the main
categories of supervised learning. Regression algorithms
attempt to uncover the best function that ﬁts points in the
training dataset. Regression algorithms include the following
main types: linear regression, multiple linear regression and
polynomial regression . Classiﬁcation algorithms, on the
other hand, aim to uncover the best ﬁt class for the input data
through assigning each input to its correct class. In this case,
the output of the predictive function is in the discrete form
and its value is one of the different classes available .
2) UNSUPERVISED LEARNING
Unlike supervised learning, this method uses an input dataset
without any labeled outputs to train the learning algorithm.
There is no right or wrong output to each input object and
no human intervention to correct or adjust as in supervised
learning. Thus, unsupervised learning is more subjective than
supervised , . The main goal of unsupervised learning is to learn more about the data through identifying the
fundamental structure or distribution patterns that is found in
the data itself. Learning by itself, the algorithm attempts to
represent a particular identiﬁed input pattern while reﬂecting
it on the overall structure of input patterns. Thus, the different
inputs are clustered into groups based on the features that
were extracted from each input object . Figure 4 represents the different stages of unsupervised machine learning
Even though the algorithm will not assign names to the
resulted clusters, it can still produce and differentiate among
them and use some of them to assign new examples into
other clusters. This approach is driven by input data and
can work well when there is adequate data available for
use. An example of that is social information ﬁltering algorithms, similar to those used by Amazon.com to recommend
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
FIGURE 4. The different stages of the unsupervised learning.
books to users. These algorithms are based on ﬁnding similar
groups of people, then adding new members to these groups
 , . Algorithms included in unsupervised learning can
be divided into three main categories, which are: clustering,
dimensionality reduction and anomaly detection .
3) SEMI-SUPERVISED LEARNING
This method falls between the supervised and unsupervised
learning methods where we have a large amount of input
data, some of which are labeled and the rest are not. Many
real life learning problems fall under this area of machine
learning. The reason for that is that semi-supervised requires
less human intervention since it utilizes very small amount of
labeled data and a large amount of unlabeled data. Utilizing
less labeled datasets is more appealing since such datasets
are very hard to collect as well as expensive and may require
access to domain experts. Unlabeled datasets on the other
hand are cheaper and easier to get access to .
Both supervised and unsupervised learning techniques can
be utilized to train the learning algorithm in semi-supervised
learning. Unsupervised learning techniques can be used to
unfold hidden structures and patterns in the input dataset.
Whereas supervised learning techniques can be utilized to
make guess predictions on the unlabeled data, feed the data
back to the learning algorithm as training data and use gained
knowledge to make predictions on new sets of data. Thus,
we can say that unlabeled data is used to modify or reprioritize prediction or hypothesis obtained from labeled data .
Figure 5 illustrates the different stages of a semi-supervised
machine learning method.
In order to make use of the unlabeled training data, all
semi-supervised learning algorithms do at least one of the
following assumptions : smoothness assumption, cluster
assumption and manifold assumption.
4) REINFORCEMENT
Reinforcement learning is learning by interacting with the
problem environment. A reinforcement learning agent learns
from its own actions rather than being speciﬁcally taught
what to do. It selects current actions based on past experiences
(exploitation) and new choices (exploration). Thus, it can be
described as a trial and error learning process. The success
of an action is determined through a signal received by the
reinforcement learning agent in the form of a numerical
reward value. The agent aims to learn to select actions that
maximize the value of the numerical reward . Actions
may affect not only the current situation and current reward
value, but also affect successive situations and reward values.
Learning agents usually have goals set and it can sense,
to some extent, the state of the environment it is in and thus
take actions that affect the state and bring it closer to the set
goals. Reinforcement learning is different from supervised
learning based on the way each method gains knowledge.
Supervised learning method learns from examples provided
by an external supervisor. Whereas reinforcement learning
uses direct interactions with the problem environment to gain
knowledge , .
5) DEEP LEARNING
Since 2006, this class of machine learning has emerged
strongly and has been incorporated in hundreds of researches
ever since. Areas in which deep learning have been incorporated ranged from information processing to artiﬁcial intelligence. Deep Learning can be described as a sub-ﬁeld of
machine learning that is based on algorithms that learn from
multiple of levels in order to provide a model that represents
complex relations among data. A hierarchy of features is
present such that high level features are deﬁned in terms of
lower level features and this is why it is referred to as deep
architecture. Most of the models incorporated under this class
are based on the unsupervised learning representations .
Deep learning is basically the intersection point between
neural networks, graphical modeling, optimization, artiﬁcial
intelligence, pattern recognition as well as signal processing.
The rationale behind the popularity of deep learning can be
summarized in the following: it helped in highly increasing
the processing abilities of computer chips, it allowed incorporation of a huge size of training data and it was the reason
for the recent advances in machine learning in the area of
information and signal processing .
a: OVERVIEW ON DEEP LEARNING
Until recently, most of the signal processing techniques
were based on the utilization of shallow structured architectures. These architectures typically contained one or two
layers at most of non-linear feature transformation. Examples of these shallow architectures include: Gaussian mixture
models (GMMs), the support vector machines (SVMs) and
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
FIGURE 5. Different stages of the semi-supervised learning.
linear or nonlinear dynamical systems . These architectures are best suited for simple or constrained problems
as their limited abilities can cause problems in large scale
complicated real world problems. Such real world problems
may include human speech, language recognition and visual
scenes which require a more deep and layered architecture to
be able to extract such complex information .
The concept of deep learning ﬁrst originated from arti-
ﬁcial network research. A good example of models with
a deep architecture is deep neural networks, which are
often described as feed-forward neural networks. Backpropagation (BP) was one of the most popular algorithms used for learning the parameters of these networks.
However, alone BP did not work well for learning networks
that contain more than a small number of hidden layers .
The persistent occurrence of local optima in the non-convex
objective function of the deep networks are the main source
of difﬁculties in the learning. The difﬁculty in optimization
with the deep models was empirically alleviated when an
unsupervised learning algorithm was introduced , .
Deep belief networks (DBN), which is a class of deep generative models, were introduced. DBN consists of a stack
of restricted Boltzmann machines (RBMs). At the core of
the DBN is a greedy learning algorithm that optimizes DBN
weights at time complexity linear to the size and depth of the
networks .
Incorporating hidden layers with a huge number of neurons
in a DNN has shown to highly improve the modeling abilities
of the DNN and therefore create many closely optimal con-
ﬁgurations . Even in the case where parameter learning
was trapped into a local optimum, the resulting DNN is
still capable of performing quite well since the chance of
having a poor local optimum gets lower and lower as the
number of neurons used is high. However, using deep neural
networks would require high computational power during the
training process. Since huge computational capabilities were
not easily available in the past, it was not until recent years
that researchers have started seriously exploring deep neural
b: CLASSES OF DEEP LEARNING
The term ‘‘deep learning’’ refers to a wide range of machine
learning techniques as well as architectures which are based
on the use of many layers of non-linear information processing that are considered hierarchical in nature. Depending on the intention behind using deep learning, whether it
synthesis or recognition, generation or classiﬁcation, a broad
classiﬁcation can be used that deﬁnes three different classes
of deep learning, which are: deep networks intended for
unsupervised (generative) learning, deep networks for supervised learning and hybrid deep networks. As for the ﬁrst
class, it aims to capture high order correlation of the visible data for the purpose of synthesis or pattern analysis
given that no information is available about the target class
labels. The second class aims to directly provide the discriminative power for the purpose of pattern classiﬁcation.
Labeled data are always present in the direct or indirect
form for supervised learning. Finally, the third class aims
to perform discriminations which are often assisted with the
outcomes of generative or unsupervised deep networks. This
is done usually by better optimization of the deep networks
in class 2 . There are many different deep learning algorithms, two of these popular algorithms are brieﬂy discussed
c: CONVOLUTIONAL NEURAL NETWORKS (CNN)
These networks are considered a type of discriminative deep
architecture in which every model contains a convolutional
layer and a pooling layer and are stacked on top of each
other . Many weights are shared in the convolutional
layer, the pooling layer on the other hand sub-samples the
output coming from the convolutional layer and decreases
the data rate of the below layer. The weight sharing together
with properly chosen pooling schemes, results in invariance
properties of the CNN. Some have argued that the limited
invariance found in CNN is not satisfactory for complicated
pattern recognition tasks. However, the CNNs have shown
effectiveness when used in computer vision or image recognition tasks , . Also, with some appropriate changes
in the CNN for image analysis purposes such that it incorporates speech properties, the CNN can be utilized in speech
recognition as well . The drive for using the convolution
operator in such applications, which is considered a specialized linear operator, is the fact that it uses three important
concepts, which are: sparse interactions, parameter sharing,
and equivariant representation.
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
d: RECURRENT NEURAL NETWORKS
Recurrent Neural Networks (RNNs) are considered as a class
of deep networks for the use in unsupervised learning in the
cases where the depth of the input data sequence can be
as large as the length since RNNs allow parameter sharing
through the different layers of the network , . RNNs
are developed by the use of the same set of weights in a
recursive manner over a tree like structure, and the tree is
traversed in topological order , . The RNN is used
mainly for the purpose of predicting the future data sequence
through the use of previous data samples. The RNN is very
prevailing when it comes to modeling sequence data such as
speech or text. However, until recently, these networks were
not widely used since they are considered hard to train such
that it captures the long term dependencies . In recent
years, advances in Hessian free optimization have helped
in overcoming this obstacle through the use of approximated second order information or stochastic curvature estimates. In some of the recent published work , RNNs
trained with Hessian free optimization are demonstrated to
be well capable of generating sequential text characters.
IV. METHODOLOGY
The survey conducted in this paper is based on the Systematic Literature Review (SLR) presented by Kitchenham and
Charters methodology . Their methodology divides work
into several phases where each phase includes several stages,
which are: the planning phase, conducting phase, and ﬁnally
reporting phase. We divided the ﬁrst phase into six different
stages. The ﬁrst stage was identifying the research questions which were based on the objectives set for the review.
The second stage was specifying the research strategy used to
retrieve related research papers, at this stage we also speciﬁed
the proper search terms as well as the proper paper selection criteria. The third stage was specifying a proper study
selection measures which includes the inclusion/exclusion
rules. The fourth stage was designing quality assessment rules
which were used to ﬁlter the research papers. Stage ﬁve
was designing the data extraction approach which was used
to answer the research questions raised. The last stage was
synthesizing the extracted data from the research papers. The
following subsections demonstrate the review protocol that
was followed in this paper.
A. RESEARCH QUESTIONS
The main goal of this review paper is to identify and examine
articles that implement deep neural networks in the area of
speech. Based on that, the following research questions were
identiﬁed:
• RQ1: What are the different types of papers that were
included in the study?
• RQ2: What are the different types of speech identiﬁed
in the research papers?
• RQ3: What are the different types of database used to
test and train the algorithm in each paper?
• RQ4: What are the different database languages identi-
ﬁed in the research papers?
• RQ5: What type of environment was used to conduct the
• RQ6: How features were extracted from speech?
• RQ7: What type of evaluation technique was used in the
research papers?
• RQ8: What types of deep neural network models have
been used?
B. SEARCH STRATEGY
Below is a detailed explanation on the search strategy that
was implemented in this review:
1) SEARCH TERMS
The search terms were identiﬁed based on the following :
1) The research questions were used to identify the main
search terms.
2) New terms were identiﬁed based on published papers
and books.
3) Boolean operators were used (ANDs and ORs) in order
to limit the search results.
The search terms that were used include the following:
• ‘‘deep neural network’’ AND ‘‘speech’’
• ‘‘deep neural networks’’ AND ‘‘speech’’
• DNN AND speech
• ‘‘deep neural network’’ OR ‘‘deep neural networks’’ OR
DNN AND speech
• ‘‘deep learning’’ AND Speech
2) SURVEY RESOURCES
The following digital libraries were used to search for the
needed research papers:
• Google Scholar
• IEEE Explorer
• Science Direct
• ResearchGate
• Springer
3) SEARCH PHASES
The search terms listed earlier were used to retrieve the
research papers from the speciﬁed digital libraries. The inclusion/exclusion criteria used is explained in details in the coming section. Based on our used inclusion/exclusion criteria,
174 publications were used in this review.
C. STUDY SELECTION
Originally, we obtained 230 papers which were based on the
search conducted using the listed search terms. Further ﬁltration was performed by the authors to ensure only relevant
papers were included in this review and the results found were
discussed in scheduled regular meetings. The selection and
ﬁltration steps that were used are listed below:
1) Step 1: removing all duplicate research papers that were
obtained from different digital libraries.
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
inclusion/exclusion
ensure only relevant papers are included in this
3) Step 3: removing review papers from the list of papers.
It is important to note that these identiﬁed review
papers were used to conduct a comparison with our
4) Step 4: applying the quality assessment in order to
include papers with the highest quality which give the
best answers to the raised research questions.
The used inclusion/exclusion criteria in this review paper is
deﬁned below: Inclusion criteria:
• Include papers that use deep neural networks in the area
of speech.
• Include papers that use deep learning in the area of
Exclusion criteria:
• Exclude papers that use deep neural networks in an area
other than speech.
• Exclude papers that are related to speech but do not use
deep neural networks.
• Exclude papers with no clear publication information.
D. QUALITY ASSESSMENT RULES
Applying quality assessment rules was the ﬁnal step used to
identify the ﬁnal list of papers included in this review paper.
QARs were applied to evaluate the quality of the research
papers in accordance with the set research questions. Ten
QARs were identiﬁed, each worth 1 mark out of 10. The score
of each QAR was selected as follows: when fully answered
score = 1, answer above average score = 0.75, average answer
score = 0.5, below average answer score = 0.25, completely
not answered score = 0. The summation of all ten QARs
represents the overall score of each paper. A score of 6 or less
means the paper was excluded from this review. The QARs
that were used to evaluate the quality of the papers are the
following:
1) QAR 1: Is the paper well organized?
2) QAR 2: Are the research objectives identiﬁed clearly
in the paper?
3) QAR 3: Is there sufﬁcient background information provided in the paper?
4) 4. QAR 4: Is the speciﬁc area of speech used clearly
5) QAR 5: Does the paper include practical experimentations?
6) QAR 6: Is the conducted experiment suitable and
acceptable?
7) QAR 7: Is the data set used clearly identiﬁed?
8) QAR 8: Are the results of the conducted experiments
clearly identiﬁed and reported?
9) QAR 9: Are the methods used to analyze the results
appropriate?
10) QAR 10: Overall, is the paper considered useful?
FIGURE 6. Percentage of papers in each type.
TABLE 1. Distribution of papers over the identified 13 conferences.
TABLE 2. Distribution of journal papers.
E. DATA EXTRACTION STRATEGY
In this stage, the ﬁnalized list of papers was used to extract
needed information to answer the set of research questions.
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
TABLE 3. Different areas of speech the papers fall under.
The information extracted from each paper included the following: paper ID, paper title, publication year, publication
type, domain, RQ1, RQ2, RQ3, RQ4, RQ5, RQ6 and RQ7.
Some difﬁculties occurred during the extraction process. For
instance, in some papers different error percentages were
presented to showcase the advantage of their technique in
comparison to others without actually explaining how this
error percentage was calculated. It is also important to note
that not all papers answered all research questions.
F. SYNTHESIS OF EXTRACTED DATA
information
RQ1 – RQ5 and RQ7 were tabulated and presented as quantitative data that was used to develop a statistical comparison
between the different ﬁndings for each research question.
These developed statistics helped uncover certain research
patterns as well as research directions that were carried over
the past decade. As for RQ6, since extracted data was qualitative, a descriptive comparison was carried which focused
on the main similarities and differences that were spotted
between the included research papers.
V. RESULTS
A. RESEARCH QUESTION 1
The 174 papers that were included in the study fall into four
main different types, which are: conference papers, journal
papers, workshop papers and ﬁnally research institute publications. Figure 6 provides the distribution of papers between
these main types.
The majority of papers used in the study, at a 60%, were
identiﬁed as conference papers, as it can be clearly seen
above. The rest 40% were distributed between the journal
papers, workshop papers and research institute papers at a
16%, 11% and 13% respectively. In order to provide even
further details about the papers that were used, a detailed
statistical data was derived on the different conferences and
journals that these papers were published in. Table 1 displays the distribution of papers in the 13 different identiﬁed
conferences.
As it can be seen, the majority of conference papers,
at about a 54%, were published in ICASSP ‘‘IEEE
International Conference on Acoustics, Speech and Signal
Processing’’. This was followed by 31% published in Interspeech. The remaining 15% was divided between the rest
of the conferences at a 3% for ChinaSIP ‘‘ IEEE China
Summit and International Conference on Signal and Information Processing’’, 2% for ICML ‘‘International Conference on Machine Learning’’ and 2% for ICSP, then ﬁnally
1% for each of the remaining 8 conferences. Similarly,
Table 2 presents the distribution of journal papers.
As it can be seen, the majority of journal papers, at a 46%,
were published in the IEEE Transaction on Audio, Speech
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
and Language Processing. This was followed by 21% published in EURASIP ‘‘The European Association for Signal
Processing’’, 11% in IEEE Signal Processing Letters, 7% in
Speech Communications, and ﬁnally 4% in all of the following: Scholarpedia, IEEE Journal of Selected Topics in Signal
Processing, Neural Networks and the IEEE Transactions of
Affective Computing.
B. RESEARCH QUESTION 2
Among the 174 papers, different areas of speech were identiﬁed which include: speaker identiﬁcation, speech emotion
recognition, speech enhancement, speech recognition, speech
transcription among others. The percentage of papers in each
of these speech areas is shown in Table 3.
The majority of papers fall under the speech recognition
area at a 79%, followed by about 9% in the speech enhancement area and 3% in the speaker identiﬁcation, speech emotion recognition and the speech transcription area. Also, 3%
of the papers were categorized as other, this category includes
the areas of speech that had less than 1% of papers and
include: speaker veriﬁcation, language identiﬁcation, speech
pattern classiﬁcation and spoken language understanding.
Since a huge percentage of papers fall under the area of
speech recognition, further analysis was carried on these
papers in order to identify even more details on the different
speech recognition areas each paper is focused on. Different
subareas were identiﬁed including: automatic speech recognition, large vocabulary speech recognition, low resource
speech recognition, multilingual speech recognition, noise
robust speech recognition, phone recognition, sequence classiﬁcation, speaker adaptation and speech separation among
others. Table 4 provides detailed information about these
subcategories.
In speech recognition, the area of large vocabulary speech
recognition had the most number of published papers at a
20%, which was followed by Noise robust speech recognition
at about 14%. Further, 13% of the papers only mentioned
speech recognition without any further details, 10% fall under
automatic speech recognition, 6% fall under speaker adaptation, 4% under phone recognition and speech separation,
3% under sequence classiﬁcation, multilingual speech and
low resource speech recognition. Also, 20% of the speech
recognition papers were classiﬁed as other since they include
sub areas that have a less than 1% publication percentage.
C. RESEARCH QUESTION 3
To train and test algorithms, several databases were used in
the research papers. Some were private while the majority
of the databases, at 83%, were public and available on the
web. More details are shown in Table 5. Some of the public databases that were used include: TIMIT dataset, ATIS
dataset, Switchboard Hub5 task, Aurora 4, Babel corpus,
AMI corpus1 among others.
D. RESEARCH QUESTION 4
Table 6 displays the different languages that were identiﬁed in the research papers that used to train and test
TABLE 4. The specific speech recognition areas identified in the papers.
TABLE 5. Type of database used in the research papers.
the algorithms. As it can be clearly seen, the majority of the
papers at 85% used an English language based dataset. Only
9% of the papers used multiple languages based datasets,
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
TABLE 6. Different languages used in the research papers.
TABLE 7. Type of environment used in the research papers.
TABLE 8. Identified evaluation techniques in the research papers.
2% of the papers did not mention the used language and the
5% remaining papers used Chinese, Italian, French, Japanese
and South African languages at 1% each.
E. RESEARCH QUESTION 5
The environment used to train and test used algorithms varied
between noisy, neutral and emotional. 71% of the papers
either mentioned using a neutral environment or not mention
anything at all and thus assumed neutral. As for noisy environment, 27% of the papers mentioned building a noisy robust
system. Only 2% of the papers mentioned using emotional
speech to test and train the algorithms. Table 7 provides more
details on this.
F. RESEARCH QUESTION 6
TFeatures were extracted from speech using different techniques. It was found that the most popular is the melfrequency cepstrum coefﬁcients (MFCCs), as 69.5% of the
papers (121 papers) used MFCCs to extract features from
speech. On the other hand, almost 10% of the papers used
the linear discriminate analysis (LDA) transform and almost
5% used the HLDA transform. In addition, it was found that
almost 2% of the papers used the short time Fourier transform
(STFT). Finally, the rest of the papers used other techniques
TABLE 9. Standalone deep neural networks.
TABLE 10. Hybrid deep neural networks.
that include: MLLT, perceptual linear predictive (PLP), log
power spectral (LPS), Bark-frequency cepstrum coefﬁcients
(BFCC), batch normalization, maximum likelihood linear
transform (MLLT) and residual connections.
G. RESEARCH QUESTION 7
Several evaluation techniques were used in the research
papers to evaluate the overall performance of the developed
system. Table 8 illustrates the different identiﬁed techniques,
as well as the percentage of papers that used each technique.
As it can be seen, 56% of the papers used Word Error
Rate (WER) to evaluate the performance of their system,
whereas 11% used for each Phone Error Rate (PER), error
rate calculations and accuracy calculations. On the other
hand 2% of the papers used dB SIR Gain calculations, 1%
used Label Error Rate (LER) and Phoneme Classiﬁcation
Performance. On the other hand, 15% of the papers were classiﬁed as ‘‘other’’ since used techniques by each of the papers
scored less than 1%. Some of the techniques that fall under
this category include: Root Mean Square Error (RMSE),
Sentence Accuracy, Query Error Rate (QER), unweighted
classiﬁcation accuracy, Gain in dB among others.
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
TABLE 11. List of extracted papers.
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
TABLE 11. (Continued.)
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
TABLE 11. (Continued.)
H. RESEARCH QUESTION 8
Regarding the types of deep neural network used in speech
recognition, out of 174 papers, 132 papers used DNN models
as standalone models and 42 papers used hybrid models
(two or more models). Tables 9 and 10 display the standalone
and hybrid models.
VI. CONCLUSIONS
This paper provided a thorough statistical analysis on the use
of deep learning in speech related applications by extracting
speciﬁc information from 174 papers published between the
years 2006 and 2018. The majority of the papers identiﬁed
(40%) were conference papers and more than 50% of these
papers were published in ICASSP. As for the journal papers,
it was found that 46% of them were published in the IEEE
Transactions on Audio, Speech, and Language Processing.
The majority of the papers focused on speech recognition as
the application in use. As for the utilized data bases in the
included study, they were mostly public and in English and
the environment was mostly natural non noisy. As for the
VOLUME 7, 2019
A. B. Nassif et al.: Speech Recognition Using Deep Neural Networks: A Systematic Review
evaluation technique used, it was found that the majority of
the papers used the WER (word error rate) to determine the
efﬁciency of their systems. We hope that the results provided
in this study would help future researchers identify new and
interesting research topics that has not been examined yet,
as well as highlight some of the gaps in the existing studies.
It is surprising to see that most of the researchers still
use MFCCs as feature extraction for speech signals in deep
learning models. MFCCs were heavily used in classical classiﬁers such as HMM and GMM. It is worth trying when using
deep learning models other feature extraction methods such
as Linear Predictive Coding (LPC).
As seen in Tables 9 and 10, 75% of DNN models were
standalone models where only 25% of the models used hybrid
models. Authors are encouraged to use hybrid models as
research showed that using HMM or GMM inform of a DNN
model gives better results .
Another observation is that there is little work on
speech recognition using Recurrent Neural Networks (RNN).
Authors are highly recommended to conduct research using
deep RNN in the future since RNN models, especially Long
Short Time Memory (LSTM), are very powerful in speech
recognition .
See Table 11.