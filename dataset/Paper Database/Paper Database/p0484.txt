What makes ImageNet good for transfer learning?
Minyoung Huh
Pulkit Agrawal
Alexei A. Efros
Berkeley Artiﬁcial Intelligence Research (BAIR) Laboratory
UC Berkeley
{minyoung,pulkitag,aaefros}@berkeley.edu
The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks raises the question:
what is it about the ImageNet dataset that makes the learnt
features as good as they are? This work provides an empirical investigation into the various facets of this question,
such as, looking at the importance of the amount of examples, number of classes, balance between images-per-class
and classes, and the role of ﬁne and coarse grained recognition. We pre-train CNN features on various subsets of the
ImageNet dataset and evaluate transfer performance on a
variety of standard vision tasks. Our overall ﬁndings suggest that most changes in the choice of pre-training data
long thought to be critical, do not signiﬁcantly affect transfer performance.
1. Introduction
It has become increasingly common within the computer vision community to treat image classiﬁcation on ImageNet not as an end in itself, but rather as a “pretext task” for training deep convolutional neural networks
(CNNs ) to learn good general-purpose features.
This practice of ﬁrst training a CNN to perform image classiﬁcation on ImageNet (i.e. pre-training) and then adapting
these features for a new target task (i.e. ﬁne-tuning) has become the de facto standard for solving a wide range of computer vision problems. Using ImageNet pre-trained CNN
features, impressive results have been obtained on several
image classiﬁcation datasets , as well as object detection , action recognition , human pose estimation , image segmentation , optical ﬂow , image captioning and others .
Given the success of ImageNet pre-trained CNN features, it is only natural to ask: what is it about the ImageNet
dataset that makes the learnt features as good as they are?
One school of thought believes that it is the sheer size of
the dataset (1.2 million labeled images) that forces the representation to be general. Others argue that it is the large
number of distinct object classes (1000), which forces the
network to learn a hierarchy of generalizable features. Yet
others believe that the secret sauce is not just the large number of classes, but the fact that many of these classes are
visually similar (e.g. many different breeds of dogs), turning this into a ﬁne-grained recognition task and pushing the
representation to “work harder”. But, while almost everyone in computer vision seems to have their own opinion on
this hot topic, little empirical evidence has been produced
In this work, we systematically investigate which aspects of the ImageNet task are most critical for learning
good general-purpose features. We evaluate the features by
ﬁne-tuning on three tasks: object detection on PASCAL-
VOC 2007 dataset (PASCAL-DET), action classiﬁcation
on PASCAL-VOC 2012 dataset (PASCAL-ACT-CLS) and
scene classiﬁcation on the SUN dataset (SUN-CLS); see
Section 3 for more details.
The paper is organized as a set of experiments answering
a list of key questions about feature learning with ImageNet.
The following is a summary of our main ﬁndings:
1. How many pre-training ImageNet examples are sufﬁcient
for transfer learning? Pre-training with only half the ImageNet data (500 images per class instead of 1000) results
in only a small drop in transfer learning performance (1.5
mAP drop on PASCAL-DET). This drop is much smaller
than the drop on the ImageNet classiﬁcation task itself. See
Section 4 and Figure 1 for details.
2. How many pre-training ImageNet classes are sufﬁcient
for transfer learning? Pre-training with an order of magnitude fewer classes (127 classes instead of 1000) results
in only a small drop in transfer learning performance (2.8
mAP drop on PASCAL-DET). Curiously, we also found
that for some transfer tasks, pre-training with fewer classes
leads to better performance. See Section 5.1 and Figure 2
for details.
 
Figure 1: Change in transfer task performance of a CNN pre-trained
with varying number of images per ImageNet class. The left y-axis
is the mean class accuracy used for SUN and ImageNet CLS. The
right y-axis measures mAP for PASCAL DET and ACTION-CLS.
The number of examples per class are reduced by random sampling. Accuracy on the ImageNet classiﬁcation task increases faster
as compared to performance on transfer tasks.
Figure 2: Change in transfer task performance with varying number
of pre-training ImageNet classes. The number of ImageNet classes
are varied using the technique described in Section 5.1. With only
486 pre-training classes, transfer performances are unaffected and
only a small drop is observed when only 79 classes are used for pretraining. The ImageNet classiﬁcation performance is measured by
ﬁntetuning the last layer to the original 1000-way classiﬁcation.
3. How important is ﬁne-grained recognition for learning
good features for transfer learning? Features pre-trained
with a subset of ImageNet classes that do not require ﬁnegrained discrimination still demonstrate good transfer performance. See Section 5.2 and Figure 2 for details.
4. Does pre-training on coarse classes produce features capable of ﬁne-grained recognition (and vice versa) on ImageNet itself? We found that a CNN trained to classify only
between the 127 coarse ImageNet classes produces features capable of telling apart ﬁne-grained ImageNet classes
whose labels it has never seen in training (section 5.3).
Likewise, a CNN trained to classify the 1000 ImageNet
classes is able to distinguish between unseen coarse-level
classes higher up in the WordNet hierarchy (section 5.4).
5. Given the same budget of pre-training images, should we
have more classes or more images per class? Training with
fewer classes but more images per class performs slightly
better at transfer tasks than training with more classes but
fewer images per class. See Section 5.5 and Table 2 for
6. Is more data always helpful? We found that training with
771 ImageNet classes (out of 1000) that exclude all PAS-
CAL VOC classes, achieves nearly the same performance
on PASCAL-DET as training on complete ImageNet. Further experiments conﬁrm that blindly adding more training
data does not always lead to better performance and can
sometimes hurt performance. See Section 6, and Table 9
for more details.
2. Related Work
A number of papers have studied transfer learning in
CNNs, including the various factors that affect pre-training
and ﬁne-tuning. For example, the question of whether pretraining should be terminated early to prevent over-ﬁtting
and what layers should be used for transfer learning was
studied by . A thorough investigation of good architectural choices for transfer learning was conducted by ,
while propose an approach to ﬁne-tuning for new tasks
without ”forgetting” the old ones.
In contrast to these
works, we use a ﬁxed ﬁne-tuning pr
One central downside of supervised pre-training is that
large quantity of expensive manually-supervised training
data is required.
The possibility of using large amounts
of unlabelled data for feature learning has therefore been
very attractive. Numerous methods for learning features
by optimizing some auxiliary criterion of the data itself
have been proposed. The most well-known such criteria
are image reconstruction (see for
a comprehensive overview) and feature slowness .
Unfortunately, features learned using these methods turned
out not to be competitive with those obtained from supervised ImageNet pre-training . To try and force better
feature generalization, more recent “self-supervised” methods use more difﬁcult data prediction auxiliary tasks in an
effort to make the CNNs “work harder”. Attempted selfsupervised tasks include predictions of ego-motion ,
spatial context , temporal context , and even
color and sound . While features learned using
these methods often come close to ImageNet performance,
to date, none have been able to beat it.
Figure 3: An illustration of the bottom up procedure used to construct different label sets using the WordNet tree. Each node of the
tree represents a class and the leaf nodes are shown in red. Different label sets are iteratively constructed by clustering together all
the leaf nodes with a common parent. In each iteration, only leaf
nodes are clustered. This procedure results into a sequence of label
sets for 1.2M images, where each consequent set contains labels
coarser than the previous one. Because the WordNet tree is imbalanced, even after multiple iterations, label sets contain some
classes that are present in the 1000 way ImageNet challenge.
A reasonable middle ground between the expensive,
fully-supervised pre-training and free unsupervised pretraining is to use weak supervision. For example, use
the YFCC100M dataset of 100 million Flickr images labeled with noisy user tags as pre-training instead of ImageNet. But yet again, even though YFCC100M is almost
two orders of magnitude larger than ImageNet, somewhat
surprisingly, the resulting features do not appear to give any
substantial boost over these pre-trained on ImageNet.
Overall, despite keen interest in this problem, alternative methods for learning general-purpose deep features
have not managed to outperform ImageNet-supervised pretraining on transfer tasks.
The goal of this work is to try and understand what is the
secret to ImageNet’s continuing success.
3. Experimental Setup
The process of using supervised learning to initialize
CNN parameters using the task of ImageNet classiﬁcation
is referred to as pre-training. The process of adapting pretrained CNN to continuously train on a target dataset is
referred to as ﬁnetuning. All of our experiments use the
Caffe implementation of the a single network architecture proposed by Krizhevsky et al. . We refer to this
architecture as AlexNet.
We closely follow the experimental setup of Agrawal
et al. for evaluating the generalization of pre-trained
features on three transfer tasks: PASCAL VOC 2007 object detection (PASCAL-DET), PASCAL VOC 2012 action
recognition (PASCAL-ACT-CLS) and scene classiﬁcation
on SUN dataset (SUN-CLS).
• For PASCAL-DET, we used the PASCAL VOC 2007
train/val for ﬁnetuning using the experimental setup and
Pre-trained Dataset
127 Classes
Table 1: The transfer performance of a network pre-trained using 127 (coarse) classes obtained after top-down clustering of the
WordNet tree is comparable to a transfer performance after ﬁnetuning on all 1000 ImageNet classes.
This indicates that ﬁnegrained recognition is not necessary for learning good transferable
code provided by Faster-RCNN and report performance on the test set. Finetuning on PASCAL-DET was
performed by adapting the pre-trained convolution layers
of AlexNet. The model was trained for 70K iterations
using stochastic gradient descent (SGD), with an initial
learning rate of 0.001 with a reduction by a factor of 10
at 40K iteration.
• For PASCAL-ACT-CLS, we used PASCAL VOC 2012
train/val for ﬁnetuning and testing using the experimental setup and code provided by R*CNN . The ﬁnetuning process for PASCAL-ACT-CLS mimics the procedure described for PASCAL-DET.
• For SUN-CLS we used the same train/val/test splits as
used by . Finetuning on SUN was performed by ﬁrst
replacing the FC-8 layer in the AlexNet model with a randomly initialized, and fully connected layer with 397 output units. Finetuning was performed for 50K iterations
using SGD with an initial learning rate of 0.001 which
was reduced by a factor of 10 every 20K iterations.
Faster-RCNN and R*CNN are known to have variance
across training runs; we therefore run it three times and report the mean ± standard deviation. On the other hand, ,
reports little variance between runs on SUN-CLS so we report our result using a single run.
In some experiments we pre-train on ImageNet using a
different number of images per class. The model with 1000
images/class uses the original ImageNet ILSVRC 2012
training set. Models with N images/class for N < 1000 are
trained by drawing a random sample of N images from all
images of that class made available as part of the ImageNet
training set.
4. How does the amount of pre-training data
affect transfer performance?
For answering this question, we trained 5 different
AlexNet models from scratch using 50, 125, 250, 500 and
1000 images per each of the 1000 ImageNet classes using
the procedure described in Section 3. The variation in performance with amount of pre-training data when these models are ﬁnetuned for PASCAL-DET, PASCAL-ACT-CLS
Figure 4: Does a CNN trained for discriminating between coarse classes learns a feature embedding capable of distinguishing between ﬁne
classes? We quantiﬁed this by measuring the induction accuracy deﬁned as following: after training a feature embedding for a particular
set of classes (set A), the induction accuracy is the nearest neighbor (top-1 and top-5) classiﬁcation accuracy measured in the FC8 feature
space of the subset of 1000 ImageNet classes not present in set A. The syntax on the x-axis A Classes(B) indicates that the network
was trained with A classes and the induction accuracy was measured on B classes. The baseline accuracy is the accuracy on B classes when
the CNN was trained for all 1000 classes. The margin between the baseline and the induction accuracy indicates a drop in the network’s
ability to distinguish ﬁne classes when being trained on coarse classes. The results show that features learnt by pre-training on just 127
classes still lead to fairly good induction.
and SUN-CLS is shown in Figure 1. For PASCAL-DET, the
mean average precision (mAP) for CNNs with 1000, 500
and 250 images/class is found to be 58.3, 57.0 and 54.6. A
similar trend is observed for PASCAL-ACT-CLS and SUN-
CLS. These results indicate that using half the amount of
pre-training data leads to only a marginal reduction in performance on transfer tasks. It is important to note that the
performance on the ImageNet classiﬁcation task (the pretraining task) steadily increases with the amount of training
data, whereas on transfer tasks, the performance increase
with respect to additional pre-training data is signiﬁcantly
slower. This suggests that while adding additional examples to ImageNet classes will improve the ImageNet performance, it has diminishing return for transfer task performance.
5. How does the taxonomy of the pre-training
task affect transfer performance?
In the previous section we investigated how varying
number of pre-training images per class effects the performance in transfer tasks. Here we investigate the ﬂip side:
keeping the amount of data constant while changing the
nomenclature of training labels.
5.1. The effect of number of pre-training classes on
transfer performance
The 1000 classes of the ImageNet challenge are derived from leaves of the WordNet tree . Using this tree,
it is possible to generate different class taxonomies while
keeping the total number of images constant. One can generate taxonomies in two ways: (1) bottom up clustering,
wherein the leaf nodes belonging to a common parent are
iteratively clustered together (see Figure 3), or (2) by ﬁxing the distance of the nodes from the root node (i.e. top
down clustering). Using bottom up clustering, 18 possible
taxonomies can be generated. Among these, we chose 5
sets of labels constituting 918, 753, 486, 79 and 9 classes
respectively. Using top-down clustering only 3 label sets
of 127, 10 and 2 can be generated, and we used the one
with 127 classes. For studying the effect of number of pretraining classes on transfer performance, we trained separate AlexNet CNNs from scratch using these label sets.
Figure 2 shows the effect of number of pre-training
classes obtained using bottom up clustering of WordNet tree
on transfer performance. We also include the performance
of these different networks on the Imagenet classiﬁcation
task itself after ﬁnetuning only the last layer to distinguish
between all the 1000 classes. The results show that increase
in performance on transfer tasks is signiﬁcantly slower with
increase in number of classes as compared to performance
on Imagenet itself. Using only 486 classes results in a performance drop of 1.7 mAP for PASCAL-DET, 0.8% accuracy for SUN-CLS and a boost of 0.6 mAP for PASCAL-
ACT-CLS. Table 1 shows the transfer performance after
pre-training with 127 classes obtained from top down clustering. The results from this table and the ﬁgure indicate
that only diminishing returns in transfer performance are
observed when more than 127 classes are used. Our results
also indicate that making the ImageNet classes ﬁner will not
help improve transfer performance.
It can be argued that the PASCAL task requires discrimination between only 20 classes and therefore pre-training
with only 127 classes should not lead to substantial reduction in performance. However, the trend also holds true for
SUN-CLS that requires discrimination between 397 classes.
These two results taken together suggest that although training with a large number of classes is beneﬁcial, diminishing
returns are observed beyond using 127 distinct classes for
Figure 5: Can feature embeddings obtained by training on coarse classes be able to distinguish ﬁne classes they were never trained on? E.g.
by training on monkeys, can the network pick out macaques? Here we look at the FC7 nearest neighbors (NN) of two randomly sampled
images: a macaque (left column) and a giant schnauzer (right column), with each row showing feature embeddings trained with different
number of classes (from ﬁne to coarse). The row(s) above the dotted line indicate that the image class (i.e. macaque/giant schnauzer) was
one of the training classes, whereas in rows below the image class was not present in the training set. Images in green indicate that the
NN image belongs to the correct ﬁne class (i.e. either macaque or giant schnauzer); orange indicates the correct coarse class (based on the
WordNet hierarchy) but incorrect ﬁne class; red indicated incorrect coarse class. All green images below the dotted line indicate instances
of correct ﬁne-grain nearest neighbor retrieval for features that were never trained on that class.
pre-training.
Furthermore, for PASCAL-ACT-CLS and SUN-CLS,
ﬁnetuning on CNNs pre-trained with class set sizes of 918,
and 753 actually results in better performance than using
all 1000 classes. This may indicate that having too many
classes for pre-training works against learning good generalizable features. Hence, when generating a dataset, one
should be attentive of the nomenclature of the classes.
5.2. Is ﬁne-grain recognition necessary for learning
transferable features?
ImageNet challenge requires a classiﬁer to distinguish
between 1000 classes, some of which are very ﬁne-grained,
such as different breeds of dogs and cats. Indeed, most humans do not perform well on ImageNet unless speciﬁcally
trained , and yet are easily able to perform most everyday visual tasks. This raises the question: is ﬁne-grained
recognition necessary for CNN models to learn good feature representations, or is coarse-grained object recognition
(e.g. just distinguishing cats from dogs) is sufﬁcient?
Note that the label set of 127 classes from the previous
experiment contains 65 classes that are present in the original set of 1000 classes and the remainder are inner nodes of
the WordNet tree. However, all these 127 classes (see supplementary materials) represent coarse semantic concepts.
As discussed earlier, pre-training with these classes results
in only a small drop in transfer performance (see Table 1).
This suggests that performing ﬁne-grained recognition is
only marginally helpful and does not appear to be critical
for learning good transferable features.
5.3. Does training with coarse classes induce features relevant for ﬁne-grained recognition?
Earlier, we have shown that the features learned on the
127 coarse classes perform almost as well on our transfer
tasks as the full set of 1000 ImageNet classes. Here we
will probe this further by asking a different question: is the
feature embedding induced by the coarse class classiﬁcation task capable of separating the ﬁne labels of ImageNet
(which it never saw at training)?
To investigate this, we used top-1 and top-5 nearest
neighbors in the FC7 feature space to measure the accuracy of identifying ﬁne-grained ImageNet classes after
training only on a set of coarse classes. We call this measure, “induction accuracy”. As a qualitative example, Figure 5 shows nearest neighbors for a macaque (left) and a
schnauzer (right) for feature embeddings trained on ImageNet but with different number of classes.
All greenborder images below the dotted line indicate instances of
correct ﬁne-grain nearest neighbor retrieval for features that
were never trained on that class.
Quantitative results are shown in Figure 4. The results
show that when 127 classes are used, ﬁne-grained recognition k-NN performance is only about 15% lower compared
to training directly for these ﬁne-grained classes (i.e. baseline accuracy). This is rather surprising and suggests that
CNNs implicitly discover features capable of distinguishing between ﬁner classes while attempting to distinguish
between relatively coarse classes.
Figure 6: Does the network learn to discriminate coarse semantic concepts by training only on ﬁner sub-classes? The degree to
which the concept of coarse class is learnt was quantiﬁed by measuring the difference (in percentage points) between the accuracy
of classifying the coarse class and the average accuracy of individually classifying all the sub-classes of this coarse class. Here,
the top and bottom classes sorted by this metric are shown using
the label set of size 127 with classes with at least 5 subclasses.
We observe that classes whose subclasses are visually consistent
(e.g. mammal) are better represented than these that are visually
dissimilar (e.g. home appliance).
5.4. Does training with ﬁne-grained classes induce
features relevant for coarse recognition?
Investigating whether the network learns features relevant for ﬁne-grained recognition by training on coarse
classes raises the reverse question: does training with ﬁnegrained classes induce features relevant for coarse recognition? If this is indeed the case, then we would expect
that when a CNN makes an error, it is more likely to confuse a sub-class (i.e. error in ﬁne-grained recognition) with
other sub-classes of the same coarse class. This effect can
be measured by computing the difference between the accuracy of classifying the coarse class and the average accuracy
of individually classifying all the sub-classes of this coarse
class (please see supplementary materials for details).
Figure 6 shows the results. We ﬁnd that coarse semantic classes such as mammal, fruit, bird, etc. that contain
visually similar sub-classes show the hypothesized effect,
whereas classes such as tool and home appliance that contain visually dissimilar subclasses do not exhibit this effect.
These results indicate that subclasses that share a common
visual structure allow the CNN to learn features that are
more generalizable. This might suggest a way to improve
feature generalization by making class labels respect visual
commonality rather than simply WordNet semantics.
5.5. More Classes or More Examples Per Class?
Results in previous sections show that it is possible to
achieve good performance on transfer tasks using signiﬁcantly less pre-training data and fewer pre-training classes.
However it is unclear what is more important – the number
of classes or the number or examples per class. One ex-
More examples/class
More classes
Table 2: For a ﬁxed budget of pre-training data, is it better to have
more examples per class and fewer classes or vice-versa? The
row ‘more examples/class‘ was pretrained with subsets of ImageNet containing 500, 250 and 125 classes with 1000 examples
each. The row ‘more classes‘ was pretrained with 1000 classes,
but 500, 250 and 125 examples each. Interestingly, the transfer
performance on both PASCAL and SUN appears to be broadly
similar under both scenarios.
Pre-trained Dataset
58.3 ± 0.3
Pascal removed ImageNet
57.8 ± 0.1
53.8 ± 0.1
Table 3: PASCAL-DET results after pre-training on entire ImageNet, PASCAL-removed-ImageNet and Places data sets. Removing PASCAL classes from ImageNet leads to an insigniﬁcant
reduction in performance.
treme is to only have 1 class and all 1.2M images from this
class and the other extreme is to have 1.2M classes and 1
image per class. It is clear that both ways of splitting the
data will result in poor generalization, so the answer must
lie somewhere in-between.
To investigate this, we split the same amount of pretraining data in two ways: (1) more classes with fewer images per class, and (2) fewer classes with more images per
class. We use datasets of size 500K, 250K and 125K images for this experiment. For 500K images, we considered
two ways of constructing the training set – (1) 1000 classes
with 500 images/class, and (2) 500 classes with 1000 images/class. Similar splits were made for data budgets of
250K and 125K images. The 500, 250 and 125 classes for
these experiments were drawn from a uniform distribution
among the 1000 ImageNet classes. Similarly, the image
subsets containing 500, 250 and 125 images were drawn
from a uniform distribution among the images that belong
to the class.
The results presented in Table 2 show that having more
images per class with fewer number of classes results in
features that perform very slightly better on PASCAL-
DET, whereas for SUN-CLS, the performance is comparable across the two settings.
5.6. How important is to pre-train on classes that
are also present in a target task?
It is natural to expect that higher correlation between pretraining and transfer tasks leads to better performance on a
transfer task. This indeed has been shown to be true in .
One possible source of correlation between pre-training and
Figure 7: An illustration of the procedure used to split the ImageNet dataset. Splits were constructed in 2 different ways. The
random split selects classes at random from the 1000 ImageNet
classes. The minimal split is made in a manner that ensures no
two classes in the same split have a common ancestor up to depth
four of WordNet tree. Collage in Figure 8 visualizes the random
and minimal splits.
transfer tasks are classes common to both tasks. In order
to investigate how strong is the inﬂuence of these common
classes, we ran an experiment where we removed all the
classes from ImageNet that are contained in the PASCAL
challenge. PASCAL has 20 classes, some of which map
to more than one ImageNet class and thus, after applying
this exclusion criterion we are only left with 771 ImageNet
Table 3 compares the results on PASCAL-DET when
the PASCAL-removed-ImageNet is used for pre-training
against the original ImageNet and a baseline of pretraining on the Places dataset. The PASCAL-removed-
ImageNet achieves mAP of 57.8 (compared to 58.3 with the
full ImageNet) indicating that training on ImageNet classes
that are not present in PASCAL is sufﬁcient to learn features
that are also good for PASCAL classes.
6. Does data augmentation from non-target
classes always improve performance?
The analysis using PASCAL-removed ImageNet indicates that pre-training on non-PASCAL classes aids performance on PASCAL. This raises the question: is it always
better to add pre-training data from additional classes that
are not part of the target task? To investigate and test this
hypothesis, we chose two different methods of splitting the
ImageNet classes. The ﬁrst is random split, in which the
1000 ImageNet classes are split randomly; the second is a
minimal split, in which the classes are deliberately split to
ensure that similar classes are not in the same split, (Figure 7). In order to determine if additional data helps performance for classes in split A, we pre-trained two CNNs – one
for classifying all classes in split A and the other for classifying all classes in both split A and B (i.e. full dataset).
We then ﬁnetuned the last layer of the network trained on
the full dataset on split A only. If it is the case that addi-
Figure 8: Visualization of the random and minimal splits used for
testing - is adding more pre-training data always useful? The two
minimal sets contain disparate sets of objects. The minimal split
A and B consists mostly of inanimate objects and living things respectively. On the other hand, random splits contain semantically
similar objects.
tional data from split B helps performance on split A, then
the CNN pre-trained with the full dataset should perform
better than CNN pre-trained only on split A.
Using the random split, Figure 9 shows that the results
of this experiment conﬁrms the intuition that additional data
is indeed useful for both splits. However, under a random
class split within ImageNet, we are almost certain to have
extremely similar classes (e.g. two different breeds of dogs)
ending up on the different sides of the split. So, what we
have shown so far is that we can improve performance on,
say, husky classiﬁcation by also training on poodles. Hence,
the motivation for the minimal split: does adding arbitrary,
unrelated classes, such as ﬁre trucks, help dog classiﬁcation?
The classes in minimal split A do not share any common
ancestor with minimal split B up until the nodes at depth 4
of the WordNet hierarchy (Figure 7). This ensures that any
class in split A is sufﬁciently disjoint from split B. Split A
has 522 classes and split B has 478 classes (N.B.: for consistency, random splits A and B also had the same number
of classes). In order to intuitively understand the difference
between min splits A and B, we have visualized a random
sample of images in these splits in Figure 8. Min split A
consists of mostly static images and min split B consists of
living objects.
Contrary to the earlier observation, Figure 9 shows that
both min split A and B performs better than the full dataset
when we ﬁnetune only the last layer. This result is quite surprising because it shows that ﬁnetuning the last layer from
a network pre-trained on the full dataset, it is not possible
Figure 9: Does adding arbitrary classes to pre-training data always improve transfer performance? This question was tested by
training two CNNs, one for classifying classes in split A and other
for classifying classes in split A and B both. We then ﬁnetuned
the CNN trained on both the splits on split A. If it is the case
that adding more pre-training data helps, then performance of the
CNN pre-trained on both the splits (black) should be higher than
a CNN pre-trained on a single split (orange). For random splits,
this indeed is the case, whereas for minimal splits adding more
pre-training data hurts performance. This suggests, that additional
pre-training data is useful only if it is correlated to the target task.
to match the performance of a network trained on just one
split. We have observed that when training all the layers for
an extensive amount of time (420K iterations), the accuracy
of min split A does beneﬁt from pre-training on split B but
does not for min split B. One explanation could be that images in split B (e.g. person) is contained in images in split
A, (e.g. buildings, clothing) but not vice versa.
While it might be possible to recover performance with
very clever adjustments of learning rates, current results
suggest that training with data from unrelated classes may
push the network into a local minimum from which it might
be hard to ﬁnd a better optima that can be obtained by training the network from scratch.
7. Discussion
In this work we analyzed factors that affect the quality
of ImageNet pre-trained features for transfer learning. Our
goal was not to consider alternative neural network architectures, but rather to establish facts about which aspects of
the training data are important for feature learning.
The current consensus in the ﬁeld is that the key to learning highly generalizable deep features is the large amounts
of training data and the large number of classes.
To quote the inﬂuential R-CNN paper: “..success resulted from training a large CNN on 1.2 million labeled
images...” . After the publication of R-CNN, most researchers assumed that the full ImageNet is necessary to
pre-train good general-purpose features. Our work quantitatively questions this assumption, and yields some quite
surprising results. For example, we have found that a signiﬁcant reduction in the number of classes or the number
of images used in pre-training has only a modest effect on
transfer task performance.
While we do not have an explanation as to the cause
of this resilience, we list some speculative possibilities that
should inform further study of this topic:
• In our experiments, we investigated only one CNN architecture – AlexNet. While ImageNet-trained AlexNet
features are currently the most popular starting point
for ﬁne-tuning on transfer tasks, there exist deeper
architectures such as VGG , ResNet , and
GoogLeNet . It would be interesting to see if our
ﬁndings hold up on deeper networks. If not, it might
suggest that AlexNet capacity is less than previously
• Our results might indicate that researchers have been
overestimating the amount of data required for learning good general CNN features. If that is the case, it
might suggest that CNN training is not as data-hungry
as previously thought. It would also suggest that beating ImageNet-trained features with models trained on a
much bigger data corpus will be much harder than once
• Finally, it might be that the currently popular target tasks,
such as PASCAL and SUN, are too similar to the original ImageNet task to really test the generalization of the
learned features. Alternatively, perhaps a more appropriate approach to test the generalization is with much less
ﬁne-tuning (e.g. one-shot-learning) or no ﬁne-tuning at
all (e.g. nearest neighbour in the learned feature space).
In conclusion, while the answer to the titular question
“What makes ImageNet good for transfer learning?” still
lacks a deﬁnitive answer, our results have shown that a lot
of “folk wisdom” on why ImageNet works well is not accurate. We hope that this paper will pique our colleagues’
curiosity and facilitate further research on this fascinating
8. Acknowledgements
This work was supported in part by ONR MURI
N00014-14-1-0671. We gratefully acknowledge NVIDIA
corporation for the donation of K40 GPUs and access to
the NVIDIA PSG cluster for this research. We would like
to acknowledge the support from the Berkeley Vision and
Learning Center (BVLC) and Berkeley DeepDrive (BDD).
Minyoung Huh was partially supported by the Rose Hill
Foundation.