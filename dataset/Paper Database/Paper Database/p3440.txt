Distilling Provider-Independent Data for General
Detection of Non-Technical Losses
Jorge Augusto Meira, Patrick Glauner,
Radu State and Petko Valtchev
Interdisciplinary Centre for Security, Reliability and Trust
University of Luxembourg, Luxembourg
{ﬁrst.last}@uni.lu
Lautaro Dolberg, Franck Bettinger and
Diogo Duarte
CHOICE Technologies Holding S`arl, Luxembourg
{ﬁrst.last}@choiceholding.com
Abstract—Non-technical losses (NTL) in electricity distribution are caused by different reasons, such as poor equipment
maintenance, broken meters or electricity theft. NTL occurs
especially but not exclusively in emerging countries. Developed
countries, even though usually in smaller amounts, have to deal
with NTL issues as well. In these countries the estimated annual
losses are up to six billion USD. These facts have directed
the focus of our work to the NTL detection. Our approach is
composed of two steps: 1) We compute several features and
combine them in sets characterized by four criteria: temporal,
locality, similarity and infrastructure. 2) We then use the sets of
features to train three machine learning classiﬁers: random forest,
logistic regression and support vector vachine. Our hypothesis is
that features derived only from provider-independent data are
adequate for an accurate detection of non-technical losses. We
used Area Under the Receiver-operating Curve (AUC) to assess
the results.
Keywords—Artiﬁcial intelligence, big data, electricity theft,
feature engineering, machine learning, non-technical losses.
INTRODUCTION
Electricity is a key factor to reduce the poverty and improve
the quality of life all around the world. Around 84% of the
global population has access to electricity and this number
tends to grow according to The World Bank1. There are several
sources of electricity, such as nuclear plants, hydroelectric,
natural gas, wind turbines, etc. Once the electricity is generated, it is distributed using power grids. During the distribution
phase losses happen quite commonly and are classiﬁed in
two groups: technical or non-technical losses (NTL). Technical
losses are naturally caused by dissipation, while non-technical
losses include poor equipment maintenance, broken meters,
un-metered supply and electricity theft . In this paper we
consider NTL as a black box, which means we make no
distinction between the different types of NTL.
Non-technical losses occur especially but not exclusively in
emerging countries. For example, the NTL estimation in India
is around US$ 4.5 billion. In other emerging countries such
as Brazil, Malaysia and Lebanon, NTL take up to 40% of the
total electricity distributed. Developed countries, even though
usually in smaller amounts, have to deal with NTL issues as
well. UK and USA estimate these losses in a range between
1 
one and six billion USD. These facts have directed the focus
of our work to the NTL detection.
We claim that a robust and portable approach may not rely
on database particularities from different electricity providers.
In this sense, we argue that a reliable set of features should
be supported by common data to any electricity providers’
Overall, this work makes the following contributions:
We compute several sets of features using data from a
real Big Data base from Choice Technologies Holding
Sarl company2 (190M meter readings, 3.5M customers
and 3M inspection results)
We show the impact of the computed sets on supervised machine learning.
We successful demonstrate that features supported
only by raw consumption data presents satisfactory
results when compared with ”provider dependent”
This paper is organized as follows: Section II presents some
background on machine learning and discusses related work.
Section III presents our feature engineering step in detail.
Section IV shows the outcomes and we conclude in Section V.
BACKGROUND AND RELATED WORK
In this section we ﬁrst provide some background on machine learning and feature engineering. Next, we discuss the
related work on NTL detection.
A. Background
Machine learning is the ability of a software to learn
autonomously . We highlight two particular classes of
machine learning algorithms used in this work: supervised and
unsupervised learning. On the one hand, supervised learning
algorithms are trained with data containing known labels (e.g.,
NTL or non-NTL) to produce a mathematical model as output.
Later, this model is used to make predictions from unlabeled
data. On the other hand, unsupervised learning algorithms use
only unlabeled data in order to draw inferences from data (e.g,
k-means) . In both cases, it is essential to choose relevant
data to create a meaningful set of features to support a robust
learning model. This task is called feature engineering.
2 
978-1-5090-5550-0/17/$31.00 c⃝2017 IEEE
SET OF FEATURES
Set of features
Description
Meter reader’s notes
Consumption (C)
Fixed Interval + Fixed Lag
Consumption & Notes (CN)
Fixed Interval and Notes
Neighbourhood (Ng)
Intra Group (geographical neighbourhood)
Transformers (T)
Intra Group (Transformers)
Consumption Proﬁle (CP)
Intra Group (k-means clustering)
Consumption and Neighbourhood
Consumption and Consumption Proﬁle
N+C+Ng+CP+T
B. Related Work
NTL detection can be treated as an anomaly or fraud
detection problem. Comprehensive surveys of the ﬁeld of NTL
detection are provided in , and . Surveys of how
an advanced metering infrastructure can be manipulated, are
provided in and .
One method to detect NTL is to derive features from the
customer consumption time series, such as in : average consumption, maximum consumption, standard deviation, number
of inspections and average consumption of the residential
neighborhood. These features are then grouped into c classes
using fuzzy c-means clustering. Next, customers are classiﬁed
into NTL or non-NTL using the fuzzy memberships. An
average precision of 0.745 is achieved on the test set.
Daily average consumption features of the last 25 months
are used in for less than 400 out of a highly imbalanced
data set of 260K customers. These features are then used in
a support vector machine (SVM) with a Gaussian kernel for
NTL prediction, for which a test recall of 0.53 is achieved.
The class imbalance problem has been addressed in . In
that paper, an ensemble of two SVMs, an optimum-path forest
and a decision tree is applied to 300 test data. While the class
imbalance problem is addressed, the degree of imbalance of
the 1.5K training examples is not reported.
The consumption proﬁles of 5K Brazilian industrial customer proﬁles are analyzed in . Each customer proﬁle
contains 10 features including the demand billed, maximum
demand, installed power, etc. A SVM and k-nearest neighbors
perform similarly well with test accuracies of 0.962. Both
outperform a neural network, which achieves a test accuracy
In authors particularly addressed the class imbalance
of NTL detection and how to assess models in such environment by comparing Boolean and fuzzy expert systems.
Addressing the same imbalanced class, the authors proposed
neighbourhood features in and demonstrated why these
features are statistically meaningful.
FEATURE ENGINEERING
Feature engineering is a key task to support learning
algorithms, as mentioned in Section II-A. We compute several
features using the following criteria:
Temporal: Seasonal, Monthly, Semiannual, Quarterly,
Intra Year;
Locality: Geographical Neighbourhoods;
Similarity: k-means clustering using consumption
Infrastructure: Transformers.
The unit to build the features is the monthly consumption
of a given customer, described as follows:
i , . . . , Cn
where Ci is a certain customer and Cn
i is its consumption
along n months.
Thus, each feature is calculated using the customer’s consumption in a period of time according to a given criteria
(Temporal, Locality, Similarity, Infrastructure).
1) Fixed Interval: The ﬁxed interval calculates the difference between the current consumption and the average
consumption in a period of time:
where K assumes the set of values .
2) Fixed Lag: The ﬁxed lag calculates the Intra Year
difference:
where K = 12.
3) Window: The window calculates the seasonal difference
(Intra Year):
+ Cj−(K−1)
where K = 12.
4) Intra Group: The Intra Group is calculated over a
grouping criteria (Locality, Similarity and Infrastructure):
i , . . . , N n
i ] with N j
where N assumes different group’s id according to Locality,
Similarity and Infrastructure criteria.
A. Sets of Features
We created nine sets of features (see Table I). These sets
are composed by combinations of features computed using the
criteria presented in the previous section.
B. Features Correlation
Features correlation draws the similarities between features
and supports the feature engineering task. In the authors
present the following hypothesis: ”A good feature subset is one
that contains features highly correlated with (predictive of) the
class, yet uncorrelated with (not predictive of) each other”.
We used the Pearson product-moment correlation coefﬁcient,
which gives us the linear dependence between two variables X
and Y, pairs of features in our case. The Paerson’s correlation
is given by:
3The goal is to replace Geographical Neighbourhoods by creating groups
of customers with similar consumption.
Pearson’s correlation of all features according to the label: in the
left side non NTL and in the right side NTL, where 1 (dark orange) indicates
a perfect positive linear correlation, 0 (white) indicates no linear correlation,
and 1 (dark blue) indicates total negative linear correlation).
ρX,Y = cov(X, Y )
where cov is the covariance and σ is the standard deviation
of X and Y.
The correlation between NTL customers and non-NTL
costumers can be visualized in Figure 1. It shows an interesting
ﬁnding related to NTL behaviour. The NTL customers presents
higher features correlation when compared with non-NTL ones,
which leads us to believe that NTL customers behave in a
more similar way than non-NTL customers. Furthermore, we
found that the set of features ”CP - Consumption Proﬁle”
corroborates with the hypothesis mentioned in this section.
EVALUATION
In this section, we evaluate the impact of different sets
of features for NTL detection (see Table I). The experiments
were run in a Intel(R) Xeon(R) CPU E5-2620 2.00 GHz
machine with 128 GB RAM running Ubuntu 14.04.4. The
code is written in Python using Apache Spark’s (1.6.1) scalable
machine learning library (MLlib)4.
In historical data, up to 40% of the inspections end up in
NTL. However, the model must be able to predict in different
proportions of NTL and non-NTL. Thus, we generated data
sets with NTL proportions from 10% to 90%, as follows: 10%,
30%, 40%, 50%, 60%, 70% and 90%.
The performance measure used in the following experiments is the area under the receiver-operating curve (AUC)
 . It plots the true positive rate or recall against the false
positive rate:
AUC = Recall + Speciﬁcity
4 
where the recall measures the proportion of the true positives
and the speciﬁcity measures the proportion of the true negatives.
For a binary classiﬁcation problem (e.g., NTL | non-NTL),
a AUC score of 0.5 is equivalent to chance and a score of
greater than 0.5 is better than chance.
The data set is split into training, validation and test sets
with a ratio of 80%, 10% and 10%, respectively. For each of
the three models the trained classiﬁer that performed the best
on the validation set in any of the 10 folds is selected and tested
on the test set to report the test AUC. This methodology is
related to . Overall, all three classiﬁers perform in the same
regime, as their mean AUC scores over all NTL proportions
are very close. This observation is often made in machine
learning, as the actual algorithm is less important, but having
more and representative data is generally considered to be more
important . This can also be justiﬁed by the no free lunch
theorem, which states that no learning algorithm is generally
better than others . For this reason, in the next section we
only present the results achieved by Random Forest classiﬁer,
which is slightly better than Logistic Regression and Support
Vector Machine.
Random Forest: A random forest is an ensemble estimator
that comprises a number of decision trees . Each tree is
trained on a subsample of the data and feature set in order to
control overﬁtting. In the prediction phase, a majority vote is
made of the predictions of the individual trees.
D. Results
Figure 2 shows the AUC performance of Random Forest
classiﬁer. First, Figure 2(a) draws a comparative overview
between all sets of features presented on Table I using seven
NTL proportions. The best results are zoomed in on Figure
The set of features that cover all features overperforms any
other set, but when compared with neighbourhood sets (i.e.,
C & Ng, C & CP) the difference is not relevant, around 1.5%
better for NTL proportion of 70%. A more detailed comparison
is presented on Table II.
RANDOM FOREST PERFORMANCE ON THE BEST THREE SET
OF FEATURES
NTL proportion
We pinpoint the performance with the NTL proportion
of 40%, which is one of the most common NTL proportion
found on real electricity distribution5. For this proportion, the
5Emerging countries such as Brazil, Malaysia and Lebanon, NTL take up
to 40% of the total electricity distributed.
(a) All sets of features
(b) Best sets of features (zoom in)
AUC performance of Random Forest on different NTL proportions trained with the sets of features presented in Table I.
performance difference for the best three sets of features is
about 1%, and it is a case where the set of features ”C & CP -
Consumption and Consumption Proﬁle” overperforms the set
”C & Ng - Consumption and Neighbourhood”.
E. Discussion
Overall we highlight two sets of features: ”C & Ng” and
”C & CP”. These sets use the ”Locality” and ”Similarity”
as criteria to compute Intra Group features. In the ﬁrst case
the intra group is based on Geographical Neighbourhoods and
in the second case the intra group is based on consumption
similarity. The performance of these sets are very similar to
the performance of the complete set of features: ”All”. Thus,
we argue it is more likely to provide a ”provider independent”
classiﬁcation model supported only by the set of features ”C
& CP”, since this set only uses features supported by raw
consumption data.
CONCLUSION
In this paper, we proposed a feature engineering approach
to NTL detection. We evaluated three machine learning classiﬁers over several sets of features computed using four
criteria: temporal, locality, similarity and infrastructure. The
experimental results show that sets of features supported only
by raw consumption data can achieve satisfactory performance
when compared with sets composed of ”providers’ dependent
features”, such as notes or transformers. We also found out
that for NTL detection the actual algorithm is less important
than having representative set of features.
Based on our approach Company6 carried out real inspections. The preliminary results show that common patterns for
NTL, such as consumption downfall, are not a strict rule
and costumers with consumption increasing may be also good
In our future research, we intend to investigate covariate
shift data issues in NTL (i.e., bias) and how to develop a robust
classiﬁcation method for this problem. We also plan to study
in more detail the feature correlation in order to understand
better customers’ behaviour.
6Details omitted for double-blind reviewing.