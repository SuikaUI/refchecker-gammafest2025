A Markov Clustering Topic Model for Mining Behaviour in Video
Timothy Hospedales, Shaogang Gong and Tao Xiang
School of Electronic Engineering and Computer Science
Queen Mary University of London, London E1 4NS, UK
{tmh,sgg,txiang}@dcs.qmul.ac.uk
This paper addresses the problem of fully automated
mining of public space video data. A novel Markov Clustering Topic Model (MCTM) is introduced which builds on
existing Dynamic Bayesian Network models (e.g. HMMs)
and Bayesian topic models (e.g. Latent Dirichlet Allocation), and overcomes their drawbacks on accuracy, robustness and computational efﬁciency. Speciﬁcally, our model
proﬁles complex dynamic scenes by robustly clustering visual events into activities and these activities into global
behaviours, and correlates behaviours over time. A collapsed Gibbs sampler is derived for ofﬂine learning with
unlabeled training data, and signiﬁcantly, a new approximation to online Bayesian inference is formulated to enable
dynamic scene understanding and behaviour mining in new
video data online in real-time. The strength of this model
is demonstrated by unsupervised learning of dynamic scene
models, mining behaviours and detecting salient events in
three complex and crowded public scenes.
1. Introduction
The proliferation of cameras in modern society is producing an ever increasing volume of video data which is
thus far only weakly and inefﬁciently exploited.
data is frequently stored passively for record purposes. If
the video data is to be actively analyzed, expert knowledge
about the scene and laborious manual analysis and labeling
of the dataset is required. There has been some effort on
developing methods for automatically learning visual behaviour models without human expertise or labour, and using such models to cluster and classify video data, or to
screen for interesting events automatically .
This is a challenging problem for various reasons. Classes
of ‘subjectively interesting behaviour’ to a user can be de-
ﬁned task-speciﬁcally by various factors: the activity of a
single object over time (e.g. its track), the correlated spatial
state of multiple objects (e.g. a piece of abandoned luggage
is deﬁned by separation from its owner) or both spatial and
temporal considerations (e.g. trafﬁc ﬂow at an intersection
might have a particular order dictated by the lights). The
spatial or temporal range over which correlations might be
important may be short or long. Typical public scenes are
crowded, creating difﬁculties for segmentation or tracking.
In this paper we introduce a new model to address the problem of unsupervised mining of multi-object spatio-temporal
behaviours in crowded and complex public scenes by discovering underlying spatio-temporal regularities in video so
as to detect irregular patterns that can be consistently interpreted as ‘salient behaviours’ by human users. A system
based on our model can answer queries such as: “Give me
a summary of the typical activities and scene behaviour in
this scene” and “Show me the (ranked) most interesting (irregular) events happened in the past 24 hours”.
1.1. Related Work
Recent research on dynamic scene understanding has
broadly fallen into object-centric tracking based and nonobject-centric statistical approaches.
Tracking based approaches clearly represent the spatial state of visual objects over time. This allows them to easily model
behaviours like typical ﬂows of trafﬁc, and detect unusual
events such as u-turns.
Such models only work well if
complete tracks can be reliably obtained in training and
test data. For improving robustness to track failures, nonparametric representations of track statistics have been exploited . However, a major limitation of tracking
based approaches is the difﬁculty in modeling behaviours
characterized by coordinated activity of multiple objects.
To improve robustness and enable multi-object spatiotemporal correlation modeling, statistical methods have
been devised to process directly on quantized pixel data
 or other low level ‘event’ features in video .
These methods typically employ a Dynamic Bayesian Network (DBN) such as a Hidden Markov Model (HMM)
 , or a probabilistic topic model (PTM) such as
Latent Dirichlet Allocation (LDA) or extensions. DBNs
are natural for modeling dynamics of behaviour, and with
hierarchical structure also have the potential to perform
clustering of both activities and behaviours simultaneously.
Nevertheless, modeling the temporal order of visual events
explicitly is risky, because noise in the event representation
can easily propagate through the model, and be falsely detected as salient . To overcome this problem, PTMs
were borrowed from text document analysis . These
“bag of words” models represent feature co-occurrence,
completely ignoring temporal order information. Therefore
robustness to noise is at the cost of discarding vital dynamic
information about behaviour. PTMs also suffer from ambiguity in determining the temporal window extent for collecting the bag of words. Large windows risk overwhelming behaviours of shorter duration, and small windows risk
breaking up behaviours arbitrarily. This is especially damaging since correlation between bags is not modeled.
1.2. Our Approach
In this paper, a novel Markov Clustering Topic Model
(MCTM) is introduced which builds on the strength of existing DBNs and PTMs, but crucially is able to overcome
their drawbacks on accuracy, robustness and computational
efﬁciency. In particular, the model makes two important
novel contributions to LDA: (1) Hierarchical modeling, allowing simple actions to be combined into complex global
behaviours; and (2) temporal modeling, enabling the correlation of different behaviours over time to be modeled.
By introducing a Markov chain to model behaviour dynamics, this model deﬁnes a DBN generalization of LDA. This
gains strength in representing temporal information, while
being robust to noise due to its bag of words modeling of visual features. Learning from unlabeled training data is performed ofﬂine with Gibbs sampling; and a novel Bayesian
inference algorithm enables dynamic scene understanding
and behaviour mining in new video data online and in realtime where existing approaches fail .
2. Spatio-Temporal Video Mining
2.1. Video Representation
We wish to construct a generative model capable of automatic mining and screening irregular spatio-temporal patterns as ‘salient behaviours’ in video data captured from
single ﬁxed cameras monitoring public spaces with people
and vehicles at both far and near-ﬁeld views (see Sec. 4.1).
These camera views contain multiple groups of heterogeneous objects, occlusions, and shadows. Local motions are
used as low level features. Speciﬁcally, a camera view is
divided into C × C pixel-cells, and optical ﬂow computed
in each cell. When the magnitude of the ﬂow is greater
than a threshold Tho, the ﬂow is deemed reliable and quantized into one of four cardinal directions. A discrete visual
event is deﬁned based on the position of the cell and the
motion direction. For a 320 × 240 video frame and with
cell size of 10 × 10, a total of 3072 different discrete visual
events may occur in combination. For visual scenes where
objects may remain static for sustained period of time (e.g.
people waiting for trains at a underground station), we also
use background subtraction to generate a ﬁfth – stationary
foreground pixel – state for each cell, giving a visual event
codebook size of 3840. This illustrates the ﬂexibility of of
our approach: it can easily incorporate other kinds of ‘metadata’ features that may be relevant in a given scene. The
input video is uniformly segmented into one-second clips,
and the input to our model at second t is the bag of all visual
events occurring in video clip t, denoted as xt.
2.2. Markov Clustering Topic Model (MCTM)
Standard LDA (see Fig. 1(a)) is an unsupervised
learning model of text documents xm, m = 1..M. A document m is represented as a bag of i = 1..Nm unordered
words xi,m, each of which is distributed according to a
multinomial distribution p(xi,m|φyi,m) indexed by the current topic of discussion yi,m. Topics are chosen from a perdocument multinomial distribution θm. Inference of latent
topics y and parameters θ and φ given data xm effectively
clusters co-occurring words into topics.
This statistical
topic based representation of text documents can facilitate,
e.g., comparison and searching. For mining behaviours in
video, we consider that visual events correspond to words,
simple actions (co-occurring events) to topics, and complex
behaviours (co-occurring actions) to document categories.
occurrence
clips/documents X = {xt} where t = 1..T as having a
three layer latent structure: events, actions and behaviours,
as illustrated by the graphical model in Fig. 1(b). The generative model is deﬁned as follows: Suppose the data contains
T clips, each of which exhibits a particular category of
behaviour, represented by zt. The behaviour category zt
is assumed to vary systematically over time from clip to
clip according to some unknown multinomial distribution,
p(zt|zt−1, ψ) (denoted Multi(·)). Within each clip t, Nt
simple actions {yi,t}Nt
i=1 are chosen independently based
on the clip category, yi,t ∼p(yi,t|zt, θ). Finally, each
observed visual event xi,t is chosen based on the associated
action yi,t, xi,t ∼p(xi,t|yi,t, φ). All the multinomial parameters {φ, ψ, θ} are treated as unknowns with Dirichlet
priors (denoted Dir(·)). The complete generative model is
speciﬁed by:
Dir(ψz; γ),
Dir(θz; α),
Dir(φy; β),
p(zt+1|zt, ψ)
Multi(zt; ψzt),
p(yi,t|zt, θ)
Multi(yi,t; θzt),
p(xi,t|yi,t, φ)
Multi(xi,t; φyi,t).
The full joint distribution of variables {xt, yt, zt}T
Visual Words
Behaviours
Figure 1. Graphical models representing:
(a) Standard LDA
model , (b) Our MCTM model.
rameters θ, φ, ψ given the hyper-parameters α, β, γ is:
p({xt, yt, zt}T
1 , φ, ψ, θ|α, β, γ)
p(φ|β)p(ψ|γ)p(θ|α)
p(xi,t|yi,t)p(yi,t|zt)
p(zt|zt−1).
2.3. Model Learning
As for LDA, exact inference in our model is intractable,
but it is possible to derive a collapsed Gibbs sampler for
approximate MCMC learning and inference. The Dirichlet-
Multinomial conjugate structure of the model allows the parameters {φ, θ, ψ} to be integrated out automatically in the
Gibbs sampling procedure. The Gibbs sampling update for
the action yi,t is derived by integrating out the parameters φ
and θ in its conditional probability given the other variables:
p(yi,t|y\i,t, z, x) ∝
y,z + Nyα.
Here y\i,t denotes all the y variables excluding yi,t; n−
denotes the counts of feature x being associated to action y;
y,z denotes the counts of action y being associated to behaviour z. Superscript “−” denotes counts over the remaining dataset excluding item (i, t). Nx is the size of the visual
event codebook, and Ny the number of simple actions.
The Gibbs sampling update for cluster zt is derived
by integrating out parameters ψ and θ in the conditional
p(zt|y, z\t, x), and must account for the possible transitions
between zt−1 and zt+1 along the Markov chain of clusters:
p(zt|y, z\t, x)
y Γ(α + ny,zt)Γ(Nyα + n−
y Γ(α + n−
y,zt)Γ(Nyα + n·,zt)
z′,z + Nzγ
nzt+1,zt + I(zt−1 = zt)I(zt = zt+1) + γ
n·,zt + I(zt−1 = zt) + Nzγ
Here nz′,z are the counts of behaviour z′ following behaviour z, n·,z ≜P
z′ nz′,z, and Nz is the number of clusters. I is the identity function that returns 1 if its argument
is true, and Γ is the gamma function. Note that we do not
obtain the simpliﬁcation of gamma functions as in standard
LDA and Eq. (2), because the inclusive and exclusive counts
may differ by more than 1, but this is not prohibitively
costly, as Eq. (3) is computed only once per clip. Iterations
of Eqs.(2) and (3) entail inference by eventually drawing
samples from the posterior p({yt, zt}T
1 , α, β, γ). Parameters {φ, ψ, θ} may be estimated from the expectation
of their distribution given any full set of samples , e.g.
n·,y + Nxβ .
3. Online Inference and Saliency Detection
A limitation of the (standard) model learning and inference method described above, also adopted by , is
that they are ofﬂine, batch procedures. For on-the-ﬂy behaviour mining in video, we formulate a new real-time ﬁltered (or smoothed) inference algorithm for our MCTM after an ofﬂine batch learning phase.
Given a training dataset of Ttr
{{yt, zt}Ttr
, ˆφ, ˆψ, ˆθ}Ns
distribution
p({yt, zt}Ttr
t=1|{x}Ttr
, α, β, γ).
We assume that no further adaptation of the parameters is necessary, i.e.
training dataset is representative, so p(φ, ψ, θ|xt′>Ttr) =
p(φ, ψ, θ|x1:Ttr).
We then perform Bayesian ﬁltering in
the Markov chain of clusters to infer the current clip’s
behaviour p(zt|x1:t) by approximating the required integral over the parameters with sums over their Gibbs samples . Conditioned on each set of (sampled) parameters,
the other action yi,t and behaviour zt variables decorrelate,
so efﬁcient recursions can be derived to compute the behaviour category for each clip online:
p(zt+1|x1:t+1) =
p(xt+1, zt+1|zt, φ, θ, ψ, x1:t) p(zt, φ, θ, ψ|x1:t)
p(xt+1|x1:t)
p(xt+1|zt+1, φs, θs) p(zt+1|zs
p(xt+1|x1:t)
Bayesian surprise (saliency, or irregularity), is optimally
measured by the marginal likelihood of the new observation
given all the others, p(xt+1|x1:t). This can be determined
from the normalization constant of Eq. (5), or explicitly as:
p(xt+1|x1:t)=
p(xt+1|ψ, θ, φ, x1:t) p(zt, φ, ψ, θ|x1:t),
p(xt+1, zt+1|ψs, θs, φs, zs
Without the iterative sweeps of the Gibbs sampler, even
summing over samples s, behaviour inference (or clip categorization) and saliency detection can be performed online
and in real-time by Eq. (5) and Eq. (6). Note that in practice Eq. (5) may suffer from label switching , so a single sample should be used for interpretable results. Eq. (6)
is independent of label switches and should be used with
all samples. This online approach has no direct analogy in
vanilla LDA (Fig. 1(a)), as the per document parameter θ requires iterative computation to infer. We compare
the computational cost of our MCTM, LDA , Dual-HDP
 and HMMs in Sec. 4.4.
The Bayesian measure of saliency p(xt+1|x1:t) of test
point xt+1 given training data x1:Ttr and other previous test
data xt>Ttr is used to detect irregularity. p(xt+1|x1:t) re-
ﬂects the following salient aspects of the data:
Intrinsic: xt rarely occurred in training data x1:Ttr.
Actions: xi,ts rarely occurred together in the same topic in
Behaviours: xt occurred together in topics, but such topics
did not occur together in clusters in x1:Ttr.
Dynamics: xt occurred together in a cluster zt, but zt did
not occur following the same cluster zt−1 in x1:Ttr.
Such detections are made possible because the hierarchical
structure of our model represents behaviour at different levels (events, actions, behaviours, behaviour dynamics).
4. Experiments
4.1. Datasets and Settings
Experiments were carried out using video data from
three complex and crowded public scenes. Street Intersection Dataset: This contained 45 minutes of 25 fps video
of a busy street intersection where three trafﬁc ﬂows in different directions are regulated by the trafﬁc lights, in a certain temporal order (see Fig. 3(a)-(e)). The frame size is
360 × 288. Pedestrian Crossing Dataset: This also consists of 45 minutes of 360×288 pixel 25 fps video, and captures a busy street intersection with particularly busy pedestrian activity (see Fig. 3(f)-(i)). Typical behaviours here are
pedestrian crossings alternating with two main trafﬁc ﬂows.
Subway Platform Dataset: A total of 18 minutes of videos
from the UK Home Ofﬁce i-LIDS dataset is selected for the
third experiment. Though equally busy, the visual scene
in this dataset differs signiﬁcantly from the other two in
that it is indoor and features mainly people and trains (see
Fig. 3(j)-(n)). In addition, the camera was mounted much
closer to the objects and lower, causing more severe occlusions. Typical behaviours in this scene include people waiting for the train on the platform, and getting on or off the
train. The video frame size is 640 × 480 captured at 25 fps.
We used 5 minutes from each dataset for training, and
tested (Eqs. (5) and (6)) on the remaining data. The cell size
for both of the two street datasets was 8×8, and 16×16 for
the subway dataset. Optical ﬂow computed in each cell is
quantized into 4 directions for the two street datasets and 5
for the subway dataset, with the ﬁfth corresponding to stationery foreground objects common in the subway scene.
We run the Gibbs sampler (Eqs. (2) and (3)) for a total of
1500 complete sweeps, discarding the ﬁrst 1000 as burn-in,
and then taking 5 samples at a lag of 100 as independent
samples of the posterior p({yt, zt}Ttr
|x1:Ttr, α, β, γ). In
each case we selected the number of actions as Ny =8 and
the number of behaviour clusters as Nz = 4; except for the
pedestrian crossing dataset, where we used Nz = 3 because
there are clearly three trafﬁc ﬂows. We ﬁxed these numbers
for ease of illustration. Larger Ny and Nz result in a more
ﬁne-grained decomposition of scene behaviour. Dirichlet
hyper-parameters were ﬁxed at {α = 8, β = 0.05, γ = 1}
for all experiments to encourage composition of speciﬁc
actions into general topics, but these could be empirically
estimated during sampling .
4.2. Unsupervised Scene Interpretation
Clustering Visual Events into Actions:
The learned topics of our MCTM correspond to actions consisting of cooccurring visual events. These actions are typically associated with patterns of moving objects. Fig. 2 shows some
example actions/topics y discovered by way of plotting the
visual events x in the top 50% of the mass of the distribution
p(x|y, ˆφs
y) (Eq. 4). Note that each action has a clear semantic meaning. In the street intersection dataset, Figs. 2(a) and
(b) represent vertical left lane and horizontal leftwards traf-
ﬁc respectively, while Fig. 2(c) represents the vertical trafﬁc
vehicles turning right at the ﬁlter. In the pedestrian crossing
dataset, Figs. 2(d) and (e) illustrate two independent vertical
trafﬁc ﬂows, and Fig. 2(f) represents diagonal trafﬁc ﬂow
and pedestrians crossing at the lights while the ﬂows of (d)
and (e) have stopped. For the subway dataset, Fig. 2(g) includes people leaving (yellow arrows) from a stopped train
(cyan dots on the train). Fig. 2(h) includes people walking
up the platform and Fig. 2(i) shows people sitting on the
bench waiting.
Street Intersection
Pedestrian Crossing
Figure 2. Example topics/actions learned in each of the three scenarios illustrated by the most likely visual events for each ˆφs
y. Arrow directions and colors represent ﬂow direction of the event.
Discovering Behaviours and their Dynamics:
Cooccurring topics are automatically clustered into behaviours
z via matrix θz (Sec. 2.3), each of which corresponds to
a complex behaviour pattern involving multiple interacting objects. Complex behaviour clusters discovered for the
three dynamic scenes in the 5 minutes of training data, are
depicted in Fig. 3. Speciﬁcally, Figs. 3(a) and (b) represent horizontal left and right trafﬁc ﬂows respectively including right turn trafﬁc (compare horizontal only trafﬁc in
Fig. 2(b)). Figs. 3(c) and (d) represent vertical trafﬁc ﬂow
with and without interleaved turning trafﬁc. The temporal
duration and order of each trafﬁc ﬂow is also discovered accurately. For example, the long duration and exclusiveness
of the horizontal trafﬁc ﬂows (a) and (b) – and the interleaving of the vertical trafﬁc (c) and vertical turn trafﬁc (d) – are
clear from the learned transition distribution ˆψs (Fig. 3(e)).
For the pedestrian crossing dataset, three behaviour clusters are learned. Fig. 3(f), diagonal ﬂow of far trafﬁc and
downwards vertical trafﬁc ﬂow at the right, excluding the
crossing zone where there is pedestrian ﬂow (horizontal yellow arrows). Figs. 3(g) and (h) show outer diagonal and
vertical trafﬁc, and inner vertical trafﬁc respectively with no
pedestrians crossing. The activity of the pedestrian crossing
light is evident by the switching between (f) and (g) in the
learned transition distribution (Fig. 3(i), top left).
The four behaviour categories discovered in the subway
scene were: People walking towards (red & green arrows)
an arriving train (green arrows on train) (Fig 3(j)); People
boarding a stopped train (cyan dots on the track) or leaving
the station (Fig 3(k)); People leaving the station while the
trains wait (Fig 3(l)) (in this dataset, the train usually waited
for longer than it took everyone to board; hence this cluster); People waiting for the next train by sitting on the bench
(Fig 3(m)). Our model is also able to discover the cycle of
Document Likelihoods
Estimated Scene
Sample Frame
Figure 4. An example of online processing.
behaviour on the platform triggered by arrival and departure of trains (Fig. 3(n)). For example, the long duration of
waiting periods (m) between trains, broken primarily by the
train arriving state (j), (see Fig. 3(n), fourth column).
4.3. Online Video Screening
The model was learned for each scenario before new
video data was screened online.
The overall behaviours
were identiﬁed using Eq. (5), and visual saliency (irregularity) measured using Eq. (6). Fig. 4 shows an example of
online processing on test data from the street intersection
dataset. The MAP estimated behaviour ˆzt at each time is illustrated by the colored bar, and reports the trafﬁc phase:
turning, vertical ﬂow, left ﬂow and right ﬂow.
graph shows the likelihood p(xt|x1:t−1) of each clip as it
is processed online. Three examples are shown including
two typical clips (turning vertical trafﬁc and ﬂowing vertical trafﬁc categories) and one irregular clip where a vehicle
drives in the wrong lane. Each is highlighted with the ﬂow
vectors (blue arrows) on which computation is based.
We manually examined the top 1% most surprising clips
screened by the model in the test data. Here we discuss
some examples of ﬂagged surprises. In Fig. 5(a) and (b),
another vehicle drives in the wrong lane. This is surprising, because that region of the scene typically only includes
down and leftward ﬂows. This clip is intrinsically, (Sec. 3)
unlikely, as these events were rare in the training data under
any circumstances. In Fig. 5(c) and (d), a police car breaks
a red light and turns right through opposing trafﬁc. Here
the right ﬂow of the other trafﬁc is a typical action, as is
the left ﬂow of the police car. However, their conjunction
(forbidden by the lights) is not. Moreover some clips in this
multi-second series alternately suggest left and right ﬂows,
but such dynamics are unlikely under the learned temporal
model (Fig. 3(e)). Therefore this whole series of clips is behaviorally and dynamically (Sec. 3) unlikely given global
and temporal constraints entailed by p(xt|x1:t−1).
Street Intersection
Pedestrian Crossing
Figure 3. Behaviour and dynamics in each of the three scenarios, illustrated by the most likely visual words/events for each behaviour ˆθs
and the transitions between behaviours ˆψs
Figure 5. Sample salient clips discovered. Arrows/dots indicate input events and red boxes highlight regions discussed in the text.
Another behavioral (action concurrence) surprise to the
model is the jay-walker in Fig. 5(e-f). Here a person runs
across the intersection to the left, narrowly avoiding the
right trafﬁc ﬂow. Both left and right ﬂows are typical, but
again their concurrence in a single document, or rapid alteration in time is not. Fig. 5(g) shows the detection of a
jaywalker triggered by intrinsically unlikely horizontal motion across the street. In contrast, Fig. 5(h) illustrates two
plausible pedestrian actions of crossing left and right at the
crosswalk, but doing so at the same time as the vertical
trafﬁc ﬂow. This is multi-object situation is behaviorally,
(Sec. 3) irregular. In Fig. 5(i) a train arrives, and three people typically (Fig. 3(j)) walk towards the train for boarding.
However, unusually, other people walk away from the train
down the platform, a behaviorally unlikely concurrence. In
Fig. 5(k), the train is now stationary. While most people perform the typical paired action of boarding (Fig. 3(k)), others
walk away from the train down the platform, a multi-object
behaviour detected due to low likelihood p(xt|x1:t−1).
Figs. 5(c-f) illustrate an important feature of our model
that gives a signiﬁcant advantage over non-temporal LDA
based models : Our model is intrinsically less constrained by bag-of-words size, i.e. determining a suitable
temporal window (clip) size. With standard LDA, larger
bag sizes would increase the chance that vertical and horizontal ﬂows here were captured concurrently and therefore
ﬂagged as surprising. However, larger bag sizes also capture much more data, risking loosing interesting events in
a mass of normal ones. Our model facilitates the use of
a small one second bag size, by providing temporal information so as to penalize unlikely behaviour switches. As a
result, our model can discover not only quick events such
as Fig. 5(a) and (b) that might be lost in larger bags, but
also longer time-scale events such as Fig. 5(c-f) that could
be lost in many independently distributed smaller bags.
To demonstrate the breadth of irregular behavioural patterns our model is capable of consistently identifying, some
of which are visually subtle and difﬁcult to detect even
by human observation, we provide a human interpreted
summary of the categories of screened salient clips in Table 1. We compare the results with two alternatives, LDA
 with Ny topics, and a HMM with Nz states.
with no clear salient behaviour were labeled “uninteresting”. These were variously due to camera glitches, exposure
compensation, birds, very large trucks, and limited training
data to accurately proﬁle typical activities. There is no algorithmic way to determine “why” (i.e. action, behaviour,
dynamics) events were surprising to the model, so we do
not attempt to quantify this. Our MCTM outperforms the
other two models especially in the more complex behaviour
categories of red-light-breaking, u-turns and jaywalking. In
these cases, the saliency of the behaviour is deﬁned by
an atypical concurrence of actions and/or sequence of behaviours over time, i.e. a surprise is deﬁned by complex
spatio-temporal correlations of actions rather than simple
individual actions. In contrast, conventional LDA can infer
actions, but cannot reason about their concurrence or temporal sequence simultaneously. HMMs can reason about
sequences of behaviours, but with point (EM) learning, and
lacking the intermediate action representation, HMMs suffer from severe over-ﬁtting. All the models do fairly well
at detecting intrinsically unlikely words which are visually
well-deﬁned independently, e.g. wrong way driving.
For the pedestrian crossing dataset, the result is shown
in Table 2. Atypical pedestrian behaviours were jaywalking
far from the crosswalk (intrinsically unlikely visual events),
and crossing at the crosswalk through trafﬁc (unlikely action concurrence; Fig. 3(f) vs (g),(h)). Our MCTM was
more adept than both LDA and HMM at detecting the more
subtle behaviours. This is due to the same reasons of simultaneous hierarchical and temporal modeling of actions
Street Intersection
Break Red Light
Illegal U-Turn
Jaywalking
Drive Wrong Way
Unusual Turns
Uninteresting
Table 1. Summary of human meaningful clip types discovered by
different models for the street intersection dataset.
Pedestrian Cross
Jaywalking
Through Trafﬁc
Uninteresting
Subway Platform
Uninteresting
Table 2. Summary of human meaningful clip types discovered by
different models for crossing and subway platform datasets.
and improved robustness due to Bayesian parameter learning compared to HMMs especially. Finally, for the subway
dataset (Table 2) the only interesting behaviours observed
were people moving away from the train during clips where
typical behaviour was approaching trains and boarding passengers, this was detected by our model and not the others.
4.4. Computational Cost
The computational cost of MCMC learning in any model
is hard to quantify, because assessing convergence is itself an open question , as also highlighted by . In
training, our model is dominated by the O(NT Ny) cost of
resampling the total number NT of input features in the
dataset per Gibbs sweep, which is the same as . In
testing, our model requires O(N 2
z ) + O(NT NyNz) time
per parameter sample. In practice using Matlab code on
a 3GHz CPU, this meant that training on 5 minutes of our
data required about 4 hours. Using our model to process one
hour of test data online took only 4 seconds in Matlab. Processing the same data with (Variational) LDA in C took
about 20 and 8 seconds respectively, while (EM) HMM in
Matlab took 64 seconds and 26 seconds. Wang et al. 
reported that Gibbs sampling in their HDP model required
8 hours to process each hour of data from their quieter (and
therefore fewer words, so quicker) dataset; and they do not
propose an online testing solution. These numbers should
not be compared literally given the differences in implementations and datasets; however the important thing to
note is that while our model is competitive in training speed
to sophisticated contemporary models , it is much faster
for online testing. Moreover, it is faster than the simple
models which it outperforms in saliency detection.
5. Discussion
We introduced a novel Bayesian topic model for simultaneous hierarchical and temporal clustering of visual events
into actions and global behaviours. The model addresses
two critical tasks for unsupervised video mining: modeling scene behavioral characteristics under-pinned at different spatial and temporal levels, and online behaviour
screening and saliency detection. Our Gibbs learning procedure has proven effective at learning actions, behaviours
and temporal correlations in three diverse and challenging
datasets. We showed how to use the Gibbs samples for rapid
Bayesian inference of clip category and saliency. Evaluating the salient clips returned from our diverse datasets,
our MCTM outperforms LDA and HMMs for unsupervised
mining and screening of salient behaviours, especially for
visually subtle, and temporally extended activity. This was
because we model simultaneously temporal evolution of behaviour (unlike LDA), the hierarchical composition of action into behaviours (unlike LDA and HMM) and Bayesian
parameter learning (unlike HMM). Compared to objectcentric approaches such as , our simple and reliable
visual features improve robustness to clutter and occlusion.
We have not addressed the issue of determining the optimal number of behaviours and actions in a given dataset,
as was done in . For our model, Bayesian model selection can readily be done ofﬂine once per scene in a principled if computationally intensive way: maximizing the
marginal likelihood p(x|Nz, Ny) based on the Gibbs output, or Eq. (6). This approach retains the option of subsequent online real-time processing, in contrast to which
does not propose an online solution, and whose batch solution is in the order of ten times slower than real time .
To put our theoretical modeling contribution in context, it contrasts other hierarchical work which clusters actions, but not over time , and other non-hierarchical
work which temporally correlates words within (rather
than across) documents or provides continuous variation
(rather than discrete clustering) of parameters over time .
In summary, we have presented a uniﬁed model for
completely unsupervised learning of scene characteristics,
dynamically screening and identifying irregular spatiotemporal patterns as salient behaviour clips that may be of
interest to a human user. An important feature of our approach is the breadth of different kinds of behaviours that
may be modeled and ﬂagged as salient due to our simultaneous hierarchical topic modeling and temporal correlation globally optimized in a uniﬁed model. For example,
temporally extended events typically only ﬂagged by object/tracking centric models such as u-turns as well
as multi-object events typically only ﬂagged by statistical
event models such as jaywalking . Finally, the speciﬁc
formulation of our model also permits Bayesian saliency
discovery of these type of events online in real-time.
Acknowledgment:
This research was partially funded by
EU FP7 project SAMURAI with grant no. 217899.