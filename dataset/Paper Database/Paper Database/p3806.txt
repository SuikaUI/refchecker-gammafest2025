Elsevier required licence: ¬© <2021>. This manuscript version is made available under the CC-BY-NC-
ND 4.0 license 
The definitive publisher version is available online at
[ 
Hunger Games Search: Visions, Conception, Implementation,
Deep Analysis, Perspectives, and Towards Performance Shifts
Yutao Yang1, Huiling Chen1*, Ali Asghar Heidari2,3, Amir H Gandomi4
1College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou, Zhejiang 325035,
 , ,
2School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran
1439957131, Iran
 , 
3Department of Computer Science, School of Computing, National University of Singapore, Singapore 117417,
 , 
4 Faculty of Engineering & Information Technology, University of Technology Sydney, NSW 2007, Australia
 
ÔÄ™ Corresponding Author: Huiling Chen
E-mail: (Huiling Chen)
A recent set of overused population-based methods have been published in recent years.
Despite their popularity, as a result of manipulated systematic internet marketing, product
bundling, and advertising techniques, most of them have uncertain, immature performance,
partially done verifications, similar overused metaphors, similar immature exploration and
exploitation components and operations, and an insecure tradeoff between exploration and
exploitation trends in most of the new real-world cases. Therefore, all users need to
extensively modify and adjust their operations based on main evolutionary methods to reach
faster convergence, more stable balance, and high-quality results. To move the optimization
community one step ahead toward more focus on performance rather than change of
metaphor, a general-purpose population-based optimization technique called Hunger Games
Search (HGS) is proposed in this research with a simple structure, special stability features
and very competitive performance to realize the solutions of both constrained and
unconstrained problems more effectively. The proposed HGS is designed according to the
hunger-driven activities and behavioural choice of animals. This dynamic, fitness-wise search
method follows a simple concept of ‚ÄúHunger‚Äù as the most crucial homeostatic motivation
and reason for behaviours, decisions, and actions in the life of all animals to make the process
of optimization more understandable and consistent for new users and decision-makers. The
Hunger Games Search incorporates the concept of hunger into the feature process; in other
words, an adaptive weight based on the concept of hunger is designed and employed to
simulate the effect of hunger on each search step. It follows the computationally logical rules
(games) utilized by almost all animals and these rival activities and games are often adaptive
evolutionary by securing higher chances of survival and food acquisition. This method's main
feature is its dynamic nature, simple structure, and high performance in terms of convergence
and acceptable quality of solutions, proving to be more efficient than the current optimization
methods. The effectiveness of HGS was verified by comparing HGS with a comprehensive
set of popular and advanced algorithms on 23 well-known optimization functions and the
IEEE CEC 2014 benchmark test suite. Also, the HGS was applied to several engineering
problems to demonstrate its applicability. The results validate the effectiveness of the
proposed optimizer compared to popular essential optimizers, several advanced variants of
the existing methods, and several CEC winners and powerful differential evolution (DE)based methods abbreviated as LSHADE, SPS_L_SHADE_EIG, LSHADE_cnEpSi,
SHADE, SADE, MPEDE, and JDE methods in handling many single-objective problems.
We designed this open-source population-based method to be a standard tool for
optimization in different areas of artificial intelligence and machine learning with several new
exploratory and exploitative features, high performance, and high optimization capacity. The
method is very flexible and scalable to be extended to fit more form of optimization cases in
both structural aspects and application sides. This paper's source codes, supplementary files,
Latex and office source files, sources of plots, a brief version and pseudocode, and an opensource software toolkit for solving optimization problems with Hunger Games Search and
online web service for any question, feedback, suggestion, and idea on HGS algorithm will
be available to the public at 
Hunger Games Search; Optimization; Swarm-intelligence; Metaheuristic; Engineering design
1 Introduction
The real-world applications in expert systems, information systems, and knowledge-based
systems often have a limited feature space and constraints based on the priorities and budget
limits of the project owners. Decision-makers, developers and computer scientists need to
find some feasible, explainable and sufficient details and solutions during a reasonable time
using any family of deterministic or approximated algorithms for problems in different areas
such as operation optimization , image segmentation , target tracking systems , location-based services , image big data , opportunistic networks , multipath routing
 , supply chain development , hydrothermal systems , engineering
applications , video deblurring , social recommendation and QOS-aware
service composition , image recovery and alignment , recognizing landmark architectures , human
articulated body recognition , secure encryption , image filtering , image editing ,
structural topology optimization , scheduling
problem , face
recognition and micro-expression recognition , gold price prediction , epileptic seizure
detection , and wireless communication and network
systems , to name a few
potential areas for future users in optimization and artificial intelligence (AI) community. On
the other hand, the effectiveness and complexity of the developed solvers is a central concern
when the characteristics of the problems get more dynamic or complicated in terms of
multimodality, uncertainty and vagueness of feature space. For instance, we can point to a
set of applications in cross-field and computer science such as parameters optimization , bankruptcy prediction ,
Prediction problems in educational field , brain disease
diagnosis , thyroid cancer diagnosis ,
tuberculous pleural effusion diagnosis , paraquat-poisoned patients
diagnosis , Parkinson disease diagnosis , and other medical problems
 , deployment optimization
 , optimal resource allocation , smart agriculture , and intelligent traffic management . Optimization is a "should" behind most of the AI and industrial
problems in different disciplines such as neural networks and
deep learning . It can be in the form of single-objective
 that we need to prepare all objective in a
single known function, but it has been extended to many more forms such as multiobjective
 , robust optimization , many
objectives , fuzzy optimization
 , large scale optimization , and memetic methods . There are also two
philosophical viewpoints to deal with problems and mathematical models that one of them
rely on the utilization of the gradient and deterministic equations when solving the problem
 and another viewpoint has a trial and
error nature using recursive sensing and evaluating the landscape of the problem based on
some approximated metrics and info about the problem basin or in a stochastic way.
Evolutionary and swarm-based optimization method or metaheuristic methods are widely
used approach in this class .
Finding optimal solutions to multimodal rotated, or composition problems is a difficult
task without having any gradient information about an objective function. Over the past few
years, users have become more interested in estimating the best solutions, then utilizing these
solutions depending on their accuracy level. Hence, the meta-heuristic algorithms (MAs) have
attracted substantial attention, and they have been applied to various fields of machine
learning, engineering, and science. The main reason for such a trend is that there is an
overflow of new problems in the real world and, as such, increasing demands for these solvers
when the problems become more challenging. The characteristics of MAs, such as avoiding
local optimum, simplicity, and gradient-free steps, makes it possible to provide satisfactory
solutions to such complex problems, which typically have many local optima and challenging
search space. Dealing with multimodal spaces with iterative exploratory and exploitative
procedures is the central feature of all MAs in literature.
Nevertheless, there are also some gaps, concerns, and drawbacks within the previous
swarm-based optimization methods. Recently, some popular methods have been
proposed that are based on the characters of animals. However, various studies revealed
that these performance methods were not studied deeply in the original work and their
mathematical models also suffer from structural defects, mediocre performance,
problematic verification methods, the apparent similarity in their structure, and slightly
modified components. As per our rich experience on these methods , such issues affect their reliability in the optimization
community without sufficient attention to the performance aspects, complexity, the
tuning of parameters, comparison with advanced and high-tech optimizers, verification
using CEC competition sets, and wise interactions among the components. These
aspects play significant roles when decision-makers or practitioners need to deal with
some real-world problems. These disputes motivated us to investigate the algorithmic
behaviours further and develop a more stable logic, especially considering that these
popular methods require much effort and modifications to jump out of local optima and
stagnation and their shortcomings. Although general users in industry and inexperienced
code users can barely detect these issues, these methods are still difficult to
understand. Hence, we attempted to highlight more aspects in this research and
compared them to other methods to shift the preferences of the field toward the
performance.
The focus of almost all methods is to iteratively evolve the population that appeared in
the genetic algorithm (GA) and particle swarm optimizer (PSO), which
were later divided into the evolutionary algorithm (EA) and Swarm Intelligence (SI)
optimizers. Biological evolutionary operations support the logic of evolutionary
algorithms and can tackle optimization problems by three operations: selection,
reorganization, and mutation. The GA is a basic EA proposed by
Holland based on Darwin's theory of evolution. Simulating organisms' evolution or the
ideal solution can be performed in the solution space. The evolutionary process of the
Differential Evolution (DE) algorithm is very similar to that of GA,
but its specific definition of operation is different. At the same time, it uses the
cooperative relationship between individuals within a group and the swarm intelligence
generated by competition to guide the direction of evolution. Besides, EA includes
Genetic Programming (GP) , Evolution Strategy (ES) , and Evolutionary Programming (EP) (Yao, Liu, & Lin,
SI mainly simulates natural organisms' collective behaviour and uses social wisdom to
search for optimal searching space cooperatively. Ant Colony Optimization (ACO) simulates the food collection conducted in ant colonies and has been applied
in many discrete problems. PSO mimics the regularity of bird
clusters' activities, using information sharing among individuals in the group to move the
whole group. In addition to the above two representative algorithms, the more recent SI
algorithms are biogeography-based optimization (BBO) , Bat-inspired
Algorithm (BA) , Monarch Butterfly Optimization (MBO) , Cuckoo Search (CS) , Artificial Bee Colony
(ABC) , and Harris Hawk Optimizer (HHO)1 . Another promising method is the slime mould algorithm (SMA)2 , which has been recently developed based on slime mould and is
gaining more attention from experts. In Figure 1, the classification of methods based on
algorithmic behaviours is shown. Please refer to the original research presented by Molina et
al. for complete data and further study.
Figure 1 Classification of optimizers based on the behaviour taxonomy.
Although MAs are divided into several categories, they share the same characteristics in
that the search steps consist of exploration and exploration. In the first stage, we need to
ensure the randomness of the search as much as possible and explore the search space
1 The info and source codes of the HHO algorithm are publicly available at 
2 The info and source codes of the SMA algorithm are publicly available at 
broadly. In the second stage, we need to accurately focus on specific regions of the feature
space found in the previous stage. A promising area focuses on the local search capacity, so
balancing these two stages is crucial to the algorithm's performance.
Although many MAs have been proposed, there is no free lunch in the world, and no algorithm can solve all optimization problems as the best method.
Since each algorithm shows superiority in some specific optimization problems, researchers
continuously work to explore and develop better algorithms. Hence, this work proposes a
new meta-heuristic algorithm, Hunger Games Search (HGS), which is inspired by social
animals' cooperative behaviour where search activity is proportional to their level of hunger.
This algorithm is designed and implemented based on the common characteristics of social
animals and their food search.
The remainder of this paper is structured as follows. Section 2 expounds on the
enlightenment of HGS and establishes the corresponding mathematical model. Section 3
depicts the experiments involved in this work, qualitative analysis, and comparison with
traditional and advanced algorithms on 23 benchmark functions, and IEEE CEC benchmark
functions, and application to engineering problems. Section 4 summarizes the full text and
future research direction.
2 Hunger Games Search (HGS)
In this chapter, the HGS algorithm's details, along with its mathematical model, will be
introduced.
2.1 Logic of search, behavioural choice, and hunger-driven games
Animal follows their sensory info based on some computational rules and in interaction
with their environment (as a part of their environment) that these rules make the basis of
their decisions and choices and support for the evolution of their cognitive architecture. It is
verified that these computationally logical rules utilized by animals will often be adaptive
evolutionary by securing higher chances of survival, reproduction, and food acquisition . Hunger is responsible for one of the most crucial homeostatic motivations and reasons
for behaviours, decisions, and actions in the life of animals. In spite of the wide variety of
stimuli and competing demands that always and certainly impinge upon the quality of life of
animals, they should select and pursue food sources when they face caloric insufficiency. To
deal with this homeostatic imbalance, they must regularly search for food and move around
their surroundings in ways that need switching between exploratory, defensive, and
competing activities, indicating incredible smoothness in feeding strategies . For any animal, neuroscientists agree that the hunger3 is a strong motivating
force for activity, learning, and searching for food and it acts as a force toward changing the
life condition to a more stable state4 . Hunger can surpass and
influence competing drives states such as thirstiness, nervousness, fear of hunters, and
communal requirements, according to experiments in "Hunger-Driven Motivational State
Competition" published at Neuron . Hence, neuroscientists discovered that
hunger possibly is at the top of the motivation hierarchy5. Hunger also trumps communal
desires for animals when they can find the food and consume it .
Social life helps animals to avoid predators and find food sources, both other animals and
vegetables, as they work in natural collaboration, which enhances their chance of survival.
This is the nature of evolution, whereby healthier animals can find sources of food better and
have a greater chance of survival over weaker animals. This can be termed as a hunger games
in nature. Any wrong decision may change the game's outcome, leading to the death of an
individual or even extinction of an entire species. For example, after hunting, ravens and rats
tell their companions that their next meal reduces the uncertainty of their next meal . The daily behaviour of animals is highly influenced by some
motivational situations, such as hunger and nervousness of being killed by hunters . Hunger is a characteristic of ‚Äúnot eating‚Äù for a long time , whereby the stronger the hunger, the stronger the craving for food,
and the more active the organism will be in searching for food in a short time before it gets
too late and causes starvation or death . Otherwise, the chance
of survival will be too low, and the animal dies. Hence, when the source of food is limited,
there is a logical game between hungry animals to find the source of food and win the
situation . The game is thus based on the logical decisions
and motions of species.
2.2 Mathematical model
In this sub-section, the mathematical model and proposed HGS method are described in
detail. Please note that we are constrained to build a mathematical model according to the
hunger-driven activities and behavioural choice, and it should be as simple as possible and at
the same time, most efficient performance.
2.2.1 Approach food
Social animals often cooperate with each other during foraging, but the possibility that a
few individuals do not participate in the collaboration is not excluded 
4 To read more about the motivations, preferences and choices of animals, interested readers can read more at:
 
5 For more info and learning, interested readers can watch a supplementary video at
 or 
The following game instructions represent the central equation of the HGS algorithm for
individual cooperative communication and foraging behaviour:
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó =
ùê∫ùëéùëöùëí1: ùëã(ùë°)
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó ‚àô(1 + ùëüùëéùëõùëëùëõ(1)), ùëü1 < ùëô
‚Éó‚Éó‚Éó‚Éó + ùëÖ‚Éó ‚àôùëä2
‚Éó‚Éó‚Éó‚Éó‚Éó ‚àô|ùëãùëè
‚Éó‚Éó‚Éó‚Éó ‚àíùëã(ùë°)
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó | , ùëü1 > ùëô, ùëü2 > ùê∏
‚Éó‚Éó‚Éó‚Éó ‚àíùëÖ‚Éó ‚àôùëä2
‚Éó‚Éó‚Éó‚Éó‚Éó ‚àô|ùëãùëè
‚Éó‚Éó‚Éó‚Éó ‚àíùëã(ùë°)
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó | , ùëü1 > ùëô, ùëü2 < ùê∏
where ùëÖ‚Éó is in the range of [‚àíùëé, ùëé];
ùëü1 and ùëü2 represent two random numbers, which are in the range of ;
ùëüùëéùëõùëëùëõ(1) is a random number satisfying normal distribution;
ùë° indicates the current iterations;
‚Éó‚Éó‚Éó‚Éó‚Éó and ùëä2
‚Éó‚Éó‚Éó‚Éó‚Éó represent the weights of hunger; which we designed them based on the fact of
hunger-driven signals ;
‚Éó‚Éó‚Éó‚Éó represents the location of the best individual of this iteration;
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó represents each individual's location;
the value of l will be discussed in the parameter setting experiment, and it is a parameter
which is designed to improve the algorithm.
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó ‚àô(1 + ùëüùëéùëõùëëùëõ(1)) represents how an agent can search for food hungrily and randomly
at the current location;
‚Éó‚Éó‚Éó‚Éó ‚àíùëã(ùë°)
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó | models the range of activity of the current individual in the current time and it is
multiplied by ùëä2
‚Éó‚Éó‚Éó‚Éó‚Éó to affect the influence of hunger on the range of activity. Since an individual
will stop searching when it be not hungry anymore, ùëÖ‚Éó is a ranging controller added to limit
the range of activity, in which the range of ùëÖ‚Éó is gradually reduced to 0. Adding or subtracting
the range of activity based on ùëä1
‚Éó‚Éó‚Éó‚Éó simulates the current individual informed by its peers
when arriving at the food location, and then searching for food again at the current location
after the acquisition of the food. ùëä1
‚Éó‚Éó‚Éó‚Éó‚Éó is introduced as the error in grasping the actual position
in reality. The definition of the formula of ùê∏, which is a variation control for all positions, is
as follows:
ùê∏= sech(|ùêπ(ùëñ) ‚àíùêµùêπ|)
where ùëñ‚àà1,2,‚Ä¶ ,ùëõ, ùêπ(ùëñ) represents the fitness value of each individual;
ùêµùêπ is the best fitness obtained in the current iteration process (so far);
Sech is a hyperbolic function (sech(ùë•) =
The formula for ùëÖ‚Éó is as follows:
ùëÖ‚Éó = 2 √ó ùë†‚Ñéùëüùëñùëõùëò√ó ùëüùëéùëõùëë‚àíùë†‚Ñéùëüùëñùëõùëò
ùë†‚Ñéùëüùëñùëõùëò= 2 √ó (1 ‚àíùë°
where ùëüùëéùëõùëë is a random number in the range of ; and ùëá stands for the maximum number
of iterations.
Figure 2 displays the process of searching and logic of HGS in the spaces based on the
rule in Eq. (2.1).
As can be seen in the graph, the search directions can be divided into two categories
according to the classification of source points:
1. Search on the basis of ùëø‚Éó‚Éó : The first game instruction simulates the self-dependent one,
which it has no teamwork spirit, and not involved in the cooperation phase and just
wants to search for food hungrily.
2. Search on the basis of ùëøùíÉ
‚Éó‚Éó‚Éó‚Éó‚Éó : The second game instruction is closely related to the
variables ùëÖ‚Éó Ôºåùëä1
‚Éó‚Éó‚Éó‚Éó‚Éó and ùëä2
‚Éó‚Éó‚Éó‚Éó‚Éó . By a refinement of these three factors, the individual's
position can be evolved within the feature space. This method simulates the cooperation
between several entities when they search for food.
The laws or rules in Eq. (2.1) allows individuals to explore possible locations near the optimal
solution and locations far away from the optimal solution, which guarantees the search of all
locations inside the boundaries of solution space to a certain extent (diversification). The
same concept can also be applied to high-dimensional search space.
Figure 2 The logic of Hunger Games Search (HGS) algorithm during optimization.
2.2.2 Hunger role
In this part, the starvation characteristics of individuals in search are simulated by a
proposed model, mathematically.
The formula of ùëä1
‚Éó‚Éó‚Éó‚Éó‚Éó in Eq. (2.1) is as follows:
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó = {‚Ñéùë¢ùëõùëîùëüùë¶(ùëñ) ‚àô
ùëÜùêªùë¢ùëõùëîùëüùë¶√ó ùëü4, ùëü3 < ùëô
The formula of ùëä2
‚Éó‚Éó‚Éó‚Éó‚Éó in Eq. (2.1) is shown as follows:
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó = (1 ‚àíùëíùë•ùëù(‚àí|‚Ñéùë¢ùëõùëîùëüùë¶(ùëñ) ‚àíùëÜùêªùë¢ùëõùëîùëüùë¶|)) √ó ùëü5 √ó 2
where ‚Ñéùë¢ùëõùëîùëüùë¶ represents the hunger of each individual;
ùëÅ represents the number of individuals;
ùëÜùêªùë¢ùëõùëîùëüùë¶ is the sum of hungry feelings of all individuals, that is ùë†ùë¢ùëö(‚Ñéùë¢ùëõùëîùëüùë¶);
ùëü3, ùëü4 and ùëü5 are random numbers in the range of .
The formula for ‚Ñéùë¢ùëõùëîùëüùë¶(ùëñ) is provided below:
‚Ñéùë¢ùëõùëîùëüùë¶(ùëñ) = {0, ùê¥ùëôùëôùêπùëñùë°ùëõùëíùë†ùë†(ùëñ) == ùêµùêπ
‚Ñéùë¢ùëõùëîùëüùë¶(ùëñ) + ùêª, ùê¥ùëôùëôùêπùëñùë°ùëõùëíùë†ùë†(ùëñ)! = ùêµùêπ
where ùê¥ùëôùëôùêπùëñùë°ùëõùëíùë†ùë†(ùëñ) preserves the fitness of each individual in the current iteration. In each
iteration, the best individual's hunger was set to 0. For other individuals, a new hunger (ùêª) is
added based on the original hunger. Hence, we understand that the corresponding H of
different individuals will be different.
The formula for ùêª can be seen as follows:
ùëáùêª= ùêπ(ùëñ) ‚àíùêµùêπ
ùëäùêπ‚àíùêµùêπ√ó ùëü6 √ó 2 √ó (ùëàùêµ‚àíùêøùêµ)
ùêª= {ùêøùêª√ó (1 + ùëü), ùëáùêª< ùêøùêª
where ùëü6 is a random number in the range of ;
ùêπ(ùëñ) represents the fitness value of each individual;
ùêµùêπ is the best fitness obtained in the current iteration process (so far);
WF stands for the worst fitness obtained in the current iteration process (so far);
ùëàùêµ and ùêøùêµ indicate the upper and lower bounds of the feature space, respectively.
The hunger sensation ùêª is limited to a lower bound, ùêøùêª, In
order to make the algorithm get a better performance, we control the upper and lower limits
of hunger, and the value of ùêøùêª will be discussed in the parameter setting experiment.
Since hunger may have both positive and negative effects on the range of activity, ùëä1
‚Éó‚Éó‚Éó‚Éó‚Éó are simulated.
In Eq. (2.8), the difference between UB and LB is used to reflect the individual's greatest
hunger in different environments;
ùêπ(ùëñ) ‚àíùêµùêπ represents the amount of food that an individual still needs not to be hungry
anymore; In every iteration, the hunger of individual will change.
ùëäùêπ‚àíùê∑ùêπ yields the total foraging capacity of an individual in the current process;
ùëäùêπ‚àíùê∑ùêπ represents the hunger ratio;
ùëü6 √ó 2 gives the positive or negative effects of environmental factors on hunger.
While the HGS algorithm proposed can yield the commonness of social animals, it still
has much room for improvement. For instance, the algorithm can be improved by mapping
it according to a living organism and integrating the living organism's unique characteristics.
It can also be improved by adding other mechanisms. We simplify the algorithm as much as
possible to maximize its scalability.
Algorithm 1 shows the pseudo-code of the proposed Hunger Games Search. Also, the
flowchart is represented in Figure 3.
Algorithm 1 Pseudo-code of Hunger Games Search (HGS)
Initialize the parameters ùëÅ, ùëá, ùëô,ùê∑,ùëÜùêªùë¢ùëõùëîùëüùë¶
Initialize the positions of Individuals ùëãùëñ(ùëñ= 1,2,‚Ä¶ ,ùëÅ)
While (ùë°‚â§ùëá)
Calculate the fitness of all Individuals
ùëàùëùùëëùëéùë°ùëí ùêµùêπ,ùëäùêπ, ùëãùëè,ùêµùêº
Calculate the ùêªùë¢ùëõùëîùëüùë¶ by Eq. (2.7)
Calculate the ùëä1 by Eq. (2.5)
Calculate the ùëä2 by Eq. (2.6)
For ùëíùëéùëê‚Ñé ùêºùëõùëëùëñùë£ùëñùëëùë¢ùëéùëôùë†
Calculate ùê∏ by Eq. (2.2)
ùëàùëùùëëùëéùë°ùëí ùëÖ by Eq. (2.3)
ùëàùëùùëëùëéùë°ùëí ùëùùëúùë†ùëñùë°ùëñùëúùëõùë† ùëèùë¶ ùêÑùê™.(2.1)
Return ùêµùêπ, ùëãùëè
Figure 3 Flowchart of Hunger Games Search (HGS) algorithm.
2.3 Theoretical and structural qualities of the Hunger Games Search
As a gradient-free, population-based optimizer, the proposed HGS exhibits efficient performance due
to the following unique advantages:
It is a population-based method with stochastic switching elements that enrich its main exploratory and
exploitative behaviours and flexibility of HGS in dealing with challenging problem landscapes.
The adaptive and time-varying mechanisms of HGS allow this method to handle multi-modality, and
local optima problems more effectively.
The consideration of hunger ratio and influence of hunger on the range of activity make the HGS more
flexible and capable of changing the performance in a fitness-wise fashion.
The application of individual fitness values enables HGS to consider historical info if it is required to
change the behaviour.
Parameters ùëô and ùê∏ assist HGS in evolving the initial positions and search mode to ensure the
exploration of the whole solution space as far as possible and enhance the diversification capacity of
the algorithm to a great extent.
The hunger weights ùëä1
‚Éó‚Éó‚Éó‚Éó‚Éó and ùëä2
‚Éó‚Éó‚Éó‚Éó‚Éó increase the perturbation of HGS during the search process and
prevent the algorithm from trapping in a local optimum.
The parameter ùëÖ‚Éó ensures that the search step of HGS is reduced at a specific rate; therefore satisfying
the need to explore the target solution space in a broad range in the early stage and exploit the depth of
the target search basin in the later stages.
The Hunger Games Search can evolve the search agents with regards to best solutions (Xb) and normal
solutions (X), which is a simple idea to ensure more exploration patterns and more coverage on the
hidden areas of the feature space.
The structure and logic of Hunger Games Search are straightforward, and it is easy to be integrated with
other evolutionary mechanisms for dealing with new practical problems in science and engineering.
Despite the simple equations and compared to the existing methods, the Hunger Games Search has a
very superior performance with high-quality results compared to well-known basic and advanced
methods for studied benchmark problems.
The codes of Hunger Games Search will be publicly available in different languages, and users can easily
access the software codes and apply it to their target problem based on functional programming.
An online, public web service at will be responsible for all users
regarding any assistance and required supplementary material.
2.4 Computational complexity analysis
The proposed Hunger Games Search mainly includes the following parts: initialization, fitness
evaluation, sorting, hunger updating, weight updating, and location updating. In the associated formulas, ùëÅ
indicates the number of individuals in the population, ùê∑ is the dimension of the problem, and ùëá represents
the maximum quantity of iterations. During the initial stage, the computational complexity of fitness
evaluation and hunger update are both ùõ∞(ùëÅ), the computational complexity of sorting is ùõ∞(ùëÅùëôùëúùëîùëÅ), and
the computational complexity of weight and location update is ùõ∞(ùëÅ√ó ùê∑). From the above analysis, we can
acquire the complexity of the whole algorithm: ùõ∞(ùëÅ‚àó(1 + ùëá‚àóùëÅ‚àó(2 + ùëôùëúùëîùëÅ+ 2 ‚àóùê∑))).
3 Experiments and results
In this chapter, the proposed HGS algorithm is compared against some well-established counterparts.
All experiments were conducted on a Windows Server 2008 R2 operating system with Intel (R) Xeon (R)
CPU E5-2650 v4 (2.20‚ÄØGHz) and 128‚ÄØGB of RAM. All algorithms were coded in the MATLAB R2014b
for a fair comparison.
3.1 Qualitative analysis
Figure 4 shows the qualitative analysis of 23 well-known benchmark functions using the HGS algorithm,
which includes the search history, trajectory of the first individual, average fitness of all individuals, and
convergence behaviour. The search history shows the location and distribution of individuals in each
iteration. The first individual's trajectory reveals the motion patterns of the first individual in the whole
iteration process. The average fitness of all individuals monitors how the average fitness of the entire
population changes during optimization. The convergence behaviour reveals the changing trend of optimal
fitness and indirectly shows how well exploratory trends change to exploitative drifts.
By observing the individual's historical position, we can first observe that the individual has explored
major portions of the search space, showcasing that the algorithm has a strong search ability and can avoid
falling into a locally optimal solution. Simultaneously, we also see that most of the search locations are
around the optimal solution, which indicates that the algorithm can accurately progress in the target area,
and the convergence speed is fast. The algorithm has a good measurement between the two phases of
exploration and exploration, in which we can detect the advantages of HGS.
The trajectory graph shows that individuals have strong fluctuations in the initial stage of the search, and
the range of fluctuation coverage exceeds 50% of the solution space. This proves that the search ability of
the HGS algorithm is very strong, and it can focus on high-quality solutions. As the number of iterations
increases, the fluctuation tends to be more stable, which indicates that the algorithm has found a promising
region, and it is still exploring the region. For some functions, such as F7 and F8, it is apparent that the
fluctuation tends to stabilize and then oscillation occurs, meaning that the HGS algorithm can jump out of
local optimum and avoid falling into local optima, which is also a validation of the balanced performance
of the proposed algorithm.
The algorithm tends to converge very quickly in the early stages of iteration by monitoring the overall
average fitness. Although the downward trend slows down with the iteration and is accompanied by
variations, the average fitness gradually decreases, reflecting the well-prepared search and high searching
capabilities of the algorithm. The convergence curve reveals the convergence speed of the algorithm and
the time point of conversion between exploration and exploration. The convergence curves show that HGS
can demonstrate a fast tendency in dealing with F8-F10, and there is no stagnation problem.
Figure 4 Qualitative analysis of HGS on some typical functions
3.2 Validation on commonly used benchmark functions
In this part, we tested the proposed HGS algorithm on 23 benchmark unimodal and multi-modal
functions. Details of these 23 functions can be found in Table 1, where Dim denotes the dimensions of the
functions, Range refers to the definition domain of the function, and ùëìùëöùëñùëõ reveals the optimal solution of
the function.
One point is so critical in the verification of computational intelligence methods, and it is the detailed
report of the used parameters for a fair, justifiable comparative analysis and the same conditions of test
 . This matter is to ensure the results of any kind of algorithm
are gathered in the same condition and with no bias toward any specific method that used a better testing
condition, as it followed by reference literature as well .
For the experimental results' credibility, all experiments were conducted under the same conditions:
population size was set to 30; and maximum iterations and dimensions were set to 1000 and 30, respectively.
At the same time, to exclude the influence of random factors, we tested each algorithm 30 times. For this
paper, the Friedman test and the Wilcoxon sign-rank test were applied to identify the algorithms' significant differences. The
Friedman test is a non-parametric statistical program that allows us to perform further analysis through the
algorithm's average performance ranking. The Wilcoxon sign-rank test is used as a statistical significance
test, where a p-value lower than 0.05 reveals that HGS performs significantly better than its competitors.
Table 1 Description of the 23 benchmark functions
Function Equation
[-100,100]
[-100,100]
ùëì4(ùë•) = ùëöùëéùë•ùëñ{|ùë•ùëñ|,1 ‚â§ùëñ‚â§ùëõ}
[-100,100]
[100(ùë•ùëñ+1 ‚àíùë•ùëñ
2)2 + (ùë•ùëñ‚àí1)2]
([ùë•ùëñ+ 0.5])2
[-100,100]
4 + ùëüùëéùëõùëëùëúùëö[0,1)
[-1.28,1.28]
‚àíùë•ùëñùë†ùëñùëõ (‚àö|ùë•ùëñ|)
[-500,500]
ùëì9 (ùë•) = ‚àë
2 ‚àí10 ùëêùëúùë†(2ùúãùë•ùëñ) + 10]
[-5.12,5.12]
ùëì10(ùë•) = ‚àí20 ùëíùë•ùëù{‚àí0.2‚àö
} + 20 + ùëí
[-600,600]
ùëõ{10 ùë†ùëñùëõ(ùëéùë¶1) + ‚àë
(ùë¶ùëñ‚àí1)2[1 +
10ùë†ùëñùëõ2(ùúãùë¶ùëñ+1)] + (ùë¶ùëõ‚àí1)2 +
ùúá(ùë•ùëñ,10,100,4)
ùëì13(ùë•) = 0.1{ùë†ùëñùëõ2(3ùúãùë•ùëñ) + ‚àë
(ùë•ùëñ‚àí1)2[1 +
ùë†ùëñùëõ2(3ùúãùë•ùëñ+ 1)] + (ùë•ùëõ‚àí1)2[1 +
ùë†ùëñùëõ2(2ùúãùë•ùëõ)] + ‚àë
ùúá(ùë•ùëñ,5,100,4)
3.2.1 Comparison with basic optimizers
In this part, HGS was compared with 15 other methods that can be categorized into two classes: wellestablished methods and recent methods. The recent methods6 include Sine Cosine Algorithm (SCA)
 , Salp Swarm Algorithm (SSA) , Grey Wolf Optimizer (GWO) , Moth-flame Optimization (MFO) , Whale Optimization
Algorithm (WOA) , Grasshopper Optimization Algorithm (GOA) , Dragonfly Algorithm (DA) , Ant Lion Optimizer (ALO)
 , and Multi-Verse Optimizer (MVO) . The
well-established methods include Biogeography-based Optimization (BBO) , Particle Swarm
Optimization (PSO) , Differential Evolution (DE) ,
Firefly Algorithm (FA) , Bat Algorithm (BA)7 , and Flower Pollination
Algorithm (FPA) . For complete descriptions of those
ùëì15(ùë•) = ‚àë
2 + ùëèùëñùë•3 + ùë•4
ùëì16(ùë•) = 4ùë•1
6 + ùë•1ùë•2 ‚àí4ùë•2
ùëì17(ùë•) = (ùë•2 ‚àí
8ùúã) ùëêùëúùë†ùë•1 + 10
ùëì18(ùë•) = [1 + (ùë•1 + ùë•2 + 1)2(19 ‚àí14ùë•1
2 ‚àí14ùë•2+ 6ùë•1ùë•2
√ó [30 + (2ùë•1 ‚àí3ùë•2)2 √ó (18 ‚àí32ùë•1 + 12ùë•1
+ 48ùë•2 ‚àí36ùë•1ùë•2 + 27ùë•2
ùëì19 (ùë•) = ‚àí‚àë
ùëì20(ùë•) = ‚àí‚àë
ùëì21(ùë•) = ‚àí‚àë
[(ùëã‚àíùëéùëñ)(ùëã‚àíùëéùëñ)ùëá+ ùëêùëñ]‚àí1
ùëì22(ùë•) = ‚àí‚àë
[(ùëã‚àíùëéùëñ)(ùëã‚àíùëéùëñ)ùëá+ ùëêùëñ]‚àí1
ùëì23(ùë•) = ‚àí‚àë
[(ùëã‚àíùëéùëñ)(ùëã‚àíùëéùëñ)ùëá+ ùëêùëñ]‚àí1
methods, please refer to the original research. The parameter settings of these algorithms are shown in
Table 2 Parameter settings of the involved MAs
Parameter settings
Wellestablished
ùëíùëôùëñùë°ùëñùë†ùëö= 2; ùúÜùëôùëúùë§ùëíùëü= 0; ùúÜùë¢ùëùùëùùëíùëü= 1; ùë†ùë°ùëíùëù ùë†ùëñùëßùëí= 1
ùëê1 = 2; ùëê2 = 2; ùë£ùëÄùëéùë•= 6; ùë§= 1
ùë†ùëêùëéùëôùëñùëõùëî ùëìùëéùëêùë°ùëúùëü= 0.5; ùëêùëüùëúùë†ùë†ùëúùë£ùëíùëü ùëùùëüùëúùëèùëéùëèùëñùëôùëñùë°ùë¶= 0.5
ùõº= 0.5; ùõΩ= 0.2; ùõæ= 1
ùê¥= 0.5; ùëü= 0.5
ùëà~ùëÅ(0,ùúé2); ùëâ~ùëÅ(0,1);ùëù= 0.5
ùëê1 ‚àà[0 1]; ùëê2 ‚àà[0 1];
ùëè= 1; ùë°= [‚àí1,1]; ùëé‚àà[‚àí1,‚àí2]
ùëé1 = ; ùëé2 = [‚àí2,‚àí1]; ùëè= 1
ùëêùëöùëéùë•= 1; ùëêùëöùëñùëõ= 0.00001; ùëè= 1
ùëíùë•ùëñùë†ùë°ùëíùëõùëêùëí ùëùùëüùëúùëèùëéùëèùëñùëôùëñùë°ùë¶‚àà[0.2 1]; ùë°ùëüùëéùë£ùëíùëôùëôùëñùëõùëî ùëëùëñùë†ùë°ùëéùëõùëêùëí ùëüùëéùë°ùëí‚àà[0.6 1]
The data in Table 3 represent the results of comparing HGS with other traditional MAs, where "+",
"‚àí" and "=" indicate that HGS performs better, worse, and equal to the corresponding algorithm,
respectively. Avg, which is the average ranking result of the algorithm, is based on the Freidman test. From
the table, we can intuitively find that HGS ranks first. For each opponent, it is difficult to defeat HGS on
most 23 benchmark functions. Although DE defeats HGS in the largest number of functions, only five,
and other algorithms do not even defeat HGS on anyone. The average value of our method is only 2.17,
which is much smaller than other algorithms. Compared with the second-ranked DE, the average value of
HGS is about half of DE. We can conclude that the performance of HGS is superior to the other
counterparts.
Table A.1 in Appendix A shows the consequences of the Wilcoxon sign-rank test performed by HGS
and other algorithms. Most p-values are less than 0.05, accounting for 93.0% of all data. Even in SCA, PSO,
BA, and FPA, all p-values are less than 0.05. Although the numbers of p-values that are higher than 0.05
are the largest on DE and MFO, there are only five cases. This fact further shows that HGS has a strong
statistical significance compared to the other methods.
Table 3 Comparison results of HGS algorithm on the 23 benchmark functions with traditional algorithms
Figure 5 Comparisons between HGS and traditional MAs.
Inspecting the results in Figure 5 shows that the convergence rate of HGS is fast. From F1 to F4, we can
see that HGS converges the fastest among all the algorithms, other algorithms converge quite slowly, and
some even fall into local optimum. F5 and F9-F11 indicate that HGS has high accuracy in solving problems
and can quickly find the global optimum at the beginning of the iteration. Although some algorithms'
convergence speed is also very competitive in some stages, the accuracy of the solution of those methods
is not as high as that of HGS, and the solution found by HGS has a higher quality. Based on the results of
F7 and F8, the convergence speed of HGS slows down, but it still finds the global optimum first compared
to the other algorithms. Some algorithms even fall into local optimum at the beginning of the iterations.
Observing the performance algorithms on F22 and F23 functions, it can be concluded that HGS has a
strong ability for global exploration. L can effectively switch between the two modes of starvation, and LH
intuitively defines the minimum value of an individual's hunger. To prevent HGS from falling into local
optima when faced with some multimodal landscapes, both of them directly affect hunger weights, which
contribute to the improved rates of HGS in the iterative process and a better balance of the search and
discovery stages. In the search phase, the solution space can be searched as complete as possible, so that
the algorithm can achieve the effect of fast convergence at the early stage. In the mining stage, the optimal
solution can be found nearby, which ensures the accuracy of the solution.
3.2.2 Comparison with improved metaheuristic methods
To further illustrate the effectiveness of the HGS algorithm, we compared HGS with ten state-of-theart advanced algorithms: IWOA , OBWOA , ACWOA , SCADE , CGSCA , m_SCA , RCBA
 , CBA , and
CDLOBA . For the full names of these methods and complete descriptions,
please refer to the original works.
Based on the test data in Table 4, it can be recognized that HGS exhibits powerful performance on
multimodal functions, especially on fixed dimension multimodal functions. The average value, based on
Friedman test's value, is only 1.78, which is much smaller than other algorithms and is only about a fifth of
the maximum average of CDLOBA. None of the five algorithms, IWOA, CGSCA, RCBA, CBA, or
CDLOBA, can beat HGS on 23 benchmark functions. Although SCADE defeats HGS in dealing with
some functions, there are only five cases. This observation clarifies that HGS has strong optimization ability
in terms of exploration and exploitation trends. One of the effective mechanisms resulting in the improved
solutions of the proposed HGS is that it is equipped with two rates, l and E. These features assist HGS in
changing the initial positions and search mode, which ensure the in-depth exploration of the whole solution
space as far as possible and enhance the exploratory traits of the algorithm to a great extent. Also, hunger
weights can emphasize the perturbation trends of the HGS optimizer during iteration. This feature also
reduces the change in stagnation due to the existence of several local optima.
Table A.2 in Appendix A reveals the p-value of HGS and its comparisons on all test functions. From
the table, we can see that all values in CDLOBA are less than 0.05. The CGSCA, RCBA, and CBA have
only one data value greater than 0.05 at most. Although OBWOA values greater than 0.05 are the most,
there are only seven cases. We can also see from the table that the difference between values higher than
0.05 and 0.05 is not significant. These test results indicate that the HGS algorithm is significantly superior
compared to the other algorithms.
Table 4 Comparison results on the 23 benchmark functions with advanced algorithms
The convergence curves of HGS are depicted in Figure 6, which shows that the speed and accuracy of
HGS are better than its competitors. On the F4 test function, the convergence rate of HGS is relatively
constant, and the global optimal solution is found at a very fast speed during the entire process. At the same
time that HGS finds the optimal solution, some algorithms have just started to converge to some solutions.
From curves of F5 and F9-F11, it can be observed that HGS finds the optimal solution at a very fast speed
during the initial iterations, but some of the compared algorithms that have fallen into local optimum. From
these results, we can infer that the HGS has a strong ability of exploration and exploration propensities,
and the two phases have excellent stability due to the impacts of L and LH. In the search phase, both of
them expand the search scope as much as possible and ensure that the individual can search in a small range
in the mining stage.
Figure 6 Comparisons between HGS and advanced MAs.
3.3 Validation of IEEE CEC2014 functions
To further illustrate the performance of the HGS algorithm, we tested it on the IEEE CEC2014
benchmark set. The data set is divided into Unimodal Functions, Simple Multimodal Functions, Hybrid
Functions, and Composition Functions. Details of the functions can be found in Table 5. In this part, for
the reliability of the experiment, the conditions related to the test were adjusted the same as before: the
maximum number of iterations was set to 1000, the population size and dimension were set to 30, and the
involved algorithm was tested 30 times randomly on each function. The Friedman test and Wilcoxon
sign-rank test were utilized to evaluate the experimental results.
Table 5 Description of the IEEE CEC2014 functions
3.3.1 Comparison with other optimizers
The proposed HGS was compared with 12 traditional MAs on the IEEE CEC 2014 dataset, including
SCA , SSA , GWO , MFO ,
WOA , GOA , DA , ALO , PSO , DE , BA, and FPA
 . The parameter settings of the mentioned algorithms are listed in Table 2.
The detailed comparison results are listed in Table 6. We found that HGS ranks first among all
algorithms, with a much smaller Avg. HGS shows a strong ability to search for optimal solutions on most
of the functions. It is well known that DE exhibits excellent performance on contest datasets, but it only
defeats HGS on eight functions, while HGS defeats HGS on 19 functions. As a fixed-dimensional multimodal function, composition functions have a large number of local optima, which requires an algorithm
Function Equation
Unimodal Functions
Rotated High Conditioned Elliptic Function
[-100,100]
Rotated Bent Cigar Function
[-100,100]
Rotated Discus Function
[-100,100]
Simple Multimodal Functions
Shifted and Rotated Rosenbrock‚Äôs Function
[-100,100]
Shifted and Rotated Ackley‚Äôs Function
[-100,100]
Shifted and Rotated Weierstrass Function
[-100,100]
Shifted and Rotated Griewank‚Äôs Function
[-100,100]
Shifted Rastrigin‚Äôs Function
[-100,100]
Shifted and Rotated Rastrigin‚Äôs Function
[-100,100]
Shifted Schwefel‚Äôs Function
[-100,100]
Shifted and Rotated Schwefel‚Äôs Function
[-100,100]
Shifted and Rotated Katsuura Function
[-100,100]
Shifted and Rotated HappyCat Function
[-100,100]
Shifted and Rotated HGBat Function
[-100,100]
Shifted and Rotated Expanded Griewank‚Ä≤s plus
Rosenbrock‚Äôs Function
[-100,100]
Shifted and Rotated Expanded Scaffer‚Äôs F6 Function
[-100,100]
Hybrid Functions
Hybrid Function 1 (N = 3)
[-100,100]
Hybrid Function 2 (N = 3)
[-100,100]
Hybrid Function 3 (N = 4)
[-100,100]
Hybrid Function 4 (N = 4)
[-100,100]
Hybrid Function 5 (N = 5)
[-100,100]
Hybrid Function 6 (N = 5)
[-100,100]
Composition Functions
Composition function 1 (N = 5)
[-100,100]
Composition function 2 (N = 3)
[-100,100]
Composition function 3 (N = 3)
[-100,100]
Composition function 4 (N = 5)
[-100,100]
Composition function 5 (N = 5)
[-100,100]
Composition function 6 (N = 5)
[-100,100]
Composition function 7 (N = 3)
[-100,100]
Composition function 8 (N = 3)
[-100,100]
with excellent performance. HGS ranks first in the composite functions, including F23, F24, F25, F27, F28,
and F30, which shows that the overall performance of HGS is powerful so that it can perform a smoother
transition between exploration and exploration trends.
Table A.3 in Appendix A lists the p-value of HGS versus the other algorithms. Among the 360 data
sets, 318 are less than 0.05, comprising 88.1% of the total data. It is worth noting that these data sets are
far less than 0.05. Although there are more than 0.05 data in ALO, there are only 7 data sets. The number
of it in SCA and DA is even reduced to only one. This shows that HGS has statistical advantages over the
other competitive MAs.
Table 6 Comparison results on the CEC2014 functions with traditional MAs
According to the analysis of Figure 7, we see that the convergence speed of HGS in F8, F10, and F11
is fast, and the accuracy of the solution is very high. Some algorithms even fall into the local optimum in
the middle of the iteration. F23-F25, F27, F28, F30 are composite functions with a large number of local
optima. Interestingly, we can observe that the convergence speed of the HGS algorithm is superior and fast
on these types of problems. The target region can be found in the initial iteration period, which shows that
the exploratory trends of HGS are influential and can effectively avoid falling into local optimum. These
rates more intuitively show that HGS has the right sense of balance between exploration and exploration.
Composite cases can challenge the capacity of utilized methods in harmonizing the main searching phases.
The results show that HGS yields superior results and satisfactory performance. The reason for the
satisfactory efficacy of HGS is the high capacity of this method in harmonizing the diversity of solutions
and focusing on the locality of high-quality solutions in later phases. These two reasons are based on the L
and LH parameters, which weigh the change of individual search range in the process of iteration. The
HGS has a useful feature that ensures the search steps of HGS will be concentrated based on a specific
rate. This feature assists this method in exploring the solution space in-depth, while it explores the feature
space extensively during the initial stages.
Figure 7 Comparisons between HGS and traditional MAs.
3.3.2 Comparison with advanced MAs
To further prove the effectiveness of the proposed HGS, we further compared HGS with some stateof-the-art advanced algorithms on CEC2014 benchmark functions.
Table 7 shows the comparison between HGS and the advanced MAs on the CEC2014 test suite. As
shown from the results, we can intuitively see that HGS ranks first amongst ten algorithms and first on 17
functions, accounting for 56.7% of the total number of functions, concentrating on simple multimodal
functions and composition functions. From this point, we can see that HGS has excellent performance.
The average value of HGS is only 2.37, which is about half of the average value of m_SCA, which ranks
second. This indicates that the search ability of HGS is efficient, and it can avoid falling into local optimum.
Table A.4 in Appendix A lists the p-value of HGS versus the other involved MAs. Data sets less than
0.05 in Table A.4 in Appendix A accounted for 91.1% of the total, revealing that HGS has distinct statistical
advantages compared with its competitors. For IWOA, HGS has statistical significance on all functions.
Table 7 Comparison results on the CEC2014 functions with advanced algorithms
Figure 8 shows the convergence curves of the algorithms. At the beginning of the iteration, the
convergence speed of HGS is very fast. With the increase of iteration times, the convergence speed slows
down, but it is still the first one to find the optimal solution with high accuracy. F6, F8-12, and F16 show
that HGS has a distinct advantage over simple multimodal functions. F29-30 reveals that HGS can find a
better solution to composition functions with much faster convergence than the other counterparts. In the
search phase, l and LH can dynamically expand the scope of individual search with the iteration to ensure
that the algorithm can search the solution space as much as possible and can converge quickly. In the
mining stage, after finding the possible region of the optimal solution, the search scope can be reduced to
achieve the purpose of excavation and ensure the high-precision solution. The combination of these two
phases can effectively balance the search and excavation phases.
Figure 8 Comparisons between HGS and advanced MAs.
3.4 Comparisons with DE variants
This chapter compares HGS with some improved versions of DE, including MPEDE , SPS_L_SHADE_EIG ,
LSHADE_cnEpSi , SHADE , SADE , LSHADE , JDE and DE on 21 functions, which were selected from the first
13 of 23 benchmark functions and the last 8 composite functions of CEC2014 functions. All functions can
be divided into three categories: single-mode (F1- F7), multimodal (F8-F13), and composite functions (F14-
F21). In this experiment, the population size N was set to 30, the dimension of the optimization problem
D was taken as 30, the maximum evaluation number MaxFES was taken as 300000 times, and each
algorithm was executed 30 times randomly.
Table 8 shows the comparison between HGS and the improved version of DE. The results show that
the HGS algorithm ranks first among the ten algorithms and first among the 15 functions, accounting for
71.4% of the total number of functions. From this point, we can see that HGS exhibits excellent
performance with an average value of only 2.33. These results indicate that the search ability of HGS is
effective and can avoid falling into local optimum.
Table 8 Comparison results with traditional DE variants
SPS_L_SHADE_EIG
LSHADE_cnEpSi
SPS_L_SHADE_EIG
LSHADE_cnEpSi
SPS_L_SHADE_EIG
LSHADE_cnEpSi
SPS_L_SHADE_EIG
LSHADE_cnEpSi
SPS_L_SHADE_EIG
LSHADE_cnEpSi
SPS_L_SHADE_EIG
LSHADE_cnEpSi
SPS_L_SHADE_EIG
LSHADE_cnEpSi
SPS_L_SHADE_EIG
LSHADE_cnEpSi
Based on the analysis in Figure 9, we can observe that the convergence rate of HGS in F1, F2, and F11
is fast, the solution accuracy is very high, and the optimal solution is found in the early iteration stage.
Through the convergence graphs of F10, F14, F15, F18, and F19, it can be found that although all the
algorithms have fast convergence speed in the initial stage, the convergence accuracy is not as high as HGS.
On F7 and F21, HGS has high convergence accuracy and can find the global optimum.
Figure 9 Comparisons between HGS and DE variants.
3.5 Parameter sensitivity analysis
In this chapter, we analyze the parameters involved in the algorithms: population size (ùëÅ), the maximum
number of iterations (ùëá), parameter (ùëô), moreover, and hunger threshold (ùêøùêª). These parameters affect
the convergence speed and accuracy of HGS. When testing ùëô, we fixed ùêøùêª to 100, and set l to start at 0.01,
with a step of 0.01 between every two numbers, a total of 10 values. Similarly, when we analyzed ùêøùêª, we
initialized ùëô to 0.08 and ùêøùêª as 10, 100, 1000, and 1000. When testing ùëô and ùêøùêª, ùëÅ and ùëá were set to 30 and
1000, respectively, and remain unchanged. Each algorithm was tested 30 times. All experiments were
conducted on 23 well-regarded benchmark functions.
The comparison results of the different values for parameter ùëô are found in Table 9. From the table, we
see that ùëô has a significant influence on the performance of the algorithm. In the experiment, when ùëô was
0.08, the performance is the best. Also, the maximum difference between the average values can reach 2.66.
The average value of 0.01 is about 1.88 times that of 0.08.
Table 10 presents the comparison of different values of ùêøùêª. Of the four values in this experiment,
ùêøùêª ranked first when ùêøùêª was 10000. Nevertheless, the influence of ùêøùêª is less exaggerated than that of ùëô.
From the above results, we can draw the following conclusions: L and LH's values have a certain impact
on the search ability and solution accuracy of HGS. The balance between the two stages of exploration and
exploration is closely related to these two parameters. Readers can set values for both variables according
to specific conditions.
Table 9 Ranking of results with different values of parameter ùëô
ùëô=0.01 ùëô=0.02 ùëô=0.03 ùëô=0.04 ùëô=0.05 ùëô=0.06 ùëô=0.07 ùëô=0.08 ùëô=0.09 ùëô=0.1
Table 10 Ranking of results with different values of parameter ùêøùêª
When testing the influence of ùëÅ and ùëá on HGS, we use F13 in 23 benchmark functions as the test
examples. Note that ùëÅ was set to 5, 10, 30, 50, 100, and 200, respectively, and ùëá was initialized to 50,100,
200, 500, 1000 and 2000. The test results can be visually observed in Figure 10. The increase of ùëÅ and ùëá
will improve the solution accuracy of HGS, but after reaching a certain level, this effect will become
minimal. Given the long-time consumed when the value is too large, and the unsatisfactory experimental
results are too small, the user can set it according to the experiment's actual needs.
Figure 10 The influence of ùëÅ and ùëá (it is shown by Max_iter in the above plot)
3.6 Experiments on engineering design problems
It is well known that there are many constraints in practical problems. In dealing with engineering
scenarios, there is one main difference with global benchmark cases, and there is a concern on how to
restrictions
constraints
minimization/maximization of the objective function . Therefore, we further evaluated the efficiency of HGS by applying
it to engineering problems. Several constraint handling methods were considered, including the death
penalty, annealing, static, dynamic, co-evolutionary, and adaptive . When searching
individuals violate any constraints, the method assigns a large objective function value to them. In the
optimization of the heuristic algorithm, this method will help to eliminate infeasible solutions automatically,
so it is not necessary to calculate this scheme's infeasibility. The death penalty's most prominent advantages
are simplicity and low time consumption .
In this work, HGS was tested on four engineering constraints: welded beam, I-beam, and multiple disk
clutch brake.
3.6.1 Welded beam design problem
The welded beam design problem aims to find the lowest consumption of welded beams under the four
constraints of shear stress (ùúè), bending stress (ùúÉ), buckling load (ùëÉùëê) and deflection (ùõø) . The problem involves the following four variables: welding seam thickness (‚Ñé); welding joint
length (ùëô); beam width (ùë°); beam thickness (ùëè). The mathematical model is as follows:
Consider ùë•‚Üí= [ùë•1,ùë•2,ùë•3,ùë•4] = [‚Ñé ùëô ùë° ùëè]
Minimize ùëì( ùë•‚Üí) = 1.10471ùë•1
2 + 0.04811ùë•3ùë•4(14.0 + ùë•4)
Subject to ùëî1( ùë•‚Üí) = ùúè( ùë•‚Üí) ‚àíùúèùëöùëéùë•‚â§0
ùëî2( ùë•‚Üí) = ùúé( ùë•‚Üí) ‚àíùúéùëöùëéùë•‚â§0
ùëî3( ùë•‚Üí) = ùõø( ùë•‚Üí) ‚àíùõøùëöùëéùë•‚â§0
ùëî4( ùë•‚Üí) = ùë•1 ‚àíùë•4 ‚â§0
ùëî5( ùë•‚Üí) = ùëÉ‚àíùëÉùê∂( ùë•‚Üí) ‚â§0
ùëî6( ùë•‚Üí) = 0.125 ‚àíùë•1 ‚â§0
ùëî7( ùë•‚Üí) = 1.10471ùë•1
2 + 0.04811ùë•3ùë•4(14.0+ ùë•2)‚àí5.0 ‚â§0
Variable range 0.1 ‚â§ùë•1 ‚â§2, 0.1 ‚â§ùë•2 ‚â§10, 0.1 ‚â§ùë•3 ‚â§10, 0.1 ‚â§ùë•4 ‚â§2
where ùúè( ùë•‚Üí) = ‚àö(ùúè‚Ä≤)2 + 2ùúè‚Ä≤ùúè‚Ä≤‚Ä≤ ùë•2
2ùëÖ+ (ùúè‚Ä≤‚Ä≤)2 ùúè‚Ä≤ =
‚àö2ùë•1ùë•2 ùúè‚Ä≤‚Ä≤ =
ùêΩ= 2 {‚àö2ùë•1ùë•2[ùë•22
ùë•4ùë•32Ôºåùõø( ùë•‚Üí) =
4.013ùê∏‚àöùë•32ùë•46
ùëÉ= 60001ùëèÔºåùêø= 14 ‚àà..ùõøùëöùëéùë•= 0.25 ‚àà..
ùê∏= 30 √ó 16ùëùùë†ùëñÔºåùê∫= 12 √ó 106ùëùùë†ùëñ
ùúèùëöùëéùë•= 13600ùëùùë†ùëñÔºåùúéùëöùëéùë•= 30000ùëùùë†ùëñ
On this subject, HGS was compared with HS and CBO .
The results in Table 11 show that HGS has the best performance and solution. The main reason is the
satisfactory performance of this method in harmonizing the diversity of solutions and later focusing on the
locality of high-quality variable sets inside a constrained feature space.
Table 11 Results of welded beam design problem compared with other methods
Optimal values for variables
HS 
3.6.2 I-beam design problem
The goal of this problem is to decrease the vertical deflection of the I-beams based on related parameters,
including are length, height, and two thicknesses. The mathematical model of the problem is as follows:
Consider ùë• = [ùë•1,ùë•2,ùë•3,ùë•4] = [ùëè ‚Ñé ùë°ùë§ ùë°ùëì]
Objective ùëì(ùë• )ùëöùëñùëõ=
ùë°ùë§(‚Ñé‚àí2ùë°ùëì)3
Subject to g(ùë• ) = 2bùë°ùë§+ ùë°ùë§(h‚àí2ùë°ùëì) ‚â§0
Variable range 10 ‚â§ùë•1 ‚â§50
10 ‚â§ùë•2 ‚â§80
0.9 ‚â§ùë•3 ‚â§5
0.9 ‚â§ùë•4 ‚â§5
Table 12 presents the comparisons between HGS and ARSM IARSM , CS , and SOS on the I-beam problem.
From the table, we see that HGS minimizes the vertical deflection of the I-beam more than the other four
algorithms, demonstrating its superior efficacy for this engineering problem.
Table 12 Results of I-beam design problem compared with other methods
Optimal values for variables
ARSM 
IARSM 
CS 
SOS 
3.6.3 Multiple disk clutch brake
The objective of this minimization problem, categorized as a discrete optimization problem, is to use
five discrete design variables to minimize the quality of multi-disc clutch brakes. The five variables are
actuating force, inner and outer radius, number of 27 friction surfaces, and thickness of discs. The
mathematical model for this problem is as follows:
ùëì(ùë•) = Œ†(ùëü0
2)ùë°(ùëç+ 1)œÅ
subject to:
g1(ùë•) = ùëü0 ‚àíùëüùëñ‚àí‚àÜùëü‚â•0
g2(ùë•) = ùëôùëöùëéùë•‚àí(ùëç+ 1)(ùë°+ ùõø) ‚â•0
g3(ùë•) = ùëÉùëöùëéùë•‚àíùëÉùëüùëß‚â•0
g4(ùë•) = ùëÉùëöùëéùë•ùë£ùë†ùëüùëöùëéùë•‚àíùëÉùëüùëßùë£ùë†ùëü‚â•0
g5(ùë•) = ùë£ùë†ùëüùëöùëéùë•‚àíùë£ùë†ùëü‚â•0
g6 = ùëáùëöùëéùë•‚àíùëá‚â•0
g7(ùë•) = ùëÄ‚Ñé‚àíùë†ùëÄùë†‚â•0
g8(ùë•) = ùëá‚â•0
2ùõ±ùëõ , PVS , and TLBO . Details of the comparison results can be found in Table 13. From the table, we can see that
the quality of HGS is far less than that of other algorithms, which shows that HGS has better vital
optimization ability and can find more high-quality solutions.
Table 13 Results of Multiple disk clutch brake compared with other methods
Optimal cost
WCA 
PVS 
4 Conclusions and future perspectives
This study presents a novel population-based model to tackle optimization problems based on social
animals' characteristics in searching for food. More specifically, in each iteration, the algorithm searches
around the optimal location, in the same manner, that animals forage, where the weights, or hunger values,
mimic the impact of hunger on an animal‚Äôs individual activity. Qualitative analysis of the algorithm was
carried out using four indicators, including search history, the trajectory of the first dimension, average
fitness, and convergence curve. The proposed Hunger Games Search (HGS) was validated on a
comprehensive collection of 23 benchmark functions and IEEE CEC2014 functions. The Wilcoxon sign
rank test and the Freidman test were utilized to assess the statistical significance between HGS and other
well-known MAs. The experimental results show that HGS has an efficient searching ability compared with
other algorithms, and it can quickly find and develop the target solution space. Overall, HGS is very good
at balancing exploration and exploration. Simultaneously, to confirm the applicability of HGS to practical
problems, four engineering problems were considered, including welded beam, I-beam, and multiple disk
clutch brake. From the experimental results, HGS can satisfy the optimization effect of production
engineering problems and significantly reduce manufacturing costs.
In this paper, we followed the most straightforward rules for developing HGS to make it easier to expand
and integrate with existing artificial intelligence methods. There are many windows for the future directions
of this new efficient HGS algorithm. First, researchers can investigate the effectiveness of this open-source
HGS code for solving real-world problems in parameter optimization for machine learning models, binary
feature selection, and image segmentation. Another window is how to enhance the performance of the
basic version proposed in this research. Possible chances and future proposals are the application of
oppositional based learning (OBL), Orthogonal learning (OL), chaotic signals instead of random variables,
applying evolutionary population dynamic (EPD), advantages of mutation and crossover on the exploration
and exploitation cores of the method, ensemble mutation-based strategies, role of levy flight, application
of greedy search, co-evolutionary methods, quantum computing, parallel computing, ranking-based
schemes, random spare schemes, multi-population structures, dimension-wise operations, and their various
combination8. Lastly, the proposed Hunger Games Search is a single-objective approach in the currently
released version, and the next task can be to develop the binary, multiobjective and many-objective variants
of the developed Hunger Games Search to deal with more variety of multiobjective, binary, and manyobjective problems.
Acknowledgements
This research is supported by the National Natural Science Foundation of China (62076185, U1809209).
We acknowledge the comments the anonymous reviewers and respected editor that enhanced the quality
of this research. We also acknowledge Seyedali Mirjalili for reviewing and editing the first version of the
manuscript.
8 Literature and guidance on most of these mechanisms are publicly available at and
 
Appendix A
Tables A.1-A.4 Describing the corresponding p-values of the four experiments
Table A.1 The p-value of the Wilcoxon test obtained from comparison with traditional algorithms on 23 benchmark
Table A.1(continued) The p-value of Wilcoxon test obtained from comparison with traditional algorithms on 23
benchmark functions
Table A.2 The p-value of Wilcoxon test obtained from comparison with advanced algorithms on 23 benchmark
Table A.3 P-value of Wilcoxon test obtained from HGS versus other traditional algorithms on IEEE CEC2014
Table A.3(continued) The p-value of Wilcoxon test obtained from HGS versus other traditional algorithms on
IEEE CEC2014 functions
Table A.4 P-value of Wilcoxon test obtained from comparison with advanced algorithms on IEEE CEC2014