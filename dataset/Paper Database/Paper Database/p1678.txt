Springer Texts in Statistics
George Casella
Stephen Fienberg
Ingram Olkin
springer Texts in Statistics
Alfred: Elements of Statistics for the Life and Social Sciences
Berger: An Introduction to Probability and Stochastic Processes
Bilodeau and Brenner: Theory of Multivariate Statistics
Blom: Probability and Statistics: Theory and Applications
Brockwell and Davis: Introduction to Times Series and Forecasting, Second
Carmona: Statistical Analysis of Financial Data in S-Plus
Chow and Teicher: Probability Theory: Independence, Interchangeability,
Martingales, Third Edition
Christensen: Advanced Linear Modeling: Multivariate, Time Series, and
Spatial Data—Nonparametric Regression and Response Surface
Maximization, Second Edition
Christensen: Log-Linear Models and Logistic Regression, Second Edition
Christensen: Plane Answers to Complex Questions: The Theory of Linear
Models, Third Edition
Creighton: A First Course in Probability Models and Statistical Inference
Davis: Statistical Methods for the Analysis of Repeated Measurements
Dean and Voss: Design and Analysis of Experiments
du Toit, Steyn, and Stumpf: Graphical Exploratory Data Analysis
Durrett: Essentials of Stochastic Processes
Edwards: Introduction to Graphical Modelling, Second Edition
Finkelstein and Levin: Statistics for Lawyers
Flury: A First Course in Multivariate Statistics
Ghosh, Delampady and Samanta: An Introduction to Bayesian Analysis:
Theory and Methods
Gut: Probability: A Graduate Course
Heiberger and Holland: Statistical Analysis and Data Display:
An Intermediate Course with Examples in S-PLUS, R, and SAS
Job son: Applied Multivariate Data Analysis, Volume I: Regression and
Experimental Design
Jobson: Applied Multivariate Data Analysis, Volume II: Categorical and
Multivariate Methods
Kalbfleisch: Probability and Statistical Inference, Volume I: Probability,
Second Edition
Kalbfleisch: Probability and Statistical Inference, Volume II: Statistical
Inference, Second Edition
Karr: Probability
Keyfitz: Applied Mathematical Demography, Second Edition
Kiefer: Introduction to Statistical Inference
Kokoska and Nevison: Statistical Tables and Formulae
Kulkarni: Modeling, Analysis, Design, and Control of Stochastic Systems
Lange: Applied Probability
Lange: Optimization
Lehmann: Elements of Large-Sample Theory
(continued after index)
Jayanta K. Ghosh
Mohan Delampady
Tapas Samanta
An Introduction to
Bayesian Analysis
Theory and Methods
With 13 Illustrations
Jayanta K. Ghosh
Department of Statistics
Purdue University
150 N. University Street
West Lafayette,
IN 47907-2067
 
Indian Statistical Institute
203 B.T. Road
Kolkata 700108, India
 
Mohan Delampady
Indian Statistical Institute,
8th Mile, Mysore Road,
R.V. College Post,
Bangalore 560059, India
mohan@isibang. ac. in
Tapas Samanta
Indian Statistical Institute
203 B.T. Road
Kolkata 700108, India
 
Editorial Board
George Casella
Department of Statistics
University of Florida
Gainesville, FL 32611-8545
Stephen Fienberg
Department of Statistics
Carnegie Mellon University
Pittsburgh, PA 15213-3890
Ingram Olkin
Department of Statistics
Stanford University
Stanford, CA 94305
Library of Congress Control Number: 2006922766
ISBN-10: 0-387-40084-2
e-ISBN: 0-387-35433-6
ISBN-13: 978-0387-40084-6
Printed on acid-free paper.
©2006 Springer Science+Business Media, LLC
All rights reserved. This work may not be translated or copied in whole or in part without the
written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street,
New York, NY 10013, USA), except for brief excepts in connection with reviews or scholarly
analysis. Use in connection with any form of information storage and retrieval, electronic
adaptation, computer software, or by similar or dissimilar methodology now known or hereafter
developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if
they are not identified as such, is not to be taken as an expression of opinion as to whether or
not they are subject to proprietary rights.
Printed in the United States of America.
9 8 7 6 5 4 3 2 1
sprmger.com
To Ira, Shobha, and Shampa
Though there are many recent additions to graduate-level introductory books
on Bayesian analysis, none has quite our blend of theory, methods, and applications. We believe a beginning graduate student taking a Bayesian course
or just trying to find out what it means to be a Bayesian ought to have some
familiarity with all three aspects. More specialization can come later.
Each of us has taught a course like this at Indian Statistical Institute or
Purdue. In fact, at least partly, the book grew out of those courses. We would
also like to refer to the review ) that first made
us think of writing a book. The book contains somewhat more material than
can be covered in a single semester. We have done this intentionally, so that
an instructor has some choice as to what to cover as well as which of the
three aspects to emphasize. Such a choice is essential for the instructor. The
topics include several results or methods that have not appeared in a graduate
text before. In fact, the book can be used also as a second course in Bayesian
analysis if the instructor supplies more details.
Chapter 1 provides a quick review of classical statistical inference. Some
knowledge of this is assumed when we compare different paradigms. Following
this, an introduction to Bayesian inference is given in Chapter 2 emphasizing
the need for the Bayesian approach to statistics. Objective priors and objective Bayesian analysis are also introduced here. We use the terms objective
and nonsubjective interchangeably. After briefly reviewing an axiomatic development of utility and prior, a detailed discussion on Bayesian robustness is
provided in Chapter 3. Chapter 4 is mainly on convergence of posterior quantities and large sample approximations. In Chapter 5, we discuss Bayesian
inference for problems with low-dimensional parameters, specifically objective priors and objective Bayesian analysis for such problems. This covers
a whole range of possibilities including uniform priors, Jeffreys' prior, other
invariant objective priors, and reference priors. After this, in Chapter 6 we
discuss some aspects of testing and model selection, treating these two problems as equivalent. This mostly involves Bayes factors and bounds on these
computed over large classes of priors. Comparison with classical P-value is
also made whenever appropriate. Bayesian P-value and nonsubjective Bayes
factors such as the intrinsic and fractional Bayes factors are also introduced.
Chapter 7 is on Bayesian computations. Analytic approximation and the
E-M algorithm are covered here, but most of the emphasis is on Markov chain
based Monte Carlo methods including the M-H algorithm and Gibbs sampler,
which are currently the most popular techniques. Follwing this, in Chapter 8
we cover the Bayesian approach to some standard problems in statistics. The
next chapter covers more complex problems, namely, hierarchical Bayesian
(HB) point and interval estimation in high-dimensional problems and parametric empirical Bayes (FEB) methods. Superiority of HB and FEB methods
to classical methods and advantages of HB methods over FEB methods are
discussed in detail. Akaike information criterion (AIC), Bayes information
criterion (BIC), and other generalized Bayesian model selection criteria, highdimensional testing problems, microarrays, and multiple comparisons are also
covered here. The last chapter consists of three major methodological applications along with the required methodology.
We have marked those sections that are either very technical or are very
specialized. These may be omitted at first reading, and also they need not be
part of a standard one-semester course.
Several problems have been provided at the end of each chapter. More
problems and other material will be placed at '^
tapas/book
Many people have helped - our mentors, both friends and critics, from
whom we have learnt, our family and students at ISI and Furdue, and the
anonymous referees of the book. Special mention must be made of Arijit
Chakrabarti for Sections 9.7 and 9.8, Sudipto Banerjee for Section 10.1, Fartha
F. Majumder for Appendix D, and Kajal Dihidar and Avranil Sarkar for help
in several computations. We alone are responsible for our philosophical views,
however tentatively held, as well as presentation.
Thanks to John Kimmel, whose encouragement and support, as well as
advice, were invaluable.
Indian Statistical Institute and Furdue University
Jayanta K. Ghosh
Indian Statistical Institute
Mohan Delampady
Indian Statistical Institute
Tapas Samanta
February 2006
Statistical Preliminaries
Common Models
Exponential Families
Location-Scale Families
Regular Family
Likelihood Function
Sufficient Statistics and Ancillary Statistics
Three Basic Problems of Inference in Classical Statistics
Point Estimates
Testing Hypotheses
Interval Estimation
Inference as a Statistical Decision Problem
The Changing Face of Classical Inference
Bayesian Inference and Decision Theory
2.1 Subjective and Frequentist Probability
2.2 Bayesian Inference
2.3 Advantages of Being a Bayesian
2.4 Paradoxes in Classical Statistics
2.5 Elements of Bayesian Decision Theory
2.6 Improper Priors
2.7 Common Problems of Bayesian Inference
Point Estimates
Credible Intervals
Testing of a Sharp Null Hypothesis Through Credible
2.8 Prediction of a Future Observation
2.9 Examples of Cox and Welch Revisited
2.10 Elimination of Nuisance Parameters
2.11 A High-dimensional Example
2.12 Exchangeability
2.13 Normative and Descriptive Aspects of Bayesian Analysis,
Elicitation of Probability
2.14 Objective Priors and Objective Bayesian Analysis
2.15 Other Paradigms
2.16 Remarks
2.17 Exercises
Utility, Prior, and Bayesian Robustness
3.1 Utility, Prior, and Rational Preference
Utility and Loss
3.3 Rationality Axioms Leading to the Bayesian Approach
Bayesian Analysis with Subjective Prior
Robustness and Sensitivity
Classes of Priors
Conjugate Class
Neighborhood Class
Density Ratio Class
Posterior Robustness: Measures and Techniques
Global Measures of Sensitivity
Belief Functions
Interactive Robust Bayesian Analysis
Other Global Measures
Local Measures of Sensitivity
Inherently Robust Procedures
3.10 Loss Robustness
3.11 Model Robustness
3.12 Exercises
Large Sample Methods
4.1 Limit of Posterior Distribution
Consistency of Posterior Distribution
Asymptotic Normality of Posterior Distribution
Asymptotic Expansion of Posterior Distribution
Determination of Sample Size in Testing
4.3 Laplace Approximation
4.3.1 Laplace's Method
Tierney-Kadane-Kass Refinements
Choice of Priors for Low-dimensional Parameters
5.1 DiflFerent Methods of Construction of Objective Priors
Uniform Distribution and Its Criticisms
Jeffreys Prior as a Uniform Distribution
Jeffreys Prior as a Minimizer of Information
Jeffreys Prior as a Probability Matching Prior
Conjugate Priors and Mixtures
Invariant Objective Priors for Location-Scale Families . . 135
Left and Right Invariant Priors
Properties of the Right Invariant Prior for
Location-Scale Families
General Group Families
5.1.10 Reference Priors
5.1.11 Reference Priors Without Entropy Maximization
5.1.12 Objective Priors with Partial Information
5.2 Discussion of Objective Priors
5.3 Exchangeability
5.4 Elicitation of Hyperparameters for Prior
5.5 A New Objective Bayes Methodology Using Correlation
5.6 Exercises
Hypothesis Testing and Model Selection
6.1 Preliminaries
BIG Revisited
6.2 P-value and Posterior Probability of HQ as Measures of
Evidence Against the Null
6.3 Bounds on Bayes Factors and Posterior Probabilities
Introduction
Choice of Classes of Priors
Multiparameter Problems
Invariant Tests
Interval Null Hypotheses and One-sided Tests
6.4 Role of the Choice of an Asymptotic Framework
Comparison of Decisions via P-values and Bayes
Factors in Bahadur's Asymptotics
Pitman Alternative and Rescaled Priors
6.5 Bayesian P-value
6.6 Robust Bayesian Outlier Detection
6.7 Nonsubjective Bayes Factors
The Intrinsic Bayes Factor
The Fractional Bayes Factor
Intrinsic Priors
6.8 Exercises
Bayesian Computations
7.1 Analytic Approximation
7.2 The E-M Algorithm
7.3 Monte Carlo Sampling
7.4 Markov Chain Monte Carlo Methods
Introduction
Markov Chains in MCMC
Metropolis-Hastings Algorithm
Gibbs Sampling
Rao-Blackwellization
Convergence Issues
7.5 Exercises
Some Common Problems in Inference
Comparing Two Normal Means
Linear Regression
8.3 Logit Model, Probit Model, and Logistic Regression
The Logit Model
The Probit Model
High-dimensional Problems
9.1 Exchangeability, Hierarchical Priors, Approximation to
Posterior for Large p, and MCMC
MCMC and E-M Algorithm
Parametric Empirical Bayes
PEB and HB Interval Estimates
9.3 Linear Models for High-dimensional Parameters
Stein's Frequentist Approach to a High-dimensional Problem.. 264
Comparison of High-dimensional and Low-dimensional
9.6 High-dimensional Multiple Testing (PEB)
Nonparametric Empirical Bayes Multiple Testing
False Discovery Rate (FDR)
9.7 Testing of a High-dimensional Null as a Model Selection
9.8 High-dimensional Estimation and Prediction Based on Model
Selection or Model Averaging
Discussion
9.10 Exercises
10 Some Applications
10.1 Disease Mapping
10.2 Bayesian Nonparametric Regression Using Wavelets
10.2.1 A Brief Overview of Wavelets
10.2.2 Hierarchical Prior Structure and Posterior
Computations
10.3 Estimation of Regression Function Using Dirichlet
Multinomial Allocation
10.4 Exercises
Common Statistical Densities
A.l Continuous Models
A.2 Discrete Models
Birnbaum's Theorem on Likelihood Principle
Microarray
Bayes Sufficiency