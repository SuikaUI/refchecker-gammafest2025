Static Neural Compiler Optimization via
Deep Reinforcement Learning
Rahim Mammadli
Technische Universit¨at Darmstadt
Graduate School of Excellence
Computational Engineering
 
Ali Jannesari
Iowa State University
Department of Computer Science
 
Felix Wolf
Technische Universit¨at Darmstadt
Department of Computer Science
 
Abstract—The phase-ordering problem of modern compilers
has received a lot of attention from the research community
over the years, yet remains largely unsolved. Various optimization sequences exposed to the user are manually designed by
compiler developers. In designing such a sequence developers
have to choose the set of optimization passes, their parameters
and ordering within a sequence. Resulting sequences usually
fall short of achieving optimal runtime for a given source
code and may sometimes even degrade the performance when
compared to unoptimized version. In this paper, we employ
a deep reinforcement learning approach to the phase-ordering
problem. Provided with sub-sequences constituting LLVM’s O3
sequence, our agent learns to outperform the O3 sequence on
the set of source codes used for training and achieves competitive
performance on the validation set, gaining up to 1.32x speedup on
previously-unseen programs. Notably, our approach differs from
autotuning methods by not depending on one or more test runs
of the program for making successful optimization decisions. It
has no dependence on any dynamic feature, but only on the
statically-attainable intermediate representation of the source
code. We believe that the models trained using our approach
can be integrated into modern compilers as neural optimization
agents, at ﬁrst to complement, and eventually replace the handcrafted optimization sequences.
Index Terms—code optimization, phase-ordering, deep learning, neural networks, reinforcement learning
I. INTRODUCTION
Code optimization remains one of the hardest problems
of software engineering. Application developers usually rely
on a compiler’s ability to generate efﬁcient code and rarely
extend its standard optimization routines by selecting individual passes. The diverse set of applications and compute
platforms make it very hard for compiler developers to produce
a robust and effective optimization strategy. Modern compilers
allow users to specify an optimization level which triggers a
corresponding sequence of passes that is applied to the code.
These passes are initialized with some pre-deﬁned parameter
values and are executed in a pre-deﬁned order, regardless
of the code being optimized. Intuitively, this rigidness limits
the effectiveness of the optimization routine. Indeed, a recent
study has shown that even the highest optimization level
of different compilers leaves plenty of room for improvement.
In order to establish an even playing ﬁeld with existing
optimization sequences, we limit the scope of our approach by
allowing as input only information which is statically available during compilation. This sets us apart from autotuning
approaches where the data gathered from one or multiple runs
of the program is used to supplement the optimization strategy.
Our predictive model uses the intermediate representation (IR)
of the source code to evaluate and rank different optimization
decisions. By iteratively following the suggestions of our
model, we are able to produce an optimization strategy tailored
to a given IR. This is the main difference of our approach from
pre-deﬁned optimization strategies shipped as part of modern
compilers.
More formally, we rephrase the phase-ordering problem
as a reinforcement learning problem. The environment is
represented by the operating system and the LLVM optimizer.
The agent is a deep residual neural network, which interacts
with the environment by means of actions. The actions can be
of various levels of abstraction, but ultimately they translate to
passes run by LLVM’s optimizer on the IR. We will discuss
actions in greater detail in Section III. The state information
that is used by the agent to make predictions is represented
by the IR of the source code and the history of actions
that produces the IR. In response to actions, the environment
returns the new state and the reward. The new state is produced
by the LLVM optimizer which runs the selected pass(-es)
on the current IR and produces the new IR. The reward
is calculated by benchmarking the new IR and comparing
its runtime to that of the original IR (i.e., a reduction in
runtime produces a positive reward while an increase produces
a negative one). Through interchanging steps of exploration
and exploitation we train an agent that learns to correctly value
the various optimization strategies.
We believe that the agents produced by our approach could
be integrated into existing compilers alongside other routines,
such as O2 or O3. Our agent outperforms O3 in multiple
scenarios, achieving up to 1.32x speedup on previously-unseen
programs, and can therefore be beneﬁcial in a toolkit of
optimization strategies offered to an application developer.
While the agent learns to achieve superior performance on the
training set, it is, on average, inferior to O3 on the validation
set. However, we believe that this is due to current limitations
of encoding we use for the IRs and the relatively small size of
our dataset. We are convinced that the optimization strategies
 
of the future will resemble learned agents rather than manually
designed sequences, and that reinforcement learning will likely
be the framework used to produce these agents. This work
intends to be one of the ﬁrst steps in this direction.
The static phase-ordering problem considered in this work
is challenging because of several factors. First, the limited
amount of information available during compilation, such
as the unknown input size already reduces the optimization
potential of the agent. In order to partially offset this we
include benchmarks with various problem sizes in our dataset.
Next, the number of possible optimization sequences grows
exponentially with the number of passes. The space grows
even more if we consider possible parameterizations of distinct
passes. To deal with the large optimization space we employ
several strategies: (i) we make the agent pick only a single
action at a time instead of predicting the whole sequence
from scratch, (ii) we experiment with different levels of
abstraction for our actions, from triggering a sequence of
optimization passes down to selecting a parameter for a single
pass. Moreover, the space of possible IRs of each source code
can also be quite large, therefore to encode the IRs we use the
embeddings by Ben-Nun et al. . Another challenge is that
the efﬁcacy of different optimizations might vary depending
on the underlying hardware. In our approach we use only one
out of two available distinct system conﬁgurations per agent to
run all of the benchmarks. This means that the learned agents
are ﬁne-tuned for the given hardware. However, this is not
necessarily a disadvantage, because it is possible to train an
agent once per processing unit and ship it alongside a compiler
optimizer. Moreover, it could also be possible to train a single
versatile agent by supplementing state with the information
about the underlying hardware.
We use a dataset of 109 single-source benchmarks from the
LLVM test suite to train and evaluate our model. The model
is trained using IRs of source codes from the training set and
evaluated on the source codes in the validation set. Using
passes from the existing O3 sequence of the LLVM optimizer
we are able to train an agent which is on average 2.24x faster
than the unoptimized version of a program in the training
set, whereas O3 is 2.17x faster. The best-performing agent on
the validation set achieves an average of 2.38x speedup over
the unoptimized version of the code, while the O3 sequence
achieves an average of 2.67x speedup.
Most of the prior work related to ours , focuses on
the autotuning problem, where a program has to be run one
or more times before it is possible to choose the optimization
sequence. The advantage of these approaches is that the
dynamic information gathered during program runs provides
an accurate characterization of the program. These methods are
therefore usually quite successful in outperforming compilers’
pre-deﬁned optimization sequences. However, a big disadvantage of these approaches is that they require extra developer
effort to run the program and gather the necessary information,
which prevents them from being integrated into compilers
as part of a standard compilation routine. The supervised
learning methods applied to compiler optimization problem
require a pre-existing labeled dataset that is then used to
train a model. Producing such a dataset is not an easy task
because the search space is usually very large and the value of
different data points is unkonwn beforehand. Reinformcement
learning, in contrast to supervised learning, allows the trained
agent to explore the environment and continuosly choose the
data points itself as it learns. The problem of developing
methods competing with pre-deﬁned optimization sequences
using only static information has not gained much attention
in the scientiﬁc literature in recent years. This is partially
because the problem is very challenging. Nonetheless, we
believe that this problem is at least of equal importance
and to the best of our knowledge we are the ﬁrst to apply
deep1 reinforcement learning to solve it. This paper makes
the following contributions:
• A novel deep reinforcement learning approach to static
code optimization. The approach does not rely on manual
feature engineering by a human, but instead learns by
observing the effects of the various optimizations on the
IR and the rewards from the environment. The approach
is therefore fully automatic and relies only on the initial
supply of the source codes.
• A trained optimization agent that can be integrated into
modern compilers alongside existing optimization sequences exposed through compiler ﬂags such as -O2, -O3,
etc. The agent can produce IRs that are up to 1.32x faster
than the ones resulting from using the O3 optimization
• An efﬁcient framework ”Compiler Optimization via Reinforcement Learning” (CORL) allowing fast exploration
and exploitation in batches. The dynamic load-balancing
mechanism distributes the benchmarking workload across
the number of available workers and facilitates efﬁcient
exploration. Using a large replay memory allows for fast
off-policy training of the agent. The results of benchmarks are further stored in a local database to allow
reproducibility as well as higher efﬁciency of subsequent
This paper is structured in the following manner. We start
by providing background information in Section II, before
introducing our approach and CORL framework in Section III.
We evaluate our approach in Section IV and describe the
related work in Section V. Finally, we conclude our paper
and sketch future work in Section VI.
II. BACKGROUND
Modern compilers expose multiple optimization levels via
their command line interface. For example, the current version
of LLVM2 offers a selection of seven such levels. These aim to
strike a certain trade-off between the size of the produced binary and its performance. Each optimization level corresponds
1Deep reinforcement learning encompasses a subset of reinforcement
learning methods where the learning part is performed by a deep neural
2 
html#code-generation-options, Access date: 22.06.2020
to a unique sequence of passes that are run on the source
code. These passes are constructed using hard-coded values
matching the selected optimization level. Having to maintain
multiple manually-designed optimization sequences is one of
the drawbacks of the current design. Another disadvantage is
that while the optimization sequences are generally efﬁcient,
they are not optimal, and in certain cases can even increase
the runtime when compared to an unoptimized version. For
example, after applying the O3 optimization sequence to
the evalloop.c benchmark from the LLVM test suite we
observed a more than three-fold slowdown. In comparison
to hand-crafted sequences of passes our method is fullyautomatic and can learn to achieve any sort of a trade-off
given the correct reward function. Constructing such a reward
function for the size of the binary or the runtime is trivial, as
discussed in Section III-B.
The majority of existing machine learning methods to
compiler optimization tackle the problem of autotuning. This
means that they depend on dynamic runtime information and
require at least one run of the source program to make
a prediction. Apart from this, some of these methods rely
on static features extracted from the source code, such as
tokens , IR , etc. In contrast to these methods we abandon
the dependence on the dynamic features and aim to achieve
the best-possible performance by only using the static features
extracted from the IR .
III. APPROACH
We ﬁrst give a high level overview of our approach in
Section III-A, before formally deﬁning the problem in Section III-B. Then, we discuss the three levels of abstraction
for actions we consider and the corresponding action spaces
in Section III-C, before introducing the tools used to map
actions to concrete LLVM passes in Section III-D. We ﬁnish by
going over the functionality offered by the CORL framework
in Section III-E.
A. Overview
The high-level overview of our approach is shown in
Figure 1. The agent takes the IR of the source code as
well as the initially empty history of actions and calculates
the expected cumulative rewards for different actions. If the
predicted reward of an action is positive then it is expected
to eventually lead to a speedup, while the negative rewards
are expected to result in a slowdown. To ensure this, the
rewards are calculated as log(speedup) during training. Next,
the action with the highest reward is chosen and if its value is
positive LLVM optimizer applies the chosen optimization(s)
to the input IR. The produced IR alongside the updated
history of actions is then fed into the agent once again
and the cycle continues. Eventually, the cycle breaks when
the highest predicted reward is negative or the maximum
number of allowed optimizations is applied. Having an upperbound on the number of optimizations prevents the agent from
potentially being stuck in an inﬁnite loop.
input_ir.ll
Predictions
Action Reward
Fig. 1: The CORL workﬂow.
B. Problem Deﬁnition
We deﬁne the phase-ordering problem as a reinforcement
learning problem. Since the IR of a source code carries only
static information that we use to represent states, the states
lack the Markovian property. Moreover, using embeddings
produced by Ben-Nun et al. results in a further loss
of information about the IR such as immediate values of
instructions. To enrich the state representation we supplement
it with the history of actions performed by the agent.
For a set of all possible states S and actions A, our goal
is to learn the value function Q(s, a, w) parameterized with
weights w, such that for any state s ∈S and action a ∈A,
the function predicts the highest cumulative reward attainable
by taking that action. In order to learn the value function we
enforce consistency:
Q(St, At, w) = R(St, At) + γ max
a∈A Q(St+1, a, w)
In the equation, R(St, At) is the reward awarded for taking
action At in state St, after which the agent ends up in state
St+1, and γ ∈ is the discount factor for future rewards.
Assuming that function T(s) represents the runtime of the
executable produced by compiling the IR corresponding to
state s ∈S, the reward for the action transitioning the agent
from state St to state St+1 is calculated as follows:
R = ln T(St)
Representing the reward as the logarithm of the attained
speedup or slowdown allows for the rewards to be accumulated
across transitions. Notably, to train an agent to minimize the
size of the produced binary instead of its runtime, one would
only need to update the reward function. Speciﬁcally, the
function T(s) calculating the runtime of an executable would
need to be replaced with another function calculating its size.
Similarly, using both the runtime and the size of an executable
in the reward calculation would stimulate the agent to learn
the trade-off between the two.
In order to learn an approximation of a function Q(s, a, w)
we ﬁrst initialize a deep residual neural network (DQN) with
random weights w. Then, we use a replay memory to sample
experiences each represented as a set {St, At, R, St+1} and
compute the loss (TD-error) of our DQN as the squared mean
of the difference between the left and right sides of Equation 1.
C. Action Spaces
In order to produce an optimization sequence for a given
IR an agent must decide on a chain of actions. To represent
the actions, we experiment with three different levels of
abstraction, which are illustrated in Figure 2. At the highest
level of abstraction an action triggers a series of passes to be
applied to an IR. At the middle level of abstraction each action
corresponds to an individual pass. Finally, at the lowest level of
abstraction an action might select a pass or a parameter value
for an already selected pass. For high and middle level actions
the passes are initialized with pre-deﬁned parameter values.
The lower the level of abstraction for actions, the harder is
the learning problem.
In this work we experiment with all three levels of abstraction. We label the action spaces produced by high, middle and
low level actions as H, M, L respectively. The size of each
action space has exponential dependence on the maximum
allowed number of consecutive actions, which we designate
as parameter µ. Selecting larger values for the parameters
µ and γ allows an agent to learn the existence of rewards
lying many steps ahead. However, having µ too large may
unnecessarily complicate the learning problem if such longterm dependences among actions do not exist. Furthermore,
compilation time potentially also increases proportionally to µ.
Note that in action space L only actions selecting individual
passes and not parameters contribute towards µ. Moreover,
since only the last parameter selection for every pass with
multiple parameters makes it possible to construct and evaluate
a pass, all the preceding intermediate actions produce a reward
of 0. Therefore, to allow the agent to learn the values of
different parameter selections of a given pass the value of γ
for all intermediate actions is set to 1.
Parameter #1
Parameter #2
Fig. 2: Three levels of abstraction for actions.
D. Implementation
Some of the passes in LLVM’s O3 sequence are initialized
with non-default constructors, and are therefore impossible
to replicate using the command line interface of the LLVM
optimizer opt. To allow experimentation at the highest level
of abstraction in action space H using the exact passes from
LLVM’s O3 sequence, we create a special optimizer opt corl.
This optimizer alters the functionality of opt by using one
or more user-speciﬁed subsequences of passes out of O3 to
optimize a given IR. Each subsequence of passes is speciﬁed
using its starting and ending indexes within the O3 sequence.
Both opt corl and opt allow experimentation in action
space M. However, for the sake of generality, we only use opt
for the action spaces M and L, since it allows us to specify
both individual passes and set their parameters. When dealing
with action space L, opt is only invoked when both pass and
parameter selections have been ﬁnalized.
E. CORL Framework
The majority of reinforcement learning algorithms can be
described as iterative processes with interchanging exploration
and exploitation steps performed in a loop. The sequential
nature of these algorithms is usually not an issue for many
reinforcement learning problems for which the exploration
step completes in a short amount of time. Receiving a quick
response to an action from the environment allows for fast
generation of training data and consequently faster training ,
 . In contrast, for our problem the benchmarking step required to calculate the reward takes a relatively long time to
complete. Therefore, waiting for the exploration step to ﬁnish
before proceeding with the exploitation is suboptimal both
in terms of the agent’s training and efﬁcient use of compute
resources. To that end, we devise an algorithm which allows
for the exploration and exploitation steps to be performed in
Figure 3 illustrates the essential elements of the CORL
framework, which is designed as a client-server architecture.
The server-side functionality is divided across several objects
responsible for training agents, managing workers and replay
memory, and visualizing progress. As part of the exploration
process the learner object generates new tasks in the form of
state-action pairs and sends them to the manager object. Afterwards, as part of exploitation process, the learner continuously
samples batches of experiences from the replay memory and
trains the agent. The manager distributes the tasks generated
by the learner across workers and updates the replay memory
with the newly-generated experiences. Both the learner and
the manager run in separate server-side processes, allowing
for exploration and exploitation to be performed in parallel.
Below we describe the various functionalities of the CORL
framework in a greater level of detail.
1) Initialization: The server-side logic starts with the manager scanning the source codes provided by the user and
splitting them into training and validation sets. The programs
are randomly shufﬂed and assigned to respective sets based on
the user-speciﬁed ratio. Then, the manager loads previouslysaved IRs and transitions from the SQL database into memory
and populates the replay memory with experiences. Afterwards, workers are utilized to produce and benchmark unop-
Exploration
Exploitation
Evaluation
Optimization
Visualization
Visualizer
Benchmarking
Persistence
Distribution
Fig. 3: Overview of the CORL framework.
timized base IR and its O3-level optimized version for every
source code in the dataset if not already present in memory.
All the data generated at this stage and during exploration
is asynchronously saved to the database. Upon completing
this step the initialization is ﬁnished and the learner starts
exploration.
2) Benchmarking: A single exploration step involves applying selected pass(-es) to a given IR, producing a new IR, which
is then benchmarked to calculate the reward. Benchmarking
any program is prone to noise and based on our observations
the variation in runtime in terms of percentage of deviation
from the mean is itself dependent on runtime. For the source
codes in our dataset the bigger the runtime the smaller is the
observed variation. Therefore, to calculate the runtime of an IR
a worker runs it between 20 and 1000 times, depending on its
runtime, and sends the median runtime back to the manager.
To hide the latency induced by benchmarking, we perform
exploration in batches.
3) Exploration: To perform exploration we use an ϵ-greedy
strategy. The value of ϵ is linearly-annealed throughout training. The agent starts every exploration step by sampling a
batch of base states. These states correspond to unoptimized
versions of the IR for every source code in the training set. For
each sampled state an agent selects an action either greedily
or randomly based on the toss of a coin. State-action pairs
already present in memory are used to perform server-side
state transitions and the exploration proceeds with the new
state until the transition for a selected action is not yet present
in memory. Finally, an assembled set of state-action pairs is
sent to the manager and the agent proceeds to exploitation.
4) Exploitation: Before the exploitation process starts the
replay memory has to be populated with a sufﬁcient number
of experiences. Once the replay memory is large enough, the
learner starts to train the agent by minimizing the loss function
described previously in Section III-B. To stabilize training we
use ﬁxed Q-targets which are updated once every τ steps.
Every δ steps, where δ is a multiple of τ, the framework
switches to evaluation mode, during which both exploration
and exploitation halts and the agent’s performance is evaluated.
5) Evaluation, Logging, and Visualization: Evaluation is
performed similarly to exploration, with two main differences.
First, instead of sampling base states from the training set,
the agent is evaluated in all of the base states in the dataset,
including the validation set. Second, instead of letting the
toss of a coin determine the chosen action, the agent always
behaves greedily. The learner logs all of the data pertaining
to a single run of the CORL framework, including evaluation
and exploitation progress, to a separate ﬁle. The visualizer
object continuously scans the logs directory and performs
visualization using the VisDom framework3.
IV. EVALUATION
We ﬁrst explain the experimental setup in Section IV-A,
before discussing the quality of the ﬁt achieved by our agents
in Section IV-B. Then, we introduce the metrics we use
to evaluate the performance of our agents in Section IV-C.
We conclude with reviewing the results of our evaluation in
Sections IV-D and IV-E.
A. Experimental Setup
The dataset for training our optimizing agents consists of
109 single-source benchmarks from the LLVM test suite.
Single-source benchmarks were chosen as they provide a
simple and convenient way of building and executing the
benchmarks. The complete list of benchmarks and source
codes is available in Table I. The programs were split between training and validation sets in a 4:1 ratio. To speed-up
experimentation we excluded top-four source codes with the
longest runtime when dealing with action spaces M and L.
To execute optimization sequences we use LLVM optimizer
version 3.8. The choice of this particular version is motivated
by the ability to use pre-trained embeddings from the study by
Ben-Nun et al. . However, our approach can be used with
the newer versions of the LLVM optimizer as well.
To deﬁne the action space H, we partition the O3 sequence of the LLVM optimizer into eight different actions,
3 
TABLE I: The list of benchmarks and source codes used for
evaluation.
correlation.c covariance.c 2mm.c 3mm.c atax.c
cholesky.c
trisolv.c trmm.c durbin.c dynprog.c gramschmidt.c
lu.c ludcmp.c ﬂoyd-warshall.c reg detect.c adi.c
fdtd-2d.c fdtd-apml.c jacobi-1d-imper.c jacobi-2dimper.c seidel-2d.c
ackermann.c ary3.c ﬁb2.c hash.c heapsort.c lists.c
matrix.c methcall.c nestedloop.c objinst.c random.c
sieve.c strcat.c ackermann.cpp ﬁbo.cpp heapsort.cpp
matrix.cpp methcall.cpp random.cpp except.cpp
dt.c evalloop.c fbench.c ffbench.c ﬂops-1.c ﬂops-
2.c ﬂops-3.c ﬂops-4.c ﬂops-5.c ﬂops-6.c ﬂops-7.c
ﬂops-8.c ﬂops.c fp-convert.c himenobmtxpa.c lowercase.c mandel-2.c mandel.c matmul f64 4x4.c
oourafft.c
ReedSolomon.c
Bits.c richards benchmark.c salsa20.c whetstone.c
mandel-text.cpp oopack v1p8.cpp sphereﬂake.cpp
Bubblesort.c FloatMM.c IntMM.c Oscar.c Perm.c
Puzzle.c Queens.c Quicksort.c RealMM.c Towers.c
Treesort.c
BenchmarkGame
fannkuch.c n-body.c nsieve-bits.c partialsums.c puzzle.c recursive.c spectral-norm.c fasta.c
linpack-pc.c
chomp.c misr.c queens.c
dry.c ﬂdry.c
CoyoteBench
almabench.c huffbench.c lpbench.c
smallpt.cpp
as shown in Table II. The division follows the observation
that the optimization sequence consists of smaller logical subsequences ending with a simplifycfg pass. To allow a
fair comparison of the results of our experiments we deﬁne
the actions in spaces M and L using 42 unique transformation
passes which are part of actions in space H. In action space M,
the passes are initialized with the default parameter values,
while in action space L agents also choose the parameter
values. Table III lists the passes in action space L which have
tunable parameters along with the values for these parameters.
The value µ = 16, the maximum number of actions, is used
in all of the experiments.
For our experiments, we run the server-side and the clientside logic of the CORL framework on two different hardware
architectures. Below we describe these architectures in detail.
1) Server: The server-side logic responsible for training the
agents and distributing tasks to clients was run on a single
server with two Intel(R) Xeon(R) Gold 6126 2.60GHz CPUs,
64GBs of main memory, two NVIDIA GeForce GTX 1080 Ti
GPUs, and Ubuntu 16.04 LTS operating system. We trained
our models using a single GPU.
2) Client: We ran the clients on the nodes of the Hardware
Phases I and II of the Lichtenberg High Performance Computer. The nodes within Hardware Phases I and II each have
two Intel(R) Xeon(R) E5-2670 CPUs and Intel(R) Xeon(R)
TABLE II: Passes within the O3 sequence of the LLVM
optimizer version 3.8, divided into eight different actions for
experiments in the action space H.
Order Pass
scoped-noalias
simplifycfg
lower-expect
targetlibinfo
forceattrs
scoped-noalias
inferattrs
deadargelim
instcombine
simplifycfg
globals-aa
functionattrs
argpromotion
jump-threading
correlated-propagation
simplifycfg
instcombine
tailcallelim
simplifycfg
reassociate
loop-rotate
loop-unswitch
simplifycfg
Order Pass
instcombine
loop-idiom
loop-deletion
loop-unroll
mldst-motion
instcombine
jump-threading
correlated-propagation
simplifycfg
instcombine
rpo-functionattrs
elim-avail-extern
globals-aa
loop-rotate
loop-vectorize
instcombine
slp-vectorizer
simplifycfg
instcombine
loop-unroll
instcombine
alignment-from
-assumptions
strip-dead-prototypes
constmerge
E5-2680 v3 CPUs respectively, 64GBs of main memory, and
run CentOS Linux version 7. Each node ran a single client
at a time, with the number of clients dynamically changing
throughout the runs as the workers were added and removed
from the pool. Due to availability constraints experiments with
action space H were performed on the nodes of Hardware
Phase II while experiments with action spaces M and L were
performed on the nodes of Hardware Phase I.
B. Convergence
To measure the quality of the ﬁt achieved by our agents
we record the mean value of the loss function for sampled
batches of experiences throughout training. Figure 4 shows
how the loss converges in all three action spaces. We achieve
the best ﬁt in the action space H with the relatively high value
of γ = 0.9 which allows the network to account for long-term
rewards when predicting the values of different actions. We use
γ = 0.5 and increase the value of τ for larger action spaces
to stabilize the training. While the loss converges in action
TABLE III: Passes in the action space L that have tunable parameters. The ﬁrst value is the default for each parameter.
loop-vectorize
vectorizer-maximize-bandwidth
[false, true]
max-interleave-group-factor
 
enable-interleaved-mem-accesses
[false, true]
vectorizer-min-trip-count
 
enable-mem-access-versioning
[true, false]
max-nested-scalar-reduction-interleave
enable-cond-stores-vec
[false, true]
enable-ind-var-reg-heur
[true, false]
vectorize-num-stores-pred
enable-if-conversion
[true, false]
enable-loadstore-runtime-interleave
[true, false]
loop-vectorize-with-block-frequency
[false, true]
small-loop-cost
 
simplifycfg
bonus-inst-threshold
phi-node-folding-threshold
simplifycfg-dup-ret
[false, true]
simplifycfg-sink-common
[true, false]
simplifycfg-hoist-cond-stores
[true, false]
simplifycfg-merge-cond-stores
[true, false]
simplifycfg-merge-cond-stores-aggressively [false, true]
speculate-one-expensive-inst
[true, false]
max-speculation-depth
 
loop-unroll
percent-dynamic-cost-saved-threshold
 
[false, true]
allow-partial
[false, true]
max-iteration-count-to-analyze
 
dynamic-cost-savings-discount
 
 
slp-vectorizer
slp-vectorize-hor
[true, false]
slp-threshold
slp-vectorize-hor-store
[false, true]
slp-max-reg-size
 
slp-schedule-budget
 
inlinecold-threshold
 
inline-threshold
 
inlinehint-threshold
 
loop-unswitch
with-block-frequency
[false, true]
 
coldness-threshold
liv-reduce
[true, false]
verify-indvars
[true, false]
replexitval
[cheap, never, always]
enable-pre
[true, false]
enable-load-pre
[true, false]
max-recurse-depth
 
sroa-random-shufﬂe-slices
[false, true]
sroa-strict-inbounds
[false, true]
jump-threading implication-search-threshold 
 
loop-rotate
rotation-max-header-size
 
disable-licm-promotion
[true, false]
lower-expect
likely-branch-weight
 
ﬂoat2int-max-integer-bw
 
space M, it diverges in action space L in spite of larger values
of τ. The disadvantage of increasing τ is that the training
time also increases proportionally. As can be observed from
Figure 4c, using larger values of τ in action space L stabilizes
training. However, it also prohibitively increases training time
and therefore we refrain from further experiments with even
bigger values of τ.
C. Metrics
In order to evaluate the optimization potential of an agent
we compare its performance with that of LLVM’s built-in
O3 optimization sequence. To do that, we ﬁrst calculate the
speedup achieved by an agent on every source code in the
dataset. Then we aggregate these values across training and
validation sets by computing geometric means of speedup for
source codes in the respective sets. We do similar calculations
for LLVM’s O3 sequence and compare the computed metrics
to evaluate the performance of an agent.
For an agent to learn the values of taking different actions, these actions have to be explored ﬁrst. As the agent
continuously explores its environment, it accumulates new
experiences which potentially yield higher speedups. In other
words, highest observed speedups on source codes in the
dataset continue to grow over time. These values put an
upper bound on the agent’s performance and enable us to tell
how close it is to the best possible one. Therefore, during
evaluation we also record the highest observed speedup for
every source code in the dataset. Below we ﬁrst present the
results for aggregate metrics before showing the performance
of our agents on individual source codes.
D. Aggregate Results
As can be observed in Figure 5a the agent learns to outperform the O3 strategy on the training set in action space H,
achieving an average speedup of 2.24x over the unoptimized
version, while the O3 sequence achieves an average speedup
of around 2.17x. The agent’s performance is nearly 95% of the
observed best-possible performance, which conﬁrms that the
model achieves a good ﬁt on the training data. Figure 5d shows
that, while the validation set performance also increases over
time, it only approaches the performance of the O3 strategy,
achieving an average speedup of 2.38x over the unoptimized
version versus the 2.67x average speedup achieved by O3.
The growing best-observed performance on the validation
set shows that by behaving greedily the agent independently
discovers states corresponding to IRs with lower runtime than
those produced by O3. As we will see later, while the agent
seldom signiﬁcantly outperforms the O3 strategy, it fails to
be equally robust across all the source codes. We attribute
this mainly to a lack of diversity in the distribution of source
codes in our training set and believe that having a larger more
diverse training set would likely solve the issue. Nonetheless,
that we were able to discover the IRs with lower runtime by
re-arranging sub-sequences of passes comprising LLVM’s O3
routine shows that it is far from optimal.
(a) Action space H
(b) Action space M
=0.5 =1000
(c) Action space L
=0.5 =4000
=0.5 =6000
Fig. 4: From left to right, convergence of the loss value during training for action spaces H, M and L. Loss values are running
means on logarithmic scale. As the size of the action space increases, the quality of the ﬁt achieved by our model decreases.
For action space L, the loss value diverges even despite increasing parameter τ.
TABLE IV: Top 5 best and worst performances on individual programs of an agent trained in action space H.
Source Code
Action Sequence
Agent vs O3
ﬂoyd-warshall.c
4→7→7→6→6→6→6→6→6→6→4→6→6→6→6→6
reg detect.c
5→5→0→5→1→0→0→0→0→0→0→0→0→0→0→0
4→7→7→4→4→4→7→7→7→4→4→7→4→4→4→4
fdtd-apml.c
4→4→1→4→4→1→7→7→1→7→1→1→1→1→1→1
2→4→0→2→2→2→6→6→6→5→6→6→6→6→1→1
1→6→1→1→1→1→1→1→1→1→1→1→1→1→1→1
himenobmtxpa.c
1→4→4→0→5→0→0→3→0→3→3→0→0→0→0→0
0→4→6→2→2→4→2→2→2→4→2→2→2→2→4→2
1→6→2→7→3→7→3→7→2→7→3→2→7→2→2→4
Bubblesort.c
0→4→7→5→5→3→3→3→0→6→6→3→3→3→6→6
Validation
recursive.c
1→6→6→6→6→6→6→6→6→6→6→5→6→6→0→0
7→6→2→0→0→0→0→1→1→0→0→0→0→0→0→0
0→2→6→2→2→2→2→2→2→2→2→2→2→2→2→2
oourafft.c
3→4→6→3→3→3→3→3→3→3→3→3→3→3→3→3
ackermann.c
6→7→3→6→6→7→7→7→3→6→7→6→1→2→2→2
5→0→4→0→0→0→0→0→0→0→0→0→0→0→0→0
1→6→5→1→5→1→5→6→5→5→5→5→6→5→6→5
0→1→2→2→2→0→1→1→1→1→1→1→1→1→1→1
jacobi-1d-imper.c
1→0→7→6→5→3→3→3→3→2→3→5→3→0→0→0
4→5→4→5→0→5→5→7→3→5→1→3→3→3→3→3
At every step in action space M, our agent has to choose one
of 42 actions, each corresponding to a particular LLVM pass.
Since this space is much larger than space H it takes much
longer for the agent to discover advantageous states. Figure 5b
shows that after more than forty evaluation steps, which
includes nearly six days of exploration within that period,
the agent is able to observe experiences yielding the same
average speedup over the baseline as the O3 sequence. During
this time the agent continuously improves its performance on
the training set and given enough time is likely to achieve
and surpass the performance of the O3 strategy. However, its
performance on the validation set does not seem to improve
as seen in Figure 5e. Therefore, in view of the limited access
to compute resources, we terminate the experiment in action
space M after 56 evaluations. We believe that given a larger
number of actions in space M when compared to H it is
easier for the agent to memorize action sequences yielding
high speedups on speciﬁc source codes in the training set.
Similar to action space H, increasing the size and diversity of
the training set is likely to force the agent to generalize and
achieve better performance on the validation set.
Given that in our experiments in the action space L the loss
function diverges, we do not see any meaningful improvement
in agent’s performance during the evaluation, as shown in
Figures 5c and 5f.
E. Performance on Individual Programs
To examine the behavior of an agent trained in action
space H on individual source codes, we record the sequence of
actions chosen by the model for every program. This allows
us to verify that the model does indeed produce a different
optimization strategy for different programs. Furthermore, we
calculate the speedup achieved by our agent and LLVM’s O3
strategy over the unoptimized base version of the IR of every
source code in our dataset. Table IV presents top ﬁve best and
worst performance results in both training and validation sets.
(a) Space H, training, δ=4K
Evaluation Iteration
Speedup (gmean)
(b) Space M, training, δ=10K
Evaluation Iteration
Speedup (gmean)
(c) Space L, training, δ=20K
Evaluation Iteration
Speedup (gmean)
(d) Space H, validation, δ=4K
Evaluation Iteration
Speedup (gmean)
(e) Space M, validation, δ=10K
Evaluation Iteration
Speedup (gmean)
(f) Space L, training, δ=20K
Evaluation Iteration
Speedup (gmean)
Fig. 5: Aggregate speedups in all three action spaces. The three curves in every plot show the average performance of a model,
the average of the best observed performance on every program in the speciﬁed set and the average performance of LLVM’s
O3 sequence.
By observing the results we can conclude that the agent does
indeed produce specialized optimization strategy for every
source code. Interestingly, the agent utilizes the balance of
µ = 16 available actions to the fullest in almost all cases,
except some that are not shown in Table IV. This means that
in most cases the agent predicts at least one action to yield
positive reward. Although the IR itself does not necessarily
change as a result of every action, the history of actions
is always updated to store the latest action. Since the state
consists of both the IR and the history of actions, it changes
after every action of the agent. Therefore we stop sampling
the agent only when all of the actions are predicted to lead
to slowdown, i.e., have value 0 or less, or after the maximum
number of actions µ is taken.
V. RELATED WORK
Compiler optimization problem has been in focus of research community for several decades, with earliest works
dating back to late 1970s . Its subproblems of various
complexity, ranging from the simplest, parameter value selection, to the most complex, phase-ordering, were tackled via
different classes of methods . Among these methods are
iterative search techniques , genetic algorithms – ,
and machine learning methods [?], , with deep learning
methods gaining popularity in recent years , .
In order to leverage the advantages of (deep) machine
learning methods when it comes to compiler optimization,
several challenges need to be addressed: (i) correctly deﬁning
the learning problem, (ii) choosing or building the right set
of features to represent the program, (iii) generating the
dataset for training, and (iv) selecting the right neural network
architecture which is both expressive enough to learn the
task and allows efﬁcient training. The learning problem is
deﬁned as either an unsupervised learning problem, often used
to learn features , , , , a supervised learning
problem , , or a reinformcement learning problem . A
set of features includes statically-available ones, such as code
token sequences , , , abstract syntax trees (AST)
and AST paths – , IRs and learned representations built
on top of IRs , , , – . An additional set of
features includes the problem size and dynamic performance counters . Training data is often generated manually
for supervised learning methods , , while reinforcement
learning methods use initial training set to generate data via
exploration . Unsupervised learning methods can take the
advantage of the large code corpora available online , .
There also exist methods for automatic generation of training
data using deep neural networks .
Our work is most similar to the approach by Kulkarni
et al. , who also use reinforcement learning and train a
neural network to tackle the phase-ordering problem. However,
important differences from the above work are the following:
(i) our approach does not depend on dynamic features and
therefore does not require a program to be run to make
a prediction, (ii) the search space of possible optimizations
considered in our work is much larger, (iii) our approach
depends on the IR of the program and is therefore agnostic
to the front-end language a program is written in, and (iv)
instead of NEAT, we use gradient-based optimization to train
our neural network.
Ashouri et al. developed the MiCOMP framework to
tackle the phase-ordering problem by ﬁrst clustering LLVM
passes composing the O3 sequence of the LLVM optimizer
and then using a supervised learning approach to devise
an iterative compilation strategy which outperforms the O3
sequence within several trials. Similar to Kulkarni et al. ,
they use dynamic features and consider a smaller search space
of size 56 compared to 816, which is the size of H, the smallest
action space considered in our work.
VI. CONCLUSION
We formulated compiler phase-ordering as a deep reinforcement learning problem and developed the CORL framework,
which allows for efﬁcient training of optimizing agents. Our
approach is fully automatic and relies only on the initial supply
of a dataset of programs. We were able to train the agents
which surpass the performance of LLVM’s hard-coded O3
optimization sequence on the observed set of source codes and
achieve competitive performance on the validation set, gaining
up to 1.32x speedup over the O3 sequence with previously
unseen programs. We believe these results exhibit the big
potential of deep reinforcement learning in tackling phaseordering problem of compilers.
Our approach has several shortcomings, which we plan
to address in the future. Firstly, increasing the size of the
dataset to include a more diverse set of source programs
might be enough to achieve superior performance compared
with the hard-coded optimization strategy. Secondly, using
higher-quality embeddings for the IR and the appropriate
neural architecture can result in more efﬁcient and robust
optimizing agents. Next, current design requires that the
programs are compiled and benchmarked on every new target
system, which requires substantial computational resources.
While calculation of rewards by running the benchmarks
on the end systems is at the center of our approach, we
believe the data efﬁciency of the learning procedure could
be improved by including a self-supervised learning step by
the agent. This would potentially result in a more efﬁcient
exploration strategy, and reduce the computational burden by
allowing faster convergence of an agent. Finally, optimizing
the agents’ training procedure could allow for similar results
to be achieved in higher dimensional action spaces.
ACKNOWLEDGMENTS
This work is supported by the Graduate School CE within
the Centre for Computational Engineering at Technische Universit¨at Darmstadt and by the Hessian LOEWE initiative
within the Software-Factory 4.0 project. The calculations for
this research were conducted on the Lichtenberg Cluster of
TU Darmstadt.