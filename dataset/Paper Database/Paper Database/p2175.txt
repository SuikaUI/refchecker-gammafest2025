Unsupervised Learning of Visual
Representations by Solving Jigsaw Puzzles
Mehdi Noroozi and Paolo Favaro
Institute for Informatiks
University of Bern
{noroozi,paolo.favaro}@inf.unibe.ch
Abstract. In this paper we study the problem of image representation
learning without human annotation. By following the principles of selfsupervision, we build a convolutional neural network (CNN) that can
be trained to solve Jigsaw puzzles as a pretext task, which requires no
manual labeling, and then later repurposed to solve object classiﬁcation
and detection. To maintain the compatibility across tasks we introduce
the context-free network (CFN), a siamese-ennead CNN. The CFN takes
image tiles as input and explicitly limits the receptive ﬁeld (or context)
of its early processing units to one tile at a time. We show that the CFN
includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles,
we learn both a feature mapping of object parts as well as their correct
spatial arrangement. Our experimental evaluations show that the learned
features capture semantically relevant content. Our proposed method for
learning visual representations outperforms state of the art methods in
several transfer learning benchmarks.
Introduction
Visual tasks, such as object classiﬁcation and detection, have been successfully
approached through the supervised learning paradigm , where one
uses labeled data to train a parametric model. However, as manually labeled
data can be costly, unsupervised learning methods are gaining momentum.
Recently, Doersch et al. , Wang and Gupta and Agrawal et al. 
have explored a novel paradigm for unsupervised learning called self-supervised
learning. The main idea is to exploit diﬀerent labelings that are freely available
besides or within visual data, and to use them as intrinsic reward signals to learn
general-purpose features. uses the relative spatial co-location of patches in
images as a label. uses object correspondence obtained through tracking in
videos, and uses ego-motion information obtained by a mobile agent such as
the Google car . The features obtained with these approaches have been successfully transferred to classiﬁcation and detections tasks, and their performance
is very encouraging when compared to features trained in a supervised manner.
A fundamental diﬀerence between and is that the former method
uses single images as the training set and the other two methods exploit multiple images related either through a temporal or a viewpoint transformation.
 
M. Noroozi and P. Favaro
Fig. 1: Learning image representations by solving Jigsaw puzzles. (a) The image
from which the tiles (marked with green lines) are extracted. (b) A puzzle obtained by shuﬄing the tiles. Some tiles might be directly identiﬁable as object
parts, but others are ambiguous (e.g., have similar patterns) and their identi-
ﬁcation is much more reliable when all tiles are jointly evaluated. In contrast,
with reference to (c), determining the relative position between the central tile
and the top two tiles from the left can be very challenging .
While it is true that biological agents typically make use of multiple images and
also integrate additional sensory information, such as ego-motion, it is also true
that single snapshots may carry more information than we have been able to extract so far. This work shows that this is indeed the case. We introduce a novel
self-supervised task, the Jigsaw puzzle reassembly problem (see Fig. 1), which
builds features that yield high performance when transferred to detection and
classiﬁcation tasks.
We argue that solving Jigsaw puzzles can be used to teach a system that
an object is made of parts and what these parts are. The association of each
separate puzzle tile to a precise object part might be ambiguous. However, when
all the tiles are observed, the ambiguities might be eliminated more easily because the tile placement is mutually exclusive. This argument is supported by
our experimental validation. Training a Jigsaw puzzle solver takes about 2.5
days compared to 4 weeks of . Also, there is no need to handle chromatic
aberration or to build robustness to pixelation. Moreover, the features are highly
transferrable to detection and classiﬁcation and yield the highest performance
to date for an unsupervised method.
Related work
This work falls in the area of representation/feature learning, which is an unsupervised learning problem . Representation learning is concerned with building
intermediate representations of data useful to solve machine learning tasks. It
also involves transfer learning , as one applies and repurposes features that
have been learned by solving the Jigsaw puzzle to other tasks such as object
classiﬁcation and detection. In our experiments we do so via the pre-training +
Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
ﬁne-tuning scheme, as in prior work . Pre-training corresponds to the feature
learning that we obtain with our Jigsaw puzzle solver. Fine-tuning is instead the
process of updating the weights obtained during pre-training to solve another
task (object classiﬁcation or detection).
Unsupervised Learning. There is a rich literature in unsupervised learning
of visual representations . Most techniques build representations by exploiting general-purpose priors such as smoothness, sharing of factors, factors organized hierarchically, belonging to a low-dimension manifold, temporal and spatial coherence, and sparsity. Unfortunately, a general criterion to design a visual
representation is not available. Nonetheless, a natural choice is the goal of disentangling factors of variation. For example, several factors such as the object
shapes, the object materials, and the light sources, combine to create complex
eﬀects such as shadows, shading, color patterns and reﬂections in images. Ideal
features would separate each of these factors so that other learning tasks (e.g.,
classiﬁcation based on just shape or surface materials) can be handled more
easily. In this work we design features to separate the appearance from the arrangement (geometry) of parts of objects.
Because of the relevance to contemporary research and to this work, we
discuss mainly methods in deep learning. In general one can group unsupervised learning methods into: probabilistic, direct mapping (autoencoders), and
manifold learning ones. Probabilistic methods divide variables of a network into
observed and latent ones. Learning is then associated with determining model
parameters that maximize the likelihood of the latent variables given the observations. A family of popular probabilistic models is the Restricted Boltzmann
Machine (RBM) , which makes training tractable by imposing a bipartite graph between latent and observed variables. Unfortunately, these models
become intractable when multiple layers are present and are not designed to produce features in an eﬃcient manner. The direct mapping approach focuses on
the latter aspect and is typically built via autoencoders . Autoencoders
specify explicitly the feature extraction function (encoder) in a parametric form
as well as the mapping from the feature back to the input (decoder). These direct mappings are trained by minimizing the reconstruction error between the
input and the output produced by the autoencoder (obtained by applying the
encoder and decoder sequentially). A remarkable example of a very large scale
autoencoder is the work of Le et al. . Their results showed that robust human
and cat faces as well as human body detectors could be built without human
If the data structure suggests that data points might concentrate around a
manifold, then manifold learning techniques can be employed . This representation allows to map directly smooth variations of the factors to smooth
variations of the observations. Some of the issues with manifold learning techniques are that they might require computing nearest neighbors (which scales
quadratically with the number of samples) and that they need a suﬃciently
high density of samples around the manifold (and this becomes more diﬃcult to
achieve with high-dimensional manifolds).
M. Noroozi and P. Favaro
In the context of Computer Vision, it is worth mentioning some early work on
unsupervised learning of models for classiﬁcation. For instance introduced
methods to build a probabilistic representation of objects as constellations of
parts. A limitation is the high computational complexity of these models. As we
will see later, training the Jigsaw puzzle solver also amounts to building a model
of both appearance and conﬁguration of the parts.
Self-supervised Learning. This learning strategy is a recent variation on the
unsupervised learning theme that exploits labeling that comes for “free” with
the data . We make a distinction between labels that are easily accessible
and are associated with a non-visual signal (for example, ego-motion , but also
one could consider audio, text and so on), and labels that are obtained from the
structure of the data . Our work relates to the latter case as we simply
re-use the input images and exploit the pixel arrangement as a label.
Doersch et al. train a convolutional network to classify the relative position between two image patches. One tile is kept in the middle of a 3 × 3 grid
and the other tile can be placed in any of the other 8 available locations (up to
some small random shift). In Fig. 1 (c) we show an example where the relative
location between the central tile and the top-left and top-middle tiles is ambiguous. In contrast, the Jigsaw puzzle problem is solved by observing all the tiles
at the same time. This allows the trained network to intersect all ambiguity sets
and possibly reduce them to a singleton.
The method of Wang and Gupta builds a metric to deﬁne similarity between patches. Three patches are used as input, where two patches are matched
via tracking in a video and the third one is arbitrarily chosen. The main advantage of this method is that labeling requires just using a tracking method (they
use SURF interest points to detect initial bounding boxes and then tracking
via the KCF method ). The matched patches will have intraclass variability
due to changes in illumination, occlusion, viewpoint, pose, occlusions, and clutter factors. However, because the underlying object is the same, the estimated
features may not necessarily cluster patches with two diﬀerent instances of the
same object (i.e., based on their semantic content). The method proposed by
Agrawal et al. exploits labeling (egomotion) provided by other sensors. The
advantage is that this labeling is freely available in most cases or is quite easy to
obtain. They show that egomotion is a useful supervisory signal when learning
features. They train a siamese network to estimate egomotion from two image
frames and compare it to the egomotion measured with odometry sensors. The
trained features will build an invariance similar to that of . However, because
the object identity is the same in both images, the intraclass variability may be
limited. With two images of the same instance, learned features focus on their
similarities (such as color and texture) rather than their high-level structure. In
contrast, the Jigsaw puzzle approach ignores similarities between tiles (such as
color and texture), as they do not help their localization, and focuses instead
on their diﬀerences. In Fig. 2 we illustrate this concept with two examples: Two
cars that have diﬀerent colors and two dogs with diﬀerent fur patterns. The
features learned to solve puzzles in one (car/dog) image will apply also to the
Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
Fig. 2: Most of the shape of these 2 pairs of images is the same (two separate
instances within the same categories). However, some low-level statistics are
diﬀerent (color and texture). The Jigsaw puzzle solver learns to ignore such
statistics when they do not help the localization of parts.
other (car/dog) image as they will be invariant to shared patterns. The ability
of the Jigsaw puzzle solver to cluster together object parts can be seen in the
top 16 activations shown in Fig. 4 and in the image retrieval samples in Fig. 5.
Jigsaw Puzzles. Jigsaw puzzles have been associated with learning since their
inception. They were introduced in 1760 by John Spilsbury as a pretext to help
children learn geography. The ﬁrst puzzle was a map attached to a wooden
board, which was then sawed in pieces corresponding to countries . Studies
in Psychonomic show that Jigsaw puzzles can be used to assess visuospatial
processing in humans . Indeed, the Hooper Visual Organization Test is
routinely used to measures an individual’s ability to organize visual stimuli. This
test uses puzzles with line drawings of simple objects and requires the patient to
recognize the object without moving the tiles. Instead of using Jigsaw puzzles to
assess someone’s visuospatial processing ability, in this paper we propose to use
Jigsaw puzzles to develop a visuospatial representation of objects in the context
There is also a sizeable literature on solving Jigsaw puzzles computationally
(see, for example, ). However, these methods rely on the shape of the
tiles or on texture especially in the proximity of the borders of the tiles. These
are cues that we avoid when training the Jigsaw puzzle solver, as they do not
carry useful information when learning a part detector.
Solving Jigsaw Puzzles
At the present time, the design of convolutional neural networks (CNN) is still
an art that relies on extensive experience. Here we provide a brief discussion of
how we arrived at a convolutional architecture capable of solving Jigsaw puzzles
while learning general-purpose features.
An immediate approach to solve Jigsaw puzzles is to stack the tiles of the
puzzle along the channels (i.e., the input data would have 9 × 3 = 27 channels)
and then correspondingly increase the depth of the ﬁlters of the ﬁrst convolutional layer in AlexNet . The problem with this design is that the network
prefers to identify correlations between low-level texture statistics across tiles
M. Noroozi and P. Favaro
rather than between the high-level primitives. Low-level statistics, such as similar structural patterns and texture close to the boundaries of the tile, are simple
cues that humans actually use to solve Jigsaw puzzles. However, solving a Jigsaw puzzle based on these cues does not require any understanding of the global
object. Thus, here we present a network that delays the computation of statistics across diﬀerent tiles (see Fig. 3). The network ﬁrst computes features based
only on the pixels within each tile (one row in Fig. 3). Then, it ﬁnds the parts
arrangement just by using these features (last fully connected layers in Fig. 3).
The objective is to force the network to learn features that are as representative
and discriminative as possible of each object part for the purpose of determining
their relative location.
The Context-Free Architecture
We build a siamese-ennead convolutional network (see1 Fig. 3), where each row
up to the ﬁrst fully connected layer (fc6) uses the AlexNet architecture with
shared weights. Similar schemes were used in prior work . The outputs
of all fc6 layers are concatenated and given as input to fc7. All the layers in the
rows share the same weights up to and including fc6.
We call this architecture the context-free network (CFN) because the data
ﬂow of each patch is explicitly separated until the fully connected layer and
context is handled only in the last fully connected layers. We verify that this
architecture performs as well as AlexNet in the classiﬁcation task on the ImageNet 2012 dataset . In this test we resize the input images to 225 × 225
pixels, split them into a 3 × 3 grid and then feed the full 75 × 75 tiles to the
network. We ﬁnd that the CFN achieves 57.1% top-1 accuracy while AlexNet
achieves 57.4% top-1 accuracy. However, the CFN architecture is more compact
than AlexNet. It depends on only 27.5M parameters, while AlexNet uses 61M
parameters. The fc6 layer includes 4×4×256×512 ∼2M parameters while the
fc6 layer of AlexNet includes 6 × 6 × 256 × 4096 ∼37.5M parameters. However,
the fc7 layer in our architecture includes 2M parameters more than the same
layer in AlexNet.
This network can thus be used interchangeably for diﬀerent tasks including
detection and classiﬁcation. In the next section we show how to train the CFN
for the Jigsaw puzzle reassembly.
The Jigsaw Puzzle Task
To train the CFN we deﬁne a set of Jigsaw puzzle permutations, e.g., a tile
conﬁguration S = (3, 1, 2, 9, 5, 4, 8, 7, 6), and assign an index to each entry. We
randomly pick one such permutation, rearrange the 9 input patches according to
1 In earlier versions of this publication we reported transfer learning results where
AlexNet had a stride 2 in the ﬁrst convolutional layer as used during the training
for the puzzle task. This arXiv version introduces new updated results. See Fig. 3
caption for more information and the Experiments section.
Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
Fig. 3: Context Free Network. The ﬁgure illustrates how a puzzle is generated
and solved. We randomly crop a 225 × 225 pixel window from an image (red
dashed box), divide it into a 3 × 3 grid, and randomly pick a 64 × 64 pixel tiles
from each 75 × 75 pixel cell. These 9 tiles are reordered via a randomly chosen
permutation from a predeﬁned permutation set and are then fed to the CFN. The
task is to predict the index of the chosen permutation (technically, we deﬁne as
output a probability vector with 1 at the 64-th location and 0 elsewhere). The
CFN is a siamese-ennead CNN. For simplicity, we do not indicate the maxpooling and ReLU layers. These shared layers are implemented exactly as in
AlexNet . In the transfer learning experiments we show results with
the trained weights transferred on AlexNet (precisely, stride 4 on the
ﬁrst layer). The training in the transfer learning experiment is the
same as in the other competing methods. Notice instead, that during
the training on the puzzle task, we set the stride of the ﬁrst layer of
the CFN to 2 instead of 4.
that permutation, and ask the CFN to return a vector with the probability value
for each index. Given 9 tiles, there are 9! = 362,880 possible permutations. From
our experimental validation, we found that the permutation set is an important
factor on the performance of the representation that the network learns. We
perform an ablation study on the impact of the permutation set in subsection 4.2.
Training the CFN
The output of the CFN can be seen as the conditional probability density function (pdf) of the spatial arrangement of object parts (or scene parts) in a partbased model, i.e.,
p(S|A1, A2, . . . , A9) = p(S|F1, F2, . . . , F9)
where S is the conﬁguration of the tiles, Ai is the i-th part appearance of the
object, and {Fi}i=1,...,9 form the intermediate feature representation. Our objective is to train the CFN so that the features Fi have semantic attributes that
can identify the relative position between parts.
M. Noroozi and P. Favaro
Given the limited amount of data that we can use to build an approximation
of this very high-dimensional pdf, close attention must be paid to the training
strategy. One problem is when the CFN learns to associate each appearance Ai to
an absolute position. In this case, the features Fi would carry no semantic
meaning, but just information about an arbitrary 2D position. This
problem could happen if we generate just 1 Jigsaw puzzle per image. Then, the
CFN could learn to cluster patches only based on their absolute position in the
puzzle, and not on their textural/structural content. If we write the conﬁguration
S as a list of tile positions S = (L1, . . . , L9) then in this case the conditional pdf
p(S|F1, F2, . . . , F9) would factorize into independent terms
p(L1, . . . , L9|F1, F2, . . . , F9) =
where each tile location Li is fully determined by the corresponding feature Fi.
More in general, a self-supervised learning system might lead to representations that are suitable to solve the pre-text task, but not the target tasks, e.g.,
object classiﬁcation, detection, and segmentation. In this regard, an important
factor to learn better representations is to prevent our model from taking these
undesirable solutions, such as the one just described above, to solve the pre-text
task. We call these solutions shortcuts. Other shortcuts that the model can use to
solve the Jigsaw puzzle task include exploiting low-level statistics, such as edge
continuity, the pixel intensity/color distribution, and chromatic aberration.
To avoid shortcuts we employ multiple techniques. To prevent mapping the
appearance to an absolute position we feed multiple Jigsaw puzzles of the same
image to the CFN (an average of 69 out of 1000 possible puzzle conﬁgurations)
and make sure that the tiles are shuﬄed as much as possible by choosing conﬁgurations with suﬃciently large average Hamming distance. In this way the same
tile would have to be assigned to multiple positions (possibly all 9) thus making the mapping of features Fi to any absolute position equally likely. To avoid
shortcuts due to edge continuity and pixel intensity distribution we also leave a
random gap between the tiles. This discourages the CFN from learning low-level
statistics and was also done in . During training we resize each input image
until either the height or the width matches 256 pixels and preserve the original aspect ratio. Then, we crop a random region from the resized image of size
225 × 225 and split it into a 3 × 3 grid of 75 × 75 pixels tiles. We then extract a
64 × 64 region from each tile by introducing random shifts and feed them to the
network. Thus, we have an average gap of 11 pixels between the tiles. However,
the gaps may range from a minimum of 0 pixels to a maximum of 22 pixels. To
avoid shortcuts due to chromatic aberration we jitter the color channels and use
grayscale images (see more details in the Experiments section). In subsection 4.2
we perform ablation studies on the techniques we use to prevent the shortcuts.
We used Caﬀe and modiﬁed the code to choose random image patches
and permutations during the training time. This allowed us to keep the dataset
small (1.3M images from ImageNet) and the training eﬃcient, while the CFN
Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
could see an average of 69 diﬀerent puzzles per image (that is about 90M diﬀerent
Jigsaw puzzles).
Implementation Details
We use stochastic gradient descent without batch normalization on one
Titan X GPU. The training uses 1.3M color images of 256×256 pixels and minibatches with a batch size of 256 images. The images are resized by preserving
the aspect ratio until either the height or the width matches 256 pixels. Then
the other dimension is cropped to 256 pixels. The training converges after 350K
iterations with a basic learning rate of 0.01 and takes 59.5 hours in total (∼2.5
days). If we take 122% = 3072cores@1000Mhz
2880cores@875Mhz = 6,144GFLOPS
5,040GFLOPS as the best possible
performance ratio between the Titan X and the Tesla K40 (used for ) we can
predict that the CFN would have taken ∼72.5 hours (∼3 days) on a Tesla K40.
We compute that on average each image is used 350K × 256/1.3M ≃69 times.
That is, we solve on average 69 Jigsaw puzzles per image.
Experiments
We ﬁrst evaluate the performance of our learned representations on diﬀerent
transfer learning benchmarks. We then perform ablation studies on our proposed
method. We also visualize the neurons of the intermediate layers of our network.
Finally, we compare our features with those of both qualitatively and
quantitatively on image retrieval.
Transfer Learning
We evaluate our learned features as pre-trained weights for classiﬁcation, detection, and semantic segmentation tasks on the PASCAL VOC dataset . We
also introduce a novel benchmark to evaluate methods for unsupervised/selfsupervised representation learning. After training the CFN on the self-supervised
learning task, we use the CFN weights to initialize all the conv layers of a standard AlexNet network (stride 4 on the ﬁrst layer). Then, we retrain the rest of
the network from scratch (Gaussian noise as initial weights) for object classiﬁcation on ImageNet dataset. Notice that while during the training of the Jigsaw
task we use stride 2 in the ﬁrst layer of our CFN, we use a standard AlexNet
(stride 4 on the ﬁrst layer) to make the comparison with competing methods in
all the experiments directly comparable.
Pascal VOC We ﬁne-tune the Jigsaw task features on the classiﬁcation task
on PASCAL VOC 2007 by using the framework of Kr¨ahenb¨uhl et al. and
on the object detection task by using the Fast R-CNN framework. We also
ﬁne-tune our weights for the semantic segmentation task using the framework
 on the PASCAL VOC 2012 dataset. Because our fully connected layers are
M. Noroozi and P. Favaro
Table 1: Results on PASCAL VOC 2007 Detection and Classiﬁcation. The results
of the other methods are taken from Pathak et al. .
Pretraining time
Supervision
Classiﬁcation
Segmentation
Krizhevskyet al. 
1000 class labels
Wang and Gupta 
Doersch et al. 
Pathak et al. 
diﬀerent from those of the standard AlexNet, we select one row of the CFN (up
to conv5), copy only the weights of the convolutional layers, and ﬁll the fully
connected layers with Gaussian random weights with mean 0.1 and standard
deviation 0.001. The results are summarized in Table 1.
Our features achieve 53.2% mAP using multi-scale training and testing,
67.6% in classiﬁcation, and 37.6% in semantic segmentation thus outperforming
all other methods and closing the gap with features obtained with supervision.
ImageNet Classiﬁcation Yosinski et al. have shown that the last layers
of AlexNet are speciﬁc to the task and dataset used for training, while the ﬁrst
layers are general-purpose. In the context of transfer learning, this transition
from general-purpose to task-speciﬁc determines where in the network one should
extract the features. In this section we try to understand where this transition
occurs in our learned representation. We repurpose our weights, , and to
the classiﬁcation task on the ImageNet 2012 dataset . Table 2 summarizes the
results. The analysis consists of training each network with the labeled data from
ImageNet 2012 by locking a subset of the layers and by initializing the unlocked
layers with random values. If we train AlexNet, we obtain the reference maximum
accuracy of 57.4%. Our method achieves 34.6% when only fully connected layers
are trained. There is a signiﬁcant improvement (from 34.6% to 45.3%) when
the conv5 layer is also trained. This shows that the conv5 layer starts to be
specialized on the Jigsaw puzzle reassembly task.
We also perform a novel experiment to understand whether semantic classiﬁcation is useful to solve Jigsaw puzzles, and thus to see how much object
classiﬁcation and Jigsaw puzzle reassembly tasks are related. We take the pretrained AlexNet and transfer its features to solve Jigsaw puzzles. We also use
the same locking scheme to see the transferability of features at diﬀerent layers.
The performance is shown in Table 3. Compared to the maximum accuracy of
the Jigsaw task, 88%, we can see that semantic training is quite helpful towards
recognizing object parts. Indeed, the performance is very high up to conv4.
Ablation Studies
We perform ablation studies on our proposed methods to show the impact of
each component during the training of Jigsaw task. We train under diﬀerent
Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
Table 2: Comparison of classiﬁcation results on ImageNet 2012 . The numbers
are obtained by averaging 10 random crops predictions.
Doersch et al. 
Wang and Gupta 
Table 3: Transfer learning of AlexNet from a classiﬁcation task to the Jigsaw
puzzle reassembly problem. The j-th column indicates that all layers from conv1
to conv-j were locked and all subsequent layers were randomly initialized and
retrained. Notice how the ﬁrst 4 layers provide very good features for solving
puzzles. This shows that object classiﬁcation and the Jigsaw puzzle problems
are related.
AlexNet 
scenarios and evaluate the performance on detection task on PASCAL VOC
Permutation Set. The permutation set controls the ambiguity of the task.
If the permutations are close to each other, the Jigsaw puzzle task is more
challenging and ambiguous. For example, if the diﬀerence between two diﬀerent
permutations lies only in the position of two tiles and there are two similar
tiles in the image, the prediction of the right solution will be impossible. The
challenge here is a weaker version of what happens in the method of Doersch et
al. . To show this issue quantitatively, we compare the performance of the
learned representation on the PASCAL VOC 2007 detection task by generating
several permutation sets based on the following three criteria:
I) Cardinality. We train the network with a diﬀerent number of permutations
and see what impact this has on the learned features. We ﬁnd that as the total
number of permutations increases, the training on the Jigsaw task becomes more
and more diﬃcult. Also, we ﬁnd that the performance of the detection task
increases with a growing number of permutations.
II) Average Hamming distance. We use a subset of 1000 permutations and select
them based on their Hamming distance (i.e., the number of diﬀerent tile locations between 2 permutations S1 and S2). One can see that the average Hamming distance between permutations controls the diﬃculty of the Jigsaw puzzle
M. Noroozi and P. Favaro
Table 4: Ablation study on the impact of the permutation set.
Average hamming
Minimum hamming
Jigsaw task
permutations
performance
Table 5: Ablation study on the impact of the shortcuts.
Normalization
Color jittering
Jigsaw task accuracy
Detection performance
Algorithm 1. Generation of the maximal Hamming distance permutation set
\\ number of permutations
\\ maximal permutation set
1: ¯P ←all permutations [ ¯P1, . . . , ¯P9!]
\\ ¯P is a 9 × 9! matrix
3: j ∼U[1, 9!]
\\ uniform sample out of 9! permutations
P ←[P ¯Pj]
\\ add permutation ¯Pj to P
¯P ←[ ¯P1, . . . , ¯Pj−1, ¯Pj+1, . . . ]
\\ remove ¯Pj from ¯P
D ←Hamming(P, P ′)
\\ D is an i × (9! −i) matrix
\\ ¯D is a 1 × (9! −i) row vector
j ←arg maxk ¯Dk
\\ ¯Dk denotes the k-th entry of ¯D
12: until i ≤N
reassembly task, and it also correlates with the object detection performance.
We ﬁnd that as the average Hamming distance increases, the CFN yields lower
Jigsaw puzzle solving errors and lower object detection errors with ﬁne-tuning.
In the Experiments section we compare the performance on object detection of
CFNs trained with 3 choices for the Hamming distance: minimal, average and
maximal (see Table 4). From those tests we can see that large Hamming distances are desirable. We generate this permutation set iteratively via a greedy
algorithm. We begin with an empty permutation set and at each iteration select
the one that has the desired Hamming distance to the current permutation set.
Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
Algorithm 1 provides more details about the algorithm. For the minimal and
middle case, the arg maxk function at line 10 is replaced by arg mink and uniform sampling respectively. Note that the permutation set is generated before
III) Minimum hamming distance. To increase the minimum possible distance
between permutations, we remove similar permutations in a maximal set with
100 initial entries. As argued before, the minimum distance helps to make the
task less ambiguous. The performance results showing the impact of each component are summarized in Table 4. The best performing permutation set is a
trade oﬀbetween the number of permutations and how dissimilar they are from
each other.
The outcome of this ablation study seems to point to the following ﬁnal
consideration:
A good self-supervised task is neither simple nor ambiguous.
Preventing Shortcuts In a self-supervised learning method, shortcuts exploit
information useful for solving the pre-text task, but not for a target task, such
as detection. Similar to , we experimentally show that the CFN can take the
following shortcuts to solve the Jigsaw Puzzle task:
Low level statistics. Adjacent patches include similar low-level statistics like the
mean and standard deviation of the pixel intensities. This allows the model to
ﬁnd the arrangement of the patches. To avoid this shortcut, we normalize the
mean and the standard deviation of each patch independently.
Edge continuity. A strong cue to solve Jigsaw puzzles is the continuity of edges.
We select the 64×64 pixel tiles randomly from the 85×85 pixel cells. This allows
us to have a 21 pixel gap between tiles.
Chromatic Aberration. Chromatic aberration is a relative spatial shift between
color channels that increases from the images center to the borders. This type of
distortion helps the network to estimate the tile positions. To avoid this shortcut,
we use three techniques: i) We crop the central square of the original image and
resize it to 255 × 255; ii) We train the network with both color and grayscale
images. Our training set is a composition of grayscale and color images with
a ratio of 30% to 70%; iii) We (spatially) jitter the color channels of the color
images of each tile randomly by ±0, ±1, ±2 pixels.
Table 5 shows the performance of transfer learning our CFN, trained under diﬀerent combinations of the above techniques to avoid shortcuts, to the
detection task on Pascal VOC.
M. Noroozi and P. Favaro
CFN ﬁlter activations
Some recent work has devoted eﬀorts towards the visualization of CNNs to better understand how they work and how we can exploit them . Some
of these works aim at obtaining the input image that best represents a category
according to a given neural network. This has shown that CNNs retain important
information about the categories. Here instead we analyze the CFN by considering the units at each layer as object part detectors as in . We extract 1M
patches from the ImageNet validation set (20 randomly sampled 64×64 patches)
and feed them as input to the CFN. At each layer (conv1, conv2, conv3, conv4,
conv5) we consider the outputs of one channel and compute their ℓ1 norm. We
then rank the patches based on the ℓ1 norm and select the top 16 ones that belong to diﬀerent images. Since each layer has several channels, we hand-pick the
6 most signiﬁcant ones. In Fig. 4 we show the top-16 activation patches for only 6
channels per layer. These activations show that the CFN features correspond to
patterns sharing similar shapes and that there is a good correspondence based
on object parts (in particular see the conv4 activations for dog parts). Some
channels seem to be good face detectors (see conv3, but the same detectors can
be seen in other channels, not shown, in conv4 and conv5) and others seem to
be good texture detectors (e.g., grass, water, fur). In Fig. 4(f) we also show the
ﬁlters of the conv1 layer of the CFN. We can see that these ﬁlters are quite
strong and our transfer learning experiments in the next sections show that they
are as eﬀective as those trained in a supervised manner.
Image Retrieval
We also evaluate the features qualitatively (see Fig. 5) and quantitatively (see
Fig. 6) for image retrieval with a simple image ranking.
We ﬁnd the nearest neighbors (NN) of pool5 features using the bounding
boxes of the PASCAL VOC 2007 test set as query and bounding boxes of the
trainval set as the retrieval entries. We discard bounding boxes with fewer than
10K pixels inside. In Fig. 5 we show some examples of image retrievals (top-4)
obtained by ranking the images based on the inner product between normalized
features of a query image and normalized features of the retrieval set. We can see
that the features of the CFN are very sensitive to objects with similar shape and
often these are within the same category. In Fig. 6 we compare CFN with the
pre-trained AlexNet, , , and AlexNet with random weights. The precisionrecall plots show that and CFN features perform equally well. However,
the real potential of CFN features is demonstrated when the feature metric is
learned. In Table 2 we can see how CFN features surpass other features trained
in an unsupervised way by a good margin. In that test the dataset (ImageNet)
is more challenging because there are more categories and the bounding box is
Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
(a) conv1 activations
(b) conv2 activations
(c) conv3 activations
(d) conv4 activations
(e) conv5 activations
(f) conv1 ﬁlters without color jittering
(g) conv1 ﬁlters with color jittering
Fig. 4: Visualization of the top 16 activations for 6 units of the conv1, conv2,
conv3, conv4, conv5 layers in our CFN trained without blocking chromatic aberration. (f),(g) we show the ﬁlters of conv1 trained without and with blocking
chromatic aberration. The selection of the top activations is identical to the visualization method of Girshick et al. , except that we compute the average
response rather than the maximum. We show some of the most signiﬁcant units.
We can see that in the ﬁrst (a) and second (b) layers the ﬁlters specialize on
diﬀerent types of textures. On the third layer (c) the ﬁlters become more specialized and we have a ﬁrst face detector (later layers will also have face detectors in
some units) and some part detectors (e.g., the bottom corner of the butterﬂies
wing). On the fourth layer (d) we have already quite a number of part detectors.
We purposefully choose all the dog part detectors: head top, head center, neck,
back legs, and front legs. Notice the intraclass variation of the parts. Lastly, the
ﬁfth convolutional layer (e) has some other part detectors and some scene part
detectors.
M. Noroozi and P. Favaro
Fig. 5: Image retrieval (qualitative evaluation). (a) query images; (b) top-4
matches with AlexNet; (c) top-4 matches with the CFN trained without blocking chromatic aberration; (d) top-4 matches with Doersch et al. ; (e) top-4
matches with Wang and Gupta ; (f) top-4 matches with AlexNet with random weights.
Supervised
Doersch et al
Wang and Gupta
Fig. 6: Image retrieval (quantitative evaluation). We compare the precision-recall
for image retrieval on the PASCAL VOC 2007. The ranking of the retrieved
images is based on the inner products between normalized features extracted
from a pre-trained AlexNet, the CFN, Doersch et al. , Wang and Gupta 
and from AlexNet with random weights. The performance of CFN and are
very similar when using this simple ranking metric. When the metric is instead
learned with two fully connected layers, then we see that CFN features yield a
clearly higher performance than all other features from self-supervised learning
methods (see Table 2).
Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
Conclusions
We have introduced the context-free network (CFN), a CNN whose features
can be easily transferred between detection/classiﬁcation and Jigsaw puzzle reassembly tasks. The network is trained in an unsupervised manner by using the
Jigsaw puzzle as a pretext task. We have built a training scheme that generates,
on average, 69 puzzles for 1.3M images and converges in only 2.5 days. The key
idea is that by solving Jigsaw puzzles the CFN learns to identify each tile as an
object part and how parts are assembled in an object. The learned features are
evaluated on both classiﬁcation and detection and the experiments show that
we outperform the previous state of the art. More importantly, the performance
of these features is closing the gap with those learned in a supervised manner.
We believe that there is a lot of untapped potential in self-supervised learning
and in the future it will provide a valid alternative to costly human annotation.