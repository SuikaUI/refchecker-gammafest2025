FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks
Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox
University of Freiburg, Germany
{ilg,mayern,saikiat,keuper,dosovits,brox}@cs.uni-freiburg.de
The FlowNet demonstrated that optical ﬂow estimation
can be cast as a learning problem. However, the state of
the art with regard to the quality of the ﬂow has still been
deﬁned by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete
with variational methods. In this paper, we advance the
concept of end-to-end learning of optical ﬂow and make it
work really well. The large improvements in quality and
speed are caused by three major contributions: ﬁrst, we
focus on the training data and show that the schedule of
presenting data during training is very important. Second,
we develop a stacked architecture that includes warping
of the second image with intermediate optical ﬂow. Third,
we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only
marginally slower than the original FlowNet but decreases
the estimation error by more than 50%. It performs on par
with state-of-the-art methods, while running at interactive
frame rates. Moreover, we present faster variants that allow optical ﬂow computation at up to 140fps with accuracy
matching the original FlowNet.
1. Introduction
The FlowNet by Dosovitskiy et al. represented a
paradigm shift in optical ﬂow estimation. The idea of using
a simple convolutional CNN architecture to directly learn
the concept of optical ﬂow from data was completely disjoint from all the established approaches. However, ﬁrst implementations of new ideas often have a hard time competing with highly ﬁne-tuned existing methods, and FlowNet
was no exception to this rule. It is the successive consolidation that resolves the negative effects and helps us appreciate the beneﬁts of new ways of thinking.
At the same time, it resolves problems with small displacements and noisy artifacts in estimated ﬂow ﬁelds. This
leads to a dramatic performance improvement on real-world
applications such as action recognition and motion segmentation, bringing FlowNet 2.0 to the state-of-the-art level.
FlowNet 2.0
Figure 1. We present an extension of FlowNet. FlowNet 2.0 yields
smooth ﬂow ﬁelds, preserves ﬁne motion details and runs at 8 to
140fps. The accuracy on this example is four times higher than
with the original FlowNet.
The way towards FlowNet 2.0 is via several evolutionary,
but decisive modiﬁcations that are not trivially connected
to the observed problems. First, we evaluate the inﬂuence
of dataset schedules. Interestingly, the more sophisticated
training data provided by Mayer et al. leads to inferior results if used in isolation. However, a learning schedule consisting of multiple datasets improves results signiﬁcantly. In this scope, we also found that the FlowNet version
with an explicit correlation layer outperforms the version
without such layer. This is in contrast to the results reported
in Dosovitskiy et al. .
As a second contribution, we introduce a warping operation and show how stacking multiple networks using this
operation can signiﬁcantly improve the results. By varying
the depth of the stack and the size of individual components
we obtain many network variants with different size and
runtime. This allows us to control the trade-off between accuracy and computational resources. We provide networks
for the spectrum between 8fps and 140fps.
Finally, we focus on small, subpixel motion and realworld data. To this end, we created a special training dataset
and a specialized network. We show that the architecture
trained with this dataset performs well on small motions
typical for real-world videos. To reach optimal performance
on arbitrary displacements, we add a network that learns to
fuse the former stacked network with the small displace-
 
Large Displacement
Brightness
Brightness
Brightness
Brightness
Displacement
Large Displacement
Small Displacement
FlowNet-SD
Figure 2. Schematic view of complete architecture: To compute large displacement optical ﬂow we combine multiple FlowNets. Braces
indicate concatenation of inputs. Brightness Error is the difference between the ﬁrst image and the second image warped with the previously
estimated ﬂow. To optimally deal with small displacements, we introduce smaller strides in the beginning and convolutions between
upconvolutions into the FlowNetS architecture. Finally we apply a small fusion network to provide the ﬁnal estimate.
ment network in an optimal manner.
The ﬁnal network outperforms the previous FlowNet by
a large margin and performs on par with state-of-the-art
methods on the Sintel and KITTI benchmarks. It can estimate small and large displacements with very high level
of detail while providing interactive frame rates.
2. Related Work
End-to-end optical ﬂow estimation with convolutional
networks was proposed by Dosovitskiy et al. in . Their
model, dubbed FlowNet, takes a pair of images as input
and outputs the ﬂow ﬁeld.
Following FlowNet, several
papers have studied optical ﬂow estimation with CNNs:
featuring a 3D convolutional network , an unsupervised learning objective , carefully designed rotationally invariant architectures , or a pyramidal approach
based on the coarse-to-ﬁne idea of variational methods .
None of these methods signiﬁcantly outperforms the original FlowNet.
An alternative approach to learning-based optical ﬂow
estimation is to use CNNs for matching image patches.
Thewlis et al. formulate Deep Matching as a convolutional network and optimize it end-to-end. Gadot &
Wolf and Bailer et al. learn image patch descriptors using Siamese network architectures. These methods
can reach good accuracy, but require exhaustive matching
of patches. Thus, they are restrictively slow for most practical applications. Moreover, patch based approaches lack
the possibility to use the larger context of the whole image
because they operate on small image patches.
Convolutional networks trained for per-pixel prediction
tasks often produce noisy or blurry results. As a remedy,
out-of-the-box optimization can be applied to the network
predictions as a postprocessing operation, for example, optical ﬂow estimates can be reﬁned with a variational approach .
In some cases, this reﬁnement can be approximated by neural networks: Chen & Pock formulate reaction diffusion model as a CNN and apply it to image denoising, deblocking and superresolution. Recently,
it has been shown that similar reﬁnement can be obtained
by stacking several convolutional networks on top of each
other. This led to improved results in human pose estimation and semantic instance segmentation . In
this paper we adapt the idea of stacking multiple networks
to optical ﬂow estimation.
Our network architecture includes warping layers that
compensate for some already estimated preliminary motion
in the second image. The concept of image warping is common to all contemporary variational optical ﬂow methods
and goes back to the work of Lucas & Kanade . In Brox
et al. it was shown to correspond to a numerical ﬁxed
point iteration scheme coupled with a continuation method.
The strategy of training machine learning models on a
series of gradually increasing tasks is known as curriculum
learning . The idea dates back at least to Elman ,
who showed that both the evolution of tasks and the network
architectures can be beneﬁcial in the language processing
scenario. In this paper we revisit this idea in the context
of computer vision and show how it can lead to dramatic
performance improvement on a complex real-world task of
optical ﬂow estimation.
3. Dataset Schedules
High quality training data is crucial for the success of
supervised training. We investigated the differences in the
quality of the estimated optical ﬂow depending on the presented training data. Interestingly, it turned out that not only
the kind of data is important but also the order in which it is
presented during training.
The original FlowNets were trained on the FlyingChairs dataset (we will call it Chairs). This rather simplistic dataset contains about 22k image pairs of chairs
superimposed on random background images from Flickr.
Random afﬁne transformations are applied to chairs and
background to obtain the second image and ground truth
ﬂow ﬁelds. The dataset contains only planar motions.
The FlyingThings3D (Things3D) dataset proposed by
Mayer et al. can be seen as a three-dimensional version
of the FlyingChairs. The dataset consists of 22k renderings
of random scenes showing 3D models from the ShapeNet
dataset moving in front of static 3D backgrounds. In
contrast to Chairs, the images show true 3D motion and
lighting effects and there is more variety among the object
We tested the two network architectures introduced by
Dosovitskiy et al. : FlowNetS, which is a straightforward encoder-decoder architecture, and FlowNetC, which
includes explicit correlation of feature maps. We trained
FlowNetS and FlowNetC on Chairs and Things3D and an
equal mixture of samples from both datasets using the different learning rate schedules shown in Figure 3. The basic
schedule Sshort (600k iterations) corresponds to Dosovitskiy et al. except some minor changes1. Apart from
this basic schedule Sshort, we investigated a longer schedule Slong with 1.2M iterations, and a schedule for ﬁnetuning Sﬁne with smaller learning rates. Results of networks trained on Chairs and Things3D with the different
schedules are given in Table 1. The results lead to the following observations:
The order of presenting training data with different
properties matters. Although Things3D is more realistic,
training on Things3D alone leads to worse results than training on Chairs. The best results are consistently achieved
when ﬁrst training on Chairs and only then ﬁne-tuning on
Things3D. This schedule also outperforms training on a
mixture of Chairs and Things3D. We conjecture that the
simpler Chairs dataset helps the network learn the general
concept of color matching without developing possibly confusing priors for 3D motion and realistic lighting too early.
The result indicates the importance of training data schedules for avoiding shortcuts when learning generic concepts
with deep networks.
1(1) We do not start with a learning rate of 1e −6 and increase it ﬁrst,
but we start with 1e −4 immediately. (2) We ﬁx the learning rate for 300k
iterations and then divide it by 2 every 100k iterations.
Learning Rate
Figure 3. Learning rate schedules: Sshort is similar to the schedule
in Dosovitskiy et al. . We investigated another longer version
Slong and a ﬁne-tuning schedule Sﬁne.
Architecture
Slong Sﬁne
Chairs→Things3D
Chairs→Things3D
Table 1. Results of training FlowNets with different schedules on
different datasets (one network per row). Numbers indicate endpoint errors on Sintel train clean. mixed denotes an equal mixture
of Chairs and Things3D. Training on Chairs ﬁrst and ﬁne-tuning
on Things3D yields the best results (the same holds when testing
on the KITTI dataset; see supplemental material). FlowNetC performs better than FlowNetS.
FlowNetC outperforms FlowNetS. The result we got
with FlowNetS and Sshort corresponds to the one reported
in Dosovitskiy et al. . However, we obtained much better results on FlowNetC. We conclude that Dosovitskiy et
al. did not train FlowNetS and FlowNetC under the
exact same conditions. When done so, the FlowNetC architecture compares favorably to the FlowNetS architecture.
Improved results. Just by modifying datasets and training schedules, we improved the FlowNetS result reported
by Dosovitskiy et al. by ∼25% and the FlowNetC result by ∼30%.
In this section, we did not yet use specialized training
sets for specialized scenarios. The trained network is rather
supposed to be generic and to work well in various scenarios. An additional optional component in dataset schedules
is ﬁne-tuning of a generic network to a speciﬁc scenario,
such as the driving scenario, which we show in Section 6.
Loss after
EPE on Chairs
EPE on Sintel
architecture
train clean
Net1 + Net2
Net1 + Net2
Net1 + Net2
Net1 + W + Net2
Net1 + W + Net2
Net1 + W + Net2
Evaluation of options when stacking two FlowNetS networks (Net1 and Net2). Net1 was trained with the Chairs→Things3D
schedule from Section 3. Net2 is initialized randomly and subsequently, Net1 and Net2 together, or only Net2 is trained on Chairs with
Slong; see text for details. When training without warping, the stacked network overﬁts to the Chairs dataset. The best results on Sintel are
obtained when ﬁxing Net1 and training Net2 with warping.
4. Stacking Networks
4.1. Stacking Two Networks for Flow Reﬁnement
All state-of-the-art optical ﬂow approaches rely on iterative methods . Can deep networks also beneﬁt
from iterative reﬁnement? To answer this, we experiment
with stacking multiple FlowNetS and FlowNetC architectures.
The ﬁrst network in the stack always gets the images I1
and I2 as input. Subsequent networks get I1, I2, and the
previous ﬂow estimate wi = (ui, vi)⊤, where i denotes the
index of the network in the stack.
To make assessment of the previous error and computing
an incremental update easier for the network, we also optionally warp the second image I2(x, y) via the ﬂow wi and
bilinear interpolation to ˜I2,i(x, y) = I2(x+ui, y+vi). This
way, the next network in the stack can focus on the remaining increment between I1 and ˜I2,i. When using warping, we
additionally provide ˜I2,i and the error ei = ||˜I2,i −I1|| as
input to the next network; see Figure 2. Thanks to bilinear
interpolation, the derivatives of the warping operation can
be computed (see supplemental material for details). This
enables training of stacked networks end-to-end.
Table 2 shows the effect of stacking two networks, the
effect of warping, and the effect of end-to-end training.
We take the best FlowNetS from Section 3 and add another FlowNetS on top. The second network is initialized
randomly and then the stack is trained on Chairs with the
schedule Slong. We experimented with two scenarios: keeping the weights of the ﬁrst network ﬁxed, or updating them
together with the weights of the second network. In the latter case, the weights of the ﬁrst network are ﬁxed for the ﬁrst
400k iterations to ﬁrst provide a good initialization of the
second network. We report the error on Sintel train clean
and on the test set of Chairs. Since the Chairs test set is
much more similar to the training data than Sintel, comparing results on both datasets allows us to detect tendencies to
over-ﬁtting.
We make the following observations: (1) Just stacking
networks without warping improves results on Chairs but
decreases performance on Sintel, i.e. the stacked network
is over-ﬁtting. (2) With warping included, stacking always
improves results. (3) Adding an intermediate loss after Net1
is advantageous when training the stacked network end-toend. (4) The best results are obtained when keeping the ﬁrst
network ﬁxed and only training the second network after the
warping operation.
Clearly, since the stacked network is twice as big as the
single network, over-ﬁtting is an issue. The positive effect
of ﬂow reﬁnement after warping can counteract this problem, yet the best of both is obtained when the stacked networks are trained one after the other, since this avoids over-
ﬁtting while having the beneﬁt of ﬂow reﬁnement.
4.2. Stacking Multiple Diverse Networks
Rather than stacking identical networks, it is possible to
stack networks of different type (FlowNetC and FlowNetS).
Reducing the size of the individual networks is another valid
option. We now investigate different combinations and additionally also vary the network size.
We call the ﬁrst network the bootstrap network as it
differs from the second network by its inputs.
The second network could however be repeated an arbitray number of times in a recurrent fashion. We conducted this experiment and found that applying a network with the same
weights multiple times and also ﬁne-tuning this recurrent
part does not improve results (see supplemental material for
details). As also done in , we therefore add networks
with different weights to the stack. Compared to identical
weights, stacking networks with different weights increases
the memory footprint, but does not increase the runtime. In
this case the top networks are not constrained to a general
improvement of their input, but can perform different tasks
at different stages and the stack can be trained in smaller
Number of Channels Multiplier
EPE on Sintel train clean
Network Forward Pass Time
Figure 4. Accuracy and runtime of FlowNetS depending on the
network width. The multiplier 1 corresponds to the width of the
original FlowNet architecture. Wider networks do not improve the
accuracy. For fast execution times, a factor of 3
8 is a good choice.
Timings are from an Nvidia GTX 1080.
pieces by ﬁxing existing networks and adding new networks
one-by-one.
We do so by using the Chairs→Things3D
schedule from Section 3 for every new network and the
best conﬁguration with warping from Section 4.1. Furthermore, we experiment with different network sizes and alternatively use FlowNetS or FlowNetC as a bootstrapping
network. We use FlowNetC only in case of the bootstrap
network, as the input to the next network is too diverse to be
properly handeled by the Siamese structure of FlowNetC.
Smaller size versions of the networks were created by taking only a fraction of the number of channels for every layer
in the network. Figure 4 shows the network accuracy and
runtime for different network sizes of a single FlowNetS.
8 yields a good trade-off between speed and accuracy when aiming for faster networks.
Chairs→Things3D schedule from Section 3 starting
with FlowNet2.
Networks in a stack are trained with
this schedule one-by-one. For the stack conﬁguration we
append upper- or lower-case letters to indicate the original
FlowNet or the thin version with 3
8 of the channels. E.g:
FlowNet2-CSS stands for a network stack consisting of
one FlowNetC and two FlowNetS. FlowNet2-css is the
same but with fewer channels.
Table 3 shows the performance of different network
stacks. Most notably, the ﬁnal FlowNet2-CSS result improves by ∼30% over the single network FlowNet2-C from
Section 3 and by ∼50% over the original FlowNetC .
Furthermore, two small networks in the beginning always outperform one large network, despite being faster
and having fewer weights: FlowNet2-ss (11M weights)
over FlowNet2-S (38M weights), and FlowNet2-cs (11M
weights) over FlowNet2-C (38M weights). Training smaller
units step by step proves to be advantageous and enables
Number of Networks
Architecture
Architecture
Architecture
Architecture
Table 3. Results on Sintel train clean for some variants of stacked
FlowNet architectures following the best practices of Section 3
and Section 4.1. Each new network was ﬁrst trained on Chairs
with Slong and then on Things3D with Sﬁne (Chairs→Things3D
schedule). Forward pass times are from an Nvidia GTX 1080.
us to train very deep networks for optical ﬂow. At last,
FlowNet2-s provides nearly the same accuracy as the original FlowNet , while running at 140 frames per second.
5. Small Displacements
5.1. Datasets
While the original FlowNet performed well on the
Sintel benchmark, limitations in real-world applications
have become apparent. In particular, the network cannot
reliably estimate small motions (see Figure 1).
counter-intuitive, since small motions are easier for traditional methods, and there is no obvious reason why networks should not reach the same performance in this setting. Thus, we examined the training data and compared it
to the UCF101 dataset as one example of real-world
data. While Chairs are similar to Sintel, UCF101 is fundamentally different (we refer to our supplemental material for
the analysis): Sintel is an action movie and as such contains
many fast movements that are difﬁcult for traditional methods, while the displacements we see in the UCF101 dataset
are much smaller, mostly smaller than 1 pixel. Thus, we
created a dataset in the visual style of Chairs but with very
small displacements and a displacement histogram much
more like UCF101. We also added cases with a background
that is homogeneous or just consists of color gradients. We
call this dataset ChairsSDHom and will release it upon publication.
5.2. Small Displacement Network and Fusion
We ﬁne-tuned our FlowNet2-CSS network for smaller
displacements by further training the whole network
stack on a mixture of Things3D and ChairsSDHom
and by applying a non-linearity to the error to downweight large displacements2. We denote this network by
FlowNet2-CSS-ft-sd.
This increases performance on
small displacements and we found that this particular mixture does not sacriﬁce performance on large displacements.
However, in case of subpixel motion, noise still remains a
problem and we conjecture that the FlowNet architecture
might in general not be perfect for such motion. Therefore,
we slightly modiﬁed the original FlowNetS architecture and
removed the stride 2 in the ﬁrst layer. We made the beginning of the network deeper by exchanging the 7×7 and 5×5
kernels in the beginning with multiple 3×3 kernels2. Because noise tends to be a problem with small displacements,
we add convolutions between the upconvolutions to obtain
smoother estimates as in . We denote the resulting architecture by FlowNet2-SD; see Figure 2.
FlowNet2-CSS-ft-sd and FlowNet2-SD (see Figure 2). The
fusion network receives the ﬂows, the ﬂow magnitudes and
the errors in brightness after warping as input. It contracts
the resolution two times by a factor of 2 and expands again2.
Contrary to the original FlowNet architecture it expands to
the full resolution. We ﬁnd that this produces crisp motion
boundaries and performs well on small as well as on large
displacements. We denote the ﬁnal network as FlowNet2.
6. Experiments
We compare the best variants of our network to stateof-the-art approaches on public bechmarks. In addition, we
provide a comparison on application tasks, such as motion
segmentation and action recognition. This allows benchmarking the method on real data.
6.1. Speed and Performance on Public Benchmarks
We evaluated all methods3 on a system with an Intel
Xeon E5 with 2.40GHz and an Nvidia GTX 1080. Where
applicable, dataset-speciﬁc parameters were used, that yield
best performance. Endpoint errors and runtimes are given
in Table 4.
Sintel: On Sintel, FlowNet2 consistently outperforms
DeepFlow and EpicFlow and is on par with Flow-
Fields. All methods with comparable runtimes have clearly
inferior accuracy. We ﬁne-tuned FlowNet2 on a mixture
of Sintel clean+ﬁnal training data (FlowNet2–ft-sintel). On
the benchmark, in case of clean data this slightly degraded
the result, while on ﬁnal data FlowNet2–ft-sintel is on par
with the currently published state-of-the art method Deep-
DiscreteFlow .
KITTI: On KITTI, the results of FlowNet2-CSS are
comparable to EpicFlow and FlowFields .
2For details we refer to the supplemental material
3An exception is EPPM for which we could not provide the required
Windows environment and use the results from .
MPI Sintel (train ﬁnal)
Average EPE
Runtime (milliseconds per frame)
LDOF (GPU)
PCA-Layers
FN2-css-ft-sd
FN2-CSS-ft-sd
Figure 5. Runtime vs. endpoint error comparison to the fastest
existing methods with available code. The FlowNet2 family outperforms other methods by a large margin. The behaviour for the
KITTI dataset is the same; see supplemental material.
tuning on small displacement data degrades the result. This
is probably due to KITTI containing very large displacements in general.
Fine-tuning on a combination of the
KITTI2012 and KITTI2015 training sets reduces the error
roughly by a factor of 3 (FlowNet2-ft-kitti). Among nonstereo methods we obtain the best EPE on KITTI2012 and
the ﬁrst rank on the KITTI2015 benchmark. This shows
how well and elegantly the learning approach can integrate
the prior of the driving scenario.
Middlebury: On the Middlebury training set FlowNet2
performs comparable to traditional methods. The results on
the Middlebury test set are unexpectedly a lot worse. Still,
there is a large improvement compared to FlowNetS .
Endpoint error vs.
runtime evaluations for Sintel are
provided in Figure 4. One can observe that the FlowNet2
family outperforms the best and fastest existing methods
by large margins. Depending on the type of application,
a FlowNet2 variant between 8 to 140 frames per second can
6.2. Qualitative Results
Figures 6 and 7 show example results on Sintel and on
real-world data. While the performance on Sintel is similar to FlowFields , we can see that on real world data
FlowNet 2.0 clearly has advantages in terms of being robust
to homogeneous regions (rows 2 and 5), image and compression artifacts (rows 3 and 4) and it yields smooth ﬂow
ﬁelds with sharp motion boundaries.
6.3. Performance on Motion Segmentation and Action Recognition
To assess performance of FlowNet 2.0 in real-world applications, we compare the performance of action recognition and motion segmentation. For both applications, good
Sintel clean
Sintel ﬁnal
KITTI 2012
KITTI 2015
Middlebury
ms per frame
EpicFlow† 
DeepFlow† 
FlowFields 
LDOF (CPU) 
LDOF (GPU) 
PCA-Layers 
PCA-Flow 
DIS-Fast 
FlowNetS 
FlowNetC 
FlowNet 2.0
FlowNet2-s
FlowNet2-ss
FlowNet2-css
FlowNet2-css-ft-sd
FlowNet2-CSS
FlowNet2-CSS-ft-sd
FlowNet2-ft-sintel
FlowNet2-ft-kitti
Table 4. Performance comparison on public benchmarks. AEE: Average Endpoint Error; Fl-all: Ratio of pixels where ﬂow estimate is
wrong by both ≥3 pixels and ≥5%. The best number for each category is highlighted in bold. See text for details. †train numbers for
these methods use slower but better "improved" option. ‡For these results we report the ﬁne-tuned numbers (FlowNetS-ft and FlowNetC-ft).
Image Overlay
Ground Truth
FlowFields 
PCA-Flow 
FlowNetS 
(22,810ms)
Figure 6. Examples of ﬂow ﬁelds from different methods estimated on Sintel. FlowNet2 performs similar to FlowFields and is able to
extract ﬁne details, while methods running at comparable speeds perform much worse (PCA-Flow and FlowNetS).
optical ﬂow is key. Thus, a good performance on these tasks
also serves as an indicator for good optical ﬂow.
For motion
segmentation,
rely on the
wellestablished approach of Ochs et al. to compute long
term point trajectories. A motion segmentation is obtained
from these using the state-of-the-art method from Keuper et
al. . The results are shown in Table 5. The original
model in Ochs et al. was built on Large Displacement
Optical Flow . We included also other popular optical
ﬂow methods in the comparison. The old FlowNet 
was not useful for motion segmentation. In contrast, the
FlowNet2 is as reliable as other state-of-the-art methods
while being orders of magnitude faster.
Optical ﬂow is also a crucial feature for action recognition. To assess the performance, we trained the temporal stream of the two-stream approach from Simonyan et
al. with different optical ﬂow inputs. Table 5 shows
that FlowNetS did not provide useful results, while the
Image Overlay
FlowFields 
DeepFlow 
LDOF (GPU) 
PCA-Flow 
FlowNetS 
Figure 7. Examples of ﬂow ﬁelds from different methods estimated on real-world data. The top two rows are from the Middlebury data
set and the bottom three from UCF101. Note how well FlowNet2 generalizes to real-world data, i.e. it produces smooth ﬂow ﬁelds, crisp
boundaries and is robust to motion blur and compression artifacts. Given timings of methods differ due to different image resolutions.
ﬂow from FlowNet 2.0 yields comparable results to stateof-the art methods.
7. Conclusions
We have presented several improvements to the FlowNet
idea that have led to accuracy that is fully on par with stateof-the-art methods while FlowNet 2.0 runs orders of magnitude faster. We have quantiﬁed the effect of each contribution and showed that all play an important role. The experiments on motion segmentation and action recognition show
that the estimated optical ﬂow with FlowNet 2.0 is reliable
on a large variety of scenes and applications. The FlowNet
2.0 family provides networks running at speeds from 8 to
140fps. This further extends the possible range of applications. While the results on Middlebury indicate imperfect
performance on subpixel motion, FlowNet 2.0 results highlight very crisp motion boundaries, retrieval of ﬁne structures, and robustness to compression artifacts. Thus, we
expect it to become the working horse for all applications
that require accurate and fast optical ﬂow computation.
Acknowledgements
We acknowledge funding by the ERC Starting Grant
VideoLearn, the DFG Grant BR-3815/7-1, and the EU Hori-
Motion Seg.
Action Recog.
LDOF-CPU 
DeepFlow 
EpicFlow 
FlowFields 
FlowNetS 
FlowNet2-css-ft-sd
FlowNet2-CSS-ft-sd
Table 5. Motion segmentation and action recognition using different methods; see text for details. Motion Segmentation: We report results using on the training set of FBMS-59 
with a density of 4 pixels. Different densities and error measures
are given the supplemental material. “Extracted objects” refers to
objects with F ≥75%. ‡FlowNetS is evaluated on 28 out of 29
sequences; on the sequence lion02, the optimization did not converge even after one week. Action Recognition: We report classi-
ﬁcation accuracies after training the temporal stream of . We
use a stack of 5 optical ﬂow ﬁelds as input. Due to long training
times only selected methods could be evaluated. †To reproduce results from , for action recognition we use the OpenCV LDOF
implementation. Note the generally large difference for FlowNetS
and FlowNet2 and the performance compared to traditional methods.
zon2020 project TrimBot2020.