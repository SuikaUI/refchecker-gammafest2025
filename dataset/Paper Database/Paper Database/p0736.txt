IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 40, NO. 1, JANUARY 2010
RUSBoost: A Hybrid Approach to
Alleviating Class Imbalance
Chris Seiffert, Taghi M. Khoshgoftaar, Member, IEEE, Jason Van Hulse, Member, IEEE, and Amri Napolitano
Abstract—Class imbalance is a problem that is common to many
application domains. When examples of one class in a training
data set vastly outnumber examples of the other class(es), traditional data mining algorithms tend to create suboptimal classiﬁcation models. Several techniques have been used to alleviate
the problem of class imbalance, including data sampling and
boosting. In this paper, we present a new hybrid sampling/boosting
algorithm, called RUSBoost, for learning from skewed training
data. This algorithm provides a simpler and faster alternative to
SMOTEBoost, which is another algorithm that combines boosting
and data sampling. This paper evaluates the performances of
RUSBoost and SMOTEBoost, as well as their individual components (random undersampling, synthetic minority oversampling technique, and AdaBoost). We conduct experiments using
15 data sets from various application domains, four base learners,
and four evaluation metrics. RUSBoost and SMOTEBoost both
outperform the other procedures, and RUSBoost performs comparably to (and often better than) SMOTEBoost while being a
simpler and faster technique. Given these experimental results,
we highly recommend RUSBoost as an attractive alternative for
improving the classiﬁcation performance of learners built using
imbalanced data.
Index Terms—Binary classiﬁcation, boosting, class imbalance,
RUSBoost, sampling.
I. INTRODUCTION
REATING an effective classiﬁcation model can be a
challenging endeavor if the data used to train the model
are imbalanced. When examples of one class greatly outnumber examples of the other class(es), traditional data mining
algorithms tend to favor classifying examples as belonging
to the overrepresented (majority) class. Such a model would
be ineffective at identifying examples of the minority class,
which is frequently the class of interest (the positive class).
Typically, it is the examples of this positive class that carry the
highest cost of misclassiﬁcation. Therefore, new techniques are
required to ensure that a model can effectively identify these
most important yet rarely occurring examples.
Many techniques have been proposed to alleviate the problem of class imbalance. Two such techniques are data sampling
and boosting . Data sampling balances the class distribution
Manuscript received April 18, 2008. First published October 30, 2009;
current version published December 16, 2009. This paper was recommended
by Associate Editor M. Dorneich.
C. Seiffert, deceased, was with the Department of Computer Science and
Engineering, Florida Atlantic University, Boca Raton, FL 33431 USA (e-mail:
 ).
T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano are with the Department of Computer Science and Engineering, Florida Atlantic University, Boca
Raton, FL 33431 USA (e-mail: ; ;
 ).
Digital Object Identiﬁer 10.1109/TSMCA.2009.2029559
in the training data by either adding examples to the minority
class (oversampling) or removing examples from the majority class (undersampling). Several techniques for performing
undersampling and oversampling have been proposed. The
simplest method for resampling a data set is random resampling. Random oversampling balances a data set by duplicating
examples of the minority class until a desired class ratio is
achieved. Similarly, random undersampling (RUS) removes
examples (randomly) from the majority class until the desired
balance is achieved. There are also “intelligent” methods for
oversampling and undersampling.
Both undersampling and oversampling have their beneﬁts
and drawbacks. The main drawback associated with undersampling is the loss of information that comes with deleting
examples from the training data . It has the beneﬁt, however,
of decreasing the time required to train the models since the
training data set size is reduced. Consider a data set with
20 000 examples, where 400 (2%) of the examples belong to
the positive class, while 19 600 examples belong to the negative
class. After performing undersampling to achieve a balanced
(50 : 50) class distribution, the new training data set will contain
only 800 examples (400 positive and 400 negative). The beneﬁt
of using undersampling to balance the class distribution in this
data set is that the time required to train the classiﬁer will
be relatively short, but a smaller training data set also has
its drawbacks—the information contained within the 19 200
deleted examples is lost.
On the other hand, oversampling results in no lost information. Every example in the original training data set also appears
in the resampled training data set. Depending on the oversampling algorithm used, the resampled data set also includes either
duplicate or new minority class examples. While no information is lost during oversampling, it is not without its drawbacks.
When oversampling is performed by duplicating examples,
it can lead to overﬁtting . Whether duplicating examples
or creating new ones, oversampling increases model training
times. In our previous example, the data set contained only
400 minority class examples but 19 600 majority class examples. While undersampling (to achieve a 50 : 50 class ratio) results in a data set with only 800 examples, oversampling would
create a training data set with 39 200 examples (19 600 positive
and 19 600 negative). Training a model on such a large data
set would obviously take much longer than if undersampling
Another technique that can be used to improve classiﬁcation
performance is boosting. While many data sampling techniques
are designed speciﬁcally to address the class imbalance problem, boosting is a technique that can improve the performance
1083-4427/$26.00 © 2009 IEEE
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 40, NO. 1, JANUARY 2010
of any weak classiﬁer (regardless of whether the data are imbalanced). The most common boosting algorithm is AdaBoost
 , which iteratively builds an ensemble of models. During
each iteration, example weights are modiﬁed with the goal of
correctly classifying examples in the next iteration, which were
incorrectly classiﬁed during the current iteration. Upon completion, all constructed models participate in a weighted vote
to classify unlabeled examples. Such a technique is particularly
effective at dealing with class imbalance because the minority
class examples are most likely to be misclassiﬁed and therefore
given higher weights in subsequent iterations. In fact, boosting
has been referred to as a kind of advanced data sampling technique . Boosting can be performed either by “reweighting”
or “resampling” . When boosting by reweighting, the modiﬁed example weights are passed directly to the base learner
during each iteration. However, not all learning algorithms
are designed to incorporate example weights in their decisionmaking processes. Therefore, in this paper, we utilize boosting
by resampling, which resamples the training data according to
the examples’ assigned weights. It is this resampled training
data set that is used to construct the iteration’s model.
In this paper, we present a novel hybrid data sampling/
boosting algorithm called RUSBoost, which is designed to
improve the performance of models trained on skewed data. We
compare the performance of RUSBoost to that of SMOTEBoost
 , which is another algorithm that combines boosting with
data sampling. Both RUSBoost and SMOTEBoost introduce
data sampling into the AdaBoost algorithm. SMOTEBoost does
so using an oversampling technique (synthetic minority oversampling technique (SMOTE) ) which creates new minority
class examples by extrapolating between existing examples.
RUSBoost applies RUS, which is a technique that randomly
removes examples from the majority class. RUS has been
shown to perform very well despite its simplicity . The
motivations for introducing RUS into the boosting process are
simplicity, speed, and performance.
SMOTE is a more complex and time-consuming data sampling technique than RUS. Therefore, SMOTEBoost (which
uses SMOTE) is more complex and time consuming to perform
than RUSBoost. Furthermore, SMOTE is an oversampling technique which carries the drawback of increased model training
time. SMOTEBoost magniﬁes this drawback since boosting
requires the training of an ensemble of models (many models
with increased training times must be constructed). Consider
our previous example data set, where only 400 of the 20 000
examples belong to the minority class. Using SMOTEBoost, n
(where n is the number of boosting iterations to be performed)
models would have to be built using data sets with as many
as 39 988 examples. This number is larger than the previously
mentioned 39 200 examples due to the application of boosting
by resampling . When SMOTEBoost is implemented using
boosting by reweighting, training data sets with 39 200 examples are used.
On the other hand, RUS decreases the time required to
construct a model, which is a key beneﬁt particularly when
creating an ensemble of models, which is the case in boosting.
While both RUSBoost and SMOTEBoost would require that n
models be trained, in our example data set, the n models built
using RUSBoost would be trained using as few as two examples. This unlikely scenario would only occur under extreme
conditions using boosting by resampling. Using boosting by
reweighting, the number of examples used to train each model
in the RUSBoost ensemble would be the previously mentioned
800 (compared to 39 200 examples using SMOTEBoost). RUS-
Boost, therefore, has a clear advantage over SMOTEBoost with
respect to training time. The main drawback of RUS, which is
the loss of information, is greatly overcome by combining it
with boosting. While certain information may be absent during
a given iteration of boosting, it will likely be included when
training models during other iterations. Our results, which are
based on four different performance metrics, demonstrate the
strength of RUSBoost. Even using performance metrics where
RUS performs poorly (due to loss of information), RUSBoost
performs extremely well. Our results show that the simpler and
quicker RUSBoost algorithm performs favorably when compared to SMOTEBoost, often resulting in signiﬁcantly better
classiﬁcation performance.
We present a comprehensive investigation of RUSBoost,
comparing its performance to that of SMOTEBoost, as well as
AdaBoost, RUS, and SMOTE, which are the main components
of RUSBoost and SMOTEBoost. Using four different learners
and four performance metrics, we evaluate the performance of
each of these algorithms using 15 skewed data sets from various
application domains. Our results show that SMOTEBoost and
RUSBoost both result in signiﬁcantly better performance than
their components and that RUSBoost typically performs as well
as or better than SMOTEBoost despite its relative simplicity
and efﬁciency.
The remainder of this paper is organized as follows.
Section II presents related works, and Section III describes
the RUSBoost algorithm. Section IV provides the details of
our experiments, and the experimental results are provided in
Section V. We conclude in Section VI.
II. RELATED WORK
Much research has been performed with respect to the class
imbalance problem. Weiss provides a survey of the class
imbalance problem and techniques for reducing the negative
impact imbalance that has on classiﬁcation performance. The
study identiﬁes many methods for alleviating the problem of
class imbalance, including data sampling and boosting, which
are the two techniques investigated in this paper. Japkowicz 
presents another study addressing the issue of class imbalance,
including an investigation of the types of imbalance that most
negatively impact classiﬁcation performance, and a small case
study comparing several techniques for alleviating the problem.
Data sampling has received much attention in research related to class imbalance. Data sampling attempts to overcome
imbalanced class distributions by adding examples to (oversampling) or removing examples from (undersampling) the data set.
The simplest form of undersampling is RUS. RUS randomly
removes examples from the majority class until a desired class
distribution is found. While there is no universally accepted
optimal class distribution, a balanced (50 : 50) distribution is
often considered to be near optimal . However, when
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
SEIFFERT et al.: RUSBOOST: A HYBRID APPROACH TO ALLEVIATING CLASS IMBALANCE
examples from the minority class are very rare, a ratio closer
to 35 : 65 (minority:majority) may result in better classiﬁcation
performance .
In addition to random data sampling techniques, several
more “intelligent” algorithms for resampling data have been
proposed. Barandela et al. and Han et al. examine
the performance of some of these “intelligent” data sampling
techniques, such as SMOTE, borderline SMOTE, and Wilson’s
editing. Van Hulse et al. examine the performance of
seven different data sampling techniques (both “intelligent” and
random) using a large number of different learning algorithms
and experimental data sets, ﬁnding both RUS and SMOTE to
be very effective data sampling techniques.
Another technique for dealing with class imbalance is boosting. While boosting is not speciﬁcally designed to handle the
class imbalance problem, it has been shown to be very effective
in this regard . The most commonly used boosting algorithm is AdaBoost , which has been shown to improve the
performance of any weak classiﬁer, provided that the classiﬁer
results in better performance than random guessing. Several
variations have been proposed to make AdaBoost cost sensitive
 – or to improve its performance on imbalanced data
 – . One of the most promising of these techniques
is SMOTEBoost . SMOTEBoost combines an intelligent
oversampling technique (SMOTE) with AdaBoost, resulting in
a highly effective hybrid approach to learning from imbalanced
data. Our proposed technique, which is RUSBoost, is based on
the SMOTEBoost algorithm but provides a faster and simpler
alternative for learning from imbalanced data with performance that is usually as good (and often better) than that of
SMOTEBoost.
Closely related to the issue of class imbalance is costsensitive learning. Weiss et al. compare the performances
of oversampling, undersampling, and cost-sensitive learning
when dealing with data that have both an imbalanced class
distribution and unequal error costs. Sun et al. present an
in-depth examination of cost-sensitive boosting. Chawla et al.
 perform a detailed evaluation of a wrapper-based sampling approach to minimize misclassiﬁcation cost. A detailed
evaluation of RUSBoost as a cost-sensitive learning technique,
including a comparison to existing methodologies, is left as a
future work.
A. Background
This paper compares the performance of RUSBoost to that of
its components (RUS and AdaBoost), showing that RUSBoost
results in better classiﬁcation performance than either of its
components alone. In addition, we compare the performance of
RUSBoost to that of its predecessor, which is SMOTEBoost, as
well as SMOTE. The following sections brieﬂy describe these
techniques. In addition, we present the results for each baseline
learner without using either sampling or boosting and label the
results as “None.” For simplicity, throughout this paper, we
refer to all of these techniques (and RUSBoost) as “sampling
techniques.”
1) AdaBoost: Boosting is a metalearning technique designed to improve the classiﬁcation performance of weak learners by iteratively creating an ensemble of weak hypotheses
which are combined to predict the class of unlabeled examples.
This paper uses AdaBoost, which is a well-known boosting
algorithm shown to improve the classiﬁcation performance of
weak classiﬁers. We present a brief synopsis of AdaBoost. For
the complete details of the AdaBoost algorithm, please refer to
Freund and Schapire’s work .
Initially, all examples in the training data set are assigned
with equal weights. During each iteration of AdaBoost, a
weak hypothesis is formed by the base learner. The error
associated with the hypothesis is calculated, and the weight
of each example is adjusted such that misclassiﬁed examples
have their weights increased while correctly classiﬁed examples
have their weights decreased. Therefore, subsequent iterations
of boosting will generate hypotheses that are more likely to
correctly classify the previously mislabeled examples. After
all iterations are completed, a weighted vote of all hypotheses
is used to assign a class to the unlabeled examples. In this
paper, all boosting algorithms (AdaBoost, SMOTEBoost, and
RUSBoost) are performed using ten iterations. Preliminary
experiments with AdaBoost using the same data sets as in
this paper showed no signiﬁcant improvement between 10 and
50 iterations.
Since boosting assigns higher weights to misclassiﬁed examples and minority class examples are those most likely to be
misclassiﬁed, it stands to reason that minority class examples
will receive higher weights during the boosting process, making
it similar in many ways to cost-sensitive classiﬁcation 
(a technique for alleviating the class imbalance problem that
is not discussed in this paper). In effect, boosting alters the
distribution of the training data (through weighting, not altering
the number of examples); thus, it can also be thought of as
an advanced data sampling technique . For simplicity, we
refer to boosting as one of the ﬁve “sampling techniques”
investigated in this paper.
2) RUS: Data sampling techniques attempt to alleviate the
problem of class imbalance by adjusting the class distribution
of the training data set. This can be accomplished by either
removing examples from the majority class (undersampling)
or adding examples to the minority class (oversampling). One
of the most common data sampling techniques (largely due to
its simplicity) is RUS. Unlike more complex data sampling
algorithms, RUS makes no attempt to “intelligently” remove
examples from the training data. Instead, RUS simply removes
examples from the majority class at random until a desired class
distribution is achieved. In this paper, we use three postsampling class distributions: 35, 50, and 65. These numbers represent the percentage of examples in the postsampling data set,
which belong to the minority class. The same three values are
used when performing SMOTE, SMOTEBoost, and RUSBoost.
3) SMOTE: Chawla et al. proposed an intelligent oversampling method called SMOTE. SMOTE adds new artiﬁcial
minority examples by extrapolating between preexisting minority instances rather than simply duplicating original examples.
The newly created instances cause the minority regions of the
feature space to be fuller and more general. The technique
ﬁrst ﬁnds the k nearest neighbors of each minority example
(this paper recommends k = 5). The artiﬁcial examples are
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 40, NO. 1, JANUARY 2010
RUSBoost algorithm.
then generated in the direction of some or all of the nearest
neighbors, depending on the amount of oversampling desired.
If 200% oversampling is speciﬁed, then synthetic examples
are generated randomly along the line segments connecting
each minority example to two of its ﬁve nearest neighbors. If
xi is the minority example being examined, xj is one of the
selected nearest neighbors of xi, and xn is the new example
being added to the data set, then xn = u(xj −xi) + xi, where
u is a uniform random number between zero and one. This
sampling process causes a classiﬁer to learn a larger and more
general decision region in the feature space, ideally alleviating
the problem caused by class imbalance.
4) SMOTEBoost: SMOTEBoost, which was proposed by
Chawla et al. , combines the SMOTE algorithm with Ada-
Boost, resulting in a hybrid sampling/boosting algorithm that
outperforms both SMOTE and AdaBoost. As is the case with
RUSBoost, SMOTEBoost is based on the AdaBoost.M2 algorithm. Prior to constructing the weak hypothesis during each
round of boosting, SMOTE is applied to the training data to
achieve a more balanced training data set. Therefore, the algorithm for SMOTEBoost is very similar to that of RUSBoost,
which is presented in Section III. Fig. 1 can be modiﬁed to
represent the SMOTEBoost algorithm by changing step 2a to
Create temporary training data set S′
with distribution D′
t using SMOTE
The application of SMOTE at this point has two drawbacks
that RUSBoost is designed to overcome. First, it increases the
complexity of the algorithm. SMOTE must ﬁnd the k nearest neighbors of the minority class examples and extrapolate
between them to make new examples. RUS, on the other
hand, simply deletes the majority class examples at random.
Second, since SMOTE is an oversampling technique, it results
in longer model training times. This effect is compounded by
SMOTEBoost’s use of boosting, since many models must be
built using larger training data sets. On the other hand, RUS
results in smaller training data sets and, therefore, shorter model
training times.
III. RUSBOOST
Fig. 1 shows the RUSBoost algorithm. RUSBoost is based
on the SMOTEBoost algorithm , which is, in turn, based on
the AdaBoost.M2 algorithm . SMOTEBoost improves upon
AdaBoost by introducing an intelligent oversampling technique
(SMOTE ), which helps to balance the class distribution,
while AdaBoost improves the classiﬁer performance using
these balanced data. RUSBoost achieves the same goal but uses
RUS rather than SMOTE. The result is a simpler algorithm with
faster model training times and favorable performance.
Let xi be a point in the feature space X and yi be a class
label in a set of class labels Y . Each of the m examples in the
data set (S) can be represented by the tuple (xi, yi). Let t be an
iteration between one and the maximum number of iterations T
(number of classiﬁers in the ensemble), ht be the weak hypothesis (trained using some classiﬁcation algorithm, WeakLearn)
trained on iteration t, and ht(xi) be the output of hypothesis ht,
for instance, xi (which may be a numeric conﬁdence rating).
Let Dt(i) be the weight of the ith example on iteration t
(the weights are typically normalized to sum to one).
In step 1, the weights of each example are initialized to 1/m,
where m is the number of examples in the training data set. In
step 2, T weak hypotheses are iteratively trained, as shown in
steps 2a–2g. In step 2a, RUS is applied to remove the majority
class examples until N% of the new (temporary) training data
t belongs to the minority class. For example, if the desired
class ratio is 50 : 50, then the majority class examples are
randomly removed until the numbers of majority and minority
class examples are equal. As a result, S′
t will have a new
weight distribution D′
t. In step 2b, S′
t are passed to the
base learner, WeakLearn, which creates the weak hypothesis ht
(step 2c). The pseudoloss ϵt (based on the original training data set S and weight distribution Dt) is calculated
in step 2d. In step 2e, the weight update parameter α is calculated as ϵt/(1 −ϵt). Next, the weight distribution for the next
iteration Dt+1 is updated (step 2f) and normalized (step 2g).
After T iterations of step 2, the ﬁnal hypothesis H(x) is returned as a weighted vote of the T weak hypotheses (step 3).
IV. EXPERIMENTS
A. Data Sets
The performance of various imbalance-handling techniques
(including our new algorithm, RUSBoost) is evaluated using
15 data sets from various application domains. Table I provides
the characteristics of these data sets, including the number of
examples (Size) in each data set, the number of minority class
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
SEIFFERT et al.: RUSBOOST: A HYBRID APPROACH TO ALLEVIATING CLASS IMBALANCE
DATA SET CHARACTERISTICS
examples (#min), the percentage of all examples belonging
to the minority class (%min; commonly referred to as the
level of imbalance), and the number of attributes in the data
sets, including the dependent attribute (#attr). These data sets
represent a wide variety of data set sizes, imbalance levels, and
application domains. The size of the data sets ranges from 214
examples (Glass3) to 11 183 examples (Mammography). The
data sets have anywhere from 7 to 43 attributes. In Table I, the
data sets are sorted from the most imbalanced (SP3 with only
1.33% examples belonging to the minority class) to the least
imbalanced (Vehicle1 with 25.06% examples from the minority
Three of the data sets (SP1, SP3, and CCCS12) are proprietary software project data sets obtained from Nortel Networks
and the Department of Defense. The remaining 12 data sets are
publicly available. PC1 and CM1 can be downloaded from the
National Aeronautics and Space Administration Metric Data
Program website . The Mammography data set is from
the medical diagnosis domain and was generously provided
by Dr. N. Chawla . The remaining data sets were obtained
from the popular University of California–Irvine repository
 , and they represent various application domains. Since this
paper considers only the binary classiﬁcation problem, it was
necessary to transform some of the data sets to have a binary
class. In the case of multiclass data sets, a single class was
selected to be the positive class, while the remaining classes
were combined to make up the negative class. In the case of data
sets with a continuous dependent variable, a domain-speciﬁc
threshold was applied to divide the examples into two classes.
Each of the techniques evaluated in this paper, however, can be
applied in a multiclass environment. AdaBoost can be applied
to multiclass data, and data sampling (RUS or SMOTE) can be
performed to achieve any class distribution, regardless of the
number of classes.
B. Learners
This paper uses four learners, all of which are implemented
in Weka , which is an open-source data mining suite.
C4.5 is a decision tree learner that uses an entropy-based
splitting criterion stemming from information theory . Two
versions of C4.5 are used in our experiments. C4.5D uses the
default Weka parameters, while C4.5N disables pruning and
uses Laplace smoothing . The Weka implementation of
C4.5 is J48. Repeated Incremental Pruning to Produce Error
Reduction (RIPPER) is a rule-based learner that modi-
ﬁes the incremental reduced error pruning algorithm to
improve accuracy without sacriﬁcing efﬁciency. Naive Bayes
(NB) utilizes Bayes’s rule of conditional probability to
classify examples. It is “naive” in that it assumes that all
predictor variables are conditionally independent. The default
Weka parameters were used for both RIPPER and NB.
C. Performance Metrics
This paper uses four different performance metrics to evaluate the models constructed for our experiments, all of which are
more suitable than the overall accuracy when dealing with class
imbalance. While many studies select just one metric, we recognize that there is no universally accepted metric for evaluating
learner performance when data are imbalanced. Depending on
the selection of performance metric, very different results can
be observed. Although an evaluation of different performance
metrics is beyond the scope of this paper, we include these four
metrics to illustrate the power of RUSBoost, which performs
well even when its components (RUS and AdaBoost) do not.
In binary classiﬁcation problems, classiﬁcations can be
grouped into one of four categories. These four categories are
used to derive the four performance metrics used in this paper. If
xi is an example from a data set D with class cj, j = 0 or 1, and
if c(xi) and ˆc(xi) are the actual and predicted classes of xi, then
xi is a true positive (tpos) if c(xi) = c1 = ˆc(xi)
xi is a true negative (tneg) if c(xi) = c0 = ˆc(xi)
xi is a false positive (fpos) if c(xi) = c0 ̸= ˆc(xi)
xi is a false negative (fneg) if c(xi) = c1 ̸= ˆc(xi).
Based on the aforementioned deﬁnitions, several basic performance metrics can be deﬁned, which are the basis for the
four metrics that are used in this paper. The total number of
instances in a data set that are false negatives is denoted by
#fneg and, similarly for #fpos, #tpos and #tneg. If Ncj
denotes the number of instances in class cj, then let N =
Nc0 + Nc1. Three basic measures of classiﬁer performance can
be deﬁned as
true positive rate (or recall) = #tpos
false positive rate = #fpos
precision =
#tpos + #fpos.
1) ROC Curves: One of the most popular methods for evaluating the performance of learners built using imbalanced data
is receiver operating characteristic , or ROC, curves. ROC
curves graph the true positive rate on the y-axis versus the
false positive rate on the x-axis. The resulting curve illustrates
the tradeoff between detection and false alarm rates. The ROC
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 40, NO. 1, JANUARY 2010
curve illustrates the performance of a classiﬁer across the
complete range of possible decision thresholds and accordingly,
does not assume any particular misclassiﬁcation costs or class
prior probabilities. The area under the ROC curve (A-ROC) is
used to provide a single numerical metric for comparing model
performances.
2) PRC Curves: The precision–recall characteristic (PRC)
 curve has also been used to evaluate the performance of
models built using skewed data. Like ROC curves, the PRC
curve plots the recall, or true positive rate, on the y-axis.
However, the difference between the ROC and PRC curves is
that the PRC curve plots the precision on the x-axis. Precision
is the proportion of examples predicted as belonging to the
positive class to examples that actually belong to the positive
class. The implications of this difference have been investigated
by Davis and Goadrich . As with the ROC curves, the area
under the PRC curve (A-PRC) is used as a single metric for
comparing the performances of different models.
3) K–S Statistic: The Kolmogorov–Smirnov (K–S) statistic
measures the maximum difference between the empirical distribution functions of the posterior probabilities of instances in
each class. In other words, let Fci(t) = P(p(x) ≤t | ci), where
0 ≤t ≤1. Fci(t) is estimated by the proportion of class ci
instances ≤t
Fci(t)= #class ci instances with posterior probability≤t
#class ci instances
Then, the K–S statistic is deﬁned as
t∈ |Fc1(t) −Fc2(t)| .
The larger the distance between the two distribution functions,
the better the learner is able to separate the two classes. The
maximum possible value for K–S is one (perfect separation),
with a minimum of zero (no separation). The K–S statistic is
a commonly used metric of classiﬁer performance in the credit
scoring application domain .
4) F−Measure: The ﬁnal performance metric used in this
paper is the F−measure. Like the PRC curve, this metric is
derived from recall and precision. It is the only metric used
in this paper that is decision threshold dependent. A default
decision threshold of 0.5 is used. The formula for F−measure,
which is given in (4), uses a tunable parameter β to indicate
the relative importance of recall and precision. That is, one can
modify β to place more emphasis on either recall or precision.
Typically, β = 1 is used (as is the case in our study)
F−measure = (1 + β2) × recall × precision
β2 × recall + precision
D. ANOVA Analysis
The results presented in this paper are tested for statistical
signiﬁcance at the α = 5% level using a one-factor analysis
of variance (ANOVA) . An ANOVA model can be used
to test the hypothesis that the classiﬁcation performances for
each level of the main factor(s) are equal against the alternative
hypothesis that at least one is different. In this paper, we use a
one-factor ANOVA model, which can be represented as
ψjn = μ + θj + ϵjn
response (i.e., A-ROC, K–S, A-PRC, or F−measure)
for the nth observation of the jth level of experimental
overall mean performance;
mean performance of level j for factor θ;
random error.
The main factor θ is the sampling technique. That is, we
are testing to see if the average performances of the six levels (RUSBoost, SMOTEBoost, RUS, SMOTE, AdaBoost, and
None) of θ are equal. If the alternative hypothesis (that at least
one level of θ is different) is accepted, numerous procedures
can be used to determine which are different. This involves
a pairwise comparison of each sampling technique’s mean
performance, with the null hypothesis that they are equal. In this
paper, we use Tukey’s honestly signiﬁcant difference (HSD)
test to identify which levels of θ are signiﬁcantly different.
E. Experimental Design Summary
Each of the four learners, as well as AdaBoost, is implemented within the Weka data mining suite . Weka was extended by our research group to include the four data sampling
techniques (SMOTE, SMOTEBoost, RUS, and RUSBoost) and
to provide the four performance metrics used to evaluate the
classiﬁcation performance. A statistical analysis, via ANOVA
modeling, was performed using SAS .
experiments
crossvalidation. Each data set is split into ten partitions, nine of
which are used to train the model, while the remaining (holdout) partition is used to test the model. This process is repeated
ten times so that each partition acts as test data once. In
addition, ten independent runs of this procedure are performed
to eliminate any biasing that may occur during the random partitioning process. Therefore, each data set results in 10 × 10 =
100 experimental data sets. We use 15 data sets from various
application domains with various levels of imbalance and size,
resulting in a total of 15 × 100 = 1500 derived experimental
data sets.
Using each of these experimental data sets, four base learners
are used to train the models in conjunction with each of the
six techniques: RUSBoost, SMOTEBoost, RUS, SMOTE, AdaBoost, and None. Four of the techniques (RUSBoost, SMOTE-
Boost, RUS, and SMOTE) are each performed three times,
each with a different “target class distribution” (35%, 50%, or
65%). AdaBoost and None (no sampling) are each performed
once. Therefore, 4 × 3 + 2 = 14 sampling technique/parameter
combinations are used. Using four base learners, there are
4 × 14 = 56 combinations of sampling technique and learner
evaluated. In total, 1500 data sets × 56 combinations = 84 000
models are evaluated to formulate the results presented in
this paper. All boosting algorithms were performed using ten
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
SEIFFERT et al.: RUSBOOST: A HYBRID APPROACH TO ALLEVIATING CLASS IMBALANCE
AVERAGE PERFORMANCES OF THE SAMPLING TECHNIQUES ACROSS ALL LEARNERS AND DATA SETS
iterations. Experiments performed using more iterations did not
result in signiﬁcantly different results.
V. EMPIRICAL RESULTS
This section presents the results of our experiments
with RUSBoost, SMOTEBoost, and their components (RUS,
SMOTE, and AdaBoost). For comparison purposes, the performance of the baseline learner without either sampling or
boosting is included and denoted by “None.” The results are
presented in a top–down fashion. That is, we begin by comparing the average performances of these sampling techniques
across all 15 data sets and all four learners in Section V-A.
We then narrow our view, examining the performance of the
techniques on a per-learner basis in Section V-B. Section V-C
investigates the performance of each technique for each combination of data set and learner. Finally, Section V-D directly
compares RUSBoost and SMOTEBoost.
A. Overall Results
Table II presents the performance of each sampling technique
averaged across all learners and data sets. This table gives
a general view of the performance of each technique using
each of the four performance metrics. Table II provides both
the numerical average performance (Mean) and the results of
Tukey’s HSD test. If two techniques have the same letter in
the column labeled “HSD,” then their mean performances were
not signiﬁcantly different according to the HSD test at the 95%
conﬁdence level. For example, using A-ROC to measure the
performance of the learners, the performances of RUSBoost
and SMOTEBoost are not signiﬁcantly different (they are both
in group A), but both RUSBoost and SMOTEBoost perform
signiﬁcantly better than AdaBoost (which belongs to group B).
Although it does not occur in this table, it is possible for a
technique to belong to more than one group.
For a more detailed description of the entire distribution of
classiﬁcation performance across all data sets and learners, two
boxplot comparisons of the six methodologies for handling
imbalance are shown in Figs. 2 and 3 (we exclude the boxplots
for A-ROC and F−measure since the boxplot of A-ROC is
similar to that of K–S and the boxplot of F−measure is similar
to that of A-PRC). Boxplots allow for an easy comparison of
the techniques at various points in the distribution, not just the
mean as in Table II. For example, it is clear that the distribution
of the A-PRC values for RUSBoost is much more favorable
than that of RUS—the 75th, median, and 25th percentiles of
the distribution of the A-PRC values for RUSBoost are much
larger than that of RUS.
K–S boxplot.
A-PRC boxplot.
The following observations are derived from the data in
Table II and Figs. 2 and 3.
1) According to the HSD test, there is no signiﬁcant difference between the performances of RUSBoost and
SMOTEBoost, regardless of the performance metric,
when averaging across all data sets and learners. In all
cases, both SMOTEBoost and RUSBoost belong to group
A. The boxplots of RUSBoost and SMOTEBoost are also
very similar.
2) Using A-ROC and K–S to measure performance, RUS-
Boost has a slightly better performance (presented in the
Mean column), but the difference is not signiﬁcant. A
similar conclusion holds for SMOTEBoost using A-PRC
and F−measure.
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 40, NO. 1, JANUARY 2010
PERFORMANCES OF THE SAMPLING TECHNIQUES AVERAGED ACROSS ALL DATA SETS
3) Both RUSBoost and SMOTEBoost perform signiﬁcantly
better than AdaBoost, RUS, and SMOTE. In other words,
the application of hybrid sampling/boosting is better than
boosting or sampling alone.
4) AdaBoost
signiﬁcantly
SMOTE, and None for three of the four performance
metrics, conﬁrming the results in a previous work ,
which claimed that boosting frequently outperforms data
sampling when the training data are skewed.
5) While there is no signiﬁcant difference between the performances of SMOTE and RUS using A-ROC to measure
performance, SMOTE signiﬁcantly outperforms RUS for
the other three metrics. In fact, when using A-PRC
to measure performance, RUS results in a signiﬁcantly
worse performance than if no sampling technique was
applied at all (None).
6) While RUS is usually outperformed by SMOTE, it is
not the case that RUSBoost is outperformed by SMOTE-
Boost. Instead, the performances of SMOTEBoost and
RUSBoost are very similar. This indicates that the combination of RUS with boosting indeed helps overcome
the main drawback of RUS (the loss of information),
which is evident when measuring performance with
A-PRC and F−measure. However, SMOTE’s main
drawback (increased model training time due to larger
training data sets) is ampliﬁed by its combination
with boosting. Given the similar performance between
SMOTEBoost and RUSBoost, one would prefer the simpler and faster technique: RUSBoost.
B. Results by Learner
This section investigates the performance of the ﬁve sampling techniques by learner when classiﬁcation models are
trained using C4.5D, C4.5N, NB, and RIPPER. Table III shows
the performance of each learner/sampling technique pair for
each of the four performance measures. As in Table II, this
table presents the average performance (across all 15 data sets)
and the results of HSD testing. When two sampling techniques
have the same letter in the HSD column, there is no statistically
signiﬁcant difference in their performances.
Many of the trends are similar to what was previously
observed in Section V-A; however, we summarize the following
new ﬁndings here.
1) With RIPPER as the base learner and using the
F−measure and A-PRC to measure performance,
SMOTEBoost is preferred over RUSBoost. In all other
situations, RUSBoost and SMOTEBoost are not signiﬁcantly different from one another, as determined by the
2) In almost all situations, RUSBoost and SMOTEBoost are
signiﬁcantly better than RUS, SMOTE, and AdaBoost
exceptions
F−measure, and RIPPER with F−measure). That is,
the improvements obtained by hybrid sampling/boosting
are not speciﬁc to any single learner or performance
3) AdaBoost does not perform well with the NB classiﬁer.
Relative to the other three learners, a comparison between
AdaBoost and sampling (either RUS or SMOTE) shows
that RUS or SMOTE is signiﬁcantly better than AdaBoost
in only three of the 12 scenarios. In other words, with the
exception of NB, AdaBoost generally performs similar to,
or better than, sampling.
4) For the NB classiﬁer, SMOTE, RUS, and AdaBoost do
not signiﬁcantly improve the performance of the baseline
learner and, in some cases, can hurt performance. Despite
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
SEIFFERT et al.: RUSBOOST: A HYBRID APPROACH TO ALLEVIATING CLASS IMBALANCE
RESULTS FOR ALL DATA SETS USING C4.5D (A-ROC)
this, both RUSBoost and SMOTEBoost do signiﬁcantly
improve the performance of this learner.
5) The results using C4.5D and RIPPER with the A-ROC
performance measure agree with the results from a previous study , where AdaBoost was shown to signiﬁcantly outperform RUS, which, in turn, signiﬁcantly
outperformed SMOTE.
6) The performance using A-PRC and F−measure is considerably different compared to that of A-ROC or K–S,
particularly where RUS is concerned. In general, RUS
(which is typically one of the best sampling techniques
when evaluated with A-ROC or K–S) performs very
poorly using A-PRC and F−measure. In most cases,
RUS does not improve the baseline learner and often results in signiﬁcantly worse performance. Although RUS
is shown to be a poorly performing sampling technique
using these two measures, RUSBoost performs quite
well, once again demonstrating how the combination of
boosting and RUS can help alleviate the main drawback
(information loss) obtained using RUS alone.
C. Results by Data Set and Learner
In this section, we expand upon the results presented in
Section V-B by data set. That is, we no longer consider the
average performance across all data sets but instead investigate
the performance of the various sampling techniques on a perdata-set basis. Although the results for all learners and all
performance metrics cannot be included, we include details for
a few of the learner/metric combinations and summarize the
results for the remaining combinations.
Table IV shows the results of evaluating C4.5D models using
A-ROC. This table presents the average of the ten runs of
tenfold cross-validation for each of the 15 data sets using each
sampling technique. The HSD values in this table are meant to
be read horizontally, rather than vertically, as was the case in
the previous tables. That is, if two techniques have the same
letter in a given row, then the performance of those techniques
was not statistically different according to the HSD test with
95% conﬁdence for the given data set (each row represents
a data set). The ﬁnal row of this table presents the average
performance across all 15 data sets (as in Table III) and the
number of data sets where the given technique was in group A
(#A’s), which is the best performing group.
RUSBoost is in group A for all 15 of the data sets in our
experiments, which means that it did not perform signiﬁcantly
worse than any other technique in any of the 15 data sets (using
C4.5D and A-ROC). SMOTEBoost was in group A for 11 of the
15 data sets. There were four data sets where RUSBoost signiﬁcantly outperformed SMOTEBoost (Contraceptive, Mammography, SolarFlare, and SP3), and conversely, there were
no data sets where SMOTEBoost (or any other sampling
technique) signiﬁcantly outperformed RUSBoost. AdaBoost
performed nearly as well as SMOTEBoost, being in group A
for ten of the 11 data sets. RUS was in group A for two data
sets (Contraceptive and SolarFlare), while SMOTE was not in
group A for any data set. None was in group A for the data
set Contraceptive, indicating that no technique signiﬁcantly
improved the A-ROC of C4.5D for this data set. This moderately sized (1473 examples) data set has only slight imbalance
(22.61% of the samples belong to the minority class).
Note that the number of A’s only indicates the number of data
sets where the given technique was in group A. It does not accurately reﬂect the relative performance of the techniques where
neither technique was in group A. For example, SMOTEBoost
is in group A 11 times, while AdaBoost is in group A ten times.
That does not mean that SMOTEBoost only outperformed
AdaBoost on one data set. To compare the performances of
these techniques, one must look at the other letter assignments
as well. In the case of SMOTEBoost and AdaBoost, SMOTE-
Boost signiﬁcantly outperformed AdaBoost on two data sets
(Mammography and SP3) even though SMOTEBoost was not
in group A for either of these data sets. For the data set SP1,
SMOTEBoost was in both groups A and B, while AdaBoost
was only in group B. Even though SMOTEBoost was in group
A for this data set, it did not signiﬁcantly outperform AdaBoost,
which was not in group A (they are both in group B). In other
words, the “number of A’s” only indicates how many times
the given technique was not signiﬁcantly outperformed by any
other technique.
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 40, NO. 1, JANUARY 2010
RESULTS FOR ALL DATA SETS USING RIPPER (A-PRC)
NUMBER OF DATA SETS WHERE EACH TECHNIQUE WAS RANKED IN GROUP A
Table V shows similar details as Table IV using RIPPER
to build models and A-PRC to evaluate them. This is the
learner/metric combination where RUSBoost resulted in its
worst performance. Its worst performance, however, is still
very good. Table V shows that RUSBoost is in group A for
12 of the 15 data sets used in our study. There are only three
data sets (Car, Glass, and SatImg) where RUSBoost is not in
group A, which means that, for these three data sets, some
other procedure (in these cases, SMOTEBoost) signiﬁcantly
outperformed RUSBoost. Instead, for each of these data sets,
RUSBoost is in group B, which is the second best group.
SMOTEBoost, on the other hand, is in group A for all but
one of the experimental data sets. Using A-PRC to measure
the performance of RIPPER models, SMOTEBoost has a slight
edge over RUSBoost. AdaBoost is in group A for seven data
sets, while SMOTE, RUS, and None are always outperformed
by at least one of the other techniques (they are never in
Note that, in most cases, RUS is the worst of the ﬁve
techniques for handling class imbalance when A-PRC is used
to measure the performance of RIPPER models. There are only
four data sets where RUS performs signiﬁcantly better than the
baseline learner RIPPER, yet RUSBoost signiﬁcantly outperforms None on all data sets, further demonstrating RUSBoost’s
ability to negate the drawbacks of RUS.
A detailed view of the results, as presented in Tables IV
and V, cannot be included for all 16 combinations of learner
and performance metric. However, a summary of these 16
combinations can be found in Table VI. This table indicates
the number of data sets (out of 15) that the given sampling
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
SEIFFERT et al.: RUSBOOST: A HYBRID APPROACH TO ALLEVIATING CLASS IMBALANCE
technique was assigned group A for the given learner and
performance metric. For example, using A-ROC to evaluate
C4.5N models, RUSBoost was in group A for all 15 data sets,
while using K–S to measure the performance of C4.5N models,
RUSBoost was in group A for 14 of the 15 data sets. The
rows labeled “Total” present the sums of these values for each
performance metric (up to a maximum of 60). For example,
using A-ROC to measure performance, RUSBoost was in group
A for 58 of the 60 learner/data set combinations. At the bottom
of Table VI (the section labeled “All”), the results for each
learner/sampling technique combination (summed across the
four performance measures) are presented. For example, using
the C4.5D learner, RUSBoost is in group A for 59 of the 60 data
set/metric combinations (15 using A-ROC + 15 using K–S +
15 using F−measure + 14 using A-PRC).
The results show that the performance of RUSBoost is very
favorable when compared to the other techniques, particularly
using A-ROC or K–S to measure performance. Even using
A-PRC and F−measure, its performance is very competitive,
usually performing at least as well as SMOTEBoost. SMOTE-
Boost performs slightly better than RUSBoost using NB, but
for the other three learners, RUSBoost is in group A more frequently than SMOTEBoost. Summing the number of times each
sampling technique is assigned group A for all four learners
using the A-ROC metric, RUSBoost is in group A for 58 of the
60 possible data set/learner combinations, while SMOTEBoost
is in group A for 51 of these combinations. In other words,
RUSBoost was signiﬁcantly outperformed by at least one other
technique, as measured by A-ROC, for two of the 60 scenarios.
On the other hand, SMOTEBoost was outperformed by at least
one other technique in nine of the 60 scenarios. A-PRC is
the only performance metric where the per-data-set analysis
favors SMOTEBoost. Of the 60 data set/learner combinations,
SMOTEBoost beats RUSBoost by one data set (56 data sets to
55 data sets).
Overall, of the 240 combinations of learner, metric, and data
set used in this study, RUSBoost is in group A for 224 (93.33%)
of the combinations. SMOTEBoost, which is the next best
technique, is in group A for 207 (86.25%) of these 240 combinations. Even using RIPPER, where RUSBoost performed
worst (when evaluated using A-PRC), RUSBoost is in group A
for more (51) experiments than SMOTEBoost (49). NB is the
only learner where SMOTEBoost is in group A more frequently
than RUSBoost, but examining the data in Tables IV–VI, in
a relative sense, there is not as much of a difference in the
performances between all ﬁve techniques using NB.
D. Comparing RUSBoost and SMOTEBoost
Finally, we directly compare RUSBoost and SMOTEBoost in
Table VII. The previous sections have used the HSD statistical
test to perform multiple pairwise comparisons between the
mean performances of the different techniques. The HSD test
has the advantage that adjustments are made to reduce the
likelihood of obtaining a type I error (i.e., incorrectly rejecting
the null hypothesis of equality between two means) of the entire
experiment. An alternative is to perform multiple pairwise
t-tests; however, as the number of comparisons increases, the
t-TEST COMPARISON OF RUSBOOST AND SMOTEBOOST
likelihood of obtaining a type I error of the entire experiment
increases. Since it is more conservative, HSD is often preferred
for multiple mean comparisons; however, it has the drawback
that it has a higher likelihood to make a type II error, i.e., failing
to correctly reject the null hypothesis of equality between
two means. In this section, we analyze only RUSBoost and
SMOTEBoost—therefore, we revert to computing a standard
t-statistic to compare the means of these two techniques to
obtain a more precise comparison.
Table VII compares only RUSBoost and SMOTEBoost, with
a two-sample t-statistic calculated for each learner and data set,
presented by a performance metric. Therefore, each column
totals to 60, and in total, 240 pairwise comparisons between
RUSBoost and SMOTEBoost were performed, each with a 95%
conﬁdence level. The ﬁrst row represents the number of times
that RUSBoost signiﬁcantly outperformed SMOTEBoost, the
second row is the number of times that SMOTEBoost signiﬁcantly outperformed RUSBoost, and the ﬁnal row represents the
cases with no signiﬁcant difference between SMOTEBoost and
RUSBoost. Overall, RUSBoost is preferred over SMOTEBoost,
particularly relative to the A-ROC, K–S, and F−measure
performance metrics.
VI. CONCLUSION
Many domains are affected by the problem of class imbalance. That is, when examples of one class greatly outnumber
examples of the other class(es), it can be challenging to construct a classiﬁer that effectively identiﬁes the examples of the
underrepresented class. Several techniques have been proposed
for dealing with the problem of class imbalance. In this paper,
we have proposed a new algorithm, called RUSBoost, for
alleviating the problem of class imbalance. We compare the performance of RUSBoost with that of several of the most popular
and most effective techniques for learning from skewed data,
ﬁnding that RUSBoost performs favorably when compared to
these other techniques.
RUSBoost presents a simpler, faster, and less complex alternative to SMOTEBoost for learning from imbalanced data.
SMOTEBoost combines a popular oversampling technique
(SMOTE) with AdaBoost, resulting in a hybrid technique that
surpasses the performance of its components. SMOTEBoost,
however, has two main drawbacks: complexity and increased
model training time. By using RUS instead of SMOTE, RUS-
Boost achieves results that are comparable to (and often better
than) SMOTEBoost using an algorithm that is simpler to implement and that results in faster model training times.
This paper compares the performances of RUSBoost and
SMOTEBoost, as well as their individual components: RUS,
SMOTE, and AdaBoost. Models were constructed using four
different base learners, including two versions of C4.5 decision
Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on April 09,2010 at 17:31:54 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 40, NO. 1, JANUARY 2010
trees, RIPPER and NB. Experiments were conducted using
15 data sets of various sizes with various levels of imbalance
from many different application domains. In addition, as there
is no universally agreed upon performance metric for measuring classiﬁcation performance when data are imbalanced, we
evaluate all models using four different performance metrics:
ROC curves, PRC curves, K–S statistic, and F−measure.
An interesting result presented in this paper relates to the
use of different performance measures and the performances
of RUSBoost, SMOTEBoost, SMOTE, and RUS using these
different metrics. Using a performance metric such as A-ROC,
RUS signiﬁcantly improved the classiﬁcation performance, despite its relative simplicity and its inherent drawback (loss of
information). Conversely, applying RUS can result in worse
classiﬁcation performance using A-PRC. If the objective of
the model construction is to maximize A-PRC, it is often
signiﬁcantly better to build models without using sampling
than to apply RUS. On the contrary, SMOTE results in an
improved performance regardless of performance metric selection. Therefore, one might expect SMOTEBoost (which
uses SMOTE) to result in better performance than RUSBoost
(which uses RUS) when evaluating models using A-PRC. However, we ﬁnd that there is no signiﬁcant difference between
the performances of RUSBoost and SMOTEBoost using this
performance measure (as shown in Table III). SMOTEBoost
does outperform RUSBoost using one learner (RIPPER) when
A-PRC is used to measure performance; however, as shown in
Table VI, the performances of RUSBoost and SMOTEBoost are
very similar, even when using this metric. In general, as shown
in Table VI, RUSBoost obtained the best performance (of the
ﬁve techniques used in our study) more often than any of the
other techniques investigated in this paper.
This paper has presented a comprehensive empirical investigation comparing the performances of several techniques
for improving classiﬁcation performance when data are imbalanced, including our proposed RUSBoost algorithm. Considering 240 different combinations of learner, performance
metric, and data set, we ﬁnd that our proposed RUSBoost
algorithm most frequently achieves the best performance. Although the overall performances of RUSBoost and SMOTE-
Boost are not signiﬁcantly different (when averaged across all
four learners and 15 data sets), RUSBoost signiﬁcantly outperforms SMOTEBoost about twice as often as SMOTEBoost
outperforms RUSBoost. This superior performance is achieved
while reducing the model training times. It is this combination
of simplicity, speed, and excellent performance that makes
RUSBoost an attractive algorithm for learning from skewed
training data.
Future work will continue to investigate the performance
of RUSBoost. RUSBoost will be evaluated using additional
learners, and its effectiveness in application domains such as
bioinformatics and life sciences will be assessed. In addition,
the effects of parameter selection on RUSBoost (the number of
boosting iterations and the level of sampling) will be evaluated
in an attempt to identify potentially favorable parameter values.
Finally, the performance of RUSBoost will be compared to
that of additional techniques for dealing with imbalanced data,
including, but not limited to, additional boosting algorithms.
ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
and the Associate Editor for the constructive evaluation of this
paper and also the various members of the Data Mining and
Machine Learning Laboratory, Florida Atlantic University, for
the assistance with the reviews.