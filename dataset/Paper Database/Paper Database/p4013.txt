Attention to Scale: Scale-aware Semantic Image Segmentation
Liang-Chieh Chen∗
Yi Yang, Jiang Wang, Wei Xu
Alan L. Yuille
 
{yangyi05, wangjiang03, wei.xu}@baidu.com
 
 
Incorporating multi-scale features in fully convolutional
neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features
is to feed multiple resized input images to a shared deep
network and then merge the resulting features for pixelwise classiﬁcation. In this work, we propose an attention
mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train
with multi-scale input images and the attention model. The
proposed attention model not only outperforms averageand max-pooling, but allows us to diagnostically visualize
the importance of features at different positions and scales.
Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate
the effectiveness of our model with extensive experiments on
three challenging datasets, including PASCAL-Person-Part,
PASCAL VOC 2012 and a subset of MS-COCO 2014.
1. Introduction
Semantic image segmentation, also known as image labeling or scene parsing, relates to the problem of assigning
semantic labels (e.g., “person” or “dog”) to every pixel in
the image. It is a very challenging task in computer vision
and one of the most crucial steps towards scene understanding . Successful image segmentation techniques could
facilitate a large group of applications such as image editing
 , augmented reality and self-driving vehicles .
Recently, various methods based
on Fully Convolutional Networks (FCNs) demonstrate
astonishing results on several semantic segmentation benchmarks. Among these models, one of the key elements to
successful semantic segmentation is the use of multi-scale
features .
In the FCNs setting,
∗Work done in part during an internship at Baidu USA.
Deep Convolutional
Neural Network
Deep Convolutional
Neural Network
Image with scale = 1
Image with scale = 0.5
Attention to
Figure 1. Model illustration. The attention model learns to put
different weights on objects of different scales. For example, our
model learns to put large weights on the small-scale person (green
dashed circle) for features from scale = 1, and large weights on the
large-scale child (magenta dashed circle) for features from scale
= 0.5. We jointly train the network component and the attention
there are mainly two types of network structures that exploit multi-scale features .
The ﬁrst type, which we refer to as skip-net, combines
features from the intermediate layers of FCNs . Features within a skip-net are multi-scale in nature
due to the increasingly large receptive ﬁeld sizes.
During training, a skip-net usually employs a two-step process
 , where it ﬁrst trains the deep network backbone and then ﬁxes or slightly ﬁne-tunes during multi-scale
feature extraction. The problem with this strategy is that
the training process is not ideal (i.e., classiﬁer training and
feature-extraction are separate) and the training time is usually long (e.g., three to ﬁve days ).
The second type, which we refer to as share-net, resizes
the input image to several scales and passes each through
a shared deep network. It then computes the ﬁnal prediction based on the fusion of the resulting multi-scale features
 . A share-net does not need the two-step training
process mentioned above. It usually employs average- or
max-pooling over scales . Features at each
scale are either equally important or sparsely selected.
Recently, attention models have shown great success in
several computer vision and natural language processing
 
tasks . Rather than compressing an entire image or sequence into a static representation, attention allows
the model to focus on the most relevant features as needed.
In this work, we incorporate an attention model for semantic image segmentation. Unlike previous work that employs
attention models in the 2D spatial and/or temporal dimension , we explore its effect in the scale dimension.
In particular, we adapt a state-of-the-art semantic segmentation model to a share-net and employ a soft attention model to generalize average- and max-pooling
over scales, as shown in Fig. 1. The proposed attention
model learns to weight the multi-scale features according
to the object scales presented in the image (e.g., the model
learns to put large weights on features at coarse scale for
large objects). For each scale, the attention model outputs
a weight map which weights features pixel by pixel, and
the weighted sum of FCN-produced score maps across all
scales is then used for classiﬁcation.
Motivated by , we further introduce extra
supervision to the output of FCNs at each scale, which we
ﬁnd essential for a better performance. We jointly train the
attention model and the multi-scale networks. We demonstrate the effectiveness of our model on several challenging
datasets, including PASCAL-Person-Part , PASCAL
VOC 2012 , and a subset of MS-COCO 2014 . Experimental results show that our proposed method consistently improves over strong baselines. The attention component also gives a non-trivial improvement over averageand max-pooling methods. More importantly, the proposed
attention model provides diagnostic visualization, unveiling
the black box network operation by visualizing the importance of features at each scale for every image position.
2. Related Work
Our model draws success from several areas, including
deep networks, multi-scale features for semantic segmentation, and attention models.
Deep networks: Deep Convolutional Neural Networks
(DCNNs) have demonstrated state-of-the-art performance on several computer vision tasks, including image classiﬁcation and object detection
 . For the semantic image segmentation task, stateof-the-art methods are variants of the fully convolutional
neural networks (FCNs) , including .
In particular, our method builds upon the current state-ofthe-art DeepLab model .
Multi-scale features: It is known that multi-scale features are useful for computer vision tasks, e.g., . In
the context of deep networks for semantic segmentation, we
mainly discuss two types of networks that exploit multiscale features.
The ﬁrst type, skip-net, exploits features
from different levels of the network. For example, FCN-8s
 gradually learns ﬁner-scale prediction from lower lay-
Concatenated Features
(a) skip-net
(b) share-net
Figure 2. Different network structures for extracting multi-scale
features: (a) Skip-net: features from intermediate layers are fused
to produce the ﬁnal output.
(b) Share-net: multi-scale inputs
are applied to a shared network for prediction.
In this work,
we demonstrate the effectiveness of the share-net when combined
with attention mechanisms over scales.
ers (initialized with coarser-scale prediction). Hariharan et
al. classiﬁed a pixel with hypercolumn representation
(i.e., concatenation of features from intermediate layers).
Mostajabi et al. classiﬁed a superpixel with features
extracted at zoom-out spatial levels from a small proximal
neighborhood to the whole image region. DeepLab-MSc
(DeepLab with Multi-Scale features) applied Multi-
Layer Perceptrons (MLPs) to the input image and to the
outputs of pooling layers, in order to extract multi-scale features. ParseNet aggregated features over the whole image to provide global contextual information.
The second type, share-net, applies multi-scale input images to a shared network. For example, Farabet et al. 
employed a Laplacian pyramid, passed each scale through
a shared network, and fused the features from all the scales.
Lin et al. resized the input image for three scales and
concatenated the resulting three-scale features to generate
the unary and pairwise potentials of a Conditional Random
Field (CRF). Pinheiro et al. , instead of applying multiscale input images at once, fed multi-scale images at different stages in a recurrent convolutional neural network.
This share-net strategy has also been employed during the
test stage for a better performance by Dai et al. . In this
work, we extend DeepLab to be a type of share-net and
demonstrate its effectiveness on three challenging datasets.
Note that Eigen and Fergus fed input images to DC-
NNs at three scales from coarse to ﬁne sequentially. The
DCNNs at different scales have different structures, and a
two-step training process is required for their model.
Attention models for deep networks: In computer vision, attention models have been used widely used for image classiﬁcation and object detection .
Mnih et al. learn an attention model that adaptively selects image regions for processing. However, their attention
model is not differentiable, which is necessary for standard
backpropagation during training. On the other hand, Gregor
et al. employ a differentiable attention model to specify
where to read/write image regions for image generation.
Bahdanau et al. propose an attention model that
softly weights the importance of input words in a source
sentence when predicting a target word for machine translation. Following this, Xu et al. and Yao et al. use
attention models for image captioning and video captioning respectively. These methods apply attention in the 2D
spatial and/or temporal dimension while we use attention to
identify the most relevant scales.
Attention to scale: To merge the predictions from multiscale features, there are two common approachs: averagepooling or max-pooling over scales. Motivated by , we propose to jointly learn an attention model
that softly weights the features from different input scales
when predicting the semantic label of a pixel. The ﬁnal output of our model is produced by the weighted sum of score
maps across all the scales. We show that the proposed attention model not only improves performance over averageand max-pooling, but also allows us to diagnostically visualize the importance of features at different positions and
scales, separating us from existing work that exploits multiscale features for semantic segmentation.
3.1. Review of DeepLab
FCNs have proven successful in semantic image segmentation . In this subsection, we brieﬂy review
the DeepLab model , which is a variant of FCNs .
DeepLab adopts the 16-layer architecture of state-of-theart classiﬁcation network of (i.e., VGG-16 net). The
network is modiﬁed to be fully convolutional , producing dense feature maps.
In particular, the last fullyconnected layers of the original VGG-16 net are turned into
convolutional layers (e.g., the last layer has a spatial convolutional kernel with size 1×1). The spatial decimation factor
of the original VGG-16 net is 32 because of the employment
of ﬁve max-pooling layers each with stride 2. DeepLab reduces it to 8 by using the `a trous (with holes) algorithm ,
and employs linear interpolation to upsample by a factor of
8 the score maps of the ﬁnal layer to original image resolution. There are several variants of DeepLab . In this
work, we mainly focus on DeepLab-LargeFOV. The sufﬁx,
LargeFOV, comes from the fact that the model adjusts the
ﬁlter weights at the convolutional variant of fc6 (fc6 is the
original ﬁrst fully connected layer in VGG-16 net) with `a
trous algorithm so that its Field-Of-View is larger.
two scales
Weight Maps
Split for two scales
Figure 3. (a) Merging score maps (i.e., last layer output before
SoftMax) for two scales. (b) Our proposed attention model makes
use of features from FCNs and produces weight maps, reﬂecting
how to do a weighted merge of the FCN-produced score maps at
different scales and at different positions.
3.2. Attention model for scales
Herein, we discuss how to merge the multi-scale features
for our proposed model. We propose an attention model that
learns to weight the multi-scale features. Average pooling
 or max pooling over scales to merge features can be considered as special cases of our method.
Based on share-net, suppose an input image is resized to
several scales s ∈{1, ..., S}. Each scale is passed through
the DeepLab (the FCN weights are shared across all scales)
and produces a score map for scale s, denoted as f s
i ranges over all the spatial positions (since it is fully convolutional) and c ∈{1, ..., C} where C is the number of
classes of interest. The score maps f s
i,c are resized to have
the same resolution (with respect to the ﬁnest scale) by bilinear interpolation. We denote gi,c to be the weighted sum
of score maps at (i, c) for all scales, i.e.,
The weight ws
i is computed by
t=1 exp(ht
i is the score map (i.e., last layer output before Soft-
Max) produced by the attention model at position i for scale
s. Note ws
i is shared across all the channels. The attention
model is parameterized by another FCN so that dense maps
are produced. The proposed attention model takes as input
the convolutionalized fc7 features from VGG-16 , and
it consists of two layers (the ﬁrst layer has 512 ﬁlters with
kernel size 3×3 and second layer has S ﬁlters with kernel
size 1×1 where S is the number of scales employed). We
will discuss this design choice in the experimental results.
The weight ws
i reﬂects the importance of feature at position i and scale s. As a result, the attention model decides
how much attention to pay to features at different positions
and scales. It further enables us to visualize the attention
for each scale by visualizing ws
i . Note in our formulation,
average-pooling or max-pooling over scales are two special
cases. In particular, the weights ws
i in Eq. (1) will be replaced by 1/S for average-pooling, while the summation in
Eq. (1) becomes the max operation and ws
i = 1 ∀s and i in
the case of max-pooling.
We emphasize that the attention model computes a soft
weight for each scale and position, and it allows the gradient
of the loss function to be backpropagated through, similar
to . Therefore, we are able to jointly train the attention
model as well as the FCN (i.e., DeepLab) part end-to-end.
One advantage of the proposed joint training is that tedious
annotations of the “ground truth scale” for each pixel is
avoided, letting the model adaptively ﬁnd the best weights
on scales.
3.3. Extra supervision
We learn the network parameters using training images
annotated at the pixel-level. The ﬁnal output is produced by
performing a softmax operation on the merged score maps
across all the scales. We minimize the cross-entropy loss
averaged over all image positions with Stochastic Gradient Descent (SGD). The network parameters are initialized
from the ImageNet-pretrained VGG-16 model of .
In addition to the supervision introduced to the ﬁnal output, we add extra supervision to the FCN for each scale .
The motivation behind this is that we would like to merge
discriminative features (after pooling or attention model)
for the ﬁnal classiﬁer output. As pointed out by , discriminative classiﬁers trained with discriminative features
demonstrate better performance for classiﬁcation tasks. Instead of adding extra supervision to the intermediate layers
 , we inject extra supervision to the ﬁnal output
of DeepLab for each scale so that the features to be merged
are trained to be more discriminative. Speciﬁcally, the total
loss function contains 1 + S cross entropy loss functions
(one for ﬁnal output and one for each scale) with weight
one for each. The ground truths are downsampled properly
w.r.t. the output resolutions during training.
4. Experimental Evaluations
In this section, after presenting the common setting
for all the experiments, we evaluate our method on three
datasets, including PASCAL-Person-Part , PASCAL
VOC 2012 , and a subset of MS-COCO 2014 .
Network architectures: Our network is based on the
publicly available model, DeepLab-LargeFOV , which
modiﬁes VGG-16 net to be FCN . We employ the
same settings for DeepLab-LargeFOV as .
Baseline: DeepLab-LargeFOV
Merging Method
Scales = {1, 0.5}
Max-Pooling
Average-Pooling
Scales = {1, 0.75, 0.5}
Max-Pooling
Average-Pooling
Table 1. Results on PASCAL-Person-Part validation set. E-Supv:
extra supervision.
Torso U-arms L-arms U-legs L-legs
81.47 59.06
93.65 56.39
Table 2. Per-part results on PASCAL-Person-Part validation set
with our attention model.
Training: SGD with mini-batch is used for training. We
set the mini-batch size of 30 images and initial learning rate
of 0.001 (0.01 for the ﬁnal classiﬁer layer). The learning
rate is multiplied by 0.1 after 2000 iterations. We use the
momentum of 0.9 and weight decay of 0.0005. Fine-tuning
our network on all the reported experiments takes about
21 hours on an NVIDIA Tesla K40 GPU. During training, our model takes all scaled inputs and performs training jointly. Thus, the total training time is twice that of a
vanilla DeepLab-LargeFOV. The average inference time for
one PASCAL image is 350 ms.
Evaluation metric:
The performance is measured
in terms of pixel intersection-over-union (IOU) averaged
across classes .
Reproducibility:
The proposed methods are implemented by extending Caffe framework . The code and
models are available at 
com/projects/DeepLab.html.
Experiments:
To demonstrate the effectiveness of
our model, we mainly experiment along three axes: (1)
multi-scale inputs (from one scale to three scales with
s ∈{1, 0.75, 0.5}), (2) different methods (average-pooling,
max-pooling, or attention model) to merge multi-scale features, and (3) training with or without extra supervision.
4.1. PASCAL-Person-Part
Dataset: We perform experiments on semantic part segmentation, annotated by from the PASCAL VOC 2010
dataset. Few works have worked on the animal
part segmentation for the dataset. On the other hand, we
focus on the person part for the dataset, which contains
more training data and large scale variation. Speciﬁcally,
the dataset contains detailed part annotations for every person, including eyes, nose, etc. We merge the annotations
to be Head, Torso, Upper/Lower Arms and Upper/Lower
Legs, resulting in six person part classes and one background class. We only use those images containing persons
for training (1716 images) and validation (1817 images).
Improvement over DeepLab: We report the results in
Tab. 1 when employing DeepLab-LargeFOV as the baseline. We ﬁnd that using two input scales improves over using only one input scale, and it is also slightly better than
using three input scales combined with average-pooling or
attention model. We hypothesize that when merging three
scale inputs, the features to be merged must be sufﬁciently
discriminative or direct fusion degrades performance. On
the other hand, max-pooling seems robust to this effect.
No matter how many scales are used, our attention model
yields better results than average-pooling and max-pooling.
We further visualize the weight maps produced by maxpooling and our attention model in Fig. 4, which clearly
shows that our attention model learns better interpretable
weight maps for different scales. Moreover, we ﬁnd that
by introducing extra supervision to the FCNs for each scale
signiﬁcantly improves the performance (see the column w/
E-Supv), regardless of what merging scheme is employed.
The results show that adding extra supervision is essential
for merging multi-scale features. Finally, we compare our
proposed method with DeepLab-MSc-LargeFOV, which exploits the features from the intermediate layers for classiﬁcation (MSc denotes Multi-Scale features). Note DeepLab-
MSc-LargeFOV is a type of skip-net.
Our best model
(56.39%) attains 2.67% better performance than DeepLab-
MSc-LargeFOV (53.72%).
Design choices: For all the experiments reported in this
work, our proposed attention model takes as input the convolutionalized fc7 features , and employs a FCN consisting of two layers (the ﬁrst layer has 512 ﬁlters with kernel size 3×3 and the second layer has S ﬁlters with kernel
size 1×1, where S is the number of scales employed). We
have experimented with different settings, including using
only one layer for the attention model, changing the kernel
of the ﬁrst layer to be 1×1, and varying the number of ﬁlters for the ﬁrst layer. The performance does not vary too
much; the degradation ranges from 0.1% to 0.4%. Furthermore, we ﬁnd that using fc8 as features for the attention
model results in worse performance (drops ∼0.5%) with
similar results for fc6 and fc7. We also tried adding one
more scale (four scales in total: s ∈{1, 0.75, 0.5, 0.25}),
however, the performance drops by 0.5%. We believe the
score maps produced from scale s = 0.25 were simply too
small to be useful.
Qualitative results: We visualize the part segmentation
results as well as the weight maps produced by the attention model in Fig. 5. Merging the multi-scale features with
Baseline: DeepLab-LargeFOV
Merging Method
Scales = {1, 0.5}
Max-Pooling
Average-Pooling
Scales = {1, 0.75, 0.5}
Max-Pooling
Average-Pooling
Table 3. Results on PASCAL VOC 2012 validation set, pretrained
with ImageNet. E-Supv: extra supervision.
the attention model yields not only better performance but
also more interpretable weight maps. Speciﬁcally, scale-1
attention (i.e., the weight map learned by attention model
for scale s = 1) usually focuses on small-scale objects,
scale-0.75 attention concentrates on middle-scale objects,
and scale-0.5 attention usually puts large weight on largescale objects or background, since it is easier to capture the
largest scale objects or background contextual information
when the image is shrunk to be half of the original resolution.
Failure modes: We show two failure examples in the
bottom of Fig. 5.
The failure examples are due to the
extremely difﬁcult human poses or the confusion between
cloth and person parts. The ﬁrst problem may be resolved
by acquiring more data, while the second one is challenging
because person parts are usually covered by clothes.
Supplementary materials: In the supplementary materials, we apply our trained model to some videos from
MPII Human Pose dataset . The model is not ﬁne-tuned
on the dataset, and the result is run frame-by-frame. As
shown in the video, even for images from another dataset,
our model is able to produce reasonably and visually good
part segmentation results and it infers meaningful attention
for different scales. Additionally, we provide more qualitative results for all datasets in the supplementary materials.
4.2. PASCAL VOC 2012
Dataset: The PASCAL VOC 2012 segmentation benchmark consists of 20 foreground object classes and one
background class. Following the same experimental protocol , we augment the original training set from
the annotations by . We report the results on the original
PASCAL VOC 2012 validation set and test set.
Pretrained with ImageNet: First, we experiment with
the scenario where the underlying DeepLab-LargeFOV is
only pretrained on ImageNet .
Our reproduction of
DeepLab-LargeFOV and DeepLab-MSc-LargeFOV yields
performance of 62.28% and 64.39% on the validation
(a) Scale-1 Attention
(b) Scale-0.75 Attention
(c) Scale-0.5 Attention
(a) Scale-1 Attention
(b) Scale-0.75 Attention
(c) Scale-0.5 Attention
Figure 4. Weight maps produced by max-pooling (row 2) and by attention model (row 3). Notice that our attention model learns better
interpretable weight maps for different scales. (a) Scale-1 attention (i.e., weight map for scale s = 1) captures small-scale objects, (b)
Scale-0.75 attention usually focuses on middle-scale objects, and (c) Scale-0.5 attention emphasizes on background contextual information.
Upper Arms
Lower Arms
Upper Legs
Lower Legs
(b) Baseline
(c) Our model
(d) Scale-1 Attention
(e) Scale-0.75 Attention
(f) Scale-0.5 Attention
Figure 5. Results on PASCAL-Person-Part validation set. DeepLab-LargeFOV with one scale input is used as the baseline. Our model
employs three scale inputs, attention model and extra supervision. Scale-1 attention captures small-scale parts, scale-0.75 attention catches
middle-scale torsos and legs, while scale-0.5 attention focuses on large-scale legs and background. Bottom two rows show failure examples.
set, respectively. They are similar to those (62.25% and
64.21%) reported in . We report results of the proposed
methods on the validation set in Tab. 3. We observe similar experimental results as PASCAL-Person-Part dataset:
(1) Using two input scales is better than single input scale.
(2) Adding extra supervision is necessary to achieve better
performance for merging three input scales, especially for
average-pooling and the proposed attention model. (3) The
best performance (6.8% improvement over the DeepLab-
LargeFOV baseline) is obtained with three input scales, attention model, and extra supervision, and its performance is
4.69% better than DeepLab-MSc-LargeFOV (64.39%).
Pretrained with ImageNet
DeepLab-LargeFOV 
DeepLab-MSc-LargeFOV 
TTI zoomout v2 
ParseNet 
DeepLab-LargeFOV-AveragePooling
DeepLab-LargeFOV-MaxPooling
DeepLab-LargeFOV-Attention
Pretrained with MS-COCO
DeepLab-CRF-COCO-LargeFOV 
DeepLab-MSc-CRF-COCO-LargeFOV 
CRF-RNN 
BoxSup 
Adelaide 
DeepLab-CRF-COCO-LargeFOV-Attention
DeepLab-CRF-COCO-LargeFOV-Attention+
DeepLab-CRF-Attention-DT 
Table 4. Labeling IOU on the PASCAL VOC 2012 test set.
We also report results on the test set for our best model
in Tab. 4.
First, we observe that the attention model
yields a 1% improvement over average pooling, consistent with our results on the validation set. We then compare our models with DeepLab-LargeFOV and DeepLab-
MSc-LargeFOV . We ﬁnd that our proposed model improves 6.4% over DeepLab-LargeFOV, and gives a 4.5%
boost over DeepLab-MSc LargeFOV. Finally, we compare
our models with two other methods: ParseNet and
TTI zoomout v2 .
ParseNet incorporates the imagelevel feature as global contextual information. We consider
ParseNet as a special case to exploit multi-scale features,
where the whole image is summarized by the image-level
feature. TTI zoomout v2 also exploits features at different
spatial scales. As shown in the table, our proposed model
outperforms both of them. Note none of the methods discussed here employ a fully connected CRF .
Pretrained with MS-COCO: Second, we experiment
with the scenario where the underlying baseline, DeepLab-
LargeFOV, has been pretrained on the MS-COCO 2014
dataset .
The goal is to test if we can still observe
any improvement with such a strong baseline. As shown in
Tab. 5, we again observe similar experimental results, and
our best model still outperforms the DeepLab-LargeFOV
baseline by 3.84%. We also report the best model on the
test set in the bottom of Tab. 4. For a fair comparison with
the reported DeepLab variants on the test set, we employ a
fully connected CRF as post processing. As shown in
the table, our model attains the performance of 75.1%, out-
Baseline: DeepLab-LargeFOV
Merging Method
Scales = {1, 0.5}
Max-Pooling
Average-Pooling
Scales = {1, 0.75, 0.5}
Max-Pooling
Average-Pooling
Table 5. Results on PASCAL VOC 2012 validation set, pretrained
with MS-COCO. E-Supv: extra supervision.
performing DeepLab-CRF-LargeFOV and DeepLab-MSc-
CRF-LaregeFOV by 2.4%, and 1.5%, respectively. Motivated by , incorporating data augmentation by randomly scaling input images (from 0.6 to 1.4) during training
brings extra 0.6% improvement in our model.
Note our models do not outperform current best models
 , which employ joint training of CRF (e.g., with the
spatial pairwise term) and FCNs . However, we believe
our proposed method (e.g., attention model for scales) could
be complementary to them. We emphasize that our models
are trained end-to-end with one pass to exploit multi-scale
features, instead of multiple training steps. Recently, 
has been shown that further improvement can be attained
by combining our proposed model and a discriminatively
trained domain transform .
4.3. Subset of MS-COCO
Dataset: The MS-COCO 2014 dataset contains 80
foreground object classes and one background class. The
training set has about 80K images, and 40K images for validation. We randomly select 10K images from the training set and 1,500 images from the validation set . The goal is to demonstrate
our model on another challenging dataset.
Improvement over DeepLab: In addition to observing similar results as before, we ﬁnd that the DeepLab-
LargeFOV baseline achieves a low mean IOU 31.22% in
Tab. 6 due to the difﬁculty of MS-COCO dataset (e.g.,
large object scale variance and more object classes). However, employing multi-scale inputs, attention model, and extra supervision can still bring 4.6% improvement over the
DeepLab-LargeFOV baseline, and 4.17% over DeepLab-
MSc-LargeFOV (31.61%). We ﬁnd that the results of employing average-pooling and the attention model as merging
methods are very similar. We hypothesize that many small
object classes (e.g., fork, mouse, and toothbrush) with extremely low prediction accuracy reduce the improvement.
(b) Baseline
(c) Our model
(d) Scale-1 Attention
(e) Scale-0.75 Attention
(f) Scale-0.5 Attention
Figure 6. Results on PASCAL VOC 2012 validation set. DeepLab-LargeFOV with one scale input is used as baseline. Our model employs
three scale inputs, attention model and extra supervision. Scale-1 attention captures small-scale dogs (dark blue label), scale-0.75 attention
concentrates on middle-scale dogs and part of sofa (light green label), while scale-0.5 attention catches largest-scale dogs and sofa.
(b) Baseline
(c) Our model
(d) Scale-1 Attention
(e) Scale-0.75 Attention
(f) Scale-0.5 Attention
Figure 7. Results on subset of MS-COCO 2014 validation set. DeepLab-LargeFOV with one scale input is used as baseline. Our model
employs three scale inputs, attention model and extra supervision. Scale-1 attention captures small-scale person (dark red label) and
umbrella (violet label). Scale-0.75 attention concentrates on middle-scale umbrella and head, while scale-0.5 attention catches large-scale
person torso.
This challenging problem (i.e., segment small objects and
handle imbalanced classes) is considered as future work.
On the other hand, we show the performance for the person class in Tab. 7 because it occurs most frequently and
appears with different scales (see Fig. 5(a), and Fig. 13(b)
in ) in this dataset. As shown in the table, the improve-
Baseline: DeepLab-LargeFOV
Merging Method
Scales = {1, 0.5}
Max-Pooling
Average-Pooling
Scales = {1, 0.75, 0.5}
Max-Pooling
Average-Pooling
Table 6. Results on the subset of MS-COCO validation set with
DeepLab-LargeFOV as the baseline. E-Supv: extra supervision.
Baseline: DeepLab-LargeFOV
Merging Method
Scales = {1, 0.5}
Max-Pooling
Average-Pooling
Scales = {1, 0.75, 0.5}
Max-Pooling
Average-Pooling
Table 7. Person class IOU on subset of MS-COCO validation set
with DeepLab-LargeFOV as baseline. E-Supv: extra supervision.
ment from the proposed methods becomes more noticeable
in this case, and we observe the same results as before. The
qualitative results are shown in Fig. 7.
5. Conclusion
For semantic segmentation, this paper adapts a state-ofthe-art model (i.e., DeepLab-LargeFOV) to exploit multiscale inputs.
Experiments on three datasets have shown
that: (1) Using multi-scale inputs yields better performance
than a single scale input. (2) Merging the multi-scale features with the proposed attention model not only improves
the performance over average- or max-pooling baselines,
but also allows us to diagnostically visualize the importance
of features at different positions and scales. (3) Excellent
performance can be obtained by adding extra supervision to
the ﬁnal output of networks for each scale.
Acknowledgments
This work wast partly supported by
ARO 62250-CS and NIH Grant 5R01EY022247-03. We
thank Xiao-Chen Lian for valuable discussions. We also
thank Sam Hallman and Haonan Yu for the proofreading.
Supplementary Material
We include as appendix: (1) more qualitative results on
PASCAL-Person-Part, PASCAL VOC 2012, and subset of
MS-COCO 2014 datasets.
A. More qualitative results
We show more qualitative results on PASCAL-Person-
Part in Fig. 8, on PASCAL VOC 2012 in Fig. 9,
and on subset of MS-COCO 2014 in Fig. 10.