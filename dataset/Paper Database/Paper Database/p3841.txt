Ensemble Learning for Conﬁdence Measures in Stereo Vision
Ralf Haeusler
Computer Science Department
The University of Auckland
 
Rahul Nair, and Daniel Kondermann
Heidelberg Collaboratory for Image Processing
University of Heidelberg
 
With the aim to improve accuracy of stereo conﬁdence
measures, we apply the random decision forest framework
to a large set of diverse stereo conﬁdence measures. Learning and testing sets were drawn from the recently introduced
KITTI dataset, which currently poses higher challenges to
stereo solvers than other benchmarks with ground truth for
stereo evaluation.
We experiment with semi global matching stereo (SGM)
and a census dataterm, which is the best performing realtime capable stereo method known to date.
On KITTI images, SGM still produces a signiﬁcant
amount of error. We obtain consistently improved area under curve values of sparsiﬁcation measures in comparison
to best performing single stereo conﬁdence measures where
numbers of stereo errors are large. More speciﬁcally, our
method performs best in all but one out of 194 frames of the
KITTI dataset.
1. Introduction
A vast amount of algorithms to solve the stereo problem
have been proposed with the target to yield improved error
statistics on popular benchmarking datasets. It is now well
known that good rankings in benchmarks do not imply satisfying results for challenging image data. Recently, this issue has been approached through deﬁnition of a more challenging benchmark , and further improvements on performance of stereo solvers are anticipated. However, little
attention has been paid to the question whether current solutions in increasingly challenging matching problems are actually reliable. This question becomes more important, with
increasing degree of ill-conditioning in a matching task. We
illustrate this for the stereo case: If, in a worst case scenario,
one of the two cameras fails, dense matching results can be
computed, but these are not reliable in any location.
Related areas of mismatches need to be detected. A common method is to match in both directions and evaluate the
consistency. We illustrate that this method is not perfect but
quite effective, by plotting consistency gaps over disparity
errors, see Figure 1.
Applications where accurate stereo conﬁdence measures
are essential in raising reliability of computer vision include
sparse or dense 3D scene reconstructions.
Proposals have been made in the literature on how
matching reliability could be captured . However,
each of the proposals incorporate certain weaknesses, that
is, these may be suitable only for speciﬁc image data and
fail in situations where discriminative power for particular
matching errors is low. This has initiated attempts to combine several conﬁdence measures with the aim of achieving
superior accuracy in detection of bad matching estimates.
Previous solutions were based on a very limited
set of features capturing conﬁdence and were tested only on
data not presenting much challenge to stereo.
In this paper, we employ strong energy based conﬁdence
clues and use a larger and signiﬁcantly more challenging
stereo dataset introduced recently , where results compare much better to real-world scenarios than was the case
with benchmarks proposed previously.
The paper is organized as follows: Section 2 provides
a brief overview of related work. Section 3 details challenges in deﬁning conﬁdence for matching tasks, compiles
some proposals for stereo conﬁdence deﬁnition and introduces new conﬁdence deﬁnitions used in this paper. Section 4 explains the machine learning framework used for
conﬁdence accuracy improvements. Section 5 describes experiments conducted. Sections 6 and 7 contain results and
discussion. Section 8 concludes.
2. Related Work
Kong and Tao proposed a stereo matcher, where distributions of labels for good, bad and foreground fattening
affected disparities are estimated in a MAP-MRF framework based on horizontal texture and distances to closest
foreground objects drawn from ground truth.
al. derived binary conﬁdence labels by learning from
a larger set of features amenable to hardware processing using decision trees and ANNs. Both approaches were evalu-
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.46
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.46
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.46
Figure 1. Error of SGM stereo result to ground truth plotted against
left-right difference of corresponding points in disparity maps of
both views. In challenging images, a signiﬁcant proportion of bad
pixels is not ﬁltered (false negative). Likewise, many good estimates are removed (false positives).
ated only on well-behaved stereo data.
For optical ﬂow, Gehrig and Scharw¨achter used
Gaussian mixtures to model a feature space composed of
spatial and temporal ﬂow variance, residual ﬂow energy and
structure tensor eigenvalues on small image patches. Mixture coefﬁcients and diagonal covariances were estimated
using supervised learning (ﬁfteen classes, deﬁned by intervals of ﬂow end point error) in an expectation maximization
framework. Multi-cue conﬁdence was deﬁned as classiﬁcation outcome according to the highest class posterior. It
was not clear whether the combination consistently outperformed single features.
Aodha et al. used a similar set of features as Gehrig
and Scharw¨achter to estimate multi-feature ﬂow conﬁdence, but also included image gradient and distance transform on the Canny image of the estimated ﬂow ﬁeld. Additionally, these features were densely scale-space sampled
with a rescaling factor of 0.8. Learning was performed using a random decision forest framework. It was demonstrated, that the combination outperforms single features on
many elements of a medium sized dataset of engineered and
synthetic images.
Work of Aodha et al. makes the assumption that solutions of the ﬂow problem are well deﬁned in general. This
is expressed in the idea that a conﬁdence measure can successfully select best ﬁtting results from multiple algorithms,
ignoring the fact that ﬂow is often undeﬁned, e.g., in areas
becoming occluded in subsequent frames.
Quality of results in combinaton-by-classiﬁcation approaches entirely depends on the strength of contributing
features and on the capability of the learning algorithm to
deal with correlated variables.
Regarding above mentioned conﬁdence features for optical ﬂow , image gradients in conjunction with ﬂow variance are likely to detect lowly textured areas in input images with high variance in ﬂow. Indeed, such ﬂow results
are likely to be unreliable in our experience.
However, in stereo and motion alike, reasons for failure
may not be restricted to low texturedness. Hence, using a
more diversiﬁed set of conﬁdence measures as contributing features is very likely to result in improved accuracy
for good or bad pixel detection due to consideration for an
increased number of possible reasons for algorithm failure.
So, in the following section, we discuss various stereo
conﬁdence measures proposed in the literature, and attempt
to motivate a selection of most promising measures.
3. Conﬁdence Measures for Stereo
Causes for errors in disparity estimation within a global
stereo optimization framework can be based on inappropriate model assumptions, highly nonconvex energies causing
multiple strong local minima or numerically instable global
Conﬁdence is commonly understood either as error prediction or only as a measure for uncertainty of results. Assuming error prediction worked out, we would know error
magnitudes and could plug these into the stereo estimation
model to improve stereo results directly.
However, we can only hope to gain knowlege about suitability of signals to provide good estimates of stereo disparities in most cases, e.g., in untextured or repetitive image
regions. If this is the case, all we can do is to attempt prediction of potentially large matching errors.
In the absense of a strong theoretical foundation to account for properties of global energies in commonplace
stereo aggregation schemes, many spatially local stereo
conﬁdence measures have been proposed . However,
evaluation has been carried out for a local stereo matching
algorithm and on a small dataset only. Also, these measures
may be accurate only in speciﬁc matching situations. Below
we brieﬂy discuss the most prominent proposals for stereo
conﬁdence.
To clarify the intention behind deﬁning conﬁdence measures for matching, we would like to point out again, that
conﬁdence is not supposed to be a measure for potential
disparity error magnitudes. Rather it should be a measure
for the likelihood of an algorithm to fail due to high challenges of a speciﬁc matching situation. Failing means to
exceed a certain error bound. For low conﬁdence matching
situations, no improved or specialist algorithm may exist
for obtaining a solution. Good conﬁdence measures detect
areas that cannot be matched reliably.
In the following deﬁnitions, c refers to matching costs
resulting from a Semi global matching (SGM) aggregation scheme.
Curvature of a parabola ﬁt to matching costs c for subpixel estimation at a pixel p is frequently considered to be
a conﬁdence measure. However, this curvature rarely provides accurate information about gross mismatches. It may
only be useful to estimate variances of disparities, given that
a match is known to be correct.
The peak ratio measure is widely used in descriptor
matching to reject correspondences with close matching
costs which are believed to be ambiguous. In the following, d1 denominates the disparity with lowest associated
cost c(p, d1) and d2 is a disparity where c(p, d2) is a local minimum with second lowest cost at pixel p. The peak
ratio for a disparity at pixel p is then deﬁned as
Γ0(p) = c(p, d1)/c(p, d2) .
Note that with some dataterms (e.g. census), image noise
can propagate through aggregation and lead to large peak
ratios even in reliable matches. This can be the case if |d1 −
d2| is very small.
Entropy of disparity costs for controlling a diffusion process in cost aggregation attracted some attention as a
potential conﬁdence measure. Certainly, ﬂat or noisy cost
functions contain little information and are less likely to result in a good correspondence. For deﬁning entropy, costs c
need to be normalized into a probability distribution p:
p(d) log p(d) with p(d) =
d′ e−c(p,d′)
Merrell et al. propose another measure integrating
costs for all disparity estimates. We coin it Perturbation
measure due to its design target to capture the deviation of
cost function c to an ideal function which is large at all locations except at the minimum d1. The deﬁnition is
e−(c(p,d1)−c(p,d))2
We found careful scaling with parameter s crucial in avoiding numerical problems related to ﬂoating point accuracy.
Though not perfect, as illustrated in Fig. 1, consistency
between left and right disparity is an established criterion
for identiﬁcation of mismatches and occlusions . The
deﬁnition requires disparity maps Dl and Dr of left and
right image:
Dl(p) −Dr(p −(dl
Image gradient determines the ability of data terms to
generate distinctive scores.
In stereo, low texture along
epipolar lines is critical. This motivates the deﬁnition of
horizontal gradient as a conﬁdence measure:
Γ4(p) =∥∇xIl(p) ∥
Note, however, that estimated depth edges often do not
coincide with image gradients due to foreground fattening .
Disparity map variance, deﬁned as
Γ5(p) =∥∇Dl
is usually a good indication of problematic correspondences
as errors occur often on or near depth discontinuities. However, Γ5 may be less suitable if used in conjunction with
stereo algorithms that frequently locate discontinuities well.
This may be the case in segmentation based stereo approaches.
A measure coined disparity ambiguity here is introduced
to capture potential error magnitudes for the case of mismatches resulting from matching ambiguities (which may
be detected by peak ratio Γ0 deﬁned above).
Γ6(p) = |Dl
Although not beneﬁcial as a conﬁdence measure itself, inclusion of disparity ambiguity into a learning framework is
an attempt to separate small from large errors in image locations where the peak ratio may fail as explained above.
As an additional conﬁdence measure, we use Zero mean
Sum of Absolute Differences (ZSAD) matching costs between (left and right) image intensities Il and Ir for the
winning disparity d1:
Γ7(p) = ZSAD
Il(p), Ir 
Another proposal for conﬁdence is what we call semi global
energy: We compute the sum of data and smoothness term
in a small neighborhood for each pixel, choosing a patch
size of 25 × 25 and aggregate along emerging rays in eight
directions r for these experiments. The feature is deﬁned in
analogy to the SGM objective energy, but with the winning
disparity d1 = Dp ﬁxed:
c(q, d1) + b1t(|Dq −DN(q)| = 1)
+ b2t(|Dq −DN(q)| > 1)
Here, Nq denotes the successor of q in the set of pixels r(p)
along ray r emerging from p. b1 and b2 are distinct penalties
for different magnitudes of disparity map gradient, and t is
a decision function.
Feature Vector Setup
We deﬁne one feature vector f7 ∈R7, containing only
information derived from input images and computed disparity maps. It is deﬁned pixel-wise as follows (we omit
argument p and learning sample indices here):
Most features are included for three scales with a rescaling factor of two. The notation indicates this with superscripts. Features for lower scales are separately extracted
from stereo computed on down-scaled images and not by
downscaling of feature maps. Bi-linear interpolation was
used for upscaling.
Vector f7 can be computed for arbitrary stereo results.
Another feature vector, f23 ∈R23, in addition contains information of spatially aggregated costs, as captured by the
features deﬁned above. This feature vector is therefore de-
ﬁned only for stereo schemes with pixel-wise cost computations for each matching candidate. We deﬁne:
4. Ensemble Learning for Conﬁdence Measures
In the following, we explain the machine learning approach chosen for combining conﬁdence measures. In particular, we motivate to formulate this as a standard classiﬁcation problem over feature vectors deﬁned in the previous
section, that is, estimation of a mapping
R : F →{−1, 1} ,
where R maps to each sample f ∈F one of two class labels, depending on stereo error bounds derived from ground
truth. Separate models are created with f7 and f23 feature
vectors, denoted here RDF7 and RDF23.
We choose a classiﬁcation approach instead of regression, as conﬁdence measures do not contain matching error
magnitude information as explained previously.
amenable properties over other learning approaches, are
used for this study. Advantages over other classiﬁcation
methods include robustness towards parametrization, low
tendency of overﬁtting data and interpretation of feature relevance.
Each decision tree in the random forest partitions feature space recursively by greedily choosing a feature and
a binary test thereupon, which minimizes an entropy based
objective function. Once the resulting partition is pure or
some other stopping criterion is met, class counts in this
partition are recorded. This corresponds to a tree structure
that can then be traversed during prediction time from root
to the leaf containing the predicted density by performing
the binary tests learned during training.
In random tree ensembles, T randomized decision trees
are grown independently with two ways of introducing randomization:
• Bagging: Each tree only uses a random subset R of all
available samples.
• Random subspace selection: For each space partitioning decision, only the best possible split of a random
subset of all possible variables is considered.
During prediction, each tree casts a vote for a class density.
Random tree ensembles can also provide information on
variable importance. Two different measures are used here
for discussion : The GINI importance measures the contribution of each variable to the decrease of the objective
function, while the permutation importance calculates the
decrease in accuracy on the out of bag samples after permuting the values of the feature.
5. Experiments
Stereo estimates are computed using semi global matching stereo (penalties b1 = 20, b2 = 100) with a binary
census data term on 7 × 7 matching windows. The choice
of this algorithm is due to best overall performance on unconstrained image data in terms of stereo accuracy 
as well as computational costs low enough for on-line results in, e.g., automotive applications . We restrict our
experiments to this powerful stereo algorithm, as we are not
interested in stereo errors introduced through weak models.
We intend to work only on genuinely hard matching problems.
We select training data from a few frames of KITTI with
depth ground truth available, consisting of laser range ﬁnder
measurements aggregated over ﬁve consecutive frames using ego-motion compensation . In an effort to reduce
adaptation to a speciﬁc matching problem domain, these
frames are selected such that a variety of different challenges are posed to the stereo algorithm, including textureless areas, very large baseline, repetitive structures,
transparencies and specular reﬂections. In particular, these
frames are those containing following numbers in their ﬁlenames: 43, 71, 82, 87, 94, 120, 122, 180.
Samples of the
above described feature vector are collected only in locations where data term values for stereo matching are available (that is, these are not set to be invalid) on all scales
and for all disparity candidates. In practice, this excludes
areas along image boundaries, in particular where occlusions are present near the left image border. The intention
is to avoid biases in learning and classiﬁcation due to nonuniform scaling of some of the used cost function based features in the presence of undeﬁned matching cost values.
Area under sparsification curve
Perturbation
R/L difference
Disparity variance
Peak ratio
Ground truth
KITTI sequence frame, permutated according to RDF 23D sorting order
Figure 2. Area under curve measures of our result (red), in comparison to four conﬁdence measures that usually perform best. Lower values
are better. The proposed method outperforms other approaches consistently in all but one frame.
As conﬁdence measures generally contain no information about error magnitudes, solving a regression problem
for feature combination is not likely to yield the intended
results. Therefore, we solve a classiﬁcation task taking into
account only error bounds as follows: Learning samples are
categorized into two classes: good and bad disparities. The
class boundary is deﬁned by a threshold of 3 px between
ground truth disparities and stereo estimates, in line with
the default of the KITTI online evaluation.
Presumably,
higher accuracies of laser range ﬁnder measurements cannot be guaranteed.
Due to very high quality of stereo results on KITTI in
general, these two classes are highly unbalanced, which
may deterioate class model quality and result in unnecesary computational costs due to high data volumes. Therefore, we apply stratiﬁed sampling to balance training data.
Learning was conducted using the machine learning module
of the Vigra library . Generalisation error is monitored
within the random forest framework by computing out of
bag errors for increasingly large stratiﬁed random subsets
of the training set.
Decision forest parameters are chosen as follows: Number of trees: T = 50, Selection ratio: R = 0.6, Minimum
sample size in each node to split: M = 20. Parameters T,
R and M were tuned to achieve an optimum in computational cost and minimize the out of bag error of the random
decision forest.
Combined conﬁdence measures for f7 and f23 alike are
deﬁned as the posterior probability of the bad disparity
Conﬁdence measures, including decision forest results,
are compared using the sparsiﬁcation strategy: Pixels in
disparity maps are successively removed, in the order of descending conﬁdence measure values, until the disparity map
is empty. Stereo error measures are computed on remaining
pixels in each iteration. If the area under the resulting curve
(AUC) is smaller than for concurrent conﬁdence measures,
it indicates that this measure is more accurate. AUC values are normalized such that conﬁdence measures discarding pixels randomly yield a value of 0.5.
6. Results
Area under the curve (AUC) values for the proposed
RDF 23 conﬁdence measure indicate superior accuracy
compared to best performing of all single conﬁdence measures on 193 out of 194 frames on the KITTI dataset, see
Fig. 2. Our result is slightly inferior only to the perturbation
measure on KITTI Frame 30. The respective sparsiﬁcation
plot for this Frame is displayed in Figure 4. On few other
frames (Frames 13,20 and 89), our method is just on a par
with the best performing single measure in terms of AUC
In the presence of frequent gross stereo errors which are
generally detected well by all features including the semi
global energy feature proposed, the RDF 23 results still
show a slight improvement, see Fig. 5. Even if a single
contributing conﬁdence measure fails (see Fig. 6), results
of RDF 23 are not compromised.
Outstanding accuracy gains from RDF 23 results are not
achieved if the conﬁdence feature set is reduced to such
Area under sparsification curve
Perturbation
R/L difference
Disparity variance
Peak ratio
Ground truth
KITTI sequence frame, permutated according to RDF 7D sorting order
Figure 3. Area under curve measures of our result when the feature set is reduced to information from disparity maps and image intensities.
Again, we compare to best performing single conﬁdence measures. Lower values are better. Frequently, results from this reduced feature
set are outperformed by single features. This demonstrates that energy based features as included in f23 are essential.
variables that can be obtained solely from disparity maps
and image intensities, assuming the stereo algorithm be a
black box (see Fig. 3). Still, results are above the average
of single features.
In RDF 23 estimation, disparity variance, perturbation,
peak ratio and left-right difference have the largest contribution according to Gini importance in decision forest estimation (see Tab. 1).
In the reduced feature set f7, Gini importance is highest
for the disparity variance variable as well (see Tab. 2).
Possible reductions of false positives and negatives of the
proposed method in comparison to the standard consistency
check method are illustrated in Fig. 7. A signifant reduction
in both, false positives and false negatives, can be observed
on depicted road surface and vehicles.
Out of bag errors do not decrease signiﬁcantly when
adding data beyond the choosen training set of size 2.2·105.
We provide a complete set of sparsiﬁcation plots, i.e.
plots for each KITTI frame .
7. Discussion
Class posteriors of f7 features yielding inferior results
to those of f23 (compare Figures 2 and 3) samples is not
surprising, as the main reason for stereo failure detectable
by f7 is textureless areas with co-located disparity discontinuities, which are less frequent in KITTI data, as related
objects (e.g. sky areas) are not covered by ground truth.
Samples from f23 better cover a wider range of potential
matching problems, such as errors at depth discontinuities,
Sparsification [%]
Number of bad disparity estimates
Sparsification on KITTI Frame 30
Ground truth
Perturbation
Left-Right difference
Disparity variance
Peak ratio
SGM energy
Figure 4. Sparsiﬁcation plot for the worst result of our method on
KITTI data on Frame 30. Note, however, that stereo estimates are
almost perfect for this frame. So, this single negative result is little
signiﬁcant.
despite the most important variable according to the Gini
measure in both feature spaces being disparity variance.
The perturbation measure attracting higher variable importance on a smaller scale suggests that conﬁdence may be
more appropriate to be looked upon at superpixel level.
In opposition to Aodha et al. , who apply a leave-oneout strategy for learning and testing, we use only a very
small fraction of data for training. This is a closer match to
applications in practice, where an extensive training dataset
Sparsification [%]
Number of bad disparity estimates
Sparsification on KITTI Frame 123
Ground truth
Perturbation
Left-Right difference
Disparity variance
Peak ratio
SGM energy
Figure 5. KITTI Frame 123, resulting in a signiﬁcant amount of
SGM stereo errors (approx. 30 percent), results in all conﬁdence
measures responding well. Our method still achieves an improvement on top of this.
Sparsification [%]
Number of bad disparity estimates
Sparsification on KITTI Frame 151
Ground truth
Perturbation
Left-Right difference
Disparity variance
Peak ratio
SGM energy
Figure 6. Though one of the contributing measures, SGM energy,
fails on Frame 151, our method results in superior accuracy compared to all single measures over the entire sparsiﬁcation range.
cannot be made available due to prohibitive costs or technical limitations. Still, failure of our method is extremely
infrequent. For the only instance on KITTI Frame 30, error
rates of the stereo algorithm are very low. In such a case,
failure is of no relevance in practical applications.
Even if our class posteriors were only on par with the
best single measure in each frame, this would be an advantage as each single measure may fail in some situations.
Undeﬁned stereo values due to occluded regions cannot
be handled separately in this study, as corresponding ground
truth data is not yet made public in KITTI. However, this
does not affect outcomes, as occlusions are simply considered to be a subclass of mismatches. Yet, separate evaluations, as done in stereo benchmarking, would be of interest.
Permutation
Disparity variance
Perturbation
Peak ratio
Left right difference
Perturbation
Peak ratio
Disparity variance
Semi global energy
Disparity variance
Perturbation
Peak ratio
Disparity ambiguity
Disparity ambiguity
Disparity ambiguity
Table 1. Variable importance in f23
Permutation
Disparity variance
Disparity variance
Left right difference
Disparity variance
Table 2. Variable importance in f7
8. Conclusion
We have demonstrated that learning a classiﬁer on multivariate conﬁdence measures is an appropriate approach to
increase accuracy in stereo error detection if a suitable set
of conﬁdence features is selected. In particular, variance
based features on image intensities and matching results
as previously applied to the optical ﬂow problem are insufﬁcient for consistently outperforming contributing con-
ﬁdence measures in stereo analysis. This requires strong
energy based features. Additionally, we conﬁrm that scale
space sampling of features is a crucial contributing factor
for success. This suggests, that modeling of spatial dependencies may further improve results.
Apart from bias that may have been introduced due to
Figure 7. Visualization of true positives (green), false positives
(red), true negatives (blue) and false negatives (yellow) according to the denominations given in the plot of Fig. 1 on KITTI
Frame 112. Our result, based on 23 dimensional features (bottom),
signiﬁcantly reduces false positives and false negatives compared
to left-right difference results (top).
ﬂaws in the ground truth data used here, advantages
of the proposed method are larger where stereo is more
challenging and hence produces more error prone results.
Yet, to shed light on this, new challenges for stereo need
to be deﬁned (and come with ground truth), beyond what
is present in KITTI data. These challenges could include
dark scenes, harsh backlight or any kind of image degradation, including issues resulting from compromised recording equipment. This would help to shift attention to speciﬁc
problems which need to be addressed before stereo vision
systems can conﬁdently be used in applicantions relevant to
safety, such as driver assistance systems.
Acknowledgements
This work was supported by Prof. Bernd J¨ahne and Prof.
Akira Nakamura. We thank Dr. Ullrich K¨othe for technical