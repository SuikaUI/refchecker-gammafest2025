QUASI-MARTINGALES
DONALD L. FISK
Introduction.
The basic ideas contained in this paper were first introduced by
Professor Herman Rubin in an invited address at the Institute of Mathematical
Statistics meetings at the University of Washington
in WSóí1).
In this paper we investigate necessary and sufficient conditions for a stochastic
process XT to have a decomposition into the sum of a martingale process and a
process having almost every sample function of bounded variation on T. Such a
process is called a quasi-martingale.
Necessary and sufficient conditions for such a decomposition have already been
obtained by P. Meyer when the process is a sub-martingale.
Johnson and
Helms have given conditions equivalent to Meyer's when the sub-martingale
is sample continuous.
Our main result, Theorem 3.3, gives necessary and sufficient conditions for a
sample continuous process XT to have the above decomposition, where both the
processes in the decomposition are sample continuous and the process of sample
bounded variation has finite expected variation. When the process is a sample
continuous sub-martingale, the conditions reduce to those given in .
It is further proved that the decomposition
of Theorem 3.3 is unique. The
uniqueness follows from Lemma 3.3.1 where we have proved that a martingale
which is sample continuous,
and of sample bounded
has constant
sample functions. This property, known true for Brownian motion, is seen to be
true for all sample continuous martingales.
The dominating
technique used throughout
the paper is random stopping
times defined in terms of the sample functions of the process. The major result
involving stopping times is Theorem 2.2 which allows us to approximate a sample
continuous
process by a sequence of sample equicontinuous
and uniformly
bounded processes.
1. Notation, definitions and examples.
Let (Q,F,P) be a probability space on
which is defined a family of random variables (r.v.'s) {X(t); teT}
where Tis a
subset of the real line. Let {F(t); te T} be a family of sub cr-fields of F with
Received by the editors June 15, 1964 and, in revised form, January 4, 1965.
i1) This paper was written as a thesis for the Ph. D. degree in the Department of Statistics
at Michigan State University, under the direction of Professor Herman Rubin. The work was
partially supported by the Office of Naval Research (Nonr 2587-02). This paper in whole or
in part may be reproduced for any purpose of the United States government.
D. L. FISK
Fis) c Fit) for every s,teT
with s ^ t. The family of r.v.'s is said to be well
adapted to the family of rr-fields if Xit) is F(r) measurable for every teT, and we
will write {A^i), F(i); teT}
to indicate this relation. Whenever we speak of a
stochastic process X, we will be referring to a family {Xit), Fit); teT} as defined
above. In many cases F(r) is the minimal o--field with respect to (w.r.t.) which the
family of r. v. 's {Xis) ; s _ 1} is measurable. Such tr-fields are denoted by ßiXis) ;s^¡t).
We will assume Tis the closed unit interval .
Definition
1.1. A process {Xit), Fit); teT} is almost surely sample continuous
if there exists A e F with P(A) = 0 such that for every teT
lim Xis, œ) = Xit, co) if ea $ A.
We will assume the separability properties of stochastic processes are well
known [1, p. 50]. We note here that if X is almost surely (a.s.) sample continuous,
then X is a separable process w.r.t. the class of closed sets; and if T0 is any denumerable dense subset of T, then it is a separating set.
Definition
1.2. A process {Xit),Fit);teT}
is called a martingale
if EiXit)) exists for every t e T and if for every s, t e T with s =" t, £(X(i) | F(s)) = Xis)
a.s., and is caleld a sub-martingale, super-martingale,
if respectively
EiXit) | Fis)) = Xis), EiXit) | Fis)) = Xis) a.s.
Definition
1.3. A process {X(r),F(r);re
T }is a.s. of bounded variation (b.v.)
if there exists a set A e F with P(A) = 0 such that each sample function Xi ■ , co)
with co 4 A is of b.v. on T.
Definition
1.4. The process {Xit), Fit) ;teT}
will be called a quasi-martingale
if there exists a martingale
process {Xyit),Fit);teT}
and a process
{X2it),Fit);teT}
with a.e. sample function of b.v. on T such that
Pi[Xit) = Xyit) + X2it);teTj)
where [•••] denotes the subset of £2 for which "••• " is true.
If X is a quasi-martingale,
we will let [Xjy and [X~\2 denote respectively the
martingale process and process of b.v. in the decomposition of X.
We now give two simple examples of quasi-martingales.
Let {X(t),F(t);teT)
be a process
with independent
increments
Fit) = ßiXis); s S t), and let £(X(í)) = m(í) exist for all t e T. Assuming m(0) = 0,
EiXit)\Fis)) = Xis) + mit)-mis).
Define X,(i) = mit) a.s. and
Xyit) = Xit) — X2(t) for every t e T. The X-process is a quasi-martingale if m(t)
is of b.v. on T.
Let {Z(0, F(t) ;teT}
be the Brownian motion process ; i.e. the process has
independent, normally distributed increments with E(Z(r) — Z(s)) = 0 and
QUASI-MARTINGALES
£(|X(t) - X(s)\2) = cr2| t - s| where er > 0 is fixed. We assume Z(O) = O a.s. so
that the process is a martingale. We further assume F(t) = ß(X(s); s ^ r).
Define X(t) = exp[Z(i)x] for every í e T, where x is an arbitrary positive real
number. If u > 0, t + u g 1,
E(X(t + u) | F(t)) = £(exp[(Z(0 + Z(t + u) - Z(t))x] | F(t))
= exp[Z(r)x]£(exp[(Z(i
+ u) - Z(r))x]) = X(t) exp[u(erx)2/2].
X(s)ds for every t e T,
then the process {X2(r),F(t); te T} has a.e. sample function of b.v. on T. It is
easily verified that the process {Xy(t) = X(f) — X2(t),F(t); t e T} is a martingale.
Work on the decomposition of super-martingales has been done by P. Meyer 
and Johnson and Helms . We will obtain the necessary and sufficient conditions
given by Johnson and Helms for the decomposition
of an a.s. sample continuous
super-martingale as a corollary to our decomposition theorem for quasi-martingales.
2. Stopping times.
We consider briefly random stopping of a process [2, pp.
530-535] and prove a basic theorem to be used throughout the paper.
Let {X(t),F(t);teT}
be a process defined on the probability space (Q,F,P)
and let x(ca) be a r.v. defined on the same probability space with range T. If for
each / e T, [x(ca) ^ t] e F(t) the r.v. t(co) is called a stopping time of the X-process.
If we define
= X(x(co),co) t>x(co)
then XT(t, • ) is F(t) measurable for every t e Tand the process {XT(t),F(t); t e T}
is called the X-process stopped at t.
The following is a standard theorem which we state here for reference [2, p. 533].
Theorem 2.1. If {X(t),F(t);teT}
is an a.s. sample right continuous submartingale
(martingale)
and if x is a stopping
time of the process, then the
stopped process {XT(t),F(t);teT}
is also a sub-martingale
(martingale).
Definition
2.2. A process {X(t),F(t);teT}
is a.s. sample equicontinuous if
for each t e T, there is a set A, e F with P(A() = 0 such that if e > 0 is given there
exists a ô > 0 such that | X(t, of) — X(s, ca) | < e whenever 11 — s \ < ô for every
co ^ A,. If T is compact, then we can find AeF independent of t e T with P(A) = 0.
Theorem 2.2. Let {X(t),F(t);teT}
be an a.s. sample continuous process.
There is a sequence of processes {Xv(t),F(t);teT}
with the following
properties'-
D. L. FISK
(i) For each v _ 1 the Xv-process is
(a) a.s. sample equicontinuous, and
(b) uniformly bounded by v.
(ii) There is a set AeF
with P(A) = 0 such that ifco^A,
then there exists
vico) such that Xit,co) = Xvit,eo)for
every t eT if v~¿. vico).
(iii) // lim,-«, rP([sup( | Xit) | è r]) - 0, then
lim £(|Xv(í)-X(í)|)
= 0 for every teT.
The proof of this theorem is easily obtained from the following lemmas.
Lemma 2.2.1. // the X-process is as defined in Theorem 2.2, then there
exists a sequence of processes satisfying
conditions (ia) and (ii).
Proof. We show there exists a sequence of processes Xv, vkl
P([XV#X])<2~V.
It then follows from the Borel-Cantelli lemma that property (ii) is satisfied.
By the a.s. sample continuity of the X-process, for each n Si 1 we can find
<5nv > 0 such that
sup \X(t,co)-X(s,œ)\\>lln\)
for every v = 1. We can assume that for each n, Snl > 5n2 > •■■. Let rnv(eo) be
the first t such that
|X(s,co)-X(s',o)|^l/n.
gá„v;s,s'áí
If no such t exists define t„v(cü) = 1. Then 0 < t„v = 1 a.s., and t„v is a "stopping
time of the X-process since for any teT,
> t] = I sup
|Xis',co) - Xis,co)| < 1 /n \.
L|s-s'|S<5nv;s,s'gt
Define tv(w) = inf„T„v(co). 0 z% tv(co) z% 1 a.s. for all v _ 1. tv(cu) will be a stopping
time for the X-process if [tv(<u) ^ i] differs from an F(i) set by a set of measure
zero [1, p. 365].
Let Av(r) = [>„„(«) > t for every n], v =■ I, í e [0,1). Then
[xv(o>) •£ q = {[tv(w) = t] n av(0} u {[tv(co) £ i] n ~ av(í)}.
If to e {[tv((«) ^ í] O Av(i)}, then for every s > 0 such that t + s < I,
t < Tnv(co) < t + E
for infinitely many n. Consequently
QUASI-MARTINGALES
co e [t„v(co) < 1 for infinitely many ri].
Thus for every v ^ 1, / e [0,1),
{[t„v(co) g t] nA,(l)}
c lim sup [t„(o>) < 1].
fn = 1 n = m
g lim I P(|>„v(co) < 1]) = lim 2"(m + v-1) = 0.
m-^co n =m
Now, {[t„(«j) g t] n ~AV(0} -(jT-ifr^")
= t] and thus
{[tv(co) g i] -
[t„v(co) g /] ) j c Av(0 e F{t)
and P(Av(i)) = 0. Hence, [tv g í] differs from a F(t) set by a set of measure zero.
Xv(r,cu) = X(r,co) if t^xfco)
= X(tv(co),cu) if í > tv(co)
for each v ^ 1, the lemma is now evident.
Lemma 2.2.2.
If the X-process
is as defined in Theorem 2.2, then there
exists a sequence of processes satisfying
conditions
(ib) and (ii).
For every v ^ 1, define tv(co) to be the first t such that supsS,| X(s,ca) 15; v.
If no such í exists, let xfco) = 1. Clearly xfca) defines a stopping time for the
.X-process. Define Xv, v ^ 1, as usual. Then Xv is uniformly bounded by v. Since
a.e. sample function has an absolute maximum on T, property (ii) of Theorem 2.2
is obviously satisfied.
The proof of Theorem 2.2 is now trivial. For let x'v(co) and x"(co), v 2; 1, be the
stopping times of the X-process defined respectively in Lemmas 2.2.1 and 2.2.2.
Ifxfco) = min[T¿(co), x" (co)], then tv is a stopping time of the X-process, and the
stopped X-processes Xv, v «£ 1, have properties (i) and (ii).
We now show property (iii). Let Av = [tv(co) < 1],
£(|Xv(i)-X(0|) = f \Xft)-X(t)\dP
g vp(av) + r \x(t)\dp.
The second term clearly goes to zero as v -» co. The first term is bounded by
D. L. FISK
vP([< < 1]) + vP([< < 1]) = v2_v + vP([sup| Xit) | ^ v]) -> 0 as v - oo.
Some additional notation and theorems will be needed to prove the general
decomposition theorem. Although the theorems are somewhat specialized, they
are not in the trend of the argument leading to the general decomposition theorem
and are therefore stated and proved at this time.
(2.2) We let {T„; n = 1} be a sequence of partitions of T= with the
following properties:
(i) Tn+ y is a refinement of T„ for every n, and
(ii) NiT„) -* 0 as n -> co, where AtTJ denotes the norm of the partition. The
points of the partition
T„ will be denoted as follows:
0 = í„,o<ín.i<---<ín.N„
If m > n, we let
*»,./ »nmj',0 *■» tnmj
••• < tnmj,k„mj
denote the points of Tm contained in the jth subinterval of the T„ partition.
Let {X(0,Fit):teT}
be a real-valued process with £(X(i)) existing for every
t e T. The following notation will be used in an effort to avoid, as much as possible,
the cumbersome notation of sums and multiple subscripts.
(2.3) For any partition T„ we write
= X(ín>J. + 1)-X(íBj),
CnJiX) = £(Anj,(X)|F„,,),
r„(X) = Z CnJiX).
If n < m so that Tm is a refinement of T„, we can write
TnmjiX) = Z CnmjikiX)
and then Tm(X) = 1,-T^/X).
(2.4) We define
0 = rn>0 = t < tn>1,
£c„,,(x) =
Z cnJiX), «„¿S*<«„,*+,.,láfeáiV,
We note that Z0Cn>J(X) e 0, but that if t e Tand t >0, then for all n sufficiently
large we will have t ¡S tmi.
QUASI-MARTINGALES
Theorem 2.3.
Let {X(t),F(t);teT}
be a second-order
process. Let [a,ß]
be a closed subinterval
of Tand let a = a0 < ay < ■■■ < an + 1 = ß be a partition
of [a,/?].
Let e > 0 be given. If
sxß = ess.sup. ma\\X(ß,ca)
— X(ak,co)\ < e,
Ck(X)\ £e])
gJE(| I Q(X)|2)/(e - sxß2,
where Ck(X) = E(X(ak+1) - X(ak) \ F(ak)).
Proof. The argument is the following: Assume AeF,
\A\ ^e and A is F
measurable. Assume further | E(B | F) | g ô < e. Then
f (A2 + 2AB + B2)dP = f (A2 + 2AE(B \ F) + E(B2 \ F))dP
^ í (A2 + 2AE(B | F) + [E(B \ F)]2)dP
^ f (A2 + ô2 -2\A\ó)dP
ô)2dP ^ (e - <5)2P(A).
Now we let
i* = o Ck(X) < e for v < m and
f Ck(X) ^e]
for 0 g m g n. Then Am e Fm, since Ck(X) is measurable w.r.t. Fm for 0 g k g m.
We also have Am f\Aj = 0 for m # ;', and
max I Ck(X) ^ 8 = [J Am.
2\ " r i m
I Ck(X) = I
Z Ck(X) + I C^X)
/ m = 0 «/Am ' le =0
Letting Xm = l,k=0Ck(X), Bm = T,k=m+yCk(X), and replacing A and F with A„
and Fm respectively and ¿> with ex ß, the above argument gives
E(\lCk(X)\2)
= (e-eXrß)2p((j
D. L. FISK
Let {Xit),Fit); teT} be a second order process which is
a.s. sample equicontinuous.
Let {Tn; n j= 1} be a sequence of partitions as defined
in (2.2). Given any s > 0, there exists nie) such that if m > n = n(e) then
cnmj>iix) >s
|WX)|2)/(£-£„)2,
P I max max
£„ = ess. sup.max sup | X(i, co) — X(s, co) | < £.
j t„,j£s,tSt„,jy.i
Because of the a.s. uniform sample equicontinuity
of the X-process,
given any e > 0 there exists (5(e) and A with P(A) = 0 such that
sup sup | X(i, co) — Xis, co) | < e.
<u*A |r-s|
Let n(e) be such that N(T„) < <5(fi) if n ^ «(e). Then e„ < e, and for all m> n,
sup max max |X(i„mi>it) - X(r„mM0 \z%e„<e.
J 0£k,k'£k„mJ
Using Theorem 2.3, we now have
P ([maxmax | ZQ Cnm;,(X) | = £ j) ^ Z p( max | Zq C„mj,,<X)| ^ a j j
|rnm/x)py(£-£n)2.
Theorem 2.4.
Assume {X„(r); re T), n i= 1, is a sequence of processes with
the following
properties:
(i) There is a countable dense subset T0 of T, containing
the points 0 and 1,
such that P lim„_0CX„(r) exists for each teT0.
(ii) Given £,y > 0, there exists n = n(£,y) and ö — öis,y) such that if m'Sz n,
( sup |Xm(i)-Xm(s)|>£
\L|í-s|*gí
Then there exists a subsequence of processes {X„k(i); te T}, k _ 1, and a process
{Xit); teT} such that
(i)' P([lim,Xni = X]) = 1 and
(ii)' the X-process is a.s. sample continuous. Furthermore,
(iii)' P lim X„(í) = Xit) for every teT.
Proof. We first show that conditions (i) and (ii) imply that
lim sup P sup X„(í) - Xm(0 > £
for every £ > 0.
QUASI-MARTINGALES
Let {Tv; v = 1} be a sequence of partitions of Tdefined as in (2.2) but with the
points of each Tv a subset of T0. Let e,y > 0 be given. First choose nt = ny(e,y)
and ö = <5(e, y) such that
p(\ sup |X„(0-X„(s)|>e/3l)
for every n^Uy.
This can be done by condition (ii). Now choose v such that
N(TV) < Ö. Next choose nv such that P([maXj\X„(tVJ) - Xm(tvj) \ > a/3]) < y/3
for every m > n §: nv. This is possible because there are only a finite number of
points in Tv and for each tvJe Tv, Plimn_0OXB(iVji;.) exists. Now let n= max(n!,nv)
and consider
p( sup |X„(0-X,„(i)|>£])
= P([maxsup|X„(0-Xm(0|>£])
g P([maxsup | X„(0 - Xm(ívJ) | > e/3])
\Xn(tVJ) - Xm(tvJ)\ > e/3])
+ P([max sup | Xm(tyJ) - Xm(i) | > e/3]) < y,
where the supremun is over t with tnJ g t g tnJ+y and the maximum is over j.
Now lim sup„ „.„^([sup,!
X„(r) - Xm(t) | > e]) = 0 for every e > 0 implies the
existence of a subsequence {X„k(t); t e T}, /c S; 1, a process {X(t); teT}
set A with P(A) = 0 such that if cx>
lim (sup|X„k(í)-X(í)|)=0.
We have now established
Since the X„-processes are not necessarily sample continuous, the convergence
in the supremum metric does not imply the a.s. sample continuity of the limit
process. However, condition (ii) could be called "asymptotic sample continuity"
and this is sufficient.
Let {X'k(t), k ^ 1} be the above derived subsequence. By (ii), for every n^lwe
can find a k„ and a ô(n) such that for k^kn
sup \X'k(t)-X'k(s)\>n-1])
Let X*(t) = X'kn(t) for every teT
Let At = [suP|;_5|^(n)|X*(0
- X„*(s)| > ii"1].
If B* =U."=»4r. then A* = limsup^* = f|« = iB« and *W = °-
If co £ B*, then for every n¿im,
D. L. FISK
sup IX^O-X^l^m"1.
|í-s|-sa(n)
Consider the following inequality true for any ö > 0;
sup | Xit) - Xis) | = 2 sup | X*(i) - Xit) |
+ sup \Xlit) - Xtis)\.
Now if eo$ {Au A*}, and £ > 0 is given, we first choose m0(co) such that
2 sup,|X*(r) — X(i)| < fi/2 for every m = m0(co). This can be done since co^A.
Next choose myiea) such that co $Bmi(a>) and m^w)"1
< e/ 2. If m(co) = max{m0(co),
myiea)} and ¿> rg ó(m(eo)), we have
sup | X(0 - X(s) | = £.
Then for co ¿ {A n X*}, P(A n ¿*) = 0,
lim sup | X(0 - X(s)| = 0.
a-»o |í-s|sá
Therefore the X-process is a.s. sample continuous.
Property (iii)' is now easily derived using the continuity of the X-process and
the conditions of the theorem. For we have,
P([| X„(0 - Xit) | > «J) g P ([| X„(i) - X„(i0) I > 6/3])
+ P([|X„(i0)-*(<o)|>8/3])
+ P([|X(i0)-X(0|>8/3])
where t0eT0.
3. Decomposition theorem.
If one is familiar with the decomposition
sub-martingale sequence into the sum of a martingale sequence and a sequence
which is a.s. non-negative and nondecreasing, then it will be seen that this simple
decomposition is a motivating force in what follows.
Let {X(r),F(r); / e T} be a process and let {T„; n ^ 1} be a sequence of partitions
of Tas defined in (2.2). From the given process we construct a sequence of what
can be called "simple"
quasi-martingales
(in that the sample functions of each
are step functions) as follows:
For each partition
Tn, n ^ 1, we define
Xnit) = XitnJ))
. } f„J = i<fnJ+1;
F„it) = FitnJ)
X2nit)= ZC„,,(X), teT.
QUASI-MARTINGALES
If we now let XlB(t) = XB(r) - X2„(t), teT,
then for each
n 21, {Xytt(t),Fn(t);teT}
is a martingale process as is easily verified, and clearly the process
{X2n(t),Fn(t);teT}
has a.e. sample function of b.v. on T.
If the X-process has any continuity properties, then it is the limit of a sequence
of "simple" quasi-martingales. Thus wemightexpect that under certain continuity,
and possibly other conditions, the X-process will itself be a quasi-martingale.
We now prove three lemmas which will give us rather strong sufficient conditions
for the X-process to be a quasi-martingale.
Lemma 3.1.1.
Let {X(t), F(t);teT}
be continuous
in the mean, and let
{X2„(i); teT}
be defined as in (3.1). Assume there is a process {X2(t),F(t);t e T}
such that E(\ X2n(t) - X2(t) |) -* 0, t e T. Then the process
{Xy(t) = X(t)-X2(t),F(t);teT}
is a martingale.
Proof. We need to show jAXy(t)dP = jAXy(s)dP for all A e F(s). Assume s g t,
and let í„>fc- g s < t„k.+1, tnk g t < iB>fc+1,
where of course the fe and k' will change
with n. By the mean continuity of X and the mean convergence of X2n, we have
!Xy(t)dP= Í (X(t)-X2(t))dP
= lim f (Xn(t)-X2n(t))dP
= lim f (x(tntk)- ¿ C„,,(X)-¿ CnJ(X))dP
n-»oo J A \
= lim í íx(tnA) - ¿ Cn;,(X)-[X(í„,,)-X(íB,K.)]W
lim í (X„(S)-X2„(5))dP=f Xy(s)dP.
Our problem now is to obtain conditions to insure the existence of the X2-process
alluded to in Lemma 3.1.1 and to insure this process has a.e. sample function or
b.v. on T.
It is trivial to show that £( Z| CnJ(X) |) is monotone nondecreasingin n. We are
then led naturally to processes satisfying the following condition.
D. L. FISK
(3.2) The process {Xit), Fit); teT}
is such that there exists a sequence of
partitions
of T, as defined in (2.2) such that
lim£(I|CB,/x)|)
If X is a quasi-martingale
with [X]2 = X2 the process of b.v., and if
EiVico)) < co where Vico) is the total variation of X2( • ,co) over T, then condition
(3.2) is satisfied for any sequence of partitions. For we have
£( Z| CnJix) |) = £( Z| C„,,(X2) |) = £( Z| A„,,(X2) |)
z% EiVieo)) < oo.
We also note that if the X2 process is a.s. sample continuous,
F(co) = lim Z|A„>AX2)|a.s.
Lemma 3.1.2. Let the process {Xit), Fit); t e T} satisfy (3.2) and
{X2n(t), Fn(t); te T}, n ^ 1, be as defined
in (3.1). If there exists
{X2(t), F(t); t e T} such that P([lim X2„(i) = X2(f); t e T]) = 1, then the X2
process has a.e. sample function of b.v. on T. Furthermore,
if the X2 process is
a.s. sample continuous, the total variation, V(co), ofX2( ■ , co) over Tis a r.v. with
E(V(co)) = lim£( Z | Cn,/X)|) = Kx.
Let R~(co) = liminf Z | C„/X)|,
then condition
lemma imply R(co) is a.s. finite and integrable. Let 0 = ax < a2 < ■■■ <am+1 = 1
be any partition of T. We write
Z c„,,(x)=Z cn>/x)-Z
Z |A;(X2)| = Z
lim A;(X2„) = Z lim Z CnJiX)
Z CnJ(X) = Z liminf Z |CnJ(X)|
= liminf Z Z |CnJ(X)| =K(co)a.s.
The assertions are now evident.
Lemma 3.1.3.
Let {Xit),Fit); teT}
be a uniformly
bounded, a.s. sample
equicontinuous
process satisfying
condition (3.2). If the sequence of processes
{X2nit),Fj(t);teT},n^l,
are defined as in (3.1), then
(a) for each teT,
the sequence {X2„(i); n = 1} is uniformly
integrable;
(b) the sequence of processes {X2n; n ^ 1} satisfy the hypotheses of Theorem
2.4 with T0=\Jn«LyTn.
QUASI-MARTINGALES
Proof. We show (a) is satisfied by showing £(|X2„(i)|2) = X < co for every
n ^ 1 and t e T. Let Mx =supii0)| X(t,co) | and let Kx be as defined in (3.2). We have
E(\X2n(t)\2) =£(|
Z CnJ(X)\2)
= £ ( I | C„,,(X) |2 + 2 I C„,,(X) [ Z CnJpL) 1 )
= E ( Z | C„j(X) |2) + 2£ ( Z CnJ(X)E [ Z C„,,(X)| £„,,] j
|C„,,(X)||An,,(X)|)
+ 2£ (l\CnJ(X)\\X(tnJit))
- X(r„J+1)|)
where ./(0 denotes the last j such that tnJ g í. We now prove condition (i) of
Theorem 2.4 by showing £(|X2„(t) - X2m(i)|2)-*0
for every teT0. If teT0,
there exists n, such that t e T„ for every n ^ nt. We assume now m> n> nt.
Then we can write
£(rnmiJ.(x)|FB>J)
*2„(0 = Z TnmJ(X),
where TBm /X) is as defined in (2.4). Now using orthogonality (Parseval's Identity),
E(\X2m(t) - X2n(t)\2) = E ([ Z T„m,,(X) - £(T„m,,(X)|F„,,)|2j
= £ Í Z | TnmJ(X) - E(TnmJ(X) | FnJ) |2 j
= £ ( Z | TnmJ(X) |2j - £( Z I £(T„m,,(X) | FnJ |2)
= E ( Z | T„m,/X) |2) = £( Z Z | Cnmj¡k(X) |2)
+ 2£ ( Z ZC„mj,fc(X)£ ( Z CBm;,;(X)
| F„mj.;fcj
g £(max | A„m;,ft(X) | Z Z | Cnmj,k(X) |)
+ 2£(max|X(íBj+i)-X(íBmM)|
ZZ|CBmM(X)[).
D. L. FISK
£„ = ess.sup. max
|X(í,co) — X(s,co)|,
j t„,j£s,t¿t„,j+í
£(|X2m(0-^2„(0|2)
and e„ -> 0 as n -> oo by the a.s. sample equicontinuity of the X-process.
To prove condition (ii) of Theorem 2.4, let £,v > 0 be given. We can choose
n = n(e,y) such that
£„ = ess. sup max
| X(t, co) — X(s, co) | < £
tn, jas.» S'n, (+1
3e„Kx/(e-£„)2<y.
If ô < min | t„J+ y - tnj |, then for every m 2: n, we have by Corollary 2.3.1
\X2mit)-X2mis)\>3E\)
^ P I I max max
< £(| TnmJ(X) |2)/(£ - £„)2 = 3snKJiE - £„)2 < y.
Theorem 3.1.
// {X(r),F(f); te T} is a uniformly
a.s. sample
equicontinuous
process satisfying
condition (3.2), then the process is a quasimartingale.
If[X~\2 = X2, then the X2 process satisfies the following conditions:
(i) // the sequence of processes {X2n; n _ 1} are as defined in (3.1), then P
lim X2„(0 = X2(0, teT.
(ii) The X2 process is a.s. sample continuous;
(iii) if Vico) denotes the variation ofX2( • ,co) over T, then £(F(co)) z%Kx< oo.
Proof. The proof is now a matter of applying the three lemmas.
By Lemma 3.1.3 the sequence of processes {X2n; n ^ 1} as defined in (3.1)satisfies the conditions of Theorem 2.4 and hence there is an a.s. sample continuous
process {X2(í),F(í); te T} such that P lim X2„(0 = X2(i), teT, and there is a
subsequence
of processes {X2„k; k = 1} such that P([limkX2nfc(i) = X2(r); te T])
= 1. Also according to Lemma 3.1.3, for each te T, the sequence {X2n(i); n ^ 1}
is uniformly integrable.
The X-process is obviously continuous in the mean so that from P limX2„(t)
= X2(i), t e T, and the uniform integrability of the sequence {X2„(r) ; n = 1} ; ( e T,
we can conclude by Lemma 3.1.1 that the process
{Xt(i) = X(() — X2(r),
Fit); teT}
is a martingale.
That the X-process satisfies condition (3.2) and P([limX2„t(r) = X2(r); t e T])
= 1 allows us to conclude by Lemma 3.1.2 that the X2-process is a.s. of b.v.
QUASI-MARTINGALES
Thus the X-process is a quasi-martingale, with the stated properties.
With a little more work we will be in a position to prove our main decomposition
Lemma 3.2.1.
Let {Y(t), F(t) ; t e T} be a martingale
process having a.e.
sample function
continuous and of b.v. on T. Then P([Y(t) = Y"(0);reT])=
Since the Y-process has a.e. sample function continuous and of b.v.
on T, V(t, co), the variation of Y( - , co) on [0, r] is sample continuous and monotone nondecreasing on T. Further, F( • ,co) is measurable w.r.t. Fit), teT.
Thus we can apply Theorem
2.2 to both the V and
Y processes. Let
z'v and x'l, v = 1, be the stopping times defined in Theorem 2.2 for the Fand
processes respectively. If tv = min{r¿,T¿'}, then tv is a stopping time for both V
and Y. Let Fv and Yv be the stopped processes. By Theorem 2.1, each Y~v
is again a
martingale process. It is clear that Fv(r, co) is the variation of Yv( • ,co) over [0, t\.
Let {T„; n _ 1} be a sequence of partitions
as defined in (2.1) and let
7,0=Un00=lT»-A1Solet
£„v = ess. sup. max|A„ j-( yv) |.
Then because of the equicontinuity
of the Tv-process, lim„ £ „v = 0 for each v ¡S 1.
If t e T0, then there exists nt such that t e T„ for n 2: nt. Assume now n 2: nt.
Since each Yv-process is a uniformly bounded martingale, it has orthogonal
increments.
Thus we have
£(| yv(0 - tv(0)|2)
AnJ(yv) |2 )
= f(Z |A„,y(yv)|2)=£(£„v Z \KJ.Y,)\)
z% £„„£(Fv(l, co)) z% £nvv -» 0 as n -» co.
Thus, P([Tv(0 = yv(0); t e T0]) = 1. But T0 is dense in T, and since each Yv is a.s.
sample continuous
we have P([ Tv(0 = Tv(0) ; t e T]) = 1 for every v = 1. By
Theorem 2.2, for a.e. co, when v is sufficiently large Yv(t, co) = Y(t, co) for every
t e T. Hence P([Y(t) = T(0); t e T]) = 1.
Theorem 3.2.
If {X(t),F(t);teT}
is a quasi-martingale
with the following
decompositions
P([X = X*y + X*]) = P([X = X, + X2]) = 1
where X¡ and X* (i = 1,2) are a.s. sample continuous processes, then
P([Xy(t) = X*y(t) + (X.(0) - Xî(0)); t e T]) = 1.
Proof. Let T, = X» - Xf, T2=X2*- X2. Then
D. L. FISK
P&Yy = Y2]) = 1.
The conclusion now follows from Lemma 3.2.1.
Lemma 3.3.1. Let {X(t),F(t);teT}
be a.s. sample continuous.
(i) linw rP([suPi|X(01 ^ r]) = 0 and
(ii) the X-process satisfies condition
If the sequence of processes {Xv(í), F(t); te T} are as defined in Theorem 2.2,
then each Xv process also satisfies condition (3.2) and the bound K is independent
Proof. We wish to prove the existence of a K > 0 such that
lim£(Z|CBjJ(Xv)|)giC
for every v ^ 1. Let A(v; n,j) = [xfco) ^ t„J],
Q(v; n,j) = [t„j ^ tv(co) < t„J+i]
(\CnJ(Xv)\-\CnJ(X)\)dP
|CB,,(Xv)-CB>J.(X)|c/P
|Xv(<„J+i)-X(tBj+1)|c/P
J A(v;n,j)
|X(ÍBJ+1)|6ÍP + VP([TV(C0)<1]),
\CnJ(X)\dP= Z Z f
|CBj(X)|cfP
J ~A(v;n,j)
j k<j J Q(.v;n,k)
= Z f (Z \cnJ(X)\)dP
k J Q(v;n,k) \j>k
(Z £(-sgnX(iB;,+ i)A„;,(X)|FBi,)W
k jQ(\>;n,k) \j>k
fZ -sgnX(fB,t+1)ABiJ(X))c/P
k J Q(v;n,k) \j>k
(\X(tn¡k+l)\-\X(l)\)dP
k J Q(v;n,k)
\X(tntk+y)\dP-Í
J Q(v;n,k)
J [tv(o>)<l]
Now by (1) and (2)
QUASI-MARTINGALES
£(Z|CnjJ.(Xv)|)-£(Z|Cfl>,(X)|)
i\CnJiXv)\-\CnJiX)\)dP
- Z f \cnJiX)\dP
J ~A{v;n,j)
= vP([tv(co) < 1]) + Í
J[rv(<B)>l]
The theorem is now evident.
Theorem 3.3.
In order that the a.s. sample continuous first order process
{Xit),Fit);
have a decomposition
into the sum of two processes,
P([X = Xy+ X2J) m 1,
(i) {Xy(t), F(t);teT}
is an a.s. sample
continuous
martingale,
(ii) {X2(t),F(t);teT}
has a.e. sample function
continuous,
of b.v. on T,
and having finite expected variation,
(iii) X2(i) = P linw
ZtC„,/X), t e T
it is nessary and sufficient that
(i)' lim rP([sup(|X(r,co)| =■ r]) = 0, and
(ii)' For any sequence of partitions
{T„; n ïï 1} of T with lim N(T„) = 0 and
T„+1 a refinement of T„for every n, lim E\ Z(C„ ;(X)|) z% K0 < co.
(1) Necessity.
If the X-process is a quasi-martingale
with the stated
decomposition,
we have already indicated (ii)' is true. We need to prove (i)'.
rp( sup\X(t)\^r
\ z^rp( sup|X(r) - X(0)| ^ r^V)
+ rP([\X(0)\\>r¡2A)
the second term goes to zero as r -* co. Now
rp([sup|X(0-X(0)|>ir/2
) = rP([sup|X1(i)
- X.(0)| ^ r/4])
+ rp([sup|X2(i)-X2(0)|^r/4]j.
The process {(Xt(i) - X^O)), Fit); te T} is a martingale and hence [1,Theorem
3.2, §11, Chapter VII] we have the first term bounded by
D. L. FISK
\Xy(l)-Xy(0)\dP-+0
J lmpt\X,(t)-Xi(0)\Zr/4i
We also have the second term bounded by
supF(i,w)^r/4|]
g rP([F(l,cü)^r/4])->0
as r -» oo, where F(i, of) denotes the variation of X2( • , of) over the interval
(2) Sufficiency.
Let xfco), v 2; 1 be the stopping times defined in Theorem 2.2
and let {Xv(0 ; F(t) ; t e T} be the corresponding sequence of stopped processes.
(a) By Lemma 3.3.1, each Xv-process satisfies condition (3.2) and hence by
3.1, each Xv is a quasi-martingale.
Let ~ Av = [Xv = Xlv + X2v],
then P(Av) = 0,v^l.
(b) By Theorem 2.2, there is a set A with P(A) = 0 such that if co $ A, then there
exists v(co) such that:
Xv(r,co) = Xv,(t,co) = X(r,co) for all teT
if v, v* ^ v(of).
Since X2v(0) = 0 for every co, by Theorem 3.2 we have:
Xiv(i, co) = Xiv.(r, co) for all / e T if v ^ v* ^ v(co) and co ¿ (\}% tAv) U A = A0.
We can therefore define
Xi(t,co) = lim Xiv(í,co) for all ieTand
We then have
P([X = X1 + X2]) = P(~A0) = 1.
It thus remains to show that the given decomposition has the stated properties.
By definition of Xy and X2, they are a.s. sample continuous since each Xlv
and X2v are a.s. sample continuous according to Theorem 3.1. Also by the definition
of X2, it is a.s. of sample b.v. since each X2v is a.s. of sample b.v.
We will now show that E(V(l,co))< oo and that £(|X2v(f) - X2(i)|)->0
v -» co. In view of the fact that condition (i)' implies, according to Theorem 2.2,
£(| Xv(i) - X(0 [) -► 0 as v -+ oo, it follows that £(| Xiv(r) - Xy(t) |) -> 0 as v -+ co,
and hence, Xy being the limit in the mean of a sequence of martingale processes
is itself a martingale process.
Letting V(t, co) and Vft, co) denote respectively the variation of X2( • , co) and
-^2v( ' >œ) over [0, t], it is clear that Vv(t,co) is V(t,co) stopped at tv(co), and
P([limV¿t,co) = V(t,co)]) = l.
Since tv(co) is a.s. nondecreasing in v, Vv(l,co) = V(xv(co),co) is a.s. monotone
nondecreasing in v. By Lemma 3.1.2 and Lemma 3.3.1 limE(Vv(l,co)) ^ K <co
and hence £(| Vfl,co) — F(l,co)|)->0
as v-»co by the monotone convergence
theorem. Now
QUASI-MARTINGALES
sup |X2(í,co)-X2v(í,co)|
= sup |X2(í,co)-X2(tv(co),co)|
S sup | F(r,co) - F(tv(co),co)|
= F(l,co)-Fv(l,co),
and hence £(sup,|X2(i)
- X2v(/)|) ú £(F(l,co)
- Fv(l,co))->0
We now prove property (iii). Let
„Aev = [sup,| Z.(Cn>J.(X)-C„tJ.(Xv))|2i£],
= [tv(co) ^ fnJ],
= [r„,^Tv(co)<rnJ+1],
n [rv(co) < l]) + P(X
n [tv(co) = l]).
The first term is bounded by P([tv(co) < 1]) and goes to zero as v-* co. The
second term is bounded by
(i/£) Z f |cn>;(x2)-cn>,(x2v)|dP
J A(v;n,j)
f |x2(íBiJ.+1)-x2v(í„,,.+1)|rfP
J Ai.\,n,j)
= (1/£)Z Í
\X2itnJ+y)-X2vitnJ+y)\dP
= (l/£) Z f
2F(l,C0)dP = (l/£) f
2Vil,co)dP.
•'ö(v.n,j')
J[tv(<a)<l]
Hence, since this last term also goes to zero, P(„AJ) -► 0 as v -> oo uniformly in «.
The conclusion is now apparent.
We make the following observation:
If the process {X(r),F(l); 0 _ t < oo}
satisfies the conditions of Theorem 3.3 for every interval [0, b~\, then the process
is a quasi-martingale. This follows trivially from the uniqueness of the decomposition on every finite interval.
It is also clear that the process of bounded variation in the decomposition need
not have finite expected variation. If, however, the conditions of Theorem 3.3 are
met and if
lim£(Z|CBj(X)|)<co,
where we have a sequence of partitions becoming dense in [0, oo) and such that
D. L. FISK
Tn+1 is a refinement of T„, then the process of bounded variation in the decomposition will have finite expected variation.
An immediate corollary to Theorem 3.3 is
3.3.1. If {X(t),F(t);teT}
is an a.s. sample continuous submartingale,
then it has the decomposition stated in Theorem 3.3 if and only if
(i) lixn^xrP([supt\X(t,co)\
^ r]) = 0.
//, in particular,
the X-process has a.e. sample function
non-negative,
is always satisfied.
Proof. We need only show condition (ii) is satisfied. But
£( Z | CnJ(X) |) = £( Z | £(AB;,(X) | F„J |)
= E(2Z\E(X(tnJ+y)\FnJ)-X(tnJ)\)
= £( Z (£(X(fBj,+ y) | FnJ) - X(tnJ))) = £(X(1)) - £(X(0)).
If the X-process has a.e. sample function non-negative, then by a fundamental
sub-martingale inequality [2, p. 524]
rP (Tsup|X(r)| Sir j = rP(\sup X(t,co)^r J
J[suPtX(0&r]
which goes to zero as r -» co.
We saw previously that if Z is the Brownian motion process, then
X(t) = exp[xZ(r)], x > 0
is a quasi-martingale with
X2(t) = f ^-exp[xZ(s)]ds,
Consider now
Z£(ABJ(X)|F„,J.)
= Z £(exp[xZ(iBJ+1)]
- exp[xZ(tnJ)]
= Z exp[xZ(tB>J.)]£(exp[x(Z(iBiJ+y)
- Z(tnJ))] - 1 \ FnJ)
= Z exp|>Z(iBjJ)]
(fn>J.+ 1 - tnJ) ] - l]
= Z exP[xZ(íB>,)][-^(íB>,+
QUASI-MARTINGALES
^ = ^-j-iK,i+i-t„,f).
a.s. lim Z £(A„;J.(X) | FnJ) =J" ^
exp[xZ(S)]rfs
as was to be expected.