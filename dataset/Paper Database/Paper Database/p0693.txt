Published as a conference paper at ICLR 2017
DENSITY ESTIMATION USING REAL NVP
Laurent Dinh∗
Montreal Institute for Learning Algorithms
University of Montreal
Montreal, QC H3T1J4
Jascha Sohl-Dickstein
Google Brain
Samy Bengio
Google Brain
Unsupervised learning of probabilistic models is a central yet challenging problem
in machine learning. Speciﬁcally, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space
of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful, stably invertible, and learnable transformations, resulting
in an unsupervised learning algorithm with exact log-likelihood computation, exact
and efﬁcient sampling, exact and efﬁcient inference of latent variables, and an
interpretable latent space. We demonstrate its ability to model natural images
on four datasets through sampling, log-likelihood evaluation, and latent variable
manipulations.
Introduction
The domain of representation learning has undergone tremendous advances due to improved supervised learning techniques. However, unsupervised learning has the potential to leverage large pools of
unlabeled data, and extend these advances to modalities that are otherwise impractical or impossible.
One principled approach to unsupervised learning is generative probabilistic modeling. Not only do
generative probabilistic models have the ability to create novel content, they also have a wide range
of reconstruction related applications including inpainting , denoising , colorization
 , and super-resolution .
As data of interest are generally high-dimensional and highly structured, the challenge in this domain
is building models that are powerful enough to capture its complexity yet still trainable. We address
this challenge by introducing real-valued non-volume preserving (real NVP) transformations, a
tractable yet expressive approach to modeling high-dimensional data.
This model can perform efﬁcient and exact inference, sampling and log-density estimation of data
points. Moreover, the architecture presented in this paper enables exact and efﬁcient reconstruction
of input images from the hierarchical features extracted by this model.
Related work
Substantial work on probabilistic generative models has focused on training models using maximum
likelihood. One class of maximum likelihood models are those described by probabilistic undirected
graphs, such as Restricted Boltzmann Machines and Deep Boltzmann Machines . These
models are trained by taking advantage of the conditional independence property of their bipartite
structure to allow efﬁcient exact or approximate posterior inference on latent variables. However,
because of the intractability of the associated marginal distribution over latent variables, their
training, evaluation, and sampling procedures necessitate the use of approximations like Mean
Field inference and Markov Chain Monte Carlo, whose convergence time for such complex models
∗Work was done when author was at Google Brain.
 
Published as a conference paper at ICLR 2017
Data space X
Latent space Z
Generation
x = f −1 (z)
Figure 1: Real NVP learns an invertible, stable, mapping between a data distribution ˆpX and a latent
distribution pZ (typically a Gaussian). Here we show a mapping that has been learned on a toy
2-d dataset. The function f (x) maps samples x from the data distribution in the upper left into
approximate samples z from the latent distribution, in the upper right. This corresponds to exact
inference of the latent state given the data. The inverse function, f −1 (z), maps samples z from the
latent distribution in the lower right into approximate samples x from the data distribution in the
lower left. This corresponds to exact generation of samples from the model. The transformation of
grid lines in X and Z space is additionally illustrated for both f (x) and f −1 (z).
remains undetermined, often resulting in generation of highly correlated samples. Furthermore, these
approximations can often hinder their performance .
Directed graphical models are instead deﬁned in terms of an ancestral sampling procedure, which is
appealing both for its conceptual and computational simplicity. They lack, however, the conditional
independence structure of undirected models, making exact and approximate posterior inference
on latent variables cumbersome . Recent advances in stochastic variational inference 
and amortized inference , allowed efﬁcient approximate inference and learning of
deep directed graphical models by maximizing a variational lower bound on the log-likelihood .
In particular, the variational autoencoder algorithm simultaneously learns a generative
network, that maps gaussian latent variables z to samples x, and a matched approximate inference
network that maps samples x to a semantically meaningful latent representation z, by exploiting the
reparametrization trick . Its success in leveraging recent advances in backpropagation in
deep neural networks resulted in its adoption for several applications ranging from speech synthesis
 to language modeling . Still, the approximation in the inference process limits its ability
to learn high dimensional deep representations, motivating recent work in improving approximate
inference .
Such approximations can be avoided altogether by abstaining from using latent variables. Autoregressive models can implement this strategy while typically retaining a great deal of
ﬂexibility. This class of algorithms tractably models the joint distribution by decomposing it into a
product of conditionals using the probability chain rule according to a ﬁxed ordering over dimensions,
simplifying log-likelihood evaluation and sampling. Recent work in this line of research has taken
advantage of recent advances in recurrent networks , in particular long-short term memory ,
and residual networks in order to learn state-of-the-art generative image models and
language models . The ordering of the dimensions, although often arbitrary, can be critical to the
training of the model . The sequential nature of this model limits its computational efﬁciency. For
example, its sampling procedure is sequential and non-parallelizable, which can become cumbersome
in applications like speech and music synthesis, or real-time rendering.. Additionally, there is no
natural latent representation associated with autoregressive models, and they have not yet been shown
to be useful for semi-supervised learning.
Published as a conference paper at ICLR 2017
Generative Adversarial Networks (GANs) on the other hand can train any differentiable generative network by avoiding the maximum likelihood principle altogether. Instead, the generative
network is associated with a discriminator network whose task is to distinguish between samples
and real data. Rather than using an intractable log-likelihood, this discriminator network provides
the training signal in an adversarial fashion. Successfully trained GAN models can
consistently generate sharp and realistically looking samples . However, metrics that measure the
diversity in the generated samples are currently intractable . Additionally, instability in
their training process requires careful hyperparameter tuning to avoid diverging behavior.
Training such a generative network g that maps latent variable z ∼pZ to a sample x ∼pX does not
in theory require a discriminator network as in GANs, or approximate inference as in variational
autoencoders. Indeed, if g is bijective, it can be trained through maximum likelihood using the change
of variable formula:
pX(x) = pZ(z)
This formula has been discussed in several papers including the maximum likelihood formulation of
independent components analysis (ICA) , gaussianization and deep density models
 . As the existence proof of nonlinear ICA solutions suggests, auto-regressive
models can be seen as tractable instance of maximum likelihood nonlinear ICA, where the residual
corresponds to the independent components. However, naive application of the change of variable
formula produces models which are computationally expensive and poorly conditioned, and so large
scale models of this type have not entered general use.
Model deﬁnition
In this paper, we will tackle the problem of learning highly nonlinear models in high-dimensional
continuous spaces through maximum likelihood. In order to optimize the log-likelihood, we introduce
a more ﬂexible class of architectures that enables the computation of log-likelihood on continuous
data using the change of variable formula. Building on our previous work in , we deﬁne a
powerful class of bijective functions which enable exact and tractable density evaluation and exact
and tractable inference. Moreover, the resulting cost function does not to rely on a ﬁxed form
reconstruction cost such as square error , and generates sharper samples as a result. Also,
this ﬂexibility helps us leverage recent advances in batch normalization and residual networks
 to deﬁne a very deep multi-scale architecture with multiple levels of abstraction.
Change of variable formula
Given an observed data variable x ∈X, a simple prior probability distribution pZ on a latent variable
z ∈Z, and a bijection f : X →Z (with g = f −1), the change of variable formula deﬁnes a model
distribution on X by
pX(x) = pZ
log (pX(x)) = log
where ∂f(x)
∂xT is the Jacobian of f at x.
Exact samples from the resulting distribution can be generated by using the inverse transform sampling
rule . A sample z ∼pZ is drawn in the latent space, and its inverse image x = f −1(z) = g(z)
generates a sample in the original space. Computing the density on a point x is accomplished by
computing the density of its image f(x) and multiplying by the associated Jacobian determinant
. See also Figure 1. Exact and efﬁcient inference enables the accurate and fast evaluation
of the model.
Published as a conference paper at ICLR 2017
(a) Forward propagation
(b) Inverse propagation
Figure 2: Computational graphs for forward and inverse propagation. A coupling layer applies a
simple invertible transformation consisting of scaling followed by addition of a constant offset to
one part x2 of the input vector conditioned on the remaining part of the input vector x1. Because of
its simple nature, this transformation is both easily invertible and possesses a tractable determinant.
However, the conditional nature of this transformation, captured by the functions s and t, signiﬁcantly
increase the ﬂexibility of this otherwise weak function. The forward and inverse propagation
operations have identical computational cost.
Coupling layers
Computing the Jacobian of functions with high-dimensional domain and codomain and computing
the determinants of large matrices are in general computationally very expensive. This combined
with the restriction to bijective functions makes Equation 2 appear impractical for modeling arbitrary
distributions.
As shown however in , by careful design of the function f, a bijective model can be learned which
is both tractable and extremely ﬂexible. As computing the Jacobian determinant of the transformation
is crucial to effectively train using this principle, this work exploits the simple observation that the
determinant of a triangular matrix can be efﬁciently computed as the product of its diagonal terms.
We will build a ﬂexible and tractable bijective function by stacking a sequence of simple bijections.
In each simple bijection, part of the input vector is updated using a function which is simple to invert,
but which depends on the remainder of the input vector in a complex way. We refer to each of these
simple bijections as an afﬁne coupling layer. Given a D dimensional input x and d < D, the output
y of an afﬁne coupling layer follows the equations
y1:d = x1:d
yd+1:D = xd+1:D ⊙exp
+ t(x1:d),
where s and t stand for scale and translation, and are functions from Rd 7→RD−d, and ⊙is the
Hadamard product or element-wise product (see Figure 2(a)).
Properties
The Jacobian of this transformation is
 exp [s (x1:d)]
where diag
 exp [s (x1:d)]
is the diagonal matrix whose diagonal elements correspond to the vector
exp [s (x1:d)]. Given the observation that this Jacobian is triangular, we can efﬁciently compute
its determinant as exp
j s (x1:d)j
. Since computing the Jacobian determinant of the coupling
layer operation does not involve computing the Jacobian of s or t, those functions can be arbitrarily
complex. We will make them deep convolutional neural networks. Note that the hidden layers of s
and t can have more features than their input and output layers.
Another interesting property of these coupling layers in the context of deﬁning probabilistic models
is their invertibility. Indeed, computing the inverse is no more complex than the forward propagation
Published as a conference paper at ICLR 2017
Figure 3: Masking schemes for afﬁne coupling layers. On the left, a spatial checkerboard pattern
mask. On the right, a channel-wise masking. The squeezing operation reduces the 4 × 4 × 1 tensor
(on the left) into a 2 × 2 × 4 tensor (on the right). Before the squeezing operation, a checkerboard
pattern is used for coupling layers while a channel-wise masking pattern is used afterward.
(see Figure 2(b)),
= xd+1:D ⊙exp
 yd+1:D −t(y1:d)
meaning that sampling is as efﬁcient as inference for this model. Note again that computing the
inverse of the coupling layer does not require computing the inverse of s or t, so these functions can
be arbitrarily complex and difﬁcult to invert.
Masked convolution
Partitioning can be implemented using a binary mask b, and using the functional form for y,
y = b ⊙x + (1 −b) ⊙
We use two partitionings that exploit the local correlation structure of images: spatial checkerboard
patterns, and channel-wise masking (see Figure 3). The spatial checkerboard pattern mask has value
1 where the sum of spatial coordinates is odd, and 0 otherwise. The channel-wise mask b is 1 for the
ﬁrst half of the channel dimensions and 0 for the second half. For the models presented here, both
s(·) and t(·) are rectiﬁed convolutional networks.
Combining coupling layers
Although coupling layers can be powerful, their forward transformation leaves some components
unchanged. This difﬁculty can be overcome by composing coupling layers in an alternating pattern,
such that the components that are left unchanged in one coupling layer are updated in the next (see
Figure 4(a)).
The Jacobian determinant of the resulting function remains tractable, relying on the fact that
(xa) = ∂fa
(xa) · ∂fb
 xb = fa(xa)
det(A · B) = det(A) det(B).
Similarly, its inverse can be computed easily as
(fb ◦fa)−1 = f −1
Published as a conference paper at ICLR 2017
(a) In this alternating pattern, units which remain identical in one
transformation are modiﬁed in the next.
(b) Factoring out variables.
At each step, half the variables are directly modeled as
Gaussians, while the other
half undergo further transformation.
Figure 4: Composition schemes for afﬁne coupling layers.
Multi-scale architecture
We implement a multi-scale architecture using a squeezing operation: for each channel, it divides the
image into subsquares of shape 2 × 2 × c, then reshapes them into subsquares of shape 1 × 1 × 4c.
The squeezing operation transforms an s × s × c tensor into an s
2 × 4c tensor (see Figure 3),
effectively trading spatial size for number of channels.
At each scale, we combine several operations into a sequence: we ﬁrst apply three coupling layers
with alternating checkerboard masks, then perform a squeezing operation, and ﬁnally apply three
more coupling layers with alternating channel-wise masking. The channel-wise masking is chosen so
that the resulting partitioning is not redundant with the previous checkerboard masking (see Figure
3). For the ﬁnal scale, we only apply four coupling layers with alternating checkerboard masks.
Propagating a D dimensional vector through all the coupling layers would be cumbersome, in terms
of computational and memory cost, and in terms of the number of parameters that would need to be
trained. For this reason we follow the design choice of and factor out half of the dimensions at
regular intervals (see Equation 14). We can deﬁne this operation recursively (see Figure 4(b)),
(z(i+1), h(i+1)) = f (i+1)(h(i))
z(L) = f (L)(h(L−1))
z = (z(1), . . . , z(L)).
In our experiments, we use this operation for i < L. The sequence of coupling-squeezing-coupling
operations described above is performed per layer when computing f (i) (Equation 14). At each
layer, as the spatial resolution is reduced, the number of hidden layer features in s and t is doubled.
All variables which have been factored out at different scales are concatenated to obtain the ﬁnal
transformed output (Equation 16).
As a consequence, the model must Gaussianize units which are factored out at a ﬁner scale (in an
earlier layer) before those which are factored out at a coarser scale (in a later layer). This results in the
deﬁnition of intermediary levels of representation corresponding to more local, ﬁne-grained
features as shown in Appendix D.
Moreover, Gaussianizing and factoring out units in earlier layers has the practical beneﬁt of distributing the loss function throughout the network, following the philosophy similar to guiding intermediate
layers using intermediate classiﬁers . It also reduces signiﬁcantly the amount of computation and
memory used by the model, allowing us to train larger models.
Published as a conference paper at ICLR 2017
Batch normalization
To further improve the propagation of training signal, we use deep residual networks with
batch normalization and weight normalization in s and t. As described in Appendix E
we introduce and use a novel variant of batch normalization which is based on a running average over
recent minibatches, and is thus more robust when training with very small minibatches.
We also use apply batch normalization to the whole coupling layer output. The effects of batch
normalization are easily included in the Jacobian computation, since it acts as a linear rescaling on
each dimension. That is, given the estimated batch statistics ˜µ and ˜σ2, the rescaling function
has a Jacobian determinant
This form of batch normalization can be seen as similar to reward normalization in deep reinforcement
learning .
We found that the use of this technique not only allowed training with a deeper stack of coupling
layers, but also alleviated the instability problem that practitioners often encounter when training
conditional distributions with a scale parameter through a gradient-based approach.
Experiments
The algorithm described in Equation 2 shows how to learn distributions on unbounded space. In
general, the data of interest have bounded magnitude. For examples, the pixel values of an image
typically lie in D after application of the recommended jittering procedure . In order to
reduce the impact of boundary effects, we instead model the density of logit(α+(1−α)⊙x
256), where
α is picked here as .05. We take into account this transformation when computing log-likelihood and
bits per dimension. We also augment the CIFAR-10, CelebA and LSUN datasets during training to
also include horizontal ﬂips of the training examples.
We train our model on four natural image datasets: CIFAR-10 , Imagenet , Large-scale Scene
Understanding (LSUN) , CelebFaces Attributes (CelebA) . More speciﬁcally, we train on the
downsampled to 32 × 32 and 64 × 64 versions of Imagenet . For the LSUN dataset, we train on
the bedroom, tower and church outdoor categories. The procedure for LSUN is the same as in :
we downsample the image so that the smallest side is 96 pixels and take random crops of 64×64. For
CelebA, we use the same procedure as in : we take an approximately central crop of 148 × 148
then resize it to 64 × 64.
We use the multi-scale architecture described in Section 3.6 and use deep convolutional residual
networks in the coupling layers with rectiﬁer nonlinearity and skip-connections as suggested by .
To compute the scaling functions s, we use a hyperbolic tangent function multiplied by a learned
scale, whereas the translation function t has an afﬁne output. Our multi-scale architecture is repeated
recursively until the input of the last recursion is a 4 × 4 × c tensor. For datasets of images of size
32 × 32, we use 4 residual blocks with 32 hidden feature maps for the ﬁrst coupling layers with
checkerboard masking. Only 2 residual blocks are used for images of size 64 × 64. We use a batch
size of 64. For CIFAR-10, we use 8 residual blocks, 64 feature maps, and downscale only once. We
optimize with ADAM with default hyperparameters and use an L2 regularization on the weight
scale parameters with coefﬁcient 5 · 10−5.
We set the prior pZ to be an isotropic unit norm Gaussian. However, any distribution could be used
for pZ, including distributions that are also learned during training, such as from an auto-regressive
model, or (with slight modiﬁcations to the training objective) a variational autoencoder.
Published as a conference paper at ICLR 2017
PixelRNN 
Conv DRAW 
IAF-VAE 
Imagenet (32 × 32)
3.86 (3.83)
4.28 (4.26)
< 4.40 (4.35)
Imagenet (64 × 64)
3.63 (3.57)
3.98 (3.75)
< 4.10 (4.04)
LSUN (bedroom)
2.72 (2.70)
LSUN (tower)
2.81 (2.78)
LSUN (church outdoor)
3.08 (2.94)
3.02 (2.97)
Table 1: Bits/dim results for CIFAR-10, Imagenet, LSUN datasets and CelebA. Test results for
CIFAR-10 and validation results for Imagenet, LSUN and CelebA (with training results in parenthesis
for reference).
Figure 5: On the left column, examples from the dataset. On the right column, samples from the
model trained on the dataset. The datasets shown in this ﬁgure are in order: CIFAR-10, Imagenet
(32 × 32), Imagenet (64 × 64), CelebA, LSUN (bedroom).
We show in Table 1 that the number of bits per dimension, while not improving over the Pixel RNN
 baseline, is competitive with other generative methods. As we notice that our performance
increases with the number of parameters, larger models are likely to further improve performance.
For CelebA and LSUN, the bits per dimension for the validation set was decreasing throughout
training, so little overﬁtting is expected.
We show in Figure 5 samples generated from the model with training examples from the dataset
for comparison. As mentioned in , maximum likelihood is a principle that values diversity
Published as a conference paper at ICLR 2017
Figure 6: Manifold generated from four examples in the dataset. Clockwise from top left: CelebA,
Imagenet (64 × 64), LSUN (tower), LSUN (bedroom).
over sample quality in a limited capacity setting. As a result, our model outputs sometimes highly
improbable samples as we can notice especially on CelebA. As opposed to variational autoencoders,
the samples generated from our model look not only globally coherent but also sharp. Our hypothesis
is that as opposed to these models, real NVP does not rely on ﬁxed form reconstruction cost like an L2
norm which tends to reward capturing low frequency components more heavily than high frequency
components. Unlike autoregressive models, sampling from our model is done very efﬁciently as it is
parallelized over input dimensions. On Imagenet and LSUN, our model seems to have captured well
the notion of background/foreground and lighting interactions such as luminosity and consistent light
source direction for reﬂectance and shadows.
We also illustrate the smooth semantically consistent meaning of our latent variables. In the latent
space, we deﬁne a manifold based on four validation examples z(1), z(2), z(3), z(4), and parametrized
by two parameters φ and φ′ by,
z = cos(φ)
 cos(φ′)z(1) + sin(φ′)z(2)
 cos(φ′)z(3) + sin(φ′)z(4)
We project the resulting manifold back into the data space by computing g(z). Results are shown
Figure 6. We observe that the model seems to have organized the latent space with a notion of
meaning that goes well beyond pixel space interpolation. More visualization are shown in the
Appendix. To further test whether the latent space has a consistent semantic interpretation, we trained
a class-conditional model on CelebA, and found that the learned representation had a consistent
semantic meaning across class labels (see Appendix F).
Discussion and conclusion
In this paper, we have deﬁned a class of invertible functions with tractable Jacobian determinant,
enabling exact and tractable log-likelihood evaluation, inference, and sampling. We have shown that
this class of generative model achieves competitive performances, both in terms of sample quality
and log-likelihood. Many avenues exist to further improve the functional form of the transformations,
for instance by exploiting the latest advances in dilated convolutions and residual networks
architectures .
This paper presented a technique bridging the gap between auto-regressive models, variational
autoencoders, and generative adversarial networks. Like auto-regressive models, it allows tractable
and exact log-likelihood evaluation for training. It allows however a much more ﬂexible functional
form, similar to that in the generative model of variational autoencoders. This allows for fast
and exact sampling from the model distribution. Like GANs, and unlike variational autoencoders,
our technique does not require the use of a ﬁxed form reconstruction cost, and instead deﬁnes a
cost in terms of higher level features, generating sharper images. Finally, unlike both variational
Published as a conference paper at ICLR 2017
autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which
is as high dimensional as the input space. This may make the algorithm particularly well suited to
semi-supervised learning tasks, as we hope to explore in future work.
Real NVP generative models can additionally be conditioned on additional variables (for instance
class labels) to create a structured output algorithm. More so, as the resulting class of invertible
transformations can be treated as a probability distribution in a modular way, it can also be used to
improve upon other probabilistic models like auto-regressive models and variational autoencoders.
For variational autoencoders, these transformations could be used both to enable a more ﬂexible
reconstruction cost and a more ﬂexible stochastic inference distribution . Probabilistic
models in general can also beneﬁt from batch normalization techniques as applied in this paper.
The deﬁnition of powerful and trainable invertible functions can also beneﬁt domains other than
generative unsupervised learning. For example, in reinforcement learning, these invertible functions
can help extend the set of functions for which an argmax operation is tractable for continuous Qlearning or ﬁnd representation where local linear Gaussian approximations are more appropriate
Acknowledgments
The authors thank the developers of Tensorﬂow . We thank Sherry Moore, David Andersen and
Jon Shlens for their help in implementing the model. We thank Aäron van den Oord, Yann Dauphin,
Kyle Kastner, Chelsea Finn, Maithra Raghu, David Warde-Farley, Daniel Jiwoong Im and Oriol
Vinyals for fruitful discussions. Finally, we thank Ben Poole, Rafal Jozefowicz and George Dahl for
their input on a draft of the paper.