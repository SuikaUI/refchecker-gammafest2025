An Introduction to Boosting and Leveraging
Ron Meir1 and Gunnar R¨atsch2
1 Department of Electrical Engineering, Technion, Haifa 32000, Israel
 ,
 
2 Research School of Information Sciences & Engineering
The Australian National University, Canberra, ACT 0200, Australia
 
 
Abstract. We provide an introduction to theoretical and practical aspects of Boosting and Ensemble learning, providing a useful reference for
researchers in the ﬁeld of Boosting as well as for those seeking to enter
this fascinating area of research. We begin with a short background concerning the necessary learning theoretical foundations of weak learners
and their linear combinations. We then point out the useful connection
between Boosting and the Theory of Optimization, which facilitates the
understanding of Boosting and later on enables us to move on to new
Boosting algorithms, applicable to a broad spectrum of problems. In order to increase the relevance of the paper to practitioners, we have added
remarks, pseudo code, “tricks of the trade”, and algorithmic considerations where appropriate. Finally, we illustrate the usefulness of Boosting
algorithms by giving an overview of some existing applications. The main
ideas are illustrated on the problem of binary classiﬁcation, although several extensions are discussed.
A Brief History of Boosting
The underlying idea of Boosting is to combine simple “rules” to form an ensemble such that the performance of the single ensemble member is improved,
i.e. “boosted”. Let h1, h2, . . . , hT be a set of hypotheses, and consider the composite ensemble hypothesis
Here αt denotes the coeﬃcient with which the ensemble member ht is combined;
both αt and the learner or hypothesis ht are to be learned within the Boosting
procedure.
The idea of Boosting has its roots in PAC learning (cf. ). Kearns and
Valiant proved the astonishing fact that learners, each performing only
slightly better than random, can be combined to form an arbitrarily good ensemble hypothesis (when enough data is available). Schapire was the ﬁrst
S. Mendelson, A.J. Smola (Eds.): Advanced Lectures on Machine Learning, LNAI 2600, pp. 118–183, 2003.
⃝Springer-Verlag Berlin Heidelberg 2003
An Introduction to Boosting and Leveraging
to provide a provably polynomial time Boosting algorithm, while were the
ﬁrst to apply the Boosting idea to a real-world OCR task, relying on neural
networks as base learners.
The AdaBoost (Adaptive Boosting) algorithm by (cf. Algorithm 2.1) is generally considered as a ﬁrst step towards more practical Boosting
algorithms. Very similar to AdaBoost is the Arcing algorithm, for which convergence to a linear programming solution can be shown (cf. ). Although
the Boosting scheme seemed intuitive in the context of algorithmic design, a
step forward in transparency was taken by explaining Boosting in terms of
a stage-wise gradient descent procedure in an exponential cost function (cf.
 ). A further interesting step towards practical applicability is worth mentioning: large parts of the early Boosting literature persistently
contained the misconception that Boosting would not overﬁt even when running for a large number of iterations. Simulations by on data sets with
higher noise content could clearly show overﬁtting eﬀects, which can only be
avoided by regularizing Boosting so as to limit the complexity of the function
class (cf. Section 6).
When trying to develop means for achieving robust Boosting it is important
to elucidate the relations between Optimization Theory and Boosting procedures
(e.g. ). Developing this interesting relationship opened the
ﬁeld to new types of Boosting algorithms. Among other options it now became
possible to rigorously deﬁne Boosting algorithms for regression (cf. ),
multi-class problems , unsupervised learning (cf. ) and to establish convergence proofs for Boosting algorithms by using results from the Theory of Optimization. Further extensions to Boosting algorithms can be found in
 .
Recently, Boosting strategies have been quite successfully used in various
real-world applications. For instance and earlier and used boosted
ensembles of neural networks for OCR. proposed a non-intrusive monitoring
system for assessing whether certain household appliances consume power or not.
In Boosting was used for tumor classiﬁcation with gene expression data. For
further applications and more details we refer to the Applications Section 8.3
and to 
Although we attempt a full and balanced treatment of most available literature, naturally the presentation leans in parts towards the authors’ own work.
In fact, the Boosting literature, as will become clear from the bibliography, is so
extensive, that a full treatment would require a book length treatise. The present
review diﬀers from other reviews, such as the ones of , mainly in the
choice of the presented material: we place more emphasis on robust algorithms
for Boosting, on connections to other margin based approaches (such as support
vector machines), and on connections to the Theory of Optimization. Finally,
we discuss applications and extensions of Boosting.
The content of this review is organized as follows. After presenting some basic ideas on Boosting and ensemble methods in the next section, a brief overview
of the underlying theory is given in Section 3. The notion of the margin and its
R. Meir and G. R¨atsch
connection to successful Boosting is analyzed in Section 4, and the important
link between Boosting and Optimization Theory is discussed in Section 5. Subsequently, in Section 6, we present several approaches to making Boosting more
robust, and proceed in Section 7 by presenting several extensions of the basic
Boosting framework. Section 8.3 then presents a short summary of applications,
and Section 9 summarizes the review and presents some open problems.
An Introduction to Boosting and Ensemble Methods
This section includes a brief deﬁnition of the general learning setting addressed
in this paper, followed by a discussion of diﬀerent aspects of ensemble learning.
Learning from Data and the PAC Property
We focus in this review (except in Section 7) on the problem of binary classiﬁcation . The task of binary classiﬁcation is to ﬁnd a rule (hypothesis),
which, based on a set of observations, assigns an object to one of two classes.
We represent objects as belonging to an input space X, and denote the possible
outputs of a hypothesis by Y. One possible formalization of this task is to estimate a function f : X →Y, using input-output training data pairs generated
independently at random from an unknown probability distribution P(x, y),
(x1, y1), . . . , (xn, yn) ∈Rd × {−1, +1}
such that f will correctly predict unseen examples (x, y). In the case where
Y = {−1, +1} we have a so-called hard classiﬁer and the label assigned to an
input x is given by f(x). Often one takes Y = R, termed a soft classiﬁer, in
which case the label assignment is performed according to sign(f(x)). The true
performance of a classiﬁer f is assessed by
λ(f(x), y)dP(x, y),
where λ denotes a suitably chosen loss function. The risk L(f) is often termed the
generalization error as it measures the expected loss with respect to examples
which were not observed in the training set. For binary classiﬁcation one often
uses the so-called 0/1-loss λ(f(x), y) = I(yf(x) ≤0), where I(E) = 1 if the
event E occurs and zero otherwise. Other loss functions are often introduced
depending on the speciﬁc context.
Unfortunately the risk cannot be minimized directly, since the underlying
probability distribution P(x, y) is unknown. Therefore, we have to try to estimate a function that is close to the optimal one based on the available information, i.e. the training sample and properties of the function class F from which
the solution f is chosen. To this end, we need what is called an induction principle. A particularly simple one consists of approximating the minimum of the
risk (2) by the minimum of the empirical risk
An Introduction to Boosting and Leveraging
λ(f(xn), yn).
From the law of large numbers (e.g. ) one expects that ˆL(f) →L(f) as N →
∞. However, in order to guarantee that the function obtained by minimizing ˆL(f)
also attains asymptotically the minimum of L(f) a stronger condition is required.
Intuitively, the complexity of the class of functions F needs to be controlled,
since otherwise a function f with arbitrarily low error on the sample may be
found, which, however, leads to very poor generalization. A suﬃcient condition
for preventing this phenomenon is the requirement that ˆL(f) converge uniformly
(over F) to L(f); see for further details.
While it is possible to provide conditions for the learning machine which
ensure that asymptotically (as N →∞) the empirical risk minimizer will perform
optimally, for small sample sizes large deviations are possible and overﬁtting
might occur. Then a small generalization error cannot be obtained by simply
minimizing the training error (3). As mentioned above, one way to avoid the
overﬁtting dilemma – called regularization – is to restrict the size of the function
class F that one chooses the function f from . The intuition, which will
be formalized in Section 3, is that a “simple” function that explains most of the
data is preferable to a complex one which ﬁts the data very well (Occam’s razor,
e.g. ).
For Boosting algorithms which generate a complex composite hypothesis as
in (1), one may be tempted to think that the complexity of the resulting function
class would increase dramatically when using an ensemble of many learners. However this is provably not the case under certain conditions (see e.g. Theorem 3),
as the ensemble complexity saturates (cf. Section 3). This insight may lead us to
think that since the complexity of Boosting is limited we might not encounter the
eﬀect of overﬁtting at all. However when using Boosting procedures on noisy realworld data, it turns out that regularization (e.g. ) is mandatory
if overﬁtting is to be avoided (cf. Section 6). This is in line with the general experience in statistical learning procedures when using complex non-linear models,
for instance for neural networks, where a regularization term is used to appropriately limit the complexity of the models (e.g. ).
Before proceeding to discuss ensemble methods we brieﬂy review the strong
and weak PAC models for learning binary concepts . Let S be a sample
consisting of N data points {(xn, yn)}N
n=1, where xn are generated independently
at random from some distribution P(x) and yn = f(xn), and f belongs to some
known class F of binary functions. A strong PAC learning algorithm has the
property that for every distribution P, every f ∈F and every 0 ≤ϵ, δ ≤1/2
with probability larger than 1−δ, the algorithm outputs a hypothesis h such that
Pr[h(x) ̸= f(x)] ≤ϵ. The running time of the algorithm should be polynomial
in 1/ε, 1/δ, n, d, where d is the dimension (appropriately deﬁned) of the input
space. A weak PAC learning algorithm is deﬁned analogously, except that it is
only required to satisfy the conditions for particular ε and δ, rather than all
pairs. Various extensions and generalization of the basic PAC concept can be
found in .
R. Meir and G. R¨atsch
Ensemble Learning, Boosting, and Leveraging
Consider a combination of hypotheses in the form of (1). Clearly there are many
approaches for selecting both the coeﬃcients αt and the base hypotheses ht. In
the so-called Bagging approach , the hypotheses {ht}T
t=1 are chosen based
on a set of T bootstrap samples, and the coeﬃcients αt are set to αt = 1/T (see
e.g. for more reﬁned choices). Although this algorithm seems somewhat
simplistic, it has the nice property that it tends to reduce the variance of the
overall estimate f(x) as discussed for regression and classiﬁcation . Thus, Bagging is quite often found to improve the performance of complex
(unstable) classiﬁers, such as neural networks or decision trees .
For Boosting the combination of the hypotheses is chosen in a more sophisticated manner. The evolution of the AdaBoost algorithm can be understood
from Figure 1. The intuitive idea is that examples that are misclassiﬁed get
higher weights in the next iteration, for instance the examples near the decision
boundary are usually harder to classify and therefore get high weights after a few
iterations. This idea of iterative reweighting of the training sample is essential
to Boosting.
More formally, a non-negative weighting d(t) = (d(t)
1 , . . . , d(t)
N ) is assigned
to the data at step t, and a weak learner ht is constructed based on d(t). This
weighting is updated at each iteration t according to the weighted error incurred
by the weak learner in the last iteration (see Algorithm 2.1). At each step t, the
weak learner is required to produce a small weighted empirical error deﬁned by
1st Iteration
2nd Iteration
3rd Iteration
5th Iteration
10th Iteration
100th Iteration
Fig. 1. Illustration of AdaBoost on a 2D toy data set: The color indicates the label
and the diameter is proportional to the weight of the examples in the ﬁrst, second,
third, 5th, 10th and 100th iteration. The dashed lines show the decision boundaries of
the single classiﬁers (up to the 5th iteration). The solid line shows the decision line of
the combined classiﬁer. In the last two plots the decision line of Bagging is plotted for
a comparison. (Figure taken from .)
An Introduction to Boosting and Leveraging
Algorithm 2.1 The AdaBoost algorithm .
1. Input: S = {(x1, y1), . . . , (xN, yN)}, Number of Iterations T
2. Initialize: d(1)
= 1/N for all n = 1, . . . , N
3. Do for t = 1, . . . , T,
a) Train classiﬁer with respect to the weighted sample set {S, d(t)} and
obtain hypothesis ht : x →{−1, +1}, i.e. ht = L(S, d(t))
b) Calculate the weighted training error εt of ht:
n I(yn ̸= ht(xn)) ,
2 log 1 −εt
d) Update weights:
n exp {−αtynht(xn)} /Zt ,
where Zt is a normalization constant, such that N
n=1 d(t+1)
4. Break if εt = 0 or εt ≥1
2 and set T = t −1.
5. Output: fT (x) =
εt(ht, d(t)) =
n I(yn ̸= ht(xn)).
After selecting the hypothesis ht, its weight αt is computed such that it minimizes
a certain loss function (cf. step (3c)). In AdaBoost one minimizes
exp {−yn (αht(xn) + ft−1(xn))} ,
where ft−1 is the combined hypothesis of the previous iteration given by
ft−1(xn) =
Another very eﬀective approach, proposed in , is the LogitBoost algorithm,
where the cost function analogous to (5) is given by
log {1 + exp (−yn(αht(xn) + ft−1(xn)))} .
R. Meir and G. R¨atsch
An important feature of (7) as compared to (5) is that the former increases only
linearly for negative values of yn(αht(xn) + ft−1(xn)), while the latter increases
exponentially. It turns out that this diﬀerence is important in noisy situations,
where AdaBoost tends to focus too much on outliers and noisy data. In Section 6
we will discuss other techniques to approach this problem. Some further details
concerning LogitBoost will be given in Section 5.2.
For AdaBoost it has been shown that αt in (5) can be computed analytically leading to the expression in step (3c) of Algorithm 2.1. Based on the new
combined hypothesis, the weighting d of the sample is updated as in step (3d)
of Algorithm 2.1. The initial weighting d(1) is chosen uniformly: d(1)
The so-called Arcing algorithms proposed in are similar to AdaBoost,
but use a diﬀerent loss function.1 Each loss function leads to diﬀerent weighting
schemes of the examples and hypothesis coeﬃcients. One well-known example
is Arc-X4 where one uses a fourth order polynomial to compute the weight of
the examples. Arcing is the predecessor of the more general leveraging algorithms discussed in Section 5.2. Moreover, for one variant, called Arc-GV, it has
been shown to ﬁnd the linear combination that solves a linear optimization
problem (LP) (cf. Section 4).
The AdaBoost algorithm presented above is based on using binary (hard)
weak learners. In and much subsequent work, real-valued (soft) weak learners were used. One may also ﬁnd the hypothesis weight αt and the hypothesis
ht in parallel, such that (5) is minimized. Many other variants of Boosting algorithms have emerged over the past few years, many of which operate very
similarly to AdaBoost, even though they often do not possess the PAC-boosting
property. The PAC-boosting property refers to schemes which are able to guarantee that weak learning algorithms are indeed transformed into strong learning
algorithms in the sense described in Section 2.1. Duﬀy and Helmbold reserve
the term ‘boosting’ for algorithms for which the PAC-boosting property can be
proved to hold, while using ‘leveraging’ in all other cases. Since we are not overly
concerned with PAC learning per se in this review, we use the terms ‘boosting’
and ‘leveraging’ interchangeably.
In principle, any leveraging approach iteratively selects one hypothesis ht ∈
H at a time and then updates their weights; this can be implemented in diﬀerent
ways. Ideally, given {h1, . . . , ht}, one solves the optimization problem for all
hypothesis coeﬃcients {α1, . . . , αt}, as proposed by . In contrast to
this, the greedy approach is used by the original AdaBoost/Arc-GV algorithm
as discussed above: only the weight of the last hypothesis is selected, while
minimizing some appropriate cost function . Later we will show
in Sections 4 and 5 that this relates to barrier optimization techniques 
and coordinate descent methods . Additionally, we will brieﬂy discuss
relations to information geometry and column
generation techniques (cf. Section 6.2).
An important issue in the context of the algorithms discussed in this review
pertains to the construction of the weak learner (e.g. step (3a) in Algorithm 2.1).
1 Any convex loss function is allowed that goes to inﬁnity when the margin goes to
minus inﬁnity and to zero when the margin goes to inﬁnity.
An Introduction to Boosting and Leveraging
At step t, the weak learner is constructed based on the weighting d(t). There are
basically two approaches to taking the weighting into consideration. In the ﬁrst
approach, we assume that the learning algorithm can operate with reweighted
examples. For example, if the weak learner is based on minimizing a cost function (see Section 5), one can construct a revised cost function which assigns a
weight to each of the examples, so that heavily weighted examples are more in-
ﬂuential. For example, for the least squares error one may attempt to minimize
n (yn −h(xn))2. However, not all weak learners are easily adaptable to
the inclusion of weights. An approach proposed in is based on resampling
the data with replacement based on the distribution d(t). The latter approach
is more general as it is applicable to any weak learner; however, the former approach has been more widely used in practice. Friedman has also considered
sampling based approaches within the general framework described in Section 5.
He found that in certain situations (small samples and powerful weak learners) it
is advantageous to sample a subset of the data rather than the full data set itself,
where diﬀerent subsets are sampled at each iteration of the algorithm. Overall,
however, it is not yet clear whether there is a distinct advantage to using either
one of the two approaches.
Learning Theoretical Foundations of Boosting
The Existence of Weak Learners
We have seen that AdaBoost operates by forming a re-weighting d of the data
at each step, and constructing a base learner based on d. In order to gauge the
performance of a base learner, we ﬁrst deﬁne a baseline learner.
Deﬁnition 1. Let d = (d1, . . . , dN) be a probability weighting of the data points
S. Let S+ be the subset of the positively labeled points, and similarly for S−. Set
n:yn=+1 dn and similarly for D−. The baseline classiﬁer fBL is deﬁned
as fBL(x) = sign(D+ −D−) for all x. In other words, the baseline classiﬁer
predicts +1 if D+ ≥D−and −1 otherwise. It is immediately obvious that for
any weighting d, the error of the baseline classiﬁer is at most 1/2.
The notion of weak learner has been introduced in the context of PAC learning at the end of Section 2.1. However, this deﬁnition is too limited for most
applications. In the context of this review, we deﬁne a weak learner as follows.
A learner is a weak learner for sample S if, given any weighting d on S, it is able
to achieve a weighted classiﬁcation error (see (4)) which is strictly smaller than
A key ingredient of Boosting algorithms is a weak learner which is required
to exist in order for the overall algorithm to perform well. In the context of
binary classiﬁcation, we demand that the weighted empirical error of each weak
learner is strictly smaller than 1
2γ, where γ is an edge parameter quantifying
the deviation of the performance of the weak learner from the baseline classiﬁer
introduced in Deﬁnition 1. Consider a weak learner that outputs a binary classiﬁer h based on a data set, S = {(xn, yn)}N
n=1 each pair (xn, yn) of which is
weighted by a non-negative weight dn. We then demand that
R. Meir and G. R¨atsch
dnI[yn ̸= h(xn)] ≤1
For simple weak learners, it may not be possible to ﬁnd a strictly positive
value of γ for which (8) holds without making any assumptions about the
data. For example, consider the two-dimensional xor problem with N = 4,
x1 = (−1, −1), x2 = (+1, +1), x3 = (−1, +1), x4 = (+1, +1), and corresponding
labels {−1, −1, +1, +1}. If the weak learner h is restricted to be an axis-parallel
half-space, it is clear that no such h can achieve an error smaller than 1/2 for a
uniform weighting over the examples.
We consider two situations, where it is possible to establish the strict positivity of γ, and to set a lower bound on its value. Consider a mapping f from the binary cube X = {−1, +1}d to Y = {−1, 1}, and assume the true labels yn are given
by yn = f(xn). We wish to approximate f by combinations of binary hypotheses
ht belonging to H. Intuitively we expect that a large edge γ can be achieved if
we can ﬁnd a weak hypothesis h which correlates well with f (cf. Section 4.1).
Let H be a class of binary hypotheses (Boolean functions), and let D be a distribution over X. The correlation between f and H, with respect to D, is given by
CH,D(f) = suph∈H ED{f(x)h(x)}. The distribution-free correlation between f
and H is given by CH(f) = infD CH,D(f). shows that if T > 2 log(2)dCH(f)−2
then f can be represented exactly as f(x) = sign
. In other words,
if H is highly correlated with the target function f, then f can be exactly represented as a convex combinations of a small number of functions from H. Hence,
after a suﬃciently large number of Boosting iterations, the empirical error can
be expected to approach zero. Interestingly, this result is related to the Min-Max
theorem presented in Section 4.1.
Fig. 2. A single convex set containing the positively labeled examples separated from the negative examples by a gap of η.
The results of address only the case of Boolean functions and a known
target function f. An important question that arises relates to the establishment
of geometric conditions for which the existence of a weak learner can be guaranteed. Consider the case where the input patterns x belong to Rd, and let H, the
class of weak learners, consisting of linear classiﬁers of the form sign(w⊤x + b).
It is not hard to show , that for any distribution d over a training set of
An Introduction to Boosting and Leveraging
distinct points, ε(h, d) ≤
2 −c/N, where c is an absolute constant. In other
words, for any d one can ﬁnd a linear classiﬁer that achieves an edge of at least
c/N. However, as will become clear in Section 3.4 such an advantage does not
suﬃce to guarantee good generalization. This is not surprising, since the claim
holds even for arbitrarily labeled points, for which no generalization can be expected. In order to obtain a larger edge, some assumptions need to be made
about the data. Intuitively, we expect that situations where the positively and
negatively labeled points are well separated are conducive to achieving a large
edge by linear classiﬁers. Consider, for example, the case where the positively
labeled points are enclosed within a compact convex region in K ⊂Rd, while
the remaining points are located outside of this region, such that the distance
between the oppositely labeled points is at least η, for some η > 0 (see Figure 2).
It is not hard to show that in this case γ ≥γ0 > 0, namely the edge is
strictly larger than zero, independently of the sample size (γ0 is related to the
number of faces of the smallest convex polytope that covers only the positively
labeled points, see also discussion in Section 4.2).
In general situations, the data may be strongly overlapping. In order to deal
with such cases, much more advanced tools need to be wielded based on the
theory of Geometric Discrepancy . The technical details of this development are beyond the scope of this paper. The interested reader is referred to
 for further details. In general, there has not been much work on establishing geometric conditions for the existence of weak learners for other types of
classiﬁers.
Convergence of the Training Error to Zero
As we have seen in the previous section, it is possible under appropriate conditions to guarantee that the weighted empirical error of a weak learner is smaller
2γ, γ > 0. We now show that this condition suﬃces to ensure that
the empirical error of the composite hypothesis converges to zero as the number
of iterations increases. In fact, anticipating the generalization bounds in Section 3.4, we present a somewhat stronger result. We establish the claim for the
AdaBoost algorithm; similar claims can be proven for other Boosting algorithms
(cf. Section 4.1 and ).
Keeping in mind that we may use a real-valued function f for classiﬁcation,
we often want to take advantage of the actual value of f, even though classiﬁcation is performed using sign(f). The actual value of f contains information about
the conﬁdence with which sign(f) is predicted (e.g. ). For binary classiﬁcation
(y ∈{−1, +1}) and f ∈R we deﬁne the margin of f at example (xn, yn) as
ρn(f) = ynf(xn).
Consider the following function deﬁned for 0 ≤θ ≤1
if 0 < z ≤θ,
otherwise .
R. Meir and G. R¨atsch
Let f be a real-valued function taking values in [−1, +1]. The empirical margin
error is deﬁned as
ˆLθ(f) = 1
ϕθ(ynf(xn)).
It is obvious from the deﬁnition that the classiﬁcation error, namely the fraction
of misclassiﬁed examples, is given by θ = 0, i.e. ˆL(f) = ˆL0(f). In addition,
ˆLθ(f) is monotonically increasing in θ. We note that one often uses the so-called
0/1-margin error deﬁned by
˜Lθ(f) = 1
I(ynf(xn) ≤θ).
Noting that ϕθ(yf(x)) ≤I(yf(x) ≤θ), it follows that ˆLθ(f) ≤˜Lθ(f). Since we
use ˆLθ(f) as part of an upper bound on the generalization error in Section 3.4,
the bound we obtain using it is tighter than would be obtained using ˜Lθ(f).
Theorem 1 ( ). Consider AdaBoost as described in Algorithm 2.1. Assume
that at each round t, the weighted empirical error satisﬁes ε(ht, d(t)) ≤1
Then the empirical margin error of the composite hypothesis fT obeys
ˆLθ(fT ) ≤
2 (1 + γt)
Proof. We present a proof from for the case where ht ∈{−1, +1}. We begin
by showing that for every {αt}
˜Lθ(fT ) ≤exp
By deﬁnition
n e−ynαtht(xn)
n:yn=ht(xn)
n:yn̸=ht(xn)
= (1 −εt)e−αt + εteαt.
From the deﬁnition of fT it follows that
yfT (x) ≤θ
αtht(x) + θ
which we rewrite as
An Introduction to Boosting and Leveraging
I[Y fT (X) ≤θ] ≤exp
αtht(x) + θ
exp (−αT ynhT (xn))
t=1 αtynht(xn)
(by induction).
Using (13) and (14) we ﬁnd that
˜Lθ(f) = 1
I[ynfT (xn) ≤θ]
αtht(xn) + θ
Next, set αt = (1/2) log((1 −εt)/εt) as in Algorithm 2.1, which easily implies
εt(1 −εt).
Substituting this result into (12) we ﬁnd that
(1 −εt)1+θ
which yields the desired result upon recalling that εt = 1/2 −γt/2, and noting
that ˆLθ(f) ≤˜Lθ(f).
In the special case that θ = 0, i.e. one considers the training error, we ﬁnd that
ˆL(fT ) ≤e−T
from which we infer that the condition T
t →∞suﬃces to guarantee that
ˆL(fT ) →0. For example, the condition γt ≥c/
t suﬃces for this.2 Clearly
2 When one uses binary valued hypotheses, then γt ≥c/t is already suﬃcient to
achieve a margin of at least zero on all training examples (cf. , Lemma 2.2, 2.3).
R. Meir and G. R¨atsch
this holds if γt ≥γ0 > 0 for some positive constant γ0. In fact, in this case
ˆLθ(fT ) →0 for any θ ≤γ0/2.
In general, however, one may not be interested in the convergence of the
empirical error to zero, due to overﬁtting (see discussion in Section 6). Suﬃcient
conditions for convergence of the error to a nonzero value can be found in .
Generalization Error Bounds
In this section we consider binary classiﬁers only, namely f : X →{−1, +1}.
A learning algorithm can be viewed as a procedure for mapping any data set
S = {(xn, yn)}N
n=1 to some hypothesis h belonging to a hypothesis class H
consisting of functions from X to {−1, +1}. In principle, one is interested in
quantifying the performance of the hypothesis ˆf on future data. Since the data
S consists of randomly drawn pairs (xn, yn), both the data-set and the generated
hypothesis ˆf are random variables. Let λ(y, f(x)) be a loss function, measuring
the loss incurred by using the hypothesis f to classify input x, the true label
of which is y. As in Section 2.1 the expected loss incurred by a hypothesis f is
given by L(h) = E{λ(y, f(x))}, where the expectation is taken with respect to
the unknown probability distribution generating the pairs (xn, yn). In the sequel
we will consider the case of binary classiﬁcation and 0/1-loss
λ(y, f(x)) = I[y ̸= f(x)],
where I[E] = 1, if the event E occurs and zero otherwise. In this case it is easy
to see that L(f) = Pr[y ̸= h(x)], namely the probability of misclassiﬁcation.
A classic result by Vapnik and Chervonenkis relates the empirical classiﬁcation error of a binary hypothesis f, to the probability of error Pr[y ̸= f(x)].
For binary functions f we use the notation P(y ̸= f(x)) for the probability of
error and ˆP(y ̸= f(x) for the empirical classiﬁcation error. Before presenting
the result, we need to deﬁne the VC-dimension of a class of binary hypotheses
F. Consider a set of N points X = (x1, . . . , xN). Each binary function f deﬁnes
a dichotomy (f(x1), . . . , f(xN)) ∈{−1, +1}N on these points. Allowing f to
run over all elements of F, we generate a subset of the binary N-dimensional
cube {−1, +1}N, denoted by FX, i.e. FX = {(f(x1), . . . , f(xN)) : f ∈F}. The
VC-dimension of F, VCdim(F), is deﬁned as the maximal cardinality of the set
of points X = (x1, . . . , xN) for which |FX| = 2N. Good estimates of the VC
dimension are available for many classes of hypotheses. For example, in the ddimensional space Rd one ﬁnds for hyperplanes a VC dimension of d + 1, while
for rectangles the VC dimension is 2d. Many more results and bounds on the
VC dimension of various classes can be found in and .
We present an improved version of the classic VC bound, taken from .
Theorem 2 ( ). Let F be a class of {−1, +1}-valued functions deﬁned over
a set X. Let P be a probability distribution on X × {−1, +1}, and suppose that
N-samples S = {(x1, y1), . . . , (xN, yN)} are generated independently at random
according to P. Then, there is an absolute constant c, such that for any integer
N, with probability at least 1 −δ over samples of length N, every f ∈F satisﬁes
An Introduction to Boosting and Leveraging
P(y ̸= f(x)) ≤ˆP(y ̸= f(x)) + c
VCdim(F) + log(1/δ)
We comment that the original bound in contained an extra factor of
log N multiplying VCdim(F) in (15).
The ﬁnite-sample bound presented in Theorem 2 has proved very useful in
theoretical studies. It should also be stressed that the bound is distribution
free, namely it holds independently of the underlying probability measure P.
Moreover, it can be shown ( , Theorem 14.5) to be optimal in rate in a precise
minimax sense. However, in practical applications it is often far too conservative
to yield useful results.
Margin Based Generalization Bounds
In spite of the claimed optimality of the above bound, there is good reason to
investigate bounds which improve upon it. While such an endeavor may seem
futile in light of the purported optimality of (15), we observe that the bound
is optimal only in a distribution-free setting, where no restrictions are placed
on the probability distribution P. In fact, one may want to take advantage of
certain regularity properties of the data in order to improve the bound. However,
in order to retain the appealing distribution-free feature of the bound (15), we
do not want to impose any a-priori conditions on P. Rather, the idea is to
construct a so-called luckiness function based on the data , which yields
improved bounds in situations where the structure of the data happens to be
‘simple’. In order to make more eﬀective use of a classiﬁer, we now allow the
class of classiﬁers F to be real-valued. In view of the discussion in Section 3.1, a
good candidate for a luckiness function is the margin-based loss ˆLθ(f) deﬁned
in (10). The probability of error is given by L(f) = P(y ̸= sign(f(x))).
For real-valued classiﬁers F, deﬁne a new notion of complexity, related to the
VC dimension but somewhat more general. Let {σ1, . . . , σN} be a sequence of
{−1, +1}-valued random variables generated independently by setting each σn
to {−1, +1} with equal probability. Additionally, let {x1, . . . , xN} be generated
independently at random according to some law P. The Rademacher complexity
 of the class F is given by
RN(F) = E sup
where the expectation is taken with respect to both {σn} and {xn}. The
Rademacher complexity has proven to be essential in the derivation of eﬀective generalization bounds (e.g. ).
The basic intuition behind the deﬁnition of RN(F) is its interpretation as
a measure of correlation between the class F and a random set of labels. For
very rich function classes F we expect a large value of RN(F) while small function classes can only achieve small correlations with random labels. In the
special case that the class F consists of binary functions, one can show that
RN(F) = O(
VCdim(F)/N). For real-valued functions, one needs to extend the
R. Meir and G. R¨atsch
notion of VC dimension to the so-called pseudo-dimension (e.g. ), in which
case one can show that RN(F) = O(
Pdim(F)/N). It is not hard to show that
VCdim(sign(F)) ≤Pdim(F).
The following theorem (cf. ) provides a bound on the probability of
misclassiﬁcation using a margin-based loss function.
Theorem 3 ( ). Let F be a class of real-valued functions from X to
[−1, +1], and let θ ∈ . Let P be a probability distribution on X × {−1, +1},
and suppose that N-samples S = {(x1, y1), . . . , (xN, yN)} are generated independently at random according to P. Then, for any integer N, with probability
at least 1 −δ over samples of length N, every f ∈F satisﬁes
L(f) ≤ˆLθ(f) + 4RN(F)
In order to understand the signiﬁcance of Theorem 3, consider a situation
where one can ﬁnd a function f which achieves a low margin error ˆLθ(f) for
a large value of the margin parameter θ. In other words, ˆLθ(f) ≈ˆL(f) for
large values of θ. In this case the second term on the r.h.s. in (16) can be
made to be smaller than the corresponding term in (15) (recall that RN(F) =
Pdim(F)/N) for classes with ﬁnite pseudo-dimension), using the standard
VC bounds. Note that Theorem 3 has implications concerning the size of the
margin θ. In particular, assuming F is a class with ﬁnite VC dimension, we have
that the second term on the r.h.s. of (16) is of the order O(
VCdim(F)/Nθ2).
In order that this term decrease to zero it is mandatory that θ ≫1/
other words, in order to lead to useful generalization bounds the margin must
be suﬃciently large with respect to the sample size N.
However, the main signiﬁcance of (16) arises from its application to the speciﬁc class of functions arising in Boosting algorithms. Recall that in Boosting,
one generates a hypothesis f which can be written as fT (x) = T
t=1 αtht(x),
namely a linear combination with non-negative coeﬃcients of base hypotheses
h1, . . . , ht, each belonging to the class H. Consider the class of functions
f : f(x) =
αtht(x) : αt ≥0,
corresponding to the ℓ1-normalized function output by the Boosting algorithm.
The convex hull is given by letting T →∞in coT .
The key feature in the application of Theorem 3 to Boosting is the observation
that for any class of real-valued functions H, RN(coT (H)) = RN(H) for any T
(e.g. ), namely the Rademacher complexity of coT (H) is not larger than that
of the base class H itself. On the other hand, since ht(x) are in general non-linear
functions, the linear combination T
t=1 αtht(x) represents a potentially highly
complex function which can lead to very low margin error ˆLθ(fT ). Combining
these observations, we obtain the following result, which improves upon the ﬁrst
bounds of this nature presented in .
An Introduction to Boosting and Leveraging
Corollary 1. Let the conditions of Theorem 3 hold, and set F = coT (H). Then,
for any integer N, with probability at least 1−δ over samples of length N, every
f ∈coT (H) satisﬁes
L(f) ≤ˆLθ(f) + 4RN(H)
Had we used a bound of the form of (15) for f ∈coT (H), we would have obtained
a bound depending on Pdim(coT (H)), which often grows linearly in T, leading to
inferior results. The important observation here is that the complexity penalty
in (17) is independent of T, the number of Boosting iterations.
As a ﬁnal comment we add that a considerable amount of recent work has
been devoted to the derivation of so-called data-dependent bounds (e.g. ), where the second term on the r.h.s. of (16) is made to depend explicitly on
the data. Data-dependent bounds depending explicitly on the weights αt of the
weak learners are given in . In addition, bounds which take into account
the full margin distribution are presented in . Such results are
particularly useful for the purpose of model selection, but are beyond the scope
of this review.
Consistency
The bounds presented in Theorems 2 and 3, depend explicitly on the data, and
are therefore potentially useful for the purpose of model selection. However,
an interesting question regarding the statistical consistency of these procedures
arises. Consider a generic binary classiﬁcation problem, characterized by the class
conditional distribution function P(y|x), where τ(x) = P(y = 1|x) is assumed
to belong to some target class of functions T. It is well known that in this
case the optimal classiﬁer is given by the Bayes classiﬁer fB(x) = sign(τ(x)−1
leading to the minimal error LB = L(fB). We say that an algorithm is strongly
consistent with respect to T if, based on a sample of size N, it generates an
empirical classiﬁer ˆfN for which L( ˆfN) →LB almost surely for N →∞, for every
τ ∈T. While consistency may seem to be mainly of theoretical signiﬁcance, it
is reassuring to have the guarantee that a given procedure ultimately performs
optimally. However, it turns out that in many cases inconsistent procedures
perform better for ﬁnite amounts of data, than consistent ones. A classic example
of this is the so-called James-Stein estimator ( , Section 2.4.5).
In order to establish consistency one needs to assume (or prove in speciﬁc
cases) that as T →∞the class of functions coT (H) is dense in T. The consistency
of Boosting algorithms has recently been established in , following
related previous work . The work of also includes rates of convergence
for speciﬁc weak learners and target classes T. We point out that the full proof
of consistency must tackle at least three issues. First, it must be shown that
the speciﬁc algorithm used converges as a function of the number of iterations
- this is essentially an issue of optimization. Furthermore, one must show that
the function to which the algorithm converges itself converges to the optimal
R. Meir and G. R¨atsch
estimator as the sample size increases - this is a statistical issue. Finally, the
approximation theoretic issue of whether the class of weak learners is suﬃciently
powerful to represent the underlying decision boundary must also be addressed.
Boosting and Large Margins
In this section we discuss AdaBoost in the context of large margin algorithms.
In particular, we try to shed light on the question of whether, and under what
conditions, boosting yields large margins. In it was shown that AdaBoost
quickly ﬁnds a combined hypothesis that is consistent with the training data.
 and indicated that AdaBoost computes hypotheses with large margins,
if one continues iterating after reaching zero classiﬁcation error. It is clear that
the margin should be as large as possible on most training examples in order to
minimize the complexity term in (17). If one assumes that the base learner always
achieves a weighted training error ϵt ≤1/2 −γ/2 with γ > 0, then AdaBoost
generates a hypothesis with margin larger than γ/2 . However, from the
Min-Max theorem of linear programming (see Theorem 4 below) one ﬁnds
that the achievable margin ρ∗is at least γ .
We start with a brief review of some standard deﬁnitions and results for the
margin of an example and of a hyperplane. Then we analyze the asymptotic
properties of a slightly more general version, called AdaBoostϱ, which is equivalent to AdaBoost for ϱ = 0, while assuming that the problem is separable.
We show that there will be a subset of examples – the support patterns –
asymptotically having the same smallest margin. All weights d are asymptotically concentrated on these examples. Furthermore, we ﬁnd that AdaBoostϱ is
able to achieve larger margins, if ϱ is chosen appropriately. We brieﬂy discuss
two algorithms for adaptively choosing ϱ to maximize the margin.
Weak Learning, Edges, and Margins
The assumption made concerning the base learning algorithm in the PAC-
Boosting setting (cf. Section 3.1) is that it returns a hypothesis h from a ﬁxed
set H that is slightly better than the baseline classiﬁer introduced in Deﬁnition 1.
This means that for any distribution, the error rate ε is consistently smaller than
2γ for some ﬁxed γ > 0.
Recall that the error rate ε of a base hypothesis is deﬁned as the weighted
fraction of points that are misclassiﬁed (cf. (8)). The weighting d = (d1, . . . , dN)
of the examples is such that dn ≥0 and N
n=1 dn = 1. A more convenient
quantity to measure the quality of the hypothesis h is the edge , which is
also applicable and useful for real-valued hypotheses:
dnynh(xn).
The edge is an aﬃne transformation of ε(h, d) in the case when h(x) ∈{−1, +1}:
ε(h, d) = 1
2γ(h, d) . Recall from Section 3.2 that the margin of a
function f for a given example (xn, yn) is deﬁned by ρn(f) = ynf(xn) (cf. (9)).
An Introduction to Boosting and Leveraging
Assume for simplicity that H is a ﬁnite hypothesis class H = {˜hj | j =
1, . . . , J}, and suppose we combine all possible hypotheses from H. Then the
following well-known theorem establishes the connection between margins and
edges (ﬁrst noted in connection with Boosting in ). Its proof follows
directly from the duality of the two optimization problems.
Theorem 4 (Min-Max-Theorem, ).
dnyn˜h(xn)
where d ∈PN, w ∈PJ and Pk is the k-dimensional probability simplex.
Thus, the minimal edge γ∗that can be achieved over all possible weightings d
of the training set is equal to the maximal margin ϱ∗of a combined hypothesis
from H. We refer to the left hand side of (19) as the edge minimization problem.
This problem can be rewritten as a linear program (LP):
dn = 1 and
dnyn˜h(xn) ≤γ
For any non-optimal weightings d and w we always have max˜h∈H γ(˜h, d) ≥γ∗=
ϱ∗≥minn=1,... ,N ynfw(xn), where
If the base learning algorithm is guaranteed to return a hypothesis with edge
at least γ for any weighting, there exists a combined hypothesis with margin
at least γ. If γ = γ∗, i.e. the lower bound γ is as large as possible, then there
exists a combined hypothesis with margin exactly γ = ϱ∗(only using hypotheses
that are actually returned by the base learner). From this discussion we can
derive a suﬃcient condition on the base learning algorithm to reach the maximal
margin: If it returns hypotheses whose edges are at least γ∗, there exists a linear
combination of these hypotheses that has margin γ∗= ϱ∗. This explains the
termination condition in Algorithm 2.1 (step (4)).
Remark 1. Theorem 4 was stated for ﬁnite hypothesis sets H. However, the
same result holds also for countable hypothesis classes. For uncountable classes,
one can establish the same results under some regularity conditions on H,
in particular that the real-valued hypotheses h are uniformly bounded (cf.
 ).
R. Meir and G. R¨atsch
To avoid confusion, note that the hypotheses indexed as elements of the
hypothesis set H are marked by a tilde, i.e. ˜h1, . . . , ˜hJ, whereas the hypotheses
returned by the base learner are denoted by h1, . . . , hT . The output of AdaBoost
and similar algorithms is a sequence of pairs (αt, ht) and a combined hypothesis
ft(x) = t
r=1 αrhr(x). But how do the α’s relate to the w’s used in Theorem 4?
At every step of AdaBoost, one can compute the weight for each hypothesis ˜hj
in the following way:3
αrI(hr = ˜hj),
j = 1, . . . , J.
It is easy to verify that t
r=1 αrhr(x) = J
j˜hj(x). Also note, if the α’s are
positive (as in Algorithm 2.1), then ∥w∥1 = ∥α∥1 holds.
Geometric Interpretation of p-Norm Margins
Margins have been used frequently in the context of Support Vector Machines
(SVMs) and Boosting. These so-called large margin algorithms focus on generating hyperplanes/functions with large margins on most training
examples. Let us therefore study some properties of the maximum margin hyperplane and discuss some consequences of using diﬀerent norms for measuring
the margin (see also Section 6.3).
Suppose we are given N examples in some space F: S = {(xn, yn)}N
n=1, where
(xn, yn) ⊆F × {−1, 1}. Note that we use x as elements of the feature space F
instead of x as elements of the input space X (details below). We are interested
in the separation of the training set using a hyperplane A through the origin4
A = {x | ⟨x, w⟩= 0} in F determined by some vector w, which is assumed to
be normalized with respect to some norm. The ℓp-norm margin of an example
(xn, yn) with respect to the hyperplane A is deﬁned as
n(w) := yn⟨xn, w⟩
where the superscript p ∈[1, ∞] speciﬁes the norm with respect to which w is
normalized (the default is p = 1). A positive margin corresponds to a correct
classiﬁcation. The margin of the hyperplane A is deﬁned as the minimum margin
over all N examples, ρp(w) = minn ρp
To maximize the margin of the hyperplane one has to solve the following
convex optimization problem :
ρp(w) = max
3 For simplicity, we have omitted the normalization implicit in Theorem 4.
4 This can easily be generalized to general hyperplanes by introducing a bias term.
An Introduction to Boosting and Leveraging
The form of (23) implies that without loss of generality we may take ∥w∥p = 1,
leading to the following convex optimization problem:
yn⟨xn, w⟩≥ρ,
n = 1, 2, . . . , N
Observe that in the case where p = 1 and wj ≥0, we obtain a Linear Program
(LP) (cf. (19)). Moreover, from this formulation it is clear that only a few of
the constraints in (24) will typically be active. These constraints correspond to
the most diﬃcult examples, called the support patterns in boosting and support
vectors in SVMs.5
The following theorem gives a geometric interpretation to (23): Using the ℓpnorm to normalize w corresponds to measuring the distance to the hyperplane
with the dual ℓq-norm, where 1/p + 1/q = 1.
Theorem 5 ( ). Let x ∈F be any point which is not on the plane A :=
{˜x | ⟨˜x, w⟩= 0}. Then for p ∈[1, ∞]:
= ∥x −A∥q,
where ∥x −A∥q = min˜x∈A ∥x −˜x∥q denotes the distance of x to the plane A
measured with the dual norm ℓq.
Thus, the ℓp-margin of xn is the signed ℓq-distance of the point to the hyperplane.
If the point is on the correct side of the hyperplane, the margin is positive (see
Figure 3 for an illustration of Theorem 5).
Fig. 3. The maximum margin solution for diﬀerent norms on a toy
example: ℓ∞-norm (solid) and ℓ2norm (dashed). The margin areas are indicated by the dashdotted lines. The examples with label +1 and −1 are shown as ‘x’
and ‘◦’, respectively. To maximize
the ℓ∞-norm and ℓ2-norm margin,
we solved (23) with p = 1 and
p = 2, respectively. The ℓ∞-norm
often leads to fewer supporting examples and sparse weight-vectors.
Let us discuss two special cases: p = 1 and p = 2:
5 In Section 4.4 we will discuss the relation of the Lagrange multipliers of these constraints and the weights d on the examples.
R. Meir and G. R¨atsch
Boosting (p = 1). In the case of Boosting, the space F is spanned by the base
hypotheses. One considers the set of base hypotheses that could be generated by
the base learner – the base hypothesis set – and constructs a mapping Φ from
the input space X to the “feature space” F:
In this case the margin of an example is (cf. (22))
n(w) = yn⟨xn, w⟩
j wj˜hj(xn)
t αtht(xn)
as used before in Algorithm 2.1, where we used the ℓ1-norm for normalization
and without loss of generality assumed that the w’s and α’s are non-negative
(assuming H is closed under negation). By Theorem 5, one therefore maximizes
the ℓ∞-norm distance of the mapped examples to the hyperplane in the feature
space F. Under mild assumptions, one can show that the maximum margin
hyperplane is aligned with most coordinate axes in the feature space, i.e. many
wj’s are zero (cf. Figure 3 and Section 6.3).
Support Vector Machines (p = 2). Here, the feature space F is implicitly de-
ﬁned by a Mercer kernel k(x, y) , which computes the inner product of two
examples in the feature space F . One can show that for every such
kernel there exists a mapping Φ : X →F, such that k(x, y) = ⟨Φ(x), Φ(y)⟩for
all x, y ∈X. Additionally, one uses the ℓ2-norm for normalization and, hence,
the Euclidean distance to measure distances between the mapped examples and
the hyperplane in the feature space F:
n(w) = yn⟨Φ(xn), w⟩
i=1 βi k(xn, xi)
i,j=1 βiβj k(xi, xj)
where we used the Representer Theorem that shows that the maximum
margin solution w can be written as a sum of the mapped examples, i.e. w =
n=1 βnΦ(xn).
AdaBoost and Large Margins
We have seen in Section 3.4 that a large value of the margin is conducive to good
generalization, in the sense that if a large margin can be achieved with respect
to the data, then an upper bound on the generalization error is small (see also
discussion in Section 6). This observation motivates searching for algorithms
which maximize the margin.
An Introduction to Boosting and Leveraging
Convergence properties of AdaBoost. We analyze a slightly generalized
version of AdaBoost. One introduces a new parameter, ϱ, in step (3c) of Algorithm 2.1 and chooses the weight of the new hypothesis diﬀerently:
2 log 1 + γt
2 log 1 + ϱ
This algorithm was ﬁrst introduced in as unnormalized Arcing with exponential function and in as AdaBoost-type algorithm. Moreover, it is similar
to an algorithm proposed in (see also ). Here we will call it AdaBoostϱ.
Note that the original AdaBoost algorithm corresponds to the choice ϱ = 0.
Let us for the moment assume that we chose ϱ at each iteration diﬀerently,
i.e. we consider sequences {ϱt}T
t=1, which might either be speciﬁed before running
the algorithm or computed based on results obtained during the running of the
algorithm. In the following we address the issue of how well this algorithm,
denoted by AdaBoost{ϱt}, is able to increase the margin, and bound the fraction
of examples with margin smaller than some value θ.
We start with a result generalizing Theorem 1 (cf. Section 3.2) to the case
ϱ ̸= 0 and a slightly diﬀerent loss:
Proposition 1 ( ). Let γt be the edge of ht at the t-th step of AdaBoostϱ.
Assume −1 ≤ϱt ≤γt for t = 1, . . . , T. Then for all θ ∈[−1, 1]
ˆLθ(fT ) ≤1
I(ynfT (xn) ≤θ) ≤
The algorithm makes progress, if each of the products on the right hand side of
(27) is smaller than one.
Suppose we would like to reach a margin θ on all training examples, where
we obviously need to assume θ ≤ϱ∗(here ϱ∗is deﬁned in (19)). The question
arises as to which sequence of {ϱt}T
t=1 one should use to ﬁnd such a combined
hypothesis in as few iterations as possible (according to (27)). One can show
that the right hand side of (27) is minimized for ϱt = θ and, hence, one should
always use this choice, independent of how the base learner performs.
Using Proposition 1, we can determine an upper bound on the number of
iterations needed by AdaBoostϱ for achieving a margin of ϱ on all examples,
given that the maximum margin is ϱ∗(cf. ):
Corollary 2. Assume the base learner always achieves an edge γt ≥ϱ∗. If 0 ≤
ϱ ≤ϱ∗−ν, ν > 0, then AdaBoostϱ will converge to a solution with margin of at
least ϱ on all examples in at most ⌈2 log(N)(1−ϱ2)
⌉+ 1 steps.
Maximal Margins. Using the methodology reviewed so far, we can also analyze
to what value the maximum margin of the original AdaBoost algorithm converges
asymptotically. First, we state a lower bound on the margin that is achieved by
AdaBoostϱ. We ﬁnd that the size of the margin is not as large as it could be
theoretically based on Theorem 4. We brieﬂy discuss below two algorithms that
are able to maximize the margin.
R. Meir and G. R¨atsch
As long as each factor on the r.h.s. of (27) is smaller than 1, the bound
decreases. If it is bounded away from 1, then it converges exponentially fast to
zero. The following corollary considers the asymptotic case and provides a lower
bound on the margin when running AdaBoostϱ forever.
Corollary 3 ( ). Assume AdaBoostϱ generates hypotheses h1, h2, . . . with
edges γ1, γ2, . . . . Let γ = inft=1,2,... γt and assume γ > ϱ. Then the smallest
margin ρ of the combined hypothesis is asymptotically (t →∞) bounded from
ρ ≥log(1 −ϱ2) −log(1 −γ2)
From (28) one can understand the interaction between ϱ and γ: if the diﬀerence
between γ and ϱ is small, then the middle term of (28) is small. Thus, if ϱ is
large (assuming ϱ ≤γ), then ρ must be large, i.e. choosing a larger ϱ results in
a larger margin on the training examples.
Remark 2. Under the conditions of Corollary 3, one can compute a lower bound
on the hypothesis coeﬃcients in each iteration. Hence the sum of the hypothesis
coeﬃcients will increase to inﬁnity at least linearly. It can be shown that this
suﬃces to guarantee that the combined hypothesis has a large margin, i.e. larger
than ϱ (cf. , Section 2.3).
However, in Section 4.1 we have shown that the maximal achievable margin
is at least γ. Thus if ϱ is chosen to be too small, then we guarantee only a
suboptimal asymptotic margin. In the original formulation of AdaBoost we have
ϱ = 0 and we guarantee only that AdaBoost0 achieves a margin of at least
γ/2.6 This gap in the theory led to the so-called Arc-GV algorithm and the
Marginal AdaBoost algorithm .
Arc-GV. The idea of of Arc-GV is to set ϱ to the margin that is currently
achieved by the combined hypothesis, i.e. depending on the previous performance
of the base learner:7
n=1,... ,N ynft−1(xn)
There is a very simple proof using Corollary 3 that Arc-GV asymptotically
maximizes the margin . The idea is to show that ϱt converges to ρ∗–
the maximum margin. The proof is by contradiction: Since {ϱt} is monotonically
increasing on a bounded interval, it must converge. Suppose {ϱt} converges to a
value ϱ∗smaller than ρ∗, then one can apply Corollary 3 to show that the margin
of the combined hypothesis is asymptotically larger than ρ∗+ϱ∗
. This leads to a
contradiction, since ϱt is chosen to be the margin of the previous iteration. This
shows that Arc-GV asymptotically maximizes the margin.
6 Experimental results in conﬁrm this analysis and illustrate that the bound
given in Corollary 3 is tight.
7 The deﬁnition of Arc-GV is slightly modiﬁed. The original algorithm did not use the
An Introduction to Boosting and Leveraging
Marginal AdaBoost. Unfortunately, it is not known how fast Arc-GV converges
to the maximum margin solution. This problem is solved by Marginal AdaBoost
 . Here one also adapts ϱ, but in a diﬀerent manner. One runs AdaBoostϱ
repeatedly, and determines ϱ by a line search procedure: If AdaBoostϱ is able to
achieve a margin of ϱ then it is increased, otherwise decreased. For this algorithm
one can show fast convergence to the maximum margin solution .
Relation to Barrier Optimization
We would like to point out how the discussed algorithms can be seen in the
context of barrier optimization. This illustrates how, from an optimization point
of view, Boosting algorithms are related to linear programming, and provide
further insight as to why they generate combined hypotheses with large margins.
The idea of barrier techniques is to solve a sequence of unconstrained optimization problems in order to solve a constrained optimization problem (e.g.
 ). We show that the exponential function acts as a barrier
function for the constraints in the maximum margin LP (obtained from (24) by
setting p = 1 and restricting wj ≥0):
yn⟨xn, w⟩≥ρ
n = 1, 2, . . . , N
wj ≥0,  wj = 1.
Following the standard methodology and using the exponential barrier function8 β exp(−z/β) , we ﬁnd the barrier objective for problem (30):
Fβ(w, ρ) = −ρ + β
ρ −ynfw(xn)
which is minimized with respect to ρ and w for some ﬁxed β. We denote the
optimum by wβ and ρβ. One can show that any limit point of (wβ, ρβ) for β →0
is a solution of (30) . This also holds when one only has a sequence
of approximate minimizers and the approximation becomes better for decreasing
β . Additionally, the quantities ˜dn = exp(ρβ
j wj −ynfwβ(xn))/Z (where
Z is chosen such that 
n ˜dn = 1) converge for β →0 to the optimal Lagrange
multipliers d∗
n of the constraints in (30) (under mild assumptions, see , Theorem 2, for details). Hence, they are a solution of the edge minimization problem
To see the connection between Boosting and the barrier approach, we chose
. If the sum of the hypothesis coeﬃcients increases to inﬁnity
(cf. Remark 2 in Section 4.3), then β converges to zero. Plugging in this choice
of β into (31) one obtains: −ρ + (
j wj)−1 N
j wj −ynfw(xn)
Without going into further details , one can show that this function
8 Other barrier functions are β log(z) (log-barrier) and βz log(z) (entropic barrier).
R. Meir and G. R¨atsch
is minimized by the AdaBoostϱ algorithm. Thus, one essentially obtains the
exponential loss function as used in AdaBoostϱ for ρ = ϱ.
Seen in this context, AdaBoostϱ is an algorithm that ﬁnds a feasible solution,
i.e. one that has margin at least ϱ. Also, it becomes clear why it is important
that the sum of the hypothesis coeﬃcients goes to inﬁnity, an observation already
made in the previous analysis (cf. Remark 2): otherwise β would not converge
to 0 and the constraints may not be enforced. Arc-GV and Marginal AdaBoost
are extensions, where the parameter ρ is optimized as well. Hence, AdaBoost,
AdaBoostϱ, Arc-GV and Marginal AdaBoost can be understood as particular
implementations of a barrier optimization approach, asymptotically solving a
linear optimization problem.
Leveraging as Stagewise Greedy Optimization
In Section 4 we focused mainly on AdaBoost. We now extend our view to more
general ensemble learning methods which we refer to as leveraging algorithms
 . We will relate these methods to numerical optimization techniques. These
techniques served as powerful tools to prove the convergence of leveraging algorithms (cf. Section 5.4).
We demonstrate the convergence of ensemble learning methods such as AdaBoost and Logistic Regression (LR) , although the techniques are
much more general. These algorithms have in common that they iteratively call
a base learning algorithm L (a.k.a. weak learner) on a weighted training sample. The base learner is expected to return at each iteration t a hypothesis ht
from the hypothesis set H that has small weighted training error (see (4)) or
large edge (see (18)). These hypotheses are then linearly combined to form the
ﬁnal or composite hypothesis fT as in (1). The hypothesis coeﬃcient αt is determined at iteration t, such that a certain objective is minimized or approximately
minimized, and it is ﬁxed for later iterations.
For AdaBoost and the Logistic Regression algorithm it has been shown
 that they generate a composite hypothesis minimizing a loss function G –
in the limit as the number of iterations goes to inﬁnity. The loss depends only
on the output of the combined hypothesis fT on the training sample. However,
the assumed conditions (discussed later in detail) in on the performance of
the base learner are rather strict and can usually not be satisﬁed in practice.
Although parts of the analysis in hold for any strictly convex cost function
of Legendre-type (cf. , p. 258), one needs to demonstrate the existence of a
so-called auxiliary function (cf. ) for each cost function other
than the exponential or the logistic loss. This has been done for the general case
in under very mild assumptions on the base learning algorithm and the
loss function.
We present a family of algorithms that are able to generate a combined
hypothesis f converging to the minimum of some loss function G[f] (if it exists).
Special cases are AdaBoost , Logistic Regression and LS-Boost . While
assuming rather mild conditions on the base learning algorithm and the loss
function G, linear convergence rates (e.g. ) of the type G[ft+1] −G[f ∗] ≤
An Introduction to Boosting and Leveraging
η(G[ft] −G[f ∗]) for some ﬁxed η ∈[0, 1) has been shown. This means that the
deviation from the minimum loss converges exponentially to zero (in the number
of iterations). Similar convergence rates have been proven for AdaBoost in the
special case of separable data (cf. Section 4.3 and ). In the general case these
rates can only be shown when the hypothesis set is ﬁnite. However, in practice
one often uses inﬁnite sets (e.g. hypotheses parameterized by some real valued
parameters). Recently, Zhang has shown order one convergence for such
algorithms, i.e. G[ft+1] −G[f ∗] ≤c/t for some ﬁxed c > 0 depending on the loss
Preliminaries
In this section and the next one we show that AdaBoost, Logistic Regression and
many other leveraging algorithms generate a combined hypothesis f minimizing
a particular loss G on the training sample.
The composite hypothesis fw is a linear combination of the base hypotheses:
fw ∈lin(H) :=
w˜h˜h | ˜h ∈H, w˜h ∈R
where the w’s are the combination coeﬃcients. Often one would like to ﬁnd
a combined function with small classiﬁcation error, hence one would like to
minimize the 0/1-loss:
G0/1(f, S) :=
I(yn ̸= sign(fw(xn))).
However, since this loss is non-convex and not even diﬀerentiable, the problem of
ﬁnding the best linear combination is a very hard problem. In fact, the problem is
provably intractable . One idea is to use another loss function which bounds
the 0/1-loss from above. For instance, AdaBoost employs the exponential loss
GAB(fw, S) :=
exp(−ynfw(xn)),
and the LogitBoost algorithm uses the Logistic loss:
GLR(fw, S) :=
log2 (1 + exp(−ynfw(xn))) .
As can be seen in Figure 4, both loss functions bound the classiﬁcation error
G0/1(f, S) from above. Other loss functions have been proposed in the literature,
e.g. .
It can be shown that in the inﬁnite sample limit, where the
sample average converges to the expectation, minimizing either GAB(fw, S) or
R. Meir and G. R¨atsch
Squared loss
Logistic loss
Hinge loss
Fig. 4. The exponential (dashed)
functions,plotted against ρ = yf(x).
GLR(fw, S) leads to the same classiﬁer as would be obtained by minimizing
the probability of error. More precisely, let GAB(f) = E{exp(−yf(x))}, and
set G0/1(f) = E{I[y ̸= sign(f(x))]} = Pr[y ̸= sign(f(x))]. Denote by f ∗the
function minimizing GAB(f). Then it can be shown that f ∗minimizes Pr[y ̸=
sign(f(x))], namely achieves the Bayes risk. A similar argument applies to other
loss functions. This observation forms the basis for the consistency proofs in
 (cf. Section 3.5).
Note, that the exponential and logistic losses are both convex functions.
Hence, the problem of ﬁnding the global optimum of the loss with respect to
the combination coeﬃcients can be solved eﬃciently. Other loss functions have
been used to approach multi-class problems , ranking problems ,
unsupervised learning and regression . See Section 7 for more
details on some of these approaches.
A Generic Algorithm
Most Boosting algorithms have in common that they iteratively run a learning algorithm to greedily select the next hypothesis h ∈H. In addition, they
use in some way a weighting on the examples to inﬂuence the choice of the
base hypothesis. Once the hypothesis is chosen, the algorithm determines the
combination weight for the newly obtained hypothesis. Diﬀerent algorithms use
diﬀerent schemes to weight the examples and to determine the new hypothesis
In this section we discuss a generic method (cf. Algorithm 5.2) and the
connection between the loss function minimized and the particular weighting
Many of the proposed algorithms can be derived from this scheme.
Let us assume the loss function G(f, S) has the following additive form9
G(f, S) :=
g(f(xn), yn),
and we would like to solve the optimization problem
9 More general loss functions are possible and have been used, however, for the sake
of simplicity, we chose this additive form.
An Introduction to Boosting and Leveraging
Algorithm 5.2 – A Leveraging algorithm for the loss function G.
1. Input: S = ⟨(x1, y1), . . . , (xN, yN)⟩, No. of Iterations T
Loss function G : RN →R
2. Initialize: f0 ≡0, d1
n = g′(f0(xn), yn) for all n = 1 . . . N
3. Do for t = 1, . . . , T,
a) Train classiﬁer on {S, dt} and obtain hypothesis H ∈ht : X →Y
b) Set αt = argminα∈R G[ft + αht]
c) Update ft+1 = ft + αtht and d(t+1)
= g′(ft+1(xn), yn), n = 1, . . . , N
4. Output: fT
f∈lin(H) G(f, S) = min
g(fw(xn), yn)
In step (2) of Algorithm 5.2 the example weights are initialized using the derivative g′ of the loss function g with respect to the ﬁrst argument at (f0(xn), yn)
(i.e. at (0, yn)).
At each step t of Algorithm 5.2 one can compute the weight wt
j assigned to
each base hypothesis ˜hj, such that (cf. (22))
fwt := ft =
In iteration t of Algorithm 5.2 one chooses a hypothesis ht, which corresponds
to a weight (coordinate) wht. This weight is then updated to minimize the loss
(cf. step (3b)): w(t+1)
ht + αt. Since one always selects a single variable for
optimization at each time step, such algorithms are called coordinate descent
Observe that if one uses AdaBoost’s exponential loss in Algorithm 5.2, then
= g′(ft(xn), yn) = yn exp(−ynft(xn)). Hence, in distinction from the
original AdaBoost algorithm (cf. Algorithm 2.1), the d’s are multiplied by the
labels. This is the reason why we will later use a diﬀerent deﬁnition of the edge
(without the label).
Consider what would happen, if one always chooses the same hypothesis
(or coordinate). In this case, one could not hope to prove convergence to the
minimum. Hence, one has to assume the base learning algorithm performs well
in selecting the base hypotheses from H. Let us ﬁrst assume the base learner
always ﬁnd the hypothesis with maximal edge (or minimal classiﬁcation error in
the hard binary case):
ht = argmax
This is a rather restrictive assumption, being one among many that can be made
(cf. ). We will later signiﬁcantly relax this condition (cf. (40)).
R. Meir and G. R¨atsch
Let us discuss why the choice in (34) is useful. For this we compute the
gradient of the loss function with respect to the weight w˜h of each hypothesis ˜h
in the hypothesis set:
∂G(fwt, S)
g′(ft(xn), yn)˜h(xn) =
Hence, ht is the coordinate that has the largest component in the direction of
the gradient vector of G evaluated at wt.10 If one optimizes this coordinate one
would expect to achieve a large reduction in the value of the loss function.
This method of selecting the coordinate with largest absolute gradient component is called Gauss-Southwell-method in the ﬁeld of numerical optimization
(e.g. ). It is known to converge under mild assumptions (see details in
 and Section 5.4).
Performing the explicit calculation in (35) for the loss functions GAB (see (5))
yields the AdaBoost approach described in detail in Algorithm 2.1. In this case
g(f(x), y) = exp(−yf(x)) implying that d(t)
n ∝yn exp(−ynft−1(xn)), which, up
to a normalization constant and the factor yn is the form given in Algorithm 2.1.
The LogitBoost algorithm mentioned in Section 2.2 can be derived in
a similar fashion using the loss function GLR (see (7)). In this case we ﬁnd
∝yn/(1 + exp(ynft−1(xn))). Observe that in this case, the weights
assigned to misclassiﬁed examples with very negative margins is much smaller
than the exponential weights assigned by AdaBoost. This may help to explain
why LogitBoost tends to be less sensitive to noise and outliers than AdaBoost
 . Several extensions of the AdaBoost and LogitBoost algorithms based on
Bregman distances were proposed in following earlier work of and
 (cf. Section 5.3).
The Dual Formulation
AdaBoost was originally derived from results on online-learning algorithms ,
where one receives at each iteration one example, then predicts its label and
incurs a loss. The important question in this setting relates to the speed at
which the algorithm is able to learn to produce predictions with small loss. In
 the total loss was bounded in terms of the loss of the best predictor. To derive these results, Bregman divergences and generalized projections
were extensively used. In the case of boosting, one takes a dual view : Here
the set of examples is ﬁxed, whereas at each iteration the base learning algorithm generates a new hypothesis (the “online example”). Using online-learning
techniques, the convergence in the online learning domain – the dual domain –
has been analyzed and shown to lead to convergence results in the primal domain (cf. Section 4.3). In the primal domain the hypothesis coeﬃcients w are
optimized, while the weighting d are optimized in the dual domain.
10 If one assumes the base hypothesis set to be closed under negation (h ∈H ⇒
−h ∈H), then this is equivalent to choosing the hypothesis with largest absolute
component in the direction of the gradient.
An Introduction to Boosting and Leveraging
In AdaBoost was interpreted as entropy projection in the dual
domain. The key observation is that the weighting d(t+1) on the examples in the
t-th iteration is computed as a generalized projection of d(t) onto a hyperplane
(deﬁned by linear constraints), where one uses a generalized distance ∆(d, ˜d)
measure. For AdaBoost the unnormalized relative entropy is used, where
∆(d, ˜d) =
dn log(dn/ ˜dn) −dn + ˜dn.
The update of the distribution in steps (3c) and (3d) of Algorithm 2.1 is the
solution to the following optimization problem (“generalized projection”):
d(t+1) = argmind∈RN
+ ∆(d, d(t))
n=1 dnyn˜ht(xn) = 0,
Hence, the new weighting d(t+1) is chosen such that the edge of the previous hypothesis becomes zero (as e.g. observed in ) and the relative entropy between
the new d(t+1) and the old d(t) distribution is as small as possible . After
the projection the new d lies on the hyperplane deﬁned by the corresponding
constraint (cf. Figure 5).
Fig. 5. Illustration of generalized projections: one projects a point d(t) onto a plane
˜H by ﬁnding the point d(t+1) on the plane
that has the smallest generalized distance
from d(t). If G(d) = 1
2, then the generalized distance is equal to the squared Euclidean distance, hence, the projected point
is the closest (in the common sense)
point on the hyperplane. For another Bregman function G, one ﬁnds another projected
point d(t+1)
, since closeness is measured differently.
The work of and later of lead to a more general
understanding of Boosting methods in the context of Bregman distance optimization and Information Theory. Given an arbitrary strictly convex function
+ →R (of Legendre-type), called a Bregman function, one can deﬁne a
Bregman divergence (“generalized distance”):
∆G(x, y) := G(x) −G(y) −⟨∇G(y), x −y⟩.
AdaBoost, Logistic regression and many other algorithms can be understood
in this framework as algorithms that iteratively project onto diﬀerent hyper-
R. Meir and G. R¨atsch
planes.11 A precursor of such algorithms is the Bregman algorithm and
it is known that the sequence {d(t)} converges to a point on the intersection
of all hyperplanes which minimizes the divergence to the ﬁrst point d(0) – in
AdaBoost the uniform distribution. Hence they solve the following optimization
problem:12
+ ∆G(d, d(0))
n=1 dnyn˜h(xn) = 0
An interesting questions is how generalized projections relate to coordinate
descent discussed in Section 5.2. It has been shown (see also )
that a generalized projection in the dual domain onto a hyperplane (deﬁned by
h; cf. (37)) corresponds to the optimization of the variable corresponding to h in
the primal domain. In particular, if one uses a Bregman function G(d), then this
corresponds to a coordinate-wise descent in the loss function G(J
j=1 wj˜hj(X)+
(∇G)−1(d(0))), where ˜hj(X) = (˜hj(x1), . . . , ˜hj(xN)) and G : RN →R is the
convex conjugate of G.13 In the case of AdaBoost one uses G(d) = 
n dn log dn,
leading to the relative entropy in (36) and (∇G)−1(d(0)) is zero, if d(0) is uniform.
Convergence Results
There has recently been a lot of work on establishing the convergence of several
algorithms, which are all very similar to Algorithm 5.2. Since we cannot discuss
all approaches in detail, we only provide an overview of diﬀerent results which
have been obtained in the past few years (cf. Table 1). In the following few
paragraphs we brieﬂy discuss several aspects which are important for proving
convergence of such algorithms.
Conditions on the Loss Function. In order to prove convergence to a global
minimum one usually has to assume that the loss function is convex (then every
local minimum is a global one). If one allows arbitrary loss functions, one can only
show convergence to a local minimum (i.e. gradient converges to zero, e.g. ).
Moreover, to prove convergence one usually assumes the function is reasonably
smooth, where some measure of smoothness often appears in convergence rates.
For some results it is important that the second derivatives are strictly positive,
i.e. the loss function is strictly convex (“function is not ﬂat”) – otherwise, the
algorithm might get stuck.
11 In a linear programming approach (“column generation”) for simultaneously
projection onto a growing set of hyperplanes has been considered. Some details are
described in Section 6.2.
12 These algorithms also work, when the equality constraints in (37) are replaced by
inhomogeneous inequality constraints: Then one only projects to the hyperplane,
when the constraint is not already satisﬁed.
13 In the case of Bregman functions, the convex conjugate is given by G(o) =
⟨(∇G)−1(o), o⟩−G((∇G)−1(o)).
An Introduction to Boosting and Leveraging
Relaxing Conditions on the Base learner. In Section 5.2 we made the
assumption that the base learner always returns the hypothesis maximizing the
edge (cf. (34)). In practice, however, it might be diﬃcult to ﬁnd the hypothesis
that exactly minimizes the training error (i.e. maximizes the edge). Hence, some
relaxations are necessary. One of the simplest approaches is to assume that the
base learner always returns a hypothesis with some edge larger that say γ (“γrelaxed”, e.g. ). This condition was used to show that AdaBoost’s training
error converges exponentially to zero. However, since the edge of a hypothesis
is equal to the gradient with respect to its weight, this would mean that the
gradient will not converge to zero. In the case of AdaBoost this actually happens
and one converges to the minimum of the loss, but the length of the solution
vector does not converge (cf. Section 3.2). More realistically one might assume
that the base learner returns a hypothesis that approximately maximizes the
edge – compared to the best possible hypothesis. considered an additive
term converging to zero (“α-relaxed”) and used a multiplicative term (“βrelaxed”). We summarize some senses of approximate edge-maximization (e.g.
 )14
α-relaxed:
nht(xn) ≥argmax
γ-relaxed:
nht(xn) ≥γ,
τ-relaxed:
nht(xn) ≥τ
β-relaxed:
nht(xn) ≥β argmax
for some ﬁxed constants α > 0, β > 0, γ > 0 and some strictly monotone and
continuous function τ : R →R with τ(0) = 0.
Size of the Hypothesis Set. For most convergence results the size of the
hypothesis set does not matter. However, in order to be able to attain the minimum, one has to assume that the set is bounded and closed (cf. ). In
 ﬁnite hypothesis sets have been assumed in order to show convergence and rates. In the case of classiﬁcation, where the base hypotheses are
discrete valued, this holds true. However, in practice, one often wants to use
real valued, parameterized hypotheses. Then the hypothesis set is uncountable.
 presents rather general convergence results for an algorithm related
to that in Algorithm 5.2, which do not depend on the size of the hypothesis sets
and is applicable to uncountable hypothesis sets.
14 Note that the label yn may be absorbed into the d’s.
R. Meir and G. R¨atsch
Regularization Terms on the Hypothesis Coeﬃcients. In Section 6 we
will discuss several regularization approaches for leveraging algorithms. One important ingredient is a penalty term on the hypothesis coeﬃcients, i.e. one seeks
to optimize
g(fw(xn), yn) + CP(w)
where C is the regularization constant and P is some regularization operator
(e.g. some ℓp-norm). We have seen that AdaBoost implicitly penalizes the ℓ1norm (cf. Section 4.3); this is also true for the algorithm described by . The use of the ℓ2-norm has been discussed in . Only a
small fraction of the analyses found in the literature allow the introduction of a
regularization term. Note, when using regularized loss functions, the optimality
conditions for the base learner change slightly (there will be one additional term
depending on the regularization constant, cf. ). Some regularization
functions will be discussed in Section 6.3.
Convergence Rates. The ﬁrst convergence result was obtained for AdaBoost and the γ-relaxed condition on the base learner. It showed that the
objective function is reduced at every step by a factor. Hence, it decreases exponentially. Here, the convergence speed does not directly depend on the size
of the hypothesis class and the number of examples. Later this was generalized
to essentially exponentially decreasing functions (like in Logistic Regression, but
still α-relaxed condition, cf. ). Also, for ﬁnite hypothesis sets and strictly convex loss one can show the exponential decrease of the loss towards its minimum
 (see also ) – the constants, however, depend heavily on the size of the
training set and the number of base hypotheses. The best known rate for convex
loss functions and arbitrary hypothesis sets is given in . Here the rate is
O(1/t) and the constants depend only on the smoothness of the loss function.
In this approach it is mandatory to regularize with the ℓ1-norm. Additionally,
the proposed algorithm does not exactly ﬁt into the scheme discussed above (see
 for details).
Convexity with respect to parameters. In practical implementations of
Boosting algorithms, the weak learners are parameterized by a ﬁnite set of parameters θ = (θ1, . . . , θs), and the optimization needs to be performed with
respect to θ rather than with respect to the weak hypothesis h directly. Even if
the loss function G is convex with respect to h, it is not necessarily convex with
respect to θ. Although this problem may cause some numerical problems, we do
not dwell upon it in this review.
Robustness, Regularization, and Soft-Margins
It has been shown that Boosting rarely overﬁts in the low noise regime (e.g.
 ); however, it clearly does so for higher noise levels (e.g. [145, 27, 81,
An Introduction to Boosting and Leveraging
Table 1. Summary of Results on the Convergence of Greedy Approximation Methods
Ref. Cost Function Base
Hypothesis
Convergence
Regularization
 exponential
γ-relaxed uncountable global minimum,
if loss zero
 Lipschitz
differentiable
uncountable gradient zero
 convex,
Lipschitz diﬀerentiable
uncountable global minimum no
 convex,
diﬀerentiable
global minimum no
 essentially
decreasing
γ-relaxed uncountable global minimum,
if loss zero
 strictly
diﬀerentiable⋆⋆⋆
(dual problem)
 linear⋆
global minimum no
 squared
γ-relaxed uncountable global minimum
 strictly
diﬀerentiable⋆⋆⋆
β-relaxed ﬁnite
global minimum,
unique dual sol.
 linear⋆
β-relaxed uncountable,
global minimum no
 strictly convex τ-relaxed uncountable,
global minimum no
 convex
uncountable global inﬁmum
⋆This also includes piecewise linear, convex functions like the soft-margin or
the ε-insensitive loss.
⋆⋆Extended to τ-relaxed in .
⋆⋆⋆There are a few more technical assumptions. These functions are usually refereed to as functions of Legendre-type .
153, 127, 12, 51]). In this section, we summarize techniques that yield state-ofthe-art results and extend the applicability of boosting to the noisy case.
The margin distribution is central to the understanding of Boosting. We
have shown in Section 4.3 that AdaBoost asymptotically achieves a hard margin
separation, i.e. the algorithm concentrates its resources on a few hard-to-learn
examples. Since a hard margin is clearly a sub-optimal strategy in the case of
noisy data, some kind of regularization must be introduced in the algorithm to
R. Meir and G. R¨atsch
alleviate the distortions that single diﬃcult examples (e.g. outliers) can cause
the decision boundary.
We will discuss several techniques to approach this problem. We start with
algorithms that implement the intuitive idea of limiting the inﬂuence of a single
example. First we present AdaBoostReg which trades oﬀthe inﬂuence with
the margin, then discuss BrownBoost which gives up on examples for which
one cannot achieve large enough margins within a given number of iterations.
We then discuss SmoothBoost which prevents overﬁtting by disallowing
overly skewed distributions, and ﬁnally brieﬂy discuss some other approaches of
the same ﬂavor.
In Section 6.2 we discuss a second group of approaches which are motivated
by the margin-bounds reviewed in Section 3.4. The important insight is that
it is not the minimal margin which is to be maximized, but that the whole
margin distribution is important: one should accept small amounts of marginerrors on the training set, if this leads to considerable increments of the margin.
First we describe the DOOM approach that uses a non-convex, monotone
upper bound to the training error motivated from the margin-bounds. Then we
discuss a linear program (LP) implementing a soft-margin and outline
algorithms to iteratively solve the linear programs .
The latter techniques are based on modifying the AdaBoost margin loss
function to achieve better noise robustness. However, DOOM as well as the
LP approaches employ ℓ∞-margins, which correspond to a ℓ1-penalized cost
function. In Section 6.3 we will discuss some issues related to other choices of
the penalization term.
Reducing the Inﬂuence of Examples
The weights of examples which are hard to classify are increased at every step
of AdaBoost. This leads to AdaBoost’s tendency to exponentially decrease the
training error and to generate combined hypotheses with large minimal margin.
To discuss the suboptimal performance of such a hard margin classiﬁer in the
presence of outliers and mislabeled examples in a more abstract way, we analyze
Figure 6. Let us ﬁrst consider the noise free case [left]. Here, we can estimate a
separating hyperplane correctly. In Figure 6 [middle] we have one outlier, which
corrupts the estimation. Hard margin algorithms will concentrate on this outlier
and impair the good estimate obtained in the absence of the outlier. Finally,
let us consider more complex decision boundaries (cf. Figure 6 [right]). Here the
overﬁtting problem is even more pronounced, if one can generate more and more
complex functions by combining many hypotheses. Then all training examples
(even mislabeled ones or outliers) can be classiﬁed correctly, which can result in
poor generalization.
From these cartoons, it is apparent that AdaBoost and any other algorithm
with large hard margins are noise sensitive. Therefore, we need to relax the hard
margin and allow for a possibility of “mistrusting” the data. We present several
algorithms which address this issue.
An Introduction to Boosting and Leveraging
Fig. 6. The problem of ﬁnding a maximum margin “hyperplane” on reliable data [left],
data with outlier [middle] and with a mislabeled example [right]. The solid line shows
the resulting decision boundary, whereas the dashed line marks the margin area. In
the middle and on the left the original decision boundary is plotted with dots. The
hard margin implies noise sensitivity, because only one example can spoil the whole
estimation of the decision boundary. (Figure taken from .)
AdaBoostReg. Examples that are mislabeled and usually more diﬃcult to classify should not be forced to have a positive margin. If one knew beforehand which
examples are “unreliable”, one would simply remove them from the training set
or, alternatively, not require that they have a large margin. Assume one has
deﬁned a non-negative mistrust parameter ζn, which expresses the “mistrust” in
an example (xn, yn). Then one may relax the hard margin constraints (cf. (23)
and (24)) leading to:
ρn(w) ≥ρ −Cζn,
n = 1, . . . , N
where ρn(w) = yn⟨xn, w⟩/∥w∥1, C is an a priori chosen constant.15 Now one
could maximize the margin using these modiﬁed constraints (cf. Section 4).
Two obvious questions arise from this discussion: ﬁrst, how can one determine
the ζ’s and second, how can one incorporate the modiﬁed constraints into a
boosting-like algorithm. In the AdaBoostReg algorithm one tries to solve
both problems simultaneously. One uses the example weights computed at each
iteration to determine which examples are highly inﬂuential and hard to classify.
Assuming that the hard examples are “noisy examples”, the algorithm chooses
the mistrust parameter at iteration t, ζ(t)
n , as the amount by which the example
(xn, yn) inﬂuenced the decision in previous iterations:
r=1 αrd(r)
15 Note that we use w to mark the parameters of the hyperplane in feature space and
α to denote the coeﬃcients generated by the algorithm (see Section 4.1).
R. Meir and G. R¨atsch
where the αt’s and d(t)
n ’s are the weights for the hypotheses and examples, respectively. Moreover, one deﬁnes a new quantity, the soft margin ˜ρn(w) :=
ρn(w) + Cζn of an example (xn, yn), which is used as a replacement of the margin in the AdaBoost algorithm. So, the idea in AdaBoostReg is to ﬁnd a solution
with large soft margin on all examples, i.e. maximize ρ w.r.t. w and ρ such that
˜ρn(w) ≥ρ, n = 1, . . . , N. In this case one expects to observe less overﬁtting.
One of the problems of AdaBoostReg is that it lacks a convergence proof.
Additionally, it is not clear what the underlying optimization problem is. The
modiﬁcation is done at an algorithmic level, which makes it diﬃcult to relate
to an optimization problem. Nevertheless, it was one of the ﬁrst boosting-like
algorithm that achieved state-of-the art generalization results on noisy data (cf.
 ). In experimental evaluations, it was found that this algorithm is among
the best performing ones (cf. ).
BrownBoost. The BrownBoost algorithm is based on the Boosting by
Majority (BBM) algorithm . An important diﬀerence between BBM and
AdaBoost is that BBM uses a pre-assigned number of boosting iterations. As
the algorithm approaches its predetermined termination, it becomes less and
less likely that examples which have large negative margins will eventually become correctly labeled. Then, the algorithm “gives up” on those examples and
concentrates its eﬀort on those examples whose margin is, say, a small negative
number . Hence, the inﬂuence of the most diﬃcult examples is reduced in
the ﬁnal iterations of the algorithm. This is similar in spirit to the AdaBoostReg
algorithm, where examples which are diﬃcult to classify are assigned reduced
weights in later iterations.
To use BBM one needs to pre-specify two parameters: (i) an upper bound
on the error of the weak learner ε ≤1
2γ (cf. Section 3.1) and (ii) a “target
error” ϵ > 0. In it was shown that one can get rid of the γ-parameter as in
AdaBoost. The target error ϵ is interpreted as the training error one wants to
achieve. For noisy problems, one should aim for non-zero training error, while
for noise free and separable data one can set ϵ to zero. It has been shown that
letting ϵ in the BrownBoost algorithm approach zero, one recovers the original
AdaBoost algorithm.
To derive BrownBoost, the following “thought experiment” – inspired by
ideas from the theory of Brownian motion – is made: Assume the base learner
returns a hypothesis h with edge larger than γ. Fix some 0 < δ < γ and deﬁne
a new hypothesis h′ which is equal to h with probability δ/γ and random otherwise. It is easy to check that the expected edge of h′ is δ. Since the edge is
small, the change of the combined hypothesis and the example weights will be
small as well. Hence, the same hypothesis may have an edge greater than δ for
several iterations (depending on δ) and one does not need to call the base learner
again. The idea in BrownBoost is to let δ approach zero and analyze the above
“dynamics” with diﬀerential equations. The resulting BrownBoost algorithm is
very similar to AdaBoost but has some additional terms in the example weighting, which depend on the remaining number of iterations. In addition, in each
iteration one needs to solve a diﬀerential equation with boundary conditions to
compute how long a given hypothesis “survives” the above described cycle, which
An Introduction to Boosting and Leveraging
determines its weight. More details on this algorithm and theoretical results on
its performance can be found in . Whereas the idea of the algorithm seems
to be very promising, we are not aware of any empirical results illustrating the
eﬃciency of the approach.
SmoothBoost. The skewness of the distributions generated during the Boosting process suggests that one approach to reducing the eﬀect of overﬁtting is
to impose limits on this skewness. A promising algorithm along these lines was
recently suggested in following earlier work . The SmoothBoost algorithm was designed to work eﬀectively with malicious noise, and is provably
eﬀective in this scenario, under appropriate conditions. The algorithm is similar
to AdaBoost in maintaining a set of weights d(t)
at each boosting iteration, except that there is a cutoﬀof the weight assigned to examples with very negative
margins. The version of SmoothBoost described in requires two parameters as input: (i) κ, which measures the desired error rate of the ﬁnal classiﬁer,
(ii) and γ which measures the guaranteed edge of the hypothesis returned by
the weak learner. Given these two parameters SmoothBoost can be shown to
converge within a ﬁnite number of steps to a composite hypothesis f such that
|{n : ynf(xn) ≤θ}| < κN, where θ ≤κ. Moreover, it can be shown that the
weights generated during the process obey d(t)
≤1/κN, namely the weights
cannot become too large (compare with the ν-LP in Section 6.2). Although the
convergence time of SmoothBoost is larger than AdaBoost, this is more than
compensated for by the robustness property arising from the smoothness of the
distribution. We observe that SmoothBoost operates with binary or real-valued
hypotheses.
While SmoothBoost seems like a very promising algorithm for dealing with
noisy situations, it should be kept in mind that it is not fully adaptive, in that
both κ and γ need to be supplied in advance (cf. recent work by ). We are
not aware of any applications of SmoothBoost to real data.
Other approaches aimed at directly reducing the eﬀect of diﬃcult examples
can be found in .
Optimization of the Margins
Let us return to the analysis of AdaBoost based on margin distributions as
discussed in Section 3.4. Consider a base-class of binary hypotheses H, characterized by VC dimension VCdim(H). From (16) and the fact that RN(H) =
VCdim(H)/N) we conclude that with probability at least 1 −δ over the
random draw of a training set S of size N the generalization error of a function
f ∈co(H) with margins ρ1, . . . , ρN can be bounded by
I(ρn ≤θ) + O
where θ ∈(0, 1] (cf. Corollary 1). We recall again that, perhaps surprisingly, the
bound in (43) is independent of the number of hypotheses ht that contribute
R. Meir and G. R¨atsch
to deﬁning f = 
t αtht ∈co(H). It was stated that a reason for the success
of AdaBoost, compared to other ensemble learning methods (e.g. Bagging ),
is that it generates combined hypotheses with large margins on the training
examples. It asymptotically ﬁnds a linear combination fw of base hypotheses
satisfying
ρn(w) = ynfw(xn)
n = 1, . . . , N,
for some large margin ρ (cf. Section 4). Then the ﬁrst term of (43) can be made
zero for θ = ρ and the second term becomes small, if ρ is large. In 
algorithms have been proposed that generate combined hypotheses with even
larger margins than AdaBoost. It was shown that as the margin increases, the
generalization performance can become better on data sets with almost no noise
(see also ). However, on problems with a large amount of noise,
it has been found that the generalization ability often degrades for hypotheses
with larger margins (see also ).
In Section 4 we have discussed connections of boosting to margin maximization. The algorithms under consideration approximately solve a linear programming problem, but tend to perform sub-optimally on noisy data. From the margin
bound (43), this is indeed not surprising. The minimum on the right hand side
of (43) is not necessarily achieved with the maximum (hard) margin (largest θ).
In any event, one should keep in mind that (43) is only an upper bound, and
that more sophisticated bounds (e.g. ), based on looking at the full
margin distribution as opposed to the single hard margin θ, may lead to diﬀerent
conclusions.
In this section we discuss algorithms, where the number of margin errors can
be controlled, and hence one is able to control the contribution of both terms in
(43) separately. We ﬁrst discuss the DOOM approach and then present an
extended linear program – the ν-LP problem – which allows for margin errors.
Additionally, we brieﬂy discuss approaches to solve the resulting linear program
 .
DOOM. (Direct Optimization Of Margins) The basic idea of is
to replace AdaBoost’s exponential loss function with another loss function with
ℓ1-normalized hypothesis coeﬃcients. In order to do this one deﬁnes a class of
B-admissible margin cost functions, parameterized by some integer M.
These loss functions are motivated from the margin bounds of the kind discussed in Section 3.4, which have a free parameter θ. A large value of M ∼
corresponds to a “high resolution” and a high eﬀective complexity of the convex
combination (small margin θ), whereas smaller values of M correspond to larger
θ and therefore smaller eﬀective complexity.
The B-admissibility condition ensures that the loss functions appropriately
take care of the trade-oﬀbetween complexity and empirical error. Following
some motivation (which we omit here), derived an optimal family
of margin loss functions (according to the margin bound). Unfortunately, this
loss function is non-monotone and non-convex (cf. Figure 7, [left]), leading to a
very diﬃcult optimization problem (NP hard).
An Introduction to Boosting and Leveraging
The idea proposed in is to replace this loss function with a piece-wise
linear loss function, that is monotone (but non-convex, cf. Figure 7, [right]):
: −1 ≤z ≤0,
: 0 ≤z ≤θ,
0.1(1 −z)/(1 −θ)
: θ ≤z ≤1.
The optimization of this margin loss function to a global optimum is still very
diﬃcult, but good heuristics for optimization have been proposed .
Theoretically, however, one can only prove convergence to a local minimum (cf.
Section 5.4).
Fig. 7. [left] The “optimal” margin loss functions CM(z), for M = 20, 50 and 200,
compared to the 0/1-loss. [right] Piecewise linear upper bound on the functions CM(z)
and the 0/1-loss (Figure taken from .)
Despite the above mentioned problems the DOOM approach seems very
promising. Experimental results have shown that, in noisy situations, one can
considerably improve the performance compared to AdaBoost. However, there
is the additional regularization parameter θ (or M), which needs to be chosen
appropriately (e.g. via cross validation).
Linear Programming Approaches. We now discuss ideas aimed at directly
controlling the number of margin errors. Doing so, one is able to directly control
the contribution of both terms on the r.h.s. of (43) separately. We ﬁrst discuss
an extended linear program, the ν-LP problem, analyze its solution and review
a few algorithms for solving this optimization problem.
The ν-LP Problem. Let us consider the case where we have given a set H = {hj :
x →[−1, 1], j = 1, . . . , J} of J hypotheses. To ﬁnd the coeﬃcients w for the
combined hypothesis fw(x) in (21), we extend the linear problem (30) and solve
the following linear optimization problem (see also ), which we call the
ν-LP problem:
R. Meir and G. R¨atsch
ynfw(xn) ≥ρ −ξn
n = 1, . . . , N
j = 1, . . . , J,
n = 1, . . . , N
j=1 wj = 1,
where ν ∈(1/N, 1] is a parameter of the algorithm. Here, one does not force all
margins to be large and obtains a soft margin hyperplane.16 The dual optimization problem of (45) (cf. (46)) is the same as the edge minimization problem
(20) with one additional constraint: dn ≤
νN . Since dn is the Lagrange multiplier associated with the constraint for the n-th example, its size characterizes
how much the example inﬂuences the solution of (45). In this sense, the ν-LP
also implements the intuition discussed in Section 6.1. In addition, there seems
to be a direct connection to the SmoothBoost algorithm.
The following proposition shows that ν has an immediate interpretation:
Proposition 2 (ν-Property, e.g. ). The solution to the optimization problem (45) possesses the following properties:
1. ν upper-bounds the fraction of margin errors.
2. 1 −ν is greater than the fraction of examples with a margin larger than ρ.
Since the slack variables ξn only enter the cost function linearly, their gradient is constant (once ξn > 0) and the absolute size is not important. Loosely
speaking, this is due to the fact that for the optimum of the primal objective
function, only derivatives w.r.t. the primal variables matter, and the derivative
of a linear function is constant. This can be made more explicit: Any example
outside the margin area, i.e. satisfying ynfw(xn) > ρ, can be shifted arbitrarily,
as long as it does not enter the margin area. Only if the example is exactly on the
edge of the margin area, i.e. ynfw(xn) = ρ, then (almost) no local movement is
possible without changing the solution. If the example (xn, yn) is in the margin
area, i.e. ξn > 0, then it has been shown that almost any local movements are
allowed (cf. for details).
A Column Generation Approach. Recently, an algorithm for solving (45) has
been proposed (see also in the early work of ). It uses the Column Generation (CG) method known since the 1950’s in the ﬁeld of numerical optimization
(e.g. , Section 7.4). The basic idea of column generation is to iteratively construct the optimal ensemble for a restricted subset of the hypothesis set, which is
iteratively extended. To solve (45), one considers its dual optimization problem:
n=1 yndnhj(xn) ≤γ
j = 1, . . . , J
n=1 dn = 1
n = 1, . . . , N.
16 See Figure 4 for the soft margin loss, here one uses a scaled version.
An Introduction to Boosting and Leveraging
At each iteration t, (46) is solved for a small subset of hypotheses Ht ⊆H.
Then the base learner is assumed to ﬁnd a hypothesis ht that violates the ﬁrst
constraint in (46) (cf. τ-relaxation in Section 5.4). If there exists such an hypothesis, then it is added to the restricted problem and the process is continued.
This corresponds to generating a column in the primal LP (45). If all hypotheses
satisfy their constraint, then the current dual variables and the ensemble (primal
variables) are optimal, as all constraints of the full master problem are fulﬁlled.
The resulting algorithm is a special case of the set of algorithms known as
exchange methods (cf. , Section 7 and references therein). These methods are
known to converge (cf. for ﬁnite and inﬁnite hypothesis sets). To the
best of our knowledge, no convergence rates are known, but if the base learner
is able to provide “good” hypotheses/columns, then it is expected to converge
much faster than simply solving the complete linear program. In practice, it
has been found that the column generation algorithm halts at an optimal solution in a relatively small number of iterations. Experimental results in 
show the eﬀectiveness of the approach. Compared to AdaBoost, one can achieve
considerable improvements of the generalization performance .
A Barrier Algorithm. Another idea is to derive a barrier algorithm for (45) along
the lines of Section 4.4. Problem (45) is reformulated: one removes the constant
νN in the objective of (45), ﬁxes ρ = 1 and adds the sum of the hypothesis
coeﬃcients multiplied with another regularization constant C (instead of
One can in fact show that the modiﬁed problem has the same solution as (45):
for any given ν one can ﬁnd a C such that both problems have the same solution
(up to scaling) . The corresponding barrier objective Fβ for this problem
can be derived as in Section 4.4. It has two sets of parameters, the combination
coeﬃcients w and the slack variables ξ ∈RN. By setting ∇ξ Fβ = 0, we can
ﬁnd the minimizing ξ for given β and w, which one plugs in and obtains
"1 −ynfw(xn)
If one sets β = 1 and C = 0 (i.e. if we do not regularize), then one obtains a
formulation which is very close to the logistic regression approach as in .
The current approach can indeed be understood as a leveraging algorithm as in
Section 5.2 with regularization. Furthermore, if we let β approach zero, then the
scaled logistic loss in (47) converges to the soft-margin loss N
n=1 max(0, 1 −
ynfw(xn)).
Using the techniques discussed in Section 5, one can easily derive algorithms
that optimize (47) for a ﬁxed β up to a certain precision. Here one needs to
take care of the ℓ1-norm regularization, which can be directly incorporated into
a coordinate descent algorithm (e.g. ). The idea is to reduce β, when a
certain precision is reached and then the optimization is continued with the
modiﬁed loss function. If one chooses the accuracy and the rate of decrease
of β appropriately, one obtains a practical algorithm, for which one can show
asymptotic convergence (for ﬁnite hypothesis sets see ).
R. Meir and G. R¨atsch
An earlier realization of the same idea was proposed in , and termed
ν-Arc. The idea was to reformulate the linear program with soft margin into a
non-linear program with maximum hard margin, by appropriately redeﬁning the
margin (similar to AdaBoostReg in Section 6.1). Then, as a heuristic, Arc-GV
(cf. Section 4.3) was employed to maximize this newly deﬁned margin.17
The barrier approach and ν-Arc led to considerable improvements compared
to AdaBoost on a large range of benchmark problems . Since they
try to solve a similar optimization problem as the column generation algorithm,
one would expect very minor diﬀerences. In practice, the column generation
algorithm often converges faster, however, it has a problem in combination with
some base learners : In the CG approach, most of the example weights will be
zero after a few iterations (since the margin in these examples is larger than ρ and
the margin constraints are not active). Then the base learner may have problems
to generate good hypotheses. Since the barrier approach is much “smoother”,
this eﬀect is reduced considerably.
Regularization Terms and Sparseness
We will now discuss two main choices of regularizers that have a drastic inﬂuence
on the properties of the solution of regularized problems. Let us consider optimization problems as in Section 5 with a regularization term in the objective:18
g(fw(xn), yn) + C
s.t. 0 ≤w ∈RJ,
where C is a regularization constant, g is a loss function and p : R+ →R+ is a
diﬀerentiable and monotonically increasing function with p(0) = 0. For simplicity
of the presentation we assume the hypothesis set is ﬁnite, but the statements
below also hold for countable hypothesis sets; in the case of uncountable sets
some further assumptions are required (cf. , Theorem 4.2, and [149, 148,
We say a set of hypothesis coeﬃcients (see (21)) is sparse, if it contains
O(N) non-zero elements. The set is not sparse if it contains e.g. O(J) non-zero
elements, since the feature space is assumed to be much higher dimensional than
N (e.g. inﬁnite dimensional). We are interested in formulations of the form (48)
for which the optimization problem is tractable, and which, at the same time,
lead to sparse solutions. The motivation for the former aspect is clear, while the
motivation for sparsity is that it often leads to superior generalization results
(e.g. ) and also to smaller, and therefore computationally more eﬃcient,
ensemble hypotheses. Moreover, in the case of inﬁnite hypothesis spaces, sparsity
leads to a precise representation in terms of a ﬁnite number of terms.
17 A Matlab implementation can be downloaded at
 
18 Here we force the w’s to be non-negative, which can be done without loss of generality, if the hypothesis set is closed under negation.
An Introduction to Boosting and Leveraging
Let us ﬁrst consider the ℓ1-norm as regularizer, i.e. p(wj) = wj. Assume further that w∗is the optimal solution to (48) for some C ≥0. Since the regularizer
is linear, there will be a (J −1)-dimensional subspace of w’s having the same
ℓ1-norm as w∗. By further restricting fw(xn) = fw∗(xn), n = 1, . . . , N, one
obtains N additional constraints. Hence, one can choose a w from a J −N −1
dimensional space that leads to the same objective value as w∗. Therefore, there
exists a solution that has at most N +1 non-zero entries and, hence, the solution
is sparse. This observations holds for arbitrary loss functions and any concave
regularizer such as ℓ1-norm . Note that N + 1 is an upper bound on the
number of non-zero elements – in practice much sparser solutions are observed
 .
In neural networks, SVMs, matching pursuit (e.g. ) and many other
algorithms, one uses the ℓ2-norm for regularization. In this case the optimal
solution w∗can be expressed as a linear combination of the mapped examples
in feature space (cf. (26)),
Hence, if the vectors Φ(xn) are not sparse and the β’s are not all zero, the solution
w∗is also unlikely to be sparse (since it is a linear combination of non-sparse vectors). Consider, for instance, the case where the J vectors (hj(x1), . . . , hj(xN))
(j = 1, . . . , J), interpreted as points in an N dimensional space are in general
position (any subset of N points span the full N dimensional space. For instance,
this occurs with probability 1 for points drawn independently at random from
a probability density supported over the whole space). If J ≫N, then one can
show that there is a fraction of at most O
coeﬃcients that are zero.
Hence, for large J almost all coeﬃcients are non-zero and the solution is not
sparse. This holds not only for the ℓ2-norm regularization, but for any other
strictly convex regularizer . This observation can be intuitively explained
as follows: There is a J −N dimensional subspace leading to the same output
fw(xn) on the training examples. Assume the solution is sparse and one has a
small number of large weights. If the regularizer is strictly convex, then one can
reduce the regularization term by distributing large weights over many other
weights which were previously zero (while keeping the loss term constant).
We can conclude that the type of regularizer determines whether there exists an optimal hypothesis coeﬃcient vector that is sparse or not. Minimizing a
convex loss function with a strictly concave regularizer will generally lead to nonconvex optimization problems with potentially many local minima. Fortunately,
the ℓ1-norm is concave and convex. By employing the ℓ1-norm regularization,
one can obtain sparse solutions while retaining the computational tractability of
the optimization problem. This observation seems to lend strong support to the
ℓ1-norm regularization.
R. Meir and G. R¨atsch
Extensions
The bulk of the work discussed in this review deals with the case of binary
classiﬁcation and supervised learning. However, the general Boosting framework
is much more widely applicable. In this section we present a brief survey of
several extensions and generalizations, although many others exist, e.g.
158, 62, 18, 31, 3, 155, 15, 44, 52, 82, 164, 66, 38, 137, 147, 16, 80, 185, 100, 14].
Single Class
A classical unsupervised learning task is density estimation. Assuming that the
unlabeled observations x1, . . . , xN were generated independently at random according to some unknown distribution P(x), the task is to estimate its related
density function. However, there are several diﬃculties to this task. First, a density function need not always exist — there are distributions that do not possess
a density. Second, estimating densities exactly is known to be a hard task. In
many applications it is enough to estimate the support of a data distribution
instead of the full density. In the single-class approach one avoids solving the
harder density estimation problem and concentrate on the simpler task, i.e. estimating quantiles of the multivariate distribution.
So far there are two independent algorithms to solve the problem in a
boosting-like manner. They mainly follow the ideas proposed in for
kernel feature spaces, but have the beneﬁt of better interpretability of the generated combined hypotheses (see discussion in ). The algorithms proposed in
 and diﬀer in the way they solve very similar optimization problems –
the ﬁrst uses column generation techniques whereas the latter uses barrier optimization techniques (cf. Section 6.2). For brevity we will focus on the underlying
idea and not the algorithmic details. As in the SVM case , one computes
a hyperplane in the feature space (here spanned by the hypothesis set H, cf.
Section 4.2) such that a pre-speciﬁed fraction of the training example will lie
beyond that hyperplane, while at the same time demand that the hyperplane
has maximal ℓ∞-distance (margin) to the origin – for an illustration see Figure 8.
This is realized by solving the following linear optimization problem:
w∈RJ,ξ∈RN,ρ∈R −νρ + 1
j=1 wjhj(xn) ≥ρ −ξn,
n = 1, . . . , N
∥w∥1 = 1, ξn ≥0,
n = 1, . . . , N
which is essentially the same as in the two-class soft-margin approach (cf. Section 6.2), but with all labels equal to +1. Hence, the derivation appropriate
optimization algorithms is very similar.
Multi-class
There has been much recent interest in extending Boosting to multi-class problems, where the number of classes exceeds two. Let S = {(x1, y1), . . . , (xN, yN)}
An Introduction to Boosting and Leveraging
νN outliers
Fig. 8. Illustration of single-class
idea. A hyperplane in the feature
space F is constructed that maximizes the distance to the origin
while allowing for ν outliers. (Figure taken from .)
be a data set, where, for a K-class problem, yn is a K-dimensional vector of the
form (0, 0, . . . , 0, 1, 0, . . . , 0) with a 1 in the k-th position if, and only if, xn
belongs to the k-th class. The log-likelihood function can then be constructed as
 (see also Page 84)
{yn,k log p(k|xn) + (1 −yn,k) log(1 −p(k|xn))} ,
where yn,k is the k’th component of the vector yn, and p(k|xn) is the model
probability that xn belongs to the k’th class. Using the standard softmax representation,
k′=1 eFk′(x) ,
and substituting in (50) we obtain a function similar in form to others considered
so far. This function can then be optimized in a stagewise fashion as described
in Section 5.2, yielding a solution to the multi-class problem. This approach was
essentially the one used in .
Several other approaches to multi-category classiﬁcation were suggested in
 and applied to text classiﬁcation in . Recall that in Boosting, the
weights of examples which are poorly classiﬁed by previous weak learners become ampliﬁed. The basic idea in the methods suggested in was to maintain
a set of weights for both examples and labels. As boosting progresses, training
examples and their corresponding labels that are hard to predict correctly get increasingly higher weights. This idea led to the development of two multi-category
algorithms, titled AdaBoost.MH and AdaBoost.MR. Details of these algorithms
can be found in .
In addition to these approaches, there has recently been extensive activity
on recoding multi-class problems as a sequence of binary problems, using the
R. Meir and G. R¨atsch
idea of error-correcting codes . A detailed description of the approach can
be found in in a general context as well as in the Boosting setup. Two special cases of this general approach are the so-called one-against-all and all-pairs
approaches, which were extensively used prior to the development of the error
correcting code approach. In the former case, given a k-class classiﬁcation problem, one constructs k (usually soft) binary classiﬁers each of which learns to
distinguish one class from the rest. The multi-category classiﬁer then uses the
highest ranking soft classiﬁer. In the all-pairs approach, all possible
classiﬁcation problems are learned and used to form the multi-category classiﬁer
using some form of majority voting. Using soft classiﬁers helps in removing possible redundancies. Connections of multi-class boosting algorithms with column
generations techniques are discussed in .
Finally, we comment that an interesting problem relates to situations where
the data is not only multi-class, but is also multi-labeled in the sense that each
input x may belong to several classes . This is particularly important in
the context of text classiﬁcation, where a single document may be relevant to
several topics, e.g. sports, politics and violence.
Regression
The learning problem for regression is based on a data set S = {(xn, yn)}N
where in contrast to the classiﬁcation case, the variable y can take on real values, rather than being restricted to a ﬁnite set. Probably the ﬁrst approach to
addressing regression in the context of Boosting appeared in , where the
problem was addressed by translating the regression task into a classiﬁcation
problem (see also ). Most of the Boosting algorithms for binary classiﬁcation described in this review, are based on a greedy stage-wise minimization of a
smooth cost function. It is therefore not surprising that we can directly use such a
smooth cost function in the context of regression where we are trying to estimate
a smooth function. Probably the ﬁrst work to discuss regression in the context of
stage-wise minimization appeared in (LS-Boost and other algorithms), and
later further extended by others (e.g. ). The work in further
addressed regression in connection with barrier optimization (essentially with a
two-sided soft-margin loss – the ϵ-insensitive loss). This approach is very similar in spirit to one of the approaches proposed in (see below). Later it was
extended in to arbitrary strictly convex loss functions and discussed
in connection with inﬁnite hypothesis sets and semi-inﬁnite programming. The
resulting algorithms were shown to be very useful in real world problems.
We brieﬂy describe a regression framework recently introduced in , where
several Boosting like algorithms were introduced. All these algorithms operate
using the following scheme, where diﬀerent cost functions are used for each algorithm. (i) At each iteration the algorithm modiﬁes the sample to produce a new
sample ˜S = {(x1, ˜y1), . . . , (xN, ˜yN)} where the y’s have been modiﬁed, based on
the residual error at each stage. (ii) A distribution d over the modiﬁed samples
˜S is constructed and a base regression algorithm is called. (iii) The base learner
produces a function f ∈F with some edge on ˜S under d (for a deﬁnition of
An Introduction to Boosting and Leveraging
an edge in the regression context see ). (iv) A new composite regressor of
the form F + αf is formed, where α is selected by solving a one-dimensional
optimization problem.
Theoretical performance guarantees for the regression algorithms introduced
in were analyzed in a similar fashion to that presented in Section 3 for
binary classiﬁcation. Under the assumption that a strictly positive edge was obtained at each step, it was shown that the training error converges exponentially
fast to zero (this is analogous to Theorem 1 for classiﬁcation). Signiﬁcantly, the
optimality of the base learners was not required for this claim, similarly to related results described in Section 5 for classiﬁcation. Finally, generalization error
bounds were derived in , analogous to Theorem 3. These bounds depend on
the edge achieved by the weak learner.
Localized Boosting
All of the Boosting approaches discussed in this review construct a composite
ensemble hypothesis of the form 
t αtht(x). Observe that the combination parameters αt do not depend on the input x, and thus contribute equally at each
point in space. An alternative approach to the construction of composite classi-
ﬁers would be to allow the coeﬃcients αt themselves to depend on x, leading to
a hypothesis of the form 
t αt(x)ht(x). The interpretation of this form of functional representation is rather appealing. Assume that each hypothesis ht(x)
represents and expert, while αt(x) assigns a non-negative weight which is attached to the prediction of the t-th expert, where we assume that 
t αt(x) = 1.
Given an input x, each of the experts makes a prediction ht(x), where the prediction is weighted by a ‘conﬁdence’ parameter αt(x). Note that if αt(x) are
indicator functions, then for each input x, there is a single hypothesis ht which
is considered. For linear hypotheses we can think of this approach as the representation of a general non-linear function by piece-wise linear functions. This
type of representation forms the basis for the so-called mixture of experts models
(e.g. ). An important observation concerns the complexity of the functions
αt(x). Clearly, by allowing these functions to be arbitrarily complex (e.g. δfunctions), we can easily ﬁt any ﬁnite data set. This stresses the importance of
regularization in any approach attempting to construct such a mixture of expert
representation.
A Boosting like approach to the construction of mixture of experts representations was proposed in and termed Localized Boosting. The basic observations in this work were the relationship to classic mixture models in statistics,
where the EM algorithm has proven very eﬀective, and the applicability of the
general greedy stagewise gradient descent approaches described in Section 5.2.
The algorithm developed in , termed LocBoost, can be thought of a stagewise EM algorithm, where similarly to Boosting algorithms a single hypothesis
is added on to the ensemble at each stage, and the functions αt(·) are also estimated at each step. Regularization was achieved by restricting αt to simple
parametric forms. In addition to the algorithm described in detail in , gen-
R. Meir and G. R¨atsch
eralization bounds similar to Theorem 3 were established. The algorithm has
been applied to many real-world data-sets leading to performance competitive
with other state-of-the-art algorithms.19
Other Extensions
We brieﬂy mention other extensions of Boosting algorithms. In a method
was introduced for learning multiple models that use diﬀerent (possibly overlapping) sets of features. In this way, more robust learning algorithms can be
constructed. An algorithm combining Bagging and Boosting, aimed at improving performance in the case of noisy data was introduced in . The procedure
simply generates a set of Bootstrap samples as in Bagging, generates a boosted
classiﬁer for each sample, and combines the results uniformly. An online version
of a Boosting algorithm was presented in , which was shown to be comparable in accuracy to Boosting, while much faster in terms of running time. Many
more extensions are listed at the beginning of the present section.
Evaluation and Applications
On the Choice of Weak Learners for Boosting
A crucial ingredient for the successful application of Boosting algorithms is the
construction of a good weak learner. As pointed out in Section 3, a weak learner
which is too weak cannot guarantee adequate performance of the composite
hypothesis. On the other hand, an overly complex weak learner may lead to
overﬁtting, which becomes even more severe as the algorithm proceeds. Thus,
as stressed in Section 6, regularization, in addition to an astute choice of weak
learner, plays a key role in successful applications.
Empirically we often observed that a base learner that already performs quite
well but are slightly too simple for the data at hand, are best suited for use with
boosting. When one uses bagging, then using base learners that are slightly too
complex perform best. Additionally, real-valued (soft) hypotheses often lead to
considerable better results.
We brieﬂy mention some weak learners which have been used successfully in
applications.
Decision trees and stumps. Decision trees have been widely used for many
years in the statistical literature (e.g. ) as powerful, eﬀective and
easily interpretable classiﬁcation algorithms that are able to automatically select
relevant features. Hence, it is not surprising that some of the most successful
initial applications of Boosting algorithms were performed using decision trees
as weak learners. Decision trees are based on the recursive partition of the input
19 A Matlab implementation of the algorithm, as part of an extensive Pattern Recognition toolbox, can be downloaded from
 
An Introduction to Boosting and Leveraging
space into a set of nested regions, usually of rectangular shape. The so-called
decision stump is simply a one-level decision tree, namely a classiﬁer formed
by splitting the input space in an axis-parallel fashion once and then halting. A
systematic approach to the eﬀective utilization of trees in the context of Boosting
is given in , while several other approaches based on logistic regression with
decision trees are described in . This work showed that Boosting signiﬁcantly
enhances the performance of decision trees and stumps.
Neural networks. Neural networks (e.g. ) were extensively used during
the 1990’s in many applications. The feed-forward neural network, by far the
most widely used in practice, is essentially a highly non-linear function representation formed by repeatedly combining a ﬁxed non-linear transfer function.
It has been shown in that any real-valued continuous function over Rd can
be arbitrarily well approximated by a feed-forward neural network with a single
hidden layer, as long as the transfer function is not a polynomial. Given the potential power of neural networks in representing arbitrary continuous functions,
it would seem that they could easily lead to overﬁtting and not work eﬀectively
in the context of Boosting. However, careful numerical experiments conducted
by demonstrated that AdaBoost can signiﬁcantly improve the performance
of neural network classiﬁers in real world applications such as OCR.
Kernel Functions and Linear combinations. One of the problems of using
neural networks as weak learners is that the optimization problems often become unwieldy. An interesting alternative is forming a weak learner by linearly
combining a set of ﬁxed functions, as in
The functions Gk could, for example, be kernel functions, i.e. Gk(x) = k(x, xk),
centered around the training examples. The set {hβ} is an inﬁnite hypothesis set
and is unbounded, if β is unbounded. Hence, maximizing the edge as discussed in
Section 5.4, would lead to diverging β’s. Keeping in mind the overﬁtting issues
discussed in Section 6, we attempt to restrict the complexity of the class of
functions by limiting the norm of β. We discuss two approaches.
ℓ1-norm Penalized Combinations. Here we set H := {hβ | ∥β∥1 ≤1, β ∈RN}.
Then ﬁnding the edge-maximizing β has a closed form solution. Let
k∗= argmax
k=1,... ,K
Then hβ with β = (0, . . . , 0, βk∗, 0, . . . , 0) maximizes the edge, where βk∗=
n=1 dnk(xk∗, xn)
. This means that we will be adding in exactly one
basis function gk∗(·) per iteration. Hence, this approach reduces to the case of
R. Meir and G. R¨atsch
using the single functions. A variant of this approach has been used in .
A related classiﬁcation function approach was used for solving a drug discovery
task in .
In so-called active kernel functions have been used, where the set of
functions were kernel-like functions hn,p(x) = kp(x, xn), and kp is a SVM kernel
function parameterized by p (e.g. the variance in a RBF kernel). In this case one
has to ﬁnd the example xn and the parameter p that maximize the edge. This
may lead to a hard non-convex optimization problem, for which quite eﬃcient
heuristics have been proposed in .
ℓ2-norm Penalized combinations. Another way is to penalize the ℓ2-norm of the
combination coeﬃcients. In this case H := {hγ | ∥γ∥2 ≤1, γ ∈RN}, and one
attempts to solve
γ = argmin
(dn −hγ(xn))2 + C∥γ∥2
The problem has a particularly simple solution: γk = 1
n=1 dngk(xn). A similar approach in the form of RBF networks has been used in in connection
with regularized versions of boosting.
Evaluation on Benchmark Data Sets
Boosting and Leveraging have been applied to many benchmark problems. The
AdaBoost algorithm and many of its early variants were tested on standard
data sets from the UCI repository, and often found to compare favorably with
other state of the art algorithms (see, for example, . However, it
was clear from that AdaBoost tends to overﬁt if the data is noisy
and no regularization is enforced. More recent experiments, using the regularized
forms of Boosting described in Section 6 lead to superior performance on noisy
real-world data. In fact, these algorithms often do signiﬁcantly better than the
original versions of Boosting, comparing very favorably with the best classiﬁers
available (e.g. SVMs). We refer the reader to for results of these benchmark
studies. These results place regularized boosting techniques into the standard
toolbox of data analysis techniques.
Applications
The list of applications of Boosting and Leveraging methods is growing rapidly.
We discuss three applications in detail and then list a range of other applications.
Non-intrusive Power Monitoring System.
One of the regularized approaches described in Section 6, ν-Arc, was applied to the problem of power
appliances monitoring . The most diﬃcult problem for power companies is
the handling of short-term peak loads for which additional power plants need
to be built to provide security against a peak load instigated power failure. A
An Introduction to Boosting and Leveraging
prerequisite for controlling the electric energy demand, however, is the ability
to correctly and non-intrusively detect and classify the operating status of electric appliances of individual households. The goal in was to develop such
a non-intrusive measuring system for assessing the status of electric appliances.
This is a hard problem, in particular for appliances with inverter systems,20
whereas non-intrusive measuring systems have already been developed for conventional on/oﬀ(non-inverter) operating electric equipments (cf. ). The
study in presents a ﬁrst evaluation of machine learning techniques to classify the operating status of electric appliances (with and without inverter) for the
purpose of constructing a non-intrusive monitoring system. In this study, RBF
networks, K-nearest neighbor classiﬁers (KNNs) (e.g. ), SVMs and ν-Arc (cf.
Section 6.2) were compared.
The data set available for this task is rather small (36 examples), since the
collection and labeling of data is manual and therefore expensive. As a result
of this, one has to be very careful with ﬁnding good model parameters. All
model parameters were found using the computationally expensive but generally
reliable leave-one-out method.
The results reported in demonstrate that the ν-Arc algorithm with RBF
networks as base learner performs better on average than all other algorithms
studied, followed closely by the SVM classiﬁer with a RBF kernel. These results
suggest that the goal of a control system to balance load peaks might be a
feasible prospect in the not too distant future.
Tumor Classiﬁcation with Gene Expression Data. Micro-array experiments generate large datasets with expression values for thousands of genes but
not more than a few dozen of examples. Accurate supervised classiﬁcation of
tissue samples in such high-dimensional problems is diﬃcult but often crucial
for successful diagnosis and treatment (in typical cases the sample size is in the
range of 20-100 and the number of features varies between 2,000 and 20,000;
clearly here the potential for overﬁtting is huge). The goal is is to predict the
unknown class label of a new individual on the basis of its gene expression pro-
ﬁle. Since this task is of great potential value, there have been many attempts
to develop eﬀective classiﬁcation procedures to solve it.
Early work applied the AdaBoost algorithm to this data; however, the results seemed to be rather disappointing. The recent work in applied the
LogitBoost algorithm , using decision trees as base learners, together with
several modiﬁcations, and achieved state of the art performance on this dif-
ﬁcult task. It turned out that in order to obtain high quality results, it was
necessary to preprocess the data by scoring each individual feature (gene) according to its discriminatory power using a non-parametric approach (details
can be found in ). Moreover, it was found that the simple one-against-all
approach to multi-category classiﬁcation led to much better results than the
direct multi-class approach presented in based on the log-likelihood function (cf. Section 7.2). Interestingly, the authors found that the quality of the
20 An inverter system controls for example the rotation speed of a motor (as in airconditioners) by changing the frequency of the electric current.
R. Meir and G. R¨atsch
results degraded very little as a function of the number of Boosting iterations
(up to 100 steps). This is somewhat surprising given the small amount of data
and the danger of overﬁtting. The success of the present approach compared to
results achieved by AdaBoost, tends to corroborate the assertions made in 
concerning the eﬀectiveness of the LogitBoost approach for noisy problems.
Text Classiﬁcation. The problem of text classiﬁcation is playing an increasingly important role due to the vast amount of data available over the web and
within internal company repositories. The problem here is particularly challenging since the text data is often multi-labeled, namely each text may naturally
fall into several categories simultaneously (e.g. Sports, Politics and Violence). In
addition, the diﬃculty of ﬁnding an appropriate representation for text is still
The work in presented one of the ﬁrst approaches to using Boosting for
text classiﬁcation. In particular, the approaches to multi-class multi-label classi-
ﬁcation developed in , were used for the present task. The weak learner used
was a simple decision stump (single level decision tree), based on terms consisting of single words and word pairs. The text categorization experiments reported
in were applied to several of the standard text classiﬁcation benchmarks
(Reuters, AP titles and UseNet groups) and demonstrated that the approach
yielded, in general, better results than other methods to which it was compared.
Additionally, it was observed that Boosting algorithms which used real-valued
(soft) weak learners performed better than algorithms using only binary weak
learners. A reported drawback of the approach was the very large time required
for training.
Other applications. We brieﬂy mention several applications of Boosting and
Leveraging methods in other problems.
The group at AT&T has been involved in many applications of Boosting
approaches beyond the text classiﬁcation task discussed above. For example, the
problems of text ﬁltering and routing were addressed as well as that of
ranking and combining references . More recently the problem of combining
prior knowledge and boosting for call classiﬁcation in spoken language dialogue
was studied in and applications to the problem of modeling auction price
uncertainty was introduced in .
Applications of boosting methods to natural language processing has been
reported in , and approaches to Melanoma Diagnosis are presented
in . Some further applications to Pose Invariant Face Recognition , Lung
Cancer Cell Identiﬁcation and Volatility Estimation for Financial Time
Series have also been developed.
A detailed list of currently known applications of Boosting and Leveraging methods will be posted on the web at the Boosting homepage
 
An Introduction to Boosting and Leveraging
Conclusions
We have presented a general overview of ensemble methods in general and the
Boosting and Leveraging family of algorithms in particular. While Boosting was
introduced within a rather theoretical framework in order to transform a poorly
performing learning algorithm into a powerful one, this idea has turned out to
have manifold extensions and applications, as discussed in this survey.
As we have shown, Boosting turns out to belong to a large family of models
which are greedily constructed by adding on a single base learner to a pool of
previously constructed learners using adaptively determined weights. Interestingly, Boosting was shown to be derivable as a stagewise greedy gradient descent
algorithm attempting to minimize a suitable cost function. In this sense Boosting is strongly related to other algorithms that were known within the Statistics
literature for many years, in particular additive models and matching pursuit . However, the recent work on Boosting has brought to the fore many
issues which were not studied previously. (i) The important concept of the margin, and its impact on learning and generalization has been emphasized. (ii)
The derivation of sophisticated ﬁnite sample data-dependent bounds has been
possible. (iii) Understanding the relationship between the strength of the weak
learner, and the quality of the composite classiﬁer (in terms of training and generalization errors). (iv) The establishment of consistency (v) The development
of computationally eﬃcient procedures.
With the emphasis of much of the Boosting work on the notion of the margin,
it became clear that Boosting is strongly related to another very successful current algorithm, namely the Support Vector Machine . As was pointed out
in , both Boosting and the SVM can be viewed as attempting to
maximize the margin, except that the norm used by each procedure is diﬀerent.
Moreover, the optimization procedures used in both cases are very diﬀerent.
While Boosting constructs a complex composite hypothesis, which can in
principle represent highly irregular functions, the generalization bounds for
Boosting turn out to lead to tight bounds in cases where large margins can
be guaranteed. Although initial work seemed to indicate that Boosting does not
overﬁt, it was soon realized that overﬁtting does indeed occur under noisy conditions. Following this observation, regularized Boosting algorithms were developed which are able to achieve the appropriate balance between approximation
and estimation required to achieve excellent performance even under noisy conditions. Regularization is also essential in order to establish consistency under
general conditions.
We conclude with several open questions.
1. While it has been possible to derive Boosting-like algorithms based on many
types of cost functions, there does not seem to be at this point a systematic
approach to the selection of a particular one. Numerical experiments and
some theoretical results indicate that the choice of cost function may have
a signiﬁcant eﬀect of the performance of Boosting algorithms (e.g. [74, 126,
2. The selection of the best type of weak learner for a particular task is also not
entirely clear. Some weak learners are unable even in principle to represent
R. Meir and G. R¨atsch
complex decision boundaries, while overly complex weak learners quickly
lead to overﬁtting. This problem appears strongly related to the notoriously
diﬃcult problem of feature selection and representation in pattern recognition, and the selection of the kernel in support vector machines. Note,
however, that the single weak learner in Boosting can include multi-scaling
information whereas in SVMs one has to ﬁx a kernel inducing the kernel
Hilbert space. An interesting question relates to the possibility of using very
diﬀerent types of weak learners at diﬀerent stages of the Boosting algorithm,
each of which may emphasize diﬀerent aspects of the data.
3. An issue related to the previous one is the question of the existence of weak
learners with provable performance guarantees. In Section 3.1 we discussed
suﬃcient conditions for the case of linear classiﬁers. The extension of these
results to general weak learners is an interesting and diﬃcult open question.
4. A great deal of recent work has been devoted to the derivation of ﬂexible
data-dependent generalization bounds, which depend explicitly on the algorithm used. While these bounds are usually much tighter than classic bounds
based on the VC dimension, there is still ample room for progress here, the
ﬁnal objective being to develop bounds which can be used for model selection in actual experiments on real data. Additionally, it would be interesting
to develop bounds or eﬃcient methods to compute the leave-one-out error
as done for SVMs in .
5. In the optimization section we discussed many diﬀerent approaches addressing the convergence of boosting-like algorithms. The result presented in 
is the most general so far, since it includes many special cases which have
been analyzed by others. However, the convergence rates in do not
seem to be optimal and additional eﬀort needs to be devoted to ﬁnding tight
bounds on the performance. In addition, there is the question of whether it is
possible to establish super-linear convergence for some variant of leveraging,
which ultimately would be lead to much more eﬃcient leveraging algorithms.
Finally, since many algorithms use parameterized weak learners, it is often
the case that the cost function minimized by the weak learners is not convex
with respect to the parameters (see Section 5.4). It would be interesting to
see whether this problem could be circumvented (e.g. by designing appropriate cost functions as in ).
Acknowledgements. We thank Klaus-R. M¨uller for discussions and his contribution to writing this manuscript. Additionally, we thank Shie Mannor, Sebastian Mika, Takashi Onoda, Bernhard Sch¨olkopf, Alex Smola and Tong Zhang
for valuable discussions. R.M. acknowledges partial support from the Ollendorﬀ
Center at the Electrical Engineering department at the Technion and from the
fund for promotion of research at the Technion. G.R. gratefully acknowledge
partial support from DFG (JA 379/9-1, MU 987/1-1), NSF and EU (NeuroColt
II). Furthermore, Gunnar R¨atsch would like to thank UC Santa Cruz, CRIEPI
in Tokyo and Fraunhofer FIRST in Berlin for their warm hospitality.
An Introduction to Boosting and Leveraging