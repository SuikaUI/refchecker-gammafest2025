MIT Open Access Articles
Contrastive Multiview Coding
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Tian, Yonglong, Krishnan, Dilip and Isola, Phillip. 2020. "Contrastive Multiview Coding."
Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence
and Lecture Notes in Bioinformatics), 12356.
As Published: 10.1007/978-3-030-58621-8_45
Publisher: Springer International Publishing
Persistent URL: 
Version: Author's final manuscript: final author's manuscript post peer review, without
publisher's formatting or copy editing
Terms of use: Creative Commons Attribution-Noncommercial-Share Alike
Contrastive Multiview Coding
Yonglong Tian
 
Dilip Krishnan
Google Research
 
Phillip Isola
 
Humans view the world through many sensory channels,
e.g., the long-wavelength light channel, viewed by the left
eye, or the high-frequency vibrations channel, heard by the
right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend
to be shared between all views (e.g., a “dog” can be seen,
heard, and felt). We investigate the classic hypothesis that
a powerful representation is one that models view-invariant
factors. We study this hypothesis under the framework of
multiview contrastive learning, where we learn a representation that aims to maximize mutual information between
different views of the same scene but is otherwise compact.
Our approach scales to any number of views, and is viewagnostic. We analyze key properties of the approach that
make it work, ﬁnding that the contrastive loss outperforms
a popular alternative based on cross-view prediction, and
that the more views we learn from, the better the resulting
representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video
unsupervised learning benchmarks. Code is released at:
 
1. Introduction
A foundational idea in coding theory is to learn compressed representations that nonetheless can be used to reconstruct the raw data. This idea shows up in contemporary
representation learning in the form of autoencoders and
generative models , which try to represent a data
point or distribution as losslessly as possible. Yet lossless
representation might not be what we really want, and indeed
it is trivial to achieve – the raw data itself is a lossless representation. What we might instead prefer is to keep the
“good” information (signal) and throw away the rest (noise).
How can we identify what information is signal and what is
To an autoencoder, or a max likelihood generative model,
a bit is a bit. No one bit is better than any other. Our conjecture in this paper is that some bits are in fact better than
others. Some bits code important properties like semantics, physics, and geometry, while others code attributes that
we might consider less important, like incidental lighting
Matching views
Unmatching view
Figure 1: Given a set of sensory views, a deep representation is
learnt by bringing views of the same scene together in embedding
space, while pushing views of different scenes apart. Here we show
and example of a 4-view dataset (NYU RGBD ) and its learned
representation. The encodings for each view may be concatenated
to form the full representation of a scene.
conditions or thermal noise in a camera’s sensor.
We revisit the classic hypothesis that the good bits are the
ones that are shared between multiple views of the world,
for example between multiple sensory modalities like vision,
sound, and touch . Under this perspective “presence of
dog” is good information, since dogs can be seen, heard,
and felt, but “camera pose” is bad information, since a camera’s pose has little or no effect on the acoustic and tactile
properties of the imaged scene. This hypothesis corresponds
to the inductive bias that the way you view a scene should
not affect its semantics. There is signiﬁcant evidence in
the cognitive science and neuroscience literature that such
view-invariant representations are encoded by the brain (e.g.,
 ). In this paper, we speciﬁcally study the setting
where the different views are different image channels, such
as luminance, chrominance, depth, and optical ﬂow. The fundamental supervisory signal we exploit is the co-occurrence,
in natural data, of multiple views of the same scene. For
example, we consider an image in Lab color space to be a
paired example of the co-occurrence of two views of the
scene, the L view and the ab view: {L, ab}.
Our goal is therefore to learn representations that capture
 
information shared between multiple sensory channels but
that are otherwise compact (i.e. discard channel-speciﬁc
nuisance factors). To do so, we employ contrastive learning,
where we learn a feature embedding such that views of the
same scene map to nearby points (measured with Euclidean
distance in representation space) while views of different
scenes map to far apart points. In particular, we adapt the
recently proposed method of Contrastive Predictive Coding
(CPC) , except we simplify it – removing the recurrent
network – and generalize it – showing how to apply it to
arbitrary collections of image channels, rather than just to
temporal or spatial predictions. In reference to CPC, we term
our method Contrastive Multiview Coding (CMC), although
we note that our formulation is arguably equally related to
Instance Discrimination . The contrastive objective in
our formulation, as in CPC and Instance Discrimination,
can be understood as attempting to maximize the mutual
information between the representations of multiple views
of the data.
We intentionally leave “good bits” only loosely deﬁned
and treat its deﬁnition as an empirical question. Ultimately,
the proof is in the pudding: we consider a representation
to be good if it makes subsequent problem solving easy, on
tasks of human interest. For example, a useful representation
of images might be a feature space in which it is easy to learn
to recognize objects. We therefore evaluate our method by
testing if the learned representations transfer well to standard
semantic recognition tasks. On several benchmark tasks, our
method achieves results competitive with the state of the
art, compared to other methods for self-supervised representation learning. We additionally ﬁnd that the quality of
the representation improves as a function of the number of
views used for training. Finally, we compare the contrastive
formulation of multiview learning to the recently popular
approach of cross-view prediction, and ﬁnd that in head-tohead comparisons, the contrastive approach learns stronger
representations.
The core ideas that we build on: contrastive learning,
mutual information maximization, and deep representation
learning, are not new and have been explored in the literature
on representation and multiview learning for decades . Our main contribution is to set up a framework to
extend these ideas to any number of views, and to empirically
study the factors that lead to success in this framework. A
review of the related literature is given in Section 2; and Fig.
1 gives a pictorial overview of our framework. Our main
contributions are:
• We apply contrastive learning to the multiview setting,
attempting to maximize mutual information between
representations of different views of the same scene (in
particular, between different image channels).
• We extend the framework to learn from more than two
views, and show that the quality of the learned representation improves as number of views increase. Ours is
the ﬁrst work to explicitly show the beneﬁts of multiple
views on representation quality.
• We conduct controlled experiments to measure the effect of mutual information estimates on representation
quality. Our experiments show that the relationship
between mutual information and views is a subtle one.
• Our representations rival state of the art on popular
benchmarks.
• We demonstrate that the contrastive objective is superior to cross-view prediction.
2. Related work
Unsupervised representation learning is about learning
transformations of the data that make subsequent problem
solving easier . This ﬁeld has a long history, starting with
classical methods with well established algorithms, such as
principal components analysis (PCA ) and independent
components analysis (ICA ). These methods tend to
learn representations that focus on low-level variations in
the data, which are not very useful from the perspective of
downstream tasks such as object recognition.
Representations better suited to such tasks have been
learnt using deep neural networks, starting with seminal
techniques such as Boltzmann machines , autoencoders , variational autoencoders , generative adversarial networks and autoregressive models . Numerous other works exist, for a review see . A powerful family of models for unsupervised representations are
collected under the umbrella of “self-supervised” learning
 . In these models, an input X to
the model is transformed into an output ˆX, which is supposed to be close to another signal Y (usually in Euclidean
space), which itself is related to X in some meaningful way.
Examples of such X/Y pairs are: luminance and chrominance color channels of an image , patches from a single
image , modalities such as vision and sound or the
frames of a video . Clearly, such examples are numerous
in the world, and provides us with nearly inﬁnite amounts
of training data: this is one of the appeals of this paradigm.
Time contrastive networks use a triplet loss framework
to learn representations from aligned video sequences of
the same scene, taken by different video cameras. Closely
related to self-supervised learning is the idea of multi-view
learning, which is a general term involving many different
approaches such as co-training , multi-kernel learning
 and metric learning ; for comprehensive surveys
please see . Nearly all existing works have dealt with
one or two views such as video or image/sound. However, in
many situations, many more views are available to provide
training signals for any representation.
The objective functions used to train deep learning based
representations in many of the above methods are either
reconstruction-based loss functions such as Euclidean losses
in different norms e.g. , adversarial loss functions 
that learn the loss in addition to the representation, or contrastive losses e.g. that
take advantage of the co-occurence of multiple views.
Some of the prior works most similar to our own (and
inspirational to us) are Contrastive Predictive Coding (CPC)
 , Deep InfoMax , and Instance Discrimination .
These methods, like ours, learn representations by contrasting between congruent and incongruent representations of
a scene. CPC learns from two views – the past and future –
and is applicable to sequential data, either in space or in time.
Deep Infomax considers the two views to be the input
to a neural network and its output. Instance Discrimination
learns to match two sub-crops of the same image. CPC and
Deep InfoMax have recently been extended in and 
respectively. These methods all share similar mathematical
objectives, but differ in the deﬁnition of the views. Our
method differs from these works in the following ways: we
extend the objective to the case of more than two views, and
we explore a different set of view deﬁnitions, architectures,
and application settings. In addition, we contribute a unique
empirical investigation of this paradigm of representation
The idea of contrastive learning has also started to spread
over many other tasks in various other domains .
Our goal is to learn representations that capture information shared between multiple sensory views without human
supervision. We start by reviewing previous predictive learning (or reconstruction-based learning) methods, and then
elaborate on contrastive learning within two views. We show
connections to mutual information maximization and extend
it to scenarios including more than two views. We consider
a collection of M views of the data, denoted as V1, . . . , VM.
For each view Vi, we denote vi as a random variable representing samples following vi ∼P(Vi).
3.1. Predictive Learning
Let V1 and V2 represent two views of a dataset. For instance, V1 might be the luminance of a particular image and
V2 the chrominance. We deﬁne the predictive learning setup
as a deep nonlinear transformation from v1 to v2 through latent variables z, as shown in Fig. 2. Formally, z = f(v1) and
ˆv2 = g(z), where f and g represent the encoder and decoder
respectively and ˆv2 is the prediction of v2 given v1. The parameters of the encoder and decoder models are then trained
using an objective function that tries to bring ˆv2 “close to”
v2. Simple examples of such an objective include the L1
or L2 loss functions. Note that these objectives assume independence between each pixel or element of v2 given v1,
Figure 2: Predictive Learning vs Contrastive Learning. Cross-view
prediction (Top) learns latent representations that predict one view
from another, with loss measured in the output space. Common
prediction losses, such as the L1 and L2 norms, are unstructured, in
the sense that they penalize each output dimension independently,
perhaps leading to representations that do not capture all the shared
information between the views. In contrastive learning (Bottom),
representations are learnt by contrasting congruent and incongruent
views, with loss measured in representation space. The red dotted
outlines show where the loss function is applied.
i.e., p(v2|v1) = Πip(v2i|v1), thereby reducing their ability
to model correlations or complex structure. The predictive
approach has been extensively used in representation learning, for example, colorization and predicting sound
from vision .
3.2. Contrastive Learning with Two Views
The idea behind contrastive learning is to learn an embedding that separates (contrasts) samples from two different
distributions. Given a dataset of V1 and V2 that consists of
a collection of samples {vi
i=1, we consider contrasting congruent and incongruent pairs, i.e. samples from the
joint distribution x ∼p(v1, v2) or x = {vi
2}, which we
call positives, versus samples from the product of marginals,
y ∼p(v1)p(v2) or y = {vi
2}, which we call negatives.
We learn a “critic” (a discriminating function) hθ(·)
which is trained to achieve a high value for positive pairs and
low for negative pairs. Similar to recent setups for contrastive
learning , we train this function to correctly select
a single positive sample x out of a set S = {x, y1, y2, ..., yk}
that contains k negative samples:
Lcontrast = −E
hθ(x) + Pk
i=1 hθ(yi)
To construct S, we simply ﬁx one view and enumerate positives and negatives from the other view, allowing us to
rewrite the objective as:
contrast = −
2,...,vk+1
j=1 hθ({v1
where k is the number of negative samples vj
2 for a given
1. In practice, k can be extremely large (e.g., 1.2
million in ImageNet), and so directly minimizing Eq. 2 is
infeasible. In Section 3.4, we show two approximations that
allow for tractable computation.
Implementing the critic
We implement the critic hθ(·) as
a neural network. To extract compact latent representations
of v1 and v2, we employ two encoders fθ1(·) and fθ2(·) with
parameters θ1 and θ2 respectively. The latent representions
are extracted as z1 = fθ1(v1), z2 = fθ2(v2). We compute
their cosine similarity as score and adjust its dynamic range
by a hyper-parameter τ:
hθ({v1, v2}) = exp(
fθ1(v1) · fθ2(v2)
∥fθ1(v1)∥· ∥fθ2(v2)∥· 1
Loss LV1,V2
contrast in Eq. 2 treats view V1 as anchor and
enumerates over V2. Symmetrically, we can get LV2,V1
by anchoring at V2. We add them up as our two-view loss:
L(V1, V2) = LV1,V2
contrast + LV2,V1
After the contrastive learning phase, we use the representation z1, z2, or the concatenation of both, [z1, z2], depending
on our paradigm. This process is visualized in Fig. 1.
Connecting to mutual information
The optimal critic h∗
is proportional to the density ratio between the joint distribution p(z1, z2) and the product of marginals p(z1)p(z2)
(proof provided in supplementary material):
θ({v1, v2})
p(z1)p(z2) ∝p(z1|z2)
This quantity is the pointwise mutual information, and its
expectation, in Eq. 2, yields an estimator related to mutual
information. A formal proof is given by , which we
recapitulate in supplement, showing that:
I(zi; zj) ≥log(k) −Lcontrast
where, as above, k is the number of negative pairs in sample
set S. Hence minimizing the objective L maximizes the
lower bound on the mutual information I(zi; zj), which is
bounded above by I(vi; vj) by the data processing inequality.
The dependency on k also suggests that using more negative
samples can lead to an improved representation; we show
that this is indeed the case (see supplement). We note that
recent work shows that the bound in Eq. 6 can be very
weak; and ﬁnding better estimators of mutual information is
an important open problem.
(a) Core View
(b) Full Graph
Figure 3: Graphical models and information diagrams associated with the core view and full graph paradigms, for the case of 4
views, which gives a total of 6 learning objectives. The numbers
within the regions show how much “weight” the total loss places on
each partition of information (i.e. how many of the 6 objectives that
partition contributes to). A region with no number corresponds to 0
weight. For example, in the full graph case, the mutual information
between all 4 views is considered in all 6 objectives, and hence is
marked with the number 6.
3.3. Contrastive Learning with More than Two
We present more general formulations of Eq. 2 that can
handle any number of views. We call them the “core view”
and “full graph” paradigms, which offer different tradeoffs
between efﬁciency and effectiveness. These formulations
are visualized in Fig. 3.
Suppose we have a collection of M views V1, . . . , VM.
The “core view” formulation sets apart one view that we want
to optimize over, say V1, and builds pair-wise representations
between V1 and each other view Vj, j > 1, by optimizing
the sum of a set of pair-wise objectives:
A second, more general formulation is the “full graph” where
we consider all pairs (i, j), i ̸= j, and build
relationships
in all. By involving all pairs, the objective function that we
optimize is:
Both these formulations have the effect that information is
prioritized in proportion to the number of views that share
that information. This can be seen in the information diagrams visualized in Fig. 3. The number in each partition of
the diagram indicates how many of the pairwise objectives,
L(Vi, Vj), that partition contributes to. Under both the core
view and full graph objectives, a factor, like “presence of
dog”, that is common to all views will be preferred over a
factor that affects fewer views, such as “depth sensor noise”.
The computational cost of the bivariate score function in
the full graph formulation is combinatorial in the number of
views. However, it is clear from Fig. 3 that this enables the
full graph formulation to capture more information between
different views, which may prove useful for downstream
tasks. For example, the mutual information between V2
and V3 or V2 and V4 is completely ignored in the core view
paradigm (as shown by a 0 count in the information diagram).
Another beneﬁt of the full graph formulation is that it can
handle missing information (e.g. missing views) in a natural
3.4. Implementing the Contrastive Loss
Better representations using LV1,V2
contrast in Eqn. 2 are learnt
by using many negative samples. In the extreme case, we
include every data sample in the denominator for a given
dataset. However, computing the full softmax loss is prohibitively expensive for large dataset such as ImageNet. One
way to approximate this full softmax distribution, as well as
alleviate the computational load, is to use Noise-Contrastive
Estimation (see supplement). Another solution,
which we also adopt here, is to randomly sample m negatives and do a simple (m+1)-way softmax classiﬁcation. This
strategy is also used in and dates back to .
Memory bank. Following , we maintain a memory
bank to store latent features for each training sample. Therefore, we can efﬁciently retrieve m negative samples from the
memory buffer to pair with each positive sample without recomputing their features. The memory bank is dynamically
updated with features computed on the ﬂy. The beneﬁt of a
memory bank is to allow contrasting against more negative
pairs, at the cost of slightly stale features.
4. Experiments
We extensively evaluate Contrastive Multiview Coding
(CMC) on a number of datasets and tasks. We evaluate on
two established image representation learning benchmarks:
ImageNet and STL-10 (see supplement). We further validate our framework on video representation learning
tasks, where we use image and optical ﬂow modalities, as
the two views that are jointly learned. The last set of experiments extends our CMC framework to more than two views
and provides empirical evidence of its effectiveness.
4.1. Benchmarking CMC on ImageNet
Following , we evaluate task generalization of the
learned representation by training 1000-way linear classi-
ﬁers on top of different layers. This is a standard benchmark
that has been adopted by many papers in the literature.
57.5 / 80.3
64.0 / 85.5
68.3 / 88.2
58.4 / 81.2
64.8 / 86.1
69.0 / 88.9
{Y, DbDr} + RA
60.0 / 82.3
66.2 / 87.0
70.6 / 89.7
Table 1: Top-1 / Top-5 Single crop classiﬁcation accuracy (%)
on ImageNet with a supervised logistic regression classiﬁer. We
evaluate CMC using ResNet50 with different width as encoder for
each of the two views (e.g., L and ab). “RA” stands for RandAugment .
Setup. Given a dataset of RGB images, we convert them
to the Lab image color space, and split each image into L
and ab channels, as originally proposed in SplitBrain autoencoders . During contrastive learning, L and ab from the
same image are treated as the positive pair, and ab channels
from other randomly selected images are treated as a negative pair (for a given L). Each split represents a view of
the orginal image and is passed through a separate encoder.
As in SplitBrain, we design these two encoders by evenly
splitting a given deep network, such as AlexNet , into
sub-networks across the channel dimension. By concatenating representations layer-wise from these two encoders, we
achieve the ﬁnal representation of an input image. As proposed by previous literature , the quality
of such a representation is evaluated by freezing the weights
of encoder and training linear classiﬁer on top of each layer.
Implementation. Unless otherwise speciﬁed, we use Py-
Torch default data augmentation. Following , we
set the temperature τ as 0.07 and use a momentum 0.5 for
memory update. We use 16384 negatives. The supplementary material provides more details on our hyperparameter
CMC with AlexNet. As many previous unsupervised methods are evaluated with AlexNet on ImageNet , we also include the the
results of CMC using this network. Due to the space limit,
we present this comparison in supplementary material.
CMC with ResNets. We verify the effectiveness of CMC
with larger networks such as ResNets . We experiment
on learning from luminance and chrominance views in two
colorspaces, {L, ab} and {Y, DbDr} (see sec. 4.4 for validation of this choice), and we vary the width of the ResNet
encoder for each view. We use the feature after the global
pooling layer to train the linear classiﬁer, and the results
are shown in Table 1. {L, ab} achieves 68.3% top-1 single
crop accuracy with ResNet50x2 for each view, and switching to {Y, DbDr} further brings about 0.7% improvement.
On top of it, strengthening data augmentation with RandAugment yields better or comparable results to other
state-of-the-art methods .
# of Views UCF-101 HMDB-51
VGAN* 
LT-Motion* 
TempCoh 
Shufﬂe and Learn 
Geometry 
ST Order 
Cross and Learn 
Table 2: Test accuracy (%) on UCF-101 which evaluates task
transferability and on HMDB-51 which evaluates task and dataset
transferability. Most methods either use single RGB view or additional optical ﬂow view, while VGAN explores sound as the second
view. * indicates different network architecture.
4.2. CMC on videos
We apply CMC on videos by drawing insight from the
two-streams hypothesis , which posits that human
visual cortex consists of two distinct processing streams:
the ventral stream, which performs object recognition, and
the dorsal stream, which processes motion. In our formulation, given an image it that is a frame centered at time t,
the ventral stream associates it with a neighbouring frame
it+k, while the dorsal stream connects it to optical ﬂow ft
centered at t. Therefore, we extract it, it+k and ft from two
modalities as three views of a video; for optical ﬂow we use
the TV-L1 algorithm . Two separate contrastive learning
objectives are built within the ventral stream (it, it+k) and
within the dorsal stream (it, ft). For the ventral stream, the
negative sample for it is chosen as a random frame from
another randomly chosen video; for the dorsal stream, the
negative sample for it is chosen as the ﬂow corresponding
to a random frame in another randomly chosen video.
Pre-training. We train CMC on UCF101 and use two
CaffeNets for extracting features from images and optical ﬂows, respectively. In our implementation, ft represents
10 continuous ﬂow frames centered at t. We use batch size of
128 and contrast each positive pair with 127 negative pairs.
Action recognition. We apply the learnt representation to
the task of action recognition. The spatial network from 
is a well-established paradigm for evaluating pre-trained
RGB network on action recognition task. We follow the
same spirit and evaluate the transferability of our RGB CaffeNet on UCF101 and HMDB51 datasets. We initialize the
action recognition CaffeNet up to conv5 using the weights
from the pre-trained RGB CaffeNet. The averaged accuracy
over three splits is present in Table 2. Unifying both ventral
and dorsal streams during pre-training produces higher accuracy for downstream recognition than using only single
stream. Increasing the number of views of the data from 2
to 3 (using both streams instead of one) provides a boost for
4.3. Extending CMC to More Views
We further extend our CMC learning framework to multiview scenarios. We experiment on the NYU-Depth-V2 
dataset which consists of 1449 labeled images. We focus on
a deeper understanding of the behavior and effectiveness of
CMC. The views we consider are: luminance (L channel),
chrominance (ab channel), depth, surface normal , and
semantic labels.
Setup. To extract features from each view, we use a neural
network with 5 convolutional layers, and 2 fully connected
layers. As the size of the dataset is relatively small, we adopt
the sub-patch based contrastive objective (see supplement)
to increase the number of negative pairs. Patches with a size
of 128×128 are randomly cropped from the original images
for contrastive learning (from images of size 480×640). For
downstream tasks, we discard the fully connected layers and
evaluate using the convolutional layers as a representation.
Does representation quality improve as number
of views increases?
To measure the quality of the learned representation, we
consider the task of predicting semantic labels from the representation of L. We follow the core view paradigm and
use L as the core view, thus learning a set of representations by contrasting different views with L. A UNet style
architecture is utilized to perform the segmentation task.
Contrastive training is performed on the above architecture
that is equivalent of the UNet’s encoder. After contrastive
training is completed, we initialize the encoder weights of
the UNet from the L encoder (which are equivalent architectures) and keep them frozen. Only the decoder is trained
during this ﬁnetuning stage.
Since we use the patch-based contrastive loss, in the 1
view setting case, CMC coincides with DIM . The 2-4
view cases contrast L with ab, and then sequentially add
depth and surface normals. The semantic labeling results
are measured by mean IoU over all classes and pixel accuracy, shown in Fig. 4. We see that the performance steadily
improves as new views are added. We have tested different
orders of adding the views, and they all follow a similar
We also compare CMC with two baselines. First, we randomly initialize and freeze the encoder, and we call this the
Random baseline; it serves as a lower bound on the quality
since the representation is just a random projection. Rather
than freezing the randomly initialized encoder, we could
Number of Views
Supervised
Number of Views
Pixel Accuracy (%)
Supervised
Figure 4: We show the Intersection over Union (IoU) (left) and
Pixel Accuracy (right) for the NYU-Depth-V2 dataset, as CMC is
trained with increasingly more views from 1 to 4. As more views
are added, both these metrics steadily increase. The views are (in
order of inclusion): L, ab, depth and surface normals.
Pixel Accuracy (%) mIoU (%)
CMC (core-view)
CMC (full-graph)
Supervised
Table 3: Results on the task of predicting semantic labels from
L channel representation which is learnt using the patch-based
contrastive loss and all 4 views. We compare CMC with Random
and Supervised baselines, which serve as lower and upper bounds
respectively. Th core-view paradigm refers to Fig. 3(a), and fullview Fig. 3(b).
train it jointly with the decoder. This end-to-end Supervised
baseline serves as an upper bound. The results are presented
in Table 3, which shows our CMC produces high quality
feature maps even though it’s unaware of the downstream
Is CMC improving all views?
A desirable unsupervised representation learning algorithm
operating on multiple views or modalities should improve
the quality of representations for all views. We therefore
investigate our CMC framwork beyond L channel. To treat
all views fairly, we train these encoders following the full
graph paradigm, where each view is contrasted with all other
We evaluate the representation of each view v by predicting the semantic labels from only the representation of v,
where v is L, ab, depth or surface normals. This uses the
full-graph paradigm. As in the previous section, we compare
CMC with Random and Supervised baselines. As shown in
Table 4, the performance of the representations learned by
CMC using full-graph signiﬁcantly outperforms that of randomly projected representations, and approaches the performance of the fully supervised representations. Furthermore,
the full-graph representation provides a good representation
Metric (%)
Supervised
Table 4: Performance on the task of using single view v to predict
the semantic labels, where v can be L, ab, depth or surface normal. Our CMC framework improves the quality of unsupervised
representations towards that of supervised ones, for all of views
investigated. This uses the full-graph paradigm Fig. 3(b).
Accuracy on STL-10 (%)
Predictive
Contrastive
L, Seg. Map
Supervised
Table 5: We compare predictive learning with contrastive learning
by evaluating the learned encoder on unseen dataset and task. The
contrastive learning framework consistently outperforms predictive
learnt for all views, showing the importance of capturing
different types of mutual information across views.
Predictive Learning vs. Contrastive Learning
While experiments in section 4.1 show that contrastive learning outperforms predictive learning in the context of
Lab color space, it’s unclear whether such an advantage is
due to the natural inductive bias of the task itself. To further
understand this, we go beyond chrominance (ab), and try to
answer this question when geometry or semantic labels are
We consider three view pairs on the NYU-Depth dataset:
(1) L and depth, (2) L and surface normals, and (3) L and
segmentation map. For each of them, we train two identical
encoders for L, one using contrastive learning and the other
with predictive learning. We then evaluate the representation
quality by training a linear classiﬁer on top of these encoders
on the STL-10 dataset.
The comparison results are shown in Table 5, which
shows that contrastive learning consistently outperforms predictive learning in this scenario where both the task and
the dataset are unknown. We also include “random” and
“supervised” baselines similar to that in previous sections.
Though in the unsupervised stage we only use 1.3K images
from a dataset much different from the target dataset STL-10,
Mutual Information Estimated by MINE
 (nat)
Classification Accuracy (%)
Mutual Information Estimated by MINE
 (nat)
Classification Accuracy (%)
Figure 5: How does mutual information between views relate to representation quality? (Left) Classiﬁcation accuracy against estimated MI
between channels of different color spaces; (Right) Classiﬁcation accuracy vs estimated MI between patches at different distances (distance
in pixels is denoted next to each data point). MI estimated using MINE .
the object recognition accuracy is close to the supervised
method, which uses an end-to-end deep network directly
trained on STL-10.
Given two views V1 and V2 of the data, the predictive
learning approach approximately models p(v2|v1). Furthermore, losses used typically for predictive learning, such as
pixel-wise reconstruction losses usually impose an independence assumption on the modeling: p(v2|v1) ≈Πip(v2i|v1).
On the other hand, the contrastive learning approach by construction does not assume conditional independence across
dimensions of v2. In addition, the use of random jittering
and cropping between views allows the contrastive learning
approach to beneﬁt from spatial co-occurrence (contrasting
in space) in addition to contrasting across views. We conjecture that these are two reasons for the superior performance
of contrastive learning approaches over predictive learning.
4.4. How does mutual information affect representation quality?
Given a ﬁxed set of views, CMC aims to maximize the
mutual information between representations of these views.
We have found that maximizing information in this way
indeed results in strong representations, but it would be incorrect to infer that information maximization (infomax) is
the key to good representation learning. In fact, this paper
argues for precisely the opposite idea: that cross-view representation learning is effective because it results in a kind of
information minimization, discarding nuisance factors that
are not shared between the views.
The resolution to this apparent dilemma is that we want
to maximize the “good” information – the signal – in our
representations, while minimizing the “bad” information –
the noise. The idea behind CMC is that this can be achieved
by doing infomax learning on two views that share signal but
have independent noise. This suggests a “Goldilocks principle” : a good collection of views is one that shares some
information but not too much. Here we test this hypothesis
on two domains: learning representations on images with
different colorspaces forming the two views; and learning
representations on pairs of patches extracted from an image,
separated by varying spatial distance.
In patch experiments we randomly crop two RGB patches
of size 64x64 from the same image, and use these patches as
the two views. Their relative position is ﬁxed. Namely, the
two patches always starts at position (x, y) and (x+d, y+d)
with (x, y) being randomly sampled. While varying the
distance d, we start from 64 to avoid overlapping. There is
a possible bias that with an image of relatively small size
(e.g., 512x512), a large d (e.g., 384) will always push these
two patches around boundary. To minimize this bias, we use
high resolution images (e.g. 2k) from DIV2K dataset.
Fig. 5 shows the results of these experiments. The left
plot shows the result of learning representations on different
colorspaces (splitting each colorspace into two views, such
as (L, ab), (R, GB) etc). We then use the MINE estimator 
to estimate the mutual information between the views. We
measure representation quality by training a linear classiﬁer
on the learned representations on the STL-10 dataset .
The plots clearly show that using colorspaces with minimal
mutual information give the best downstream accuracy (For
the outlier HSV in this plot, we conjecture the representation
quality is harmed by the periodicity of H. Note that the H
in HED is not periodic.). On the other hand, the story is
more nuanced for representations learned between patches
at different offsets from each other (Fig. 5, right). Here
we see that views with too little or too much MI perform
worse; a sweet spot in the middle exists which gives the best
representation. That there exists such a sweet spot should be
expected. If two views share no information, then, in principle, there is no incentive for CMC to learn anything. If two
views share all their information, no nuisances are discarded
and we arrive back at something akin to an autoencoder or
generative model, that simply tries to represent all the bits in
the multiview data.
These experiments demonstrate that the relationship between mutual information and representation quality is meaningful but not direct. Selecting optimal views, which just
share relevant signal, may be a fruitful direction for future
5. Conclusion
We have presented a contrastive learning framework
which enables the learning of unsupervised representations
from multiple views of a dataset. The principle of maximization of mutual information enables the learning of powerful
representations. A number of empirical results show that our
framework performs well compared to predictive learning
and scales with the number of views.
Acknowledgements Thanks to Devon Hjelm for providing
implementation details of Deep InfoMax, Zhirong Wu and
Richard Zhang for helpful discussion and comments. This
material is based on resources supported by Google Cloud.