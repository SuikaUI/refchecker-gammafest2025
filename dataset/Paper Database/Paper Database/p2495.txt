HAL Id: hal-01023596
 
Submitted on 14 Jul 2014
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Analysis of purely random forests bias
Sylvain Arlot, Robin Genuer
To cite this version:
Sylvain Arlot, Robin Genuer. Analysis of purely random forests bias. [Research Report] -. 2014.
￿hal-01023596￿
Analysis of purely random forests bias
Sylvain Arlot∗1 and Robin Genuer†2,3
1CNRS; Sierra Project-Team; Departement d’Informatique de l’Ecole Normale Superieure
(DI/ENS) (CNRS/ENS/INRIA UMR 8548); 23 avenue d’Italie, CS 81321, 75214 PARIS
Cedex 13 - France
2Univ. Bordeaux, ISPED, Centre INSERM U-897, 146 rue L´eo Saignat, F-33076
Bordeaux Cedex, France
3INRIA Bordeaux Sud-Ouest, Equipe SISTM
July 14, 2014
Random forests are a very eﬀective and commonly used statistical method, but their full
theoretical analysis is still an open problem. As a ﬁrst step, simpliﬁed models such as purely
random forests have been introduced, in order to shed light on the good performance of
random forests. In this paper, we study the approximation error (the bias) of some purely
random forest models in a regression framework, focusing in particular on the inﬂuence of the
number of trees in the forest. Under some regularity assumptions on the regression function,
we show that the bias of an inﬁnite forest decreases at a faster rate (with respect to the size
of each tree) than a single tree. As a consequence, inﬁnite forests attain a strictly better risk
rate (with respect to the sample size) than single trees. Furthermore, our results allow to
derive a minimum number of trees suﬃcient to reach the same rate as an inﬁnite forest. As
a by-product of our analysis, we also show a link between the bias of purely random forests
and the bias of some kernel estimators.
Introduction
Random Forests (RF henceforth) are a very eﬀective and increasingly used statistical machine
learning method.
They give outstanding performances in lots of applied situations for both
classiﬁcation and regression problems. However, their theoretical analysis remains a diﬃcult and
open problem, especially when dealing with the original RF algorithm, introduced by Breiman
Few theoretical results exist on RF, mainly the analysis of Bagging of B¨uhlmann and Yu
 —bagging, introduced by Breiman , can be seen a posteriori as a particular case of
RF— and the link between RF and nearest neighbors have been introduced.
Breiman ﬁrst began to study such
simpliﬁed RF , and then well-established results were obtained by Biau et al.
 . The main diﬀerence between PRF and RF is that, in PRF, partitioning of the input
∗ 
† 
space is performed independently from the dataset, using random variables independent from the
data. The ﬁrst reason why it is easier to handle theoretically PRF is that the random partitioning
(associated to a tree) is thus independent of the prediction made within a given element of the
partition.
Secondly, random mechanisms used to obtain the partitioning of PRF are usually
simple enough to allow an exact calculation of several quantities of interest.
In addition to theoretical analysis of PRF models described below, some empirical studies tried
these methods. Cutler and Zhao compared performances of PERT (PErfect Random Tree
ensemble) with original RF. Geurts et al. studied “Extremely Randomized Trees”, which
are not exactly PRF but lay between standard RF and PRF. These results are encouraging
since PRF or “Extremely Randomized Trees” reach very good performances on real datasets.
Thus, understanding such PRF models could give birth to simple but performing RF variants,
in addition to the original goal of understanding the original RF model.
RF and PRF partitioning schemes
We now precisely deﬁne some RF and PRF models, focusing on the regression setting that we
consider in the paper.
Following the usual terminology of RF, in this paper, any partitioning of the input space
X ⊂Rd is called a tree.
Classical tree-based estimators are related to trees because of the
recursive aspect of the partitioning mechanism. In order to simplify further discussion, we make
a slight language abuse by also calling a tree a partitioning obtained in a non-recursive way. The
leaves of the tree (its terminal nodes) are the elements of the ﬁnal partition. Inner nodes of the
tree are also useful for determining (recursively) to which element of the partition belongs some
x ∈X, as usual with decision trees.
Furthermore, as in classical tree-based estimators we focus on partitions of X made of hyperrectangles and we denote an hyperrectangle by λ =
λj where λ1, . . . , λd are intervals of
To each tree corresponds a tree estimator, obtained by assigning a real number to each leaf
of the tree, which is the (constant) value of the estimator on the corresponding element of the
partition. Throughout the paper, we always consider regressograms, that is, the value assigned
to each leaf is the average of the response variable values among observations falling into this
Finally, a forest is a sequence of trees, and the corresponding forest estimator is obtained by
aggregating the corresponding tree estimators, that is, averaging them.
We can now describe precisely some important RF and PRF models. Original RF are deﬁned as follows.
Each randomized tree is obtained from independent bootstrap
samples of the original data set, by the following recursive partitioning of the input space, which
is a variant of the CART algorithm of Breiman et al. :
Model 1 (Original RF model).
• Put X at the root of the tree.
• Repeat (until a stopping criterion is met), for each leaf λ of the current tree:
– choose mtry variables (uniformly, and without replacement, among all variables),
– ﬁnd the best split (i.e., the best couple { split variable j, split point t }, among all
possible ones involving the mtry selected variables) and perform the split, that is, put
{x ∈λ / xj < t} and {x ∈λ / xj ≥t} at the two children nodes below λ.
The parameter mtry ≤d is crucial for the method and is ﬁxed for all nodes of all trees of the
forest. The best split is found by minimizing an heterogeneity measure, which is related to some
quadratic risk .
One of the main reasons of the diﬃculty to theoretically analyze this algorithm comes from
the fact that the partitioning is data-dependent, and that the same data are used to optimize the
partition and to allocate values to tree leaves.
The ﬁrst PRF model was introduced in Breiman . In comparison to another model
introduced in Section 6 (Balanced PRF, BPRF), we name it UBPRF (UnBalanced PRF). The
input space is set to X = [0, 1)d, and the random partitioning mechanism is the following:
Model 2 (UBPRF model).
• Put [0, 1)d at the root of the tree.
• Repeat k times:
– randomly choose a node λ, to be splitted, uniformly among all terminal nodes,
– randomly choose a split variable j (uniformly among the d coordinates),
– randomly choose a split point t uniformly over λj and perform the split, that is, put
{x ∈λ / xj < t} and {x ∈λ / xj ≥t} at the two children nodes below λ.
Biau et al. established a universal consistency result in a classiﬁcation framework, for
trees and forests associated to this PRF model, provided that input variables have a uniform
distribution on [0, 1)d.
In this paper, we do not study the UBPRF model but we consider a very close one in Section 6
(BPRF). The only diﬀerence is that at each step, all nodes are split, resulting in balanced trees.
Assuming X = [0, 1), another PRF model, introduced in Genuer and called PURF
(Purely Uniformly Random Forests), is obtained by drawing k points independently with a uniform distribution on [0, 1), and by taking them as split points for the partitioning. An equivalent
recursive deﬁnition of the PURF model is the following:
Model 3 (PURF model).
• Put [0, 1) at the root of the tree.
• Repeat k times:
– choose a terminal node λ, to be splitted, each with a probability equal to its length,
– choose a split point t uniformly over λ and perform the split, that is, put {x ∈λ / x < t}
and {x ∈λ / x ≥t} at the two children nodes below λ.
Compared to UBPRF, d = 1 and the probability to choose a terminal node for being splitted
is not uniform but equal to its length. Genuer proved for the PURF model the estimation
error is strictly smaller for an inﬁnite forest than for a single tree, and that when k is well chosen,
both trees and forests of the PURF model reach the minimax rate of convergence when the
regression function is Lipschitz.
Contributions
This paper compares the performances of a forest estimator and a single tree, for three PRF
models, in the regression framework with an input space X ⊂Rd. Section 2 presents a general
decomposition of the quadratic risk of a general PRF estimator into three terms, which can be
interpreted as a decomposition into approximation error and estimation error. The rest of the
paper focuses mostly on the approximation error terms.
Section 3 shows general bounds on
the approximation error under smoothness conditions on the regression function. These bounds
allow us to compare precisely the rates of convergence of the approximation error and of the
quadratic risk of trees and forests for three PRF models: a toy model (Section 4), the PURF
model (Section 5) and the BPRF model (Section 6). For all three models, the approximation
error decreases to zero as the number of leaves of each tree tends to inﬁnity, with a faster rate for
an inﬁnite forest than for a single tree. As a consequence, when the sample size tends to inﬁnity
and assuming the number of leaves is well-chosen, the quadratic risk decreases to zero faster
for an inﬁnite forest estimator than for a single tree estimator. As a by-product, our analysis
provides theoretical grounds for choosing the number of trees in a forest in order to perform
almost as well as an inﬁnite forest, contrary to previous results on this question that were only
empirical . Furthermore, we show a link between the bias of the
inﬁnite forest and the bias of some kernel estimator, which enlightens the diﬀerent rates obtained
in Sections 4–6. Finally, our theoretical analysis is illustrated by some simulation experiments in
Section 7, for the three models of Sections 4–6 and for another PRF model closer to original RF.
Throughout the paper, L(∗) denotes a constant depending only on quantities appearing in ∗,
that can vary from one line to another or even within the same line.
Decomposition of the risk of purely random forests
This section introduces the framework of the paper and provides a general decomposition of the
risk of purely random forests, on which the rest of the paper is built.
Let X be some measurable set and s : X 7→R some measurable function. Let us assume we
observe a learning sample Dn = (Xi, Yi)1≤i≤n ∈(X × R)n of n independent observations with
common distribution P such that
∀i ∈{1, . . . , n} ,
E [Yi| Xi ] = s(Xi)
(Yi −s(Xi))2
= σ2 > 0 .
The goal is to estimate the function s in terms of quadratic risk. Let (X, Y ) ∼P be independent from Dn. Then, the quadratic risk of some (possibly data-dependent) estimator t : X 7→R
of s is deﬁned by
(s(X) −t(X))2 i
Purely random forests
In this paper, we consider random forest estimators which are the aggregation of several tree
estimators, that is, several regressograms.
For every ﬁnite partition U of X, the tree (regressogram) estimator on U is deﬁned by
bs(x; U; Dn) = bs(x; U) :=
i=1 Yi1Xi∈λ
Card {1 ≤i ≤n / Xi ∈λ}1x∈λ ,
with the convention 0/0 = 0 for dealing with the case where no Xi belong to some λ ∈U. Note
that U can be any partition of X, not necessarily obtained from a decision tree, even if we always
call it a tree.
Let q ≥1 be some integer. Given a sequence Vq = (Uj)1≤j≤q of ﬁnite partitions of X, the
associated forest estimator is deﬁned by
bs(x; Vq; Dn) = bs(x; Vq) = bsVq(x) := 1
bs(x; Uj; Dn) .
This paper considers random forests, that is, for which U1, . . . , Uq are independent ﬁnite
partitions of X with common distribution U. More precisely, we focus on purely random forest,
that is, we assume
Vq = (U1, . . . , Uq) is independent from the data Dn = (Xi, Yi)1≤i≤n .
Decomposition of the risk
For any ﬁnite partition U of X, we deﬁne
βλ := E [s(X)| X ∈λ]
is well-deﬁned for every λ ⊂X such that P(X ∈λ) > 0. So, ˜sU(X) is a.s. well-deﬁned. The
function ˜sU minimizes the least-squares risk among functions that are constant on every λ ∈U.
For any ﬁnite sequence Vq = (Uj)1≤j≤q, we deﬁne
Then, as noticed in Genuer , assuming (PR), the (point-wise) quadratic risk of bs(·; Vq)
can be decomposed as the sum of two terms: for every x ∈X,
(s(x) −bs(x; Vq; Dn))2 i
h s(x) −˜sVq(x)
h ˜sVq(x) −bs(x; Vq; Dn)
E [bs(x; Vq; Dn)| Vq ] = ˜sVq(x) .
Furthermore, the ﬁrst term in the right-hand side of Eq. (1) can be decomposed as follows.
Proposition 1. Let U be some distribution over the set of ﬁnite partitions of X, q ≥1 some
integer and x ∈X. Then,
h s(x) −˜sVq(x)
= (s(x) −EU∼U [ ˜sU(x)])2 + varU∼U ( ˜sU(x))
Proof of Proposition 1. Remark that
s(x) −˜sVq(x) = 1
(s(x) −˜sUj(x))
is the average of q independent random variables, with the same mean
s(x) −EU∼U [˜sU(x)]
and variance
varU∼U ( ˜sU(x)) ,
which directly leads to Eq. (2).
Hence, for every x ∈X, we get a decomposition of the (point-wise) quadratic risk of bs(·; Vq)
into three terms: for every x ∈X, if (PR) holds true,
(s(x) −bs(x; Vq; Dn))2 i
= (s(x) −EU∼U [˜sU(x)])2 + varU∼U ( ˜sU(x))
approximation error or bias
h ˜sVq(x) −bs(x; Vq; Dn)
estimation error or variance
which can be rewritten as
(s(x) −bs(x; Vq; Dn))2 i
= (BU,∞(x))2 + VU(x)
approximation error
h ˜sVq(x) −bs(x; Vq; Dn)
estimation error
BU,∞(x) := (s(x) −EU∼U [ ˜sU(x)])2
VU(x) := varU∼U (˜sU(x)) .
We choose to name (point-wise) approximation error (or bias)
BU,q(x) := EVq∼U⊗q
h s(x) −˜sVq(x)
= BU,∞(x) + VU(x)
for consistency with the case of a single tree, where bs(·; U; Dn) is a regressogram (conditionally
to U) and the (point-wise) approximation error is
(s(x) −˜sU(x))2 i
= BU,1(x) .
Note that in all examples we consider in the following, BU,∞(x) and VU(x) are asymptotically
decreasing functions of the number of leaves in the tree, as expected for an approximation error.
Remark also that by Eq. (5),
q→+∞BU,q(x) = BU,∞(x), which justiﬁes the notation BU,∞(x).
Let us emphasize other authors such as Geurts et al. call bias (of any tree or forest) the
BU,∞:= E [BU,∞(X)] = E
(s(X) −EU∼U [ ˜sU(X)])2 i
that we call (integrated) bias of the inﬁnite forest, so their simulation results must be compared
with our theoretical statements with caution.
The main goal of the paper is to study the (integrated) bias
BU,q := E [BU,q(X)] = E
h s(X) −˜sVq(X)
of a forest of q trees, in particular how it depends on q. By Eq. (5), BU,q is a non-increasing
function of the number q of trees in the forest. Furthermore, we can write the ratio between the
(integrated) approximation errors of a single tree and of a forest as
VU := E [VU(X)] .
So, taking an inﬁnite forest instead of a single tree decreases the bias by the factor given by
Eq. (6), which is larger or equal to one.
The following sections compute in several cases the two key quantities BU,∞and VU, showing
the ratio BU,1
can be much larger than one.
General bounds on the variance term
The variance term in Eq. (3), also called estimation error, is not the primary focus of the paper,
but we need some bounds on its integrated version for comparing the risks of tree and forest
estimators. The following proposition provides the bounds we use throughout the paper.
Proposition 2. Let q ∈[1, +∞] and Vq = (U1, . . . , Uq) be a sequence of independent random
partitions of X with common distribution U, such that Card(U1) = k ∈[1, +∞) almost surely. If
assumption (PR) holds true, then,
h ˜sVq(X) −bs(X; Vq)
h ˜sU1(X) −bs(X; U1)
h ˜sU1(X) −bs(X; U1)
2σ2 + 9 ∥s∥2
h ˜sU1(X) −bs(X; U1)
k −2EU1∼U
exp (−nP (X ∈λ))
If we assume in addition that s is K-Lipschitz with respect to some distance δ on X, then
h ˜sU1(X) −bs(X; U1)
σ2k + K2EU1∼U
(diamδ(λ))2
exp (−nP (X ∈λ))
Proposition 2 is proved in Section A.1. It shows the variance of a forest is upper bounded
by the variance of a single tree—with Eq. (7)—, and that the variance of a single tree is roughly
proportional to k/n where k is the (deterministic) number of leaves of the tree—with Eq. (8)–(10).
Approximation of the bias under smoothness conditions
We now focus on the multidimensional case, say X = [0, 1)d for some integer d ≥1, and we only
consider partitions U of X into hyper-rectangles, such that each λ ∈U has the form
[Ai, Bi) ,
with 0 ≤Ai < Bi ≤1, for all i = 1, . . . , d. All RF models lead to such partitions. For the sake
of simplicity, we assume from now on that
X has a uniform distribution over [0, 1)d ,
so that for each λ =
[Ai, Bi) ∈U,
βλ = E [s(X)| X ∈λ] = 1
where |λ| =
(Bi −Ai) denotes the volume of λ.
Let us now ﬁx some x ∈X. For every partition U of X, IU(x) :=
[Ai,U(x) , Bi,U(x)) denotes
the unique element of U to which x belongs. Then, Eq. (12) implies
In order to compute BU,∞(x) and VU(x) we need some smoothness assumption about s among
the following:
s is diﬀerentiable on X and ∃C2 > 0 ,
∀t, x ∈X ,
|s(t) −s(x) −∇s(x) · (t −x)| ≤C2 ∥t −x∥2
s is twice diﬀerentiable on X and ∇(2)s is bounded
s is twice diﬀerentiable on X and ∃C3 > 0 ,
∀t, x ∈X ,
s(t) −s(x) −∇s(x) · (t −x) −1
2(t −x)⊤∇(2)s(x)(t −x)
≤C3 ∥t −x∥3
s is three times diﬀerentiable on X and ∇(3)s is bounded
where for any v ∈Rd, ∥v∥2
i and ∥v∥3
|vi|3. We denote by ∥∇(2)s∥∞,2 (resp.
∥∇(3)s∥∞,3) the sup-norm of ∇(2)s (resp. ∇(3)s):
∥v∥2≤1, y∈[0,1)d
∥v∥3≤1, y∈[0,1)d
By Taylor-Lagrange inequality, (H2) implies (H2a) with C2 = ∥∇(2)s∥∞,2/2. Similarly, (H3)
implies (H3a) with C3 = ∥∇(3)s∥∞,3/6.
Let us deﬁne, for every i, j ∈{1, . . . , d} and x ∈X,
mA,i,U,x := E [xi −Ai,U(x)]
mB,i,x,U := E [Bi,U(x) −xi ]
mAA,i,x,U := E
(xi −Ai,U(x))2 i
mBB,i,x,U := E
(Bi,U(x) −xi )2 i
mAAA,i,x,U := E
(xi −Ai,U(x))3 i
mBBB,i,x,U := E
(Bi,U(x) −xi )3 i
mAAAA,i,x,U := E
(xi −Ai,U(x))4 i
mBBBB,i,x,U := E
(Bi,U(x) −xi )4 i
mAB,i,x,U := E [(xi −Ai,U(x)) (Bi,U(x) −xi )]
mB−A,i,j,x,U := E [(Bi,U(x) −xi −(xi −Ai,U(x))) (Bj,U(x) −xj −(xj −Aj,U(x)))]
and for any x ∈X , assuming either (H2a) or (H3a),
M1,U,x := 1
(x) (mB,i,x,U −mA,i,x,U )
M2,U,x := 1
(x) (mAA,i,x,U + mBB,i,x,U −mAB,i,x,U )
(x)mB−A,i,j,x,U
N2,U,x := 1
(mAA,i,x,U + mBB,i,x,U −2 mAB,i,x,U )
(x)mB−A,i,j,x,U
R2,U,x := C2
(mAA,i,x,U + mBB,i,x,U −mAB,i,x,U )
R3,U,x := C3
(mAAA,i,x,U + mBBB,i,x,U )
R4,U,x := 2dC2
(mAAAA,i,x,U + mBBBB,i,x,U ) .
We can now state a general result on the two terms appearing in decomposition (5) of the
approximation error of a forest of size q when assumption (PR) holds true.
Proposition 3. Let X = [0, 1)d and assume (H2a) and (Unif) hold true. Then, for every
(M1,U,x )2 −2M1,U,xR2,U,x ≤BU,∞(x) ≤(M1,U,x + R2,U,x )2
N2,U,x −(M1,U,x )2 
N2,U,x −(M1,U,x )2 
+ R4,U,x .
Furthermore, if (H3a) also holds true (which implies (H2a) holds with C2 = ∥∇(2)s∥∞,2/2), for
every x ∈X ,
BU,∞(x) −(M1,U,x + M2,U,x )2
≤2 |R3,U,x (M1,U,x + M2,U,x )| + (R3,U,x )2
Proposition 3 is proved in Section A.2.
As we will see in the following, Proposition 3 is
precise. Indeed, under (H2a), we get a gap of an order of magnitude between the upper bound
on BU,∞(x) in Eq. (14) and the lower bound on VU(x) in Eq. (15). Thus from Eq. (6), it comes
that the bias of inﬁnite forests is much smaller than the bias of single trees. Furthermore, under
(H3a), we get a tight lower bound for BU,∞(x), which shows the upper bound in Eq. (14) gives
the actual rate of convergence, at least when s is smooth enough.
A toy model of PRF is when the random partition is obtained by translation of a regular partition
of X = [0, 1) into k + 1 ≥2 pieces. Formally, U ∼Utoy
is deﬁned by
where T is a random variable with uniform distribution over [0, 1).
This random partition scheme is very close to the example of random binning features in
Section 4 of Rahimi and Recht , the main diﬀerence being that here X = [0, 1) instead of
Link between the bias of the inﬁnite forest and the bias of some kernel
First, we show that the expected inﬁnite forest, deﬁned by ˜s∞(x) := EU∼Utoy
[˜sU(x)], can be
expressed as a convolution between s and some kernel function.
Proposition 4. Assume that k ≥2, (Unif) holds true and consider the purely random forest
model Utoy
. For any x ∈
, the expected inﬁnite forest at point x satisﬁes:
˜s∞(x) = EU∼Utoy
[ ˜sU(x)] =
k (t −x) dt
k(1 −ku) if 0 ≤u ≤1
k(1 + ku) if −1
0 if |u| ≥1
Proposition 4 is proved in Section B.2. One key quantity for our bias analysis is BUtoy
(˜s∞(x) −s(x))2, which is, according to Proposition 4, close to the bias of the kernel estimator
associated to hUtoy
 . This point will enlighten the bias
decreasing rates of the next section. See also Figure 2 for a plot of hUtoy
k , and a comparison with
other PRF models.
Finally, we point out that a similar remark has been made by Rahimi and Recht with
a diﬀerent goal, where hUtoy
is called the “hat kernel”.
Bias for twice diﬀerentiable functions
As a corollary of Proposition 3, we get the following estimates of the terms appearing in decomposition (5) of the bias for the toy model.
Corollary 5. Let k ≥2, εtoy
= 1/k and assume (H2a) and (Unif) hold true. Then, for every
k (x) −(s′(x))2
≤2 ∥s′∥∞C2 + C2
and for every x ∈(0, 1)\
,∞(x) ≤(s′(x))2
+ C2 ∥s′∥∞+ C2
k (x) −(s′(x))2 Q(k min {x, 1 −x})
≤2 ∥s′∥∞C2 + C2
for some polynomial Q such that sup
|Q(t)| ≤1. As a consequence,
,∞(x)dx ≤∥s′∥2
+ C2 ∥s′∥∞+ 2C2
≤2 ∥s′∥∞C2 + C2
2 + 3 ∥s′∥2
,∞(x)dx ≤C2
Corollary 5 is proved in Section B.4. The order of magnitude of the bounds on BUtoy
correct ones (up to constants) when s is smooth enough, as shown by Corollary 6 in Section 4.4.
Inequalities (23) and (24) give the ﬁrst order of the bias of a tree:
2 dx, which
is the classical bias term of a regular regressogram . This is not surprising because a random
tree in the toy model is very close to a regular regressogram, the only diﬀerence being that the
regular partition of [0, 1) is randomly translated. So at ﬁrst order, the bias of the histogram built
on the regular subdivision of [0, 1) and of the one built on the randomly translated one are equal.
Remark 1 (Border eﬀects). The border eﬀects, also known as boundary bias, highlighted by
Corollary 5 is a well-known phenomenon for kernel estimators . Since the inﬁnite forest is equivalent to a kernel estimator in terms of bias, it suﬀers from
the same phenomenon. We could use standard techniques to suppress these border eﬀects (e.g.
by working on the torus instead of interval [0, 1)), but this is out of the scope of this paper.
Discussion: single tree vs. inﬁnite forest
We can now compare a single tree and an inﬁnite forest for the toy model Utoy
, ﬁrst in terms of
approximation error for a given k, then in terms of risk for a well-chosen k. In this section, we
assume (H2a) and (Unif) hold true.
Approximation error
Corollary 5 and Eq. (6) allow to compare the approximation errors of a single tree and of an
inﬁnite forest: for all x ∈[k−1, 1 −k−1],
,∞(x) ≤L(C2)
2 −L(∥s′∥∞, C2)
is much larger, where we recall that notation L(·) is deﬁned at the end of Section 1. The same
comparison occurs when integrating over x ∈[k−1, 1 −k−1]. Therefore, considering an inﬁnite
forest instead of a single tree decreases the approximation error from an order of magnitude, and
not only from a constant factor, when the number k + 1 of leaves of each tree tends to inﬁnity.
More precisely, the bias decreasing rate of an inﬁnite forest is smaller or equal to the square of
the bias rate of a single tree.
Risk bounds for a well-chosen k
Combining Eq. (4) with approximation error controls (Corollary 5) and the general bounds on
the estimation error (Proposition 2), we can compare the statistical risks of estimators built on
a single tree and on an inﬁnite forest, respectively. For all q ∈[1, +∞] and k ≥1, suppose
)⊗q and n ≥1 data points are available. Let ε ∈]0, 1/2[ and consider only trees with
k ≥1/ε leaves and points x ∈[ε, 1 −ε], in order to avoid border eﬀects. Then,
(bs (x; Vq; Dn ) −s(x))2 i
h bs (x; Vq; Dn ) −˜sVq(x)
h bs (x; Vq; Dn ) −˜sVq(x)
,q(x)dx + 2σ2(k + 1)
(k −1)e−n/k + 2
using Eq. (10) and that if U ∼Utoy
diamL2(λ) ≤k−1 and P(X ∈λ) = k−1 for every
So, if we are able to choose the number of leaves k + 1 optimally—for instance by crossvalidation —, the risk of an inﬁnite forest estimator, deﬁned
bs∞(x, Dn ) := EU∼U [bs (x, U, Dn )| Dn ] ,
is upper bounded as follows: if n ≥1/ε and if (H2a) holds true,
(bs∞(x, Dn ) −s(x))2 i
+ 2σ2(k + 1)
(k −1)e−n/k + 2
by Lemma 19 in Section E, assuming in addition n ≥L(σ2, ε, ∥s∥∞,
∞). Thus, we recover
the classical convergence risk rate of a kernel estimator when the regression function satisﬁes
(H2a) .
For q = 1, the risk of a tree estimator is lower bounded by the following. We again suppose that ε ∈]0, 1/2[, and in addition, we assume that
2 dx > 0 and ﬁx k0 =
24L(∥s′∥∞, C2)
(s′(x))2 dx
+ 1. From a slight adaptation of Eq. (9) in Proposition 2 (by
integrating only over leaves λ ∈U such that λ ⊂[ε, 1 −ε]), if U ∼Utoy
(bs (x; U; Dn ) −˜sU(x))2 i
n [1 −2 exp(−n/k)] × Card {λ ∈U s.t. λ ⊂[ε, 1 −ε]}
n [1 −2 exp(−n/k)] k(1 −2ε)
so that if n ≥k0,
(bs (x, U, Dn ) −s(x))2 i
(˜sU(x) −s(x))2 i
(bs (x, U, Dn ) −˜sU(x))2 i
,1(x)dx + σ2k(1 −2ε) [1 −2 exp (−n/k)]
∞, C2)k−3 + σ2k(1 −2ε) [1 −2 exp (−1)]
2 dx + σ2k(1 −2ε) [1 −2 exp (−1)]
by Lemma 20, assuming in addition n ≥L(σ2, ε).
Here, we recover the classical risk rate of a regular histogram estimator .
Therefore, an inﬁnite forest estimator attains (up to some constant)
the minimax rate of convergence over the set of C2
functions—all C2 functions satisfy (H2a)—, whereas a single tree estimator does not (except
maybe for constant functions s).
Note ﬁnally that when taking care of the borders, even an inﬁnite forest estimator is not
suﬃcient for attaining the minimax rate of convergence (at least, with our upper bounds, but
they are tight under additional assumptions according to Corollary 6 in the next section). So,
as for classical kernel regression estimators, taking into account border eﬀects can be crucial for
some random forests estimators.
Tighter bound for three times diﬀerentiable functions
Corollary 6. Let k ≥2, εtoy
= 1/k and assume (H3a) and (Unif) hold true. Then, for every
,∞(x) −(s′′(x))2
 ∥s′′∥∞+ 3C3
and for every x ∈(0, 1)\
,∞(x) −(s′(x))2 (1 −k min {x, 1 −x})4
≤∥s′∥∞∥s′′∥∞+ 2C3 ∥s′∥∞+ ∥s′′∥2
∞+ C3 ∥s′′∥∞+ 2C2
As a consequence,
,∞(x) dx −
(1 −kx)4 dx
≤∥s′∥∞∥s′′∥∞+ 2C3 ∥s′∥∞+ ∥s′′∥2
∞+ 2C3 ∥s′′∥∞+ 4C2
,∞(x) dx −
 ∥s′′∥∞+ 3C3
Corollary 6 is proved in Section B.5. Hence, if s satisﬁes (H3a) the inﬁnite forest bias is of
the order of k−4 (without taking into account borders). This shows, at least for s smooth enough,
that upper bounds of Corollary 5 involve the correct rates.
Size of the forest
According to Eq. (2), taking q = ∞is not necessary for reducing the bias of a tree from an order
of magnitude. In particular, even without border eﬀects, BUtoy
,q is of the same order as BUtoy
when q ≥k2 under assumption (H3a). So, we get a practical hint for choosing the size of the
forest, leading to an estimator that can be computed since it does not need q to be inﬁnite.
Purely uniformly random forests
We now consider a PRF model introduced by Genuer , called Purely Uniformly Random
Forests (PURF).
For every integer k ≥1, the random partition U ∼Upurf
is deﬁned as follows. Let ξ1, . . . , ξk be
independent random variables with uniform distribution over X = [0, 1) and let ξ(1) < · · · < ξ(k)
the corresponding order statistics. Then, U is deﬁned by
ξ(1), ξ(2)
PURF (k=128, x=0.5)
PURF (k=128, x=0.2)
PURF (k=128, x=0.1)
Plot of hUpurf
(t, x) as a function of t −x for x ∈{0.1, 0.2, 0.5}. The values have been
estimated by a Monte-Carlo approximation with 10 000 realizations of U.
Interpretation of the bias of the inﬁnite forest
Similarly to Proposition 4, we can try to interpret the bias of the inﬁnite forest for any purely
random forest. Indeed, as in the proof of Proposition 4, for any x ∈[0, 1), by Fubini’s theorem,
˜s∞(x) = EU [ ˜sU(x)] =
 1t∈IU(x)
s(t)hU(t, x)dt
where IU(x) denotes the unique interval of U containing x and
hU(t, x) := EU∼U
 1t∈IU(x)
In the toy model case, it turns out that hUtoy
k (t, x) only depends on t −x (when x is far enough
from the boundary), so we have an exact link with a kernel estimator. In the PURF model case,
(t, x) does not only depend on t −x, but only mildly as shown by numerical computations
(Figure 1). Hence, for the PURF model, the bias of the inﬁnite forest is equal to the bias of an
estimator close to a kernel estimator. Note that hUpurf
is compared to hU for the other random
forest models considered in this paper on Figure 2.
Bias for twice diﬀerentiable functions
As a corollary of Proposition 3, we get the following estimates of the terms appearing in decomposition (5) of the bias for the PURF model.
Corollary 7. Let k ≥1, x ∈[0, 1) and assume (H2a) and (Unif) hold true. Then,
,∞(x) ≤(s′(x))2
(x) ≤(s′(x))2
Let k ≥27 and εpurf
:= 4 log k
. Then, for every x ∈
, 1 −εpurf
,∞(x) ≤2C2
k4 + (s′(x))2
(x) −(s′(x))2
As a consequence, if k ≥27,
,∞(x)dx ≤4 ∥s′∥2
,∞(x)dx ≤2C2
k4 + ∥s′∥2
(log(k) + 1) ∥s′∥2
Corollary 7 is proved in Section C.3.
Discussion: single tree vs. inﬁnite forest
Results of Corollary 7 involve the same rates as in Corollary 5, so, the discussion of Section 4.3
is also valid for the PURF model (with boundaries of size εpurf
instead of εtoy) except for the
lower bound of the estimation error when avoiding border eﬀects. However, we conjecture that
the result is the same than for the toy model, but solving all technical issues for proving this
is beyond the scope of the paper. So, to sum up, for n suﬃciently large, we would again have
that the inﬁnite forest decreasing rate smaller or equal to the square of the single tree one. This
implies that inﬁnite forests would reach the minimax rate of convergence for C2 functions whereas
single tree does not.
Tighter bound for three times diﬀerentiable functions
When s is smooth enough, the rates obtained in Corollary 7 are tight, as shown by the following
corollary of Proposition 3.
Corollary 8. Let k ≥27 and assume (H3a) and (Unif) hold true.
Then, for every x ∈
, 1 −εpurf
BU,∞(x) −(s′′(x))2
272 |s′(x)| + 2 |s′′(x)|
and for every x ∈(0, 1)\
, 1 −εpurf
(s′(x))2 
xk+1 −(1 −x)k+1 2
≤(C3 + |s′(x)| + |s′′(x)|)2
As a consequence,
BU,∞(x) dx −(s′(0))2 + (s′(1))2
≤6 (C3 + ∥s′∥∞+ ∥s′′∥∞)2 log k
BU,∞(x) dx −
272 ∥s′∥∞+ 2 ∥s′′∥∞
Corollary 8 is proved in Section C.4. As for the toy model, Corollary 8 implies that under
(H3a), a PURF with q trees behaves as the inﬁnite forest as soon as q ≥k2.
Balanced purely random forests
We consider in this section the following multidimensional PRF model, that we call Balanced
Purely Random Forests (BPRF).
Description of the model
Let d ≥1 be ﬁxed and X = [0, 1)d. We deﬁne the sequence (Up)p∈N of random partitions (or
random trees) as follows:
• U0 = [0, 1)d a.s.
• for every p ∈N, given Up, we deﬁne Up+1 by splitting each piece λ ∈Up into two pieces,
where the split is made along some random direction (chosen uniformly over {1, . . . , d}) at
some point chosen uniformly.
Formally, given Up = {λ1,p, . . . , λ2p,p }, let L1,p, . . . , L2p,p, Z1,p, . . . , Z2p,p be independent
random variables, independent from Up, such that
∀j ∈{1, . . . , 2p } ,
Lj,p ∼U ({1, . . . , d})
Zj,p ∼U ( ) .
Then, Up+1 is deﬁned as follows: for every j ∈{1, . . . , 2p }, λj,p =
[Ai, Bi) is split into
λ2j−1,p+1 =
[Ai, Bi) × [ALj,p, (1 −Zj,p ) ALj,p + Zj,pBLj,p) ×
[Ai, Bi) × [(1 −Zj,p ) ALj,p + Zj,pBLj,p, BLj,p) ×
[Ai, Bi) .
Then, for every p ∈N, we get a random partition Up ∼Ubprf
of X = [0, 1)d into k = 2p
This model is very close to the UBPRF model introduced in Breiman and theoretically
studied by Biau et al. . The only diﬀerence is that, at each step all sets of the current
partition are split in BPRF, resulting with balanced trees, whereas in UBPRF, only one set (randomly selected with a uniform distribution) of the current partition is split; see also Section 6.4
for a comparison of these two models.
We also point out a similitude between BPRF and another model: Rahimi and Recht 
as random partitioning scheme, but without considering the same forest estimator at
the end: instead of averaging the tree estimators with uniform weights as we do, Rahimi and
Recht make a weighted average with data-driven weights.
Interpretation of the bias of the inﬁnite forest
As in Section 5.1, we can try to interpret the bias of the inﬁnite forest for Ubprf
as being equal
to the bias of an estimator close to a kernel estimator with “kernel function” hUbprf
Eq. (31). Contrary to the PURF model case, t −x 7→hUbprf
(t, x) strongly depends on x, as
shown by the left plot of Figure 2. The right plot of Figure 2 compares hU with U = Ubprf
for a ﬁxed x = 1/2 and k = 2p = 128: it turns out that hUtoy and hUpurf are the
narrowest—hUpurf appearing as a smooth approximation of hUtoy—whereas hUbprf is signiﬁcantly
ﬂatter than the others. This relative ﬂatness can explain the slower rates obtained for the bias
of the BPRF model in the next section.
BPRF (k=128, x=0.5)
BPRF (k=128, x=0.2)
BPRF (k=128, x=0.1)
toy (k=128, x=0.5)
PURF (k=128, x=0.5)
BPRF (k=128, x=0.5)
Left: Plot of hUbprf
(t, x) as a function of t −x for x ∈{0.1, 0.2, 0.5}. Right: Plot
of hU(t, x) with U ∈
128, Upurf
128 , Ubprf
as a function of t for x = 0.5. The values have been
estimated by a Monte-Carlo approximation with 10 000 realizations of U.
Bias for twice diﬀerentiable functions
As a corollary of Proposition 3, we get the following estimates of the terms appearing in decomposition (5) of the bias for the BPRF model.
Corollary 9. Let p ≥2 and assume (H2a) and (Unif) hold true. Then, for every x ∈[0, 1)d,
2 (∇s(x) · (1 −2x))2 + 2d2C2
x∈[0,1)d ∥∇s(x)∥2
x∈[0,1)d ∥∇s(x)∥2
As a consequence,
[0,1)d BUbprf
[0,1)d (∇s(x) · (1 −2x))2 dx + 2d2C2
x∈[0,1)d ∥∇s(x)∥2
[0,1)d VUbprf
x∈[0,1)d ∥∇s(x)∥2
Corollary 9 is proved in Section D.4. Remark that contrary to the toy and PURF model,
there is no border eﬀect for the approximation error in the BPRF model.
Discussion: single tree vs. inﬁnite forest
We can now compare a single tree and an inﬁnite forest for the toy model Ubprf
, ﬁrst in terms of
approximation error for a given p, then in terms of risk for a well-chosen p. In this section, we
assume (H2a) and (Unif) hold true.
Approximation error
be such that
Corollary 9 and Eq. (6) allow to compare the approximation errors of a single tree and of an
inﬁnite forest:
BU,∞≤L(s, d)k−2α
BU,1 ≥L(s, d)k−α −L(s, d)k−2α .
Therefore, considering an inﬁnite forest instead of a single tree decreases the approximation
error from an order of magnitude, and not only from a constant factor when the height of the
trees tends to inﬁnity. We emphasize that, as in Section 4.3 and 5.3, we get an inﬁnite forest bias
decreasing rate smaller or equal to the square of the single tree one.
Nevertheless, the single tree bias rate is strictly slower than the bias rate of a classical regular
partitioning estimate (with a cubic partition in k sets), which is k−2/d . Indeed, we have that for all d ≥1,
2 log(2)d < 2
since log(1 + u) ≤u for all u > −1.
Risk bounds for a well-chosen p
The above controls on the approximation errors imply controls on the statistical risk of the
estimators built on a single tree and on an inﬁnite forest, respectively. Indeed, for all q ∈[1, +∞],
if n ≥1 data points are available, the statistical risk of the estimator built upon a random forest
of q trees with k = 2p ≥2 leaves can be bounded by Eq. (4) and Proposition 2. In order to apply
Proposition 2, we need the following lemma.
Lemma 10. Let p ≥0. Then,
(diamL2(λ))2
and for every u > 0,
exp (−n|λ|)
−ne−( p+√up) 
In particular, if n ≥exp
exp (−n|λ|)
κ := 1 + 4e−1
Lemma 10 is proved in Section D.6. The proof of Lemma 10 in Section D.6 also shows the
volume of each element of a partition U ∼Ubprf
is typically of order exp (−p ± L√p), so it
is hopeless to consider values of p such that this typical volume is smaller than 1/n. Hence,
throughout this subsection, for comparing risks with a well-chosen p, we only consider values of
p such that
4 + log n −
Remark that under assumption (H2a), s is K-Lipschitz with respect to the L2 distance on
X with K = sup
∥∇s(x)∥2 = ∥∇s∥∞,2. So, Proposition 2 shows that for the BPRF model with
trees having k = 2p leaves, if n ≥1 data points are available and if Eq. (51) holds true,
(bs∞(X; Dn ) −s(X))2 i
u/p)/ log 2 
for every u ≥1, where
So, since BUbprf
,∞≤L(s, d)k−2α, if we are able to choose the number of leaves k = 2p optimally
(with an estimator selection procedure, such as cross-validation), the risk of the inﬁnite forest
estimator is upper bounded as follows:
(bs∞(X; Dn ) −s(X))2 i
u≥1, k=2p, 0≤p≤
k−2α + σ2k
u/p)/ log 2 
Now, for upper bounding the inﬁmum, two cases must be distinguished: (i) when d ≤3, so that
1/(1 + 2α) < log 2, and (ii) when d ≥4, so that 1/(1 + 2α) > log 2.
In case (i), some nonnegative integer p∗≤
4 + log n −
exists such that
1/(1+2α) 
if n ≥L(σ2). Since α ≥log(6/5)/ log(2) > (1/ log(2) −1)/2, for some (small enough) numerical
constants δ1, δ2 > 0, if n ≥L(σ2),
(1+2α) log(2) ≥nδ2 ,
taking u = δ2
1p∗in Eq. (52) yields
(bs∞(X; Dn ) −s(X))2 i
as soon as n ≥L(σ2).
In case (ii), a similar reasoning with some integer
4 + log n −
4 + log n −
(bs∞(X; Dn ) −s(X))2 i
≤L(s, d, σ2)n−2α log 222α
In particular, we get a rate of order n−(2α log 2−δ) for every δ > 0, which is slightly worse than the
rate n−2α/(2α+1) for d ≥4 since log 2 ≤1/(1 + 2α).
For lower bounding the risk of a single tree, we apply Eq. (9) in Proposition 2. By Eq. (47),
if p ≥p0 = L(s, d) and k = 2p,
,1 ≥L(s, d)k−α −L(s, d)k−2α ≥L(s, d)k−α
so that Eq. (4), Proposition 2 and Eq. (50) in Lemma 10 imply, if U ∼Ubprf
with p0 ≤p ≤
4 + log n −
(bs (X; U; Dn ) −s(X))2 i
k=2p,p0≤p≤
L(s, d)k−α + σ2k (1 −2κ)
Here, again, we must distinguish the cases (i) d ≤3 and (ii) d ≥4. If d ≤3, by Lemma 20,
Eq. (54) shows that if in addition n ≥L(d, σ2),
(bs (X; U; Dn ) −s(X))2 i
If d ≥4, Eq. (54) shows that
(bs (X; U; Dn ) −s(X))2 i
0≤k≤25/2nlog(2)
≥L(s, d)n−α log(2)
for n ≥L(σ2, d), since the function x →x−α + σ2x
is then decreasing on (0, (nα/σ2)1/(α+1)] and
1 + log(8/7)
> log(2) .
So, in both cases (d ≤3 or d ≥4), the inﬁnite forest has a faster rate of convergence (in terms
of risk) than a single tree. But, even with an inﬁnite forest with d ≤3, since
1 + 2 log(2)d <
the rate obtained is slower than the minimax rate n−4/(d+4) over the set of C2 functions .
Intuitively, the BPRF model is not minimax because it is not adaptive enough. Indeed, the
partitioning process splits each set of the current partition regardless of its size: so a relatively
small set is still split the same number of times than a relatively large set. We conjecture that
a partitioning scheme with a random choice of the next set to be split, with a probability of
choosing each set proportional to its size—as in the PURF model, see Section 1.1—, would be
better and could reach the minimax rate for C2 functions. This is proved for d = 1 in Section 5.
Finally, we note that the UBPRF model 2 would certainly suﬀer from the same lack of
adaptivity because the next set to be split is chosen with a uniform distribution on all sets. So,
this model would certainly not be minimax either, and we conjecture that it would be even worse
than the BPRF model.
Tighter bound for three times diﬀerentiable functions
The bounds in Corollary 9 are tight when s is smooth enough, as shown by the following corollary
of Proposition 3.
Corollary 11. Let p ≥2 and assume (H3a) and (Unif) hold true. Then, for every x ∈[0, 1)d,
∇s(x) · (1 −2x) +
(x)xi(1 −xi)
As a consequence,
[0,1)d BUbprf
,∞(x) dx −1
∇s(x) · (1 −2x) +
(x)xi(1 −xi)
x∈[0,1)d ∥∇s(x)∥2
x∈[0,1)d max
Corollary 11 is proved in Section D.5.
Size of the forest
As for the previous models, under (H3a), Corollary 11 and Eq. (2) show a BPRF forest of size
q has an approximation error of the same order of magnitude as an inﬁnite forest when
q ≥kα = 2αp =
Simulation experiments
In order to illustrate mathematical results from previous sections, we lead some simulation experiments with R , focusing on approximation errors, as deﬁned in Section 2.3.
We consider the models from Sections 4–6 (toy, PURF, BPRF), with d = 1 for toy and PURF,
and d ∈{1, 5, 10} for BPRF. In addition, we consider a PRF model discussed in Section 3 of
Biau , that we call Hold-out RF in the following. Hold-out RF is the original RF model 1
except that the tree partitioning is performed using an extra sample D′
n, independent from the
learning sample Dn. As a consequence, assumption (PR) holds for the Hold-out RF model,
so decomposition (3) is valid and we can compute the corresponding approximation error, as a
function of the number q of trees in the forest.
For all experiments, we take the input space X = [0, 1)d and suppose that (Unif) holds. We
choose the following regression functions:
• sinusoidal (if d = 1): x 7→sin(2πx),
• absolute value (if d = 1): x 7→
• sum (for any d ≥1): x 7→
• Friedman1 (for any d ≥5):
x 7→1/10 ×
10 sin(πx1x2) + 20(x3 −0.5)2 + 10x4 + 5x5
which is proportional to the Friedman1 function that was introduced by Friedman .
Here we add the scaling factor 1/10 in order to have a function with a range comparable
to that of sum.
For all PRF models, we choose k, the number of leaves (minus one for toy and PURF), among
25, 26, 27, 28, 29
; the last value 29 is sometimes removed for computational reasons.
Quantities BU,1 and BU,∞are estimated by Monte-Carlo approximation using:
• 1000 realizations of X,
• for BU,1, 500 realizations of U,
• for BU,∞, k2 realizations of U for toy and PURF models, k2α realizations for BPRF model
with α = −log(1 −1/(2d))/ log(2) (which ensures our estimation of the convergence rates
is precise enough, according to our theoretical results), and k2 realizations for Hold-out
RF model, which empirically appears to be suﬃcient for estimating the convergence rates
correctly for this RF model.
Furthermore, for each computation of BU,1 and BU,∞we add some “borderless” estimations of
the bias, that is, integrating only over x ∈[ǫ, 1−ǫ] with ǫ = εtoy
depending on the model.
In addition, for the Hold-out RF model:
• we simulate the D′
n sample with n = k2 (for each value of k) and choose a gaussian random
noise with variance σ2 = 1/16,
• we use the randomForest R package to build the trees on the
n: we use parameters maxnodes (to control the number of leaves) and ntree (to
set the number of trees), and take the default values for all other parameters (in particular
Finally, for each scenario, we plot the bias as a function of k in log2-log2 scale, and estimate
the slope of the plot by ﬁtting a simple linear model in order to get an approximation of the
convergence rates.
One-dimensional input space
We consider in this subsection the one-dimensional case (d = 1). Figure 3 shows results for
the sinusoidal regression function. Plots are in log2-log2 scale, so as expected we obtain linear
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−1.99)
tree BL (r=−1.98)
forest (r=−2.94)
forest BL (r=−3.88)
(a) toy, d = 1
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−1.9)
tree BL (r=−1.86)
forest (r=−3.05)
forest BL (r=−3.96)
(b) PURF, d = 1
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.97)
tree BL (r=−1)
forest (r=−1.85)
forest BL (r=−1.87)
(c) BPRF, d = 1
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−1.24)
tree BL (r=−1.27)
forest (r=−1.35)
forest BL (r=−1.37)
(d) Hold-out RF, d = 1
Plot of BU,1 and BU,∞(in log2-scale) against k (in log2-scale) for (a) toy, (b) PURF,
(c) BPRF and (d) Hold-out RF models, for the sinusoidal regression function. BL corresponds
to borderless situations and r denotes the slope of a linear model ﬁtted to the scatter plot.
behaviors. For toy and PURF models (top graphs) we get decreasing rates very close to what
can be expected from Sections 4–5: k−2 for trees (with or without borders), k−3 for forests and
k−4 for borderless forests. Similarly, we get the right decreasing rates for BPRF model (bottom
left graph): indeed, if d = 1 then α = 1, so trees and forests rates are respectively k−1 and k−2.
For Hold-out RF model we get rates about k−1.25 for trees and k−1.35 for forests as expected from
Section 6. These rates are surprisingly slow (in particular compared to toy and PURF models)
and a forest does not bring much improvement compared to a single tree. But as shown in the
next section, the one-dimensional case is not the best framework for the Hold-Out-RF model
compared to other PRF models.
Results for the absolute value regression function are presented in Figure 4. The absolute
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−2)
tree BL (r=−2.01)
forest (r=−2.89)
forest BL (r=−3.04)
(a) toy, d = 1
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−1.95)
tree BL (r=−1.97)
forest (r=−2.86)
forest BL (r=−2.78)
(b) PURF, d = 1
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.93)
tree BL (r=−0.94)
forest (r=−2.04)
forest BL (r=−2.02)
(c) BPRF, d = 1
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−1.1)
tree BL (r=−1.18)
forest (r=−1.14)
forest BL (r=−1.25)
(d) Hold-out RF, d = 1
Plot of BU,1 and BU,∞(in log2-scale) against k (in log2-scale) for (a) toy, (b) PURF, (c)
BPRF and (d) Hold-out RF models, for the absolute value regression function. BL corresponds
to borderless situations and r denotes the slope of a linear model ﬁtted to the scatter plot.
value regression function presents a singularity at point x = 1/2, and it acts as a border point.
Hence, compared to the sinusoidal regression function, the only change is that there is no diﬀerences between borderless and regular approximation errors of forests: both reach the rate k−3
for toy and PURF models. The Hold-out RF model again reaches relatively poor rates, and the
forest does not improve signiﬁcantly the bias compared to a single tree.
Multidimensional input space
For d > 1, we investigate the behaviors of BPRF and Hold-out RF models. First, Figure 5 shows
the results for the sum regression function when d ∈{5, 10}.
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.154)
forest (r=−0.309)
(a) BPRF, d = 5
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.072)
forest (r=−0.147)
(b) BPRF, d = 10
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.263)
forest (r=−0.435)
(c) Hold-out RF, d = 5
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.203)
forest (r=−0.332)
(d) Hold-out RF, d = 10
Plot of BU,1 and BU,∞(in log2-scale) against k (in log2-scale) for BPRF with (a) d = 5
and (b) d = 10, and Hold-out RF model with (c) d = 5 and (d) d = 10, for the sum regression
function. r denotes the slope of a linear model ﬁtted to the scatter plot.
As in the one-dimensional case, we observe linear behaviors in log2-log2 scale. For BPRF
(top graphs), trees and forests reach approximately the decreasing rates we can expect from
Section 6, respectively k−α and k−2α with α = log(10/9)/ log(2) ≈0.152 when d = 5 and
α = log(20/19)/ log(2) ≈0.074 when d = 10.
Compared to BPRF, the Hold-out RF model reaches better rates in the multidimensional
framework. Moreover, it suﬀers less from the increase of the dimension: BPRF rates are divided
by 2.1 when d increases from 5 to 10, whereas Hold-out RF model rates are only divided by 1.3.
Forests rates with the Hold-out RF model are about 1.6 times faster than tree rates, which
illustrates a signiﬁcant gain brought by forests. Note however the comparison with BPRF model
is partly unfair, because Hold-out RF can make use of an extra sample D′
n for building appropriate
partitions of X; nevertheless, with BPRF, if such an extra sample is available, it can only be
used for reducing the ﬁnal risk by a constant factor (since it doubles the sample size) but not for
improving the risk rate.
Results for the Friedman1 regression function are shown in Figure 6. Note that when d = 10,
the last ﬁve variables are non-informative since the regression function does not depend on them.
For BPRF, rates are slightly worse than for sum, and we still observe a factor of 2 between
trees and forests rates. The decrease of the rates might be explained by two reasons: the complexity of Friedman1 function, and when d = 10 the presence of ﬁve non-informative variables.
For the Hold-out RF model, rates are still much better than for BPRF, and forest rates
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.132)
forest (r=−0.24)
(a) BPRF, d = 5
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.058)
forest (r=−0.121)
(b) BPRF, d = 10
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.23)
forest (r=−0.352)
(c) Hold-out RF, d = 5
Bias (log−scale)
Cardinal of partitions (log−scale)
tree (r=−0.291)
forest (r=−0.393)
(d) Hold-out RF, d = 10
Plot of BU,1 and BU,∞(in log2-scale) against k (in log2-scale) for BPRF with (a) d = 5
and (b) d = 10, and Hold-out RF model with (c) d = 5 and (d) d = 10, for the Friedman1
regression function. r denotes the slope of a linear model ﬁtted to the scatter plot.
are better than tree rates by a factor 1.5. When d = 5, rates are smaller than for the sum
regression function, probably due to the complexity of the regression function. When d = 10, we
surprisingly get faster rates than when d = 5. Obtaining approximation rates as good for d = 10
as for d = 5 is expected since the number of informative variables is the same in both cases, and
Hold-out RF are known to adapt to the sparsity of the regression function (see Section 8.1 for
more details). But obtaining signiﬁcantly better rates for a more diﬃcult problem (when d = 10)
is quite surprising. Investigating this phenomenon requires a more systematic simulation study
with more examples, which is out of the scope of the present paper.
Discussion
In this paper, we analyze several purely random forests models and show, for each of them, that
an inﬁnite forest improves by an order of magnitude the approximation error of a single tree,
assuming the regression function is smooth. Since the estimation error of a forest is smaller or
equal to that of a single tree, we deduce that forests reach a strictly better risk convergence rate.
Comparison between the toy, PURF and BPRF models
In dimension d = 1, we can compare the results for the BPRF model in Section 6 with the results
obtained in Sections 4 and 5 for the toy and PURF models. Indeed, we get
so that the approximation error of an inﬁnite forest is of order k−2 for BPRF, instead of k−4 for
toy and PURF (when avoiding border eﬀects).
Intuitively, it seems the BPRF model tends to leave large “holes” in the space X, whereas the
toy model is almost regular, and the PURF model stays close to regular. Recall the PURF model
can be seen as a recursive tree construction, where only one leaf is split into two leaves at each
step, and the choice of this leaf is made with probability equal to its size. On the contrary, the
BPRF model keeps splitting all leaves whatever their size, which may lead to very small leaves
but also to much larger ones in some signiﬁcant part of the space X.
Comparison with other random forest models
Another random forest model has been suggested by Breiman and was more precisely
analyzed by Biau . In the latter paper, the random partitioning—which depends on some
parameters (pi)1≤i≤d ∈[0, 1)d with p1 + · · · + pd = 1—is as follows:
• Start from [0, 1)d,
• Repeat k times: for each set λ of the current partition,
– choose a split variable j randomly, with probability distribution over {1, . . . , d} given
by (pi)1≤i≤d,
– split λ along coordinate j at the midpoint tj of λj, that is put {x ∈λ / xj < tj} and
{x ∈λ / xj ≥tj} at the two children nodes below λ.
In a framework where only S variables (among d) are “strong” (i.e. have an eﬀect on the
output variable), the main result of Biau is that if the probability weights are well tuned
(i.e., roughly, if pj ≈1/S for a strong variable and pj ≈0 for a noisy variable), then for model 4,
the inﬁnite forest rate of convergence only depends on S and is faster than the minimax rate in
dimension d. In other words, Biau shows that such random forests adapt to sparsity.
Even if the framework of Biau is quite diﬀerent from ours, let us give a quick comparison
between the diﬀerent rates obtained when S = d. Assuming the regression function is Lipschitz,
for the model studied by Biau , the inﬁnite forest bias is at most of order k−θ with θ =
3/(4 log(2)d). This is comparable to our result k−2α for BPRF, because 2α ≈1/(log(2)d) when
d is large enough. Our rate for BPRF is a little bit faster, but recall that we make a stronger
assumption on the smoothness of the regression function (C2 instead of Lipschitz), and we only
consider d = S. The problem of knowing if the combination of these two analyses could give
better rates (in a sparse framework with C2 regression function) is beyond the scope of the paper
and we let this point for future research.
Finally, as mentioned in Section 6.4, we conjecture the BPRF model reaches better rates
than the UBPRF model 2. Intuitively, as we discuss in Section 8.1, UBPRF model tends to leave
even larger “holes” in X than BPRF, because instead of constructing balanced trees, it randomly
chooses at each step the next leaf to be split, with a uniform distribution on all leaves.
General conclusions
For all PRF models studied in this paper, we get that the inﬁnite forest bias order of magnitude
is equal to the square of the single tree bias. Consequently, if a single tree reaches the minimax
convergence rate for C1 functions, we directly have that a large enough forest is minimax for C2
functions. So, compared to trees, forests can well approximate regression functions with one more
level of smoothness. Further research is needed to know whether this phenomenon is general for
all PRF models.
Interestingly, our analysis helps to suggest better PRF partitioning mechanism. It seems PRF
models beneﬁt from a choice of the next set of the current partition to be split with a probability
proportional to the size of the set. This statement is justiﬁed by the comparison between BPRF
and PURF models in dimension 1, and it leads us to conjecture that PRF models reaching C2minimax rates of convergence could also be derived in dimension d. For instance, this should
be the case with the generalization of the PURF model to any d ≥1, consisting in replacing
“length” by “volume” in Model 3, and by choosing uniformly a coordinate j before performing
the split along it.
For practical use of PRF models, we suggest an order of magnitude for the number of trees
in a forest that is suﬃcient to get a bias term as small as for an inﬁnite forest. More precisely, if
the bias of a single tree is of order k−γ for some γ > 0, our results suggest it is suﬃcient to build
q = kγ trees to get a forest which reaches same rates as the (theoretical) inﬁnite forest.
Finally, we mention that all our general results in Sections 2–3 can be applied for any random
forest satisfying assumption (PR), that is, when random partitions are obtained independently
from the learning sample Dn.
Hence, if for a random forest model, we are able to compute
quantities appearing in Proposition 3, we can deduce results on the bias and risk convergence
rates for these random forests. In particular, we have in mind the Hold-out RF model deﬁned in
Section 7. Addressing this point appears to be an interesting future research topic, not only from
the theoretical point of view, but also in practice because the Hold-out RF model can achieve
very good performances.
Acknowledgments
The authors are grateful to Guillaume Obozinski and Francis Bach for several discussions. The
authors acknowledge the partial support of French Agence Nationale de la Recherche, under grants
Detect (ANR-09-JCJC-0027-01) and Calibration .