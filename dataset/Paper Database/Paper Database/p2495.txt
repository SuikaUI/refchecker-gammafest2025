HAL Id: hal-01023596
 
Submitted on 14 Jul 2014
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.
Analysis of purely random forests bias
Sylvain Arlot, Robin Genuer
To cite this version:
Sylvain Arlot, Robin Genuer. Analysis of purely random forests bias. [Research Report] -. 2014.
ï¿¿hal-01023596ï¿¿
Analysis of purely random forests bias
Sylvain Arlotâˆ—1 and Robin Genuerâ€ 2,3
1CNRS; Sierra Project-Team; Departement dâ€™Informatique de lâ€™Ecole Normale Superieure
(DI/ENS) (CNRS/ENS/INRIA UMR 8548); 23 avenue dâ€™Italie, CS 81321, 75214 PARIS
Cedex 13 - France
2Univ. Bordeaux, ISPED, Centre INSERM U-897, 146 rue LÂ´eo Saignat, F-33076
Bordeaux Cedex, France
3INRIA Bordeaux Sud-Ouest, Equipe SISTM
July 14, 2014
Random forests are a very eï¬€ective and commonly used statistical method, but their full
theoretical analysis is still an open problem. As a ï¬rst step, simpliï¬ed models such as purely
random forests have been introduced, in order to shed light on the good performance of
random forests. In this paper, we study the approximation error (the bias) of some purely
random forest models in a regression framework, focusing in particular on the inï¬‚uence of the
number of trees in the forest. Under some regularity assumptions on the regression function,
we show that the bias of an inï¬nite forest decreases at a faster rate (with respect to the size
of each tree) than a single tree. As a consequence, inï¬nite forests attain a strictly better risk
rate (with respect to the sample size) than single trees. Furthermore, our results allow to
derive a minimum number of trees suï¬ƒcient to reach the same rate as an inï¬nite forest. As
a by-product of our analysis, we also show a link between the bias of purely random forests
and the bias of some kernel estimators.
Introduction
Random Forests (RF henceforth) are a very eï¬€ective and increasingly used statistical machine
learning method.
They give outstanding performances in lots of applied situations for both
classiï¬cation and regression problems. However, their theoretical analysis remains a diï¬ƒcult and
open problem, especially when dealing with the original RF algorithm, introduced by Breiman
Few theoretical results exist on RF, mainly the analysis of Bagging of BÂ¨uhlmann and Yu
 â€”bagging, introduced by Breiman , can be seen a posteriori as a particular case of
RFâ€” and the link between RF and nearest neighbors have been introduced.
Breiman ï¬rst began to study such
simpliï¬ed RF , and then well-established results were obtained by Biau et al.
 . The main diï¬€erence between PRF and RF is that, in PRF, partitioning of the input
âˆ— 
â€  
space is performed independently from the dataset, using random variables independent from the
data. The ï¬rst reason why it is easier to handle theoretically PRF is that the random partitioning
(associated to a tree) is thus independent of the prediction made within a given element of the
partition.
Secondly, random mechanisms used to obtain the partitioning of PRF are usually
simple enough to allow an exact calculation of several quantities of interest.
In addition to theoretical analysis of PRF models described below, some empirical studies tried
these methods. Cutler and Zhao compared performances of PERT (PErfect Random Tree
ensemble) with original RF. Geurts et al. studied â€œExtremely Randomized Treesâ€, which
are not exactly PRF but lay between standard RF and PRF. These results are encouraging
since PRF or â€œExtremely Randomized Treesâ€ reach very good performances on real datasets.
Thus, understanding such PRF models could give birth to simple but performing RF variants,
in addition to the original goal of understanding the original RF model.
RF and PRF partitioning schemes
We now precisely deï¬ne some RF and PRF models, focusing on the regression setting that we
consider in the paper.
Following the usual terminology of RF, in this paper, any partitioning of the input space
X âŠ‚Rd is called a tree.
Classical tree-based estimators are related to trees because of the
recursive aspect of the partitioning mechanism. In order to simplify further discussion, we make
a slight language abuse by also calling a tree a partitioning obtained in a non-recursive way. The
leaves of the tree (its terminal nodes) are the elements of the ï¬nal partition. Inner nodes of the
tree are also useful for determining (recursively) to which element of the partition belongs some
x âˆˆX, as usual with decision trees.
Furthermore, as in classical tree-based estimators we focus on partitions of X made of hyperrectangles and we denote an hyperrectangle by Î» =
Î»j where Î»1, . . . , Î»d are intervals of
To each tree corresponds a tree estimator, obtained by assigning a real number to each leaf
of the tree, which is the (constant) value of the estimator on the corresponding element of the
partition. Throughout the paper, we always consider regressograms, that is, the value assigned
to each leaf is the average of the response variable values among observations falling into this
Finally, a forest is a sequence of trees, and the corresponding forest estimator is obtained by
aggregating the corresponding tree estimators, that is, averaging them.
We can now describe precisely some important RF and PRF models. Original RF are deï¬ned as follows.
Each randomized tree is obtained from independent bootstrap
samples of the original data set, by the following recursive partitioning of the input space, which
is a variant of the CART algorithm of Breiman et al. :
Model 1 (Original RF model).
â€¢ Put X at the root of the tree.
â€¢ Repeat (until a stopping criterion is met), for each leaf Î» of the current tree:
â€“ choose mtry variables (uniformly, and without replacement, among all variables),
â€“ ï¬nd the best split (i.e., the best couple { split variable j, split point t }, among all
possible ones involving the mtry selected variables) and perform the split, that is, put
{x âˆˆÎ» / xj < t} and {x âˆˆÎ» / xj â‰¥t} at the two children nodes below Î».
The parameter mtry â‰¤d is crucial for the method and is ï¬xed for all nodes of all trees of the
forest. The best split is found by minimizing an heterogeneity measure, which is related to some
quadratic risk .
One of the main reasons of the diï¬ƒculty to theoretically analyze this algorithm comes from
the fact that the partitioning is data-dependent, and that the same data are used to optimize the
partition and to allocate values to tree leaves.
The ï¬rst PRF model was introduced in Breiman . In comparison to another model
introduced in Section 6 (Balanced PRF, BPRF), we name it UBPRF (UnBalanced PRF). The
input space is set to X = [0, 1)d, and the random partitioning mechanism is the following:
Model 2 (UBPRF model).
â€¢ Put [0, 1)d at the root of the tree.
â€¢ Repeat k times:
â€“ randomly choose a node Î», to be splitted, uniformly among all terminal nodes,
â€“ randomly choose a split variable j (uniformly among the d coordinates),
â€“ randomly choose a split point t uniformly over Î»j and perform the split, that is, put
{x âˆˆÎ» / xj < t} and {x âˆˆÎ» / xj â‰¥t} at the two children nodes below Î».
Biau et al. established a universal consistency result in a classiï¬cation framework, for
trees and forests associated to this PRF model, provided that input variables have a uniform
distribution on [0, 1)d.
In this paper, we do not study the UBPRF model but we consider a very close one in Section 6
(BPRF). The only diï¬€erence is that at each step, all nodes are split, resulting in balanced trees.
Assuming X = [0, 1), another PRF model, introduced in Genuer and called PURF
(Purely Uniformly Random Forests), is obtained by drawing k points independently with a uniform distribution on [0, 1), and by taking them as split points for the partitioning. An equivalent
recursive deï¬nition of the PURF model is the following:
Model 3 (PURF model).
â€¢ Put [0, 1) at the root of the tree.
â€¢ Repeat k times:
â€“ choose a terminal node Î», to be splitted, each with a probability equal to its length,
â€“ choose a split point t uniformly over Î» and perform the split, that is, put {x âˆˆÎ» / x < t}
and {x âˆˆÎ» / x â‰¥t} at the two children nodes below Î».
Compared to UBPRF, d = 1 and the probability to choose a terminal node for being splitted
is not uniform but equal to its length. Genuer proved for the PURF model the estimation
error is strictly smaller for an inï¬nite forest than for a single tree, and that when k is well chosen,
both trees and forests of the PURF model reach the minimax rate of convergence when the
regression function is Lipschitz.
Contributions
This paper compares the performances of a forest estimator and a single tree, for three PRF
models, in the regression framework with an input space X âŠ‚Rd. Section 2 presents a general
decomposition of the quadratic risk of a general PRF estimator into three terms, which can be
interpreted as a decomposition into approximation error and estimation error. The rest of the
paper focuses mostly on the approximation error terms.
Section 3 shows general bounds on
the approximation error under smoothness conditions on the regression function. These bounds
allow us to compare precisely the rates of convergence of the approximation error and of the
quadratic risk of trees and forests for three PRF models: a toy model (Section 4), the PURF
model (Section 5) and the BPRF model (Section 6). For all three models, the approximation
error decreases to zero as the number of leaves of each tree tends to inï¬nity, with a faster rate for
an inï¬nite forest than for a single tree. As a consequence, when the sample size tends to inï¬nity
and assuming the number of leaves is well-chosen, the quadratic risk decreases to zero faster
for an inï¬nite forest estimator than for a single tree estimator. As a by-product, our analysis
provides theoretical grounds for choosing the number of trees in a forest in order to perform
almost as well as an inï¬nite forest, contrary to previous results on this question that were only
empirical . Furthermore, we show a link between the bias of the
inï¬nite forest and the bias of some kernel estimator, which enlightens the diï¬€erent rates obtained
in Sections 4â€“6. Finally, our theoretical analysis is illustrated by some simulation experiments in
Section 7, for the three models of Sections 4â€“6 and for another PRF model closer to original RF.
Throughout the paper, L(âˆ—) denotes a constant depending only on quantities appearing in âˆ—,
that can vary from one line to another or even within the same line.
Decomposition of the risk of purely random forests
This section introduces the framework of the paper and provides a general decomposition of the
risk of purely random forests, on which the rest of the paper is built.
Let X be some measurable set and s : X 7â†’R some measurable function. Let us assume we
observe a learning sample Dn = (Xi, Yi)1â‰¤iâ‰¤n âˆˆ(X Ã— R)n of n independent observations with
common distribution P such that
âˆ€i âˆˆ{1, . . . , n} ,
E [Yi| Xi ] = s(Xi)
(Yi âˆ’s(Xi))2
= Ïƒ2 > 0 .
The goal is to estimate the function s in terms of quadratic risk. Let (X, Y ) âˆ¼P be independent from Dn. Then, the quadratic risk of some (possibly data-dependent) estimator t : X 7â†’R
of s is deï¬ned by
(s(X) âˆ’t(X))2 i
Purely random forests
In this paper, we consider random forest estimators which are the aggregation of several tree
estimators, that is, several regressograms.
For every ï¬nite partition U of X, the tree (regressogram) estimator on U is deï¬ned by
bs(x; U; Dn) = bs(x; U) :=
i=1 Yi1XiâˆˆÎ»
Card {1 â‰¤i â‰¤n / Xi âˆˆÎ»}1xâˆˆÎ» ,
with the convention 0/0 = 0 for dealing with the case where no Xi belong to some Î» âˆˆU. Note
that U can be any partition of X, not necessarily obtained from a decision tree, even if we always
call it a tree.
Let q â‰¥1 be some integer. Given a sequence Vq = (Uj)1â‰¤jâ‰¤q of ï¬nite partitions of X, the
associated forest estimator is deï¬ned by
bs(x; Vq; Dn) = bs(x; Vq) = bsVq(x) := 1
bs(x; Uj; Dn) .
This paper considers random forests, that is, for which U1, . . . , Uq are independent ï¬nite
partitions of X with common distribution U. More precisely, we focus on purely random forest,
that is, we assume
Vq = (U1, . . . , Uq) is independent from the data Dn = (Xi, Yi)1â‰¤iâ‰¤n .
Decomposition of the risk
For any ï¬nite partition U of X, we deï¬ne
Î²Î» := E [s(X)| X âˆˆÎ»]
is well-deï¬ned for every Î» âŠ‚X such that P(X âˆˆÎ») > 0. So, ËœsU(X) is a.s. well-deï¬ned. The
function ËœsU minimizes the least-squares risk among functions that are constant on every Î» âˆˆU.
For any ï¬nite sequence Vq = (Uj)1â‰¤jâ‰¤q, we deï¬ne
Then, as noticed in Genuer , assuming (PR), the (point-wise) quadratic risk of bs(Â·; Vq)
can be decomposed as the sum of two terms: for every x âˆˆX,
(s(x) âˆ’bs(x; Vq; Dn))2 i
h s(x) âˆ’ËœsVq(x)
h ËœsVq(x) âˆ’bs(x; Vq; Dn)
E [bs(x; Vq; Dn)| Vq ] = ËœsVq(x) .
Furthermore, the ï¬rst term in the right-hand side of Eq. (1) can be decomposed as follows.
Proposition 1. Let U be some distribution over the set of ï¬nite partitions of X, q â‰¥1 some
integer and x âˆˆX. Then,
h s(x) âˆ’ËœsVq(x)
= (s(x) âˆ’EUâˆ¼U [ ËœsU(x)])2 + varUâˆ¼U ( ËœsU(x))
Proof of Proposition 1. Remark that
s(x) âˆ’ËœsVq(x) = 1
(s(x) âˆ’ËœsUj(x))
is the average of q independent random variables, with the same mean
s(x) âˆ’EUâˆ¼U [ËœsU(x)]
and variance
varUâˆ¼U ( ËœsU(x)) ,
which directly leads to Eq. (2).
Hence, for every x âˆˆX, we get a decomposition of the (point-wise) quadratic risk of bs(Â·; Vq)
into three terms: for every x âˆˆX, if (PR) holds true,
(s(x) âˆ’bs(x; Vq; Dn))2 i
= (s(x) âˆ’EUâˆ¼U [ËœsU(x)])2 + varUâˆ¼U ( ËœsU(x))
approximation error or bias
h ËœsVq(x) âˆ’bs(x; Vq; Dn)
estimation error or variance
which can be rewritten as
(s(x) âˆ’bs(x; Vq; Dn))2 i
= (BU,âˆ(x))2 + VU(x)
approximation error
h ËœsVq(x) âˆ’bs(x; Vq; Dn)
estimation error
BU,âˆ(x) := (s(x) âˆ’EUâˆ¼U [ ËœsU(x)])2
VU(x) := varUâˆ¼U (ËœsU(x)) .
We choose to name (point-wise) approximation error (or bias)
BU,q(x) := EVqâˆ¼UâŠ—q
h s(x) âˆ’ËœsVq(x)
= BU,âˆ(x) + VU(x)
for consistency with the case of a single tree, where bs(Â·; U; Dn) is a regressogram (conditionally
to U) and the (point-wise) approximation error is
(s(x) âˆ’ËœsU(x))2 i
= BU,1(x) .
Note that in all examples we consider in the following, BU,âˆ(x) and VU(x) are asymptotically
decreasing functions of the number of leaves in the tree, as expected for an approximation error.
Remark also that by Eq. (5),
qâ†’+âˆBU,q(x) = BU,âˆ(x), which justiï¬es the notation BU,âˆ(x).
Let us emphasize other authors such as Geurts et al. call bias (of any tree or forest) the
BU,âˆ:= E [BU,âˆ(X)] = E
(s(X) âˆ’EUâˆ¼U [ ËœsU(X)])2 i
that we call (integrated) bias of the inï¬nite forest, so their simulation results must be compared
with our theoretical statements with caution.
The main goal of the paper is to study the (integrated) bias
BU,q := E [BU,q(X)] = E
h s(X) âˆ’ËœsVq(X)
of a forest of q trees, in particular how it depends on q. By Eq. (5), BU,q is a non-increasing
function of the number q of trees in the forest. Furthermore, we can write the ratio between the
(integrated) approximation errors of a single tree and of a forest as
VU := E [VU(X)] .
So, taking an inï¬nite forest instead of a single tree decreases the bias by the factor given by
Eq. (6), which is larger or equal to one.
The following sections compute in several cases the two key quantities BU,âˆand VU, showing
the ratio BU,1
can be much larger than one.
General bounds on the variance term
The variance term in Eq. (3), also called estimation error, is not the primary focus of the paper,
but we need some bounds on its integrated version for comparing the risks of tree and forest
estimators. The following proposition provides the bounds we use throughout the paper.
Proposition 2. Let q âˆˆ[1, +âˆ] and Vq = (U1, . . . , Uq) be a sequence of independent random
partitions of X with common distribution U, such that Card(U1) = k âˆˆ[1, +âˆ) almost surely. If
assumption (PR) holds true, then,
h ËœsVq(X) âˆ’bs(X; Vq)
h ËœsU1(X) âˆ’bs(X; U1)
h ËœsU1(X) âˆ’bs(X; U1)
2Ïƒ2 + 9 âˆ¥sâˆ¥2
h ËœsU1(X) âˆ’bs(X; U1)
ï£­k âˆ’2EU1âˆ¼U
exp (âˆ’nP (X âˆˆÎ»))
If we assume in addition that s is K-Lipschitz with respect to some distance Î´ on X, then
h ËœsU1(X) âˆ’bs(X; U1)
ï£­Ïƒ2k + K2EU1âˆ¼U
(diamÎ´(Î»))2
exp (âˆ’nP (X âˆˆÎ»))
Proposition 2 is proved in Section A.1. It shows the variance of a forest is upper bounded
by the variance of a single treeâ€”with Eq. (7)â€”, and that the variance of a single tree is roughly
proportional to k/n where k is the (deterministic) number of leaves of the treeâ€”with Eq. (8)â€“(10).
Approximation of the bias under smoothness conditions
We now focus on the multidimensional case, say X = [0, 1)d for some integer d â‰¥1, and we only
consider partitions U of X into hyper-rectangles, such that each Î» âˆˆU has the form
[Ai, Bi) ,
with 0 â‰¤Ai < Bi â‰¤1, for all i = 1, . . . , d. All RF models lead to such partitions. For the sake
of simplicity, we assume from now on that
X has a uniform distribution over [0, 1)d ,
so that for each Î» =
[Ai, Bi) âˆˆU,
Î²Î» = E [s(X)| X âˆˆÎ»] = 1
where |Î»| =
(Bi âˆ’Ai) denotes the volume of Î».
Let us now ï¬x some x âˆˆX. For every partition U of X, IU(x) :=
[Ai,U(x) , Bi,U(x)) denotes
the unique element of U to which x belongs. Then, Eq. (12) implies
In order to compute BU,âˆ(x) and VU(x) we need some smoothness assumption about s among
the following:
s is diï¬€erentiable on X and âˆƒC2 > 0 ,
âˆ€t, x âˆˆX ,
|s(t) âˆ’s(x) âˆ’âˆ‡s(x) Â· (t âˆ’x)| â‰¤C2 âˆ¥t âˆ’xâˆ¥2
s is twice diï¬€erentiable on X and âˆ‡(2)s is bounded
s is twice diï¬€erentiable on X and âˆƒC3 > 0 ,
âˆ€t, x âˆˆX ,
s(t) âˆ’s(x) âˆ’âˆ‡s(x) Â· (t âˆ’x) âˆ’1
2(t âˆ’x)âŠ¤âˆ‡(2)s(x)(t âˆ’x)
â‰¤C3 âˆ¥t âˆ’xâˆ¥3
s is three times diï¬€erentiable on X and âˆ‡(3)s is bounded
where for any v âˆˆRd, âˆ¥vâˆ¥2
i and âˆ¥vâˆ¥3
|vi|3. We denote by âˆ¥âˆ‡(2)sâˆ¥âˆ,2 (resp.
âˆ¥âˆ‡(3)sâˆ¥âˆ,3) the sup-norm of âˆ‡(2)s (resp. âˆ‡(3)s):
âˆ¥vâˆ¥2â‰¤1, yâˆˆ[0,1)d
âˆ¥vâˆ¥3â‰¤1, yâˆˆ[0,1)d
By Taylor-Lagrange inequality, (H2) implies (H2a) with C2 = âˆ¥âˆ‡(2)sâˆ¥âˆ,2/2. Similarly, (H3)
implies (H3a) with C3 = âˆ¥âˆ‡(3)sâˆ¥âˆ,3/6.
Let us deï¬ne, for every i, j âˆˆ{1, . . . , d} and x âˆˆX,
mA,i,U,x := E [xi âˆ’Ai,U(x)]
mB,i,x,U := E [Bi,U(x) âˆ’xi ]
mAA,i,x,U := E
(xi âˆ’Ai,U(x))2 i
mBB,i,x,U := E
(Bi,U(x) âˆ’xi )2 i
mAAA,i,x,U := E
(xi âˆ’Ai,U(x))3 i
mBBB,i,x,U := E
(Bi,U(x) âˆ’xi )3 i
mAAAA,i,x,U := E
(xi âˆ’Ai,U(x))4 i
mBBBB,i,x,U := E
(Bi,U(x) âˆ’xi )4 i
mAB,i,x,U := E [(xi âˆ’Ai,U(x)) (Bi,U(x) âˆ’xi )]
mBâˆ’A,i,j,x,U := E [(Bi,U(x) âˆ’xi âˆ’(xi âˆ’Ai,U(x))) (Bj,U(x) âˆ’xj âˆ’(xj âˆ’Aj,U(x)))]
and for any x âˆˆX , assuming either (H2a) or (H3a),
M1,U,x := 1
(x) (mB,i,x,U âˆ’mA,i,x,U )
M2,U,x := 1
(x) (mAA,i,x,U + mBB,i,x,U âˆ’mAB,i,x,U )
(x)mBâˆ’A,i,j,x,U
N2,U,x := 1
(mAA,i,x,U + mBB,i,x,U âˆ’2 mAB,i,x,U )
(x)mBâˆ’A,i,j,x,U
R2,U,x := C2
(mAA,i,x,U + mBB,i,x,U âˆ’mAB,i,x,U )
R3,U,x := C3
(mAAA,i,x,U + mBBB,i,x,U )
R4,U,x := 2dC2
(mAAAA,i,x,U + mBBBB,i,x,U ) .
We can now state a general result on the two terms appearing in decomposition (5) of the
approximation error of a forest of size q when assumption (PR) holds true.
Proposition 3. Let X = [0, 1)d and assume (H2a) and (Unif) hold true. Then, for every
(M1,U,x )2 âˆ’2M1,U,xR2,U,x â‰¤BU,âˆ(x) â‰¤(M1,U,x + R2,U,x )2
N2,U,x âˆ’(M1,U,x )2 
N2,U,x âˆ’(M1,U,x )2 
+ R4,U,x .
Furthermore, if (H3a) also holds true (which implies (H2a) holds with C2 = âˆ¥âˆ‡(2)sâˆ¥âˆ,2/2), for
every x âˆˆX ,
BU,âˆ(x) âˆ’(M1,U,x + M2,U,x )2
â‰¤2 |R3,U,x (M1,U,x + M2,U,x )| + (R3,U,x )2
Proposition 3 is proved in Section A.2.
As we will see in the following, Proposition 3 is
precise. Indeed, under (H2a), we get a gap of an order of magnitude between the upper bound
on BU,âˆ(x) in Eq. (14) and the lower bound on VU(x) in Eq. (15). Thus from Eq. (6), it comes
that the bias of inï¬nite forests is much smaller than the bias of single trees. Furthermore, under
(H3a), we get a tight lower bound for BU,âˆ(x), which shows the upper bound in Eq. (14) gives
the actual rate of convergence, at least when s is smooth enough.
A toy model of PRF is when the random partition is obtained by translation of a regular partition
of X = [0, 1) into k + 1 â‰¥2 pieces. Formally, U âˆ¼Utoy
is deï¬ned by
where T is a random variable with uniform distribution over [0, 1).
This random partition scheme is very close to the example of random binning features in
Section 4 of Rahimi and Recht , the main diï¬€erence being that here X = [0, 1) instead of
Link between the bias of the inï¬nite forest and the bias of some kernel
First, we show that the expected inï¬nite forest, deï¬ned by Ëœsâˆ(x) := EUâˆ¼Utoy
[ËœsU(x)], can be
expressed as a convolution between s and some kernel function.
Proposition 4. Assume that k â‰¥2, (Unif) holds true and consider the purely random forest
model Utoy
. For any x âˆˆ
, the expected inï¬nite forest at point x satisï¬es:
Ëœsâˆ(x) = EUâˆ¼Utoy
[ ËœsU(x)] =
k (t âˆ’x) dt
k(1 âˆ’ku) if 0 â‰¤u â‰¤1
k(1 + ku) if âˆ’1
0 if |u| â‰¥1
Proposition 4 is proved in Section B.2. One key quantity for our bias analysis is BUtoy
(Ëœsâˆ(x) âˆ’s(x))2, which is, according to Proposition 4, close to the bias of the kernel estimator
associated to hUtoy
 . This point will enlighten the bias
decreasing rates of the next section. See also Figure 2 for a plot of hUtoy
k , and a comparison with
other PRF models.
Finally, we point out that a similar remark has been made by Rahimi and Recht with
a diï¬€erent goal, where hUtoy
is called the â€œhat kernelâ€.
Bias for twice diï¬€erentiable functions
As a corollary of Proposition 3, we get the following estimates of the terms appearing in decomposition (5) of the bias for the toy model.
Corollary 5. Let k â‰¥2, Îµtoy
= 1/k and assume (H2a) and (Unif) hold true. Then, for every
k (x) âˆ’(sâ€²(x))2
â‰¤2 âˆ¥sâ€²âˆ¥âˆC2 + C2
and for every x âˆˆ(0, 1)\
,âˆ(x) â‰¤(sâ€²(x))2
+ C2 âˆ¥sâ€²âˆ¥âˆ+ C2
k (x) âˆ’(sâ€²(x))2 Q(k min {x, 1 âˆ’x})
â‰¤2 âˆ¥sâ€²âˆ¥âˆC2 + C2
for some polynomial Q such that sup
|Q(t)| â‰¤1. As a consequence,
,âˆ(x)dx â‰¤âˆ¥sâ€²âˆ¥2
+ C2 âˆ¥sâ€²âˆ¥âˆ+ 2C2
â‰¤2 âˆ¥sâ€²âˆ¥âˆC2 + C2
2 + 3 âˆ¥sâ€²âˆ¥2
,âˆ(x)dx â‰¤C2
Corollary 5 is proved in Section B.4. The order of magnitude of the bounds on BUtoy
correct ones (up to constants) when s is smooth enough, as shown by Corollary 6 in Section 4.4.
Inequalities (23) and (24) give the ï¬rst order of the bias of a tree:
2 dx, which
is the classical bias term of a regular regressogram . This is not surprising because a random
tree in the toy model is very close to a regular regressogram, the only diï¬€erence being that the
regular partition of [0, 1) is randomly translated. So at ï¬rst order, the bias of the histogram built
on the regular subdivision of [0, 1) and of the one built on the randomly translated one are equal.
Remark 1 (Border eï¬€ects). The border eï¬€ects, also known as boundary bias, highlighted by
Corollary 5 is a well-known phenomenon for kernel estimators . Since the inï¬nite forest is equivalent to a kernel estimator in terms of bias, it suï¬€ers from
the same phenomenon. We could use standard techniques to suppress these border eï¬€ects (e.g.
by working on the torus instead of interval [0, 1)), but this is out of the scope of this paper.
Discussion: single tree vs. inï¬nite forest
We can now compare a single tree and an inï¬nite forest for the toy model Utoy
, ï¬rst in terms of
approximation error for a given k, then in terms of risk for a well-chosen k. In this section, we
assume (H2a) and (Unif) hold true.
Approximation error
Corollary 5 and Eq. (6) allow to compare the approximation errors of a single tree and of an
inï¬nite forest: for all x âˆˆ[kâˆ’1, 1 âˆ’kâˆ’1],
,âˆ(x) â‰¤L(C2)
2 âˆ’L(âˆ¥sâ€²âˆ¥âˆ, C2)
is much larger, where we recall that notation L(Â·) is deï¬ned at the end of Section 1. The same
comparison occurs when integrating over x âˆˆ[kâˆ’1, 1 âˆ’kâˆ’1]. Therefore, considering an inï¬nite
forest instead of a single tree decreases the approximation error from an order of magnitude, and
not only from a constant factor, when the number k + 1 of leaves of each tree tends to inï¬nity.
More precisely, the bias decreasing rate of an inï¬nite forest is smaller or equal to the square of
the bias rate of a single tree.
Risk bounds for a well-chosen k
Combining Eq. (4) with approximation error controls (Corollary 5) and the general bounds on
the estimation error (Proposition 2), we can compare the statistical risks of estimators built on
a single tree and on an inï¬nite forest, respectively. For all q âˆˆ[1, +âˆ] and k â‰¥1, suppose
)âŠ—q and n â‰¥1 data points are available. Let Îµ âˆˆ]0, 1/2[ and consider only trees with
k â‰¥1/Îµ leaves and points x âˆˆ[Îµ, 1 âˆ’Îµ], in order to avoid border eï¬€ects. Then,
(bs (x; Vq; Dn ) âˆ’s(x))2 i
h bs (x; Vq; Dn ) âˆ’ËœsVq(x)
h bs (x; Vq; Dn ) âˆ’ËœsVq(x)
,q(x)dx + 2Ïƒ2(k + 1)
(k âˆ’1)eâˆ’n/k + 2
using Eq. (10) and that if U âˆ¼Utoy
diamL2(Î») â‰¤kâˆ’1 and P(X âˆˆÎ») = kâˆ’1 for every
So, if we are able to choose the number of leaves k + 1 optimallyâ€”for instance by crossvalidation â€”, the risk of an inï¬nite forest estimator, deï¬ned
bsâˆ(x, Dn ) := EUâˆ¼U [bs (x, U, Dn )| Dn ] ,
is upper bounded as follows: if n â‰¥1/Îµ and if (H2a) holds true,
(bsâˆ(x, Dn ) âˆ’s(x))2 i
+ 2Ïƒ2(k + 1)
(k âˆ’1)eâˆ’n/k + 2
by Lemma 19 in Section E, assuming in addition n â‰¥L(Ïƒ2, Îµ, âˆ¥sâˆ¥âˆ,
âˆ). Thus, we recover
the classical convergence risk rate of a kernel estimator when the regression function satisï¬es
(H2a) .
For q = 1, the risk of a tree estimator is lower bounded by the following. We again suppose that Îµ âˆˆ]0, 1/2[, and in addition, we assume that
2 dx > 0 and ï¬x k0 =
24L(âˆ¥sâ€²âˆ¥âˆ, C2)
(sâ€²(x))2 dx
+ 1. From a slight adaptation of Eq. (9) in Proposition 2 (by
integrating only over leaves Î» âˆˆU such that Î» âŠ‚[Îµ, 1 âˆ’Îµ]), if U âˆ¼Utoy
(bs (x; U; Dn ) âˆ’ËœsU(x))2 i
n [1 âˆ’2 exp(âˆ’n/k)] Ã— Card {Î» âˆˆU s.t. Î» âŠ‚[Îµ, 1 âˆ’Îµ]}
n [1 âˆ’2 exp(âˆ’n/k)] k(1 âˆ’2Îµ)
so that if n â‰¥k0,
(bs (x, U, Dn ) âˆ’s(x))2 i
(ËœsU(x) âˆ’s(x))2 i
(bs (x, U, Dn ) âˆ’ËœsU(x))2 i
,1(x)dx + Ïƒ2k(1 âˆ’2Îµ) [1 âˆ’2 exp (âˆ’n/k)]
âˆ, C2)kâˆ’3 + Ïƒ2k(1 âˆ’2Îµ) [1 âˆ’2 exp (âˆ’1)]
2 dx + Ïƒ2k(1 âˆ’2Îµ) [1 âˆ’2 exp (âˆ’1)]
by Lemma 20, assuming in addition n â‰¥L(Ïƒ2, Îµ).
Here, we recover the classical risk rate of a regular histogram estimator .
Therefore, an inï¬nite forest estimator attains (up to some constant)
the minimax rate of convergence over the set of C2
functionsâ€”all C2 functions satisfy (H2a)â€”, whereas a single tree estimator does not (except
maybe for constant functions s).
Note ï¬nally that when taking care of the borders, even an inï¬nite forest estimator is not
suï¬ƒcient for attaining the minimax rate of convergence (at least, with our upper bounds, but
they are tight under additional assumptions according to Corollary 6 in the next section). So,
as for classical kernel regression estimators, taking into account border eï¬€ects can be crucial for
some random forests estimators.
Tighter bound for three times diï¬€erentiable functions
Corollary 6. Let k â‰¥2, Îµtoy
= 1/k and assume (H3a) and (Unif) hold true. Then, for every
,âˆ(x) âˆ’(sâ€²â€²(x))2
 âˆ¥sâ€²â€²âˆ¥âˆ+ 3C3
and for every x âˆˆ(0, 1)\
,âˆ(x) âˆ’(sâ€²(x))2 (1 âˆ’k min {x, 1 âˆ’x})4
â‰¤âˆ¥sâ€²âˆ¥âˆâˆ¥sâ€²â€²âˆ¥âˆ+ 2C3 âˆ¥sâ€²âˆ¥âˆ+ âˆ¥sâ€²â€²âˆ¥2
âˆ+ C3 âˆ¥sâ€²â€²âˆ¥âˆ+ 2C2
As a consequence,
,âˆ(x) dx âˆ’
(1 âˆ’kx)4 dx
â‰¤âˆ¥sâ€²âˆ¥âˆâˆ¥sâ€²â€²âˆ¥âˆ+ 2C3 âˆ¥sâ€²âˆ¥âˆ+ âˆ¥sâ€²â€²âˆ¥2
âˆ+ 2C3 âˆ¥sâ€²â€²âˆ¥âˆ+ 4C2
,âˆ(x) dx âˆ’
 âˆ¥sâ€²â€²âˆ¥âˆ+ 3C3
Corollary 6 is proved in Section B.5. Hence, if s satisï¬es (H3a) the inï¬nite forest bias is of
the order of kâˆ’4 (without taking into account borders). This shows, at least for s smooth enough,
that upper bounds of Corollary 5 involve the correct rates.
Size of the forest
According to Eq. (2), taking q = âˆis not necessary for reducing the bias of a tree from an order
of magnitude. In particular, even without border eï¬€ects, BUtoy
,q is of the same order as BUtoy
when q â‰¥k2 under assumption (H3a). So, we get a practical hint for choosing the size of the
forest, leading to an estimator that can be computed since it does not need q to be inï¬nite.
Purely uniformly random forests
We now consider a PRF model introduced by Genuer , called Purely Uniformly Random
Forests (PURF).
For every integer k â‰¥1, the random partition U âˆ¼Upurf
is deï¬ned as follows. Let Î¾1, . . . , Î¾k be
independent random variables with uniform distribution over X = [0, 1) and let Î¾(1) < Â· Â· Â· < Î¾(k)
the corresponding order statistics. Then, U is deï¬ned by
Î¾(1), Î¾(2)
PURF (k=128, x=0.5)
PURF (k=128, x=0.2)
PURF (k=128, x=0.1)
Plot of hUpurf
(t, x) as a function of t âˆ’x for x âˆˆ{0.1, 0.2, 0.5}. The values have been
estimated by a Monte-Carlo approximation with 10 000 realizations of U.
Interpretation of the bias of the inï¬nite forest
Similarly to Proposition 4, we can try to interpret the bias of the inï¬nite forest for any purely
random forest. Indeed, as in the proof of Proposition 4, for any x âˆˆ[0, 1), by Fubiniâ€™s theorem,
Ëœsâˆ(x) = EU [ ËœsU(x)] =
 1tâˆˆIU(x)
s(t)hU(t, x)dt
where IU(x) denotes the unique interval of U containing x and
hU(t, x) := EUâˆ¼U
 1tâˆˆIU(x)
In the toy model case, it turns out that hUtoy
k (t, x) only depends on t âˆ’x (when x is far enough
from the boundary), so we have an exact link with a kernel estimator. In the PURF model case,
(t, x) does not only depend on t âˆ’x, but only mildly as shown by numerical computations
(Figure 1). Hence, for the PURF model, the bias of the inï¬nite forest is equal to the bias of an
estimator close to a kernel estimator. Note that hUpurf
is compared to hU for the other random
forest models considered in this paper on Figure 2.
Bias for twice diï¬€erentiable functions
As a corollary of Proposition 3, we get the following estimates of the terms appearing in decomposition (5) of the bias for the PURF model.
Corollary 7. Let k â‰¥1, x âˆˆ[0, 1) and assume (H2a) and (Unif) hold true. Then,
,âˆ(x) â‰¤(sâ€²(x))2
(x) â‰¤(sâ€²(x))2
Let k â‰¥27 and Îµpurf
:= 4 log k
. Then, for every x âˆˆ
, 1 âˆ’Îµpurf
,âˆ(x) â‰¤2C2
k4 + (sâ€²(x))2
(x) âˆ’(sâ€²(x))2
As a consequence, if k â‰¥27,
,âˆ(x)dx â‰¤4 âˆ¥sâ€²âˆ¥2
,âˆ(x)dx â‰¤2C2
k4 + âˆ¥sâ€²âˆ¥2
(log(k) + 1) âˆ¥sâ€²âˆ¥2
Corollary 7 is proved in Section C.3.
Discussion: single tree vs. inï¬nite forest
Results of Corollary 7 involve the same rates as in Corollary 5, so, the discussion of Section 4.3
is also valid for the PURF model (with boundaries of size Îµpurf
instead of Îµtoy) except for the
lower bound of the estimation error when avoiding border eï¬€ects. However, we conjecture that
the result is the same than for the toy model, but solving all technical issues for proving this
is beyond the scope of the paper. So, to sum up, for n suï¬ƒciently large, we would again have
that the inï¬nite forest decreasing rate smaller or equal to the square of the single tree one. This
implies that inï¬nite forests would reach the minimax rate of convergence for C2 functions whereas
single tree does not.
Tighter bound for three times diï¬€erentiable functions
When s is smooth enough, the rates obtained in Corollary 7 are tight, as shown by the following
corollary of Proposition 3.
Corollary 8. Let k â‰¥27 and assume (H3a) and (Unif) hold true.
Then, for every x âˆˆ
, 1 âˆ’Îµpurf
BU,âˆ(x) âˆ’(sâ€²â€²(x))2
272 |sâ€²(x)| + 2 |sâ€²â€²(x)|
and for every x âˆˆ(0, 1)\
, 1 âˆ’Îµpurf
(sâ€²(x))2 
xk+1 âˆ’(1 âˆ’x)k+1 2
â‰¤(C3 + |sâ€²(x)| + |sâ€²â€²(x)|)2
As a consequence,
BU,âˆ(x) dx âˆ’(sâ€²(0))2 + (sâ€²(1))2
â‰¤6 (C3 + âˆ¥sâ€²âˆ¥âˆ+ âˆ¥sâ€²â€²âˆ¥âˆ)2 log k
BU,âˆ(x) dx âˆ’
272 âˆ¥sâ€²âˆ¥âˆ+ 2 âˆ¥sâ€²â€²âˆ¥âˆ
Corollary 8 is proved in Section C.4. As for the toy model, Corollary 8 implies that under
(H3a), a PURF with q trees behaves as the inï¬nite forest as soon as q â‰¥k2.
Balanced purely random forests
We consider in this section the following multidimensional PRF model, that we call Balanced
Purely Random Forests (BPRF).
Description of the model
Let d â‰¥1 be ï¬xed and X = [0, 1)d. We deï¬ne the sequence (Up)pâˆˆN of random partitions (or
random trees) as follows:
â€¢ U0 = [0, 1)d a.s.
â€¢ for every p âˆˆN, given Up, we deï¬ne Up+1 by splitting each piece Î» âˆˆUp into two pieces,
where the split is made along some random direction (chosen uniformly over {1, . . . , d}) at
some point chosen uniformly.
Formally, given Up = {Î»1,p, . . . , Î»2p,p }, let L1,p, . . . , L2p,p, Z1,p, . . . , Z2p,p be independent
random variables, independent from Up, such that
âˆ€j âˆˆ{1, . . . , 2p } ,
Lj,p âˆ¼U ({1, . . . , d})
Zj,p âˆ¼U ( ) .
Then, Up+1 is deï¬ned as follows: for every j âˆˆ{1, . . . , 2p }, Î»j,p =
[Ai, Bi) is split into
Î»2jâˆ’1,p+1 =
[Ai, Bi) Ã— [ALj,p, (1 âˆ’Zj,p ) ALj,p + Zj,pBLj,p) Ã—
[Ai, Bi) Ã— [(1 âˆ’Zj,p ) ALj,p + Zj,pBLj,p, BLj,p) Ã—
[Ai, Bi) .
Then, for every p âˆˆN, we get a random partition Up âˆ¼Ubprf
of X = [0, 1)d into k = 2p
This model is very close to the UBPRF model introduced in Breiman and theoretically
studied by Biau et al. . The only diï¬€erence is that, at each step all sets of the current
partition are split in BPRF, resulting with balanced trees, whereas in UBPRF, only one set (randomly selected with a uniform distribution) of the current partition is split; see also Section 6.4
for a comparison of these two models.
We also point out a similitude between BPRF and another model: Rahimi and Recht 
as random partitioning scheme, but without considering the same forest estimator at
the end: instead of averaging the tree estimators with uniform weights as we do, Rahimi and
Recht make a weighted average with data-driven weights.
Interpretation of the bias of the inï¬nite forest
As in Section 5.1, we can try to interpret the bias of the inï¬nite forest for Ubprf
as being equal
to the bias of an estimator close to a kernel estimator with â€œkernel functionâ€ hUbprf
Eq. (31). Contrary to the PURF model case, t âˆ’x 7â†’hUbprf
(t, x) strongly depends on x, as
shown by the left plot of Figure 2. The right plot of Figure 2 compares hU with U = Ubprf
for a ï¬xed x = 1/2 and k = 2p = 128: it turns out that hUtoy and hUpurf are the
narrowestâ€”hUpurf appearing as a smooth approximation of hUtoyâ€”whereas hUbprf is signiï¬cantly
ï¬‚atter than the others. This relative ï¬‚atness can explain the slower rates obtained for the bias
of the BPRF model in the next section.
BPRF (k=128, x=0.5)
BPRF (k=128, x=0.2)
BPRF (k=128, x=0.1)
toy (k=128, x=0.5)
PURF (k=128, x=0.5)
BPRF (k=128, x=0.5)
Left: Plot of hUbprf
(t, x) as a function of t âˆ’x for x âˆˆ{0.1, 0.2, 0.5}. Right: Plot
of hU(t, x) with U âˆˆ
128, Upurf
128 , Ubprf
as a function of t for x = 0.5. The values have been
estimated by a Monte-Carlo approximation with 10 000 realizations of U.
Bias for twice diï¬€erentiable functions
As a corollary of Proposition 3, we get the following estimates of the terms appearing in decomposition (5) of the bias for the BPRF model.
Corollary 9. Let p â‰¥2 and assume (H2a) and (Unif) hold true. Then, for every x âˆˆ[0, 1)d,
2 (âˆ‡s(x) Â· (1 âˆ’2x))2 + 2d2C2
xâˆˆ[0,1)d âˆ¥âˆ‡s(x)âˆ¥2
xâˆˆ[0,1)d âˆ¥âˆ‡s(x)âˆ¥2
As a consequence,
[0,1)d BUbprf
[0,1)d (âˆ‡s(x) Â· (1 âˆ’2x))2 dx + 2d2C2
xâˆˆ[0,1)d âˆ¥âˆ‡s(x)âˆ¥2
[0,1)d VUbprf
xâˆˆ[0,1)d âˆ¥âˆ‡s(x)âˆ¥2
Corollary 9 is proved in Section D.4. Remark that contrary to the toy and PURF model,
there is no border eï¬€ect for the approximation error in the BPRF model.
Discussion: single tree vs. inï¬nite forest
We can now compare a single tree and an inï¬nite forest for the toy model Ubprf
, ï¬rst in terms of
approximation error for a given p, then in terms of risk for a well-chosen p. In this section, we
assume (H2a) and (Unif) hold true.
Approximation error
be such that
Corollary 9 and Eq. (6) allow to compare the approximation errors of a single tree and of an
inï¬nite forest:
BU,âˆâ‰¤L(s, d)kâˆ’2Î±
BU,1 â‰¥L(s, d)kâˆ’Î± âˆ’L(s, d)kâˆ’2Î± .
Therefore, considering an inï¬nite forest instead of a single tree decreases the approximation
error from an order of magnitude, and not only from a constant factor when the height of the
trees tends to inï¬nity. We emphasize that, as in Section 4.3 and 5.3, we get an inï¬nite forest bias
decreasing rate smaller or equal to the square of the single tree one.
Nevertheless, the single tree bias rate is strictly slower than the bias rate of a classical regular
partitioning estimate (with a cubic partition in k sets), which is kâˆ’2/d . Indeed, we have that for all d â‰¥1,
2 log(2)d < 2
since log(1 + u) â‰¤u for all u > âˆ’1.
Risk bounds for a well-chosen p
The above controls on the approximation errors imply controls on the statistical risk of the
estimators built on a single tree and on an inï¬nite forest, respectively. Indeed, for all q âˆˆ[1, +âˆ],
if n â‰¥1 data points are available, the statistical risk of the estimator built upon a random forest
of q trees with k = 2p â‰¥2 leaves can be bounded by Eq. (4) and Proposition 2. In order to apply
Proposition 2, we need the following lemma.
Lemma 10. Let p â‰¥0. Then,
(diamL2(Î»))2
and for every u > 0,
exp (âˆ’n|Î»|)
âˆ’neâˆ’( p+âˆšup) 
In particular, if n â‰¥exp
exp (âˆ’n|Î»|)
Îº := 1 + 4eâˆ’1
Lemma 10 is proved in Section D.6. The proof of Lemma 10 in Section D.6 also shows the
volume of each element of a partition U âˆ¼Ubprf
is typically of order exp (âˆ’p Â± Lâˆšp), so it
is hopeless to consider values of p such that this typical volume is smaller than 1/n. Hence,
throughout this subsection, for comparing risks with a well-chosen p, we only consider values of
p such that
4 + log n âˆ’
Remark that under assumption (H2a), s is K-Lipschitz with respect to the L2 distance on
X with K = sup
âˆ¥âˆ‡s(x)âˆ¥2 = âˆ¥âˆ‡sâˆ¥âˆ,2. So, Proposition 2 shows that for the BPRF model with
trees having k = 2p leaves, if n â‰¥1 data points are available and if Eq. (51) holds true,
(bsâˆ(X; Dn ) âˆ’s(X))2 i
u/p)/ log 2 
for every u â‰¥1, where
So, since BUbprf
,âˆâ‰¤L(s, d)kâˆ’2Î±, if we are able to choose the number of leaves k = 2p optimally
(with an estimator selection procedure, such as cross-validation), the risk of the inï¬nite forest
estimator is upper bounded as follows:
(bsâˆ(X; Dn ) âˆ’s(X))2 i
uâ‰¥1, k=2p, 0â‰¤pâ‰¤
kâˆ’2Î± + Ïƒ2k
u/p)/ log 2 
Now, for upper bounding the inï¬mum, two cases must be distinguished: (i) when d â‰¤3, so that
1/(1 + 2Î±) < log 2, and (ii) when d â‰¥4, so that 1/(1 + 2Î±) > log 2.
In case (i), some nonnegative integer pâˆ—â‰¤
4 + log n âˆ’
exists such that
1/(1+2Î±) 
if n â‰¥L(Ïƒ2). Since Î± â‰¥log(6/5)/ log(2) > (1/ log(2) âˆ’1)/2, for some (small enough) numerical
constants Î´1, Î´2 > 0, if n â‰¥L(Ïƒ2),
(1+2Î±) log(2) â‰¥nÎ´2 ,
taking u = Î´2
1pâˆ—in Eq. (52) yields
(bsâˆ(X; Dn ) âˆ’s(X))2 i
as soon as n â‰¥L(Ïƒ2).
In case (ii), a similar reasoning with some integer
4 + log n âˆ’
4 + log n âˆ’
(bsâˆ(X; Dn ) âˆ’s(X))2 i
â‰¤L(s, d, Ïƒ2)nâˆ’2Î± log 222Î±
In particular, we get a rate of order nâˆ’(2Î± log 2âˆ’Î´) for every Î´ > 0, which is slightly worse than the
rate nâˆ’2Î±/(2Î±+1) for d â‰¥4 since log 2 â‰¤1/(1 + 2Î±).
For lower bounding the risk of a single tree, we apply Eq. (9) in Proposition 2. By Eq. (47),
if p â‰¥p0 = L(s, d) and k = 2p,
,1 â‰¥L(s, d)kâˆ’Î± âˆ’L(s, d)kâˆ’2Î± â‰¥L(s, d)kâˆ’Î±
so that Eq. (4), Proposition 2 and Eq. (50) in Lemma 10 imply, if U âˆ¼Ubprf
with p0 â‰¤p â‰¤
4 + log n âˆ’
(bs (X; U; Dn ) âˆ’s(X))2 i
k=2p,p0â‰¤pâ‰¤
L(s, d)kâˆ’Î± + Ïƒ2k (1 âˆ’2Îº)
Here, again, we must distinguish the cases (i) d â‰¤3 and (ii) d â‰¥4. If d â‰¤3, by Lemma 20,
Eq. (54) shows that if in addition n â‰¥L(d, Ïƒ2),
(bs (X; U; Dn ) âˆ’s(X))2 i
If d â‰¥4, Eq. (54) shows that
(bs (X; U; Dn ) âˆ’s(X))2 i
0â‰¤kâ‰¤25/2nlog(2)
â‰¥L(s, d)nâˆ’Î± log(2)
for n â‰¥L(Ïƒ2, d), since the function x â†’xâˆ’Î± + Ïƒ2x
is then decreasing on (0, (nÎ±/Ïƒ2)1/(Î±+1)] and
1 + log(8/7)
> log(2) .
So, in both cases (d â‰¤3 or d â‰¥4), the inï¬nite forest has a faster rate of convergence (in terms
of risk) than a single tree. But, even with an inï¬nite forest with d â‰¤3, since
1 + 2 log(2)d <
the rate obtained is slower than the minimax rate nâˆ’4/(d+4) over the set of C2 functions .
Intuitively, the BPRF model is not minimax because it is not adaptive enough. Indeed, the
partitioning process splits each set of the current partition regardless of its size: so a relatively
small set is still split the same number of times than a relatively large set. We conjecture that
a partitioning scheme with a random choice of the next set to be split, with a probability of
choosing each set proportional to its sizeâ€”as in the PURF model, see Section 1.1â€”, would be
better and could reach the minimax rate for C2 functions. This is proved for d = 1 in Section 5.
Finally, we note that the UBPRF model 2 would certainly suï¬€er from the same lack of
adaptivity because the next set to be split is chosen with a uniform distribution on all sets. So,
this model would certainly not be minimax either, and we conjecture that it would be even worse
than the BPRF model.
Tighter bound for three times diï¬€erentiable functions
The bounds in Corollary 9 are tight when s is smooth enough, as shown by the following corollary
of Proposition 3.
Corollary 11. Let p â‰¥2 and assume (H3a) and (Unif) hold true. Then, for every x âˆˆ[0, 1)d,
âˆ‡s(x) Â· (1 âˆ’2x) +
(x)xi(1 âˆ’xi)
As a consequence,
[0,1)d BUbprf
,âˆ(x) dx âˆ’1
âˆ‡s(x) Â· (1 âˆ’2x) +
(x)xi(1 âˆ’xi)
xâˆˆ[0,1)d âˆ¥âˆ‡s(x)âˆ¥2
xâˆˆ[0,1)d max
Corollary 11 is proved in Section D.5.
Size of the forest
As for the previous models, under (H3a), Corollary 11 and Eq. (2) show a BPRF forest of size
q has an approximation error of the same order of magnitude as an inï¬nite forest when
q â‰¥kÎ± = 2Î±p =
Simulation experiments
In order to illustrate mathematical results from previous sections, we lead some simulation experiments with R , focusing on approximation errors, as deï¬ned in Section 2.3.
We consider the models from Sections 4â€“6 (toy, PURF, BPRF), with d = 1 for toy and PURF,
and d âˆˆ{1, 5, 10} for BPRF. In addition, we consider a PRF model discussed in Section 3 of
Biau , that we call Hold-out RF in the following. Hold-out RF is the original RF model 1
except that the tree partitioning is performed using an extra sample Dâ€²
n, independent from the
learning sample Dn. As a consequence, assumption (PR) holds for the Hold-out RF model,
so decomposition (3) is valid and we can compute the corresponding approximation error, as a
function of the number q of trees in the forest.
For all experiments, we take the input space X = [0, 1)d and suppose that (Unif) holds. We
choose the following regression functions:
â€¢ sinusoidal (if d = 1): x 7â†’sin(2Ï€x),
â€¢ absolute value (if d = 1): x 7â†’
â€¢ sum (for any d â‰¥1): x 7â†’
â€¢ Friedman1 (for any d â‰¥5):
x 7â†’1/10 Ã—
10 sin(Ï€x1x2) + 20(x3 âˆ’0.5)2 + 10x4 + 5x5
which is proportional to the Friedman1 function that was introduced by Friedman .
Here we add the scaling factor 1/10 in order to have a function with a range comparable
to that of sum.
For all PRF models, we choose k, the number of leaves (minus one for toy and PURF), among
25, 26, 27, 28, 29
; the last value 29 is sometimes removed for computational reasons.
Quantities BU,1 and BU,âˆare estimated by Monte-Carlo approximation using:
â€¢ 1000 realizations of X,
â€¢ for BU,1, 500 realizations of U,
â€¢ for BU,âˆ, k2 realizations of U for toy and PURF models, k2Î± realizations for BPRF model
with Î± = âˆ’log(1 âˆ’1/(2d))/ log(2) (which ensures our estimation of the convergence rates
is precise enough, according to our theoretical results), and k2 realizations for Hold-out
RF model, which empirically appears to be suï¬ƒcient for estimating the convergence rates
correctly for this RF model.
Furthermore, for each computation of BU,1 and BU,âˆwe add some â€œborderlessâ€ estimations of
the bias, that is, integrating only over x âˆˆ[Ç«, 1âˆ’Ç«] with Ç« = Îµtoy
depending on the model.
In addition, for the Hold-out RF model:
â€¢ we simulate the Dâ€²
n sample with n = k2 (for each value of k) and choose a gaussian random
noise with variance Ïƒ2 = 1/16,
â€¢ we use the randomForest R package to build the trees on the
n: we use parameters maxnodes (to control the number of leaves) and ntree (to
set the number of trees), and take the default values for all other parameters (in particular
Finally, for each scenario, we plot the bias as a function of k in log2-log2 scale, and estimate
the slope of the plot by ï¬tting a simple linear model in order to get an approximation of the
convergence rates.
One-dimensional input space
We consider in this subsection the one-dimensional case (d = 1). Figure 3 shows results for
the sinusoidal regression function. Plots are in log2-log2 scale, so as expected we obtain linear
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’1.99)
tree BL (r=âˆ’1.98)
forest (r=âˆ’2.94)
forest BL (r=âˆ’3.88)
(a) toy, d = 1
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’1.9)
tree BL (r=âˆ’1.86)
forest (r=âˆ’3.05)
forest BL (r=âˆ’3.96)
(b) PURF, d = 1
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.97)
tree BL (r=âˆ’1)
forest (r=âˆ’1.85)
forest BL (r=âˆ’1.87)
(c) BPRF, d = 1
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’1.24)
tree BL (r=âˆ’1.27)
forest (r=âˆ’1.35)
forest BL (r=âˆ’1.37)
(d) Hold-out RF, d = 1
Plot of BU,1 and BU,âˆ(in log2-scale) against k (in log2-scale) for (a) toy, (b) PURF,
(c) BPRF and (d) Hold-out RF models, for the sinusoidal regression function. BL corresponds
to borderless situations and r denotes the slope of a linear model ï¬tted to the scatter plot.
behaviors. For toy and PURF models (top graphs) we get decreasing rates very close to what
can be expected from Sections 4â€“5: kâˆ’2 for trees (with or without borders), kâˆ’3 for forests and
kâˆ’4 for borderless forests. Similarly, we get the right decreasing rates for BPRF model (bottom
left graph): indeed, if d = 1 then Î± = 1, so trees and forests rates are respectively kâˆ’1 and kâˆ’2.
For Hold-out RF model we get rates about kâˆ’1.25 for trees and kâˆ’1.35 for forests as expected from
Section 6. These rates are surprisingly slow (in particular compared to toy and PURF models)
and a forest does not bring much improvement compared to a single tree. But as shown in the
next section, the one-dimensional case is not the best framework for the Hold-Out-RF model
compared to other PRF models.
Results for the absolute value regression function are presented in Figure 4. The absolute
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’2)
tree BL (r=âˆ’2.01)
forest (r=âˆ’2.89)
forest BL (r=âˆ’3.04)
(a) toy, d = 1
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’1.95)
tree BL (r=âˆ’1.97)
forest (r=âˆ’2.86)
forest BL (r=âˆ’2.78)
(b) PURF, d = 1
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.93)
tree BL (r=âˆ’0.94)
forest (r=âˆ’2.04)
forest BL (r=âˆ’2.02)
(c) BPRF, d = 1
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’1.1)
tree BL (r=âˆ’1.18)
forest (r=âˆ’1.14)
forest BL (r=âˆ’1.25)
(d) Hold-out RF, d = 1
Plot of BU,1 and BU,âˆ(in log2-scale) against k (in log2-scale) for (a) toy, (b) PURF, (c)
BPRF and (d) Hold-out RF models, for the absolute value regression function. BL corresponds
to borderless situations and r denotes the slope of a linear model ï¬tted to the scatter plot.
value regression function presents a singularity at point x = 1/2, and it acts as a border point.
Hence, compared to the sinusoidal regression function, the only change is that there is no diï¬€erences between borderless and regular approximation errors of forests: both reach the rate kâˆ’3
for toy and PURF models. The Hold-out RF model again reaches relatively poor rates, and the
forest does not improve signiï¬cantly the bias compared to a single tree.
Multidimensional input space
For d > 1, we investigate the behaviors of BPRF and Hold-out RF models. First, Figure 5 shows
the results for the sum regression function when d âˆˆ{5, 10}.
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.154)
forest (r=âˆ’0.309)
(a) BPRF, d = 5
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.072)
forest (r=âˆ’0.147)
(b) BPRF, d = 10
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.263)
forest (r=âˆ’0.435)
(c) Hold-out RF, d = 5
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.203)
forest (r=âˆ’0.332)
(d) Hold-out RF, d = 10
Plot of BU,1 and BU,âˆ(in log2-scale) against k (in log2-scale) for BPRF with (a) d = 5
and (b) d = 10, and Hold-out RF model with (c) d = 5 and (d) d = 10, for the sum regression
function. r denotes the slope of a linear model ï¬tted to the scatter plot.
As in the one-dimensional case, we observe linear behaviors in log2-log2 scale. For BPRF
(top graphs), trees and forests reach approximately the decreasing rates we can expect from
Section 6, respectively kâˆ’Î± and kâˆ’2Î± with Î± = log(10/9)/ log(2) â‰ˆ0.152 when d = 5 and
Î± = log(20/19)/ log(2) â‰ˆ0.074 when d = 10.
Compared to BPRF, the Hold-out RF model reaches better rates in the multidimensional
framework. Moreover, it suï¬€ers less from the increase of the dimension: BPRF rates are divided
by 2.1 when d increases from 5 to 10, whereas Hold-out RF model rates are only divided by 1.3.
Forests rates with the Hold-out RF model are about 1.6 times faster than tree rates, which
illustrates a signiï¬cant gain brought by forests. Note however the comparison with BPRF model
is partly unfair, because Hold-out RF can make use of an extra sample Dâ€²
n for building appropriate
partitions of X; nevertheless, with BPRF, if such an extra sample is available, it can only be
used for reducing the ï¬nal risk by a constant factor (since it doubles the sample size) but not for
improving the risk rate.
Results for the Friedman1 regression function are shown in Figure 6. Note that when d = 10,
the last ï¬ve variables are non-informative since the regression function does not depend on them.
For BPRF, rates are slightly worse than for sum, and we still observe a factor of 2 between
trees and forests rates. The decrease of the rates might be explained by two reasons: the complexity of Friedman1 function, and when d = 10 the presence of ï¬ve non-informative variables.
For the Hold-out RF model, rates are still much better than for BPRF, and forest rates
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.132)
forest (r=âˆ’0.24)
(a) BPRF, d = 5
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.058)
forest (r=âˆ’0.121)
(b) BPRF, d = 10
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.23)
forest (r=âˆ’0.352)
(c) Hold-out RF, d = 5
Bias (logâˆ’scale)
Cardinal of partitions (logâˆ’scale)
tree (r=âˆ’0.291)
forest (r=âˆ’0.393)
(d) Hold-out RF, d = 10
Plot of BU,1 and BU,âˆ(in log2-scale) against k (in log2-scale) for BPRF with (a) d = 5
and (b) d = 10, and Hold-out RF model with (c) d = 5 and (d) d = 10, for the Friedman1
regression function. r denotes the slope of a linear model ï¬tted to the scatter plot.
are better than tree rates by a factor 1.5. When d = 5, rates are smaller than for the sum
regression function, probably due to the complexity of the regression function. When d = 10, we
surprisingly get faster rates than when d = 5. Obtaining approximation rates as good for d = 10
as for d = 5 is expected since the number of informative variables is the same in both cases, and
Hold-out RF are known to adapt to the sparsity of the regression function (see Section 8.1 for
more details). But obtaining signiï¬cantly better rates for a more diï¬ƒcult problem (when d = 10)
is quite surprising. Investigating this phenomenon requires a more systematic simulation study
with more examples, which is out of the scope of the present paper.
Discussion
In this paper, we analyze several purely random forests models and show, for each of them, that
an inï¬nite forest improves by an order of magnitude the approximation error of a single tree,
assuming the regression function is smooth. Since the estimation error of a forest is smaller or
equal to that of a single tree, we deduce that forests reach a strictly better risk convergence rate.
Comparison between the toy, PURF and BPRF models
In dimension d = 1, we can compare the results for the BPRF model in Section 6 with the results
obtained in Sections 4 and 5 for the toy and PURF models. Indeed, we get
so that the approximation error of an inï¬nite forest is of order kâˆ’2 for BPRF, instead of kâˆ’4 for
toy and PURF (when avoiding border eï¬€ects).
Intuitively, it seems the BPRF model tends to leave large â€œholesâ€ in the space X, whereas the
toy model is almost regular, and the PURF model stays close to regular. Recall the PURF model
can be seen as a recursive tree construction, where only one leaf is split into two leaves at each
step, and the choice of this leaf is made with probability equal to its size. On the contrary, the
BPRF model keeps splitting all leaves whatever their size, which may lead to very small leaves
but also to much larger ones in some signiï¬cant part of the space X.
Comparison with other random forest models
Another random forest model has been suggested by Breiman and was more precisely
analyzed by Biau . In the latter paper, the random partitioningâ€”which depends on some
parameters (pi)1â‰¤iâ‰¤d âˆˆ[0, 1)d with p1 + Â· Â· Â· + pd = 1â€”is as follows:
â€¢ Start from [0, 1)d,
â€¢ Repeat k times: for each set Î» of the current partition,
â€“ choose a split variable j randomly, with probability distribution over {1, . . . , d} given
by (pi)1â‰¤iâ‰¤d,
â€“ split Î» along coordinate j at the midpoint tj of Î»j, that is put {x âˆˆÎ» / xj < tj} and
{x âˆˆÎ» / xj â‰¥tj} at the two children nodes below Î».
In a framework where only S variables (among d) are â€œstrongâ€ (i.e. have an eï¬€ect on the
output variable), the main result of Biau is that if the probability weights are well tuned
(i.e., roughly, if pj â‰ˆ1/S for a strong variable and pj â‰ˆ0 for a noisy variable), then for model 4,
the inï¬nite forest rate of convergence only depends on S and is faster than the minimax rate in
dimension d. In other words, Biau shows that such random forests adapt to sparsity.
Even if the framework of Biau is quite diï¬€erent from ours, let us give a quick comparison
between the diï¬€erent rates obtained when S = d. Assuming the regression function is Lipschitz,
for the model studied by Biau , the inï¬nite forest bias is at most of order kâˆ’Î¸ with Î¸ =
3/(4 log(2)d). This is comparable to our result kâˆ’2Î± for BPRF, because 2Î± â‰ˆ1/(log(2)d) when
d is large enough. Our rate for BPRF is a little bit faster, but recall that we make a stronger
assumption on the smoothness of the regression function (C2 instead of Lipschitz), and we only
consider d = S. The problem of knowing if the combination of these two analyses could give
better rates (in a sparse framework with C2 regression function) is beyond the scope of the paper
and we let this point for future research.
Finally, as mentioned in Section 6.4, we conjecture the BPRF model reaches better rates
than the UBPRF model 2. Intuitively, as we discuss in Section 8.1, UBPRF model tends to leave
even larger â€œholesâ€ in X than BPRF, because instead of constructing balanced trees, it randomly
chooses at each step the next leaf to be split, with a uniform distribution on all leaves.
General conclusions
For all PRF models studied in this paper, we get that the inï¬nite forest bias order of magnitude
is equal to the square of the single tree bias. Consequently, if a single tree reaches the minimax
convergence rate for C1 functions, we directly have that a large enough forest is minimax for C2
functions. So, compared to trees, forests can well approximate regression functions with one more
level of smoothness. Further research is needed to know whether this phenomenon is general for
all PRF models.
Interestingly, our analysis helps to suggest better PRF partitioning mechanism. It seems PRF
models beneï¬t from a choice of the next set of the current partition to be split with a probability
proportional to the size of the set. This statement is justiï¬ed by the comparison between BPRF
and PURF models in dimension 1, and it leads us to conjecture that PRF models reaching C2minimax rates of convergence could also be derived in dimension d. For instance, this should
be the case with the generalization of the PURF model to any d â‰¥1, consisting in replacing
â€œlengthâ€ by â€œvolumeâ€ in Model 3, and by choosing uniformly a coordinate j before performing
the split along it.
For practical use of PRF models, we suggest an order of magnitude for the number of trees
in a forest that is suï¬ƒcient to get a bias term as small as for an inï¬nite forest. More precisely, if
the bias of a single tree is of order kâˆ’Î³ for some Î³ > 0, our results suggest it is suï¬ƒcient to build
q = kÎ³ trees to get a forest which reaches same rates as the (theoretical) inï¬nite forest.
Finally, we mention that all our general results in Sections 2â€“3 can be applied for any random
forest satisfying assumption (PR), that is, when random partitions are obtained independently
from the learning sample Dn.
Hence, if for a random forest model, we are able to compute
quantities appearing in Proposition 3, we can deduce results on the bias and risk convergence
rates for these random forests. In particular, we have in mind the Hold-out RF model deï¬ned in
Section 7. Addressing this point appears to be an interesting future research topic, not only from
the theoretical point of view, but also in practice because the Hold-out RF model can achieve
very good performances.
Acknowledgments
The authors are grateful to Guillaume Obozinski and Francis Bach for several discussions. The
authors acknowledge the partial support of French Agence Nationale de la Recherche, under grants
Detect (ANR-09-JCJC-0027-01) and Calibration .