Learning Spatially Regularized Correlation Filters for Visual Tracking
Martin Danelljan, Gustav H¨ager, Fahad Shahbaz Khan, Michael Felsberg
Computer Vision Laboratory, Link¨oping University, Sweden
{martin.danelljan, gustav.hager, fahad.khan, michael.felsberg}@liu.se
Robust and accurate visual tracking is one of the most
challenging computer vision problems. Due to the inherent
lack of training data, a robust approach for constructing a
target appearance model is crucial. Recently, discriminatively learned correlation ﬁlters (DCF) have been successfully applied to address this problem for tracking. These
methods utilize a periodic assumption of the training samples to efﬁciently learn a classiﬁer on all patches in the target neighborhood. However, the periodic assumption also
introduces unwanted boundary effects, which severely degrade the quality of the tracking model.
We propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. A spatial regularization component is introduced in the learning to penalize
correlation ﬁlter coefﬁcients depending on their spatial location. Our SRDCF formulation allows the correlation ﬁlters to be learned on a signiﬁcantly larger set of negative
training samples, without corrupting the positive samples.
We further propose an optimization strategy, based on the iterative Gauss-Seidel method, for efﬁcient online learning of
our SRDCF. Experiments are performed on four benchmark
datasets: OTB-2013, ALOV++, OTB-2015, and VOT2014.
Our approach achieves state-of-the-art results on all four
datasets. On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0% and 8.2% respectively, in mean overlap
precision, compared to the best existing trackers.
1. Introduction
Visual tracking is a classical computer vision problem
with many applications. In generic tracking the task is to
estimate the trajectory of a target in an image sequence,
given only its initial location. This problem is especially
challenging. The tracker must generalize the target appearance from a very limited set of training samples to achieve
robustness against, e.g. occlusions, fast motion and deformations. Here, we investigate the key problem of learning a
robust appearance model under these conditions.
Recently, Discriminative Correlation Filter (DCF) based
(a) Original image.
(b) Periodicity in correlation ﬁlters.
Figure 1. Example image (a) and the underlying periodic assumption (b) employed in the standard DCF methods. The periodic
assumption (b) leads to a limited set of negative training samples,
that fails to capture the true image content (a). As a consequence,
an inaccurate tracking model is learned.
approaches have successfully been applied to the tracking problem . These methods learn a
correlation ﬁlter from a set of training samples. The correlation ﬁlter is trained to perform a circular sliding window
operation on the training samples. This corresponds to assuming a periodic extension of these samples (see ﬁgure 1).
The periodic assumption enables efﬁcient training and detection by utilizing the Fast Fourier Transform (FFT).
As discussed above, the computational efﬁciency of the
standard DCF originates from the periodic assumption at
both training and detection. However, this underlying assumption produces unwanted boundary effects. This leads
to an inaccurate representation of the image content, since
the training patches contain periodic repetitions. The induced boundary effects mainly limit the standard DCF formulation in two important aspects. Firstly, inaccurate negative training patches reduce the discriminative power of the
learned model. Secondly, the detection scores are only accurate near the center of the region, while the remaining
scores are heavily inﬂuenced by the periodic repetitions of
the detection sample. This leads to a very restricted target
search region at the detection step.
The aforementioned limitations of the standard DCF forarXiv:1608.05571v1 [cs.CV] 19 Aug 2016
mulation hamper the tracking performance in several ways.
(a) The DCF based trackers struggle in cases with fast target
motion due to the restricted search region. (b) The lack of
negative training patches leads to over-ﬁtting of the learned
model, signiﬁcantly affecting the performance in cases with
e.g. target deformations.
(c) The mentioned limitations
in training and detection also reduce the potential of the
tracker to re-detect the target after an occlusion. (d) A naive
expansion of the image area used for training the correlation ﬁlter corresponds to using a larger periodicity (see ﬁgure 1). Such an expansion results in an inclusion of substantial amount of background information within the positive
training samples. These corrupted training samples severely
degrade the discriminative power of the model, leading to
inferior tracking results. In this work, we tackle these inherent problems by re-visiting the standard DCF formulation.
1.1. Contributions
In this paper, we propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. We introduce a spatial regularization component within the DCF formulation, to address the problems induced by the periodic
assumption. The proposed regularization weights penalize
the correlation ﬁlter coefﬁcients during learning. The spatial weights are based on the a priori information about the
spatial extent of the ﬁlter. Due to the spatial regularization,
the correlation ﬁlter can be learned on larger image regions.
This enables a larger set of negative patches to be included
in the training, leading to a more discriminative model.
Due to the online nature of the tracking problem, a computationally efﬁcient learning scheme is crucial. Therefore,
we introduce a suitable optimization strategy for the proposed SRDCF. The online capability is achieved by exploiting the sparsity of the spatial regularization function in the
Fourier domain. We propose to apply the iterative Gauss-
Seidel method to solve the resulting normal equations. Additionally, we introduce a strategy to maximize the detection
scores with sub-grid precision.
We perform comprehensive experiments on four benchmark datasets: OTB-2013 with 50 videos, ALOV++
 with 314 videos, VOT2014 with 25 videos and
OTB-2015 with 100 videos.
Compared to the best
existing trackers, our approach obtains an absolute gain of
8.0% and 8.2% on OTB-2013 and OTB-2015 respectively,
in mean overlap precision. Our method also achieves the
best overall results on ALOV++ and VOT2014. Additionally, our tracker won the OpenCV State of the Art Vision
Challenge in tracking (there termed DCFSIR).
2. Discriminative Correlation Filters
Discriminative correlation ﬁlters (DCF) is a supervised
technique for learning a linear classiﬁer or a linear regressor. The main difference from other techniques, such
as support vector machines , is that the DCF formulation exploits the properties of circular correlation for ef-
ﬁcient training and detection.
In recent years, the DCF
based approaches have been successfully applied for tracking. Bolme et al. ﬁrst introduced the MOSSE tracker,
using only grayscale samples to train the ﬁlter.
work have shown a notable improvement
by learning multi-channel ﬁlters on multi-dimensional features, such as HOG or Color-Names . However,
to become computationally viable, these approaches rely
on harsh approximations of the standard DCF formulation,
leading to sub-optimal learning. Other work have investigated ofﬂine learning of multi-channel DCF:s for object detection and recognition . But these methods are
too computationally costly for online tracking applications.
The circular correlation within the DCF formulation has
two major advantages. Firstly, the DCF is able to make
extensive use of limited training data by implicitly including all shifted versions of the given samples. Secondly, the
computational effort for training and detection is signiﬁcantly reduced by performing the necessary computations
in the Fourier domain and using the Fast Fourier Transform
(FFT). These two advantages make DCF:s especially suitable for tracking, where training data is scarce and computational efﬁciency is crucial for real-time applications.
By employing a circular correlation, the standard DCF
formulation relies on a periodic assumption of the training and detection samples. However, this assumption produces unwanted boundary effects, leading to an inaccurate description of the image.
These inaccurate training
patches severely hamper the learning of a discriminative
tracking model. Surprisingly, this problem has been largely
ignored by the tracking community. Galoogahi et al. 
investigate the boundary effect problem for single-channel
DCF:s. Their approach solve a constrained optimization
problem, using the Alternating Direction Method of Multipliers (ADMM), to ensure a correct ﬁlter size. This however
requires a transition between the spatial and Fourier domain
in each ADMM iteration, leading to an increased computational complexity. Different to , we propose a spatial
regularization component in the objective. By exploiting
the sparsity of our regularizer, we efﬁciently optimize the
ﬁlter directly in the Fourier domain. Contrary to , we
target the problem of multi-dimensional features, such as
HOG, crucial for the overall tracking performance .
2.1. Standard DCF Training and Detection
In the DCF formulation, the aim is to learn a multichannel convolution1 ﬁlter f from a set of training examples {(xk, yk)}t
k=1. Each training sample xk consists of
a d-dimensional feature map extracted from an image re-
1We use convolution for mathematical convenience, though correlation
can equivalently be used.
gion. All samples are assumed to have the same spatial
size M × N.
At each spatial location (m, n) ∈Ω:=
{0, . . . , M −1} × {0, . . . , N −1} we thus have a ddimensional feature vector xk(m, n) ∈Rd. We denote feature layer l ∈{1, . . . , d} of xk by xl
k. The desired output
yk is a scalar valued function over the domain Ω, which includes a label for each location in the sample xk.
The desired ﬁlter f consists of one M × N convolution
ﬁlter f l per feature layer. The convolution response of the
ﬁlter f on a M × N sample x is given by
Here, ∗denotes circular convolution. The ﬁlter is obtained
by minimizing the L2-error between the responses Sf(xk)
on the training samples xk, and the labels yk,
Sf(xk) −yk
Here, the weights αk ≥0 determine the impact of each
training sample and λ ≥0 is the weight of the regularization term. Eq. 2 is a linear least squares problem. Using
Parseval’s formula, it can be transformed to the Fourier domain, where the resulting normal equations have a block diagonal structure. The Discrete Fourier Transformed (DFT)
ﬁlters ˆf l = F{f l} can then be obtained by solving MN
number of d × d linear equation systems .
For efﬁciency reasons, the learned DCF is typically applied in a sliding-window-like manner by evaluating the
classiﬁcation scores on all cyclic shifts of a test sample. Let
z denote the M × N feature map extracted from an image
region. The classiﬁcation scores Sf(z) at all locations in
this image region can be computed using the convolution
property of the DFT,
Sf(z) = F −1
ˆzl · ˆf l
Here, · denotes point-wise multiplication, the hat denotes
the DFT of a function and F −1 denotes the inverse DFT.
The FFT hence allows the detection scores to be computed
in O(dMN log MN) complexity instead of O(dM 2N 2).
Note that the operation Sf(x) in (1) corresponds to applying the linear classiﬁer f, in a sliding window fashion, to
the periodic extension of the sample x (see ﬁgure 1). This
introduces unwanted periodic boundary effects in the training (2) and detection (3) steps.
3. Spatially Regularized Correlation Filters
We propose to use a spatial regularization component in
the standard DCF formulation. The resulting optimization
problem is solved in the Fourier domain, by exploiting the
sparse nature of the proposed regularization.
Figure 2. Visualization of the spatial regularization weights w employed in the learning of our SRDCF, and the corresponding image region used for training. Filter coefﬁcients residing in the
background region are penalized by assigning higher weights in
w. This signiﬁcantly mitigates the emphasis on background information in the learned classiﬁer.
3.1. Spatial Regularization
To alleviate the problems induced by the circular convolution in (1), we replace the regularization term in (2) with
a more general Tikhonov regularization. We introduce a
spatial weight function w : Ω→R used to penalize the
magnitude of the ﬁlter coefﬁcients in the learning. The regularization weights w determine the importance of the ﬁlter
coefﬁcients f l, depending on their spatial locations. Coefﬁcients in f l residing outside the target region are suppressed
by assigning higher weights in w and vice versa. The resulting optimization problem is expressed as,
Sf(xk) −yk
The regularization weights w in (4) are visualized in ﬁgure 2. Visual features close to the target edge are often less
reliable than those close to the target center, due to e.g. target rotations and occlusions. We therefore let the regularization weights change smoothly from the target region to
the background. This also increases the sparsity of w in
the Fourier domain. Note that (4) simpliﬁes to the standard
DCF (2) for uniform weights w(m, n) =
By applying Parseval’s theorem to (4), the ﬁlter f can
equivalently be obtained by minimizing the resulting loss
function (5) over the DFT coefﬁcients ˆf,
k· ˆf l−ˆyk
The second term in (5) follows from the convolution property of the inverse DFT. A vectorization of (5) gives,
k)ˆf l−ˆyk
(a) Standard DCF.
(b) Our SRDCF.
Figure 3. Visualization of the ﬁlter coefﬁcients learned using the standard DCF (a) and our approach (b). The surface plots show the ﬁlter
values f l and the corresponding image region used for training. In the standard DCF, high values are assigned to the background region.
The larger inﬂuence of background information at the detection stage deteriorates tracking performance. In our approach, the regularization
weights penalizes ﬁlter values corresponding to features in the background. This increases the discriminative power of the learned model,
by emphasizing the appearance information within the target region (green box).
Here, bold letters denote a vectorization of the corresponding scalar valued functions and D(v) denotes the diagonal matrix with the elements of the vector v in its diagonal. The MN × MN matrix C(ˆw) represents circular 2Dconvolution with the function ˆw, i.e. C(ˆw)ˆf l = vec( ˆw∗ˆf l).
Each row in C(ˆw) thus contains a cyclic permutation of ˆw.
The DFT of a real-valued function is known to be Hermitian symmetric. Therefore, minimizing (4) over the set
of real-valued ﬁlters f l, corresponds to minimizing (5) over
the set of Hermitian symmetric DFT coefﬁcients ˆf l. We
reformulate (6) to an equivalent real-valued optimization
problem, to ensure faster convergence by preserving the
Hermitian symmetry. Let ρ : Ω→Ωbe the point-reﬂection
ρ(m, n) = (−m mod M, −n mod N). The domain Ω
can be partitioned into Ω0, Ω+ and Ω−, where Ω0 = ρ(Ω0)
and Ω−= ρ(Ω+). Thus, Ω0 denote the part of the spectrum
with no corresponding reﬂected frequency, and Ω−contains
the reﬂected frequencies in Ω+. We deﬁne,
˜f l(m, n) =
ˆf l(m, n),
(m, n) ∈Ω0
f l(m,n)+ ˆ
f l(ρ(m,n))
(m, n) ∈Ω+
f l(m,n)−ˆ
f l(ρ(m,n))
(m, n) ∈Ω−
such that ˜f l is real-valued by the Hermitian symmetry of ˆf l.
Here, i denotes the imaginary unit. Eq. 7 can be expressed
by a unitary MN × MN matrix B such that ˜f l = Bˆf l. By
(7), B contains at most two non-zero entries in each row.
The reformulated variables from (6) are deﬁned as ˜yk =
k = BD(ˆxl
k)BH and C =
MN BC(ˆw)BH, where
H denotes the conjugate transpose of a matrix. Since B is
unitary, (6) can equivalently be expressed as,
˜ε(˜f 1 . . .˜f d) =
k˜f l −˜yk
All variables in (8) are real-valued. The loss function (8) is
then simpliﬁed by deﬁning the fully vectorized real-valued
ﬁlter as the concatenation ˜f =
 (˜f 1)T · · · (˜f d)TT,
Here we have deﬁned the concatenation Dk = (D1
k · · · Dd
and W to be the dMN × dMN block diagonal matrix with
each diagonal block being equal to C. Finally, (9) is minimized by solving the normal equations At˜f = ˜bt, where
kDk + W TW
Here, (10) deﬁnes a real dMN × dMN linear system
of equations. The fraction of non-zero elements in At is
smaller than 2d+K2
dMN , where K is the number of non-zero
Fourier coefﬁcients in ˆw. Thus, At is sparse if w has a
sparse spectrum. The DFT coefﬁcients for the ﬁlters are obtained by solving the system (10) and applying ˆf l = BH˜f l.
Figure 3 visualizes the ﬁlter learned by optimizing the
standard DCF loss (2) and the proposed formulation (4),
using the spatial regularization weights w in ﬁgure 2. In the
standard DCF, large values are spatially distributed over the
whole ﬁlter. By penalizing ﬁlter coefﬁcients corresponding
to background, our approach learns a classiﬁer that emphasizes visual information within the target region.
A direct application of a sparse solver to the normal
equations At˜f = ˜bt is computationally very demanding
(even when the standard regularization W TW = λI is used
and the number of features is small (d > 2)). Next, we propose an efﬁcient optimization scheme to solve the normal
equations for online learning scenarios, such as tracking.
3.2. Optimization
For the standard DCF formulation (2) the normal equations have a block diagonal structure . However, this
block structure is not attainable in our case due to the structure of the regularization matrix W TW in (10a). We propose an iterative approach, based on the Gauss-Seidel, for
efﬁcient online computation of the ﬁlter coefﬁcients.
The Gauss-Seidel method decomposes the matrix At
into a lower triangular part Lt and a strictly upper triangular part Ut such that At = Lt + Ut. The algorithm then
proceeds by solving the following triangular system for ˜f (j)
in each iteration j = 1, 2, . . .,
Lt˜f (j) = ˜bt −Ut˜f (j−1).
This lower triangular equation system is solved efﬁciently
using forward substitution and by exploiting the sparsity of
Lt and Ut. The Gauss-Seidel recursion (11) converges to
the solution of At˜f = ˜b whenever the matrix At is symmetric and positive deﬁnite. The construction of the weights w
(see section 5.1) ensures that both conditions are satisﬁed.
4. Our Tracking Framework
Here, we describe our tracking framework, based on
the Spatially Regularized Discriminative Correlation Filters
(SRDCF) proposed in section 3.
4.1. Training
At the training stage, the model is updated by ﬁrst extracting a new training sample xt centered at the target location. Here, t denotes the current frame number. We then
update At and ˜bt in (10) with a learning rate γ ≥0,
At = (1 −γ)At−1 + γ
t Dt + W TW
˜bt = (1 −γ)˜bt−1 + γDT
This corresponds to using exponentially decaying weights
αk in the loss function (4).
In the ﬁrst frame, we set
1D1 + W TW and ˜b1 = DT
1 ˜y1. Note that the
regularization matrix W TW can be precomputed once for
the entire sequence. The update strategy (12) ensures memory efﬁciency, since it does not require storage of all samples xk. After the model update (12), we perform a ﬁxed
number NGS of Gauss-Seidel iterations (11) per frame to
compute the new ﬁlter coefﬁcients.
For the initial iteration ˜f (0)
in frame t, we use the ﬁlter
computed in the previous frame, i.e. ˜f (0)
= ˜f (NGS)
t−1 . In the
ﬁrst frame, the initial estimate ˜f (0)
is obtained by solving
the MN × MN linear system,
for l = 1, . . . , d.
This provides a starting point for the
Gauss-Seidel optimization in the ﬁrst frame. The systems
in (13) share the same sparse coefﬁcients and can be solved
efﬁciently with a direct sparse solver.
4.2. Detection
At the detection stage, the location of the target in a new
frame t is estimated by applying the ﬁlter ˆft−1 that has been
updated in the previous frame. Similar to , we apply the
ﬁlter at multiple resolutions to estimate changes in the target
size. The samples {zr}r∈{⌊1−S
2 ⌋,...,⌊S−1
2 ⌋} are extracted
centered at the previous target location and at the scales ar
relative to the current target scale. Here, S denotes the number of scales and a is the scale increment factor. The sample
zr is constructed by resizing the image according to ar before the feature computation.
Fast Sub-grid Detection: Generally, the training and detection samples xk and zk are constructed using a grid
strategy with a stride greater than one pixel.
This leads
to only computing the detection scores (3) on a coarser
We employ an interpolation approach that allows
computation of pixel-dense detection scores.
The detection scores (3) are efﬁciently interpolated with trigonometric polynomials by utilizing the computed DFT coefﬁcients.
Let ˆs := F{Sf(z)} = Pd
l=1 ˆzl · ˆf l be the DFT of the
detection scores Sf(z) evaluated at the sample z. The detection scores s(u, v) at the continuous locations (u, v) ∈
[0, M) × [0, N) in z are interpolated as,
ˆs(m, n)ei2π( m
Here, i denotes the imaginary unit.
We aim to ﬁnd the
sub-grid location that corresponds to the maximum score:
(u∗, v∗) = arg max(u,v)∈[0,M)×[0,N) s(u, v). The scores
s are ﬁrst evaluated at all grid locations s(m, n) using
(3). The location of the maximal score (u(0), v(0)) ∈Ω
is used as the initial estimate. We then iteratively maximize (14) using Newton’s method, by starting at the location (u(0), v(0)). The gradient and Hessian in each iteration
are computed by analytically differentiating (14). We found
that only a few iterations is sufﬁcient for convergence.
We apply the sub-grid interpolation strategy to maximize
the classiﬁcation scores sr computed at the sample zr. The
procedure is applied for each scale level independently. The
scale level with the highest maximal detection score is then
used to update target location and scale.
Excluding the feature extraction, the total computational
complexity of our tracker sums up to O(dSMN log MN +
SMNNNe + (d + K2)dMNNGS). Here, NNe denotes the
number of iterations in the sub-grid detection. In our case,
the expression is dominated by the last term, which originates from the ﬁlter optimization.
5. Experiments
Here, we present a comprehensive evaluation of the proposed method.
Result are reported on four benchmark
datasets: OTB-2013, OTB-2015, ALOV++ and VOT2014.
5.1. Details and Parameters
The weight function w is constructed by starting from
a quadratic function w(m, n) = µ + η(m/P)2 + η(n/Q)2
with the minimum located at the sample center. Here P ×Q
denotes the target size, while µ and η are parameters. The
minimum value of w is set to µ = 0.1 and the impact of
the regularizer is set to η = 3. In practice, only a few DFT
coefﬁcients in the resulting function have a signiﬁcant magnitude. We simply remove all DFT coefﬁcients smaller than
a threshold to ensure a sparse spectrum ˆw, containing about
10 non-zero coefﬁcients. Figure 1 visualizes the resulting
weight function w used in the optimization.
Similar to recent DCF based trackers , we also
employ HOG features, using a cell size of 4×4 pixels. Samples are represented by a square M × N grid of cells (i.e.
M = N), such that the corresponding image area is proportional to the area of the target bounding box. We set the
image region area of the samples to 42 times the target area
and set the initial scale to ensure a maximum sample size of
M = 50 cells. Samples are multiplied by a Hann window
 . We set the label function yt to a sampled Gaussian with
a standard deviation proportional to the target size .
The learning rate is set to γ = 0.025 and we use NGS = 4
Gauss-Seidel iterations. All parameters remain ﬁxed for all
videos and datasets. Our Matlab implementation2 runs at 5
frames per second on a standard desktop computer.
5.2. Baseline Comparison
Here, we evaluate the impact of the proposed spatial
regularization component and compare it with the standard
DCF formulation. First, we investigate the consequence of
simply replacing the proposed regularizer with the standard
DCF regularization in (2), without altering any parameters.
This corresponds to using uniform regularization weights
λ, in our framework. We set λ = 0.01 following . For a fair comparison, we also evaluate both
our and the standard regularization using a smaller sample
size relative to the target, by setting the size as in .
Table 1 shows the mean overlap precision (OP) for the
four methods on the OTB-2013 dataset. The OP is computed as the fraction of frames in the sequence where the
intersection-over-union overlap with the ground truth exceeds a threshold of 0.5 (PASCAL criterion). The standard
DCF beneﬁts from using smaller samples to avoid corrupting the positive training samples with background informa-
2Available
 
objrec/visualtracking/regvistrack/index.html.
Conventional sample size
Expanded sample size
Regularization
Mean OP (%)
A comparison of tracking performance on OTB-2013
when using the standard regularization (2) and the proposed spatial
regularization (4), in our tracking framework. The comparison is
performed both with a conventional sample size (used in existing
DCF based trackers) and our expanded sample size.
LSHT ASLA Struck ACT TGPR KCF DSST SAMF MEEM SRDCF
OTB-2013 47.0
OTB-2015 40.0
Table 2. A comparison with state-of-the-art trackers on the OTB-
2013 and OTB-2015 datasets using mean overlap precision (in percent). The best two results for each dataset are shown in red and
blue fonts respectively. Our SRDCF achieves a gain of 8.0% and
8.2% on OTB-2013 and OTB-2015 respectively compared to the
second best tracker on each dataset.
tion. On the other hand, the proposed spatial regularization
enables an expansion of the image region used for training
the ﬁlter, without corrupting the target model. This leads to
a more discriminative model, resulting in a gain of 7.0% in
mean OP compared to the standard DCF formulation.
Additionally, we compare our method with Correlation
Filters with Limited Boundaries (CFLB) . For a fair
comparison, we use the same settings as in for our approach: single grayscale channel, no scale estimation, no
sub-grid detection and the same sample size. On the OTB-
2013, the CFLB achieves a mean OP of 48.6%. Whereas the
mentioned baseline version of our tracker obtains a mean
OP of 54.3%, outperforming by 5.7%.
5.3. OTB-2013 Dataset
We provide a comparison of our tracker with 24 state-ofthe-art methods from the literature: MIL , IVT , CT
 , TLD , DFT , EDFT , ASLA , L1APG
 , CSK , SCM , LOT , CPF , CXT ,
Frag , Struck , LSHT , LSST , ACT ,
KCF , CFLB , DSST , SAMF , TGPR 
and MEEM .
State-of-the-art Comparison
Table 2 shows a comparison with state-of-the-art methods
on the OTB-2013 dataset, using mean overlap precision
(OP) over all 50 videos. Only the results for the top 10
trackers are reported. The MEEM tracker, based on an online SVM, provides the second best results with a mean OP
of 70.1%. The best result on this dataset is obtained by our
tracker with a mean OP of 78.1%, leading to a signiﬁcant
gain of 8.0% compared to MEEM.
Figure 4a shows the success plot over all the 50 videos in
OTB-2013. The success plot shows the mean overlap preci-
Overlap threshold
Overlap Precision [%]
Success plot on OTB-2013
SRDCF [63.3]
SAMF [57.7]
MEEM [57.1]
DSST [56.0]
KCF [51.8]
TGPR [50.7]
Struck [49.2]
ASLA [48.4]
ACT [46.1]
SCM [44.2]
(a) OTB-2013
Overlap threshold
Overlap Precision [%]
Success plot on OTB-2015
SRDCF [60.5]
SAMF [54.8]
MEEM [53.8]
DSST [52.3]
KCF [47.9]
Struck [46.3]
TGPR [45.9]
ACT [44.0]
ASLA [43.2]
LSHT [37.3]
(b) OTB-2015
Figure 4. Success plots showing a comparison with state-of-the-art
methods on OTB-2013 (a) and OTB-2015 (b). For clarity, only the
top 10 trackers are displayed. Our SRDCF achieves a gain of 5.6%
and 5.7% on OTB-2013 and OTB-2015 respectively, compared to
the second best methods.
Overlap threshold
Overlap Precision [%]
Success plot of SRE on OTB-2013
SRDCF [59.5]
SAMF [54.5]
MEEM [53.6]
DSST [51.7]
TGPR [49.6]
KCF [47.7]
Struck [44.3]
Overlap threshold
Overlap Precision [%]
Success plot of TRE on OTB-2013
SRDCF [65.2]
SAMF [61.4]
MEEM [59.3]
DSST [58.1]
KCF [56.1]
TGPR [55.0]
Struck [51.6]
Figure 5. Comparison with respect to robustness to initialization
on OTB-2013. We show success plots for both the spatial (SRE)
and temporal (TRE) robustness.
Our approach clearly demonstrates robustness in both scenarios.
sion (OP), plotted over the range of intersection-over-union
thresholds. The trackers are ranked using the area under
the curve (AUC), displayed in the legend. Among previous
DCF based trackers, DSST and SAMF provides the best
performance, with an AUC score of 56.0% and 57.7%. Our
approach obtains an AUC score of 63.3% and signiﬁcantly
outperforms the best existing tracker (SAMF) by 5.6%.
Robustness to Initialization
Visual tracking methods are known to be sensitive to initialization. We evaluate the robustness of our tracker by following the protocol proposed in . Two different types of
initialization criteria, namely: temporal robustness (TRE)
and spatial robustness (SRE), are evaluated. The SRE corresponds to tracker initialization at different positions close
to the ground-truth in the ﬁrst frame. The procedure is repeated with 12 different initializations for each video in the
dataset. The TRE criteria evaluates the tracker by initializations at 20 different frames, with the ground-truth.
Figure 5 shows the success plots for TRE and SRE on the
OTB-2013 dataset with 50 videos. We include all the top 7
trackers in ﬁgure 4a for this experiment. Among the existing methods, SAMF and MEEM provide the best results.
Our SRDCF achieves a consistent gain in performance over
these trackers on both robustness evaluations.
Figure 6. Qualitative comparison of our approach with state-ofthe-art trackers on the Soccer, Human6 and Tiger2 videos. Our
approach provides consistent results in challenging scenarios, such
as occlusions, fast motion, background clutter and target rotations.
Attribute Based Comparison
We perform an attribute based analysis of our approach on
the OTB-2013 dataset. All the 50 videos in OTB-2013 are
annotated with 11 different attributes, namely: illumination
variation, scale variation, occlusion, deformation, motion
blur, fast motion, in-plane rotation, out-of-plane rotation,
out-of-view, background clutter and low resolution. Our approach outperforms existing trackers on 10 attributes.
Figure 7 shows example success plots of four different attributes. Only the top 10 trackers in each plot are
displayed for clarity.
In case of out-of-plane rotations,
(MEEM) achieves an AUC score of 57.2%. Our tracker provides a gain of 3.3% compared to MEEM. Among the existing methods, the two DCF based trackers DSST and SAMF
provide the best results in case of scale variation.
these trackers are designed to handle scale variations. Our
approach achieves a signiﬁcant gain of 4.1% over DSST.
Note that the standard DCF trackers struggle in the cases
of motion blur and fast motion due to the restricted search
area. This is caused by the induced boundary effects in the
detection samples of the standard DCF trackers. Our approach signiﬁcantly improves the performance compared to
the standard DCF based trackers in these cases. Figure 6
shows a qualitative comparison of our approach with existing methods on challenging example videos. Despite no explicit occlusion handling component, our tracker performs
favorably in cases with occlusion.
5.4. OTB-2015 Dataset
We provide a comparison of our approach on the recently
introduced OTB-2015. The dataset extends OTB-2013 and
contains 100 videos. Table 2 shows the comparison with the
top 10 methods, using mean overlap precision (OP) over
all 100 videos. Among the existing methods, SAMF and
MEEM provide the best results with mean OP of 64.7%
Overlap threshold
Overlap Precision [%]
Success plot of out-of-plane rotation (39)
SRDCF [60.5]
MEEM [57.2]
SAMF [56.0]
DSST [54.1]
KCF [49.9]
TGPR [48.6]
ASLA [46.9]
ACT [46.3]
Struck [45.3]
SCM [42.4]
Overlap threshold
Overlap Precision [%]
Success plot of scale variation (28)
SRDCF [59.3]
DSST [55.2]
SAMF [52.0]
MEEM [50.7]
ASLA [49.7]
SCM [48.1]
Struck [43.1]
KCF [42.8]
TGPR [42.4]
ACT [41.0]
Overlap threshold
Overlap Precision [%]
Success plot of motion blur (12)
SRDCF [60.8]
MEEM [56.8]
SAMF [52.4]
KCF [50.0]
Struck [47.7]
ACT [46.8]
DSST [45.8]
TGPR [42.5]
EDFT [40.5]
CFLB [36.5]
Overlap threshold
Overlap Precision [%]
Success plot of occlusion (29)
SRDCF [63.4]
SAMF [62.8]
MEEM [57.5]
DSST [53.8]
KCF [51.7]
TGPR [47.0]
ACT [45.2]
Struck [44.9]
ASLA [44.7]
SCM [42.1]
Figure 7. Attribute-based analysis of our approach on the OTB-2013 dataset with 50 videos. Success plots are shown for four attributes.
Each plot title includes the number of videos associated with the respective attribute. Only the top 10 trackers for each attribute are
displayed for clarity. Our approach demonstrates superior performance compared to existing trackers in these scenarios.
SRDCF [0.787]
SAMF [0.772]
DSST [0.759]
MEEM [0.708]
KCF [0.704]
TGPR [0.690]
STR [0.662]
FBT [0.636]
TST [0.618]
TLD [0.614]
Figure 8. Survival curves comparing our approach with 24 trackers
on ALOV++. The mean F-scores for the top 10 trackers are shown
in the legend. Our approach achieves the best overall results.
and 63.4% respectively. Our tracker outperforms the best
existing tracker by 8.2% in mean OP.
Figure 4b shows the success plot over all the 100 videos.
Among the standard DCF trackers, SAMF provides the best
results with an AUC score of 54.8%. The MEEM tracker
achieves an AUC score of 53.8%. Our tracker obtains an
AUC score of 60.5%, outperforming SAMF by 5.7%.
5.5. ALOV++ Dataset
We also perform experiments on the ALOV++ dataset
 , containing 314 videos with 89364 frames in total. The
evaluation protocol employs survival curves based on Fscore, where a higher F-score indicates better performance.
The survival curve is constructed by plotting the sorted Fscores of all 314 videos. We refer to for details.
Our approach is compared with the 19 trackers evaluated
in . We also add the top 5 methods from our OTB comparison. Figure 8 shows the survival curves and the average
F-scores of the trackers. MEEM obtains a mean F-score of
0.708. Our approach obtains the best overall performance
compared to 24 trackers with a mean F-score of 0.787.
Final Rank
Table 3. Results for the top 3 trackers on VOT2014. The mean
overlap and failure rate is reported in the ﬁrst two columns. The
accuracy rank, robustness rank and the combined ﬁnal rank are
shown in the remaining columns. Our tracker obtains the best performance on this dataset.
5.6. VOT2014 Dataset
Finally, we present results on VOT2014 . Our approach is compared with the 38 participating trackers in
the challenge. We also add MEEM in the comparison. In
VOT2014, the trackers are evaluated both in terms of accuracy and robustness. The accuracy score is based on the
overlap with ground truth, while the robustness is determined by the failure rate. The trackers are restarted at each
failure. The ﬁnal rank is based on the accuracy and robustness in each video. We refer to for details.
Table 3 shows the ﬁnal ranking scores over all the videos
in VOT2014. Among the existing methods, the DSST approach provides the best results. Our tracker achieves the
top ﬁnal rank of 8.26, outperforming DSST and SAMF.
6. Conclusions
We propose Spatially Regularized Discriminative Correlation Filters (SRDCF) to address the limitations of the
standard DCF. The introduced spatial regularization component enables the correlation ﬁlter to be learned on larger
image regions, leading to a more discriminative appearance
model. By exploiting the sparsity of the regularization operation in the Fourier domain, we derive an efﬁcient optimization strategy for learning the ﬁlter. The proposed learning
procedure employs the Gauss-Seidel method to solve for
the ﬁlter in the Fourier domain. We perform comprehensive experiments on four benchmark datasets. Our SRDCF
outperforms existing trackers on all four datasets.
Acknowledgments: This work has been supported by SSF
(CUAS) and VR (VIDI, EMC2, ELLIIT, and CADICS).