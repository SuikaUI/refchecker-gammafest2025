ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression
Jian-Hao Luo1, Jianxin Wu1, and Weiyao Lin2
1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
2Shanghai Jiao Tong University, Shanghai, China
 , , 
We propose an efﬁcient and uniﬁed framework, namely
ThiNet, to simultaneously accelerate and compress CNN
models in both training and inference stages. We focus on
the ﬁlter level pruning, i.e., the whole ﬁlter would be discarded if it is less important. Our method does not change
the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish ﬁlter pruning as an optimization problem,
and reveal that we need to prune ﬁlters based on statistics information computed from its next layer, not the current layer,
which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy,
which has advanced the state-of-the-art. We also show the
performance of ThiNet on ILSVRC-12 benchmark. ThiNet
achieves 3.31× FLOPs reduction and 16.63× compression
on VGG-16, with only 0.52% top-5 accuracy drop. Similar
experiments with ResNet-50 reveal that even for a compact
network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1% top-5 accuracy
drop. Moreover, the original VGG-16 model can be further
pruned into a very small model with only 5.05MB model
size, preserving AlexNet level accuracy but showing much
stronger generalization ability.
1. Introduction
In the past few years, we have witnessed a rapid development of deep neural networks in the ﬁeld of computer vision,
from basic image classiﬁcation tasks such as the ImageNet
recognition challenge , to some more advanced
applications, e.g., object detection , semantic segmentation , image captioning and many others. Deep
neural networks have achieved state-of-the-art performance
in these ﬁelds compared with traditional methods based on
manually designed visual features.
In spite of its great success, a typical deep model is hard
to be deployed on resource constrained devices, e.g., mobile
phones or embedded gadgets. A resource constrained scenario means a computing task must be accomplished with
limited resource supply, such as computing time, storage
space, battery power, etc. One of the main issues of deep
neural networks is its huge computational cost and storage
overhead, which constitute a serious challenge for a mobile
device. For instance, the VGG-16 model has 138.34 million parameters, taking up more than 500MB storage space,1
and needs 30.94 billion ﬂoat point operations (FLOPs) to
classify a single image. Such a cumbersome model can easily
exceed the computing limit of small devices. Thus, network
compression has drawn a signiﬁcant amount of interest from
both academia and industry.
Pruning is one of the most popular methods to reduce
network complexity, which has been widely studied in the
model compression community. In the 1990s, LeCun et
al. had observed that several unimportant weights can
be removed from a trained network with negligible loss in
accuracy. A similar strategy was also explored in . This
process resembles the biological phenomena in mammalian
brain, where the number of neuron synapses has reached the
peak in early childhood, followed by gradual pruning during
its development. However, these methods are mainly based
on the second derivative, thus are not applicable for today’s
deep model due to expensive memory and computation costs.
Recently, Han et al. introduced a simple pruning
strategy: all connections with weights below a threshold are
removed, followed by ﬁne-tuning to recover its accuracy.
This iterative procedure is performed several times, generating a very sparse model. However, such a non-structured
sparse model can not be supported by off-the-shelf libraries,
thus specialized hardwares and softwares are needed for efﬁcient inference, which is difﬁcult and expensive in real-world
applications. On the other hand, the non-structured random
connectivity ignores cache and memory access issues. As
indicated in , due to the poor cache locality and jumping
memory access caused by random connectivity, the practical
acceleration is very limited (sometimes even slows down),
even though the actual sparsity is relatively high.
To avoid the limitations of non-structured pruning men-
11 MB = 220 ≈1.048 million bytes, and 1 million is 106.
 
tioned above, we suggest that the ﬁlter level pruning would
be a better choice. The beneﬁts of removing the whole unimportant ﬁlter have a great deal: 1) The pruned model has
no difference in network structure, thus it can be perfectly
supported by any off-the-shelf deep learning libraries. 2)
Memory footprint would be reduced dramatically. Such
memory reduction comes not only from model parameter
itself, but also from the intermediate activation, which is
rarely considered in previous studies. 3) Since the pruned
network structure has not be damaged, it can be further compressed and accelerated by other compression methods, e.g.,
the parameter quantization approach . 4) More vision
tasks, such as object detection or semantic segmentation, can
be accelerated greatly using the pruned model.
In this paper, we propose a uniﬁed framework, namely
ThiNet (stands for “Thin Net”), to prune the unimportant
ﬁlters to simultaneously accelerate and compress CNN models in both training and test stages with minor performance
degradation. With our pruned network, some important transfer tasks such as object detection or ﬁne-grained recognition
can run much faster (both training and inference), especially
in small devices. Our main insight is that we establish a welldeﬁned optimization problem, which shows that whether a
ﬁlter can be pruned depends on the outputs of its next layer,
not its own layer. This novel ﬁnding differentiates ThiNet
from existing methods which prune ﬁlters using statistics
calculated from their own layer.
We then compare the proposed method with other stateof-the-art criteria. Experimental results show that our approach is signiﬁcantly better than existing methods, especially when the compression rate is relatively high. We
evaluate ThiNet on the large-scale ImageNet classiﬁcation
task. ThiNet achieves 3.31× FLOPs reduction and 16.63×
compression on VGG-16 model , with only 0.52% top-5
accuracy drop. The ResNet-50 model has less redundancy compared with classic CNN models. ThiNet can still
reduce 2.26× FLOPs and 2.06× parameters with roughly
1% top-5 accuracy drop. To explore the limits of ThiNet, we
show that the original VGG-16 model can even be pruned
into 5.05MB, but still preserving AlexNet level accuracy.
In addition, we also explore the performance of ThiNet
in a more practical task, i.e., transfer learning on small-scale
datasets. Experimental results demonstrate the excellent
effectiveness of ThiNet, which achieves the best trade-off
between model size and accuracy.
The key advantages and major contributions of this paper
can be summarized as follows.
• We propose a simple yet effective framework, namely
ThiNet, to simultaneously accelerate and compress
CNN models. ThiNet shows signiﬁcant improvements
over existing methods on numerous tasks.
• We formally establish ﬁlter pruning as an optimization
problem, and reveal that we need to prune ﬁlters using statistics information computed from its next layer,
not the current layer, which differentiates ThiNet from
existing methods.
• In experiments, the VGG-16 model can be pruned into
5.05MB, showing promising generalization ability on
transfer learning. Higher accuracy could be preserved
with a more accurate model using ThiNet.
2. Related work
Many researchers have found that deep models suffer
from heavy over-parameterization. For example, Denil et
al. demonstrated that a network can be efﬁciently reconstructed with only a small subset of its original parameters.
However, this redundancy seems necessary during model
training, since the highly non-convex optimization is hard to
be solved with current techniques . Hence, there is a
great need to reduce model size after its training.
Some methods have been proposed to pursuit a balance
between model size and accuracy. Han et al. proposed
an iterative pruning method to remove the redundancy in
deep models. Their main insight is that small-weight connectivity below a threshold should be discarded. In practice,
this can be aided by applying ℓ1 or ℓ2 regularization to push
connectivity values becoming smaller. The major weakness
of this strategy is the loss of universality and ﬂexibility, thus
seems to be less practical in the real applications.
In order to avoid these weaknesses, some attention has
been focused on the group-wise sparsity. Lebedev and Lempitsky explored group-sparse convolution by introducing the group-sparsity regularization to the loss function,
then some entire groups of weights would shrink to zeros,
thus can be removed. Similarly, Wen et al. proposed
the Structured Sparsity Learning (SSL) method to regularize
ﬁlter, channel, ﬁlter shape and depth structures. In spite of
their success, the original network structure has been destroyed. As a result, some dedicated libraries are needed for
an efﬁcient inference speed-up.
In line with our work, some ﬁlter level pruning strategies have been explored too. The core is to evaluate neuron
importance, which has been widely studied in the community . A simplest possible method is based
on the magnitude of weights. Li et al. measured the
importance of each ﬁlter by calculating its absolute weight
sum. Another practical criterion is to measure the sparsity of
activations after the ReLU function. Hu et al. believed
that if most outputs of some neurons are zero, these activations should be expected to be redundant. They compute
the Average Percentage of Zeros (APoZ) of each ﬁlter as its
importance score. These two criteria are simple and straightforward, but not directly related to the ﬁnal loss. Inspired
by this observation, Molchanov et al. adopted Taylor
expansion to approximate the inﬂuence to loss function induced by removing each ﬁlter.
prune weak filters
filters of
filters of
fine-tuning
Fine-tuned
Figure 1. Illustration of ThiNet. First, we focus on the dotted box
part to determine several weak channels and their corresponding
ﬁlters (highlighted in yellow in the ﬁrst row). These channels
(and their associated ﬁlters) have little contribution to the overall
performance, thus can be discarded, leading to a pruned model.
Finally, the network is ﬁne-tuned to recover its accuracy. (This
ﬁgure is best viewed in color.)
Beyond pruning, there are also other strategies to obtain
small CNN models. One popular approaches is parameter
quantization . Low-rank approximation is also
widely studied . Note that these methods are complementary to ﬁlter pruning, which can be combined with
ThiNet for further improvement.
In this section, we will give a comprehensive introduction to our ﬁlter level pruning approach: ThiNet. First, the
overall framework will be presented. Next, a more detailed
description of our selection algorithm would be presented.
Finally, we will show our pruning strategy, which takes both
efﬁciency and effectiveness into consideration.
3.1. Framework of ThiNet
Pruning is a classic method used for reducing model
complexity. Although vast differences exist (such as different criteria in selecting what should be pruned), the overall
framework is similar in pruning ﬁlters inside a deep neural
network. It can be summarized in one sentence: evaluate the
importance of each neuron, remove those unimportant ones,
and ﬁne-tune the whole network.
This framework is illustrated in Figure 1. In the next subsection, we will focus on the dotted box part to introduce our
data-driven channel selection method, which determines the
channels (and their associated ﬁlters) that are to be pruned
Given a pre-trained model, it would be pruned layer by
layer with a predeﬁned compression rate. We summarize our
framework as follows:
1. Filter selection. Unlike existing methods that use layer
i’s statistics to guide the pruning of layer i’s ﬁlters, we
use layer i + 1 to guide the pruning in layer i. The
key idea is: if we can use a subset of channels in layer
(i + 1)’s input to approximate the output in layer i + 1,
the other channels can be safely removed from the input
of layer i + 1. Note that one channel in layer (i + 1)’s
input is produced by one ﬁlter in layer i, hence we can
safely prune the corresponding ﬁlter in layer i.
2. Pruning. Weak channels in layer (i + 1)’s input and
their corresponding ﬁlters in layer i would be pruned
away, leading to a much smaller model. Note that, the
pruned network has exactly the same structure but with
fewer ﬁlters and channels. In other words, the original
wide network is becoming much thinner. That is why
we call our method “ThiNet”.
3. Fine-tuning. Fine-tuning is a necessary step to recover
the generalization ability damaged by ﬁlter pruning.
But it will take very long for large datasets and complex
models. For time-saving considerations, we ﬁne-tune
one or two epochs after the pruning of one layer. In
order to get an accurate model, more additional epochs
would be carried out when all layers have been pruned.
4. Iterate to step 1 to prune the next layer.
3.2. Data-driven channel selection
We use a triplet ⟨Ii, Wi, ∗⟩to denote the convolution
process in layer i, where Ii ∈RC×H×W is the input tensor,
which has C channels, H rows and W columns. And Wi ∈
RD×C×K×K is a set of ﬁlters with K×K kernel size, which
generates a new tensor with D channels.
Our goal is to remove some unimportant ﬁlters in Wi.
Note that, if a ﬁlter in Wi is removed, its corresponding
channel in Ii+1 and Wi+1 would also be discarded. However, since the ﬁlter number in layer i + 1 has not been
changed, the size of its output tensor, i.e., Ii+2, would be
kept exactly the same. Inspired by this observation, we
believe that if we can remove several ﬁlters that has little
inﬂuence on Ii+2 (which is also the output of layer i + 1), it
would have little inﬂuence on the overall performance too.
In other words, minimizing the reconstruction error of Ii+2
is closely related to the network’s classiﬁcation performance.
Collecting training examples
In order to determine which channel can be removed safely,
a training set used for importance evaluation would be collected. As illustrated in Figure 2, an element, denoted by y,
is randomly sampled from the tensor Ii+2 (before ReLU).
A corresponding ﬁlter c
W ∈RC×K×K and sliding window
x ∈RC×K×K (after ReLU) can also be determined according to its location. Here, some index notations are omitted for
a clearer presentation. Normally, the convolution operation
can be computed with a corresponding bias b as follows:
Wc,k1,k2 × xc,k1,k2 + b.
𝑥: the sliding
𝒲: the corresponding filter
𝑦: a random
sampled data
input of layer 𝑖+1
filters of layer 𝑖+1
input of layer 𝑖+2
Figure 2. Illustration of data sampling and variables’ relationship.
Now, if we further deﬁne:
Wc,k1,k2 × xc,k1,k2,
Eq. 1 can be simpliﬁed as:
in which ˆy = y −b. It is worthwhile to keep in mind that ˆx
and ˆy are random variables whose instantiations require ﬁxed
spatial locations indexed by c, k1 and k2. A key observation
is that channels in ˆx = (ˆx1, ˆx2, . . . , ˆxC) is independent: ˆxc
only depends on xc,:,:, which has no dependency relationship
with xc′,:,:, if c′ ̸= c.
In other words, if we can ﬁnd a subset S ⊂{1, 2, . . . , C}
and the equality
always holds, then we do not need any ˆxc if c /∈S and these
variables can be safely removed without changing the CNN
model’s result.
Of course, Eq. 4 cannot always be true for all instances
of the random variables ˆx and ˆy. However, we can manually
extract instances of them to ﬁnd a subset S such that Eq. 4
is approximately correct.
Given an input image, we ﬁrst apply the CNN model in
the forward run to ﬁnd the input and output of layer i + 1.
Then for any feasible (c, k1, k2) triplet, we can obtain a Cdimensional vector variable ˆx = {ˆx1, ˆx2, . . . , ˆxC} and a
scalar value ˆy using Eq. 1 to Eq. 3. Since ˆx and ˆy can be
viewed as random variables, more instances can be sampled
by choosing different input images, different channels, and
different spatial locations.
A greedy algorithm for channel selection
Now, given a set of m (the product of number of images
and number of locations) training examples {(ˆxi, ˆyi)}, the
original channel selection problem becomes the following
Algorithm 1 A greedy algorithm for minimizing Eq. 6
Training set {(ˆxi, ˆyi)}, and compression rate r
Output: The subset of removed channels: T
1: T ←∅; I ←{1, 2, . . . , C};
2: while |T| < C × (1 −r) do
min value ←+∞;
for each item i ∈I do
tmpT ←T ∪{i};
compute value from Eq. 6 using tmpT;
if value < min value then
min value ←value; min i ←i;
move min i from I into T;
12: end while
optimization problem:
|S| = C × r,
S ⊂{1, 2, . . . , C}.
Here, |S| is the number of elements in a subset S, and r
is a pre-deﬁned compression rate (i.e., how many channels
are preserved). Equivalently, let T be the subset of removed
channels (i.e., S ∪T = {1, 2, . . . , C} and S ∩T = ∅), we
can minimize the following alternative objective:
|T| = C × (1 −r),
T ⊂{1, 2, . . . , C}.
Eq. 6 is equivalent to Eq. 5, but has faster speed because |T|
is usually smaller than |S|. Solving Eq. 6 is still NP hard,
thus we use a greedy strategy (illustrated in algorithm 1).
We add one element to T at a time, and choose the channel
leading to the smallest objective value in the current iteration.
Obviously, this greedy solution is sub-optimal. But the
gap can be compensated by ﬁne-tuning. We have also tried
some other sophisticated algorithms, such as sparse coding
(speciﬁcally, the homotopy method ). However, our simple greedy approach has better performance and faster speed
according to our experiments.
Minimize the reconstruction error
So far, we have obtained the subset T such that the n-th
channel in each ﬁlter of layer i + 1 can be safely removed
if n ∈T. Hence, the corresponding ﬁlters in the previous
layer i can be pruned too.
Now we will further minimize the reconstruction error
(c.f. Eq. 5) by weighing the channels, which can be deﬁned
ˆw = arg min
(ˆyi −wTˆx∗
i indicates the training examples after channel selection. Eq. 7 is a classic linear regression problem, which
has a unique closed-form solution using the ordinary least
squares approach: ˆw = (XTX)−1XTy.
Each element in ˆw can be regarded as a scaling factor of
corresponding ﬁlter channel such that W:,i,:,: = ˆwiW:,i,:,:.
From another point of view, this scaling operation provides
a better initialization for ﬁne-tuning, hence the network is
more likely to reach higher accuracy.
3.3. Pruning strategy
There are mainly two types of different network architectures: the traditional convolutional/fully-connected architecture, and recent structural variants. The former is represented by AlexNet or VGGNet , while the latter
mainly includes some recent networks like GoogLeNet 
and ResNet . The main difference between these two
types is that more recent networks usually replace the
FC (fully-connected) layers with a global average pooling
layer , and adopt some novel network structures like
Inception in GoogLeNet or residual blocks in ResNet.
We use different strategies to prune these two types of networks. For VGG-16, we notice that more than 90% FLOPs
exist in the ﬁrst 10 layers (conv1-1 to conv4-3), while the
FC layers contribute nearly 86.41% parameters. Hence, we
prune the ﬁrst 10 layers for acceleration consideration, but
replace the FC layers with a global average pooling layer.
Although the proposed method is also valid for FC layers,
we believe removing them is simpler and more efﬁcient.
For ResNet, there exist some restrictions due to its special
structure. For example, the channel number of each block
in the same group needs to be consistent in order to ﬁnish
the sum operation (see for more details). Thus it is hard
to prune the last convolutional layer of each residual block
directly. Since most parameters are located in the ﬁrst two
layers, pruning the ﬁrst two layers is a good choice, which is
illustrated in Figure 3.
4. Experiments
We empirically study the performance of ThiNet in this
section. First, a comparison among several different ﬁlter selection criteria would be presented. Experimental results show that our method is signiﬁcantly better than others.
Then, we would report the performance on ILSCVR-12 .
Two widely used networks are pruned: VGG-16 and
ResNet-50 . Finally, we focus on a more practical scenario to show the advantages of ThiNet. All the experiments
64×256×1×1
256×64×1×1
32×256×1×1
256×32×1×1
Figure 3. Illustration of the ResNet pruning strategy. For each
residual block, we only prune the ﬁrst two convolutional layers,
keeping the block output dimension unchanged.
are conducted within Caffe .
4.1. Different ﬁlter selection criteria
There exist some heuristic criteria to evaluate the importance of each ﬁlter in the literature. We compare our selection method with two recently proposed criteria to demonstrate the effectiveness of our evaluation criterion. These
criteria are brieﬂy summarized as follows:
• Weight sum . Filters with smaller kernel weights
tend to produce weaker activations. Thus, in this strategy the absolute sum of each ﬁlter is calculated as its
importance score: si = P |W(i, :, :, :)|.
• APoZ (Average Percentage of Zeros) .
criterion calculates the sparsity of each channel in
output activations as its importance score:
|I(i,:,:)|
P P I(I(i, :, :) == 0), where |I(i, :, :)| is
the elements number in i-th channel of tensor I (after ReLU), and I(·) denotes the indicator function.
To compare these different selection methods, we evaluate their performance on the widely used ﬁne-grained dataset:
CUB-200 , which contains 11,788 images of 200 different bird species (5994/5794 images for training/test, respectively). Except for labels, no additional supervised information (e.g., bounding box) is used.
Following the pruning strategy in Section 3.3, all the FC
layers in VGG-16 are removed, and replaced with a global
average pooling layer, and ﬁne-tuned on new datasets. Starting from this ﬁne-tuned model, we then prune the network
layer by layer with different compression rate. Each pruning is followed by one epoch ﬁne-tuning, and 12 epochs
are performed in the ﬁnal layer to improve accuracy. This
procedure is repeated several times with different channel
selection strategies. Due to the random nature of ThiNet, we
repeated our method 4 times and report the averaged result.
For a fair comparison, all the settings are kept the same,
except the selection method.
Figure 4 shows the pruning results on the CUB bird
dataset. We also evaluated the performance of random selection with the same pruning strategy. In addition, another
FLOPs Reduction
Top-1 Accuracy
Weight sum
ThiNet w/o w
Figure 4. Performance comparison of different channel selection
methods: the VGG-16-GAP model pruned on CUB-200 with different compression rates. (This ﬁgure is best viewed in color and
zoomed in.)
version of ThiNet without least squares (denoted by “ThiNet
w/o ˆw”) is also evaluated to demonstrate the effectiveness of
least squares in our method. Obviously, ThiNet achieves consistently and signiﬁcantly higher accuracy compared with
other selection methods.
One interesting result is: random selection shows pretty
good performance, even better than heuristic criteria in some
cases. In fact, according to the property of distributed representations (i.e., each concept is represented by many neurons;
and, each neuron participates in the representation of many
concepts ), randomly selected channels may be quite
powerful in theory. However, this criterion is not robust. As
shown in Figure 4, it can lead to very bad result and the
accuracy is very low after all layers are compressed. Thus,
random selection is not applicable in practice.
Weight sum has pretty poor accuracy on CUB-200. This
result is reasonable, since it only takes the magnitude of kernel weights into consideration, which is not directly related
to the ﬁnal classiﬁcation accuracy. In fact, small weights
could still have large impact on the loss function. When we
discard a large number of small ﬁlters at the same time, the
ﬁnal accuracy can be damaged greatly. For example, if we
removed 60% ﬁlters in conv1-1 using the small weight criterion, the top-1 accuracy is only 40.99% (before ﬁne-tuning),
while random criterion is 51.26%. By contrast, our method
(ThiNet w/o w) can reach 68.24%, and even 70.75% with
least squares (ThiNet). The accuracy loss of weight sum is
so large that ﬁne-tuning cannot completely recover it from
In contrast, our method shows much higher and robust
results. The least squares approach does indeed aid to get a
better weight initialization for ﬁne-tuning, especially when
the compression rate is relatively high.
4.2. VGG-16 on ImageNet
We now evaluate the performance of the proposed ThiNet
method on large-scale ImageNet classiﬁcation task. The
ILSCVR-12 dataset consists of over one million training images drawn from 1000 categories. We randomly select
10 images from each category in the training set to comprise
our evaluation set (i.e., collected training examples for channel selection). And for each input image, 10 instances are
randomly sampled with different channels and different spatial locations as described in section 3.2.1. Hence, there are
in total 100,000 training samples used for ﬁnding the optimal
channel subset via Algorithm 1. We compared several different choices of image and location number, and found that
the current choice (10 images per class and 10 locations per
image) is enough for neuron importance evaluation. Finally,
top-1 and top-5 classiﬁcation performance are reported on
the 50k standard validation set, using the single-view testing
approach (central patch only).
During ﬁne-tuning, images are resized to 256 × 256, then
224 × 224 random cropping is adopted to feed the data into
network. Horizontal ﬂip is also used for data augmentation.
At the inference stage, we center crop the resized images
to 224 × 224. No more tricks are used here. The whole
network is pruned layer by layer and ﬁne-tuned in one epoch
with 10−3 learning rate. Since the last layer of each group
(i.e., conv1-2, conv2-2, conv3-3) is more important (pruning
these layers would lead to a big accuracy drop), we ﬁne-tune
these layers with additional one epoch of 10−4 learning rate
to prevent accuracy drop too much. When pruning the last
layer, more epochs (12 epochs) are adopted to get an accurate
result with learning rate varying from 10−3 to 10−5. We use
SGD with mini-batch size of 128, and other parameters are
kept the same as the original VGG paper .
We summarize the performance of the ThiNet approach
in Table 1. Here, “ThiNet-Conv” refers to the model in
which only the ﬁrst 10 convolutional layers are pruned with
compression rate 0.5 (i.e., half of the ﬁlters are removed
in each layer till conv4-3) as stated above. Because some
useless ﬁlters are discarded, the pruned model can even
outperform the original VGG-16 model. However, if we
train this model from scratch, the top-1/top-5 accuracy are
only 67.00%/87.45% respectively, which is much worse
than our pruned network. Then the FC layers are removed,
replaced with a GAP (global average pooling) layer and ﬁnetuned in 12 epochs with the same hyper-parameters, which
is denoted by “ThiNet-GAP”. The classiﬁcation accuracy
of GAP model is slightly lower than the original model,
since the model size has been reduced dramatically. Further
reduction can be obtained with a higher compression rate
(denoted by “ThiNet-Tiny”), which would be discussed later.
The actual speed-up of ThiNet is also reported. We test
the forward/backward running time of each model using
the ofﬁcial “time” command in Caffe. This evaluation is
Table 1. Pruning results of VGG-16 on ImageNet using ThiNet.
Here, M/B means million/billion (106/109), respectively; f./b. denotes the forward/backward timing in milliseconds tested on one
M40 GPU with batch size 32.
#Param. #FLOPs1
f./b. (ms)
68.34% 88.44% 138.34M 30.94B 189.92/407.56
ThiNet-Conv
69.80% 89.53% 131.44M
76.71/152.05
Train from scratch 67.00% 87.45% 131.44M
76.71/152.05
ThiNet-GAP
67.34% 87.92%
71.73/145.51
ThiNet-Tiny
59.34% 81.97%
29.51/55.83
SqueezeNet 
57.67% 80.39%
37.30/68.62
1 In this paper, we only consider the FLOPs of convolution operations,
which is commonly used for computation complexity comparison.
2 For a fair comparison, the accuracy of original VGG-16 model is evaluated on resized center-cropped images using pre-trained model as
adopted in . The same strategy is also used in ResNet-50.
Table 2. Comparison among several state-of-the-art pruning methods on the VGG-16 network. Some exact values are not reported
in the original paper and cannot be computed, thus we use ≈to
denote the approximation value.
Top-1 Acc.
Top-5 Acc.
APoZ-1 
APoZ-2 
Taylor-1 
Taylor-2 
ThiNet-WS 
ThiNet-Conv
ThiNet-GAP
conducted on one M40 GPU with batch size 32 accelerated
by cuDNN v5.1. Since convolution operations dominate
the computational costs of VGG-16, reducing FLOPs would
accelerate inference speed greatly, which is shown in Table 1.
We then compare our approach with several state-of-theart pruning methods on the VGG-16 model, which is shown
in Table 2. These methods also focus on ﬁlter-level pruning,
but with totally different selection criteria.
APoZ aims to reduce parameter numbers, but its
performance is limited. APoZ-1 prunes few layers (conv4,
conv5 and the FC layers), but leads to signiﬁcant accuracy
degradation. APoZ-2 then only prunes conv5-3 and the FC
layers. Its accuracy is improved but this model almost does
not reduce the FLOPs. Hence, there is a great need for
compressing convolution layers.
In contrast, Molchanov et al. pay their attention to
model acceleration, and only prune the convolutional layers.
They think a ﬁlter can be removed safely if it has little inﬂuence on the loss function. But the calculating procedure can
be very time-consuming, thus they use Taylor expansion to
approximate the loss change. Their motivation and goals are
similar to ours, but with totally different selection criterion
and training framework. As shown in Table 2, the ThiNet-
Conv model is signiﬁcantly better than Taylor method. Our
model can even improve classiﬁcation accuracy with more
FLOPs reduction.
As for weight sum , they have not explored its performance on VGG-16. Hence we simply replace our selection
method with weight sum in the ThiNet framework, and report the ﬁnal accuracy denoted by “ThiNet-WS”. All the
parameters are kept the same except for selection criterion.
Note that different ﬁne-tuning framework may lead to very
different results. Hence, the accuracy may be different if Li
et al. had done this using their own framework. Because
the rest setups are the same, it is fair to compare ThiNet-WS
and ThiNet, and ThiNet has obtained better results.
To explore the limits of ThiNet, we prune VGG-16 with
a larger compression rate 0.25, achieving 16× parameters
reduction in convolutional layers. The conv5 layers are also
pruned to get a smaller model. As for conv5-3, which is
directly related to the ﬁnal feature representation, we only
prune half of the ﬁlters for accuracy consideration.
Using these smaller compression ratios, we train a very
small model. Denoted as “ThiNet-Tiny” in Table 1, it only
takes 5.05MB disk space (1MB=220 bytes) but still has
AlexNet-level accuracy (the top-1/top-5 accuracy of AlexNet
is 57.2%/80.3%, respectively). ThiNet-Tiny has exactly the
same level of model complexity as the recently proposed
compact network: SqueezeNet , but showing high accuracy. Although ThiNet-Tiny needs more FLOPs, its actual
speed is even faster than SqueezeNet because it has a much
simpler network structure. SqueezeNet adopts a special
structure, namely the Fire module, which is parameter ef-
ﬁcient but relies on manual network structure design. In
contrast, ThiNet is a uniﬁed framework, and higher accuracy
would be obtained if we start from a more accurate model.
4.3. ResNet-50 on ImageNet
We also explore the performance of ThiNet on the recently proposed powerful CNN architecture: ResNet .
We select ResNet-50 as the representative of the ResNet
family, which has exactly the same architecture and little
difference with others.
Similar to VGG-16, we prune ResNet-50 from block
2a to 5c iteratively. Except for ﬁlters, the corresponding
channels in batch-normalization layer are also discarded.
After pruning, the model is ﬁne-tuned in one epoch with
ﬁxed learning rate 10−4. And 9 epochs ﬁne-tuning with
learning rate changing from 10−3 to 10−5 is performed at
the last round to gain a higher accuracy. Other parameters
are kept the same as our VGG-16 pruning experiment.
Because ResNet is a recently proposed model, the literature lack enough works that compress this network. We
report the performance of ThiNet on pruning ResNet-50,
which is shown in Table 3. We prune this model with 3
different compression rates (preserve 70%, 50%, 30% ﬁlters in each block respectively). Unlike VGG-16, ResNet is
more compact. There exists less redundancy, thus pruning
a large amount of ﬁlters seems to be more challenging. In
spite of this, our method ThiNet-50 can still prune more than
Table 3. Overall performance of pruning ResNet-50 on ImageNet
via ThiNet with different compression rate. Here, M/B means
million/billion respectively, f./b. denotes the forward/backward
speed tested on one M40 GPU with batch size 32.
f./b. (ms)
188.27/269.32
169.38/243.37
153.60/212.29
144.45/200.67
half of the parameters with roughly 1% top-5 accuracy drop.
Further pruning can also be carried out, leading to a much
smaller model at the cost of more accuracy loss.
However, reduced FLOPs can not bring the same level
of acceleration in ResNet. Due to the structure constraints
of ResNet-50, non-tensor layers (e.g., batch normalization
and pooling layers) take up more than 40% of the inference
time on GPU. Hence, there is a great need to accelerate these
non-tensor layers, which should be explored in the future.
In this experiment, we only prune the ﬁrst two layers of
each block in ResNet for simplicity, leaving the block output
and projection shortcuts unchanged. Pruning these parts
would lead to further compression, but can be quite difﬁcult,
if not entirely impossible. And this exploration seems to be
a promising extension for the future work.
4.4. Domain adaptation ability of the pruned model
One of the main advantages of ThiNet is that we have
not changed network structure, thus a model pruned on ImageNet can be easily transfered into other domains.
To help us better understand this beneﬁt, let us consider
a more practical scenario: get a small model on a domainspeciﬁc dataset. This is a very common requirement in the
real-world applications, since we will not directly apply
ImageNet models in a real application. To achieve this goal,
there are two feasible strategies: starting from a pre-trained
ImageNet model then prune on the new dataset, or train a
small model from scratch. In this section, we argue that it
would be a better choice if we ﬁne-tune an already pruned
model which is compressed on ImageNet.
These strategies are compared on two different domainspeciﬁc dataset: CUB-200 for ﬁne-grained classiﬁcation and Indoor-67 for scene recognition. We have
introduced CUB-200 in section 4.1. As for Indoor-67, we
follow the ofﬁcial train/test split (5360 training and 1340
test images) to organize this dataset. All the models are
ﬁne-tuned with the same hyper-parameters and epochs for a
fair comparison. Their performance is shown in Table 4.
We ﬁrst ﬁne-tune the pre-trained VGG-16 model on the
new dataset, which is a popular strategy adopted in numerous recognition tasks. As we can see, the ﬁne-tuned model
has the highest accuracy at the cost of huge model size and
slow inference speed. Then, we use the proposed ThiNet
approach to prune some unimportant ﬁlters (denoted by “FT
Table 4. Comparison of different strategies to get a small model on
CUB-200 and Indoor-67. “FT” stands for “Fine Tune”.
FT & prune
Train from scratch
ThiNet-Conv
ThiNet-GAP
ThiNet-Tiny
FT & prune
Train from scratch
ThiNet-Conv
ThiNet-GAP
ThiNet-Tiny
& prune”), converting the cumbersome model into a much
smaller one. With small-scale training examples, the accuracy cannot be recovered completely, i.e., the pruned model
can be easily trapped into bad local minima. However, if
we train a network from scratch with the same structure, its
accuracy can be much lower.
We suggest to ﬁne-tune the ThiNet model, which is ﬁrst
pruned using the ImageNet data. As shown in Table 4, this
strategy gets the best trade-off between model size and classiﬁcation accuracy. It is worth noting that the ThiNet-Conv
model can even obtain a similar accuracy as the original
VGG-16, but is smaller and much faster.
We also report the performance of ThiNet-Tiny on these
two datasets. Although ThiNet-Tiny has the same level of
accuracy as AlexNet on ImageNet, it shows much stronger
generalization ability. This tiny model can achieve 3% ∼8%
higher classiﬁcation accuracy than AlexNet when transferred
into domain-speciﬁc tasks with 50× fewer parameters. And
its model size is small enough to be deployed on resource
constrained devices.
5. Conclusion
In this paper, we proposed a uniﬁed framework, namely
ThiNet, for CNN model acceleration and compression. The
proposed ﬁlter level pruning method shows signiﬁcant improvements over existing methods.
In the future, we would like to prune the projection shortcuts of ResNet. An alternative method for better channel
selection is also worthy to be studied. In addition, extensive
exploration on more vision tasks (such as object detection
or semantic segmentation) with the pruned networks is an
interesting direction too. The pruned networks will greatly
accelerate these vision tasks.
Acknowledgements
This work was supported in part by the National Natural
Science Foundation of China under Grant No. 61422203.