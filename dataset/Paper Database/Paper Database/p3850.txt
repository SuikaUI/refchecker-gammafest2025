Foundations and Trends R
Information Retrieval
Vol. 2, Nos. 1–2 1–135
c⃝2008 B. Pang and L. Lee
DOI: 10.1561/1500000001
Opinion Mining and Sentiment Analysis
Bo Pang1 and Lillian Lee2
1 Yahoo! Research, 701 First Avenue, Sunnyvale, CA 94089, USA,
 
2 Computer Science Department, Cornell University, Ithaca, NY 14853,
USA, 
An important part of our information-gathering behavior has always
been to ﬁnd out what other people think. With the growing availability
and popularity of opinion-rich resources such as online review sites
and personal blogs, new opportunities and challenges arise as people
now can, and do, actively use information technologies to seek out and
understand the opinions of others. The sudden eruption of activity in
the area of opinion mining and sentiment analysis, which deals with
the computational treatment of opinion, sentiment, and subjectivity
in text, has thus occurred at least in part as a direct response to the
surge of interest in new systems that deal directly with opinions as a
ﬁrst-class object.
This survey covers techniques and approaches that promise to
directly enable opinion-oriented information-seeking systems. Our
focus is on methods that seek to address the new challenges raised by
sentiment-aware applications, as compared to those that are already
present in more traditional fact-based analysis. We include material
on summarization of evaluative text and on broader issues regarding
privacy, manipulation, and economic impact that the development of
opinion-oriented information-access services gives rise to. To facilitate
future work, a discussion of available resources, benchmark datasets,
and evaluation campaigns is also provided.
Introduction
Romance should never begin with sentiment. It should
begin with science and end with a settlement.
— Oscar Wilde, An Ideal Husband
The Demand for Information on Opinions
and Sentiment
“What other people think” has always been an important piece of information for most of us during the decision-making process. Long before
awareness of the World Wide Web became widespread, many of us
asked our friends to recommend an auto mechanic or to explain who
they were planning to vote for in local elections, requested reference
letters regarding job applicants from colleagues, or consulted Consumer
Reports to decide what dishwasher to buy. But the Internet and the Web
have now (among other things) made it possible to ﬁnd out about the
opinions and experiences of those in the vast pool of people that are neither our personal acquaintances nor well-known professional critics —
that is, people we have never heard of. And conversely, more and more
people are making their opinions available to strangers via the Internet.
Introduction
Indeed, according to two surveys of more than 2000 American adults
each ,
• 81% of Internet users (or 60% of Americans) have done online
research on a product at least once;
• 20% (15% of all Americans) do so on a typical day;
• among readers of online reviews of restaurants, hotels, and
various services (e.g., travel agencies or doctors), between
73% and 87% report that reviews had a signiﬁcant inﬂuence
on their purchase;1
• consumers report being willing to pay from 20% to 99% more
for a 5-star-rated item than a 4-star-rated item (the variance
stems from what type of item or service is considered);
• 32% have provided a rating on a product, service, or person via an online ratings system, and 30% (including 18%
of online senior citizens) have posted an online comment or
review regarding a product or service.2
We hasten to point out that consumption of goods and services
is not the only motivation behind people’s seeking out or expressing
opinions online. A need for political information is another important
factor. For example, in a survey of over 2500 American adults, Rainie
and Horrigan studied the 31% of Americans — over 60 million
people — that were 2006 campaign internet users, deﬁned as those who
gathered information about the 2006 elections online and exchanged
views via email. Of these,
• 28% said that a major reason for these online activities was
to get perspectives from within their community, and 34%
said that a major reason was to get perspectives from outside
their community;
• 27% had looked online for the endorsements or ratings of
external organizations;
1 Section 6.1 discusses quantitative analyses of actual economic impact, as opposed to consumer perception.
2 Interestingly, Hitlin and Rainie report that “Individuals who have rated something
online are also more skeptical of the information that is available on the Web.”
1.1 The Demand for Information on Opinions and Sentiment
• 28% said that most of the sites they use share their point
of view, but 29% said that most of the sites they use challenge their point of view, indicating that many people are not
simply looking for validations of their pre-existing opinions;
• 8% posted their own political commentary online.
The user hunger for and reliance upon online advice and recommendations that the data above reveals is merely one reason behind
the surge of interest in new systems that deal directly with opinions as
a ﬁrst-class object. But, Horrigan reports that while a majority of
American internet users report positive experiences during online product research, at the same time, 58% also report that online information
was missing, impossible to ﬁnd, confusing, and/or overwhelming. Thus,
there is a clear need to aid consumers of products and of information
by building better information-access systems than are currently in
existence.
The interest that individual users show in online opinions about
products and services, and the potential inﬂuence such opinions wield,
is something that vendors of these items are paying more and more
attention to . The following excerpt from a whitepaper is illustrative of the envisioned possibilities, or at the least the rhetoric surrounding the possibilities:
With the explosion of Web 2.0 platforms such as blogs,
discussion forums, peer-to-peer networks, and various
other types of social media . . . consumers have at their
disposal a soapbox of unprecedented reach and power
by which to share their brand experiences and opinions,
positive or negative, regarding any product or service.
As major companies are increasingly coming to realize,
these consumer voices can wield enormous inﬂuence in
shaping the opinions of other consumers — and, ultimately, their brand loyalties, their purchase decisions,
and their own brand advocacy. . . . Companies can
respond to the consumer insights they generate through
social media monitoring and analysis by modifying their
Introduction
marketing messages, brand positioning, product development, and other activities accordingly.
— Zabin and Jeﬀeries 
But industry analysts note that the leveraging of new media for the
purpose of tracking product image requires new technologies; here is a
representative snippet describing their concerns:
Marketers have always needed to monitor media for
information related to their brands — whether it’s
for public relations activities, fraud violations,3 or
competitive intelligence. But fragmenting media and
changing consumer behavior have crippled traditional
monitoring methods. Technorati estimates that 75,000
new blogs are created daily, along with 1.2 million new
posts each day, many discussing consumer opinions
on products and services. Tactics [of the traditional
sort] such as clipping services, ﬁeld agents, and ad hoc
research simply can’t keep pace.
— Kim 
Thus, aside from individuals, an additional audience for systems capable of automatically analyzing consumer sentiment, as expressed in no
small part in online venues, are companies anxious to understand how
their products and services are perceived.
What Might be Involved? An Example
Examination of the Construction of
an Opinion/Review Search Engine
Creating systems that can process subjective information eﬀectively
requires overcoming a number of novel challenges. To illustrate some
of these challenges, let us consider the concrete example of what building an opinion- or review-search application could involve. As we have
discussed, such an application would ﬁll an important and prevalent
3 Presumably, the author means “the detection or prevention of fraud violations,” as
opposed to the commission thereof.
1.2 What Might be Involved?
information need, whether one restricts attention to blog search 
or considers the more general types of search that have been described
The development of a complete review- or opinion-search application might involve attacking each of the following problems.
(1) If the application is integrated into a general-purpose search
engine, then one would need to determine whether the user
is in fact looking for subjective material. This may or may
not be a diﬃcult problem in and of itself: perhaps queries of
this type will tend to contain indicator terms like “review,”
“reviews,” or “opinions,” or perhaps the application would
provide a “checkbox” to the user so that he or she could indicate directly that reviews are what is desired; but in general,
query classiﬁcation is a diﬃcult problem — indeed, it was
the subject of the 2005 KDD Cup challenge .
(2) Besides the still-open problem of determining which documents are topically relevant to an opinion-oriented query,
an additional challenge we face in our new setting is
simultaneously or subsequently determining which documents or portions of documents contain review-like or opinionated material. Sometimes this is relatively easy, as in
texts fetched from review-aggregation sites in which revieworiented information is presented in relatively stereotyped
format: examples include Epinions.com and Amazon.com.
However, blogs also notoriously contain quite a bit of subjective content and thus are another obvious place to look (and
are more relevant than shopping sites for queries that concern politics, people, or other non-products), but the desired
material within blogs can vary quite widely in content, style,
presentation, and even level of grammaticality.
(3) Once one has target documents in hand, one is still faced with
the problem of identifying the overall sentiment expressed
by these documents and/or the speciﬁc opinions regarding particular features or aspects of the items or topics in
question, as necessary. Again, while some sites make this
Introduction
kind of extraction easier — for instance, user reviews posted
to Yahoo! Movies must specify grades for pre-deﬁned sets of
characteristics of ﬁlms — more free-form text can be much
harder for computers to analyze, and indeed can pose additional challenges; for example, if quotations are included in a
newspaper article, care must be taken to attribute the views
expressed in each quotation to the correct entity.
(4) Finally, the system needs to present the sentiment information it has garnered in some reasonable summary fashion.
This can involve some or all of the following actions:
(a) Aggregation of “votes” that may be registered
on diﬀerent scales (e.g., one reviewer uses a star
system, but another uses letter grades).
(b) Selective highlighting of some opinions.
(c) Representation of points of disagreement and
points of consensus.
(d) Identiﬁcation of communities of opinion holders.
(e) Accounting
among opinion holders.
Note that it might be more appropriate to produce a visualization of sentiment data rather than a textual summary of
it, whereas textual summaries are what is usually created in
standard topic-based multi-document summarization.
Our Charge and Approach
Challenges (2), (3), and (4) in the above list are very active areas of
research, and the bulk of this survey is devoted to reviewing work in
these three sub-ﬁelds. However, due to space limitations and the focus
of the journal series in which this survey appears, we do not and cannot
aim to be completely comprehensive.
In particular, when we began to write this survey, we were directly
charged to focus on information-access applications, as opposed to work
of more purely linguistic interest. We stress that the importance of work
in the latter vein is absolutely not in question.
1.4 Early History
Given our mandate, the reader will not be surprised that we describe
the applications that sentiment-analysis systems can facilitate and
review many kinds of approaches to a variety of opinion-oriented classiﬁcation problems. We have also chosen to attempt to draw attention
to single- and multi-document summarization of evaluative text, especially since interesting considerations regarding graphical visualization
arise. Finally, we move beyond just the technical issues, devoting signiﬁcant attention to the broader implications that the development of
opinion-oriented information-access services have: we look at questions
of privacy, manipulation, and whether or not reviews can have measurable economic impact.
Early History
Although the area of sentiment analysis and opinion mining has
recently enjoyed a huge burst of research activity, there has been a
steady undercurrent of interest for quite a while. One could count
early projects on beliefs as forerunners of the area . Later work
focused mostly on interpretation of metaphor, narrative, point of view,
aﬀect, evidentiality in text, and related areas .
The year 2001 or so seems to mark the beginning of widespread
awareness of the research problems and opportunities that sentiment
analysis and opinion mining raise , and subsequently there have been literally
hundreds of papers published on the subject.
Factors behind this “land rush” include:
• the rise of machine learning methods in natural language
processing and information retrieval;
• the availability of datasets for machine learning algorithms
to be trained on, due to the blossoming of the World Wide
Web and, speciﬁcally, the development of review-aggregation
web-sites; and, of course
• realization of the fascinating intellectual challenges and commercial and intelligence applications that the area oﬀers.
Introduction
A Note on Terminology: Opinion Mining, Sentiment
Analysis, Subjectivity, and All that
‘The beginning of wisdom is the deﬁnition of terms,’
wrote Socrates. The aphorism is highly applicable when
it comes to the world of social media monitoring and
analysis, where any semblance of universal agreement
on terminology is altogether lacking.
Today, vendors, practitioners, and the media alike call
this still-nascent arena everything from ‘brand monitoring,’ ‘buzz monitoring’ and ‘online anthropology,’ to
‘market inﬂuence analytics,’ ‘conversation mining’ and
‘online consumer intelligence’. . . . In the end, the term
‘social media monitoring and analysis’ is itself a verbal
crutch. It is placeholder [sic], to be used until something
better (and shorter) takes hold in the English language
to describe the topic of this report.
— Zabin and Jeﬀeries 
The above quotation highlights the problems that have arisen in
trying to name a new area. The quotation is particularly apt in the
context of this survey because the ﬁeld of “social media monitoring
and analysis” (or however one chooses to refer to it) is precisely one
that the body of work we review is very relevant to. And indeed, there
has been to date no uniform terminology established for the relatively
young ﬁeld we discuss in this survey. In this section, we simply mention
some of the terms that are currently in vogue, and attempt to indicate
what these terms tend to mean in research papers that the interested
reader may encounter.
The body of work we review is that which deals with the computational treatment of (in alphabetical order) opinion, sentiment, and subjectivity in text. Such work has come to be known as opinion mining,
sentiment analysis, and/or subjectivity analysis. The phrases review
mining and appraisal extraction have been used, too, and there are some
connections to aﬀective computing, where the goals include enabling
computers to recognize and express emotions . This proliferation
of terms reﬂects diﬀerences in the connotations that these terms carry,
1.5 A Note on Terminology
both in their original general-discourse usages4 and in the usages that
have evolved in the technical literature of several communities.
In 1994, Wiebe , inﬂuenced by the writings of the literary
theorist Banﬁeld , centered the idea of subjectivity around that of
private states, deﬁned by Quirk et al. as states that are not open to
objective observation or veriﬁcation. Opinions, evaluations, emotions,
and speculations all fall into this category; but a canonical example
of research typically described as a type of subjectivity analysis is the
recognition of opinion-oriented language in order to distinguish it from
objective language. While there has been some research self-identiﬁed
as subjectivity analysis on the particular application area of determining the value judgments (e.g., “four stars” or “C+”) expressed in the
evaluative opinions that are found, this application has not tended to
be a major focus of such work.
The term opinion mining appears in a paper by Dave et al. 
that was published in the proceedings of the 2003 WWW conference;
the publication venue may explain the popularity of the term within
communities strongly associated with Web search or information
retrieval. According to Dave et al. , the ideal opinion-mining tool
would “process a set of search results for a given item, generating a list
of product attributes (quality, features, etc.) and aggregating opinions
4 To see that the distinctions in common usage can be subtle, consider how interrelated the
following set of deﬁnitions given in Merriam-Webster’s Online Dictionary are:
Synonyms: opinion, view, belief, conviction, persuasion, sentiment mean
a judgment one holds as true.
• Opinion implies a conclusion thought out yet open to dispute
⟨each expert seemed to have a diﬀerent opinion⟩.
• View suggests a subjective opinion ⟨very assertive in stating
his views⟩.
• Belief implies often deliberate acceptance and intellectual
assent ⟨a ﬁrm belief in her party’s platform⟩.
• Conviction applies to a ﬁrmly and seriously held belief ⟨the
conviction that animal life is as sacred as human⟩.
• Persuasion suggests a belief grounded on assurance (as by
evidence) of its truth ⟨was of the persuasion that everything
• Sentiment suggests a settled opinion reﬂective of one’s feelings
⟨her feminist sentiments are well-known⟩.
Introduction
about each of them (poor, mixed, good).” Much of the subsequent
research self-identiﬁed as opinion mining ﬁts this description in its
emphasis on extracting and analyzing judgments on various aspects
of given items. However, the term has recently also been interpreted
more broadly to include many diﬀerent types of analysis of evaluative
text .
The history of the phrase sentiment analysis parallels that of “opinion mining” in certain respects. The term “sentiment” used in reference
to the automatic analysis of evaluative text and tracking of the predictive judgments therein appears in 2001 papers by Das and Chen 
and Tong , due to these authors’ interest in analyzing market sentiment. It subsequently occurred within 2002 papers by Turney and
Pang et al. , which were published in the proceedings of the annual
meeting of the Association for Computational Linguistics (ACL) and
the annual conference on Empirical Methods in Natural Language Processing (EMNLP). Moreover, Nasukawa and Yi entitled their 2003
paper, “Sentiment analysis: Capturing favorability using natural language processing”, and a paper in the same year by Yi et al. was
named “Sentiment Analyzer: Extracting sentiments about a given topic
using natural language processing techniques.” These events together
may explain the popularity of “sentiment analysis” among communities self-identiﬁed as focused on NLP. A sizeable number of papers
mentioning “sentiment analysis” focus on the speciﬁc application of
classifying reviews as to their polarity (either positive or negative), a
fact that appears to have caused some authors to suggest that the
phrase refers speciﬁcally to this narrowly deﬁned task. However, nowadays many construe the term more broadly to mean the computational
treatment of opinion, sentiment, and subjectivity in text.
Thus, when broad interpretations are applied, “sentiment analysis”
and “opinion mining” denote the same ﬁeld of study (which itself can
be considered a sub-area of subjectivity analysis). We have attempted
to use these terms more or less interchangeably in this survey. This is in
no small part because we view the ﬁeld as representing a uniﬁed body
of work, and would thus like to encourage researchers in the area to
share terminology regardless of the publication venues at which their
papers might appear.
Applications
Sentiment without action is the ruin of the soul.
— Edward Abbey
We used one application of opinion mining and sentiment analysis as a
motivating example in the Introduction, namely, web search targeted
toward reviews. But other applications abound. In this section, we seek
to enumerate some of the possibilities.
It is important to mention that because of all the possible applications, there are a good number of companies, large and small, that have
opinion mining and sentiment analysis as part of their mission. However, we have elected not to mention these companies individually due
to the fact that the industrial landscape tends to change quite rapidly,
so that lists of companies risk falling out of date rather quickly.
Applications to Review-Related Websites
Clearly, the same capabilities that a review-oriented search engine
would have could also serve very well as the basis for the creation and
automated upkeep of review- and opinion-aggregation websites. That is,
as an alternative to sites like Epinions that solicit feedback and reviews,
Applications
one could imagine sites that proactively gather such information. Topics
need not be restricted to product reviews, but could include opinions
about candidates running for oﬃce, political issues, and so forth.
There are also applications of the technologies we discuss to more
traditional review-solicitation sites, as well. Summarizing user reviews
is an important problem. One could also imagine that errors in user
ratings could be ﬁxed: there are cases where users have clearly accidentally selected a low rating when their review indicates a positive
evaluation . Moreover, as discussed later in this survey (see Section 5.2.4, for example), there is some evidence that user ratings can
be biased or otherwise in need of correction, and automated classiﬁers
could provide such updates.
Applications as a Sub-Component Technology
Sentiment-analysis and opinion-mining systems also have an important
potential role as enabling technologies for other systems.
One possibility is as an augmentation to recommendation systems
 , since it might behoove such a system not to recommend
items that receive a lot of negative feedback.
Detection of “ﬂames” (overly heated or antagonistic language) in
email or other types of communication is another possible use of
subjectivity detection and classiﬁcation.
In online systems that display ads as sidebars, it is helpful to detect
webpages that contain sensitive content inappropriate for ads placement ; for more sophisticated systems, it could be useful to bring
up product ads when relevant positive sentiments are detected, and perhaps more importantly, nix the ads when relevant negative statements
are discovered.
It has also been argued that information extraction can be improved
by discarding information found in subjective sentences .
Question answering is another area where sentiment analysis can
prove useful . For example, opinion-oriented questions
may require diﬀerent treatment. Alternatively, Lita et al. suggest
that for deﬁnitional questions, providing an answer that includes more
information about how an entity is viewed may better inform the user.
2.3 Applications in Business and Government Intelligence
Summarization may also beneﬁt from accounting for multiple viewpoints .
Additionally, there are potentially relations to citation analysis,
where, for example, one might wish to determine whether an author
is citing a piece of work as supporting evidence or as research that
he or she dismisses . Similarly, one eﬀort seeks to use semantic
orientation to track literary reputation .
In general, the computational treatment of aﬀect has been motivated in part by the desire to improve human–computer interaction
 .
Applications in Business and Government Intelligence
The ﬁeld of opinion mining and sentiment analysis is well-suited to
various types of intelligence applications. Indeed, business intelligence
seems to be one of the main factors behind corporate interest in the
Consider, for instance, the following scenario (the text of which also
appears in Lee ). A major computer manufacturer, disappointed
with unexpectedly low sales, ﬁnds itself confronted with the question:
“Why aren’t consumers buying our laptop?” While concrete data such
as the laptop’s weight or the price of a competitor’s model are obviously
relevant, answering this question requires focusing more on people’s
personal views of such objective characteristics. Moreover, subjective
judgments regarding intangible qualities — e.g., “the design is tacky”
or “customer service was condescending” — or even misperceptions —
e.g., “updated device drivers are not available” when such device drivers
do in fact exist — must be taken into account as well.
Sentiment-analysis
technologies
extracting
unstructured human-authored documents would be excellent tools
for handling many business-intelligence tasks related to the one just
described. Continuing with our example scenario: it would be diﬃcult
to try to directly survey laptop purchasers who have not bought the
company’s product. Rather, we could employ a system that (a) ﬁnds
reviews or other expressions of opinion on the Web — newsgroups,
individual blogs, and aggregation sites such as Epinions are likely to
Applications
be productive sources — and then (b) creates condensed versions of
individual reviews or a digest of overall consensus points. This would
save an analyst from having to read potentially dozens or even hundreds of versions of the same complaints. Note that Internet sources can
vary wildly in form, tenor, and even grammaticality; this fact underscores the need for robust techniques even when only one language
(e.g., English) is considered.
Besides reputation management and public relations, one might perhaps hope that by tracking public viewpoints, one could perform trend
prediction in sales or other relevant data . (See our discussion of
Broader Implications (Section 6) for more discussion of potential economic impact.)
Government intelligence is another application that has been considered. For example, it has been suggested that one could monitor
sources for increases in hostile or negative communications .
Applications Across Diﬀerent Domains
One exciting turn of events has been the conﬂuence of interest in opinions and sentiment within computer science with interest in opinions
and sentiment in other ﬁelds.
As is well known, opinions matter a great deal in politics. Some
work has focused on understanding what voters are thinking , whereas other projects have as a long term goal the clariﬁcation of politicians’ positions, such as what public ﬁgures support or
oppose, to enhance the quality of information that voters have access
to .
Sentiment analysis has speciﬁcally been proposed as a key enabling
technology in eRulemaking, allowing the automatic analysis of the opinions that people submit about pending policy or government-regulation
proposals .
On a related note, there has been investigation into opinion mining
in weblogs devoted to legal matters, sometimes known as “blawgs” .
Interactions with sociology promise to be extremely fruitful. For
instance, the issue of how ideas and innovations diﬀuse involves
the question of who is positively or negatively disposed toward whom,
2.4 Applications Across Diﬀerent Domains
and hence who would be more or less receptive to new information
transmission from a given source. To take just one other example:
structural balance theory is centrally concerned with the polarity
of “ties” between people and how this relates to group cohesion. These ideas have begun to be applied to online media analysis
 .
General Challenges
Contrasts with Standard Fact-Based Textual Analysis
The increasing interest in opinion mining and sentiment analysis is
partly due to its potential applications, which we have just discussed.
Equally important are the new intellectual challenges that the ﬁeld
presents to the research community. So what makes the treatment
of evaluative text diﬀerent from “classic” text mining and fact-based
Take text categorization, for example. Traditionally, text categorization seeks to classify documents by topic. There can be many possible
categories, the deﬁnitions of which might be user- and applicationdependent; and for a given task, we might be dealing with as few as
two classes (binary classiﬁcation) or as many as thousands of classes
(e.g., classifying documents with respect to a complex taxonomy). In
contrast, with sentiment classiﬁcation (see Section 4.1 for more details
on precise deﬁnitions), we often have relatively few classes (e.g., “positive” or “3 stars”) that generalize across many domains and users.
In addition, while the diﬀerent classes in topic-based categorization
can be completely unrelated, the sentiment labels that are widely
3.2 Factors that Make Opinion Mining Diﬃcult
considered in previous work typically represent opposing (if the task is
binary classiﬁcation) or ordinal/numerical categories (if classiﬁcation is
according to a multi-point scale). In fact, the regression-like nature of
strength of feeling, degree of positivity, and so on seems rather unique
to sentiment categorization (although one could argue that the same
phenomenon exists with respect to topic-based relevance).
There are also many characteristics of answers to opinion-oriented
questions that diﬀer from those for fact-based questions . As a
result, opinion-oriented information extraction, as a way to approach
opinion-oriented question answering, naturally diﬀers from traditional
information extraction (IE) . Interestingly, in a manner that is similar to the situation for the classes in sentiment-based classiﬁcation, the
templates for opinion-oriented IE also often generalize well across diﬀerent domains, since we are interested in roughly the same set of ﬁelds for
each opinion expression (e.g., holder, type, strength) regardless of the
topic. In contrast, traditional IE templates can diﬀer greatly from one
domain to another — the typical template for recording information
relevant to a natural disaster is very diﬀerent from a typical template
for storing bibliographic information.
These distinctions might make our problems appear deceptively
simpler than their counterparts in fact-based analysis, but this is far
from the truth. In the next section, we sample a few examples to show
what makes these problems diﬃcult compared to traditional fact-based
text analysis.
Factors that Make Opinion Mining Diﬃcult
Let us begin with a sentiment polarity text-classiﬁcation example. Suppose we wish to classify an opinionated text as either positive or
negative, according to the overall sentiment expressed by the author
within it. Is this a diﬃcult task?
To answer this question, ﬁrst consider the following example,
consisting of only one sentence (by Mark Twain): “Jane Austen’s books
madden me so that I can’t conceal my frenzy from the reader.” Just
as the topic of this text segment can be identiﬁed by the phrase “Jane
Austen,” the presence of words like “madden” and “frenzy” suggests
General Challenges
negative sentiment. So one might think this is an easy task, and
hypothesize that the polarity of opinions can generally be identiﬁed
by a set of keywords.
But, the results of an early study by Pang et al. on movie
reviews suggest that coming up with the right set of keywords might be
less trivial than one might initially think. The purpose of Pang et al.’s
pilot study was to better understand the diﬃculty of the documentlevel sentiment-polarity classiﬁcation problem. Two human subjects
were asked to pick keywords that they would consider to be good indicators of positive and negative sentiment. As shown in Figure 3.1, the
use of the subjects’ lists of keywords achieves about 60% accuracy when
employed within a straightforward classiﬁcation policy. In contrast,
word lists of the same size but chosen based on examination of the
corpus’ statistics achieves almost 70% accuracy — even though some
of the terms, such as “still,” might not look that intuitive at ﬁrst.
However, the fact that it may be non-trivial for humans to come
up with the best set of keywords does not in itself imply that the
problem is harder than topic-based categorization. While the feature
“still” might not be likely for any human to propose from introspection,
given training data, its correlation with the positive class can be
discovered via a data-driven approach, and its utility (at least in
Proposed word lists
positive: dazzling, brilliant, phenomenal, excellent,
negative: suck, terrible, awful, unwatchable,
positive: gripping, mesmerizing, riveting,
spectacular, cool, awesome, thrilling, badass,
excellent, moving, exciting
negative: bad, cliched, sucks, boring, stupid, slow
Statistics-based
positive: love, wonderful, best, great, superb, still,
negative: bad, worst, stupid, waste, boring, ?, !
Fig. 3.1 Sentiment classiﬁcation using keyword lists created by human subjects (“Human
1” and “Human 2”), with corresponding results using keywords selected via examination
of simple statistics of the test data (“Statistics-based”). Adapted from Figures 1 and 2 in
Pang et al. .
3.2 Factors that Make Opinion Mining Diﬃcult
the movie review domain) does make sense in retrospect. Indeed,
applying machine learning techniques based on unigram models can
achieve over 80% in accuracy , which is much better than the performance based on hand-picked keywords reported above. However, this
level of accuracy is not quite on par with the performance one would
expect in typical topic-based binary classiﬁcation.
Why does this problem appear harder than the traditional task
when the two classes we are considering here are so diﬀerent from each
other? Our discussion of algorithms for classiﬁcation and extraction
(Section 4) will provide a more in-depth answer to this question, but
the following are a few examples (from among the many we know)
showing that the upper bound on problem diﬃculty, from the viewpoint
of machines, is very high. Note that not all of the issues these examples
raise have been fully addressed in the existing body of work in this
Compared to topic, sentiment can often be expressed in a more
subtle manner, making it diﬃcult to be identiﬁed by any of a sentence or
document’s terms when considered in isolation. Consider the following
• “If you are reading this because it is your darling fragrance,
please wear it at home exclusively, and tape the windows
shut.” No ostensibly negative words occur.
• “She runs the gamut of emotions from A to B.” (Dorothy
Parker, speaking about Katharine Hepburn.) No ostensibly
negative words occur.
In fact, the example that opens this section, which was taken from
the following quote from Mark Twain, is also followed by a sentence
with no ostensibly negative words:
Jane Austen’s books madden me so that I can’t conceal
my frenzy from the reader. Everytime I read ‘Pride and
Prejudice’ I want to dig her up and beat her over the
skull with her own shin-bone.
General Challenges
A related observation is that although the second sentence indicates
an extremely strong opinion, it is diﬃcult to associate the presence of
this strong opinion with speciﬁc keywords or phrases in this sentence.
Indeed, subjectivity detection can be a diﬃcult task in itself. Consider
the following quote from Charlotte Bront¨e, in a letter to George Lewes:
You say I must familiarise my mind with the fact that
“Miss Austen is not a poetess, has no ‘sentiment’ ”
(you scornfully enclose the word in inverted commas),
“has no eloquence, none of the ravishing enthusiasm of
poetry”; and then you add, I must “learn to acknowledge her as one of the greatest artists, of the greatest
painters of human character, and one of the writers with
the nicest sense of means to an end that ever lived.”
Note the ﬁne line between facts and opinions: while “Miss Austen
is not a poetess” can be considered to be a fact, “none of the ravishing
enthusiasm of poetry” should probably be considered as an opinion,
even though the two phrases s (arguably) convey similar information.1
Thus, not only can we not easily identify simple keywords for subjectivity, but we also ﬁnd that like “the fact that” do not necessarily
guarantee the objective truth of what follows them — and bigrams like
“no sentiment” apparently do not guarantee the absence of opinions,
either. We can also get a glimpse of how opinion-oriented information
1 One can challenge our analysis of the “poetess” clause, as an anonymous reviewer indeed
did — which disagreement perhaps supports our greater point about the diﬃculties that
can sometimes present themselves.
Diﬀerent researchers express diﬀerent opinions about whether distinguishing between
subjective and objective language is diﬃcult for humans in the general case. For example,
Kim and Hovy note that in a pilot study sponsored by NIST, “human annotators
often disagreed on whether a belief statement was or was not an opinion.” However, other
researchers have found inter-annotator agreement rates in various types of subjectivityclassiﬁcation tasks to be satisfactory ; a summary provided by one of
the anonymous referees is that “[although] there is variation from study to study, on
average, about 85% of annotations are not marked as uncertain by either annotator, and
for these cases, inter-coder agreement is very high (kappa values over 80).” As in other
settings, more careful deﬁnitions of the distinctions to be made tend to lead to better
agreement rates.
In any event, the points we are exploring in the Bront¨e quote may be made more clear
by replacing “Jane Austen is not a poetess” with something like “Jane Austen does not
write poetry for a living, but is also no poet in the broader sense.”
3.2 Factors that Make Opinion Mining Diﬃcult
extraction can be diﬃcult. For instance, it is non-trivial to recognize
opinion holders. In the example quoted above, the opinion is not that
of the author, but the opinion of “You,” which refers to George Lewes
in this particular letter. Also, observe that given the context (“you
scornfully enclose the word in inverted commas,” together with the
reported endorsement of Austen as a great artist), it is clear that “has
no sentiment” is not meant to be a show-stopping criticism of Austen
from Lewes, and Bront¨e’s disagreement with him on this subject is also
subtly revealed.
In general, sentiment and subjectivity are quite context-sensitive,
and, at a coarser granularity, quite domain dependent (in spite of the
fact that the general notion of positive and negative opinions is fairly
consistent across diﬀerent domains). Note that although domain dependency is in part a consequence of changes in vocabulary, even the exact
same expression can indicate diﬀerent sentiment in diﬀerent domains.
For example, “go read the book” most likely indicates positive sentiment for book reviews, but negative sentiment for movie reviews.
(This example was furnished to us by Bob Bland.) We will discuss
topic-sentiment interaction in more detail in Section 4.4.
It does not take a seasoned writer or a professional journalist to
produce texts that are diﬃcult for machines to analyze. The writings
of Web users can be just as challenging, if not as subtle, in their own
way — see Figure 3.2 for an example. In the case of Figure 3.2, it
should be pointed out that might be more useful to learn to recognize
the quality of a review (see Section 5.2 for more detailed discussions
on that subject). Still, it is interesting to observe the importance of
modeling discourse structure. While the overall topic of a document
Fig. 3.2 Example of movie reviews produced by web users: a (slightly reformatted) screenshot of user reviews for The Nightmare Before Christmas.
General Challenges
should be what the majority of the content is focusing on regardless
of the order in which potentially diﬀerent subjects are presented, for
opinions, the order in which diﬀerent opinions are presented can result
in a completely opposite overall sentiment polarity.
In fact, somewhat in contrast with topic-based text categorization,
order eﬀects can completely overwhelm frequency eﬀects. Consider the
following excerpt, again from a movie review:
This ﬁlm should be brilliant. It sounds like a great plot,
the actors are ﬁrst grade, and the supporting cast is
good as well, and Stallone is attempting to deliver a
good performance. However, it can’t hold up.
As indicated by the (inserted) emphasis, words that are positive in
orientation dominate this excerpt,2 and yet the overall sentiment is
negative because of the crucial last sentence; whereas in traditional
text classiﬁcation, if a document mentions “cars” relatively frequently,
then the document is most likely at least somewhat related to cars.
Order dependence also manifests itself at more ﬁne-grained levels of
analysis: “A is better than B” conveys the exact opposite opinion from
“B is better than A.”3 In general, modeling sequential information and
discourse structure seems more crucial in sentiment analysis (further
discussion appears in Section 4.7).
As noted earlier, not all of the issues we have just discussed have
been fully addressed in the literature. This is perhaps part of the charm
of this emerging area. In the following sections, we aim to give an
overview of a selection of past heroic eﬀorts to address some of these
issues, and march through the positives and the negatives, charged with
unbiased feeling, armed with hard facts.
Fasten your seat belts. It’s going to be a bumpy night!
— Bette Davis, All About Eve,
screenplay by Joseph Mankiewicz
2 One could argue about whether in the context of movie reviews the word “Stallone” has
a semantic orientation.
3 Note that this is not unique to opinion expressions; “A killed B” and “B killed A” also
convey diﬀerent factual information.
Classiﬁcation and Extraction
“The Bucket List,” which was written by Justin Zackham and directed by Rob Reiner, seems to have been
created by applying algorithms to sentiment.
— David Denby movie review,
The New Yorker, January 7, 2007
A fundamental technology in many current opinion-mining and
sentiment-analysis applications is classiﬁcation — note that in this survey, we generally construe the term “classiﬁcation” broadly, so that it
encompasses regression and ranking. The reason that classiﬁcation is so
important is that many problems of interest can be formulated as applying classiﬁcation/regression/ranking to given textual units; examples
include making a decision for a particular phrase or document (“how
positive is it?”), ordering a set of texts (“rank these reviews by how positive they are”), giving a single label to an entire document collection
(“where on the scale between liberal and conservative do the writings of
this author lie?”), and categorizing the relationship between two entities based on textual evidence (“does A approve of B’s actions?”). This
section is centered on approaches to these kinds of problems.
Classiﬁcation and Extraction
Part One (p. 24ﬀ.) covers fundamental background. Speciﬁcally,
Section 4.1 provides a discussion of key concepts involved in common
formulations of classiﬁcation problems in sentiment analysis and opinion mining. Features that have been explored for sentiment analysis
tasks are discussed in Section 4.2.
Part Two (p. 37ﬀ.) is devoted to an in-depth discussion of diﬀerent
types of approaches to classiﬁcation, regression, and ranking problems.
The beginning of Part Two should be consulted for a detailed outline,
but it is appropriate here to indicate how we cover extraction, since it
plays a key role in many sentiment-oriented applications and so some
readers may be particularly interested in it.
First, extraction problems (e.g., retrieving opinions on various features of a laptop) are often solved by casting many sub-problems as
classiﬁcation problems (e.g., given a text span, determine whether
it expresses any opinion at all). Therefore, rather than have a separate section devoted completely to the entirety of the extraction task,
we have integrated discussion of extraction-oriented classiﬁcation subproblems into the appropriate places in our discussion of diﬀerent types
of approaches to classiﬁcation in general (Sections 4.3–4.8). Section 4.9
covers those remaining aspects of extraction that can be thought of as
distinct from classiﬁcation.
Second, extraction is often a means to the further goal of providing eﬀective summaries of the extracted information to users. Details
on how to combine information mined from multiple subjective text
segments into a suitable summary can be found in Section 5.
Part One: Fundamentals
Problem Formulations and Key Concepts
Motivated by diﬀerent real-world applications, researchers have considered a wide range of problems over a variety of diﬀerent types of
corpora. We now examine the key concepts involved in these problems.
This discussion also serves as a loose grouping of the major problems,
where each group consists of problems that are suitable for similar
treatment as learning tasks.
4.1 Problem Formulations and Key Concepts
Sentiment Polarity and Degrees of Positivity
One set of problems share the following general character: given an
opinionated piece of text, wherein it is assumed that the overall opinion in it is about one single issue or item, classify the opinion as falling
under one of two opposing sentiment polarities, or locate its position
on the continuum between these two polarities. A large portion of work
in sentiment-related classiﬁcation/regression/ranking falls within this
category. Eguchi and Lavrenko point out that the polarity or positivity labels so assigned may be used simply for summarizing the content of opinionated text units on a topic, whether they be positive or
negative, or for only retrieving items of a given sentiment orientation
(say, positive).
The binary classiﬁcation task of labeling an opinionated document
as expressing either an overall positive or an overall negative opinion is called sentiment polarity classiﬁcation or polarity classiﬁcation.
Although this binary decision task has also been termed sentiment classiﬁcation in the literature, as mentioned above, in this survey we will
use “sentiment classiﬁcation” to refer broadly to binary categorization,
multi-class categorization, regression, and/or ranking.
Much work on sentiment polarity classiﬁcation has been conducted
in the context of reviews (e.g., “thumbs up” or “thumbs down” for
movie reviews). While in this context “positive” and “negative” opinions are often evaluative (e.g., “like” vs. “dislike”), there are other
problems where the interpretation of “positive” and “negative” is subtly diﬀerent. One example is determining whether a political speech is
in support of or opposition to the issue under debate ; a related
task is classifying predictive opinions in election forums into “likely to
win” and “unlikely to win” . Since these problems are all concerned with two opposing subjective classes, as machine learning tasks
they are often amenable to similar techniques. Note that a number of
other aspects of politically oriented text, such as whether liberal or
conservative views are expressed, have been explored; since the labels
used in those problems can usually be considered properties of a set of
documents representing authors’ attitudes over multiple issues rather
than positive or negative sentiment with respect to a single issue, we
Classiﬁcation and Extraction
discuss them under a diﬀerent heading further below (“viewpoints and
perspectives,” Section 4.1.4).
The input to a sentiment classiﬁer is not necessarily always strictly
opinionated. Classifying a news article into good or bad news has been
considered a sentiment classiﬁcation task in the literature . But
a piece of news can be good or bad news without being subjective
(i.e., without being expressive of the private states of the author): for
instance, “the stock price rose” is objective information that is generally
considered to be good news in appropriate contexts. It is not our main
intent to provide a clean-cut deﬁnition for what should be considered
“sentiment polarity classiﬁcation” problems,1 but it is perhaps useful to
point out that (a) in determining the sentiment polarity of opinionated
texts where the authors do explicitly express their sentiment through
statements like “this laptop is great,” (arguably) objective information
such as “long battery life”2 is often used to help determine the overall
sentiment; (b) the task of determining whether a piece of objective
information is good or bad is still not quite the same as classifying it
into one of several topic-based classes, and hence inherits the challenges
involved in sentiment analysis; and (c) as we will discuss in more detail
later, the distinction between subjective and objective information can
be subtle. Is “long battery life” objective? Also consider the diﬀerence
between “the battery lasts 2 hours” vs. “the battery only lasts 2 hours.”
Related categories.
An alternative way of summarizing reviews is to
extract information on why the reviewers liked or disliked the product.
Kim and Hovy note that such “pro and con” expressions can diﬀer
from positive and negative opinion expressions, although the two concepts — opinion (“I think this laptop is terriﬁc”) and reason for opinion
(“This laptop only costs $399”) — are for the purposes of analyzing
evaluative text strongly related. In addition to potentially forming
the basis for the production of more informative sentiment-oriented
summaries, identifying pro and con reasons can potentially be used to
1 While it is of utter importance that the problem itself should be well-deﬁned, it is of
less, if any, importance to decide which tasks should be labeled as “polarity classiﬁcation”
2 Whether this should be considered as an objective statement may be up for debate: one
can imagine another reviewer retorting, “you call that long battery life?”
4.1 Problem Formulations and Key Concepts
help decide the helpfulness of individual reviews: evaluative judgments
that are supported by reasons are likely to be more trustworthy.
Another type of categorization related to degrees of positivity is
considered by Niu et al. , who seek to determine the polarity of
outcomes (improvement vs. death, say) described in medical texts.
Additional problems related to the determination of degree of positivity surround the analysis of comparative sentences . The main
idea is that sentences such as “The new model is more expensive than
the old one” or “I prefer the new model to the old model” are important
sources of information regarding the author’s evaluations.
Rating inference (ordinal regression).
The more general problem of
rating inference, where one must determine the author’s evaluation with
respect to a multi-point scale (e.g., one to ﬁve “stars” for a review) can
be viewed simply as a multi-class text categorization problem. Predicting degree of positivity provides more ﬁne-grained rating information;
at the same time, it is an interesting learning problem in itself.
But in contrast to many topic-based multi-class classiﬁcation
problems, sentiment-related multi-class classiﬁcation can also be naturally formulated as a regression problem because ratings are ordinal.
It can be argued to constitute a special type of (ordinal) regression
problem because the semantics of each class may not simply directly
correspond to a point on a scale. More speciﬁcally, each class may
have its own distinct vocabulary. For instance, if we are classifying
an author’s evaluation into one of the positive, neutral, and negative
classes, an overall neutral opinion could be a mixture of positive and
negative language, or it could be identiﬁed with signature words such as
“mediocre.” This presents us with interesting opportunities to explore
the relationships between classes.
Note the diﬀerence between rating inference and predicting strength
of opinion (discussed in Section 4.1.2); for instance, it is possible to feel
quite strongly (high on the “strength” scale) that something is mediocre
(middling on the “evaluation” scale).
Also, note that the label “neutral” is sometimes used as a label for
the objective class (“lack of opinion”) in the literature. In this survey,
we use neutral only in the aforementioned sense of a sentiment that lies
between positive and negative.
Classiﬁcation and Extraction
Interestingly, Cabral and Horta¸csu observe that neutral comments in feedback systems are not necessarily perceived by users as
lying at the exact mid-point between positive and negative comments;
rather, “the information contained in a neutral rating is perceived by
users to be much closer to negative feedback than positive.” On the
other hand, they also note that in their data, “sellers were less likely
to retaliate against neutral comments, as opposed to negatives: . . . a
buyer leaving a negative comment has a 40% chance of being hit back,
while a buyer leaving a neutral comment only has a 10% chance of
being retaliated upon by the seller.”
Agreement.
The opposing nature of polarity classes also gives rise to
exploration of agreement detection, e.g., given a pair of texts, deciding
whether they should receive the same or diﬀering sentiment-related
labels based on the relationship between the elements of the pair. This
is often not deﬁned as a standalone problem but considered as a subtask whose result is used to improve the labeling of the opinions held by
the entities involved . A diﬀerent type of agreement task has
also been considered in the context of perspectives, where, for example,
a label of “conservative” tends to indicate agreement with particular
positions on a wide variety of issues.
Subjectivity Detection and Opinion Identiﬁcation
Work in polarity classiﬁcation often assumes the incoming documents
to be opinionated. For many applications, though, we may need to
decide whether a given document contains subjective information or
not, or identify which portions of the document are subjective. Indeed,
this problem was the focus of the 2006 Blog track at TREC .
At least one opinion-tracking system rates subjectivity and sentiment
separately . Mihalcea et al. summarize the evidence of several projects on subsentential analysis as follows:
“the problem of distinguishing subjective versus objective instances has
often proved to be more diﬃcult than subsequent polarity classiﬁcation,
so improvements in subjectivity classiﬁcation promise to positively
impact sentiment classiﬁcation.”
4.1 Problem Formulations and Key Concepts
Early work by Hatzivassiloglou and Wiebe examined the
eﬀects of adjective orientation and gradability on sentence subjectivity. The goal was to tell whether a given sentence is subjective or not
judging from the adjectives appearing in that sentence. A number of
projects address sentence-level or sub-sentence-level subjectivity detection in diﬀerent domains .Wiebe
et al. present a comprehensive survey of subjectivity recognition
using diﬀerent clues and features.
Wilson et al. address the problem of determining clause-level
opinion strength (e.g., “how mad are you?”). Note that the problem of
determining opinion strength is diﬀerent from rating inference. Classifying a piece of text as expressing a neutral opinion (giving it a midpoint score) for rating inference does not equal classifying that piece of
text as objective (lack of opinion): one can have a strong opinion that
something is “mediocre” or “so-so.”
Recent work also considers relations between word sense disambiguation and subjectivity .
Subjectivity detection or ranking at the document level can be
thought of as having its roots in studies in genre classiﬁcation (see
Section 4.1.5 for more detail). For instance, Yu and Hatzivassiloglou
 achieve high accuracy (97%) with a Naive Bayes classiﬁer on a
particular corpus consisting of Wall Street Journal articles, where the
task is to distinguish articles under News and Business (facts) from
articles under Editorial and Letter to the Editor (opinions). (This task
was suggested earlier by Wiebe et al. , and a similar corpus was
explored in previous work .) Work in this direction is not limited to the binary distinction between subjective and objective labels.
Recent work includes the research by participants in the 2006 TREC
Blog track and others .
Joint Topic–Sentiment Analysis
One simplifying assumption sometimes made by work on documentlevel sentiment classiﬁcation is that each document under consideration
is focused on the subject matter we are interested in. This is in part
because one can often assume that the document set was created
Classiﬁcation and Extraction
by ﬁrst collecting only on-topic documents (e.g., by ﬁrst running a
topic-based query through a standard search engine). However, it is
possible that there are interactions between topic and opinion that
make it desirable to consider the two simultaneously; for example,
Rilof et al. ﬁnd that “topic-based text ﬁltering and subjectivity ﬁltering are complementary” in the context of experiments in information
extraction.
Also, even a relevant opinion-bearing document may contain oﬀtopic passages that the user may not be interested in, and so one may
wish to discard such passages.
Another interesting case is when a document contains material on
multiple subjects that may be of interest to the user. In such a setting, it is useful to identify the topics and separate the opinions associated with each of them. Two examples of the types of documents for
which this kind of analysis is appropriate are (1) comparative studies
of related products, and (2) texts that discuss various features, aspects,
or attributes.3
Viewpoints and Perspectives
Much work on analyzing sentiment and opinions in politically oriented text focuses on general attitudes expressed through texts that
are not necessarily targeted at a particular issue or narrow subject. For
instance, Grefenstette et al. experimented with determining the
political orientation of websites essentially by classifying the concatenation of all the documents found on that site. We group this type of
work under the heading of “viewpoints and perspectives,” and include
under this rubric work on classifying texts as liberal, conservative, libertarian, etc. , placing texts along an ideological scale ,
or representing Israeli versus Palestinian viewpoints .
Although binary or n-ary classiﬁcation may be used, here, the
classes typically correspond not to opinions on a single, narrowly
deﬁned topic, but to a collection of bundled attitudes and beliefs.
This could potentially enable diﬀerent approaches from polarity
3 When the context is clear, we often use the term “feature” to refer to “feature, aspect, or
attribute” in this survey.
4.1 Problem Formulations and Key Concepts
classiﬁcation. On the other hand, if we treat the set of documents as
a meta-document, and the diﬀerent issues being discussed as metafeatures, then this problem still shares some common ground with
polarity classiﬁcation or its multi-class, regression, and ranking variants. Indeed, some of the approaches explored in the literature for these
two problems individually could very well be adapted to work for either
one of them.
The other point of departure from the polarity classiﬁcation problem
is that the labels being considered are more about attitudes that do
not naturally correspond with degree of positivity. While assigningsimple labels remains a classiﬁcation problem, if we move farther away
and aim at serving more expressive and open-ended opinions to the
user, we need to solve extraction problems. For instance, one may be
interested in obtaining descriptions of opinions of a greater complexity
than simple labels drawn from a very small set, i.e., one might be
seeking something more like “achieving world peace is diﬃcult” than
like “mildly positive.” In fact, much of the prior work on perspectives
and viewpoints seeks to extract more perspective-related information
(e.g., opinion holders). The motivation was to enable multi-perspective
question answering, where the user could ask questions such as “what is
Miss America’s perspective on world peace?” rather than a fact-based
question (e.g., “who is the new Miss America?”). Naturally, such work
is often framed in the context of extraction problems, the particular
characteristics of which are covered in Section 4.9.
Other Non-Factual Information in Text
Researchers have considered various aﬀect types, such as the six
“universal” emotions : anger, disgust, fear, happiness, sadness, and
surprise . An interesting application is in human–computer
interaction: if a system determines that a user is upset or annoyed, for
instance, it could switch to a diﬀerent mode of interaction .
Other related areas of research include computational approaches
for humor recognition and generation . Many interesting aﬀectual
aspects of text like “happiness” or “mood” are also being explored in
the context of informal text resources such as weblogs . Potential
Classiﬁcation and Extraction
applications include monitoring levels of hateful or violent rhetoric,
perhaps in multilingual settings .
In addition to classiﬁcation based on aﬀect and emotion, another
related area of research that addresses non-topic-based categorization
is that of determining the genre of texts .
Since subjective genres, such as “editorial,” are often one of the possible
categories, such work can be viewed as closely related to subjectivity
detection. Indeed, this relation has been observed in work focused on
learning subjective language .
There has also been research that concentrates on classifying documents according to their source or source style, with statistically
detected stylistic variation serving as an important cue. Authorship identiﬁcation is perhaps the most salient example — Mosteller and
Wallace’s classic Bayesian study of the authorship of the Federalist Papers is one well-known instance. Argamon-Engelson et al. 
consider the related problem of identifying not the particular author
of a text, but its publisher (e.g., the New York Times vs. The Daily
News); the work of Kessler et al. on determining a document’s
“brow” (e.g., high-brow vs. “popular,” or low-brow) has similar goals.
Several recent workshops have been dedicated to style analysis in text
 . Determining stylistic characteristics can be useful in multifaceted search .
Another problem that has been considered in intelligence and security settings is the detection of deceptive language .
Converting a piece of text into a feature vector or other representation that makes its most salient and important features available is an
important part of data-driven approaches to text processing. There is
an extensive body of work that addresses feature selection for machine
learning approaches in general, as well as for learning approaches tailored to the speciﬁc problems of classic text categorization and information extraction . A comprehensive discussion of such work
is beyond the scope of this survey. In this section, we focus on ﬁndings
in feature engineering that are speciﬁc to sentiment analysis.
4.2 Features
Term Presence vs. Frequency
It is traditional in information retrieval to represent a piece of text as
a feature vector wherein the entries correspond to individual terms.
One inﬂuential ﬁnding in the sentiment-analysis area is as follows.
Term frequencies have traditionally been important in standard IR,
as the popularity of tf-idf weighting shows; but in contrast, Pang et al.
 obtained better performance using presence rather than frequency.
That is, binary-valued feature vectors in which the entries merely indicate whether a term occurs (value 1) or not (value 0) formed a more
eﬀective basis for review polarity classiﬁcation than did real-valued
feature vectors in which entry values increase with the occurrence frequency of the corresponding term. This ﬁnding may be indicative of an
interesting diﬀerence between typical topic-based text categorization
and polarity classiﬁcation: While a topic is more likely to be emphasized by frequent occurrences of certain keywords, overall sentiment
may not usually be highlighted through repeated use of the same terms.
(We discussed this point previously in Section 3.2 on factors that make
opinion mining diﬃcult.)
On a related note, hapax legomena, or words that appear a single
time in a given corpus, have been found to be high-precision indicators
of subjectivity . Yang et al. look at rare terms that are not
listed in a pre-existing dictionary, on the premise that novel versions of
words, such as “bugfested,” might correlate with emphasis and hence
subjectivity in blogs.
Term-based Features Beyond Term Unigrams
Position information ﬁnds its way into features from time to time. The
position of a token within a textual unit (e.g., in the middle vs. near
the end of a document) can potentially have important eﬀects on how
much that token aﬀects the overall sentiment or subjectivity status
of the enclosing textual unit. Thus, position information is sometimes
encoded into the feature vectors that are employed .
Whether higher-order n-grams are useful features appears to be a
matter of some debate. For example, Pang et al. report that unigrams outperform bigrams when classifying movie reviews by sentiment
Classiﬁcation and Extraction
polarity, but Dave et al. ﬁnd that in some settings, bigrams and
trigrams yield better product-review polarity classiﬁcation.
Riloﬀet al. explore the use of a subsumption hierarchy to
formally deﬁne diﬀerent types of lexical features and the relationships
between them in order to identify useful complex features for opinion
analysis. Airoldi et al. apply a Markov Blanket Classiﬁer to this
problem together with a meta-heuristic search strategy called Tabu
search to arrive at a dependency structure encoding a parsimonious
vocabulary for the positive and negative polarity classes.
The “contrastive distance” between terms — an example of a highcontrast pair of words in terms of the implicit evaluation polarity they
express is “delicious” and “dirty” — was used as an automatically
computed feature by Snyder and Barzilay as part of a ratinginference system.
Parts of Speech
Part-of-speech (POS) information is commonly exploited in sentiment
analysis and opinion mining. One simple reason holds for general textual analysis, not just opinion mining: part-of-speech tagging can be
considered to be a crude form of word sense disambiguation .
Adjectives have been employed as features by a number of
researchers . One of the earliest proposals for the data-driven
prediction of the semantic orientation of words was developed for
adjectives . Subsequent work on subjectivity detection revealed
a high correlation between the presence of adjectives and sentence
subjectivity . This ﬁnding has often been taken as evidence that
(certain) adjectives are good indicators of sentiment, and sometimes
has been used to guide feature selection for sentiment classiﬁcation,
in that a number of approaches focus on the presence or polarity of
adjectives when trying to decide the subjectivity or polarity status of
textual units, especially in the unsupervised setting. Rather than focusing on isolated adjectives, Turney proposed to detect document
sentiment based on selected phrases, where the phrases are chosen via
a number of pre-speciﬁed part-of-speech patterns, most including an
adjective or an adverb.
4.2 Features
The fact that adjectives are good predictors of a sentence being
subjective does not, however, imply that other parts of speech do not
contribute to expressions of opinion or sentiment. In fact, in a study
by Pang et al. on movie-review polarity classiﬁcation, using only
adjectives as features was found to perform much worse than using the
same number of most frequent unigrams. The researchers point out
that nouns (e.g., “gem”) and verbs (e.g., “love”) can be strong indicators for sentiment. Riloﬀet al. speciﬁcally studied the extraction
of subjective nouns (e.g., “concern,” “hope”) via bootstrapping. There
have been several targeted comparisons of the eﬀectiveness of adjectives, verbs, and adverbs, where further subcategorization often plays
a role .
There have also been attempts at incorporating syntactic relations
within feature sets. Such deeper linguistic analysis seems particularly
relevant with short pieces of text. For instance, Kudo and Matsumoto
 report that for two sentence-level classiﬁcation tasks, sentiment
polarity classiﬁcation and modality identiﬁcation (“opinion,” “assertion,” or “description”), a subtree-based boosting algorithm using
dependency-tree-based features outperformed the bag-of-words baseline (although there were no signiﬁcant diﬀerences with respect to using
n-gram-based features). Nonetheless, the use of higher-order n-grams
and dependency or constituent-based features has also been considered for document-level classiﬁcation; Dave et al. on the one hand
and Gamon , Matsumoto et al. , and Ng et al. on the
other hand come to opposite conclusions regarding the eﬀectiveness of
dependency information. Parsing the text can also serve as a basis for
modeling valence shifters such as negation, intensiﬁers, and diminishers
 . Collocations and more complex syntactic patterns have also been
found to be useful for subjectivity detection .
Handling negation can be an important concern in opinion- and
sentiment-related analysis. While the bag-of-words representations
Classiﬁcation and Extraction
of “I like this book” and “I don’t like this book” are considered
to be very similar by most commonly-used similarity measures, the
only diﬀering token, the negation term, forces the two sentences into
opposite classes. There does not really exist a parallel situation in
classic IR where a single negation term can play such an instrumental
role in classiﬁcation (except in cases like “this document is about cars”
vs. “this document is not about cars”).
It is possible to deal with negations indirectly as a second-order
feature of a text segment, that is, where an initial representation, such
as a feature vector, essentially ignores negation, but that representation
is then converted into a diﬀerent representation that is negation-aware.
Alternatively, as was done in previous work, negation can be encoded
directly into the deﬁnitions of the initial features. For example, Das
and Chen propose attaching “NOT” to words occurring close to
negation terms such as “no” or “don’t,” so that in the sentence “I
don’t like deadlines,” the token “like” is converted into the new token
“like-NOT.”
However, not all appearances of explicit negation terms reverse the
polarity of the enclosing sentence. For instance, it is incorrect to attach
“NOT” to “best” in “No wonder this is considered one of the best.”
Na et al. attempt to model negation more accurately. They look
for speciﬁc part-of-speech tag patterns (where these patterns diﬀer for
diﬀerent negation words), and tag the complete phrase as a negation
phrase. For their dataset of electronics reviews, they observe about 3%
improvement in accuracy resulting from their modeling of negations.
Further improvement probably needs deeper (syntactic) analysis of the
sentence .
Another diﬃculty with modeling negation is that negation can
often be expressed in rather subtle ways. Sarcasm and irony can be
quite diﬃcult to detect, but even in the absence of such sophisticated
rhetorical devices, we still see examples such as “[it] avoids all clich´es
and predictability found in Hollywood movies” (internet review by
“Margie24”) — the word “avoid” here is an arguably unexpected
“polarity reverser.” Wilson et al. discuss other complex negation
4.2 Features
Topic-Oriented Features
Interactions between topic and sentiment play an important role in
opinion mining. For example, in a hypothetical article on Wal-mart, the
sentences “Wal-mart reports that proﬁts rose” and “Target reports that
proﬁts rose” could indicate completely diﬀerent types of news (good vs.
bad) regarding the subject of the document, Wal-mart . To some
extent, topic information can be incorporated into features.
Mullen and Collier examine the eﬀectiveness of various features
based on topic (e.g., they take into account whether a phrase follows a
reference to the topic under discussion) under the experimental condition that topic references are manually tagged. Thus, for example, in
a review of a particular work of art or music, references to the item
receive a “THIS WORK” tag.
For the analysis of predictive opinions (e.g., whether a message M
with respect to party P predicts P to win), Kim and Hovy propose
to employ feature generalization. Speciﬁcally, for each sentence in M,
each party name and candidate name is replaced by PARTY (i.e., P)
or OTHER (not P). Patterns such as “PARTY will win,” “go PARTY
again,” and “OTHER will win” are then extracted as n-gram features.
This scheme outperforms using simple n-gram features by about 10% in
accuracy when classifying which party a given message predicts to win.
Topic–sentiment interaction has also been modeled through parse
tree features, especially in opinion extraction tasks. Relationships
between candidate opinion phrases and the given subject in a dependency tree can be useful in such settings .
Part Two: Approaches
The approaches we will now discuss all share the common theme of
mapping a given piece of text, such as a document, paragraph, or
sentence, to a label drawn from a pre-speciﬁed ﬁnite set or to a real
number.4 As discussed in Section 4.1, opinion-oriented classiﬁcation can
range from sentiment-polarity categorization in reviews to determining
4 However, unlike classiﬁcation and regression, ranking does not require such a mapping for
each individual document.
Classiﬁcation and Extraction
the strength of opinions in news articles to identifying perspectives
in political debates to analyzing mood in blogs. Part of what is particularly interesting about these problems is the new challenges and
opportunities that they present to us. In the remainder of this section,
we examine diﬀerent solutions proposed in the literature to these problems, loosely organized around diﬀerent aspects of machine learning
approaches. Although these aspects may seem to be general themes
underlying most machine learning problems, we attempt to highlight
what is unique for sentiment analysis and opinion mining tasks. For
instance, some unsupervised learning approaches follow a sentimentspeciﬁc paradigm for how labels for words and phrases are obtained.
Also, supervised and semi-supervised learning approaches for opinion
mining and sentiment analysis diﬀer from standard approaches to classiﬁcation tasks in part due to the diﬀerent features involved; but we
also see a great variety of attempts at modeling various kinds of relationships between items, classes, or sub-document units. Some of these
relationships are unique to our tasks; some become more imperative to
model due to the subtleties of the problems we address.
The rest of this section is organized as follows. Section 4.3 covers the
impact that the increased availability of labeled data has had, including
the rise of supervised learning. Section 4.4 considers issues surrounding topic and domain dependencies. Section 4.5 describes unsupervised
approaches. We next consider incorporating relationships between various types of entities (Section 4.6). This is followed by a section on
incorporating discourse structure (4.7). Section 4.8 is concerned with
the use of language models. Finally, Section 4.9 investigates certain
issues in extraction that are somewhat particular to it, and thus are
not otherwise discussed in the sections that precede it. One such issue
is the identiﬁcation of features and expressions of opinions in reviews.
Another set of issues arise when opinion-holder identiﬁcation needs to
be applied.
The Impact of Labeled Data
Work up to the early 1990s on sentiment-related tasks, such as determination of point of view and other types of complex recognition
4.3 The Impact of Labeled Data
problems, generally assumed the existence of sub-systems for sometimes rather sophisticated NLP tasks, ranging from parsing to the resolution of pragmatic ambiguities . Given the
state of the art of NLP at the time and, just as importantly, the lack of
suﬃcient amounts of appropriate labeled data, the research described
in these early papers necessarily considered only proposals for systems
or prototype systems without large-scale empirical evaluation; typically, no learning component was involved (an interesting exception is
Wiebe and Bruce , who proposed but did not evaluate the use
of decomposable graphical models). Operational systems were focused
on simpler classiﬁcation tasks, relatively speaking (e.g., categorization
according to aﬀect), and relied instead on relatively shallow analysis
based on manually constructed discriminant-word lexicons ,
since with such a lexicon in hand, one can classify a text unit by considering which indicator terms or phrases from the lexicon appear in
the given text.
The rise of the widespread availablity to researchers of organized
collections of opinionated documents (two examples: ﬁnancial-news
discussion boards and review aggregation sites such as Epinions) and
of other corpora of more general texts (e.g., newswire) and of other
resources (e.g., WordNet) was a major contributor to a large shift in
direction toward data-driven approaches. To begin with, the availability
of the raw texts themselves made it possible to learn opinion-relevant
lexicons in an unsupervised fashion, as is discussed in more detail in
Section 4.5.1, rather than create them manually. But the increase in the
amount of labeled sentiment-relevant data, in particular — where the
labels are derived either through explicit researcher-initiated manual
annotation eﬀorts or by other means (see Section 7.1.1) — was a major
contributing factor to activity in both supervised and unsupervised
learning. In the unsupervised case, described in Section 4.5, it facilitated research by making it possible to evaluate proposed algorithms
in a large-scale fashion. Unsupervised (and supervised) learning also
beneﬁtted from the improvements to sub-component systems for
tagging, parsing, and so on that occurred due to the application of
data-driven techniques in those areas. And, of course, the importance
to supervised learning of having access to labeled data is paramount.
Classiﬁcation and Extraction
One very active line of work can be roughly glossed as the application of standard text-categorization algorithms, surveyed by Sebastiani , to opinion-oriented classiﬁcation problems. For example,
Pang et al. compare Naive Bayes, Support Vector Machines,
and maximum-entropy-based classiﬁcation on the sentiment-polarity
classiﬁcation problem for movie reviews. More extensive comparisons
of the performance of standard machine learning techniques with other
types of features or feature selection schemes have been engaged in
later work ; see Section 4.2 for more detail.
We note that there has been some research that explicitly considers
regression or ordinal-regression formulations of opinion-mining problems : example questions include, “how positive is
this text?” and “how strongly held is this opinion?”
Another role that labeled data can play is in lexicon induction,
although, as detailed in Section 4.5.1, the use of the unsupervised
paradigm is more common. Morinaga et al. and Bethard et al.
 create an opinion-indicator lexicon by looking for terms that tend
to be associated more highly with subjective-genre newswire, such as
editorials, than with objective-genre newswire. Das and Chen 
start with a manually created lexicon speciﬁc to the ﬁnance domain
(example terms: “bull,” “bear”), but then assign discrimination weights
to the items in the lexicon based on their cooccurrence with positively
labeled vs. negatively labeled documents.
Other topics related to supervised learning are discussed in some of
the more speciﬁc sections that follow.
Domain Adaptation and Topic-Sentiment Interaction
Domain Considerations
The accuracy of sentiment classiﬁcation can be inﬂuenced by the
domain of the items to which it is applied .
One reason is that the same phrase can indicate diﬀerent sentiment
in diﬀerent domains: recall the Bob Bland example mentioned earlier, where “go read the book” most likely indicates positive sentiment for book reviews, but negative sentiment for movie reviews; or
consider Turney’s observation that “unpredictable” is a positive
4.4 Domain Adaptation and Topic-Sentiment Interaction
description for a movie plot but a negative description for a car’s steering abilities. Diﬀerence in vocabularies across diﬀerent domains also
adds to the diﬃculty when applying classiﬁers trained on labeled data
in one domain to test data in another.
Several studies show concrete performance diﬀerences from domain
to domain. In an experiment auxiliary to their main work, Dave et al.
 apply a classiﬁer trained on a pre-assembled dataset of reviews of
a certain type to product reviews of a diﬀerent type. But they do not
investigate the eﬀect of training-test mis-match in detail. Engstr¨om 
studies how the accuracy of sentiment classiﬁcation can be inﬂuenced
by topic. Read ﬁnds standard machine learning techniques for
opinion analysis to be both domain-dependent (with domains ranging
from movie reviews to newswire articles) and temporally dependent
(based on datasets spanning diﬀerent ranges of time periods but written
at least one year apart). Owsley et al. also show the importance
of building a domain-speciﬁc classiﬁer.
Aue and Gamon explore diﬀerent approaches to customizing a
sentiment classiﬁcation system to a new target domain in the absence of
large amounts of labeled data. The diﬀerent types of data they consider
range from lengthy movie reviews to short, phrase-level user feedback
from web surveys. Due to signiﬁcant diﬀerences in these domains along
several dimensions, simply applying the classiﬁer learned on data from
one domain barely outperforms the baseline for another domain. In fact,
with 100 or 200 labeled items in the target domain, an EM algorithm
that utilizes in-domain unlabeled data and ignores out-of-domain data
altogether outperforms the method based exclusively on (both in- and
out-of-domain) labeled data.
Yang et al. take the following simple approach to domain
transfer: they ﬁnd features that are good subjectivity indicators in both
of two diﬀerent domains (in their case, movie reviews versus product
reviews), and consider these features to be good domain-independent
Blitzer et al. explicitly address the domain transfer problem for sentiment polarity classiﬁcation by extending the structural
correspondence learning algorithm (SCL) , achieving an average of
46% improvement over a supervised baseline for sentiment polarity
Classiﬁcation and Extraction
classiﬁcation of 5 diﬀerent types of product reviews mined from Amazon.com. The success of SCL depends on the choice of pivot features
in both domains, based on which the algorithm learns a projection
matrix that maps features in the target domain into the feature space
of the source domain. Unlike previous work that applied SCL for tagging, where frequent words in both domains happened to be good
predictors for the target labels (part-of-speech tags), and were therefore good candidates for pivots, here the pivots are chosen from those
with highest mutual information with the source label. The projection is able to capture correspondences (in terms of expressed sentiment polarity) between “predictable” for book reviews and “poorly
designed” for kitchen appliance reviews. Furthermore, they also show
that a measure of domain similarity can correlate well with the ease
of adaptation from one domain to another, thereby enabling better
scheduling of annotation eﬀorts.
Cross-lingual adaptation.
Much of the literature on sentiment analysis has focused on text written in English. As a result, most of the
resources developed, such as lexica with sentiment labels, are in English.
Adapting such resources to other languages is related to domain adaptation: the former aims at adapting from the source language to the
target language in order to utilize existing resources in the source language; whereas the latter seeks to adapt from one domain to another
in order to utilize the labeled data available in the source domain.
Not surprisingly, we observe parallel techniques: instead of projecting
unseen tokens from the new domain into the old one via co-occurrence
information in the corpus , expressions in the new language can
be aligned with expressions in the language with existing resources.
For instance, one can determine cross-lingual projections through bilingual dictionaries , or parallel corpora . Alternatively,
one can simply apply machine translation as a sentiment-analysis preprocessing step .
Topic (and sub-topic or feature) Considerations
Even when one is handling documents in the same domain, there is
still an important and related source of variation: document topic. It is
4.4 Domain Adaptation and Topic-Sentiment Interaction
true that sometimes the topic is pre-determined, such as in the case of
free-form responses to survey questions. However, in many sentiment
analysis applications, topic is another important consideration; for
instance, one may be searching the blogosphere just for opinionated
comments about Cornell University.
One approach to integrating sentiment and topic when one is
looking for opinionated documents on a particular user-speciﬁed topic
is to simply ﬁrst perform one analysis pass, say for topic, and then analyze the results with respect to sentiment . (See Sebastiani 
for a survey of machine learning approaches to topic-based text categorization.) Such a two-pass approach was taken by a number of systems
at the TREC Blog track in 2006, according to Ounis et al. , and
others . Alternatively, one may jointly model topic and sentiment
simultaneously , or treat one as a prior for the other .
But even in the case where one is working with documents known
to be on-topic, not all the sentences within these documents need to be
on-topic. Hurst and Nigam propose a two-pass process similar
to that mentioned above, where each sentence in the document is ﬁrst
labeled as on-topic or oﬀ-topic, and sentiment analysis is conducted
only for those that are found to be on-topic. Their work relies on a
collocation assumption that if a sentence is found to be topical and to
exhibit a sentiment polarity, then the polarity is expressed with respect
to the topic in question. This assumption is also used by Nasukawa and
Yi and Gamon .
A related issue is that it is also possible for a document to contain
multiple topics. For instance, a review can be a comparison of two products. Or, even when a single item is discussed in a document, one can
consider features or aspects of the product to represent multiple (sub-)
topics. If all but the main topic can be disregarded, then one possibility is as follows: simply consider the overall sentiment detected within
the document — regardless of the fact that it may be formed from
a mixture of opinions on diﬀerent topics — to be associated with the
primary topic, leaving the sentiment toward other topics undetermined
(indeed, these other topics may never be identiﬁed). But it is more
common to try to identify the topics and then determine the opinions
regarding each of these topics separately. In some work, the important
Classiﬁcation and Extraction
topics are pre-deﬁned, making this task easier . In other work
in extraction, this is not the case; the problem of the identiﬁcation of
product features is addressed in Section 4.9, and Section 4.6.3 discusses
techniques that incorporate relationships between diﬀerent features.
Unsupervised Approaches
Unsupervised Lexicon Induction
Quite a number of unsupervised learning approaches take the tack of
ﬁrst creating a sentiment lexicon in an unsupervised manner, and then
determining the degree of positivity (or subjectivity) of a text unit via
some function based on the positive and negative (or simply subjective)
indicators, as determined by the lexicon, within it. Early examples of
such an approach include Hatzivassiloglou and Wiebe , Turney
 , and Yu and Hatzivassiloglou . Some interesting variants of
this general technique are to use the polarity of the previous sentence as
a tie-breaker when the scoring function does not indicate a deﬁnitive
classiﬁcation of a given sentence , or to incorporate information
drawn from some labeled data as well .
A crucial component to applying this type of technique is, of course,
the creation of the lexicon via the unsupervised labeling of words or
phrases with their sentiment polarity (also referred to as semantic orientation in the literature) or subjectivity status .
In early work, Hatzivassiloglou and McKeown present an
approach based on linguistic heuristics.5 Their technique is built on
the fact that in the case of polarity classiﬁcation, the two classes of
interest represent opposites, and we can utilize “opposition constraints”
to help make labeling decisions. Speciﬁcally, constraints between pairs
of adjectives are induced from a large corpus by looking at whether
the two words are linked by conjunctions such as “but” (evidence for
opposing orientations: “elegant but over-priced”) or “and” (evidence
for the same orientation: “clever and informative”). The task is then
cast as a clustering or binary-partitioning problem where the inferred
constraints are to be obeyed.
5 For the purposes of the current discussion, we ignore the supervised aspects of their work.
4.5 Unsupervised Approaches
Once the clustering has been completed, the labels of “positive
orientation” and “negative orientation” need to be assigned; rather
than use external information to make this decision, Hatzivassiloglou
and McKeown simply give the “positive orientation” label to the
class whose members have the highest average frequency. But in other
work, seed words for which the polarity is already known are assumed to
be supplied, in which case labels can be determined by propagating the
labels of the seed words to terms that co-occur with them in general text
or in dictionary glosses, or to synonyms, words that co-occur with them
in other WordNet-deﬁned relations, or other related words (and, along
the same lines, opposite labels can be given based on similar information) . The joint use of
mutual information and co-occurrence in a general corpus with a small
set of seed words, a technique employed by a number of researchers, was
suggested by Turney ; his idea was to essentially compare whether
a phrase has a greater tendency to co-occur within certain context windows with the word “poor” or with the word “excellent,” taking care to
account for the frequencies with which “poor” and “excellent” occur,
where the data on which such computations are to be made come from
the results of particular types of Web search-engine queries.
Much of the work cited above focuses on identifying the prior polarity of terms or phrases, to use the terminology of Wilson et al. , or
what we might by extension call terms’ and phrases’ prior subjectivity
status, meaning the semantic orientation that these items might be said
to generally bear when taken out of context. Such prior information is
meant, of course, to serve toward further identifying contextual polarity
or subjectivity .
Lexicons for generation.
It is worth noting that Higashinaka et al.
 focus on a lexicon-induction task that facilitates natural language
generation. They consider the problem of learning a dictionary that
maps semantic representations to verbalizations, where the data comes
from reviews. Although reviews are not explicitly marked up with
respect to their semantics, they do contain explicit rating and aspect
indicators. For example, from such data, they learn that one way to
express the concept “atmosphere rating:5” is “nice and comfortable.”
Classiﬁcation and Extraction
Other Unsupervised Approaches
Bootstrapping is another approach. The idea is to use the output
of an available initial classiﬁer to create labeled data, to which a
supervised learning algorithm may be applied. Riloﬀand Wiebe 
use this method in conjunction with an initial high-precision classiﬁer
to learn extraction patterns for subjective expressions. (An interesting,
if simple, pattern discovered: the noun “fact,” as in “The fact is . . . ,”
exhibits high correlation with subjectivity.) Kaji and Kitsuregawa 
use a similar method to automatically construct a corpus of HTML
documents with polarity labels. Similar work involving self-training is
described in Wiebe and Riloﬀ and Riloﬀet al. .
Pang and Lee experiment with a diﬀerent type of unsupervised approach. The problem they consider is to rank search results
for review-seeking queries so that documents that contain evaluative
text are placed ahead of those that do not. They propose a simple
“blank slate” method based on the rarity of words within the search
results that are retrieved (as opposed to within a training corpus). The
intuition is that words that appear frequently within the set of documents returned for a narrow topic (the search set) are more likely to
describe objective information, since objective information should tend
to be repeated within the search set; in contrast, it would seem that
people’s opinions and how they express them may diﬀer. Counterintuitively, though, Pang and Lee ﬁnd that when the vocabulary to be
considered is restricted to the most frequent words in the search set (as
a noise-reduction measure), the subjective documents tend to be those
that contain a higher percentage of words that are less rare, perhaps
due to the fact that most reviews cover the main features or aspects of
the object being reviewed. (This echoes our previous observation that
understanding the objective information in a document can be critical for understanding the opinions and sentiment it expresses.) The
performance of this simple method is on par with that of a method
based on a state-of-the-art subjectivity detection system, Opinion-
Finder .
A comparison of supervised and unsupervised methods can be found
in Chaovalit and Zhou .
4.6 Classiﬁcation Based on Relationship Information
Classiﬁcation Based on Relationship Information
Relationships Between Sentences and Between
One interesting characteristic of document-level sentiment analysis is
the fact that a document can consist of sub-document units (paragraphs or sentences) with diﬀerent, sometimes opposing labels, where
the overall sentiment label for the document is a function of the set
or sequence of labels at the sub-document level. As an alternative to
treating a document as a bag of features, then, there have been various attempts to model the structure of a document via analysis of
sub-document units, and to explicitly utilize the relationships between
these units, in order to achieve a more accurate global labeling. Modeling the relationships between these sub-document units may lead to
better sub-document labeling as well.
An opinionated piece of text can often consist of evaluative portions
(those that contribute to the overall sentiment of the document, e.g.,
“this is a great movie”) and non-evaluative portions (e.g., “the Powerpuﬀgirls learned that with great power comes great responsibility”).
The overlap between the vocabulary used for evaluative portions and
non-evaluative portions makes it particularly important to model the
context in which these text segments occur. Pang and Lee propose a two-step procedure for polarity classiﬁcation for movie reviews,
wherein they ﬁrst detect the objective portions of a document (e.g.,
plot descriptions) and then apply polarity classiﬁcation to the remainder of the document after the removal of these presumably uninformative portions. Importantly, instead of making the subjective–objective
decision for each sentence individually, they postulate that there might
be a certain degree of continuity in subjectivity labels (an author usually does not switch too frequently between being subjective and being
objective), and incorporate this intuition by assigning preferences for
pairs of nearby sentences to receive similar labels. All the sentences in
the document are then labeled as being either subjective or objective
through a collective classiﬁcation process, where this process employs
a reformulation of the task as one of ﬁnding a minimum s-t cut in the
appropriate graph . Two key properties of this approach are (1) it
Classiﬁcation and Extraction
aﬀords the ﬁnding of an exact solution to the underlying optimization
problem via an algorithm that is eﬃcient both in theory and in practice, and (2) it makes it easy to integrate a wide variety of knowledge
sources both about individual preferences that items may have for one
or the other class and about the pair-wise preferences that items may
have for being placed in the same class regardless of which particular
class that is. Follow-up work has used alternate techniques to determine
edge weights within a minimum-cut framework for various types of
sentiment-related binary classiﬁcation problems at the document level
 . (The more general rating-inference problem can also, in
special cases, be solved using a minimum-cut formulation .) Others
have considered more sophisticated graph-based techniques .
Relationships Between Discourse Participants
An interesting setting for opinion mining is when the texts to be analyzed form part of a running discussion, such as in the case of individual
turns in political debates, posts to online discussion boards, and comments on blog posts. One fascinating aspect of this kind of setting is the
rich information source that references between such texts represent,
since such information can be exploited for better collective labeling of
the set of documents. Utilizing such relationships can be particularly
helpful because many documents in the settings we have described
can be quite terse (or complicated), and hence diﬃcult to classify on
their own, but we can easily categorize a diﬃcult document if we ﬁnd
within it indications of agreement with a clearly, say, positive text.
Based on manual examination of 100 responses in newsgroups
devoted to three distinct controversial topics (abortion, gun control and
immigration), Agrawal et al. observe that the relationship between
two individuals in the “responded-to” network is more likely to be
antagonistic — overall, 74% of the responses examined were found to
be antagonistic, whereas only 7% were found to be reinforcing. By then
assuming that “respond-to” links imply disagreement, they eﬀectively
classify users into opposite camps via graph partitioning, outperforming methods that depend solely on the textual information within a
particular document.
4.6 Classiﬁcation Based on Relationship Information
Similarly, Mullen and Malouf examine “quoting” behavior
among users of the politics.com discussion site — a user can refer to
another post by quoting part of it or by addressing the other user by
name or user ID — who have been classiﬁed as either liberal or conservative. The researchers ﬁnd that a signiﬁcant fraction of the posts
of interest to them contain quoted material, and that, in contrast to
inter-blog linking patterns discussed in Adamic and Glance , where
liberal and conservative blog sites were found to tend to link to sites of
similar political orientations, and in accordance with the Agrawal et al.
 ﬁndings cited above, politics.com posters tend to quote users at the
opposite end of the political spectrum. To perform the ﬁnal politicalorientation classiﬁcation, users are clustered so that those who tend
to quote the same entities are placed in the same cluster. (Efron 
similarly uses co-citation analysis for the same problem.)
Rather than assume that quoting always indicates agreement or
disagreement regardless of the context, Thomas et al. build an
agreement detector for the task of analyzing transcripts of congressional
ﬂoor-debates, where the classiﬁer categorizes certain explicit references
to other speakers as representing agreement (e.g., “I heartily support
Mr Smith’s views!”) or disagreement. They then encode evidence of
a high likelihood of agreement between two speakers as a relationship
constraint between the utterances made by the speakers, and collectively classify the individual speeches as to whether they support or
oppose the legislation under discussion, using a minimum-cut formulation of the classiﬁcation problem, as described above. Follow-up work
attempts to make more reﬁned use of disagreement information .
Relationships Between Product Features
Popescu and Etzioni treat the labeling of opinion words regarding product features as a collective labeling process. They propose
an iterative algorithm wherein the polarity assignments for individual
words are collectively adjusted through a relaxation-labeling process.
Starting from “global” word labels computed over a large text collection that reﬂect the sentiment orientation for each particular word in
general settings, Popescu and Etzioni gradually re-deﬁne the label from
Classiﬁcation and Extraction
one that is generic to one that is speciﬁc to a review corpus to one that
is speciﬁc to a given product feature to, ﬁnally, one that is speciﬁc to the
particular context in which the word occurs. They make sure to respect
sentence-level local constraints that opinions connected by connectives
such as “but” or “and” should receive opposite or the same polarities.
The idea of utilizing discourse information to help with the
inference of relationships between product attributes can also be
found in the work of Snyder and Barzilay , who utilize agreement
information in a task where one must predict ratings for multiple
aspects of the same item (e.g., food and ambiance for a restaurant).
Their approach is to construct a linear classiﬁer to predict whether
all aspects of a product are given the same rating, and combine this
prediction with that of individual-aspect classiﬁers so as to minimize
a certain loss function (which they term the “grief”). Interestingly,
Snyder and Barzilay give an example where a collection of independent aspect-rating predictors cannot assign a correct set of aspect
ratings, but augmentation with their agreement classiﬁcation allows
perfect rating assignment; in their speciﬁc example, the agreement
classiﬁer is able to use the presence of the phrase “but not” to predict
a contrasting rating between two aspects. An important observation
that Snyder and Barzilay make about their formulation is that
having the piece of information that all aspect ratings agree cuts down
the space of possible rating tuples to a far greater degree than having
the information that not all the aspect ratings are the same.
Note that the considerations discussed here relate to the topicspeciﬁc nature of opinions that we discussed in the context of domain
adaptation in Section 4.4.
Relationships Between Classes
Regression formulations (where we include ordinal regression under this
umbrella term) are quite well-suited to the rating reference problem
of predicting the degree of positivity in opinionated documents such
as product reviews, and to similar problems such as determining the
strength with which an opinion is held. In a sense, regression implicitly models similarity relationships between classes that correspond to
4.6 Classiﬁcation Based on Relationship Information
points on a scale, such as the number of “stars” given by a reviewer. In
contrast, standard multi-class categorization focuses on capturing the
distinct features present in each class, and ignores the fact that “5 stars”
is much more like “4 stars” than “2 stars.” On a movie review dataset,
Pang and Lee observe that a one-vs-all multi-class categorization scheme can outperform regression for a three-class classiﬁcation
problem (positive, neutral, and negative), perhaps due to each class
exhibiting a suﬃciently distinct vocabulary, but for more ﬁne-grained
classiﬁcation, regression emerges as the better of the two.
Furthermore, while regression-based models implicitly encode the
intuition that similar items should receive similar labels, Pang and Lee
 formulate rating inference as a metric labeling problem- , so
that a natural notion of distance between classes (“2 stars” and “3
stars” are more similar to each other than “1 star” and “4 stars” are)
is captured explicitly. More speciﬁcally, an optimal labeling is computed
that balances the output of a classiﬁer that considers items in isolation
with the importance of assigning similar labels to similar items.
Koppel and Schler consider a similar version of this problem,
but where one of the classes, corresponding to “objective,” does not lie
on the positive-to-negative continuum. Goldberg and Zhu present
a graph-based algorithm that addresses the rating inference problem
in the semi-supervised learning setting, where a closed-form solution
to the underlying optimization problem is found through computation
on a matrix induced by a graph representing inter-document similarity
relationships, and the loss function encodes the desire for similar items
to receive similar labels. Mao and Lebanon (Mao and Lebanon
 is a shorter version) propose to use isotonic conditional random
ﬁelds to capture the ordinal labels of local (sentence-level) sentiments.
Given words that are strongly associated with positive and negative
sentiment, they formulate constraints on the parameters to reﬂect the
intuition that adding a positive (negative) word should aﬀect the local
sentiment label positively (negatively).
Wilson et al. treat intensity classiﬁcation (e.g., classifying an
opinion according to its strength) as an ordinal regression task.
McDonald et al. leverage relationships between labels assigned
at diﬀerent classiﬁcation stages, such as the word level or sentence level,
Classiﬁcation and Extraction
ﬁnding that a “ﬁne-to-coarse” categorization procedure is an eﬀective
Incorporating Discourse Structure
Compared to the case for traditional topic-based information access
tasks, discourse structure (e.g., twists and turns in documents) tends
to have more eﬀect on overall sentiment labels. For instance, Pang et al.
 observe that some form of discourse structure modeling can help
to extract the correct label in the following example
I hate the Spice Girls. . . . [3 things the author hates
about them]. . . Why I saw this movie is a really, really,
really long story, but I did, and one would think I’d
despise every minute of it. But. . . Okay, I’m really
ashamed of it, but I enjoyed it. I mean, I admit it’s
a really awful movie, . . . [they] act wacky as hell . . . the
ninth ﬂoor of hell . . . a cheap [beep] movie . . . The plot
is such a mess that it’s terrible. But I loved it.
In spite of the predominant number of negative sentences, the overall
sentiment toward the movie under discussion is positive, largely due to
the order in which these sentences are presented. Needless to say, such
information is lost in a bag-of-words representation.
Early work attempts to partially address this problem via incorporating location information in the feature set . Speciﬁcally, the
position at which a token appears can be appended to the token itself to
form position-tagged features, so that the same unigram appearing in,
say, the ﬁrst quarter and the last quarter of the document are treated
as two diﬀerent features; but the performance of this simple scheme
does not diﬀer greatly from that which results from using unigrams
On a related note, it has been observed that position matters in
the context of summarizing sentiment in a document. In particular, in
contrast to topic-based text summarization, where the beginnings of
articles usually serve as strong baselines in terms of summarizing the
objective information in them, the last n sentences of a review have
4.8 Language Models
been shown to serve as a much better summary of the overall sentiment
of the document than the ﬁrst n sentences, and to be almost as good
as using the n most (automatically-computed) subjective sentences, in
terms of how accurately they represent the overall sentiment of the
document .
Theories of lexical cohesion motivate the representation used by
Devitt and Ahmad for sentiment polarity classiﬁcation of ﬁnancial news.
Another way of capturing discourse structure information in documents is to model the global sentiment of a document as a trajectory of local sentiments. For example, Mao and Lebanon propose
using sentiment ﬂow as a sequential model to represent an opinionated
document. More speciﬁcally, each sentence in the document receives a
local sentiment score from an isotonic-conditional-random-ﬁeld-based
sentence level predictor. The sentiment ﬂow is deﬁned as a function
h : [0,1) →O (the ordinal set), where the interval [(t −1)/n,t/n) is
mapped to the label of the tth sentence in a document with n sentences. The ﬂow is then smoothed out through convolution with a
smoothing kernel. Finally, the distances between two ﬂows (e.g., Lp distance between the two smoothed, continuous functions) should reﬂect,
to some degree, the distances between global sentiments. On a small
dataset, Mao and Lebanon observe that the sentiment ﬂow representation (especially when objective sentences are excluded) outperforms a
plain bag-of-words representation in predicting global sentiment with
a nearest neighbor classiﬁer.
Language Models
The rise of the use of language models in information retrieval has
been an interesting recent development . They have
been applied to various opinion-mining and sentiment-analysis tasks,
and in fact the subjectivity-extraction work of Pang and Lee is a
demo application for the heavily language-modeling-oriented LingPipe
6 
Classiﬁcation and Extraction
One characteristic of language modeling approaches that diﬀerentiates them somewhat from other classiﬁcation-oriented data-driven
techniques we have discussed so far is that language models are often
constructed using labeled data, but, given that they are mechanisms
for assigning probabilities to text rather than labels drawn from a ﬁnite
set, they cannot, strictly speaking, be deﬁned as either supervised or
unsupervised classiﬁers. On the other hand, there are various ways to
convert their output to labels when necessary.
An example of work in the language-modeling vein is that of Eguchi
and Lavrenko , who rank sentences by both sentiment relevancy and
topic relevancy, based on previous work on relevance language models
 . They propose a generative model that jointly models sentiment
words, topic words, and sentiment polarity in a sentence as a triple.
Lin and Hauptmann consider the problem of examining whether
two collections of texts represent diﬀerent perspectives. In their study,
employing Reuters data, two examples of diﬀerent perspectives are the
Palestinian viewpoint vs. the Israeli viewpoint in written text and Bush
vs. Kerry in presidential debates. They base their notion of diﬀerence in
perspective upon the Kullback–Leibler (KL) divergence between posterior distributions induced from document collection pairs, and discover
that the KL divergence between diﬀerent aspects is an order of magnitude smaller than that between diﬀerent topics. This perhaps provides
yet another reason that opinion-oriented classiﬁcation has been found
to be more diﬃcult than topic-based classiﬁcation.
Research employing probabilistic latent semantic analysis (PLSA)
 or latent Dirichlet allocation (LDA) can also be cast as
language-modeling work . The basic idea is to infer language models that correspond to unobserved “factors” in the data, with
the hope that the factors that are learned represent topics or sentiment
categories.
Special Considerations for Extraction
Opinion-oriented extraction.
Many applications, such as summarization or question answering, require working with pieces of information
that need to be pulled from one or more textual units. For example,
4.9 Special Considerations for Extraction
a multi-perspective question–answering (MPQA) system might need to
respond to opinion-oriented questions such as “Was the most recent
presidential election in Zimbabwe regarded as a fair election?” ; the
answer may be encoded in a particular sentence of a particular document, or may need to be stitched together from pieces of evidence
found in multiple documents. Information extraction (IE ) is precisely
the ﬁeld of natural language processing devoted to this type of task
 . Hence, it is not surprising that the application of informationextraction techniques to opinion mining and sentiment analysis has
been proposed . In this survey, we use the term opinion-oriented
information extraction (opinion-oriented IE ) to refer to information
extraction problems particular to sentiment analysis and opinion mining. (We sometimes shorten the phrase to opinion extraction, which
should not be construed narrowly as focusing on the extraction of opinion expressions; for instance, determining product features is included
under the umbrella of this term.)
Past research in this area has been dominated by work on two types
• Opinion-oriented information extraction from reviews has,
as noted above, attracted a great deal of interest in recent
years. In fact, the term “opinion mining,” when construed
in its narrow sense, has often been used to describe work
in this context. Reviews, while typically (but not always)
devoted to a single item, such as a product, service, or event,
generally comment on multiple aspects, facets, or features
of that item, and all such commentary may be important.
Extracting and analyzing opinions associated with each individual aspect can help provide more informative summarizations or enable more ﬁne-grained opinion-oriented retrieval.
• Other work has focused on newswire. Unlike reviews, a news
article is relatively likely to contain descriptions of opinions
that do not belong to the article’s author; an example is a
quotation from a political ﬁgure. This property of journalistic
text makes the identiﬁcation of opinion holders (also known
as opinion sources) and the correct association of opinion
Classiﬁcation and Extraction
holders with opinions important tasks, whereas for reviews,
all expressed opinions are typically those of the author,
so opinion-holder identiﬁcation is a less salient problem.
Thus, when newswire articles are the focus, the emphasis
has tended to be on identifying expressions of opinions, the
agent expressing each opinion, and/or the type and strength
of each opinion. Early work in this direction ﬁrst carefully
developed and evaluated a low-level opinion annotation
scheme , which facilitated the study of
sub-tasks such as identifying opinion holders and analyzing
opinions at the phrase level .
It is important to understand the similarities and diﬀerences
between opinion-oriented IE and standard fact-oriented IE. They share
some sub-tasks in common, such as entity recognition; for example, as
mentioned above, determination of opinion holders is an active line
of research . What truly sets the problem apart from
standard or classic IE is the speciﬁc types of entities and relations
that are considered important. For instance, although identiﬁcation of
product features is in some sense a standard entity recognition problem, an opinion extraction system would be mostly interested in features for which associated opinions exist; similarly, an opinion holder
is not just any named entity in a news article, but one that expresses
opinions. Examples of the types of relations particularly pertinent to
opinion mining are those centered around comparisons — consider,
for example, the relations encoded by such sentences as “The new
model is more expensive than the old one” or “I prefer product A over
product B” ( , longer version of the latter available as Jindal
and Liu ) — or between agents and reported beliefs, as described
in Section 4.9.2. Note that the relations of interest can form a complex
hierarchical structure, as in the case where an opinion is attributed to
one party by another, so that it is unclear whether the ﬁrst party truly
holds the opinion in question .
It is also important to understand which aspects of opinion-oriented
extraction are mentioned in this section as opposed to the previous sections. As discussed earlier, many sub-problems of opinion extraction are
4.9 Special Considerations for Extraction
in fact classiﬁcation problems for relatively small textual units. Examples include both determining whether or not a text span is subjective
and classifying a given text span already determined to be subjective
by the strength of the opinion expressed. Thus, many key techniques
involved in building an opinion extraction system are already discussed
in the previous sections. In this section, we instead focus on the “missing pieces,” describing approaches to problems that are somewhat special to extraction tasks in sentiment analysis. While these sub-tasks
can be (and often are) cast as classiﬁcation problems, they do not have
natural counterparts outside of the extraction context. Speciﬁcally, Section 4.9.1 is devoted to the identiﬁcation of features and expressions of
opinions in reviews. Section 4.9.2 considers techniques that have been
employed when opinion-holder identiﬁcation is an issue.
Finally, we make the following organizational note. One may often
want to present the output of opinion extraction in summarized form;
conversely, some forms of sentiment summarization rely on the output
of opinion extraction. Opinion-oriented summarization is discussed in
Section 5.
Identifying Product Features and Opinions
in Reviews
In the context of review mining , two
important extraction-related sub-tasks are
(1) The identiﬁcation of product features, and
(2) the extraction of opinions associated with these features.
While the key features or aspects are known in some cases, many
systems start from problem (1).
As noted above, identiﬁcation of product features is in some sense a
standard information extraction task with little to distinguish it from
other non-sentiment-related problems. After all, the notion of the features that a given product has seems fairly objective. However, Hu and
Liu show that one can beneﬁt from light sentiment analysis even
for this sub-task, as described shortly.
Classiﬁcation and Extraction
Existing work on identifying product features discussed in reviews
(task (1)) often relies on the simple linguistic heuristic that (explicit)
features are usually expressed as nouns or noun phrases. This narrows
down the candidate words or phrases to be considered, but obviously
not all nouns or noun phrases are product features. Yi et al. consider three increasingly strict heuristics to select from noun phrases
based on part-of-speech-tag patterns. Hu and Liu follow the intuition that frequent nouns or noun phrases are likely to be features. They
identify frequent features through association mining, and then apply
heuristic-guided pruning aimed at removing (a) multi-word candidates
in which the words do not appear together in a certain order, and (b)
single-word candidates for which subsuming super-strings have been
collected (the idea is to concentrate on more speciﬁc concepts, so that,
for example, “life” is discarded in favor of “battery life”). These techniques by themselves outperform a general-purpose term-extraction
and -indexing system known as FASTR . Furthermore — and here
is the observation that is relevant to sentiment — the F-measure can be
further improved (although precision drops slightly) via the following
expansion procedure: adjectives appearing in the same sentence as frequent features are assumed to be opinion words, and nouns and noun
phrases co-occurring with these opinion words in other sentences are
taken to be infrequent features.
In contrast, Popescu and Etzioni consider product features to
be concepts forming certain relationships with the product (for example, for a scanner, its size is one of its properties, whereas its cover is
one of its parts) and seek to identify the features connected with the
product name through corresponding meronymy discriminators. Note
that this approach, which does not involve sentiment analysis per se
but simply focuses more on the task of identifying diﬀerent types of
features, achieved better performance than that yielded by the techniques of Hu and Liu .
There has also been work that focuses on extracting attributevalue pairs from textual product descriptions, but not necessarily in
the context of opinion mining. Of work in this vein, Ghani et al.
 directly compare against the method proposed by Hu and
Liu .
4.9 Special Considerations for Extraction
To identify expressions of opinions associated with features (task
(2)), a simple heuristic is to simply extract adjectives that appear in
the same sentence as the features . Deeper analyses can make use of
parse information and manually or semi-automatically developed rules
or sentiment-relevant lexicons .
Problems Involving Opinion Holders
In the context of analysis of newswire and related genres, we need
to identify text spans corresponding both to opinion holders and to
expressions of the opinions held by them.
As is true with other segmentation tasks, identifying opinion holders
can be viewed as a sequence labeling problem. Choi et al. experiment with an approach that combines Conditional Random Fields
(CRFs) and extraction patterns. A CRF model is trained on a
certain collection of lexical, syntactic, and semantic features. In particular, extraction patterns are learned to provide semantic tagging as
part of the semantic features. (CRFs have also been used to detect
opinion expressions .)
Alternatively, given that the status of an opinion holder depends
by deﬁnition on the expression of an opinion, the identiﬁcation of
opinion holders can beneﬁt from, or perhaps even require, accounting for opinion expressions either simultaneously or as a pre-processing
One example of simultaneous processing is the work of Bethard
et al. , who speciﬁcally address the task of identifying both opinions and opinion sources. Their approach is based on semantic parsing
where semantic constituents of sentences (e.g., “agent” or “proposition”) are marked. By utilizing opinion words automatically learned
by a bootstrapping approach, they further reﬁne the semantic roles to
identify propositional opinions, i.e., opinions that generally function as
the sentential complement of a predicate. This enables them to concentrate on verbs and extract verb-speciﬁc information from semantic
frames such as are deﬁned in FrameNet and PropBank .
As another example of the simultaneous approach, Choi et al. 
employ an integer linear programming approach to handle the joint
Classiﬁcation and Extraction
extraction of entities and relations, drawing on the work of Roth and
Yih on using global inference based on constraints.
As an alternative to the simultaneous approach, a system can start
by identifying opinion expressions, and then proceed to the analysis
of the opinions, including the identiﬁcation of opinion holders. Indeed,
Kim and Hovy deﬁne the problem of opinion holder identiﬁcation
as identifying opinion sources given an opinion expression in a sentence.
In particular, structural features from a syntactic parse tree are selected
to model the long-distance, structural relation between a holder and
an expression. Kim and Hovy show that incorporating the patterns of
paths between holder and expression outperforms a simple combination
of local features (e.g., the type of the holder node) and other nonstructural features (e.g., the distance between the candidate holder
node and the expression node).
One ﬁnal remark is that the task of determining which mentions
of opinion holders are co-referent (source coreference resolution) diﬀers
in practice in interesting ways from typical noun phrase coreference
resolution, due in part to the way in which opinion-oriented datasets
may be annotated .
Summarization
So far, we have talked about analyzing and extracting opinion information from individual documents. The focus of this section is on
aggregating and representing sentiment information drawn from an
individual document or from a collection of documents. For example, a
user might desire an at-a-glance presentation of the main points made
in a single review; creating such single-document sentiment summaries
is described in Section 5.1. Another application considered within this
paradigm is the automatic determination of market sentiment, or the
majority “leaning” of an entire body of investors, from the individual
remarks of those investors ; this is a type of multi-document
opinion-oriented summarization, described in Section 5.2.
Single-Document Opinion-Oriented Summarization
There is clearly a tight connection between extraction of topic-based
information from a single document and topic-based summarization of that document, since the information that is pulled out can serve
as a summary; see Radev et al. [247, Section 2.1] for a brief review.
Summarization
Obviously, this connection between extraction and summarization
holds in the case of sentiment-based summarization, as well.
One way in which this connection is made manifest in singledocument opinion-oriented summarization is as follows: there are
approaches that create textual sentiment summaries based on extraction of sentences or similar text units. For example, Beineke et al.
 attempt to select a single passage1 that reﬂects the opinion of
the document’s author(s), mirroring the practice of ﬁlm advertisements that present “snippets” from reviews of the movie. Training and test data is acquired from the website Rotten Tomatoes
( which provides a roughly sentencelength snippet for each review. However, Beineke et al. note
that low accuracy can result even for high-quality extraction methods because the Rotten Tomatoes data includes only a single snippet
per review, whereas several sentences might be perfectly viable alternatives. In terms of creating longer summaries, Mao and Lebanon 
suggest that by tracking the sentiment ﬂow within a document — i.e.,
how sentiment orientation changes from one sentence to the next, as
discussed in Section 4.7 — one can create sentiment summaries by
choosing the sentences at local extrema of the ﬂow (plus the ﬁrst and
last sentence). An interesting feature of this approach is that by incorporating a document’s ﬂow, the technique takes into account the entire
document in a holistic way. Both approaches just mentioned seek to
select the absolutely most important sentences to present. Alternatively, one could simply extract all subjective sentences, as was done
by Pang and Lee to create “subjectivity extracts.” They suggested that these extracts could be used as summaries, although, as
mentioned above, they focused on the use of these extracts as an aid
to downstream polarity classiﬁcation, rather than as summaries per
se. Finally, we note that sentences are also used in multi-document
sentiment summarization as well, as described in Section 5.2.
Other sentiment summarization methods can work directly oﬀ
the output of opinion-oriented information-extraction systems. Indeed,
1 Beineke et al. use the term “sentiment summary” to refer to a single passage, but we
prefer to not restrict that term’s deﬁnition so tightly.
5.1 Single-Document Opinion-Oriented Summarization
Cardie et al. , speaking about the more restricted type of extraction
referred to by the technical term “information extraction,” “propose to
view . . . summary representations as information extraction (IE) scenario templates . . . [thus] we postulate that methods from information
extraction . . . will be adequate for the automatic creation of opinionbased summary representations.” (A similar observation was made by
Dini and Mazzini .) Note that these IE templates do not form coherent text on their own. However, they can be incorporated as is into
visual summaries.
Indeed, one interesting aspect of the problem of extracting sentiment information from a single document (or from multiple documents,
as discussed in Section 5.2) is that sometimes graph-based output seems
much more appropriate or useful than text-based output. For example,
graph-based summaries are very suitable when the information that is
most important is the set of entities described and the opinions that
some of these entities hold about each other . Figure 5.1 shows
an example of a human-generated summary in the form of a graph
depicting various negative opinions expressed during the aftermath of
Hurricane Katrina. Note the inclusion of text snippets on the arrows
to support the inference of a negative opinion2; in general, providing
some sense of the evidence from which opinions are inferred is likely to
be helpful to the user.
While summarization technologies may not be able to achieve
the level of sophistication of information presentation exhibited by
Figure 5.1, current research is making progress toward that goal. In
Figure 5.2, we see a proposed summary where opinion holders and the
objects of their opinions are connected by edges, and various annotations derived from IE output are included, such as the strength of
various attitudes.
Of course, graphical elements can also be used to represent a single bit, number or grade as a very succinct summary of a document’s
2 The exceptions are the edges from “news media”and the edges from “people who didn’t
evacuate.” It is (perhaps intentionally) ambiguous whether the lack of supporting quotes
is due merely to the lack of suﬃciently “juicy” ones or is meant to indicate that it is
utterly obvious that these entities blame many others. We also note that the hurricane
itself is not represented.
Summarization
Fig. 5.1 Graphic by Bill Marsh for The New York Times, October 1, 2005, depicting negative opinions of various entities toward each other in the aftermath of Hurricane Katrina.
Relation to opinion summarization pointed out by Eric Breck (Claire Cardie, personal
communication).
sentiment. Variations of stars, letter grades, and thumbs up/thumbs
down icons are common. More complex visualization schemes applied
on a sentence-by-sentence basis have also been proposed .
Multi-Document Opinion-Oriented Summarization
Language is itself the collective art of expression,
a summary of thousands upon thousands of individual
intuitions. The individual gets lost in the collective creation, but his personal expression has left some trace
in a certain give and ﬂexibility that are inherent in all
5.2 Multi-Document Opinion-Oriented Summarization
Fig. 5.2 Figure 2 (labeled 3) of Cardie et al. : proposal for a summary representation
derived from the output of an information-extraction system.
collective works of the human spirit. — Edward Sapir,
Language and Literature, 1921. Connection to sentiment
analysis pointed out by Das and Chen .
Some Problem Considerations
There never was in the world two opinions alike, no
more than two hairs, or two grains; the most universal
quality is diversity.
— Michel de Montaigne, Essays
Where an opinion is general, it is usually correct.
— Jane Austen, Mansﬁeld Park
We brieﬂy discuss here some points to keep in mind in regards
to multi-document sentiment summarization, although to a certain
degree, work in sentiment summarization has not yet reached a level
where these problems have come to the fore.
Determining which documents or portions of documents express the
same opinion is not always an easy task; but, clearly it is one that needs
to be addressed in the summarization setting, since readers of sentiment
summaries surely are interested in the overall sentiment in the corpus —
which means the system must determine shared sentiments within the
document collection at hand.
Summarization
This issue can still arise even when labels have been predetermined, if the items that have been pre-labeled come from diﬀerent
sub-collections. For instance, some documents may have polarity labels,
whereas others may contain ratings on a 1-to-5 scale. And even when
the ratings are drawn from the same set, calibration issues may
arise. Consider the following from Rotten Tomatoes’ frequently-askedquestions page ( 
On the Blade 2 reviews page, you have a negative review
from James Berardinelli (2.5/4 stars), and a positive
review from Eric Lurio (2.5/5). Why is Berardinelli’s
review labeled Rotten and Lurio’s review labeled Fresh?
You’re seeing this discrepancy because star systems
are not consistent between critics. For critics like Roger
Ebert and James Berardinelli, 2.5 stars or lower out of 4
stars is always negative. For other critics, 2.5 stars can
either be positive or negative. Even though Eric Lurio
uses a 5 star system, his grading is very relaxed. So, 2
stars can be positive. Also, there’s always the possibility
of the webmaster or critic putting the wrong rating on
As another example, in reconciling reviews of conference submissions,
program-committee members must often take into account the fact that
certain reviewers always tend to assign low scores to papers, while others have the opposite tendency. Indeed, we believe this calibration issue
may be the reason why reviews of cars on Epinions come not only with
a “number of stars” annotation, but also a “thumbs up/thumbs down”
indicator, in order to clarify whether, regardless of the rating assigned,
the review author actually intends to make a positive recommendation
An additional observation to take note of is the fact that when
two reviewers agree on a rating, they may have diﬀerent reasons for
doing so, and it may be important to indicate these reasons in the
summary. A related point is that when a reviewer assigns a middling
rating, it may be because he or she thinks that most aspects of the item
under discussion are so-so, but it may also be because he or she sees
5.2 Multi-Document Opinion-Oriented Summarization
both strong positives and strong negatives. Or, reviewers may have
the same opinions about individual item features, but weight these
individual factors diﬀerently, leading to a diﬀerent overall sentiment.
Indeed, Rotten Tomatoes summarizes a set of reviews both with the
Tomatometer — percentage of reviews judged to be positive — and
an average rating on a 1-to-10 scale. The idea, again according to the
FAQ ( is as
The Average Rating measures the overall quality of a
product based on an average of individual critic scores.
The Tomatometer simply measures the percentage of
critics who recommend a certain product.
For example, while “Men in Black” scored 90% on the
Tomatometer, the average rating is only 7.5/10. That
means that while you’re likely to enjoy MIB, it probably
wasn’t a contender for Best Picture at the Oscars.
In contrast, “Toy Story 2” received a perfect 100% on
the Tomatometer with an average rating of 9.6/10. That
means, not only are you certain to enjoy it, you’ll also
be impressed with the direction, story, cinematography,
and all the other things that make truly great ﬁlms
The problem of deciding whether two sentences or text passages have the same semantic content is one that is faced not just
by opinion-oriented multi-document summarizers, but by topic-based
multi-document summarizers as well ; this has been one of the
motivations behind work on paraphrase recognition and
textual entailment . But, as pointed out in Ku et al. , while
in traditional summarization redundant information is often discarded,
in opinion summarization one wants to track and report the degree
of “redundancy,” since in the opinion-oriented setting the user is typically interested in the (relative) number of times a given sentiment is
expressed in the corpus.
Carenini et al. note that a challenge in sentiment summarization is that the pieces of information to be summarized — people’s
Summarization
opinions — are often conﬂicting, which is a bit diﬀerent from the usual
situation in topic-based summarization, where typically one does not
assume that there are conﬂicting sets of facts in the document set
(although there are exceptions ).
Textual Summaries
In standard topic-based multi-document summarization, creating textual summaries has been a main focus of eﬀort. Hence, despite the differences in topic- and opinion-based summarization mentioned above,
several researchers have developed systems that create textual summaries of opinion-oriented information.
Leveraging Existing Topic-Based Technologies
One line of attack is to adapt existing topic-based multi-document summarization algorithms to the sentiment setting.
Sometimes the adaptation consists simply of modifying the input
to these pre-existing algorithms. For instance, Seki et al. 
propose that one apply standard multi-document summarization to
a sub-collection of documents that are on the same topic and that
are determined to belong to some relevant genre of text, such as
“argumentative.”
In other cases, pre-existing topic-based summarization techniques
are modiﬁed. For example, Carenini et al. generate naturallanguage summaries in the form of an “evaluative argument” using
the classic natural-language generation pipeline of content selection,
lexical selection and sentence planning, and sentence realization ,
assuming the existence of a pre-deﬁned product-feature hierarchy. The
system explicitly produces textual descriptions of aggregate information. The system is capable of relaying data about the average sentiment and signaling, if appropriate, that the distribution of responses
is bi-modal (this allows one to report “split votes”). They compare
this system against a modiﬁcation of an existing sentence-extraction
system, MEAD . The former approach seems more well-suited for
general overviews, whereas the latter seems better at providing more
variety in expression and more detail; see Figure 5.3. Related to the
5.2 Multi-Document Opinion-Oriented Summarization
Summary created via a “true natural-language-generation” approach:
Almost all users loved the Canon G3 possibly because some users
thought the physical appearance was very good. Furthermore, several users found the manual features and the special features to be
very good. Also, some users liked the convenience because some
users thought the battery was excellent. Finally, some users found
the editing/viewing interface to be good despite the fact that several customers really disliked the viewﬁnder. However, there were
some negative evaluations. Some customers thought the lens was
poor even though some customers found the optical zoom capability
to be excellent. Most customers thought the quality of the images
was very good.
Summary created by a modiﬁed sentence-extraction system:
Bottom line, well made camera, easy to use, very ﬂexible and powerful features to include the ability to use external ﬂash and lense/ﬁlters
choices. It has a beautiful design, lots of features, very easy to
use, very conﬁgurable and customizable, and the battery duration
is amazing! Great colors, pictures, and white balance. The camera is
a dream to operate in automode, but also gives tremendous ﬂexibility
in aperture priority, shutter priority, and manual modes. I’d highly
recommend this camera for anyone who is looking for excellent quality pictures and a combination of ease of use and the ﬂexibility to
get advanced with many options to adjust if you like.
Fig. 5.3 Sample automatically generated summaries. Adapted from Figure 2 of Carenini
et al. .
latter approach, sentence extraction methods have also been used to
create summaries for opinion-oriented queries or topics .
While we are not aware of the following technique being used in
standard topic-based summarization, we see no reason why it is not
applicable to that setting, at least in principle. Ku et al. (short
version available as Ku et al. ) propose the following simple scheme
to create a textual summary of a set of documents known in advance
to be on the same topic. Sentences considered to be representative
of the topic are collected, and the polarity of each such sentence is
computed based on what sentiment-bearing words it contains, with
negation taken into account. Then, to create a summary of the positive
documents, the system simply returns the headline of the document
with the most positive on-topic sentences, and similarly for the negative
Summarization
documents. The authors show the following examples for the positive
and the negative summary, respectively:
• Positive: “Chinese Scientists Suggest Proper Legislation for
Clone Technology.”
• Negative: “UK Government Stops Funding for Sheep Cloning
The cleverness of this method is that headlines are, by construction,
good summaries (at least of the article they are drawn from), so that
ﬂuency and informativeness, although perhaps not appropriateness, are
guaranteed.
Another perhaps unconventional type of multi-document “summary” is the selection of a few documents of interest from the corpus
for presentation to the user. In this vein, Kawai et al. have developed a news portal site called “Fair News Reader” that attempts to
determine the aﬀect characteristics of articles the user has been reading so far (e.g., “happiness” or “fear”) and then recommends articles
that are on the same topic but have opposite aﬀect characteristics. One
could imagine extending this concept to a news portal that presented to
the user opinions opposing his or her pre-conceived ones (Phoebe Sengers, personal communication). On a related note, Liu mentions
that one might desire a summarization system to present a “representative sample” of opinions, so that both positive and negative points of
view are covered, rather than just the dominant sentiment. As of the
time of this writing, Amazon presents the most helpful favorable review
side-by-side with the most helpful critical review if one clicks on the
“[x] customer reviews” link next to the stars indicator. Additionally,
one could interpret the opinion-leader identiﬁcation work of Song et al.
 as suggesting that blog posts written by opinion leaders could
serve as an alternative type of representative sample.
Summarizing online discussions and blogs is an area of related work
 . The focus of such work is not on summarizing the
opinions per se, although Zhou and Hovy note that one may
want to vary the emphasis on the opinions expressed versus the facts
expressed.
5.2 Multi-Document Opinion-Oriented Summarization
Textual Summarization Without Topic-based
Summarization Techniques
Other work in the area of textual multi-document sentiment summarization departs from topic-based work. The main reason seems to be
that redundancy elimination is much less of a concern: users may wish
to look at many individual opinions regardless of whether these individual opinions express the same overall sentiment, and these users may
not particularly care whether the textual overview they peruse is coherent. Thus, in several cases, textual “summaries” are generated simply
by listing some or all opinionated sentences. These are often grouped by
feature (sub-topic) and/or polarity, perhaps with some ranking heuristic such as feature importance applied .
Non-textual Summaries
In the previous section, we have discussed the creation of textual summaries of the opinion information expressed within a corpus. But in
settings where the polarity or orientation of each individual document within a collection is summed up in a single bit (e.g., thumbs
up/thumbs down), number (e.g., 3.5 stars), or grade (e.g., B+), an
alternative way to obtain a succinct summary of the overall sentiment
is to report summary statistics, such as the number of reviews that are
“thumbs up” or the average number of stars or average grade. Many
systems take this approach to summarization.
Summary statistics are often quite suited to graphical representations; we describe some noteworthy visual aspects of these summaries
here (evaluation of the user-interface aspects has not been a focus of
attention in the community to date).
“Bounded” Summary Statistics: Averages
and Relative Frequencies
We use the term bounded to refer to summary statistics that lie within a
predetermined range. Examples are the average number of stars (range:
0 to 5 stars, say) or the percentage of positive opinions (range: 0%
Summarization
“Thermometer”-type images are one means for displaying such
statistics. One example is the “Tomatometer” on the Rotten Tomatoes
website, which is simply a bar broken into two diﬀerently colored
portions; the portion of the bar that is colored red indicates the
fraction of positive reviews of a given movie. This representation
extends straightforwardly to n-ary categorization schemes, such as positive/middling/negative, via the use of n colors. The thermometergraphic concept also generalizes in other ways; for instance, the
depiction of a number of stars can be considered to be a variant of
this idea.
Instead of using size or extent to depict bounded summary statistics,
as is done with thermometer representations, one can use color shading. This choice seems particularly appropriate in settings where the
amount of display real-estate that can be devoted to any particular item under evaluation is highly limited or where size or location is reserved to represent some other information. For instance,
Gamon et al. use color to represent the general assessment
of (automatically determined) product features. In Figure 5.4, we
see that each of many features or topics, such as “handling” or
“vw, service,” is represented by a shaded box. The colors for any
given box range from red to white to green, indicating gradations
of the average sentiment toward that topic, moving from negative
to neutral (or objective) to positive, respectively. Note that one
can quickly glean from such a display what was liked and what
was disliked about the product under discussion, despite the large
number of topics under evaluation — people like driving this car
but dislike the service. As shown in Figure 5.5, a similar interface (together with a usability study) is presented in Carenini et al.
 . Some diﬀerences are that natural-language summarization is
also employed, so that the summary is both “verbal” and visual;
the features are grouped into a hierarchy, thus leveraging the ability of Treemaps to display hierarchical data via nesting; and the
interface also includes a way (not depicted in the ﬁgure) to see an
“at-a-glance” summary of the polarities of the individual sentences
commenting on a particular feature. A demo is available online at
 carenini/storage/SEA/demo.html.
5.2 Multi-Document Opinion-Oriented Summarization
Fig. 5.4 Figure 2 of Gamon et al. , depicting (automatically determined) topics discussed in reviews of the Volkswagen Golf. The size of each topic box indicates the number
of mentions of that topic. The shading of each topic box, ranging from red to white to
green, indicates the average sentiment, ranging from negative to neutral/none to negative,
respectively. At the bottom, the sentences most indicative of negative sentiment for the
topic “vw, service” are displayed.
Unbounded Summary Statistics
As just described, thermometer graphics and color shading can be
used to represent bounded statistics such as the mean or, in the
case of n-color thermometers, relative distributions of ratings across
diﬀerent classes. But bounded statistics by themselves do not provide
other important pieces of information, such as the actual number
of opinions within each class. (We consider raw frequencies to be
conceptually unbounded, although there are practical limits to how
many opinions can be accounted for.) Intuitively, the observation that
50% of the reviews of a particular product are negative3 is more of a
3 We admit to being “glass-half-empty” people.
Summarization
Fig. 5.5 Figure 4 of Carenini et al. , showing a summary of reviews of a particular
product. An automatically generated text summary is on the left; a visual summary is
on the right. The size of each each box in the visual summary indicates the number of
mentions of the corresponding topic, which occupy a pre-deﬁned hierarchy. The shading of
each topic box, ranging from red to black to green, indicates the average sentiment, ranging
from negative to neutral/none to negative, respectively. At the bottom is shown the source
for the portion of the generated natural-language summary that has been labeled with the
footnote “4.”
big deal if that statistic is based on 10,000 reviews than if it based on
Another problem speciﬁc to the mean as a summary statistic is
that review-aggregation sites seem to often exhibit highly skewed rating distributions, with a particular bias toward highly positive reviews
 .4 Since there can often be a second mode, or
bump, at the extreme low end of the rating scale, indicating polarization — for example, Hu et al. remark that 54% of the items in a
sample of Amazon book, DVD, and video products with more than 20
reviews fail both statistical normality and unimodality tests — reporting only the mean rating score may not provide enough information.
To put it another way, divulging the average does not give the user
4 On a related note, William Saﬁre’s New York Times May 1, 2005 article “Blurbosphere”
quotes Charles McGrath, former editor of the New York Times Book Review, as asking,
“has there ever been a book that wasn’t acclaimed?”
5.2 Multi-Document Opinion-Oriented Summarization
enough information to distinguish between a set of middling reviews
and a set of polarized reviews.
On the other hand, it is worth pointing out that just giving the number of positive and negative reviews, respectively, on the assumption
that the user can always derive the percentages from these counts, may
not suﬃce. Cabral and Horta¸csu observe that once eBay switched
to displaying the percentage of pieces of feedback on sellers that were
negative, as opposed to simply the raw numbers, then negative reviews
began to have a measurable economic impact (see Section 6).
Hence, not surprisingly, sentiment summaries tend to include data
on the average rating, the distribution of ratings, and/or the number
of ratings.
Visualization of unbounded summary statistics.
Of the two systems
described above that represent the average polarity of opinions via
color, both represent the quantity of the opinions on a given topic
via size. This means that the count data for positive and for negative
opinions are not explicitly presented separately. In other systems, this
is not the case; rather, frequencies for diﬀerent classes are broken out
and displayed.
For instance, as of the time of this writing, Amazon displays an
average rating as a number of stars with the number of reviews next
to it; mousing over the stars brings up a histogram of reviewer ratings
annotated with counts for the 5-star reviews, 4-star reviews, etc. (Further mousing over the bars of the histogram brings up the percentage
of reviews that each of those counts represent.)
As another example, a sample output of the Opinion Observer system is depicted in Figure 5.6, where the portion of a bar projecting above the centered “horizon” line represents the number of positive
opinions about a certain product feature, and the portion of the bar
below the line represents the number of negative opinions. (The same
idea can be used to represent percentages too, of course.) A nice feature
of this visualization is that because of the use of a horizon line, two
separate frequency datapoints — the positive and negative counts —
can be represented by what is visually one object, namely, a solid bar,
and one can easily simultaneously compare negatives against negatives
Summarization
Fig. 5.6 Figure 2 of Liu et al. . Three cellphones are represented, each by a diﬀerent
color. For each feature (“General,” “LCD,” etc.), three bars are shown, one for each of
three cellphones. For a given feature and phone, the portions of the bar above and below
the horizontal line represent the number of reviews that express positive or negative views
about that camera’s feature, respectively. (The system can also plot the percentages of
positive or negative opinions, rather than the raw numbers.) The pane on the upper-right
displays the positive sentences regarding one of the products.
and positives against positives. This simultaneous comparison is made
much more diﬃcult if the bars all have one end “planted” at the same
location, as is the case for standard histograms such as the one depicted
in Figure 5.7.
While the data for the features are presented sequentially in
Figure 5.6 (ﬁrst “General,” then “LCD,” and so forth), an alternative
visualization technique called a rose plot is exempliﬁed in Figure 5.8,
which depicts a sample output of the system developed by Gregory
et al. . The median and quartiles across a document sub-collection
of the percentage of positive and negative words per document, together
with similar data for other possible aﬀect-classiﬁcation dimensions, are
represented via a variant of box plots. (Adaptation to raw counts rather
5.2 Multi-Document Opinion-Oriented Summarization
Fig. 5.7 A portion of Figure 4 of Yi and Niblack , rotated to save space and facilitate
comparison with Figure 5.6. Notice that simultaneous comparison of the negative counts
and the positive counts for two diﬀerent products is not as easy as it is in Figure 5.6.
than percentages is straightforward.) Mapping this idea to product
comparisons in the style of Opinion Observer, one could associate different features with diﬀerent “compass directions,” e.g., the feature
“battery life” with “southwest,” as long as the number of features being
reported on is not too large. The reason that this representation might
prove advantageous in some settings is that in some situations, a circular arrangement may be more compact than a sequential one, and it
may be easier for a user to remember a feature as being “southwest”
than as being “the ﬁfth of eight.” An additional functionality of the
system that is not shown in the ﬁgure is the ability to depict how much
an individual document’s positive/negative percentage diﬀers from the
average for a given document group to which the document belongs.
A similar circular layout is proposed in Subasic and Huettner for
visualizing various dimensions of aﬀect within a single document.
Morinaga et al. opt to represent degrees of association between
products and opinion-indicative terms of a pre-speciﬁed polarity. First,
Summarization
Fig. 5.8 Figure 7 of Gregory et al. . On the right are two rose plots, one for each of
two products; on the left is the plot’s legend. In each rose plot, the upper two “petals”
represent positivity and negativity, respectively (of the other six petals, the bottom two are
vice and virtue, etc.). Similarly to box plots, the median value is indicated by a dark arc,
and the quartiles by (colored) bands around the median arc. Darker shading for one of the
two petals in a pair (e.g., “positive and negative”) are meant to indicate the negative end
of the spectrum for the aﬀect dimension represented by the given petal pair. The histogram
below each rose relates to the number of documents represented.
opinions are gathered using the authors’ pre-existing system .
Coding-length and probabilistic criteria are used to determine which
terms to focus on, and principal component analysis is then applied
to produce a two-dimensional visualization, such that nearness corresponds to strength of association, as in the authors’ previous work .
Thus, in Figure 5.9, we see that cellphone A is associated with what
we recognize as positive terms, whereas cellphone C is associated with
negative terms.
5.2 Multi-Document Opinion-Oriented Summarization
Fig. 5.9 Figure 5 of Morinaga et al. : principal-components-analysis visualization of
associations between products (squares) and automatically selected opinion-oriented terms
(circles).
Temporal Variation and Sentiment Timelines
So far, the summaries we have considered do not explicitly incorporate any temporal dimension. However, time is often an important
consideration.
First, users may wish to view individual reviews in reverse chronological order, i.e., newest ﬁrst. Indeed, at the time of this writing, this
is one of the two sorting options that Amazon presents.
Summarization
Second, in many applications, analysts and other users are interested in tracking changes in sentiment about a product, political candidate, company, or issue over time. Clearly, one can create a sentiment
timeline simply by plotting the value of a chosen summary statistic at
diﬀerent times; the chosen statistic can reﬂect the prevailing polarity
 or simply the number of mentions, in which case what is
being measured is perhaps not so much public opinion, but rather public awareness . Such work is strongly related at a
conceptual level to topic detection and tracking , a review of which
is beyond the scope of this survey.
Mishne and de Rijke also depict the derivative of the summary
statistic considered as a function of time.
Review(er) Quality
How do we identify what is good? And how do we
censure what is bad? We will argue that developing
a humane reputation system ecology can provide better answers to these two general questions — restraining the baser side of human nature, while liberating
the human spirit to reach for ever higher goals. —
“Manifesto for the reputation society.” Masum and
Zhang 
When creating summaries of reviews or opinionated text, an important type of information that deserves careful consideration is whether
or not individual reviews are helpful or useful. For example, a system
might want to downweight or even discard unhelpful reviews before creating summaries or computing aggregate statistics, as in Liu et al. .
Alternatively, the system could use all reviews, but provide helpfulness
indicators for individual reviews as a summary of their expected utility. Indeed, non-summarization systems could use such information,
too: for instance, a review-oriented search engine could rank its search
results by helpfulness.
Some websites already gather helpfulness information from human
readers. For example, Amazon.com annotates reviews with comments
like “120 of 140 people found the following review helpful,” meaning
5.2 Multi-Document Opinion-Oriented Summarization
that of the 140 people who pressed one of the “yes” or “no” buttons in response to the question “Was this review helpful to you?” —
we deem these 140 people utility evaluators — 120 chose “yes.” Similarly, the Internet Movie Database (IMDb, also
annotates user comments with “x out of y people found the following
comment useful.” This similarity is perhaps not surprising due to the
fact that Amazon owns IMDb, although from a research point of view,
note that the two populations of users are probably at least somewhat
disjoint, meaning that there might be interesting diﬀerences between
the sources of data. Other sites soliciting utility evaluations include
Yahoo! Movies and Yahoo! TV, which allow the user to sort reviews by
helpfulness; CitySearch, which solicits utility evaluations from general
users and gives more helpful reviews greater prominence; and Epinions, which only allows registered members to rate reviews and does
not appear to have helpfulness as a sort criterion, at least for nonregistered visitors.5 (We learned about the solicitation of utility evaluations by IMDb from Zhuang et al. and by Citysearch from
Dellarocas .)
Despite the fact that many review-aggregation sites already provide
helpfulness information gathered from human users, there are still at
least two reasons why automatic helpfulness classiﬁcation is a useful
line of work to pursue.
Items that lack utility evaluations.
Many reviews receive very few
utility evaluations. For example, 38% of a sample of roughly 20,000
Amazon MP3-player reviews, and 31% of those aged at least three
months, received three or fewer utility evaluations . Similarly, Liu
et al. conﬁrm one’s prior intuitions that Amazon reviews that are
youngest and reviews that are most lowly ranked (i.e., determined to
be least helpful) by the site receive the fewest utility evaluations.
5 We note that we were unable to ﬁnd Amazon’s deﬁnition of “helpful,” and conclude
that they do not supply one. In contrast, Yahoo! speciﬁes the following: “Was [a review]
informative, well written or amusing — above all was it was helpful to you in learning about the [ﬁlm or show]? If so, then you should rate that review as helpful.”
It might be interesting to investigate whether these diﬀering policies have implications. There have in fact been some comments that Amazon should clarify its question ( 
Tx3QHE2JPEXQ1V7/1? encoding=UTF8&asin =B000FL7CAU).
Summarization
Perhaps some reviews receive no utility evaluations simply because
they are so obviously bad that nobody bothers to rate them. But this
does not imply that reviews without utility evaluations must necessarily
be unhelpful; certainly we can not assume this of reviews too recently
written to have been read by many people. One important role that
automated helpfulness classiﬁers can play, then, is to provide utility
ratings in the many cases when human evaluations are lacking.
Skew in utility evaluations.
Another interesting potential application
of automated helpfulness classiﬁcation is to correct for biases in humangenerated utility evaluations.
We ﬁrst consider indirect evidence that such biases exist. It turns
out that just as the distribution of ratings can often be heavily skewed
toward the positive end, as discussed in Section 5.2.3.2, the distribution
of utility evaluations can also be heavily skewed toward the helpful end,
probably due at least in part to similar reasons as in the product-ratings
case. In a crawl of approximately 4 million unique Amazon reviews for
about 670,000 books (excluding alternate editions), the average percentage of “yes” responses among the utility evaluations is between
74% and 70%, depending on whether reviews with fewer than 10 utility evaluations are excluded (Gueorgi Kossinets and Cristian Danescu
Niculescu-Mizil, personal communication). Similarly, half of a sample
of about 23,000 Amazon digital-camera reviews had helpful/unhelpful
vote ratios of over 9 to 1 . As in the ratings distribution case, one’s
intuition is that the percentage of reviews that are truly helpful is not
as high as these statistics would seem to indicate. Another type of indirect evidence of bias is that the number of utility evaluations received
by a review appears to decrease exponentially in helpfulness rank as
computed by Amazon . (Certainly there has to be some sort of
decrease, since Amazon’s helpfulness ranking is based in part on the
number of utility evaluations a review receives.) Liu et al. conjecture that reviews that have many utility evaluations will have a disproportionate inﬂuence on readers (and utility evaluators) because they are
viewed as more authoritative, but reviews could get many utility evaluations only because they are more prominently displayed, not because
readers actually compared them against other reviews. (Liu et al. 
5.2 Multi-Document Opinion-Oriented Summarization
call this tendency for often-evaluated reviews to quickly accumulate
even more utility evaluations as “winner circle” bias; in other literature on power-law eﬀects, related phenomena are also referred to as
“rich-get-richer.”)
As for more direct evidence: Liu et al. conduct an reannotation study in which the Amazon reviewers’ utility evaluations
often did not match those of the human re-labelers. However, this latter
evidence should be taken with a grain of salt. First, in some of the
experiments in the study, “ground truth” helpfulness was measured by,
among other things, the number of aspects of a product that are discussed by a review. Second, in all experiments, the test items appear to
have consisted of only the text of a single review considered in isolation.
It is not clear that the ﬁrst point corresponds to the standard that all
Amazon reviewers used, or should be required to use, and clearly, the
second point describes an isolated-text setting that is not the one that
real Amazon reviewers work in. To exemplify both these objections: a
very short review written by a reputable critic (e.g., a “top reviewer”)
that points out something that other reviews missed can, indeed, be
quite helpful, but would score poorly according to the speciﬁcation of
Liu et al. . Indeed, the sample provided of a review that should be
labeled “bad” starts,
I want to point out that you should never buy a generic
battery, like the person from San Diego who reviewed
the S410 on May 15, 2004, was recommending. Yes
you’d save money, but there have been many reports of
generic batteries exploding when charged for too long.
We would view this comment, if true, to be quite helpful, despite the
fact that it fails the speciﬁcation. Another technical issue is that the
re-labelers used a four-class categorization scheme, whereas essentially
every possible percentage of positive utility evaluations could form a
distinct class for the Amazon labels: it might have been better to treat
reviews with helpfulness percentages of 60% and 61% as equivalent,
rather than saying that Amazon reviewers rated the latter as better
than the former.
Summarization
Nonetheless, given the large predominance of “helpful” among utility evaluations despite the fact that anecdotal evidence we have gathered indicates that not all reviews deserve to be called “helpful,”
and given the suggestive results of the re-annotation experiment just
described, it is likely that some of the human utility evaluations are
not strongly related to the quality of the review at hand. Thus, we
believe that correction of these utility evaluations by automatic means
is a valid potential application.
A note regarding the eﬀect of utility evaluations.
It is important to
mention one caveat before proceeding to describe research in this area.
Park et al. attempted to determine what the eﬀect of review quality actually is on purchasing intention, running a study in which subjects engaged in hypothetical buying behavior. They found non-uniform
eﬀects: “low-involvement [i.e., motivated] consumers are aﬀected by the
quantity rather than the quality of reviews ... high-involvement consumers are aﬀected by review quantity mainly when the review quality
is high...The eﬀect of review quality on high-involvement consumers is
more pronounced with a sizable number of reviews, whereas the eﬀect
of review quantity is signiﬁcant even when the review quality is low.”
(More on the economic impacts of sentiment analysis is described in
Section 6.)
Methods for Automatically Determining Review
In a way, one could consider the review-quality determination problem
as a type of readability assessment and apply essay-scoring techniques
 . However, while some of the systems described below do try to
take into account some readability-related features, they are tailored
speciﬁcally to product reviews.
Kim et al. , Zhang and Varadarajan , and Ghose and
Ipeirotis attempt to automatically rank certain sets of reviews
on the Amazon.com website according to their helpfulness or utility,
using a regression formulation of the problem. The domains considered are a bit diﬀerent: MP3 players and digital cameras in the ﬁrst
case; Canon electronics, engineering books, and PG-13 movies in the
5.2 Multi-Document Opinion-Oriented Summarization
second case; and AV players plus digital cameras in the third case.
Liu et al. convert the problem into one of low-quality review
detection (i.e., binary classiﬁcation), experimenting mostly with manually (re-)annotated reviews of digital cameras, although CNet editorial
ratings were also considered on the assumption that these can be considered trustworthy. Rubin and Liddy also sketch a proposal to
consider whether reviews can be considered credible.
Kim et al. study which of a multitude of length-based, lexical,
POS-count, product-aspect-mention count, and metadata features are
most eﬀective when utilizing SVM regression. The best feature combination turned out to be review length plus tf-idf scores for lemmatized
unigrams in the review plus the number of “stars” the reviewer assigned
to the product. Somewhat disappointingly, the best pair of features
among these was the length of the review and the number of stars.
(Using “number of stars” as the only feature yielded similar results to
using just the deviation of the number of stars given by the particular
reviewer from the average number of stars granted by all reviewers for
the item.) The eﬀectiveness of using all unigrams appears to subsume
that of using a select subset, such as sentiment-bearing words from the
General Inquirer lexicon .
Zhang and Varadarajan use a diﬀerent feature set. They
employ a ﬁner classiﬁcation of lexical types, and more sources for subjective terms, but do not include any meta-data information. Interestingly, they also consider the similarity between the review in question
and the product speciﬁcation, on the premise that a good review should
discuss many aspects of the product; and they include the review’s
similarity to editorial reviews, on the premise that editorial reviews
represent high-quality examples of opinion-oriented text. (David and
Pinch observe, however, that editorial reviews for books are paid
for and are meant to induce sales of the book.) However, these latter two
original features do not appear to enhance performance. The features
that appear to contribute the most are the class of shallow syntactic features, which, the authors speculate, seem to characterize style;
examples include counts of words, sentences, wh-words, comparatives
and superlatives, proper nouns, etc. Review length seems to be very
weakly correlated with utility score.
Summarization
We thus see that Kim et al. ﬁnd that meta-data and very
simple term statistics suﬃce, whereas Zhang and Varadarajan 
observe that more sophisticated cues that appear correlated with linguistic aspects appear to be most important. Possibly, the diﬀerence is
a result of the diﬀerence in domain choice: we speculate that book and
movie reviews can involve more sophisticated language use than what
is exhibited in reviews of electronics.
Declaring themselves inﬂuenced by prior work on creating subjectivity extracts , Ghose and Ipeirotis take a diﬀerent approach.
They focus on the relationship between the subjectivity of a review
and its helpfulness. The basis for measuring review subjectivity is
as follows: using a classiﬁer that outputs the probability of a sentence being subjective, one can compute for a given review the average subjectiveness-probability over all its sentences, or the standard
deviation of the subjectivity scores of the sentences within the review.
They found that both the standard deviation of the sentence subjectivity scores and a readability score (review length in characters divided
by number of sentences) have a strongly statistically signiﬁcant eﬀect
on utility evaluations, and that this is sometimes true of the average
subjectiveness-probability as well. They then suggest on the basis of
this and other evidence that it is extreme reviews that are considered
to be most helpful, and develop a helpfulness predictor based on their
Liu et al. considered features related to review and sentence
length; brand, product and product-aspect mentions, with special consideration for appearances in review titles; sentence subjectivity and
polarity; and “paragraph structure.” This latter refers to paragraphs
as delimited by automatically determined keywords. Interestingly, the
technique of taking the 30 most frequent pairs of nouns or noun
phrases that appear at the beginning of a paragraph as keywords
yields separator pairs such as “pros”/“cons,” “strength”/“weakness,”
and “the upsides”/“downsides.” (Note that this diﬀers from identifying pro or con reasons themselves , or identifying the polarity
of sentences. Note also that other authors have claimed that diﬀerent techniques are needed for situations in which pro/con delimiters
are mandated by the format imposed by a review aggregation site
5.2 Multi-Document Opinion-Oriented Summarization
but a separate detailed textual description must also be included, as
in Epinions, as opposed to settings where such delimiters need not
be present or where all text is placed in the context of such delimiters .) Somewhat unconventionally with respect to other textcategorization work, the baseline was taken as SVMlight run with three
sentence-level statistics as features; that is, the performance of a classiﬁer trained using bag-of-word features is not reported. Given this
unconventional starting point, the addition of the features that do not
reﬂect subjectivity or sentiment help. Including subjectivity and polarity on top of what has already been mentioned does not yield further
improvement, and use of title-appearance for mentions did not seem
Review- or opinion-spam detection — the identiﬁcation of deliberately misleading reviews — is a line of work by Jindal and Liu ( ,
short version available as Jindal and Liu ) in the same vein. One
challenge these researchers faced was the diﬃculty in obtaining ground
truth. Therefore, for experimental purposes they ﬁrst re-framed the
problem as one of trying to recognize duplicate reviews, since a priori
it is hard to see why posting repeats of reviews is justiﬁed. (However,
one potential problem with the assumption that repeated reviews constitute some sort of manipulation attempt, at least for the Amazon
data that was considered, is that Amazon itself cross-posts reviews
across diﬀerent products — where “diﬀerent” includes diﬀerent instantiations (e.g., e-book vs. hardcover) or subsequent editions of the same
item (Gueorgi Kossinets and Cristian Danescu Niculescu-Mizil, personal communication). Speciﬁcally, in a sample of over 1 million Amazon book reviews, about one-third were duplicates, but these were all
due to Amazon’s cross-posting. Human error (e.g., accidentally hitting
the “submit” button twice) causes other cases of non-malicious duplicates.) A second round of experiments attempted to identify “reviews
on brands only,” ads, and “other irrelevant reviews containing no opinions” (e.g., questions, answers, and random texts). Some of the features
used were similar to those employed in the studies described above;
others included features on the review author and the utility evaluations themselves. The overall message was that this kind of spam is
relatively easy to detect.
Summarization
Reviewer-Identity Considerations
In the above, we have discussed determining the quality of individual
reviews. An alternate approach is to look at the quality of the reviewers; doing so can be thought of as a way of classifying all the reviews
authored by the same person at once.
Interestingly, one study has found that there is a real economic
eﬀect to be observed when factoring in reviewer credibility: Gu et al.
 note that a weighted average of message-board postings in which
poster credibility is factored in has “prediction power over future abnormal returns of the stock,” but if postings are weighted uniformly, the
predictive power disappears.
There has been work in a number of areas in the humanlanguage-technologies community that incorporates the authority,
trustworthiness, inﬂuentialness, or credibility of authors . PageRank and hubs and authorities (also known as
HITS) are very inﬂuential examples of work in link analysis on
identifying items of great importance. Trust metrics also appear in
other work, such as research into peer-to-peer and reputation networks
and information credibility .
Broader Implications
Sentiment is the mightiest force in civilization . . . —
J. Ellen Foster, What America Owes to Women, 1893
As we have seen, sentiment-analysis technologies have many potential applications. In this section, we brieﬂy discuss some of the larger
implications that the existence of opinion-oriented information-access
services has.
One point that should be mentioned is that applications that
gather data about people’s preferences can trigger concerns about privacy violations. We suspect that in many people’s minds, having one’s
public blog scanned by a coﬀee company for positive mentions of its
product is one thing; having one’s cell-phone conversations monitored
by the ruling party of one’s own country for negative mentions of government oﬃcials is quite another. It is not our intent to comment further here on privacy issues, these not being issues on which we are
qualiﬁed to speak; rather, we simply want to be thorough by reminding the reader that these issues do exist and are important, and that
these concerns apply to all data-mining technologies in general.
Broader Implications
Manipulation.
But even if we restrict attention to the apparently
fairly harmless domain of business intelligence, certain questions
regarding the potential for manipulation do arise. Companies already
participate in managing online perceptions as part of the normal course
of public-relations eﬀorts:
. . . companies can’t control consumer-generated content. They can, however, pay close attention to it. In
many cases, often to a large degree, they can even inﬂuence it. In fact, in a survey conducted by Aberdeen [of
“more than 250 enterprises using social media monitoring and analysis solutions in a diverse set of enterprises”], more than twice as many companies with social
media monitoring capabilities actively contribute to
consumer conversations than remain passive observers
(67% versus 33%). Over a third of all companies (39%)
contribute to online conversations on a frequent basis,
interacting with consumers in an eﬀort to sway opinion,
correct misinformation, solicit feedback, reward loyalty,
test new ideas, or for any number of other reasons.
— Zabin and Jeﬀeries 
And it is also the case that some arguably mild forms of manipulation have been suggested. For instance, one set of authors, in studying
the strategic implications for a company of oﬀering online consumer
reviews, notes that “if it is possible for the seller to decide the timing to oﬀer consumer reviews at the individual product level, it may
not always be optimal to oﬀer consumer reviews at a very early stage
of new product introduction, even if such reviews are available” , and others have
worked on a manufacturer-oriented system that ranks reviews “according to their expected eﬀect on sales,” noting that these might not be
the ones that are considered to be most helpful to users .
But still, there are concerns that corporations might try to further
“game the system” by taking advantage of knowledge of how ranking
systems work in order to suppress negative publicity or engage
in other so-called “black-hat search engine optimization” and related
activities. Indeed, there has already been a term — “sock puppet” —
coined to refer to ostensibly distinct online identities created to give
the false impression of external support for a position or opinion; Stone
and Richtel list several rather attention-grabbing examples of wellknown writers and CEOs engaging in sock-puppetry. On a related note,
Das and Chen recommend Leinweber and Madhavan as an
interesting review of the history of market manipulation through disinformation.
One reason these potentials for abuse are relevant to this survey
is that, as pointed out earlier in the Introduction, sentiment-analysis
technologies allow users to consult many people who are unknown to
them; but this means precisely that it is harder for users to evaluate the
trustworthiness of those people (or “people”) they are consulting. Thus,
opinion-mining systems might potentially make it easier for users to be
mis-led by malicious entities, a problem that designers of such systems
might wish to prevent. On the ﬂip side, an information-access system
that is (perhaps unfairly) perceived to be vulnerable to manipulation
is one that is unlikely to be widely used; thus, again, builders of such
systems might wish to take measures to make it diﬃcult to “game the
In the remainder of this section, then, we discuss several aspects
of the problem of possible manipulation of reputation. In particular,
we look at evidence as to whether reviews have a demonstrable economic impact: if reviews do signiﬁcantly aﬀect customer purchases,
then there is arguably an economic incentive for companies to engage
in untoward measures to manipulate public perception; if reviews do
not signiﬁcantly aﬀect customer purchases, then there is little reason, from an economic point of view, for entities to try to artiﬁcially
change the output of sentiment-analysis systems — or, as Dewally 
asserts, “the stock market does not appear to react to these recommendations. . . . The fears raised by the media about the destabilizing
power of such traders who participate in these discussions are thus
groundless.” If such claims are true, then it would seem that trying
to manipulate perceptions conveyed by online review-access systems
would oﬀer little advantages to companies, and so they would not
engage in it.
Broader Implications
Economic Impact of Reviews
As mentioned earlier in the Introduction to this survey, many readers of
online reviews say that these reviews signiﬁcantly inﬂuence their purchasing decisions . However, while these readers may have believed
that they were “signiﬁcantly inﬂuenced,” perception and reality can
diﬀer. A key reason to understand the real economic impact of reviews
is that the results of such an analysis have important implications for
how much eﬀort companies might or should want to expend on online
reputation monitoring and management.
Given the rise of online commerce, it is not surprising that a body
of work centered within the economics and marketing literature studies the question of whether the polarity (often referred to as “valence”)
and/or volume of reviews available online have a measurable, significant inﬂuence on actual consumer purchasing. Ever since the classic “market for lemons” paper demonstrating some problems for
makers of high-quality goods, economists have looked at the value of
maintaining a good reputation as a means to overcome these problems , among other strategies. (See the introduction
to Dewally and Ederington , from which the above references have
been taken, for a brief review.) One way to acquire a good reputation is,
of course, by receiving many positive reviews of oneself as a merchant;
another is for the products one oﬀers to receive many positive reviews.
For the purposes of our discussion, we regard experiments wherein the
buying is hypothetical as being out of scope; instead, we focus on economic analyses of the behavior of people engaged in real shopping and
spending real money.1
1 Note that researchers in the economics community have a tradition of circulating and
revising working papers, sometimes for years, before producing an archival version. In the
references that follow, we have cited the archival version when journal-version publication
data has been available to us, in order to enable the interested reader to access the ﬁnal,
peer-reviewed version of the work. But because of this policy, the reader who wishes to
delve into this literature further should keep in mind the following two points. First, many
citations within the literature are to preliminary working papers. This means that our
citations may not precisely match those given in the papers themselves (e.g., there may be
title mismatches). Second, work that was done earlier may be cited with a later publication
date; therefore, the dates given in our citations should not be taken to indicate research
precedence.
6.1 Economic Impact of Reviews
The general form that most studies take is to use some form of hedonic regression to analyze the value and the signiﬁcance of diﬀerent item features to some function, such as a measure of utility to the
customer, using previously recorded data. (Exceptions include Resnick
et al. , who ran an empirical experiment creating “new” sellers on
eBay, and Jin and Kato , who made actual purchases to validate
seller claims.) Speciﬁc economic functions that have been examined
include revenue (box-oﬃce take, sales rank on Amazon, etc.), revenue
growth, stock trading volume, and measures that auction-sites like eBay
make available, such as bid price or probability of a bid or sale being
made. The type of product considered varies (although, understandably, those oﬀered by eBay and Amazon have received more attention):
examples include books, collectible coins, movies, craft beer, stocks, and
used cars. It is important to note that some conclusions drawn from one
domain often do not carry over to another; for instance, reviews seem
to be inﬂuential for big-ticket items but less so for cheaper items. But
there are also conﬂicting ﬁndings within the same domain. Moreover,
diﬀerent subsegments of the consumer population may react diﬀerently:
for example, people who are more highly motivated to purchase may
take ratings more seriously. Additionally, in some studies, positive ratings have an eﬀect but negative ones do not, and in other studies the
opposite eﬀect is seen; the timing of such feedback and various characteristics of the merchant or of the feedback itself (e.g., volume) may
also be a factor.
Nonetheless, to gloss over many details for the sake of brevity:
if one allows any eﬀect — including correlation even if said correlation is shown to be not predictive — that passes a statistical signiﬁcance test at the 0.05 level to be classed as “signiﬁcant,” then
many studies ﬁnd that review polarity has a signiﬁcant economic eﬀect
 . But there are
a few studies that conclude emphatically that review positivity or negativity has no signiﬁcant economic eﬀect .
Duan et al. explicitly relate their ﬁndings to the issue of corporate manipulation: “From the managerial perspective, we show that
consumers are rational in inferring movie quality from online user
Broader Implications
reviews without being unduly inﬂuenced by the rating, thus presenting
a challenge to businesses that try to inﬂuence sales through ‘planting’
online word-of-mouth.”
With respect to eﬀects that have been found, the literature survey
contained in Resnick et al. states that
At the larger end of eﬀect sizes for positive evaluations,
the model in [Livingston ] ﬁnds that sellers with
more than 675 positive comments earned a premium
of $45.76, more than 10% of the mean selling price, as
compared to new sellers with no feedback. . . . At the
larger end of eﬀect sizes for negatives, [Lucking-Reiley
et al. ], looking at collectible coins, ﬁnds that a
move from 2 to 3 negatives cuts the price by 11%, about
$19 from a mean price of $173.
But in general, the claims of statistically signiﬁcant eﬀects that have
been made tend to be (a) qualiﬁed by a number of important caveats,
and (b) quite small in absolute terms per item, although on the other
hand again, small eﬀects per item can add up when many items are
involved. With regard to this discussion, the following excerpt from
Houser and Wooders is perhaps illuminating:
. . . on average, 3.46 percent of sales is attributable to
the seller’s positive reputation stock. Similarly, our estimates imply that the average cost to sellers stemming
from neutral or negative reputation scores is $2.28, or
0.93 percent of the ﬁnal sales price. If these percentages are applied to all of eBay’s auctions , this would imply that
sellers’ positive reputations added more than $55 million to the value of sales, while non-positives reduced
sales by about $15 million.
Ignoring for the moment the fact that, as mentioned above, other
papers report diﬀering or even opposite ﬁndings, we simply note that
the choice of whether to focus on “0.93%,” “$2.28,” or “$55 million”
6.1 Economic Impact of Reviews
(and whether to view the latter amount as seeming particularly large
or not) is one we prefer to leave to the reader.
Let us now mention some particular papers and ﬁndings of particular interest.
Surveys Summarizing Relevant Economic Literature
Resnick et al. and Bajari and Horta¸csu are good entry points
into this body of literature. They provide very thorough overviews
and discussion of the methodological issues underlying the studies
mentioned above. Hankin supplies several visual summaries that
are modeled after the literature-comparison tables in Dellarocas ,
Resnick et al. , and Bajari and Horta¸csu . A list of a number
of papers on the general concept of sentiment in behavioral ﬁnance can
be found at 
Economic-Impact Studies Employing Automated
Text Analysis
In most of the studies cited above, the orientation of a review was
derived from an explicit rating indication such as number of stars, but
a few studies applied manual or automatic sentiment classiﬁcation to
review text .
At least one related set of studies claims that “the text of the reviews
contains information that inﬂuences the behavior of the consumers, and
that the numeric ratings alone cannot capture the information in the
text” — see also Ghose et al. , who additionally attempt
to assign a “dollar value” to various adjective-noun pairs, adverb-verb
pairs, or similar lexical conﬁgurations. In a related vein, Pavlou and
Dimoka suggest that “the apparent success of feedback mechanisms to facilitate transactions among strangers does not mainly come
from their crude numerical ratings, but rather from their rich feedback text comments.” Also, Chevalier and Mayzlin interpret their
ﬁndings on the eﬀect of review length as providing some evidence that
people do read the reviews rather than simply relying on numerical
Broader Implications
On the other hand, Cabral and Horta¸csu , in an interesting
experiment, look at 41 odd cases of feedback on sellers posted on eBay:
what was unusual was that the feedback text was clearly positive, but
the numerical rating was negative (presumably due to user error). Analysis reveals that these reviews have a strongly signiﬁcant (“both economically and statistically”) detrimental eﬀect on sales growth rate —
indicating that customers seemed to ignore the text in favor of the
incorrect summary information.
In some of these text-based studies, what was analyzed was not
sentiment per se but the degree of polarization (disagreement) among
a set of opinionated documents or, inspired in part by Pang and
Lee , the average probability of a sentence being subjective within
a given review . Ghose and Ipeirotis also take into account
the standard deviation for sentence subjectivity within a review, in
order to examine whether reviews containing a mix of subjective and
objective sentences seem to have diﬀerent eﬀects from reviews that are
mostly purely subjective or purely objective.
Some initially unexpected text eﬀects are occasionally reported. For
example, Archak et al. found that “amazing camera,” “excellent
camera,” and related phrases have a negative eﬀect on demand. They
hypothesize that consumers consider such phrases, especially if few
details are subsequently furnished in the review, to indicate hyperbole
and hence view the review itself as untrustworthy. Similarly, Archak
et al. and Ghose et al. discover that apparently positive comments like “decent quality” or “good packaging” also had a negative
eﬀect, and hypothesize that the very fact that many reviews contain
hyperbolic language mean that words like “decent” are interpreted as
These ﬁndings might seem pertinent to the distinction between the
prior polarity and the contextual polarity of terms and phrases, borrowing the terminology of Wilson et al. . Prior polarity refers to
the sentiment a term evokes in isolation, as opposed to the sentiment
the term evokes within a particular surrounding context; Polanyi and
Zaenen point out that identifying prior polarity alone may not suf-
ﬁce. With respect to this distinction, the status of the observations of
Archak et al. just mentioned is not entirely clear. The superlatives
6.2 Implications for Manipulation
(“amazing”) are clearly intended to convey positive sentiment regardless of whether the review authors actually managed to convince readers; that is, context is only needed to explain the economic eﬀect of
lowered sales, not the interpretation of the review itself. In the case
of words like “decent,” one could potentially make the case that the
prior orientation of such words is in fact neutral rather than positive;
but alternatively, one could argue instead that in a setting where many
reviews are highly enthusiastic, the contextual orientation of “decent”
is indeed diﬀerent from its prior orientation.
Interactions with Word of Mouth (WOM)
One factor that some studies point out is that the number of reviews,
positive or negative, may simply reﬂect “word of mouth,” so that in
some cases, what is really the underlying correlative (if any) of economic
impact is not the amount of positive feedback per se but merely the
amount of feedback in total. This explains why in some settings (but not
all), negative feedback is seen to “increase” sales: the increased “buzz”
brings more attention to the product (or perhaps simply indicates more
attention is being paid to the product, in which case it would not be
predictive per se).
Implications for Manipulation
Regarding the incentives for manipulation, it is diﬃcult to draw a conclusion one way or the other from the studies we have just examined.
One cautious way to read the results summarized in the previous
section is as follows. While there may be some economic beneﬁt in
some settings for a corporation to plant positive reviews or otherwise
attempt to use untoward means to manufacture an artiﬁcially inﬂated
reputation or suppress negative information, it seems that in general,
a great deal of eﬀort and resources would be required to do so for
perhaps fairly marginal returns. More work is clearly required, though;
as Bajari and Horta¸csu conclude, “There is still plenty of work to
be done to understand how market participants utilize the information
contained in the feedback forum system.” Surveying the state of the
art in this subject is beyond the scope of this survey; a fairly concise
Broader Implications
review of issues regarding online reputation systems may be found in
Dellarocas .
We would like to conclude, though, by pointing out a result that
indicates that even if illegitimate reviews do get through, opinionmining systems can still be valuable to consumers. Awerbuch and
Kleinberg study the “competitive collaborative learning” setting
in which some of the n users are assumed to be “Byzantine” (malicious, dishonest, coordinated, and able to eavesdrop on communications), and product or resource quality varies over time. The authors
formulate the product selection problem as a type of “multi-armed bandit” problem. They show the striking result that even if only a constant
fraction of users are honest and (unbeknownst to them) grouped into
k market segments such that all members of a block share the same
product preferences — with the implication that the recommendations
of an honest user may be useless to honest users in diﬀerent market
segments — then there is still an algorithm by which, in time polynomial in klog(n), the average regret per honest user is arbitrarily small
(assuming that the number of products or resources on oﬀer is O(n)).
Roughly speaking, the algorithm causes users to tend to raise the probability of getting recommendations from valuable sources. Thus, even
in the face of rather stiﬀodds and formidable adversaries, honest users
can — at least in theory — still get good advice from sentiment-analysis
Publicly Available Resources
Acquiring Labels for Data
One source of opinion, sentiment, and subjectivity labels is, of course,
manual annotation .
However, researchers in the ﬁeld have also managed to ﬁnd ways to
avoid manual annotation by leveraging pre-existing resources. A common technique is to use labels that have been manually assigned, but
not by the experimenters themselves; this explains why researchers in
opinion mining and sentiment analysis have taken advantage of Rotten
Tomatoes, Epinions, Amazon, and other sites where users furnish ratings along with their reviews. Some other noteworthy techniques are as
• Sentiment summaries can be gathered by treating the review
snippets that Rotten Tomatoes furnishes as one-sentence
summaries .
• Subjective vs. non-subjective texts on the same topic can be
gathered by selecting editorials versus non-editorial newswire
Publicly Available Resources
 or by selecting movie reviews versus plot summaries .
• If sentiment-oriented search engines already exist (one example used to be Opinmind), then one can issue topical
queries to such search engines and harvest the results to get
sentiment-bearing sentences more or less guaranteed to be
on-topic . (On the other hand, there is something circular about this approach, since it bootstraps oﬀof someone
else’s solution to the opinion-mining problem.)
• One might be able to derive aﬀect labels from emoticons
• Text polarity may be inferred from correlations with stockmarket behavior or other economic indicators .
• Viewpoint labels can be derived from images of party logos
that users display .
• Negative opinions can be gathered by assuming that when
one newsgroup post cites another, it is typically done to indicate negative sentiment toward the cited post . A more
reﬁned approach takes into account indications of “shouting,” such as text rendered all in capital letters .
One point to mention with regards to sites where users rate the
contributions of other users — such as the examples of Amazon and
Epinions mentioned above — is a potential bias toward positive scores
 , as we have mentioned above. In some
cases, this comes about because of sociological eﬀects. For example,
Pinch and Athanasiades , in a study of a music-oriented site called
ACIDplanet, found that various forces tend to cause users to give high
ratings to each other’s music. The users themselves refer to this phenomenon as “R=R” (review me and I will review you), among other,
less polite, names, and the ACIDplanet administrators introduced a
form of anonymous reviewing to avoid this issue in certain scenarios.
Thus, there is the question of whether one can trust the automatically determined labels that one is training one’s classiﬁers upon. (After
all, you often get what you pay for, as they say.) Indeed, Liu et al. 
essentially re-labeled their review-quality Amazon data due to concerns
7.1 Datasets
about bias, as discussed in Section 5.2.4. On the other hand, while
this phenomenon implies that reviewers may not always be sincere, we
hypothesize that this phenomenon does not greatly aﬀect the quality
of the authors’ meta-data labels at reﬂecting the intended sentiment of
the review itself. That is, we hypothesize that in many cases one can
still trust the review’s label, even if one does not trust the review.
An Annotated List of Datasets
The following list is in alphabetical order.
[registration and fee required]
The University of Glasgow distributes this 25GB TREC test collection, consisting of blog posts over a range of topics. Access
information is available at collections/
access to data.html. Included in the data set are “top blogs” that were
provided by Nielsen BuzzMetrics and “supplemented by the University
of Amsterdam” , and some spam blogs, also known as “splogs,”
that were planted in the corpus in order to simulate a more realistic setting. Assessments include relevance judgments and labels as to whether
posts contain relevant opinions and what the polarity of the opinions
was (positive, negative, or a mixture of both). Macdonald and Ounis
 give more details on the creation of the corpus and the collection’s
features, and include some comparison with another collection of blog
postings, the BlogPulse dataset (contact information can be found on
the following agreement form: but it may be out of date).
Congressional ﬂoor-debate transcripts
URL: 
This dataset, ﬁrst introduced in Thomas et al. , includes speeches
as individual documents together with:
• Automatically derived labels for whether the speaker supported or opposed the legislation discussed in the debate the
speech appears in, allowing for experiments with this kind of
sentiment analysis.
Publicly Available Resources
• Indications of which “debate” each speech comes from, allowing for consideration of conversational structure.
• Indications of by-name references between speakers, allowing for experiments on agreement classiﬁcation if one assigns
gold-standard agreement labels from the support/oppose
labels assigned to the pair of speakers in question.
• The edge weights and other information derived to create the
graphs used in Thomas et al. , facilitating implementation of alternative graph-based methods upon the graphs
constructed in that earlier work.
Cornell movie-review datasets
URL: 
These corpora, ﬁrst introduced in Pang and Lee , consist of
the following datasets, which include automatically derived labels.
• Sentiment polarity datasets:
— document-level: polarity dataset v2.0: 1000 positive
and 1000 negative processed reviews. (An earlier version of this dataset (v1.0) was ﬁrst introduced in Pang
et al. .)
— sentence-level: sentence polarity dataset v1.0: 5331
positive and 5331 negative processed sentences/
• Sentiment-scale datasets: scale dataset v1.0: a collection of
documents whose labels come from a rating scale.
• Subjectivity dataset v1.0: 5000 subjective and 5000 objective
processed sentences.
We should point out that the existence of the polarity-based
datasets does not indicate that the curators (i.e., us) believe that
reviews with middling ratings are not important to consider in practice
(indeed, the sentiment-scale corpora contain such documents). Rather,
the rationale in creating the polarity dataset was as follows. At the
7.1 Datasets
time the corpus creation was begun, the application of machine learning techniques to sentiment classiﬁcation was very new, and, as discussed in Section 3, it was natural to assume that the problem could
be very challenging to such techniques. Therefore, the polarity corpus was constructed to be as “easy” for text-categorization techniques
as possible: the documents fell into one of two well-separated and
size-balanced categories. The point was, then, to use this corpus as
a lens to study the relative diﬃculty of sentiment polarity classiﬁcation as compared to standard topic-based classiﬁcation, where twobalanced-class problems with well-separated categories pose very little
challenge.
A list of papers that use or report performance on the Cornell
movie-review datasets can be found at 
people/pabo/movie-review-data/otherexperiments.html.
Customer review datasets
URL: 
This dataset, introduced in Hu and Liu , consists of reviews
of ﬁve electronics products downloaded from Amazon and Cnet.
The sentences have been manually labeled as to whether an opinion is expressed, and if so, what feature from a pre-deﬁned list is
being evaluated. An addendum with nine products is also available
( and has
been utilized in recent work . The curator, Bing Liu, also distributes
a comparative-sentence dataset that is available by request.
Economining
URL: 
This site, hosted by the Stern School at New York University, consists
of three sets of data:
• Transactions and price premiums.
• Feedback postings for merchants at Amazon.com.
• Automatically derived sentiment scores for frequent evaluation phrases at Amazon.com.
Publicly Available Resources
These formed the basis for the work reported in Ghose et al. , which
focuses on interactions between sentiment, subjectivity, and economic
indicators.
French sentences
URL: 
This dataset, introduced in Bestgen et al. , consists of 702 sentences
from a Belgian–French newspaper, with labels assigned by ten judges
as to unpleasant, neutral or pleasant content, using a seven-point scale.
MPQA Corpus
URL: 
The MPQA Opinion Corpus contains 535 news articles from a wide
variety of news sources, manually annotated at the sentential and subsentential level for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, and so on). Wiebe et al. describes
the overall annotation scheme; Wilson et al. describes the contextual polarity annotations and an agreement study.
Multiple-aspect restaurant reviews
URL: 
The corpus, introduced in Snyder and Barzilay , consists of 4,488
reviews, both in raw-text and in feature-vector form. Each review gives
an explicit 1-to-5 rating for ﬁve diﬀerent aspects — food, ambiance, service, value, and overall experience — along with the text of the review
itself, all provided by the review author. A rating of ﬁve was the most
common over all aspects, and Snyder and Barzilay report that
30.5% of the 3,488 reviews in their randomly selected training set had
a rating of ﬁve for all ﬁve aspects, although no other tuple of ratings was
represented by more than 5% of the training set. The code used in Snyder and Barzilay is also distributed at the aforementioned URL.
The original source for the reviews was 
data from the same website was also used by Higashinaka et al. .
Multi-Domain Sentiment Dataset
URL: 
This dataset, introduced in Blitzer et al. , consists of product
7.2 Evaluation Campaigns
reviews from several diﬀerent product types taken from Amazon.com,
some with 1-to-5 star labels, some unlabeled.
NTCIR multilingual corpus
[registration required]
The corpus for the NTCIR 6 pilot task consists of news articles in
Japanese, Chinese, and English and formed the basis of the Opinion
Analysis Task at NTCIR6 . The training data contains annotations
regarding opinion holders, the opinions held by opinion holder, and
sentiment polarity, as well as relevance information for a set of predetermined topics.
The corpus of the NTCIR Multilingual Opinion-Analysis Task
(MOAT) is drawn from Japanese, Chinese, and English blogs.
Review-search results sets
URL: 
This corpus, used by Pang and Lee , consists of the top 20 results
returned by the Yahoo! search engine in response to each of a set of
69 queries containing the word “review.” The queries were drawn from
the publicly available list of real MSN users’ queries released for the
2005 KDD Cup competition ; the KDD data itself is available
at 
The search-engine results in the corpus are annotated as to whether
they are subjective or not. Note that “sales pitches” were marked objective on the premise that they represent biased reviews that users might
wish to avoid seeing.
Evaluation Campaigns
TREC Opinion-Related Competitions
“TREC-BLOG”
 
BLOG/, is a useful source of information on the competitions sketched
TREC 2006 Blog Track.
TREC 2006 involved a Blog track, with an
opinion retrieval task designed precisely to focus on the opinionated
character that many blogs have: participating systems had to retrieve
Publicly Available Resources
blog posts expressing an opinion about a speciﬁed topic. Fourteen
groups participated; Ounis et al. give an overview of the results.
Some ﬁndings are as follows. With respect to performance on opinion
detection, the participating systems seemed to fall into two groups.
Opinion-detection ability and relevance-determination ability seemed
to be strongly correlated. While the best systems were about equally
good at detecting negative sentiment as positive sentiment, systems
performing at the median seemed to be a bit more eﬀective at locating documents with negative sentiment. Most participants followed a
pipelined approach, where ﬁrst topic relevance was tackled, and then
opinion detection was applied upon the results. Perhaps the most surprising observation was that the organizers discovered that it was possible to achieve very good relative performance by omitting the second
phase of the pipeline; but we take heart in the fact that the ﬁeld is still
relatively young and has room to grow and mature.
TREC 2007 Blog Track.
The TREC 2007 Blog track retained the
opinion retrieval task and instituted determining the sentiment status
(positive, negative, or mixed) of the retrieved opinions as a subtask.
The 2007 and 2006 Blog Track results are analyzed in Ounis et al.
 . They found that lexicon-based approaches — either where the
discriminativeness of terms was determined on labeled training data
or where the terms were manually compiled — constituted the main
eﬀective approaches.
TREC 2008 Blog Track.
In the TREC 2008 Blog track, the polarityidentiﬁcation problem was re-posed as one of ranking of positivepolarity retrieved documents by degree of positivity, and, similarly,
ranking of negative-polarity retrieved documents by degree of negativity. (“Mixed opinionated documents” were not to be included in these
rankings.)
NTCIR Opinion-Related Competitions
The National Institute of Informatics (NII) runs annual meetings codenamed NTCIR (NII Test Collection for Information Retrieval Systems).
Opinion analysis was featured at an NTCIR-5 workshop, and served as
a pilot task at NTCIR-6 and a full-blown task at NTCIR-7.
7.2 Evaluation Campaigns
newswire documents in Chinese, Japanese, and English; the organizers describe this as “what we believe to be the ﬁrst multilingual opinion analysis data set over comparable data” . The four constituent
tasks, intentionally designed to be fairly simple so as to encourage participation from many groups, were as follows:
• Detection of opinionated sentences.
• Detection of opinion holders.
• (optional) Polarity labeling of opinionated sentences as positive, negative, or neutral.
• (optional) Detection of sentences relevant to a given topic.
Due to variation in annotator labelings, two evaluation standards were
deﬁned. In the strict evaluation, an answer is considered correct if all
three annotators agreed on it. In the lenient evaluation, only a majority
(i.e., two) of the annotators were required to agree with an answer for
it to be considered correct.
Seki et al. give an overview and the results of this evaluation
exercise, noting that diﬀerences between languages make direct comparison diﬃcult, especially since precision and recall were deﬁned (slightly)
diﬀerently across languages. A shortened version of this overview also
exists .
NTCIR-7 Multilingual opinion analysis task (MOAT), 2008.
Subsequent to the NTCIR-6 pilot task, a new dataset was selected, drawn
from blogs in Japanese, traditional and simpliﬁed Chinese, and English;
according to the organizers, “We plan to select and balance useful topics for opinion mining researchers, such as topics concerning product
reviews, movie reviews, and so on.” This exercise involves six subtasks:
• Detection of opinionated sentences and opinion fragments
within opinionated sentences.
• Polarity labeling of opinion fragments as positive, negative
or neutral.
• (optional) Strength labeling of opinion fragments as very
weak, average, or very strong.
Publicly Available Resources
• (optional) Detection of opinion holders.
• (optional) Detection of opinion targets.
• (optional) Detection of sentences that are relevant to a given
As in the previous competition, both strict and lenient evaluation standards are to be applied.
OpQA Corpus
[available by request]
Stoyanov et al. describes the construction of this corpus, which is a
collection of opinion questions and answers together with 98 documents
selected from the MPQA dataset.
Lexical Resources
The following list is in alphabetical order.
General Inquirer
URL: 
This site provides entry-points to various resources associated with the
General Inquirer . Included are manually-classiﬁed terms labeled
with various types of positive or negative semantic orientation, and
words having to do with agreement or disagreement.
NTU Sentiment Dictionary
[registration required]
This sentiment dictionary listing the polarities of many Chinese
words was developed by a combination of automated and manual
means . A registration form for acquiring it is available at
 
OpinionFinder’s Subjectivity Lexicon
URL: 
The list of subjectivity clues that is part of OpinionFinder is available
for download. These clues were compiled from several sources, representing several years of eﬀort, and were used in Wilson et al. .
7.4 Tutorials, Bibliographies, and Other References
SentiWordnet
URL: 
SentiWordnet is a lexical resource for opinion mining. Each synset
of WordNet , a publicly available thesaurus-like resource, is assigned
one of three sentiment scores — positive, negative, or objective — where
these scores were automatically generated using a semi-supervised
method described in Esuli and Sebastiani .
Taboada and Grieve’s Turney adjective list
[available through the Yahoo! sentimentAI group]
Reported are the semantic-orientation values according to the method
proposed by Turney for 1700 adjectives.
Tutorials, Bibliographies, and Other References
available at 
Slides for Janyce Wiebe’s tutorial, “Semantics, opinion, and sentiment in text,” at the EUROLAN 2007 Summmer School are available at 
eurolan07wiebe.ppt.
The following are online bibliographies that contain information in
BibTeX format:
• http: / / www.cs.cornell.edu / home / llee / opinion-miningsentiment-analysis-survey.html, the main website for this
• http: / / liinwww.ira.uka.de/bibliography/Misc/Sentiment.
html, maintained by Andrea Esuli,
• http: / / research.microsoft.com / ∼jtsun / OpinionMining
PaperList. html, maintained by Jian-Tao Sun,
• http: / / www.cs.pitt.edu / ∼wiebe / pubs / papers / EURO
LAN07/eurolan07bib.html with actual .bib ﬁle at http: / /
Publicly Available Resources
www.cs.pitt.edu / ∼wiebe / pubs / papers / EUROLAN07/
eurolan07.bib, maintained by Janyce Wiebe.
Esuli and Wiebe’s sites have additional search capabilities.
Members of the Yahoo! group “sentimentAI” ( 
yahoo.com/group/SentimentAI/) have access to the resources that
have been contributed there (such as some links to corpora and
papers) and are subscribed to the associated mailing list. Joining
Concluding Remarks
When asked how he knew a piece was ﬁnished, he
responded, “When the dinner bell rings.”
— apocryphal anecdote about Alexander Calder
Our goal in this survey has been to cover techniques and approaches
that promise to directly enable opinion-oriented information-seeking
systems, and to convey to the reader a sense of our excitement about the
intellectual richness and breadth of the area. We very much encourage
the reader to take up the many open challenges that remain, and hope
we have provided some resources that will prove helpful in this regard.
On the topic of resources: we have already indicated above that
the bibliographic database used in this survey is publicly available. In
fact, the URL mentioned above, 
opinion-mining-sentiment-analysis-survey.html, is our personally maintained homepage for this survey. Any subsequent editions or versions of
this survey that may be produced, or related news, will be announced
1 Indeed, we have vague aspirations to producing a “director’s cut” one day. We certainly
have accumulated some number of outtakes: we did not manage to ﬁnd a way to work
Concluding Remarks
Speaking of resources, we have drawn considerably on those of many
others during the course of this work. We thus have a number of sincere
acknowledgments to make.
This survey is based upon work supported in part by the National
Science Foundation under grant no. IIS-0329064, a Cornell University Provost’s Award for Distinguished Scholarship, a Yahoo! Research
Alliance gift, and an Alfred P. Sloan Research Fellowship. Any opinions, ﬁndings, and conclusions or recommendations expressed are those
of the authors and do not necessarily reﬂect the views or oﬃcial policies, either expressed or implied, of any sponsoring institutions, the US
government, or any other entity.
We would like to wholeheartedly thank the anonymous referees, who
provided outstanding feedback astonishingly quickly. Their insights
contributed immensely to the ﬁnal form of this survey on many levels. It is hard to describe our level of gratitude to them for their time
and their wisdom, except to say this: we have, in various capacities, seen
many examples of reviewing in the community, but this is the best we
have ever encountered. We also thank Eric Breck for his careful reading
of and commentary on portions of this survey. All remaining errors and
faults are, of course, our own.
We are also very thankful to Fabrizio Sebastiani, for all of his editorial guidance and care. We owe him a great debt. We also greatly
appreciate the help we received from Jamie Callan, who, along with
Fabrizio, serves as Editor in Chief of the Foundations and Trends in
Information Retrieval series, and James Finlay, of Now Publishers, the
publisher of this series.
Finally, a number of unexpected health problems arose in our families during the writing of this survey. Despite this, it was our families
who sustained us with their cheerful and unlimited support (on many
levels), not the other way around. Thus — to end on a sentimental
note — this work is dedicated to them.
some variant of “Once more, with feeling” into the title, or to ﬁnd a place for the heading
“Sentiment of a woman,” or to formally prove a potential undecidability result for subjectivity detection (Jon Kleinberg, personal communication) based on reviews of Brotherhood
of the Wolf (“it’s the best darned French werewolf kung-fu movie I’ve ever seen”).
References
 A. Abbasi, “Aﬀect intensity analysis of dark web forums,” in Proceedings of
Intelligence and Security Informatics (ISI), pp. 282–288, 2007.
 L. A. Adamic and N. Glance, “The political blogosphere and the 2004 U.S.
election: Divided they blog,” in Proceedings of LinkKDD, 2005.
 A. Agarwal and P. Bhattacharyya, “Sentiment analysis: A new approach for
eﬀective use of linguistic knowledge and exploiting similarities in a set of
documents to be classiﬁed,” in Proceedings of the International Conference on
Natural Language Processing (ICON), 2005.
 R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu, “Mining newsgroups using
networks arising from social behavior,” in Proceedings of WWW, pp. 529–535,
 E. M. Airoldi, X. Bai, and R. Padman, “Markov blankets and meta-heuristic
search: Sentiment extraction from unstructured text,” Lecture Notes in Computer Science, vol. 3932 (Advances in Web Mining and Web Usage Analysis),
pp. 167–187, 2006.
 G. A. Akerlof, “The market for “Lemons”: Quality uncertainty and the market
mechanism,” The Quarterly Journal of Economics, vol. 84, pp. 488–500, 1970.
 S. M. Al Masum, H. Prendinger, and M. Ishizuka, “SenseNet: A linguistic tool
to visualize numerical-valence based sentiment of textual data,” in Proceedings of the International Conference on Natural Language Processing (ICON),
pp. 147–152, 2007. (Poster paper).
 J. Allan, “Introduction to topic detection and tracking,” in Topic Detection
and Tracking: Event-based Information Organization, (J. Allan, ed.), pp. 1–16,
Norwell, MA, USA: Kluwer Academic Publishers, ISBN 0-7923-7664-1, 2002.
References
 C. O. Alm, D. Roth, and R. Sproat, “Emotions from text: Machine learning
for text-based emotion prediction,” in Proceedings of the Human Language
Technology Conference and the Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP), 2005.
 A. Anagnostopoulos, A. Z. Broder, and D. Carmel, “Sampling search-engine
results,” World Wide Web, vol. 9, pp. 397–429, 2006.
 R. K. Ando and T. Zhang, “A framework for learning predictive structures from multiple tasks and unlabeled data,” Journal of Machine Learning
Research, vol. 6, pp. 1817–1853, 2005.
 A. Andreevskaia and S. Bergler, “Mining WordNet for a fuzzy sentiment: Sentiment tag extraction from WordNet glosses,” in Proceedings of the European
Chapter of the Association for Computational Linguistics (EACL), 2006.
 W. Antweiler and M. Z. Frank, “Is all that talk just noise? The information content of internet stock message boards,” Journal of Finance, vol. 59,
pp. 1259–1294, 2004.
 N. Archak, A. Ghose, and P. Ipeirotis, “Show me the money! Deriving the
pricing power of product features by mining consumer reviews,” in Proceedings
of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD), 2007.
 S. Argamon, ed., Proceedings of the IJCAI Workshop on DOING IT WITH
STYLE: Computational Approaches to Style Analysis and Synthesis. 2003.
 S. Argamon, J. Karlgren, and J. G. Shanahan, eds., Proceedings of the SIGIR
Workshop on Stylistic Analysis of Text For Information Access. ACM, 2005.
 S. Argamon, J. Karlgren, and O. Uzuner, eds., Proceedings of the SIGIR Workshop on Stylistics for Text Retrieval in Practice. ACM, 2006.
 S. Argamon-Engelson, M. Koppel, and G. Avneri, “Style-based text categorization: What newspaper am I reading?” in Proceedings of the AAAI Workshop on Text Categorization, pp. 1–4, 1998.
 Y. Attali and J. Burstein, “Automated essay scoring with e-rater v.2,” Journal
of Technology, Learning, and Assessment, vol. 26, February 2006.
 A. Aue and M. Gamon, “Automatic identiﬁcation of sentiment vocabulary:
Exploiting low association with known sentiment terms,” in Proceedings of
the ACL Workshop on Feature Engineering for Machine Learning in Natural
Language Processing, 2005.
 A. Aue and M. Gamon, “Customizing sentiment classiﬁers to new domains:
A case study,” in Proceedings of Recent Advances in Natural Language Processing (RANLP), 2005.
 B. Awerbuch and R. Kleinberg, “Competitive collaborative learning,” in Proceedings of the Conference on Learning Theory (COLT), pp. 233–248, 2005.
(Journal version to appear in Journal of Computer and System Sciences, special issue on computational learning theory).
 P. Bajari and A. Horta¸csu, “The winner’s curse, reserve prices, and endogenous
entry: Empirical insights from eBay auctions,” RAND Journal of Economics,
vol. 34, pp. 329–355, 2003.
 P. Bajari and A. Horta¸csu, “Economic insights from internet auctions,” Journal of Economic Literature, vol. 42, pp. 457–486, 2004.
References
 C. F. Baker, C. J. Fillmore, and J. B. Lowe, “The Berkeley Framenet Project,”
in Proceedings of COLING/ACL, 1998.
 A. Banﬁeld, Unspeakable Sentences: Narration and Representation in the Language of Fiction. Routledge and Kegan Paul, 1982.
 M. Bansal, C. Cardie, and L. Lee, “The power of negative thinking: Exploiting
label disagreement in the min-cut classiﬁcation framework,” in Proceedings of
the International Conference on Computational Linguistics (COLING), 2008.
(Poster paper).
 R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and
I. Szpektor, “The second PASCAL recognising textual entailment challenge,”
in Proceedings of the Second PASCAL Challenges Workshop on Recognising
Textual Entailment, 2006.
 R. Barzilay and L. Lee, “Learning to paraphrase: An unsupervised approach
using multiple-sequence alignment,” in Proceedings of the Joint Human Language Technology/North American Chapter of the ACL Conference (HLT-
NAACL), pp. 16–23, 2003.
 R. Barzilay and K. McKeown, “Extracting paraphrases from a parallel
corpus,” in Proceedings of the Association for Computational Linguistics
(ACL), pp. 50–57, 2001.
 S. Basuroy, S. Chatterjee, and S. A. Ravid, “How critical are critical reviews?
The box oﬃce eﬀects of ﬁlm critics, star power and budgets,” Journal of
Marketing, vol. 67, pp. 103–117, 2003.
 M. Bautin, L. Vijayarenu, and S. Skiena, “International sentiment analysis for
news and blogs,” in Proceedings of the International Conference on Weblogs
and Social Media (ICWSM), 2008.
 P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan, “Exploring sentiment summarization,” in Proceedings of the AAAI Spring Symposium on
Exploring Attitude and Aﬀect in Text, AAAI technical report SS-04-07, 2004.
 F. Benamara, C. Cesarano, A. Picariello, D. Reforgiato, and V. S. Subrahmanian, “Sentiment analysis: Adjectives and adverbs are better than adjectives
alone,” in Proceedings of the International Conference on Weblogs and Social
Media (ICWSM), 2007. (Short paper).
 J. Berger, A. T. Sorensen, and S. J. Rasmussen, “Negative publicity:
When is negative a positive?,” Manuscript. PDF ﬁle’s last modiﬁcation
date: October 16, 2007, URL: 
Negative Publicity.pdf, 2007.
 Y. Bestgen, C. Fairon, and L. Kerves, “Un barom`etre aﬀectif eﬀectif: Corpus
de r´ef´erence et m´ethode pour d´eterminer la valence aﬀective de phrases,” in
Journ´ees internationales d’analyse statistique des donn´es textuelles (JADT),
pp. 182–191, 2004.
 S. Bethard, H. Yu, A. Thornton, V. Hatzivassiloglou, and D. Jurafsky, “Automatic extraction of opinion propositions and their holders,” in Proceedings
of the AAAI Spring Symposium on Exploring Attitude and Aﬀect in Text,
 D. Biber, Variation Across Speech and Writing. Cambridge University Press,
References
 D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet allocation,” Journal
of Machine Learning Research, vol. 3, pp. 993–1022, 2003.
 J. Blitzer, M. Dredze, and F. Pereira, “Biographies, Bollywood, boom-boxes
and blenders: Domain adaptation for sentiment classiﬁcation,” in Proceedings
of the Association for Computational Linguistics (ACL), 2007.
 S. R. K. Branavan, H. Chen, J. Eisenstein, and R. Barzilay, “Learning
document-level semantic properties from free-text annotations,” in Proceedings of the Association for Computational Linguistics (ACL), 2008.
 E. Breck and C. Cardie, “Playing the telephone game: Determining the hierarchical structure of perspective and speech expressions,” in Proceedings of
the International Conference on Computational Linguistics (COLING), 2004.
 E. Breck, Y. Choi, and C. Cardie, “Identifying expressions of opinion in context,” in Proceedings of the International Joint Conference on Artiﬁcial Intelligence (IJCAI), Hyderabad, India, 2007.
 S. Brin and L. Page, “The anatomy of a large-scale hypertextual web search
engine,” in Proceedings of the 7th International World Wide Web Conference,
pp. 107–117, 1998.
 R. F. Bruce and J. M. Wiebe, “Recognizing subjectivity: A case study in
manual tagging,” Natural Language Engineering, vol. 5, 1999.
 J. K. Burgoon, J. P. Blair, T. Qin, and J. F. Nunamaker, Jr., “Detecting deception through linguistic analysis,” in Proceedings of Intelligence and Security
Informatics (ISI), number 2665 in Lecture Notes in Computer Science, p. 958,
 L. Cabral and A. Horta¸csu, “The dynamics of seller reputation: Theory and
evidence from eBay,” Working Paper, downloaded version revised in March,
 
Hortacsu Mar06.pdf, 2006.
 J. Carbonell, Subjective Understanding: Computer Models of Belief Systems.
PhD thesis, Yale, 1979.
 C. Cardie, “Empirical methods in information extraction,” AI Magazine,
vol. 18, pp. 65–79, 1997.
 C. Cardie, C. Farina, T. Bruce, and E. Wagner, “Using natural language
processing to improve eRulemaking,” in Proceedings of Digital Government
Research (dg.o), 2006.
 C. Cardie, J. Wiebe, T. Wilson, and D. Litman, “Combining low-level and
summary representations of opinions for multi-perspective question answering,” in Proceedings of the AAAI Spring Symposium on New Directions in
Question Answering, pp. 20–27, 2003.
 G. Carenini, R. Ng, and A. Pauls, “Multi-document summarization of evaluative text,” in Proceedings of the European Chapter of the Association for
Computational Linguistics (EACL), pp. 305–312, 2006.
 G. Carenini, R. T. Ng, and A. Pauls, “Interactive multimedia summaries of
evaluative text,” in Proceedings of Intelligent User Interfaces (IUI), pp. 124–
131, ACM Press, 2006.
 D. Cartwright and F. Harary, “Structural balance: A generalization of Heider’s
theory,” Psychological Review, vol. 63, pp. 277–293, 1956.
References
 P. Chaovalit and L. Zhou, “Movie review mining: A comparison between
supervised and unsupervised classiﬁcation approaches,” in Proceedings of the
Hawaii International Conference on System Sciences (HICSS), 2005.
 P.-Y. S. Chen, S.-Y. Wu, and J. Yoon, “The impact of online recommendations
and consumer feedback on sales,” in International Conference on Information
Systems (ICIS), pp. 711–724, 2004.
 Y. Chen and J. Xie, “Online consumer review: Word-of-mouth as a new
element of marketing communication mix,” Management Science, vol. 54,
pp. 477–491, 2008.
 P. Chesley, B. Vincent, L. Xu, and R. Srihari, “Using verbs and adjectives to
automatically classify blog sentiment,” in AAAI Symposium on Computational
Approaches to Analysing Weblogs (AAAI-CAAW), pp. 27–29, 2006.
 J. A. Chevalier and D. Mayzlin, “The eﬀect of word of mouth on sales: Online
book reviews,” Journal of Marketing Research, vol. 43, pp. 345–354, August
 Y. Choi, E. Breck, and C. Cardie, “Joint extraction of entities and relations for
opinion recognition,” in Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), 2006.
 Y. Choi, C. Cardie, E. Riloﬀ, and S. Patwardhan, “Identifying sources of opinions with conditional random ﬁelds and extraction patterns,” in Proceedings of
the Human Language Technology Conference and the Conference on Empirical
Methods in Natural Language Processing (HLT/EMNLP), 2005.
 E. K. Clemons, G. Gao, and L. M. Hitt, “When online reviews meet hyperdiﬀerentiation: A study of the craft beer industry,” Journal of Management
Information Systems, vol. 23, pp. 149–171, 2006.
 comScore/the Kelsey group, “Online consumer-generated reviews have signiﬁcant impact on oﬄine purchase behavior,” Press Release, 
comscore.com/press/release.asp?press=1928, November 2007.
 J. G. Conrad and F. Schilder, “Opinion mining in legal blogs,” in Proceedings
of the International Conference on Artiﬁcial Intelligence and Law (ICAIL),
pp. 231–236, New York, NY, USA: ACM, 2007.
 W. B. Croft and J. Laﬀerty, eds., Language modeling for information retrieval.
Number 13 in the Information Retrieval Series. Kluwer/Springer, 2003.
 S. Das and M. Chen, “Yahoo! for Amazon: Extracting market sentiment from
stock message boards,” in Proceedings of the Asia Paciﬁc Finance Association
Annual Conference (APFA), 2001.
 S. R. Das and M. Y. Chen, “Yahoo! for Amazon: Sentiment extraction from
small talk on the Web,” Management Science, vol. 53, pp. 1375–1388, 2007.
 S. R. Das, P. Tufano, and F. de Asis Martinez-Jerez, “eInformation: A clinical
study of investor discussion and sentiment,” Financial Management, vol. 34,
pp. 103–137, 2005.
 K. Dave, S. Lawrence, and D. M. Pennock, “Mining the peanut gallery: Opinion extraction and semantic classiﬁcation of product reviews,” in Proceedings
of WWW, pp. 519–528, 2003.
 S. David and T. J. Pinch, “Six degrees of reputation: The use and abuse
of online review and recommendation systems,” First Monday, July 2006.
(Special Issue on Commercial Applications of the Internet).
References
 C. Dellarocas, “The digitization of word-of-mouth: Promise and challenges
of online reputation systems,” Management Science, vol. 49, pp. 1407–1424,
2003. (Special issue on e-business and management science).
 C. Dellarocas, X. Zhang, and N. F. Awad, “Exploring the value of online
product ratings in revenue forecasting: The case of motion pictures,” Journal
of Interactive Marketing, vol. 21, pp. 23–45, 2007.
 A. Devitt and K. Ahmad, “Sentiment analysis in ﬁnancial news: A cohesionbased approach,” in Proceedings of the Association for Computational Linguistics (ACL), pp. 984–991, 2007.
 M. Dewally, “Internet investment advice: Investing with a rock of salt,” Financial Analysts Journal, vol. 59, pp. 65–77, July/August 2003.
 M. Dewally and L. Ederington, “Reputation, certiﬁcation, warranties, and
information as remedies for seller-buyer information asymmetries: Lessons
from the online comic book market,” Journal of Business, vol. 79, pp. 693–730,
March 2006.
 S. Dewan and V. Hsu, “Adverse selection in electronic markets: Evidence from
online stamp auctions,” Journal of Industrial Economics, vol. 52, pp. 497–516,
December 2004.
 D. W. Diamond, “Reputation acquisition in debt markets,” Journal of Political Economy, vol. 97, pp. 828–862, 1989.
 X. Ding, B. Liu, and P. S. Yu, “A holistic lexicon-based approach to opinion mining,” in Proceedings of the Conference on Web Search and Web Data
Mining (WSDM), 2008.
 L. Dini and G. Mazzini, “Opinion classiﬁcation through information extraction,” in Proceedings of the Conference on Data Mining Methods and
Engineering,
pp. 299–310, 2002.
 W. Duan, B. Gu, and A. B. Whinston, “Do online reviews matter? —
An empirical investigation of panel data,” Social Science Research Network
(SSRN) Working Paper Series, version as of
January, 2005.
 D. H. Eaton, “Valuing information: Evidence from guitar auctions on eBay,”
Journal of Applied Economics and Policy, vol. 24, pp. 1–19, 2005.
 D. H. Eaton, “The impact of reputation timing and source on auction outcomes,” The B. E. Journal of Economic Analysis and Policy, vol. 7, 2007.
 M. Efron, “Cultural orientation: Classifying subjective documents by cociation [sic] analysis,” in Proceedings of the AAAI Fall Symposium on Style and
Meaning in Language, Art, Music, and Design, pp. 41–48, 2004.
 K. Eguchi and V. Lavrenko, “Sentiment retrieval using generative models,”
in Proceedings of the Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 345–354, 2006.
 K. Eguchi and C. Shah, “Opinion retrieval experiments using generative models: Experiments for the TREC 2006 blog track,” in Proceedings of TREC,
 P. Ekman, Emotion in the Human Face. Cambridge University Press, Second
ed., 1982.
References
 J. Eliashberg and S. M. Shugan, “Film critics: Inﬂuencers or predictors?,”
Journal of Marketing, vol. 61, pp. 68–78, April 1997.
 C. Engstr¨om, Topic Dependence in Sentiment Classiﬁcation. Master’s thesis,
University of Cambridge, 2004.
 A. Esuli and F. Sebastiani, “Determining the semantic orientation of terms
through gloss analysis,” in Proceedings of the ACM SIGIR Conference on
Information and Knowledge Management (CIKM), 2005.
 A. Esuli and F. Sebastiani, “Determining term subjectivity and term orientation for opinion mining,” in Proceedings of the European Chapter of the
Association for Computational Linguistics (EACL), 2006.
 A. Esuli and F. Sebastiani, “SentiWordNet: A publicly available lexical
resource for opinion mining,” in Proceedings of Language Resources and Evaluation (LREC), 2006.
 A. Esuli and F. Sebastiani, “PageRanking WordNet synsets: An application
to opinion mining,” in Proceedings of the Association for Computational Linguistics (ACL), 2007.
 D. K. Evans, L.-W. Ku, Y. Seki, H.-H. Chen, and N. Kando, “Opinion analysis
across languages: An overview of and observations from the NTCIR6 opinion
analysis pilot task,” in Proceedings of the Workshop on Cross-Language Information Processing, vol. 4578 (Applications of Fuzzy Sets Theory) of Lecture
Notes in Computer Science, pp. 456–463, 2007.
 A. Fader, D. R. Radev, M. H. Crespin, B. L. Monroe, K. M. Quinn, and
M. Colaresi, “MavenRank: Identifying inﬂuential members of the US senate
using lexical centrality,” in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2007.
 C. Fellbaum, ed., Wordnet: An Electronic Lexical Database. MIT Press, 1998.
 D. Feng, E. Shaw, J. Kim, and E. Hovy, “Learning to detect conversation
focus of threaded discussions,” in Proceedings of the Joint Human Language
Technology/North American Chapter of the ACL Conference (HLT-NAACL),
pp. 208–215, 2006.
 A. Finn and N. Kushmerick, “Learning to classify documents according to
genre,” Journal of the American Society for Information Science and Technology (JASIST), vol. 7, 2006. .
References
 G. Forman, “An extensive empirical study of feature selection metrics for text
classiﬁcation,” Journal of Machine Learning Research, vol. 3, pp. 1289–1305,
 T. Fukuhara, H. Nakagawa, and T. Nishida, “Understanding sentiment of
people from news articles: Temporal sentiment analysis of social events,” in
Proceedings of the International Conference on Weblogs and Social Media
(ICWSM), 2007.
 M. Gamon, “Sentiment classiﬁcation on customer feedback data: Noisy data,
large feature vectors, and the role of linguistic analysis,” in Proceedings of the
International Conference on Computational Linguistics (COLING), 2004.
 M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger, “Pulse: Mining customer opinions from free text,” in Proceedings of the International Symposium
on Intelligent Data Analysis (IDA), number 3646 in Lecture Notes in Computer Science, pp. 121–132, 2005.
 R. Ghani, K. Probst, Y. Liu, M. Krema, and A. Fano, “Text mining for product
attribute extraction,” SIGKDD Explorations Newsletter, vol. 8, pp. 41–48,
 A. Ghose and P. G. Ipeirotis, “Designing novel review ranking systems: Predicting usefulness and impact of reviews,” in Proceedings of the International
Conference on Electronic Commerce (ICEC), 2007. (Invited paper).
 A. Ghose, P. G. Ipeirotis, and A. Sundararajan, “Opinion mining using econometrics: A case study on reputation systems,” in Proceedings of the Association
for Computational Linguistics (ACL), 2007.
 N. Godbole, M. Srinivasaiah, and S. Skiena, “Large-scale sentiment analysis
for news and blogs,” in Proceedings of the International Conference on Weblogs
and Social Media (ICWSM), 2007.
 A. B. Goldberg and X. Zhu, “Seeing stars when there aren’t many
stars: Graph-based semi-supervised learning for sentiment categorization,” in
TextGraphs: HLT/NAACL Workshop on Graph-based Algorithms for Natural
Language Processing, 2006.
 A. B. Goldberg, X. Zhu, and S. Wright, “Dissimilarity in graph-based semisupervised classiﬁcation,” in Artiﬁcial Intelligence and Statistics (AISTATS),
 S. Greene, Spin: Lexical Semantics, Transitivity, and the Identiﬁcation of
Implicit Sentiment. PhD thesis, University of Maryland, 2007.
 G. Grefenstette, Y. Qu, J. G. Shanahan, and D. A. Evans, “Coupling
niche browsers and aﬀect analysis for an opinion mining application,” in
Proceedings of Recherche d’Information Assist´ee par Ordinateur (RIAO),
 M. L. Gregory, N. Chinchor, P. Whitney, R. Carter, E. Hetzler, and A. Turner,
“User-directed sentiment analysis: Visualizing the aﬀective content of documents,” in Proceedings of the Workshop on Sentiment and Subjectivity in Text,
pp. 23–30, Sydney, Australia, July 2006.
 B. Gu, P. Konana, A. Liu, B. Rajagopalan, and J. Ghosh, “Predictive value of
stock message board sentiments,” McCombs Research Paper No. IROM-11-06,
version dated November, 2006.
References
 R. V. Guha, R. Kumar, P. Raghavan, and A. Tomkins, “Propagation of trust
and distrust,” in Proceedings of WWW, pp. 403–412, 2004.
 B. A. Hagedorn, M. Ciaramita, and J. Atserias, “World knowledge in broadcoverage information ﬁltering,” in Proceedings of the ACM Special Interest
Group on Information Retrieval (SIGIR), 2007. (Poster paper).
 J. T. Hancock, L. Curry, S. Goorha, and M. Woodworth, “Automated linguistic analysis of deceptive and truthful synchronous computer-mediated communication,” in Proceedings of the Hawaii International Conference on System
Sciences (HICSS), p. 22c, 2005.
 L. Hankin, “The eﬀects of user reviews on online purchasing behavior
categories,”
Information,
 
ﬁles/lhankin report.pdf, May 2007.
 V. Hatzivassiloglou and K. McKeown, “Predicting the semantic orientation of
adjectives,” in Proceedings of the Joint ACL/EACL Conference, pp. 174–181,
 V. Hatzivassiloglou and J. Wiebe, “Eﬀects of adjective orientation and gradability on sentence subjectivity,” in Proceedings of the International Conference on Computational Linguistics (COLING), 2000.
 M. Hearst, “Direction-based text interpretation as an information access
reﬁnement,” in Text-Based Intelligent Systems, (P. Jacobs, ed.), pp. 257–274,
Lawrence Erlbaum Associates, 1992.
 R. Higashinaka, M. Walker, and R. Prasad, “Learning to generate naturalistic
utterances using reviews in spoken dialogue systems,” ACM Transactions on
Speech and Language Processing (TSLP), 2007.
 P. Hitlin and L. Rainie, “The use of online reputation and rating systems,”
Pew Internet & American Life Project Memo, October 2004.
 T. Hoﬀman, “Online reputation management is hot — but is it ethical?”
Computerworld, February 2008.
 T. Hofmann, “Probabilistic latent semantic indexing,” in Proceedings of
SIGIR, pp. 50–57, 1999.
 D. Hopkins and G. King, “Extracting systematic social science meaning from
Manuscript available at 
2007 version was the one most recently consulted, 2007.
 J. A. Horrigan, “Online shopping,” Pew Internet & American Life Project
Report, 2008.
 D. Houser and J. Wooders, “Reputation in auctions: Theory, and evidence from eBay,” Journal of Economics and Management Strategy, vol. 15,
pp. 252–369, 2006.
 M. Hu and B. Liu, “Mining and summarizing customer reviews,” in Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD), pp. 168–177, 2004.
 M. Hu and B. Liu, “Mining opinion features in customer reviews,” in Proceedings of AAAI, pp. 755–760, 2004.
 M. Hu, A. Sun, and E.-P. Lim, “Comments-oriented blog summarization
by sentence extraction,” in Proceedings of the ACM SIGIR Conference on
References
Information and Knowledge Management (CIKM), pp. 901–904, 2007. (Poster
 N. Hu, P. A. Pavlou, and J. Zhang, “Can online reviews reveal a product’s true
quality?: Empirical ﬁndings and analytical modeling of online word-of-mouth
communication,” in Proceedings of Electronic Commerce (EC), pp. 324–330,
USA, New York, NY: ACM, 2006.
 A. Huettner and P. Subasic, “Fuzzy typing for document management,” in
ACL 2000 Companion Volume: Tutorial Abstracts and Demonstration Notes,
pp. 26–27, 2000.
 M. Hurst and K. Nigam, “Retrieving topical sentiments from online document
collections,” in Document Recognition and Retrieval XI, pp. 27–34, 2004.
 C. Jacquemin, Spotting and Discovering Terms through Natural Language Processing. MIT Press, 2001.
 G. Jin and A. Kato, “Price, quality and reputation: Evidence from an online
ﬁeld experiment,” The RAND Journal of Economics, vol. 37, 2006.
 X. Jin, Y. Li, T. Mah, and J. Tong, “Sensitive webpage classiﬁcation for
content advertising,” in Proceedings of the International Workshop on Data
Mining and Audience Intelligence for Advertising, 2007.
 N. Jindal and B. Liu, “Identifying comparative sentences in text documents,”
in Proceedings of the ACM Special Interest Group on Information Retrieval
(SIGIR), 2006.
 N. Jindal and B. Liu, “Mining comparative sentences and relations,” in Proceedings of AAAI, 2006.
 N. Jindal and B. Liu, “Review spam detection,” in Proceedings of WWW,
2007. (Poster paper).
 N. Jindal and B. Liu, “Opinion spam and analysis,” in Proceedings of the
Conference on Web Search and Web Data Mining (WSDM), pp. 219–230,
 N. Kaji and M. Kitsuregawa, “Automatic construction of polarity-tagged corpus from HTML documents,” in Proceedings of the COLING/ACL Main Conference Poster Sessions, 2006.
 N. Kaji and M. Kitsuregawa, “Building lexicon for sentiment analysis from
massive collection of HTML documents,” in Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 1075–1083,
 A. Kale, A. Karandikar, P. Kolari, A. Java, T. Finin, and A. Joshi, “Modeling
trust and inﬂuence in the blogosphere using link polarity,” in Proceedings of
the International Conference on Weblogs and Social Media (ICWSM), 2007.
(Short paper).
 K. Kalyanam and S. H. McIntyre, “The role of reputation in online auction
markets,” Santa Clara University Working Paper 02/03-10-WP, 2001, dated
 J. Kamps, M. Marx, R. J. Mokken, and M. de Rijke, “Using WordNet to
measure semantic orientation of adjectives,” in Proceedings of LREC, 2004.
References
 S. D. Kamvar, M. T. Schlosser, and H. Garcia-Molina, “The Eigentrust algorithm for reputation management in P2P networks,” in Proceedings of WWW,
pp. 640–651, New York, NY, USA: ACM, ISBN 1-58113-680-3, 2003.
 H. Kanayama and T. Nasukawa, “Fully automatic lexicon expansion for
domain-oriented sentiment analysis,” in Proceedings of the Conference on
Empirical Methods in Natural Language Processing (EMNLP), (Sydney,
Australia), pp. 355–363, July 2006.
 M. Kantrowitz, “Method and apparatus for analyzing aﬀect and emotion in
text,” U.S. Patent 6622140, Patent ﬁled in November 2000, 2003.
 J. Karlgren and D. Cutting, “Recognizing text genres with simple metrics
using discriminant analysis,” in Proceedings of COLING, pp. 1071–1075, 1994.
 Y. Kawai, T. Kumamoto, and K. Tanaka, “Fair news reader: Recommending news articles with diﬀerent sentiments based on user preference,” in Proceedings of Knowledge-Based Intelligent Information and Engineering Systems
(KES), number 4692 in Lecture Notes in Computer Science, pp. 612–622,
 A. Kennedy and D. Inkpen, “Sentiment classiﬁcation of movie reviews using
contextual valence shifters,” Computational Intelligence, vol. 22, pp. 110–125,
 B. Kessler, G. Nunberg, and H. Sch¨utze, “Automatic detection of text genre,”
in Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the
Association for Computational Linguistics, pp. 32–38, 1997.
 P. Kim, “The forrester wave: Brand monitoring, Q3 2006,” Forrester Wave
(white paper), 2006.
 S.-M. Kim and E. Hovy, “Determining the sentiment of opinions,” in Proceedings of the International Conference on Computational Linguistics (COL-
ING), 2004.
 S.-M. Kim and E. Hovy, “Automatic detection of opinion bearing words and
sentences,” in Companion Volume to the Proceedings of the International Joint
Conference on Natural Language Processing (IJCNLP), 2005.
 S.-M. Kim and E. Hovy, “Identifying opinion holders for question answering in
opinion texts,” in Proceedings of the AAAI Workshop on Question Answering
in Restricted Domains, 2005.
 S.-M. Kim and E. Hovy, “Automatic identiﬁcation of pro and con reasons in
online reviews,” in Proceedings of the COLING/ACL Main Conference Poster
Sessions, pp. 483–490, 2006.
 S.-M. Kim and E. Hovy, “Identifying and analyzing judgment opinions,” in
Proceedings of the Joint Human Language Technology/North American Chapter of the ACL Conference (HLT-NAACL), 2006.
 S.-M. Kim and E. Hovy, “Crystal: Analyzing predictive opinions on the web,”
in Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-
CoNLL), 2007.
 S.-M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti, “Automatically
assessing review helpfulness,” in Proceedings of the Conference on Empirical
References
Methods in Natural Language Processing (EMNLP), pp. 423–430, Sydney,
Australia, July 2006.
 B. Klein and K. Leﬄer, “The role of market forces in assuring contractual
performance,” Journal of Political Economy, vol. 89, pp. 615–641, 1981.
 J. Kleinberg, “Authoritative sources in a hyperlinked environment,” in Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms (SODA),
pp. 668–677, 1998. ,
 M. Koppel and J. Schler, “The importance of neutral examples for learning
sentiment,” in Workshop on the Analysis of Informal and Formal Information
Exchange During Negotiations (FINEXIN), 2005.
 M. Koppel and I. Shtrimberg, “Good news or bad news? Let the market
decide,” in Proceedings of the AAAI Spring Symposium on Exploring Attitude
and Aﬀect in Text: Theories and Applications, pp. 86–88, 2004.
 L.-W. Ku, L.-Y. Li, T.-H. Wu, and H.-H. Chen, “Major topic detection and
its application to opinion summarization,” in Proceedings of the ACM Special
Interest Group on Information Retrieval (SIGIR), pp. 627–628, 2005. (Poster
 L.-W. Ku, Y.-T. Liang, and H.-H. Chen, “Opinion extraction, summarization
and tracking in news and blog corpora,” in AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW), pp. 100–107, 2006.
 L.-W. Ku, Y.-T. Liang, and H.-H. Chen, “Tagging heterogeneous evaluation
corpora for opinionated tasks,” in Conference on Language Resources and
Evaluation (LREC), 2006.
 L.-W. Ku, Y.-S. Lo, and H.-H. Chen, “Test collection selection and gold standard generation for a multiply-annotated opinion corpus,” in Proceedings of
the ACL Demo and Poster Sessions, pp. 89–92, 2007.
 T. Kudo and Y. Matsumoto, “A boosting algorithm for classiﬁcation of semistructured text,” in Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), 2004.
 S. Kurohashi, K. Inui, and Y. Kato, eds., Workshop on Information Credibility
on the Web, 2007.
 N. Kwon, S. Shulman, and E. Hovy, “Multidimensional text analysis for eRulemaking,” in Proceedings of Digital Government Research (dg.o), 2006.
 J. Laﬀerty, A. McCallum, and F. Pereira, “Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data,” in Proceedings of
ICML, pp. 282–289, 2001.
References
 J. D. Laﬀerty and C. Zhai, “Document language models, query models,
and risk minimization for information retrieval,” in Proceedings of SIGIR,
pp. 111–119, 2001.
 M. Laver, K. Benoit, and J. Garry, “Extracting policy positions from political texts using words as data,” American Political Science Review, vol. 97,
pp. 311–331, 2003.
 V. Lavrenko and W. Bruce Croft, “Relevance-based language models,” in
Proceedings of SIGIR, pp. 120–127, 2001.
 C. G. Lawson and V. C. Slawson, “Reputation in an internet auction market,”
Economic Inquiry, vol. 40, pp. 533–650, 2002.
 L. Lee, ““I’m sorry Dave, I’m afraid I can’t do that”: Linguistics, statistics,
and natural language processing circa 2001,” in Computer Science: Reﬂections
on the Field, Reﬂections from the Field, (Committee on the Fundamentals
of Computer Science: Challenges and Opportunities, Computer Science and
Telecommunications Board, National Research Council, ed.), pp. 111–118, The
National Academies Press, 2004.
 Y.-B. Lee and S. H. Myaeng, “Text genre classiﬁcation with genre-revealing
and subject-revealing features,” in Proceedings of the ACM Special Interest
Group on Information Retrieval (SIGIR), 2002.
 D. Leinweber and A. Madhavan, “Three hundred years of stock market manipulation,” Journal of Investing, vol. 10, pp. 7–16, Summer 2001.
 H. Li and K. Yamanishi, “Mining from open answers in questionnaire data,”
in Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD), pp. 443–449, 2001. .
 Y. Li, Z. Zheng, and H. Dai, “KDD CUP-2005 report: Facing a great challenge,” SIGKDD Explorations, vol. 7, pp. 91–99, 2005.
 W.-H. Lin and A. Hauptmann, “Are these documents written from diﬀerent
perspectives? A test of diﬀerent perspectives based on statistical distribution
divergence,” in Proceedings of the International Conference on Computational
Linguistics (COLING)/Proceedings of the Association for Computational Linguistics (ACL), pp. 1057–1064, Sydney, Australia: Association for Computational Linguistics, July 2006.
 W.-H. Lin, T. Wilson, J. Wiebe, and A. Hauptmann, “Which side are you on?
Identifying perspectives at the document and sentence levels,” in Proceedings
of the Conference on Natural Language Learning (CoNLL), 2006.
 J. Liscombe, G. Riccardi, and D. Hakkani-T¨ur, “Using context to improve
emotion detection in spoken dialog systems,” in Interspeech, pp. 1845–1848,
 L. V. Lita, A. H. Schlaikjer, W. Hong, and E. Nyberg, “Qualitative dimensions
in question answering: Extending the deﬁnitional QA task,” in Proceedings of
AAAI, pp. 1616–1617, 2005. (Student abstract).
 B. Liu, “Web data mining; Exploring hyperlinks, contents, and usage data,”
Opinion Mining. Springer, 2006.
 B. Liu, M. Hu, and J. Cheng, “Opinion observer: Analyzing and comparing
opinions on the web,” in Proceedings of WWW, 2005.
References
 H. Liu, H. Lieberman, and T. Selker, “A model of textual aﬀect sensing using
real-world knowledge,” in Proceedings of Intelligent User Interfaces (IUI),
pp. 125–132, 2003.
 J. Liu, Y. Cao, C.-Y. Lin, Y. Huang, and M. Zhou, “Low-quality product review detection in opinion summarization,” in Proceedings of the
Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-CoNLL), pp. 334–342,
2007. (Poster paper).
 Y. Liu, “Word-of-mouth for movies: Its dynamics and impact on box oﬃce
revenue,” Journal of Marketing, vol. 70, pp. 74–89, 2006.
 Y. Liu, J. Huang, A. An, and X. Yu, “ARSA: A sentiment-aware model for
predicting sales performance using blogs,” in Proceedings of the ACM Special
Interest Group on Information Retrieval (SIGIR), 2007.
 J. A. Livingston, “How valuable is a good reputation? A sample selection
model of internet auctions,” The Review of Economics and Statistics, vol. 87,
pp. 453–465, August 2005.
 L. Lloyd, D. Kechagias, and S. Skiena, “Lydia: A system for large-scale
news analysis,” in Proceedings of String Processing and Information Retrieval
(SPIRE), number 3772 in Lecture Notes in Computer Science, pp. 161–166,
 D. Lucking-Reiley, D. Bryan, N. Prasad, and D. Reeves, “Pennies from eBay:
The determinants of price in online auctions,” Journal of Industrial Economics, vol. 55, pp. 223–233, 2007.
 C. Macdonald and I. Ounis, “The TREC Blogs06 collection: Creating and
analysing a blog test collection,” Technical Report TR-2006-224, Department
of Computer Science, University of Glasgow, 2006.
 Y. Mao and G. Lebanon, “Sequential models for sentiment prediction,” in
ICML Workshop on Learning in Structured Output Spaces, 2006.
 Y. Mao and G. Lebanon, “Isotonic conditional random ﬁelds and local sentiment ﬂow,” in Advances in Neural Information Processing Systems, 2007.
 L. W. Martin and G. Vanberg, “A robust transformation procedure for interpreting political text,” Political Analysis, vol. 16, pp. 93–100, 2008.
 H. Masum and Y.-C. Zhang, “Manifesto for the reputation society,” First
Monday, vol. 9, 2004.
 S. Matsumoto, H. Takamura, and M. Okumura, “Sentiment classiﬁcation using
word sub-sequences and dependency sub-trees,” in Proceedings of PAKDD’05,
the 9th Paciﬁc-Asia Conference on Advances in Knowledge Discovery and
Data Mining, 2005.
 R. McDonald, K. Hannan, T. Neylon, M. Wells, and J. Reynar, “Structured
models for ﬁne-to-coarse sentiment analysis,” in Proceedings of the Association
for Computational Linguistics (ACL), pp. 432–439, Prague, Czech Republic:
Association for Computational Linguistics, June 2007.
 Q. Mei, X. Ling, M. Wondra, H. Su, and C. X. Zhai, “Topic sentiment mixture:
Modeling facets and opinions in weblogs,” in Proceedings of WWW, pp. 171–
180, New York, NY, USA: ACM Press, 2007. (ISBN 978-1-59593-654-7).
References
 M. I. Melnik and J. Alm, “Does a seller’s eCommerce reputation matter? Evidence from eBay auctions,” Journal of Industrial Economics, vol. 50, pp. 337–
349, 2002.
 M. I. Melnik and J. Alm, “Seller reputation, information signals, and prices for
heterogeneous coins on eBay,” Southern Economic Journal, vol. 72, pp. 305–
328, 2005.
 R. Mihalcea, C. Banea, and J. Wiebe, “Learning multilingual subjective language via cross-lingual projections,” in Proceedings of the Association for
Computational Linguistics (ACL), pp. 976–983, Prague, Czech Republic, June
 R. Mihalcea and C. Strapparava, “Learning to laugh (automatically): Computational models for humor recognition,” Journal of Computational Intelligence, 2006.
 G. Mishne and M. de Rijke, “Capturing global mood levels using blog posts,”
in AAAI Symposium on Computational Approaches to Analysing Weblogs
(AAAI-CAAW), pp. 145–152, 2006.
 G. Mishne and M. de Rijke, “Moodviews: Tools for blog mood analysis,”
in AAAI Symposium on Computational Approaches to Analysing Weblogs
(AAAI-CAAW), pp. 153–154, 2006.
 G. Mishne and M. de Rijke, “A study of blog search,” in Proceedings of the
European Conference on Information Retrieval Research (ECIR), 2006.
 G. Mishne and N. Glance, “Predicting movie sales from blogger sentiment,”
in AAAI Symposium on Computational Approaches to Analysing Weblogs
(AAAI-CAAW), pp. 155–158, 2006.
 S. Morinaga, K. Yamanishi, K. Tateishi, and T. Fukushima, “Mining product
reputations on the Web,” in Proceedings of the ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD), pp. 341–349, 2002. , pp. 412–418, July
2004. (Poster paper).
 T. Mullen and R. Malouf, “Taking sides: User classiﬁcation for informal online
political discourse,” Internet Research, vol. 18, pp. 177–190, 2008.
 T. Mullen and R. Malouf, “A preliminary investigation into sentiment
analysis of informal political discourse,” in AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW), pp. 159–162,
 J.-C. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou, “Eﬀectiveness of simple linguistic processing in automatic sentiment classiﬁcation of product reviews,” in
Conference of the International Society for Knowledge Organization (ISKO),
pp. 49–54, 2004.
 T. Nasukawa and J. Yi, “Sentiment analysis: Capturing favorability using
natural language processing,” in Proceedings of the Conference on Knowledge
Capture (K-CAP), 2003.
References
 V. Ng, S. Dasgupta, and S. M. N. Ariﬁn, “Examining the role of linguistic knowledge sources in the automatic identiﬁcation and classiﬁcation of
reviews,” in Proceedings of the COLING/ACL Main Conference Poster Sessions, pp. 611–618, Sydney, Australia: Association for Computational Linguistics, July 2006.
 X. Ni, G.-R. Xue, X. Ling, Y. Yu, and Q. Yang, “Exploring in the weblog space
by detecting informative and aﬀective articles,” in Proceedings of WWW, 2007.
(Industrial practice and experience track).
 N. Nicolov, F. Salvetti, M. Liberman, and J. H. Martin, eds., AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW).
AAAI Press, 2006.
 K. Nigam and M. Hurst, “Towards a robust metric of polarity,” in Computing
Attitude and Aﬀect in Text: Theories and Applications, number 20 in The
Information Retrieval Series, (J. G. Shanahan, Y. Qu, and J. Wiebe, eds.),
 Y. Niu, X. Zhu, J. Li, and G. Hirst, “Analysis of polarity information in
medical text,” in Proceedings of the American Medical Informatics Association
2005 Annual Symposium, 2005.
 I. Ounis, M. de Rijke, C. Macdonald, G. Mishne, and I. Soboroﬀ, “Overview
of the TREC-2006 blog track,” in Proceedings of the 15th Text Retrieval Conference (TREC), 2006.
 I. Ounis, C. Macdonald, and I. Soboroﬀ, “On the TREC blog track,” in
Proceedings of the International Conference on Weblogs and Social Media
(ICWSM), 2008.
 S. Owsley, S. Sood, and K. J. Hammond, “Domain speciﬁc aﬀective classiﬁcation of documents,” in AAAI Symposium on Computational Approaches to
Analysing Weblogs (AAAI-CAAW), pp. 181–183, 2006.
 M. Palmer, D. Gildea, and P. Kingsbury, “The proposition bank: A corpus
annotated with semantic roles,” Computational Linguistics, vol. 31, March
 B. Pang, K. Knight, and D. Marcu, “Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences,” in Proceedings
of HLT/NAACL, 2003.
 B. Pang and L. Lee, “A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts,” in Proceedings of the Association for Computational Linguistics (ACL), pp. 271–278, 2004.
 B. Pang and L. Lee, “Seeing stars: Exploiting class relationships for sentiment
categorization with respect to rating scales,” in Proceedings of the Association
for Computational Linguistics (ACL), pp. 115–124, 2005.
 B. Pang and L. Lee, “Using very simple statistics for review search: An exploration,” in Proceedings of the International Conference on Computational Linguistics (COLING), 2008. (Poster paper).
 B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up? Sentiment classiﬁcation using machine learning techniques,” in Proceedings of the Conference
on Empirical Methods in Natural Language Processing (EMNLP), pp. 79–86,
References
 D.-H. Park, J. Lee, and I. Han, “The eﬀect of on-line consumer reviews
on consumer purchasing intention: The moderating role of involvement,”
International Journal of Electronic Commerce, vol. 11, pp. 125–148, (ISSN
1086-4415), 2007.
 P. A. Pavlou and A. Dimoka, “The nature and role of feedback text comments
in online marketplaces: Implications for trust building, price premiums, and
seller diﬀerentiation,” Information Systems Research, vol. 17, pp. 392–414,
 S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. McNaught, “Mining
opinion polarity relations of citations,” in International Workshop on Computational Semantics (IWCS), pp. 366–371, 2007. (Short paper).
 R. Picard, Aﬀective Computing. MIT Press, 1997.
 T. Pinch and K. Athanasiades, “ACIDplanet: A study of users of an on-line
music community,” 2005. camp/ﬁles/ACIDplanet
%20by%20Trevor%20Pinch.ppt, Presented at the 50th Society for Ethnomusicology (SEM) conference.
 G. Pinski and F. Narin, “Citation inﬂuence for journal aggregates of scientiﬁc
publications: Theory, with application to the literature of physics,” Information Processing and Management, vol. 12, pp. 297–312, 1976.
 L. Polanyi and A. Zaenen, “Contextual lexical valence shifters,” in Proceedings
of the AAAI Spring Symposium on Exploring Attitude and Aﬀect in Text,
AAAI technical report SS-04-07, 2004.
 J. M. Ponte and W. Bruce Croft, “A language modeling approach to information retrieval,” in Proceedings of SIGIR, pp. 275–281, 1998.
 A.-M. Popescu and O. Etzioni, “Extracting product features and opinions
from reviews,” in Proceedings of the Human Language Technology Conference
and the Conference on Empirical Methods in Natural Language Processing
(HLT/EMNLP), 2005.
 R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik, A comprehensive grammar
of the English language. Longman, 1985.
 D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. C¸elebi, S. Dimitrov,
E. Drabek, A. Hakim, W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion,
S. Teufel, M. Topper, A. Winkel, and Z. Zhang, “MEAD — A platform for
multidocument multilingual text summarization,” in Conference on Language
Resources and Evaluation (LREC), Lisbon, Portugal, May 2004.
 D. R. Radev, E. Hovy, and K. McKeown, “Introduction to the special issue
on summarization,” Computational Linguistics, vol. 28, pp. 399–408, , 2002.
 L. Rainie and J. Horrigan, “Election 2006 online,” Pew Internet & American
Life Project Report, January 2007.
 J. Read, “Using emoticons to reduce dependency in machine learning techniques for sentiment classiﬁcation,” in Proceedings of the ACL Student
Research Workshop, 2005.
 D. A. Reinstein and C. M. Snyder, “The inﬂuence of expert reviews on consumer demand for experience goods: A case study of movie critics,” Journal
of Industrial Economics, vol. 53, pp. 27–51, 2005.
References
 E. Reiter and R. Dale, Building Natural Language Generation Systems. Cambridge, 2000.
 P. Resnick, K. Kuwabara, R. Zeckhauser, and E. Friedman, “Reputation
systems,” Communications of the Association for Computing Machinery
(CACM), vol. 43, pp. 45–48, (ISSN 0001-0782), 2000.
 P. Resnick, R. Zeckhauser, J. Swanson, and K. Lockwood, “The value of reputation on eBay: A controlled experiment,” Experimental Economics, vol. 9,
pp. 79–101, 2006.
 E. Riloﬀ, S. Patwardhan, and J. Wiebe, “Feature subsumption for opinion
analysis,” in Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2006.
 E. Riloﬀand J. Wiebe, “Learning extraction patterns for subjective expressions,” in Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2003.
 E. Riloﬀ, J. Wiebe, and W. Phillips, “Exploiting subjectivity classiﬁcation
to improve information extraction,” in Proceedings of AAAI, pp. 1106–1111,
 E. Riloﬀ, J. Wiebe, and T. Wilson, “Learning subjective nouns using extraction pattern bootstrapping,” in Proceedings of the Conference on Natural Language Learning (CoNLL), pp. 25–32, 2003.
 E. Rogers, Diﬀusion of Innovations.
Free Press, New York, 1962.
0743222091. Fifth edition dated 2003).
 S. Rosen, “Hedonic prices and implicit markets: Product diﬀerentiation in pure
competition,” The Journal of Political Economy, vol. 82, pp. 34–55, Jan–Feb
 D. Roth and W. Yih, “Probabilistic reasoning for entity and relation recognition,” in Proceedings of the International Conference on Computational Linguistics (COLING), 2004.
 V. L. Rubin and E. D. Liddy, “Assessing credibility of weblogs,” in AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW),
pp. 187–190, 2006.
 W. Sack, “On the computation of point of view,” in Proceedings of AAAI,
p. 1488, 1994. (Student abstract).
 F. Sebastiani, “Machine learning in automated text categorization,” ACM
Computing Surveys, vol. 34, pp. 1–47, 2002.
 Y. Seki, K. Eguchi, and N. Kando, “Analysis of multi-document viewpoint
summarization using multi-dimensional genres,” in Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Aﬀect in Text: Theories and
Applications, pp. 142–145, 2004.
 Y. Seki, K. Eguchi, N. Kando, and M. Aono, “Multi-document summarization
with subjectivity analysis at DUC 2005,” in Proceedings of the Document
Understanding Conference (DUC), 2005.
 Y. Seki, K. Eguchi, N. Kando, and M. Aono, “Opinion-focused summarization
and its analysis at DUC 2006,” in Proceedings of the Document Understanding
Conference (DUC), pp. 122–130, 2006.
 Y. Seki, D. Kirk Evans, L.-W. Ku, H.-H. Chen, N. Kando, and C.-Y. Lin,
“Overview of opinion analysis pilot task at NTCIR-6,” in Proceedings of the
References
Workshop Meeting of the National Institute of Informatics (NII) Test Collection for Information Retrieval Systems (NTCIR), pp. 265–278, 2007.
 C. Shapiro, “Consumer information, product quality, and seller reputation,”
Bell Journal of Economics, vol. 13, pp. 20–35, 1982.
 C. Shapiro, “Premiums for high quality products as returns to reputations,”
Quarterly Journal of Economics, vol. 98, pp. 659–680, 1983.
Shneiderman,
visualization
tree-maps:
space-ﬁlling
approach,” ACM Transactions on Graphics, vol. 11, pp. 92–99, 1992.
 S. Shulman, J. Callan, E. Hovy, and S. Zavestoski, “Language processing technologies for electronic rulemaking: A project highlight,” in Proceedings of Digital Government Research (dg.o), pp. 87–88, 2005.
 B. Snyder and R. Barzilay, “Multiple aspect ranking using the Good Grief
algorithm,” in Proceedings of the Joint Human Language Technology/North
American Chapter of the ACL Conference (HLT-NAACL), pp. 300–307, 2007.
 S. Somasundaran, J. Ruppenhofer, and J. Wiebe, “Detecting arguing and
sentiment in meetings,” in Proceedings of the SIGdial Workshop on Discourse
and Dialogue, 2007.
 S. Somasundaran, T. Wilson, J. Wiebe, and V. Stoyanov, “QA with attitude:
Exploiting opinion type analysis for improving question answering in on-line
discussions and the news,” in Proceedings of the International Conference on
Weblogs and Social Media (ICWSM), 2007.
 X. Song, Y. Chi, K. Hino, and B. Tseng, “Identifying opinion leaders in the
blogosphere,” in Proceedings of the ACM SIGIR Conference on Information
and Knowledge Management (CIKM), pp. 971–974, 2007.
 E. Spertus, “Smokey: Automatic recognition of hostile messages,” in Proceedings of Innovative Applications of Artiﬁcial Intelligence (IAAI), pp. 1058–
1065, 1997.
 E. Stamatatos, N. Fakotakis, and G. Kokkinakis, “Text genre detection using
common word frequencies,” in Proceedings of the International Conference on
Computational Linguistics (COLING), 2000.
 S. S. Standiﬁrd, “Reputation and e-commerce: eBay auctions and the asymmetrical impact of positive and negative ratings,” Journal of Management,
vol. 27, pp. 279–295, 2001.
 A. Stepinski and V. Mittal, “A fact/opinion classiﬁer for news articles,”
in Proceedings of the ACM Special Interest Group on Information Retrieval
(SIGIR), pp. 807–808, New York, NY, USA: ACM Press, 2007. (ISBN 978-1-
59593-597-7).
 B. Stone and M. Richtel, “The hand that controls the sock puppet could get
slapped,” The New York Times, July 16 2007.
 P. J. Stone, The General Inquirer: A Computer Approach to Content Analysis.
The MIT Press, 1966.
 V. Stoyanov and C. Cardie, “Partially supervised coreference resolution for
opinion summarization through structured rule learning,” in Proceedings of the
Conference on Empirical Methods in Natural Language Processing (EMNLP),
pp. 336–344, Sydney, Australia: Association for Computational Linguistics,
July 2006.
References
 V. Stoyanov, C. Cardie, D. Litman, and J. Wiebe, “Evaluating an opinion annotation scheme using a new multi-perspective question and answer
corpus,” in Proceedings of the AAAI Spring Symposium on Exploring Attitude
and Aﬀect in Text, AAAI Technical Report SS-04-07.
 V. Stoyanov, C. Cardie, and J. Wiebe, “Multi-perspective question answering
using the OpQA corpus,” in Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pp. 923–930, Vancouver, British Columbia,
Canada: Association for Computational Linguistics, October 2005.
 P. Subasic and A. Huettner, “Aﬀect analysis of text using fuzzy semantic
typing,” IEEE Transactions on Fuzzy Systems, vol. 9, pp. 483–496, 2001.
 M. Taboada, C. Anthony, and K. Voll, “Methods for creating semantic orientation dictionaries,” in Conference on Language Resources and Evaluation
(LREC), pp. 427–432, 2006.
 M. Taboada, M. A. Gillies, and P. McFetridge, “Sentiment classiﬁcation techniques for tracking literary reputation,” in LREC Workshop: Towards Computational Models of Literary Analysis, pp. 36–43, 2006.
 H. Takamura, T. Inui, and M. Okumura, “Extracting semantic orientation of
words using spin model,” in Proceedings of the Association for Computational
Linguistics (ACL), pp. 133–140, 2005.
 H. Takamura, T. Inui, and M. Okumura, “Latent variable models for semantic orientations of phrases,” in Proceedings of the European Chapter of the
Association for Computational Linguistics (EACL), 2006.
 H. Takamura, T. Inui, and M. Okumura, “Extracting semantic orientations
of phrases from dictionary,” in Proceedings of the Joint Human Language
Technology/North American Chapter of the ACL Conference (HLT-NAACL),
 K. Tateishi, Y. Ishiguro, and T. Fukushima, “Opinion information retrieval
from the internet,” Information Processing Society of Japan (IPSJ) SIG
Notes, 2001, vol. 69, no. 7, pp. 75–82, 2001. (Also cited as “A reputation
search engine that gathers people’s opinions from the Internet”, IPSJ Technical Report NL-14411. In Japanese).
 J. Tatemura, “Virtual reviewers for collaborative exploration of movie
reviews,” in Proceedings of Intelligent User Interfaces (IUI), pp. 272–275,
 L. Terveen, W. Hill, B. Amento, D. McDonald, and J. Creter, “PHOAKS:
A system for sharing recommendations,” Communications of the Association
for Computing Machinery (CACM), vol. 40, pp. 59–62, 1997.
 M. Thomas, B. Pang, and L. Lee, “Get out the vote: Determining support or
opposition from congressional ﬂoor-debate transcripts,” in Proceedings of the
Conference on Empirical Methods in Natural Language Processing (EMNLP),
pp. 327–335, 2006.
 R. Tokuhisa and R. Terashima, “Relationship between utterances and ‘enthusiasm’ in non-task-oriented conversational dialogue,” in Proceedings of the
SIGdial Workshop on Discourse and Dialogue, pp. 161–167, Sydney, Australia:
Association for Computational Linguistics, July 2006.
References
 R. M. Tong, “An operational system for detecting and tracking opinions in
on-line discussion,” in Proceedings of the Workshop on Operational Text Classiﬁcation (OTC), 2001.
 R. Tumarkin and R. F. Whitelaw, “News or noise? Internet postings and
stock prices,” Financial Analysts Journal, vol. 57, pp. 41–51, May/June
 P. Turney, “Thumbs up or thumbs down? Semantic orientation applied to
unsupervised classiﬁcation of reviews,” in Proceedings of the Association for
Computational Linguistics (ACL), pp. 417–424, 2002.
 P. D. Turney and M. L. Littman, “Measuring praise and criticism: Inference
of semantic orientation from association,” ACM Transactions on Information
Systems (TOIS), vol. 21, pp. 315–346, 2003.
 S. Wan and K. McKeown, “Generating overview summaries of ongoing email
thread discussions,” in Proceedings of the International Conference on Computational Linguistics (COLING), pp. 549–555, Geneva, Switzerland, 2004.
 M. White, C. Cardie, and V. Ng, “Detecting discrepancies in numeric
estimates using multidocument hypertext summaries,” in Proceedings of the
Conference on Human Language Technology, pp. 336–341, 2002.
 M. White, C. Cardie, V. Ng, K. Wagstaﬀ, and D. McCullough, “Detecting discrepancies and improving intelligibility: Two preliminary evaluations of RIP-
TIDES,” in Proceedings of the Document Understanding Conference (DUC),
 C. Whitelaw, N. Garg, and S. Argamon, “Using appraisal groups for sentiment
analysis,” in Proceedings of the ACM SIGIR Conference on Information and
Knowledge Management (CIKM), pp. 625–631, ACM, 2005.
 J. Wiebe, “Learning subjective adjectives from corpora,” in Proceedings of
AAAI, 2000.
 J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser, D. Litman,
D. Pierce, E. Riloﬀ, T. Wilson, D. Day, and M. Maybury, “Recognizing and
organizing opinions expressed in the world press,” in Proceedings of the AAAI
Spring Symposium on New Directions in Question Answering, 2003.
 J. Wiebe and R. Bruce, “Probabilistic classiﬁers for tracking point of view,”
in Proceedings of the AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pp. 181–187, 1995.
 J. Wiebe and R. Mihalcea, “Word sense and subjectivity,” in Proceedings of
the Conference on Computational Linguistics / Association for Computational
Linguistics (COLING/ACL), 2006.
 J. Wiebe and T. Wilson, “Learning to disambiguate potentially subjective
expressions,” in Proceedings of the Conference on Natural Language Learning
(CoNLL), pp. 112–118, 2002.
 J. Wiebe, T. Wilson, and C. Cardie, “Annotating expressions of opinions and
emotions in language,” Language Resources and Evaluation (formerly Computers and the Humanities), vol. 39, pp. 164–210, 2005.
 J. M. Wiebe, “Identifying subjective characters in narrative,” in Proceedings
of the International Conference on Computational Linguistics (COLING),
pp. 401–408, 1990.
References
 J. M. Wiebe, “Tracking point of view in narrative,” Computational Linguistics,
vol. 20, pp. 233–287, 1994.
 J. M. Wiebe, R. F. Bruce, and T. P. O’Hara, “Development and use of a
gold standard data set for subjectivity classiﬁcations,” in Proceedings of the
Association for Computational Linguistics (ACL), pp. 246–253, 1999.
 J. M. Wiebe and W. J. Rapaport, “A computational theory of perspective and
reference in narrative,” in Proceedings of the Association for Computational
Linguistics (ACL), pp. 131–138, 1988.
 J. M. Wiebe and E. Riloﬀ, “Creating subjective and objective sentence
classiﬁers from unannotated texts,” in Proceedings of the Conference on
Computational Linguistics and Intelligent Text Processing (CICLing), number
3406 in Lecture Notes in Computer Science, pp. 486–497, 2005.
 J. M. Wiebe, T. Wilson, and M. Bell, “Identifying collocations for recognizing opinions,” in Proceedings of the ACL/EACL Workshop on Collocation:
Computational Extraction, Analysis, and Exploitation, 2001.
 J. M. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin, “Learning subjective language,” Computational Linguistics, vol. 30, pp. 277–308, September
 Y. Wilks and J. Bien, “Beliefs, points of view and multiple environments,”
in Proceedings of the international NATO symposium on artiﬁcial and human
intelligence, pp. 147–171, USA, New York, NY: Elsevier North-Holland, Inc.,
 Y. Wilks and M. Stevenson, “The grammar of sense: Using part-of-speech
tags as a ﬁrst step in semantic disambiguation,” Journal of Natural Language
Engineering, vol. 4, pp. 135–144, 1998.
 T. Wilson, J. Wiebe, and P. Hoﬀmann, “Recognizing contextual polarity in
phrase-level sentiment analysis,” in Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pp. 347–354, 2005.
 T. Wilson, J. Wiebe, and R. Hwa, “Just how mad are you? Finding strong and
weak opinion clauses,” in Proceedings of AAAI, pp. 761–769, 2004. .
 H. Yang, L. Si, and J. Callan, “Knowledge transfer and opinion detection in
the TREC2006 blog track,” in Proceedings of TREC, 2006.
 K. Yang, N. Yu, A. Valerio, and H. Zhang, “WIDIT in TREC-2006 blog track,”
in Proceedings of TREC, 2006.
 J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack, “Sentiment analyzer:
Extracting sentiments about a given topic using natural language processing
techniques,” in Proceedings of the IEEE International Conference on Data
Mining (ICDM), 2003.
 J. Yi and W. Niblack, “Sentiment mining in WebFountain,” in Proceedings of
the International Conference on Data Engineering (ICDE), 2005.
 P.-L. Yin, “Information dispersion and auction prices,” Social Science
Research Network (SSRN) Working Paper Series, Version dated March 2005.
 H. Yu and V. Hatzivassiloglou, “Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences,”