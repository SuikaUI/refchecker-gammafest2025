HAL Id: hal-01123765
 
Submitted on 5 Mar 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Detection and classification of acoustic scenes and
events: An IEEE AASP challenge
Dimitrios Giannoulis, Emmanouil Benetos, Dan Stowell, Mathias Rossignol,
Mathieu Lagrange, Mark D. Plumbley
To cite this version:
Dimitrios Giannoulis, Emmanouil Benetos, Dan Stowell, Mathias Rossignol, Mathieu Lagrange, et al..
Detection and classification of acoustic scenes and events: An IEEE AASP challenge. IEEE WASPAA,
Oct 2013, New Paltz, United States. ￿10.1109/WASPAA.2013.6701819￿. ￿hal-01123765￿
2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 20-23, 2013, New Paltz, NY
DETECTION AND CLASSIFICATION OF ACOUSTIC SCENES AND EVENTS:
AN IEEE AASP CHALLENGE
Dimitrios Giannoulis1, Emmanouil Benetos2, Dan Stowell1, Mathias Rossignol3, Mathieu Lagrange3
and Mark D. Plumbley1
1 Centre for Digital Music, School of EECS, Queen Mary University of London, London, UK
2 Department of Computer Science, City University London, London, UK.
3 Analysis/Synthesis Team, IRCAM, CNRS-STMS, Paris, France.
This paper describes a newly-launched public evaluation challenge
on acoustic scene classiﬁcation and detection of sound events within
a scene. Systems dealing with such tasks are far from exhibiting
human-like performance and robustness. Undermining factors are
numerous: the extreme variability of sources of interest possibly
interfering, the presence of complex background noise as well as
room effects like reverberation. The proposed challenge is an attempt to help the research community move forward in deﬁning and
studying the aforementioned tasks. Apart from the challenge description, this paper provides an overview of systems submitted to
the challenge as well as a detailed evaluation of the results achieved
by those systems.
Index Terms— Computational auditory scene analysis, acoustic scene classiﬁcation, acoustic event detection
1. INTRODUCTION
Over the last few years, there has been an increased interest in the
speech and audio processing community in code dissemination and
reproducibility of results as a means to improve the quality and relevance of published results. This can be attributed to accumulating
evidence of the beneﬁts of performing research with reproducibility in mind and making well-documented code and data publicly
available . Public evaluation of proposed methods, especially
if accompanied with open-source submissions is a key component
in the move towards this reproducibility. It can serve as a reference
point for the performance of proposed methods and can also be used
for studying performance improvements throughout the years. Numerous initiatives have reached maturity, for example the SiSEC
evaluation for signal separation , the MIREX competition for
music information retrieval and the CHiME speech separation
and recognition challenge . The research problems related with
these evaluations are well-deﬁned and have their own performance
metrics established. However, for researchers working on modeling and classiﬁction of acoustic scenes, containing non-speech and
non-music, and detecting sound events within a scene, there is not
yet a coordinated established international challenge in this area,
with the exception of the now discontinued CLEAR evaluations 
funded by the CHIL project and the Multimedia Event Detection
This work has been partly supported by ESPRC Leadership Fellowship
EP/G007144/1, by EPSRC Grant EP/H043101/1 for QMUL, and by ANR-
11-JS03-005-01 for IRCAM. D.G. is funded by a Queen Mary University of
London CDTA Research Studentship. E.B. is supported by a City University
London Research Fellowship.
of the TRECVID video retrieval evaluations, where the focus is on
audiovisual, multi-modal event detection in video recordings .
In this paper, we describe a newly-launched public evaluation
challenge on acoustic scene classiﬁcation and event detection, both
for monophonic and polyphonic audio . In Section 2, we present
the datasets that were created for the challenge, as well as the employed evaluation metrics. Participating systems are then outlined
in Section 3, and evaluation results are presented and discussed in
Section 4.
2. CHALLENGE DESCRIPTION
Acoustic scene classiﬁcation and detection of sound events within
a scene are well deﬁned engineering tasks that both fall under the
“umbrella” of computational auditory scene analysis (CASA) .
The ﬁrst task aims to characterize the acoustic environment of an
audio stream by providing a semantic label to it . The second
one aims to label temporal regions within the audio, within which
a speciﬁc event class is active, by estimating the start and end time
of each event and if necessary (i.e. for audio extraction purposes)
separate it from other overlapping events.
The present challenge consists of a set of three subtasks. The
ﬁrst one addresses the problem of identifying and classifying acoustic scenes or soundscapes. The other two subtasks address the problem of identifying individual sound events that are prominent in an
acoustic scene: one focuses on monophonic event detection without
overlapping sounds and the other focuses on polyphonic scenarios.
The polyphonic case could be considered more interesting, as in
realistic everyday scenarios most of the sounds that reach our ears
tend to stem from a multitude of sources, but at the same time it consists of a much more challenging problem. More details about the
proposed tasks along with baseline results can be found in .
2.1. Datasets
Each of the tasks is accompanied by its own dataset. The datasets
for Scene Classiﬁcation (SC) consists of two equally proportioned
parts each made up of ten 30 seconds recordings for each scene
(class), for a total of 100 recordings per dataset. One part has been
made publicly available and serves as the development set for
participants to investigate the performance of their system, whereas
the other is kept private and used for a train/test (K-fold) evaluation.
The two datasets span a pre-selected list of scene types, representing an equal balance of indoor/outdoor scenes in the London area:
bus, busystreet, ofﬁce, openairmarket, park, quietstreet, restaurant,
supermarket, tube, tubestation.
2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 20-23, 2013, New Paltz, NY
Participants
Chum et al.
Various features at 2 frame sizes, classiﬁed either: (a) per-frame SVM + majority voting; (b) HMM
Concatenation of 4 different mono mixdowns; “i-vector” analysis of MFCCs, classiﬁed by pLDA
Geiger et al.
Diverse features, classiﬁed within 4-second windows using SVM, then majority voting
and ten Holt
“Cochleogram” representation, analysed for tonelikeness in each t-f bin, classiﬁed by SVM
Wavelets, MFCCs and others, classiﬁed in 5-second windows by treebagger, majority voting
Nam et al.
Feature learning by sparse RBM, then event detection and max-pooling, classiﬁed by SVM
Nogueira et al.
MFCCs + MFCC temporal modulations + event density estimation + binaural modelling features,
feature selection, classiﬁed by SVM
Normalised compression distance (Vorbis), Euclidean embedding, classiﬁed by Random Forest
Auditory representation analysed for spectrotemporal modulations, classiﬁed within one-second windows using SVM, then weighted combination of decision probabilities
Rakotomamonjy
Computer vision features (histogram of oriented gradient) applied to constant-Q spectrogram, classi-
ﬁed by SVM
Roma et al.
Recurrence Quantiﬁcation Analysis applied to MFCC time-series, classiﬁed by SVM
MFCCs, classiﬁed with a bag-of-frames approach
Table 1: Summary of submitted scene classiﬁcation systems.
Participants
Chauhan et al.
Feature extraction - Segmentation - Likelihood ratio test classiﬁcation
Diment et al.
MFCCs (features) - HMMs (detection)
Gemmeke et al.
NMF (detection) - HMMs (postprocessing)
Niessen et al.
Hierarchical HMMs + Random Forests (classiﬁcation) - Meta-classiﬁcation
Nogueira et al.
MFCCs (features) - SVMs (classiﬁcation)
Schr¨oder et al.
Gabor ﬁlterbank features - HMMs (classiﬁcation)
Vuegen et al.
MFCCs (features) - GMMs (detection)
NMF with pre-extracted bases (detection)
Table 2: Summary of submitted event detection systems.
These recordings were made with a set of Soundman OKM II
binaural microphones. These microphones imitate a pair of in-ear
headphones that the user can wear for added portability and subtlety.
Furthermore, the data carries also binaural information about the
sound that could be utilized as cues for the sound event and scene
detection or simply be ignored by adding the two channels together
in order to obtain a mono recording.
The datasets for event detection were built from audio collected
in ofﬁce environments because of the interest of such audio to certain applications such as audio-conferencing systems etc. Two event
detection tasks are proposed, a monophonic task denoted as Ofﬁce
Live (OL) and a polyphonic task denoted as Ofﬁce Synthetic (OS).
Polyphonic data for the OS task was created using a scene synthesizer, concatenating recordings of isolated events. Each dataset
consists of three subsets (a training, a development and a testing
dataset). The training set contains instantiations of individual events
for every class and is shared between the OL and OS tasks to allow
for single training of event detection systems. The development and
testing datasets consist of roughly 1 minute long scripted recordings
of everyday audio events in a number of ofﬁce environments (different size and absorbing quality rooms, different number of people in the room and varying noise level). Event types used were:
alert (short alert (beep) sound), clearthroat (clearing throat), cough,
doorslam (door slam), drawer, keyboard (keyboard clicks), keys
(keys put on table), knock (door knock), laughter, mouse (mouse
click), pageturn, (page turning), pendrop (pen, pencil, or marker
touching table surfaces), phone, printer, speech, switch. To capture the spatial layout of the acoustic environment, recordings were
made in ﬁrst order B-format (4-channel), with a high-quality Sound-
ﬁeld SPS422B microphone system, placed in an open space in the
room, with events spatially distributed around the room. Recordings
were mixed down to stereo (using the common “Blumlein pair” con-
ﬁguration). The challenge is conducted using the stereo ﬁles, with
scope to extend the challenge to full B-format in future if there is
More details about the creation of the datasets, the annotation
process and the audio recording process can be found in .
2.2. Evaluation Metrics
For the scene classiﬁcation task, systems are evaluated with 5-fold
stratiﬁed cross validation. The raw classiﬁcation (identiﬁcation) accuracy, standard deviation and a confusion matrix for each algorithm is computed.
For the event detection tasks, in order to provide a thorough
assessment of the various systems, three types of evaluations take
place, namely a frame-based, event-based, and class-wise eventbased evaluation. The main metrics used for each evaluation type
are the F-measure (F) and the acoustic event error rate (AEER)
as described in . For the event-based and class-wise event-based
metrics, two types of evaluation will take place, an onset-only and
an onset-offset-based evaluation.
Results to onset-based metrics
(denoted without any subscript) and onset-offset-based metrics (denoted as Foﬀset and AEERoﬀset). For a complete and analytic description of the evaluation metrics employed the reader is referred
to .
2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 20-23, 2013, New Paltz, NY
Majority vote
Scene Classification: accuracy (%)
Figure 1: Classiﬁcation accuracy(%) for the SC task. Plot shows
mean over 5-fold cross-validation with 95% conﬁdence intervals.
Dashed lines indicate (left to right): chance performance (black);
baseline system performance (light blue); mean accuracy of human
listener (orange). “Majority vote” is a meta-classiﬁer using the majority decision over all submissions.
3. SUBMITTED SYSTEMS
Overall, 11 systems were submitted to the SC task, 7 systems were
submitted to the OL task, and 3 systems to the OS task. Variants for
each system were allowed that increased the total number and variety considerably. The systems submitted for the scene classiﬁcation
and event detection tasks are listed in Tables 1 and 2, respectively,
along with a short description of each system and the programming
language in which it was written.
Apart from the submitted systems, performance on the test sets
is also reported for baseline systems for the two tasks. These systems were made publicly available as open source software .
4. CHALLENGE RESULTS
Results were computed by running all the submitted systems on the
held-back testing datasets and computing the metrics as in Sec. 2.2.
Figure 1 shows the overall performance of submitted systems for
the scene classiﬁcation task. Most systems were able to outperform
the baseline system, and some matched or even slightly excelled
the mean accuracy we found in human listeners (71%; results in
preparation). The strongest performers are notably diverse in their
choice of features and their use of temporal information, though often using SVMs for classiﬁcation. Two submissions achieved good
results on the development data but not on our held-out test data.
Table 3 shows a confusion matrix for the scene labels as round
percentages of the sum of all confusion matrices for all submissions.
Confusions are mostly concentrated over classes that share some
acoustical properties such as park/quietstreet and tube/tubestation.
busystreet
openairmarket
quietstreet
restaurant
supermarket
tubestation
busystreet
openairmarket
quietstreet
restaurant
supermarket
tubestation
Table 3: Aggregate confusion matrix for scene classiﬁcation across
all submissions. Rows are ground truth, columns the inferred labels.
Values are expressed as percentages rounded to the nearest integer.
For the event detection OL and OS tasks, results are summarized in Tables 4 and 5, respectively. The baseline was outperformed
by most systems for these tasks too. The best performance for the
OL task using all types of metrics is achieved by the SCS submission, which used a Gabor ﬁlterbank feature extraction step with by
2-layer hidden Markov models (HMMs) for classifying events, followed by the NVM submission, which used a meta-classiﬁer combining hierarchical HMMs and random forests. For the OS task, the
best performance in terms of F-measure is achieved by the DHV
system, which used an iterative scheme with HMMs.
also be noted that submitted systems performed better with lower
polyphony, with the exception of the DHV system, which had better performance with higher polyphony levels. As expected, the
onset-offset evaluation produced worse results compared to onsetonly evaluation for both tasks, although the performance difference
is rather small. This may be explained by the percussive nature of
most events.
The challenge website gives detailed system descriptions
and extensive results, analytic breakdown of performance per system, as well as further error analysis.
5. CONCLUSIONS
In this paper we presented a challenge on the detection and classiﬁcation of acoustic scenes and events. We ran a scene classiﬁcation (SC) challenge, and two event detection and classiﬁcation
challenges: ofﬁce live (OL) and ofﬁce synthetic (OS). Our goal was
to provide a focus of attention for the scientiﬁc community in developing systems for CASA that will encourage sharing of ideas and
improve the state of the art, potentially leading to the development
of systems that achieve a performance close to that of humans.
The results enable us to draw some interesting conclusions
about the different problems. For scene classiﬁcation, we found
that although simple systems can do relatively well, the improvement that more complex systems achieve can bring performance to
the levels achieved by human listeners. For event detection, which
is a more challenging task, performance is much worse although
we have not performed a direct comparison with human listeners at
present. For the monophonic case, systems are able to achieve satisfactory performance with scope for improvement. For the polyphonic case, the task of recognising individual potentially overlapping sounds becomes signiﬁcantly challenging and the performance
of systems that are even prepared to deal with polyphonic content
2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 20-23, 2013, New Paltz, NY
Evaluation Method
Event-Based
Class-Wise Event-Based
Frame-Based
Foﬀset (%)
Foﬀset (%)
Table 4: Evaluation metrics for the participating systems for the (monophonic) Ofﬁce Live Event Detection task.
Event-Based
Class-Wise Event-Based
Frame-Based
Foﬀset (%)
Foﬀset (%)
Table 5: Evaluation metrics for the participating systems for the (polyphonic) Ofﬁce Synthetic Event Detection task.
falls dramatically. More details for all the submitted systems can be
found on the challenge website in .
At this point, we have just completed running the challenge.
For future work, we will consider producing a detailed performance
evaluation, creating a code repository, releasing test sets, doing a Bformat challenge, running the challenge again or doing a challenge
on world synthetic sounds (WS) as proposed in .
6. ACKNOWLEDGMENTS
The authors would like to thank all of the contributors, for taking
part in the challenge and participating in email discussions as we
were developing the challenge, and Daniele Barchiesi, for his contribution in running the evaluation of the submitted systems and providing useful feedback.
7. REFERENCES
 P. Vandewalle, J. Kovacevic, and M. Vetterli, “Reproducible
research in signal processing,” IEEE Signal Processing Magazine, vol. 26, no. 3, pp. 37–47, 2009.
 J. Kovacevic, “How to encourage and publish reproducible research,” in IEEE Int. Conf. Acoustics, Speech and Signal Processing, 2007, pp. 1273–1276.
 S. Araki, F. Nesta, E. Vincent, Z. Koldovsk`y, G. Nolte,
A. Ziehe, and A. Benichoux, “The 2011 signal separation
evaluation campaign (SiSEC2011),” in Latent Variable Analysis and Signal Separation.
Springer, 2012, pp. 414–422.
 “Music
Information
Evaluation
(MIREX),” 
 J. Barker, E. Vincent, N. Ma, H. Christensen, and P. Green,
“The PASCAL CHiME speech separation and recognition
challenge,” Computer Speech & Language, vol. 27, no. 3, pp.
621–633, May 2012.
 R. Stiefelhagen, K. Bernardin, R. Bowers, J. Garofolo,
D. Mostefa, and P. Soundararajan, “The CLEAR 2006 evaluation,” Multimodal Technologies for Perception of Humans,
pp. 1–44, 2007.
 P. Over, G. Awad, M. Michel, J. Fiscus, G. Sanders, B. Shaw,
W. Kraaij, A. F. Smeaton, and G. Quenot, “TRECVID 2012 –
an overview of the goals, tasks, data, evaluation mechanisms
and metrics,” in Proc. of TRECVID 2012.
NIST, USA, 2012.
 D. Giannoulis, E. Benetos, D. Stowell, M. Rossignol, M. Lagrange, and M. D. Plumbley, “Detection and classiﬁcation of
acoustic scenes and events, an IEEE AASP challenge,” http:
//c4dm.eecs.qmul.ac.uk/sceneseventschallenge/, Queen Mary
University of London, Tech. Rep. EECSRR-13-01, 2013.
 D. Wang and G. J. Brown, Computational auditory scene
analysis: Principles, algorithms, and applications.
Press, 2006.
 J.-J. Aucouturier, B. Defreville, and F. Pachet, “The bag-offrames approach to audio pattern recognition: A sufﬁcient
model for urban soundscapes but not for polyphonic music,”
J. Acoust. Soc. of America, vol. 122, no. 2, pp. 881–891, 2007.
 D. Giannoulis, D. Stowell, E. Benetos, M. Rossignol, M. Lagrange, and M. D. Plumbley, “A database and challenge for
acoustic scene classiﬁcation and event detection,” in European
Signal Processing Conf., 2013.