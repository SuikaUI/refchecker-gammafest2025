1. INTRODUCTION
This paper discusses the estimation of treatment effects in observational studies. This issue, which is of great practical importance because randomized experiments cannot always be
implemented, has been addressed previously by Lalonde , whose data we use in this paper.
Lalonde estimates the impact of the National Supported Work (NSW) Demonstration, a labor
training program, on post-intervention income levels, using data from a randomized evaluation
of the program. He then examines the extent to which non-experimental estimators can replicate
the unbiased experimental estimate of the treatment impact, when applied to a composite data set
of experimental treatment units and non-experimental comparison units. He concludes that standard non-experimental estimators, such as regression, fixed-effect, and latent-variable-selection
models, are either inaccurate (relative to the experimental benchmark), or sensitive to the specification used in the regression. Lalonde’s results have been influential in renewing the debate on
experimental versus non-experimental evaluations and in spurring a search for alternative estimators and specification tests .
In this paper, we apply propensity score methods to Lalonde’s data set. Propensity score methods focus on the comparability of the treatment and nonexperimental comparison groups in terms of pre-intervention variables. Controlling for differences in pre-intervention variables is difficult when the treatment and comparison groups are dissimilar and when there are many pre-intervention variables. The propensity score (the probability
of assignment to treatment, conditional on covariates) summarizes the pre-intervention variables.
We can easily control for differences between the treatment and non-experimental comparison
groups through the estimated propensity score, a single variable on the unit interval. Using propensity score methods, we are able to replicate the experimental treatment effect for a range of
specifications and estimators.
The assumption underlying the method is that assignment to treatment depends only on
observable pre-intervention variables . Though this is a strong assumption, we demonstrate that propensity score methods are an
informative starting point, because they quickly reveal the extent to which the treatment and
comparison groups overlap in terms of pre-intervention variables.
The paper is organized as follows. Section 2 reviews Lalonde’s data and replicates his
results. Section 3 identifies the treatment effect under the potential outcomes causal model, and
discusses estimation strategies for the treatment effect. In Section 4, we apply our methods to
Lalonde’s data set, and in Section 5, we discuss the sensitivity of the results to the methodology.
Section 6 concludes the paper.
2. LALONDE’S RESULTS
2.1 The Data
The National Supported Work (NSW) Demonstration was a federally-funded program implemented in the mid-1970s, with the objective of providing work experience for a period of 12 to 18 months to individuals who had
faced economic and social problems prior to enrollment in the program. Those randomly selected
to join the program participated in various types of work, ranging from operating a restaurant to
construction work. Information on pre-intervention variables (pre-intervention earnings as well
as education, age, ethnicity, and marital status) was obtained from initial surveys and Social Security Administration records. In this paper we focus on the male participants, since estimates
for this group were the most sensitive to functional-form specification, as indicated in Lalonde
 . Both the treatment and control groups participated in follow-up interviews at specific
intervals. The outcome variable of interest is post-intervention earnings. Unlike typical
clinical trials, the eligible candidates did not join the NSW program immediately, but were randomized in over a period of 51 months between March 1975 and June 1977. This introduced
what the administrators of the program have referred to as the “cohort phenomenon” : individuals who joined early in the program had different characteristics than those
who entered later.
Lalonde limits his sample to those assigned between January 1976 and July 1977 in order
to achieve homogeneity within the treatment and control groups, reducing the sample to 297
treated observations and 425 control observations for male participants. His sample is limited to
one year of pre-intervention earnings data . However, several years of pre-intervention
earnings are viewed as important in determining the effect of job training programs . Thus,
we further limit ourselves to a subset of this data in order to obtain data on earnings in 1974. Our
subset, also defined using the month of assignment, includes 185 treated and 260 control observations. Since month of assignment is a pre-treatment variable, this selection does not affect the
properties of the experimentally randomized data set: the treatment and control groups still have
the same distribution of pre-intervention variables, so that a difference in means remains an unbiased estimate of the average treatment impact.
We present the pre-intervention characteristics of the original sample and of our subset
in the first four rows of Table 1. The distribution of pre-intervention variables is very similar
across the treatment and control groups for both samples (none of the differences is statistically
significant), but our subset differs somewhat from Lalonde’s original sample, especially in terms
of 1975 earnings. Our propensity score results will be based on our subset of the data, using two
years of pre-intervention earnings. In order to render our results comparable to Lalonde’s, we
replicate his analysis on our subset (both with and without the additional year of pre-intervention
earnings data), and show that his basic conclusions remain unchanged. As well, in Section 5, we
discuss the sensitivity of our propensity score results to dropping the additional earnings data.
2.2 Lalonde’s Results
Lalonde estimates linear regression, fixed-effect, and latent variable selection models of the
treatment impact. Since our analysis focuses on the importance of pre-intervention variables, we
consider primarily the first of these. Non-experimental estimates of the treatment effect are based
on the two distinct comparison groups used by Lalonde , the Panel Study of Income Dynamics (PSID-1) and Westat’s Matched Current Population Survey-Social Security Administration File (CPS-1). Lalonde also considers subsets of these two comparison groups, PSID2-3 and
Table 1 presents the pre-intervention characteristics of the comparison groups. It is evident that both PSID-1 and CPS-1 differ dramatically from the treatment group, especially in
terms of age, marital status, ethnicity, and pre-intervention earnings (all the mean differences are
statistically significant). In order to bridge the gap between the treatment and the comparison
groups in terms of pre-intervention characteristics, Lalonde extracts subsets from PSID-1 and
CPS-1 (PSID-2 and -3, and CPS-2 and -3) which resemble the treatment group in terms of single
pre-intervention characteristics (such as age or employment status; see Table 1, notes). But as the
table indicates, the subsets still remain substantially different from the treatment group (the mean
differences in age, ethnicity, marital status, and earnings are smaller, but remain statistically significant).
Table 2 (Panel A) replicates Lalonde’s results using his original data and nonexperimental comparison groups (the results are identical to those presented in his paper, with
the exceptions noted in the footnote of Table 2). Table 2 (Panel B) applies Lalonde’s estimators
to our reduced experimental sample and the same comparison units. Comparing the two panels,
we note that the treatment effect, as estimated from the randomized experiment, is higher in
Panel B ($1,794 compared with $886). This is due to the cohort phenomenon --individuals with a
later month of assignment seem to have benefitted more from the program. Otherwise, the results
are qualitatively similar. The simple difference in means, reported in column (1), yields negative
treatment effects for the CPS and PSID comparison groups in both panels (except PSID-3). The
fixed-effect type differencing estimator in column (3) fares somewhat better, although many estimates are still negative or deteriorate when we control for covariates in both panels. The estimates in column (5) are closest to the experimental estimate, consistently closer than those in
column (2) which do not control for earnings in 1975. The treatment effect is underestimated by
about $1,000 for the CPS comparison groups and $1,500 for the PSID groups. Lalonde’s conclusion from Panel A, which also holds for our version in Panel B, is that there is no consistent estimate robust to the specification of the regression or the choice of comparison group.
The inclusion of earnings in 1974 as an additional variable in the regressions in Table 2
(Panel C) does not alter Lalonde’s basic message, although the estimates improve when compared with Panel B. In columns (1) to (3), many estimates are still negative, but less so than in
Panel B. In columns (4) and (5), the estimates are also closer to the experimental benchmark, off
by about $1,000 for PSID1-3 and CPS1-2 and by $400 for CPS-3. Overall, the best results in Table 2 are for CPS-3, Panel C. This raises a number of issues. The strategy of considering subsets
of the comparison group more comparable to the treatment group certainly seems to improve
matters, provided that we observe the key pre-intervention variables. But Lalonde creates these
subsets in an informal manner, based on one or two pre-intervention variables. Table 1 reveals
that significant differences remain between the comparison groups and the treatment group. A
more systematic means of creating such subsets should improve the estimates from both the CPS
and PSID. We undertake this in Sections 3 and 4 with propensity score methods.
3. IDENTIFYING AND ESTIMATING THE AVERAGE TREATMENT EFFECT
3.1 Identification
Let Yi1 represent the value of the outcome when unit i is subject to regime 1 (called treatment),
and Yi0 the value of the outcome when unit i is exposed to regime 0 (called control). Only one of
Yi0 or Yi1 can be observed for any unit, since we can not observe the same unit under both treatment and control. Let Ti be a treatment indicator (=1 if exposed to treatment, =0 otherwise). Then
the observed outcome for unit i is Yi = TiYi1 + (1–Ti)Yi0. The treatment effect for unit i is
In an observational study, the treatment and comparison groups are often drawn from different populations. In our application the group exposed to the treatment is drawn from the
population of interest (welfare recipients eligible for the program). The comparison group is
drawn from a different population (in our application both the CPS and PSID are more representative of the general US population). The treatment effect we are trying to identify is therefore
the treatment effect for the treated population:
This cannot be estimated directly since Yi0 is not observed for the treated units. Assuming selection on observables , namely {Yi1, Yi0 Ti}|Xi (using Dawid’s notation, is
independence), we obtain:
for j=0,1. Conditional on the observables, Xi, there is no systematic pre-treatment difference between the groups assigned to treatment and control. This allows us to identify the treatment effect for the treated:
where the outer expectation is over the distribution of Xi|Ti=1, the distribution of pre-intervention
variables in the treated population.
One method for estimating the treatment effect stems from (1): estimating
as two non-parametric equations. This estimation strategy
becomes difficult, however, if the covariates, Xi, are high dimensional. The propensity score
theorem provides an intermediate step:
Proposition 1 : Let p(Xi) be the probability of unit i having been
assigned to treatment, defined as p(Xi)≡Pr(Ti=1|Xi)=E(Ti|Xi), where 0<p(Xi)<1, ∀i. Then:
Corollary:
where the outer expectation is over the distribution of p(Xi)|Ti=1.
One intuition for the propensity score is that, whereas in equation (1) we are trying to condition
on X (intuitively, to find observations with similar covariates), in equation (2) we are trying to
condition just on the propensity score, because the proposition implies that observations with the
same propensity score have the same distribution of the full vector of covariates X.
3.2 The Estimation Strategy
Estimation is in two steps. First, we estimate the propensity score for the sample of experimental
treatment and non-experimental comparison units. We use the logistic model, but other standard
models yield similar results. An issue is what functional form of the pre-intervention variables to
include in the logit. We rely on the following proposition:
Proposition 2 :
Proposition 2 asserts that, conditional on the propensity score, the covariates are independent of
assignment to treatment, so that, for observations with the same propensity score, the distribution
of covariates should be the same across the treatment and comparison groups. Conditioning on
the propensity score, each individual has the same probability of assignment to treatment, as in a
randomized experiment.
We use this proposition to assess estimates of the propensity score. For any given specification (we start by introducing the covariates linearly), we group observations into strata defined
on the estimated propensity score and check whether we succeed in balancing the covariates
within each stratum. We use tests for the statistical significance of differences in the distribution
of covariates, focusing on first and second moments. If there are no significant differences between the two groups, then we accept the specification. If there are significant differences, we
add higher-order terms and interactions of the covariates until this condition is satisfied. Section
5 shows that the results are not sensitive to the selection of higher order and interaction variables.
In the second step, given the estimated propensity score, we need to estimate a univariate
non-parametric regression .
With stratification, observations are sorted from lowest to highest estimated propensity
score. The comparison units with an estimated propensity score less than the minimum (or
greater than the maximum) estimated propensity score for treated units are discarded. The strata,
defined on the estimated propensity score, are chosen so that the covariates within each stratum
are balanced across the treatment and comparison units (we know such strata exist from step
one). Based on equation (2), within each stratum we take a difference in means of the outcome
between the treatment and comparison groups, and weight these by the number of treated observations in each stratum. We also consider matching on the propensity score. Each treatment unit
is matched with replacement to the comparison unit with the closest propensity score; the unmatched comparison units are discarded .
There are a number of reasons to prefer this two-step approach rather than estimating
equation (1) directly. First, tackling equation (1) directly with a non-parametric regression would
encounter the curse of dimensionality as a problem in many data sets, including ours, which have
a large number of covariates. This is also true for estimating the propensity score using nonparametric techniques. Hence, we use a parametric model for the propensity score. This is preferable to applying a parametric model to equation (1) directly because, as we will see, the results
are less sensitive to the logit specification than regression models, such as those in Table 2 (and
because there is a simple criterion for determining which interactions to add to the specification).
Finally, depending on the estimator one adopts (e.g., stratification), an extremely precise estimate of the propensity score is not even needed, since the process of validating the propensity
score produces at least one partition structure which balances pre-intervention covariates across
the treatment and comparison groups within each stratum, which (by equation (1)) is all that is
needed for an unbiased estimate of the treatment impact.
4. RESULTS USING THE PROPENSITY SCORE
Using the method outlined in the previous section, we estimate the propensity score for each
comparison group separately. Figure 1 presents a histogram of the estimated propensity scores
for the treatment and PSID-1 comparison units, and Figure 2 for CPS-1 comparison units. In
Figure 2, we discard 12,611 (out of a total of 15,992) CPS units whose estimated propensity
score is less than the minimum for the treatment units. Even then, the first bin (from 0-0.05)
contains 2,969 of the remaining comparison units and only 26 treatment units. This provides a
snapshot of the fact that the comparison group, although very large, contains relatively few units
comparable to the treatment group. A similar pattern is seen in the first bin of Figure 1, but an
important difference is that in Figure 1 there is limited overlap in the estimated propensity score
between the treatment and PSID groups: there are 98 (more than half the total number of) treated
units with an estimated propensity score in excess of 0.8, and only 7 comparison units. Instead,
for the CPS, although the treatment units outnumber the comparisons for higher values of the
estimated propensity scores, for most bins there are at least a few comparison units.
We use stratification and matching on the propensity score to group the treatment units
with the small number of comparison units that are comparable (namely, those comparison units
whose estimated propensity scores are greater than the minimum -- or less than the maximum -propensity score for treatment units). The treatment effect is estimated by summing the withinstratum difference in means between the treatment and comparison observations , where the sum is weighted by the number of treated observations within each stratum
(Table 3, column (4)). An alternative is a within-block regression, again taking a weighted sum
over the strata (Table 3, column (5)). When the covariates are well balanced, such a regression
should have little effect, but it can help to eliminate the remaining within-block differences.
Likewise for matching, we can estimate a simple difference in means between the treatment and
matched comparison group for earnings in 1978 (column (7)), and also perform a regression of
1978 earnings on covariates (column (8)).
Table 3 presents the results. For the PSID sample, the stratification estimate is $1,608 and
the matching estimate is $1,691, which should be compared against the benchmark randomizedexperiment estimate of $1,794. The estimates from a difference in means, or regression control
on the full sample, are -$15,205 and $731. The propensity score estimators yield more accurate
estimates simply using a difference in means because only those comparison units similar to the
treatment group have been used. In columns (5) and (8) controlling for covariates has little impact on the stratification and matching estimates. Likewise for the CPS, the propensity-scorebased estimates from the CPS -- $1,713 and $1,582 -- are much closer to the experimental
benchmark than estimates from the full comparison sample -- -$8,498 and $972.
Another set of estimates to consider is from the subsets of the PSID and CPS. In Table 2,
the estimates tend to improve when applied to narrower subsets. However, as noted above, the
estimates still range from -$8,498 to $1,326. In Table 3, the estimates do not improve for the
subsets, although the range of fluctuation is much narrower, from $587 to $2,321. Tables 1 and 4
shed light on this.
Table 1 presents the pre-intervention characteristics of the various comparison groups.
We note that the subsets PSID-2 and -3, and CPS-2 and -3, though more closely resembling the
treatment group, are still considerably different along a number of important dimensions, including ethnicity, marital status, and especially earnings. Table 4 presents the characteristics of
the matched subsamples from the comparison groups. The characteristics of the matched subsets
of CPS-1 and PSID-1 closely correspond to the treatment group; none of the differences are statistically significant. But as we create the subsets of the comparison groups, the quality of the
matches declines, most dramatically for the PSID, with PSID-2 and -3 earnings now increasing
from 1974 to 1975, whereas for the treatment group they decline. The training literature has
identified the “dip” in earnings as an important characteristic of participants in training programs
 . The CPS sub-samples retain the dip, but for the matched subset of
CPS-3 earnings in 1974 are significantly higher than for the treatment group.
This illustrates one of the important features of propensity score methods, namely that the
creation of subsamples from the non-experimental comparison group is neither necessary nor
desirable, because subsamples created based on single pre-intervention characteristics may dispose of comparison units which nonetheless are good overall comparisons with treatment units.
The propensity score sorts out which comparison units are most relevant considering all of the
pre-intervention characteristics, not just one characteristic at a time.
Column (3) in Table 3 gives an important insight into how the estimators in columns (4)
to (8) succeed in estimating the treatment effect accurately. In column (3) we regress the outcome on a quadratic function of the estimated propensity score and a treatment indicator. The estimates are comparable to those in column (2), where we regress the outcome on all pre-intervention characteristics. This again demonstrates the ability of the propensity
score to summarize all pre-intervention variables. The estimators in columns (4) to (8) differ
from column (3) in two respects. First, their functional form is more flexible than a low-order
polynomial in the estimated propensity score. Second, rather than requiring a constant additive
treatment effect, they allow the treatment effect to vary within each stratum (for stratification) or
for each individual (for matching).
Finally, it must be noted that even though the estimates presented in Table 3 are closer to
the experimental benchmark than those presented in Table 2, with the exception of the adjusted
matching estimator, their standard errors are higher: in Table 3, column (5), the standard errors
are 1,152 and 1,581 for the CPS and PSID, compared with 550 and 886 in Table 2, column (5).
This is because the propensity score estimators use fewer observations. When stratifying on the
propensity score, we discard irrelevant controls, and so the strata may contain as few as seven
treated observations. However, the standard errors for the adjusted matching estimator (751 and
809) are similar to those in Table 2.
By summarizing all of the covariates in a single number, the propensity score method allows us to focus on the comparability of the comparison group to the treatment group. Hence, it
allows us to address the issues of functional form and treatment effect heterogeneity much more
5. SENSITIVITY ANALYSIS
5.1 Sensitivity to the Specification of the Propensity Score
How sensitive are the estimates presented to the specification of the estimated propensity score?
For the stratification estimator, as was suggested in Section 3, the exact specification of the estimated propensity score is not important as long as, within each stratum, the pre-intervention
characteristics are balanced across the treatment and comparison groups. Since this was the basis
of the specification search suggested in Section 3, either one can find a specification that balances pre-intervention characteristics, or one must conclude the treatment and comparison
groups are irreconcilably different.
The upper half of Table 5 demonstrates that the estimates of the treatment impact are not
particularly sensitive to the specification used. Specifications 1 and 4 are the same as those in
Table 3 (hence, they balance the pre-intervention characteristics). In specifications 2 to 3 and 5
to 6, we drop the squares and cubes of the covariates, and then interactions and dummy variables. In specifications 3 and 6, the logits then simply use the covariates linearly. These estimates
are worse than those in Table 3, ranging from $835 to $1,774. But compared with the range of
estimates from Table 2, these remain concentrated. Furthermore, we are unable to find a partition
structure for the alternative specifications such that the pre-intervention characteristics are balanced within each stratum. There is a well-defined criterion to reject these alternative specifications. Indeed, the specification search begins with a linear specification, and adds higher-order
and interaction terms until within-stratum balance is achieved.
5.2 Sensitivity to Selection on Observables
One important assumption underlying propensity score methods is that all of the variables that
affect assignment to treatment and are correlated with the potential outcomes, Yi1 and Yi0, are observed. This assumption led us to restrict Lalonde’s data to the subset for which two (rather than
one) years of pre-intervention earnings data is available. In Table 5 (Panel B), we consider how
our estimators would fare in the absence of two years of pre-intervention earnings data by reestimating the treatment impact without making use of earnings in 1974. For PSID-1, the stratification estimators yield less reliable estimates than in Table 3, ranging from -$1,023 to $1,727 as
compared with $1,473 to $1,691, although the matching estimator is more robust. In contrast,
even though the estimates from the CPS are farther from the experimental benchmark than those
in Table 3 , they are still more concentrated
around the experimental estimates than the regression estimates in Panel B of Table 2.
This illustrates that the results are sensitive to the set of pre-intervention variables used.
For training programs, a sufficiently lengthy pre-intervention earnings history clearly is important. Table 5 also demonstrates the value of using multiple comparison groups. Even if we did
not know the experimental estimate, in looking at Table 5 we would be concerned that the variables that we observe do not control fully for
the differences between the treatment and comparison groups, because of variation in the estimates between the CPS and PSID. If all relevant variables are observed, then the estimates from
both groups should be similar (as they are in Table 3). When an experimental benchmark is not
available, multiple comparison groups are valuable because they can suggest the existence of
important unobservables .
6. CONCLUSION
This paper demonstrates how to estimate the treatment impact in an observational study using
propensity score methods.
These methods are assessed using Lalonde’s influential re-creation of a non-experimental
setting. Our results show that the estimates of the training effect are close to the benchmark experimental estimate, and are robust to the specification of the comparison group and the functional form used to estimate the propensity score. A researcher using our method would arrive at
estimates of the treatment impact ranging from $1,473 to $1,774, very close to the benchmark
unbiased estimate from the experiment of $1,794. Furthermore, our methods succeed for a transparent reason: they use only the subset of the comparison group that is comparable to the treatment group, and discard the complement. Although Lalonde attempts to follow this strategy in
his construction of other comparison groups, his method relies on an informal selection among
the pre-intervention variables. Our application illustrates that even among a large set of potential
comparison units, very few may be relevant. But it also illustrates that even a few comparison
units can be enough to estimate the treatment impact.
The methods we suggest are not relevant in all situations: there may be important unobservable covariates, for which the propensity score method cannot account. But rather than giving up, or relying on assumptions about the unobserved variables, propensity score methods may
offer both a diagnostic on the quality of the comparison group and a means to estimate the treatment impact.