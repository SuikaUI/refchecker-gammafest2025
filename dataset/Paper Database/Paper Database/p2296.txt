On the Failure of the Bootstrap for Matching
Estimators∗
Alberto Abadie†
Guido W. Imbens‡
First Draft: May 2004
This Draft: March 2005
Matching estimators are widely used for the evaluation of programs or treatments.
Often researchers use bootstrapping methods for inference.
justiﬁcation for the use of the bootstrap has been provided. Here we show that
the bootstrap is in general not valid, even in the simple case with a single continuous covariate when the estimator is root-N consistent and asymptotically normally
distributed with zero asymptotic bias. Due to the extreme non-smoothness of nearest neighbor matching, the standard conditions for the bootstrap are not satisﬁed,
leading the bootstrap variance to diverge from the actual variance. Simulations
conﬁrm the diﬀerence between actual and nominal coverage rates for bootstrap
conﬁdence intervals predicted by the theoretical calculations. To our knowledge,
this is the ﬁrst example of a root-N consistent and asymptotically normal estimator
for which the bootstrap fails to work.
JEL Classiﬁcation: C14, C21, C52
Keywords: Average Treatment Eﬀects, Bootstrap, Matching, Conﬁdence Intervals
∗We are grateful for comments by Peter Bickel.
Financial support for this research was
generously provided through NSF grants SES-0350645 (Abadie) and SES 0136789 (Imbens).
†John F. Kennedy School of Government, Harvard University, 79 John F. Kennedy Street,
Cambridge, MA 02138, and NBER. Electronic correspondence: alberto ,
 
‡Department of Economics, and Department of Agricultural and Resource Economics, University of California at Berkeley, 330 Giannini Hall, Berkeley, CA 94720-3880, and NBER.
Electronic correspondence: , 
Introduction
Matching methods have become very popular for the estimation of treatment eﬀects.1
Often researchers use bootstrap methods for conducting inference.2 Such methods have
not been formally justiﬁed, and due to the non-smooth nature of nearest neighbor or
matching methods there is reason for concern about their validity. On the other hand,
we are not aware of any examples where an estimator is root−N consistent, as well as
asymptotically normally distributed with zero asymptotic bias and yet where the standard bootstrap fails to deliver valid conﬁdence intervals.3 Here we resolve this question.
We show in a simple case with a single continuous covariate that the standard bootstrap
does indeed fail to provide asymptotically valid conﬁdence intervals and we provide some
intuition for this failure. We present theoretical calculations for the diﬀerence between
the bootstrap and nominal coverage rates. These theoretical calculations are supported
by Monte Carlo evidence. We show that the bootstrap conﬁdence intervals can have
over– as well as under coverage.
Alternative analytic variance estimators have been proposed by Abadie and Imbens
 . Since the standard bootstrap is shown to be invalid, together
with the subsampling bootstrap these are now the only variance estimators available that
are formally justiﬁed.
The rest of the paper is organized as follows. Section 2 reviews the basic notation
and setting of matching estimators. Section 3 presents theoretical results on the lack of
1E.g., Dehejia and Wahba, . See Rosenbaum and Imbens for surveys.
2A partial list of recent papers using matching with bootstrapped standard errors includes Agodini
and Dynarski , Dehejia and Wahba , Galasso and Ravaillon , Guarcello, Mealli,
and Rosati , Heckman, Ichimura and Todd , Ichino and Becker , Imai , Jalan
and Ravallion , Lechner , Myers, Olsen, Seftor, Young, and Tuttle Pradhan, and
Rawlings , Puhani , Sianesi , Smith and Todd .
Yamani, Lauer, Starling,
Pothier, Tuzcu, Ratliﬀ, Cook, Abdo, McNeil, Crowe, Hobbs, Rincon, Bott-Silverman, McCarthy and
Young .
3Familiar counterexamples where the estimator has no limiting normal distribution include estimating
the maximum of the support of a random variable , estimating the average
of a variable with inﬁnite variance , and super-eﬃcient estimators . It
should be noted that the above conditions do imply that the subsampling and other versions of the bootstrap where the size of the bootstrap
sample is smaller than the sample size are valid.
validity of the bootstrap for matching estimators, along with simulations that conﬁrm
the formal results. Section 4 concludes. The appendix contains proofs.
Basic Model
Consider the following standard set up for estimating treatment eﬀects under exogeneity
or unconfoundedness . We are interested in the evaluation of a treatment on the basis of data
on outcomes for treated and control units and covariates. We have a random sample of
N0 units from the control population, and a random sample of N1 units from the treated
population. Each unit is characterized by a pair of potential outcomes, Yi(0) and Yi(1),
denoting the outcomes under the control and active treatment respectively. We observe
Yi(0) for units in the control sample, and Yi(1) for units in the treated sample. For
all units we observe a scalar covariate Xi.4 Let Wi indicate whether a unit is from the
control sample (Wi = 0) or the treatment group (Wi = 1). We observe for each unit the
triple (Xi, Wi, Yi) where Yi = Wi Yi(1) + (1 −Wi) Yi(0) is the observed outcome. Let X
denote the N-vector with typical element Xi, and similar for Y and W. Also, let X0
denote the N-vector with typical element (1 −Wi) Xi, and X1 the N-vector with typical
element Wi Xi. We make the following two assumptions that will justify using matching
Assumption 2.1: (unconfoundedness)
Yi(0), Yi(1)
Assumption 2.2: (overlap) For some c > 0,
c ≤Pr(Wi = 1|Xi) ≤1 −c.
4To simplify the calculations we focus on the case with a scalar covariate. With higher dimensional
covariates there is the additional complication of biases that may dominate the variance even in large
samples. See for a discussion on this Abadie and Imbens .
In this discussion we focus on the average treatment eﬀect for the treated:5
τ = E[Yi(1) −Yi(0)|Wi = 1].
We estimate this by matching each treated unit to the closest control, and then averaging
the within-pair diﬀerences. Here, we focus on the case of matching with replacement.
Formally, for all treated units i (that is, units with Wi = 1), let Di be the distance
to the closest (control) match:
j=1,...,N:Wj=0 |Xi −Xj|.
Then deﬁne
J (i) = {j ∈{1, 2, . . . , N} : Wj = 0, |Xi −Xj| ≤Di},
be the set of closest matches for treated unit i. If unit i is a control unit, then J (i)
is deﬁned to be the empty set.
When at least one of the covariates is continuously
distributed, the set J (i) will consist of a single index with probability one, but for the
bootstrap samples there will often be more than one index in this set. Next, deﬁne
be the average outcome in the set of matches, where #J (i) is the number of elements of
the set J (i). The matching estimator for τ is then
Yi −ˆYi(0)
For the subsequent discussion it is useful to write the estimator in a diﬀerent way. Let
Ki denote the weighted number of times unit i is used as a match (if unit i is a control
unit, with Ki = 0 if unit i is a treated unit):
Ki = (1 −Wi)
Wj 1{i ∈J (j)}/#J (j).
5In many cases interest is in the average eﬀect for the entire population. We focus here on the average
eﬀect for the treated because it simpliﬁes the calculations below. Since the overall average eﬀect is the
weighted sum of the average eﬀect for the treated and the average eﬀect for the controls it suﬃces to
show that the bootstrap is not valid for one of the components.
Then we can write
(Wi −(1 −Wi) Ki)Yi.
AI propose two variance estimators.
V AI,I = 1
Wi −(1 −Wi)Ki
V AI,II = 1
Yi −ˆYi(0) −ˆτ
Ki −1/#J (i)
Wi(Xi) is an unbiased estimator (although inconsistent) estimator of the conditional variance of Yi given Wi and Xi based on matching.
Abadie and Imbens show that the ﬁrst variance estimator, V AI,I is consistent for the
normalized conditional variance:
N 1 V(ˆτ|X).
The second variance estimator, V AI,II, is consistent for the normalized marginal variance:
N 1 V(ˆτ).
The limiting variances diﬀer by the normalized variance of the conditional average treatment eﬀect:
N 1 V(ˆτ) −
N 1 V(ˆτ|X) =
N 1 V(E[τ|X]).
The Bootstrap
We consider two versions of the bootstrap in this discussion. The ﬁrst centers the bootstrap distribution around the estimate ˆτ from the original sample, and the second centers
it around the mean of the bootstrap distribution of ˆτ.
Consider a sample Z = (X, W, Y) with N0 controls and N1 treated units, and matching estimator ˆτ = t(Z). We construct a bootstrap sample with N0 controls and N1 treated
by sampling with replacement from the two subsamples. We then calculate the bootstrap
estimate ˆτb. The ﬁrst way of deﬁning the bootstrap variance is
B = vI(Z) = E
(ˆτb −ˆτ)2¯¯ Z
Second, we consider the population variance of the bootstrap estimator. In other words,
we estimate the variance by centering the bootstrap estimator at its mean rather than
at the original estimate ˆτ:
B = vII(Z) = E
(ˆτb −E [ˆτb|Z])2¯¯ Z
Although these bootstrap variances are deﬁned in terms of the original sample Z, in
practice an easier way to calculate them is by drawing B bootstrap samples. Given B
bootstrap samples with bootstrap estimates ˆτb, for b = 1, . . . , B, we can obtain unbiased
estimators for these two variances as
(ˆτb −ˆτ)2 ,
We will focus on the ﬁrst bootstrap variance, V I
B and its unconditional expectation
B]. We shall show that in general N1 E[V I
B] does not converge to N1 V(ˆτ) and therefore
that this bootstrap estimator for the variance is not valid, in the sense that it does not
lead to conﬁdence intervals with large sample coverage equal to nominal coverage. In
some cases it will have coverage lower than nominal, and in other cases it will have
coverage rates higher than nominal. This will indirectly imply that conﬁdence intervals
based on V II
B are not valid either. Because
(ˆτb −ˆτ)2¯¯ Z
(ˆτb −E [ˆτb|Z])2¯¯ Z
it follows that E[V II
B ] ≥E[V I
B]. Thus in the cases where the ﬁrst bootstrap has actual
coverage lower than nominal coverage, it follows that the second bootstrap cannot be
valid either.
In most standard settings (i.e., outside of matching) both bootstrap variances would
lead to valid conﬁdence intervals. In fact, in most cases V I
B and V II
B would be identical as
typically ˆτ = E[ˆτb|Z]. For example, if we are interested in constructing a conﬁdence interval for the population mean µ = E[X] given a random sample X1, . . . , XN, the expected
value of the bootstrap statistic, E[ˆµb|X1, . . . , XN], is equal to the sample average for the
original sample, ˆµ = P
i Xi/N. In the setting studied in the current paper, however, this
is not the case and the two variance estimators will lead to diﬀerent conﬁdence intervals
with potentially diﬀerent coverage rates.
An Example where the Bootstrap Fails
In this section we discuss in detail a speciﬁc example where we can calculate both the
exact variance for ˆτ and the approximate (asymptotic) bootstrap variance, and show
that these diﬀer.
Data Generating Process
We consider a special case where the following assumptions are satisﬁed:
Assumption 3.1: The marginal distribution of the covariate X is uniform on the interval 
Assumption 3.2: The ratio of treated and control units is N1/N0 = α for some α ∈
Assumption 3.3: The propensity score e(x) = Pr(Wi = 1|Xi = x) is constant as a
function of x.
Assumption 3.4: The distribution of Yi(1) is degenerate with Pr(Yi(1) = τ) = 1, and
the conditional distribution of Yi(0) given Xi = x is normal with mean zero and variance
The implication of Assumptions 3.2 and 3.3 is that the propensity score is e(x) = α/(1+
Exact Variance and Large Sample Distribution
The data generating process implies that conditional on X = x the treatment eﬀect
is equal to E[Y (1) −Y (0)|X = x] = τ for all x.
Combined with unconfoundedness
(Assumption 2.1)(i) this implies that the average treatment eﬀect for the treated is τ.
Under this data generating process P
i Wi Yi/N1 = P
i Wi Yi(1)/N1 = τ, and so we can
simplify the expression for the estimator given in (2.3) relative to the estimand to
ˆτ −τ = −1
(1 −Wi) Ki Yi.
Conditional on X and W the only stochastic component of ˆτ is Y. By Assumption 3.4
the Yi are mean zero, unit variance, and independent of X. Thus E[ˆτ −τ|X, W] = 0.
Because (i) E[Yi Yj|Wi = 0, X, W] = 0 for i ̸= j, (ii) E[Y 2
i |Wi = 0, X, W] = 1 and (iii)
Ki is a deterministic function of X and W, it also follows that the conditional variance
of ˆτ given X and W is
V(ˆτ|X, W) = 1
(1 −Wi) K2
Because V(E[ˆτ|X, W]) = V(0) = 0, the (exact) unconditional variance of the matching
estimator is therefore equal to the expected value of the conditional variance:
V(ˆτ) = N0
Lemma 3.1: (Exact Variance of Matching Estimator)
Suppose that Assumptions 2.1, 2.2, and 3.1-3.4 hold. Then
(i) the exact variance of the matching estimator is
(N1 −1)(N0 + 8/3)
N1(N0 + 1)(N0 + 2),
(ii) as N →∞,
N1 V(ˆτ) →1 + 3
and (iii),
N 1 (ˆτ −τ)
All proofs are given in the Appendix.
The Bootstrap I Variance
Now we analyze the properties of the bootstrap variance, V I
B in (2.4). As before, let
Z = (X, W, Y) denote the original sample. Also, let t( ) be the function that deﬁnes the
estimator, so that ˆτ = t(Z) is the estimate based on the original sample. Let Zb denote
the b-th bootstrap sample, and ˆτb = t(Zb) the corresponding b-th bootstrap estimate,
for b = 1, . . . , B. As noted before, we draw bootstrap samples conditional on the two
subsample sizes N0 and N1. We will look at the distribution of statistics both conditional
on the original sample (denoted by |Z), as well as over replications of the original sample
drawn from the same distribution. In this notation,
(ˆτb −ˆτ)2¯¯ Z
(ˆτb −ˆτ)2¤
is the expected bootstrap variance. We will primarily focus on the normalized variance
Lemma 3.2: (Bootstrap Variance I) Suppose that Assumptions 3.1-3.4 hold. Then,
2 α 5 exp(−1) −2 exp(−2)
3 (1 −exp(−1))
+ 2 exp(−1).
Recall that the limit of the normalized variance of ˆτ is 1+(3/2) α. For small values of α the
bootstrap variance exceeds the true variance by the third term in (3.10), 2 exp(−1) ≈
0.74, or 74%.
For large α the second term in (3.10) dominates and the ratio of the
bootstrap and true variance is equal to the factor in the second term of (3.10) multiplying
α (3/2). Since (5 exp(−1) −2 exp(−2))/(3 (1 −exp(−1))) ≈0.83, it follows that as α
increases, the ratio of the bootstrap variance to the actual variance asymptotes to 0.83,
suggesting that bootstrap variance can under as well as over estimate the true variance.
So far, we have discussed the relation between the limiting variance of the estimator
and the average bootstrap variance. We end this section by a discussion of the implications of the previous two lemmas for the validity of the bootstrap. The ﬁrst version of the
bootstrap provides a valid estimator of the asymptotic variance of the simple matching
estimator if:
(bτb −bτ)2¯¯ Z
Lemma 3.1 shows that:
N1Var(bτ) −→1 + 3
Lemma 3.2 shows that
(bτb −bτ)2¤
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1).
Assume that the ﬁrst version of the bootstrap provides a valid estimator of the asymptotic
variance of the simple matching estimator. Then,
(bτb −bτ)2¯¯ Z
Because N1E [(bτb −bτ)2| Z] ≥0, it follows by Fatou’s Lemma, that, as N →∞
lim inf N1E
(bτb −bτ)2¯¯ Z
(bτb −bτ)2¤
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1).
However, the algebraic inequality
2 α ≤1 + 3
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1),
does not hold for large enough α. As a result, the ﬁrst version of the bootstrap does not
provide a valid estimator of the asymptotic variance of the simple matching estimator.
The second version of the bootstrap provides a valid estimator of the asymptotic
variance of the simple matching estimator if:
(bτb −E[bτb|Z])2¯¯ Z
Assume that the second version of the bootstrap provides a valid estimator of the asymptotic variance of the simple matching estimator. Then,
(bτb −E[bτb|Z])2¯¯ Z
Notice that E [(bτb −E[bτb|Z])2| Z] ≤E [(bτb −bτ)2| Z]. By Fatou’s Lemma, as N →∞
lim inf N1E
(bτb −E[bτb|Z])2¯¯ Z
lim inf N1E
(bτb −bτ)2¯¯ Z
(bτb −bτ)2¤
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1).
As a result, the second version of the bootstrap does not provide a valid estimator of the
asymptotic variance of the simple matching estimator.
Simulations
We consider three designs: N0 = N1 = 100 (Design I), N0 = 100, N1 = 1000 (Design II),
and N0 = 1000, N1 = 100 (Design III), We use 10000 replications, and 100 bootstrap
samples in each replication. These designs are partially motivated by Figure 1, which
gives the ratio of the limit of the expectation of the bootstrap variance (given in equation
(3.10)) to limit of the actual variance (given in equation (3.8)), for diﬀerent values of α.
On the horizontal axis is the log of α. As α converges to zero the ratio converges to 1.62.
At α = 0 the variance ratio is 1.13, and as α goes to inﬁnity the ratio converges to 0.88.
The vertical dashed lines indicate the three designs (α = 0.1, α = 1, and α = 10).
The simulation results are reported in Table 1.
The ﬁrst row of the table gives
the theoretical (exact) variances, as calculated in equation (3.7), normalized by N1. The
second and third rows present the normalized versions of the two Abadie-Imbens variance
estimators. The second row is the variance for the conditional average treatment eﬀect for
the treated (conditional on the covariates), and the third is the variance for the population
average treatment eﬀect. Both are valid in large samples. Since the conditional average
treatment eﬀect is zero for all values of the covariates, these two estimate the same
object in large samples. In brackets standard errors for these averages are presented. Of
most interest is to see the diﬀerence between these variance estimates and the theoretical
variance. For example, for design I, the AI Var I is on average 2.449, with a standard
error of 0.006. The theoretical variance is 2.480, so the diﬀerence between the theoretical
and AI variance I is approximately 1%, although it is statistically signiﬁcant at about
5 standard errors. Given the theoretical justiﬁcation, this diﬀerence is a ﬁnite sample
phenomenon.
The fourth row presents the theoretical calculation for the asymptotic bootstrap I
variance, as given in (3.10). The ﬁfth and sixth row give the averages of the estimated
bootstrap variances. These variances are estimated for each replication using 100 bootstrap samples, and then averaged over all replications. Again it is interesting to compare
the average of the estimated bootstrap variance to the theoretical variance, rows 5 and
4. The diﬀerence between rows 4 and 5 is small relative to the diﬀerence between the
theoretical variance and the theoretical bootstrap diﬀerence, but the diﬀerence is significantly diﬀerent from zero. These diﬀerences are the results of small sample sizes. The
limited number of bootstrap replications makes these averages noisier than they would
otherwise be, but it does not aﬀect the average diﬀerence.
The next two panels of the table gives coverage rates, ﬁrst for nominal 90% conﬁdence
intervals and then for nominal 95% conﬁdence intervals. The ﬁrst row constructs a 90%
conﬁdence interval by adding and subtracting 1.645 times the standard error based on the
theoretical exact variance (3.7). Comparison of this coverage rate to its nominal level is
informative about the quality of the normal approximation to the sampling distribution.
This appears to be good in all three designs and both levels (90%, and 95%).
second row calculates conﬁdence intervals the same way but using the Abadie-Imbens I
variance estimator. The third row calculates conﬁdence intervals the same way but using
the Abadie-Imbens II variance estimator. Both give good coverage rates, statistically
indistinguishable from the nominal levels given the number of replications (10,000).
The fourth row of the second panel calculates the coverage rate one would expect for
the bootstrap based on the diﬀerence between the theoretical bootstrap variance and the
theoretical variance. To be precise, consider Design I. The theoretical variance is 2.480.
The theoretical bootstrap variance is 2.977, or 20% larger. The ratio of the variances
is 2.977/2.480 = 1.2005. Hence, if the estimator itself is normally distributed centered
around τ = 0 and with variance 2.480, we would expect the coverage rate of a 90%
bootstrap conﬁdence interval to be Φ −Φ = 0.929. The
following row gives the actual coverage rate for a 90% conﬁdence interval obtained by
adding and subtracting 1.645 times the square root of the theoretical bootstrap variance
(2.977 for Design I), calculated by simulation with 10,000 replications. For Design I this
number is 0.931. The fact that this is very close to the coverage we expected for the bootstrap (0.929) suggests that both the normal approximation for the estimator is accurate
(conﬁrming results in the previous rows), and that the mean of the bootstrap variance is
a good indicator of the center of the distribution of the bootstrap variance. The last two
rows of this panel give the coverage rates for bootstrap conﬁdence intervals obtained by
adding and subtracting 1.645 times the square root of the estimated bootstrap variance
in each replication, again over the 10,000 replications. The standard errors for the coverage rates reﬂect the uncertainty coming from the ﬁnite number of replications (10,000).
They are equal to
p (1 −p)/Ns where for the second panel p = 0.9 and for the third
panel p = 0.95, and Ns = 10, 000 is the number of replications.
The third panel gives the corresponding numbers for 95% conﬁdence intervals.
Clearly the theoretical calculations correspond fairly closely to the numbers from
the simulations. The theoretically predicted coverage rates for the bootstrap conﬁdence
intervals are very close to the actual coverage rates. They are diﬀerent from nominal levels
in substantially important and statistically highly signiﬁcant magnitudes. In Designs
I and III the bootstrap has coverage larger the nominal coverage.
In Design II the
bootstrap has coverage smaller than nominal. In neither case the diﬀerence is huge, but
it is important to stress that this diﬀerence will not disappear with a larger sample size,
and that it may be more substantial for diﬀerent data generating processes.
The bootstrap calculations in this table are based on 100 bootstrap replications.
Theoretically one would expect that using a small number of bootstrap replications lowers
the coverage rate of the constructed conﬁdence intervals as one uses a noise measure of the
variance. Increasing the number of bootstrap replications signiﬁcantly for all designs was
infeasible as matching is already computationally expensive.6 We therefore investigated
the implications of this choice for the ﬁrst design which is the fastest to run. For the
6Each calculation of the matching estimator requires N1 searches for the minimum of an array of
length N0, so that with B bootstrap replications and R simulations one quickly requires large amounts
of computer time.
same 10,000 replications we calculated both the coverage rates for the 90% and 95%
conﬁdence intervals based on 100 bootstrap replications and based on 1,000 bootstrap
replications. For the bootstrap I the coverage rate for the 90% conﬁdence interval was
0.002 (s.e.
0.001) higher with 1,000 bootstrap replications than with 100 bootstrap
replications, and the coverage rate for the 95% conﬁdence interval was 0.003 (s.e., 0.001)
higher with 1,000 bootstrap replications than with 100 bootstrap replications. Since the
diﬀerence between the bootstrap coverage rates and the nominal coverage rates for this
design are 0.031 and 0.022 for the 90% and 95% conﬁdence intervals respectively, the
number of bootstrap replications can only explain approximately 6-15% of the diﬀerence
between the bootstrap and nominal coverage rates. We therefore conclude that using
more bootstrap replications would not substantially change the results in Table 1.
Conclusion
In this note we show theoretically that the standard bootstrap is generally not valid
for matching estimators. This is somewhat surprising because in the case with a scalar
covariate the matching estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. However, the extreme non-smooth nature of matching
estimators and the lack of evidence that the estimator is asymptotically linear suggests
that the validity of the bootstrap may be in doubt. We provide details of a set of special
cases where it is possible to work out the exact variance of the estimator as well as the
approximate bootstrap variance. We show that in this case bootstrap conﬁdence intervals can lead to under as well as over coverage. A small Monte Carlo study supports
the theoretical calculations. The implications of the theoretical arguments are that for
matching estimators one should use the variance estimators developed by Abadie and
Imbens or the subsampling bootstrap .
Before proving Lemma 3.1 we introduce some additional notation. Let Mj be the index of the closest
match for unit j. That is, if Wj = 1, then Mj is the unique index (ignoring ties), Mj with WMj = 0,
such that ∥Xj −XMj∥≤∥Xj −Xi∥, for all i such that Wi = 0. If Wj = 0, then Mj = 0. Let Ki be the
number of times unit i is the closest match:
Ki = (1 −Wi)
Wj 1{Mj = i}.
Following this deﬁnition Ki is zero for treated units. Using this notation, we can write the estimator for
the average treatment eﬀect as
Yi (Wi −(1 −Wi) Ki)
Also, let Pi be the conditional probability that the closest match for a randomly chosen treated unit j
is unit i conditional on both the vector of treatment indicators W and on vector of covariates for the
control units X0:
Pi = Pr(Mj = i|Wj = 1, W, X0).
For treated units we deﬁne Pi = 0.
First we investigate the ﬁrst two moments of Ki, starting by studying the conditional distribution of Ki
given X0 and W.
Lemma A.1: (Conditional Distribution and Moments of K(i))
Suppose that assumptions 3.1-3.3 hold. Then, the distribution of Ki conditional on Wi = 0 is binomial
with parameters (N1, Pi):
Ki|Wi = 0, W, X0 ∼B(N1, Pi).
Proof: By deﬁnition Ki = (1 −Wi) PN
j=1 Wj 1{Mj = i}. The indicator 1{Mj = i} is equal to one if
the closest control unit for Xj is i. This event has probability P 1
i . In addition, the events 1{Mj1 = i}
and 1{Mj2 = i} are independent conditional on W and X0. Because there are N1 treated units the sum
of these indicators follows a binomial distribution with parameters N1 and Pi.
This implies the following conditional moments for Ki:
E[Ki|W, X0] = (1 −Wi) N1 Pi,
i |W, X0] = (1 −Wi)
N1 Pi + N1 (N1 −1) P 2
To derive the marginal moments of Ki we need to analyze the properties of the random variable Pi.
Exchangeability of the units implies that the marginal expectation of Pi given N0, N1 and Wi = 0 is
equal to 1/N0. For deriving the second moment of Pi it is helpful to express Pi in terms of the order
statistics of the covariates for the control group. For control unit i let ι(i) be the order of the covariate
for the ith unit among control units:
(1 −Wj) 1{Xj ≤Xi}.
Furthermore, let X0(i) be the ith order statistic of the covariates among the control units, so that
X0(i) ≤X0(i+1) for i = 1, . . . , N0 −1, and for control units X0(ι(i)) = Xi. Ignoring ties, a treated unit
with covariate value x will be matched to control unit i if
X0(ι(i)−1) + X0(ι(i))
≤x ≤X0(ι(i)+1) + X0(ι(i))
if 1 < ι(i) < N0. If ι(i) = 1, then x will be matched to unit i if
x ≤X0(2) + X0(1)
and if ι(i) = N0, x will be matched to unit i if
X0(N0−1) + X0(N0)
To get the value of Pi we need to integrate the density f1(x) over these sets. With a uniform distribution
for the covariates in the treatment group (f1(x) = 1, for x ∈ ), we get the following representation
(X0(2) + X0(1))/2
if ι(i) = 1,
X0(ι(i)+1) −X0(ι(i)−1)
if 1 < ι(i) < N0,
1 −(X0(N0−1) + X0(N0))/2
if ι(i) = N0.
The representation of Pi as a linear function of order statistics facilitates deriving its distribution. In
particular with Xi|Wi = 0 uniform on , Pi can be written as a Beta random variable (if 1 < ι(i) < N)
or as a linear combination of two correlated Beta random variables in the two boundary cases (ι(i) = 1
or ι(i) = N. This leads to the following result:
Lemma A.2: (Moments of Pi)
Suppose that Assumptions 3.1–3.3 hold. Then
(i), the second moment of Pi conditional on Wi = 0 is
i |Wi = 0] =
2N0(N0 + 1)(N0 + 2),
and (ii), the Mth moment of Pi is bounded by
i |Wi = 0] ≤
Proof: First, consider (i). Since the Xi conditional on Wi = 0 come from a uniform distribution on the
interval , it follows that we can write
where the El are independent unit exponential. Hence
(2E0 + E1)
if ι(i) = 1,
Eι(i) + Eι(i)+1
if 1 < ι(i) < N0,
(2EN0 + EN0−1)
if ι(i) = N0.
The ratio PL+K
l=L+1 El/ PN
l=0 El has a Beta distribution with parameters K and N +1−K. Recall that a
Beta distribution with parameters α and β has a mean equal to α/(α + β) and second moment is equal
to (α + 1)α)/((α + β)(α + β + 1)).
So, for interior i (i such that 1 < ι(i) < N0), we have that
2 Pi ∼Beta (2, N0 −1) .
The ﬁrst and second moment of these P(i) are
E[Pi|1 < ι(i) < N0, Wi = 0] = 1
i |1 < ι(i) < N0, Wi = 0
(N0 + 1)2 +
4(N0 + 1)2(N0 + 2) = 3
(N0 + 1)(N0 + 2).
For the smallest and largest observation it is a little trickier. For i such that ι(i) = 1 or ι(i) = N0, the
distribution of Pi is as the distribution of V1 + V2/2, where
El ∼Beta (1, N0) ,
El ∼Beta (1, N0) ,
d∼(E1 + E2)
El ∼Beta (2, N0 −1) ,
with all independent unit exponential El. To get the ﬁrst and second moment of V1 + V2/2 we need the
ﬁrst and second moment of V1 and V2 (which are the identical) and the second moment of V1 + V2:
E[V1] = E[V2] =
1 ] = E[V 2
(N0 + 1)(N0 + 2),
E[V1 + V2] =
E[(V1 + V2)2] =
(N0 + 1)(N0 + 2).
Then we can back out the expectation of V1 V2:
E[V1 V2] = 1
E[(V1 + V2)2] −E[V 2
1 ] −E[V 2
(N0 + 1)(N0 + 2).
Then the ﬁrst two moments of V1 + V2/2 are
E[Pi|ι(i) ∈{1, N0}, Wi = 0] = E[V1 + V2/2] = 3
i |ι(i) ∈{1, N0}, Wi = 0] = E[(V1 + V2/2)2] = E[V 2
1 ] + E[V1 V2] + 1
2(N0 + 1)(N0 + 2),
for the two boundary units i where ι(i) ∈{1, N0}.
Averaging over all units includes two units at the boundary and N0 −2 interior values. Hence:
E[Pi|Wi = 0] = N0 −2
(N0 + 1) + 2
(N0 + 1) = 1
i |Wi = 0] = N0 −2
(N0 + 1)(N0 + 2) + 2
(N0 + 1)(N0 + 2) =
2N0(N0 + 1)(N0 + 2).
For (ii) note that by (A.2) Pi is less than (Ei−1 + Ei)/ PN0
l=0 El, which has a Beta distribution with
parameters 2 and N0 −1.
Hence the moments of Pi are bounded by those of a Beta distribution
with parameters 2 and N0 −1. The Mth moment of a Beta distribution with parameters α and β is
j=0 (α + j)/(α + β + j). This is bounded by (α + M −1)M/(α + β)M, which completes the proof of
the second part of the Lemma. □
Proof of Lemma 3.1:
First we prove (i). The ﬁrst step is to calculate E[K2
i |Wi = 0]. Using Lemmas A.1 and A.2,
i |Wi = 0] = N1 E[Pi|Wi = 0] + N1 (N1 −1) E[P 2
i |Wi = 0]
N1(N1 −1)(N0 + 8/3)
N0(N0 + 1)(N0 + 2) .
Substituting this into (3.6) we get:
V(ˆτ) = N0
E[(Ki)2|Wi = 0] = 1
(N1 −1)(N0 + 8/3)
N1(N0 + 1)(N0 + 2),
proving part (i).
Next, consider part (ii). Multiply the exact variance of ˆτ by N1 and substitute N1 = α N0 to get
N1 V(ˆτ) = 1 + 3
(α N0 −1)(N0 + 8/3)
(N0 + 1)(N0 + 2)
Then take the limit as N0 →∞to get:
N→∞N1 V(ˆτ) = 1 + 3
Finally, consider part (iii). Let S(r, j) be a Stirling number of the second kind. The Mth moment of
Ki given W and X0 is :
i |X0, Wi = 0] =
S(M, j)N0! P j
Therefore, applying Lemma A.2 (ii), we obtain that the moments of Ki are uniformly bounded:
i |Wi = 0]
S(M, j)N0!
(N0 −j)! E[P j
i |Wi = 0] ≤
S(M, j)N0!
S(M, j)(1 + M)j.
Notice that
i |Wi = 0] →1 + 3
i |Wi = 0) →0,
because cov(K2
j |Wi = Wj = 0, i ̸= j) ≤0 . Therefore:
Finally, we write
where εi = −(1 −Wi) Ki Yi. Conditional on X and W the εi are independent, with the distribution for
εi normal N(0, K2
i )). Hence, for any c ∈R:
N1(ˆτ −τ) ≤c
where Φ(·) is the cumulative distribution function of a standard normal variable. Integrating over the
distribution of X and W yields:
N1(ˆτ −τ) ≤c
Now, Slustky’s Theorem implies (iii).
Next we introduce some additional notation. Let Rb,i be the number of times unit i is in the bootstrap
sample. In addition, let Db,i be an indicator for for inclusion of unit i in the bootstrap sample, so
that Db,i = 1{Rb,i > 0}. Let Nb,0 = PN
i=1(1 −Wi) Db,i be the number of distinct control units in the
bootstrap sample. Finally, deﬁne the binary indicator Bi(x), for i = 1 . . . , N to be the indicator for the
event that in the bootstrap sample a treated unit with covariate value x would be matched to unit i .
That is, for this indicator to be equal to one the following three conditions need to be satisﬁed: (i) unit
i is a control unit, (ii) unit i is in the bootstrap sample, and (iii) the distance between Xi and x is less
than or equal to the distance between x and any other control unit in the bootstrap sample. Formally:
if |x −Xi| ≤mink:Wk=0,Db,k=1 |x −Xk|,
and Db,i = 1, Wi = 0,
otherwise.
For the N units in the original sample, let Kb,i be the number of times unit i is used as a match in the
bootstrap sample.
Kb,i = Db,i
Wj Bi(Xj) Rb,j.
We can write the estimated treatment eﬀect in the bootstrap sample as
Wi Rb,i Yi −(1 −Wi) Kb,i Yi.
Because Yi(1) = 0 by Assumption 3.4, and with Kb,i = 0 if Wi = 1, this reduces to
The diﬀerence between the original estimate ˆτ and the bootstrap estimate ˆτb is
ˆτb −ˆτ = 1
(Ki −Kb,i) Yi =
(Ki −Kb,i) Yi.
We will calculate the expectation
B] = N1 · E[(ˆτb −ˆτ)2]
(1 −Wi) (Kb,i −Ki) Yi · (1 −Wj) (Kb,j −Kj) Yj.
Using the facts that E[Y 2
i |Wi = 0] = 1, and E[Yi Yj|X, W] = 0 if i ̸= j, this is equal to
(Kb,i −Ki)2|Wi = 0
The ﬁrst step in deriving this expectation is to collect and establish some properties of Db,i, Rb,i, Nb,0,
and Bi(x).
Lemma A.3: (Properties of Db,i, Rb,i, Nb,0, and Bi(x))
Suppose that Assumptions 3.1-3.3 hold. Then, for w ∈{0, 1}, and n ∈{1, . . . , N0}
Rb,i|Wi = w, Z ∼B(Nw, 1/Nw),
Db,i|Wi = w, Z ∼B
1, 1 −(1 −1/Nw)Nw¢
Pr(Nb,0 = n) =
Pr(Bi(Xj) = 1|Wj = 1, Wi = 0, Db,i = 1, Nb,0) =
(v) for l ̸= j
Pr(Bi(Xl) Bi(Xj) = 1|Wj = Wl = 1, Wi = 0, Db,i = 1, Nb,0) =
2Nb,0(Nb,0 + 1)(Nb,0 + 2),
E[Nb,0/N0] = 1 −(1 −1/N0)N0 →1 −exp(−1),
V(Nb,0) = (N0 −1) (1 −2/N0)N0 +(1 −1/N0)N0 −N0 (1 −1/N0)2N0 →exp(−1)·(1−2 exp(−1)).
Proof: Parts (i), (ii), and (iv) are trivial. Part (iii) follows easily from equation (3.6) in page 110 of
Johnson and Kotz . Next, consider part (v). First condition on X0b and Wb, and suppose that
Db,i = 1. The event that a randomly choosen treated unit will be matched to control unit i conditional on
depends on the diﬀerence in order statistics of the control units in the bootstrap sample. The equivalent
in the original sample is Pi. The only diﬀerence is that the bootstrap control sample is of size N0,b. The
conditional probability that two randomly choosen treated units are both matched to control unit i is
the square of the diﬀerence in order statistics. It marginal expectation is the equivalent in the bootstrap
sample of E[P 2
i |Wi = 0], again with the sample size scaled back to Nb,0. Parts (vi) and (vii) can be
derived by making use of equation (3.13) on page 114 in Johnson and Kotz .
Next, we prove a general result for the bootstrap. Consider a sample of size N, indexed by i = 1, . . . , N.
Let Db,i be an indicator whether observation i is in bootstrap sample b. Let Nb = PN
i=1 Db,i be the
number of distinct observations in bootstrap sample b.
Lemma A.4: (Bootstrap) For all m ≥0:
1 −exp(−1)
Proof: From parts (vi) and (vii) of Lemma A.3 we obtain that Nb/N
p→1 −exp(−1). Convergence
of moments for the ﬁrst results follows from the fact that (N −Nb)/N ≤1. For the second result,
convergence of moments follows easily from the known fact that the tails of the occupancy distribution
in Lemma A.3 (iii) have an exponential bound .
Lemma A.5: (Approximate Bootstrap K Moments)
Suppose that Assumption 3.1 hold. Then,
b,i | Wi = 0] →2α + 3
(1 −exp(−1)),
E[Kb,i Ki|Wi = 0] →(1 −exp(−1)) ·
+ α2 · exp(−1).
Proof: First we prove part (i). Notice that for i, j, l, such that Wi = 0, Wj = Wl = 1
(Rb,j, Rb,l) ⊥⊥Db,i, Bi(Xj), Bi(Xl).
Notice also that {Rb,j : Wj = 1} are exchangeable with:
Rb,j = N1.
Therefore, for Wj = Wl = 1:
cov(Rb,j, Rb,l) = −var(Rb,j)
(N1 −1) = −1
As a result,
E[Rb,j Rb,l | Db,i = 1, Bi(Xj) = Bi(Xl) = 1, Wi = 0, Wj = Wl = 1, j ̸= l]
E[Rb,j | Db,i = 1, Bi(Xj) = Bi(Xl) = 1, Wi = 0, Wj = Wl = 1, j ̸= l]
Using the results from the two previous lemmas:
N0 Pr (Bi(Xj) = 1 | Db,i = 1, Wj = 1, Wi = 0) −→
1 −exp(−1),
0 Pr (Bi(Xj)Bi(Xl) = 1 | Db,i = 1, Wi = 0, Wj = Wl = 1, l ̸= j) −→3
1 −exp(−1)
b,i | Wi = 0]
WjWl Bi(Xj) Bi(Xl) Rb,j Rb,l
Wj Bi(Xj) R2
Wj Wl Bi(Xj) Bi(Xl) Rb,j Rb,l
b,j | Db,i = 1, Bi(Xj) = 1, Wj = 1, Wi = 0
Pr (Bi(Xj) = 1 | Db,i = 1, Wj = 1, Wi = 0)
× Pr(Wj = 1|Db,i = 1, Wi = 0) Pr(Db,i = 1 | Wi = 0)
E [Rb,j Rb,l| Db,i = 1, Bi(Xj) = Bi(Xl) = 1, Wj = Wl = 1, Wi = 0]
× Pr (Bi(Xj)Bi(Xl) = 1 | Db,i = 1, Wj = Wl = 1, Wi = 0)
× Pr(Wj = Wl = 1|Db,i = 1, Wi = 0) Pr(Db,i = 1 | Wi = 0)
(1 −exp(−1)).
This ﬁnishes the proof of part (i). Next, we prove part (ii).
E[Kb,i | X, W, Db,i = 1, Wi = 0, Nb,0] = E
Wj Rb,j Bi(Xj)
¯¯¯¯¯ X, W, Db,i = 1, Wi = 0, Nb,0
Wj E[Rb,j | X, W, Db,i = 1, Wi = 0, Wj = 1, Bi(Xj)] Bi(Xj)
¯¯¯¯¯ X, W, Db,i = 1, Wi = 0, Nb,0
¯¯¯¯¯ X, W, Db,i = 1, Wi = 0, Nb,0
= Ki + (N1 −Ki)E[Bi(Xj) | X, W, Db,i = 1, Wi = 0, Wj = 1, Mj ̸= i, Nb,0].
For some 0 < δ < 1, let cL(N0) = N δ
0 and cU(N0) = N0 −N δ
0 . For cL(N0) ≤i ≤cU(N0) and large
enough N0:
E[Bi(Xj) | X, W, Db,i = 1, Wi = 0, Wj = 1, Mj ̸= i, Nb,0]
= X0(ι(i)+2) −X0(ι(i)+1)
+ X0(ι(i)+3) −X0(ι(i)+2)
N0 −Nb,0 −1
+ X0(ι(i)−1) −X0(ι(i)−2)
+ X0(ι(i)−2) −X0(ι(i)−1)
N0 −Nb,0 −1
Using the results in Johnson, Kotz, and Balakrishnan , page 280, for l ≥1:
·µX0(ι(i)+l+1) −X0(ι(i)+l)
¶ µX0(ι(i)+1) −X0(ι(i)−1)
2(N0 + 1)(N0 + 2),
·µX0(ι(i)−l) −X0(ι(i)−l−1)
¶ µX0(ι(i)+1) −X0(ι(i)−1)
2(N0 + 1)(N0 + 2).
Therefore,
E[Kb,iKi −K2
i |Wi = 0, Db,i = 1] −→
1 −exp(−1) α2,
E[Kb,iKi|Wi = 0] −→(1 −exp(−1))
1 −exp(−1) α2
Proof of Lemma 3.2: From previous results:
b,i|Wi = 0] −2 E[Kb,i Ki|Wi = 0] + E[K2
i |Wi = 0]
(1 −exp(−1)) −2(1 −exp(−1))
1 −exp(−1) α2
2(1 −exp(−1) −3(1 −exp(−1)) −2 exp(−1) + 3
+ 2 −2 + 2 exp(−1) + 1
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1).