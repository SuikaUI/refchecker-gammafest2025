Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6053–6058
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Large-Scale Transfer Learning for Natural Language Generation
Sergey Golovanov *1, Rauf Kurbanov *1, Sergey Nikolenko *12,
Kyryl Truskovskyi *1, Alexander Tselousov *, and Thomas Wolf *3
1Neuromation OU, Liivalaia tn 45, 10145 Tallinn, Estonia
2Steklov Mathematical Institute at St. Petersburg,
nab. r. Fontanki 27, St. Petersburg 191023, Russia
3Huggingface Inc., 81 Prospect St. Brooklyn, New York 11201, USA
 , ,
 , ,
 , 
*All authors contributed equally, names in alphabetical order.
Large-scale pretrained language models deﬁne
state of the art in natural language processing, achieving outstanding performance on a
variety of tasks. We study how these architectures can be applied and adapted for natural language generation, comparing a number
of architectural and training schemes. We focus in particular on open-domain dialog as a
typical high entropy generation task, presenting and comparing different architectures for
adapting pretrained models with state of the art
Introduction
Over the past few years, the ﬁeld of natural language processing (NLP) has witnessed the emergence of transfer learning methods which have
signiﬁcantly improved the state of the art . These methods depart from classical supervised machine learning where a predictive model
for a given task is trained in isolation on a single dataset. Here, a model is pretrained on large
text corpora and then ﬁne-tuned on the target task.
Such models are usually evaluated on natural language understanding (NLU) tasks such as text
classiﬁcation or question answering , but natural language generation (NLG) tasks such as summarization, dialog, or machine translation remain relatively underexplored. At ﬁrst glance, large-scale pretrained
models appear to be a natural ﬁt for NLG since
their pretraining objectives are often derived from
language modeling.
However, interesting questions and problems still arise.
We consider a text-only NLG task where the
generation of an output sequence of symbols
y = (y1,...,ym) is conditioned on a context X =
(x1,...,xK) composed of one or several sequences
of symbols xk = (xk
n). Several types of contexts may warrant different treatment in the model.
E.g., in case of dialog generation they may include: (i) facts from a knowledge base, (ii) dialog history, and (iii) the sequence of already generated output tokens (y1,...,ym−1). Thus, there
arises a general question of how to adapt a singleinput pretrained model to a multi-input downstream generation task.
In this work, we study two general schemes to
adapt a pretrained language model to an NLG task.
In the single-input setting, contexts are concatenated to create a sequence preﬁx from which the
output is decoded as a continuation by the pretrained language model following Radford et al.
 .
The model can be used as is or
with a small number of special token embeddings
added to the vocabulary to identify the contexts.
In the multi-input setting, the pretrained model is
duplicated to form an encoder-decoder structure
where the encoder processes contexts while the
decoder generates the output.
Related work
Unsupervised pretraining for transfer learning has
a long history in natural language processing, and
a common thread has been to reduce the amount
of task-speciﬁc architecture added on top of pretrained modules. Most early methods focused on
learning word representations using shallow models, with complex recurrent or convolutional networks later added on top for speciﬁc tasks. With
Persona for Speaker 1 (P1)
I like to ski
My wife does not like me anymore
I have went to Mexico 4 times this year
I hate Mexican food
I like to eat cheetos
P2: Hello! How are you today?
P1: I am good thank you, how are you.
P2: Great, thanks! My children and I were just about to watch Game of
P1: Nice! How old are your children?
P2: I have four that range in age from 10 to 21. You?
P1: I do not have children at the moment.
P2: That just means you get to keep all the popcorn for yourself.
P1: And Cheetos at the moment!
P2: Good choice. Do you watch Game of Thrones?
P1: No, I do not have much time for TV.
P2: I usually spend my time painting: but, I love the show.
Table 1: Sample dialogue from PersonaChat with persona facts for Speaker 1 (P1). Speaker 2 (P2) also has
a random persona (not shown).
increased computing capacities, it has now become feasible to pretrain deep neural language
models. Dai and Le ; Ramachandran et al.
 proposed unsupervised pretraining of a language model for transfer learning and to initialize encoder and decoder in a seq2seq model for
machine translation tasks.
Works in zero-shot
machine translation used large corpora of monolingual data to improve performances for lowresource languages .
Most of the work transfering large-scale language
models from and for monolingual NLG tasks focus on classiﬁcation and natural language understanding studied
large-scale language models for various generation tasks in the zero-shot setting focusing on summarization and translation and Wolf et al. 
presented early work on chit-chat.
Problem setting and dataset
NLG tasks can be divided into high entropy
(story generation, chit-chat dialog) and low entropy (summarization, machine translation) tasks.
We focus on the high entropy task of chit-chat dialog to study the use and effect of various types of
contexts: facts, history and previous tokens.
Table 1 shows a typical dialog from PersonaChat , one of the largest
multi-turn open-domain dialog dataset available.
PersonaChat consists of crowdsourced conversations between real human beings who were asked
to chit-chat. Each participant was given a set of
4-5 proﬁle sentences that deﬁne his/her persona
(a) Single-input model
Single input (concatenated)
(b) Multi-input model
Figure 1: General model architectures: (a) single-input
model; (b) multi-input model.
embeddings
Positional
embeddings
Context type
embeddings
(a) Single-input model
(b) Multi-input model
Figure 2: Token embeddings: (a) single-input model
with CTE; (b) multi-input model with start/end tokens.
Input Embedding
Multi-Head Attention
Layer Normalization
Feedforward Layer
Layer Normalization
Figure 3: OpenAI GPT
Layer Normalization
Feedforward layer
Layer Normalization
Figure 4: Multi-input Transformer-based architecture.
for the conversation and asked to chitchat naturally and try to get to know each other. The dataset
contains 162,064 utterances over 10,907 dialogs
with 1,155 possible personas and 7 speaker turns
per dialogue on average. Although it is one of the
largest multi-turn dialogue datasets, PersonaChat
is still too small to train a large-scale model; state
of the art models trained directly on PersonaChat
are very prone to overﬁtting ,
hence the motivation for the present work.
Single- and multi-input adaptation
While we expect many more large-scale pretrained
language models to become publicly available
soon , our work is based
on the only large-scale pretrained language model
that was available at the time of this study, the
OpenAI GPT . We refer to
this publication for the details of the model, which
is a 12-layer decoder-only Transformer with masked multi-head attention.
The model uses a bytepair encoding (BPE) vocabulary with 40,000 merges
and learned positional embeddings for sequences
with at most 512 positions.
We now detail the various adaptation schemes
we used to adapt this model to the task of opendomain dialogue. More speciﬁcally, in our target
task the inputs to the model are: (i) a set of personality sentences, (ii) a dialog history involving two
speakers, and (iii) the history of previously generated tokens for auto-regressive generation.
In the ﬁrst adaptation setting, which we call
the single-input model, the pretrained language
model is used as is to generate an output sequence
y = (y1,...,ym) without any architectural modiﬁcations. Contexts are concatenated to create a sequence preﬁx from which the output is then decoded as a continuation.
In this direction, several ways to construct preﬁxes from heterogeneous
contexts can be investigated:
(i) concatenating
contexts with natural separators to make the test
data distribution close to the training data (in our case we added double
quotes to each utterance to mimic dialog punctuation); (ii) concatenating contexts with additional spatial-separator tokens (ﬁne-tuned on the
target task) to build an input sequence ; (iii) concatenating contexts and supplementing the input sequence with a parallel sequence of context-type embeddings (CTE) to be
added to the token and positional embeddings . Each CTE shows the context
type for its input token as shown on Fig. 2a: winfo
for persona info, wp1
CTE for dialog history coming
from person 1, and wp2
CTE for person 2. These vectors are also ﬁne-tuned on the target task.
In the second adaptation scheme, the multiinput model, the pretrained language model is
duplicated in an encoder-decoder architecture
(Fig. 1b). Similar to the single-input model, natural separators, spatial-separator tokens or contexttype embeddings can be added for each persona
fact and dialog utterance, surrounding the corresponding text with these tokens as preprocessing,
as shown on Fig. 2b. Persona information and dialogue history are successively processed in the encoder (Fig. 4) to obtain two respective sequences
of vector representations to be used as input to
the decoder model. The multi-head attention layers of the decoder are modiﬁed to process the
three inputs as follows (see Fig. 4). We copy the
multi-headed attention layer of the decoder three
times—for the embeddings of the current state,
persona facts, and dialog history—averaging the
results . The weights in both
encoder and decoder are initialized from the pretrained model.
Using both encoder and decoder allows to separate the contexts (dialogue history and persona information) and alleviate the maximal length constraint of 512 tokens.
Weight sharing between
encoder and decoder reduces the total number of
model parameters and allows for multi-task learning. On the other hand, untying the decoder and
encoder lets the attention heads and architectures
specialize for each task.
We have performed a series of quantitative evaluation on the test subset of the PersonaChat dataset
as well as a few quantitative evaluations.
Following the recommendations of the Endto-End conversation Modeling Task at DSTC-7
Workshop (Michel Galley and Gao), we evaluated the models on the following set of metrics:
METEOR , NIST-4,
BLEU as well as diversity metrics: Entropy-4, Distinct-2, and the average length of the generated utterances.
Table 2 illustrates the results for three typical models: the single-input model in the zero-shot set-
METEOR NIST-4 BLEU Entropy-4 Distinct-2 Average Length
Single-input (zero-shot)
Single-input (additional embeddings)
Multi-input
Table 2: Selected evaluation results and statistics.
Training epochs
Word counts
SIM, persona
MIM, persona
SIM, history
MIM, history
Training epochs
SIM, METEOR
MIM, METEOR
Figure 5: Results for single- (SIM) and multi-input (MIM) models; left: word statistics; right: evaluation metrics.
ting (no modiﬁcation) and with additional embeddings ﬁne-tuned on the target task, and the
multi-input model in which the encoder and decoder are not shared, which is thus a high-capacity
model in comparison to the previous two models.
We can see that both approaches reach comparable performances on the automatic metrics with
the multi-input model performing better on ME-
TEOR, NIST-4 and BLEU.
We investigated in greater detail the evolution
of the single-input and multi-input models during
training to understand the origin of their differences. To this aim, we tagged the words generated by each model according to four categories:
(i) content words that were mentioned in the persona facts, (ii) content words that were mentioned
in the dialog history, (iii) content words that were
mentioned in both, and (iv) all other generated
words. Fig. 5 shows the statistics of these types of
words along a representative training run obtained
using compare-mt .
An interesting observation is that single-input
and multi-input models adopt differing behaviors
which can be related to an intrinsic difference between two contextual inputs: dialog history and
personality facts. While dialog history is very related to sequentiality, personality facts are not sequential in essence: they are not ordered, a welltrained model should be invariant to the ordering
of the facts. Moreover, a personality fact can be
relevant anywhere in a dialog. On the contrary, dialog history is sequential; it cannot be reordered
freely without changing the meaning of the dialog
and the relevance of a particular utterance of the
dialog history is strongly dependent on its location
in the dialog: older history becomes less relevant.
This difference in nature can be related to differences in the models. Single-input adaptation is
closer to a bare language-model and the comparison with multi-input model shows that the former
tends to stay closer to the dialog history and consistently uses more words from the history than
multi-input model. On the other hand, splitting
encoder and decoder makes persona facts available to the multi-input model in a non-sequential
manner and we can see that the multi-input model
use more and more persona facts as the training
evolves, out-performing the single-input model
when it comes to reusing words from persona
We also note that the multi-input model,
with its unshared encoder and decoder, may be
able to specialize his sub-modules.
Conclusion
In this work, we have presented various ways in
which large-scale pretrained language models can
be adapted to natural language generation tasks,
comparing single-input and multi-input solutions.
This comparison sheds some light on the characteristic features of different types of contextual inputs, and our results indicate that the various archi-
tectures we presented have different inductive bias
with regards to the type of input context. Further
work on these inductive biases could help understand how a pretrained transfer learning model can
be adapted in the most optimal fashion to a given
target task.