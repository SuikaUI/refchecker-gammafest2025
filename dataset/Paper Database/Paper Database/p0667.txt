Department of Economics
University of Southampton
Southampton SO17 1BJ
Discussion Papers in
Economics and Econometrics
HOW LIKELIHOOD AND
IDENTIFICATION WENT BAYESIAN
John Aldrich
This paper is available on our website
 
How Likelihood and Identification
went Bayesian
John Aldrich
Department of Economics
University of Southampton
Southampton
Fax (0)(+44) 23 80593858
E-mail: 
This paper considers how the concepts of likelihood and identification
became part of Bayesian theory. This makes a nice study in the
development of concepts in statistical theory. Likelihood slipped in easily
but there was a protracted debate about how identification should be
treated. Initially there was no agreement on whether identification
involved the prior, the likelihood or the posterior.
October 2001
Introduction
Some of the concepts and terms associated with twentieth-century Bayesian theory were new,
like \Bayesian" itself, a few were from the remote past, like \prior" and \posterior", but
most were from non-Bayesian theory. This paper examines how two of these, likelihood and
identi¯cation, passed into Bayesian theory. Su±ciency will also be involved and the story of
the three makes a nice case-study in the development of concepts in statistical theory.
On present views identi¯cation and likelihood are related but their origins were quite
separate. \Likelihood" came from R. A. Fisher's theory of estimation and some of his theory
came too{propositions Harold Je®reys thought could be established more easily in
Bayesian theory. Later Bayesians were not so ¯xed on Fisher but likelihood stayed without
much debate. \Identi¯cation" crystallised in the 1940s when the econometrician Tjalling C.
Koopmans discussed a type of inference distinct from statistical inference. Identi¯cation then
made a double passage, into statistical inference and into Bayesian econometrics. However
there was no agreement on which propositions about identi¯cation were the important ones
or even whether identi¯cation involved the prior, the likelihood or the posterior. \Whether
Bayesian theory requires a di®erent de¯nition of identi¯cation from the classical one" was still
deemed \unresolved" by Hsiao (p. 272) in 1983. A resolution seems to have been reached,
though this disturbed past is still re°ected in recent contributions to Bayesian econometrics.
The writers involved with likelihood and identi¯cation were seldom as explicit as Kolmogorov and Fomin who commend one concept as \natural" or \fruitful" and
explain the survival of another through \historical inertia", but they had to make similar
judgements. Those judgements link decisions about de¯nitions and similar minutiae to the
bigger themes in statistical inference.
The account follows likelihood from its anti-Bayesian beginnings in Fisher's theory into
Je®reys's theory and on to the appearance of the likelihood principle around 1960, since when
there has been relative quiet. I mention some proto-identi¯cation theory but the identi¯cation
story begins in the 1940s, with Bayesian interest peaking in the 1970s. That development too
may have run its course.
The Fisher programme
In the early 1920s R. A. Fisher developed a new approach to statistical theory
based on the idea that the object in calculating a statistic is to extract as much relevant
information from the data as possible; see Aldrich for a detailed account and Box
 for biographical data. Likelihood played an important part in the new theory and,
reviewing the theory after a decade or so, Fisher judged that he had amply
demonstrated \the adequacy of the concept of likelihood for inductive reasoning". The claim
and the concept require attention because they were carried into Bayesian theory. Incidentally
Fisher rewrote the language of statistical theory on a scale unmatched by any twentieth century
Fisher's likelihood
Fisher formulated the notion of likelihood because he wanted to emphasise the distinctiveness
of his approach. Soper, Young, Cave, Lee and Pearson interpreted Fisher's phrase
\most likely value" as the value obtained from maximising the posterior distribution obtained
from a uniform prior and they criticised his choice of prior. Fisher however insisted he was
not using a prior{indeed he rejected the whole set-up:
Bayes (1763) attempted to ¯nd, by observing a sample, the actual probability that
the population value lay in any given range. ... Such a problem is indeterminate
without knowing the statistical mechanism under which di®erent values of [the
parameter] come into existence; it cannot be solved from the data supplied by a
sample, or any number of samples, of the population.
However if the statistical mechanism were known Fisher agreed that Bayes'
solution could be applied.
Fisher de¯ned likelihood in \On the Mathematical Foundations of Theoretical Statistics"
 :
The likelihood that any parameter (or set of parameters) should have any assigned
value (or set of values) is proportional to the probability that if this were so, the
totality of observations should be that observed.
The proportionality derived from the presence of the di®erential element when writing the
\chance of an observation falling in the range dx" (p. 323) but as Fisher was only ever interested in the part of the function involving the unknown parameter{he usually took logs and
di®erentiated{he could be more radical. Thus in 1932 he (pp. 258-9) ignored \the numerical
coe±cient which is independent of x [the parameter]" and wrote the likelihood for the binomial
as xa(1 ¡ x)b or \some arbitrary multiple of it":
Fisher distinguished likelihood not only from the (usually) illegitimate posterior probability
but from a legitimate frequency notion of probability. He complained that \two
radically distinct concepts have been confused under the name of `probability'..." The roles
of (legitimate) probability and likelihood were set out in the Statistical Methods for Research
Workers :
The deduction of inferences respecting samples, from assumptions respecting the
populations from which they are drawn, shows us the position in Statistics of the
Theory of Probability. ...
This is not to say that we cannot draw, from knowledge of a sample, inferences respecting the population from which the sample was drawn, but that the
mathematical concept of probability is inadequate to express our mental con¯dence or di±dence in making such inferences and that the mathematical quantity
[Likelihood] which appears to be appropriate for measuring our order of preference among di®erent possible populations does not in fact obey the laws of
probability.
Hitherto the task of expressing \mental con¯dence" had been assigned to the invalid posterior
probability. On the ¯nal point in the passage Fisher long complained that \there is still a
tendency to treat likelihood as though it were a sort of probability."
While likelihood was ¯xed by the de¯nition of 1922, its status was ambiguous. The 1925
passage and the 1921 paper (which introduced likelihood) treat likelihood as a primitive measure of belief, as it was in Barnard , Edwards and other later works. Yet the
\Foundations" and Fisher's main writings into the 1930s did not stress the primitiveness of
likelihood but rather propositions about the e±ciency of likelihood-based methods in extracting information. The case for the \adequacy" of the concept of likelihood was not a matter
of de¯nition; it rested on two argument Fisher summarised as:
First, that the particular method of estimation, arrived at by choosing those values
of the parameters the likelihood of which is greatest, is found to elicit not less
information than any other method which can be adopted. Secondly, the residual
information supplied by the sample, which is not included in a mere statement of
the parametric values which maximize the likelihood, can be obtained from other
characteristics of the likelihood function ...
The \information" here is a repeated sampling notion{the \Fisher information measure".
Information and proto-identi¯cation
Information underpinned Fisher's case for likelihood but information ideas are also relevant
to the identi¯cation story and, although they did not come into play until much later, it is
convenient to describe them here.
Behind the information measure was a primitive qualitative notion of a statistic containing
no information about a parameter. The idea that a statistic contains no information about
a parameter when its distribution does not depend on the parameter arrived with su±ciency.
Su±ciency is described in the glossary of the \Foundations" (p. 310) thus
A statistic satis¯es the criterion of su±ciency when no other statistic which can
be calculated from the sample provides any additional information as to the value
of the parameter to be estimated.
In the body of the paper Fisher (p. 316) is more formal about what the su±ciency of a
statistic, µ1; involves
In mathematical language ... if µ be the parameter to be estimated, µ1 a statistic
which contains the whole of the information as to the value of µ, which the sample
supplies and µ2 any other statistic, then the surface of distribution of pairs of
values of µ1 and µ2 , for a given value of µ, is such that for a given value of µ1, the
distribution of µ2 does not involve µ. In other words, when µ1 is known, knowledge
of the value of µ2 throws no further light on upon the value of µ.
This is hardly an elementary application of uninformativeness as variation-freeness yet it
is clear that this is the idea that is being applied. In the notation of x6.2 below, suppose
the probability density of Y , f(y; ®), is known to belong to a family of densities with ® 2 A.
There is no \information in the sample as to the value of the parameter to be estimated"
f(y; ®1) = f(y; ®2) for all y
for any pair of parameter values ®1 and ®2 in A.
Variation-freeness also ¯gured later in
Fisher's notion of ancillarity and the conditional inference theory based upon it.
Fisher discussed the elementary case in 1935 when he (p.
47) tested the information
measure against \our pre-mathematical common sense" requirements of such a measure. The
measure is zero when it should be: \when the probabilities of the di®erent kinds of observation
which can be made are all independent of a particular parameter, the observations will supply
no information about the parameter."
The identi¯cation concerns in Koopmans's writings of the 1940s{x6.1 below{were quite
di®erent and Fisher came closest to anticipating them, not in any theory, but in an example.
In the Statistical Methods Fisher considers the task of estimating µ in a restricted
multinomial distribution with cell probabilities
µ, however, is not a fundamental parameter in Fisher's genetical example, it is the product
of p and p0; the \recombination fractions" for males and females. In the ¯rst edition Fisher
did not explain why he does not estimate these parameters but subsequently he 
Since these [multinomial] probabilities involve only the quantity pp0, it is only this
and not the separate values of p and p0 that the data can provide an estimate. ...
If p and p0 were equal, then
µ would give the recombination fractions in both
sexes, and if these are unequal it will always give their geometric mean. The data
before us, however, throw direct light only on the value of µ:
The information calculations that follow concern µ; not p and p0 which drop out of sight.
This example and the issues it illustrates can be connected with the formal information
ideas{see x6.2 below{but Fisher had no occasion to do so. There was another information
context in which he did not go into details. Fisher was constantly making `no no-information'
assumptions.
Thus, in taking the reciprocal of the information measure in his maximum
likelihood theory, he was assuming that the measure is non-zero. Any careful description of
the circumstances in which a technique will work involves some kind of identi¯cation condition
and there were already such descriptions in the literature.
Fisher had been taught least squares after the fashion of Brunt . The model involved,
put in modern notation, is
y s N(X¯; ¾2I)
with X a matrix of known constants. The section (p. 79) on the \independence of the normal
equations" considers how the normal equations may not yield a \determinate solution". The
treatment can be traced back to Gauss's ¯rst publication on least squares (1809) where there is
a version of the full rank condition for \determinacy"; see Aldrich for details. Another
investigation of determinability appears in Pearson's (1894) paper on estimating a mixture of
normals. Pearson ¯rst establishes that a non-degenerate mixture is uniquely determined{\a
curve which breaks up into two normal components can break up in one way, and one way
only." (p. 74).
To return to likelihood, Fisher's case for \the adequacy of the concept of likelihood" rested
on information. I have not detailed the case for Je®reys thought the argument
\would be made much easier by an explicit use of probability" and likelihood ¯rst when
Bayesian went Je®reys made the case without information technicalities.
The Je®reys programme
The earliest Bayesian work of the physicist Harold Je®reys was oriented more to
logic and the philosophy of science than to statistical analysis but in the 1930s his seismological
research took a statistical turn and he started writing about statistics and paying attention to
Fisher. The Theory of Probability developed a science of statistics, of comparable scope
to Fisher's, founded on the theory of probability{Je®reys's name for the theory of inductive
inference based on the principle of inverse probability or Bayes' theorem; see Cook for
biographical information and Lindley for an account of Je®reys's book.
The Probability recast Fisher's methods in a consistent form and showed how some of his
larger objectives could be realised more easily. Je®reys admired Fisher's ability
to grasp the essentials of a solution by \some brilliant piece of common sense" but lamented
the lack of system. Je®reys was more in°uenced by Fisher than by writers from the Bayesian
past. He mentions earlier Bayesians, including Laplace, Edgeworth and Pearson for their views
about Bayesian theory but hardly for their work in Bayesian theory. Pearson, for instance, is
the author of the Grammar of Science, not the cooperator of 1917 or the 1907 investigator
of \the in°uence of past experience on future expectations". It has taken modern historians,
like Dale and Hald , to recover the pre-Je®reys Bayesian past.
Je®reys's likelihood
Je®reys was the ¯rst to use the now standard terminology of Bayes' formula in its application
to inference; see David . Wrinch and Je®reys contracted the widely-used
\probability a priori" and \probability a posteriori" to \prior probability" and \posterior
probability", less, Je®reys explained, for linguistic streamlining than because \a
priori" had other philosophical meanings and was open to misunderstanding. The adoption of
\likelihood" was a more signi¯cant step and Je®reys only took it after he had steeped himself
in Fisher's work. Je®reys chose to put likelihood into the Probability; he could have chosen
another word or left the component without a name. He did not explain his decision beyond
saying that Fisher's term was \convenient".
Je®reys wrote the \principle of inverse probability, ¯rst given by Bayes in
P(qr j pH) _ P(qr j H)P(p j qrH)
where the H on which all the probabilities are conditional is \previous knowledge".
interpreted the terms as follows:
If p is a description of a set of observations and the qr a set of hypotheses, the factor
P(qr j H) may be called the prior probability, P(qr j pH) the posterior probability
and P(p j qrH) the likelihood, a convenient term introduced by Professor R. A.
Fisher, though in his usage it is sometimes multiplied by a constant factor. It is the
probability of the observations given the original information and the hypothesis
under discussion.
Je®reys draws attention to and then deviates from Fisher's usage; he does not multiply by a
constant. The form in which Je®reys writes the principle is that which follows most directly
from the de¯nition of conditional probability. Presumably Je®reys saw no point in the Fisher
version. Indeed he may have thought that Fisher's separation of likelihood and probability
was unhelpful. Fisher's point about likelihood not obeying the laws of probability could be
made in Je®reys's notation as
P(p j qr or qr; H) 6= P(p j qrH) + P(p j qsH):
Je®reys's likelihood is clearly not the same as Fisher's likelihood. The likelihoods do not
refer to the same things. One is, or derives from, a family of conditional distributions where
the conditioning variable is a parameter while the other comes from a family of unconditional
distributions indexed by the parameter. This di®erence re°ects the fundamental di®erence
in the way parameters are conceived in the two theories. However the di®erence seemed to
cause nobody any trouble because it is clear how the translations should be made. Although
parameters are random variables in one paradigm only, the notions of random variable and
conditional distribution are common to the paradigms. Fisher's term would have been inconvenient and there would have been something to argue about if Bayesian theory used both the
indexing notion and the conditioning notion. On a more operational level Fisher's maximum
likelihood and Je®reys's maximum likelihood are the same functions of the data. Perhaps Jeffreys's likelihood is as near to Fisher's as it could be given the di®erent status of parameters
in the two theories. This is a big given and Bayesians could have introduced a new term.
Part of the disanalogy between Fisher's likelihood and Je®reys's likelihood was Fisher's
claim that likelihood is \appropriate for measuring our order of preference among di®erent
possible populations",something Je®reys might say of the posterior distribution.
there was a positive analogy in that some of Fisher's information claims could be sustained
by Je®reys's likelihood. Thus when he came to interpret the magic formula
Posterior Probability _ Prior Probability £ Likelihood
Je®reys wrote
where by the likelihood we understand the probability that the observations should
have occurred, given the hypothesis and the previous knowledge. The prior probability has nothing to do with the observations immediately under discussion,
though it may depend on previous observations. Consequently the whole of the
information that is relevant to the posterior probabilities of di®erent hypotheses is
summed up in the values they [the observations] give to the likelihood.
The last sentence led Berger and Wolpert and Lindley to suggest
that Je®reys understood the likelihood principle. However the likelihood principle involves a
contrast between the probability distribution of the observations and anything that works in
the formula. Je®reys, however, was using \likelihood" for the probability distribution and was
contrasting it with the prior distribution. The likelihood principle only emerged in Bayesian
statistics around 1960{see x5.1 below. Je®reys missed this corollary of the principle of inverse
probability although admittedly it would have ¯tted in well with other parts of his system
e.g. his (pp. 315-6) well-known criticism of the use of tail areas in signi¯cance testing: \a
hypothesis that may be true may be rejected because it has not predicted observable results that
have not occurred."
Unlike Fisher, Je®reys based no technical development on \information" or \the whole of
the information". He did not need to for, however they are de¯ned, they must operate through
the likelihood function as the only way the observations a®ect the posterior is through the
likelihood. It was easy for Je®reys to recast some of Fisher's core theory. We have seen how
he established the ¯rst part of the ¯rst of the \main results" of the \present system" (p. 351)
a proof independent of limiting processes that the whole information contained in
the observations with respect to the hypotheses under test is contained in the likelihood, and that where su±cient statistics exist other functions of the observations
are irrelevant.
Fisher's proof of this ¯rst part had required the notion of Fisher information which Je®reys
thought could only be justi¯ed by a large sample argument.
Je®reys also had his own method for the second su±ciency part of the result. He replaced
Fisher's original de¯nition of su±ciency (x2.2 above) with one based on the factorisation
criterion. (Fisher had used the criterion but not to de¯ne su±ciency.) Je®reys (pp. 89-90)
Whenever the likelihood, apart from factors independent of the unknown parameters to be estimated, can be expressed as a function of the unknown parameters,
the number of observations, and a number of functions of the observations equal to
the number of unknown parameters, those functions of the observations are called
su±cient statistics.
Thus he (p. 90) could say that \the whole of the information with respect to the [parameters]
that is contained in the observations is summmarized in the [su±cient statistics]".
In the third edition of the Theory Je®reys shifted the emphasis in his
account of su±ciency to the way the posterior distribution depends on the data and showed
how the Fisher 1922 de¯nition could be \proved" as a theorem. Rai®a and Schaifer adopted a similar posterior-based de¯nition: the posterior depends on the data through
the su±cient statistic. They (p. 34) establish the \complete equivalence" of the Bayesian and
classical (factorisation) de¯nitions of su±ciency; the criterion is that the two concepts have
the same extension, i.e. the same statistics from the same distributions are su±cient according
to the two de¯nitions. Rai®a and Schaifer prefer their de¯nition because it \leads naturally"
(p. xi) to the concept of marginal, or partial, su±ciency which concerns the way the data
acts on the marginal posterior. This notion (p. 35) is relative to the prior distribution that
permits the nuisance parameter to be integrated out.
Je®reys installed likelihood in Bayesian theory. His de¯nition corrected Fisher's but the
claims Fisher made about likelihood and information could be made{and proved more easily{
for their Je®reys counterparts. So there was continuity of purpose as well as continuity of
reference. When Fisher's concerns became less pressing there was more of a cost in maintaining
likelihood. Schlaifer warned his readers: \Again we emphasise that the new
term is introduced purely for convenience: a likelihood is a probability in the same sense as
any other probability." Against the \convenience" had to be set the fact that this was an
unnatural term with the wrong associations. Convenience can be another name for historical
inertia{it was not for Je®reys in 1939.
Classical interlude: Neyman and Wald
The Bayesians who came after Je®reys were not enthralled to Fisher; Rai®a and Schlaifer's
 global acknowledgment is con¯ned to \to Neyman, Pearson, Je®reys, Von Neumann, Wald, Blackwell, Girshick and Savage." Fisher's information/likelihood theory had
been replaced as the norm by the \classical" decision theory of Jerzy Neyman 
and Abraham Wald . For biographical information on Neyman and Wald see Reid
 and Wolfowitz .
Neyman's \Outline of a Theory of Statistical Estimation based on the Classical Theory of
Probability" confronted Je®reys by distinguishing (p. 341) Je®reys's theory from the
\classical theory". It did not confront Fisher, though an essential point of the new theory was
that it was not based on likelihood or information. Neyman had pressed Fisher
on whether likelihood is outside probability and whether it is possible to construct a theory
of mathematical statistics based solely on the (classical!) theory of probability. Neyman (p.
75) preferred to found estimation on the \frequency of errors in judgement". This provides
a \su±ciently simple and unquestionable principle in statistical work" while the \amount of
information is too complicated and remote to serve as a principle". Likelihood and maximum
likelihood had ¯gured in Neyman's inference theory since the Neyman-Pearson 1928 paper on
likelihood ratio tests. Neyman and Pearson (pp. 184-7) used the same de¯nitions as Fisher's
but their likelihood, likelihood ratios and maximum likelihood were not enchanted but merely
useful, their usefulness registering in results on error frequencies.
Wald gave a uni¯ed treatment of testing and estimation based on the notions of
loss (\weight") functions, risk functions and admissibility. He (p. 302) repeated Neyman's
objections to a priori probability distributions but used the concept in a technical capacity:
\it proves to be useful in deducing certain theorems and in the calculation of the best systems
of regions of acceptance". In his Statistical Decision Functions Wald extended this line
of research emphasising the minimax criterion and including (p. v) a \treatment of the design
of experimentation as a part of the general decision problem."
Bayesian decision theory
There were many publications that contributed to the modern Bayesian revival but I concentrate on two of the most in°uential: The Foundations of Statistics by L. J. Savage
 and Applied Statistical Decision Theory by Howard Rai®a and
Robert Schlaifer ; for biographical information on Savage, see Lindley . The
new Bayesian line was very di®erent from Je®reys's. The foundations combined \personalism"
with the \behavioralism" (decision-orientation) of Neyman and Wald. Savage (p. 276) considered the Theory of Probability \an ingenious and vigorous defense of a necessary view, similar
to, but more sophisticated than Laplace's". Rai®a and Schlaifer refer to some of Je®reys's
higher-level contributions, though not to his recasting of Fisher.
Unlike Fisher, these authors were not profoundly dissatis¯ed with their elders. Savage
 judged the methods of the \British-American School" as \on the whole consistent"
with the theory of probability he was proposing{indeed he tried to develop a subjectivist
interpretation of Wald's minimax theory. Rai®a and Schlaifer stated:
the so-called \Bayesian" principles underlying the methods of analysis presented in
this book are in no sense in con°ict with the principles underlying the traditional
decision theory of Neyman and Pearson.
The term \Bayesian", incidentally, had only come into use around 1950{see David .
While Rai®a and Schlaifer played down the con°ict between classical and Bayesian approaches, important di®erences were emerging. These di®erences a®ected methods as well as
foundations. Lindley recalls Savage's admission \I came to take
... Bayesian statistics ... seriously only through recognition of the likelihood principle.".
The likelihood principle: terminal and preposterior analysis
\Likelihood" on its own hardly ¯gures in Savage's 1954 book.
140) writes \The
concept of likelihood ratio, sometimes simply called likelihood, is now one of the most pervasive
concepts of statistical theory". Likelihood ratios are discussed at length and there is some
discussion of maximum likelihood.
Rai®a and Schlaifer discuss likelihood per se and use
Je®reys's de¯nition, thus writing for the discrete case (p. 29): \If the conditional measure
has a mass function, we shall denote by l(z j µ) [likelihood] the probability given µ that e [the
experiment] results in z".
Rai®a and Schlaifer introduced the re¯nement of the \likelihood kernel". The kernel was
useful in extracting distributional information from Bayes' formula and helpful in setting up
the conjugate prior theory. It is presented (p. 30) as follows:
if ½ and · are functions on Z [the observable] such that for all z and µ
l(z j µ) = ·(z j µ)½(z)
i.e. if the ratio ·(z j µ)=l(z j µ) is a constant as regards µ, we shall say that ·(z j µ)
is a (not \the") kernel of the likelihood ...
While most modern Bayesians de¯ne likelihood as the conditional measure, some, e.g. Box
and Tiao , slide between this and the likelihood kernel in the Fisher manner.
Savage previewed the likelihood principle when he wrote
it is becoming increasingly accepted that, once an experiment has been done, any
analysis or other reaction to the experiment ought to depend on the likelihoodratio function and on it alone, without any further regard to how the experiment
was actually planned or performed.
He had ¯rst heard this argument{applied to sequential sampling{from Barnard in 1952, as he
 later recalled. Savage set out the argument more fully, replacing
the \likelihood-ratio function" by the likelihood:
According to Bayes's theorem Pr(x j ¸), considered as function of ¸ constitutes
the entire evidence of the experiment, that is, it tells all that the experiment has
to tell. More fully and precisely, if y is the datum of some other experiment, and
if it happens that Pr(y j ¸) and Pr(x j ¸) are proportional functions of ¸ (that is,
constant multiples of each other), then each of the two data x and y have exactly
the same thing to say about the value of ¸ .... I, and others, call this principle the
likelihood principle. The function Pr(x j ¸){rather this function together with all
others that results from it by multiplication by a positive constant{is called the
likelihood function.
Rai®a and Schlaifer do not state the likelihood principle but their detailed comparison of
the Bayesian and classical treatments of optional stopping (pp. 36-42) became a standard
illustration. They (p. 42) summarised:
the likelihoods for all noninformative stopping processes have a common kernel,
and therefore all lead to the same posterior distribution. ...[O]n the other hand
we shall also be concerned with the problems of experimental design as they look
before any sample has actually been taken, and then we shall want to ask what
can be expected to happen if we predetermine r [the number of successes] rather
than n [the number of trials] and so forth.
Rai®a and Schlaifer [p. x] distinguish between \terminal" (post-experimental) analysis and
\preposterior" (pre-experimental) analysis, between choice of an act after an experiment has
been performed, and choice of the experiment to be performed. In pre-experimental analysis
the probability distribution of the observable is considered but post-experimental analysis
is conditional on the observed outcome and based on the likelihood function, not on the
density. Thus something of Fisher's distinction (x2.1) between probability and likelihood{and
of elaborations such as that by Barnard {reappears in the Rai®a and Schlaifer scheme.
Bayesians have varied in their attitudes to the likelihood principle. Against the enthusiasm
of Savage and Lindley, of the econometricians Leamer and Poirier , is the caution
of Gelman, Carlin, Stern and Rubin for whom the principle is of limited
interest and its suggestion that analysis be done \without further regard to how the experiment
was actually planned or performed" potentially misleading. However, despite disagreements
about the signi¯cance of the likelihood principle and di®erences in the use of likelihood and
likelihood kernel, likelihood's passage into Bayesian theory was untroubled. This was not the
case with identi¯cation.
The troubles with identi¯cation
There are two obvious ways of transferring a concept into a new theory so that some continuity
is maintained. Either take its meaning as ¯xed and consider how the concept functions in the
new theory or ¯nd something in the new theory with the same or similar function. When
Je®reys appropriated likelihood he took its meaning as ¯xed{more or less{and found that
his likelihood had a similar function to Fisher's in that some of the theorems in which it
¯gured were the same. The situation with identi¯cation was more complicated for both ways
of managing the transfer were tried{either ¯lling the blank in \from the Bayesian viewpoint,
classical identi¯cation theory is really concerned with |" ) or ¯lling
the blank in \an appropriate Bayesian de¯nition of identi¯cation is |" ). Both projects were pursued, but identi¯cation carried so much baggage that
there was more than one way of ¯lling both blanks.
It may be worth noting some points about the language of the identi¯cation community. \Identi¯ed" and \identi¯able"{and \identi¯cation" and \identi¯ability"{circulated as
synonyms, linguistic anomalies it would take too long to unravel. \Identi¯cation problem"
signi¯ed both a topic for discussion and something requiring a solution. If there were a problem, i.e. the parameter were not identi¯able, or under-identi¯ed, the solutions were to lower
one's sights to an \identi¯ed function" of the parameter or to get more information.
Koopmans's identi¯ability
The topic of identi¯cation{with its concepts of identi¯ability and observational equivalence{
was formalised in the 1940s by T. C. Koopmans , ¯rst as applied to the simultaneous
equations model of econometrics and then to scienti¯c inference in general. In econometrics
identi¯cation became a big topic, warranting a book in 1966 (F. M. Fisher's Identi¯cation
Problem in Econometrics) and a chapter in the1983 Handbook of Econometrics (by C. Hsiao).
The story of identi¯cation in econometrics is a complex one and various facets have been
discussed by Qin , Morgan , Aldrich , Rayner and Aldrich 
and others.
Koopmans's most general ideas appear in \The Identi¯cation of Structural Characteristics"
by Koopmans and Reiers¿l . The authors were particularly interested in econometrics
and factor analysis but the genetic example from Fisher's Statistical Methods (x2.2 above) can
illustrate their viewpoint. The \structural characteristics" are the elements of (p; p0); while
the distribution of the observations depends on the multinomial probabilities. Given \exact
knowledge" of those probabilities, can the value of (p; p0) be deduced? Clearly not, for in¯nitely
many pairs of values generate the same value of the product pp0 and hence the same probability
distribution for the observables. In Koopmans's language, the pair (p; p0) is not identi¯able
since for every pair there is another (in fact in¯nitely many) observationally equivalent pair(s).
Koopmans and Reiers¿l (p.
170) state that \the study of identi¯ability proceeds from a
hypothetical exact knowledge of the probability distribution of observed variables rather than
from a ¯nite sample of observations." Their \identi¯cation problem" was outside the scheme of
statistical analysis set down in R. A. Fisher's \Foundations" in which the statistician speci¯es
a population and makes inferences from the sample to the parameters of the population.
Writing of the economic context, Koopmans separates the process
of inference from sample to theoretical structure into a step from sample to population and
one from population to structure.
Statistical inference, from observations to economic behavior parameters, can be
made in two steps: inference from the observations to the parameters of the assumed joint distribution of the observations, and inference from that distribution
to the parameters of the structural equations describing economic behavior. The
latter problem of inference [is] described by the term \identi¯cation problem".
Step one is the domain of statistical inference in the narrow sense, step two the domain of
economic theory or the structural theory of whichever substantive ¯eld the observations come
from. Identi¯ability turns on the invertibility of the mapping{e.g. pp0 = µ or B¡1¡ = ¦ (see
below){from the theory parameters to the parameters that can be estimated from the data
and the typical identi¯cation theorem involved a condition on the rank of a matrix. Koopmans
was interested in the total process of inference from sample to structure and failure at the
¯rst stage wrecked the total process. For Koopmans this involved a data de¯ciency of some
kind{the most familiar being an X matrix of de¯cient rank in regression{not a failure of
identi¯cation.
Koopmans's identi¯cation concerned a layer of inference beyond statistical inference and
belonged in the structural theory. One can imagine a Bayesian and the classical Koopmans
agreeing that identi¯cation is a property of the structural speci¯cation and is the same whether
considered classically or from the Bayesian approach. However I have not found any Bayesian
taking this line for there was a powerful de°ecting in°uence in the idea that identi¯cation is
related prior information. The relationship was built into the way the simultaneous equations
model was usually presented.
Following Koopmans, Rubin and Leipnik , each equation of the simultaneous equations model has an economic \identity" as a demand, a supply or some other theoretically
meaningful equation. There is a vector zt of conditioning (exogenous) variables and a vector
yt of conditioned (endogenous) variables. The system of \structural" equations is organised
into two parts: an unrestricted structural model with parameters B; ¡; § and a set of \a priori
restrictions" expressed through the function ª:
¡zt + "t : "t v IN(0; §) : t = 1; :::; T
ª(B; ¡; §)
The commonest form of restriction was the exclusion restriction{a particular element of B or
¡ is zero signifying that a particular variable does not appear in a particular equation.
The implied \joint distribution of the observations" or \population" is given by the reduced
form equations
¦zt + vt : vt v IN(0; ­); with ¦ = B¡1¡ and ­ = B¡1§B¡10
IN(¦zt; ­):
The structure is identi¯ed when only one (B; ¡; §) satisfying the restrictions can be obtained
from an attainable ¦ and ­.
If there are more than one (B; ¡; §), Koopmans, Rubin and Leipnik (p. 73) call them
observationally equivalent; they represent \mathematically equivalent ways of writing down"
the distribution. Econometricians came to speak of identi¯cation being \achieved" or the
parameters \identi¯ed" by the imposition of \restrictions". In this spirit the identi¯cation of
the parameters in the Fisher genetic model would be achieved if, say, the restriction p = p0
were imposed.
Econometricians generally took the hierarchical formulation of the structural model for
granted, though as Malinvaud noted,
the distinction between the general statistical hypothesis and a priori restrictions
is purely conventional. Its only object is to facilitate the discussion of identi¯cation
problems, and it is based on no necessary logical principle.
Koopmans's mathematical technique for establishing identi¯cation results exploited this distinction, but it does not usually match the economics of the situation for the unrestricted
model has no obvious economic interpretation. One could just write down the `restricted'
model as Haavelmo originally did. Bayarri, DeGroot and Kadane have
drawn attention to the arbitrariness of the division into prior and likelihood in a hierarchical
model; the hierarchy was an important part of the Koopmans identi¯cation legacy.
Koopmans put the distinction, between what can be discovered from data under favourable{
even ideal{circumstances and what cannot, to the service of a view that economic theory is
indispensable. Thus Koopmans, Rubin and Leipnik (p. 64) insist that
Statistical observation will in favourable circumstances permit [the investigator]
to estimate ... the characteristics of the probability distribution of the variables.
Under no circumstances whatever will passive statistical observation permit him
to distinguish between mathematically equivalent ways of writing down that distribution.... The only way in which he can hope to identify ... equations ... is with
the help of a priori speci¯cations of the form of each structural equation.
\Favourable circumstances"{enough variety in z to deliver ¦ and ­{will not necessarily deliver
(B; ¡; §).
Identi¯cation goes statistical
In the 50s and 60s the terminology of identi¯cation came to be applied to statistical inference,
mainly in the context of regularity conditions for statistical inference procedures{see Aldrich
 . T. J. Rothenberg's \Identi¯cation in parametric models" consolidated this movement with a body of theorems. In the 1977 reprint of the Identi¯cation
Problem F. M. Fisher (p. v) distinguished the treatment of identi¯cation in a \general statistical setting" with his own which treated identi¯cation as a \branch of economics". The
contrast is genuine although the Koopmans-Fisher notion had always been amphibious for,
while it lodged in economic theory, it was always associated with statistical inference actual
or possible{Koopmans and Reiers¿l published in the Annals of Mathematical Statistics after
The root idea of the statistical notion was R. A. Fisher's uninformativeness as variation
freeness (x2.2) and the ¯rst of Rothenberg's theorems treats the non-vanishing of the Fisher
information measure as a condition for local identi¯ability. Rothenberg (p. 577) remarks, \lack
of identi¯cation is simply the lack of su±cient information to distinguish between alternative
structures [parameter values]".
Rothenberg (p. 578) gives a pair of de¯nitions for the parametric case. Here Y represents the n-dimensional outcome of some random experiment with density function f and A
represents the m-dimensional parameter space:
DEFINITION 1: Two parameter points (structures) ®1 and ®2 are said to be
observationally equivalent if f(y; ®1) = f(y; ®2) for all y in Rn:
DEFINITION 2: A parameter point ®0 in A is said to be identi¯able if there is no
other ® in A which is observationally equivalent.
These descend from de¯nitions in Koopmans and Reiers¿l (p. 169) but these had not re°ected
Koopmans's concerns with inference from population to structure and the role of prior information in making such inference possible. Rothenberg's ® may be a structural parameter or
a reduced form parameter. In terms of Koopmans's two steps, the \statistical identi¯cation
problem" can be interpreted as encompassing both steps or just the ¯rst. Rothenberg (p.
577) opts for the encompassing possibility, describing the \identi¯cation problem" as concerned with the \possibility of drawing inferences from observed samples to an underlying
theoretical structure."
Bayesian treatments of identi¯cation
The \solution" of the identi¯cation problem would be seen in the posterior distribution and so
there was a very direct way of extending identi¯cation into Bayesian theory. Morales deemed \identi¯ed" a model in which the posterior density is not °at. Rothenberg also considered the possibility:
From the Bayesian point of view, the posterior density function summarises all
the prior and sample information that is available. If the posterior distribution for
the structural parameter ® is highly concentrated around its mean, then we are
in an excellent position to distinguish between di®erent values of ®. ... Thus one
is tempted to de¯ne identi¯cation in terms of the degree of concentration of the
posterior density.
Rothenberg drew back because it becomes possible for a parameter to be identi¯able \even
though the data were completely irrelevant". So he used \estimable" for this concentration
notion, leaving \unanswered the question of an appropriate Bayesian de¯nition of identi¯cation".
Koopmans had established a body of interrelated propositions about identi¯cation in
econometrics: lack of identi¯cation matters because inference about structural parameters
is prevented or impeded; identi¯cation is about prior information making possible{or not{the
transition from the reduced form to the structural form; theorems about when identi¯cation
is achieved usually involve the rank of certain matrices. The Morales-Rothenberg proposal
built on the ¯rst of these propositions but by picking di®erent ones authors could extend
identi¯cation into Bayesian theory in any number of ways{all would satisfy one criterion of
naturalness, that when `contracted' they correspond to classical identi¯cation. The next four
sections consider di®erent Bayesian responses to identi¯cation. For accounts of the origins of
Bayesian econometrics see Drµeze and Richard and Qin .
Broadening the concept of identi¯cation
It would be too much to say that Bayesian econometrics was solely a response to the identi¯cation problem but the problem was seen as one where the new approach could make a
di®erence{see Drµeze . Lindley's remark, \it might be noted that
underidenti¯ability causes no real di±culty in the Bayesian approach" became a talisman
for Bayesian econometrics. Rai®a and Schlaifer had shown how to treat the singular X0X
regression case when there is extra-sample information and the new Bayesian theory{unlike
Je®reys's{opened up the possibility of using uncertain prior knowledge to solve or alleviate
the identi¯cation problem. From this new perspective Koopmans had entertained either certain prior knowledge (about the restrictions) or no prior knowledge (about the unrestricted
coe±cients). and Gelfand and Sahu {but it would
take too long to explore this.)
Jacques Drµeze and Arnold Zellner , the founding fathers of Bayesian
econometrics, devised formal constructions extending the Koopmans notion of identi¯cation.
They both emphasised how Bayesian analysis permits the introduction of uncertain prior
information into the simultaneous equations model. Zellner argues
Usually in the sampling theory framework exact restrictions are imposed on the
parameters of a model to achieve identi¯cation ... Since prior information in the
Bayesian framework need not necessarily take the form of exact restrictions ...
there is a need to broaden the concept of identi¯cation to allow for the more
general kind of prior information used in Bayesian analysis.
Drµeze and Zellner worked on the simultaneous equations model but{following the example
of Leamer {their approaches can be illustrated in the simpler regression framework.
Suppose we take as the basic scheme the unrestricted structural form and corresponding
reduced form the following regression speci¯cation
N(X(®1 + ®2); ¾2I)
N(X¼; ¾2I) : ¼ = ®1 + ®2:
To simplify matters further suppose that ¾2 is known. Koopmans assumes that there is no
obstacle to knowing ¼: However, as neither ®1 or ®2 can be determined from knowledge of ¼;
neither is identi¯able. Consider imposing the a priori restriction ®2 = 0. Now identi¯cation
of ®1 is achieved because ¼ determines ®1. This is an identifying restriction.
Drµeze reproduces in Bayesian terms the two step inference procedure described by Koopmans. The ¯rst step is inference to the reduced form and the second is inference from the
reduced form to the structural form. Drµeze shows how this decomposition is valid in that the
data is informative only about the reduced form parameters; the structural parameters and
the data are independent given the reduced form parameters.
f(® j¼; y) = f(® j¼)
With the restriction, ®2 = 0, a single ®1 is associated with any given ¼. Without it, in¯nitely
1s are associated with any given ¼. The identifying restriction could be interpreted as
a prior probability distribution for ®2 concentrated on 0. A way of weakening this restriction
would be to take an uninformative prior for ®1 and choose an informative prior for ®2, say:
®2 s N(0; M):
To take advantage of this extension, Drµeze introduces the notion of \identi¯cation in probability." De¯ne the parameter ®2 as \identi¯ed in probability" when the conditional prior
probability of ®2 given ¼ is proper. In the present case
®1 j¼ s N(¼; M)
Identi¯cation is the limiting case of identi¯cation in probability when the probability distribution is concentrated at a single point.
The \more general kind of prior information" envisaged by Zellner may take the form of
an informative prior on ®2, e.g.
®2 s N(0; M)
then we can integrate ®2 out of the likelihood and obtain
y s N(X®1; ¾2(I + XMX0))
In this new distribution no distinct values of ®1 are observationally equivalent. Here is a
\broadened" concept of observational equivalence on which to base a \broadened" concept of
identi¯cation. These rest on a broadened notion of likelihood{a marginal likelihood re°ecting
both the prior for ®2 and the original likelihood.
These broadened concepts involve both the likelihood and prior{as they are meant to!.An
immediate consequence is that identi¯cation in probability, say, may be lost if the prior is
changed. Of course this characteristic was inherited from the ancestor, the Koopmans hierarchical speci¯cation of unrestricted model plus a priori restrictions. However the project of
broadening the scope of identi¯cation seems to have been suddenly abandoned. Drµeze had been presenting identi¯cation in probability in lectures.
Reactions to this presentation have led me to recognise that it was misleading to
use the word \identi¯cation" in de¯ning a property of the prior density for the parameters of unidenti¯ed models. I agree with Kadane's view \that identi¯cation is
a property of the likelihood function and is the same whether considered classically
or from the Bayesian approach."
Drµeze's Damascene conversion was signi¯cant for the Louvain school of Bayesian econometrics
followed him.
The role of identi¯cation in Bayesian theory
The \view" from J. B. Kadane's \The Role of Identi¯cation in Bayesian Theory" , as a declaration that a Bayesian
de¯nition is unnecessary.
Kadane demonstrates the role of identi¯cation through theorems of relevance to Bayesians.
These expand a remark he (p. 180n) attributes to Savage: \identi¯cation is properly a part
of the study of the design of experiments". Kadane (pp. 184-189) applies the preposterior
decision machinery of Rai®a and Schaifer, considering an experiment \valuable" if it can
yield information leading to a better decision, i.e. if the minimum expected loss is reduced by
performing the experiment (see x7.4 below for examples). His Theorem 4 is a \characterization
of identi¯ed functions in terms of valuable experiments".
Zellner's comprehensive book on Bayesian econometric treats one preposterior
topic{a multi-period control problem in which control also gives an opening for learning about
the structure of the economy (Sections 11.5-6){and econometricians might have concluded
from Kadane's theorems that identi¯cation was none of their business as they rarely design
experiments{see x7.4 below. However they produced variants of Kadane's results. Leamer
 and Hsiao (p. 273) depart from the decision formulation and consider an experiment \informative" about µ if it might change your opinions about µ. A common feature
of this work and most that has followed is the emphasis on the change from the prior to the
posterior not, as in Morales (see x7.1), an emphasis on the posterior and its properties.
The various theorems di®er in scope but agree in showing that the situations{the experiments{
characterised by identi¯cation and informativeness are the same. When Kadane and Leamer
describe their theorems they bring to the surface an issue running through this entire story:
what is sameness? Kadane (p. 181) argues that \the dependence on the prior indicates that the
concepts of identi¯cation and informativeness are di®erent" while Leamer (p. 193) claims that
\the words identi¯able ... and publicly informative ... are interchangeable", which presumably
means that the corresponding concepts are the same. In x3.1 we met various de¯nitions of
su±ciency and Rai®a and Schaifer's (p. 34) demonstration of the \complete equivalence" of
the Bayesian and classical de¯nitions. The di®erences involved here seem philosophical rather
than statistical. Di®erent inference theories employ di®erent terms and the terms of the characterisations may not make sense in other theories although the objects characterised are the
same. in which a prior distribution has only a formal
signi¯cance indicates that we must take care in interpreting \making sense".) The case of
Je®reys's co-option of likelihood suggests that one consideration underlying the judgement of
similar enough to be considered the same is that enough signi¯cant propositions remain true
when the change is made. There has been a great deal of discussion amongst historians and
philosophers of science of the way concepts change{or keep{their meaning as the theory in
which they are embedded changes. See Sankey provides a useful review. The literature
is suggestive but it does not generate exploitable conclusions.
Returning to Kadane's \view", there is a second argument in his paper which provided
backing of a di®erent kind.
The duality between parameter and data
Kadane (p. 178) expounds the \analogy of the theory of identi¯ed functions to the theory of
su±cient statistics". This idea has been developed in conjunction with the notion of a duality
between parameter and data in Bayesian theory. This notion was discussed by several authors
including Florens and Mouchart , Picci and Dawid ; Dawid also refers to
earlier literature. The most detailed and abstract treatment of identi¯cation on these lines is
by Florens, Mouchart and Rolin but I will take as texts the more elementary
discussions in Gourieroux and Montfort and Dawid .
On the Bayesian view parameters as well as observations are random variables. Gourieroux
and Montfort present a pair of de¯nitions{of su±ciency of a statistic
\in the Bayesian sense" and of su±ciency of a parameter \in the Bayesian sense":
² A statistic S is su±cient in the Bayesian sense if µ and Y are conditionally independent
given S(Y ), i.e. µ ? Y j S(Y ) :
² A function g(µ) of the parameter is said to be su±cient in the Bayesian sense if Y ? µ j g(µ) :
Next they de¯ne a function of the parameter as minimal su±cient if it is su±cient and a function of any other su±cient function. They (p. 108) then state a \property"{not a de¯nition{
relating identi¯cation (Rothenberg style) to minimal su±ciency: \µ is identi¯ed if and only if
µ is minimal su±cient in the Bayesian sense."
Gourieroux and Montfort do not refer to \identi¯cation in the Bayesian sense" but their
de¯nitions and properties could be re-choreographed to produce such a notion. In his account
of conditional independence in statistical theory Dawid considers the distribution
of X as determined by a pair of parameters (£; ©): In the case when X ? © j £ ; the parameter
© is \redundant once £ is known". In such a situation the \full parameter (£; ©) is said
not to be identi¯ed". Thus the full parameter is not identi¯ed when it contains redundant
parameters.
In the basic scheme of x7.1 the parameter vector (®; ¼) contains redundant
parameters. However the redundancy idea is better applied reparametrising from ® to (¼; ±)
where ± (= ®1 ¡ ®2) is the redundant part of ®:
Using properties of conditional independence Dawid glides through de¯nitions and results,
If £ is a su±cient parameter, so that X ? © j £ ; and the parameters have a
prior distribution , then © ? X j £ ; so that p (Á j x; µ) = p (Á j µ) : We see that
the conditional distribution for the redundant part © of the parameter, given the
su±cient parameter £; is the same in the posterior distribution as in the prior:
once we have learned about £ from the data, we can learn nothing about ©, over
and above what we knew already.
The demonstration of the duality between parameters and data and the parallel between
identi¯ed functions and su±cient statistics may have given identi¯cation a new naturalness
in Bayesian theory. Of course Bayesians may not be equally impressed by this insight. For
Florens, Mouchart and Rolin the theory of reduction is a major part of the subject while
Lindley is unimpressed by the \trick" of su±ciency, classifying the di±culties
for Bayesian theory created by its absence as \numerical not inferential".
Informativeness of observations
Neath and Samaniego emphasise their distance from Kadane's concern with
learning from an experiment when they present some outcome level analysis pertaining to
nonidenti¯able models. However they do not investigate the possibility of not learning from
outcomes. Drµeze had mentioned the possibility in his outline of the inferential
use of Bayes' formula
if P(x j Bi) [the likelihood of the observation] is the same for all i, then P(Bi j x) is
equal to P(Bi); and the observation is noninformative; when this property holds
for all x, the Bi's are called \observationally equivalent".
However, he did not dwell on noninformative observations nor relate them to the identi¯cation
discussion later in his paper.
The noninformative observation can provide the basis for observational equivalence but
it has no role in unconditional classical theory. Yet it is a prima facie interesting concept
to anyone who accepts the likelihood principle and wants to do terminal analysis (x5.1).
Noninformativeness is to outcomes what Fisher's statistic supplying no information about a
parameter is to experiments (see x2.2).
In the paradigm cases of the identi¯cation problem in the regression and simultaneous
equations models all the points in the sample space are noninformative if any is{the same is
true for Neath and Samaniego's binomial model with parameter p = p1+p2 and Fisher's genetic
model. Yet it is clearly possible for an identi¯ed experiment to generate some noninformative
outcomes. I have two biassed coins: for one the probability of heads is 0:8; for the other
0:2. I have forgotten which is which and contemplate experimenting to help me decide. One
experiment would be to toss the{arbitrarily determined{`¯rst' coin twice, record the total
number of heads, Y , and base the decision on that number. Another would be to toss the ¯rst
coin and then the second and record the total number of heads and base the decision on that
number. The parameter space comprises the two possible identities of the`¯rst' coin. Using
notation based on that of x6.2, with f(0; 0:8) as the probability that Y = 0 given that the
¯rst coin is the 0:8 coin, the densities for the ¯rst experiment are
0:04 : f(1; 0:8) = 0:32 : f(2; 0:8) = 0:64
0:64 : f(1; 0:2) = 0:32 : f(2; 0:2) = 0:04
and for the second
0:16 : f(1; 0:8) = 0:68 : f(2; 0:8) = 0:16
0:16 : f(1; 0:2) = 0:68 : f(2; 0:2) = 0:16:
The ¯rst experiment is valuable in Kadane's sense (assuming a reasonable loss function and
nondogmatic prior) and can help me decide which coin is which but the second experiment
is worthless. The ¯rst coin is identi¯able from the ¯rst experiment but not from the second.
Consider now the observation Y = 1 which is noninformative in both experiments. According
to the likelihood principle, it does not matter whether this observation has come from the
informative, valuable experiment or from the uninformative, valueless one. The division of
experiments into identi¯ed ones and unidenti¯ed ones is not essential for terminal analysis;
the essential division is into informative and noninformative observations.
The special uniform distribution discussed by Barndor®-Nielsen and Cox 
provides another example where an informative outcome may fail to be realised. Consider a
random sample of size 2 from a uniform distribution on (µ ¡ 1; µ + 1) with µ 2 fµ1; µ2g. Let
Y(1) and Y(2) be the order statistics and Y the mean and R the range Y(2) ¡ Y(1). Given data
y(1) and y(2), the likelihood is °at on the interval (y ¡ 1 + 1
2r; y + 1 ¡ 1
2r) of the parameter
space. If µ1 and µ2 are both in this interval the observation is noninformative.
Poirier has written about noninformative observations under the rubric \uninformative data". He envisages a nonidenti¯ed model where part of the parameter Ã is identi¯ed
and part ¸ is unidenti¯ed. He (p. 485) de¯nes
The data y are marginally uninformative for ¸ i® f(¸ j y) = f(¸): The data y are
conditionally uninformative i® f(¸ j Ã; y) = f(¸ j Ã)
One could imagine analogous concepts at the experiment level.
Poirier's (pp. 485-6) results are on the lines of the sentences following Lindley's celebrated remark (his µ1 corresponds to ¸ and the \remaining parameters" to Ã):
In passing it might be noted that underidenti¯ability causes no real di±culty in
the Bayesian approach. If the likelihood does not involve a particular parameter,
µ1 say, when written in the natural form, then the conditional distribution of
µ1, given the remaining parameters, will be the same before and after the data.
This will not typically be true of the marginal distribution of µ1 because of the
changes in assessment of the other parameters caused by the data, though if µ1
is independent of them, it will be. For example, unidenti¯able (or unestimable)
parameters in linear least squares theory are like µ1 and do not appear in the
likelihood.
Bauwens, Lubrano and Richard consider the same set-up with identi¯ed and
unidenti¯ed sub-parameters and consider whether the marginal and conditional priors are
revised by the sample. These results have a family resemblance to those of Dawid discussed in
x7.3 as Gelfand and Sahu point out. However only Poirier seems to be working
at the outcome level.
The tags, \identi¯cation ... is the same whether considered classically or from the Bayesian
approach" and \underidenti¯ability causes no real di±culty in the Bayesian approach", sit
together uneasily even if they are not precisely contradictory. Poirier (p. 483) reducers the
tension by showing that in a nonidenti¯ed model there is \no Bayesian free lunch ... there
exist quantities about which the data are [marginally] uninformative." For Morales, say (x7.1),
the price of the \free lunch" was obtaining the prior information. Value-added accounting
focusses on the properties of the transformation of prior to posterior, not on the properties of
the posterior.
Does Bayesian theory require a di®erent de¯nition?
By way of summary, consider the generalised Hsiao question from the Introduction, \whether
Bayesian theory requires a di®erent de¯nition of | from the classical one" and the received
answers{received today, that is, for these are matters of judgement not of necessity. Of course
the fact that the question can be posed re°ects the continuing subordinate status of Bayesian
For likelihood, the received answer is tacitly no, for the di®erences are not considered
material: allowing for the altered status of parameters the de¯nitions are the same (see x3.1 and
x5.1). For su±ciency there is no received answer: the answer will depend on how \required"
and \di®erent" are interpreted. Although the original Fisher de¯nition has not been used by
Bayesian writers, it could be used without creating scandal; it would have to be linked to the
scene of its application by a chain of equivalences: equivalences in the sense of having the
same extension. It is not a natural de¯nition for it points to features of the extension which
are irrelevant for Bayesian analysis. Je®reys used the factorisation de¯nition which requires
no{or at least a shorter{chain but he and Rai®a and Schlai®er saw merit in a more explicitly
Bayesian de¯nition; this may have the same extension as the classical de¯nition but it employs
terms with no place in classical theory. The nature of the \requirement" is to facilitate the
development of other concepts. (see x3.1).
In Bayesian econometrics today the identi¯cation question is routinely asked and answered
no (see xx7.2-3). Thus Piorier and Bauwens, Lubrano and Richard endorse
the Kadane declaration while at the same time recalling the econometric past. That past
contained radical alternatives to classical identi¯cation{so radical that it is misleading to
speak of di®erent de¯nitions of the same thing. In the general statistics literature there is not
the same burden of history, only the divergence seen with su±ciency. Neath and Samaniego
 give the classical de¯nition and get on with things. Gelfand and Sahu give a
Bayesian de¯nition of identi¯cation and get on with things; their de¯nition is based on Dawid
 which relates to the classical de¯nition the way that Rai®a and Schlaifer's de¯nition of
su±ciency relates to the classical de¯nition. (see x7.3).
Identi¯cation seems to be secure in Bayesian econometrics and the project of broadening
the concept of identi¯cation has lapsed but in a sense identi¯cation has bifurcated with a
second branch based on the informativeness of observations{see x7.4. Koopmans's original
notion of identi¯ability in relation to \hypothetical exact knowledge" appears to be of no
current interest.