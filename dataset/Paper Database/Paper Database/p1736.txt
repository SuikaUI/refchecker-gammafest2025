MODÉLISATION MATHÉMATIQUE
ET ANALYSE NUMÉRIQUE
T. PHAM DINH
A. YASSINE
Training multi-layered neural network with a
trust-region based algorithm
RAIRO – Modélisation mathématique et analyse numérique,
tome 24, no 4 , p. 523-553.
< >
© AFCET, 1990, tous droits réservés.
L’accès aux archives de la revue « RAIRO – Modélisation mathématique et
analyse numérique » implique l’accord avec les conditions générales d’utilisation ( Toute utilisation commerciale ou
impression systématique est constitutive d’une infraction pénale. Toute copie
ou impression de ce ﬁchier doit contenir la présente mention de copyright.
Article numérisé dans le cadre du programme
Numérisation de documents anciens mathématiques
 
MATHEMATICA!. MOOEUJNG AND HUMERICA1ANALYSIS
MODÉUSAT1ON MATHÉMATIQUE ET ANALYSE NUMÉRIQUE
 
TRAINING MULTI-LAYERED NEURAL NETWORK
WITH A TRUST-REGION BASED ALGORITHM (*)
T. PHAM DINH O'2), S. W A N G Q-3), A. YASSINE (
Communicated by F. ROBERT
Abstract. — In this paper, we first show how the problem of training a neural network is
modelized as an optimization problem ; and the generally used training algorithm. Then we
propose a new algorithm based on a trust-région technique which is very efficient for non-convex
optimization problems. Expérimental results show that the new algorithm is much faster and
robust compared with GBP. It makes the design of neural net architecture much less problemdependenL
Resumé. — Dans ce papier; nous allons présenter d'abord la modélisation du problème
d'apprentissage par un réseau de neurones en un problème d'optimisation et l'algorithme
d'apprentissage le plus utilisé (GBP). Ensuite, nous proposerons un nouvel algorithme basé sur
la technique de région de confiance qui est très efficace pour des problèmes d'optimisation non
convexe. Les résultats expérimentaux que nous donnerons montrent que le nouvel algorithme est
beaucoup plus rapide et robuste par rapport à GBP. Et en plus, il rend la conception des réseaux
beaucoup plus indépendante du problème particulier traité.
1. INTRODUCTION
Recent years have seen explosive interests in neural networks, a new
paradigm of information processing System .
A neural network is a network of highry interconnected neuronlike
subsystems that are dynamically coupled and exhibit useful computational
properties via their collective behavior. There are many models of neural
(*) Received in December 1988.
0) Laboratoire TIM 3/INPG, 46, avenue Félix Viallet, 38031 Grenoble, France.
(2) Équipe Optimisation
(3) Équipe de Calcul Parallèle.
M2 AN Modélisation mathématique et Analyse numérique 0764-583X/90/04/523/31/$ 5.10
Mathematical Modelling and Numerical Analysis © AFCET Gauthier-Villars
T. PHAM DINH, S. WANG, A. YASSINE
networks, sortie of them have been studied over the last 40 years. The recent
résurgence of interests in neural networks is due to the rapid development
of hardware and to the discovery of new algorithms which leads to the
appearance of new models.
The model of multilayered neural network is commonly considered as a
powerful connectionist model for the tasks such as constraint satisfaction
and nonlinear classification. The example that most authors refer to for
showing its advantage over other single-layered models is the learning
problem of the exclusive-or relation (xor), because this elementary relation
is characterized by separating four non-linearly separable points on a plane.
The learning (or training) algorithm for the multilayered network, called
gradient back propagation, has been found by Hinton, Rumelhart
 and by Le Cun independently. It is based on the method of steepest descent for the solution of
optimization problems. The objective function hère dépends on the whole
set of connection weights and on the desired input-output associations
(patterns) to be realized. It is the total error committed by the network and
measured in euclidean space. The goal of the optimization is to find a set of
appropriate weights so that the objective function reaches its minimum.
However, one would expect to diminish the objective function until a very
small value, which means in practice that the network is able to do the
desired input-output associations.
Here in the following, we present the mathematical modeling of the
learning problem and the learning algorithm.
1.1. Gradient back propagation algorithm (GBP)
We consider an automata network, structured on successive layers, with
the éléments illustrated on the figure 1. From now on, we call them cells.
Aj= Èi wj * e; : Total input
f(x) = (l-exp(-« x)/(l+exp(-o( x)
Figure 1. — Elementary cell where es are its inputs, wt are the input weights
and a is the parameter determining the sigmoidal function ƒ.
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
Figure 2 gives an example of such a network. It receives inputs on the first
layer (index 0) and produces the outputs on the last layer (index N). It is
assumed that a connection between any two cells can only go from a lowerlayer cell to a higher-layer cell and that no intra-layered connection is
allowed. Each cell i of the network updates its output by :
xt = f(At)
- Layer N-l
Figure 2. — A gênerai se hem e of multilayered network.
where wl} is the connection weight from cell ƒ to cell i and ƒ is a sigmoidal
function (as shown in fig. 1), and the sum is calculated over all the (lowerlayer) cells ƒ which are connected to cell i.
At = Y wt * et : Total input
f(x) = (1 - exp(- ouc)/(l + exp(- ca)
Suppose that there are K input-output associations to be realized
Yke Rm and fc = l, ...,ÜT. Each input vector
Xk is taken as states for the cells in layer 0, and the output Sk a vector
composed of the cell states in the last layer is calculated by applying the
formula (1) to all the cells in each successive layer until the last one. The
corresponding error of the network is defined as
Ck(W,Xk,Yk)=
||S<fc)-Y(/:)||2.
vol 24, n 4, 1990
T. PHAM DINH, S. WANG, A YASSINE
And the global error function is defined as the sum of all C \
C(W)=Yd Ck(W, Xk, Yk) .
It is this error function that serves as the objective function to be minimized.
In the GBP algorithm, the gradient method is applied to each error
function Ck, one by one following a pre-defined or random order.
If we note for the £>th association (Xk
9 Yk) a set of new variables
yt defined as
the gradient of Ck with respect to the connection weight wtj can be easily
written as
The formula (4) and (5) provide a convenient way to calculate the gradient
of C*. The gradient back-propagation algorithm can be shown through three
1 ° Supply the input Xk,
then calculate the output of the network
Sk by forward propagation of the states :
If xt is on the layer 0 : xl = Xk
2° Supply the desired output Yk. Calculate the gradient yt by the backpropagation :
For the output cells :
From layer iV - 1 to 1 :
3° Modify the weights by gradient method :
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelhng and Numencal Analysis
MULTI-LAYERED NEURAL NETWORK
where Xh is a séquence of positive real numbers (converging to zero)
which defines the length of forward step that the weight vector will be
incremented in the direction of the gradient.
The algorithm is called gradient back propagation because the computation of the gradient is done in a way similar to the propagation of input
states but in the opposite direction.
The discovery of this algorithm has answered the question of existence of
multilevel learning algorithms for perceptron-like networks, put forth by
Minsky and Papert . The algorithm makes it
possible for the multilayered network to solve very complicated matching
problems, while the algorithm itself is simple. But it also has some serious
drawbacks.
1) The GBP is, from strategie point of view, too dependent on a single
direction at each step, due to the nature of the gradient method. With the
fixed forward factors \h which can not be optimal, the GBP algorithm may
cause serious problems such as misleading the searching process when
Kh are relatively large or a very slow learning process otherwise.
2) The performance of the algorithm dépends heavily on the empirical
choice of learning parameters, network architecture and initial weights.
These characteristics of networks, especially on what is concerned with the
architecture (the distribution of cells over different layers, connections and
weights), are determined by the nature of the problems to be treated . Thus a deep study of the
problem should be made before an "appropriate" net architecture can be
worked out. In other word, the design of network is very problemdependent.
3) The algorithm aims mainly to find a minimum of the objective
function, no special effort is made for finding the solution with a good
neighborhood properties.
There are many ways to improve the algorithm. By expérience, we feel
that it is necessary to use some more sophisticated techniques in order to
accelerate the searching of minimum and at the same time to have a good
eigen-property for the objective function. Stéphane, Schereiber and Wang
 show the importance of network
stability which has direct relation to the eigen-property.
Trust-Region (TR) algorithm has been developed in order to improve the
learning algorithm. It is a good algorithm for the minimization problem of
non-convex function (as for our cases). It consists in successively approximating the objective function by a quadratic form and reducing the function
via the direction that minimizes the quadratic form. The scale of the région
on which the function is approximated is determined dynamically regarding
vol. 24, ne 4, 1990
T. PHAM DINH, S. WANG, A. YASSINE
the "quality" of the approximation. Since the quadratic form is a second
order approximation depending on the gradient and the Hessian (in
practice, it is an approximation of Hessian) of the objective function, the
minimum found in this way has thus good second order properties.
In the section 2 of the paper we will present the TR algorithm in its
original form, the theoretical results on the convergence and other
properties of the algorithm. In section 3, we will discuss our adaptation of
the algorithm to the learning problem. We will give the practical algorithms
for each computing stage in the TR algorithm. The expérimental results and
the comparisons with the GBP algorithm will be given in the section 4.
2. TRUST REGION METHOD IN UNCONSTRAINED MINIMIZATION
We consider the unconstrained problem :
min {ƒ(*):* e R»}
where ƒ is a function from Un to IR. We dénote by gk the gradient of ƒ at
xk and by Hk the Hessian of/at xk or an approximation of it, which therefore
will be called "quasi-Hessian". The main purpose of this section is to
describe and analyze a technique for the solution of this problem.
The approach we shall present is well known .
It is appropriately called a model trust région approach in which the step to
a new iterate is obtained by minimizing a local quadratic model to the
objective function over a restricted spherical région centered about the
current iterate. The diameter of this région is expanded and contracted in a
controlled way based upon how well the local model predicts behavior of the
objective function. It is possible to control the itération in the way so that
convergence is forced from any starting value assuming reasonable conditions on the objective function.
In fact we shall present some very strong convergence properties for this
method in § 2.4). There it is shown that one can expect (but not ensure) that
the itération will converge to a point which satisfies the second order
necessary conditions for a minimum.
Trus région method computes a conséquence of itérâtes by solving at each
step a quadratic problem with an euclidean norm constraint
min {qk{d): \\d\\ *~hk)
where qk is a quadratic approximation of the variation of/at xk defined by :
qk(d) = (gk,d) + 1/2 (Hkd,d) .
The strictly positive number 8^ is the trust radius.
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
In trust région algorithms the computation of gradient of the objective
function is required. The use of first order information leads in gênerai to
the first order stationary point. By incorporating the second order information Hk = V2f(xk) these algorithms may satisfy the second order
necessary conditions for (F). In this case the trust région method can be
regarded as modified Newton's method applied to finding a zero gradient of
objective function. It can be described as follows :
Computing the direction dk, solution to (Pk), we can easily check the
quality of the local approximation qk(d) and hence take the appropriate
décision :
* If the approximation is satisfactory, then the solution of (Pk) yields a
new iterate, and the trust radius is increased.
* In the opposite case the iterate is unchanged, in addition the trust
radius is decreased until qk yields a satisfactory approximation inside the
trust région (which necessarily occurs for small radii since the gradient is
supposed to be exactly known and the first order term in qk becomes
dominant if U^U ^ 0).
Of course this gênerai scheme can be implemented in many different
ways. The quality of the approximation is generally examined through the
following quantity, called "quality coefficient" :
The numerator represents the actual réduction of ƒ when we move from
xk to xk + dk, the denominator represents the predicted réduction according
to the quadratic approximation. Thus, in a trust région algorithm, the main
source of computation effort, apart from the function évaluation required, is
the work on a problem of the form (Pk) to détermine the step from the
current iterate.
Trust région algorithms differ in their stratégies for approximately solving
2.1. An abstract algorithm
Let us describe an abstract algorithm at the beginning :
1. Let x0 e Rn, 80 > 0 and Ho be given
2. For ik = 0,1,2,...
a) Compute gk = Vf(xk),
if gk = 0, xk will be taken as an optimal
solution. Stop! (See §2.3).
vol. 24, nB 4, 1990
T. PHAM DINH, S. WANG, A. YASSINE
b) Détermine a solution dk to problem (Pk) and compute the quality
coefficient rk via (6).
c) Update the iterate.
d) Update the trust radius and the quasi-Hessian and go to step 2.
2.2. Computing TR itération
In the following five sub-sections we are going to discuss the last three
points a), b) and c) of the above abstract algorithm .
2.2.1. Update o f the trust radius :
The trust radius is updated according to the following rule : Let
0 < M- <c r\ < 1 and 0 <: yx < y2 < 1 < "Y 3 be specified constants.
1. If rk < fx then hk+l = Ae [yx hk, y2 hk]
2. If rk < -n then bk + 1 e [y2 hk, bk] else Ô* + 1 e [hk, y3 hk].
2.2.2. Update of the iterate :
In trust région method, the updating of the iterate is usually governed by
a parameter s such that 0 < s < 0.25 which must be kept constant
throughout the itération. The rule is the following :
1. If rk < s then xk + l = xk
2. If rk > s then xk + 1 = xk + dk.
Note that a significant decrease of the function is demanded to allow the
algorithm to move from xk to xk -+- dk.
2.2.3. Update of the quasi Hessian Hk :
In this section we will give two algorithms for updating Hk. Suppose
yk == Vf(xk
+ 1) — Vf(xk),
sk = xk + x —xk and Ho has been initialized as a
symmetrie matrix.
(i) Formula of rank 1 (DFP)
Hk + i == Hk +
where ak = - 1/4 . (Hksk - yk) and uk = Hksk-
(ii) Formula of rank 2 (BFGS)
tfjt + i = Hk + <*k • «jk • K£ + Pk • vk • <
where ^ = 1/ 4 7*, «* - 7* and P* = - 1/4 • ^ *k> vk = Hk sk.
It has been proved that if ƒ is
quadratic and A = V2f(x)
is positive défini te, the Hk calculated by the
above formula converges to V2/(x*).
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
2.2.4. The local problem and its properties :
The local problem is reduced to that of minimizing a quadratic form inside
a sphère :
min {<<?,</> + 1/2 (d,Hd)
: \\d\\ < 8}
where g is a n-vector, H is a symmetrie matrix and S is a positive number.
Since the constraint set {d e Rn : \\d\\ < 8} is compact, the problem (LP)
has a solution. If in addition H is positive definite then there is uniqueness of
solution to (LP).
Hère our purpose is to give a complete discussion of the theoretical
aspects of problem (LP) and to set out the nature of the computational
difficultés that may be met.
LEMMA 2.1 : d* is a solution to (LP), if and
only if there is JJL > 0 such that :
(i) (H + |x/ ) positive semidefinite
(ii) (H+^I)d*
(iii) ||d*|| <ô and |x(||rf*|| - 8 ) = 0. D
The solution of (LP) is straightforward if (LP) has no solution on the
boundary of {deUn:
\\d\\ < 8 } . In fact (LP) has no solution d with
\\d\\ = 8 if and only if H is positive definite and Hif" 1^ < 8. The
nonnegative scalar |x in Lemma 2.1 is called the Lagrange multiplier
associated with the constraint || d ||2 < 82.
It is worth noting that (LP) represents an interesting case of nonconvex
optimization problem whose complete characterization of solution can be
pointed out.
Now let us define a function c|> on
{ix e R : H + JJJ non singular }
where rf(|x) is solution of (ƒ/+ \xl)d = — g.
Dénote by Xx < X2 ^ * * * ^ ^« t n e eigenvalues of H and by uu u2, -.., un
their corresponding eigenvectors. We note JC(H — \x I) the null space of
(or the eigenspace relative to X^), and Jx = {i : \t = \ : } .
It is clear that the solution of problem (LP) is closely related to the
nonlinear équation <t>((x) = 8 for (JL in ]— \l9 + oo [. More precisely this is the
case where (LP) has a solution on the boundary of its constraint set and
there is |x > max(0> — \j) in Lemma 2.1. In this case the algorithm of
Hebden (which shall be presented below) is known to be reliable and
efficient for solving 4>(M>) — 8 = 0.
vol. 24, ne 4, 1990
T. PHAM DINH, S. WANG, A YASSINE
The so called "hard case" corresponds to the situation where the
coefficient jx in Lemma 2.1 must be equal to — Kv
Let us consider now the solution of the équation
(H + \x,I ) d = ~ g for fx > - X !
using the eigensystem of H. We obtain that if (x :> \ : then the solution to (7)
is defined by :
for i = l, . . . , « .
Thereforect>(fx)= | | | |
The function <})(|x) is positive and strictly decreasing in ]— \x, + oo [ with
lim ^(M') = 0, then the équation <(>(|x) = ô has at most one solution in this
1° If g is perpendicular to JV(H — kxl) then problem (7) with |x > — Xx
/ras ö/wfly5 « solution.
2° Problem (7) mï/i \y = —\x has a solution if and only if g perpendicular
Moreover the solution set to problem (7) with yu = - \ l is
d* + JV* (H + \xl ) where d* is defined by :
u\ d* = — «ƒ ^ / ( \ H- JJL)
otherwise. D
See Appendix A for the proof.
The following results (which are conséquence of the previous ones) are
helpful in understanding the algorithm described in the sequel for solving
LEMMA 2.3 : Suppose that g ^ 0 in (LPj (See Lemma 2A for the case
1° If Xx > 0 (Le. H is positive definite) then
||H"101| < 8 then d = -H~1g
is the only solution to (LPJ
(|x = 0 in Lemma 2.1 and 4>(JX) - Ô = ||d(|x)|| - ô < 0).
(ii) Otherwise
\\H'1 g\\ > ô and 4 > ( 0 ) - Ô = ||d(0)|| - Ô ^ O , therefore
there is an unique
|UL:>0 to <t>(|x) = 8.
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numencal Analysis
MULTI-LAYERED NEURAL NETWORK
2° If X: = 0 (Le. H is only positive semidefinite) then :
(i) If || (H - Xx / ) + g || < 8arc<2 (ƒ/ - Xx / ) ( # - Xi / ) + g = g flftwequality occurs if and only if g is perpendicular
to tAr(H—\1I)
Lemma 2.2) then every d = - (H - X: / ) + g + w, with ue JV(H
and \\d\\2 = \\ (H- Xx 7) + #|| 2 + ||w|| 2< Ô2 is a solution to problem
(ix = 0 in Lemma 2.1).
(ii) If \\(H - \ l i y g\\ > 8 tóen <j>(|x) - 8 = 0 admits the unique
in ]- \l7 + oo [.
3° If Xj < O
(i) / / | | ( / / - \2 / ) + g\\ <8and (H - \x I)(H - krl)+g = g (this equality occurs if and only if g is perpendicular to jV(H ~ X1I) according to
Lemma 2.2) then every d = - (H - \x I)+ g + u, VWZY/Ï u e ^ ^ - ^ Z )
and \\d\\2= \\ (H - \x I)+ ^||2 + ||w||2 = 82 is a solution to problem (LV),
(ix = - Xj => O m Lemma 2.1)
(ii) /ƒ || (/ƒ - Xx / ) + g || ;> 8 r/ïen 4>(|UL) — 0 = 0 admits the unique solution
in ]— \l9 + oo [, (|x > - \ x > O m Lemma 2.1).
4° ƒƒ Xrt < O (ï.e. ?^e quadratic function q(d) is concave) then (LP) has only
solution on the boundary of {d : ||d|| < 8} unless g and H are null
 .
(i) lf\l = O then H = 0. /n fAw ozse r/ze solution set of (LP,) w ;
if g = 0 .
Lemma 2.4)
(ii) /ƒ Xj < O //ze/ï we apply the same results as in 3°. •
The situation where # = O in (LP) is closely related to the variationai
spectral theory .
LEMMA 2.4 : If g = 0 in problem (LP) then the solution set of (LP) is :
(i) Wï/A^O;
(ii) {deJfiH-^I):
\\d\\ < 8} i/Xj = 0 ;
(iii) { J G ^ " ( i ï - Xi ƒ) : ||^|| = 8 } z/ Xi <= 0 f/w this case |x = - X iw
Lemma 2.1). D
Proof: We can use the results above or those concerning variationai
properties of the spectral theory .
2.2.5. Algorithms for the local problem (LP) ;
Three algorithms will be given for solving (LP) in different cases. The first
two, Hebden and Projection methods, are generally applied when H is
vol. 24, n° 4, 1990
T. PHAM DINH, S. WANG, A. YASSINE
positive semidefinite, and the third one using curvature information is used
to deal with the "hard cases" that correspond to 2° (i) and 3° (ii) in
Lemma 2.3.
In the hard cases, except that the eigensystem of H is easily obtainable
(See Lemma 2.2) recognizing each of the situations 2° (i) and 3° (ii) in
Lemma 2.3 may also be time consuming because of the computation of
(ƒƒ — Xj I)+ g. Fortunately there is a completely acceptable alternative due
to (See § 2.2.5) c). Their inexpensive
approximate solution technique appears to perform as well as the more
expensive exact method in practice. This third algorithm requires the
computation of the smallest eigenvalue X1 of H and an eigenvector of H
corresponding to \ v
a) Hebden algorithm
Let us describe now this algorithm for the solution problem (LP) in the
case where the nonlinear équation : 4>(|x) — 8 = 0, with :
admits an unique root in ]— \ l 5 + oo [.
 and observed independently that in order
to solve (7) the fact which should be taken into account is that the function
4>2(JJL) is a rational function in jx with second order pôles on a subset of the
négatives of the eigenvalues of H (See (7)).
The Newton's method which is based on a local linear approximation to
<(>(|x) is then not likely to the best method for solving (7) because the
rational structure of <f>2(|x) is ignored. Instead, an interation for solving (7)
can be derived based upon a local rational approximation to <(>. The itération
is obtained by requiring <t>*(jx) = y/{a. + |x) to satisfy <t>*(|x) = <K|x),
<J>*'(|x)<()'(fji) where we take \L as the current approximation to the root IJL*.
This approximation is then improved by solving for an ji that satisfies
= 8. The resulting itération is
In fact Hebden's algorithm can be viewed as Newton's algorithm applied to
the équation
M-e ]-X!, + oo[.
The local rate convergence of this itération is quadratic but the most
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
important feature of (8) is that usually the number of itérations required to
produce an acceptable approximation of fx* is very small because the
itération is based upon the rational structure of <}>2. Itération (8) can be
implemented without explicit knowledge of the eigensystem of H. This
important observation which is due to makes it possible to
implement (8) merely by solving linear system with (H + jx/ ) as coefficient
matrix. This is easy to see since
where (H + jx/) rf(jx) = - g.
Therefore the Hebden's algorithm can be described as follows : Let
|x0 => 0 with H + |x0 ƒ positive definite, and <J>(|x0) > 0.
k = 0, 1, 2, ... until convergence
1) Factor (H + jx* + / ) = RTR.
2) Solve RTRp -
3) Solve RTq = p. m
k = A; + 1 and return to 1).
In this algorithm RTR is the Cholesky factorization of (/f + jx/ ) with /?
upper triangular. The function i|i is convex and strictly decreasing on
]_Xl5 + oo[.
This fact was discovered by and follows from the expression
of 4>(l^)- It implies that Newton's method started from (XQ e ]— \1? -h oo[
with ^(^0)^0 (i.e. <(>(|UL0) —ôr>0) produces a monotonically increasing
séquence converging to the solution of <f>(|x) - 8 = 0.
b) Projection methods
i) Projected gradient method :
Let H be a n * n symmetrie positive semidefinite and g a vector in
W. We note
f(x) = l/2(x,Hx) + (g,x)
as the corresponding convex quadratic function.
Consider now the following convex optimization problem :
a = Inf {f(x):xeC}
where C is a closed convex set in R". The solution set to ( ^ ) is denoted by
vol. 24, n' 4, 1990
T. PHAM DINH, S. WANG, A. YASSINE
We shall use the following characterization of solution to ( ^ ) : x* is solution to {@>) if and only if
x* = Projc (x* - p V/(#*)) forsome p^>0.
where Projc dénotes (orthogonal) projection operator on C :
y) o \\y-x\\ =mi
The projected gradient method for solving (9* ) is then defined as :
Starting with an arbitrary x0 in C, we set
xk + 1=?mjc(xk~9kVf(xk))
where the séquence of positive scalars (pk) are chosen in order to assure the
convergence of (xk) to a solution to {0P).
We shall giv& a particular interesting choice of (pk) which is based on the
contraction property. For other choices, see .
It is clear that if p^ = p for every k then (9) is exactly the fixed point
itération relative to F9(x) = Projc (x - p Vf (x)). Our choice is :
The convergence resuit concerning this method is given in the Appendix B,
1. The projected gradient algorithm is practically interesting when the
operator Projc is easy to calculate. For instance if
C= {xeW: ||x-y||sr},C=n [«„Mor
C = Ul - {*€R n:x^0} .
2. To obtain the convergence of this algorithm, we could use its following
For y0 arbitrarily chosen, we construct the séquence (yk) by :
yt+1 = ïk + ^*(Projc (y* where X^ is such that ^ + 1 bea solution of the problem :
{f(y : y e [yk, Projc (yk - p, Vf(yk))])}
ii) Primai dual or Uzawa algorithm :
After introducing the duality (relative to the primai problem (ëP)) with
the help of Lagrangian L(x, jx)? we define the dual problem {3) and
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
describe the main relations between solutions of primai and dual problems.
Uzawa algorithm is effectively based on this duality.
For the generality of the duality theory and the details of the results
presented here, one can see .
Let us suppose that the closed convex set C be defined analytically by :
C = {xeRn:
ƒ,(*)< 0, i = l , ...,m}
where fl9 i = 1, ..., m, is convex continuous function on IR". If we define
the vector-valued function 4> as :
K 0 = (A(*), •..,ƒ„(*))
for every x in Rn. Then C can be written as
C = {xGRn:<t>(x)<0} .
We define the Lagrangian L(x, |x) on R" x R™ by
where' |x = (|x( ) and R™ = {(x e IRm : (x > 0} .
It is clear that L(x, fx) is convex (resp. concave or rather affine) in x
(resp. in \x) for |x fixed (resp. x fixed). It follows that the function
g(|x) = inf {L(x, ^) ;x e Rn} is concave.
The dual problem (®) is then defined by :
An element (x*, |x*) e R" x IR+ is called a saddle point of the Lagrangian
L ( j c , j J i ) i f
L ( ^ f j O < L ( x ^ * ) < L ( x , |x*)
for every (JC, \x)eRn xR™.
Note 5((x) for each u- e R" the solution set to the following problem :
000 = inf {L(x,\x):xeRn} .
The Uzawa algorithm can be described as follow : Starting with an arbitrary
ixo in R+ we define x0 as an element in S(|x0). If \Lk is known we take
xk in 5(jxfc) and define ixfc + 1 by :
Vk + l = Proj (M-jt + Pjt*(JCfc))
where p^ is chosen in ]0, p* [, being a constant depending on H (See § 2.2.5)
vol 24, n' 4, 1990
T. PHAM DINH, S. WANG, A. YASSINE
The theoretical results about the method is given in Appendix C. There
we can see that Uzawa algorithm solves primai and dual problems at the
same time. It is worth noting that Uzawa algorithm is projected gradient
method applied to the dual problem
i) x = Proj (y) is given by a : xt = yt if yt > 0, 0 if yt < 0, i = 1, ..., n.
ii) In trust région method, the closed convex set C is of the form
C = {x e Rn : 1/21|x \\2 < 1/2 . ô2} ;
5(|x) = {x e Un : (H + JUL/ ) x = - g} . It follows that 5(jx) is singleton if H
is positive definite or |x > 0.
Like Hebden method, the method of Uzawa works well only when the
matrix H is at last positive semidefinite.
c) The most négative curvature method
The two methods stated above for solving the problem (LP) are very
efficient when the matrix H is positive definite. If H is indefinite or only
positive semidefinite it should be préférable to use the following strategy
due to for the solution of problem (LP) in
the hard case (m = — \x in Lemma 2.1 & 2.3)
1° Taking a G ]— kly c . max(|A^|, \fl)], where c> 1 is a constant which
dépends on each problem.
2° Solving p = - (H -h a/ )"l g.
3° If ||/? || > 8, then apply the Hebden method to //:=// + a/ in order to
find a direction d.
4° If ||p || = ô, then
5° If \p\ < ô, then d = p + $uu where ux is an eigenvector of H corresponding
£ is chosen such that:
sign (g) = sign (u{p).
Compared to Hebden algorithm, the only modification introduced by the
method of négative curvature is at step 5) where we should proceed as if
ix = - Xj in Lemmas 2.1 & 2.3.
The direction d obtained by this method gives as good a decrease of
quadratic model as a direction of sufficient négative curvature and satisfies
the practical conditions 1 and 2 that will be given in § 2.4).
2,3. Convergence tests
We can use the following termination criteria in trust région algorithms :
* First order necessary condition test ;
= 0 then stop .
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
* Second order sufficient condition : when the first order necessary
condition is satisfied we can stop the algorithms or proceed to verify second
order sufficient condition as follows :
(i) If Hk is not positive definite then the algorithm continues (See
Lemma 2.4, for the solution of (LP) in the case where g = 0).
(ii) If V2/(xk) is positive definite then stop. Otherwise Hk = V2/(xk) and
we restart the algorithm from the step d) (See § 2.1).
2,4. Convergence results of the TR method
We end this section with some well-known convergence results on the TR
algorithm.
THEOREM 2.5. : If
the function fis differentiable, bounded below on Rn, and ifVf is uniformly
continuons then :
| | V / ( * t ) | | = 0 .
THEOREM 2.6 : If the function f is twice continuously differentiable and
bounded below on Un, and if V2f
is bounded on the level set
[x e Rn : ƒ(*)< f(x0)}
then trust région algorithms with Hk = V2f(xk)
possess the following properties :
lim H V ƒ (**) H = 0 .
2° If {xk} is bounded then there is a limit point x* with V2/(x*) positive
semidefinite.
3° Ifx* is an isolated limit point of {xk} then V2 f (x*) positive semidefinite,
4° IfV2f{x*)
is non-singular for some limit point x* of {xk} then
a) V2/(x*) is positive definite;
b) limx k = x* and there exists a B > 0 and integer K such
c) The convergence is superlinear.
Besides the superlinear convergency, the zero of gradient and the positive
definiteness of Hessian are also very important properties of the algorithm
which encourage us to adapt the algorithm to our learning problem.
Although in practice, we use an approximation instead of Computing the
Hessian during the optimization, these properties are still present.
In the above two theorems, we have an implicit assumption that every
step in the TR algorithm can be exactly carried. For example, dk is supposed
to be exactly solved. Apparently, it cannot be always true in the practice.
What conditions every calculated direction should satisfy in order to
preserve the above convergence properties ?
vol. 24, n 4, 1990
T. PHAM DINH, S. WANG, A, YASSINE
Before giving the convergence conditions, we define an additional
notation : let d(g, H, 8) stand for a calculated direction (depending on g, H
pred (g, H, 8) = - <<?, d(g, H, 8)> - V2{d(g, H, 8), Hd(gy H, S» .
Our conditions that a step sélection strategy rnay satisfy are :
Condition 1
There exists cu sx>0
so that Vg e R", V/feR"*" symmetrie et
VS => 0 then :
pred (g, H, 8) > C l. \\g || . min (8, st. ||ff ||/||ff|| ) .
Condition 2
There exists c 2>0 so that y g e Rn, VifeR"XM symmetrie and
V<2 > 0 then :
hère X2 is the most smallest eigenvalue of H.
Condition 3
If t h e matrix H is positive definite a n d || H~~ xg\\
< ô t h e n :
THEOREM 2.7 : Le? ƒ : Rn i-> R 6e ?mee continuously differentiable and
bounded from below, and let H{x) = V2/O) satisfy \\H{x)\\ < pj /or a//
JC e R". Suppose that an practical TR algorithm is applied to f(x), starting
from xoe Mn, generating a séquence {xk} , xk G IRrt, fc = 1, 2, ..., then :
1° If d(g,H,§)
satisfies Condition 1 and \\Hk\\ < p2 /or a// *, rte«
Vf (xk) converges to 0 (first order stationary point convergence),
2° If d(g7H,§)
satisfies Condition 1 and 3, Hk = H(xk) for ail k,
H(x) is Lipschitz continuons with constant L, and x* i$ a limit point o f
{xk} with ƒƒ(**) positive definite, then xk converges q-quadratically to
3° If d{g,Hh)
satisfies Condition 1 and 2, Hk^H(xk)
for ail k,
is uniformly continuous, and {xk} converges to x*> then
H(x*) is positive semidefinite (second order stationary point convergence,
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
3. ADAPTED TR LEARNING ALGORITHM
In this section, we will present the practical TR based algorithm utilized in
our experiment s.
In applying the TR algorithm to the learning problem by a multilayered
network, we need some special considération in programming, because the
method needs the gradient of the objective function with respect to all the
connection weights instead of the gradient with respect to all the total inputs
(to the cells) as in the GBP algorithm, and the objective function should
often be evaluated for sets of temporal connection weights.
In order to maintain the same notation, we still use ƒ to represent the
function to be minimized, that means f(W) = C (W), where W can be
viewed as a vector of connection weights. Our practical algorithm is as
0° Initialization ;
W(o): The weight set which is initialized arbitrarily between — 1.0 and
80 > 0 : initial trust radius,
e :> 0 : "zero" for gênerai tests ;
Bg > 0 : zero for ||gr|| ;
£ƒ :> 0 : zero for ƒ ;
Ho : A small-value symmetrie matrix, or if possible, taking Ho as
H(W0) the Hessian of the objective function at Wo. Evaluating the smallest
eigenvalue \ x and one corresponding eigenvector, vx ; k = 0 ; itération
1° Calculating fk = f(W{k)),
gk = V/(W<*>).
2° If \\gk\\ < e^ or ƒ*£< e ƒ then W^
is our solution ; stop.
3° Calculating dk :
If Xj > 0, solve Hkd = — gk ;
if {\\d\\^bk)
then dk = d;
else using Hebden or Projection method to find a (x > 0 so that the solution
of (Hk + \x,I)d = - gk satisfies | \\d\\ - 8fc| < e, then
else ( % < 0*) then ajk = - \j + 0.0001 ; solving/?^ = - (Hk + akl)~1
if ( \\pk || > bk) then apply the Hebden method to H = Hk + ak I in order to
«lx > 0 so that || (Hk + (|x + ak) 7)"1 gk\\ = §k
vol. 24, n° 43 1990
T. PHAM DINH, S. WANG, A. YASSINE
dk = - (Jfk+Cli +
else if (\\pk\\ = bk) then
else if (||/?jt|| <: Sfc) then dk=pk-\-^u1,
where £ is chosen so that
|| dk || = 8^ and
sign (£) = sign (u[pk) .
f{xk)-f{xk
4 Let rk —
If (rk > 0.1 ) then W{k+1) = W{k) + ^ else
5° If rk < 0.25 then hk+ x = hk/2
else if (rk > 0.75 ) then bk +
else ôfc + i = $£.
6° Updating i ^ + i by the algorithms in 3.2) formula of rankl, or of
rank 2. Using any practical method to calculate the smallest eigenvalue and
a corresponding eigenvector of Hk + 1,
T k = k + 1 and return to step 1°).
4. EXPERIMENTAL RESULTS
Three experiments are described below. The first one shows a comparison
of average performance between the GBP algorithm and the TR algorithm
for two different structures of network. The second one is to show the
advantage of the TR algorithm with regard to the "hard" cases for GBP
algorithm, although these hard cases are not frequent events when
connection weights are initialized randomly. The third one shows that the
performance of the TR algorithm is much less dependent on the network
architecture and more robust.
The testing problem is the learning problem of xor relation. For GBP
algorithm, the order of pattern présentation is fixed and the learning rate
has been initialized or fixed depending on the purpose of the tests. The
sigmoidal function for each cell is not identical, the alfa value defining each
of the functions has been initialized between 0.5 and 1.5. When there is an
initialization of the connection weights, it has always been taken between
- 1 and 1.
4.1. Average performance comparison
The two networks figure 3 (xor r 211) and figure 4 (xor_r_241) have
fixed structures as shown in the following pictures. The "blank" cells on
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
Figure 3. — xor_r_211, network for xor with 2 input cells,
1 intermediate cell, and 1 output cell.
Figure 4. — xor_r_241 : network for xor with 2 input cells,
4 intermediate cells, and 1 output cell.
these pictures are those who serve as threshold cells with 1 as their states.
Every other cell should be connected to it except for those of input.
The connection weights are initialized 100 times for each network. For
each set of initial weights, the GBP and TR are applied to improve the
weights. The maximum number of training itérations is fixed to be 1 000.
For the GBP method, the learning rate is initialized between 0 and 0.4 for
each new set of weights. The two figures (5 & 6) show the average évolution
of the objective function in each net. Only the results within 100 itérations
are shown. It is remarked that the function has rapidly converged to zero
using TR algorithm in both cases, but only the second network xor_r_241
with (too) many connections has reached a comparable performance using
GBP algorithm, training the net xor_r_211 with GBP has failed since the
total error stays approximately at 1.0 (at least until 1000 learning
itérations).
4.2. Comparison Between Particular Nets
Two fixed networks xor_S 1 and xor_S_2 shown below (fig. 7 & 8) are
among the "hard" cases for GBP. The learning rate for GBP has been
chosen to be 0.2. None of the two nets can be trained in less than 1 000
itérations by GBP.
vol. 24, n° 4, 1990
T PHAM DINH, S. WANG, A. YASSINE
Training perf. compar.
Figure 5. — Average performance of the two algorithms over 100 randomly initialized weight
sets for the network in figure 3 (xor_r_211). The legend xor_r_tr_211 corresponds to the same
net but trained by TR algo.
Training perf. compar.
Figure 6. — Average performance of the two algorithms over 100 randomly initialized weight
sets for the network in figure 4 (xor_r_241). The legend xor_r_tr_241 corresponds to the same
net but trained by TR algo.
Figure 7. — xor_S_l, a 2_1_1
net with given initial weights.
Figure 8. — xor__S_l, a 2 2_1
net with given initial weights.
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
Taining perf. corapar.
Figure 9. — xor_S_l corresponds to the net in figure 7 trained by GBP algorithm,
xor S_tr_l corresponds to the net in figure 7 trained by TR algorithm.
Training perf. compar.
Figure 10. — xor_S_ 1 corresponds to the net in figure 8 trained by GBP algorithm,
xor_S_tr_l corresponds to the net in figure 8 trained by TR algorithm.
Note : The "hard" cases for GBP observed in these two examples are not
rare when the number of connections is relatively small compared to the
number of patterns and a majority of weights have a same sign. It is
remarked that the TR algorithm may also meet difficulties (fig. 10), but it
has eventually converged while there has been no hope for GBP algorithm.
4.3, Structure independence
Many studies on the multilayered network now concern with the
architecture of network, because GBP is a slow and very inefficient
algorithm. In the real application, we have to work out many "intelligent"
net structures (especially the receiving field of hidden cell) to facilitate the
vol 24, n° 4, 1990
T. PHAM DINH, S. WANG, A. YASSINE
learning and to favorite certain generalization tasks. This is somewhat
paradoxal with the saying that GBP can learn automatically what a network
is asked to do. A more efficient algorithm is needed for training multilayered
networks because for many application problems we can not have a priori
much information about their nature which allows us to design special
The following example shows the robustness of our new algorithm with
respect to net structure. It concerns about the realization of a vectorial
boolean function by a 6_3_3 network. The function F is defined as
F: { - 1 , 1 } 6 ^ {-1,1} 3
F(xi, x2, x3, x4, x5, x6) = (Xi x2 + x2Xi, x3 x4 H- x4x3, x5 x6 + x6x5) .
It is composed of 3 xor fonctions. There are, in total, 64 associations to be
From the expérience we know that if we use GBP algorithm, one of the
best choices for the net architecture is as shown in figure 11. This is a natural
choice because each output cell dépends upon only 2 input cells and it does
not need any information from other input cells. The intermediate cells are
equally "attributed to" each of the three parts in order to make each part be
able to deal with the nonlinear separateness of the xor problem.
Figure 11. — A 6 3 3 net composed of three 2_1_1 nets
can realize F with both algorithms.
In this network there is not any connection between each output cell and
any irrelevant input cell. Ho wever, in the real application we may not get
enough information for us to détermine this "intelligent" net. We may
chose a network with full inter-layer connection as shown in figure 12. We
are going to see that the TR based algorithm works well for both networks
but GBP does not.
In designing the figure 11 (also fig. 12) we have ignored the threshold cell
(blank cell in the previous net figures) and (of course) all the connections
issued from this cell because of the space constraint. Another reason
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
connections
o o o oo o
Figure 12. — A 6^,3—3 net with full inter-layered connection.
It is very difficult to be trained be GBP but easily by TR algorithm.
allowing us to do so is that it has become a convention that each non-input
cell should be connected to a special cell of state 1, where the connection
weight acts as threshold of the previous one.
Expérimental results show that if the weights in the network of figure 11
are not too "badly" initialized, we can train this net by GBP to realize the
function although the training times may be very long.
We have done the same experiments as in § 4.1 for both networks. The
statistical result for the first network is quite the same as shown in figure 5,
because the network is composed of three nets with the same architecture as
in figure 3. Here we given only the performance result (fig. 13) about the
second network. It is seen that there is no chance for it to be trained by
Although the first net is also a good choice for TR algorithm, the result in
figure 13 shows that it is not indispensable.
Training perf. compar.
Full_conn_gbp
Full_conn_tr
Figure 13. — FuU—conn_gbp : correspond to the net in figure 12 trained by GBP algorithm.
Full conn_tr : correspond to the net in figure 12 trained by TR algorithm.
vol. 24, n° 4, 1990
T. PHAM DINH, S. WANG, A. YASSINE
DISCUSSION AND CONCLUSION
During the experiments, we have remarked that :
1) If a network has many connections and if it is far from "saturation",
the GBP algorithm is better in convergence speed, because in this case the
dimension of the "décision space" is relatively high and the gradient
calculated at each training step can give good decent to the objective
function. Since the time for each learning step is much less for GBP (only up
to one fifteenth of that for TR), it may be said that in this case GBP is
better. But in real applications, a network may be asked to store very
complicated (in terms of nonlinearity) input-output associations and to store
as many as possible, in such situation GBP fails.
2) When GBP works, TR works too. When GBP does not work, the TR
may still work unless all the weights of a net are initialized as almost the
same value. The TR algorithm has a higher capacity to deal with the "hard"
initial conditions.
3) In many cases, TR algorithm makes the Hessian of the objective
function positive définit e at the end of training. This means that the solution
has good eigen-properties.
4) For any architecture of networks, the TR algorithm can always give a
good solution. Due to the efficient use of the second order information, the
TR algorithm can overcome many difficulties encountered by GBP in the
choice of network architecture.
The TR algorithm can considerably reduce the human intervention in
training the neural networks, and make the so called "automatic learning"
(by neural net) more significant.
Alghough the use of the TR algorithm is time-consuming in each learning
step, it can be compensated by a spectacular réduction in the total number
of learning steps. The calculation of the Hessian (second order information)
can be efficiently replaced by approximations as in our experiments. The
real drawback of the TR algorithm is that it needs a great amount of physical
memory as for any optimization method which uses the second order
information. It is the memory for storing the Hessian matrix whose number
of the éléments is up to a power of two of the total number of connection
This drawback can partially be made up by an efficient utüization of
middle-size network as it is promised by the algorithm. We think that a
completely satisfactory solution to the problem may be found in a good
compromise between the use of TR principle and the choice of a class of
intermediate variables (less numerous) to which the TR algorithm is
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
applied. There are many theoretical and practical problems in the "passage"
between the intermediate variables and the "real" weights. This will be the
subject of our next paper.
APPENDIX A : PROOF TO LEMMA 2.2
It suffices to prove 2\
If ja, = - \x then the range of H + |x/ is Lin {ut : i ^ J\) which is equal to
JV(H —\1I)±.
Statement 2° is then immédiate.
It is easy to verify that d* is a solution to (7) with |x = — kx. It follows that
the solution set of problem (7) is d* + JV* (H — \ l / ) .
d* e JV (H - \x ƒ )x , d* is a solution of minimum norm to problem (7) with
||d*|| =min {\\d\\ : d e d* + JT we can write :
APPENDIX B : CONVERGENCE OF PROJECTED GRADIENT METHOD
Here is the theoretical result on the convergence of (xk) defined at
§ 2.2.5) b) i) for p^ = p. It is obtained through the study of contraction
property of Fr
THEOREM B.l : Suppose H be positive definite. Let \x (resp. \„) be the
smallest (resp. largest) eigenvalue ofH. Then (P) admits an unique solution
and we have
where q{ç>) = Max { 11
q (p ) < 1 if and only if 0 <= p < 2 /X„ .
Moreover the optimal value p* of p (Le. p* minimizes q(p) on ]0, 2/\„[) is
p* = 2/(\j
(iii) Finally the séquence (xk) defined by (9) with pk = p G ]0, 2/\n[
every k converges to the solution of (P). D
vol. 24, nE 4, 1990
T. PHAM DINH, S. WANG, A. YASSINE
Proof: It is well known that if H is positive definite then ƒ (x) is strictly
convex and
f(x) = + oo .
It follows that (P) admits an unique solution . Since the operator
Projc is non expansive , we have
\\Fp(x) - FpOOII < ||Projc (x - p V/(*)) - Projc it follows that
III — pH II = Max { 11 — pXj |, 11 — pXn | }
because the function X •-» 11 — pX | is convex. By simple calculation we
obtain (ii). The last property (iii) is simple conséquence of the contraction of
operator Fp for p e ]0, 2/XJ. D
APPENDIX C : THEORETICAL RESULTS ABOUT THE UZAWA METHOD 
THEOREM Cl
(ii) If (x*, |x*) is a saddle point of L(x, jx) then JC* is a solution to
and |x* is a solution to (£&) and a = p.
(iii) If there is x0 such that $(x0) < 0 then a = p and x* is a solution to
if and only there is |x* e IR+ such that (x*, |x*) is a saddle point of
(iv) If H is positive definite or C is bounded then S is nonempty. D
COROLLARY C.2 : If there is x0 such that <J>(x0) < 0 then x* is a solution to
{&>) if and only if there is |x* e R™ such that
ƒ(**) 4- <|x*, <|>(JI:*)) = inf {f(x) + <|x*, <|>(x)> : x e Rn}
<|>(;c*)<0 and (|x*, <j>(^*)) = 0
such an |x* is necessarily solution to (&). D
COROLLARY C.3 : (i) /ƒ |x* is a solution to (O) ) f/zen 5(|x*) contains the
solution set S to {&*). More precisely we have
M2 AN Modélisation mathématique et Analyse numérique
Mathematical Modelling and Numerical Analysis
MULTI-LAYERED NEURAL NETWORK
(ii) IfH is positive definite then 5(jx) is non empty and in f act reduced to a
single point
SGO= {s(n)}. D
THEOREM C.4 : If A is positive definite and if there is x0 such that
<K*o) then
(i) xk = s(jxk),
(ii) g is differentiatie and V^(fx) = <f>(^(^)).
(iii) The séquence (\Lk) converges to a solution &* to {£$) and the
séquence (xk) converges to a solution x* to (£P). D