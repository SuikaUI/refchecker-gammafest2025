Communicated by Andrew Barto
Improving Generalization for Temporal Difference
Learning: The Successor Representation
Peter Dayan
Computational Neurobiology Laboratory, The Salk Institute,
P.0. Box 85800, Sun Diego, CA 92186-5800 USA
Estimation of returns over time, the focus of temporal difference (TD)
algorithms, imposes particular constraints on good function approxi-
mators or representations. Appropriate generalization between states
is determined by how similar their successors are, and representations
should follow suit. This paper shows how TD machinery can be used
to learn such representations, and illustrates, using a navigation task,
the appropriately distributed nature of the result.
1 Introduction
The method of temporal differences 
is a way of estimating future outcomes in problems whose temporal struc-
ture is paramount. A paradigmatic example is predicting the long-term
discounted value of executing a particular policy in a finite Markovian
decision task. The information gathered by TD can be used to improve
policies in a form of asynchronous dynamic programming .
As briefly reviewed in the next section, TD methods apply to a learn-
ing framework, which specifies the goal for learning and precisely how
the system fails to attain this goal in particular circumstances. Just like a
proposal to minimize mean square error, TD methods lie at the heart of
different mechanisms operating over diverse representations. Representa-
tion is key-difficult problems can be rendered trivial if looked at in the
correct way. It is particularly important for systems to be able to learn
appropriate representations, since it is rarely obvious from the outset ex-
actly what they should be. For static tasks, generalization is typically
sought by awarding similar representations to states that are nearby in
some space. This concept extends to tasks involving prediction over time,
except that adjacency is defined in terms of similarity of the future course
of the behavior of a dynamic system.
Section 3 suggests a way, based on this notion of adjacency, of learning
representations that should be particularly appropriate for problems to
which TD techniques have been applied. Learning these representations
can be viewed as a task itself amenable to TD methods, and so requires
Neural Computation 5, 613-624 
@ 1993 Massachusetts Institute of Technology
Peter Dayan
no extra machinery. Section 4 shows the nature of the resulting repre-
sentation for a simple navigation task. Part of this work was reported in
Dayan .
2 TD Learning
Consider the problem of estimating expected terminal rewards, or re-
turns, in a finite absorbing Markov chain; this was studied in the context
of TD methods by Sutton . An agent makes a transition between
nonabsorbing states i and j E N according to the ijth element of the
Markov matrix Q, or to absorbing state k E 7
with probability Sik, with a
stochastic reinforcement or return whose mean is t k and whose variance
is finite. In this and the next section, the returns and transition probabil-
ities are assumed to be fixed. The immediate expected return from state
i E N, represented as the ith element of a vector h, is the sum of the
probabilities of making immediate transitions to absorbing states times
the expected returns from those states:
The overall expected returns, taking account of the possibility of making
transitions to nonabsorbing states first, are
= [h]; + [Qh]; + [Q2hIi + . . .
= [(I - Q)-'h];
where 1 is the identity matrix.
The agent estimates the overall expected return from each state (com-
piled into a vector r) with a vector-valued function i(w), which depends
on a set of parameters w whose values are determined during the course
of learning. If the agent makes the transition from state if to iltl in one
observed sequence, TD(0) specifies that w should be changed to reduce
the error:
where, for convenience, [i(w)] is taken to be the delivered return ~ i , + ~
if if+l is absorbing. This enforces a kind of consistency in the estimates
of the overall returns from successive states, which is the whole basis
of TD learning. More generally, information about the estimates from
later states [i(w)]
for s > 1 can also be used, and Sutton defined
the TD( A) algorithm, which weighs their contributions exponentially less
according to As.
With the TD algorithm specifying how the estimates should be ma-
nipulated in the light of experience, the remaining task is one of func-
tion approximation. How w should change to minimize the error fI+l
f r t i = [i(w)li,+l - ['(w)li,
Improving Generalization for Temporal Difference Learning
in equation 2.2 depends on exactly how w determines [i(w)],,. Sutton
 represented the nonabsorbing states with real-valued vectors {xi},
[r(w)li as the dot product w - xi of the state vector with w taken as a
vector of weights, and changed w in proportion to
-(w . xi,+, - w . Xi,)Xi,
using z;,+~ instead of w . xi,+l if it+l is absorbing. This is that part of
the gradient -Vwct+l that comes from the error at step xi,, ignoring the
contribution from xi,+l .
In the "batch-learning" case for which the weights are updated only
after absorption, Sutton showed that if the learning rate is sufficiently
small and the vectors representing the states are linearly independent,
then the expected values of the estimates converge appropriately. Dayan
 extended this proof to show the same was true of TD(X) for 0 <
3 Time-Based Representations
One of the key problems with TD estimation, and equivalently with TD
based control (Barto et al. 19891, is the speed of learning. Choosing a good
method of function approximation, which amounts in the linear case to
choosing good representations for the states, should make a substantial
difference. For prediction problems such as the one above, the estimated
expected overall return of one state is a biased sum of the estimated ex-
pected overall returns of its potential successors. This implies that for
approximation schemes that are linear in the weights w, a good represen-
tation for a state would be one that resembles the representations of its
successors, being only a small Euclidean distance away from them (with
the degrees of resemblance being determined by the biases). In this way,
the estimated value of each state can be partially based on the estimated
values of those that succeed it, in a way made more formal below.
For conventional, static, problems, received wisdom holds that dis-
tributed representations perform best, so long as the nature of the dis-
tribution somehow conforms with the task-nearby
points have nearby
solutions. The argument above suggests that the same is true for dy-
namic tasks, except that neighborliness is defined in terms of temporal
succession. If the transition matrix of the chain is initially unknown, this
representation will have to be learned directly through experience.
Starting at state i E N, imagine trying to predict the expected future
occupancy of all other states. For the jth state, j E N, this should be
[Xilj = [llij + [Qlij + [Q'lij + . '
= [ ( I - Q)-']ij.
where [MI;, is the ijth element of matrix M and 1 is the identity matrix.
Representing state i using Xi is called the successor representation (SR).
Peter Dayan
A TD algorithm itself is one way of learning SR. Consider a punctate
representation that devotes one dimension to each state and has the Ith
element of the vector representing state k, [x& equal to [Ilk/. Starting
from i, = i, the prediction of how often [xi,], = 1 for s 2 t is exactly the
prediction of how often the agent will visit state j in the future starting
from state i, and should correctly be [x,lj. To learn this, the future values
of [xi,], for s 2 t can be used in just the same way that the future delivery
of reinforcement or return is used in standard TD learning.
For a linear function approximator, it turns out that SR makes easy the
resulting problem of setting the optimal weights w*, which are defined
as those making i = r(w*). If X is the matrix of vectors representing the
states in the SR, [Xli, = [x,Ii, then W* is determined as
which implies, from equations 2.1 and 3.1, that
But h is just the expected immediate return from each state-it
sensitive to all the temporal dependencies that result from transitions to
nonabsorbing states.
The SR therefore effectively factors out the entire temporal component
of the task, leaving a straightforward estimation problem for which TD
methods would not be required. This can be seen in the way that the
transition matrix Q disappears from the update equation, just as would
happen for a nontemporal task without a transition matrix at all. For
instance, for the case of an absorbing Markov chain with batch-learning
updates, Sutton showed that the TD(0) update equation for the mean
value of the weights W,l satisfies
Wn+l = W,l + nXD(h + QX'W, - XTW,,)
where X is the representation, a is the learning rate, and, since the up-
dates are made after observing a whole sequence of transitions from start
to absorption rather than just a single one, D is the diagonal matrix whose
diagonal elements are the average number of times each state is visited
on each sequence. Alternatively, directly from the estimates of the values
of the states,
(XTW,I+l - i) = [I - aXTXD(Z - Q)](XTW, - i)
Using X instead, the update becomes
W,l+l = W, + aXD(h - WIl)
(Wtf+l - h) = (I - aXD)(W, - h)
Improving Generalization for Temporal Difference Learning
Since X is invertible, Sutton’s proof that XTW, + r, and therefore that
W,, + h as n .+ 00, still holds. I conjecture that the variance of these
estimates will be lower than those for other representations X multilayer backpropagation TD network
in that they are fashioned to be appropriate for learning TD predictions
but are not directly observable and so have to be learned. Whereas An-
derson‘s scheme uses a completely general technique that makes no ex-
plicit reference to states’ successors, SR is based precisely on what should
comprise a good representation for temporal tasks.
4 Navigation Illustration
Learning the shortest paths to a goal in a maze such as the one in Figure 1
was chosen by Watkins and Barto et al. as a good example
of how TD control works. For a given policy, that is, mapping from
positions in the grid to directions of motion, a TD algorithm is used to
estimate the distance of each state from the goal. The agent is provided
with a return of -1 for every step that does not take it to the goal and
future returns, that is, future steps, are weighed exponentially less using
a discount factor. The policy is improved in an asynchronous form of
Peter Dayan
Figure 1: The grid task. The agent can move one step in any of the four
directions except where limited by the barrier or by the walls.
dynamic programming’s policy iteration by making more likely those
actions whose consequences are better than expected.
Issues of representation are made particularly clear in such a simple
example. For the punctate case, there can be no generalization between
states. Distributed representations can perform better, but there are dif-
ferent methods with different qualities. Watkins , for a similar
task, used a representation inspired by Albus’ CMAC . In this
case, CMAC squares which cover patches of 3 x 3 grid points are placed
regularly over the grid such that each interior grid point is included in
9 squares. The output of the units corresponding to the squares is 0 if
the agent is outside their receptive fields, and otherwise, like a radial
basis function, is modulated by the distance of the agent from the ten-
ter of the relevant square. Over most of the maze this is an excellent
representation-locations that are close in the Manhattan metric on the
grid are generally similar distances from the goal, and are also covered
by many of the same CMAC squares. Near the barrier, however, the
distribution of the CMACs actually hinders learning-locations close in
the grid but on opposite sides of the barrier are very different distances
from the goal, and yet still share a similar CMAC square representation.
By contrast, the successor representation, which was developed in
the previous section, produces a CMAC-like representation that adapts
correctly to the barrier. If the agent explores the maze with a completely
random policy before being forced to find the goal, the learned SR would
closely resemble the example shown in Figure 2. Rather like a CMAC
square, the representation decays exponentially away from the starting
state (5,6)
in a spatially ordered fashion-however, note SRs recognition
Improving Generalization for Temporal Difference Learning
Figure 2: The predictions of future occupancy starting from (5,6) after explo-
ration in the absence of the goal. The z-coordinate shows the (normalized)
predictions, and the barrier and the goal are overlaid. The predictions decay
away exponentially from the starting location, except across the bamer.
that states on the distant side of the bamer are actually very far away
in terms of the task (and so the predictions are too small to be visible).
Simulations confirm that using the SR in conjunction with a punctate
representation leads to faster learning for this simple task (see Fig. 31,
even if the agent does not have the chance to explore the maze before
being forced to find the goal.
This example actually violates the stationarity assumption made in
Section 2 that transition probabilities and returns are fixed. As the agent
improves its policy, the mean number of steps it takes to go from one state
to another changes, and so the SR should change too. Once the agent
moves consistently along the optimal path to the goal, locations that are
not on it are never visited, and so the prediction of future occupancy
of those should be 0. Figure 4 shows the difference between the final
and initial sets of predictions of future occupancy starting from the same
location (5,6) as before. The exponential decay along the path is caused
by the discount factor, and the path taken by the agent is clear. If the
task for the agent were changed such that it had to move from anywhere
Peter Dayan
RSR, no latent learning
RsR, latent learning
Learning iterations
Figure 3 Learning curves comparing punctate representation (Rpunctate), CMAC-
squares (&MAC)
and a punctate representation augmented with the SR (RsR),
in the latter case both with and without an initial, unrewarded, latent learning
phase. TD control learning as in Barto et al. is temporarily switched off
after the number of trials shown in the x-axis, and the y-axis shows the average
number of excess steps the agent makes on the way to the goal starting from
every location in the grid. Parameters are in Dayan .
on the grid to a different goal location, this new form of the SR would
actually hinder the course of learning, since its distributed character no
longer correctly reflects the actual nature of the space. This demise is a
function of the linked estimation and control, and would not be true for
pure estimation tasks.
5 Discussion
This paper has considered some characteristics of how representation
determines the performance of TD learning in simple Markovian envi-
ronments. It suggests that what amounts to a local kernel for the Markov
Improving Generalization for Temporal Difference Learning
Figure 4: The degradation of the predictions. Both graphs show the differences
between the predictions after 2000 steps and those initially-the top graph as a
surface, with the barrier and the goal overlaid, and the bottom graph as a den-
sity plot. That the final predictions just give the path to the goal is particularly
clear from the white (positive) area of the density plot-the
black (negative)
area delineates those positions on the grid that are close to the start point (5,6),
and therefore featured in the initial predictions, but are not part of this ultimate
Peter Dayan
chain is an appropriate distributed representation, because it captures all
the necessary temporal dependencies. This representation can be con-
structed during a period of latent learning and is shown to be superior
in a simple navigation task, even over others that also share information
between similar locations.
Designing appropriate representations is a key issue for many of the
sophisticated learning control systems that have recently been proposed.
However, as Barto et al. pointed out, a major concern is that the
proofs of convergence of TD learning have not been very extensively
generalized to different approximation methods.
Both Moore and Chapman and Kaelbling sought to ex-
orcise the daemon of dimensionality by using better function approxima-
tion schemes, which is an equivalent step to using a simple linear scheme
with more sophisticated input representations. Moore used kd trees , which have the added advan-
tage of preserving the integrity of the actual values they are required to
store, and so preserve the proofs of the convergence of Q-learning . However just like the CMAC rep-
resentation described above, the quality of the resulting representation
depends on an a priori metric, and so is not malleable to the task.
Chapman and Kaelbling also used a tree-like representation for Q-
learning, but their trees were based on logical formulas satisfied by their
binary-valued input variables. If these variables do not have the ap-
propriate characteristics, the resulting representation can turn out to be
unhelpful. It would probably not afford great advantage in the present
Sutton , Thrun etal. discussed a method for control in Markovian
domains that is closely related to the SR and that uses the complete
transition matrix implicitly defined by a policy. In the notation of this
paper, they considered a recurrent network effectively implementing the
iterative scheme
where xi is the punctate representation of the current state i and Q is the
Improving Generalization for Temporal Difference Learning
transition matrix. x, converges to Xi from equation 3.1, the SR of state i.
Rather than use this for representational purposes, however, Sutton and
Pinette augmented Q so that the sum of future returns is directly pre-
dicted through this iterative process. This can be seen as an alternative
method of eliminating the temporal component of the task, although the
use of the recurrence implies that the final predictions are very sensitive
to errors in the estimate of Q.
The augmented Q matrix is learned using the discrepancies between
the predictions at adjacent time steps-however,
the iterative scheme
complicates the analysis of the convergence of this learning algorithm.
A particular advantage of their method is that a small change in the
model (e.g., a slight extension to the barrier) can instantaneously lead to
dramatic changes in the predictions. Correcting the SR would require
relearning all the affected predictions explicitly.
Issues of representation and function approximation are just as key
for sophisticated as unsophisticated navigation schemes. Having a rep-
resentation that can learn to conform to the structure of a task has been
shown to offer advantages-but any loss of the guarantee of convergence
of the approximation and dynamic programming methods is, of course,
a significant concern.
Acknowledgments
I am very grateful to Read Montague, Steve Nowlan, Rich Sutton, Terry
Sejnowski, Chris Watkins, David Willshaw, the connectionist groups at
Edinburgh and Amherst, and the large number of people who read drafts
of my thesis for their help and comments. I am especially grateful to
Andy Barto for his extensive and detailed criticism and for pointers to
relevant literature. Support was from the SERC.