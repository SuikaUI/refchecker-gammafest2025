IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Deeply Supervised Salient Object Detection
with Short Connections
Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji, Zhuowen Tu, Philip H. S. Torr
Abstract—Recent progress on salient object detection is substantial, beneﬁting mostly from the explosive development of Convolutional Neural Networks (CNNs). Semantic segmentation and salient object detection algorithms developed lately have been mostly
based on Fully Convolutional Neural Networks (FCNs). There is still a large room for improvement over the generic FCN models
that do not explicitly deal with the scale-space problem. Holistically-Nested Edge Detector (HED) provides a skip-layer structure with
deep supervision for edge and boundary detection, but the performance gain of HED on saliency detection is not obvious. In this
paper, we propose a new salient object detection method by introducing short connections to the skip-layer structures within the HED
architecture. Our framework takes full advantage of multi-level and multi-scale features extracted from FCNs, providing more advanced
representations at each layer, a property that is critically needed to perform segment detection. Our method produces state-of-theart results on 5 widely tested salient object detection benchmarks, with advantages in terms of efﬁciency (0.08 seconds per image),
effectiveness, and simplicity over the existing algorithms. Beyond that, we conduct an exhaustive analysis on the role of training data on
performance. Our experimental results provide a more reasonable and powerful training set for future research and fair comparisons.
Index Terms—Salient object detection, short connection, deeply supervised network, semantic segmentation, edge detection.
INTRODUCTION
HE goal in salient object detection is to identify the
most visually distinctive objects or regions in an image
and then segment them out from the background. Different from other segmentation-like tasks, such as semantic
segmentation, salient object detection pays more attention
to very few objects that are interesting and attractive.
Such a useful property allows salient object detection to
commonly serve as the ﬁrst step to a variety of computer
vision applications including image and video compression , , image segmentation , content-aware image
editing , , object recognition , weakly supervsied
segmantic segmentation – visual tracking , nonphoto-realist rendering , , photo synthesis , ,
information discovery , , image retrieval , ,
action recognition etc.
Earlier salient object detection methods were mainly
inspired by cognitive studies of visual attention where
contrast plays the most important role in saliency detection.
Taking this fact into consideration, various hand-crafted
features have been designed, employing either global or local cues (See , for reviews). However, as these handcrafted features are based on the prior knowledge of existing datasets, they cannot be extended to be successfully
useful in all cases. Although some works have attempted to
develop different schemes to combine these features rather
than utilizing individual ones, the resulting saliency maps
are still far away from being satisfactory, specially when
encountering complex and cluttered scenes. To overcome
Q. Hou, M.M. Cheng, and X. Hu are with CCCE, Nankai University.
M.M. Cheng is the corresponding author ( ).
A. Borji is with the Center for Research in Computer Vision, University
of Central Florida ( )
Z. Tuo is with the University of California at San Diego.
P.H.S. Torr is with the University of Oxford.
A preliminary version of this work appeared at CVPR . The source
code are publicly available via our project page: 
the drawbacks caused by human priors, learning based
methods (e.g. ) appear to better integrate different types
of features to improve the generalization ability. Nevertheless, because many fusion details are designed manually,
the enriched feature representations still suffer from low
contrast and fail to detect salient objects in cluttered scenes.
In a variety of computer vision tasks, such as image
classiﬁcation , , semantic segmentation , edge
detection , , object detection , , and pedestrian detection , convolutional neural networks (CNNs)
 have successfully broken the limits of traditional handcrafted features. The emergence of fully convolutional neural networks (FCNs) have further boosted the development of these research areas, providing a more principled
learning method. Such an end-to-end learning tool also
motivates recent research efforts of using FCNs for salient
object detection , . Beneﬁting from the enormous
amount of parameters in FCNs, a large margin of performance gain has been made compared to previous approaches. The holistically-nested edge detector (HED) 
model, which explicitly deals with the scale space problem,
has led to large improvements over generic FCN models
in the context of edge detection. Though the mechanism
of fusing the multi-level features extracted from different
scales provides a much more natural way to edge detection,
it is incompetent to do segmentation related tasks. Edge
detection is an easier task since it does not rely too much on
high-level semantic feature representations. This explains
why skip-layer structure with deep supervision in the HED
model does not lead to obvious performance gain for
saliency detection. Experimental results also support this
statement as shown in Fig. 1.
In this paper, we focus on skip-layer structure with
deep supervision. Instead of simply fusing the multi-level
features extracted from different scales, we consider such
a problem in a top-down view. As demonstrated in Fig. 1,
 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
(a) source & GT
(b) results
(c) s-out 1
(d) s-out 2
(e) s-out 3
(f) s-out 4
(g) s-out 5
(h) s-out 6
Proposed HED-based
Fig. 1. Visual comparison of saliency maps produced by the HED-based method and ours. Though saliency maps produced by deeper (4-6)
side output (s-out) look similar, because of the introduced short connections, each shallower (1-3) side output can generate satisfactory saliency
maps and hence a better output result.
we observe that 1) deeper side outputs encode high-level
semantic knowledge and hence can better locate where the
salient objects are. However, due to the down-sampling operations in FCNs, the predicted maps are normally with irregular shapes especially when the input image is complex
and cluttered (see the bottle image), and 2) shallower side
outputs capture rich spatial information. They are capable
of successfully highlighting the boundaries of those salient
objects in spite of the resulting messy prediction maps.
Based on these phenomenons, an intuitive idea for yielding
better saliency maps is to reasonably combine these multilevel features. This motivates us to develop a new method
for salient object detection by introducing short connections
to the skip-layer structure within the HED architecture.
By having a series of short connections from deeper side
outputs to the shallower ones, our new framework offers
two advantages:
high-level features can be transformed to shallower
side-output layers and thus can help them better
locate the most salient region, and
shallower side-output layers can learn rich lowlevel features that can help reﬁne the sparse and
irregular prediction maps from deeper side-output
By combining features from different levels, the resulting
architecture provides rich multi-scale feature maps at each
layer, a property that is essentially needed to do efﬁcient
salient object detection. Our approach is fully convolutional
and no other prior information such as superpixels is
needed. It takes only 0.08s to produce a prediction map
with resolution of 300 × 400 pixels. Other than improving
the state-of-the-art results, we conduct exhaustive analysis
on the behavior of different training sets as there is no
universal training set for a fair comparison in the salient
object detection ﬁeld. Our goal is to offer a more uniﬁed
training set and meanwhile build a fair benchmarking
environment for future research.
RELATED WORKS
Over the past two decades, an extremely rich set of saliency
detection methods have been developed. The majority of
salient object detection methods are based on hand-crafted
local features – , global features – , or both
 , . A complete survey of these methods is beyond
the scope of this paper and we refer the readers to recent
survey papers , for details. Here, we mainly focus
on discussing recent salient object detection methods based
on deep learning architectures.
CNN-Based Saliency Models
Compared with traditional methods that use hand-crafted
features, CNN-based methods have refreshed all the previous state-of-the-art records in nearly every sub-ﬁeld of
computer vision, including salient object detection. In ,
He et al. presented a superpixel-wise convolutional neural
network architectures by utilizing hierarchical contrast features. For each scale of superpixels, two contrast sequences
were fed into convolutional networks for building more
advanced features. Finally, different weights were learned
to fuse the multi-scale saliency maps together, yielding
a much more conﬁdent one. Li et al. proposed to
use multi-scale features extracted from a deep CNN to
derive a saliency map. By feeding different levels of image
segmentation into the deep CNN and aggregating multiple
resulting features, a stack of fully connected layers are
then used to determine on whether each segmented region
is salient. Wang et al. predicted saliency maps by
integrating both local estimation and global search. A deep
neural network is ﬁrst used to learn local patch features
to provide each pixel a saliency value. Then, the local
saliency map, global contrast, and geometric information
are merged together as the input to another deep neural
network, which is used to predict the saliency score of
each region. In , Zhao et al. presented a multi-context
deep learning framework for salient object detection. Two
different CNNs are designed to independently capture
the global and local context information of each segment
patch. A ﬁnal regressor is used for ﬁnal saliency decision
of each segment patch. Lee et al. took into account
both high-level semantic features extracted from CNNs and
hand-crafted features. To combine them together, a uniﬁed
fully connected neural network was exploited to estimate
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Fig. 2. Illustration of different architectures. (a) Hypercolumn , (b) FCN-8s (c) HED , (d) and (e) different patterns of our proposed
architecture. As can be seen, a series of short connections are introduced in our architecture for combining the advantages of both deeper layers
and shallower layers. More interestingly, the last one can be viewed as a generalized version of all the formers.
saliency of each query region. Liu et al. designed a twostage deep network, in which a coarse prediction map was
produced, followed by a recurrent CNN to reﬁne the details
of the prediction map hierarchically and progressively. In
 , a deep contrast network was proposed by leveraging
the contrast information of the input images. It combined
a pixel-level fully convolutional stream and a segmentwise spatial pooling stream. A fully connected conditional
random ﬁeld (CRF) is also used for further reﬁning the
prediction maps from the contrast network. In , Wang
et al. proposed to leverage the advantages of recurrent fully
convolutional networks. By doing so, their recurrent fully
convolutional network allowed them to continuously reﬁne
previous prediction maps by correcting prediction errors.
A pre-training strategy using semantic segmentation data
is exploited for extracting generic representations of salient
Skip-Layer Structures
Very recently, great progress has been made in segment
detection because of CNNs and their ﬂexible architectures.
Of these versatile structures, skip-layer structures have
been widely accepted by most researchers owning to their
capability of fusing multi-level and multi-scale features.
Early-stage skip-layer structures such as Hypercolumn 
and DCL have made breakthroughs in their respective
ﬁelds. They, however, only simply fuse the skip layers with
different scales for more advanced feature representation
building as shown in Fig. 2(a). Differently, FCN-like structures (see Fig. 2(b)) considered a better way to utilize
multi-level features, gradually fusing the features from
upper layers to lower ones. In , Xie and Tu proposed
a scheme with deep supervision for each side output (skip
layer). Other than fusing all skip layers together, a series of
side losses are added after each side output for preserving
more details of the edge information. Fig. 2(c) shows a
simpliﬁed version of these architecture.
Despite the fact that multi-level and multi-scale features
have been taken into account and signiﬁcant progress has
been made by these developments very recently, there is
still a large room for improvement over the generic CNN
models that do not explicitly deal with the scale-space
DEEP SUPERVISION
WITH SHORT CONNEC-
This section describes our approach and some implementation details. Before that, let us ﬁrst take a look at the
observations.
Short connection
Fusion weight
side outputs
Fig. 3. The proposed network architecture. Our architecture is based on
VGGNet for better comparison with previous CNN-based methods.
As there are totally 6 different scales in VGGNet, 6 side outputs are introduced, each of which is represented by different colors. Besides the
side loss for each side output, a fusion loss is employed for capturing
features of different levels.
Observations
As pointed out in most previous works, a good salient
object detection network should be deep enough such that
multi-level features can be learned. Further, it should have
multiple stages with different strides so as to learn more
inherent features from different scales. A good candidate
for such requirements might be the HED network , in
which a series of side-output layers are added after the
last convolutional layer of each stage in the VGGNet .
However, experimental results show that this architecture is
not suitable for salient object detection. Fig. 1 provides such
an illustration. The reasons for this phenomenon are twofold. On the one hand, saliency detection, requiring homogeneous regions, is quite different from edge detection that
demands a special treatment. A good saliency detection
algorithm should be capable of extracting the most visually
distinctive objects and regions from an image instead of
simple edge information. On the other hand, the features
generated from lower stages are too convoluted and the
saliency maps obtained from the deeper side-output layers
are short of regularity.
To overcome the aforementioned problems, we propose
a top-down method to reasonably combine both low-level
and high-level features for accurate saliency detection. The
following subsections are dedicated to a detailed description of the proposed approach.
HED-based saliency detection
To better understand our proposed approach, we start out
with the standard HED architecture as well as its
extended version, a special case of this work, for salient
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
object detection and gradually move on to our proposed
architecture.
HED architecture
In the HED architecture , 5 side outputs are introduced,
each of which is directly connected to the last convolutional
layer of each stage. Let T = {(Xn, Zn), n = 1, . . . , N}
denote the training data set, where Xn = {x(n)
1, . . . , |Xn|} is the input image and Zn = {z(n)
1, . . . , |Xn|}, z(n)
∈ denotes the corresponding continuous ground truth saliency map for Xn. In the sequel,
we omit the subscript n for notational convenience since we
assume the inputs are all independent of one another. We
denote the collection of all standard network layer parameters as W. Without loss of generality, we further suppose
that there are totally M side outputs. Each side output
is associated with a classiﬁer, in which the corresponding
weights can be represented by w = (w(1), w(2), . . . , w(M)).
Thus, the side objective function of HED can be given by
Lside(W, w) =
where αm is the weight of the mth side loss and l(m)
denotes the image-level class-balanced cross-entropy loss
function for the mth side output. Besides, a weightedfusion layer is added to better capture the advantage of
each side output. The fusion loss at the fusion layer can be
expressed as
Lfuse(W, w, f) = σ
where f = (f1, . . . , fM) is the fusion weights, A(m)
activations of the mth side output, h(·) denotes the sigmoid function, and σ(·, ·) denotes the distance between
the ground truth map and the fused predictions, which is
set to be image-level class-balanced cross-entropy loss .
Therefore, the ﬁnal loss function is given by
 W, w, f) = Lfuse
 W, w, f) + Lside
HED connects each side output to the last convolutional layer in each stage of the VGGNet , respectively
conv1 2, conv2 2, conv3 3, conv4 3, conv5 3. Each side
output is composed of a one-channel convolutional layer
with the kernel size 1×1 followed by an up-sampling layer
for learning edge information.
Enhanced HED architecture
In this part, we extend the HED architecture for salient
object detection. During our experiments, we observe that
deeper layers can better locate the most salient regions, so
based on the architecture of HED we connect another side
output to the last pooling layer (pool5) in VGGNet .
Besides, since salient object detection is a more difﬁcult
task than edge detection, we add two another convolutional
layers with different ﬁlter channels and spatial sizes in
each side output, which can be found in Fig. 4. We use
the same bilinear interpolation operation as in HED for
up-sampling. We also use a standard cross-entropy loss
and compute the loss function over all pixels in a training image X = {xj, j = 1, . . . , |X|} and saliency map
128, 3 × 3
128, 3 × 3
128, 3 × 3
128, 3 × 3
256, 5 × 5
256, 5 × 5
256, 5 × 5
256, 5 × 5
512, 5 × 5
512, 5 × 5
512, 7 × 7
512, 7 × 7
Fig. 4. Details of each side output. (n, k × k) means that the number of
channels and the kernel size are n and k, respectively. “Layer” means
which layer the corresponding side output is connected to. “1” “2” and
“3” represent three convolutional layers that are used in each side
output. Note that the ﬁrst two convolutional layers in each side output
are followed by a ReLU layer for nonlinear transformation.
Z = {zj, j = 1, . . . , |Z|}. Our loss function can be deﬁned
as follows:
side (W, ˆw(m)) = −
 zj = 1|X; W, ˆw(m)
+ (1 −zj) log Pr
 zj = 0|X; W, ˆw(m)
 zj = 1|X; W, ˆw(m)
represents the probability
of the activation value at location j in the mth side output, which can be computed by h(a(m)
), where ˆA(m)
, j = 1, . . . , |X|} are activations of the mth side
output. Similar to , we add a weighted-fusion layer to
connect each side activation. The loss function at the fusion
layer in our case can be represented by
ˆLfuse(W, ˆw, f) = ˆσ
m=1 fm ˆA(m)
where ˆA(m)
side is the new activations of the mth side output1,
M = M +1, and ˆσ(·, ·) represents the distance between the
ground truth map and the new fused predictions, which
has the same form as in Eqn. (4).
A comparison of salient object detection results between
the original HED and enhanced HED is shown in Fig. 7.
It can be easily found that a large margin of about 3%
improvement has been achieved. In spite of such improvement, as shown in Fig. 1, the saliency maps from shallower
side outputs still look messy and the deeper side outputs
produce irregular results as well. In addition, the deeper
side outputs can indeed locate the salient objects, but some
detailed information is still lost.
Short connections
The insight of our approach is that deeper side outputs are
capable of ﬁnding the location of salient regions but at the
expense of the loss of details, while shallower ones focus
on low-level features but are short of global information.
These phenomenons inspire us to utilize the following way
to appropriately combine different side outputs such that
the most visually distinctive objects can be extracted.
1. We add a new side output in our enhanced HED architecture.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Fig. 5. Illustration of short connections in Fig. 3.
Formulation
Mathematically, our new side activations ˜R(m)
side at the mth
side output can be given by
side + ˆA(m)
side , for m = 1, . . . , 5
side , for m = 6
is the weight of short connection from side
output i to side output m (i > m). We can drop out some
short connections by directly setting rm
to 0. The new side
loss function and fusion loss function can be respectively
represented by
˜Lside(W, ˜w, r) =
 W, ˜w(m), r
˜Lfuse(W, ˜w, f, r) = ˆσ
m=1 fm ˜R(m)
where r = {rm
i }, i > m. Note that this time ˜l(m)
side represents
the standard cross-entropy loss which we have deﬁned in
Eqn. (4). Thus, our new ﬁnal loss function can be written as
 W, ˜w, f, r) = ˜Lfuse
 W, ˜w, f, r) + ˜Lside
 W, ˜w, r).
Construction
The backbone of our new architecture is the enhanced HED
which has been described in Section 3.2.2. Fig. 5 illustrates
how to construct short connections from side output 4 to
side output 2. The score maps in side outputs 3 and 4 are
ﬁrst upsampled by simple bilinear interpolation and then
concatenated to the original score map in side output 2. The
hyper-parameters of bilinear interpolation can be derived
according to the context. As salient object detection is a
class-agnostic task, we further weight the foregoing score
maps which have been enclosed by a dashed bounding box
in Fig. 5 and introduce another 1 × 1 convolutional layer
as the new score map of side output 2. A similar approach
can be used for side outputs to which more than one short
connection is connected. For instance, let us assume that
3 short connections are connected to side output 2. There
would be 4 score maps being concatenated together within
the dashed bounding box.
Our architecture can be functionally considered as two
closely connected stages, which we call saliency locating
stage and details reﬁnement stage, respectively. The main
focus of saliency locating stage is on looking for the most
salient regions in a given image. For details reﬁnement
stage, we introduce a top-down method, a series of short
connections from deeper side-output layers to shallower
ones. The reason for such a consideration is that with the
help of deeper side information, lower side outputs can
both accurately predict the salient objects and reﬁne the
results from deeper side outputs, resulting in dense and
accurate saliency maps. We further test the effectiveness of
our proposed architecture by running a number of ablation
experiments and showing the corresponding quantitative
and visual results in the next section.
Implementation Details
Our network is based on the publicly available Caffe library
 and the open implementation of FCN . As mentioned above, we choose VGGNet as our pre-trained
model for better comparison with other works.
Although a series of short connections are introduced, the
quality of the prediction maps produced by the deeper and
the shallower side outputs is still unsatisfactory. Regarding
this fact, during the testing phase, we adopt a more complicated combination of these side outputs. Let ˜Z1, · · · , ˜Z6
denote the score map of each side output, respectively. They
can be computed by ˜Zm = h( ˜R(m)
side ). Recall that h(·) in our
case is the sigmoid function. Therefore, the fusion output
map can be computed by
˜Zfuse = h
To avoid the negative effect caused by the bad quality of
the prediction map from the deepest and shallowest side
outputs, we also use ˜Z2, ˆZ3, and ˆZ4 to help further ﬁll in
the lost details. As a result, the ﬁnal output map during
inference can be represented by
˜Zﬁnal = Mean( ˜Zfuse, ˜Z2, ˜Z3, ˜Z4).
Surprisingly, we found that such a combination do help
improve the results by a little margin. This is due to the
fact that although the fusion output map incorporates the
aggregation of each side output, some detailed information
in the fusion output map is still missed. Regarding the
quality of each side output map (see Fig. 1), we decide to
use Eqn. (11) as the ﬁnal output map.
Smoothing Method
Though our model can precisely ﬁnd the salient objects
in an image, the boundary information of the resulting
saliency maps is still lost for those complex scenes. To further improve spatial coherence and quality of our saliency
maps, we adopt the fully connected conditional random
ﬁeld (CRF) method as a selective layer during the
inference phase.
The energy function of CRF is given by
θij(xi, xj),
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
where x is the label prediction for pixels. To make our
model more competitive, instead of directly using the predicted maps as the input of the unary term, we leverage the
following unary term
θi(xi) = −log ˆSi
where ˆSi denotes normalized saliency value of pixel xi,
h(·) is the sigmoid function, and τ is a scale parameter. The
pairwise potential is deﬁned as
θij(xi, xj) = µ(xi, xj)
−∥pi −pj∥2
−∥pi −pj∥2
where µ(xi, xj) = 1 if xi ̸= xj and zero, otherwise. Ii
and pi are pixel value and position of xi, respectively.
Parameters w1, w2, σα, σβ, and σγ control the importance
of each Gaussian kernel.
In this paper, we employ a publicly available implementation of , called PyDenseCRF 2. Since there are only two
classes in our case, we use the inferred posterior probability
of each pixel being salient as the ﬁnal saliency map directly.
Parameters
The hyper-parameters used in this work include learning
rate (1e-8), weight decay (0.0005), momentum (0.9), loss
weight for each side output (1). We use full-resolution
images to train our network, and the mini-batch size is set
to 10. The kernel weights in newly added convolutional
layers are all initialized with random numbers. Our fusion
layer weights are all initialized with 0.1667 in the training
phase. The parameters in the fully connected CRF are
determined using cross validation on the validation set. In
our experiments, τ is set to 1.05, and w1, w2, σα, σβ, and σγ
are set to 3.0, 3.0, 60.0, 8.0, and 5.0, respectively.
EXPERIMENTS AND ANALYSES
In this section, we introduce utilized datasets and evaluation criteria and report the performance of our proposed
approach. Besides, a number of ablation experiments are
performed for analyzing the importance of each component
of our approach.
We evaluate our approach on 5 representative datasets, including MSRA-B , ECSSD , HKU-IS , PASCALS
 , and SOD , , all of which are available online.
These datasets all contain a large number of images as well
as well-segmented annotations and have been widely used
MSRA-B contains 5,000 images from hundreds of different categories. Because of its diversity and large quantity,
MSRA-B has been one of the most widely used datasets
in salient object detection literature. Most images in this
dataset have only one salient object, and hence it has
gradually become a standard dataset for evaluating the capability of processing simple scenes. ECSSD contains 1,000
2. 
semantically meaningful but structurally complex natural
images. HKU-IS is another large-scale dataset that contains
more than 4000 challenging images. Most of images in this
dataset have low contrast with more than one salient object.
PASCALS contains 850 challenging images (each composed
of several objects), all of which are chosen from the validation set of the PASCAL VOC 2010 segmentation dataset.
We also evaluate our system on the SOD dataset, which is a
subset of the BSDS dataset. It contains 300 images, most of
which possess multiple salient objects. All of these datasets
consist of ground truth human annotations.
In order to preserve the integrity of the evaluation
and obtain a fair comparison with existing approaches, we
utilize the same training and validation sets as in and
test over all of the datasets using the same model.
Evaluation Metrics
We use three universally-agreed, standard metrics (see
also , , , ) to evaluate our model including
precision-recall curves, F-measure, and the mean absolute
error (MAE). For a given continuous saliency map S, we
convert it to a binary mask B using a threshold. Then
its precision and recall are computed as precision =
|B ∩Z|/|B| and recall = |B ∩Z|/|Z|, respectively, where
| · | accumulates the non-zero entries in a mask. Averaging
the precision and recall values over the saliency maps of a
given dataset yields the PR curve.
To comprehensively evaluate the quality of a saliency
map, the F-measure metric is used, which is deﬁned as
Fβ = (1 + β2)Precision × Recall
β2Precision + Recall
As suggested by previous works, we choose β2 to be 0.3 for
stressing the importance of the precision value.
Let ˆS and ˆZ denote the continuous saliency map and
the ground truth that are normalized to . The mean
absolute error (MAE) score can be computed as
| ˆS(i, j) = ˆZ(i, j)|.
Ablation Analysis
We experiment with different design options and different
short connection patterns to illustrate the effectiveness of
each component of our method.
Various Short Connection Patterns
Our architecture as shown in Fig. 3 is so ﬂexible that can
be regarded as the generalized model of most existing
architectures, such as those depicted in Fig. 2. To better
show the strength of our proposed approach, we use different network architectures as listed in Fig. 2 for salient
object detection. Besides the Hypercolumns architecture
 and the HED-based architecture , we implement
three representative patterns using our proposed approach.
The ﬁrst one is formulated as follows, which is a similar
architecture to Fig. 2(d).
m+1 ˜R(m+1)
side , for m = 1, . . . , 5
side . for m = 6
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Side output 1
Side output 2
Side output 3
Side output 4
Side output 5
Side output 6
(128, 3 × 3) × 2
(128, 3 × 3) × 2
(256, 5 × 5) × 2
(512, 5 × 5) × 2
(1024, 5 × 5) × 2
(1024, 7 × 7) × 2
(128, 3 × 3) × 1
(128, 3 × 3) × 1
(256, 5 × 5) × 1
(256, 5 × 5) × 1
(512, 5 × 5) × 1
(512, 7 × 7) × 1
(128, 3 × 3) × 2
(128, 3 × 3) × 2
(256, 3 × 3) × 2
(256, 3 × 3) × 2
(512, 5 × 5) × 2
(512, 5 × 5) × 2
(128, 3 × 3) × 2
(128, 3 × 3) × 2
(256, 5 × 5) × 2
(256, 5 × 5) × 2
(512, 5 × 5) × 2
(512, 7 × 7) × 2
Fig. 6. Comparisons of different side output settings and their performance on PASCALS dataset . (c, k × k) × n means that there are n
convolutional layers with c channels and size k × k. Note that the last convolutional layer in each side output is unchanged as listed in Fig. 4. In
each setting, we only modify one parameter while keeping all others unchanged so as to emphasize the importance of each chosen parameter.
The second pattern is represented as follows which is much
more complex than the ﬁrst one.
side + ˆA(m)
side , for m = 1, 2, 3, 4
side . for m = 5, 6
The last pattern, the one used in this paper, is given by
side + ˆA(m)
side , for m = 1, 2
side + ˆA(m)
side , for m = 3, 4
side . for m = 5, 6
The quantitative results are listed in Fig. 7. As can be seen
from Fig. 7, by adding another side output and two additional convolutional layers in each side output, we have
a performance gain of 2.5 points in terms of F-measure.
In addition, with the increase of short connections, our
approach gradually achieves better performance. Although
there is no performance gain obtained when Pattern 1 is
used compared with the enhanced HED structure, a gain
of 0.8 points can be achieved when we turn to Pattern 2.
Another 0.6 points gain can also be obtained when Pattern
3 is considered.
Details of Side-Output Layers
We run several ablation experiments to explore the best
side output settings. The detailed information of each
side-output layer in each experiment has been shown in
Fig. 6. We use Pattern 3 in Fig. 7 as our baseline model.
To highlight the importance of different parameters, we
adopt the variable-controlling method that only changes
one parameter at a time. Besides, all the results are tested on
PASCALS dataset for fair comparison. Compared with the
fourth experiment, the ﬁrst one exploits more channels but
the same F-measure score is obtained. This means that more
channels for each side output cannot bring in additional
performance gain. In the second experiment, we tried to
reduce 1 convolutional layer in each side output but it turns
out that such an operation decreases the performance by 1.5
points. In spite of a small decrease, it is enough to account
for the importance of introducing two convolutional layers
in each side output. Furthermore, we attempt to reduce
the large kernel size in deeper side outputs. Similarly,
this leads to a slight decrease in F-measure. All the above
experiments demonstrate that the side output settings we
use are reasonable and appropriate.
Upsampling Operation
In our approach, we use the in-network bilinear interpolation to perform upsampling in each side output. As
implemented in , we use ﬁxed deconvolutional kernels
Architecture
Hypercolumns 
Original HED 
Enhanced HED
Pattern 1 (Eqn. (17))
Pattern 2 (Eqn. (18))
Pattern 3∗(Eqn. (19))
Fig. 7. The performance of different architectures on PASCALS dataset
 . ’*’ represents the pattern used in this paper.
for our side outputs with different strides. Since the prediction maps generated by deep side-output layers are not
dense enough, we also try to use the “hole algorithm” to
make the prediction maps in deep side outputs denser. We
adopt the same technique as in . However, according
to our experiments, using such a method yields a worse
performance. We notice that as the fusion prediction map
gets denser, some non-salient pixels are wrongly predicted
as salient ones even though the CRF is used thereafter. The
F-measure score on the validation set is decreased by nearly
Data Augmentation
Data augmentation has been proven to be very useful in
many learning-based vision tasks. As done in most previous works, we ﬂip all the training images horizontally,
resulting in an augmented image set with twice larger than
the original one. We found that such an operation further
improves the performance by more than 0.5%. In addition,
we also try to crop the input images to a ﬁxed size 321×321.
However, experimental results show that such an operation
decrease our performance by more than 0.5 points. This
may be because input images with full size contain richer
information that allows our network to better capture the
salient objects.
Different Backbones
We also extend our work by replacing the VGGNet with
ResNet-101 as the backbone. Taking into account the
network structure of ResNet-101, we only use the bottom 5
side outputs in Fig. 4, which are connected to conv1, res2c,
res3b3, res4b22, and res5c, respectively. We keep other
settings unchanged. We show the results on the bottom of
Fig. 10. With the same training set, there is a further onepoint improvement on each dataset in terms of F-measure
score on average.
The Proposed CRF Model
Most previous works , only use the negative log
likelihood as the unary term in their CRF model. Differently
from them, we introduce a modulating factor that aims
to give positive predictions more conﬁdence as shown in
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Eqn. (13). This is reasonable as most of the predictions
are correct through observing the MAE scores. In our
experiments, we found that adding such a modulating
factor helps little on improving the F-measure scores but is
able to further reduce the MAE scores (i.e. , reduce wrong
predictions) by around 0.3 points.
Comparison with the State-of-the-art
We compare the proposed approach with 7 recent CNNbased methods, including MDF , DS , DCL ,
ELD , MC , RFCN , and DHS . Four classical
methods are also considered including RC , CHM ,
DSR , and DRFI , which have been proven to be
the best in the benchmark study of Borji et al.
 . It is
worth mentioning that though more training images is able
to bring us better results as shown in Fig. 14, our results
here are mainly based on 2500 training images from MSRA-
B dataset for fair comparison with existing works.
Visual Comparison
To exhibit the superiority of our proposed approach compared against the above-mentioned methods, we select
multiple representative images from different datasets
which incorporate a variety of difﬁcult circumstances, including complex scenes, salient objects with center bias,
salient objects with different sizes, low contrast between
foreground and background, etc., and show the visual comparisons in Fig. 8. We manually split the selected images
into multiple groups which are separated by solid lines.
We also give each group multiple tags describing their
properties.
Taking all circumstances into account, it can be easily
seen that our proposed method not only highlights the right
salient regions but also produces coherent boundaries. It
is also worth mentioning that thanks to the short connections, our approach gives salient regions more conﬁdence,
yielding higher contrast between salient objects and the
background. More importantly, it generates connected regions, which greatly strengthens the ability of our model.
These advantages make our results very close to the ground
truth and hence better than other methods in almost all
circumstances which are shown in Fig. 8.
We compare our approach with the existing methods in
terms of PR curve here. In Fig. 9, we depict the PR curves
produced by our approach and previous state-of-the-art
methods on 3 popular datasets. It is obvious that FCNbased methods substantially outperform other methods.
More importantly, among all FCN-based methods, the PR
curve of our approach is especially outstanding in the
upper left corners of the coordinates. We can also ﬁnd that
the precision of our approach is much higher when the
recall score is close to 1, reﬂecting that our false positives
are much lower than other methods. This also indicates
that our strategy of combining low-level and high-level
features in terms of short connections is essential such that
the resultant saliency maps look much closer to the ground
F-measure and MAE
We also compare our approach with the existing methods
in terms of F-meature and MAE scores. The quantitative
results are shown in Fig. 10. As can be seen, our approach
achieves the best score (maximum F-measure and MAE)
on all datasets as listed in Fig. 10. On the ECSSD and SOD
datasets, our approach improves the current best maximum
F-measure by 1 point, which is a large margin as the
values are already very close to ideal value 1. In regard
to MAE scores, our approach achieves a more than 1-point
decrease on MSRA-B and PASCALS datasets. On the other
datasets, there are still at least 0.09 points improvements.
This implies that the number of wrong predictions in our
case is signiﬁcantly less than the other methods.
Besides, we also observe that the proposed approach
behaves even better on more difﬁcult datasets, such as
HKUIS , PASCALS , and SOD , , which contain a large number of images with multiple salient objects.
This indicates that our method is capable of detecting and
segmenting the most salient object, while other methods
often fail at one of these stages.
The Existence of Saliency
To date, most existing salient object detection methods
focus on datasets in which at least one salient object exists.
However, in many real-world scenarios, salient objects do
not always exists. Therefore, methods based on the above
assumption may easily lead to incorrect prediction results
when applied to scenes without any salient objects in them.
To solve this problem, we propose to introduce another
branch into our network to predict the saliency existence of
the input image. The new branch is composed of a global
average pooling layer, followed by a multi-layer perceptron
(MLP) as the regressor to recognize the existence of saliency
as done in many classiﬁcation networks , . The
global average pooling layer is used to transform feature
maps with different shapes into the same size so that the
resulting feature vectors can be fed into the MLP. Like ,
 , the MLP here consists of three fully-connected layers,
all of which are with 1,024 neurons except the last one
which has two. The softmax loss is used to optimize the
new branch.
In our experiments, we use the same training set as in
 , which contains 5,000 background images (i.e. images
without salient objects in them) and 5,000 images from
MSRA10K . For these background images, the gradients
from the salient object detection module are not allowed
to back-propagate so that the resulting prediction maps
would not be interfered. We found that this operation is
essential. The hyper-parameters used here are the same
to our salient object detection experiments. We train our
network for 24,000 iterations and decrease the learning
rate by a factor of 10 at 20,000 iterations. We test our
model on three datasets, including JSOD , MSRA-B
 and ECSSD . Fig. 11 lists the results compared to
another two works SSVM and Wang et al. . Since
there is a clear separation between JSOD dataset (mostly
containing pure textures) and other two datasets (MSRA-
B and ECSSD mostly contain images with clear salient
objects), the classiﬁcation results on all datasets have been
already saturated (very close to the ideal value “1”). Thus,
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Simple Scene | Complex Scene | Center Bias
Complex Scene | Small Object | Low Contrast
Low Contrast | Complex Texture
Large Object | Low Contrast
Multiple Object | Large Object | Complex Scene
Multiple Object | Transparent Object
Fig. 8. Selected results from various datasets. We split the selected images into multiple groups, which are separated by solid lines. To better
show the capability of processing different scenes for each approach, we highlight the features of images in each group.
we expect more challenging dataset which better reﬂect real
world difﬁculties would be developed in near future.
Our network is fully convolutional, which allows it to run
very fast compared against most previous salient object
detection methods. When trained on the MSRA-B dataset
which contains 2,500 training images, our network takes
less than 8 hours for 12,000 iterations. Interestingly, though
10,000 iterations are enough for convergence, we found
another 2,000 iterations still bring us a small performance
gain in MAE.
During the inference stage, it takes us about 0.08s to
process an input image of size 300 × 400. This is extremely
faster than most of the previous works, such as DCL 
which need more than 1s for each image of the same size.
With our CRF layer considered, another 0.4 seconds are
needed. As a result, our overall time cost is less than 0.5s
for an image of size 300 × 400.
DISCUSSION
In this section, we conduct useful analysis on our proposed approach, which we believe would be helpful for
researchers to develop more powerful methods.
Failure Case Analysis
Some failure predictions of our approach have been shown
in Fig. 12. As can be seen, these failure cases can be categorized into three circumstances in general. The ﬁrst one is
actually the common defect of CNN-based salient object detection methods, in which the salient objects cannot be completely segmented out, leaving a small part of the salient
object missed. Typical examples are the images shown in
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
(1) MSRA10K
Fig. 9. Precision (vertical axis) recall (horizontal axis) curves on three popular salient object datasets.
MSRA-B 
ECSSD 
HKU-IS 
PASCALS 
Fig. 10. Quantitative comparisons with 11 methods on 5 popular datasets. The ResNet-101 version of our approach (i.e. ‘Ours†’) clearly
outperforms its VGGNet version. For fair comparison, we exclude ‘Ours†’ and highlight the best result of each column in bold. Here we use the
initials of each dataset for convenience.
MSRA-B 
ECSSD 
Wang et al. 
Fig. 11. The prediction accuracy of our saliency existence branch
compared to SSVM and Wang et al. . The best result of each
column is highlighted in bold.
Fig. 12. Failure cases selected from multiple datasets. As can be seen,
most cases are caused by complex background, low contrast between
foreground and background, and transparent objects.
the ﬁrst row of Fig. 12. In the second circumstance, the
main body of the salient object cannot be extracted or nonsalient regions are predicted to be salient. As shown in
the middle row of Fig. 12, this case is mostly caused by
complex backgrounds and very low contrast. The last type
of failure cases is caused by transparent objects as shown
in the bottom row of Fig. 12. Though our approach can
detect some parts of the transparent objects, to segment the
complete objects out is still very difﬁcult.
We argue that three possible remedies can be used to
solve the aforementioned problems. First of all, a promising
solution is to provide more prior knowledge on segment
level so that regions with similar textures or colors can be
detected simultaneously. Because of the internal structure
of CNNs, the correlations of two positions in the score map
are decided by the learnable weights of the former layers,
making this problem difﬁcult to be solved by the networks themselves. Segment-level information allows CNNs
to correct those wrong predictions in the Circumstance 1
mentioned above. In addition, segment-level information
can also serve as a post-processing tool to further reﬁne
the predicted saliency maps by a simple voting strategy.
Secondly, more powerful training data should be presented,
including both simple and complex scenes. As listed in
Fig. 14, training data with complex scenes can substantially help improve the performance on both easy and
difﬁcult datasets. Another solution should be designing
more advanced models and then extracting more powerful
feature representations to deal with challenging inputs with
complex structures .
Benchmarking Training Set
The selection of training set is one of the important aspects
for a learning based algorithm. A good training set will definitely improve the learning ability, leading to a more gen-
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Training Set
MSRA-B (2500)
ECSSD (1000)
HKU-IS (2500)
DUT-OMRON (3103)
MSRA10K (6000)
Fig. 13. Performance when different training sets are used. The best results are highlighted in bold. Notice that all the results here are without
erative model that can perform well on almost all scenes,
even with complex background. However, the training sets
of recent learning based approaches are different and none
of these works have explored which training set is the best.
Fig. 10 lists the details of different training sets that existing
approaches have used. Furthermore, training on different
datasets with different sizes makes the comparisons unfair.
Albeit the number of training images is not proportional
to the performance gain, the size and quality of different
training sets break the fair comparisons among different
approaches. One can observe in Fig. 10 that some of them
only use a training set with 2,500 images while some others
leverage around 10,000 images for training.
In this section, we attempt to thoroughly analyze the effect of utilizing different datasets for training based on our
proposed approach. Our goal is to provide a new, uniﬁed,
convincing, and large-scale training set based on existing
datasets for future research. To do so, we perform a number
of experiments and show exhaustive comparisons among
6 widely-used and publicly available datasets, which can
be found in Fig. 13. Notice that all the training lists will
be made publicly available. During testing phase, we use
both the max F-measure score and MAE score as measuring
metrics. Notice that since most datasets contain more than
5,000 images, each model is trained for 16,000 iterations
here. An exception is the model trained on ECSSD with
6,000 iterations.
Dataset Quality Measuring
To exhibit the quality of datasets better, each time we train
on one of them, except for the SOD dataset which has
only 300 images and the PASCALS dataset which has a
lowly consistent behavior, and test on all the test sets. As
ECSSD contains less than 2,000 images, all the images are
used for training and hence no image is left for testing.
For the remaining large-scale datasets, if default splits are
provided then they will be used directly. Otherwise, we
split the dataset in a ratio of 6:1:3 for training, validation,
and testing, respectively.
Detailed experimental results have been shown in
Fig. 13. As there is a large overlap between MSRA-B and
MSRA10K datasets, we only show the results on MSRA-B
instead of both. According to the results shown in Fig. 13,
the following conclusion can be drawn. First, the best
result on each dataset is always obtained by training on
the corresponding training set, and the phenomenon is
especially obvious for DUT-OMRON. This might be caused
by the characteristics of the images in each dataset, making
different datasets favor different features. Consequently,
we argue that it is inappropriate to directly compare performance numbers that are achieved by different models
trained on different datasets (see also Fig. 13). Second, having more training images does not necessarily entail better
performance. As can be seen in Fig. 13, training on ECSSD
dataset allows us to achieve the best performance on the
SOD dataset despite of having only 1,000 training images.
In regard to the above-mentioned issues, a compromise
solution is to construct a uniﬁed, composite, and versatile
Beyond Training on Individual Datasets
We select 4 datasets from Fig. 13 to build composite datasets
for comparisons. Though the MSRA10K is more than twice
bigger than MSRA-B dataset, models trained on it have a
competitive performance compared to those trained on the
MSRA-B dataset. Here we just keep MSRA-B for training
due to its high-quality images and annotations. Therefore,
there are totally 11 different combinations which have been
shown in the second column of Fig. 14. During the testing
phase, we also use the six test sets mentioned above for fair
comparisons.
From the results in Fig. 14, the following conclusions can
be drawn. First of all, a larger training set does not necessarily mean higher test performance. This phenomenon can be
observed through comparing Scheme 3 with other schemes.
Despite only 3,500 training images, this combination performs better than those with more than 6,000 training
images. It is true that the quality of annotations might be
an essential reason that causes such a problem. However,
such a consideration is beyond the scope of this paper. All
conclusions here are based on the assumption that each
dataset we use is with well-segmented annotations.
Second, an inappropriate combination of datasets may
result in worse performance compared with individual
datasets. By comparing schemes 4 and 0, one can ﬁnd
that despite better performance on HKU-IS, PASCALS, and
SOD datasets there are still slight decreases when testing
on MSRA-B and DUT-OMRON datasets.
Through this series of experiments, we aimed to emphasis that a training set with a large quantity of images may
not be capable of bringing in better performance gain. A
good training set should take into account as many cases
as possible. However, because of the diversity of existing
datasets, it is hard to obtain a convincing dataset that
can behave the consistency among all existing datasets. In
regard to the current state in salient object detection, we
recommend using our Scheme 11 in Fig. 14 as training set
for fair comparison and ﬁtting decreasing performance bias
caused by different training sets. Another severe problem
in salient object detection is that most datasets are no
longer challenging. An explicit effect is that the differences
between different models are difﬁcult to be distinguished
because of the close performance on existing datasets. We
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Training Set
D + E (4103)
H + E (3500)
D + H (5603)
MB + E (3500)
MB + H (5000)
MB + D (5603)
MB + E + D (6603)
MB + H + D (8103)
MB + E + H (6000)
E + D + H (6603)
MB + E + D + H (9103)
Fig. 14. Detailed information of different training sets and the corresponding results on 5 datasets. The best results are highlighted in bold. All the
results are obtained without any post-processing. Here we use the initials of each dataset for convenience.
hope that more challenging datasets with complex scenes
and high consistency would be presented in the near future.
CONCLUSION
In this paper, we presented a deeply supervised network
for salient object detection. Instead of directly connecting
loss layers to the last layer of each stage, we introduce a
series of short connections between shallower and deeper
side-output layers. With these short connections, the activation of each side-output layer gains the capability of both
highlighting the entire salient object and accurately locating
its boundary. A fully connected CRF is also employed
for correcting wrong predictions and further improving
spatial coherence. Our experiments demonstrate that these
mechanisms result in more accurate saliency maps over a
variety of images. Our approach signiﬁcantly advances the
state-of-the-art and is capable of capturing salient regions
in both simple and difﬁcult cases, which further veriﬁes the
merit of the proposed architecture.
ACKNOWLEDGMENTS
This research was supported by NSFC (NO. 61620106008,
61572264), Huawei Innovation Research Program, CAST
YESS Program, and IBM Global SUR award.