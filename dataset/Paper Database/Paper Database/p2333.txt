Self-critical Sequence Training for Image Captioning
Steven J. Rennie1, Etienne Marcheret1, Youssef Mroueh, Jerret Ross and Vaibhava Goel1
Watson Multimodal Algorithms and Engines Group
IBM T.J. Watson Research Center, NY, USA
 , {etiennemarcheret, vaibhavagoel}@gmail.com, {mroueh, rossja}@us.ibm.com
Recently it has been shown that policy-gradient methods
for reinforcement learning can be utilized to train deep endto-end systems directly on non-differentiable metrics for the
task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, signiﬁcant gains
in performance can be realized. Our systems are built using
a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular RE-
INFORCE algorithm that, rather than estimating a “baseline” to normalize the rewards and reduce variance, utilizes
the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do)
and estimating normalization (as REINFORCE algorithms
typically do) is avoided, while at the same time harmonizing
the model with respect to its test-time inference procedure.
Empirically we ﬁnd that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly
effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best
result in terms of CIDEr from 104.9 to 114.7.
1. Introduction
Image captioning aims at generating a natural language
description of an image. Open domain captioning is a very
challenging task, as it requires a ﬁne-grained understanding of the global and the local entities in an image, as
well as their attributes and relationships. The recently released MSCOCO challenge provides a new, larger scale
platform for evaluating image captioning systems, complete with an evaluation server for benchmarking competing methods. Deep learning approaches to sequence model-
1Authors Steven J. Rennie, Etienne Marcheret, and Vaibhava Goel
were at IBM while the work was being completed.
ing have yielded impressive results on the task, dominating the task leaderboard. Inspired by the recently introduced encoder/decoder paradigm for machine translation
using recurrent neural networks (RNNs) , , and 
use a deep convolutional neural network (CNN) to encode
the input image, and a Long Short Term Memory (LSTM)
 RNN decoder to generate the output caption.
systems are trained end-to-end using back-propagation, and
have achieved state-of-the-art results on MSCOCO. More
recently in , the use of spatial attention mechanisms on
CNN layers to incorporate visual context—which implicitly
conditions on the text generated so far—was incorporated
into the generation process. It has been shown and we have
qualitatively observed that captioning systems that utilize
attention mechanisms lead to better generalization, as these
models can compose novel text descriptions based on the
recognition of the global and local entities that comprise
As discussed in , deep generative models for text are
typically trained to maximize the likelihood of the next
ground-truth word given the previous ground-truth word
using back-propagation.
This approach has been called
“Teacher-Forcing” . However, this approach creates a
mismatch between training and testing, since at test-time
the model uses the previously generated words from the
model distribution to predict the next word. This exposure
bias , results in error accumulation during generation at
test time, since the model has never been exposed to its own
predictions.
Several approaches to overcoming the exposure bias
problem described above have recently been proposed. In
 they show that feeding back the model’s own predictions
and slowly increasing the feedback probability p during
training leads to signiﬁcantly better test-time performance.
Another line of work proposes “Professor-Forcing” , a
technique that uses adversarial training to encourage the dynamics of the recurrent network to be the same when training conditioned on ground truth previous words and when
sampling freely from the network.
While sequence models are usually trained using the
 
cross entropy loss, they are typically evaluated at test time
using discrete and non-differentiable NLP metrics such as
BLEU , ROUGE , METEOR or CIDEr .
Ideally sequence models for image captioning should be
trained to avoid exposure bias and directly optimize metrics for the task at hand.
Recently it has been shown that both the exposure bias
and non-differentiable task metric issues can be addressed
by incorporating techniques from Reinforcement Learning (RL) .
Speciﬁcally in , Ranzato et al.
the REINFORCE algorithm to directly optimize nondifferentiable, sequence-based test metrics, and overcome
both issues. REINFORCE as we will describe, allows one
to optimize the gradient of the expected reward by sampling
from the model during training, and treating those samples
as ground-truth labels (that are re-weighted by the reward
they deliver). The major limitation of the approach is that
the expected gradient computed using mini-batches under
REINFORCE typically exhibit high variance, and without
proper context-dependent normalization, is typically unstable. The recent discovery that REINFORCE with proper
bias correction using learned “baselines” is effective has
led to a ﬂurry of work in applying REINFORCE to problems in RL, supervised learning, and variational inference
 . Actor-critic methods , which instead train
a second “critic” network to provide an estimate of the value
of each generated word given the policy of an actor network, have also been investigated for sequence problems
recently . These techniques overcome the need to sample from the policy’s (actor’s) action space, which can be
enormous, at the expense of estimating future rewards, and
training multiple networks based on one another’s outputs,
which as explore, can also be unstable.
In this paper we present a new approach to sequence training which we call self-critical sequence training
(SCST), and demonstrate that SCST can improve the performance of image captioning systems dramatically. SCST
is a REINFORCE algorithm that, rather than estimating the
reward signal, or how the reward signal should be normalized, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. As a result,
only samples from the model that outperform the current
test-time system are given positive weight, and inferior samples are suppressed. Using SCST, attempting to estimate
the reward signal, as actor-critic methods must do, and estimating normalization, as REINFORCE algorithms must do,
is avoided, while at the same time harmonizing the model
with respect to its test-time inference procedure. Empirically we ﬁnd that directly optimizing the CIDEr metric with
SCST and greedy decoding at test-time is highly effective.
Our results on the MSCOCO evaluation sever establish a
new state-of-the-art on the task, improving the best result in
terms of CIDEr from 104.9 to 114.7.
2. Captioning Models
In this section we describe the recurrent models that we
use for caption generation.
FC models. Similarly to , we ﬁrst encode the input
image F using a deep CNN, and then embed it through a
linear projection WI . Words are represented with one hot
vectors that are embedded with a linear embedding E that
has the same output dimension as WI. The beginning of
each sentence is marked with a special BOS token, and the
end with an EOS token. Under the model, words are generated and then fed back into the LSTM, with the image
treated as the ﬁrst word WICNN(F). The following updates for the hidden units and cells of an LSTM deﬁne the
model :
xt = E1wt−1 for t > 1, x1 = WICNN(F)
it = σ (Wixxt + Wihht−1 + bi)
(Input Gate)
ft = σ (Wfxxt + Wfhht−1 + bf) (Forget Gate)
ot = σ (Woxxt + Wohht−1 + bo) (Output Gate)
ct = it ⊙φ(W ⊗
zxxt + W ⊗
zhht−1 + b⊗
z ) + ft ⊙ct−1
ht = ot ⊙tanh(ct)
st = Wsht,
where φ is a maxout non-linearity with 2 units (⊗denotes
the units) and σ is the sigmoid function. We initialize h0
and c0 to zero. The LSTM outputs a distribution over the
next word wt using the softmax function:
wt ∼softmax (st)
In our architecture, the hidden states and word and image
embeddings have dimension 512.
Let θ denote the parameters of the model. Traditionally the parameters θ are
learned by maximizing the likelihood of the observed sequence. Speciﬁcally, given a target ground truth sequence
1, . . . , w∗
T ), the objective is to minimize the cross entropy loss (XE):
1, . . . w∗
where pθ(wt|w1, . . . wt−1) is given by the parametric
model in Equation (1).
Attention Model (Att2in). Rather than utilizing a static,
spatially pooled representation of the image, attention models dynamically re-weight the input spatial (CNN) features
to focus on speciﬁc regions of the image at each time step.
In this paper we consider a modiﬁcation of the architecture
of the attention model for captioning given in , and input
the attention-derived image feature only to the cell node of
xt = E1wt−1 for t ≥1 w0 = BOS
it = σ (Wixxt + Wihht−1 + bi)
(Input Gate)
ft = σ (Wfxxt + Wfhht−1 + bf) (Forget Gate)
ot = σ (Woxxt + Wohht−1 + bo) (Output Gate)
ct = it ⊙φ(W ⊗
zxxt + W ⊗
zIIt + W ⊗
zhht−1 + b⊗
z ) + ft ⊙ct−1
ht = ot ⊙tanh(ct)
st = Wsht,
where It is the attention-derived image feature. This feature is derived as in as follows: given CNN features at
N locations {I1, . . . IN}, It = PN
tIi, where αt =
softmax(at + bα), and ai
t = W tanh(WaIIi + Wahht−1 +
ba). In this work we set the dimension of W to 1×512, and
set c0 and h0 to zero. Let θ denote the parameters of the
model. Then pθ(wt|w1, . . . wt−1) is again deﬁned by (1).
The parameters θ of attention models are also traditionally
learned by optimizing the XE loss (2).
Attention Model (Att2all). The standard attention model
presented in also feeds then attention signal It as an input into all gates of the LSTM, and the output posterior.
In our experiments feeding It to all gates in addition to
the input did not boost performance, but feeding It to both
the gates and the outputs resulted in signiﬁcant gains when
ADAM was used.
3. Reinforcement Learning
Sequence Generation as an RL problem. As described
in the previous section, captioning systems are traditionally
trained using the cross entropy loss. To directly optimize
NLP metrics and address the exposure bias issue, we can
cast our generative models in the Reinforcement Learning
terminology as in . Our recurrent models (LSTMs) introduced above can be viewed as an “agent” that interacts with
an external “environment” (words and image features). The
parameters of the network, θ, deﬁne a policy pθ, that results in an “action” that is the prediction of the next word.
After each action, the agent (the LSTM) updates its internal “state” (cells and hidden states of the LSTM, attention
weights etc). Upon generating the end-of-sequence (EOS)
token, the agent observes a “reward” that is, for instance,
the CIDEr score of the generated sentence—we denote this
reward by r. The reward is computed by an evaluation metric by comparing the generated sequence to corresponding
ground-truth sequences. The goal of training is to minimize
the negative expected reward:
L(θ) = −Ews∼pθ [r(ws)] ,
where ws = (ws
1, . . . ws
T ) and ws
t is the word sampled from
the model at the time step t. In practice L(θ) is typically
estimated with a single sample from pθ:
L(θ) ≈−r(ws), ws ∼pθ.
Policy Gradient with REINFORCE. In order to compute
the gradient ∇θL(θ), we use the REINFORCE algorithm
 (See also Chapter 13 in ). REINFORCE is based
on the observation that the expected gradient of a nondifferentiable reward function can be computed as follows:
∇θL(θ) = −Ews∼pθ [r(ws)∇θ log pθ(ws)] .
In practice the expected gradient can be approximated using
a single Monte-Carlo sample ws = (ws
1 . . . ws
T ) from pθ,
for each training example in the minibatch:
∇θL(θ) ≈−r(ws)∇θ log pθ(ws).
REINFORCE with a Baseline. The policy gradient given
by REINFORCE can be generalized to compute the reward
associated with an action value relative to a reference reward or baseline b:
∇θL(θ) = −Ews∼pθ [(r(ws) −b)∇θ log pθ(ws)] .
The baseline can be an arbitrary function, as long as it does
not depend on the “action” ws , since in this case:
Ews∼pθ [b∇θ log pθ(ws)] = b
= b∇θ1 = 0.
This shows that the baseline does not change the expected
gradient, but importantly, it can reduce the variance of the
gradient estimate. For each training case, we again approximate the expected gradient with a single sample ws ∼pθ:
∇θL(θ) ≈−(r(ws) −b)∇θ log pθ(ws).
Note that if b is function of θ or t as in , equation (6) still
holds and b(θ) is a valid baseline.
Final Gradient Expression. Using the chain rule, and the
parametric model of pθ given in Section 2 we have:
where st is the input to the softmax function. Using RE-
INFORCE with a baseline b the estimate of the gradient of
is given by :
≈(r(ws) −b)(pθ(wt|ht) −1ws
4. Self-critical sequence training (SCST)
The central idea of the self-critical sequence training
(SCST) approach is to baseline the REINFORCE algorithm
with the reward obtained by the current model under the
inference algorithm used at test time. The gradient of the
negative reward of a sample ws from the model w.r.t. to the
softmax activations at time-step t then becomes:
= (r(ws) −r( ˆw))(pθ(wt|ht) −1ws
where r( ˆw) again is the reward obtained by the current
model under the inference algorithm used at test time. Accordingly, samples from the model that return higher reward
than ˆw will be “pushed up”, or increased in probability,
while samples which result in lower reward will be suppressed. Like MIXER , SCST has all the advantages of
REINFORCE algorithms, as it directly optimizes the true,
sequence-level, evaluation metric, but avoids the usual scenario of having to learn a (context-dependent) estimate of
expected future rewards as a baseline. In practice we have
found that SCST has much lower variance, and can be more
effectively trained on mini-batches of samples using SGD.
Since the SCST baseline is based on the test-time estimate
under the current model, SCST is forced to improve the performance of the model under the inference algorithm used
at test time. This encourages training/test time consistency
like the maximum likelihood-based approaches “Data as
Demonstrator” , “Professor Forcing” , and E2E ,
but importantly, it can directly optimize sequence metrics.
Finally, SCST is self-critical, and so avoids all the inherent training difﬁculties associated with actor-critic methods,
where a second “critic” network must be trained to estimate
value functions, and the actor must be trained on estimated
value functions rather than actual rewards.
In this paper we focus on scenario of greedy decoding,
ˆwt = arg max
wt p(wt | ht)
This choice, depicted in Figure 1, minimizes the impact of
baselining with the test-time inference algorithm on training
time, since it requires only one additional forward pass, and
trains the system to be optimized for fast, greedy decoding
at test-time.
Generalizations.
The basic SCST approach described
above can be generalized in several ways.
One generalization is to condition the baseline on what
has been generated (i.e. sampled) so far, which makes the
baseline word-dependent, and further reduces the variance
of the reward signal by making it dependent only on future rewards. This is achieved by baselining the reward for
t at timestep t with the reward obtained by the word
sequence ¯w = {ws
1:t−1, ˆwt:T }, which is generated by sampling tokens for timesteps 1 : t −1, and then executing
the inference algorithm to complete the sequence. The resulting reward signal, r(ws) −r( ¯w), is a baselined future
reward (advantage) signal that conditions on both the input
image and the sequence ws
1:t−1, and remains unbiased. We
call this variant time-dependent SCST (TD-SCST).
Another important generalization is to utilize the inference algorithm as a critic to replace the learned critic of traditional actor-critic approaches. Like for traditional actorcritic methods, this biases the learning procedure, but can be
used to trade off variance for bias. Speciﬁcally, the primary
reward signal at time t can be based on a sequence that samples only n future tokens, and then executes the inference
algorithm to complete the sequence. The primary reward is
then based on ˜w = {ws
1:t+n, ˆwt+n+1:T }, and can further be
baselined in a time-dependent manner using TD-SCST. The
resulting reward signal in this case is r( ˜w) −r( ¯w). We call
this variant True SCST.
We have experimented with both TD-SCST and “True”
SCST as described above on the MSCOCO task, but found
that they did not lead to signiﬁcant additional gain. We have
also experimented with learning a control-variate for the
SCST baseline on MSCOCO to no avail. Nevertheless, we
anticipate that these generalizations will be important for
other sequence modeling tasks, and policy-gradient-based
RL more generally.
5. Experiments
We evaluate our proposed method on the
MSCOCO dataset . For ofﬂine evaluation purposes we
used the data splits from . The training set contains
113, 287 images, along with 5 captions each. We use a set
of 5K image for validation and report results on a test set of
5K images as well, as given in . We report four widely
used automatic evaluation metrics, BLEU-4, ROUGEL,
METEOR, and CIDEr. We prune the vocabulary and drop
any word that has count less then ﬁve, we end up with a
vocabulary of size 10096 words.
Image Features 1) FC Models.
We use two type of
Features: a) (FC-2k) features, where we encode each image
with Resnet-101 (101 layers) . Note that we do not
rescale or crop each image.
Instead we encode the full
image with the ﬁnal convolutional layer of resnet, and apply
average pooling, which results in a vector of dimension
2048. b) (FC-15K) features where we stack average pooled
13 layers of Resnet-101 . These
13 layers are the odd layers of conv4 and conv5, with the
exception of the 23rd layer of conv4, which was omitted.
This results in a feature vector of dimension 15360.
2) Spatial CNN features for Attention models: (Att2in)
We encode each image using the residual convolutional
neural network Resnet-101 .
Note that we do not
rescale or crop the image.
Instead we encode the full
1, . . . , ws
T ) −r( ˆw1, . . . , ˆwT )) r✓log p✓(ws
1, . . . , ws
1, . . . , ws
1, . . . , ws
h1, c1 = LSTM(BOS, h0, c0)
h2, c2 = LSTM(ws
1, h1, c1)
1, . . . , w⇤
( ˆw1, . . . , ˆwT )
r( ˆw1, . . . , ˆwT )
1 = LSTM(BOS, h0
2 = LSTM( ˆw1, h0
1, . . . , w⇤
Figure 1: Self-critical sequence training (SCST). The weight put on words of a sampled sentence from the model is determined by the difference between the reward for the sampled sentence and the reward obtained by the estimated sentence
under the test-time inference procedure (greedy inference depicted). This harmonizes learning with the inference procedure,
and lowers the variance of the gradients, improving the training procedure.
image with the ﬁnal convolutional layer of Resnet-101,
and apply spatially adaptive max-pooling so that the output
has a ﬁxed size of 14 × 14 × 2048.
At each time step
the attention model produces an attention mask over the
196 spatial locations. This mask is applied and then the
result is spatially averaged to produce a 2048 dimension
representation of the attended portion of the image.
Implementation Details. The LSTM hidden, image, word
and attention embeddings dimension are ﬁxed to 512 for
all of the models discussed herein. All of our models are
trained according to the following recipe, except where otherwise noted. We initialize all models by training the model
under the XE objective using ADAM optimizer with an
initial learning rate of 5×10−4. We anneal the learning rate
by a factor of 0.8 every three epochs, and increase the probability of feeding back a sample of the word posterior by
0.05 every 5 epochs until we reach a feedback probability
0.25 . We evaluate at each epoch the model on the development set and select the model with best CIDEr score as an
initialization for SCST training. We then run SCST training
initialized with the XE model to optimize the CIDEr metric (speciﬁcally, the CIDEr-D metric) using ADAM with a
learning rate 5 × 10−5 1. Initially when experimenting with
FC-2k and FC-15k models we utilized curriculum learning
(CL) during training, as proposed in , by increasing the
number of words that are sampled and trained under CIDEr
by one each epoch (the preﬁx of the sentence remains under
1In the case of the Att2all models, the XE model was trained for only
20 epochs, and the learning rate was also annealed during RL training.
the XE criterion until eventually being subsumed). We have
since realized that for the MSCOCO task CL is not required,
and provides little to no boost in performance. The results
reported here for the FC-2K and FC-15K models are trained
with CL, while the attention models were trained directly on
the entire sentence for all epochs after being initialized by
the XE seed models.
Evaluation Metric
Performance of self-critical sequence training
(SCST) versus MIXER and MIXER without a baseline
(MIXER-B) on the test portion of the Karpathy splits when
trained to optimize the CIDEr metric (FC-2K models). All
improve the seed cross-entropy trained model, but SCST
outperforms MIXER.
5.1. Ofﬂine Evaluation
Evaluating different RL training strategies.
Table 1 compares the performance of SCST to MIXER
 (test set, Karpathy splits). In this experiment, we utilize
“curriculum learning” (CL) by optimizing the expected reward of the metric on the last n words of each training sentence, optimizing XE on the remaining sentence preﬁx, and
Table 2: Mean/std. performance of SCST versus REIN-
FORCE and REINFORCE with learned baseline (MIXER
less CL), for Att2all models (4 seeds) on the Karpathy test
set (CIDEr optimized). A one-sample t-test on the gain of
SCST over MIXER less CL rejects the null hypothesis on
all metrics except ROUGE for α = 0.1 (i.e. pnull < 0.1).
slowly increasing n. The results reported were generated
with the optimized CL schedule reported in . We found
that CL was not necessary to train both SCST and REIN-
FORCE with a learned baseline on MSCOCO—turning off
CL sped up training and yielded equal or better results. The
gain of SCST over learned baselines was consistent, regardless of the CL schedule and the initial seed. Figures 2 and 3
and table 2 compare the performance of SCST and MIXER
less CL for Att2all models on the Karpathy validation and
test splits, respectively. Figure 4 and 5 further compare their
gradient variance and word posterior entropy on the Karpathy training set. While both techniques are unbiased, SCST
in general has lower gradient variance, which translates to
improved training performance.
Interestingly, SCST has
much higher gradient variance than MIXER less CL during
the ﬁrst epoch of training, as most sampled sentences initially score signiﬁcantly lower than the sentence produced
by the test-time inference algorithm.
Training on different metrics.
We experimented with training directly on the evaluation
metrics of the MSCOCO challenge.
Results for FC-2K
models are depicted in table 3.
In general we can see
that optimizing for a given metric during training leads
to the best performance on that same metric at test time,
an expected result.
We experimented with training on
multiple test metrics, and found that we were unable to
outperform the overall performance of the model trained
only on the CIDEr metric, which lifts the performance of
all other metrics considerably. For this reason most of our
experimentation has since focused on optimizing CIDEr.
Single FC-Models Versus Attention Models. We trained
FC models (2K and 15 K), as well as attention models
(Att2in and Att2all) using SCST with the CIDEr metric. We
trained 4 different models for each FC and attention type,
starting the optimization from four different random seeds
2. We report in Table 4, the system with best performance
2pls. consult the supp. material for further details on model training.
Mean/Std. CIDEr-D, Val. Set (4 seeds, greedy)
MIXER - CL
Figure 2: Mean/std. CIDEr of SCST versus REINFORCE
with learned baseline (MIXER less CL) with greedy decoding, for Att2all models (4 seeds) on the Karpathy validation
set (CIDEr-D optimized).
Mean/Std. CIDEr-D, Val. Set (4 seeds, beam)
MIXER - CL
Figure 3: Mean/std. CIDEr of SCST versus REINFORCE
with learned baseline (MIXER less CL) with beam search
decoding, for Att2all models (4 seeds) on the Karpathy validation set (CIDEr-D optimized).
for each family of models on the test portion of Karpathy
splits . We see that the FC-15K models outperform the
FC-2K models. Both FC models are outperformed by the
attention models, which establish a new state of the art for
a single model performance on Karpathy splits. Note that
this quantitative evaluation favors attention models is inline
with our observation that attention models tend to generalize better and compose outside of the context of the training
Mean/Std. Gradient Variance, Training Set (4 seeds)
MIXER - CL
Figure 4: Mean/std.
gradient variance of SCST versus
REINFORCE with learned baseline (MIXER less CL),
for Att2all models (4 seeds) on the Karpathy training set
(CIDEr-D optimized).
Mean/Std. Entropy, Training Set (4 seeds)
MIXER - CL
Figure 5: Mean/std. word posterior entropy of SCST versus REINFORCE with learned baseline (MIXER less CL),
for Att2all models (4 seeds) on the Karpathy training set
(CIDEr-D optimized).
of MSCOCO, as we will see in Section 6.
Model Ensembling. In this section we investigate the performance of ensembling over 4 random seeds of the XE
and SCST-trained FC models and attention models. We see
in Table 5 that ensembling improves performance and con-
ﬁrms the supremacy of attention modeling, and establishes
yet another state of the art result on Karpathy splits .
Note that in our case we ensemble only 4 models and we
Evaluation Metric
Table 3: Performance on the test portion of the Karpathy
splits as a function of training metric ( FC-2K models).
Optimizing the CIDEr metric increases the overall performance under the evaluation metrics the most signiﬁcantly.
The performance of the seed cross-entropy (XE) model is
also depicted. All models were decoded greedily, with the
exception of the XE beam search result, which was optimized to beam 3 on the validation set.
don’t do any ﬁne-tuning of the Resnet. NIC , in contrast, used an ensemble of 15 models with ﬁne-tuned CNNs.
5.2. Online Evaluation on MS-COCO Server
Table 6 reports the performance of two variants of 4 ensembled attention models trained with self-critical sequence
training (SCST) on the ofﬁcial MSCOCO evaluation server.
The previous best result on the leaderboard is also depicted. We outperform the previous best
system on all evaluation metrics.
6. Example of Generated Captions
Here we provide a qualitative example of the captions
generated by our systems for the image in ﬁgure 6. This
picture is taken from the objects out-of-context (OOOC)
dataset of images . It depicts a boat situated in an unusual context, and tests the ability of our models to compose
descriptions of images that differ from those seen during
training. The top 5 captions returned by the XE and SCSTtrained FC-2K, FC-15K, and attention model ensembles
when deployed with a decoding “beam” of 5 are depicted in
ﬁgure 7 3. On this image the FC models fail completely, and
the SCST-trained ensemble of attention models is the only
system that is able to correctly describe the image. In general we found that the performance of all captioning systems
on MSCOCO data is qualitatively similar, while on images
containing objects situated in an uncommon context 
(i.e. unlike the MSCOCO training set) our attention models perform much better, and SCST-trained attention models output yet more accurate and descriptive captions. In
general we qualitatively found that SCST-trained attention
models describe images more accurately, and with higher
3pls. consult the the supp. material for further details on beam search.
Single Best Models (XE)
Evaluation Metric
Single Best Models (SCST unless noted o.w.)
Evaluation Metric
(MIXER-CL)
Table 4: Performance of the best XE and corr. SCST-trained
single models on the Karpathy test split (best of 4 random
seeds). The results obtained via the greedy decoding and
optimized beam search are depicted. Models learned using
SCST were trained to directly optimize the CIDEr metric.
MIXER less CL results (MIXER-) are also included.
conﬁdence, as reﬂected in Figure 7, where the average of
the log-likelihoods of the words in each generated caption
are also depicted. Additional examples can be found in the
supplementary material. Note that we found that Att2in attention models actually performed better than our Att2all
models when applied to images “from the wild”, so here we
focus on demonstrating them.
7. Discussion and Future Work
In this paper we have presented a simple and efﬁcient
approach to more effectively baselining the REINFORCE
algorithm for policy-gradient based RL, which allows us
to more effectively train on non-differentiable metrics, and
leads to signiﬁcant improvements in captioning performance on MSCOCO—our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task.
The self-critical approach, which normalizes the reward ob-
Ensembled Models (XE)
Evaluation Metric
(RL seeds)
Ensembled Models (SCST unless o.w. noted)
Evaluation Metric
(MIXER-CL)
Table 5: Performance of Ensembled XE and SCST-trained
models on the Karpathy test split (ensembled over 4 random
The models learned using self-critical sequence
training (SCST) were trained to optimize the CIDEr metric. MIXER less CL results (MIXER-) are also included.
Evaluation Metric
SCST models
Ens. 4 (Att2all)
Ens. 4 (Att2in)
Previous best
Performance of 4 ensembled attention models trained with self-critical sequence training (SCST)
on the ofﬁcial MSCOCO evaluation server (5 reference
captions).
leaderboard
04/10/2017)
 Table C5,
Watson Multimodal).
tained by sampled sentences with the reward obtained by
the model under the test-time inference algorithm is intuitive, and avoids having to estimate both action-dependent
and action-independent reward functions.
Figure 6: An image from the objects out-of-context (OOOC) dataset of images from .
(a) Ensemble of 4 Attention models
(Att2in) trained with XE.
(c) Ensemble of 4 FC-2K
models trained with XE.
(d) Ensemble of 4 FC-2K
models trained with SCST.
(b) Ensemble of 4 Attention models
(Att2in) trained with SCST.
(f) Ensemble of 4 FC-15K
models trained with SCST.
(e) Ensemble of 4 FC-15K
models trained with XE.
1. a blue of a building with a blue umbrella on it ­1.234499
2. a blue of a building with a blue and blue umbrella ­1.253700
3. a blue of a building with a blue umbrella ­1.261105
4. a blue of a building with a blue and a blue umbrella on top of it ­1.277339
5. a blue of a building with a blue and a blue umbrella ­1.280045
1. a blue boat is sitting on the side of a building ­0.194627
2. a blue street sign on the side of a building ­0.224760
3. a blue umbrella sitting on top of a building ­0.243250
4. a blue boat sitting on the side of a building ­0.248849
5. a blue boat is sitting on the side of a city street ­0.265613
1. a couple of bikes parked next to each other ­1.005856
2. a couple of bikes parked next to each other on a of a building ­1.039497
3. a couple of bikes parked next to each other on a building ­1.050528
4. a couple of bikes parked next to each other on a street ­1.055674
5. a couple of bikes parked next to a building ­1.070224
1. a statue of a man on a bike with a building ­0.376297
2. a statue of a building with a bicycle on a street ­0.414397
3. a statue of a bicycle on a building with a surfboard ­0.423379
4. a statue of a bicycle on a building with a umbrella ­0.430222
5. a statue of a building with a umbrella ­0.435535
1. a couple of bikes that are next to a building ­0.898723
2. a couple of bikes parked next to a building ­0.932335
3. a row of bikes parked next to a building ­0.950412
4. a row of bicycles parked next to a building ­0.971651
5. a couple of bikes parked next to each other ­0.985120
1. a scooter parked in front of a building ­0.326988
2. a group of a motorcycle parked in front of a building ­0.366700
3. a group of surfboards in front of a building ­0.386932
4. a scooter parked in front of a building with a clock ­0.429441
5. a scooter parked in front of a building with a building ­0.433893
Figure 7: Captions generated for the image depicted in Figure 6 by the various models discussed in the paper. Beside each
caption we report the average log probability of the words in the caption. On this image, which presents an object situated in
an atypical context , the FC models fail to give an accurate description, while the attention models handle the previously
unseen image composition well. The models trained with SCST return a more accurate and more detailed summary of the