Semi-supervised protein classiﬁcation using
cluster kernels
Jason Weston∗
Max Planck Institute for Biological Cybernetics,
72076 T¨ubingen, Germany
 
Christina Leslie
Department of Computer Science,
Columbia University
 
Dengyong Zhou, Andre Elisseeff
Max Planck Institute for Biological Cybernetics,
72076 T¨ubingen, Germany
 
William Stafford Noble
Department of Genome Sciences
University of Washington
 
A key issue in supervised protein classiﬁcation is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classiﬁcation performance. However, such representations are based only on labeled data — examples
with known 3D structures, organized into structural classes — while
in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that
our methods greatly improve the classiﬁcation performance of string kernels and outperform standard approaches for using unlabeled data, such
as adding close homologs of the positive examples to the training data.
We achieve equal or superior performance to previously presented cluster
kernel methods while achieving far greater computational efﬁciency.
Introduction
A central problem in computational biology is the classiﬁcation of proteins into functional
and structural classes given their amino acid sequences. The 3D structure that a protein
assumes after folding largely determines its function in the cell. However, it is far easier
to determine experimentally the primary sequence of a protein than it is to solve the 3D
structure. Through evolution, structure is more conserved than sequence, so that detecting
even very subtle sequence similarities, or remote homology, is important for predicting
The major methods for homology detection can be split into three basic groups: pairwise
sequence comparison algorithms , generative models for protein families , and
discriminative classiﬁers . Popular sequence comparison methods such as BLAST
∗Supplemental information for the paper, including the data sets and Matlab source code can be
found on this author’s web page at 
and Smith-Waterman are based on unsupervised alignment scores. Generative models such
as proﬁle hidden Markov models (HMMs) model positive examples of a protein family,
but they can be trained iteratively using both positively labeled and unlabeled examples
by pulling in close homologs and adding them to the positive set. A compromise between
these methods is PSI-BLAST , which uses BLAST to iteratively build a probabilistic
proﬁle of a query sequence and obtain a more sensitive sequence comparison score. Finally,
classiﬁers such as SVMs use both positive and negative examples and provide state-of-theart performance when used with appropriate kernels . However, these classiﬁers still
require an auxiliary method (such as PSI-BLAST) to handle unlabeled data: one generally
adds predicted homologs of the positive training examples to the training set before training
the classiﬁer.
In practice, relatively little labeled data is available — approximately 30,000 proteins with
known 3D structure, some belonging to families and superfamilies with only a handful of
labeled members — whereas there are close to one million sequenced proteins, providing
abundant unlabeled data. New semi-supervised learning techniques should be able to make
better use of this unlabeled data.
Recent work in semi-supervised learning has focused on changing the representation
given to a classiﬁer by taking into account the structure described by the unlabeled data
 . These works can be viewed as cases of cluster kernels, which produce similarity metrics based on the cluster assumption: namely, two points in the same “cluster”
or region of high density should have a small distance to each other. In this work, we
investigate the use of cluster kernels for protein classiﬁcation by developing two simple
and scalable methods for modifying a base kernel. The neighborhood kernel uses averaging over a neighborhood of sequences deﬁned by a local sequence similarity measure,
and the bagged kernel uses bagged clustering of the full sequence data set to modify the
base kernel. In both the semi-supervised and transductive settings, these techniques greatly
improve classiﬁcation performance when used with mismatch string kernels, and the techniques achieve equal or superior results to all previously presented cluster kernel methods
that we tried. Moreover, the neighborhood and bagged kernel approaches are far more
computationally efﬁcient than these competing methods.
Representations and kernels for protein sequences
Proteins can be represented as variable length sequences, typically several hundred characters long, from the alphabet of 20 amino acids. In order to use learning algorithms that
require vector inputs, we must ﬁrst ﬁnd a suitable feature vector representation, mapping
sequence x into a vector space by x 7→Φ(x). If we use kernel methods such as SVMs,
which only need to compute inner products K(x, y) = ⟨Φ(x), Φ(y)⟩for training and testing, then we can accomplish the above mapping using a kernel for sequence data.
Biologically motivated sequence comparison scores, like Smith-Waterman or BLAST, provide an appealing representation of sequence data. The Smith-Waterman (SW) algorithm
 uses dynamic programming to compute the optimal local gapped alignment score between two sequences, while BLAST approximates SW by computing a heuristic alignment score. Both methods return empirically estimated E-values indicating the conﬁdence
of the score. These alignment-based scores do not deﬁne a positive deﬁnite kernel; however, one can use a feature representation based on the empirical kernel map
Φ(x) = ⟨d(x1, x), . . . , d(xm, x)⟩
where d(x, y) is the pairwise score (or E-value) between x and y and xi, i = 1 . . . m,
are the training sequences. Using SW E-values in this fashion gives strong classiﬁcation
performance . Note, however, that the method is slow, both because computing each SW
score is O(|x|2) and because computing each empirically mapped kernel value is O(m).
Another appealing idea is to derive the feature representation from a generative model for
a protein family. In the Fisher kernel method , one ﬁrst builds a proﬁle HMM for the
positive training sequences, deﬁning a log likelihood function log P(x|θ) for any protein
sequence x. Then the gradient vector ∇θ log P(x|θ)|θ=θ0, where θ0 is the maximum likelihood estimate for model parameters, deﬁnes an explicit vector of features, called Fisher
scores, for x. This representation gives excellent classiﬁcation results, but the Fisher scores
must be computed by an O(|x|2) forward-backward algorithm, making the kernel tractable
It is possible to construct useful kernels directly without explicitly depending on generative
models by using string kernels. For example, the mismatch kernel is deﬁned by a
histogram-like feature map that uses mismatches to capture inexact string matching. The
feature space is indexed by all possible k-length subsequences α = a1a2 . . . ak, where each
ai is a character in the alphabet A of amino acids. The feature map is deﬁned on k-gram α
by Φ(α) = (φβ(α))Ak where φβ(α) = 1 if α is within m mismatches of β, 0 otherwise,
and is extended additively to longer sequences: Φ(x) = P
k-grams∈x Φ(α). The mismatch
kernel can be computed efﬁciently using a trie data structure: the complexity of calculating
K(x, y) is O(cK(|x|+|y|)), where cK = km+1|A|m. For typical kernel parameters k = 5
and m = 1 , the mismatch kernel is fast, scalable and yields impressive performance.
Many other interesting models and examples of string kernels have recently been presented.
A survey of related string kernel work is given in the longer version of this paper.
String kernel methods with SVMs are a powerful approach to protein classiﬁcation and
have consistently performed better than non-discriminative techniques . However,
in a real-world setting, protein classiﬁers have access to unlabeled data. We now discuss
how to incorporate such data into the representation given to SVMs via the use of cluster
Cluster kernels for protein sequences
In semi-supervised learning, one tries to improve a classiﬁer trained on labeled data by
exploiting (a relatively large set of) unlabeled data. An extensive review of techniques
can be found in . It has been shown experimentally that under certain conditions, the
decision function can be estimated more accurately in a semi-supervised setting, yielding
lower generalization error. The most common assumption one makes in this setting is
called the “cluster assumption,” namely that the class does not change in regions of high
Although classiﬁers implement the cluster assumption in various ways, we focus on classiﬁers that re-represent the given data to reﬂect structure revealed by unlabeled data. The
main idea is to change the distance metric so that the relative distance between two points
is smaller if the points are in the same cluster. If one is using kernels, rather than explicit
feature vectors, one can modify the kernel representation by constructing a cluster kernel.
In , a general framework is presented for producing cluster kernels by modifying the
eigenspectrum of the kernel matrix. Two of the main methods presented are the random
walk kernel and the spectral clustering kernel.
The random walk kernel is a normalized and symmetrized version of a transition matrix
corresponding to a t-step random walk. The random representation described in interprets an RBF kernel as a transition matrix of a random walk on a graph with vertices
xi, P(xi →xj) =
P Kip . After t steps, the probability of going from a point xi to a
point xj should be high if the points are in the same cluster. This transition probability
can be calculated for the entire matrix as P t = (D−1K)t, where D is a diagonal matrix
such that Dii = P
p Kip. To obtain a kernel, one performs the following steps. Compute L = D−1/2KD−1/2 and its eigendecomposition L = UΛU ⊤. let λi ←λt
λi = Λii, and let ˜L = U ˜ΛU ⊤. Then the new kernel is ˜K = ˜D1/2 ˜L ˜D1/2, where ˜D is a
diagonal matrix with ˜
Dii = 1/Lii.
The spectral clustering kernel is a simple use of the representation derived from spectral
clustering using the ﬁrst k eigenvectors. One computes the eigenvectors (v1, . . . , vk)
2 , with D deﬁned as before, giving the representation φ(xi)p = vpi. This
vector can also then be normalized to have length 1. This approach has been shown to
produce a well-clustered representation. While in spectral clustering, one then performs kmeans in this representation, here one simply gives the representation as input to a classiﬁer.
A serious problem with these methods is that one must diagonalize a matrix the size of the
set of labeled and unlabeled data. Other methods of implementing the cluster assumption
such as transductive SVMs also suffer from computational efﬁciency issues. A second
drawback is that these kernels are better suited to a transductive setting (where one is given
both the unlabeled and test points in advance) rather than a semi-supervising setting. In
order to estimate the kernel for a sequence not present during training, one is forced to
solve a difﬁcult regression problem . In the next two sections we will describe two
simple methods to implement the cluster assumption that do not suffer from these issues.
The neighborhood mismatch kernel
In most current learning applications for prediction of protein properties, such as prediction of three-state secondary structure, neural nets are trained on probabilistic proﬁles of
a sequence window — a matrix of position-speciﬁc emission and gap probabilities —
learned from a PSI-BLAST alignment rather than an encoding of the sequence itself. In this
way, each input sequence is represented probabilistically by its “neighborhood” in a large
sequence database, where PSI-BLAST neighbors are sequences that are closely related
through evolution. We wish to transfer the notion of proﬁles to our mismatch representation of protein sequences.
We use a standard sequence similarity measure like BLAST or PSI-BLAST to deﬁne a
neighborhood Nbd(x) for each input sequence x as the set of sequences x′ with similarity
score to x below a ﬁxed E-value threshold, together with x itself. Now given a ﬁxed original
feature representation, we represent x by the average of the feature vectors for members of
its neighborhood: Φnbd(x) =
x′∈Nbd(x) Φorig(x′). The neighborhood kernel
is then deﬁned by:
Knbd(x, y) =
|Nbd(x)||Nbd(y)|
x′∈Nbd(x),y′∈Nbd(y)
Korig(x′, y′).
We will see in the experimental results that this simple neighborhood-averaging technique,
used in a semi-supervised setting with the mismatch kernel, dramatically improves classi-
ﬁcation performance.
To see how the neighborhood approach ﬁts with the cluster assumption, consider a set of
points in feature space that form a “cluster” or dense region of the data set, and consider
the region R formed by the union of the convex hulls of the neighborhood point sets. If the
dissimilarity measure is a true distance, the neighborhood averaged vector Φnbd(x) stays
inside the convex hull of the vectors in its neighborhood, all the neighborhood vectors stay
within region R. In general, the cluster contracts inside R under the averaging operation.
Thus, under the new representation, different clusters can become better separated from
each other.
The bagged mismatch kernel
There exist a number of clustering techniques that are much more efﬁcient than the methods
mentioned in Section 3. For example, the classical k-means algorithm is O(rkmd), where
m is the number of data points, d is their dimensionality, and r is the number of iterations
required. Empirically, this running time grows sublinearly with k, m and d. In practice, it
is computationally efﬁcient even to run k-means multiple times, which can be useful since
k-means can converge to local minima. We therefore consider the following method:
1. Run k-means n times, giving p = 1, . . . , n cluster assignments cp(xi) for each i.
2. Build a bagged-clustering representation based upon the fraction of times that xi
and xj are in the same cluster:
Kbag(xi, xj) =
p[cp(xi) = cp(xj)]
3. Take the product between the original and bagged kernel:
K(xi, xj) = Korig(xi, xj) · Kbag(xi, xj)
Because k-means gives different solutions on each run, step (1) will give different results;
for other clustering algorithms one could sub-sample the data instead. Step (2) is a valid
kernel because it is the inner product in an nk-dimensional space Φ(xi) = ⟨[cp(xi) = q] :
p = 1, . . . , n, q = 1, . . . , k⟩, and products of kernels as in step (3) are also valid kernels.
The intuition behind the approach is that the original kernel is rescaled by the “probability”
that two points are in the same cluster, hence encoding the cluster assumption. To estimate
the kernel on a test sequence x in a semi-supervised setting, one can assign x to the nearest
cluster in each of the bagged runs to compute Kbag(x, xi). We apply the bagged kernel
method with Korig as the mismatch kernel and Kbag built using PSI-BLAST.
Experiments
We measure the recognition performance of cluster kernels methods by testing their ability
to classify protein domains into superfamilies in the Structural Classiﬁcation of Proteins
(SCOP) . We use the same 54 target families and the same test and training set splits
as in the remote homology experiments in . The sequences are 7329 SCOP domains
obtained from version 1.59 of the database after purging with astral.stanford.edu so that no
pair of sequences share more than 95% identity. Compared to , we reduce the number
of available labeled training patterns by roughly a third. Data set sequences that were
neither in the training nor test sets for experiments from are included as unlabeled
data. All methods are evaluated using the receiver operating characteristic (ROC) score
and the ROC-50, which is the ROC score computed only up to the ﬁrst 50 false positives.
More details concerning the experimental setup can be found at 
columbia.edu/compbio/svm-pairwise.
In all experiments, we use an SVM classiﬁer with a small soft margin parameter, set as
in . The SVM computations are performed using the freely available Spider Matlab machine learning package available at 
bs/people/spider. More information concerning the experiments, including data
and source code scripts, can be found at 
bs/people/weston/semiprot.
Semi-supervised setting.
Our ﬁrst experiment shows that the neighborhood mismatch
kernel makes better use of unlabeled data than the baseline method of “pulling in homologs” prior to training the SVM classiﬁer, that is, simply ﬁnding close homologs of
Number of families
Using PSI−BLAST for homologs & neighborhoods
mismatch(5,1)
mismatch(5,1)+homologs
neighborhood mismatch(5,1)
Neighborhood Mismatch(5,1) ROC−50
Mismatch(5,1)+homologs ROC−50
Figure 1: Comparison of protein representations and classiﬁers using unlabeled data.
The mismatch kernel is used to represent proteins, with close homologs being pulled in
from the unlabeled set with PSI-BLAST. Building a neighborhood with the neighborhood
mismatch kernel improves over the baseline of pulling in homologs.
mismatch kernel
mismatch kernel + homologs
neighborhood mismatch kernel
Table 1: Mean ROC-50 and ROC scores over 54 target families for semi-supervised experiments, using BLAST and PSI-BLAST.
the positive training examples in the unlabeled set and adding them to the positive training
set for the SVM. Homologs come from the unlabeled set (not the test set), and “neighbors” for the neighborhood kernel come from the training plus unlabeled data. We compare the methods using the mismatch kernel representation with k = 5 and m = 1, as
used in . Homologs are chosen via PSI-BLAST as having a pairwise score (E-value)
with any of the positive training samples less than 0.05, the default parameter setting .
The neighborhood mismatch kernel uses the same threshold to choose neighborhoods.
For the neighborhood kernel, we normalize before and after the averaging operation via
KiiKjj. The results are given in Figure 1 and Table 1. The former plots
the number of families achieving a given ROC-50 score, and a strongly performing method
thus produces a curve close to the top right of the plot. A signed rank test shows that the
neighborhood mismatch kernel yields signiﬁcant improvement over adding homologs (pvalue 3.9e-05). Note that the PSI-BLAST scores in these experiments are built using the
whole database of 7329 sequences (that is, test sequences in a given experiment are also
available to the PSI-BLAST algorithm), so these results are slightly optimistic. However,
the comparison of methods in a truly inductive setting using BLAST shows the same improvement of the neighborhood mismatch kernel over adding homologs (p-value 8.4e-05).
Adding homologs to the (much larger) negative training set in addition to pulling in the positive homologs gives poorer performance than only adding the positive homologs (results
not shown).
Transductive setting.
In the following experiments, we consider a transductive setting,
in which the test points are given to the methods in advance as unlabeled data, giving
slightly improved results over the last section. Although this setting is unrealistic for a
real protein classiﬁcation system, it more easily enables comparison with random walk
and spectral clustering kernels, which do not easily work in another setting. In Figure 2
(left), we again show the mismatch kernel compared with pulling in homologs and the
neighborhood kernel. This time we also compare with the bagged mismatch kernel using
Number of families
Mismatch kernel, PSI−BLAST distance
mismatch(5,1)
mismatch(5,1)+homologs
neighborhood mismatch(5,1)
bagged mismatch(5,1) k=100
Number of families
PSI−BLAST kernel, varying methods
+ close homologs
spectral cluster, k=100
random walk, t=2
Figure 2: Comparison of protein representations and classiﬁers using unlabeled data
in a transductive setting. Neighborhood and bagged mismatch kernels outperform pulling
in close homologs (left) and equal or outperform previous semi-supervised methods (right).
mismatch kernel
PSI-BLAST kernel
mismatch kernel + homologs
PSI-BLAST+homologs kernel
neighborhood mismatch kernel
spectral clustering kernel
bagged mismatch kernel (k = 100)
random walk kernel
bagged mismatch kernel (k = 400)
transductive SVM
Table 2: Mean ROC-50 and ROC scores over 54 target families for transductive experiments.
bagged k-means with k = 100 and n = 100 runs, which gave the best results. We found
the method quite insensitive to k. The result for k = 400 is also given in Table 2.
We then compare these methods to using random walk and spectral clustering kernels.
Both methods do not work well for the mismatch kernel (see online supplement), perhaps
because the feature vectors are so orthogonal. However, for a PSI-BLAST representation
via empirical kernel map, the random walk outperforms pulling in homologs. We take the
empirical map with Φ(x) = ⟨exp(−λd(x1, x)), . . . , exp(−λ(d(xm, x))⟩, where d(x, y)
are PSI-BLAST E-values and λ =
1000, which improves over a linear map. We report
results for the best parameter choices, t = 2 for the random walk and k = 200 for spectral
clustering. We found the latter quite brittle with respect to the parameter choice; results
for other parameters can be found on the supplemental web site. For pulling in close
homologs, we take the empirical kernel map only for points in the training set and the
chosen close homologs. Finally, we also run transductive SVMs. The results are given
in Table 2 and Figure 2 (right). A signed rank test (with adjusted p-value cut-off of 0.05)
ﬁnds no signiﬁcant difference between the neighborhood kernel, the bagged kernel (k =
100), and the random walk kernel in this transductive setting. Thus the new techniques are
comparable with random walk, but are feasible to calculate on full scale problems.
Discussion
Two of the most important issues in protein classication are representation of sequences
and handling unlabeled data. Two developments in recent kernel methods research, string
kernels and cluster kernels, address these issues separately. We have described two kernels
— the neighborhood mismatch kernel and the bagged mismatch kernel — that combine
both approaches and yield state-of-the-art performance in protein classiﬁcation. Practical
use of semi-supervised protein classiﬁcation techniques requires computational efﬁciency.
Many cluster kernels require diagonalization of the full labeled plus unlabeled data kernel
matrix. The neighborhood and bagged kernel approaches, used with an efﬁcient string kernel, are fast and scalable cluster kernels for sequence data. Moreover, these techniques can
be applied to any problem with a meaningful local similarity measure or distance function.
Future work will deal with additional challenges of protein classiﬁcation: addressing the
full multi-class problem, which potentially involves thousands of classes; handling very
small classes with few homologs; and dealing with missing classes, for which no labeled
examples exist.
Acknowledgments
We would like to thank Eleazar Eskin for discussions that contributed to the neighborhood
kernel and Olivier Chapelle and Navin Lal for their help with this work.