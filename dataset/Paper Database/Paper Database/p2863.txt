Scaling Vision Transformers to Gigapixel Images via
Hierarchical Self-Supervised Learning
Richard J. Chen1, Chengkuan Chen1, Yicong Li1, Tiffany Y. Chen1,
Andrew D. Trister2, Rahul G. Krishnan3,∗, Faisal Mahmood1,∗
1Harvard, BWH, Broad Institute
2Bill & Melinda Gates Foundation
3University of Toronto
 , 
Vision Transformers (ViTs) and their multi-scale and hierarchical variations have been successful at capturing image representations but their use has been generally studied for low-resolution images (e.g. 256 × 256, 384 × 384).
For gigapixel whole-slide imaging (WSI) in computational
pathology, WSIs can be as large as 150000×150000 pixels
at 20× magnification and exhibit a hierarchical structure of
visual tokens across varying resolutions: from 16 × 16 images capturing individual cells, to 4096×4096 images characterizing interactions within the tissue microenvironment.
We introduce a new ViT architecture called the Hierarchical
Image Pyramid Transformer (HIPT), which leverages the
natural hierarchical structure inherent in WSIs using two
levels of self-supervised learning to learn high-resolution
image representations. HIPT is pretrained across 33 cancer types using 10,678 gigapixel WSIs, 408,218 4096×4096
images, and 104M 256 × 256 images. We benchmark HIPT
representations on 9 slide-level tasks, and demonstrate that:
1) HIPT with hierarchical pretraining outperforms current
state-of-the-art methods for cancer subtyping and survival
prediction, 2) self-supervised ViTs are able to model important inductive biases about the hierarchical structure of
phenotypes in the tumor microenvironment.
1. Introduction
Tissue phenotyping is a fundamental problem in computational pathology (CPATH) that aims at characterizing
objective, histopathologic features within gigapixel wholeslide images (WSIs) for cancer diagnosis, prognosis, and
the estimation of response-to-treatment in patients . Unlike natural images, whole-slide imaging is a challenging computer vision domain in which image resolutions can be as large as 150000 × 150000 pixels, with
many methods using the following three-stage, weaklysupervised framework based on multiple instance learning
∗Contributed Equally.
Hierarchical Structure of Whole-Slide Images
(WSIs). Left. Unlike natural images, since WSIs have a fixed
scale, there exists a hierarchical structure of visual tokens at varying image resolutions. Right. In addition to formulating a single
256 × 256 image as as sequence of 256 [16 × 16] tokens, we can
also view these 256 × 256 image as being part of a larger, disjoint
sequence of [256 × 256] tokens in a 4096 × 4096 region.
(MIL): 1) tissue patching at a single magnification objective (“zoom”), 2) patch-level feature extraction to construct
a sequence of embedding instances, and 3) global pooling of
instances to construct a slide-level representation for weaksupervision using slide-level labels (e.g. - subtype, grade,
stage, survival, origin) .
Though achieving “clinical-grade” performance on
many cancer subtyping and grading tasks, this three-stage
process has a few important design limitations. First, patching and feature extraction are generally fixed to [256 × 256]
context regions. Though able to discern fine-grained mor-
phological features such as nuclear atypia or tumor presence, depending on the cancer type, [256 × 256] windows have limited context in capturing coarser-grained features such as tumor invasion, tumor size, lymphocytic infiltrates, and the broader spatial organization of these phenotypes in the tissue microenvironment, as depicted in Figure
1 . Second, in contrast with other image-based sequence modeling approaches such as Vision Transformers
(ViTs), MIL uses only global pooling operators due to the
large sequence lengths of WSIs . As a result, this limitation precludes the application of Transformer attention
for learning long-range dependencies between phenotypes
such as tumor-immune localization, an important prognostic feature in survival prediction . Lastly, though
recent MIL approaches have adopted self-supervised learning as a strategy for patch-level feature extraction (called
tokenization in ViT literature), parameters in the aggregation layers still require training . In
viewing patch-based sequence modeling of WSIs in relation to ViTs, we note that the architectural design choice of
using Transformer attention enables pretraining of both the
tokenization and aggregation layers in ViT models, which
is important in preventing MIL models from over- or underfitting in low-data regimes .
To address these issues, we explore the challenge of developing a Vision Transformer for slide-level representation
learning in WSIs. In comparison to natural images which
are actively explored by ViTs, we note a key difference in
modeling WSIs is that visual tokens would always be at a
fixed scale for a given magnification objective. For instance,
scanning WSIs at a 20× objective results in a fixed scale
of approximately 0.5µm per pixel, allowing for consistent
comparison of visual elements that may elucidate important
histomorphological features beyond their normal reference
ranges. Moreover, WSIs also exhibit a hierarchical structure of visual tokens at varying image resolutions at 20×
magnification: the 16 × 16 images encompass the bounding box of cells and other fine-grained features (stroma, tumor cells, lymphocytes) , 256 × 256 images capture local clusters of cell-to-cell interactions (tumor cellularity) , 1024 × 1024-4096 × 4096 images further characterize macro-scale interactions between clusters
of cells and their organization in tissue (the extent of tumorimmune localization in describing tumor-infiltrating versus
tumor-distal lymphocytes) , and finally the overall
intra-tumoral heterogeneity of the tissue microenvironment
depicted at the slide-level of the WSI . The
hypothesis that this work tests is that the judicious use of
this hierarchy in self-supervised learning results in better
slide-level representations.
We introduce a Transformer-based architecture for hierarchical aggregation of visual tokens and pretraining in gigapixel pathology images, called Hierarchical Image Pyramid Transformer (HIPT). We approach the task of slidelevel representation learning in a manner similar to learning long document representations in language modeling,
in which we develop a three-stage hierarchical architecture
that performs bottom-up aggregation from [16 × 16] visual
tokens in their respective 256 × 256 and 4096 × 4096 windows to eventually form the slide-level representation, as
demonstrated in Figure 2 . Our work pushes the
boundaries of both Vision Transformers and self-supervised
learning in two important ways. By modeling WSIs as a
disjoint set of nested sequences, within HIPT: 1) we decompose the problem of learning a good representation of
a WSI into hierarchically-related representations each of
which can be learned via self-supervised learning, and 2) we
use student-teacher knowledge distillation (DINO ) to
pretrain each aggregation layers with self-supervised learning on regions as large as 4096 × 4096.
We apply HIPT to the task of learning representations
of gigapixel histopathological images extracted at 20× resolution. We show that our method achieves superior performance to conventional MIL approaches. The difference
is pronounced in context-aware tasks such as survival prediction in which larger context is appreciated in characterizing broader prognostic features in the tissue microenvironment . Using K-Nearest Neighbors on
the 4096 × 4096 representations of our model, we outperform several weakly-supervised architectures in slide-level
classification – an important step forward in achieving selfsupervised slide-level representations. Finally, akin to selfsupervised ViTs on natural images that can perform semantic segmentation of the scene layout, we find that the multihead self-attention in self-supervised ViTs learn visual concepts in histopathology tissue (from fine-grained visual concepts such as cell locations in the ViT256-16 to coarsegrained visual concepts such as broader tumor cellularity in
the ViT4096-256), as demonstrated in Figure 3, 4. We make
code available at 
2. Related Work
Multiple Instance Learning in WSIs.
In general setbased deep learning, Edwards & Storkey and Zaheer et
al. proposed the first network architectures operating on
set-based data structures, with Brendel et al. demonstrating “bag-of-features” able to reach high accuracy on ImageNet . Concurrently in pathology, Ilse et al. extended set-based network architectures as an approach for
multiple instance learning in histology region-of-interests,
with Campanella et al. later extending end-to-end weaksupervision on gigapixel WSIs . Lu et al. demonstrated that by using a pretrained ResNet-50 encoder on ImageNet for instance-level feature extraction, only a global
pooling operator needs to be trained for weakly-supervised
slide-level tasks . Following Lu et al., there have been
many variations of MIL that have adapted image pretraining techniques such as VAE-GANs, SimCLR, and MOCO
as instance-level feature extraction . Recent variations of MIL have also evolved to extend the aggregation layers and scoring functions .
Li et al. proposed a multi-scale MIL approach that performs patching and self-supervised instance learning at 20×
and 5× resolution, followed by spatially-resolved alignment of patches . The integration of magnification objectives within WSIs has been followed in other works as
well , however, we note that combining visual
tokens across objectives would not share the same scale. In
this work, patching is done at a single magnification objective, with larger patch sizes used to capture macro-scale
morphological features, which we hope will contribute towards a shift in rethinking context modeling of WSIs.
Vision Transformers and Image Pyramids. The seminal
work of Vaswani et al. has led to remarkable developments
in not only language modeling, but also image representation learning via Vision Transformers (ViTs), in which
256 × 256 images are formulated as an image patch sequence of [16 × 16] visual tokens . Motivated by
multiscale, pyramid-based image processing , recent progress in ViT architecture development has focused
on efficiency and integration of multiscale information (e.g.
- Swin, ViL, TNT, PVT, MViT) in addressing the varying
scale / aspect ratios of visual tokens . In
contrast with pathology, we highlight that learning scale invariance may not be necessary if the image scale is fixed at
a given magnification. Similar to our work is NesT and Hierarchical Perciever, which similarly partitions and then aggregates features from non-overlapping image regions via
Transformer blocks . A key difference is that we
show ViT blocks at each stage can be separately pretrained
for high-resolution encoding (up to 4096 × 4096).
3.1. Problem Formulation
Patch Size and Visual Token Notation: We use the following notation to distinguish between the sizes of “images”
and “tokens” that correspond to that image. For an image x
with resolution L × L (or xL), we refer to sequence of extracted visual tokens from non-overlapping patches (of size
[l×l]) within xL as {x(i)
i=1 ∈RM×dl, where M is the sequence length and d is the embedding dimension extracted
for l-sized tokens. In working with multiple image resolutions (and their respective tokens) in a WSI, we additionally
denote the shape of visual tokens (and the patching parameter) within xL image as [l ×l] (using brackets). For natural
images with size x256, ViTs generally use l = L1/2 = 16
which results in a sequence length of M = 256. Additionally, we denote a ViT working on a L-sized image resolution
with [l × l] tokens as ViTL −l. For xWSI (referring to the
slide-level resolution of the WSI), MIL approaches choose
l = 256 which fits the input shape of CNN encoders that
can be pretrained and using for tokenization, resulting in
M > 10, 000 (variable due to the total area of segmented
tissue content).
Slide-Level Weak Supervision:
For a WSI xWSI with
outcome y, the goal is to solve the slide-level classification task P(y|xWSI).
Conventional approaches for solving this task use a three-stage MIL framework which performs: 1) [256 × 256]-patching, 2) tokenization, and 3)
global attention pooling. xWSI is formulated as the sequence
i=1 ∈RM×1024 which results from using a ResNet-
50 encoder pretrained on ImageNet (truncated after the 3rd
residual block).
Due to the large sequence lengths with
l = 256, neural network architectures in this task are limited to per-patch and global pooling operators in extracting
a slide-level embedding for downstream tasks.
3.2. Hierarchical Image Pyramid Transformer
(HIPT) Architecture
In adapting ViTs for slide-level representation learning,
we reiterate two important challenges distinct from computer vision in natural images: 1) the fixed scale of visual tokens and their hierarchical relationships across image resolutions, and 2) the large sequence lengths of unrolled WSIs. As mentioned, visual tokens in histopathology are generally object-centric (and vary in granularity)
across image resolutions, and also have important contextual dependencies such as tumor-immune (inferring favorable prognosis) or tumor-stroma interactions (inferring invasion). Patching with small visual tokens at high objectives (x256 at 20×) results in large sequence lengths that
make self-attention intractable, whereas patching with large
visual tokens at low objectives results in loss-of-detail of
fine-grained morphological structures (x256 at 5×) that still
requires [256 × 256] patching at 20×.
To capture this hierarchical structure and the important
dependencies that may exist at each image resolution, we
approach WSIs similar to long documents as a nested aggregation of visual tokens that recursively break down into
smaller tokens until the cell-level (Figure 2), written as:
& \opera t orname {HIP
I}}) = \operator
409 6 }\big (\b
name {CLS}_{4096}^{(k)} \big \}_{k=1}^{M}\big ) \\ &\rightarrow \operatorname {CLS}_{4096}^{(k)} = \operatorname {ViT_{4096}\textrm {-}256} \big ( \{ \operatorname {CLS}_{256}^{(j)} \}_{i=1}^{256} \big ) \\ &\rightarrow \operatorname {CLS}_{256}^{(j)} = \operatorname {ViT_{256}\textrm {-}16} \big ( \{ \textbf {x}_{16}^{(i)} \}_{i=1}^{256} \big )
where 256 is the sequence length of [16 × 16]- and [256 ×
256]-patching in x256 and x4096 images respectively, and M
is the total number of x4096 images in xWSI. For ease of notation, we refer to x16 images as being at the cell-level, x256
Figure 2. HIPT Architecture. Motivated by the use of hierarchical representations in natural language processing, where embeddings can
be aggregated at the character-, word-, sentence- and paragraph-level to form document representations, we aggregate visual tokens at the
x16 cell-, x256 patch-, x4096 region-level to form slide representations. To also model important dependencies between visual concepts at
each stage, we adapt Transformer self-attention as a permutation-equivariant aggregation layer. Note that since the complexity of patching
x4096 regions with x256 tokens is the same as patching x256 images with x16 tokens, we can pretrain aggregation layers for high-resolution
images using similar self-supervised ViT techniques for low-resolution images.
images as being at the patch-level1, x4096 images as being
at the region-level, with the overall WSI being the slidelevel. In choosing these image sizes, the input sequence
length of tokens is always M = 256 in the forward passes
for the ViT256-16 and ViT4096-256 (cell- and patch-level
aggregation), and usually M < 256 in the forward pass for
the ViTWSI-4096 (slide-level aggregation). The [CLS] tokens from ViT256-16 (the output of the model) are used as
the input sequence for ViT4096-256, with the [CLS] tokens
from ViT4096-256 subsequently used as the input sequence
for ViTWSI-4096, with the number of total visual tokens at
each stage decreasing geometrically by a factor of 256. In
choosing small ViT backbones for each stage, HIPT has less
than 10M parameters and is easy-to-implement and train on
commercial workstations. We describe each stage below.
ViT256-16 for Cell-Level Aggregation. The computation
of x16 cell-level token aggregation within x256 windows
follows implementing the vanilla ViT in natural images
 . Given a x256 patch, the ViT unrolls this image as
a sequence of non-overlapping [16 × 16] tokens followed
by a linear embedding layer with added position embed-
1“Patch” is most often used to describe 256×256 images in pathology,
though we note “patching” an image into smaller images can refer to any
resolution.
dings to produce a set of 384-dim embeddings {x(i)
R256×384, with a learnable [CLS] token added to aggregate cell embeddings across the sequence.
l = 16 in this setting to not only follow conventional ViT
architectures, but also model important inductive biases in
histopathology as at this resolution, a [16 × 16] bounding
box at 20× ≈8µm2 area encodes visual concepts that are
object-centric in featurizing single cells (e.g. - cell identity,
shape, roundness).
ViT4096-256 for Patch-Level Aggregation. To represent
x4096 regions, despite the image resolution being much
larger than conventional natural images, the number of tokens remains the same since the patch size scales with
the image resolution.
From the previous stage, we use
ViT256-16 to tokenize non-overlapping x256 patches within
each x4096 region, forming the sequence {[CLS](j)
that can be plugged into a ViT block to model larger image
contexts. We use a ViT4096-256(n = 4, h = 3, d = 192)
with output [CLS]4096.
ViTWSI-4096 for Region-Level Aggregation.
In computing the slide-level representation for xWSI, we use a
ViTWSI-4096(n = 2, h = 3, d = 192) in aggregating the
[CLS]4096 tokens. M ranges from 1 −256 in our observa-
Multi-Head Self-Attention Visualization of Self-
Supervised ViTs. For Invasive Ductal Carcinoma (IDC), We show
self-supervised visualizations for ViT256-16 and ViT4096-256,
pretrained on x256 and x4096 regions respectively.
patches, ViT256-16 is able to delineate stroma, cell, and ”white
space” presence in x16 tokens. For x4096 regions, ViT4096-256
delineates coarse-grained morphological features such as tumor
nests and their surrounding desmoplastic (loose) stroma.
tions depending on size of the WSI. Due to potential tissue
segmentation irregularities in patching at [4096×4096], we
ignore positional embeddings at this stage.
3.3. Hierarchical Pretraining
In building a MIL framework using only Transformer
blocks, we additionally explore and pose a new challenge
referred to as slide-level self-supervised learning - which
aims at extracting slide-level feature representations in gigapixel images for downstream diagnostic and prognostic
tasks. This is an important problem as current slide-level
training datasets in CPATH typically have between 100 to
10,000 data points, which may cause MIL methods to overfit due to over-parameterization and lack of labels.2 To address this problem, we hypothesize that the recursive nature
of HIPT in using Transformer blocks for image representation learning can enable conventional ViT pretraining techniques (such as DINO ) to generalize across stages (of
similar subproblems) for high-resolution images. To pretrain HIPT, first, we leverage DINO to pretrain ViT256-16.
Then, keeping fixed the weights of ViT256-16, we re-use
ViT256-16 as the embedding layer for ViT4096-256 in a
second stage of DINO. We refer to this procedure as hierarchical pretraining, which is similarly performed in the context of learning deep belief networks and hierarchical
transformers for long documents . Though hierarchical
2For rare disease subtypes and clinical trials that study disease progression over the time-course of years, the collection of large patient datasets
is difficult to scale for machine learning application.
pretraining does not reach the slide-level, we show that: 1)
pretrained x4096 representations in self-supervised evaluation are competitive with supervised methods for slide-level
subtyping, and that 2) HIPT with two-stage hiearchical pretraining can reach state-of-the-art performance.
Stage 1: 256 × 256 Patch-Level Pretraining. To pretrain ViT256-16, we use the the DINO framework for
pretraining of x256 patches, in which a student network
ϕs256 is trained to match the probability distribution of
a siamese teacher network ϕt256 using a cross-entropy
loss −pt256(·) log ps256(·) with momentum encoding, with
pt256, ps256 denoting the outputs of ϕt256(·), ϕs256(·) respectively for x256. As data augmentation for each x256 patch,
DINO constructs a set of Ml = 8 local views (x96 crops,
passed through ϕs256) and Mg = 2 global views (x224
crops, passed through ϕt256) to encourage local-to-global
correspondences between the student and teacher, minimizing the function:
pt256(x(i)
224), ps256
An intriguing property that makes this data augmentation
suitable for histology data is again the natural part-whole hierarchy of cells in a tissue patch. In comparison to natural
images in which [96 × 96] crops may capture only colors
and textures without any semantic information, at 20×, local [96 × 96] crops would capture the context of multiple
cells and their surrounding extracellular matrices, which has
shared mutual information with the broader cellular communities. Similar to the original DINO implementation, we
use horizontal flips and color jittering for all views, with
solarizing performed on one of the global views.
Stage 2: 4096 × 4096 Region-Level Pretraining. With
the sequence lengths and computational complexity in tokenizing x4096 regions similar to that of x256 patches, we
can also borrow an almost identical DINO recipe in also
pretraining ViT4096-256 and defining student-teacher networks ϕs4096(·), ϕt4096(·) at this stage. Following extracting
[CLS]256 tokens from ViT256-16 as input for ViT4096-256
input, we rearrange {[CLS](j)
as a 16 × 16 × 384
2D feature grid for data augmentations, performing [6 ×
6], [14 × 14] local-global crops in matching the scale of
[96 × 96], [224 × 224] crops for 256 × 256 inputs.
additional data augmentation, We apply standard dropout
(p = 0.10) to all views following work in Gao et al. .
4. Experiments
Pretraining:.
We pretrain ViT256-16 and ViT4096-256
in different stages, using 10,678 FFPE (formalin-fixed,
BRCA Subtyping
NSCLC Subtyping
RCC Subtyping
Architecture
25% Training
100% Training
25% Training
100% Training
25% Training
100% Training
0.673 ± 0.112
0.778 ± 0.091
0.857 ± 0.059
0.892 ± 0.042
0.904 ± 0.055
0.959 ± 0.015
CLAM-SB 
0.796 ± 0.063
0.858 ± 0.067
0.852 ± 0.034
0.928 ± 0.021
0.957 ± 0.012
0.973 ± 0.017
DeepAttnMISL 
0.685 ± 0.110
0.784 ± 0.061
0.663 ± 0.077
0.778 ± 0.045
0.904 ± 0.024
0.943 ± 0.016
GCN-MIL 
0.727 ± 0.076
0.840 ± 0.073
0.748 ± 0.050
0.831 ± 0.034
0.923 ± 0.012
0.957 ± 0.012
DS-MIL 
0.760 ± 0.088
0.838 ± 0.074
0.787 ± 0.073
0.920 ± 0.024
0.949 ± 0.028
0.971 ± 0.016
0.821 ± 0.069
0.874 ± 0.060
0.923 ± 0.020
0.952 ± 0.021
0.974 ± 0.012
0.980 ± 0.013
ResNet-50IN (Mean)
0.638 ± 0.089
0.667 ± 0.070
0.696 ± 0.055
0.794 ± 0.035
0.862 ± 0.030
0.951 ± 0.016
ViT256-16 (Mean)
0.605 ± 0.092
0.725 ± 0.083
0.622 ± 0.067
0.742 ± 0.045
0.848 ± 0.032
0.899 ± 0.027
ViT4096-256 (Mean)
0.682 ± 0.055
0.775 ± 0.042
0.773 ± 0.048
0.889 ± 0.027
0.916 ± 0.022
0.974 ± 0.016
Table 1. Slide-Level Classification. Top Row. Ablation study assessing 10-fold cross-validated AUC performance of HIPT across other
weakly-supervised architectures. For RCC subtyping, we report the macro-averaged AUC performance across the three subtypes. Bottom
Row. Ablation study assessing K-Nearest Neighbors (KNN) performance using the average pre-extracted embeddings.
paraffin-embedded) H&E-stained diagnostic slides from 33
cancer types in the The Genome Cancer Atlas (TCGA), and
extracted 408,218 x4096 regions at an 20× objective (M ≈
38 regions per slide) for pretraining ViT4096-256, with a
total of 104M x256 patches for pretraining ViT256-16 .
For ViT256-16, we trained for 400,000 iterations using the
AdamW optimizer with a batch size of 256, base learning rate of 0.0005, with the first 10 epochs used to warm
up to the base learning rate followed by decay using a
cosine schedule. A similar implementation was used for
ViT4096-256, with the model trained for 200,000 iterations
using the pre-extracted [CLS] tokens from ViT256-16.
Fine-tuning:.
Following hierarchical pretraining, we
use the pretrained weights to initialize (and freeze) the
ViT256-16 and ViT4096-256 subnetworks, with only a
lightweight ViTWSI-4096 finetuned.
Our work can be
viewed as a formulation of MIL that pretrains not only the
[256 × 256] instance-level feature extraction step, but also
the downstream aggregation layers which extract coarsegrained morphological features. We finetuned HIPT (and
its comparisons) for 20 epochs using the Adam optimizer,
batch size of 1 with 32 gradient accumulation steps, and a
learning rate of 0.01. For survival prediction, we used the
survival cross-entropy loss by Zadeh & Schmidt .
Tasks & Comparisons:. We experiment on several slidelevel classification and survival outcome prediction tasks
across different organ types in the TCGA . In comparisons with state-of-the-art weakly-supervised architectures, we tested Attention-Based MIL (ABMIL), and it’s
variants that use clustering losses (CLAM-SB), clustering
prototypes (DeepAttnMISL), modified scoring & pooling
functions (DS-MIL), and graph message passing (GCN-
MIL), which used the same hyperparameters as HIPT. Since
these methods are agnostic of input features, all comparisons used the pretrained ViT256-16 as instance-level feature extraction. In addition, we also compared variations
of HIPT without pretraining and self-attention. Finally, we
qualitatively study the attention maps that hierarchical selfsupervised ViTs learn in computational histopathology.
4.1. Slide-Level Classification
Dataset Description. We follow the study design in ;
we examined the following tasks evaluated using a 10fold cross-validated AUC: 1) Invasive Ductal (IDC) versus Invasive Lobular Carcinoma (ILC) in Invasive Breast
Carcinoma (BRCA) subtyping, 2) Lung Adenocarcinoma
(LUAD) versus Lung Squamous Cell Carcinoma (LUSC)
in Non-Small Cell Lung Carcinoma (NSCLC) subtyping,
and 3) Clear Cell, Papillary, and Chromophobe Renal Cell
Carcinoma (CCRCC vs. PRCC vs. CHRCC) subtyping,
with all methods finetuned (for 20 epochs) with varying
percentange folds of training data (100% / 25%) as data efficiency experiments. Despite RCC subtyping being a relative easy slide-level task due to having distinct subtypes,
we ultimately include this task as a benchmark for selfsupervised comparisons.
Weakly-Supervised Comparison.
Classification results
are summarized in Table 1. Overall, across all tasks and
different percentage folds, HIPT consistently achieves the
highest macro-averaged AUC performance across all tasks.
In comparison with the best performing baseline, CLAM-
SB, HIPT achieves a performance increase of 1.86%,
2.59%, 0.72% on BRCA, NSCLC and RCC subtyping respectively using 100% of training data, with the margin in
performance increase widening to 3.14%, 8.33%, 1.78% respectively using 25% of training data. Similar performance
increases are demonstrated on other tasks. HIPT demonstrates the most robust performance when limiting training
data, with AUC decreasing slightly from 0.980 to 0.974.
K-Nearest Neighbor (KNN). We take the mean embedding
Architecture
ABMIL 
0.487 ± 0.079
0.566 ± 0.075
0.561 ± 0.074
0.671 ± 0.076
0.584 ± 0.054
0.562 ± 0.049
DeepAttnMISL 
0.472 ± 0.023
0.561 ± 0.088
0.521 ± 0.084
0.472 ± 0.162
0.563 ± 0.037
0.563 ± 0.067
GCN-MIL 
0.534 ± 0.060
0.538 ± 0.049
0.591 ± 0.093
0.636 ± 0.066
0.592 ± 0.070
0.513 ± 0.069
DS-MIL 
0.472 ± 0.020
0.470 ± 0.053
0.548 ± 0.057
0.654 ± 0.134
0.537 ± 0.061
0.546 ± 0.047
0.634 ± 0.050
0.608 ± 0.088
0.642 ± 0.028
0.670 ± 0.065
0.538 ± 0.044
0.570 ± 0.081
Table 2. Survival Prediction. Ablation study assessing cross-validated c-Index of HIPT across other weakly-supervised architectures.
of the pre-extracted embeddings, followed by a KNN evaluation for the above tasks. As a baseline, we use a ResNet-
50 pretrained on ImageNet to extract patch-level embeddings.
We compare with pre-extracted ViT256-16 patch
embeddings from DINO pretraining, and pre-extracted
ViT4096-256 region-level embeddings from hierarchical
pretraining, with results summarized also in Table 1. In
using the average embedding of each WSI as the “slidelevel representation”, we find that ViT4096-256 region-level
embeddings in HIPT outperform patch-level embeddings
across all tasks, which can be attributed to the broader
image contexts used in the WSI for pretraining, and can
be intuitively viewed as a closer proxy to the slide-level
view than small patches. ViT4096-256 region-level embeddings surpass the AUC performance of weakly-supervised
approaches in BRCA and RCC subtyping using 100% of
training data.
4.2. Survival Prediction
Dataset Description. For survival outcome prediction, we
validated on the IDC, CCRCC, PRCC, and LUAD cancer
types which have relatively large sample sizes in the TCGA,
in addition to Colon & Rectal (CRC) and Stomach Adenocarcinoma (STAD) which have been frequently evaluated
in real-world clinical studies due to their substantial human
intra-observer variability . All tasks were evaluated using cross-validated concordance index (c-Index).
Weakly-Supervised Comparison. For the following survival prediction tasks in which learning context-aware relationships are important, we observe much larger increases
in performance, summarized in Table 2.
Overall, HIPT
achieves the best c-Index performance in the IDC, COAD-
READ, CCRCC, and STAD cancer types, with the largest
improvement demonstrated in IDC (0.634) and COAD-
READ (0.608) in comparison to other methods. Though
other methods such as GCN-MIL use message passing for
learning context-aware features, we note that the number of
layers needed to achieve similar image receptive fields may
cause the number of neighbors to grow exponentially .
In modeling important long-range dependencies between
instances using self-attention across various stages of the hierarchy, the Transformer attention in HIPT is able to capture
regional perturbations that have been well characterized as
portending worse outcome across different cancer types, as
further visualized in Figure 3, 4 .
4.3. Self-Supervised ViTs Find Unique Morphological Phenotypes
ViT256-16 Attention Maps. For x256 patches, we visualize
the different attention heads in MHSA and reveal that ViTs
in pathology are able to isolate distinct morphological features. From visual assessment by a board-certified pathologist across several different cancer types, we observe
that MHSA in ViT256-16(n = 8, h = 6, d = 384) captures
three distinct fine-grained morphological phenotypes as illustrated in Figure 3, with general stroma tissue and red
blood cells attended in h = 1, 2, cells (normal, atypical,
lymphocyte) attended in h = 3, 4, and “white spaces” (luminal spaces, fat regions, air pockets) attended in h = 5, 6.
This observation is in line with current studies that have
introspected self-supervised ViT models, in which the attention heads can be used as a method for object localization or discovery . In the application to histopathology tissue, our introspection reveals that the visual tokens at
the [16 × 16] cell-level directly corroborate with semantic,
object-centric structures at the 20× objective.
ViT4096-256 Attention Maps. For x4096 regions, we further visualize the attention heads in MHSA from our pretrained ViT4096-256(n = 4, h = 6, d = 192) model, capturing two distinct coarse-grained phenotypes:
tumorstroma interface attended in h = 1, 2, 3, and nested tumor
cells and other high tumor cellularity regions in h = 4, 5, 6.
In comparison with the ViT256-16 attention maps which
may capture only nuclear features (e.g. - nuclear atypia,
shape and size of cells), ViT4096-256 attention maps are
able to model the patterns of nested tumor growth, tumor
invasion into fat and stroma regions, and other tissue-totissue relationships (Figure 3). In factorizing the attention
distribution of [16 × 16] cells from ViT256-16 onto highlyattended [256 × 256] patches from ViT4096-256, we can
create a hierarchical attention map, which is able to distinguish tumor cells in stroma tissue from tumor cells in dense
tumor cellularity regions (Figure 4). Overall, these captured coarse- and fine-grained morphological features corroborate with the observed performance increases in both
finetuning HIPT in weakly-supervised learning and using
averaged HIPT features in KNN evaluation. Additional vi-
Figure 4. Hierarchical Attention Maps in HIPT. For Colorectal Cancer (CRC), we observe similar delineation of stroma, cells, and
”white space” presence in ViT256-16, and localizing tumor invasion into stroma and muscle (A) and poorly-differentiated glands (B) from
ViT4096-256. In factorizing these attention distributions together, we develop hierarchical attention visualizations which can visualize
tumor cells with associated stromal tissue and high tumor cellularity regions containing poorly-differentiated glands.
sualizations are found in the Supplement.
4.4. Further Ablation Experiments
Additional experiments are included in the Supplementary Materials, with main findings highlighted below:
The role of pretraining.
Hierarchical pretraining of
ViT4096-256 is an important component in our method, as
HIPT variants without pretraining overfit in MIL tasks.
Comparing patch-level representations.
We assessed
quality of other embedding types, and found that ViT256-16
achieves strong representation quality of image patches.
Organ-specific versus pan-cancer pretraining. We additionally assessed the performance of ViT256-16 pretraining
on different data distributions, with improved performance
in cell localization with pan-cancer pretraining.
5. Conclusion
We believe our work is an important step towards selfsupervised slide-level representation learning, demonstrating pretrained and finetuned HIPT features achieve superior performance on weakly-supervised and KNN evaluation respectively. Though DINO was used for hierarchical
pretraining with conventional ViT blocks, we hope to explore other pretraining methods such as mask patch prediction and efficient ViT architectures .
Limitations: A limitation of HIPT is the difficulty in pretraining the last aggregation layer due to the small number
of WSI data points. In addition, end-to-end hierarchical pretraining of HIPT is computationally intractable on commercial workstations, with pretraining needed to be performed
in stages. Lastly, the study design of this work has several
constraints, such as: 1) excluded slides in each TCGA cohort due to limited tissue content and difficulty patching at
[4096 × 4096], 2) ViT256-16 pretraining performed on almost all of TCGA and evaluation lacking independent test
cohorts, 3), analysis limited to TCGA, which overrepresents
patients with European ancestry and not representative of
the rich genetic diversity in the world .
Broader Impacts:
Many problems in biology and
medicine have hierarchical-like relationships .
For instances, DNA motifs within exon sequences which
contributes towards protein structure, gene expression, and
genetic traits . Our idea of pretraining neural networks based on hierarchical relationships in large, heterogeneous data modalities to derive a patient- or populationlevel representation can be extended to other domains.
6. Acknowledgements
We thank Felix Yu, Ming Y. Lu, Chunyuan Li, and the
BioML group at Microsoft Research New England for their
feedback. This work was supported in part by the BWH
president’s fund, BWH & MGH Pathology, Google Cloud
Research Award, and NIGMS R35GM138216 (F.M.).
R.J.C. was also supported by the NSF Graduate Fellowship. T.Y.C. was also supported by the NIH T32CA251062.
R.G.K. gratefully acknowledges funding from CIFAR.