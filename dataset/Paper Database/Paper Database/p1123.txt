Deep Autoencoding Models for Unsupervised
Anomaly Segmentation in Brain MR Images
Christoph Baur1, Benedikt Wiestler3, Shadi Albarqouni1, and Nassir Navab1,2
1 Computer Aided Medical Procedures (CAMP), TU Munich, Germany
2 Whiting School of Engineering, Johns Hopkins University, Baltimore, United States
3 Neuroradiology Department, Klinikum Rechts der Isar, TU Munich, Germany
Abstract. Reliably modeling normality and diﬀerentiating abnormal
appearances from normal cases is a very appealing approach for detecting
pathologies in medical images. A plethora of such unsupervised anomaly
detection approaches has been made in the medical domain, based on
statistical methods, content-based retrieval, clustering and recently also
deep learning. Previous approaches towards deep unsupervised anomaly
detection model patches of normal anatomy with variants of Autoencoders or GANs, and detect anomalies either as outliers in the learned
feature space or from large reconstruction errors. In contrast to these
patch-based approaches, we show that deep spatial autoencoding models can be eﬃciently used to capture normal anatomical variability of
entire 2D brain MR images. A variety of experiments on real MR data
containing MS lesions corroborates our hypothesis that we can detect
and even delineate anomalies in brain MR images by simply comparing
input images to their reconstruction. Results show that constraints on
the latent space and adversarial training can further improve the segmentation performance over standard deep representation learning.
Introduction
Brain MR images are frequently acquired for detecting and diagnosing pathologies, monitoring disease progression and treatment planning. The manual identiﬁcation and segmentation of pathologies in brain MR data is a tedious and
time-consuming task. In an attempt to aid the detection and delineation of brain
lesions arising from Multiple Sclerosis (MS), tumors or ischemias, the medical
image analysis community has proposed a great variety of methods. Outstanding
levels of performance have been achieved with recent supervised deep learning
methods. However, their training requires vast amounts of labeled data which
often is not available. Further, these approaches suﬀer from limited generalization since in general, training data rarely comprises the gamut of all possible
pathological appearances . Given the constrained anatomical variability of
the healthy brain, an alternative approach is to model the distribution of healthy
brains, and both detect and delineate pathologies as deviations from the norm.
Here, we formulate the problem of brain lesion detection and delineation as an
unsupervised anomaly detection (UAD) task based on state-of-the-art deep representation learning and adversarial training, requiring only a set of normal data
 
Baur et al.
Fig. 1. The proposed anomaly detection oncept at a glance. A simple subtraction of
the reconstructed image from the input reveals lesions in the brain.
and no labels at all. The detection and delineation of pathologies are thereby
obtained from a pixel-wise reconstruction error (Fig. 1). To the best of our knowledge, this is the ﬁrst application of deep convolutional representation learning
for UAD in brain MR images which operates on entire MR slices.
Related work In the medical ﬁeld, many eﬀorts have been made towards
UAD, which can be grouped into methods based on statistical modeling, contentbased retrieval or clustering and outlier detection . Weiss et al. employed Dictionary Learning and Sparse Coding to learn a representation of normal brain patches in order to detect MS lesions. Other unsupervised MS lesion
segmentation methods rely on thresholding and 3D connected component analysis or fuzzy c-means clustering with topology constraints . Notably, only
few approaches have been made towards deep learning based UAD. Vaidhya
et al. utilized unsupervised 3D Stacked Denoising Autoencoders for patchbased glioma detection and segmentation in brain MR images, however only as
a pretraining step for a supervised model. Recently, Schlegl et al. presented
the AnoGAN framework, in which they create a rich generative model of normal
retinal OCT patches using a GAN. Assuming that the model cannot properly
reconstruct abnormal samples, they classify query patches as either anomalous
or normal by trying to optimize the latent code based on a novel mapping score,
eﬀectively also leading to a delineation of the anomalous region in the input data.
In earlier work, Seeb¨ock et al. trained an Autoencoder and utilized a oneclass SVM on the compressed latent space to distinguish between normal and
anomalous OCT patches. A plethora of work in the ﬁeld of deep learning based
UAD has been devoted to videos primarily based on Autoencoders (AEs) due
to their ability to express non-linear transformations and the ability to detect
anomalies directly from poor reconstructions of input data . Very recently,
ﬁrst attempts have also been made with deep generative models such as Variational Autoencoders (VAEs), however limited to dense neural networks and
1D data. Noteworthy, most of this work focused on the detection rather than
the delineation of anomalies.
A major advantage of AEs is their ability to reconstruct images with fairly
high resolution thanks to a supervised training signal coming from the reconstruction objective. Unfortunately, they suﬀer from memorization and tend to
Deep Unsupervised Anomaly Detection
Fig. 2. An overview of our AnoVAEGAN
produce blurry images. GANs have shown to produce very sharp images due
to adversarial training, however the training is very unstable and the generative
process is prone to collapse to a few single samples. The recent formulation of
VAEs has also shown that AEs can be turned into generative models which can
mimic data distributions, and both concepts have also been combined into the
VAEGAN , yielding a framework with the best of both worlds.
Contribution Inarguably, AnoGAN is a great concept for UAD in patchbased and small resolution scenarios, but as our experiments show, GANs lack
the capability to reliably synthesize complex, high resolution brain MR images.
Further, the approach requires a time-consuming iterative optimization. To overcome these issues, we propose AnoVAEGAN: We leverage a deep generative
model in the form of spatial VAEs to build a model that captures “global” normal
anatomical appearance rather than the variety of local patches. The reconstruction objective allows to train a generative model on complex, high resolution
data such as brain MR slices. In order to avoid the memorization pitfalls of
AEs and to improve realism of the reconstructed samples, we train the decoder
part of the network with the help of an adversarial network, ultimately turning
the model into a VAEGAN . In our experiments, we rank the AnoVAEGAN
against the AnoGAN framework as well as both dense and spatial variants of
the VAE, AE and Context Encoders (here referred to as “AE-GAN”) in the
tasks of unsupervised MS lesion delineation and report signiﬁcant improvements
of spatial autoencoding models over traditional ones.
Methodology
In this work, we employ deep generative representation learning to model the distribution of the healthy brain, which should enable the model to fully reconstruct
healthy brain anatomy while failing to reconstruct anomalous lesions in images
of a diseased brain. Therefore, we utilize an adaptation of the VAEGAN to
establish a parametric mapping from input images x ∈RH×W to a lower dimensional representation z ∈Rd and back to high quality image reconstructions
ˆx ∈RH×W using an encoder Enc(·; θ) and a decoder Dec(·; φ):
z ∼Enc(x; θ),
ˆx = Dec(z; φ),
z ∼N(0, I)
Baur et al.
Like in , the latent space z is constrained to follow a multivariate normal
distribution (MVN) N(0, I), which we leverage for encoding images of normal
brain anatomy. Further, we employ a discriminator network Dis(·) which classi-
ﬁes its input as either real or reconstructed.
Training. We optimize the framework using two loss functions in an alternating
fashion. The VAE component of the model is optimized using
LV AE = λ1Lrec + λ2Lprior + λ3Ladv
= λ1∥x −ˆx∥1 + λ2DKL(z||N(0, I)) −λ3 log(Dis(Dec(Enc(x)))),
, while the discriminator is trained as commonly seen in the GAN framework :
LDis = −log(Dis(x)) −log(1 −Dis(Dec(Enc(z)))),
Originally, VAEGAN used an abstract reconstruction loss in the latent space
of the discriminator rather than a pixelwise reconstruction objective, which was
not helpful for our purpose. For Lrec, we thus used the pixelwise ℓ1-distance
between input image and reconstruction. Lprior is the KL-Divergence between
the distribution of generated z and a MVN, which is only used to regularize the
weights θ of the encoder. The third part Ladv is the adversarial loss which forces
the decoder to generate images that are likely to fool the discriminator in its
task to distinguish between real and reconstructed images.
A peculiarity of our approach is the fully convolutional encoder-decoder architecture which we use in order to preserve spatial information in the latent
space, i.e. z ∈Rh×w×c is a multidimensional tensor. Fig. 2 shows our AnoVAE-
GAN, and a depiction of diﬀerent AE architectures is given in Fig. 3.
Anomaly Detection Anomalies are detected and delineated by 1) computing
the pixelwise ℓ1-distance between an input image and its reconstruction and 2)
thresholding the resulting residual image to obtain a binary segmentation.
Experiments and Results
Given the variants of AE and our proposed framework, we investigate i) whether
autoencoding deep networks can be utilized in general to learn to reconstruct
complex brain MR images, ii) how the dimensionality of z aﬀects the reconstruction capabilities of a model, iii) the eﬀect of constraining z to be well structured
and iv) if adversarial training enhances the quality of reconstructed images. In
the following paragraphs we ﬁrst introduce the dataset, provide implementational details and then describe the conducted experiments.
Deep Unsupervised Anomaly Detection
(a) Dense Autoencoder dAE
(b) Spatial Autoencoder sAE
(c) Dense Variational Autoencoder dVAE (d) Spatial Variational Autoencoder sVAE
Fig. 3. An overview of diﬀerent Autoencoder frameworks
Datasets. For our experiments, we use an inhouse dataset which provides a rich
variety of images of healthy brain anatomy - a neccessity for our approach. The
dataset consists of FLAIR and T1 image pairs from 83 patients (1360 slices)
with healthy brains and 49 patients with MS lesions (980 slices). All images
have been acquired with a Philips Achieva 3T scanner. To reduce variability and
relax the reconstruction problem, all images have been rigidly co-registered to the
SRI24 ATLAS . Further, the skull has been stripped with ROBEX . The
resulting images have been denoised using CurvatureFlow and normalized
into the range . From every patient, we extracted 20 consecutive axial slices
of resolution 256 × 256px around the midline.
Fig. 4. Realistic (left) and
unrealistic (right) samples
generated with AnoGAN.
Implementation. We build upon the basic architecture proposed in and perform only minor modiﬁcations aﬀecting the latent space (see Table 1). Across
diﬀerent architectures we keep the model complexity
of the encoder-decoder part the same to allow for a
valid comparison. All models have been trained for
150 epochs in minibatches of size 8, using a learning rate of 0.001 for the reconstruction objective and
0.0001 for the adversarial training on a single nVidia
1080Ti GPU with 8GB of memory.
Evaluation Metrics. We measure the performance of the diﬀerent models by the
mean and standard deviation of the Dice-Score across diﬀerent testing patients
as well as the average time required for reconstructing and segmenting a sample.
Anomaly Detection
We ﬁrst trained normal convolutional AE & VAE with a dense latent space of
dimensionality 512 and found that, besides not being capable of reconstructing
Baur et al.
Fig. 5. 1st Column: a selected axial slice and its ground-truth segmentation; Succeeding
columns show the ﬁltered diﬀerence images (top row) and the resulting segmentation
augmented to the input image (bottom row) for the following models (in order): dAE,
sAE3, sAE-GAN, sVAE and sVAE-GAN.
brain lesions, they also lack the capability to reconstruct ﬁne details such as the
brain convolutions (Fig. 5). Similar to , we then make the architecture fully
convolutional to ensure that spatial information is not lost in the bottleneck
of the model. Notably, this heavily increases the dimensionality of z. We thus
vary the number of featuremaps of the spatial AE to investigate the impact on
reconstruction quality of normal and anomalous samples. We identify z = 16 ×
16×64 as a good parameterization and use it in further experiments on a spatial
VAE, a spatial AE-GAN and our AnoVAEGAN. Further, we also trained an
AnoGAN on the same set of normal axial slices for 150 epochs. After approx. 82
epochs of training, we obtained realistically looking images, however continuation
of the training led to instabilities which resulted in unrealistic samples (Fig.
4). Thus, we evaluated the AnoGAN approach after 82 epochs. The required
iterative reconstruction of testing samples was computed in 100 steps.
AnoVAEGAN.
Postprocessing After reconstruction of all the slices,
we apply some postprocessing steps to reduce the
number of False Positives. For every patient, we apply
a 5 × 5 × 5 median ﬁlter to the reconstructed subvolumes to ﬁlter out small residuals, usually belonging to
brain convolutions. Further, we multiply the residuals
with slightly eroded brain masks, threshold the resulting volumes to obtain a binary segmentation mask and
remove tiny 3D connected components with an area
less than 6 voxels as they are unlikely to constitute lesions. The threshold is model speciﬁc and determined
as the 98th percentile of the models reconstruction
errors on the training dataset. We chose this percentile empirically from the histogram of residuals obtained from both normal and abnormal data (Fig. 6). The
Deep Unsupervised Anomaly Detection
Table 1. Results of our experiments on unsupervised MS lesion segmentation. We
report the Dice-Score (mean and std. deviation across patients) as well as the avg.
reconstruction time per sample in seconds. Preﬁxes d or s stand for dense or spatial.
MODELTYPE z
DICE (µ ± σ)
avg. Reco.-time [s]
0.12764 ± 0.14617
8 × 8 × 64
0.19739 ± 0.19061
16 × 16 × 64 0.58558 ± 0.19845
sAE-GAN 
16 × 16 × 64 0.52636 ± 0.19780
0.16619 ± 0.17790
16 × 16 × 64 0.59227 ± 0.19585
AnoVAEGAN 16 × 16 × 64 0.60508 ± 0.19277 0.01541
AnoGAN 
0.37489 ± 0.21926
performance of each model is reported in Table 1. A comparison of processed
residual images and ﬁnal segmentations of various models can be seen in Fig. 5.
The highest average Dice-score for MS lesion segmentation has been obtained
with our AnoVAEGAN. The spatial VAEs and AEs which do not leverage adversarial training produce only slightly inferior scores, however. All spatial autoencoding models signiﬁcantly outperform the ones with a dense bottleneck and,
except for sAE1, also the AnoGAN. Interestingly, the spatial AE with adversarial training performs worse than its generative counterpart and the other spatial
AEs with the same bottleneck resolution.
Discussion and Conclusion
Our experiments show that AE & VAE models with dense bottlenecks cannot
reconstruct anomalies, but at the same time lack the capability to reconstruct important ﬁne details in brain MR images such as brain convolutions. By utilizing
spatial AEs with suﬃcient bottleneck resolution, i.e. 16×16px sized featuremaps,
we can mitigate this problem. Noteworthy, a smaller bottleneck resolution of
8 × 8px seems to lead to a severe information loss and thus to large reconstruction errors in general. By further constraining the latent space to follow a MVN
distribution and introducing adversarial training, we notice marginal improvements over the non-generative models. As expected, spatial autoencoding clearly
outperforms the AnoGAN and is considerably faster. While AnoGAN requires
an iterative optimization, which consumes ∼19 seconds for a single reconstruction, all of the AE models require only a fraction of a second. Interestingly, even
though the models operate on 2D data, the segmentations seem very consistent
among neighboring axial slices.
In summary, we presented a novel and fast UAD approach based on generative deep representation learning which encodes the full context of brain MR
slices. We believe that the approach does not only open up opportunities for
unsupervised brain lesion segmentation, but can also act as prior information
for supervised deep learning.
Baur et al.
In future work, we also aim to investigate the projection of healthy anatomy
into a latent space which follows a Gaussian Mixture Model rather than a single
multivariate normal distribution, and intend to utilize 3D autoencoding models
for unsupervised anomaly detection.
Acknowledgements
We thank our clinical partners from Klinikum Rechts der Isar for providing us
with their broad dataset of patients with healthy anatomy as well as the MS
lesion dataset.