ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
Informed Democracy: Voting-based Novelty
Detection for Action Recognition
Alina Roitberg*
 
Ziad Al-Halah*
 
Rainer Stiefelhagen
 
Karlsruhe Institute of Technology,
76131 Karlsruhe,
Novelty detection is crucial for real-life applications. While it is common in activity
recognition to assume a closed-set setting, i.e. test samples are always of training categories, this assumption is impractical in a real-world scenario. Test samples can be of
various categories including those never seen before during training. Thus, being able to
know what we know and what we don’t know is decisive for the model to avoid what can
be catastrophic consequences. We present in this work a novel approach for identifying
samples of activity classes that are not previously seen by the classiﬁer. Our model employs a voting-based scheme that leverages the estimated uncertainty of the individual
classiﬁers in their predictions to measure the novelty of a new input sample. Furthermore, the voting is privileged to a subset of informed classiﬁers that can best estimate
whether a sample is novel or not when it is classiﬁed to a certain known category. In a
thorough evaluation on UCF-101 and HMDB-51, we show that our model consistently
outperforms state-of-the-art in novelty detection. Additionally, by combining our model
with off-the-shelf zero-shot learning (ZSL) approaches, our model leads to a signiﬁcant
improvement in action classiﬁcation accuracy for the generalized ZSL setting.
Introduction
Human activity recognition from video is a very active research ﬁeld, with a long list of
potential application domains, ranging from autonomous driving to security surveillance . However, the vast majority of published approaches are developed under the assumption
that all categories are known a priori . This closed set constraint represents a signiﬁcant bottleneck in the real world, where the system will probably encounter
samples from various categories including those never seen during development. The set of
possible actions is dynamic by its nature, possibly changing over time. Hence, collecting
and maintaining large scale application-speciﬁc datasets of video data is especially costly
and impractical. This raises a crucial need for the developed models to be able to identify
cases where they are faced with samples out of their knowledge domain. In this work, we
* Equal contribution
c⃝2018. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.
 
ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
explore the ﬁeld of activity recognition under open set conditions , a setting which
has been little-explored before especially in the action recognition domain .
In an open world application scenario, an action recognition model should be able to
handle three different tasks: 1) the standard classiﬁcation of previously seen categories; 2)
knowledge transfer for generalization to new unseen classes (e.g. through zero-shot learning); 3) and knowing how to automatically discriminate between those two cases. The third
component of an open set model lies in its ability to identify samples from unseen classes
(novelty detection). This is closely linked to the classiﬁer’s conﬁdence in its own predictions,
i.e. how can we build models, that know, what they do not know? A straight-forward way
is to employ the Softmax output of a neural network (NN) model as the basis for a rejection
threshold . Traditionally, action recognition algorithms focus on maximizing the
top-1 performance on a static set of actions. Such optimization leads to Softmax scores of
the winning class being strongly biased towards very high values . While giving excellent results in closed set classiﬁcation, such overly self-conﬁdent models become a
burden under open set conditions. A better way to asses NN’s conﬁdence, is to rather predict
the probability distribution with Bayesian neural networks (BNN). Recently, Gal et al. 
introduced a way of efﬁciently approximating BNN modeled as a Gaussian Process and
using dropout-based Monte-Carlo sampling (MC-Dropout) . We leverage the ﬁndings of
 and exploit the predictive uncertainty in order to identify activities of previously unseen
This work aims at bringing conventional activity recognition to a setting where new
categories might occur at any time and has the following main contributions: 1) We present
a new model for novelty detection for action recognition based on the predictive uncertainty
of the classiﬁers. Our main idea is to estimate the novelty of a new sample based on the
uncertainty of a selected group of output classiﬁers in a voting-like manner. The choice of
the voting classiﬁers depends on how conﬁdent they are in relation to the currently predicted
class. 2) We adapt zero-shot action recognition models, which are conventionally applied
solely on samples of the unseen classes, to the generalized case (i.e. open set scenario) where
a test sample may originate from either known or novel categories. We present a generic
framework for generalized zero-shot action recognition, where our novelty detection model
serves as a ﬁlter to distinguish between seen and novel categories, passing the sample either
to a standard classiﬁer or a zero-shot model accordingly. 3) We extend the custom evaluation
setup for action recognition to the open-set scenario and formalize the evaluation protocol
for the tasks of novelty detection and zero-shot action recognition in the generalized case
on two well-established datasets, UCF-101 and HMDB-51 . The evaluation shows,
that our model consistently outperforms conventional NNs and other baseline methods in
identifying novel activities and was highly successful when applied to generalized zero-shot
Related Work
Novelty Detection
Various machine learning methods have been used for quantifying the
normality of a data sample. An overview of the existing approaches is provided by . A lot of today’s novelty detection research is handled from the probabilistic point of
view , modeling the probability density function (PDF) of the training data,
with Gaussian Mixture Models (GMM) being a popular choice . The One-class SVM
introduced by Schölkopf et al. is another widely used unsupervised method for novelty
ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
detection, mapping the training data into the feature space and maximizing the margin of
separation from the origin. Anomaly detection with NNs has been addressed several times
using encoder-decoder-like architectures and the reconstruction error . A common way
for anomaly detection is to threshold the output of the neuron with the highest value . Recently, Hendrycks et al. presented a baseline for deep-learning based visual
recognition using the top-1 Softmax scores and pointed out, that this area is under-researched
in computer vision.
The research of novelty detection in videos has been very limited.
A related topic
of anomaly detection has been studied for very speciﬁc applications, such as surveillance
 or personal robotics . Surveillance however often has anomalies, such as Robbery or Vandalism, present in the training set in some form which violates our
open-set assumption. The work most similar to ours is the one of Moerland et al. where
Hidden-Markov-Model is used to detect unseen actions from skeleton features. However,
 considers only a simpliﬁed evaluation setting using only a single unseen action category in testing. In contrast to our model is based on a deep neural architecture for
detecting novel actions which makes it applicable to a wide range of modern action recognition models. Furthermore, we consider a challenging evaluation setting on well-established
datasets where novel classes are as diverse as those seen before. Additionally, we go beyond
novelty detection and evaluate how well our model generalizes to classifying novel classes
through zero-shot learning. Our model leverages approximation of BNN using MC-Dropout
as proposed by Gal et al. , which has been successfully applied in semantic segmentation and active learning . We extend the BNN approximation to the context of open
set action recognition where we incorporate the uncertainty of the output neurons in a voting
scheme for novelty detection.
Zero-Shot Action Recognition
Research on human activity recognition under open set
conditions has been sparse so far. A related ﬁeld of Zero-Shot Learning (ZSL) attempts to
classify new actions without any training data by linking visual features and the high-level
semantic descriptions of a class, e.g. through action labels. The description is often represented with word vectors by a skip-gram model (e.g. word2vec ) previously trained on a
large-scale text corpus. ZSL for action recognition gained popularity over the past few years
and has also been improving slowly but steadily . In all of these works,
the categories used for training and testing are disjoint and the method is evaluated on unfamiliar actions only. This is not a realistic scenario, since it requires the knowledge of whether
the activity belongs to a known or novel category a priori. Generalized zero-shot learning
(GZSL) has been recently studied for image recognition and a drastic performance drop of
classical ZSL approaches such as ConSE and Devise , has been reported . As
the main application of our novelty detection approach, we implement a framework for ZSL
in the generalized case and integrate our novelty detection method to distinguish between
known and unknown actions.
Novelty Detection via Informed Voting
We present a new approach for novelty detection in action recognition. That is, given a new
video sample x, our goal is to ﬁnd out whether x is a sample of a previously known category
or if it belongs to a novel action category not seen before during training.
Let A = {A1,...AK} be the set of all K known categories in our dataset. Then p(Ai|x)
is the classiﬁer probability of action category Ai given sample x. Conceptually, our novelty
ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
Training data
Known classes
Known classes
Unseen classes
Predicted distribution mean E(A*|x) of the leading Classifier A*
Predictive Uncertainty
Figure 1: Distribution of predictive mean and uncertainty as a 2-D histogram of the leading
classiﬁer (highest predictive mean) for the input with known and unseen actions (HMDB-51
dataset). Red denotes common cases (high frequency), blue denotes unlikely cases.
detection model is composed of two main components: 1) the leader and 2) the council.
The leader refers to the classiﬁer with the highest conﬁdence score in predicting the class
of a certain sample x. For example, in classiﬁcation neural networks it is common to select
the leader based on the highest softmax prediction score. The leader votes for sample x
being of its own category and assuming that the class of x is one of the known categories,
i.e. class(x) = A∗∈A. The council, on the other hand, is a subset of classiﬁers that will
help us validating the decision of a speciﬁc leader. In other words, the council members of
a leader representing the selected class A∗are a subset of the classiﬁers representing the rest
of the classes, i.e. CA∗⊆A \ {A∗}. These members are elected for each leader individually,
i.e. each category classiﬁer in our model has its own council. A council member is selected
based on its certainty variance in relation to a leader. Whenever a leader decides on the
category of a sample x, its council will convene and vote on the leader decision. Then, the
council members will jointly decide whether the leader made the correct decision or it was
mistaken because the sample is actually from a novel category.
Next, we explain in details how we measure the uncertainty of a classiﬁer (Section 3.1);
choosing a leader and its council members (Section 3.2); and, ﬁnally, the novelty voting
procedure given new sample (Section 3.3).
Measuring Classiﬁer Uncertainty
In this section, we tackle the problem of quantifying the uncertainty of a classiﬁer given a
new sample. The estimated uncertainty is leveraged later by our model to select the council
members as we will see in Section 3.2.
In the context of deep learning, it is common to consider the single point estimates
for each category, represented by the output of the softmax layer, as a conﬁdence measure . However, this practice has been highlighted in literature to be inaccurate
since a model can be highly uncertain even when producing high prediction scores .
Bayesian neural networks (BNNs) offer us an alternative to the point estimate models and
are known to provide a well calibrated estimation of the network uncertainty in its output.
Given the network parameters ω and a training set S, the predictive probability of the BNN
is obtained by integrating over the parameter space. The prediction p(Ai|x,S) is therefore
the mean over all possible parameter combinations weighted by their posterior probability:
p(Ai|x,S) =
ω p(Ai|x,ω)p(ω|S)dω
ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
Leading classifier
Leading classifier
Leading classifier
brush_hair
sword_exercise
brush_hair
sword_exercise
brush_hair
sword_exercise
Expected Predictive Uncertainty
Confident: voting council member
Uncertain: non−voter
Figure 2: Council members and uncertainty statistics for three different leaders (HMDB-51).
The classiﬁer’s average uncertainty and its variance (area surrounding the point) illustrate
how it changes its belief in the leader for different data inputs. Blue points are in the council
of the current leader, while red points are classiﬁers that did not pass the credibility threshold.
However, BNNs are known to have a difﬁcult inference scheme and high computation cost .
Therefore, we leverage the robust model proposed by to approximate the predictive mean
and uncertainty of the BNN posterior distribution with network parameters modeled as a
Gaussian Process (GP). This method is based on dropout regularization , a widely used
technique which has proven to be very effective against overﬁtting. That is, it leverages
the dropout at each layer in the network to draw the weights from a Bernoulli distribution
with probability p. At test time, the dropout is iteratively applied for M forward passes for
each individual sample. Then, the statistics of the neural network output represents a Monte-
Carlo (MC) approximation of the neuron’s posterior distribution This approach is referred to
as MC-Dropout .
Speciﬁcally, let x be a representation generated by a convolutional neural network (CNN)
for an input sample z. We add a feedforward network on top of the CNN with two fullyconnected layers with weight matrices W1 and W2. Instead of using a deterministic Softmax
estimate in a single forward pass as it is common with CNNs, we now compute the mean
over M stochastic iterations as our prediction score:
E(Ai|x) ≈1
softmax(relu(xTD1W1 +b1)D2W2),
where relu(·) is the rectiﬁed linear unit (ReLU) activation function, b1 is the bias vector of
the ﬁrs layer. Additionally, D1 and D2 are diagonal matrices where the diagonal elements
contain binary values, such that they are set to 1 with probability 1−p and otherwise to 0.
We further empirically compute the model’s predictive uncertainty as the distribution
U(Ai|x) ≈s2 =
[softmax(relu(xTD1W1 +b1)D2W2)−E(Ai|x)]2
Fig. 1 shows how predictive mean and uncertainty are distributed for samples of known
and novel classes. The plot depicts clearly different patterns for the resulting probability
distributions in these two cases which illustrates the potential of Bayesian uncertainty for
novelty detection.
Selecting the Leader and its Council
Now that we can estimate the conﬁdence and uncertainty of each category classiﬁer in our
model, we describe in this section how to choose the leader and select it council members.
ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
The Leader.
Rather than selecting the leader using a point estimate based on the softmax
scores of the output layer, we leverage here the more stable dropout-based estimation of
the prediction mean. Hence, the leader is selected as the classiﬁer with highest expected
prediction score over M sampling iterations:
A∗= argmax
where E(Ai|x) is estimated according to Eq. 2.
The Council.
The leader by itself can sometimes produce highly conﬁdent predictions for
samples of unseen categories . Hence, we can not rely solely on the leader conﬁdence to
estimate whether a sample is of a novel category or not. Here, the rest of the classiﬁers can
help in checking the validity of the leader’s decision. We notice that these classiﬁer exhibit
unique patterns in regard to a certain leader. They can be grouped into two main groups:
the ﬁrst shows high uncertainty when the leader is correctly classifying a sample; while the
second shows a very low uncertainty and are in agreement with the leader.
Guided by this observation, we select the members of the Council C∗
A for a certain
leader A∗based on their uncertainty variance in regards to samples of the leader’s category, i.e. x ∈A∗. In other words, those classiﬁers that exhibit very low uncertainty when the
leader is classifying samples of its own category are elected to join its council. During the
training phase, we can select the council members for each classiﬁer in our model. Here, we
randomly split the initial set into a training set Strain which is used for model optimization
and parameter estimation, and a holdout set Sholdout which is used for choosing the council
member for all the classiﬁers iteratively. Speciﬁcally, we use a 9/1 split for the training and
the holdout splits. We ﬁrst estimate the parameters of our deep model ω using Strain. Then,
we evaluate our model over all samples from Sholdout. For each category classiﬁer in our
model, we construct a set of true positive samples SAi
tp ⊆Sholdout. For each sample xn ∈SAi
we estimate the uncertainty U(A j|xn) of the rest of the classiﬁers Aj ∈A \ {Ai} using the
MC-Dropout approach. Then, the variance of these classiﬁers’ uncertainty is estimated as:
Var(A j|Ai) = 1
(U(A j|xn)−E[U(Aj|x)])2
where N = |SAi
tp| and E[U(A j|x)] is the expectation of the uncertainty of the classiﬁer Aj over
samples x ∈SAi
tp. Finally, classiﬁers with a variance lower than a ﬁxed credibility threshold
Var(A j|Ai) < c are then elected as members of Ai council.
Fig. 2 shows three leaders and their elected councils according to our approach. We see,
for example, that eight classiﬁers did not pass the credibility threshold for the leader drink
and were excluded from its council. The variance of the uncertainty is especially high for sit
and eat in this case. This is expected since those actions often occur in a similar context.
Voting for Novelty
Given the trained deep model and the sets of all council members from the previous step,
we can now generate a novelty score for a new sample x as follows. First, we calculate
the prediction mean E[p(Ai|x)] and uncertainty U(Ai|x) of all the action classiﬁers using M
stochastic forward passes and MC-Dropout. Then, the classiﬁer with the maximum predicted
mean is chosen as the leader. Finally, the council members of the chosen leader vote for the
novelty of sample x based on their estimated uncertainty (see Algorithm 3.3).
ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
Examples of such voting outcome for three different leaders are illustrated in Fig. 3.
In case of category cartwheel, we can see that when the leader is voting indeed for the
correct category, all council members show low uncertainty values therefore resulting in a
low novelty score, as uninformed classiﬁers (marked in red) are excluded. However, we
observe very different measurements for an example from an unseen category clap which is
also predicted as cartwheel. Here, multiple classiﬁers which are in the council (marked in
blue) show unexpected high uncertainty values (e.g. eat, laugh), therefore discrediting the
leader decision and voting for a high novelty score.
Algorithm Novelty Detection by Voting of the Council Neurons
Input: Input sample x, Classiﬁcation Model ω, K sets of Council members for each
Leader: {CA1,...,CAK}
Output: Novelty score υ(⃗x)
1: Inference using MC-Dropout
Preform M stochastic forward passes: pm
Ai = p(Ai|x,ωm);
2: for all Ai ∈A do
Calculate the prediction mean and uncertainty: E(p(Ai|x)) and U(Ai|x)
4: end for
5: Find the Leader:
A∗= argmax
6: Select the Council:
7: Compute the novelty score : υ(x) =
∑Ai∈CA∗U(Ai|x)
Model variants.
We refer to our previous model as the Informed Democracy model since
voting is restricted to the council members which are chosen in an informed manner to check
the decision of the leader. In addition to the previous model, we consider two other variants
of our model:
1. The Uninformed Democracy model: Here, there is no council and all classiﬁers have
the right to vote for any leader. Hence, step 7 in Algorithm 3.3 is replaced with υ(x) =
∑Ai∈AU(Ai|x)
2. The Dictator model: unlike the previous model, this one leverages only the leader’s
uncertainty in its own decision to predict the novelty of the sample, i.e. υ = U(A∗|⃗x).
Open set and zero-shot learning
Once our model generated the novelty score υ(x), we
can decide whether x is a sample from a novel category or not using a sensitivity threshold τ.
This threshold can be estimated from a validation set using the equal error rate of the receiver
operating characteristic curve (ROC). Then, if υ(x) < τ the Council votes in favor of the
Leader and its category is taken as our ﬁnal classiﬁcation result. Otherwise, an unknown
activity class has been identiﬁed. In this case, the input could be passed further to a module
in charge of handling unfamiliar data, such as a zero-shot learning model or a user to give
the sample a new label in the context of active learning.
Evaluation
Evaluation setup
Since there is no established evaluation procedure available for action recognition in open-set conditions, we adapt existing evaluation protocols for two well-
ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
GGGGGGGGGGGGGG
Leading classifier
True category
brush_hair
sword_exercise
Classifier Uncertainty
Leading classifier
True category
brush_hair
sword_exercise
Classifier Uncertainty
Leading classifier
True category
brush_hair
sword_exercise
Classifier Uncertainty
Leading classifier True category
brush_hair
sword_exercise
PredictionScoreVarTopClass
Confident: voting council member
Uncertain: non−voter
Leading classifier
True category
brush_hair
sword_exercise
Classifier Uncertainty
Leading classifier True category
brush_hair
sword_exercise
Classifier Uncertainty
Figure 3: Examples of selective voting for the novelty score of different activities. The ﬁrst
row depicts the case where the samples are of known classes and second row for those of
novel classes. Red points highlight classiﬁers, which were excluded from to the council of
the current leader. Their uncertainty is, therefore, ignored when inferring the novelty score.
established datasets, HMDB-51 and UCF-101 , for our task†. We evenly split each
dataset into seen/unseen categories (26/25 for HMDB-51 and 51/50 for UCF-101). Samples
of unseen classes will not be available during training, while samples of the remaining set
of seen classes is further split into training (70%) and testing (30%) sets, thereby adapting
the evaluation framework of for the generalized ZS learning scenario. For each dataset,
we randomly generate 10 splits and report the average and standard deviation of the recognition accuracy. Using a separate validation split, we optimize the credibility threshold c and
compute the threshold for rejection τ for each category as the Equal Error Rate of the ROC.
Architecture details
We augment the RGB-stream of the I3D architecture with MC-
Dropout. The model is pre-trained on the Kinetics dataset, as described in . The last
average pooling is connected to two fully connected layers: a hidden layer of size 256 and
the ﬁnal softmax-classiﬁer layer. These are optimized using SGD with momentum of 0.9,
learning rate of 0.005 and dropout probability of 0.7 for 100 epochs. We sample the output scores for M = 100 stochastic forward passes applied on the two layers preceding the
classiﬁer, while the credibility threshold c is set to 0.001.
We compare our model to three popular methods for novelty and outlier detection: 1) a One Class SVM with RBF kernel (upper bound on the fraction of training
errors ν set to 0.1); 2) a GMM with 8 components; 3) and Softmax probabilities
 as the value for thresholding. Both SVM and GMM were trained on normalized features obtained from last average pooling layer of I3D pre-trained on the Kinetics dataset .
Novelty Detection
We evaluate the novelty detection accuracy in terms of a binary classi-
ﬁcation problem, using the area under curve (AUC) values of the receiver operating characteristic (ROC) and the precision-recall (PR) curves.
We show the robustness of our approach in comparison to the baseline methods in
All variants of our model clearly outperform the conventional approaches and
achieve an ROC-AUC gain of over 7% on both datasets. Along our model variants, Informed
†Dataset splits used for novelty detection and generalized zero-shot action recognition are provided at
 
ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
Novelty Detection Model
Baseline Models
One-class SVM
54.09 (±3.0)
77.86 (±4.0)
53.55 (±2.0)
78.57 (±2.4)
Gaussian Mixture Model
56.83 (±4.2)
78.40 (±3.6)
59.21 (±4.2)
79.50 (±2.2)
Conventional NN Conﬁdence
67.58 (±3.3)
84.21 (±3.0)
84.28 (±1.9)
93.92 (±0.7)
Our Proposed Model based on Bayesian Uncertainty
71.78 (±1.8)
86.81 (±2.5)
91.43 (±2.3)
96.72 (±1.0)
Uninformed Democracy
73.81 (±1.7)
87.83 (±2.3)
92.13 (±1.8)
97.15 (±0.7)
Informed Democracy
75.33 (±2.7)
88.66 (±2.3)
92.94 (±1.7)
97.52 (±0.6)
Table 1: Novelty detection results evaluated as area under the ROC and PR-curves for identifying previously unseen categories (mean and standard deviation over ten dataset splits).
Democracy has proven to be the most effective strategy for novelty score voting, outperforming the Dictator by 5.5% and 1.4%, while Uninformed Democracy achieved second-best results. We believe that smaller differences in performance gain on the UCF-101 data are due
to the much higher supervised classiﬁcation accuracy on this dataset. Since the categories of
UCF-101 are easier to distinguish visually and the confusion is low, there is more agreement
between the neurons in terms of their conﬁdence.
Generalized Zero-Shot Learning (GZSL)
Next, we evaluate our approach in the context
of GZSL, where our novelty detection model serves as a ﬁlter to distinguish whether the observed example should be classiﬁed with the I3D model in the standard classiﬁcation setup,
or mapped to one of the unknown classes via a ZSL model. We compare two prominent ZSL
methods: ConSE and DeViSE . The ConSE model starts by predicting probabilities
of the seen classes, and then takes the convex combination of word embeddings of the top
K most possible seen classes and select its nearest neighbor from the novel classes in the
word2vec space. For DeViSE, we train a separate model to regress word2vec representations from the visual features. We use the publicly available word2vec model that is trained
on Google News articles .
For consistency, we ﬁrst report the results for the standard ZS case (i.e. U→U) and further extend to the generalized case (i.e. U→U+S and U+S→U+S) as shown in Table 2. In
the more realistic GZSL setup, our model is not restricted to any group of target labels and is
evaluated on actions of seen and unseen category using the harmonic mean of accuracies for
seen and unseen classes as proposed by . Table 2 shows a clear advantage of employing
novelty detection as part of a GZSL framework. While failure of the original ConSE and De-
ViSE models might be surprising at ﬁrst glance, such performance drops have been discussed
in previous work on ZSL for image recognition and is due to the fact that both models
are biased towards labels that were used during training. Our Informed Democracy model
yields the best recognition rates in every setting and can therefore be indeed successfully
applied for multi-label action classiﬁcation in case of new activities.
Conclusion
We introduce a new approach for novelty detection in action recognition. Our model leverages the estimated uncertainty of the category classiﬁers to detect samples from novel categories not encountered during training. This is achieved by selecting a council of classiﬁers
for each leader (i.e. the most conﬁdent classiﬁer). The council will validate the decision
ROITBERG, AL-HALAH, STIEFELHAGEN: NOVELTY DETECTION FOR ACTION RECOGNITION
Zero-Shot Approach
Standard ConSe Model
21.03 (±2.07)
17.85 (±1.95)
0.07 (±0.10)
0.13 (±0.20)
Standard Devise Model
17.27 (±2.01)
0.26 (±0.37)
0.52 (±0.73)
14.48 (±1.13)
0.81 (±0.36)
1.61 (±0.71)
ConSe + Novelty Detection
One-class SVM
21.03 (±2.07)
10.99 (±1.83)
17.40 (±2.41)
17.85 (±1.95)
10.37 (±1.59)
16.55 (±1.91)
Gaussian Mixture Model
21.03 (±2.07)
13.30 (±2.58)
19.91 (±3.32)
17.85 (±1.95)
9.31 (±1.30)
15.98 (±1.99)
Conventional NN Conﬁdence
21.03 (±2.07)
10.96 (±0.87)
18.56 (±1.22)
17.85 (±1.95)
12.19 (±1.72)
20.91 (±2.59)
Informed Democracy (ours)
21.03 (±2.07)
13.67 (±1.31)
22.27 (±1.79)
17.85 (±1.95)
13.62 (±1.94)
23.42 (±2.97)
Devise + Novelty Detection
One-class SVM
17.27 (±2.01)
8.92 (±1.89)
14.67 (±2.74)
14.48 (±1.13)
8.65 (±1.59)
14.25 (±2.00)
Gaussian Mixture Model
17.27 (±2.01)
10.61 (±2.22)
16.72 (±3.1)
14.48 (±1.13)
7.26 (±0.84)
12.88 (±1.40)
Conventional NN Conﬁdence
17.27 (±2.01)
15.17 (±1.56)
14.48 (±1.13)
10.08 (±1.59)
17.69 (±2.33)
Informed Democracy (ours)
17.27 (±2.01)
10.73 (±1.47)
18.18 (±2.21)
14.48 (±1.13)
11.03 (±1.42)
19.48 (±2.21)
Table 2: Accuracy for GZS action recognition with the proposed novelty detection model.
U→U: test set consists of unseen actions, the prediction labels are restricted to the unseen labels (standard). U→U+S: test set consists of unseen actions, both unseen and seen labels are
possible for prediction. U+S→U+S: generalized ZSL case, both unseen and seen categories
are among the test examples and in the set of possible prediction labels (harmonic mean of
the seen and unseen accuracies reported.)
made by the leader through voting. Hence, either conﬁrming the classiﬁcation decision for
a sample of a known category or revoking the leader decision and deeming the sample to
be novel. We show in a thorough evaluation on two challenging benchmark, that our model
outperforms the state-of-the-art in novelty detection. Furthermore, we demonstrate that our
model can be easily integrated in a generalized zero-shot learning framework. Combining
our model with off-the-shelf zero-shot approaches leads to signiﬁcant improvements in classiﬁcation accuracy.
Acknowledgements
This work has been partially funded by the German Federal Ministry
of Education and Research (BMBF) within the PAKoS project.