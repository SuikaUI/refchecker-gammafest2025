DeepReID: Deep Filter Pairing Neural Network for Person Re-Identiﬁcation
Xiaogang Wang∗
The Chinese University of Hong Kong, Hong Kong
 , {rzhao, xiaotong, xgwang}@ee.cuhk.edu.hk
Person re-identiﬁcation is to match pedestrian images
from disjoint camera views detected by pedestrian detectors. Challenges are presented in the form of complex variations of lightings, poses, viewpoints, blurring effects, image
resolutions, camera settings, occlusions and background
clutter across camera views. In addition, misalignment introduced by the pedestrian detector will affect most existing
person re-identiﬁcation methods that use manually cropped
pedestrian images and assume perfect detection.
In this paper, we propose a novel ﬁlter pairing neural
network (FPNN) to jointly handle misalignment, photometric and geometric transforms, occlusions and background
All the key components are jointly optimized to
maximize the strength of each component when cooperating with others. In contrast to existing works that use handcrafted features, our method automatically learns features
optimal for the re-identiﬁcation task from data. The learned
ﬁlter pairs encode photometric transforms. Its deep architecture makes it possible to model a mixture of complex
photometric and geometric transforms. We build the largest
benchmark re-id dataset with 13,164 images of 1,360 pedestrians. Unlike existing datasets, which only provide manually cropped pedestrian images, our dataset provides automatically detected bounding boxes for evaluation close
to practical applications. Our neural network signiﬁcantly
outperforms state-of-the-art methods on this dataset.
1. Introduction
The purpose of person re-identiﬁcation is to match
pedestrians observed in non-overlapping camera views with
visual features . It has important applications in video surveillance, such as cross-camera tracking , multi-camera
event detection , and pedestrian retrieval .
problem is extremely challenging because it is difﬁcult to
∗This work is supported by the General Research Fund sponsored by
the Research Grants Council of Hong Kong (Project No. CUHK 417110,
CUHK 417011, CUHK 429412)
(a) Samples from our new dataset, CUHK03
(b) Samples from the VIPeR dataset 
Figure 1. Samples of pedestrian images observed in different camera views in person re-identiﬁcation. The two adjacent images
have the same identity.
match the visual features of pedestrians captured in different camera views due to the large variations of lightings,
poses, viewpoints, image resolutions, photometric settings
of cameras, and cluttered backgrounds. Some examples are
shown in Figure 1.
The typical pipeline of a person re-identiﬁcation system is shown in Figure 2. In practice, it should start with
automatic pedestrian detection, which is an essential step
for extracting pedestrians from long-hour recorded videos.
Given a pedestrian detection bounding box, manually designed features are used to characterize the image region
in all the existing works, although they may be suboptimal
for the task of person re-identiﬁcation. Image regions of
the same person undergo photometric transforms due to the
change of lighting conditions and camera settings. Their
geometric transforms are caused by misalignment and the
Figure 2. Pipeline of person re-identiﬁcation.
change of viewpoints and poses. Such transforms could be
normalized by learning mapping functions or similarity metrics . It is also supposed to be robust to
occlusions and background clutter. All the existing works
optimize each module in the pipeline either separately or
sequentially. If useful information is lost in previous steps,
it cannot be recovered later. Establishing automatic interaction among these components in the training process is
crucial for the overall system performance.
The contribution of this paper is three-fold. Firstly, we
propose a ﬁlter pairing neural network (FPNN) for person
re-identiﬁcation. This deep learning approach has several
important strengths and novelties compared with existing
(1) It jointly handles misalignment, photometric
and geometric transforms, occlusions and background clutter under a uniﬁed deep neural network. During training,
all the key components in Figure 2 are jointly optimized.
Each component maximizes its strength when cooperating
with others. (2) Instead of using handcrafted features, it automatically learns optimal features for the task of person reidentiﬁcation from data, together with the learning of photometric and geometric transforms. Two paired ﬁlters are applied to different camera views for feature extraction. The
ﬁlter pairs encode photometric transforms. (3) While existing works assume cross-view transforms to be unimodal,
the deep architecture and its maxout grouping layer allow
to model a mixture of complex transforms.
Secondly, we train the proposed neural network with
carefully designed training strategies including dropout,
data augmentation, data balancing, and bootstrapping.
These strategies address the problems of misdetection of
patch correspondence, overﬁtting, and extreme unbalance
of positive and negative training samples in this task.
Thirdly, we re-examine the person re-identiﬁcation problem and build a large scale dataset that can evaluate the effect introduced by automatic pedestrian detection. All the
existing datasets are small in size,
which makes it difﬁcult for them to train a deep neural network. Our dataset has 13,164 images of 1,360 pedestrians;
see a comparison in Table 1. Existing datasets only provide
manually cropped pedestrian images and assume perfect detection in evaluation protocols. As shown in Figure 1, automatic detection in practice introduces large misalignment
and may seriously affect the performance of existing methods. Our dataset provides both manually cropped images
and automatically detected bounding boxes with a state-ofthe-art detector for comprehensive evaluation.
2. Related Work
A lot of studies aimed to improve individual components of the pipeline in Figure 2 .
The visual features used in the existing person re-identiﬁcation systems
are manually designed.
Global features characterize the
distributions of color and texture with the histograms of
visual words . They have some invariance to misalignment, pose variation, and the change of viewpoints.
However, their discriminative power is low because of losing spatial information. In order to increase the discriminative power, patch-based local features have been used
 . When computing the
similarity between two images, visual features of two corresponding patches are compared. The challenge is to match
patches in two camera views when tackling the misalignment problem. Handcrafted features are difﬁcult to achieve
the balance between discriminative power and robustness.
The optimal feature design depends on photometric and geometric transforms across camera views. For example, if
the illumination variation is larger, the color space should
be quantized at a coarser scale. It is hard to achieve such optimization if feature design is independent of other components in Figure 2. Although the features can be selected and
weighted in later steps, the performance will decline if the
feature pool is not optimally designed. The right way is to
automatically learn features from data together with other
components. This is hard to achieve without deep learning.
One could assume the photometric or geometric transform models and learn the model parameters from training
samples . For example, Prosser et al. assumed the photometric transform to be bi-directional Cumulative Brightness Transfer Functions, which map color
observed in one camera view to another.
Porikli 
learned the color distortion function between camera views
with correlation matrix analysis. They assume transforms
to be unimodal. In our proposed ﬁlter pairing neural network, photometric transforms are learned with ﬁlter pairs
and a maxout grouping layer.
On the other hand, geometric transforms are learned with a patch matching layer,
a convolutional-maxpooling layer and a fully connected
layer. The proposed neural network can model a mixture
of complex transforms.
The effect of cross-camera transforms, occlusions and
background clutter can be further depressed by learning a
proper distance/similarity metric.
Gray et al.
Prosser et al.
 use boosting and RankSVM, respectively, to select features and compute the distance between
images. There are also many metric learning algorithms
 designed for person reidentiﬁcation. All the components in Figure 2 are optimized
either separately or sequentially in the existing works.
Convolution and
Maxpooling
Convolution (with
shared weights)
and Maxpooling
Fully Connected
Figure 3. Filter pairing neural network.
Deep learning has achieved great success in solving
many computer vision problems, including hand-written
digit recognition , object recognition , object
detection , image classiﬁcation ,
scene understanding , and face recognition . Although some deep learning works share the
spirit of jointly optimizing components of vision systems,
their problems, challenges, models and training strategies
are completely different from ours. They did not design
special layers to explicitly handle cross-view photometric
and geometric transforms, misdetection of patch matching
and background clutter. To our knowledge, this paper is the
ﬁrst work to use deep learning for person re-identiﬁcation.
The architecture of the proposed FPNN is shown in Figure 3. It is composed of six layers to handle misalignment,
cross-view photometric and geometric transforms, occlusions and background clutter in person re-identiﬁcation.
The design of each layer is described below.
3.1. Feature extraction
The ﬁrst layer is a convolutional and max-pooling layer.
It takes two pedestrian images I and J observed in different camera views as input. They have three color channels (RGB or LAB) and have the size of Him × Wim.
The photometric transforms are modeled with a convolutional layer that outputs local features extracted by ﬁlter
pairs. By convoluting a ﬁlter with the entire image, the responses at all the local patches are extracted as local features.
The ﬁlters (Wk, Vk) applied to different camera
views are paired. If K1 ﬁlter pairs are used and each ﬁlter is in size of m1 ×m1 ×3, the output map for each image
has K1 channels and is in size of H0 × W0 × K1, where
H0 = Him −m1 + 1 and W0 = Wim −m1 + 1. We deﬁne
the ﬁltering functions f, g : RHim×Wim×3 →RH0×W0×K1
ij =σ((Wk ∗I)ij + bI
ij =σ((Vk ∗J)ij + bJ
The convolution operation is denoted as ∗. A nonlinear activation function σ(·) is used to re-scale the linear
output and chosen as σ(x) = max(x, 0). After ﬁltering,
each patch is represented by a K1-channel feature vector.
The activation function normalizes and balances different
feature channels.
The parameters {(Wk, Vk, bI
the ﬁlter pairs are automatically learned from data. Two
paired ﬁlters represent the same feature most discriminative for person re-identiﬁcation. They are applied to different camera views and their difference reﬂects the photometric transforms. The convolutional layer is followed by
max-pooling, which makes the features robust to local misalignment. Each feature map is partitioned into H1 × W1
subregions and the maximum response in each subregion is
taken as the output. The output of the max-pooling layer is
a H1 × W1 × K1 feature map.
3.2. Patch matching
The second patch matching layer is to match the ﬁlter
responses of local patches across views. Considering the
geometric constraint, a pedestrian image is divided into M
Figure 4. Illustration of patch matching in FPNN. One stripe generates two patch displacement matrices because there are two ﬁlter
pairs. One detects blue color and the other detects green.
horizontal stripes(height factoring in Figure 3), and each
stripe has W1 patches. Image patches are matched only
within the same stripe. Since there are K1 ﬁlter pairs representing different features, the outputs of the patch matching
layer are K1M W1 ×W1 patch displacement matrices. The
output of the patch matching layer is
(i,j)(i′,j′) = f k
These displacement matrices encode the spatial patterns of
patch matching under the different features. An illustration
is shown in Figure 4. If a matrix element Sk
(i,j)(i′,j′) has a
high value, it indicates that patches (i, j) and (i′, j′) both
have high responses on a speciﬁc feature encoded by the
ﬁlter pair (Wk, Vk).
3.3. Modeling mixture of photometric transforms
Due to various intra- and inter-view variations, one visual feature (such as red clothes) may undergo multiple photometric transforms. In order to improve the robustness on
patch matching, a maxout-grouping layer is added. The
patch displacement matrices of K1 feature channels are divided into T groups. Within each group, only the maximum
activation is passed to the next layer. In this way, each feature is represented by multiple redundant channels. It allows to model a mixture of photometric transforms. During
the training process, with the backpropagation algorithm,
only the ﬁlter pair with the maximum response recieves the
gradients and is updated. It drives ﬁlter pairs in the same
group to compete for the gradients. Eventually, only one
ﬁlter has large response to a training sample. Therefore,
image patches have sparse responses with the learned ﬁlter
pairs. It is well known that sparsity is a property to eliminate
noise and redundancy. The output of the maxout grouping
layer is TM W1 × W1 displacement matrices. This is illustrated in Figure 5.
3.4. Modeling part displacement
Body parts can be viewed as adjacent patches. Another
convolution and max-pooling layer is added on the top of
patch displacement matrices to obtain the displacement matrices of body parts on a larger scale.
It takes the MT
Figure 5. Maxout pooling. Left: Responses of patches to four ﬁlter
pairs (indicated by the colors of yellow, purple, green and white)
on two stripes. Middle: Four patch displacement matrices after
passing the patch matching layer. Without maxout grouping, each
matrix only has one patch with large response. Right: Group four
channels together and take the maximum value to form a single
channel output. A line structure is formed.
W1 × W1 patch displacement matrices as input and treat
them as M W1 × W1 images with T channels. Similar to
the ﬁrst convolutional layer, K2 m2 × m2 × T ﬁlters are
applied to all the M images, and the output of this layer is
M W2 × W2 × K2 maps. The learned ﬁlters capture the
local patterns of part displacements.
3.5. Modeling pose and viewpoint transforms
Pedestrians undergo various pose and viewpoint transforms. Such global geometric transforms can be viewed as
different combinations of part displacement and their distributions are multi-modal. For example, two transforms
can share the same displacement on upper bodies, but are
different in the displacement of legs. Each output of a hidden node in the convolutional and maxpooling layer can be
viewed as a possible part displacement detected with a particular visual feature. All of these hidden nodes form the
input vector of the next fully connected layer. In the next
layer, each hidden node is a combination of all the possible
part displacements and represents a global geometric transform. N hidden nodes are able to model a mixture of global
geometric transforms.
3.6. Identity Recognition
The last softmax layer uses the softmax function to measure whether two input images belong to the same person
or not given the global geometric transforms detected in the
previous layer. Its output is a binary variable y deﬁned as
p(y = i|a0, a1, b0, b1, x) =
e(ai·x+bi)
i e(ai·x+bi) .
Let y = 1 if two pedestrian images (In, Jn) are matched,
otherwise y = 0. x is the input from the previous layer. a0,
a1, b0 and b1 are the combination weights and bias terms
to be learned. Given the class labels of H training sample pairs, the negative log-likelihood is used as the cost for
training and could be written as
yn log(p(y = 1|Φ, (In, Jn)))
+ (1 −yn) log(1 −p(y = 1|Φ, (In, Jn))).
It exerts large penalty for misclassiﬁed samples. For example, if yn = 0 and p(y = 1|Φ, (In, Jn)) = 1, (1 −
yn)log(1−p(y = 1|Φ, (In, Jn))) →−∞. Φ represents the
set of parameters of the whole neural network to be learned.
4. Training Strategies
Our training algorithm adopts the mini-batch stochastic
gradient descent proposed in . The training data is divided into mini-batches. The training errors are calculated
upon each mini-batch in the soft-max layer and get backpropogated to the lower layers. In addition, several carefully
designed training strategies are proposed.
4.1. Dropout
In person re-identiﬁcation, due to large cross-view variations, misalignment, pose variations, and occlusions, it is
likely for some patches on the same person (but in different
views) to be mismatched. To make the trained FPNN tolerable to misdetection of patch correspondences, the dropout
strategy is adopted. For each training sample as input
at each training iteration, some outputs of the ﬁrst convolutional layer (that is, extracted features with the ﬁlter pairs)
are randomly selected and set as zeros. Gradients in backpropogation are calculated with those randomly muted ﬁlter
responses to make the trained model more stable.
4.2. Data Augmentation
In the training set, the matched sample pairs (positive
samples) are several orders fewer than non-matched pairs
(negative samples). If they are directly used for training,
the network tends to predict all the inputs as being nonmatched. We augment data by simple translational transforms on each pedestrian image.
For an original pedestrian image of size Him × Wim, ﬁve images of the same
size are randomly sampled around the original image center
and their translations are from a uniform distribution in the
range of [−0.05Him, 0.05Him] × [−0.05Wim, 0.05Wim].
The matched sample pairs are enlarged by a factor of 25.
4.3. Data balancing
Each mini-batch keeps all the positive training samples
and randomly selects the same number of negative training samples at the very beginning of the training process.
The network achieves a resonably good conﬁguration after
the initial training. As the training process goes along, we
gradually increase the number of negative samples in each
mini-batch up to the ratio of 5 : 1.
4.4. Bootstrapping
After the network has been stabilized, we continue to
select difﬁcult negative samples, which are predicted as
matched pairs with high probabilities by the current network, and combine them with all the positive samples to further train the network iteratively. Because of the large number of negative training samples, it is very time-consuming
to re-predict all the negative samples with the current network after each epoch. We only re-predict hard samples
selected in the previous epoch. Since these samples have
been used to update the network, their predictions are expected to have larger changes than other samples after the
Each negative sample x is assigned with a score sk after
each epoch k. Samples with the smallest sk are selected to
re-train the network. At the beginning,
s0 = 1 −p(x is a matched pair|Φ0),
where Φ0 is the conﬁguration of the network. If x is selected as a hard sample for training in the previous epoch k,
its score is updated as
sk = 1 −p(x is a matched pair|Φk) + sk−1
where Φk is the conﬁguration of the network trained after
epoch k; otherwise, sk = λsk−1. The diminishing parameter λ is set as 0.99. This increases the chance of those
negative samples not being selected for a long time.
5. Dataset
All of the existing datasets are too small to train deep
neural networks. We build a much larger dataset1 which includes 13, 164 images of 1, 360 pedestrians. It is named
CUHK03, since we already published two re-id datasets
(CUHK01 and CUHK02 ) in previous works. A
comparison of the scales can be found in Table 1. The whole
dataset is captured with six surveillance cameras.
identity is observed by two disjoint camera views and has
an average of 4.8 images in each view. Some examples are
shown in Figure 1(a). Besides the scale, it has the following
characteristics.
(1) Apart from manually cropped pedestrian images, we
provide samples detected with a state-of-the-art pedestrian
detector .
This is a more realistic setting and poses
new problems rarely seen in existing datasets. From Figure 1(a), we can see that misalignment, occlusions and body
part missing are quite common in this dataset. The inaccurate detection also makes the geometric transforms complex. We further provide the original image frames and researchers can try their own detectors on this dataset.
1The dataset is available at 
˜xgwang/CUHK_identification.html
Table 1. Compare the sizes of our dataset (CUHK03) and existing person re-identiﬁcation datasets.
VIPeR 
i-LIDS 
CAVIAR 
Re-ID 2011 
CUHK01 
CUHK02 
No. of images
No. of persons
(2) Some existing datasets assume a single pair of camera views and their cross-view transforms are relatively simple. In our dataset, samples collected from multiple pairs of
camera views are all mixed and they form complex crossview transforms. Moreover, our cameras monitor an open
area where pedestrians walk in different directions, which
leads to multiple view transforms even between the same
pair of cameras.
(3) Images are obtained from a series of videos recorded
over months. Illumination changes are caused by weather,
sun directions, and shadow distributions even within a single camera view. Our cameras have different settings, which
also leads to photometric transforms.
6. Experimental Results
Most of the evaluations are conducted on the new
dataset, since existing datasets are too small to train the
deep model. An additional evaluation is on the CUHK01
 . Our dataset is partitioned into training set (1160 persons), validation set (100 persons), and test set (100 persons). Each person has roughly 4.8 photos per view, which
means there are almost 26, 000 positive training pairs before data augmentation. A mini-batch contains 512 images
pairs. Thus it takes about 300 mini-batches to go through
the training set. The validation set is used to design the
network architecture (the parameters of which are shown
in Table 2). The experiments are conducted with 20 random splits and all the Cumulative Matching Characteristic
(CMC) curves are single-shot results.
Each image is preprocessed with histogram equalization
and transformed to the LAB color space. It is normalized to
the size of (64 × 32 × 3), and subtracted with the mean of
all the pixels in that location. Our algorithm is implemented
with GTX670 GPU. The training process takes about ﬁve
hours to converge.
We compare with three person re-identiﬁcation methods (KISSME , eSDC , and SDALF ), four
state-of-the-art metric learning methods (Information Theoretic Metric Learning (ITML) , Logistic Distance Metric Learning (LDM) , Largest Margin Nearest Neighbor
(LMNN) , and Metric Learning to Rank (RANK) ),
and directly using Euclidean distance to compare features.
LMNN and ITML are widely used metric learning algorithms and have been used for person re-identiﬁcation in
 . RANK is optimized for ranking problems, while person re-identiﬁcation is a ranking problem. LDM is specifically designed for face and person identiﬁcation problems. When using metric learning methods and Euclidean
Table 2. Settings of the ﬁlter pairing neural network.
distance, the handcrafted features of dense color histograms and dense SIFT uniformly sampled from patches
are adopted. Through extensive experimental evaluation in
 , it has been shown that these local features are more effective on person re-identiﬁcation than most other features
and the implementation is publicly available.
6.1. Experiments on our new dataset
On our CUHK03 dataset, we conduct comparisons using both manually labeled pedestrian bounding boxes and
automatically detected bounding boxes. Figure 6(a) plots
the CMC curves of using manually labeled bounding boxes.
Our FPNN outperforms all the methods in comparison with
large margins.
The relative improvement on the Rank-1
identiﬁcation rate is 46% compared with the best performing approach.
Figure 6(b) shows the results of using automatically detected bounding boxes, which cause misalignment.
performance of other methods drop signiﬁcantly. For example, the Rank-1 identiﬁcation rate of the best performing KISSME drops by 2.47%, while FPNN only drops by
0.76%. It shows that FPNN is more robust to misalignment.
In order to compare the learning capacity and generalization capability of different learning methods, we did another
experiment by adding 933 images of 107 pedestrians to the
training set, while keep the test set unchanged. Therefore,
the training set has 1, 267 persons. These additional 933
images are captured from four camera views different from
those in the test set. Adding training samples, which do not
accurately match the photometric and geometric transforms
in the test set, makes the learning more difﬁcult. Figure 6(c)
shows the changes of Rank-1 identiﬁcation rates of different methods. It is observed that the performance of most of
the methods drops, because their limited learning capacity
cannot effectively handle a more complex training set and
the mismatch between the training and test sets. On the contrary, the performance of our FPNN is improved because of
its large learning capacity and also the fact that extra training samples improve the learned low-level features which
can be shared by different camera settings.
Identification Rate
FPNN (20.65%)
Euclidean ( 5.64%)
ITML ( 5.53%)
LMNN ( 7.29%)
RANK (10.42%)
LDM (13.51%)
SDALF ( 5.60%)
eSDC ( 8.76%)
KISSME (14.17%)
Identification Rate
FPNN (19.89%)
Euclidean ( 4.94%)
ITML ( 5.14%)
LMNN ( 6.25%)
RANK ( 8.52%)
LDM (10.92%)
SDALF ( 4.87%)
eSDC ( 7.68%)
KISSME (11.70%)
Relative Rank-1 Improvement
Figure 6. Experimental results on our new dataset using manually labeled pedestrian bounding boxes (a) and automatically detected
bounding boxes (b). Rank-1 identiﬁcation rates are shown in parentheses. (c): After adding another 933 images of 107 persons to the
training set, Rank-1 rate changes of different methods. The added images are collected from another four camera views different from
those used in the test set. Automatically detected bounding boxes are used in (c).
0k 1k 2k 3k 4k
Number of Training Mini-batches
Validation Rank-1 Accurarcy
No Dropout
Identification Rate
With Bootstrap(19.89%)
Without Bootstrap(15.66%)
Identification Rate
FPNN (27.87%)
Euclidean (10.52%)
ITML (17.10%)
LMNN (21.17%)
RANK (20.61%)
LDM (26.45%)
SDALF ( 9.90%)
eSDC (22.83%)
KISSME (29.40%)
Figure 7. (a): Rank-1 identiﬁcation of FPNN on the validation set after different number of training mini-batches. (b): CMC curves of
FPNN with and without bootstrap in training. Both (a) and (b) are evaluated on our new dataset. (c): CMC curves on the CUHK01 dataset.
6.2. Evaluation of training strategies
Experiments in Figure 7 (a) and (b) show the effectiveness of our dropout and bootstrapping training strategies.
Figure 7(a) shows the Rank-1 identiﬁcation rates after different numbers of training mini-batches on the validation
set with dropout rates ranging from 0% to 20%. Without
dropout, the identiﬁcation rate decreases with more training mini-batches. It indicates that overﬁtting happens. With
a 5% dropout rate, the identiﬁcation rate is high and converges on the validation set.
Dropout makes the trained
FPNN tolerable to misdetection of patch correspondences
and have good generalization power. If the dropout rate is
high (e.g. 20%2), it cannot reach a good identiﬁcation rate,
even though the generalization power is good, because not
enough features are passed to the next layer.
Figure 7(b) shows the CMC curves of FPNN with and
without the bootstrapping strategy.
Bootstrapping is effective in improving the Rank-1 identiﬁcation rate from
15.66% to 19.89%. However, there is less difference on
Rank-20. This may be attributed to the samples missed af-
2In our case, 20% dropout in the ﬁrst layer means on average roughly
36% of the patch matching layer outputs are set to zero due to Eqn 3.
ter Rank-20 are particularly difﬁcult, while FPNN has given
up ﬁtting these extreme cases in order to be robust.
6.3. Experiments on the CUHK01 dataset
We further evaluate FPNN on the CUHK01 dataset released in . In this dataset, there are 971 persons and
each person only has two images in either camera view.
Again, 100 persons are chosen for test and the remaining
871 persons for training and validation. This dataset is challenging for our approach, since the small number of samples cannot train the deep model very well. There are only
around 3, 000 pairs of positive training samples on it (compared with 26, 000 in our new dataset). Nevertheless, our
FPNN outperforms most of the methods in comparison, except that its Rank-1 rate is slightly lower than KISSME. But
its Rank-n (n > 10) rates are comparable to KISSME.
7. Conclusion
In this paper, we propose a new ﬁlter pairing neural network for person re-identiﬁcation. This method jointly optimizes feature learning, photometric transforms, geometric transforms, misalignment, occlusions and classiﬁcation
under a uniﬁed deep architecture. It learns ﬁlter pairs to
encode photometric transforms. Its large learning capacity
allows to model a mixture of complex photometric and geometric transforms. Some effective training strategies are
adopted to train the network well. It outperforms state-ofthe-art methods with large margins on a large scale benchmark dataset.