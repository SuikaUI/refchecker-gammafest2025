Stacked Attention Networks for Image Question Answering
Zichao Yang1, Xiaodong He2, Jianfeng Gao2, Li Deng2, Alex Smola1
1Carnegie Mellon University, 2Microsoft Research, Redmond, WA 98052, USA
 , {xiaohe, jfgao, deng}@microsoft.com, 
This paper presents stacked attention networks (SANs)
that learn to answer natural language questions from images. SANs use semantic representation of a question as
query to search for the regions in an image that are related
to the answer. We argue that image question answering
(QA) often requires multiple steps of reasoning. Thus, we
develop a multiple-layer SAN in which we query an image
multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate
that the proposed SANs signiﬁcantly outperform previous
state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the
relevant visual clues that lead to the answer of the question
layer-by-layer.
1. Introduction
With the recent advancement in computer vision and
in natural language processing (NLP), image question answering (QA) becomes one of the most active research areas . Unlike pure language based QA systems that have been studied extensively in the NLP community , image QA systems are designed to
automatically answer natural language questions according
to the content of a reference image.
Most of the recently proposed image QA models are
based on neural networks . A commonly
used approach was to extract a global image feature vector
using a convolution neural network (CNN) and encode
the corresponding question as a feature vector using a long
short-term memory network (LSTM) and then combine
them to infer the answer. Though impressive results have
been reported, these models often fail to give precise answers when such answers are related to a set of ﬁne-grained
regions in an image.
By examining the image QA data sets, we ﬁnd that it is
often that case that answering a question from an image requires multi-step reasoning. Take the question and image in
Fig. 1 as an example. There are several objects in the image: bicycles, window, street, baskets and
What are sitting
in the basket on
a bicycle?
Attention layer 1
Attention layer 2
feature vectors of diﬀerent
parts of image
(a) Stacked Attention Network for Image QA
Original Image
First Attention Layer
Second Attention Layer
(b) Visualization of the learned multiple attention layers.
stacked attention network ﬁrst focuses on all referred concepts,
e.g., bicycle, basket and objects in the basket (dogs) in
the ﬁrst attention layer and then further narrows down the focus in
the second layer and ﬁnds out the answer dog.
Figure 1: Model architecture and visualization
dogs. To answer the question what are sitting in
the basket on a bicycle, we need to ﬁrst locate
those objects (e.g.
basket, bicycle) and concepts
(e.g., sitting in) referred in the question, then gradually rule out irrelevant objects, and ﬁnally pinpoint to the region that are most indicative to infer the answer (i.e., dogs
in the example).
In this paper, we propose stacked attention networks
(SANs) that allow multi-step reasoning for image QA.
SANs can be viewed as an extension of the attention mechanism that has been successfully applied in image captioning and machine translation . The overall architecture of SAN is illustrated in Fig. 1a. The SAN consists of
three major components: (1) the image model, which uses
 
a CNN to extract high level image representations, e.g. one
vector for each region of the image; (2) the question model,
which uses a CNN or a LSTM to extract a semantic vector
of the question and (3) the stacked attention model, which
locates, via multi-step reasoning, the image regions that are
relevant to the question for answer prediction. As illustrated
in Fig. 1a, the SAN ﬁrst uses the question vector to query
the image vectors in the ﬁrst visual attention layer, then
combine the question vector and the retrieved image vectors
to form a reﬁned query vector to query the image vectors
again in the second attention layer. The higher-level attention layer gives a sharper attention distribution focusing on
the regions that are more relevant to the answer. Finally, we
combine the image features from the highest attention layer
with the last query vector to predict the answer.
The main contributions of our work are three-fold. First,
we propose a stacked attention network for image QA tasks.
Second, we perform comprehensive evaluations on four
image QA benchmarks, demonstrating that the proposed
multiple-layer SAN outperforms previous state-of-the-art
approaches by a substantial margin. Third, we perform a
detailed analysis where we visualize the outputs of different attention layers of the SAN and demonstrate the process
that the SAN takes multiple steps to progressively focus the
attention on the relevant visual clues that lead to the answer.
2. Related Work
Image QA is closely related to image captioning . In , the system ﬁrst extracted a high
level image feature vector from GoogleNet and then fed it
into a LSTM to generate captions. The method proposed in
 went one step further to use an attention mechanism in
the caption generation process. Different from , the
approach proposed in ﬁrst used a CNN to detect words
given the images, then used a maximum entropy language
model to generate a list of caption candidates, and ﬁnally
used a deep multimodal similarity model (DMSM) to rerank the candidates. Instead of using a RNN or a LSTM,
the DMSM uses a CNN to model the semantics of captions.
Unlike image captioning, in image QA, the question is
given and the task is to learn the relevant visual and text representation to infer the answer. In order to facilitate the research of image QA, several data sets have been constructed
in either through automatic generation based
on image caption data or by human labeling of questions
and answers given images. Among them, the image QA
data set in is generated based on the COCO caption
data set. Given a sentence that describes an image, the authors ﬁrst used a parser to parse the sentence, then replaced
the key word in the sentence using question words and the
key word became the answer. created an image QA data
set through human labeling. The initial version was in Chinese and then was translated to English. also created an
image QA data set through human labeling. They collected
questions and answers not only for real images, but also for
abstract scenes.
Several image QA models were proposed in the literature. used semantic parsers and image segmentation
methods to predict answers based on images and questions.
 both used encoder-decoder framework to generate
answers given images and questions.
They ﬁrst used a
LSTM to encoder the images and questions and then used
another LSTM to decode the answers. They both fed the
image feature to every LSTM cell.
 proposed several neural network based models, including the encoderdecoder based models that use single direction LSTMs and
bi-direction LSTMs, respectively.
However, the authors
found the concatenation of image features and bag of words
features worked the best. ﬁrst encoded questions with
LSTMs and then combined question vectors with image
vectors by element wise multiplication. used a CNN
for question modeling and used convolution operations to
combine question vectors and image feature vectors. We
compare the SAN with these models in Sec. 4.
To the best of our knowledge, the attention mechanism,
which has been proved very successful in image captioning,
has not been explored for image QA. The SAN adapt the attention mechanism to image QA, and can be viewed as a
signiﬁcant extension to previous models in that multiple attention layers are used to support multi-step reasoning
for the image QA task.
3. Stacked Attention Networks (SANs)
The overall architecture of the SAN is shown in Fig. 1a.
We describe the three major components of SAN in this section: the image model, the question model, and the stacked
attention model.
3.1. Image Model
The image model uses a CNN to get the
representation of images. Speciﬁcally, the VGGNet is
used to extract the image feature map fI from a raw image
feature map
Figure 2: CNN based image model
fI = CNNvgg(I).
Unlike previous studies that use features from the
last inner product layer, we choose the features fI from the
last pooling layer, which retains spatial information of the
original images. We ﬁrst rescale the images to be 448×448
pixels, and then take the features from the last pooling layer,
which therefore have a dimension of 512×14×14, as shown
in Fig. 2. 14 × 14 is the number of regions in the image and
512 is the dimension of the feature vector for each region.
Accordingly, each feature vector in fI corresponds to a 32×
32 pixel region of the input images. We denote by fi, i ∈
 the feature vector of each image region.
Then for modeling convenience, we use a single layer
perceptron to transform each feature vector to a new vector that has the same dimension as the question vector (described in Sec. 3.2):
vI = tanh(WIfI + bI),
where vI is a matrix and its i-th column vi is the visual
feature vector for the region indexed by i.
3.2. Question Model
As show that LSTMs and CNNs are powerful
to capture the semantic meaning of texts, we explore both
models for question representations in this study.
LSTM based question model
Figure 3: LSTM based question model
The essential structure of a LSTM unit is a memory cell
ct which reserves the state of a sequence. At each step,
the LSTM unit takes one input vector (word vector in our
case) xt and updates the memory cell ct, then output a hidden state ht. The update process uses the gate mechanism.
A forget gate ft controls how much information from past
state ct−1 is preserved. An input gate it controls how much
the current input xt updates the memory cell. An output
gate ot controls how much information of the memory is
fed to the output as hidden state. The detailed update process is as follows:
it =σ(Wxixt + Whiht−1 + bi),
ft =σ(Wxfxt + Whfht−1 + bf),
ot =σ(Wxoxt + Whoht−1 + bo),
ct =ftct−1 + it tanh(Wxcxt + Whcht−1 + bc),
ht =ot tanh(ct),
where i, f, o, c are input gate, forget gate, output gate and
memory cell, respectively. The weight matrix and bias are
parameters of the LSTM and are learned on training data.
Given the question q = [q1, ...qT ], where qt is the one hot
vector representation of word at position t, we ﬁrst embed
the words to a vector space through an embedding matrix
xt = Weqt. Then for every time step, we feed the embedding vector of words in the question to LSTM:
xt =Weqt, t ∈{1, 2, ...T},
ht =LSTM(xt), t ∈{1, 2, ...T}.
As shown in Fig. 3, the question what are sitting
in the basket on a bicycle
LSTM. Then the ﬁnal hidden layer is taken as the representation vector for the question, i.e., vQ = hT .
CNN based question model
max pooling
convolution
Figure 4: CNN based question model
In this study, we also explore to use a CNN similar
to for question representation. Similar to the LSTMbased question model, we ﬁrst embed words to vectors
xt = Weqt and get the question vector by concatenating
the word vectors:
x1:T = [x1, x2, ..., xT ].
Then we apply convolution operation on the word embedding vectors. We use three convolution ﬁlters, which have
the size of one (unigram), two (bigram) and three (trigram)
respectively. The t-th convolution output using window size
c is given by:
hc,t = tanh(Wcxt:t+c−1 + bc).
The ﬁlter is applied only to window t : t + c −1 of size c.
Wc is the convolution weight and bc is the bias. The feature
map of the ﬁlter with convolution size c is given by:
hc = [hc,1, hc,2, ..., hc,T −c+1].
Then we apply max-pooling over the feature maps of the
convolution size c and denote it as
t [hc,1, hc,2, ..., hc,T −c+1].
The max-pooling over these vectors is a coordinate-wise
max operation. For convolution feature maps of different
sizes c = 1, 2, 3, we concatenate them to form the feature
representation vector of the whole question sentence:
h = [˜h1, ˜h2, ˜h3],
hence vQ = h is the CNN based question vector.
The diagram of CNN model for question is shown in
Fig. 4. The convolutional and pooling layers for unigrams,
bigrams and trigrams are drawn in red, blue and orange, respectively.
3.3. Stacked Attention Networks
Given the image feature matrix vI and the question feature vector vQ, SAN predicts the answer via multi-step reasoning.
In many cases, an answer only related to a small region
of an image. For example, in Fig. 1b, although there are
multiple objects in the image: bicycles, baskets,
window, street and dogs and the answer to the question only relates to dogs. Therefore, using the one global
image feature vector to predict the answer could lead to suboptimal results due to the noises introduced from regions
that are irrelevant to the potential answer. Instead, reasoning via multiple attention layers progressively, the SAN are
able to gradually ﬁlter out noises and pinpoint the regions
that are highly relevant to the answer.
Given the image feature matrix vI and the question vector vQ, we ﬁrst feed them through a single layer neural network and then a softmax function to generate the attention
distribution over the regions of the image:
hA = tanh(WI,AvI ⊕(WQ,AvQ + bA)),
pI =softmax(WP hA + bP ),
where vI ∈Rd×m, d is the image representation dimension and m is the number of image regions, vQ ∈Rd is a
d dimensional vector. Suppose WI,A, WQ,A ∈Rk×d and
WP ∈R1×k, then pI ∈Rm is an m dimensional vector,
which corresponds to the attention probability of each image region given vQ. Note that we denote by ⊕the addition
of a matrix and a vector. Since WI,AvI ∈Rk×m and both
WQ,AvQ, bA ∈Rk are vectors, the addition between a matrix and a vector is performed by adding each column of the
matrix by the vector.
Based on the attention distribution, we calculate the
weighted sum of the image vectors, each from a region, ˜vi
as in Eq. 17. We then combine ˜vi with the question vector vQ to form a reﬁned query vector u as in Eq. 18. u is
regarded as a reﬁned query since it encodes both question
information and the visual information that is relevant to the
potential answer:
u =˜vI + vQ.
Compared to models that simply combine the question vector and the global image vector, attention models construct a more informative u since higher weights
are put on the visual regions that are more relevant to
the question. However, for complicated questions, a single attention layer is not sufﬁcient to locate the correct
region for answer prediction. For example, the question
in Fig. 1 what are sitting in the basket on
a bicycle refers to some subtle relationships among
multiple objects in an image.
Therefore, we iterate the
above query-attention process using multiple attention layers, each extracting more ﬁne-grained visual attention information for answer prediction. Formally, the SANs take the
following formula: for the k-th attention layer, we compute:
A = tanh(W k
I,AvI ⊕(W k
Q,Auk−1 + bk
I =softmax(W k
where u0 is initialized to be vQ. Then the aggregated image
feature vector is added to the previous query vector to form
a new query vector:
That is, in every layer, we use the combined question
and image vector uk−1 as the query for the image. After the
image region is picked, we update the new query vector as
I + uk−1. We repeat this K times and then use the
ﬁnal uK to infer the answer:
pans =softmax(WuuK + bu).
Fig. 1b illustrates the reasoning process by an example. In the ﬁrst attention layer, the model identiﬁes roughly
the area that are relevant to basket, bicycle, and
sitting in. In the second attention layer, the model focuses more sharply on the region that corresponds to the
answer dogs. More examples can be found in Sec. 4.
4. Experiments
4.1. Data sets
We evaluate the SAN on four image QA data sets.
DAQUAR-ALL is proposed in . There are 6, 795
training questions and 5, 673 test questions. These questions are generated on 795 and 654 images respectively. The
images are mainly indoor scenes. The questions are categorized into three types including Object, Color and Number.
Most of the answers are single words. Following the setting
in , we exclude data samples that have multiple
words answers. The remaining data set covers 90% of the
original data set.
DAQUAR-REDUCED
DAQUAR-ALL. There are 3, 876 training samples and
297 test samples. This data set is constrained to 37 object
categories and uses only 25 test images. The single word
answers data set covers 98% of the original data set.
COCO-QA is proposed in . Based on the Microsoft
COCO data set, the authors ﬁrst parse the caption of the image with an off-the-shelf parser, then replace the key components in the caption with question words for form questions. There are 78736 training samples and 38948 test samples in the data set. These questions are based on 8, 000 and
4, 000 images respectively. There are four types of questions including Object, Number, Color, and Location. Each
type takes 70%, 7%, 17%, and 6% of the whole data set,
respectively. All answers in this data set are single word.
VQA is created through human labeling . The data
set uses images in the COCO image caption data set .
Unlike the other data sets, for each image, there are three
questions and for each question, there are ten answers labeled by human annotators.
There are 248, 349 training
questions and 121, 512 validation questions in the data set.
Following , we use the top 1000 most frequent answer
as possible outputs and this set of answers covers 82.67%
of all answers. We ﬁrst studied the performance of the proposed model on the validation set. Following , we split
the validation data set into two halves, val1 and val2. We
use training set and val1 to train and validate and val2 to
test locally. The results on the val2 set are reported in Table. 6. We also evaluated the best model, SAN(2, CNN),
on the standard test server as provided in and report the
results in Table. 5.
4.2. Baselines and evaluation methods
We compare our models with a set of baselines proposed
recently on image QA. Since the results
of these baselines are reported on different data sets in different literature, we present the experimental results on different data sets in different tables.
For all four data sets, we formulate image QA as a classiﬁcation problem since most of answers are single words.
We evaluate the model using classiﬁcation accuracy as reported in . The reference models also report the
Wu-Palmer similarity (WUPS) measure . The WUPS
measure calculates the similarity between two words based
on their longest common subsequence in the taxonomy tree.
We can set a threshold for WUPS, if the similarity is less
than the threshold, then it is zeroed out. Following the reference models, we use WUPS0.9 and WUPS0.0 as evaluation
metrics besides the classiﬁcation accuracy. The evaluation
on the VQA data set is different from other three data sets,
since for each question there are ten answer labels that may
or may not be the same. We follow to use the following
metric: min(# human labels that match that answer/3, 1),
which basically gives full credit to the answer when three
or more of the ten human labels match the answer and gives
partial credit if there are less matches.
4.3. Model conﬁguration and training
For the image model, we use the VGGNet to extract features. When training the SAN, the parameter set of the CNN
of the VGGNet is ﬁxed. We take the output from the last
pooling layer as our image feature which has a dimension
of 512 × 14 × 14 .
For DAQUAR and COCO-QA, we set the word embedding dimension and LSTM’s dimension to be 500 in the
question model. For the CNN based question model, we
set the unigram, bigram and trigram convolution ﬁlter size
to be 128, 256, 256 respectively. The combination of these
ﬁlters makes the question vector size to be 640. For VQA
dataset, since it is larger than other data sets, we double the
model size of the LSTM and the CNN to accommodate the
large data set and the large number of classes. In evaluation,
we experiment with SAN with one and two attention layers.
We ﬁnd that using three or more attention layers does not
further improve the performance.
In our experiments, all the models are trained using
stochastic gradient descent with momentum 0.9. The batch
size is ﬁxed to be 100. The best learning rate is picked
using grid search.
Gradient clipping technique and
dropout are used.
4.4. Results and analysis
The experimental results on DAQUAR-ALL, DAQUAR-
REDUCED, COCO-QA and VQA are presented in Table. 1
to 6 respectively. Our model names explain their settings:
SAN is short for the proposed stacked attention networks,
the value 1 or 2 in the brackets refer to using one or two
attention layers, respectively. The keyword LSTM or CNN
refers to the question model that SANs use.
The experimental results in Table. 1 to 6 show that
the two-layer SAN gives the best results across all data
sets and the two kinds of question models in the SAN,
LSTM and CNN, give similar performance. For example,
on DAQUAR-ALL (Table. 1), both of the proposed twolayer SANs outperform the two best baselines, the IMG-
CNN in and the Ask-Your-Neuron in , by 5.9%
and 7.6% absolute in accuracy, respectively. Similar range
of improvements are observed in metrics of WUPS0.9 and
WUPS0.0. We also observe signiﬁcant improvements on
DAQUAR-REDUCED (Table. 2), i.e., our SAN(2, LSTM)
Multi-World: 
Multi-World
Ask-Your-Neurons: 
Language + IMG
SAN(1, LSTM)
SAN(1, CNN)
SAN(2, LSTM)
SAN(2, CNN)
Human : 
Table 1: DAQUAR-ALL results, in percentage
Multi-World: 
Multi-World
Ask-Your-Neurons: 
Language + IMG
2-VIS+BLSTM
SAN(1, LSTM)
SAN(1, CNN)
SAN(2, LSTM)
SAN(2, CNN)
Human : 
Table 2: DAQUAR-REDUCED results, in percentage
outperforms the IMG-CNN , the 2-VIS+BLSTM ,
the Ask-Your-Neurons approach and the Multi-World
 by 6.5%, 10.4%, 11.5% and 33.5% absolute in accuracy, respectively. On the larger COCO-QA data set, the
proposed two-layer SANs signiﬁcantly outperform the best
baselines from (IMG-CNN) and (IMG+BOW and
2-VIS+BLSTM) by 5.1% and 6.6% in accuracy (Table. 3).
2-VIS+BLSTM
SAN(1, LSTM)
SAN(1, CNN)
SAN(2, LSTM)
SAN(2, CNN)
Table 3: COCO-QA results, in percentage
2-VIS+BLSTM
SAN(1, LSTM)
SAN(1, CNN)
SAN(2, LSTM)
SAN(2, CNN)
Table 4: COCO-QA accuracy per class, in percentage
SAN(2, CNN)
Table 5: VQA results on the ofﬁcial server, in percentage
Table. 5 summarizes the performance of various models on
VQA, which is the largest among the four data sets. The
overall results show that our best model, SAN(2, CNN),
SAN(1, LSTM)
SAN(1, CNN)
SAN(2, LSTM)
SAN(2, CNN)
Table 6: VQA results on our partition, in percentage
outperforms the LSTM Q+I model, the best baseline from
 , by 4.8% absolute. The superior performance of the
SANs across all four benchmarks demonstrate the effectiveness of using multiple layers of attention.
In order to study the strength and weakness of the SAN
in detail, we report performance at the question-type level
on the two large data sets, COCO-QA and VQA, in Table. 4 and
5, respectively. We observe that on COCO-
QA, compared to the two best baselines, IMG+BOW and
2-VIS+BLSTM, out best model SAN(2, CNN) improves
7.2% in the question type of Color, followed by 6.1% in
Objects, 5.7% in Location and 4.2% in Number. We observe similar trend of improvements on VQA. As shown
in Table. 5, compared to the best baseline LSTM Q+I, the
biggest improvement of SAN(2, CNN) is in the Other type,
9.7%, followed by the 1.4% improvement in Number and
0.4% improvement in Yes/No. Note that the Other type in
VQA refers to questions that usually have the form of “what
color, what kind, what are, what type, where” etc., which
are similar to question types of Color, Objects and Location in COCO-QA. The VQA data set has a special Yes/No
type of questions. The SAN only improves the performance
of this type of questions slightly. This could due to that the
answer for a Yes/No question is very question dependent, so
better modeling of the visual information does not provide
much additional gains. This also conﬁrms the similar observation reported in , e.g., using additional image information only slightly improves the performance in Yes/No,
as shown in Table. 5, Q+I vs Question, and LSTM Q+I vs
Our results demonstrate clearly the positive impact of
using multiple attention layers. In all four data sets, twolayer SANs always perform better than the one-layer SAN.
Speciﬁcally, on COCO-QA, on average the two-layer SANs
outperform the one-layer SANs by 2.2% in the type of
Color, followed by 1.3% and 1.0% in the Location and Objects categories, and then 0.4% in Number. This aligns to
the order of the improvements of the SAN over baselines.
Similar trends are observed on VQA (Table. 6), e.g., the
two-layer SAN improve over the one-layer SAN by 1.4%
for the Other type of question, followed by 0.2% improvement for Number, and ﬂat for Yes/No.
4.5. Visualization of attention layers
In this section, we present analysis to demonstrate that
using multiple attention layers to perform multi-step reasoning leads to more ﬁne-grained attention layer-by-layer
in locating the regions that are relevant to the potential answers. We do so by visualizing the outputs of the attention layers of a sample set of images from the COCO-QA
test set. Note the attention probability distribution is of size
14 × 14 and the original image is 448 × 448, we up-sample
the attention probability distribution and apply a Gaussian
ﬁlter to make it the same size as the original image.
Fig. 5 presents six examples. More examples are presented in the appendix. They cover types as broad as Object,
Numbers, Color and Location. For each example, the three
images from left to right are the original image, the output
of the ﬁrst attention layer and the output of the second attention layer, respectively. The bright part of the image is
the detected attention. Across all those examples, we see
that in the ﬁrst attention layer, the attention is scattered on
many objects in the image, largely corresponds to the objects and concepts referred in the question, whereas in the
second layer, the attention is far more focused on the regions that lead to the correct answer. For example, consider
the question what is the color of the horns,
which asks the color of the horn on the woman’s head in
Fig. 5(f). In the output of the ﬁrst attention layer, the model
ﬁrst recognizes a woman in the image. In the output of the
second attention layer, the attention is focused on the head
of the woman, which leads to the answer of the question:
the color of the horn is red.
4.6. Errors analysis
We randomly sample 100 images from the COCO-QA
test set that the SAN make mistakes. We group the errors
into four categories: (i) the SANs focus the attention on the
wrong regions (22%), e.g., the example in Fig. 6(a); (ii) the
SANs focus on the right region but predict a wrong answer
(42%), e.g., the examples in Fig. 6(b)(c)(d); (iii) the answer
is ambiguous, the SANs give answers that are different from
labels, but might be acceptable (31%). E.g., in Fig. 6(e), the
answer label is pot, but out model predicts vase, which
is also visually reasonable; (iv) the labels are clearly wrong
(5%). E.g., in Fig. 6(f), our model gives the correct answer
trains while the label cars is wrong.
5. Conclusion
In this paper, we propose a new stacked attention network (SAN) for image QA. SAN uses a multiple-layer attention mechanism that queries an image multiple times to
locate the relevant visual region and to infer the answer progressively. Experimental results demonstrate that the proposed SAN signiﬁcantly outperforms previous state-of-theart approaches by a substantial margin on all four image QA
Figure 5: Visualization of two attention layers
Figure 6: Examples of mistakes
data sets. The visualization of the attention layers further illustrates the process that the SAN focuses the attention to
the relevant visual clues that lead to the answer of the question layer-by-layer.