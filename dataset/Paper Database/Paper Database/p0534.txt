IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 27, NO. 4, APRIL 2019
Deep Learning for Electromyographic Hand
Gesture Signal Classiﬁcation Using
Transfer Learning
Ulysse Côté-Allard
, Cheikh Latyr Fall
, Alexandre Drouin, Alexandre Campeau-Lecours,
Clément Gosselin, Kyrre Glette, François Laviolette, and Benoit Gosselin
Abstract— In recent years, deep learning algorithms have
become increasingly more prominent for their unparalleled ability to automatically learn discriminant features
from large amounts of data. However, within the ﬁeld of
electromyography-based gesture recognition, deep learning algorithms are seldom employed as they require an
unreasonable amount of effort from a single person, to generate tens of thousands of examples. This paper’s hypothesis is that general, informative features can be learned
from the large amounts of data generated by aggregating
the signals of multiple users, thus reducing the recording
burden while enhancing gesture recognition. Consequently,
this paper proposes applying transfer learning on aggregated data from multiple users while leveraging the capacity
of deep learning algorithms to learn discriminant features
from large datasets. Two datasets comprised 19 and 17 ablebodied participants, respectively (the ﬁrst one is employed
for pre-training), were recorded for this work, using the Myo
armband. A third Myo armband dataset was taken from the
NinaPro database and is comprised ten able-bodied participants. Three different deep learning networks employing
three different modalities as input (raw EMG, spectrograms,
and continuous wavelet transform (CWT)) are tested on
the second and third dataset. The proposed transfer learning scheme is shown to systematically and signiﬁcantly
enhance the performance for all three networks on the
two datasets, achieving an ofﬂine accuracy of 98.31% for
7 gestures over 17 participants for the CWT-based ConvNet
Manuscript received July 4, 2018; revised November 18, 2018 and
December 31, 2018; accepted January 22, 2019. Date of publication
January 31, 2019; date of current version April 8, 2019. This work was
supported in part by the Natural Sciences and Engineering Research
Council of Canada (NSERC) under Grant 401220434, in part by the
Institut de recherche Robert-Sauvé en santé et en sécurité du travail
(IRSST), in part by the Fondation Famille Choquette, and in part by
the Research Council of Norway through its Centres of Excellence
Scheme under Project 262762. (Corresponding author: Ulysse Côté-
U. Côté-Allard, C. L. Fall, and B. Gosselin are with the Department
of Computer and Electrical Engineering, Université Laval, Québec City,
QC G1V 0A6, Canada (e-mail: ).
A. Drouin and F. Laviolette are with the Department of Computer Science and Software Engineering, Université Laval, Québec City, QC G1V
0A6, Canada.
A. Campeau-Lecours and C. Gosselin are with the Department of
Mechanical Engineering, Université Laval, Québec City, QC G1V 0A6,
K. Glette is with RITMO, University of Oslo, 0315 Oslo, Norway, and
also with the Department of Informatics, University of Oslo, 0315 Oslo,
This paper has supplementary downloadable material available at
 provided by the author.
Digital Object Identiﬁer 10.1109/TNSRE.2019.2896269
and 68.98% for 18 gestures over 10 participants for the raw
EMG-based ConvNet. Finally, a use-case study employing
eight able-bodied participants suggests that real-time feedback allows users to adapt their muscle activation strategy
which reduces the degradation in accuracy normally experienced over time.
Index Terms— Surface electromyography, EMG, transfer
learning, domain adaptation, deep learning, convolutional
networks, hand gesture recognition.
I. INTRODUCTION
OBOTICS and artiﬁcial intelligence can be leveraged to
increase the autonomy of people living with disabilities.
This is accomplished, in part, by enabling users to seamlessly
interact with robots to complete their daily tasks with increased
independence. In the context of hand prosthetic control, muscle activity provides an intuitive interface on which to perform
hand gesture recognition . This activity can be recorded by
surface electromyography (sEMG), a non-invasive technique
widely adopted both in research and clinical settings. The
sEMG signals, which are non-stationary, represent the sum
of subcutaneous motor action potentials generated through
muscular contraction . Artiﬁcial intelligence can then be
leveraged as the bridge between sEMG signals and the prosthetic behavior.
The literature on sEMG-based gesture recognition primarily
focuses on feature engineering, with the goal of characterizing
sEMG signals in a discriminative way – . Recently,
researchers have proposed deep learning approaches – ,
shifting the paradigm from feature engineering to feature
learning. Regardless of the method employed, the end-goal
remains the improvement of the classiﬁer’s robustness. One
of the main factors for accurate predictions, especially when
working with deep learning algorithms, is the amount of
training data available. Hand gesture recognition creates a
peculiar context where a single user cannot realistically be
expected to generate tens of thousands of examples in a
single sitting. Large amounts of data can however be obtained
by aggregating the recordings of multiple participants, thus
fostering the conditions necessary to learn a general mapping
of users’ sEMG signal. This mapping might then facilitate
the hand gestures’ discrimination task with new subjects.
1534-4320 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See for more information.
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
CÔTÉ-ALLARD et al.: DEEP LEARNING FOR ELECTROMYOGRAPHIC HAND GESTURE SIGNAL CLASSIFICATION USING TL
Consequently, deep learning offers a particularly attractive
context from which to develop a Transfer Learning (TL)
algorithm to leverage inter-user data by pre-training a model
on multiple subjects before training it on a new participant.
As such, the main contribution of this work is to present a
new TL scheme employing a convolutional network (ConvNet)
to leverage inter-user data within the context of sEMG-based
gesture recognition. A previous work has already shown
that learning simultaneously from multiple subjects significantly enhances the ConvNet’s performance whilst reducing the size of the required training dataset typically seen
with deep learning algorithms. This paper expands upon the
aforementioned conference paper’s work, improving the TL
algorithm to reduce its computational load and improving
its performance. Additionally, three new ConvNet architectures, employing three different input modalities, speciﬁcally
designed for the robust and efﬁcient classiﬁcation of sEMG
signals are presented. The raw signal, short-time Fourier
transform-based spectrogram and Continuous Wavelet Transform (CWT) are considered for the characterization of the
sEMG signals to be fed to these ConvNets. To the best of
the authors’ knowledge, this is the ﬁrst time that CWTs are
employed as features for the classiﬁcation of sEMG-based
hand gesture recognition (although they have been proposed
for the analysis of myoelectric signals ). Another major contribution of this article is the publication of a new sEMG-based
gesture classiﬁcation dataset comprised of 36 able-bodied participants. This dataset and the implementation of the ConvNets
along with their TL augmented version are made readily available.1 Finally, this paper further expands the aforementioned
conference paper by proposing a use-case experiment on the
effect of real-time feedback on the online performance of
a classiﬁer without recalibration over a period of fourteen
days. Note that, due to the stochastic nature of the algorithms
presented in this paper, unless stated otherwise, all experiments
are reported as an average of 20 runs.
This paper is organized as follows. An overview of the
related work in hand gesture recognition through deep learning
and transfer learning/domain adaptation is given in Sec. II.
Sec. III presents the proposed new hand gesture recognition
dataset, with data acquisition and processing details alongside
an overview of the NinaPro DB5 dataset. A presentation
of the different state-of-the-art feature sets employed in this
work is given in Sec. IV. Sec. V thoroughly describes the
proposed networks’ architectures, while Sec. VI presents the
TL algorithm used to augment said architecture. Moreover,
comparisons with the state-of-the-art in gesture recognition
are given in Sec. VII. A real-time use-case experiment on the
ability of users to counteract signal drift from sEMG signals is
presented in Sec. VIII. Finally, results are discussed in Sec. IX.
II. RELATED WORK
sEMG signals can vary signiﬁcantly between subjects,
even when precisely controlling for electrode placement .
Regardless, classiﬁers trained from a user can be applied
to new participants achieving slightly better than random
1 
performances and high accuracy (85% over 6 gestures)
when augmented with TL on never before seen subjects .
As such, sophisticated techniques have been proposed to
leverage inter-user information. For example, research has
been done to ﬁnd a projection of the feature space that
bridges the gap between an original subject and a new
user , . Several works have also proposed leveraging
a pre-trained model removing the need to simultaneously
work with data from multiple users – . These non-deep
learning TL approaches showed important performance gains
compared to their non-augmented versions. Although, some of
these gains might be due to the baseline’s poorly optimized
hyperparameters .
Short-Time Fourier Transform (STFT) have been sparsely
employed in the last decades for the classiﬁcation of sEMG
data , . A possible reason for this limited interest
in STFT is that much of the research on sEMG-based gesture recognition focuses on designing feature ensembles .
Because STFT on its own generates large amounts of features
and are relatively computationally expensive, they can be
challenging to integrate with other feature types. Additionally, STFTs have also been shown to be less accurate than
Wavelet Transforms on their own for the classiﬁcation of
sEMG data. Recently however, STFT features, in the form of
spectrograms, have been applied as input feature space for the
classiﬁcation of sEMG data by leveraging ConvNets , .
CWT features have been employed for electrocardiogram
analysis , electroencephalography and EMG signal
analysis, but mainly for lower limbs , . Wavelet-based
features have been used in the past for sEMG-based hand
gesture recognition . The features employed however,
are based on the Discrete Wavelet Transform and the
Wavelet Packet Transform (WPT) instead of the CWT.
This preference might be due to the fact that both DWT
and WPT are less computationally expensive than the CWT
and are thus better suited to be integrated into an ensemble
of features. Similarly to spectrograms however, CWT offers
an attractive image-like representation to leverage ConvNets
for sEMG signal classiﬁcation and can now be efﬁciently
implemented on embedded systems (see Appendix C). To the
best of the authors’ knowledge, this is the ﬁrst time that CWT
is utilized for sEMG-based hand gesture recognition.
Recently, ConvNets have started to be employed for hand
gesture recognition using single array , and matrix 
of electrodes. Additionally, Du et al. applied deep learning
in conjunction with domain adaptation techniques but for
inter-session classiﬁcation as opposed to the inter-subject
context of this paper. A thorough overview of deep learning
techniques applied to EMG classiﬁcation is given in .
To the best of our knowledge, this paper, which is an extension
of , is the ﬁrst time inter-user data is leveraged through TL
for training deep learning algorithms on sEMG data.
III. SEMG DATASETS
A. Myo Dataset
One of the major contributions of this article is to provide a
new, publicly available, sEMG-based hand gesture recognition
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 27, NO. 4, APRIL 2019
dataset, referred to as the Myo Dataset. This dataset contains
two distinct sub-datasets with the ﬁrst one serving as the pretraining dataset and the second as the evaluation dataset. The
former, which is comprised of 19 able-bodied participants,
should be employed to build, validate and optimize classi-
ﬁcation techniques. The latter, comprised of 17 able-bodied
participants, is utilized only for the ﬁnal testing. To the best
of our knowledge, this is the largest dataset published utilizing
the commercially available Myo Armband (Thalmic Labs)
and it is our hope that it will become a useful tool for the
sEMG-based hand gesture classiﬁcation community.
The data acquisition protocol was approved by the Comités
d’Éthique de la Recherche avec des êtres humains de
l’Université Laval and informed consent was obtained from all participants.
electromyographic
activity of each subject’s forearm was recorded with the
Myo Armband; an 8-channel, dry-electrode, low-sampling rate
(200Hz), low-cost consumer-grade sEMG armband.
The Myo is non-intrusive, as the dry-electrodes allow
users to simply slip the bracelet on without any preparation.
Comparatively, gel-based electrodes require the shaving and
washing of the skin to obtain optimal contact between the
subject’s skin and electrodes. Unfortunately, the convenience
of the Myo Armband comes with limitations regarding the
quality and quantity of the sEMG signals that are collected.
Indeed, dry electrodes, such as the ones employed in the
Myo, are less accurate and robust to motion artifact than
gel-based ones . Additionally, while the recommended
frequency range of sEMG signals is 5-500Hz requiring
a sampling frequency greater or equal to 1000Hz, the Myo
Armband is limited to 200Hz. This information loss was shown
to signiﬁcantly impact the ability of various classiﬁers to
differentiate between hand gestures . As such, robust and
adequate classiﬁcation techniques are needed to process the
collected signals accurately.
2) Time-Window Length: For real-time control in a closed
loop, input latency is an important factor to consider. A maximum latency of 300ms was ﬁrst recommended in .
Even though more recent studies suggest that the latency
should optimally be kept between 100-250ms , ,
the performance of the classiﬁer should take priority over
speed , . As is the case in , a window size of 260ms
was selected to achieve a reasonable number of samples
between each prediction due to the low frequency of the Myo.
3) Labeled Data Acquisition Protocol: The seven hand/wrist
gestures considered in this work are depicted in Fig. 1. For
both sub-datasets, the labeled data was created by requiring the
user to hold each gesture for ﬁve seconds. The data recording
was manually started by a researcher only once the participant
correctly held the requested gesture. Generally, ﬁve seconds
was given to the user between each gesture. This rest period
was not recorded and as a result, the ﬁnal dataset is balanced
for all classes. The recording of the full seven gestures for
ﬁve seconds is referred to as a cycle, with four cycles forming
a round. In the case of the pre-training dataset, a single round
is available per subject. For the evaluation dataset three rounds
are available with the ﬁrst round utilized for training (i.e. 140s
Fig. 1. The 7 hand/wrist gestures considered in the Myo Dataset.
Examples of the range of armband placements on the subjects’
per participant) and the last two for testing (i.e.
participant).
During recording, participants were instructed to stand up
and have their forearm parallel to the ﬂoor and supported by
themselves. For each of them, the armband was systematically
tightened to its maximum and slid up the user’s forearm, until
the circumference of the armband matched that of the forearm.
This was done in an effort to reduce bias from the researchers,
and to emulate the wide variety of armband positions that
end-users without prior knowledge of optimal electrode placement might use (see Fig. 2). While the electrode placement
was not controlled for, the orientation of the armband was
always such that the blue light bar on the Myo was facing
towards the hand of the subject. Note that this is the case for
both left and right handed subjects. The raw sEMG data of
the Myo is what is made available with this dataset.
Signal processing must be applied to efﬁciently train a
classiﬁer on the data recorded by the Myo armband. The data
is ﬁrst separated by applying sliding windows of 52 samples
(260ms) with an overlap of 235ms (i.e. 7 × 190 samples for
one cycle (5s of data)). Employing windows of 260ms allows
40ms for the pre-processing and classiﬁcation process, while
still staying within the 300ms target . Note that utilizing
sliding windows is viewed as a form of data augmentation
in the present context (see Appendix B). This is done for
each gesture in each cycle on each of the eight channels.
As such, in the dataset, an example corresponds to the eight
windows associated with their respective eight channels. From
there, the processing depends on the classiﬁcation techniques
employed which will be detailed in Sec. IV and V.
B. NinaPro DB5
The NinaPro
sEMG-based gesture recognition algorithms . This dataset,
which was recorded with the Myo Armband, contains data
from 10 able-bodied participants performing a total of 53 different movements (including neutral) divided into three
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
CÔTÉ-ALLARD et al.: DEEP LEARNING FOR ELECTROMYOGRAPHIC HAND GESTURE SIGNAL CLASSIFICATION USING TL
exercise sets. The second exercise set, which contains 17 gestures + neutral gesture, is of particular interest, as it
includes all the gestures considered so far in this work.
The 11 additional gestures which are presented in 
include wrist pronation, wrist supination and diverse ﬁnger
extension amongst others. While this particular dataset was
recorded with two Myo Armband, only the lower armband
is considered as to allow direct comparison to the preceding
1) Data Acquisition and Processing: Each participant was
asked to hold a gesture for ﬁve seconds followed by three seconds of neutral gesture and to repeat this action ﬁve more
times (total of six repetitions). This procedure was repeated
for all the movements contained within the dataset. The ﬁrst
four repetitions serve as the training set (20s per gesture) and
the last two (10s per gesture) as the test set for each gesture.
Note that the rest movement (i.e. neutral gesture) was treated
identically as the other gestures (i.e. ﬁrst four repetitions for
training (12s) and the next two for testing (6s)).
All data processing (e.g. window size, window overlap) are
exactly as described in the previous sections.
IV. CLASSIC SEMG CLASSIFICATION
Traditionally, one of the most researched aspects of
sEMG-based gesture recognition comes from feature engineering (i.e. manually ﬁnding a representation for sEMG signals
that allows easy differentiation between gestures). Over the
years, several efﬁcient combinations of features both in the
time and frequency domain have been proposed – .
This section presents the feature sets used in this work. See
Appendix D for a description of each feature.
A. Feature Sets
As this paper’s main purpose is to present a deep
learning-based TL approach to the problem of sEMG hand
gesture recognition, contextualizing the performance of the
proposed algorithms within the current state-of-the-art is
essential. As such, four different feature sets were taken
from the literature to serve as a comparison basis. The four
feature sets will be tested on ﬁve of the most common classi-
ﬁers employed for sEMG pattern recognition: Support Vector
Machine (SVM) , Artiﬁcial Neural Networks (ANN) ,
Random Forest (RF) , K-Nearest Neighbors (KNN) 
and Linear Discriminant Analysis (LDA) . Hyperparameters for each classiﬁer were selected by employing three fold
cross-validation alongside random search, testing 50 different
combinations of hyperparameters for each participant’s dataset
for each classiﬁer. The hyperparameters considered for each
classiﬁer are presented in Appendix E.
As is often the case, dimensionality reduction is applied ,
 , . LDA was chosen to perform feature projection as
it is computationally inexpensive, devoid of hyperparameters
and was shown to allow for robust classiﬁcation accuracy
for sEMG-based gesture recognition , . A comparison
of the accuracy obtained with and without dimensionality
reduction on the Myo Dataset is given in Appendix F.
This comparison shows that in the vast majority of cases,
the dimensionality reduction both reduced the computational
load and enhanced the average performances of the feature
implementation
classiﬁers
comes from the scikit-learn (v.1.13.1) Python package .
The four feature sets employed for comparison purposes
1) Time Domain Features (TD) : This set of features,
which is probably the most commonly employed in , often
serves as the basis for bigger feature sets
 , , .
As such, TD is particularly well suited to serve as a
baseline comparison for new classiﬁcation techniques. The
four features are:
Absolute Value
(MAV), Zero
Crossing (ZC), Slope Sign Changes (SSC) and Waveform
Length (WL).
2) Enhanced TD : This set of features includes the
TD features in combination with Skewness, Root Mean
Square (RMS), Integrated EMG (IEMG), Autoregression
Coefﬁcients (AR) (P = 11) and the Hjorth Parameters. It was
shown to achieve excellent performances on a setup similar to
the one employed in this article.
3) Nina Pro Features , : This set of features was
selected as it was found to perform the best in the article
introducing the NinaPro dataset. The set consists of the the following features: RMS, Marginal Discrete Wavelet Transform
(mDWT) (wavelet = db7, S = 3), EMG Histogram (HIST)
(bins = 20, threshold = 3σ) and the TD features.
4) SampEn Pipeline : This last feature combination was
selected among ﬁfty features that were evaluated and ranked
to ﬁnd the most discriminating ones. The SampEn feature was
ranked ﬁrst amongst all the others. The best multi-features set
found was composed of: SampEn(m = 2, r = 0.2σ), Cepstral
Coefﬁcient (order = 4), RMS and WL.
V. DEEP LEARNING CLASSIFIERS OVERVIEW
ConvNets tend to be computationally expensive and thus
ill-suited for embedded systems, such as those required
prosthetic.
algorithmic improvements and new hardware architectures
have allowed for complex networks to run on very low
power systems (see Appendix C). As previously mentioned,
the inherent limitations of sEMG-based gesture recognition
force the proposed ConvNets to contend with a limited
amount of data from any single individual. To address the
over-ﬁtting issue, Monte Carlo Dropout (MC Dropout) ,
Batch Normalization (BN) , and early stopping are
A. Batch Normalization
BN is a technique that accelerates training and provides
some form of regularization with the aims of maintaining a standard distribution of hidden layer activation values
throughout training . BN accomplishes this by normalizing
the mean and variance of each dimension of a batch of
examples. To achieve this, a linear transformation based on
two learned parameters is applied to each dimension. This
process is done independently for each layer of the network.
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 27, NO. 4, APRIL 2019
Typical slow-fusion ConvNet architecture . In this graph,
the input (represented by gray rectangles) is a video (i.e. a sequence
of images). The model separates the temporal part of the examples
into disconnected parallel layers, which are then slowly fused together
throughout the network.
Once training is completed, the whole dataset is fed through
the network one last time to compute the ﬁnal normalization
parameters in a layer-wise fashion. At test time, these parameters are applied to normalize the layer activations. BN was
shown to yield faster training times whilst allowing better
generalization.
B. Proposed Convolutional Network Architectures
Videos are a representation of how spatial information (images) change through time. Previous works have
combined this representation with ConvNets to address classi-
ﬁcation tasks , . One such successful algorithm is the
slow-fusion model (see Fig. 3).
When calculating the spectrogram of a signal, the information
structured in
Frequency fashion
(Time × Scale for CWT). When the signal comes from an
array of electrodes, these examples can naturally be structured
as Time × Spatial × Frequency (Time × Spatial × Scale
for CWT). As such, the motivation for using a slow-fusion
architecture based ConvNet in this work is due to the similarities between videos data and the proposed characterization
of sEMG signals, as both representations have analogous
structures (i.e. Time × Spatial × Spatial for videos) and
can describe non-stationary information. Additionally, the proposed architectures inspired by the slow-fusion model were by
far the most successful of the ones tried on the pre-training
1) ConvNet for Spectrograms: The spectrograms, which are
fed to the ConvNet, were calculated with Hann windows of
length 28 and an overlap of 20 yielding a matrix of 4×15. The
ﬁrst frequency band was removed in an effort to reduce baseline drift and motion artifact. As the armband features eight
channels, eight such spectrograms were calculated, yielding a
ﬁnal matrix of 4 × 8 × 14 (Time × Channel × Frequency).
The implementation of the spectrogram ConvNet architecture (Appendix A, Fig. 1) was created with Theano 
and Lasagne . As usual in deep learning, the architecture
was created in a trial and error process taking inspiration
from previous architectures (primarily , , , ).
The non-linear activation functions employed are the parametric exponential linear unit (PELU) and PReLU .
ADAM is utilized for the optimization of the ConvNet
(learning rate = 0.00681292). The deactivation rate for MC
Dropout is set at 0.5 and the batch size at 128. Finally,
to further reduce overﬁtting, early stopping is employed by
randomly removing 10% of the data from the training and
using it as a validation set at the beginning of the optimization
process. Note that learning rate annealing is applied with
a factor of 5 when the validation loss stops improving.
The training stops when two consecutive decays occurs with
no network performance amelioration on the validation set.
All hyperparameter values were found by a random search on
the pre-training dataset.
2) ConvNet for Continuous Wavelet Transforms: The architecture for the CWT ConvNet, (Appendix A, Fig. 2), was
built in a similar fashion as the spectrogram ConvNet one.
Both the Morlet and Mexican Hat wavelet were considered
for this work due to their previous application in EMG-related
work , . In the end, the Mexican Hat wavelet was
selected, as it was the best performing during cross-validation
on the pre-training dataset. The CWTs were calculated with
32 scales yielding a 32 × 52 matrix. Downsampling is then
applied at a factor of 0.25 employing spline interpolation of
order 0 to reduce the computational load of the ConvNet
during training and inference. Following downsampling, similarly to the spectrogram, the last row of the calculated CWT
was removed as to reduce baseline drift and motion artifact.
Additionally, the last column of the calculated CWT was
also removed as to provide an even number of time-columns
from which to perform the slow-fusion process. The ﬁnal
matrix shape is thus 12 × 8 × 7 (i.e. Time × Channel
× Scale). The MC Dropout deactivation rate, batch size,
optimization algorithm, and activation functions remained
unchanged. The learning rate was set at 0.0879923 (found by
cross-validation).
3) ConvNet for Raw EMG: A third ConvNet architecture
taking the raw EMG signal as input is also considered. This
network will help assess if employing time-frequency features
lead to sufﬁcient gains in accuracy performance to justify the
increase in computational cost. As the raw EMG represents
a completely different modality, a new type of architecture
must be employed. To reduce bias from the authors as much
as possible, the architecture considered is the one presented
in . The raw ConvNet architecture can be seen in Appendix A, Fig. 3. This architecture was selected as it was also
designed to classify a hand gesture dataset employing the
Myo Armband. The architecture implementation (in PyTorch
v.0.4.1) is exactly as described in except for the learning rate (= 1.1288378916846883e −5) which was found
by cross-validation (tested 20 uniformly distributed values
between 1e −6 to 1e −1 on a logarithm scale) and extending
the length of the window size as to match with the rest of this
manuscript.
The raw ConvNet is further enhanced by introducing a second convolutional and pooling layer as well as adding dropout,
BN, replacing RELU activation function with PReLU and
using ADAM (learning rate = 0.002335721469090121) as the
optimizer. The enhanced raw ConvNet’s architecture, which is
shown in Appendix A, Fig. 4, achieves an average accuracy
of 97.88% compared to 94.85% for the raw ConvNet. Consequently, all experiments using raw emg as input will employ
the raw enhanced ConvNet.
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
CÔTÉ-ALLARD et al.: DEEP LEARNING FOR ELECTROMYOGRAPHIC HAND GESTURE SIGNAL CLASSIFICATION USING TL
VI. TRANSFER LEARNING
One of the main advantages of deep learning comes from
its ability to leverage large amounts of data for learning.
As it would be too time-consuming for a single individual to
record tens of thousands of examples, this work proposes to
aggregate the data of multiple individuals. The main challenge
thus becomes to ﬁnd a way to leverage data from multiple users, with the objective of achieving higher accuracy
with less data. TL techniques are well suited for such a
task, allowing the ConvNets to generate more general and
robust features that can be applied to a new subject’s sEMG
As the data recording was purposefully as unconstrained
as possible, the armband’s orientation from one subject to
another can vary widely. As such, to allow for the use of TL,
automatic alignment is a necessary ﬁrst step. The alignment
for each subject was made by identifying the most active
channel (calculated using the IEMG feature) for each gesture
on the ﬁrst subject. On subsequent subjects, the channels were
then circularly shifted until their activation for each gesture
matched those of the ﬁrst subject as closely as possible.
A. Progressive Neural Networks
Fine-tuning is the most prevalent TL technique in deep
learning , . It consists of training a model on a
source domain (abundance of labeled data) and using the
trained weights as a starting point when presented with a
new task. However, ﬁne-tuning can suffer from catastrophic
forgetting , where relevant and important features learned
during pre-training are lost on the target domain (i.e. new
task). Moreover, by design, ﬁne-tuning is ill-suited when
signiﬁcant differences exist between the source and the target,
as it can bias the network into poorly adapted features for
the task at hand. Progressive Neural Networks (PNN) 
attempt to address these issues by pre-training a model on
the source domain and freezing its weights. When a new task
appears, a new network, with random initialization, is created
and connected in a layer-wise fashion to the original network.
This connection is done via non-linear lateral connections
(See for details).
B. Adaptive Batch Normalization
In opposition to the PNN architecture, which uses a different
network for the source and the target, AdaBatch employs the
same network for both tasks. The TL occurs by freezing all the
network’s weights (learned during pre-training) when training
on the target, except for the parameters associated with BN.
The hypothesis behind this technique is that the label-related
information (i.e. gestures) rests in the network model weights
whereas the domain-related information (i.e. subjects) is stored
in their BN statistic. In the present context, this idea can be
generalized by applying a multi-stream AdaBatch scheme .
Instead of employing one Source Network per subject during
pre-training, a single network is shared across all participants.
However, the BN statistics from each subject are calculated
independently from one another, allowing the ConvNet to
extract more general and robust features across all participants.
As such, when training the source network, the data from all
subjects are aggregated and fed to the network together. It is
important to note that each training batch is comprised solely
of examples that belong to a single participant. This allows
the update of the participant’s corresponding BN statistic.
C. Proposed Transfer Learning Architecture
The main tenet behind TL is that similar tasks can be completed in similar ways. The difﬁculty in this paper’s context is
then to learn a mapping between the source and target task as
to leverage information learned during pre-training. Training
one network per source-task (i.e. per participant) for the PNN
is not scalable in the present context. However, by training
a Source Network (presented in Sec. V) shared across all
participants of the pre-training dataset with the multi-stream
AdaBatch and adding only a second network for the target
task using the PNN architecture, the scaling problem in the
current context vanishes. This second network will hereafter
be referred to as the Second Network. The architecture of the
Second Network is almost identical to the Source Network.
The difference being in the activation functions employed.
The Source Network leveraged a combination of PReLU and
PELU, whereas the Second Network only employed PELU.
This architecture choice was made through trial and error
and cross-validation on the pre-training dataset. Additionally,
the weights of both networks are trained and initialized independently. During pre-training, only the Source Network is
trained to represent the information of all the participants
in the pre-training dataset. The parameters of the Source
Network are then frozen once pre-training is completed, except
for the BN parameters as they represent the domain-related
information and thus must retain the ability to adapt to new
Due to the application of the multi-stream AdaBatch
scheme, the source task in the present context is to learn the
general mapping between muscle activity and gestures. One
can see the problem of learning such mapping between the
target and the source task as learning a residual of the source
task. For this reason, the Source Network shares information
with the Second Network through an element-wise summation
in a layer-by-layer fashion (see Fig. 4). The idea behind
the merging of information through element-wise summation
is two-fold. First, compared to concatenating the features
maps (as in ) or employing non-linear lateral connections
(like in ), element-wise summation minimizes the computational impact of connecting the Source Network and the
Second Network together. Second, this provides a mechanism
that fosters residual learning as inspired by Residual Networks . Thus, the Second Network only needs to learn
weights that express the difference between the new target
and source task. All outputs from the Source Network layers
to the Second Network are multiplied by learnable coefﬁcients
before the sum-connection. This scalar layer provides an easy
mechanism to neuter the Source Network’s inﬂuence on a
layer-wise level. This is particularly useful if the new target
task is so different that for some layers the information
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 27, NO. 4, APRIL 2019
CLASSIFICATION ACCURACY OF THE CONVNETS ON THE Evaluation Dataset WITH RESPECT TO THE NUMBER OF TRAINING CYCLES PERFORMED
from the Source Network actually hinders learning. Note
that a single-stream scheme (i.e. all subjects share statistics
and BN parameters are also frozen on the Source Network)
was also tried. As expected, this scheme’s performances
started to rapidly worsen as the number of source participants
augmented, lending more credence to the initial AdaBatch
hypothesis.
The combination of the Source Network and Second Network will hereafter be referred to as the Target Network.
An overview of the ﬁnal proposed architecture is presented
in Fig. 4. During training of the Source Network (i.e. pretraining), MC Dropout rate is set at 35% and when training the Target Network the rate is set at 50%. Note that
different architecture choices for the Source Network and
Second Network were required to augment the performance
of the system as a whole. This seems to indicate that the two
tasks (i.e. learning a general mapping of hand gestures and
learning a speciﬁc mapping), might be different enough that
even greater differentiation through specialization of the two
networks might increase the performance further.
VII. CLASSIFIER COMPARISON
A. Myo Dataset
All pre-trainings in this section were done on the pretraining dataset and all training (including for the traditional
machine learning algorithms) were done on the ﬁrst round of
the evaluation dataset.
1) Comparison With Transfer Learning: Considering each
participant as a separate dataset allows for the application of
the one-tail Wilcoxon signed-rank test (n = 17). Table I
shows a comparison of each ConvNet with their TL augmented
version. Accuracies are given for one, two, three and four
cycles of training.
2) Comparison With State of the Art: A comparison between
the proposed CWT-based ConvNet and a variety of classiﬁers
trained on the features sets presented in Sec. IV-A is given in
The PNN-inspired architecture. This ﬁgure represents the case
with the spectrogram ConvNet. Note that the TL behavior is the same for
the Raw-based or CWT-based ConvNet. C1,2,3 and F.C.4,5 correspond
to the three stages of convolutions and two stages of fully connected
layers respectively. The Si (i = 1..5) boxes represent a layer that scales
its inputs by learned coefﬁcients. The number of learned coefﬁcients in
one layer is the number of channels or the number of neurons for the
convolutional and fully connected layers respectively. For clarity’s sake,
the slow fusion aspect is omitted from the representation although they
are present for both the spectrogram and CWT-based ConvNet). The +
boxes represent the merging through an element-wise summation of the
ConvNets’ corresponding layers.
As suggested in , a two-step procedure is employed to
compare the deep learning algorithms with the current stateof-the-art. First, Friedman’s test ranks the algorithms amongst
each other. Then, Holm’s post-hoc test is applied (n = 17)
using the best ranked method as a comparison basis.
B. NinaPro Dataset
1) Comparison With Transfer Learning: Performance of the
proposed ConvNet architecture alongside their TL augmented
versions are investigated on the NinaPro DB5. As no speciﬁc
pre-training dataset is available for the NinaPro DB5, the pretraining for each participant is done employing the training
sets of the remaining nine participants. Table III shows the
average accuracy over the 10 participants of the NinaPro DB5
for one to four cycles. Similarly to Sec. VII-A.1, the one-tail
Wilcoxon Signed rank test is performed for each cycle between
each ConvNet and their TL augmented version.
2) Comparison
Sec. VII-A.2, a comparison between the TL-augmented ConvNet and the traditional classiﬁer trained on the state-of-theart feature set is given in Table IV. The accuracies are given
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
CÔTÉ-ALLARD et al.: DEEP LEARNING FOR ELECTROMYOGRAPHIC HAND GESTURE SIGNAL CLASSIFICATION USING TL
CLASSIFIERS COMPARISON ON THE Evaluation Dataset WITH RESPECT TO THE NUMBER OF TRAINING CYCLES PERFORMED
CLASSIFICATION ACCURACY OF THE CONVNETS ON THE NinaPro DB5 WITH RESPECT TO THE NUMBER OF TRAINING CYCLES PERFORMED
for one, two, three and four cycles of training. A two-step
statistical test with the Friedman test as the ﬁrst step and Holm
post-hoc as the second step is again employed.
3) Out-of-Sample Gestures: A ﬁnal test involving the
NinaPro DB5 was conducted to evaluate the impact on the
proposed TL algorithm when the target is comprised solely of
out-of-sample gestures (i.e. never-seen-before gestures). To do
so, the proposed CWT ConvNet was trained and evaluated on
the training and test set of the NinaPro DB5 as described
before, but considering only the gestures that were absent
from the pre-training dataset (11 total). The CWT ConvNet
was then compared to its TL augmented version which was
pre-trained on the pre-training dataset. Fig. 5 presents the
accuracies obtained for the classiﬁers with different number of
repetitions employed for training. The difference in accuracy
is considered statistically signiﬁcant by the one-tail Wilcoxon
Signed rank test for all cycles of training. Note that, similar,
statistically signiﬁcant results were obtained for the raw-based
and spectrogram-based ConvNets.
VIII. REAL-TIME CLASSIFICATION AND MEDIUM TERM
PERFORMANCES (CASE STUDY)
This last experiment section proposes a use-case study of
the online (i.e. real-time) performance of the classiﬁer over a
period of 14 days for eight able-bodied participants. In previous literature, it has been shown that, when no re-calibration
occur, the performance of a classiﬁer degrades over time due
to the non-stationary property of sEMG signals . The main
goal of this use-case experiment is to evaluate if users are able
to self-adapt and improve the way they perform gestures based
on visual feedback from complex classiﬁers (e.g. CWT+TL),
thus reducing the expected classiﬁcation degradation.
To achieve this, each participant recorded a training set as
described in Sec. III. Then, over the next fourteen days, a daily
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 27, NO. 4, APRIL 2019
CLASSIFIERS COMPARISON ON THE NinaPro DB5 WITH RESPECT TO THE NUMBER OF REPETITIONS USED DURING TRAINING.
Classiﬁcation accuracy of the CWT-based ConvNets on the
NinaPro DB5 with respect to the number of repetitions employed during training. The pre-training was done using the pre-training dataset.
Training and testing only considered the 11 gestures from the NinaPro
DB5 not included in the pre-training. The error bars correspond to the
STD across all ten participants.
Average accuracy over 14 days without recalibration of the
CWT+TL ConvNet The blue circles represent data from the Feedback
group whereas the orange triangles represent data from the Without
Feedback group. The translucent bands around the linear regressions
represent the conﬁdence interval (95%) estimated by bootstrap.
session was recorded based on the participant’s availability.
A session consisted of holding a set of 30 randomly selected
gestures (among the seven shown in Fig. 1) for ten seconds
each, resulting in ﬁve minutes of continuous sEMG data. Note
that to be more realistic, the participants began by placing
The average accuracy of the eight participants over all the
ﬁve minute sessions recorded to evaluate the effect of muscle fatigue
on the classiﬁer performance. During each session of the experiment,
participants were asked to hold a total of 30 random gestures for ten seconds each. As such, a dot represents the average accuracy across all
participants over one of the ten second periods. The translucent bands
around the linear regression represent the conﬁdence intervals (95%)
estimated by bootstrap.
the armband themselves, leading to slight armband position
variations between sessions.
The eight participants were randomly separated into two
equal groups. The ﬁrst group, referred to as the Feedback
group, received real-time feedback on the gesture predicted
by the classiﬁer in the form of text displayed on a computer
screen. The second group, referred to as the Without Feedback group, did not receive classiﬁer feedback. The classiﬁer
employed in this experiment is the CWT+TL, as it was the
best performing classiﬁer tested on the Evaluation Dataset.
Because the transitions are computer-speciﬁed, there is a
latency between a new requested gesture and the participant’s
reaction. To reduce the impact of this phenomenon, the data
from the ﬁrst second after a new requested gesture is ignored
from this section results. The number of data points generated
by a single participant varies between 10 and 16 depending on the participant’s availability during the experiment
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
CÔTÉ-ALLARD et al.: DEEP LEARNING FOR ELECTROMYOGRAPHIC HAND GESTURE SIGNAL CLASSIFICATION USING TL
The proposed spectrogram ConvNet architecture to leverage spectrogram examples employing 67 179 learnable parameters. To allow the
slow fusion process, the input is ﬁrst separated equally into two parts with respect to the time axis. The two branches are then fused together by
element-wise summing the feature maps together. In this ﬁgure, Conv refer to Convolution and F.C. to Fully Connected layers.
As it can be observed in Fig. 6, while the Without Feedback
group did experience accuracy degradation over the 14 days,
the Feedback group was seemingly able to counteract this
degradation. Note that, the average accuracy across all participants for the ﬁrst recording session was 95.42%.
Many participants reported experiencing muscular fatigue
during the recording of both this experiment and the evaluation
dataset. As such, in an effort to quantify the impact of
muscle fatigue on the classiﬁer’s performance, the average
accuracy of the eight participants over the ﬁve minute session
is computed as a function of time. As can be observed from
the positive slope of the linear regression presented in Fig. 7,
muscle fatigue, does not seem to negatively affect the proposed
ConvNet’s accuracy.
IX. DISCUSSION
III show that,
TL augmented ConvNets signiﬁcantly outperformed their
non-augmented versions, regardless of the number of training
cycles. As expected, reducing the amount of training cycles
systematically degraded the performances of all tested methods (see Table I, II, III, IV and Fig. 5), with the non-TL
ConvNets being the most affected on the Myo Dataset. This is
likely due to overﬁtting that stems from the small size of the
dataset. However, it is worth noting that, when using a single
cycle of training, augmenting the ConvNets with the proposed
TL scheme signiﬁcantly improves their accuracies. In fact,
with this addition, the accuracies of the ConvNets become
the highest of all methods on both tested datasets. Overall,
the proposed TL-augmented ConvNets were competitive with
the current state-of-the-art, with the TL augmented CWT-based
ConvNet achieving a higher average accuracy than the traditional sEMG classiﬁcation technique on both datasets for all
training cycles. It is also noteworthy that while the raw+TL
ConvNet was the worst amongst the TL augmented ConvNet
on the Myo Dataset, it achieved the highest accuracy on the
NinaPro DB5. Furthermore, the TL method outperformed the
non-augmented ConvNets on the out-of-sample experiment.
The difference in accuracy of the two methods was deemed
signiﬁcant by the Wilcoxon Signed Rank Test (p < 0.05)
for all training repetitions. This suggests that the proposed
TL algorithm enables the network to learn features that can
generalize not only across participants but also for never-seenbefore gestures. As such, the weights learned from the pretraining dataset can easily be re-used for other work that
employs the Myo Armband with different gestures.
While in this paper, the proposed source and second network were almost identical they are performing different
tasks (see Sec. VI-C). As such further differentiation of
both networks might lead to increased performance. At ﬁrst
glance, the element-wise summation between the source and
second network might seem to impose a strong constraint on
the architecture of the two networks. However, one could
replace the learned scalar layers in the target network by
convolutions or fully connected layers to bridge the dimensionality gap between potentially vastly different source and
second networks.
Additionally, a difference in the average accuracy between
the real-time experiment (Sec. VIII) and the Evaluation
Dataset (Sec. VII-A.2) was observed (95.42% vs 98.31%
respectively). This is likely due to the reaction delay of the
participants, but more importantly to the transition between
gestures. These transitions are not part of the training dataset,
because they are too time consuming to record as the number
of possible transitions equals n2 −n where n is the number of
gestures. Consequently, it is expected that the classiﬁers predictive power on transition data is poor in these circumstances.
As such, being able to accurately detect such transitions in an
unsupervised way might have a greater impact on the system’s
responsiveness than simply reducing the window size. This and
the aforementioned point will be investigated in future works.
The main limitation of this study is the absence of tests
with amputees. Additionally, the issue of electrode shifts has
not been explicitly studied and the variability introduced by
various limb positions was not considered when recording the
dataset. A limitation of the proposed TL scheme is its difﬁculty
to adapt when the new user cannot wear the same amount of
electrodes as the group used for pre-training. This is because
changing the number of channels changes the representation
of the phenomena (i.e. muscle contraction) being fed to the
algorithm. The most straightforward way of addressing this
would be to numerically remove the relevant channels from the
dataset used for pre-training. Then re-running the proposed TL
algorithm on an architecture adapted to the new representation
fed as input. Another solution is to consider the EMG channels
Authorized licensed use limited to: BIBLIOTHEQUE DE L'UNIVERSITE LAVAL. Downloaded on January 30,2023 at 17:44:00 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 27, NO. 4, APRIL 2019
in a similar way as color channels in image. This type of
architecture seems, however, to perform worse than the ones
presented in this paper (see Appendix G).
X. CONCLUSION
This paper presents three novel ConvNet architectures that
were shown to be competitive with current sEMG-based classi-
ﬁers. Moreover, this work presents a new TL scheme that systematically and signiﬁcantly enhances the performances of the
tested ConvNets. On the newly proposed evaluation dataset,
the TL augmented ConvNet achieves an average accuracy
of 98.31% over 17 participants. Furthermore, on the NinaPro
DB5 dataset (18 hand/wrist gestures), the proposed classiﬁer
achieved an average accuracy of 68.98% over 10 participants
on a single Myo Armband. This dataset showed that the
proposed TL algorithm learns sufﬁciently general features to
signiﬁcantly enhance the performance of ConvNets on outof-sample gestures. Showing that deep learning algorithms
can be efﬁciently trained, within the inherent constraints of
sEMG-based hand gesture recognition, offers exciting new
research avenues for this ﬁeld.
Future works will focus on adapting and testing the proposed TL algorithm on upper-extremity amputees. This will
provide additional challenges due to the greater muscle variability across amputees and the decrease in classiﬁcation accuracy compared to able-bodied participants . Additionally,
tests for the application of the proposed TL algorithm for
inter-session classiﬁcation will be conducted as to be able to
leverage labeled information for long-term classiﬁcation.
ACKNOWLEDGEMENTS
F. Laviolette and B. Gosselin share senior authorship.