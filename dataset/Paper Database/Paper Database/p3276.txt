HAL Id: hal-02060044
 
Submitted on 7 Mar 2019
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Distributed under a Creative Commons Attribution 4.0 International License
Evaluating Explanations by Cognitive Value
Ajay Chander, Ramya Srinivasan
To cite this version:
Ajay Chander, Ramya Srinivasan. Evaluating Explanations by Cognitive Value. 2nd International
Cross-Domain Conference for Machine Learning and Knowledge Extraction (CD-MAKE), Aug 2018,
Hamburg, Germany. pp.314-328, ￿10.1007/978-3-319-99740-7_23￿. ￿hal-02060044￿
Evaluating Explanations by Cognitive Value
Ajay Chander and Ramya Srinivasan
Fujitsu Laboratories of America, Sunnyvale, CA, 94085, USA
Abstract. The transparent AI initiative has ignited several academic
and industrial endeavors and produced some impressive technologies and
results thus far. Many state-of-the-art methods provide explanations that
mostly target the needs of AI engineers. However, there is very little work
on providing explanations that support the needs of business owners,
software developers, and consumers who all play signiﬁcant roles in the
service development and use cycle. By considering the overall context
in which an explanation is presented, including the role played by the
human-in-the-loop, we can hope to craft eﬀective explanations. In this
paper, we introduce the notion of the “cognitive value” of an explanation
and describe its role in providing eﬀective explanations within a given
context. Speciﬁcally, we consider the scenario of a business owner seeking
to improve sales of their product, and compare explanations provided by
some existing interpretable machine learning algorithms (random forests,
scalable Bayesian Rules, causal models) in terms of the cognitive value
they oﬀer to the business owner. We hope that our work will foster future
research in the ﬁeld of transparent AI to incorporate the cognitive value
of explanations in crafting and evaluating explanations.
Keywords: Explanations · AI · Cognitive Value· Business Owner · Causal
Introduction
Consumers, policymakers, and technologists are becoming increasingly concerned
about AI as a ‘black-box’ technology. In order to engender trust in the user and
facilitate comfortable interactions, it has become increasingly important to create AI systems that can explain their decisions to their users. Across a variety
of ﬁelds, from healthcare to education to law enforcement and policy making,
there exists a need for explaining the decisions of AI systems. In response to this,
both the scientiﬁc and industrial communities have shown a growing interest in
making AI technologies more transparent. The new European General Data
Protection Regulation, the U.S. Defense Advanced Research Projects Agency’s
XAI program , and institutional initiatives to ensure the safe development of
AI such as those of the Future of Life Institute, are a few of the many business, research, and regulatory incentives being created to make AI systems more
transparent.
Many state-of-the-art methods provide explanations that mostly target the
needs of AI engineers . In other words, explanations assume some domain knowledge, or are generated for people with domain expertise. As the use of
Ajay Chander and Ramya Srinivasan
AI becomes widespread, there is an increasing need for creating AI systems that
can explain their decisions to a large community of users who are not necessarily
domain experts. These users could include software engineers, business owners,
and end-users. By considering the overall context in which an explanation is
presented, including the role played by the human-in-the-loop, we can hope to
craft eﬀective explanations.
Cognitive Value of an Explanation
The role of explanations and the way they should be structured is not new
and dates back to the time of Aristotle . The authors in highlight the
functions of explanations. They mention that explanations should accommodate
novel information in the context of prior beliefs, and do so in a way that fosters
generalization. Furthermore, researchers have also studied if certain structures
of an explanation are inherently more appealing than others . The authors in
 state that explanations are social in that they are meant to transfer knowledge, presented as part of a conversation or interaction and are thus presented
relative to the explainer’s beliefs about the user’s (i.e., explainee’s) beliefs.
We posit that an explanation is a ﬁlter on facts, and is presented and consumed as part of a larger context. Here, fundamental aspects of the context
include: the entity presenting the explanation (“explainer”), the entity consuming the explanation (“explainee”), the content of the explanation itself, where
the explanation is being presented, amongst others.
Let’s ﬁrst understand the role of the explainee as it is the most crucial element
of an explanation’s context. As discussed earlier, a wide variety of users are now
interested in understanding the decisions of AI systems. There are at least four
distinct kinds of users .
– AI Engineers: These are generally people who have knowledge about the
mathematical theories and principles of various AI models. These people are
interested in explanations of a functional nature, e.g. the eﬀects of various
hyperparameters on the performance of the network or methods that can be
used for model debugging.
– Software Developers and/or Integrators: These are application builders who
make software solutions. These users often make use of oﬀ-the-shelf AI modules, and integrate them with various software components. Developers are
interested in explanation methods that allow them to seamlessly integrate
various AI module into the use cases of their interest.
– Business Owners: These people are usually stakeholders who own the service and are interested in commercialization. The owner is concerned with
explainability aspects that can elucidate ways in which the application can
be improved to increase ﬁnancial gains, to justify predictions in order to aid
in product design and people management, etc.
– End-Users: These are consumers of the AI service. These people are interested in understanding why certain recommendations were made, how they
can use the information provided by the AI, how the recommendations will
beneﬁt them, etc.
Evaluating Explanations by Cognitive Value
As described above, users expect certain “cognitive values” from the explanations of AI systems. The term cognitive value can be best explained via
examples. Some users may primarily expect explanations to account for personal
values (e.g., privacy, safety, etc.) in the recommendations made by AI systems.
In this case, the cognitive value of the explanation is to engender trust in the
user. Some other users may largely expect explanations to be elucidating functional aspects of the AI models such as accuracy, speed and robustness; here the
cognitive value of explanation is in aiding troubleshooting and/or design. Some
users may expect explanations to help them understand the AI’s recommendation and aid them in analysis; in this case the cognitive value of explanation is
in educating the user and help them take an appropriate action. Based on the
task, any of the aforementioned cognitive values may be important to any of the
user-types described. There could be many more cognitive values, but we believe
that trust, troubleshooting, design, education and action are the most important
cognitive values.
Thus, it becomes important to evaluate explanations based on their cognitive
value in a given context. As an example, consider a business executive who wants
to understand how to improve sales of the company. So, the operational goals of
the explanation is largely in aiding action (i.e., the AI should help the business
executive in specifying the steps that need to be taken in order to improve sales)
and in education (i.e., the AI should inform the executive of the factors that
determine sales, etc.). Consider some hypothetical explanations generated by an
AI system as listed below.
– Factors X and Y are the most important factors in determining sales
– Factors X and Y are the most important factors in determining sales, whenever X > 5 and Y < 4, the sales is 90%.
– Factors X and Y are the most important factors responsible for sales in the
past. Changing X to X+10 will improve the sales by 5%.
At a high-level, all of the aforementioned explanations look reasonable. Let us
delve a little deeper. Suppose X was the amount of the product and Y was
the location of the sale. Now, in explanation 2, the phrase “Y < 4” does not
convey a semantic meaning to the business owner. To the AI engineer, it may be
still meaningful as the model might have mapped various locations to numbers.
However, the business owner is not aware about this encoding. Even if she was
made aware of what the individual numbers denoted ( such as if the location is
NYC, Tokyo, or Hamburg), as the number of such choices increases, the cognitive
burden on the business owner increases and does not aid in educating him/her
or aiding in their action of how they can improve sales. Although explanation 1
provides semantically relevant information, it does not help the business owner
in providing actionable insights in improving the sales. Explanation 3 not only
educates the business owner in terms of the most important factors for improving
sales, but more importantly also aids in action by suggesting how the sales can
be improved.
The contributions of the paper are as follows: First, to the best of our knowledge, our work is the ﬁrst to introduce the notion of “cognitive value” of an ex-
Ajay Chander and Ramya Srinivasan
planation and elaborate on the role of cognitive values in providing explanations
to various kinds of users. Second, we compare three state-of-the-art explanation
methods namely Scalable Bayesian Rule Lists , Random Forests, and Causal
models in terms of their cognitive value to the business owner. In particular,
through a case study of a car dealer who is wanting to improve car sales, we
show how causal models designed for explaining issues concerning fairness and
discrimination can be modiﬁed to provide explanations of cognitive value to
this car dealer. Third, we discuss the merits and shortcomings of each of the
aforementioned methods. We hope that our work will foster future research in
the ﬁeld of transparent AI to incorporate the cognitive value of explanations in
evaluating the AI-generated explanations.
The rest of the paper is organized as follows. An overview of related work
is provided in Section 2. The case study and the dataset is described in Section
3. Section 4 provides background on causal models, scalable bayesian rule lists
and random forest algorithms. It also includes a description of how the causal
model proposed in for detecting discrimination can be leveraged to provide
explanations of cognitive value. The types of explanations obtained from the
three models are summarized in Section 5. A discussion of the relative merits
and shortcomings of the explanations obtained by each of the aforementioned
methods is also provided in Section 5. Conclusions are provided in Section 6.
Related Work
The new European General Data Protection Regulation (GDPR and ISO/IEC
27001) and the U.S. Defense Advanced Research Projects Agency’s XAI program are probably the most important initiatives towards transparent AI. As
a result, several academic as well as industrial groups are looking to address issues concerning AI transparency. Subsequently, a series of workshops, industrial
meetings and discussion panels related to the area have taken place contributing
to some impressive results.
Most of the work in the area is oriented towards the AI engineer and is
technical. For example, in , the authors highlight the regions in an image
that were most important to the AI in classifying it. However, such explanations
are not useful to an end-user in either understanding the AI’s decision or in
debugging the model . In , the authors discuss the main factors used by
the AI system in arriving at a certain decision and also discuss how changing
a factor changes the decision. This kind of explanation helps in debugging for
the AI engineers. Researchers are also expanding the scope of explanations to
AI agents by proposing frameworks wherein an AI agent explains its behavior
to its supervisor . The authors in propose a model agnostic explanation
framework and has been instrumental in several subsequent research eﬀorts.
There are several other impressive works across various ﬁelds catered towards
helping the AI engineers . A nice summary concerning
explainability from an AI engineer’s perspective is provided in .
Evaluating Explanations by Cognitive Value
More recently, there have been eﬀorts in understanding the human interpretability of AI systems. The authors in provide a taxonomy for human
interpretability of AI systems. A nice non-AI engineer perspective regarding
explanations of AI system is provided in . The authors in studied how
explanations are related to user trust. They conducted a user study on healthcare
professionals in AI-assisted clinical decision systems to validate their hypotheses.
A nice perspective of user-centered explanations is provided in . The author
emphasizes the need for persuasive explanations. The authors in explore
the notion of interactivity from the lens of the user. With growing interest in
the concept of interpretability, various measures for quantifying interpretability
have also been proposed in .
The closest to our work is perhaps wherein the authors discuss how
humans understand explanations from machine learning systems through a userstudy. The metrics used to measure human interpretability are those concerning
explanation length, number of concepts used in the explanation, and the number
of repeated terms. Interpretability is measured in terms of the time to response
and the accuracy of the response. Our measure is on the cognitive value an
explanation oﬀers as opposed to time to response or other such quantitative
Case Study and Dataset
Our focus is on non-AI engineers. As a case study, we consider a scenario involving a business owner. Speciﬁcally, we consider a car dealer who wants to
improve the sales of the cars. Thus, this user will beneﬁt from knowing the steps
that need to be taken in order to increase the sales of the cars. Thus, the cognitive value an explanation oﬀers in this scenario should be in guiding towards an
appropriate action and justifying the same.
We consider the car evaluation dataset for our analysis, obtained
from the UCI Machine learning repository. Although relatively an old dataset,
it is appropriate for the problem at hand. The dataset is a collection of six attributes of cars as listed in Table 1. In the original dataset, the output attributes
are “acceptable”, “unacceptable”, “good”, and “very good”. For the speciﬁc case
study considered, we map acceptance to sales. For evaluation purposes, we map
probability of acceptability to probability of selling the car and probability of
unacceptability to probability of not being able to sell the car. There are 1728
instances in the dataset. The car dealer is interested in knowing what changes
need to be done i.e., what factors of the cars need to be changed in order to
improve sales.
Background
We consider three state-of-the-art algorithms for comparing the cognitive value
they oﬀer in the context of the aforementioned case study. We consider Random
forests as this model is one of the earliest interpretable models. We also
Ajay Chander and Ramya Srinivasan
Table 1. Description of input variables in the Car Evaluation Dataset.
Input Variable
Buying price
vhigh, high, med, low
Price of the maintenance
vhigh, high, med, low
Number of doors
2, 3, 4, 5more
Persons capacity in terms of persons to carry 2, 4, more
Size of luggage boot
small, med, big
Estimated safety of the car
low, med, high
consider two recent models scalable Bayesian rules proposed in 2017, and
causal models proposed in 2018. For completeness, we provide some basic
background about these models in the context of interpretability.
Random Forests
Random forests are a class of ensemble learning methods for classiﬁcation and
regression tasks . The training algorithm for random forests applies the general
technique of bootstrap aggregating, or bagging, to tree learners. Given a training
set X = x1, ..., xn with labels Y = y1, ..., yn, bagging repeatedly (B times) selects
a random sample with replacement of the training set and ﬁts trees to these
samples, i.e.,
For b = 1, ..., B :
1. Sample, with replacement, n training examples from X, Y; call these Xb, Yb.
2. Train a classiﬁcation or regression tree fb on Xb, Yb.
After training, predictions for unseen samples x’s can be made by averaging the
predictions from all the individual regression trees on x’s or by taking a majority
vote in the case of classiﬁcation trees.
When considering a decision tree, for each decision that a tree (or a forest)
makes there is a path (or paths) from the root of the tree to the leaf, consisting
of a series of decisions, guarded by a particular feature, each of which contribute
to the ﬁnal predictions. The decision function returns the probability value at
the leaf nodes of the tree and the importance of individual input variables can
be captured in terms of various metrics such as the Gini impurity.
Scalable Bayesian Rule Lists (SBRL)
SBRLs are a competitor for decision tree and rule learning algorithms in terms of
accuracy, interpretability, and computational speed . Decision tree algorithms
are constructed using greedy splitting from the top down. They also use greedy
pruning of nodes. They do not globally optimize any function, instead they
are composed entirely of local optimization heuristics. If the algorithm makes a
mistake in the splitting near the top of the tree, it is diﬃcult to undo it, and
consequently the trees become long and uninterpretable, unless they are heavily
pruned, in which case accuracy suﬀers . SBRLs overcome these shortcomings
of decision trees.
Evaluating Explanations by Cognitive Value
Bayesian Rule Lists is an associative classiﬁcation method, in the sense that
the antecedents are ﬁrst mined from the database, and then the set of rules and
their order are learned. The rule mining step is fast, and there are fast parallel
implementations available. The training set is (xi, yi)n
i , where the xi ∈X encode
features, and yi are labels, which are generally either 0 or 1. The antecedents
are conditions on the x that are either true or false. For instance, an antecedent
could be: if x is a patient, antecedent aj is true when the value of x is greater
than 60 years and x has diabetes, otherwise false. Scalable Bayesian Rule Lists
maximizes the posterior distribution of the Bayesian Rule Lists algorithm by
using a Markov Chain Monte Carlo method. We refer interested readers to 
for greater details related to the working of the algorithm.
Causal Models
Causal models are amenable towards providing explanations as they naturally
uncover the cause-eﬀect relationship . Before describing how causal models
can be used to elicit explanations, we list some basic deﬁnitions used.
Terminologies: A structural causal model (SCM) M is a tuple h = [U, V, F, P(U)i]
where: U is a set of exogenous (unobserved) variables, which are determined by
factors outside of the model; V is a set V1, ..., Vn of endogenous (observed) variables that are determined by variables in the model; F is a set of structural
functions f1, ..., fn where each fi is a process by which Vi is assigned a value vi;
P(u) is a distribution over the exogenous variables U .
Each SCM M is associated with a causal diagram G, which is a directed
acyclic graph where nodes correspond to the endogenous variables (V ) and
the directed edges denote the functional relationships. An intervention, denoted
by do(X = x) , represents a model manipulation where the values of a set
of variables X are set ﬁxed to x regardless of how their values are ordinarily
determined (fx). The counterfactual distribution is represented by P(YX=x = y)
denotes the causal eﬀect of the intervention do(X = x) on the outcome Y , where
the counterfactual variable YX=x (Yx , for short) denotes the potential response
of Y to intervention do(X = x). We will consistently use the abbreviation P(yx)
for the probabilities P(YX=x = y), so does P(y|x) = P(Y = y|X = x).
For our analysis, we consider a standard model provided in as depicted
in Figure 1. We wish to determine the eﬀect of X on Y ( say X= safety and
Y = car sales). In this context, X would be the input factor and Y would be the
output factor. There could be other factors Z and W aﬀecting sales as shown
in Figure 1. Here the factor Z is a common cause and is often referred to as a
confounder. The factor W is called a mediator, because X could have a causal
eﬀect on Y through W.
There are three types of causal eﬀects deﬁned with respect to Fig. 1. The
direct eﬀect is modeled by the direct causal path X →Y in Fig. 1. Indirect eﬀect
is modeled by the path X →W →Y and spurious eﬀect is jointly modeled by
the paths Z →X and Z →Y .
Ajay Chander and Ramya Srinivasan
Fig. 1. Structural Causal Model considered for our analysis.
For each SCM, one can obtain the direct, indirect and spurious eﬀects of X
on Y . In particular, the authors in deﬁne the concepts of counterfactual direct
eﬀect, counterfactual indirect eﬀect and counterfactual spurious eﬀects in order
to estimate the discover discrimination and argue that by disentangling each of
the causal eﬀects, it can be ascertained whether there was genuine discrimination
or not. The direct, (D.E.) indirect (I.E) and spurious (S.E.) causal eﬀects of
changing the various factors on the output can be obtained from the following
equations as provided in . For more elaborate details, we refer readers to .
D.Ex0,x1(y|x) =
((P(y|x1, w, z) −P(y|x0, w, z))P(w|x0, z)P(z|x)
I.Ex0,x1(y|x) =
(P(y|x0, w, z)(P(w|x1, z) −P(w|x0, z))P(z|x)
S.Ex0,x1(y) =
(P(y|x0, w, z)P(w|x0, z(P(z|x1) −P(z|x0))
Adaptation of Causal Models to provide Cognitive Value:
Although the purpose of the authors of was to explain discrimination, it is
straightforward to extend this to obtain explanations that can oﬀer cognitive values. Below, we describe the steps that need to be followed to obtain explanations
of cognitive value.
Put in other words, we consider all possible SCMs for the choice of factors
[X, Z, W] as input, mediator and confounder. Note, for the standard model considered, only one confounder and one mediator is allowed. For the car evaluation
dataset, we consider 4 factors for each SCM. Let us understand the above process
for the car evaluation dataset.
Evaluating Explanations by Cognitive Value
1 Estimate the counterfactual direct eﬀects for all possible combinations of SCMs
for a given input X and output Y .
2 Repeat Step 1 for all possible choice of input factors X.
3 For each choice of input factor, generate textual statements highlighting the
diﬀerential probability in output (e.g. diﬀerential probability in selling car) for
change in the value of the input factor (e.g. changing the safety of the car from
low to high).
4 The factors corresponding to highest diﬀerential probabilities oﬀer the most
cognitive value (i.e. to increase sales) to the user (e.g. a car dealer).
Let us understand the usability of the aforementioned algorithm for the case
study considered. We are interested in explaining how to improve the sales to
the business owner (who is the car dealer in this example). So, the factor Y
corresponds to sales. Suppose, X =safety and Y = sale. In the model shown in Fig
1, one possibility could be W = number of persons and Z could be maintenance.
This means, safety could aﬀect car sales through the factor number of persons,
and the factor maintenance could be a confounder aﬀecting both safety and sales.
Another possibility could be that W = maintenance and Z could be number of
persons. In this case, the factor number of persons is a confounder and aﬀects
both sales and safety, and maintenance is a mediator.
Let us ﬁrst consider the case wherein X is safety, Z is maintenance and let W
be number of persons. Putting this in the standard model of Fig.1 and using Eq.
1, we can estimate the counterfactual direct eﬀect of safety on sales. The concept of counterfactual direct eﬀect can be best understood through an example.
Suppose there is a car with low safety. All other factors unchanged, if the factor
safety alone were to be changed to high, then the quantity “counterfactual direct
eﬀect” can provide a measure of the improvement in sales for this factor change.
Please note, in reality, since all the cars are manufactured, none of the factors
can be changed. But, for the time being, assume an imaginary car whose factors
can be changed. In that scenario, if the safety of the imaginary car were to be
high, then one can ascertain if that change in safety contributes to rise or fall of
sales and by how much. Knowing this diﬀerential sales probability will help in
future design of such cars for the car dealer. Thus, it provides the cognitive value
in taking appropriate action to the car dealer. We compute counterfactual direct
eﬀect for all possible choices of input factors X. Since the output factor is the
sales, we conclude that factors with the highest magnitude of the counterfactual
direct eﬀect are the most important ones for the car dealer in improving the
Dataset Analysis and Results
In this section, we state the results obtained from each of the three methods
discussed in Section 4. We compare the three methods in terms of their cognitive
value to the car dealer.
Ajay Chander and Ramya Srinivasan
Results from Random Forests
The algorithm returns the probability value of sales for individual cars. In addition, variable importance scores in terms of mean decreasing impurity is provided
that explains the importance of individual factors (i.e. safety,number of persons,
etc.) in determining the sale of a car. Table 2 lists the variable importance scores
in terms of mean decreasing Gini.
Table 2. Results from Random Forest Algorithm.
Input Factor
Importance
Buying price
Price of the maintenance
Number of doors
Persons capacity in terms of persons to carry 178.52
Size of luggage boot
Estimated safety of the car
It is apparent from the above table that safety and number of persons that
can be accommodated are the most important factors in determining the sales of
the cars. This information can certainly educate the car dealer about the most
important factors that determine the sales.
Let us next consider the result from scalable Bayes Rules List. As stated earlier,
it is in the form of “if-then” associative rules. The results for the car evaluation
dataset is as follows. Please note, the phrase ‘positive probability’ refers to the
sale of the car. The rule numbers are generated by the algorithm and simply
refer to the condition mentioned beside it in text. For example, rule refers
to the condition ‘number of persons =2’.
If [persons=2] (rule ) then positive probability = 0.00173010
else if [safety=low] (rule ) then positive probability = 0.00259067
else if [doors=2,lug-boot=small] (rule ) then positive probability = 0.28787879
else if [buying=med,safety=high] (rule ) then positive probability = 0.98888889
else if [buying=low] (rule ) then positive probability = 0.94382022
else if [maint=vhigh,lug-boot=small] (rule ) then positive probability = 0.03125000
else if [buying=med] (rule ) then positive probability = 0.84523810
else if [lug-boot=small,safety=med] (rule ) then positive probability = 0.02631579
else if [maint=vhigh] (rule ) then positive probability = 0.01515152
else if [lug-boot=med,safety=med] (rule ) then positive probability = 0.52000000
else if [buying=high] (rule ) then positive probability = 0.98913043
else if [maint=high] (rule ) then positive probability = 0.03125000
Evaluating Explanations by Cognitive Value
Thus, SBRLs provide various conditions and state the probability of sales
under that condition. Thus, if the number of persons is 2, the probability of
sales is 0.1%.
Causal Models
Table 3 provides a summary of the results obtained from the causal model described in Section 4.4.
Table 3. Results from Causal Explanation.
Input Factor
Real Car Imaginary Car Diﬀerential Probabilities ( expressed as %)
in selling Real car- selling Imaginary car
Number of persons 2
Number of persons 4
Maintenance
Maintenance
The results summarized in Table 3 can be understood via examples. As an
instance, consider the ﬁrst row corresponding to safety. The result of that row
states - “All other factors unchanged, if the safety of the car is changed from low
to high, there will be 36.36% improvement in sales. The next row corresponding
to safety reads thus : “All other factors unchanged, if the safety of the car is
changed from high to low, there will be 50% drop in sales.” . A positive value
of diﬀerential probability indicates that there will improvement in sales upon
changing the corresponding input factor (e.g. sales) in the stated manner (i.e.
from low to high safety). A negative diﬀerential probability corresponds to a
drop in sales.
Table 3 re-iterates the result of random forest. It can be noted that safety and
number of persons are the most important factors in determining sales. Note,
Table 3 highlights the most important factors in improving sales and hence some
factors (e.g. lug-boot) are omitted from the table.
Discussion
In this section, we discuss the merits and de-merits of all the three methods from
the perspective of cognitive value the respective explanations oﬀer to the users
Random Forest: The random forest educates the car dealer about the most
important factors responsible for car sales in a relative manner. The signiﬁcance
of absolute values of the importance scores is not clear as their range is unknown.
Furthermore, knowing the factor importance scores does not help the car dealer
in understanding what needs to be done in order to improve the sales. The result
may thus only educate the car dealer in knowing the most important factors
Ajay Chander and Ramya Srinivasan
aﬀecting sales, but it is unclear as to how those factors need to be changed in
order to improve sales.
SBRLs: There are many if-else statements in the explanation provided by
SBRLs. The speciﬁc conditions are chosen automatically by the algorithm and
can consist of multiple conditions that may be diﬃcult for the user to interpret.
Even if one parses for the ones with highest positive probabilities ( implying
sales of cars), it neither conveys semantically relevant information nor provides
actionable insights to the car dealer. For example, the highest probability of 0.989
corresponds to the rule “if buying= high”. Does this mean cars with high buying
price sell more? If true, it does not seem practically very true or compelling. Even
assuming that it is true, it does not provide actionable insights to the car owner.
By how much can the price be increased to achieve a certain sales target? Such
kind of information is lacking in this model’s result.
Causal Models: Unlike random forest which could not ascertain how those
factors are important and in particular how the car dealer should change those to
improve sales, the explanation from the causal model provides actionable insights
to the car dealer in improving sales. Furthermore, the results from the causal
model is semantically meaningful and practically relevant.
Although the explanations based on causal modeling oﬀers cognitive value to
the users, it comes at a price. Speciﬁcally, one has to try with diﬀerent structural
assumptions. For a non-domain expert, this can really be time consuming. Also,
the causal explanation formula works best for binary data. While this is good in
providing instance level explanations (local explanations), it may not be easy to
derive global explanations.
Table 4 provides a comparison of the three methods in terms of their cognitive
value to the car dealer.
Table 4. Comparison of Results: RF denotes random forests, CM denotes causal models, SBRL denotes Scalable Bayesian Rule Lists
Method Educates
Provides Actionable
insights to the user
comprehend
provides relative
Range of variable
importance of factors in sales
importance is not clear
Informs about sales
several conditions to parse
under certain conditions
provides relative
explains how the sales Yes
importance of factors in sales can be improved
Conclusions
We introduced the concept of “cognitive value” of explanations of AI systems to
users. We considered the scenario of a business owner seeking to improve sales
Evaluating Explanations by Cognitive Value
of their product and compared explanations provided by some state-of-the-art
AI methods in terms of the cognitive value they oﬀer to the business owner.
Speciﬁcally, we studied random forest, scalable bayesian rule lists and causal
explanations towards this end. For the context considered, causal explanations
provided the best cognitive value in terms of providing the business owner with
actionable insights in improving his/her sales. We hope our work will foster
future research in the ﬁeld of transparent AI to incorporate the cognitive value
of explanations in assessing explanations.