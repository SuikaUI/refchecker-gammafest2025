Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 89–98,
Beijing, China, July 26-31, 2015. c⃝2014 Association for Computational Linguistics
Annotating Targets of Opinions in Arabic using Crowdsourcing
Noura Farra
Columbia University
New York, NY 10027, USA
 
Kathleen McKeown
Columbia University
New York, NY 10027, USA
 
Nizar Habash
New York University
Abu Dhabi, UAE
 
We present a method for annotating targets
of opinions in Arabic in a two-stage process using the crowdsourcing tool Amazon
Mechanical Turk. The ﬁrst stage consists
of identifying candidate targets “entities”
in a given text. The second stage consists
of identifying the opinion polarity (positive, negative, or neutral) expressed about
a speciﬁc entity.
We annotate a corpus
of Arabic text using this method, selecting our data from online commentaries in
different domains. Despite the complexity
of the task, we ﬁnd high agreement. We
present detailed analysis.
Introduction
An important task in subjectivity analysis of text
is the identiﬁcation of targets - also often called
topics or subjects - of opinionated text. Knowledge of the target is important for making sense
of an opinion (e.g in ‘The will of the people will
prevail over the regime’s brutality’, the opinion
is positive towards ‘the people’ and negative towards ‘the regime’).
An opinion system which
can identify both targets and polarities of opinions, and which can summarize the opinions of
writers towards different targets, will be more informative than one which only identiﬁes the overall sentiment of the text. This problem has started
gaining interest in the product review domain , news and social
media ,
and in general language and discourse .
Annotating targets of opinion is a difﬁcult
and expensive task, requiring deﬁnition of what
constitutes a target, whether targets are linked to
opinion expressions, and how the boundaries of
target spans should be deﬁned (e.g ‘the people’
‘the will of the people’ or ‘the regime’
vs. ‘the regime’s brutality’), a problem which
annotators often disagree on .
Additionally, it is not always straightforward to
attribute a target to a speciﬁc opinion phrase.
Consider for example the following statement:
‘The Lebanese PM said he was convinced
that there would be a consensus on the presidential election,
because since the moment
the US and Iran had reached an understanding
in the region, things were starting to look positive.’
Which is the opinion expression that leads us to
believe that the PM is optimistic about the target
presidential election? Is it ‘convinced’, ‘consensus’, ‘reached an understanding’, or ‘look positive’, or a combination of the above? Such decisions are difﬁcult for annotators to agree on; many
studies have noted these challenges which can
make the task complex.
Compared to the amount of resources available
for sentiment and subjectivity analysis, there is
much less annotated data available for this more
ﬁne-grained type of analysis.
Due to the difﬁculty of the task, most of the available datasets of
ﬁne-grained subjectivity have been annotated by
trained annotators or expert linguists, making the
process slower and more expensive.
In this work, we consider annotation of targets
using a sequence of simple crowdsourced substeps.
We focus on Arabic, where subjectivity
analysis is of growing interest, and where there
are no publicly available resources for ﬁne-grained
opinion analysis. We assume targets of opinions to
be noun phrase entities: people, places, things or
ideas. We develop a two-stage annotation process
for annotating targets of opinions using Amazon
Mechanical Turk. In the ﬁrst, annotators list all
important ‘entities’, and in the second, they choose
the polarity expressed (positive, negative, or neutral) towards any given entity. We select online
data from multiple domains: politics, sports, and
culture; and we provide a new publicly available
resource for Arabic by annotating it for targets
of opinions along with their polarities.
we evaluate the quality of the data at different
stages, obtaining majority agreement on sentiment
for 91.8% of entities in a corpus of 1177 news article comments. We also ﬁnd that the morphology
and grammar of Arabic lends itself to even more
variations in identifying the boundaries of targets.
Section 2 describes related annotation work.
Section 3 describes the Amazon Mechanical Turk
tasks design, the data selection, and the annotation process. In Section 4, we examine and analyze the annotations, evaluate the inter-annotator
agreement, and provide detailed examples.
conclude in section 5.
Related Work
Annotating Targets in English
Fine-grained subjectivity annotation in the English language has recently started gaining interest, where annotation can include opinion targets,
opinion sources, or phrase-level opinion expressions.
One of the early datasets collected for
identifying opinion targets is that of , where product features (e.g price, quality)
were annotated in customer reviews of consumer
electronics.
These consisted of mostly explicit
product features annotated by one person.
Also in the product review domain, the Sem-
Eval Task on aspect feature mining in 2014 was concerned with ﬁnding aspect features of products with the polarities towards them. The products (e.g ‘restaurant’) and
coarse-grained features (e.g ‘service’) were provided to annotators, who identiﬁed the aspects (e.g
‘waiter’) and the corresponding sentiment.
The MPQA corpus is an in-depth and generalpurpose resource for ﬁne-grained subjectivity annotations , containing annotations of opinion expressions at the
phrase level while specifying polarities, sources,
and target spans.
The annotation scheme links
each subjective expression to one or more attitudes, which in turn can have one or more or no
targets. The target annotations include the full target spans, but do not necessarily identify target entities within the span. Stoyanov and Cardie 
extended part of the MPQA corpus by annotating it for ‘topics’, arguing that ‘targets’ refer to
the syntactic span of text that identiﬁes the content of an opinion, while ‘topic’ is the real-world
object or entity corresponding to the primary subject of the opinion. Using trained annotators, they
identify ‘topic clusters’, which group together all
opinions referring to the same topic. In parallel
with this work, part of the MPQA corpus was recently annotated for entity-level targets by specifying target entities within
the MPQA span, leading to the annotation of 292
targets by two annotators. The entities were anchored to the head word of the noun phrase or
verb phrase that refers to the entity or event. In our
work, we only consider noun phrase entities, and
we consider the noun phrase itself as an entity.
Other ﬁne-grained annotation studies include
that of Toprak et al. who enrich target and
holder annotations in consumer reviews with measures such as relevancy and intensity, and Somasundaran et al. who perform discourselevel annotation of opinion frames, which consist
of opinions whose targets are described by similar
or contrasting relations.
In these studies, the annotation was usually
done by trained individuals or someone who has
knowledge and experience in the task. Our study
is different in that it utilizes crowdsourcing for the
annotation process, and it focuses on the marking of important entities and concepts as targets
of opinions in the more noisy online commentary
We view targets as ‘real-world entities’,
similar to the topics discussed by Stoyanov and
Cardie , and the targets in , and we annotate multiple targets in the text.
Carvalho et al. also annotated targets in
online commentary data; here targets were considered to be human entities, namely political and
media personalities. This annotation was done by
one trained annotator where agreement was computed for a portion of the data. Another related
task was that of Lawson et al. who describe
a Mechanical Turk annotation study for annotating named entities in emails, with favorable agreement results. The tasks for identifying the spans of
and labeling the named entities were grouped in a
single Human Intelligence Task (HIT).
Annotation Studies in Arabic
Abdul-Mageed and Diab performed a
sentence-level annotation study for Modern Standard Arabic (MSA) newswire data which covered
multiple domains including politics, sports, economy, culture, and others. Both the domains and the
sentence-level sentiment were annotated by two
trained annotators. Our data also comes from different domains, but it is from the genre of online
commentaries, which have greater prevalence of
dialect, imperfect grammar, and spelling errors.
Also, to select less prevalent domains from our
comments corpus, we used topic modeling.
There have been other MTurk studies in Arabic;
among them Zaidan and Callison-Burch
 who annotated dialectness, Denkowski
et al. who annotated machine translation
pairs, and Higgins et al. who annotated
Arabic nicknames. To the best of our knowledge,
there are no known studies for target or topic annotation for Arabic.
Annotation Process
We describe the crowdsourcing process for annotating targets of opinions, including the choices
which motivated our design, the tasks we designed
on Amazon Mechanical Turk, and the way we selected our data.
Scope and Decisions
We assume targets of opinions to be nouns and
noun phrases representing entities and concepts,
which could be people, places, things, or important ideas. Consider for example:
‘It is great that so many people showed up to
the protest.’
The full target span is marked in bold, but the
actual entity which receives the positive opinion
is ‘the protest’.
We are interested in such entities; for example, entities could be politicians,
organizations, events, sports teams, companies,
products, or important concepts and ideas such
as ‘democracy’ or entities representing ideological belief.
Given the complexity of the task, we annotate targets without specifying opinion expressions
that are linked to them, as in , although the dataset can be extended for this purpose to provide richer information for modeling. We assume the availability of
an Arabic opinion lexicon, to identify the opinion
words. We don’t consider targets of subjectiveneutral judgments (e.g "I expect it will rain tomorrow"). For this corpus, we are interested only
in targets of polar positive or negative opinions;
everything else we regard as neutral. Moreover,
since our data comes from online commentaries,
we assume that in the majority of cases, the opinion holder is the writer of the post.
Amazon Mechanical Turk Tasks
Instead of asking annotators to directly identify
targets of opinions, which we believed to be a
much harder task, we broke the annotation into
two stages, each in a different series of HITs (Human Intelligence Tasks). The task guidelines were
presented in Modern Standard Arabic (MSA) to
guarantee that only Arabic speakers would be able
to understand and work on them. Many of the insights in the task design were gained from an extensive pilot study.
Task 1: Identifying Candidate Entities
an article comment, annotators are asked to list
the main nouns and noun phrases that correspond
to people, places, things, and ideas. This task, or
HIT, is given to three annotators and a few examples of appropriate answers are provided.
The answers from the three annotators are then
combined by taking the intersection of common
noun phrases listed by all three responses. If they
only agree on a subset of the noun phrase, we
choose the maximal phrase among agreed entities
in order to determine the entity span. For example,
if two annotators specify the president and a third
speciﬁes the election of the president, we keep
the election of the president. The maximal noun
phrase was also chosen by Pontiki et al. 
when resolving disagreements on target spans.
We allowed annotators to list references in the
comment to the same entity (e.g ‘The president’
and ‘President Mubarak’) as separate entities.
Insights from Pilot We asked speciﬁcally for the
main noun phrases, after we found that annotators in the pilot over-generated nouns and noun
phrases, listing clearly unimportant entities (such
Ë@ ‘today/this day’, and ÐCË@ ‘hello/the
greeting’), which would make Task 2 unnecessarily expensive.
They would also break up noun
phrases which clearly referred to a single entity
(such as separating ú
Q»‘the seat’ and éA
presidency’ from éA
Q» ‘the presidency’s
seat’), so we instructed them to keep such cases as
a single entity. These reasons also support choosing the maximal agreeing noun phrase provided by
annotators. By making these changes, the average
number of entities resolved per comment was reduced from 8 entities in the pilot study to 6 entities
in the full study.
We paid 30 cents for Task 1, due to its importance and due to the time it took workers to complete (2-3 minutes on average).
Task 2: Identifying Sentiment towards Entities
In the second task (HIT), annotators are presented
with an article comment and a single entity, and
are asked to specify the opinion of the comment
towards this entity, termed a ‘topic’ ¨ñ ñÓ. The
entities are chosen from the resolved responses in
Task 1. The question is in multiple-choice form
where they can choose from options: positive,
negative, or neutral.
Each HIT is given to ﬁve
annotators, and the entities which are speciﬁed as
positive or negative with majority agreement of
3 are considered to be targets. Entities with disagreement, or with neutral majority, are discarded
as non-targets. In this question, we tell annotators
that opinions can include sentiment, belief, feelings, or judgments, and that the neutral option
should be selected if the comment reveals either no
opinion or an unbiased opinion towards this particular entity. We provide multiple examples. For
this task, we paid workers 5 cents per HIT, which
took 30 seconds to 1 minute on average.
Insights from Pilot In our pilot study, we had an
additional question in this HIT which asks annotators to specify the holder of the opinion, which
could be the writer or someone else mentioned in
the text. However, we removed this question in the
ﬁnal study due to the low quality of responses in
the pilot, some of which reﬂected misunderstanding of the question or were left blank.
Additionally, we found that some annotators
speciﬁed the overall sentiment of the comment
rather than the sentiment about the topic. We thus
emphasized, and included an additional English
translation of the instruction that the opinion polarity should be about the speciﬁc topic and not of
the whole comment.
We completed the full annotation study in ﬁve
rounds of a few hundred comments each. For the
ﬁrst two rounds of annotation, we rejected all HITs
that were clearly spamming the task or were not
Arabic speakers. After that we created task qualiﬁcations and allowed only a qualiﬁed group of
# Comments
Distribution(%)
Table 1: Distribution of article comments by domain
workers (5 for Task 1 and 10 for Task 2) to access the tasks, based on their performance in the
previous tasks.
Data Selection
Our data is selected from the Qatar Arabic Language Bank (QALB) , which includes online commentaries to Aljazeera newspaper articles.
Topic Modeling
We initially selected a random
sample of data from the Aljazeera corpus, which
contains mostly political data. In our pilot study
and ﬁrst annotation round, we found that this data
was biased towards negative sentiment. We thus
used topic modeling to select data from other domains which
we thought might contain more positive sentiment. Upon applying a topic model specifying 40
topics to the Aljazeera corpus, we found a general "sports" topic and a general "culture" (language, science, technology, society) topic among
the other political topics.
We chose sports and
culture comments by taking the top few hundred
comments having the highest probability score for
these topics, to guarantee that the content was indeed relevant to the domain. Table 1 shows the
distribution of the ﬁnal data used for annotation,
consisting of 1177 news article comments.
Data Characteristics
The average length of
comments is 51 words, spanning 1-3 Arabic sentences. We do not correct the data for spelling errors; we annotate the raw text because we want to
avoid any alteration that may affect the interpretation of sentiment, and we would like to keep the
data as real as possible. However, it is possible to
correct this output automatically or manually.
We performed a manual analysis of 100 comments from a randomly selected subset of the
dataset and having the same domain distribution.
We found that 43% of the comments contain at
least one spelling error including typos, word
merges and splits,1 15% contain at least one dialect word, 20% contain a run-on sentence not
separated by any conjunction or punctuation, and
98% contain subjective opinions on any topic.
We believe this is a good dataset for annotation because it contains real-world data, and many strong
opinions on controversial topics.
Experimental Results
This section describes results and analyses of the
crowdsourced annotations.
We report the interannotator agreement at each of the two annotation stages, the distribution of the sentiment of collected targets by domain, and a manual analysis of
our target entities. We also provide examples of
our ﬁnal annotations.
Inter-annotator agreement
Task 1: Agreement on Important Noun Phrases
To compute the agreement between annotators
on important entities in a HIT, we compute the
average precision pHIT . pHIT is then averaged
over all HITs to obtain the agreement.
#phrases_a1 +
#phrases_a2 +
#phrases_a3)
An average precision of 0.38 was obtained using exact matching of entities and 0.75 using subset matching: i.e a match occurs if the three annotators all list a sub-phrase of the same noun phrase.
(Recall that the ﬁnal entities were chosen according to subset agreement.)
Our noun phrase agreement numbers are comparable to the target span subset agreement numbers of Somasundaran et al. in English discourse data, and lower than that of Toprak et al.
 , who annotated targets in the consumer review domain. Note that besides the language difference, the task itself is different, since we annotate important noun phrases rather than opinion
targets; a lower agreement on this task essentially
indicates that fewer entities are being passed on
to the next task for consideration as targets, the assumption being that only important entities will be
agreed upon by all three annotators. Since we had
three rather than two annotators, the agreement using exact match is expected to be low.
1We don’t count the different variations of Alef @, ø
è/è, forms, which are often normalized during model training
and evaluation.
# Entities
Majority Agree (%)
Table 2: Agreement on entity-level sentiment annotation
Task 2: Sentiment agreement Table 2 shows
the annotator agreement for the task of identifying sentiment towards given entities. A majority
agreement occurs when 3 out of 5 annotators of
an entity agree on whether the sentiment towards
it is positive, negative, or neutral.
We see that
the agreement (91.8%) is reasonably high. Abdul-
Mageed and Diab have reported overall
agreement of 88% for annotating sentence-level
Arabic sentiment (as positive, negative, neutral,
or objective) using two trained annotators.
note that after assigning our task to only the quali-
ﬁed group of workers, the annotator agreement increased from 80% and 88% in the ﬁrst two annotation rounds, to 95% in the remaining rounds.2
Sentiment Distribution
Table 3 shows the distribution of the sentiment of the ﬁnal targets by domain. The ﬁnal targets of opinions correspond to
entities which were agreed to be positive or negative by majority agreement. We can see that
the politics and sports domains are biased towards
negative and positive sentiment respectively, while
targets in the culture domain have a mostly even
distribution of sentiment. We also note that overall, 95% of all comments had at least one target of
opinion, and 41% of those comments had multiple
targets with both positive and negative sentiment.
This veriﬁes our hypothesis about the sentiment
diversity and need for ﬁner-level opinion analysis
for this dataset.
Finally, we found that the majority of targets are
composed of 2 words (38% of targets), followed
by 1-word targets (25% of targets), 3-word targets
(18%), and 4-word targets (9%), while 10% of all
targets are composed of more than 4 words.
Manual Analysis
We manually examined 200 randomly selected targets from our ﬁnal dataset, and found a num-
2In the ﬁnal dataset, we include the annotations organized
by each annotation round. We mark the entities with disagreement as ‘undetermined’.
Table 3: Distribution of sentiment in ﬁnal targets
Spelling errors 2.5%
I. ª Ë@ HX@P@
“the people’s will"
Punctuation 5%
.ÉK.@ HAj. J JÓ
“Apple’s products."
Prep & Conj clitics 8.5%
“to Manchester United"
Non-noun phrases 3%
GAJ.B@ PðYË@ É¢
“Barcelona (is) the champion
of the Spanish league"
Targets with sentiment 5.5%
PñË@ I. ª Ë@
"the free Syrian people"
Propositional entities 3%
JkAJ.Ë@ ©J
"encouraging researchers"
Table 4: Target phrase observations
ber of observations, many of which are languagespeciﬁc, that deserve to be highlighted. They are
summarized in Table 4.
We ﬁrst note orthographic observations such
as spelling errors, which come mostly from the
original text, and punctuations attached to targets,
which may easily be stripped from the text. The
punctuations result from our decision to take the
maximal noun phrase provided by annotators.
Prepositional and conjunctional clitics result
from Arabic morphology which attaches prepositions such as l+ È (to) and b+ H. (in), or conjunctions w+ ð (and) to the noun preceding them.
They can be removed by tokenization , but we preserve them for completeness and
their usefulness for allowing us to distinguish between different mentions of the same target.
Non-noun phrases mainly come from nominal
sentences speciﬁc to Arabic syntax éJ
@ éÊÔg. ;
these are problematic because they may be interpreted as either noun phrases or full sentences that
begin with a nominal. We also observed a number
of verbal phrase targets (e.g “éJ
YËAK. ÉJ.ÊJ. K"
“we confuse democracy"), but these were very
few; the majority of this class of observations
comes from verbless nominal phrases.
Targets containing sentiment words appear
since sentiment words can be part of the noun
phrase and are not always independent of the topic
As for propositional entities, they result
from process nominals PYÓ which can have a
verbal reading but are
correctly considered to be nouns. We ﬁnd that they
occur mostly in the culture domain, where more
discussions occur about ‘important concepts’.
We also found from our manual inspection that
our ﬁnal entity spans reasonably corresponded to
what would be expected to be targets of opinions for the topic in context. From our 200 randomly selected targets, we found 6 cases where
the polarity of the noun phrase potentially negated
the polarity towards a shorter entity within the
noun phrase.
However, in most of these cases,
the noun phrase resolved from the annotations
correctly represents the actual target of opinion:
e.g.“depletion of ozone" àð PðB@ I. ®K, “bombing of houses" È PA JÖÏ@
­¯, and “methodology of
teaching Arabic" éJ
ÊªK H. ñÊ@. We found
one case “absence of Messi" ú
«, labeled
negative, where it could be argued that either
Messi (positive) or his absence (negative) is the
correct target. We generally preferred target annotations which correspond to the topic or event
being discussed in the context of the comment.
We provide examples of the annotations, shown in Table 5. Note that we have preserved all spelling errors in the original Arabic
text. As it is common in Arabic to write very long
sentences, we have added punctuation to make the
English translation more readable.
Example (1) is from the culture domain. We see
that it summarizes the writer’s opinions towards
all important topics. Note that the direct reference
to the target “e-book" ú
ºËB@ H. AJºË@ is the ﬁrst
mention (the second mention is preceeded by the
preposition to È). However, we generally assume
that the opinion towards a target is deduced from
the entire comment (i.e from both the phrase ‘despite the popularity of the e-book’ and the phrase
‘there is no place for an e-book in my dictionary’).
Ideally, the annotators should also have marked
traditional book ø
Ê®JË@ H. AJºË@ as a positive target; although the opinion expressed towards it is
less direct, it can also be inferred by co-reference
with paper book ú
¯PñË@ H. AJºË@ .
Example (2) lists an entity that doesn’t appear
in the text “(to) the Arab team the world cup"
Y Kñ JÖÏ@ ú
G.QªË@ I. jJÒÊË; this likely results from
an error in Task 1 where the phrase got picked up
as the maximal common noun phrase. The annotator might have meant that Arab team in the world
cup is a topic that the writer feels positively about;
however, our current annotation scheme only considers entities that strictly appear in the text. We
also see that annotators disagreed on the polarity of the propositional entity “either team qualifying" á
AK, likely because they were
not sure whether it should be marked as neutral
or positive.
In addition, this example contains
an over-generated target “world cup" ÈAK
Y Kñ JÖÏ@,
which would have been best marked as neutral.
Example (3) is from the politics domain. It correctly annotates multiple references of the Iraqi
government and captures the sentiment towards
important entities in the text. The target “the only
neighboring country" èYJ
kñË@ èPAm.Ì'@ éËðYË@ can be
considered an over-generation; a better interpretation might be to consider this phrase part of
the opinion expression itself ("the only neighboring country with whom we have ties that are not
just based on interests is Turkey"). Nonetheless,
this extra annotation may provide helpful information for future modeling. Notice that the Arabic comment for this example, in addition to being long, has no punctuation other than the period
ending the sentence. It is common in Arabic to encounter such constructions, whereby conjunctions
and transitional words are enough to determine the
separation between clauses or sentence phrases.
We have added punctuation to the English translation of this example.
We generally found that the annotations were
a good representation of the diverse opinions of
online writers, correctly covering sentiment towards essential targets and mostly complying with
our deﬁnition of entities. The annotations contain
some errors, but these are expected in a crowdsourcing task, especially one that relies so heavily
on subjective interpretation. We noticed that annotators tended to over-generate targets rather than
miss out on essential targets. We believe that even
annotation of secondary targets may prove useful
for future modeling tasks.
Conclusions
We developed a two-stage method for annotating targets of opinions using Amazon Mechanical
Turk, where we consider targets to be noun phrase
entities. This method was applied to Arabic, yielding a new, publicly available resource for ﬁnegrained opinion analysis.3 We found high agreement on the task of identifying sentiment towards
entities, leading to the conclusion that it is possible to carry out this task using crowdsourcing,
especially when qualiﬁed workers are available.
Unlike some of the previous work, our focus
was on annotating target entities rather than the
full target spans; and we developed a unique approach for identifying these entities using Amazon
Mechanial Turk. The ﬁrst task involves marking
important entities, while the second task involves
ﬁnding targets by assessing the sentiment towards
each entity in isolation. We found that although
the agreement was generally high for both tasks,
it was not as high for the entity identiﬁcation task
as it was for the second and easier task of ﬁnding
sentiment towards entities.
We also found that the morphological complexity of Arabic, as well as the variation in acceptable
syntax for noun phrases, creates additional annotation challenges for deciphering the boundaries of
entities. We also anticipate that the long structure
of Arabic comments will create interesting challenges for future modeling tasks.
In the future, we hope to extend this dataset
by mapping the targets to speciﬁc opinion phrases
and identifying which targets refer to repeated
mentions (e.g the team) or aspects (e.g defense)
of the same target (e.g the Algerian team), in addition to annotating conﬂicting sentiment towards
the same entity. We also hope to create a manually reviewed version of the corpus corrected for
spelling errors and non-noun phrase targets.
Acknowledgments
This work was made possible by grant NPRP 6-
716-1-138 from the Qatar National Research Fund
(a member of the Qatar Foundation). The statements made are solely the responsibility of the authors. We thank anonymous reviewers for their
helpful comments. We would also like to thank
Debanjan Ghosh, Owen Rambow, and Ramy Eskander for helpful discussions and feedback. We
thank the AMT annotators for all their hard work,
insightful questions, and for continuing to participate in multiple rounds of our annotation tasks.
3The corpus is available and can be downloaded from
www.cs.columbia.edu/~noura/Resources.html
Example Comment
Example (1)
éKAj ® I. J
Ê®K úæk .. ¨ñJ.¢ÖÏ@ H. AJºË@ I. k@ .èXñk. ð I
¯PñË@ H. AJºË@ à@ B@ ú
B@ H. AJºË@ PA 
Domain: Culture
@ B .. é A Ë@ ÈC g áÓ H. AJºË@ èZ@Q¯ ÉÒJk
K. ñëð éKZ@Q¯ Y J« ÉÔg.
B@ð .. éªJÓ AîE. Yg.
ù£A Ë@ úÎ« èQ
¯ PA¢®Ë@ ú
¯ éJ.JºÖÏ@ ú
Ê®JË@ H. AJºË@ ..¨@YË@ð Zñ Ë@ i. ëð ÉÒm' ú
¯ P@QÒJB@
B@ H. AJºÊË àA¾ÓB .. éJ
English Translation
Despite the popularity of the e-book, the paper book has proven itself. I like the printed book...
I even ﬁnd a pleasure in turning its pages ... and it is nice is to read it while it is in my hands ...
I cannot stand reading a book through a screen ... I cannot bear the glare of light and the
headaches...I can read a traditional book in the library on the train in the airplane on the beach
in the garden in anywhere I am comfortable .. there is no place for the e-book in my dictionary.
Annotated Targets
negative: the e-book ú
ºËB@ H. AJºË@
positive: the paper book ú
¯PñË@ H. AJºË@
positive: the printed book ¨ñJ.¢ÖÏ@ H. AJºË@
negative: reading a book through a screen é A Ë@ ÈC g áÓ H. AJºË@ èZ@Q¯
Example (2)
É¾Ë@ Éªk. éJ.A JÖÏAK. ø
K@ Qm.Ì'@ I. j
JÖÏ@ éK. ú
YË@ Ñ«YË@ð . àAK
ñ¯ àAJ. j
K@ Qm.Ì'@ð ø
QåÖÏ@ àAJ. j
Domain: Sports
K@ Qm.Ì'@ K
Q ®Ë@ I. k@ ú
Y Kñ JÖÏ@ ú
K@ Qm.Ì'@ K
à@ ú æÖß @ ð á
¯ Q ¯ Yg. ñK
Bð QKñJÓ
Y Kñ JÖÏ@ ú
JÖß ák@ ú
G.QªË@ I. jJÒÊË àñºK
à@ ú æÖß @ ð YJ
JÒJË@ ÑêÖÏ@ð . ø
QåÖÏ@ I. j
JÖÏ@ I. KAg. úÍ@
English Translation
The Egyptian and Algerian teams are strong teams. The support gained by the Algerian team
for this occasion has made everyone nervous and there is no difference in either team qualifying
and I hope that the Algerian team gets qualiﬁed to the world cup because I like the Algerian team
alongside the Egyptian team. The important thing is good representation and I hope
that the Arab team will be best represented in the world cup.
Annotated Targets
positive: The Egyptian and Algerian teams ø
K@ Qm.Ì'@ð ø
QåÖÏ@ àAJ. j
positive: the Algerian team ‘elect’ ø
K@ Qm.Ì'@ I. j
positive: the Algerian team ø
K@ Qm.Ì'@ K
positive: the world cup ÈAK
positive: (to) the Arab team the world cup ÈAK
Y Kñ JÖÏ@ ú
G.QªË@ I. jJÒÊË
undetermined: either team qualifying á
Example (3)
lÌ'AÓ áÓ Q
»@ AêªÓ A J¢
kñË@ èPAm.Ì'@ éËðYË@ à
Ë@ áÓ ÑîD ®K
¯@QªË@ éÓñºmÌ'@
Domain: Politics
ÈðYË@  ¯A JK Ij
.@ Aî EB AêªÓ A JJ¯C« ø
ñ® K à@ A JJ
«A J lÌ'AÓ úÍ@ èAJ
J.¢Ë@ XP@ñÖÏ@ áÓ
. @QªË@ ©k. P Y¯ð Pñ¢JË@ ÑêÒîE
B éÊ A ®Ë@ ú
¾ËAÖÏ@ éÓñºk áºËð Aî DÓ XA ®J
. ­Ê mÌ'@ úÍ@
English Translation
Unfortunately the Iraqi government understands nothing of politics because the only neighboring
country with whom we have ties that are not just based on interests - such as natural resources
like water and industrial interests - is Turkey, so we have to strengthen our relationship with it
because it is now a competitor with European nations, we should beneﬁt from it but
Maliki’s failed government cares nothing for progress and Iraq has gone back hundreds of years
because of these people.
Annotated Targets
negative: the Iraqi government éJ
¯@QªË@ éÓñºmÌ'@
positive: the only neighboring country èYJ
kñË@ èPAm.Ì'@ éËðYË@
positive: Turkey AJ
negative: Maliki’s failed government éÊ A ®Ë@ ú
¾ËAÖÏ@ éÓñºk
negative: Iraq @QªË@
Table 5: Examples of Annotations. The original spelling errors are preserved.