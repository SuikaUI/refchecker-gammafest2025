On the Detection of Digital Face Manipulation
Joel Stehouwer*
Xiaoming Liu
Department of Computer Science and Engineering
Michigan State University, East Lansing MI 48824
Detecting manipulated facial images and videos is an increasingly important topic in digital media forensics. As advanced face synthesis and manipulation methods are made
available, new types of fake face representations are being created which have raised signiﬁcant concerns for their
use in social media. Hence, it is crucial to detect manipulated face images and localize manipulated regions. Instead
of simply using multi-task learning to simultaneously detect manipulated images and predict the manipulated mask
(regions), we propose to utilize an attention mechanism to
process and improve the feature maps for the classiﬁcation task. The learned attention maps highlight the informative regions to further improve the binary classiﬁcation
(genuine face v. fake face), and also visualize the manipulated regions. To enable our study of manipulated face detection and localization, we collect a large-scale database
that contains numerous types of facial forgeries. With this
dataset, we perform a thorough analysis of data-driven fake
face detection. We show that the use of an attention mechanism improves facial forgery detection and manipulated
region localization. The code and database are available at
cvlab.cse.msu.edu/project-ffd.html.
1. Introduction
Human faces play an important role in human-human
communication and association of side information, e.g.,
gender and age with identity. For instance, face recognition is increasingly utilized in our daily life for applications such as access control and payment . However,
these advances also entice malicious actors to manipulate
face images to launch attacks, aiming to be authenticated as
the genuine user. Moreover, manipulation of facial content
has become ubiquitous, and raises new concerns especially
in social media content . Recent advances in deep
learning have led to a dramatic increase in the realism of
face synthesis and enabled a rapid dissemination of “fake
*denotes equal contribution by the authors.
Figure 1. Given one genuine face image, there are three types of
facial forgery attacks: physical spooﬁng attack (print and replay
attack), adversarial attack , and digital manipulation attack.
news” . Therefore, to mitigate the adverse impact and
beneﬁt both public security and privacy, it is crucial to develop effective solutions against these facial forgery attacks.
As shown in Fig. 1, there are three main types of facial
forgery attacks. i) Physical spooﬁng attacks can be as simple as face printed on a paper, replaying image/video on
a phone, or as complicated as a 3D mask .
ii) Adversarial face attacks generate high-quality and perceptually imperceptible adversarial images that can evade
automated face matchers . iii) Digital manipulation attacks, made feasible by Variational AutoEncoders (VAEs) and Generative Adversarial Networks (GANs) , can generate entirely or partially modiﬁed photorealistic face images. Among these three types,
this work addresses only digital manipulation attacks, with
the objectives of automatically detecting manipulated faces,
as well as localizing modiﬁed facial regions. We use the
term “face manipulation detection” or “face forgery detection” to describe our objective.
Digital facial manipulation methods fall into four categories: expression swap, identity swap, attribute manipulation and entire face synthesis (Fig. 2). 3D face reconstruction and animation methods are widely
used for expression swap, such as Face2Face . These
methods can transfer expressions from one person to another in real time with only RGB cameras. Identity swap
methods replace the face of one person with the face of anarXiv:1910.01717v5 [cs.CV] 24 Oct 2020
Figure 2. Our facial forgery detection method tackles faces generated by the four types of face manipulation methods. Given a face
image, our approach outputs a binary decision (genuine v. manipulated face) and localizes the manipulated regions via an estimated
attention map. For real or entirely synthetic faces, our estimated
maps are assumed to be uniformly distributed in .
other. Examples include FaceSwap , which inserts
famous actors into movie clips in which they never appeared
and DeepFakes , which performs face swapping via deep
learning algorithms.
Attribute manipulation edits single or multiple attributes
in a face, e.g., gender, age, skin color, hair, and glasses. The
adversarial framework of GANs is used for image translation or manipulation in a given context ,
which diversiﬁes facial images synthesis. FaceApp has
popularized facial attribute manipulation as a consumerlevel application, providing 28 ﬁlters to modify speciﬁc attributes . The fourth category is entire face synthesis.
Fueled by the large amounts of face data and the success
of GANs, any user is capable of producing a completely
synthetic facial image, whose realism is such that even humans have difﬁculty assessing if it is genuine or manipulated .
Research on face manipulation detection has been
seriously hampered by the lack of large-scale datasets
of manipulated faces.
Existing approaches are often
evaluated on small datasets with limited manipulation
types, including Zhou et al. , Deepfake , and
FaceForensics/FaceForensics++ . To remedy this
issue, we collect a Diverse Fake Face Dataset (DFFD) of
2.6 million images from all four categories of digital face
manipulations.
Due to the fact that the modiﬁcation of a face image can
be in whole or in part, we assume that a well-learned network would gather different information spatially in order
to detect manipulated faces. We hypothesize that correctly
estimating this spatial information can enable the network
to focus on these important spatial regions to make its decision. Hence, we aim to not only detect manipulated faces,
but also automatically locate the manipulated regions by
estimating an image-speciﬁc attention map, as in Fig. 3.
We present our approach to estimate the attention map in
both supervised and weakly-supervised fashions. We also
demonstrate that this attention map is beneﬁcial to the ﬁnal
task of facial forgery detection. Finally, in order to quantify
the attention map estimation, we propose a novel metric for
attention map accuracy evaluation. In the future, we anticipate the predicted attention maps for manipulated face
images and videos would reveal hints about the type, magnitude, and even intention of the manipulation.
In summary, the contributions of this work include:
⋄A comprehensive fake face dataset including 0.8M real
and 1.8M fake faces generated by a diverse set of face modi-
ﬁcation methods and an accompanying evaluation protocol.
⋄A novel attention-based layer to improve classiﬁcation
performance and produce an attention map indicating the
manipulated facial regions.
⋄A novel metric, termed Inverse Intersection Non-
Containment (IINC), for evaluating attention maps that produces a more coherent evaluation than existing metrics.
⋄State-of-the-art performance of digital facial forgery
detection for both seen and unseen manipulation methods.
2. Related Work
Digital Face Manipulation Methods.
With the rapid
progress in computer graphics and computer vision, it is becoming difﬁcult for humans to tell the difference between
genuine and manipulated faces . Graphics-based approaches are widely used for identity or expression transfer
by ﬁrst reconstructing 3D models for both source and target faces, and then exploiting the corresponding 3D geometry to warp between them. In particular, Thies et al. 
present expression swap for facial reenactment with an
RGB-D camera. Face2Face is a real-time face reenactment system using only an RGB camera. Instead of manipulating expression only, the extended work transfers the full 3D head position, rotation, expression, and eye
blinking from a source actor to a portrait video of a target
actor. “Synthesizing Obama” animates the face based
on an input audio signal. FaceSwap replaces the identity of
3D models while preserving the expressions.
Deep learning techniques, not surprisingly, are popular
in synthesizing or manipulating faces . The term Deepfakes has become a synonym for deep learning based face
identity replacement . There are various public implementations of Deepfakes, most recently by ZAO and
FaceAPP . FaceAPP can selectively modify facial attributes . GAN-based methods can produce entire synthetic faces, including non-face background .
Fake Face Benchmarks.
Unfortunately, large and diverse datasets for face manipulation detection are limited
in the community. Zhou et al. collected a dataset with
face-swapped images generated by an iOS app and an opensource software. Video-based face manipulation became
available with the release of FaceForensics , which contains 0.5M Face2Face manipulated frames from over 1,000
videos. An extended version, FaceForensics++ , further
Figure 3. The architecture of our face manipulation detection. Given any backbone network, our proposed attention-based layer can be
inserted into the network. It takes the high-dimensional feature F as input, estimates an attention map Matt using either MAM-based or
regression-based methods, and channel-wise multiplies it with the high-dimensional features, which are fed back into the backbone. In
addition to the binary classiﬁcation supervision Lclassiﬁer, either a supervised or weakly supervised loss, Lmap, can be applied to estimate the
attention map, depending on whether the ground truth manipulation map Mgt is available.
augments the collection with Deepfake and FaceSwap
manipulations. However, these datasets are still limited to
two fake types: identity and expression swap. To overcome
this limitation, we collect the ﬁrst fake face dataset with diverse fake types, including identity and expression swapped
images from FaceForensics++, face attribute manipulated
images using FaceAPP, and complete fake face images using StyleGAN and PGGAN .
Manipulation Localization.
There are two main approaches to localize manipulated image regions: segmenting the entire image , and repeatedly performing binary classiﬁcation via a sliding window . These methods are often implemented via multi-task learning with additional supervision, yet they do not necessarily improve the
ﬁnal detection performance. In contrast, we propose an attention mechanism to automatically detect the manipulated
region for face images, which requires very few additional
trainable parameters. In computer vision, attention models
have been widely used for image classiﬁcation ,
image inpainting and object detection . Attention not only serves to select a focused location but also
enhances object representations at that location, which is effective for learning generalizable features for a given task.
A number of methods utilize an attention mechanism to enhance the accuracy of CNN classiﬁcation models. Residual Attention Network improves the accuracy
of the classiﬁcation model using 3D self-attention maps.
Choe et al. propose an attention-based dropout layer
to process the feature maps of the model, which improves
the localization accuracy of CNN classiﬁers. To our knowledge, this is the ﬁrst work to develop the attention mechanism to face manipulation detection and localization.
3. Proposed Method
We pose the manipulated face detection as a binary classiﬁcation problem using a CNN-based network. We further
propose to utilize the attention mechanism to process the
feature maps of the classiﬁer model. The learned attention
maps can highlight the regions in an image which inﬂuence
the CNN’s decision, and further be used to guide the CNN
to discover more discriminative features.
3.1. Motivation for the Attention Map
Assuming the attention map can highlight the manipulated image regions, and thereby guide the network to detect these regions, this alone should be useful for the face
forgery detection. In fact, each pixel in the attention map
would compute a probability that its receptive ﬁeld corresponds to a manipulated region in the input image. Digital
forensics has shown that camera model identiﬁcation is possible due to “ﬁngerprints” in the high-frequency information of a real image . It is thus feasible to detect abnormalities in this high-frequency information due to algorithmic processing. Hence we insert the attention map into the
backbone network where the receptive ﬁeld corresponds to
appropriately sized local patches. Then, the features before
the attention map encode the high-frequency ﬁngerprint of
the corresponding patch, which may discriminate between
real and manipulated regions at the local level.
Three major factors were considered during the construction and development of our attention map; i) explainability, ii) usefulness, and iii) modularity.
Explainability: Due to the fact that a face image can be
modiﬁed entirely or in part, we produce an attention map
that predicts where the modiﬁed pixels are. In this way,
an auxiliary output is produced to explain which spatial regions the network based its decision on. This differs from
prior works in that we use the attention map as a mask to remove any irrelevant information from the high-dimensional
features within the network. During training, for a face image where the entire image is real, the attention map should
ignore the entire image. For a modiﬁed or generated face, at
least some parts of the image are manipulated, and therefore
the ideal attention map should focus only on these parts.
Usefulness: One objective of our proposed attention
map is that it enhances the ﬁnal binary classiﬁcation performance of the network. This is accomplished by feeding the
attention map back into the network to ignore non-activated
regions. This follows naturally from the fact that modiﬁed
images may only be partially modiﬁed. Through the attention map, we can remove the real regions of a partial fake
image so that the features used for ﬁnal binary classiﬁcation
are purely from modiﬁed regions.
Modularity: To create a truly utilitarian solution, we
take great care to maintain the modularity of the solution.
Our proposed attention map can be implemented easily and
plugged into existing backbone networks, through the inclusion of a single convolution layer, its associated loss functions, and masking the subsequent high-dimensional features. This can even be done while leveraging pre-trained
networks by initializing only the weights that are used to
produce the attention map.
3.2. Attention-based Layer
As shown in Fig. 3, the attention-based layer can be applied to any feature map of a classiﬁcation model, and focus
the network’s attention on discriminative regions. Specifically, the input of the attention-based layer is a convolutional feature map F ∈RH×W ×C, where H, W, C
are height, width, and the number of channels, respectively. For simplicity, we omit the mini-batch dimension
in this notation. Then we can generate an attention map
Matt = Φ(F) ∈RH×W by processing F, where Φ(·) denotes the processing operator. The output of attention module is the reﬁned feature map F′, which is calculated as:
F′ = F ⊙Sigmoid(Matt),
where ⊙denotes element-wise multiplication. The intensity
of each pixel in the attention map is close to 0 for the real
regions, and close to 1 for the fake regions. In other words,
the pixel of the attention map indicates the probability of
the original image patch being a fake region. This helps
the subsequent backbone network to focus its processing
to the non-zeros areas of the attention map, i.e., the fake
regions. Here, we propose two approaches to implement
Φ(·): manipulation appearance model and direct regression.
Manipulation Appearance Model (MAM). We assume
that any manipulated map can be represented as a linear
combination of a set of map prototypes:
Matt = ¯M + A · α,
where ¯M ∈R(H·W )×1 and A ∈R(H·W )×n are the predeﬁned average map and basis functions of maps. Thus the
attention map generation can be translated to estimate the
Figure 4. Mean map ¯
M and 10 basis components A.
weight parameter α ∈Rn×1, for each training image. We
utilize one additional convolution and one fully connected
layer to regress the weights from the feature map F (Fig. 3).
The beneﬁt of our proposed MAM is two fold. First,
this constrains the solution space of map estimation. Second, the complexity of the attention estimation is decreased,
which is helpful for generalization. To calculate the statistical bases A, we apply Principal Component Analysis
(PCA) to 100 ground-truth manipulation masks computed
from FaceAPP. The ﬁrst 10 principal components are used
as bases, i.e., n = 10. Fig. 4 shows the mean map and 10
bases (or templates).
Direct Regression.
Another way to implement Φ(·) is to
estimate the attention map via a convolutional operation f:
f→Matt. f can consist of multiple convolutional layers or a single layer. This direct regression method is simple, yet effective, for adaptive feature reﬁnement. Later we
show that the beneﬁts of our proposed attention-based layer
are realized regardless of the choice of backbone networks.
This further validates our claim that the proposed solution is
modular and improves the usefulness and ﬂexibility of the
attention map.
3.3. Loss Functions
To train the binary classiﬁcation network, we may begin
with a pre-trained backbone network or to learn the backbone from scratch. Either way, the overall training loss is:
L = Lclassiﬁer + λ ∗Lmap,
where Lclassiﬁer is the binary classiﬁcation loss of Softmax
and Lmap is the attention map loss. λ is the loss weight.
For attention map learning, we consider three different
cases: supervised, weakly supervised, and unsupervised.
Supervised learning.
If the training samples are paired
with ground truth attention masks, we can train the network
in a supervised fashion, using Eqn. 4.
Lmap = ||Matt −Mgt||1,
where Mgt is the ground truth manipulation mask. We use
zero-maps as the Mgt for real faces, and one-maps as the
Mgt for entirely synthesized fake faces. For partially manipulated faces, we pair fake images with their corresponding source images, compute the absolute pixel-wise difference in the RGB channels, convert into grayscale, and di-
Table 1. Comparison of fake face datasets along different aspects: number of still images, number of videos, number of fake types (identity
swap (Id. swap), expression swap (Exp. swap), attributes manipulation, and entire image synthesis (Entire syn.)) and pose variation.
# Still images
# Video clips
# Fake types
Attr. mani.
Entire syn.
Zhou et al. 
Yang et al. 
Deepfake 
FaceForensics++ 
[−30◦, 30◦]
FakeSpotter 
DFFD (our)
[−90◦, 90◦]
vide by 255 to produce a map in the range of . We empirically determine the threshold of 0.1 to obtain the binary
modiﬁcation map as Mgt. We posit this strong supervision
can help attention-based layer to learn the most discriminative regions and features for fake face detection.
Weakly supervised learning.
For partially manipulated faces, sometimes the source images are not available.
Hence, we can not obtain the ground truth manipulation
mask as described above. However, we would still like to
include these faces in learning the attention maps. To this
end, we propose a weak supervision map loss as in Eqn. 5:
|Sigmoid(Matt) −0|,
| max(Sigmoid(Matt)) −0.75|.
This loss drives the attention map to remain un-activated for
real images, i.e., all 0. For fake images, regardless of entire
or partial manipulation, the maximum map value across the
entire map should be sufﬁciently large, 0.75 in our experiments. Hence, for partial manipulation, an arbitrary number
of the map values can be zeros, as long as at least one modiﬁed local region has a large response.
Unsupervised learning.
The proposed attention module
can also allow us to train the network without any map
supervision when λm is set to 0. With only image-level
classiﬁcation supervision, the attention map learns informative regions automatically. More analysis of these losses is
available in the experiments section.
4. Diverse Fake Face Dataset
One of our contributions is the construction of a dataset
with diverse types of fake faces, termed Diverse Fake
Face Dataset (DFFD). Compared with previous datasets in
Tab. 1, DFFD contains greater diversity, which is crucial for
detection and localization of face manipulations.
Data Collection.
In Sec. 1, we introduced four main facial manipulation types: identity swap, expression swap, attribute manipulation, and entire synthesized faces. We thus
collect data from these four categories by adopting respective state-of-the-art (SOTA) methods to generate fake images. Among all images and video frames, 47.7% are from
male subjects, 52.3% are from females, and the majority of
samples are from subjects in the range 21-50 years of age.
For the face size, both real and fake samples have both low
quality and high quality images. This ensures that the distributions of gender, age, and face size are less biased.
Real face images.
We utilize FFHQ and
CelebA datasets as our real face samples since the faces
contained therein cover comprehensive variations in race,
age, gender, pose, illumination, expression, resolution, and
camera capture quality. We further utilize the source frames
from FaceForensics++ as additional real faces.
Identity and expression swap.
For facial identity and
expression swap, we use all the video clips from Face-
Forensics++ .
The FaceForensics++ contains 1,000
real videos collected from YouTube and their corresponding 3,000 manipulated versions which are divided into two
groups: identity swap using FaceSwap and Deepfake ,
and expression swap using Face2Face . From a public
website , we collect additional identity swap data, which
are videos generated by Deep Face Lab (DFL) .
Attributes
manipulation.
methods FaceAPP and StarGAN to generate attribute
manipulated images, where 4,000 faces of FFHQ and
2,000 faces of CelebA are the respective input real images.
FaceAPP, as a consumer-level smartphone app, provides 28
ﬁlters to modify speciﬁed facial attributes, e.g., gender, age,
hair, beard, and glasses. The images are randomly modi-
ﬁed with an automated script running on Android devices.
For each face in FFHQ, we generate three corresponding
fake images: two with a single random manipulation ﬁlter, and one with multiple manipulation ﬁlters. For each
face in CelebA, we generate 40 fake images by StarGAN,
a GAN-based image-to-image translation method. In total,
we collect 92K attribute manipulated images.
Entire face synthesis.
Recent works such as PG-
GAN and StyleGAN achieve remarkable success
in realistic face image synthesis. PGGAN proposes a progressive training scheme both for generator and discriminator, which can produce high-quality images. StyleGAN
redesigns the generator by borrowing from style transfer literature. Thus, we use the pre-trained model of PGGAN and
StyleGAN to create 200k and 100k high-quality entire fake
images, respectively. Figure 5 shows examples of DFFD.
Pre-processing. InsightFace is utilized to estimate the
bounding box and 5 landmarks for each image. We discard
images whose detection or alignment fails. We further gen-
Figure 5. Example faces in our DFFD. (a) Real images/frames
from FFHQ, CelebA and FaceForensics++ datasets; (b) Paired
face identity swap images from FaceForensics++ dataset; (c)
Paired face expression swap images from FaceForensics++
dataset; (d) Attributes manipulated examples by FaceAPP and
StarGAN; (e) Entire synthesized faces by PGGAN and StyleGAN.
erate ground truth manipulation masks for fake images as
described in Sec. 3.3. To enforce consistency, if a fake face
image is derived from a source real face image, we use the
same landmarks of the real face image for face cropping.
Protocols. We collect 781,727 samples for real image, and
1,872,007 samples for fake ones. Within these samples, we
randomly select a subset of 58,703 real images and 240,336
fake ones to make the size of our dataset manageable and
to balance the size of each sub-category. For video samples, we extract one frame per second in order to reduce
the size without sacriﬁcing the diversity of DFFD. We randomly split the data into 50% for training, 5% for validation
and 45% for testing. All fake images manipulated from the
same real image are in the same set as the source image.
5. Experimental Results
5.1. Experimental Setup
Implementation Details: The loss weight λ is set to 1
and the batch size is 16, where each mini-batch consists of
8 real and 8 fake images. We use XceptionNet and
VGG16 as backbone networks. Both two networks
were pre-trained on ImageNet and ﬁne-tuned on DFFD.
Adam optimizer is used with a learning rate of 0.0002 in
all experiments. Depending on the backbone architecture,
we train for 75k-150k iterations, which requires less than 8
hours on an NVidia GTX 1080Ti.
Table 2. Ablation for the beneﬁt of the attention map, with various
combinations of map generation methods and supervisions. [Key:
top performance for supervised and weakly supervised methods]
Map Supervision
+ Reg., unsup.
+ Reg., weak sup.
+ Reg., sup.
+ Reg., sup. - map
+ MAM, unsup.
+ MAM, weak sup.
+ MAM, sup.
+ MAM, sup. - map
Metrics: For all experiments, we utilize the protocols
deﬁned in Sec. 4.
For detection, we report Equal Error
Rate (EER), Area Under Curve (AUC) of ROC, True Detect Rate (TDR) at False Detect Rate (FDR) of 0.01% (denoted as TDR0.01%), and TDR at FDR of 0.1% (denoted
as TDR0.1%). For localization, with known ground-truth
masks, we report Pixel-wise Binary Classiﬁcation Accuracy
(PBCA), which treats each pixel as an independent sample
to measure classiﬁcation accuracy, Intersection over Union
(IoU), and Cosine similarity between two vectorized maps.
We also propose a novel metric, termed Inverse Intersection
Non-Containment (IINC) for evaluating face manipulation
localization performance, as described in Sec. 5.4.
5.2. Ablation Study
Beneﬁt of Attention map: We utilize the SOTA XceptionNet as our backbone network. It is based on depthwise separable convolution layers with residual connections. We convert XceptionNet into our model by inserting
the attention-based layer between Block 4 and Block 5 of
the middle ﬂow, and then ﬁne-tune on DFFD training set.
In Tab. 2, we show a comparison of the direct regression
(Reg.) and manipulation appearance model (MAM) with
different supervision strategies, i.e., unsupervised (unsup.),
weakly supervised (weak sup.) and supervised (sup.) learning. While four detection metrics are listed for completeness, considering the overall strong performance of some
metrics and the preferred operational point of low FDR in
practice, TDR at low FDR (i.e., TDR0.01%) should be the
primary metric for comparing various methods.
Unsurprisingly, the supervised learning outperforms the
weakly supervised and unsupervised, in both the detection
and localization accuracies. Additionally, comparing two
map estimation approaches, regression-based method performs better with supervision.
In contrast, MAM-based
method is superior for weakly supervised or unsupervised
cases as MAM offers strong constraint for map estimation.
Finally, instead of using the softmax output, an alternative is to use the average of the estimated attention map for
detection, since loss functions encourage low attention values for real faces while higher values for fake ones. The
Table 3. Our attention layer in two backbone networks.
Xception + Reg.
Xception + MAM
VGG16 + Reg.
VGG16 + MAM
Figure 6. Forgery detection ROCs of the XceptionNet backbone
with and without the attention mechanism.
performance of this alternative is shown in the rows of ‘*,
sup. - map’ in Tab. 2. Although this is not superior to the
softmax output, it demonstrates that the attention map is itself useful for the facial forgery detection task.
Effect on Backbone Networks: We also report the results of a shallower backbone network VGG16 . Tab. 3
compares XceptionNet and VGG16 with and without the
attention layer. Both Reg. and MAM models are trained in
the supervised case. We observe that using attention mechanism does improve the detection on both backbones.
Speciﬁcally, with a large and deep network (Xception-
Net), the attention map can be directly produced by the network given the large parameter space. This directly produced attention map can better predict the manipulated regions than a map estimated from the MAM bases. Inversely,
when using a smaller and shallower network (VGG16), we
ﬁnd that the direct production of the attention map causes
contention in the parameter space.
Hence including the
prior of the MAM bases reduces this contention and allows
for increased detection performance, though its estimation
of the manipulated regions is constrained by the map bases.
5.3. Forgery Detection Results
We ﬁrst show the ROCs on our DFFD in Fig. 6. Obviously, the direct regression approach for the attention map
produces the best performing network at low FDR, which
is not only the most challenging scenario, but also the most
relevant to the practical applications. In addition, the proposed attention layer substantially outperforms the conventional XceptionNet, especially at lower FDR. Fig. 7 plots
binary classiﬁcation accuracy of our Reg., sup and baseline
for different fake types of DFFD. The proposed approach
beneﬁts forgery detection of all considered fake types, es-
Figure 7. Binary classiﬁcation accuracy for different fake types.
Table 4. AUC (%) on UADFV and Celeb-DF. All baseline results
are quoted from 
Training data
UADFV 
Celeb-DF 
Two-stream 
Private data
Private data
MesoInception4 
HeadPose 
VA-MLP 
Private data
VA-LogReg 
Multi-task 
Xception-FF++ 
UADFV, DFFD
Xception+Reg.
Xception+Reg.
Xception+Reg.
UADFV, DFFD
pecially for the facial identity and expression swap.
We further validate our model on public datasets, where
SOTA facial forgery detection methods have been tested.
Table 4 summarizes the performance of all methods. Note
that the performances shown here are not strictly comparable since not all methods are trained on the same dataset.
First, we evaluate on the UADFV and Celeb-DF datasets
with the models trained with DFFD. As shown in Tab. 4,
our proposed approach signiﬁcantly outperforms all the
baselines on Celeb-DF and achieves competitive results on
UADFV. FWA and HeadPose demonstrate superior performance on UADFV partially because they are
trained on the same UADFV dataset, while this data source
is not in our DFFD. Second, for a fair comparison, we
train our method and baseline Xception on UADFV training
set. In this case, our method outperforms all baselines on
UADFV, and still shows superior generalization on Celeb-
DF. Third, the results in Tab. 4 also help us to identify
both the source and amount of improvements. For example, 75.6% →84.2% is an improvement due to attention
mechanism while 52.2% →63.9% and 57.1% →64.4%
are due to the greater diversity of DFFD dataset.
5.4. Manipulation Localization Results
We utilize three metrics for evaluating the attention
maps: Intersection over Union (IoU), Cosine Similarity, and
Pixel-wise Binary Classiﬁcation Accuracy (PBCA). However, these three metrics are inadequate for robust evaluation of these diverse maps.
Thus, we propose a novel
metric deﬁned in Eqn. 6, termed Inverse Intersection Non-
Figure 8. Estimated attention maps by applying Xception + Reg. sup. model to real and 4 types of manipulated images, with IINC and
PBCA scores computed w.r.t. ground truth. While the overall areas of the attention maps are correct, their ﬁdelity could be further improved.
Table 5. Evaluating manipulation localization with 4 metrics.
Cosine Similarity ↓
Containment (IINC), to evaluate the predicted maps:
if Mgt = 0 and Matt = 0
if Mgt = 0 xor Matt = 0
otherwise,
where I and U are the intersection and union between the
ground truth map, Mgt, and the predicted map, Matt, respectively. M and |M| are the mean and L1 norm of M,
respectively. The two fractional terms measure the ratio of
the area of the intersection w.r.t. the area of each map, respectively. IINC improves upon other metrics by measuring
the non-overlap ratio of both maps, rather than their combined overlap, as in IoU. Additionally, the IoU and Cosine
Similarity are undeﬁned when either map is uniformly 0,
which is the case for real face images.
The beneﬁts of IINC as compared to other metrics are
shown in Fig. 9. Note that IOU and Cosine similarity are
not useful for cases (a-c), where the scores are the same,
but the maps have vastly different properties.
Similarly,
PBCA is not useful for the cases (e-g), as the ratio of misclassiﬁcation is not represented in PBCA. For example, case
(g) over-estimates by a factor of 100% and case (e) overestimates by 200%, while case (f) both over- and underestimates by 150%. The IINC provides the optimal ordering
by producing the same order as IOU when it is useful, cases
(d-g), and similarly with PBCA when it is useful, cases (ac). Thus, IINC is a more robust metric for comparing the
attention maps than the previous metrics.
The localization ability of our Xception + Reg. sup.
model to predict the attention maps is shown in Tab. 5. In
Fig. 8, we show the IINC and PBCA for some test examples.
Figure 9. A toy-example comparing 4 metrics in evaluating attention maps. White is the manipulated pixel and black is the real
pixel. IOU and Cosine metrics do not adequately reﬂect the differences in cases (a-c), while PBCA is not useful for cases (e-g). In
contrast, the proposed IINC is discriminative in all cases.
The ordering of the IINC scores aligns with qualitative human analysis. The ﬁrst cases in (d) and (e) are examples
where the PBCA is high only because the majority of each
map is non-activated. The IINC is more discriminative in
these cases due to the non-overlap between the maps. For
the third cases in (d) and (e), the IINC produces the same
score because the maps display the same behavior (a large
amount of over-activation), whereas the PBCA prefers the
example in (d) because its maps have fewer activations.
6. Conclusion
We tackle the digitally manipulated face image detection and localization task. Our proposed method leverages
an attention mechanism to process the feature maps of the
detection model. The learned attention maps highlight the
informative regions for improving the detection ability and
also highlight the manipulated facial regions. In addition,
we collect the ﬁrst facial forgery dataset that contains diverse types of fake faces. Finally, we empirically show that
the use of our attention mechanism improves facial forgery
detection and manipulated facial region localization. This
is the ﬁrst uniﬁed approach that tackles a diverse set of face
manipulation attacks, and also achieves the SOTA performance in comparison to previous solutions.
Supplementary
In this supplementary material, we provide some details
and additional experimental results.
A. Details
Here we will further detail the Diverse Fake Face Dataset
and proposed attention map, and analyze additional experiments.
A.1. DFFD Dataset Details
The DFFD was constructed from large and commonly
used facial recognition datasets. This widespread use of
FFHQ and CelebA validate our decision to utilize these as
our real images, and the generation of manipulated images
from them. As shown in Fig. 10, the DFFD encompasses
large variance in both face size and human age, for both
real and manipulated images. Details about the images from
datasets used to construct the DFFD are available in Tab. 6.
A.2. Network Architecture Details
In Fig. 11, we show a simpliﬁed diagram of the placement of the attention layer within the Xception network.
Due to its modularity, the attention layer can easily be added
to any network, in a similar fashion to placing the attention
layer in a different location in the Xception network.
B. Additional Experimental Results
B.1. Human Study
We conduct a human study to determine the ability of humans to distinguish between real and manipulated images
in the DFFD. 10 humans participated in the study. This
was accomplished using a random set of 110 images from
the DFFD, where 10 images were taken from each row in
Tab 6. For each image, the human was required to classify
between Real, Entire Fake, and Partial Fake, and additionally required to provide polygon-based regions of interest
(attention maps) for Partial Fakes. The results of this study
are shown in Tab. 7. It is clear that humans have signiﬁcant difﬁculty in the binary classiﬁcation task (Entire Fake
and Partial Fake are considered a single class), while our
attention based solution performs almost perfectly.
In Fig. 12, we show the manipulation maps produced by
our proposed solution compared to the maps produced by
humans. Humans focus largely on semantic concepts such
as image quality, large artifacts, or strange lighting/color
when judging between real and fake images. Due to this,
humans do not detect the very subtle difference in the image
“ﬁngerprint”, which our proposed solution is able to detect.
B.2. Additional Performance Evaluation
For our best performing model, Xception Regression
Map with supervision, we conduct analysis in two aspects.
i) Fig. 13 shows the worst 3 test samples among the real
Figure 10. (a) Distribution of the face bounding box sizes (pixel)
and (b) Age distribution of our DFFD.
Figure 11. The overall architecture of XceptionNet and its enhancement with our proposed attention later. The original XceptionNet has entry ﬂow, middle ﬂow, and exit ﬂow, where the middle ﬂow is composed of 8 blocks. Our attention layer can be added
after any of the blocks.
Figure 12. The attention maps produced by our proposed solution
and humans during the human study.
test faces and each fake types. For example, the images in
the ﬁrst column have the lowest Softmax probability of being the real class. Among these samples, some have heavy
makeup, and others are of low image quality. Meanwhile,
the failure cases for the manipulated or entirely synthetic
images are high quality and devoid of defects or artifacts.
ii) Tab. 8 shows the accuracy of testing samples in each fake
type. The completely synthesized images are the easiest to
detect. This is due to the artiﬁcial “ﬁngerprint” these methods leave on the generated images, which is easily distinguishable from real images. In contrast, identity and expression manipulated images are the most challenging to detect,
where image is of good quality and no noticeable artifacts
exist, as in the 2nd and 3rd columns in Fig. 13.
Table 6. Statistics of our DFFD composition and protocol.
# Total Samples
# Training
# Validation
Average face width (pixel)
CelebA 
Original @ FaceForensics++ 
Deepfakes @ FaceForensics++ 
FaceSwap @ FaceForensics++ 
Face2Face @ FaceForensics++ 
Attr. Manip.
FaceAPP 
StarGAN 
Entire Syn.
PGGAN 
StyleGAN 
Table 7. Comparison between the proposed solution and humans
for detecting manipulated images and localization of the manipulated regions. Larger values are better for all but the EER.
XceptionRegSup
Table 8. Fake face detection performance of the Xception Regression Map with supervision for each fake type.
EXP Manip.
Attr. Manip.
Entire Syn.
Figure 13. Failure examples of the Xception with Regression Map
under supervision. From left to right, the columns are top 3 worst
samples of real, identity manipulated, expression manipulated,
completely generated, and attribute modiﬁed, respectively.
B.3. Additional Ablation Study
In Fig 9, we show an ablation for the placement of the
attention layer within the middle ﬂow of the Xception network. Two trends emerge from this; i) the AUC and EER
decrease as the attention layer is placed later in the network,
and ii) the PBCA increases as the attention layer is placed
later in the network. This second trend is expected, the network is able to produce a more ﬁnely-tuned attention map
given more computational ﬂexibility and depth. The ﬁrst
trend is more intriguing, because it shows that earlier focus
from the attention map is more beneﬁcial for the network
Table 9. The performance of the attention map at different placements in the middle ﬂow of the XceptionNet architecture.
Map position
Figure 14. The attention map estimation performance of the proposed method when using different thresholds to binarize the predicted map (a) and the ground truth map (b). The threshold for the
other map in either case was 0.1.
than a ﬁnely-tuned attention map later. This earlier attention provides the network with additional time to inspect
the features selected by the attention map in order to distinguish between real and manipulated images at a semantic
In Fig 14, we show the empirical decision for the threshold of 0.1 that we used to convert maps from continuous
values (in the range ) to binary values. This provides
strong performance in both graphs of Fig. 14, while being
semantically reasonable. A modiﬁcation of 0.1 corresponds
to a modiﬁcation of magnitude equal to 25 in the typical
RGB range of . While a modiﬁcation of small magnitude (< 10) is almost undetectable by a human, a modiﬁcation of larger magnitude (> 25) is signiﬁcant. Therefore,
all experiments presented utilized this empirical threshold
value of 0.1.