HAL Id: hal-00363242
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Probabilistic forecasts, calibration and sharpness.
Tilmann Gneiting, Fadoua Balabdaoui, Adrian Raftery
To cite this version:
Tilmann Gneiting, Fadoua Balabdaoui, Adrian Raftery. Probabilistic forecasts, calibration and sharpness.. 2007. ￿hal-00363242￿
Probabilistic Forecasts, Calibration and Sharpness
JRSSB Submission B6257 Revision 1
Tilmann Gneiting1, Fadoua Balabdaoui2 and Adrian E. Raftery1
 , , 
Corresponding author: Tilmann Gneiting
1Department of Statistics, University of Washington
Seattle, Washington 98195-4322, U.S.A.
2Institut f¨ur Mathematische Stochastik, Georg-August-Universit¨at G¨ottingen
37073 G¨ottingen, Germany
Probabilistic forecasts of a continuous variable take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation
of predictive performance that is based on the paradigm of maximizing the sharpness of the
predictive distributions subject to calibration. Calibration refers to the statistical consistency
between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive
distributions and is a property of the forecasts only. A simple theoretical framework phrased
in terms of a game between nature and forecaster allows us to distinguish probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking
calibration and sharpness, among them the probability integral transform (PIT) histogram,
marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the
Stateline wind energy center in the U.S. Paciﬁc Northwest. In combination with cross-validation
or in the time series context, our proposal provides very general, nonparametric alternatives to
the use of information criteria for model diagnostics and model selection.
Keywords: Cross-validation; Density forecast; Ensemble prediction system; Ex post evaluation; Forecast veriﬁcation; Model diagnostics; Posterior predictive assessment; Predictive
distribution; Prequential principle; Probability integral transform; Proper scoring rule
Introduction
A major human desire is to make forecasts for the future. Forecasts characterize and reduce but
generally do not eliminate uncertainty. Consequently, forecasts should be probabilistic in nature,
taking the form of probability distributions over future events . Indeed, over the past
two decades the quest for good probabilistic forecasts has become a driving force in meteorology
 . Major economic forecasts such as the quarterly Bank of England
inﬂation report are issued in terms of predictive distributions, and the rapidly growing area of
ﬁnancial risk management is dedicated to probabilistic forecasts of portfolio values .
In the statistical literature, advances in Markov chain Monte Carlo
methodology have led to explosive growth in the use of
predictive distributions, mostly in the form of Monte Carlo samples from the posterior predictive
distribution of quantities of interest.
It is often critical to assess the predictive ability of forecasters, or to compare and rank competing
forecasting methods. Atmospheric scientists talk of forecast veriﬁcation when they refer to this
process , and much of the underlying methodology has been developed
by meteorologists. There is also a relevant strand of work in the econometrics literature . Murphy and Winkler proposed
a general framework for the evaluation of point forecasts that uses a diagnostic approach based on
graphical displays, summary measures and scoring rules. In this paper, we consider probabilistic
forecasts (as opposed to point forecasts) of continuous and mixed discrete-continuous variables,
such as temperature, wind speed, precipitation, gross domestic product, inﬂation rates and portfolio
values. In this situation, probabilistic forecasts take the form of predictive densities or predictive
cumulative distribution functions (CDFs), and the diagnostic approach faces a challenge, in that
the forecasts take the form of probability distributions while the observations are real-valued.
We employ the following, simple theoretical framework to provide guidance in our methodological work. At times or instances t = 1, 2, . . ., nature chooses a distribution, Gt, which we think of
as the true data generating process, and the forecaster picks a probabilistic forecast in the form
of a predictive cumulative distribution function, Ft. The outcome, xt, is a random number with
distribution Gt. Throughout, we assume that nature is omniscient, in the sense that the forecaster’s
information basis is at most that of nature. Hence, if
we talk of the ideal forecaster. In practice, the true distribution, Gt, remains hypothetical, and
the predictive distribution, Ft, is an expert opinion that may or may not derive from a statistical
prediction algorithm. In accordance with Dawid’s prequential principle, the predictive distributions need to be assessed on the basis of the forecast-observation pairs (Ft, xt) only, irrespectively
of their origins. Dawid and Diebold et al. proposed the use of the probability integral
transform (PIT) value,
pt = Ft(xt),
for doing this. If the forecasts are ideal and Ft is continuous, then pt has a uniform distribution.
Hence, the uniformity of the probability integral transform is a necessary condition for the forecaster
to be ideal, and checks for its uniformity have formed a cornerstone of forecast evaluation. In the
classical time series framework, each Ft corresponds to a one-step ahead forecast, and checks for
the uniformity of the PIT values have been supplemented by tests for independence .
Hamill gave a thought-provoking example of a forecaster for whom the histogram of the
PIT values is essentially uniform, even though every single probabilistic forecast is biased. His
example aimed to show that the uniformity of the PIT values is a necessary but not a suﬃcient
condition for the forecaster to be ideal. To ﬁx the idea, we consider a simulation study based on the
scenario described in Table 1. At times or instances t = 1, 2, . . ., nature draws a standard normal
random number µt and selects the data generating distribution, Gt = N(µt, 1). In the context of
weather forecasts, we might think of µt as an accurate description of the latest observable state
of the atmosphere, summarizing all information that a forecaster might possibly have access to.
The ideal forecaster is an expert meteorologist who conditions on the current state, µt, and issues
an ideal probabilistic forecast, Ft = Gt.
The climatological forecaster takes the unconditional
distribution, Ft = N(0, 2), as probabilistic forecast. The unfocused forecaster observes the current
state, µt, but adds a mixture component to the forecast, which can be interpreted as distributional
Table 1: Scenario for the simulation study. At times t = 1, 2, . . ., nature chooses a distribution,
Gt, and forecaster chooses a probabilistic forecast, Ft. The observations are independent random
numbers xt with distribution Gt. We write N(µ, σ2) for the normal distribution with mean µ and
variance σ2, respectively. The sequences (µt)t=1,2,..., (τt)t=1,2,... and (δt, σ2
t )t=1,2,... are independent
identically distributed, and independent of each other and the observations.
Gt = N(µt, 1) where µt ∼N(0, 1)
Ideal forecaster
Ft = N(µt, 1)
Climatological forecaster
Ft = N(0, 2)
Unfocused forecaster
2 ( N(µt, 1) + N(µt + τt, 1))
where τt = ±1 with probability 1
Hamill’s forecaster
Ft = N(µt + δt, σ2
where (δt, σ2
2, 1), (−1
2, 1) or (0, 169
100) with probability 1
bias. A similar comment applies to Hamill’s forecaster. Clearly, our forecasters are caricatures; yet,
climatological reference forecasts and conditional biases are frequently observed in practice. The
observation, xt, is a random draw from Gt, and we repeat the prediction experiment 10000 times.
Figure 1 shows that the PIT histograms for the four forecasters are essentially uniform.
In view of the reliance on the probability integral transform in the literature, this is a disconcerting result. As Diebold et al. pointed out, the ideal forecaster is preferred by all users,
regardless of the respective loss function. Nevertheless, the probability integral transform is unable to distinguish between the ideal forecaster and her competitors. To address these limitations,
we propose a diagnostic approach to the evaluation of predictive performance that is based on
the paradigm of maximizing the sharpness of the predictive distributions subject to calibration.
Calibration refers to the statistical consistency between the distributional forecasts and the observations, and is a joint property of the predictions and the observed values. Sharpness refers to
the concentration of the predictive distributions and is a property of the forecasts only. The more
concentrated the predictive distributions, the sharper the forecasts, and the sharper the better,
subject to calibration.
The remainder of the paper is organized as follows. Section 2 develops our theoretical framework
for the assessment of predictive performance. We introduce the notions of probabilistic, exceedance
and marginal calibration, give examples and counterexamples, and discuss a conjectured sharpness
principle. In Section 3, we propose diagnostic tools such as marginal calibration plots and sharpness
diagrams that complement the PIT histogram. Proper scoring rules address calibration as well as
sharpness and allow to rank competing forecast procedures. Section 4 turns to a case study on
probabilistic forecasts at the Stateline wind energy center in the U.S. Paciﬁc Northwest.
diagnostic approach yields a clear-cut ranking of statistical algorithms for forecasts of wind speed,
and suggests improvements that can be addressed in future research.
Similar approaches hold
considerable promise as very general, nonparametric tools for statistical model selection and model
Perfect Forecaster
Probability Integral Transform
Relative Frequency
Climatological Forecaster
Probability Integral Transform
Relative Frequency
Unfocused Forecaster
Probability Integral Transform
Relative Frequency
Hamill’s Forecaster
Probability Integral Transform
Relative Frequency
Figure 1: Probability integral transform (PIT) histograms.
diagnostics. The paper closes with a discussion in Section 5 that emphasizes the need for routine
assessments of sharpness in the evaluation of predictive performance.
Modes of calibration
In our theoretical framework, we consider probabilistic forecasting as a game played between nature
and forecaster.
At times or instances t = 1, 2, . . ., nature picks a probability distribution, Gt,
and forecaster chooses a probabilistic forecast in the form of a predictive distribution, Ft. The
observation, xt, is a random draw from nature’s proposal distribution, Gt. Throughout, we assume
that nature is omniscient, in the sense that the information basis of the forecaster is at most that
of nature. For simplicity, we assume that Ft and Gt are continuous and strictly increasing on R.
Some critical comments are in order. Evidently, Gt is not observed in practice and any operational evaluation must be performed on the basis of the forecasts, Ft, and the outcomes, xt, only.
The game theoretic framework of Shafer and Vovk and Vovk and Shafer also views
forecasting as a game, with three players: forecaster, sceptic, and reality or nature. Forecaster and
sceptic have opposite goals, and one of them wins, while the other loses. There is no goal assigned
to nature, who directly chooses and reveals the outcome, xt, without recourse to any underlying
data generating distribution. The key question in this deep strand of literature, which culminates
in Theorem 3 of Vovk and Shafer , is that for the existence of certain types of strategies
for forecaster. Shafer and Vovk consider probability forecasts for dichotomous events, rather than
distributional forecasts of real-valued quantities, and they do not consider the problem tackled here,
namely the comparative evaluation of competing forecasters, for which they hint at future work
 .
In comparing forecasters, we take the pragmatic standpoint of a user, who is to rank and choose
among a number of competitors, as exempliﬁed in Section 4. In this type of situation, it is absolute
performance that matters, rather than relative performance that may result from the use of possibly
distinct information bases.
Our approach seems slightly broader than Dawid’s prequential framework, in that we
think of (Ft)t=1,2,... as a general countable sequence of forecasts, with the index referring to time,
space or subjects, depending on the prediction problem at hand. The forecasts need not be sequential, and when Ft+1 is issued, xt may or may not be available yet.
Probabilistic calibration, exceedance calibration and marginal calibration
Henceforth, (Ft)t=1,2,... and (Gt)t=1,2,... denote sequences of continuous and strictly increasing CDFs,
possibly depending on stochastic parameters. We think of (Gt)t=1,2,... as the true data generating
process and of (Ft)t=1,2,... as the associated sequence of probabilistic forecasts. The following deﬁnition refers to the asymptotic compatibility between the data generating process and the predictive
distributions in terms of three major modes of calibration. Given that (Ft)t=1,2,... and (Gt)t=1,2,...
might depend on stochastic parameters, convergence is understood as almost sure convergence and
is denoted by an arrow. For now, these notions are of theoretical interest only; in Section 3 below,
they lend support to our methodological proposals.
Deﬁnition 1 (modes of calibration)
(a) The sequence (Ft)t=1,2,... is probabilistically calibrated relative to the sequence (Gt)t=1,2,... if
p ∈(0, 1).
(b) The sequence (Ft)t=1,2,... is exceedance calibrated relative to (Gt)t=1,2,... if
◦Ft(x) −→x
(c) The sequence (Ft)t=1,2,... is marginally calibrated relative to (Gt)t=1,2,... if the limits ¯G(x) =
t=1 Gt(x) and ¯F(x) = limT →∞1
t=1 Ft(x) exist and equal each other for all
x ∈R, and if the common limit distribution places all mass on ﬁnite values.
(d) The sequence (Ft)t=1,2,... is strongly calibrated relative to (Gt)t=1,2,... if it is probabilistically
calibrated, exceedance calibrated and marginally calibrated.
If each subsequence of (Ft)t=1,2,... is probabilistically calibrated relative to the associated subsequence of (Gt)t=1,2,..., we talk of complete probabilistic calibration. Similarly, we deﬁne completeness for exceedance, marginal and strong calibration. Probabilistic calibration is essentially
equivalent to the uniformity of the PIT values. Exceedance calibration is deﬁned in terms of thresholds, and marginal calibration requires that the limit distributions ¯G and ¯F exist and equal each
other. The existence of ¯G is a natural assumption in meteorological problems and corresponds to
the existence of a stable climate. Hence, marginal calibration can be interpreted in terms of the
equality of observed and forecast climatology.
Various authors have studied calibration in the context of probability forecasts for sequences of
binary events . The progress is impressive
and culminates in the elegant game theoretic approach of Vovk and Shafer . Krzysztofowicz
 discussed calibration in the context of Bayesian forecasting systems, and Krzysztofowicz
and Sigrest studied calibration for quantile forecasts of quantitative precipitation. We are
unaware of any prior discussion of notions of calibration for probabilistic forecasts of continuous
variables.
The examples in this section illustrate the aforementioned modes of calibration and discuss some of
the forecasters in our initial simulation study. Throughout, (µt)t=1,2,..., (σt)t=1,2,... and (τt)t=1,2,...
denote independent sequences of independent identically distributed random variables. We write
N(µ, σ2) for the normal distribution with mean µ and variance σ2, identify distributions and CDFs,
and let Φ denote the standard normal cumulative.
In each example, nature draws a standard
normal random number, µt, that corresponds to the information basis at time t and picks the data
generating distribution Gt = N(µt, 1).
Example 1 (ideal forecaster) The predictive distribution of the ideal forecaster equals nature’s
proposal distribution, that is, Ft = Gt = N(µt, 1) for all t. This forecaster is strongly calibrated.
Example 2 (climatological forecaster) The climatological forecaster issues the distributional
forecast Ft = N(0, 2), irrespectively of t. This forecaster is probabilistically calibrated and marginally calibrated. However,
◦Ft(x) = 1
for x ∈R, in violation of exceedance calibration.
The characteristic property in Example 2 is that each predictive distribution, Ft, equals nature’s
limiting distribution, ¯G. We call any forecaster with this property a climatological forecaster. For
climatological forecasts, probabilistic calibration is essentially equivalent to marginal calibration.
Indeed, if ¯G is continuous and strictly increasing, then putting p = Ft(x) = ¯G(x) in (3) recovers the
marginal calibration condition. In practice, climatological forecasts are constructed from historical
records of observations, and they are often used as reference forecasts.
Example 3 (unfocused forecaster) The predictive distribution of the unfocused forecaster is
the mixture distribution
2 (N(µt, 1) + N(µt + τt, 1)),
Table 2: The three major modes of calibration are logically independent of each other and may
occur in any combination. For instance, the unfocused forecaster in Example 3 is probabilistically
calibrated (P), but neither exceedance calibrated (E) nor marginally calibrated (P).
Properties
Example 1 (ideal forecaster)
Gt = Ft = N(t, 1)
Example 2 (climatological forecaster)
Example 3 (unfocused forecaster)
Example 4 (mean-biased forecaster)
Example 5 (sign-biased forecaster)
Example 6 (mixed forecaster)
Gt = N(0, 1), Ft = N(1, 1)
where τt is either 1 or −1, with equal probabilities, and independently of µt. This forecaster is
probabilistically calibrated, but neither exceedance calibrated nor marginally calibrated. To prove
the claim for probabilistic calibration, put Φ±(x) = 1
2(Φ(x) + Φ(x ∓1)) and note that
+ (p) + Φ ◦Φ−1
where the equality follows upon putting p = Φ+(x), substituting and simplifying.
Exceedance
calibration does not hold, because
◦Ft(x) −→1
Φ−1 ◦Φ+(x) + Φ−1 ◦Φ−(x)
in general.
The marginal calibration condition is violated, because nature’s limit distribution,
¯G = N(0, 2), does not equal ¯F = 1
2 N(0, 2) + 1
4 N(−1, 2) + 1
4 N(1, 2).
Example 4 (mean-biased forecaster) This forecaster issues the probabilistic forecast Ft =
N(µt + τt, 1), where, again, τt is either 1 or −1, with equal probabilities, and independently of
µt. The mean-biased forecaster is exceedance calibrated but neither probabilistically calibrated nor
marginally calibrated.
Example 5 (sign-biased forecaster) The predictive distribution of the sign-biased forecaster
is Ft = N(−µt, 1). This forecaster is exceedance calibrated and marginally calibrated, but not
probabilistically calibrated.
Example 6 (mixed forecaster) The mixed forecaster randomizes between the climatological
and the sign-biased forecast, with equal probabilities and independently of µt.
This forecaster
is marginally calibrated, but neither probabilistically calibrated nor exceedance calibrated.
The examples in this section show that probabilistic calibration, exceedance calibration and
marginal calibration are logically independent of each other and may occur in any combination.
Table 2 summarizes the respective results.
Hamill’s forecaster
We add a discussion of Hamill’s forecaster. As previously, nature picks Gt = N(µt, 1), where µt is
standard normal. Hamill’s forecaster is a master forecaster who assigns the prediction task with
equal probability to any of three student forecasters, each of whom is biased, as described in Table
1. For Hamill’s forecaster,
Φ−1(p) + 1
= p + ϵ(p),
where |ϵ(p)| ≤0.0032 for all p but ϵ(p) ̸= 0 in general. The probabilistic calibration condition (3) is
violated, but only slightly so, resulting in deceptively uniform PIT histograms. As for exceedance
calibration, we note that
◦Ft(p) −→1
for x ∈R. Hence, Hamill’s forecaster is not exceedance calibrated either, nor marginally calibrated,
given that ¯G = N(0, 2) while ¯F = 1
2, 2) + N( 1
2, 2) + N(0, 269
Sharpness principle
In view of our assumption that forecaster’s information basis is at most that of nature, the best
situation that we can possibly hope for is the equality (1) of Ft and Gt that characterizes the ideal
forecaster. Operationally, we adopt the paradigm of maximizing the sharpness of the predictive
distributions subject to calibration. Our conjectured sharpness principle contends that the two
goals — ideal forecasts and the maximization of sharpness subject to calibration — are equivalent.
This conjectured equivalence, which we deliberately state loosely, could be explained in two ways.
One explanation is that suﬃciently stark notions of calibration, such as complete strong calibration
across many dynamic subsequences, imply asymptotic equivalence to the ideal forecaster. Strong
calibration alone, without the completeness condition, does not seem to impose enough restrictions,
but we are unaware of a counterexample and would like to know of one. An alternative and weaker
explanation states that any suﬃciently calibrated forecaster is at least as spread out as the ideal
forecaster.
With respect to this latter explanation, none of probabilistic, exceedance or marginal calibration
alone is suﬃciently stark. In the examples below it will be convenient to consider a probabilistic
calibration condition,
p ∈(0, 1),
for ﬁnite sequences (Ft)1≤t≤T relative to (Gt)1≤t≤T , and similarly for exceedance calibration and
marginal calibration. The examples extend to countable sequences in obvious ways. Now suppose
that σ > 0, a > 1, 0 < λ < 1/a and T = 2. Let G1 and G2 be continuous and strictly increasing
distributions functions with associated densities that are symmetric about zero and have ﬁnite
variance, var(G1) = σ2 and var(G2) = λσ2. If we deﬁne
G1(x) + G2
F2(x) = F1(ax),
var(F1) + var(F2) = 1
(1 + a2λ2) σ2 < (1 + λ2) σ2 = var(G1) + var(G2),
even though the ﬁnite probabilistic calibration condition (5) holds. A similar example can be given
for exceedance calibration. Suppose that σ > 0, 0 < a < 1 and
Let G1 and G2 be as above and deﬁne
F1(x) = G1
F2(x) = G2
var(F1) + var(F2) = 1
4 (1 + a)2
σ2 < (1 + λ2) σ2 = var(G1) + var(G2),
even though the ﬁnite exceedance calibration condition holds.
Evidently, a forecaster can be
marginally calibrated yet sharper than the ideal forecaster.
For climatological forecasts, however, ﬁnite probabilistic calibration and ﬁnite marginal calibration are equivalent, and a weak form of the sharpness principle holds.
Theorem 1 Suppose that G1, . . . , GT and F1 = · · · = FT = F have second moments and satisfy
the ﬁnite probabilistic calibration condition (5). Then
var(Ft) = var(F) ≥1
with equality if and only if E(G1) = · · · = E(GT ).
The proof of Theorem 1 is given in the appendix. We are unaware of any other results in this
direction; in particular, we do not know whether a non-climatological forecaster can be probabilistically calibrated and marginally calibrated yet sharper than the ideal forecaster.
Diagnostic tools
We now discuss diagnostic tools for the evaluation of predictive performance. In accordance with
Dawid’s prequential principle, the assessment of probabilistic forecasts needs to be based on
the predictive distributions and the observations only. Previously, we deﬁned notions of calibration
in terms of the asymptotic consistency between the probabilistic forecasts and the data generating
distributions, which are unavailable in practice. Hence, we turn to sample versions, by substituting
empirical distribution functions based on the outcomes, resulting in methodological tools that stand
in their own right as well. In the following, this program is carried out for probabilistic and marginal
calibration. Exceedance calibration does not allow for an obvious sample analogue, and it is not
clear whether such exists. We discuss graphical displays of sharpness and propose the use of proper
scoring rules, that assign numerical measures of predictive performance and ﬁnd key applications
in the ranking of competing forecast procedures.
Assessing probabilistic calibration
The probability integral transform (PIT) is the value that the predictive CDF attains at the observation.
Speciﬁcally, if Ft is the predictive distribution and xt materializes, the transform is
deﬁned as pt = Ft(xt). The literature usually refers to Rosenblatt , although the probability
integral transform can be traced back at least to Pearson . The connection to probabilistic
calibration is established by substituting the empirical distribution function 1{xt ≤x} for the data
generating distribution Gt(x), x ∈R in the probabilistic calibration condition (3), and noting that
(p) if and only if pt ≤p. The following theorem characterizes the asymptotic uniformity
of the empirical sequence of PIT values in terms of probabilistic calibration. We state this result
under the assumption of a ∗-mixing sequence of observations .
The proof is
deferred to the appendix.
Theorem 2 Let (Ft)t=1,2,... and (Gt)t=1,2,... be sequences of continuous, strictly increasing distribution functions. Suppose that xt has distribution Gt and that the xt form a ∗-mixing sequence of
random variables. Then
1{pt < p} −→p
almost surely
if and only if (Ft)t=1,2,... is probabilistically calibrated with respect to (Gt)t=1,2,....
We emphasize that condition (6) stands in its own right as a criterion for the validity of probabilistic forecasts, independently of our theoretical framework. Indeed, following the lead of Dawid
 and Diebold et al. , checks for the uniformity of the PIT values have formed a cornerstone of forecast evaluation.
Uniformity is usually assessed in an exploratory sense, and one way of doing this is by plotting
the empirical CDF of the PIT values and comparing to the identity function. This approach is
adequate for small sample sizes and notable departures from uniformity, and its proponents include
Sta¨el von Holstein , Seillier-Moiseiwitsch , Hoeting , Fr¨uhwirth-
Schnatter , Raftery, Madigan and Hoeting , Clements and Smith , Moyeed and
Papritz , Wallis and Boero and Marrocu . Histograms of the PIT values accentuate departures from uniformity when the sample size is large and the deviations from uniformity
are small. This alternative type of display has been used by Diebold et al. , Weigend and
Shi , Bauwens et al. and Gneiting et al. , among others, and 10 or 20 histogram
bins generally seem adequate. Figure 1 employs 20 bins and shows the PIT histograms for the
various forecasters in our initial simulation study. The histograms are essentially uniform. Table
3 shows the empirical coverage of the associated central 50% and 90% prediction intervals. This
information is redundant, since the empirical coverage can be read oﬀthe PIT histogram, as the
area under the 10 and 18 central bins, respectively.
Probabilistic weather forecasts are typically based on ensemble prediction systems, which generate a set of perturbations of the best estimate of the current state of the atmosphere, run each
of them forward in time using a numerical weather prediction model, and use the resulting set of
forecasts as a sample from the predictive distribution of future weather quantities . The principal device for assessing the calibration of ensemble forecasts is the veriﬁcation rank histogram or Talagrand diagram, proposed independently by Anderson
 , Hamill and Colucci and Talagrand et al. , and extensively used since. To obtain a veriﬁcation rank histogram, ﬁnd the rank of the observation when pooled within the ordered
Table 3: Empirical coverage of central prediction intervals. The nominal coverage is 50% and 90%,
respectively.
Perfect forecaster
Climatological forecaster
Unfocused forecaster
Hamill’s forecaster
ensemble values and plot the histogram of the ranks. If we identify the predictive distribution with
the empirical CDF of the ensemble values, this technique is seen to be equivalent to plotting a PIT
histogram. A similar procedure could be drawn on fruitfully to assess samples from posterior predictive distributions obtained by Markov chain Monte Carlo techniques. Shephard 
gave an instructive example of how this could be done.
Visual inspection of a PIT or rank histogram can provide hints to the reasons for forecast de-
ﬁciency. Hump shaped histograms indicate overdispersed predictive distributions with prediction
intervals that are too wide on average. U-shaped histograms often correspond to predictive distributions that are too narrow. Triangle-shaped histograms are seen when the predictive distributions
are biased. Formal tests of uniformity can be employed and have been studied by Anderson ,
Talagrand et al. , Noceti et al. , Garratt et al. , Wallis and Candille and
Talagrand , among others. However, the use of formal tests is often hindered by complex dependence structures, particularly in cases in which the PIT values are spatially aggregated. Hamill
 gave a thoughtful discussion of the associated issues and potential fallacies.
In the time series context, the observations are sequential, and the predictive distributions
correspond to sequential k-step ahead forecasts. The probability integral transforms for ideal kstep ahead forecasts are at most (k−1)-dependent, and this assumption can be checked empirically,
by plotting the sample autocorrelation function for the PIT values and their moments . Smith , Fr¨uhwirth-Schnatter and Berkowitz proposed an assessment
of independence based on the transformed PIT values, Φ−1(pt), which are Gaussian under the
assumption of ideal forecasts. This further transformation has obvious advantages when formal
tests of independence are employed, and seems to make little diﬀerence otherwise.
Assessing marginal calibration
Marginal calibration concerns the equality of forecast climate and actual climate. To assess marginal
calibration, we propose a comparison of the average predictive CDF,
¯FT (x) = 1
to the empirical CDF of the observations,
ˆGT (x) = 1
Indeed, if we substitute the indicator function 1{xt ≤x} for the data generating distribution Gt(x),
x ∈R, in the deﬁnition of marginal calibration, we are led to the asymptotic equality of ¯FT and ˆGT .
Threshold Value
F.fcast − F.obs
Cumulative Probability
Q.fcast − Q.obs
Figure 2: Marginal calibration plot for the ideal forecaster (solid line), climatological forecaster
(short dashes), unfocused forecaster (dot-dashed line) and Hamill’s forecaster (long dashes). The
presentation is in terms of CDFs (left) and in terms of quantiles (right), respectively.
Theorem 3 provides a rigorous version of this correspondence. Under mild regularity conditions,
marginal calibration is a necessary and suﬃcient condition for the asymptotic equality of ˆGT and
¯FT . The proof of this result is deferred to the appendix.
Theorem 3 Let (Ft)t=1,2,... and (Gt)t=1,2,... be sequences of continuous, strictly increasing distribution functions. Suppose that each xt has distribution Gt and that the xt form a ∗-mixing
sequence of random variables. Suppose furthermore that ¯F(x) = limT →∞1
t=1 Ft(x) exists for
all x ∈R and that the limit function is strictly increasing on R. Then
ˆGT (x) = 1
1{xt ≤x} −→¯F(x)
almost surely
for all x ∈R
if and only if (Ft)t=1,2,... is marginally calibrated with respect to (Gt)t=1,2,....
We note that (9) stands in its own right as a criterion for the validity of probabilistic forecasts.
Still, Theorems 2 and 3 provide reassurance, in that (6) and (9) will be satisﬁed almost surely if
the forecaster issues the same sequence of distributions that nature uses to generate the outcomes,
assuming mixing conditions. These results are also of interest because they characterize situations
under which (6) and (9) lead us to accept as valid forecasts that might in fact be far from ideal.
The most obvious graphical device in the assessment of marginal calibration is a plot of ˆGT (x)
and ¯FT (x) versus x. However, it is often more instructive to plot the diﬀerence of the two CDFs,
as on the left-hand side of Figure 2, which shows the diﬀerence
¯FT (x) −ˆGT (x),
Table 4: Average width of central prediction intervals. The nominal coverage is 50% and 90%,
respectively.
Ideal forecaster
Climatological forecaster
Unfocused forecaster
Hamill’s forecaster
for the various forecasters in our initial simulation study. We call this type of display a marginal
calibration plot. Under the hypothesis of marginal calibration, we expect minor ﬂuctuations about
zero only, and this is indeed the case for the ideal forecaster and the climatological forecaster. The
unfocused forecaster and Hamill’s forecaster lack marginal calibration, resulting in major excursions
from zero. The same information can be visualized in terms of quantiles, as on the right-hand side
of Figure 2, which shows the diﬀerence
Q( ¯FT , q) −Q( ˆGT , q),
q ∈(0, 1),
of the quantile functions for ¯FT and ˆGT , respectively. Under the hypothesis of marginal calibration,
we again expect minor ﬂuctuations about zero only, and this is the case for the ideal forecaster
and the climatological forecaster. The unfocused forecaster and Hamill’s forecaster show quantile
diﬀerence functions that increase from negative to positive values, indicating forecast climates that
are too spread out.
Assessing sharpness
Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts
only. The more concentrated the predictive distributions, the sharper the forecasts, and the sharper
the better, subject to calibration. To assess sharpness, we use numerical and graphical summaries of
the width of prediction intervals. For instance, Table 4 shows the average width of the central 50%
and 90% prediction intervals for the forecasters in our simulation study. The ideal forecaster is the
sharpest, followed by Hamill’s, the unfocused and the climatological forecaster. In our simplistic
simulation study, the width of the prediction intervals is ﬁxed, expect for Hamill’s forecaster, and
the tabulation is perfectly adequate. In real-world applications, conditional heteroscedasticity often
leads to considerable variability in the width of the prediction intervals. The average width then
is insuﬃcient to characterize sharpness, and we follow Bremnes in proposing boxplots as a
more instructive graphical device. We refer to this type of display as a sharpness diagram, and an
example thereof is shown in Figure 9 below.
Proper scoring rules
Scoring rules assign numerical scores to probabilistic forecasts and form attractive summary measures of predictive performance, in that they address calibration and sharpness simultaneously. We
write s(F, x) for the score assigned when the forecaster issues the predictive distribution F and x
materializes, and we take scores to be penalties that the forecaster wishes to minimize. A scoring
Table 5: Average logarithmic score (LogS) and continuous ranked probability score (CRPS).
Ideal forecaster
Climatological forecaster
Unfocused forecaster
Hamill’s forecaster
rule is proper if the expected value of the penalty s(F, x) for an observation x drawn from G is minimized if F = G. It is strictly proper if the minimum is unique. Winkler gave an interesting
discussion of the ways in which proper scoring rules encourage honest and sharp forecasts.
The logarithmic score is the negative of the logarithm of the predictive density evaluated at the
observation . This scoring rule is proper and has many desirable properties , but lacks robustness .
The continuous ranked probability score is deﬁned directly in terms of the predictive cumulative
distribution function, F, namely as
crps(F, x) =
(F(y) −1(y ≥x))2 dy,
and provides a more robust alternative. Gneiting and Raftery gave an alternative representation and showed that
crps(F, x) = EF |X −x| −1
where X and X′ are independent copies of a random variable with cumulative distribution function
F and ﬁnite ﬁrst moment. The representation (13) shows that the continuous ranked probability
score generalizes the absolute error, to which it reduces if F is a point forecast. It is reported in
the same unit as the observations. The continuous ranked probability score is proper, and we rank
competing forecast procedures based on its average,
crps(Ft, xt)=
where BS(y) =
t=1 (Ft(y) −1{xt ≤y})2 denotes the Brier score for probability
forecasts of the binary event at the threshold value y ∈R. Like all proper scoring rules for binary
probability forecasts, the Brier score allows for the distinction of a calibration component and
a reﬁnement component . Candille and
Talagrand discuss calibration-sharpness decompositions of the continuous ranked probability
Table 5 shows the logarithmic score and the continuous ranked probability score for the various
forecasters in our initial simulation study, averaged over the 10000 replicates of the prediction
experiment. As expected, both scoring rules rank the ideal forecaster highest, followed by Hamill’s,
the unfocused and the climatological forecaster. Figure 3 plots the Brier score for the associated
binary forecasts in dependence on the threshold value, illustrating the integral representation on
the right-hand side of (14). This type of display was proposed by Gerds and
Schumacher et al. who called the graphs prediction error curves.
Threshold Value
Brier score
Figure 3: Brier score plot for the ideal forecaster (solid line), climatological forecaster (short dashes),
unfocused forecaster (dot-dashed line), and Hamill’s forecaster (long dashes). The graphs show the
Brier score as a function of the threshold value. The area under each forecaster’s curve equals the
CRPS value (14).
Case study:
Probabilistic forecasts at the Stateline wind energy center
Wind power is the fastest-growing energy source today. Estimates are that within the next 15
years wind energy will ﬁll about 6% of the electricity supply in the United States. In Denmark,
wind energy already meets 20% of the country’s total energy needs. However, arguments against
the proliferation of wind energy have been put forth, often focusing on the perceived inability to
forecast wind resources with any degree of accuracy. The development of advanced probabilistic
forecast methodologies helps address these concerns.
The prevalent approach to short-range forecasts of wind speed and wind power at prediction
horizons up to a few hours is based on on-site observations and autoregressive time series models
 .
Gneiting et al. proposed a novel spatio-temporal approach, the regime-switching space-time (RST) method, that merges meteorological and statistical
expertise to obtain fully probabilistic forecasts of wind resources. Henceforth, we illustrate our
diagnostic approach to the evaluation of predictive performance by a comparison and ranking of
three competing methodologies for two-hour ahead forecasts of hourly average wind speed at the
Stateline wind energy center. The evaluation period is May through November 2003, resulting in
a total of 5136 probabilistic forecasts.
Persistence Forecast
Probability Integral Transform
Relative Frequency
ACF of PIT
ACF of (PIT−1/2)^2
ACF of (PIT−1/2)^3
Figure 4: Probability integral transform (PIT) histogram and sample autocorrelation functions for
the ﬁrst three centered moments of the PIT values, for persistence forecasts of hourly average wind
speed at the Stateline wind energy center.
Predictive distributions for hourly average wind speed
We consider three competing statistical prediction algorithms for two-hour ahead probabilistic
forecasts of hourly average wind speed, wt, at the Stateline wind energy center. Stateline is located
on the Vansycle ridge at the border between the states of Oregon and Washington in the U.S. Paciﬁc
Northwest. The data source is described in Gneiting et al. .
The ﬁrst method is the persistence forecast, a naive yet surprisingly skillful, nonparametric
reference forecast.
The persistence point forecast is simply the most recent observed value of
hourly average wind speed at Stateline. To obtain a predictive distribution, we dress the point
forecast with the 19 most recent observed values of the persistence error, similarly to the approach
proposed by Roulston and Smith . Speciﬁcally, the predictive CDF for wt+2 is the empirical
distribution function of the set
{max(wt −wt−h + wt−h−2, 0) : h = 0, . . . , 18}.
The second technique is the autoregressive time series approach, which was proposed by Brown
et al. and has found widespread use since. To apply this technique, we ﬁt and extract a
diurnal trend component based on a sliding 40-day training period, ﬁt a stationary autoregression
Autoregressive Forecast
Probability Integral Transform
Relative Frequency
ACF of PIT
ACF of (PIT−1/2)^2
ACF of (PIT−1/2)^3
Figure 5: Same as Figure 4, but for autoregressive forecasts.
to the residual component and ﬁnd a Gaussian predictive distribution in the customary way. The
predictive distribution assigns a typically small positive mass to the negative half-axis, and in view
of the nonnegativity of the predictand, we redistribute this mass to wind speed zero. The details
are described in Gneiting et al. , where the method is referred to as the AR-D technique.
The third method is the regime-switching space-time (RST) approach of Gneiting et al. .
The RST model is parsimonious, yet takes account of all the salient features of wind speed: alternating atmospheric regimes, temporal and spatial autocorrelation, diurnal and seasonal nonstationarity, conditional heteroscedasticity and non-Gaussianity. The method uses oﬀsite information from the nearby meteorological towers at Goodnoe Hills and Kennewick, identiﬁes atmospheric
regimes and ﬁts conditional predictive models for each regime, based on a sliding 45-day training
period. Details can be found in Gneiting et al. , where the method is referred to as the
RST-D-CH technique. Any minor discrepancies in the results reported below and in Gneiting et
al. stem from the use of R versus Splus and diﬀerences in optimization routines.
Assessing calibration
Figures 4, 5 and 6 show the probability integral transform (PIT) histograms for the three forecast
techniques, along with the sample autocorrelation functions for the ﬁrst three centered moments
of the PIT values and the respective Bartlett conﬁdence intervals. The PIT histograms for the
RST Forecast
Probability Integral Transform
Relative Frequency
ACF of PIT
ACF of (PIT−1/2)^2
ACF of (PIT−1/2)^3
Figure 6: Same as Figure 4, but for RST forecasts.
persistence and RST forecasts appear uniform. The histogram for the autoregressive forecasts is
hump shaped, thereby suggesting departures from probabilistic calibration.
Table 6 shows the
empirical coverage of central prediction intervals.
The PIT values for ideal 2-step ahead forecasts are at most 1-dependent, and the sample autocorrelation functions for the RST forecasts seem compatible with this assumption. The sample
autocorrelations for the persistence forecasts were nonnegligible at lag 2, and the centered second
moment showed notable negative correlations at lags between 15 and 20 hours. These features
indicate a lack of ﬁt of the predictive model, even though they seem hard to interpret diagnostically. The respective sample autocorrelations for the autoregressive forecast were positive and
nonnegligible at lags up to ﬁve hours, suggesting conditional heteroscedasticity in the wind speed
series. Indeed, Gneiting et al. showed that the autoregressive forecasts improve when a conditionally heteroscedastic model is employed. In the current classical autoregressive formulation
the predictive variance varies as a result of the sliding training period, but high-frequency changes
in predictability are not taken into account.
Figure 7 shows marginal calibration plots for the three forecasts, both in terms of CDFs and
in terms of quantiles. The graphs show the diﬀerentials (10) and (11) and point at nonnegligible
excursions from zero, particularly at small wind speeds and for the autoregressive forecast. The
lack of predictive model ﬁnds an explanation in Figure 8, which shows the empirical cumulative
distribution function, ¯FT , of hourly average wind speed. Hourly average wind speeds less than 1
Wind Speed
F.fcast − F.obs
Cumulative Probability
Q.fcast − Q.obs
Figure 7: Marginal calibration plot for persistence forecasts (dashed line), autoregressive forecasts
(dot-dashed line) and RST forecasts (solid line) of hourly average wind speed at the Stateline wind
energy center in terms of CDFs (left) and in terms of quantiles (right), in m · s−1.
Wind Speed
Empirical CDF
Figure 8: Empirical CDF of hourly average wind speed at the Stateline wind energy center in May
through November 2003, in m · s−1.
Table 6: Empirical coverage of central prediction intervals. The nominal coverage is 50% and 90%,
respectively.
Persistence forecast
Autoregressive forecast
RST forecast
Table 7: Average width of central prediction intervals, in m·s−1. The nominal coverage is 50% and
90%, respectively.
Persistence forecast
Autoregressive forecast
RST forecast
m · s−1 were almost never observed, even though the predictive distributions assign positive point
mass to wind speed zero.
Assessing sharpness
The sharpness diagram in Figure 9 shows boxplots that illustrate the width of central prediction
intervals for the 5136 predictive distributions in the evaluation period. The prediction intervals
for the persistence forecast varied the most in width, followed by the RST and autoregressive
forecasts. Table 7 shows the respective average widths. The RST method was by far the sharpest,
with prediction intervals that were about 20% shorter on average than those for the autoregressive
technique.
Continuous ranked probability score
Table 8 shows the CRPS value (14) for the various techniques. We report the scores month by
month, which allows for an assessment of seasonal eﬀects and straightforward tests of the null
hypothesis of no diﬀerence in predictive performance. For instance, the RST method showed lower
CRPS than the autoregressive technique in each month during the evaluation period, May through
November 2003.
Under the null hypothesis of equal predictive performance this happens with
probability gave a thoughtful discussion of these issues, and we refer to their work for a comprehensive
account of tests of predictive performance. Figure 10 illustrates the Brier score decomposition (14)
of the CRPS value for the entire evaluation period. The RST method outperformed the other
techniques at all thresholds.
Figure 9: Sharpness diagram for persistence (PS), autoregressive (AR) and RST forecasts of hourly
average wind speed at the Stateline wind energy center. The boxplots show the 5th, 25th, 50th,
75th and 95th percentile of the width of the central prediction interval, in m · s−1. The smaller the
width, the sharper. The nominal coverage is 50% (left) and 90% (right), respectively.
We noted in Section 3.4 that the continuous ranked probability score generalizes the absolute
error, and reduces to the latter for point forecasts. Table 9 shows the respective mean absolute error
(MAE) for the persistence, autoregressive and RST point forecasts. The persistence point forecast is
the most recent observed value of hourly average wind speed at Vansycle. The autoregressive point
forecast is the mean of the respective predictive distribution, and similarly for the RST forecast.
The RST method had the lowest MAE, followed by the autoregressive and persistence techniques.
The MAE and CRPS values are reported in the same unit as the wind speed observations, that
is, in m · s−1, and can be directly compared. The insights that the monthly scores provide are
indicative of the potential beneﬁts of thoughtful stratiﬁcation.
The CRPS and MAE values establish a clear-cut ranking of the forecast methodologies that
places the RST method ﬁrst, followed by the autoregressive and persistence techniques. The RST
method also performed best in terms of probabilistic and marginal calibration, and the RST forecasts were by far the sharpest. The diagnostic approach points at forecast deﬁciencies and suggests
potential improvements to the predictive models. In particular, the marginal calibration plots in
Figure 7 suggest a modiﬁed version of the RST technique that uses truncated normal rather than
Table 8: CRPS value (14) for probabilistic forecasts of hourly average wind speed at the Stateline
wind energy center in March through November 2003, month by month and for the entire evaluation
period, in m · s−1.
Persistence forecast
Autoregressive forecast
RST forecast
Table 9: Mean absolute error (MAE) for point forecasts of hourly average wind speed at the
Stateline wind energy center in March through November 2003, month by month and for the entire
evaluation period, in m · s−1.
Persistence forecast
Autoregressive forecast
RST forecast
cut-oﬀnormal predictive distributions. This modiﬁcation yields small but consistent improvements
in predictive performance .
Discussion
Our paper addressed the important issue of evaluating predictive performance for probabilistic
forecasts of continuous variables. Following the lead of Dawid and Diebold et al. ,
predictive distributions have traditionally been evaluated by checking the uniformity of the probability integral transform (PIT). We introduced the pragmatic and ﬂexible paradigm of maximizing
the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the predictive distributions and the associated observations and is a joint
property of the predictions and the values that materialize. Sharpness refers to the concentration
of the predictive distributions and is a property of the forecasts only.
We interpreted probabilistic forecasting within a simple theoretical framework that allowed us
to distinguish probabilistic, exceedance and marginal calibration, and that lends support to the use
of diagnostic tools in evaluating and comparing probabilistic forecasters. Probabilistic calibration
corresponds to the uniformity of the PIT values, and the PIT histogram remains a key tool in the
diagnostic approach to forecast evaluation. In addition, we proposed the use of marginal calibration
plots, sharpness diagrams and proper scoring rules, which form powerful tools for learning about
forecast deﬁciencies and ranking competing forecast methodologies.
Our own applied work on
probabilistic forecasting has beneﬁtted immensely from these tools, as documented in Section 4
and in the partial applications in Gneiting et al. , Raftery et al. and Gneiting et
al. . Predictive distributions can be reduced to point forecasts, or to probability forecasts of
binary events, and the associated forecasts can be assessed using the diagnostic devices described
by Murphy, Brown and Chen and Murphy and Winkler , among others.
Wind Speed
Brier score
Figure 10: Brier score plot for persistence forecasts (dashed line), autoregressive forecasts (dotdashed line) and RST forecasts (solid line) of hourly average wind speed at the Stateline wind
energy center, in m·s−1. The graphs show the Brier score as a function of the threshold value. The
area under each forecast’s curve equals the CRPS value (14).
If we were to reduce our conclusions to a single recommendation, we would close with a call for
the assessment of sharpness, particularly when the goal is that of ranking. Previous comparative
studies of the predictive performance of probabilistic forecasts have focused on calibration. For
instance, Moyeed and Papritz compared spatial prediction techniques, Clements and Smith
 and Boero and Marrocu evaluated linear and non-linear time series models, Garrat et
al. assessed macroeconomic forecast models, and Bauwens et al. studied the predictive
performance of ﬁnancial duration models. In each of these works, the assessment was best on the
predictive performance of the associated point forecasts, and on the uniformity of the PIT values.
We contend that comparative studies of these types call for routine assessments of sharpness, in
the form of sharpness diagrams and through the use of proper scoring rules.
Despite the frequentist ﬂavor of our diagnostic approach, calibration and sharpness are properties that are relevant to Bayesian forecasters as well. Rubin argued
that “the probabilities attached to Bayesian statements do have frequency interpretations that
tie the statements to veriﬁable real world events.” Consequently, a “Bayesian is calibrated if his
probability statements have their asserted coverage in repeated experience.” Gelman et al. 
developed Rubin’s posterior predictive approach, proposed posterior predictive checks as Bayesian
counterparts to the classical tests for goodness of ﬁt, and advocated their use in judging the ﬁt of
Bayesian models. This relates to our diagnostic approach, which emphasizes the need for understanding the ways in which predictive distributions fail or succeed. Indeed, the diagnostic devices
posited herein form powerful tools for Bayesian as well as frequentist model diagnostics and model
choice. Tools such as the PIT histogram, marginal calibration plots, sharpness diagrams and proper
scoring rules are widely applicable, since they are nonparametric, do not depend on nested models,
allow for structural change, and apply to predictive distributions that are represented by samples,
as they arise in a rapidly growing number of Markov chain Monte Carlo methodologies and ensemble prediction systems. In the time series context, the predictive framework is natural and model
ﬁt can be assessed through the performance of the time-forward predictive distributions .
In other types of situations, cross-validatory
approaches often apply fruitfully .
Proof of Theorem 1
Consider the random variable U = F(x1)z1F(x2)z2 · · · F(xT )zT where x1 ∼G1, . . . , xT ∼GT and
(z1, . . . , zT )′ is multinomial with equal probabilities. The ﬁnite probabilistic calibration condition
implies that U is uniformly distributed. By the variance decomposition formula,
var(F) = var(F −1(U)) = E
F −1(U)|z1, . . . , zT
F −1(U)|z1, . . . , zT
The ﬁrst term in the decomposition equals
var(xt) = 1
and the second term is nonnegative and vanishes if and only if E(G1) = · · · = E(GT ).
Proof of Theorem 2
For p ∈(0, 1) and t = 1, 2, . . ., put Yt = 1{pt < p} −Gt ◦F −1
(p) and note that E(Yt) = 0. By
Theorem 2 of Blum et al. ,
1{pt < p} −Gt ◦F −1
almost surely. The uniqueness of the limit implies that (6) is equivalent to the probabilistic calibration condition (3).
Proof of Theorem 3
For x ∈R let q = ¯F(x), and for t = 1, 2, . . . put qt = ¯F(xt). Then
ˆGT (x) = 1
1{xt ≤x} = 1
By Theorem 2 with Ft = ¯F for t = 1, 2, . . ., we have that
t=1 1{qt ≤q} →q almost surely if
and and only if 1
t=1 Gt ◦¯F −1(q) →q almost surely. Hence, marginal calibration is equivalent
Acknowledgements
We thank John B. Bremnes, Barbara G. Brown, Richard E. Chandler, Theo Eicher, Hans R. K¨unsch,
Kristin Larson, Jon A. Wellner, Anton H. Westveld and Kenneth Westrick for discussions, comments and references, and we are grateful to an anonymous reviewer for a wealth of helpful suggestions. Our work has been supported by the DoD Multidisciplinary University Research Initiative
(MURI) program administered by the Oﬃce of Naval Research under Grant N00014-01-10745.
Tilmann Gneiting furthermore acknowledges support by the National Science Foundation under
Award no. 0134264 and by the Washington Technology Center, and thanks the Soil Physics group
at Universit¨at Bayreuth, Germany, where part of this work was performed.