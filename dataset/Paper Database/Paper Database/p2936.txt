Interleaved Text/Image Deep Mining
on a Large-Scale Radiology Database
Hoo-Chang Shin
Lauren Kim
Jianhua Yao
Ronald M. Summers
Imaging Biomarkers and Computer-Aided Diagnosis Laboratory
Radiology and Imaging Sciences
National Institutes of Health Clinical Center
Bethesda, MD 20892-1182
{hoochang.shin, le.lu, lauren.kim2, ari.seff, rms}@nih.gov, 
Despite tremendous progress in computer vision, effective learning on very large-scale (> 100K patients) medical image databases has been vastly hindered. We present
an interleaved text/image deep learning system to extract
and mine the semantic interactions of radiology images and
reports from a national research hospital’s picture archiving and communication system. Instead of using full 3D
medical volumes, we focus on a collection of representative ~216K 2D key images/slices (selected by clinicians for
diagnostic reference) with text-driven scalar and vector labels. Our system interleaves between unsupervised learning (e.g., latent Dirichlet allocation, recurrent neural net
language models) on document- and sentence-level texts to
generate semantic labels and supervised learning via deep
convolutional neural networks (CNNs) to map from images
to label spaces. Disease-related key words can be predicted
for radiology images in a retrieval manner. We have demonstrated promising quantitative and qualitative results. The
large-scale datasets of extracted key images and their categorization, embedded vector labels and sentence descriptions can be harnessed to alleviate the deep learning “datahungry” obstacle in the medical domain.
1. Introduction
The ImageNet Large Scale Visual Recognition Challenge provides more than one million labeled images
from 1,000 object categories. The accessibility of a huge
amount of well-annotated image data in computer vision
rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object
class recognition tasks. Deep CNNs can perform signiﬁcantly better than traditional shallow learning methods, but
total number of
# words in documents
# image modalities
# documents
∼1 billion
# vocabulary
Table 1. Some statistics of the dataset.
“Others” include CR
(Computed Radiography), RF (Radio Fluoroscopy), and US (Ultrasound).
unremarkable
impression
Table 2. Examples of the most frequently occurring words in the
radiology report documents.
usually requires much more training data . In the
medical domain, however, there are no similar large-scale
labeled image datasets available. On the other hand, gigantic collections of radiology images and reports are stored
in many modern hospitals’ picture archiving and communication system (PACS). The invaluable semantic diagnostic knowledge inhabiting the mapping between hundreds of
thousands of clinician-created high quality text reports and
linked image volumes remains largely unexplored. One of
our primary goals is to extract and associate radiology images with clinically semantic scalar and vector labels via
interleaved text/image data mining and deep learning on a
large-scale PACS database (~780K imaging examinations).
To the best of our knowledge, this is the ﬁrst reported work
of “Deep Mining into PACS” at a very large scale.
Building the ImageNet database was mainly a manual
process : harvesting images returned from Google image search engine (according to the WordNet ontology hi-
erarchy) and pruning falsely tagged images using crowdsourcing such as Amazon Mechanical Turk (AMT). This
does not meet our data collection and labeling needs due to
the demanding difﬁculties of medical annotation tasks and
the data privacy reasons. Thus we propose to mine image
categorization labels from hierarchical, Bayesian document
clustering method, e.g., generative latent Dirichlet allocation (LDA) topic modeling , using all available radiology text reports in PACS. The Radiology reports are text
documents describing patient history, symptoms, image observations and impressions written by board-certiﬁed radiologists. However, the reports do not contain speciﬁc image labels to be trained my a machine learning algorithm.
We ﬁnd that LDA-generated image categorization labels
are valid, demonstrating good semantic coherence among
clinician observers , and can be effectively learned
using deep CNNs with image inputs alone . Our
deep CNN models on medical image modalities (mostly CT,
MRI) are initialized with the model parameters pre-trained
from ImageNet using Caffe framework. Kulkarni et
al. have spearheaded the efforts of learning the semantic connections between image contents and the sentences
describing them (i.e., captions). Detecting objects of interest, attributes and prepositions and applying contextual regularization with a conditional random ﬁeld (CRF) is a feasible approach because many useful tools are available in
computer vision. There has not yet been much comparable
development on large-scale medical imaging understanding.
Our work has been inspired by the works building very
large-scale image databases and the works establishing semantic connections of texts and images . We
observe good semantic coherence between labels obtained
by hierarchical document topic models and clinician’s
assessment.
Based on this, both unsupervised (recurrent
neural net language models ) and supervised deep
CNNs with categorization and regression losses are used
for annotating large collection of radiology images. The
fact that deep learning requires no hand-crafted image features is very desirable since signiﬁcant adaption would be
needed to apply conventional image features, e.g., HOG,
SIFT for medical image learning. The large-scale datasets
of extracted key images and their categorization, vector labels, describing sentences can be harnessed to alleviate deep
learning’s “data-hungry” challenge in the medical domain1.
1.1. Related Work
The ImageCLEF medical image annotation tasks of
2005-2007 have 9,000 training and 1,000 testing 2D images
(converted as 32×32 pixel thumbnails in ) with 57 labels.
Local image descriptors and intensity histograms are used
1We are currently working on the institutional review board approval to share our extracted data (not original full radiology reports).
We make our code and trained deep text/image models available in
 
in a bag-of-features approach in that work for this scene
recognition-like problem. Unsupervised latent Dirichlet allocation based matching from lung disease words (e.g., ﬁbrosis, emphysema) in radiology reports to 2D image blocks
from axial CT chest scans (of 24 patients) is studied in .
This work is motivated by generative models of combining
words and images under a very limited word/image
vocabulary.
The most related works are which ﬁrst map
words into vector space using recurrent neural networks and
then project images into the label-associated word-vector
embeddings by minimizing the L2 ( ) or hinge rank
losses ( ) between the visual and label manifolds. The
language model is trained on the texts of Wikipedia and
tested on label-associated images from the CIFAR 
and ImageNet datsets. In comparison, our work is
on a large, unlabeled medical dataset of associated images
and text, where the text-derived labels are computed and
veriﬁed with human intervention. Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K
 , Flickr8K , Flickr30K ) in , where such
caption datasets are not available in the medical domain.
Graphical models have been employed to predict image attributes ( ), or to describe images ( ) using manually annotated datasets ( ). Automatic label mining
on large, unlabeled datasets is presented in , however the variety of the label-space is limited (image text annotations). We analyze/mine the medical image semantics
on both document and sentence levels, and deep CNNs are
adapted to learn them from image contents .
To gain the most comprehensive understanding of diagnostic semantics, we use all available radiology reports of
around ~780K imaging examinations, stored in the PACS
of National Institutes of Health Clinical Center since the
year 2000. Around 216K key 2D image slices (instead of
all 3D image volumes) are studied here. Within 3D patient scans, most of the imaging information represented is
normal anatomy, i.e, not the focus of the radiology reports.
These “key images” were referenced (see Figure 1) by radiologists manually during radiology report writing, to provide a visual reference to pathologies or other notable ﬁndings. Therefore 2D key images are more correlated with the
diagnostic semantics in the reports than the whole 3D scans,
but not all reports have referenced key images (215, 786 images from 61, 845 unique patients). Table 1 provides extracted database statistics, and Table 2 shows examples of
the most frequently occurring words in radiology reports.
Leveraging our deep learning models exploited in this paper will make it possible to automatically select key images
from 3D patient scans to avoid mis-referencing.
0001#REPORT#:#REASON#FOR#EXAM#(Entered#by#ordering#clinician#into#CRIS):#hx#of#head#and#neck#
cancer.#needs#scan#CT#of#the#nasopharynx.#
HISTORY:#Head#and#neck#cancer.#
TECHNIQUE:# ConLguous# 2.5# mm# axial# images# of# the# nasopharynx# were# performed# without# IV#
contrast.#COMPARISON:#xx/xx/xxxx.#
FINDINGS:# No# soV# Lssue# masses# are# seen# within# the# soV# Lssues# of# the# neck.# The# paroLd# and#
submandibular# glands# are# predominantly# faWyXreplaced.# SoV# Lssues# of# the# Naso,# oropharynx# are#
unremarkable.#There#may#be#mild#fat#stranding#of#the#right#parapharyngeal#soV#Lssues#(series#1001,#
image#32).#No#abnormal#masses#are#seen#at#that#site.#No#bulky#lymphadenopathy#is#seen.#There#is#a#
fusiform#aneurysm#of#the#basilar#artery#as#previously#described.#It#appears#to#the#mildly#increased#in#
size# and# currently# measures# 2.0# cm# in# transverse# dimensions# and# previously# measured# 1.8# cm.# It#
measured#1.5#cm#in#transverse#dimensions#on#xx/xx/xxxx.#AtheroscleroLc#calciﬁcaLons#are#also#seen#
within#the#caroLd#arteries#bilaterally.#There#is#nearXcomplete#opaciﬁcaLon#of#the#maxillary#sinuses#
bilaterally.# This# has# increased# predominantly# within# the# leV# maxillary# sinus# and# mildly# within# the#
right# maxillary# sinus.# The# ethmoidal# air# cells# are# clear.# Sphenoidal# and# frontal# sinuses# are# clear.#
DegeneraLve#changes#of#the#cervical#spine#are#noted.##
IMPRESSIONS:#1.#No#soV#Lssue#masses#however,#mild#right#parapharyngeal#fat#stranding#is#seen#it#
may# be# postoperaLve# or# post# radiaLon# in# nature.# 2.# Basilar# artery# aneurysm# that# has# gradually#
increased#in#size#when#compared#to#prior#examinaLons.#3.#AtheroscleroLc#disease#of#the#coronary#
arteries#bilaterally..##
0001#Report:#CHEST,#ABDOMEN,#PELVIS#CT:#MulLdetector#helical#(5#mm,#quad)#images#following,#and#
abdomen# images# prior# to# vascular# contrast# infusion# (45# s# delay,# 2# cc/s,# 130# cc# Isovue)# obtained#
without#apparent#complicaLon.#History:#renal#cell#pt#on#Medarex#protocol#here#for#end#of#course#
evaluaLon.#
CHEST:#MulLple#right,#and#at#least#one#leV#lung#masses#minimallyXmoderately#increasing#since#xx/xx/
xxxx,#compaLble#with#metastases#despite#moderate#decrease#in#at#least#one#right#midXlung#mass#(e.g.#
series#4#image#30).#Minimal#pretracheal#and#subcarinal#adenopathy#increasing.#Spine#osteophytes.#
Enlargement#thyroid#on#right#side,#and#thyroid#heterogeneity#unchanged,#possibly#due#to#goiter.#No#
evidence#of#pleural#or#pericardial#eﬀusion,#axilla#or#leV#hilum#adenopathy.#
ABDOMEN,#PELVIS:#Few#right#and#leV#liver#foci,#leV#periaorLc#and#leV#adrenal#fossa,#and#right#sacrum#
mass# and# lyLc# lesion# (series# 3# image# 88X95)# increasing# minimally,# compaLble# with# metastases.#
ScaWered# lumbar# vertebra# and# bilateral# ilium# foci# (e.g.# series# 3# image# 55,# 60,# 80,# 84X7,# 96)# foci#
possibly#due#to#bone#metastases.#Uterine#fundus#focus#(series#3#image#95)#increasing#in#density#since#
xx/xx/xxxx,# possibly# due# to# ﬁbroid,# metastasis.# No# evidence# of# splenomegaly,# hydronephrosis,#
gallbladder#calciﬁcaLon,#or#bulky#mesenteric#adenopathy.#
Figure 1. Two examples of radiology reports and the referenced
“key images” (providing a visual reference to pathologies or other
notable ﬁndings).
Finding and extracting key images from radiology reports is done by natural language processing (NLP), i.e,
ﬁnding a sentence mentioning a referenced image. For example, “There may be mild fat stranding of the right parapharyngeal soft tissues (series 1001, image 32)” is listed
in Figure 1. The NLP steps are sentence tokenization,
word/number matching and stemming, and rule-based information extraction (e.g., translating “image 1013-78” to
“images 1013-1078”).
A total of ~187K images can be
retrieved and matched in this manner, whereas the rest of
~28K key images are extracted according to their reference
accession numbers in PACS. Our report-extracted key image database is the largest one ever reported and is highly
representative of the huge collection of radiology diagnostic semantics over the last decade. Exploring effective deep
learning models on this database opens new ways to parse
and understand large-scale radiology image informatics.
3. Document Topic Learning with Latent
Dirichlet Allocation
We propose to mine image categorization labels using
unsupervised document topic-modeling algorithm, e.g. latent Dirichlet allocation (LDA) , on the ~780K radiology
text reports in PACS. Unlike images from ImageNet 
which often have a dominant object appearing in the center,
our key images are CT/MRI slices showing several coexisting organs/pathologies. There are high amounts of intrinsic
ambiguity in deﬁning and assigning a semantic label set to
images, even for experienced clinicians. Our hypothesis is
that the large collection of sub-million radiology reports statistically deﬁnes the categories meaningful for topic-mining
(LDA) and visual learning (deep CNN).
LDA was originally proposed to ﬁnd latent topic
models for a collection of text documents (e.g., newspapers).
There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic
Analysis (pLSA) and Non-negative Matrix Factorization (NMF) . We choose LDA for extracting latent topic
labels among radiology report documents because LDA is
shown to be more ﬂexible yet learns more coherent topics
over large sets of documents . Furthermore, pLSA can
be regarded as a special case of LDA and NMF as a
semi-equivalent model of pLSA .
LDA offers a hierarchy of extracted topics and the number of topics can be chosen by evaluating each model’s perplexity score (Equation 1), which is a common way to measure how well a probabilistic model generalizes by evaluating the log-likelihood of the model on a held-out test set.
For an unseen document set Dtest, the perplexity score is
deﬁned as in Equation 1, where M is the number of documents in the test set (unseen hold-out set of documents),
wd the words in the unseen document d, Nd the number of
words in document d, with Φ the topic matrix, and α the
hyperparameter for topic distribution of the documents.
perplexity(Dtest) = exp
d=1 log p(wd|Φ, α)
A lower perplexity score generally implies a better ﬁt of the
model for a given document set .
Based on the perplexity score evaluated on 80% of the
total documents used for training and 20% used for testing,
the number of topics chosen is 80 for the document-level
model using perplexity scores for model selection (Figure
2). Although the document distribution in the topic space is
approximately balanced2, the distribution of image counts
for the topics is unbalanced. Speciﬁcally, topic #77 (nonprimary metastasis spreading across a variety of body parts)
contains nearly half of the ~216K key images. To address
the data bias, a second-hierarchy topics are obtained for
each of the ﬁrst document-level topics, resulting in 800 topics, where the number of second-hierarchy topics is also
chosen based on the average perplexity scores evaluated on
each document-level topic. Lastly, to compare the method
of using the whole report with using only the sentence directly describing the key images for latent topic mining,
a sentence-level (third-hierarchy) LDA topics are obtained
based on three sentences only: the sentence mentioning the
key-image (Figure 1) and its adjacent sentences as proximal context. The perplexity scores keep decreasing with an
increasing number of topics; we choose the topic count to
be 1000 as the rate of the perplexity score decrease is very
small beyond that point (Figure 2).
2Please refer to the supplementary material for the distribution of documents and images for LDA topics.
perplexity scores for sentence-level topic model
perplexity
perplexity scores for document-level topic model
perplexity
Figure 2. Perplexity scores for document-/sentence- level topic
models. Number of topics with low perplexity score is selected
as the optimal (80 for document-level, 1000 for sentence-level).
We observe that LDA-generated image categorization
labels are valid, demonstrating good semantic coherence
among clinician observers . The lists of key words
and sampled images per topic label are subjected to boardcertiﬁed radiologist’s review and validation. Some examples of document-level topics with their corresponding images and topic key words are shown in Figure 3. Based on
radiologists’ review, our LDA topics discover semantics at
different levels: 73 low-level concepts (e.g., pathology examination of certain body regions and organs: topic #47
- sinus diseases; #2 - lesions of solid abdominal organs,
primarily kidney; #10 - pulmonary diseases; #13 - brain
MRI; #19 - renal diseases on mixed imaging modalities;
#36 - brain tumors). There are 7 mid- to high-level concepts (e.g., topic #77 - non-primary metastasis spreading
across a variety of body parts; topic #79 - cases with high
diagnosis uncertainty/equivocation; #72 - indeterminate lesions; #74 - instrumentation artifacts limiting interpretation). Low-level topic images are visually more coherent
(i.e., may be easier to learn). High-level topics may be analogous to . About half of the key images are associated with topic #77, implying that the clinicians’ image
referencing behavior patterns heavily focuses on metastatic
patients. For more details and the image-topic associations,
refer to Figure 3, Figure 4 and supplementary material.
Even though LDA labels are computed with text information only, we next investigate the plausibility of mapping
images to the topic labels (at all semantic levels) via deep
CNN models.
4. Image to Document Topic Mapping with
Deep Convolutional Neural Networks
For each level of topics discussed in Section 3, we train
deep CNNs to map the images into document categories using Caffe framework. While the images of some topic
categories and some body parts are easily distinguishable
(e.g. Figure 3), the visual differences in abdominal parts are
rather subtle (e.g. Figure 4). Distinguishing the subtleties
and high-level concept categories in the images could bene-
ﬁt from a more complex model so that the model can handle
these subtleties.
We split our whole key image dataset as follows: 85%
used as the training dataset, 5% as the cross-validation (CV)
and 10% as the test dataset. If a topic has too few images
to be divided into training/CV/test for deep CNN learning
(normally rare imaging protocols), then that topic is neglected for the CNN training (e.g., topic #5 Abdominal ultrasound, #28, #49 DEXA scans of different usages). In
total, 60 topics were used for the document-level imagetopic mapping, 385 for the second-hierarchy documentlevel mapping, and 717 for the sentence-level mapping.
Surprisingly, we ﬁnd that transfer learning from the ImageNet pre-trained CNN parameters on natural images to
our medical image modalities (mostly CT, MRI) signiﬁcantly helps the image classiﬁcation performance3. Thus
our CNN models are ﬁned-tuned from the ImageNet CNN
models by default. Similar ﬁndings of the deep feature generality across different image modalities have been reported
 but are empirically veriﬁed with only much smaller
datasets than ours. Our key image dataset is ~1/5 size of ImageNet as the largest annotated medical image dataset
Implementation & Results:
All our CNN network
settings are similar4 or same as the ImageNet Challenge
“AlexNet” and “VGG-19” models. For image categorization, we change the numbers of output nodes in the
last softmax classiﬁcation layer, i.e., 60, 385 and 717 for
the document-level, document-level-h2, and sentence-level
respectively. The networks for ﬁrst-level semantic labels
are ﬁne-tuned from the pre-trained ImageNet models, where
the networks for the lower-level semantic labels are ﬁnetuned from the models of the higher-level semantic labels 5.
For all the CNN layers except the newly modiﬁed ones, the
learning rate is set 0.001 for weights and biases, momentum 0.9, weight decay 0.0005 and a smaller batch size 50
(as opposed to 256 ). These adapted layers are initialized from random and their learning rates are set higher –
learning rate: 0.01 for weight; 0.02 for bias; weight decay:
1 for weight; 0 for bias. All the key images are resampled to
the spatial resolution of 256 × 256 pixels, mostly from the
original 512 × 512. Then we follow to crop the input
images from 256 × 256 to 227 × 227 for training.
We would expect that the level of difﬁculties for learning and classifying the images into the LDA-induced topics will be different for each semantic level. Low-level semantic classes can have key images of axial/sagittal/coronal
slices with position variations and across MRI/CT modalities. Some body parts and topics, e.g., #63 pelvic (female
reproductive tract) imaging, are visually more challenging
3Please refer to the supplementary material for the details of the experiments.
4We used Caffe reference network which is a slight modiﬁcation
to the “AlexNet” .
5Detailed experiments showing beneﬁts of transferring the parameters
from the related tasks can be found in the supplementary material.
Topic&04:&
axial,contrast,mri,sagiWal,post,ﬂair,enha
ncement,blood,dynamic,brain,relaLve,v
olume,this,precontrast,from,tesla,fse,di
ﬀusion,gradient,resecLon,comparisons,
maps,philips,progression,some,suscepL
bility,perfusion,stable,achieva,techniqu
e,echo,weighted,1.5,evidence,mass,#
ﬁndings,hemorrhage,enhanced,impressi
on,frontal,signal,coronal,dL,tumor,t1X
ﬀe,hydrocephalus,magnevist,reformaLo
ns,bolus,lesion#
Topic&17:&
breast,performed,suspicious,breasts,see
n,impression,mass,screening,mammogr
am,dated,annual,cancer,mri,benign,bila
teral,was,biXrads,mammograms,#
NegaLve,dense,history,calciﬁcaLons,im
ages,views,studies,quadrant,mammogra
phy,volume,organ,aspect,suggested,cat
egory,mastectomy,before,Lssue,enhanc
ement,microcalciﬁcaLons,heterogeneo
usly,prior,family,examinaLon,recomme
nd,malignancy,high,suggest,outer,mass
es,developing,clip,paLent#
Topic&31:&
spine,cord,cervical,thoracic,spinal,level,
canal,lumbar,sagiWal,vertebral,neural,di
sc,signal,mri,body,technique,levels,ﬁndi
ngs,foramina,mild,disk,nerve,within,sm
all,marrow,central,bodies,normal,impre
ssion,enhancing,conus,syrinx,this,narro
wing,lesions,roots,contrast,throughout,
bone,degeneraLve,foramen,protrusion,
mulLple,l5Xs1,also,abnormal,c5Xc6,#
posterior,changes,heights#
Topic&78:&
bone,lesion,hip,knee,femoral,lyLc,femu
r,proximal,head,scleroLc,joint,shoulder,
hips,evidence,pelvis,distal,lesions,ﬁndin
gs,humeral,lateral,fracture,medial,hum
erus,focal,impression,bony,prosthesis,hi
story,iliac,pain,bilateral,blasLc,avn,acet
abulum,seen,marrow,sclerosis,view,bot
h,osteolyLc,corLcal,heads,area,cortex,e
ﬀusion,replacement,Lbial,involving,con
sistent,views#
Figure 3. Examples of LDA generated document-level topics with corresponding images and key words. Topic #4 MRI of brain tumor;
#17 breast imaging; #31 degenerative spine disc disease; #78 bone metastases.
Topic&77.0:&
kidney,images,abdomen,e.g,prior,mass,
pancreas,following,cysts,adrenal,liver,f
oci,renal,contrast,approximate,includin
g,focus,cyst,bilateral,masses,size,enhan
cing,for,also,given,possibly,mid,
2.5,vascular,without,due,nephrectomy,
please,1.5,from,few,mulLphase,#
subcenLmeter,least,comparison,paLen
t,dualXphase,length,apparent,#
complicaLon,obtained,upper,study,low
Topic&77.2:&
bulky,pelvis,bone,gross,since,liver,abdo
men,calciﬁcaLon,vascular,study,lung,m
ass,isovue,dfov,without,contrast,admin
istraLon,impression,metastasis,chest,fo
r,images,mesenteric,axilla,following,hil
um,cc/s,helical,mulLdetector,ascites,#
enteric,reason,apparent,complicaLon,p
leural,splenomegaly,pericardial,hydron
ephrosis,delay,eﬀusion,mediasLnum,o
btained,300,spine,gallbladder,report,
130,retroperitoneal,spleen,e.g#
Topic&77.5:&
images,axial,t1Xweighted,without,prior,#
liver,following,t2Xweighted,tesla,fatX
suppressed,mulLple,sequences,e.g,cha
racterisLc,obtained,1.5,foci,fat,#
abdomen,for,prolonged,coronal,includi
ng,relaxaLon,hydronephrosis,mri,magn
evist,splenomegaly,complicaLon,appar
ent,vascular,pleural,impression,report,
eﬀusion,contrast,reason,study,mass,ad
ministraLon,since,focus,mulLphase,de
ﬁnite,echo,defect,gross,ﬁlling,ascites,in
Topic&77.9:&
lung,chest,pleural,images,bilateral,mini
mal,eﬀusion,lower,obtained,pericardial
,mulLdetector,helical,axilla,study,repor
t,mass,inﬁltrate,for,scarring,since,bulky
,and/or,clinical,splenomegaly,dfov,#
cavity,e.g,impression,decreasing,inﬁltra
tes,focal,mediasLnum,disease,atelectas
is,hydronephrosis,small,reason,upper,u
ntoward,history,probable,appearing,cal
ciﬁcaLon,lobe,8Xchannel,supine,#
scaWered,prone,bone,intervals#
Document.level&Topic&77:&
compaLble,adenopathy,series,unchanged,image,evidence,images,e.g,pelvis,lung,since,abdomen,vascular,minimal,foci,bulky,mass,calciﬁcaLon,bone,chest,contrast,liver,e
ﬀusion,pleural,obtained,gross,following,without,splenomegaly,axilla,hydronephrosis,metastasis,bilateral,pericardial,increasing,helical,mulLdetector,apparent,complicaL
on,hilum,due,spine,gallbladder,administraLon,mesenteric,fat,dfov,cc/s,appearing,delay#
Examples of some second-hierarchy topics of document-level topic #77, with corresponding images and topic key-words.
The key-words and the images for the document-level topic (#77) indicates metastatic disease.
The key-words for topic #77 are:
[abdomen,pelvis,chest,contrast,performed,oral,was,present,masses,stable,intravenous,adenopathy,liver,retroperitoneal,comparison,administration,scans,130
,small,parenchymal,mediastinal,dated,after,which,evidence,were,pulmonary,made,adrenal,prior,pelvic,without,cysts,spleen,mass,disease,multiple,isovue-
300,obtained,areas,consistent,nodules,changes,pleural,lesions,following,abdominal,that,hilar,axillary].
AlexNet 8-layers
VGG 19-layers
document-level
document-level-h2
sentence-level
Table 3. Validation and top-1, top-5 test scores in classiﬁcation
accuracy using AlexNet and VGG-19 deep CNN models.
Figure 5. Confusion matrices of (a) document-, (b) secondhierarchy document-, (c) sentence- level classiﬁcation ((b)
and (c) can be viewed best in electronic version of this document).
than others. Mid- to high-level concepts all demonstrate
much larger within-class variations in their visual appearance since they are diseases occurring within different organs and are only coherent at high level semantics. Table
3 provides the validation and top-1, top-5 testing in classiﬁcation accuracies for each level of topic models using
AlexNet and VGG-19 based deep CNN models.
Out of the three tasks, document-level-h2 is the hardest with
document-level being relatively the easiest. Our top-1 testing accuracies are closely comparable with the validation
ones, showing good training/testing generality and no observable over-ﬁtting. All top-5 accuracy scores are significantly higher than top-1 values (increasing from 0.658 to
0.946 using VGG-19, or 0.607 to 0.929 via AlexNet in
document-level), which indicates the classiﬁcation errors
or fusions are not uniformly distributed among other false
classes. Latent “blocky subspace of classes” may exist (i.e.,
several topic classes form a tightly correlated subgroup) in
our discovered label space, where the confusion matrices in
Figure 5 verify this ﬁnding.
It is shown that the deeper 19-layer model (VGG-19
 ) performs consistently better than the 8-layer model
(AlexNet ) in classiﬁcation accuracy, especially for
document-level-h2. Compared with the ImageNet 2014 results, top-1 error rates are moderately higher (34% versus 30%) and top-5 test errors 6%~8% are comparable.
In summary, our quantitative results are very encouraging
given fewer image categorization labels (60 versus 1000)
but much higher annotation uncertainties because of the unsupervised LDA topic models. Multi-level semantic concepts show good image learnability by deep CNN models
which sheds light on the feasibility of automatically parsing
very large-scale radiology image databases.
heterogeneous
heterogenously
inhomogeneously
heterogenously
lumbosacra
pancreatic
demonstrates
transverse
comminuted
dislocatio
clinically
evaluation
radiological
documented
calcification
calcifications
appearances
thickening
hyperdense
cardiacbeat
regurgitation
pathological
appearance
surrounding
Figure 6. Example words embedded in the vector space using
word-to-vector modeling ( 
visualized on 2D space, showing (clinical) words with similar
meanings are located nearby in the vector space.
5. Generating Image-to-Text Description
The deep CNN image categorization on multi-level document topic labels in Section 4 demonstrates promising results. The ontology of document clustering-discovered categories needs to be further reviewed and reﬁned through a
“clinician in-the-loop” process. In order to help understand
the semantic contents of a given image in more detail, we
propose to generate relevant key-word text descriptions 
using deep language/image learning models.
Word-to-Vector
Word-Level Ambiguity
In radiology reports, there exist many recurring word
morphisms in text identiﬁcation, e.g., [mr, mri, t1-/t2weighted6], [cyst, cystic, cysts], [tumor, tumour, tumors,
metastasis, metastatic], etc. We train a deep word-to-vector
model to address this word-level labeling space
ambiguity. A total of ~1.2 billion words from our radiology
reports as well as from biomedical research articles obtained
from OpenI are used. Words with similar meaning are
mapped or projected to closer locations in the vector space
than dissimilar ones (i.e., locality-preserving mapping). An
example visualization of the word vectors on the 2-D space
using PCA is shown in Figure 6.
A skip-gram model is employed with the mapping vector dimension of R256 per word, trained using the
hierarchical softmax cost function, sliding-window size of
10 and frequent words sub-sampled in 0.01% frequency. It
is found that combining an additional (more diverse) set of
related documents, such as OpenI biomedical research articles, is helpful for the model to learn a better vector representation while keeping all the hyperparameters the same.
6Natural language expressions for imaging modalities of magnetic resonance imaging (MRI).
#words/sentence
reports-wide
image references
image references, no stopwords no digits
image references, disease terms only
Table 4. Some statistics about number of words per sentence –
across the radiology reports (reports-wide), across the sentences
referring the “key images” and its two adjacent ones (image references) and these not counting stopwords and digits as well as
counting disease related words only.
Some examples of query words and their corresponding
closest words in terms of cosine similarity for the wordvector models trained on radiology reports only (total
of ~1 billion words) and with additional OpenI articles (total of ~1.2 billion words) can be found in the supplementary
5.2. Image-to-Description Relation Mining and
The sentence (and adjacent sentences) referring to a key
image may contain a variety of words, but we are mostly interested in the disease-related terms (which are highly correlated to diagnostic semantics). To obtain only the diseaserelated terms, we exploit the human disease terms and their
synonyms from the Disease-Ontology (DO) , a collection of 8,707 unique disease-related terms. While the sentences referring to an image and their adjacent sentences
have 50.08 words on average, the number of disease-related
terms in the three consecutive sentences is 5.17 on average with a standard deviation of 2.5. Therefore we chose
to use bi-grams for the image descriptions, to achieve a
good trade-off between the medium level complexity and
not neglecting too many text-image pairs. Complete statistics about the number of words in the documents are shown
in Table 4.
Bi-gram disease terms are extracted so that we can train
a deep CNN (in Section 5.3) to predict the vector/wordlevel image representation (R256×2). If multiple bi-grams
can be extracted per image from the sentence referring the
image and the two adjacent ones, the image is trained as
many times as the number of bi-grams with different target
vectors (R256×2). If a disease term cannot form a bi-gram,
then the term is ignored. This is a challenging weakly annotated learning problem using referring sentences for labels. This process is shown in Figure 7, and the illustrations
for the complete work-ﬂow can be found in the supplementary material. The bi-grams of DO disease-related terms in
the vector representation (R256×2) are analogous to detecting multiple objects of interest and describing their spatial
conﬁgurations in the image caption . A deep regression CNN model is employed here to map an image to a
continuous output word-vector space from an image. The
resulting bi-gram vector can be matched against a reference
Figure 7. An example illustration of how word sequences are
learned for an image.
Bi-grams are selected from the image’s
reference sentences containing disease-related terms from the disease ontology (DO) . Each bi-gram is converted to a vector
of Z ∈R256×2 to learn from an image. Image input vectors as
{X ∈R256×256}) are learned through a CNN by minimizing the
cross-entropy loss between the target vector and output vector. The
words “nodes” and “hepatis” in the second line are DO terms but
are ignored since they can not form a bi-gram. This ﬁgure was reproduced with permission to use the DO logo from 
disease-related vocabulary in the word-vector space using
cosine similarity.
5.3. Image-to-Words Deep CNN Regression
It has been shown that deep recurrent neural networks (RNN7) can learn the language representation for machine translation. To learn the image-to-text representation,
we map the images to the vectors of word sequences describing the image. This can be formulated as a regression
CNN, replacing the softmax cost in Section 4 with the crossentropy cost function for the last output layer of VGG-19
CNN model :
[g(z)nˆg(¯zn) + (1 −g(zn)) log(1 −g(ˆzn))],
where zn or ˆzn is any uni-element of the target word vectors Zn or optimized output vectors ˆZn, g(x) is the sigmoid
7While RNN is the popular choice for learning language models , deep CNN is more suitable for image classiﬁcation.
function (g(x) = 1/(1 + ex)), and n is the number of samples in the database.
We adopt the CNN model of for the image-to-text
representation since it works consistently better than the
other relatively simpler model in our image categorization task. We ﬁne-tune the parameters of the CNNs for predicting the topic-level labels in Section 4 with the modiﬁed
cost function, to model the image-to-text representation instead of classifying images into categories. The newly modiﬁed output layer has 512 nodes for bi-grams as 256 × 2
(double the dimensionality of the word vectors), with the
cross-entropy cost decreasing and converging during training in about 10,000 iterations.
5.4. Word Prediction from Images as Retrieval
For any key image in testing, ﬁrst we predict its categorization topic labels for each hierarchy (document-level,
document-level-h2, sentence-level) using the three deep
CNN models in Section 4. Top 50 key-words in each
LDA document-topics are mapped into the word-to-vector
space as multivariate variables R256 (Section 5.1). Then,
the image is mapped to a R256×2 output vector using the
bi-gram CNN model in Section 5.3. Lastly, we match each
of the 50 topic key-word vectors (R256) against the ﬁrst and
second half of the R256×2 output vector (i.e., treated as two
words in the word-to-word matching) using cosine similarity.
The closest key-words at three hierarchy levels (with
the highest cosine similarity against either of the bi-gram
words) are kept per image. The rate of predicted diseaserelated words matching the actual words in the report sentences (recall-at-K, K=1 (R@1 score)) was 0.56. Text generation examples are shown in Figure 8, with three key-words
from three categorization levels per image. We only report
R@1 score on disease-related words compared to the previous works where they report from R@1 up to
R@20 on the entire image caption words (e.g. R@1=0.16
on Flickr30K dataset ). As we used NLP to parse and
extract image-describing sentences from the whole radiology reports, our ground-truth image-to-text associations are
much noisier than the caption dataset used in . Also
for that reason, our generated image-to-text associations are
not as exact as the generated descriptions in .
6. Conclusion & Discussion
It has been unclear how to extend the signiﬁcant success in image classiﬁcation using deep convolutional neural
networks from computer vision to medical imaging. Open
questions remain such as deﬁning clinically relevant image
labels, how to annotate the huge amount of medical images
required by deep learning models, and to what extent and
scale the deep CNN architecture is generalizable in medical
image analysis.
Input image
Output text
Original text
Figure 8. Examples of text key-word generation results, and average cosine distances between the generated words from the
disease-related words in the original texts. It is also noticeable that
kidney and adenopathy appear in the 3rd and 4th row images but
were not mentioned in the reports. The rate of predicted diseaserelated words matching the actual words in the report sentences
(recall-at-K, K=1 (R@1 score)) was 0.56.
In this paper, we present an interleaved text/image deep
mining system to extract the semantic interactions of radiology reports and diagnostic key images at a very large
and unprecedented scale in the medical domain. Images are
classiﬁed into different levels of semantic topics according
to their associated documents, and a neural language model
is learned to assign ﬁeld-speciﬁc (disease) terms to predict
what is in the radiology image. We demonstrate promising
quantitative and qualitative results, suggesting a way to extend the deep image classiﬁcation systems to learning medical imaging informatics from “big-data” at a modern hospital scale.
To the best of our knowledge, this is the ﬁrst study
performing a large-scale image/text analysis on a hospital picture archiving and communication system (PACS)
database. We hope that this study will inspire and encourage
other institutions in mining other large unannotated clinical
databases, to achieve towards establishing a central training
resource and performance benchmark for large-scale medical image research, similarly to the ImageNet for
computer vision.
Acknowledgments
This work was supported in part by the Intramural Research Program of the National Institutes of Health Clinical Center, and in part by a grant from the KRIBB Research Initiative Program (Korean Visiting Scientist Training Award), Korea Research Institute of Bioscience and
Biotechnology, Republic of Korea. This study utilized the
high-performance computational capabilities of the Biowulf
Linux cluster at the National Institutes of Health, Bethesda,
MD ( and we thank NVIDIA for the
GPU donation of K40.