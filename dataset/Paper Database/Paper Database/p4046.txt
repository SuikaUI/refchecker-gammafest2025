SMU Technical Report 1 1-100
Submitted 10/18; Published 10/18
Online Learning: A Comprehensive Survey
Steven C. H. Hoi
 
School of Information Systems, Singapore Management University, Singapore
Doyen Sahoo
 
School of Information Systems, Singapore Management University, Singapore
 
Peilin Zhao
 
Tencent AI Lab
Editor: XYZ
Online learning represents a family of machine learning methods, where a learner attempts to tackle some predictive (or any type of decision-making) task by learning from a
sequence of data instances one by one at each time. The goal of online learning is to maximize the accuracy/correctness for the sequence of predictions/decisions made by the online
learner given the knowledge of correct answers to previous prediction/learning tasks and
possibly additional information. This is in contrast to traditional batch or oﬄine machine
learning methods that are often designed to learn a model from the entire training data set
at once. Online learning has become a promising technique for learning from continuous
streams of data in many real-world applications. This survey aims to provide a comprehensive survey of the online machine learning literature through a systematic review of basic
ideas and key principles and a proper categorization of diﬀerent algorithms and techniques.
Generally speaking, according to the types of learning tasks and the forms of feedback information, the existing online learning works can be classiﬁed into three major categories:
(i) online supervised learning where full feedback information is always available, (ii) online
learning with limited feedback, and (iii) online unsupervised learning where no feedback is
available. Due to space limitation, the survey will be mainly focused on the ﬁrst category,
but also brieﬂy cover some basics of the other two categories. Finally, we also discuss some
open issues and attempt to shed light on potential future research directions in this ﬁeld.
Online learning, Online convex optimization, Sequential decision making
1. Introduction
Machine learning plays a crucial role in modern data analytics and artiﬁcial intelligence
(AI) applications. Traditional machine learning paradigms often work in a batch learning
or oﬄine learning fashion (especially for supervised learning), where a model is trained by
some learning algorithm from an entire training data set at once, and then the model is
deployed for inference without (or seldom) performing any update afterwards. Such learning
methods suﬀer from expensive re-training cost when dealing with new training data, and
thus are poorly scalable for real-world applications. In the era of big data, traditional batch
learning paradigms become more and more restricted, especially when live data grows and
c⃝2018 Hoi, Sahoo, Lu, Zhao.
 
Hoi, Sahoo, Lu and Zhao
evolves rapidly. Making machine learning scalable and practical especially for learning from
continuous data streams has become an open grand challenge in machine learning and AI.
Unlike traditional machine learning, online learning is a subﬁeld of machine learning
and includes an important family of learning techniques which are devised to learn models
incrementally from data in a sequential manner. Online learning overcomes the drawbacks
of traditional batch learning in that the model can be updated instantly and eﬃciently by
an online learner when new training data arrives. Besides, online learning algorithms are
often easy to understand, simple to implement, and often founded on solid theory with
rigorous regret bounds. Along with urgent need of making machine learning practical for
real big data analytics, online learning has attracted increasing interest in recent years.
This survey aims to give a comprehensive survey of online learning literature. Online
learning 1 has been extensively studied across diﬀerent ﬁelds, ranging from machine learning,
data mining, statistics, optimization and applied math, to artiﬁcial intelligence and data
science. This survey aims to distill the core ideas of online learning methodologies and
applications in literature. This survey is written mainly for machine learning audiences,
and assumes readers with basic knowledge in machine learning.
While trying our best
to make the survey as comprehensive as possible, it is very diﬃcult to cover every detail
since online learning research has been evolving rapidly in recent years. We apologize in
advance for any missing papers or inaccuracies in description, and encourage readers to
provide feedback, comments or suggestions. Finally, as a supplemental document to this
survey, readers may check our updated version online at: [broken link redacted by arXiv
administrators].
1.1 What is Online Learning?
Traditional machine learning paradigm often runs in a batch learning fashion, e.g., a supervised learning task, where a collection of training data is given in advance to train a model
by following some learning algorithm. Such a paradigm requires the entire training data set
to be made available prior to the learning task, and the training process is often done in an
oﬄine environment due to the expensive training cost. Traditional batch learning methods
suﬀer from some critical drawbacks: (i) low eﬃciency in both time and space costs; and
(ii) poor scalability for large-scale applications because the model often has to be re-trained
from scratch for new training data.
In contrast to batch learning algorithms, online learning is a method of machine learning
for data arriving in a sequential order, where a learner aims to learn and update the best
predictor for future data at every step. Online learning is able to overcome the drawbacks
of batch learning in that the predictive model can be updated instantly for any new data
instances. Thus, online learning algorithms are far more eﬃcient and scalable for large-scale
machine learning tasks in real-world data analytics applications where data are not only
large in size, but also arriving at a high velocity.
1. The term of “online learning” in this survey is not related to “e-learning” in the online education ﬁeld.
Online Learning: A Comprehensive Survey
1.2 Tasks and Applications
Similar to traditional (batch) machine learning methods, online learning techniques can
be applied to solve a variety of tasks in a wide range of real-world application domains.
Examples of online learning tasks include the following:
Supervised learning tasks: Online learning algorithms can be derived for supervised
learning tasks.
One of the most common tasks is classiﬁcation, aiming to predict the
categories for a new data instance belongs to, on the basis of observing past training data
instances whose category labels are given. For example, a commonly studied task in online
learning is online binary classiﬁcation (e.g., spam email ﬁltering) which only involves two
categories (“spam” vs “benign” emails); other types of supervised classiﬁcation tasks include
multi-class classiﬁcation, multi-label classiﬁcation, and multiple-instance classiﬁcation, etc.
In addition to classiﬁcation tasks, another common supervised learning task is regression
analysis, which refers to the learning process for estimating the relationships among variables
(typically between a dependent variable and one or more independent variables). Online
learning techniques are naturally applied for regression analysis tasks, e.g., time series
analysis in ﬁnancial markets where data instances naturally arrive in a sequential way.
Besides, another application for online learning with ﬁnancial time series data is online
portfolio section where an online learner aims to ﬁnd a good (e.g., proﬁtable and low-risk)
strategy for making a sequence of decisions for portfolio selection.
Bandit learning tasks: Bandit online learning algorithms, also known as Multi-armed
bandits (MAB), have been extensively used for many online recommender systems, such
as online advertising for internet monetization, product recommendation in e-commerce,
movie recommendation for entertainment, and other personalized recommendation, etc.
Unsupervised learning tasks: Online learning algorithms can be applied for unsupervised
learning tasks. Examples include clustering or cluster analysis — a process of grouping
objects such that objects in the same group (“cluster”) are more similar to each other than
to objects in other clusters. Online clustering aims to perform incremental cluster analysis
on a sequence of instances, which is common for mining data streams.
Other learning tasks: Online learning can also be used for other kinds of machine learning
tasks, such as learning for recommender systems, learning to rank, or reinforcement learning.
For example, collaborative ﬁltering with online learning can be applied to enhance the
performance of recommender systems by learning to improve collaborative ﬁltering tasks
sequentially from continuous streams of ratings/feedback information from users.
Last but not least, we note that online learning techniques are often used in two major
scenarios. One is to improve eﬃciency and scalability of existing machine learning methodologies for batch machine learning tasks where a full collection of training data must be
made available before the learning tasks. For example, Support Vector Machines (SVM) is
a well-known supervised learning method for batch classiﬁcation tasks, in which classical
SVM algorithms ) suﬀer from poor scalability for very large-scale applications. In literature, various online learning algorithms have
been explored for training SVM in an online (or stochastic) learning manner , making it more eﬃcient and scalable than conventional batch
SVMs. The other scenario is to apply online learning algorithms to directly tackle online
streaming data analytics tasks where data instances naturally arrive in a sequential manner
Hoi, Sahoo, Lu and Zhao
and the target concepts may be drifting or evolving over time. Examples include time series
regression, such as stock price prediction, where data arrives periodically and the learner
has to make decisions immediately before getting the next instance.
SMU Classification: Restricted
Online Learning
Online Learning with Full Feedback
Online Learning with Partial Feedback (Bandits)
Stochastic Bandit
Stochastic Multi-armed Bandit
Adversarial Multi-armed Bandit
Bayesian Bandit
Combinatorial Bandit
Stochastic Contextual Bandit
Adversarial Contextual Bandit
Statistical Learning Theory
Game Theory
Convex Optimization Theory
Adversarial Bandit
Online Clustering
Online Density Estimation
Online Dimension Reduction
Online Anomaly Detection
Online Active Learning
Online Semi-supervised Learning
Selective Sampling
Online Manifold Regularization
Active Learning with Expert Advice
Transductive Online Learning
First-order Online Learning
Online Learning with Regularization
Second-order Online Learning
Online Learning with Kernels
Prediction with Expert Advice
Online to Batch Conversion
Online Supervised Learning
Cost-Sensitive Online Learning
Online Collaborative Filtering
Online Multi-task Learning
Online Learning to Rank
Online Multi-view Learning
Distributed Online Learning
Online Transfer Learning
Online Learning with Neural Networks
Online Metric Learning
Online Portfolio Selection
Applied Online Learning
Online Unsupervised Learning (no feedback)
Figure 1: Taxonomy of Online Learning Techniques
1.3 Taxonomy
To help readers better understand the online learning literature as a whole, we attempt to
construct a taxonomy of online learning methods and techniques, as summarized in Figure 1. In general, from a theoretical perspective, online learning methodologies are founded
based on theory and principles from three major theory communities: learning theory, optimization theory, and game theory. From the perspective of speciﬁc algorithms, we can
further group the existing online learning techniques into diﬀerent categories according to
their speciﬁc learning principles and problem settings. Speciﬁcally, according to the types
of feedback information and the types of supervision in the learning tasks, online learning
techniques can be classiﬁed into the following three major categories:
• Online supervised learning: This is concerned with supervised learning tasks
where full feedback information is always revealed to a learner at the end of each
online learning round. It can be further divided into two groups of studies: (i) “Online
Supervised Learning” which forms the fundamental approaches and principles for
Online Supervised Learning; and (ii) “Applied Online Learning” which constitute
more non-traditional online supervised learning, where the fundamental approaches
cannot be directly applied, and algorithms have been appropriately tailored to suit
the non-traditional online learning setting.
Online Learning: A Comprehensive Survey
• Online learning with limited feedback: This is concerned with tasks where an
online learner receives partial feedback information from the environment during the
online learning process. For example, consider an online multi-class classiﬁcation task,
at a particular round, the learner makes a prediction of class label for an incoming
instance, and then receives the partial feedback indicating whether the prediction is
correct or not, instead of the particular true class label explicitly. For such tasks,
the online learner often has to make the online updates or decisions by attempting
to achieve some tradeoﬀbetween the exploitation of disclosed knowledge and the
exploration of unknown information with the environment.
• Online unsupervised learning: This is concerned with online learning tasks where
the online learner only receives the sequence of data instances without any additional
feedback (e.g., true class label) during the online learning tasks. Unsupervised online
learning can be considered as a natural extension of traditional unsupervised learning
for dealing with data streams, which is typically studied in batch learning fashion.
Examples of unsupervised online learning include online clustering, online dimension
reduction, and online anomaly detection tasks, etc. Unsupervised online learning has
less restricted assumptions about data without requiring explicit feedback or label
information which could be diﬃcult or expensive to acquire.
This article will conduct a systematic review of existing work for online learning, especially for online supervised learning and online learning with partial feedback. Finally, we
note that it is always very challenging to make a precise categorization of all the existing
online learning work, and it is likely that the above proposed taxonomy may not fully cover
all the existing online learning work in literature, though we have tried our best to cover as
much as possible.
1.4 Related Work and Further Reading
This paper attempts to make a comprehensive survey of online learning research work. In
literature, there are some related books, PHD theses, and articles published over the past
years dedicated to online learning ,
in which many of them also include rich discussions on related work on online learning.
For example, the book titled “Prediction, Learning, and Games” gave a nice introduction about some niche subjects of online learning, particularly
for online prediction with expert advice and online learning with partial feedback. Another
work titled “Online Learning and Online Convex Optimization” gave
a nice tutorial about basics of online learning and foundations of online convex optimization.
In addition, there are also quite a few PHD theses dedicated to addressing diﬀerent subjects
of online learning . Readers
are also encouraged to read some older related books, surveys and tutorial notes about
online learning and online algorithms .
Finally, readers who are interested in applied online
learning can explore some open-source toolboxes, including LIBOL and Vowpal Wabbit .
Hoi, Sahoo, Lu and Zhao
2. Problem Formulations and Related Theory
Without loss of generality, we ﬁrst give a formal formulation of a classic online learning
problem, i.e., online binary classiﬁcation, and then introduce basics of statistical learning
theory, online convex optimization and game theory as the theoretical foundations for online
learning techniques.
2.1 Problem Settings
Consider an online binary classiﬁcation task, online learning takes place in a sequential
way. On each round, a learner receives a data instance, and then makes a prediction of the
instance, e.g., classifying it into some predeﬁned categories. After making the prediction,
the learner receives the true answer about the instance from the environment as a feedback.
Based on the feedback, the learner can measure the loss suﬀered, depending on the diﬀerence
between the prediction and the answer. Finally, the learner updates its prediction model
by some strategy so as to improve predictive performance on future received instances.
Consider spam email detection as a running example of online binary classiﬁcation,
where the learner answers every question in binary: yes or no.
The task is supervised
binary classiﬁcation from a machine learning perspective. More formally, we can formulate
the problem as follows: consider a sequence of instances/objects represented in a vector
space, xt ∈Rd, where t denotes the t-th round and d is the dimensionality, and we use
yt ∈{+1, −1} to denote the true class label of the instance. The online binary classiﬁcation
takes place sequentially. On the t-th round, an instance xt is received by the learner, which
then employs a binary classiﬁer wt to make a prediction on xt, e.g., ˆyt = sign(w⊤
t xt) that
outputs ˆyt = +1 if w⊤
t xt ≥0 and ˆyt = −1 otherwise. After making the prediction, the
learner receives the true class label yt and thus can measure the suﬀered loss, e.g., using the
hinge-loss ℓt(wt) = max
 0, 1 −ytw⊤
. Whenever the loss is nonzero, the learner updates
the prediction model from wt to wt+1 by applying some strategy on the training example
(xt, yt). The procedure of Online Binary Classiﬁcation is summarized in Algorithm 1.
Algorithm 1: Online Binary Classiﬁcation process.
Initialize the prediction function as w1;
for t = 1, 2, . . . , T do
Receive instance: xt ∈Rd;
Predict byt = sign(w⊤
t xt) as the label of xt;
Receive the true class label: yt ∈{−1, +1};
Suﬀer loss: ℓt(wt) which is a convex loss function on both w⊤
t xt and yt;
Update the prediction model wt to wt+1;
By running online learning over a sequence of T rounds, the number of mistakes made
by the online learner can be measured as MT = PT
t=1 I(byt ̸= yt). In general, the classic
goal of an online learning task is to minimize the regret of the online learner’s predictions
Online Learning: A Comprehensive Survey
against the best ﬁxed model in hindsight, deﬁned as
ℓt(wt) −min
where the second term is the loss suﬀered by the optimal model w∗that can only be known
in hindsight after seeing all the instances and their class labels. From the theoretical perspective of regret minimization, if an online algorithm guarantees that its regret is sublinear
as a function of T, i.e., RT = o(T), it implies that limT→∞R(T)/T = 0 and thus on average
the learner performs almost as well as the best ﬁxed model in hindsight.
2.2 Statistical Learning Theory
Statistical learning theory, ﬁrst introduced in the late 1960’s, is one of key foundations for
theoretical analysis of machine learning problems, especially for supervised learning. In
literature, there are many comprehensive survey articles and books . In the following, we introduce some basic concept and framework.
2.2.1 Empirical Error Minimization
Assume instance xt is generated randomly from a ﬁxed but unknown distribution P(x) and
its class label y is also generated with a ﬁxed but unknown distribution P(y|x). The joint
distribution of labeled data is P(x, y) = P(x)P(y|x). The goal of a learning problem is to
ﬁnd a prediction function f(x) that minimizes the expected value of the loss function:
ℓ(y, f(x))dP(x, y)
which is also termed as the True Risk function. The solution f∗= arg min R(f) is the
optimal predictor. In general, the true risk function R(f) cannot be computed directly
because of the unknown distribution P(x, y). In practice, we approximate the true risk
by estimating the risk over a ﬁnite collection of instances (x1, y1), ..., (xN, yN) drawn i.i.d.,
which is called the “Empirical Risk” or “Empirical Error”
Remp(f) = 1
ℓ(yn, f(xn))
The problem of learning via the Empirical Error Minimization (ERM) is to ﬁnd a hypothesis
f over a hypothesis space F by minimizing the Empirical Error:
ˆfn = arg min
f∈F Remp(f)
ERM is the theoretical base for many machine learning algorithms. For example, in the
problem of binary classiﬁcation, when assuming F is the set of linear classiﬁers and the
hinge loss is used, the ERM principle indicates that the best linear model w can be trained
by minimizing the following objective
Remp(w) = 1
max(0, 1 −ynw⊤xn)
Hoi, Sahoo, Lu and Zhao
2.2.2 Error Decomposition
The diﬀerence between the optimal predictor f∗and the empirical best predictor ˆfn can be
measured by the Excess Risk, which can be decomposed as follows:
R( ˆfn) −R(f∗) =
R( ˆfn) −inf
f∈F R(f) −R(f∗)
where the ﬁrst term is called the Estimation Error due to the ﬁnite amount of training
samples that may not be enough to represent the unknown distribution, and the second
term is called the Approximation Error due to the restriction of model class F that may
not be ﬂexible enough to include the optimal predictor f∗. In general, the estimation error
will be reduced when increasing the amount of training data, while the approximation error
can be reduced by increasing the model complexity/capacity. However, the estimation error
often increases when the model complexity grows, making it challenging for model selection.
2.3 Convex Optimization Theory
Many online learning problems can essentially be (re-)formulated as an Online Convex
Optimization (OCO) task. In the following, we introduce some basics of OCO.
An online convex optimization task typically consists of two major elements: a convex
set S and a convex cost function ℓt(·). At each time step t, the online algorithm decides to
choose a weight vector wt ∈S; after that, it suﬀers a loss ℓt(wt), which is computed based
on a convex cost function ℓt(·) deﬁned over S. The goal of the online algorithm is to choose
a sequence of decisions w1, w2, . . . such that the regret in hindsight can be minimized.
More formally, an online algorithm aims to achieve a low regret RT after T rounds,
where the regret RT is deﬁned as:
ℓt(wt) −inf
where w∗is the solution that minimizes the convex objective function PT
t=1 ℓt(w) over S.
For example, consider an online binary classiﬁcation task for training online Support
Vector Machines (SVM) from a sequence of labeled instances (xt, yt), t = 1, . . . , T, where
xt ∈Rd and yt × {+1, −1}. One can deﬁne the loss function ℓ(·) as ℓt(wt) = max(0, 1 −
ytw⊤x) and the convex set S as {∀w ∈Rd|∥w∥≤C} for some constant parameter C.
There are a variety of algorithms to solve this problem.
For a comprehensive treatment of this subject, readers are referred to the books in . Below we brieﬂy review three major families of online
convex optimization (OCO) methods, including ﬁrst-order algorithms, second-order algorithms, and regularization based approaches.
2.3.1 First-order Methods
First order methods aim to optimize the objective function using the ﬁrst order (sub)
gradient information.
Online Gradient Descent (OGD) can be viewed as
an online version of Stochastic Gradient Descent (SGD) in convex optimization, and is one
of the simplest and most popular methods for convex optimization.
Online Learning: A Comprehensive Survey
At every iteration, based on the loss suﬀered on instance xt, the algorithm takes a step
from the current model to update to a new model, in the direction of the gradient of the
current loss function. This update gives us u = wt −ηt∇ℓt(wt). The resulting update may
push the model to lie outside the feasible domain. Thus, the algorithm projects the model
onto the feasible domain, i.e., ΠS(u) = arg minw∈S ∥w −u∥(where ΠS denotes the projection operation). OGD is simple and easy to implement, but the projection step sometimes
may be computationally intensive which depends on speciﬁc tasks. In theory , OGD achieves sublinear regret O(
T) for an arbitrary sequence of T convex cost
functions (of bounded gradients), with respect to the best single decision in hindsight.
2.3.2 Second-order Methods.
Second-order methods aim to exploit second order information to speed up the convergence of the optimization. A popular approach is the Online Newton Step Algorithm. The
Online Newton Step can be viewed as an online analogue of the
Newton-Raphson method in batch optimization. Like OGD, ONS also performs an update
by subtracting a vector from the current model in each online iteration. While the vector subtracted by OGD is the gradient of the current loss function based on the current
model, in ONS the subtracted vector is the inverse Hessian multiplied by the gradient, i.e.,
t ∇ℓt(wt) where At is related to the Hessian. At is also updated in each iteration as
At = At−1 + ∇ℓt(wt)∇ℓt(wt)⊤. The updated model is projected back to the feasible domain as wt+1 = ΠAt
S (wt −ηA−1
t ∇ℓt(wt)), where ΠA
S (u) = arg minw∈S(w −u)⊤A(w −u).
Diﬀerent from OGD where the projection is made under the Euclidean norm, ONS projects
under the norm induced by the matrix At. Although ONS’s time complexity O(n2) is higher
than OGD’s O(n), it guarantees a logarithmic regret O(log T) under relatively weaker assumptions of exp-concave cost functions.
2.3.3 Regularization
Unlike traditional convex optimization, the aim of OCO is to optimize the regret. Traditional approaches (termed as Follow the Leader (FTL)) can be unstable, leading to high
regret (e.g. linear regret) in the worst case . This motivates the need to
stabilize the approaches through regularzation. Here we discuss the common regularization
approaches.
Follow-the-Regularized-Leader (FTRL). The idea of Follow-the-Regularized-Leader (FTRL) is to stablize the prediction of the
Follow-the-Leader (FTL) by adding a regularization term R(w) which is strongly convex, smooth and twice diﬀerentiable. The idea is
to solve the following optimization problem in each iteration:
wt+1 = arg min
∇ℓs(ws)⊤w + R(w)
where S is the feasible convex set and η is the learning rate. In theory, the FTRL algorithm
in general achieves a sublinear regret bound O(
Online Mirror Descent (OMD). OMD is an online version of the Mirror Descent (MD)
method in batch convex optimization. The OMD algorithm
Hoi, Sahoo, Lu and Zhao
behaves like OGD, in that it updates the model using a simple gradient rule. However, it
generalizes OGD as it performs updates in the dual space. This duality is induced by the
choice of the regularizer: the gradient of the regularization serves as a mapping from Rd to
itself. Due to this transformation by the regularizer, OMD is able to obtain better bounds
in terms of the geometry of the space.
In general, OMD has two variants of algorithms: lazy OMD and active OMD. The lazy
version keeps track of a point in Euclidean space and projects it onto the convex feasible
domain only when making prediction, while the active version keeps a feasible model all
the time, which is a direct generalization of OGD. Unlike OGD, the projection step in
OMD is based on the Bregman Divergence BR, i.e., wt+1 = arg minw∈S BR(w∥vt+1), where
vt+1 is the updated model after the gradient step.
In general, the lazy OMD has the
same regret bound as FTRL. The active OMD also has a similar regret bound.
2, OMD recovers OGD. If we use other functions as R, we can also recover
some other interesting algorithms, such as the Exponential Gradient (EG) algorithm below.
Exponential Gradient (EG). Let R(w) = w ln w be the negative entropy function and
the feasible convex domain be the simplex S = ∆d = {w ∈Rd
i wi = 1}, then OMD will
recover the Exponential Gradient (EG) algorithm . In this
special case, the induced projection is the normalization by the L1 norm, which indicates
wt,i exp[−η(∇ℓt(wt))i]
j wt,j exp[−η(∇ℓt(wt))j]
As a special case of OMD, the regret of EG is bounded by O(
Adaptive (Sub)-Gradient Methods. In the previous algorithms, the regularization function R is always ﬁxed and data independent, during the whole learning process. Adaptive (Sub)-Gradient (AdaGrad) algorithm is an algorithm that can
be considered as online mirror descent with adaptive regularization, i.e., the regularization
function R can change over time. The regularizer R at the t-th step, is actually the function
w, which is constructed from the (sub)-gradients received
before (and including) the t-th step. In each iteration the model is updated as:
wt+1 = arg min
w −[wt −ηA
where At is updated as:
At = At−1 + ∇ℓt(wt)∇ℓt(wt)⊤
We also note that there are also other emerging online convex optimization methods,
such as Online Convex Optimization with long term constraints ,
which assumes that the constraints are only required to be satisﬁed in long term, and
Online ADMM which is an online version for the Alternating
Direction Method of Multipliers (ADMM) 
and is particularly suitable for distributed optimization applications. The RESCALED-
EXP algorithm , proposed recently, does not use any prior
knowledge about the loss functions and does not require the tuning of learning rate.
Online Learning: A Comprehensive Survey
2.4 Game Theory
Game theory is closely related to online learning. In general, an online prediction task can
be formulated as a problem of learning to play a repeated game between a learner and an
environment . Consider online classiﬁcation as an example,
during each iteration, the algorithm chooses one class from a ﬁnite number of classes and the
environment reveals the true class label. Assume the environment is stable (e.g., i.i.d), i.e.,
not played by an adversary. The algorithm aims to perform as well as the best ﬁxed strategy.
The classic online classiﬁcation problem thus can be modeled by the game theory under
the simplest assumption, full feedback and a stable environment. More generally, various
settings in game theory can be related to many other types of online learning problems. For
example, the feedback may be partly observed, or the environment is not i.i.d. or can be
operated by an adversary who aims to maximize the loss of the predictor. In this section,
we will introduce some basic concepts about game theory and some fundamental theory of
learning in games. We will focus on regret-based minimization procedures and limit our
attention to ﬁnite strategic or normal form games. A more comprehensive study on this
subject can be found in .
2.4.1 Game Playing and Nash Equilibrium
K-Player Normal-Form Games.
Consider a game with K players (1 < K < ∞),
where each player k ∈{1, ..., K} can take Nk possible actions. The players’ actions can be
represented by a vector i = (i1, ..., iK), where ik ∈{1, ..., Nk} denotes the action of player
k. The loss suﬀered by the player k is denoted by ℓ(k)(i) since the loss is related to not only
the action of player k but the action of all the other players. During each iteration of the
game, each player tries to take actions in order to minimize its own loss.
Using a mixed strategy, player k takes actions based on a probability distribution p(k) =
1 , . . . , p(k)
Nk) over the set of {1, . . . , Nk} actions. In particular, the actions played by all
the K players can be denoted as a random vector I = (I1, ..., IK), where Ik is the action
played by player k which is a random variable taking value over the set of {1, . . . , Nk}
actions distributed according to p(k). The expected loss of player k can be computed as
Eℓ(k)(I) =
i1 × · · · × p(K)
iK ℓ(k)(i1, ..., iK)
Nash Equilibrium.
This is an important notion in game theory.
In particular, a
collective strategy of all players p(1) × · · · × p(K) is called a Nash equilibrium if any mixed
strategy among the K players p(k) is replaced by any new mixed strategy q(k) while all
other K −1 players’ mixed strategies make no change, we have
Eℓ(k)(I) ≤Eℓ(k)(I′)
where I′ denotes the actions played by the K players using the new strategies. This deﬁnition
means that in a Nash Equilibrium, no player can achieve a lower loss by only changing its
own strategy if other players do not change. In a Nash Equilibrium, each player gets its
own optimal strategy and has no incentive of changing its strategy. One can prove that
every ﬁnite game has at least one Nash equilibrium, but a game may have multiple Nash
equilibria depending on the structure of the game and the loss functions.
Hoi, Sahoo, Lu and Zhao
2.4.2 Repeated Two-player Zero-Sum Games
A simple but important special class of K-Player Normal Form Games is the class of twoplayer zero-sum games where only one player plays against one opponent, i.e., K = 2.
Zero-sum means that for any action, the sum of losses of all players is zero. This indicates
that the game is purely competitive and a player’s loss results in another player’s gain. In
such games, the ﬁrst player is often called the row player, and the second player is called the
column player whose goal is to maximize the loss of the ﬁrst player. To simplify notation,
we consider the row player has N possible actions and the column player has M possible
actions. We denote by L ∈ N×M where L(i, j) is the loss of the row player taking
action i while the column player chooses action j, and the mixed strategies for the row and
column players denoted by p = (p, . . . , pN) and q = (q, . . . , qN), respectively. For the two
mixed strategies p and q, the expected loss for the row player (which is equivalent to the
expected gain of the column player) can be computed by
p(i)q(j)L(i, j)
A pair of mixed strategies (p, q) is a Nash equilibrium if and only if
L(p, q′) ≤L(p, q) ≤L(p′, q),
One natural solution to the two-player zero-sum games is to follow the minimax solution.
In particular, for the row player using some strategy p, the worst-case loss is at most
maxq L(p, q) if the column player makes the decision after seeing p. Therefore, the worstcase optimal strategy (also called the minimax optimal strategy) for the row player is
p∗= arg minp maxq L(p, q). Similarly, the maximin optimal strategy for the column player
is q∗= arg maxq minp L(p, q). The pair of (p∗, q∗) is called a minimax solution of the game.
Surprisingly there is no diﬀerence between minp maxq L(p, q) and maxq minp L(p, q), which
is known as the von Neumann’s minimax theorem, a fundamental result of game theory.
Theorem 1 (von Neumann’s minimax theorem) In a two-player zero-sum game, when two
players follow the strategies of the minimax solution, they reach the same optimal value
L(p, q) = max
V ∗is called the value of the game which is unique for a two-player zero-sum game. A pair
of mixed strategies (p, q) is a Nash equilibrium if and only if it achieves the value of game.
We can now relate game theory to online learning as the problem of learning to play
repeated two-player zero-sum games. In the context of online learning, the row player is
also called the learner and the column player is called the environment. The repeated game
playing between the row player and the column player is treated as a sequence of T rounds
of interactions between the learner and the environment. On each round t = 1, . . . , T,
• the leaner chooses a mixed strategy pt;
• the environment chooses a mixed strategy qt (may be chosen by the knowledge pt);
• the learner observes the losses L(i, qt)
In general, the goal of the learner is to minimize the cumulative loss, i.e., PT
t=1 L(pt, qt).
Online Learning: A Comprehensive Survey
3. Online Supervised Learning
3.1 Overview
In this section, we survey a family of “online supervised learning” algorithms which de-
ﬁne the fundamental approaches and principles for online learning methodologies toward
supervised learning tasks Shalev-Shwartz ; Rakhlin et al. .
We ﬁrst discuss linear online learning methods, where a target model is a linear function.
More formally, consider an input domain X and an output domain Y for a learning task,
we aim to learn a hypothesis f : X 7→Y, where the target model f is linear. For example,
consider a typical linear binary classiﬁcation task, our goal is to learn a linear classiﬁer
f : X 7→{+1, −1} as follows: f(xt; w) = sgn(w · xt), where X is typically a d-dimensional
vector space Rd, w ∈X is a weight vector speciﬁed for the classiﬁer to be learned, and sgn(z)
is an indicator function that outputs +1 when z > 0 and -1 otherwise. We review two major
types of linear online learning algorithms: ﬁrst-order online learning and second-order online
learning algorithms. Following this, we discuss Prediction with expert advice, and Online
Learning with Regularization. This is followed by reviewing nonlinear online learning using
kernel based methods. We discuss a variety of kernel-based online learning approaches,
their computational challenges, and several approximation strategies for eﬃcient learning.
We end this section by discussing the theory for converting using online learning algorithms
to learn a batch model that can generalize well.
3.2 First-order Online Learning
In the following, we survey a family of ﬁrst-order linear online learning algorithms, which
exploit the ﬁrst order information of the model during learning process.
3.2.1 Perceptron
Perceptron is the oldest algorithm for
online learning. Algorithm 2 gives the Perceptron algorithm for online binary classiﬁcation.
Algorithm 2: Perceptron
INIT: w1 = 0
for t = 1, 2, . . . , T do
Given an incoming instance xt, predict
ˆyt = ft(xt) = sign(wt · xt);
Receive the true class label yt ∈{+1, −1};
if ˆyt ̸= yt then
wt+1 ←wt + ytxt;
In theory, by assuming the data is separable with some margin γ, the Perceptron algorithm makes at most
2 mistakes, where the margin γ is deﬁned as γ = mint∈[T] |xt · w∗|
Hoi, Sahoo, Lu and Zhao
and R is a constant such that ∀t ∈[T], ∥xt∥≤R. The larger the margin γ is, the tighter
the mistake bound will be.
In literature, many variants of Perceptron algorithms have been proposed. One simple
modiﬁcation is the “normalized Perceptron” algorithm that diﬀers only in the updating rule
as follows:
wt+1 = wt + yt
The mistake bound of the “normalized Perceptron” algorithm can be improved from
2 for the separable case due to the normalization eﬀect.
3.2.2 Winnow
Unlike the Perceptron algorithm that uses additive updates, Winnow 
employs multiplicative updates. The problem setting is slightly diﬀerent from the Perceptron: X = {0, 1}d and y ∈{0, 1}. The goal is to learn a classiﬁer f(x1, . . . , xn) = xi1∨...∨xik
called monotone disjunction, where ik ∈1, . . . , d. The separating hyperplane for this classiﬁer is given by xi1 + ... + xik. The Winnow algorithm is outlined in Algorithm 3.
Algorithm 3: Winnow
INIT: w1 = 1d, constant α > 1 (e.g.,α = 2)
for t = 1, 2, . . . , T do
Given an instance xt, predict ˆyt = Iwt·xt≥θ (outputs 1 if statement holds and 0
otherwise);
Receive the true class label yt ∈{1, 0};
if ˆyt = 1, yt = 0 then
set wi = 0 for all xt,i = 1 (“elimination” or “demotion”),
if ˆyt = 0, yt = 1 then
set wi = αwi for all xt,i = 1 (“promotion”).
The Winnow algorithm has a mistake bound αk(logα θ + 1) + n/θ where α > 1 and
θ ≥1/α and the target function is a k-literal monotone disjunction.
3.2.3 Passive-Aggressive Online Learning (PA)
This is a popular family of ﬁrst-order online learning algorithms which generally follows the
principle of margin-based learning . Speciﬁcally, given an instance
xt at round t, PA formulates the updating optimization as follows:
wt+1 = arg min
2||w −wt||2
s.t. ℓt(w) = 0
where ℓt(w) = max(0, 1 −ytw · xt) is the hinge loss. The above resulting update is passive
whenever the hinge loss is zero, i.e., wt+1 = wt whenever ℓ= 0. In contrast, whenever the
Online Learning: A Comprehensive Survey
loss is nonzero, the approach will force wt+1 aggressively to satisfy the constraint regardless
of any step-size; the algorithm is thus named as “Passive-Aggressive” (PA) . Speciﬁcally, PA aims to keep the updated classiﬁer wt+1 stay close to the previous
classiﬁer (“passiveness”) and ensure every incoming instance to be classiﬁed correctly by the
updated classiﬁer (“aggressiveness”). The regular PA algorithm assumes training data is
always separable, which may not be true for noisy training data in real-world applications.
To overcome such limitation, two variants of PA relax the assumption as:
PA −I : wt+1 = arg min
2||w −wt||2 + Cξ
subject to
ℓt(w) ≤ξ and ξ ≥0
PA −II : wt+1 = arg min
2||w −wt||2 + Cξ2
subject to
where C is a positive parameter to balance the tradeoﬀbetween “passiveness” (ﬁrst regularization term) and “aggressiveness” (second slack-variable term). By solving the three
optimization tasks, we can derive the closed-form updating rules of three PA algorithms:
wt+1 = wt + τtytxt,
ℓt/||xt||2
min{C, ℓt/||xt||2}
||xt||2+ 1
It is important to note a major diﬀerence between PA and Perceptron algorithms. Perceptron makes an update only when there is a classiﬁcation mistake. However, PA algorithms
aggressively make an update whenever the loss is nonzero (even if the classiﬁcation is correct). In theory , PA algorithms have comparable mistake bounds
as the Perceptron algorithms, but empirically PA algorithms often outperform Perceptron
signiﬁcantly. The PA algorithms are outlined in Algorithm 4.
Algorithm 4: Passive Aggressive Algorithms
INIT: w1, Aggressiveness Parameter C;
for t = 1, 2, . . . , T do
Receive xt ∈Rd, predict ˆyt using wt;
Suﬀer loss ℓt(wt);
ℓt/||xt||2
min{C, ℓt/||xt||2}
||xt||2+ 1
Update wt+1 = wt + τtytxt;
3.2.4 Online Gradient Descent (OGD)
Many online learning problems can be formulated as an online convex optimization task,
which can be solved by applying the OGD algorithm. Consider the online binary classiﬁca-
Hoi, Sahoo, Lu and Zhao
tion as an example, where we use the hinge loss function, i.e., ℓt(w) = max(0, 1 −ytw · xt).
By applying the OGD algorithm, we can derive the updating rule as follows:
wt+1 = wt + ηtytxt
where ηt is the learning rate (or step size) parameter. The OGD algorithm is outlined in
Algorithm 5, where any generic convex loss function can be used. ΠS is the projection
function to constrain the updated model to lie in the feasible domain.
Algorithm 5: Online Gradient Descent
INIT: w1, convex set S, step size ηt;
for t = 1, 2, . . . , T do
Receive xt ∈Rd, predict ˆyt using wt;
Suﬀer loss ℓt(wt);
Update wt+1 = ΠS(wt −ηt∇ℓt(wt))
OGD and PA share similar updating rules but diﬀer in that OGD often employs some
predeﬁned learning rate scheme while PA chooses the optimal learning rate τt at each round
(but subject to a predeﬁned cost parameter C). In literature, diﬀerent OGD variants have
been proposed to improve either theoretical bounds or practical issues, such as adaptive
OGD , and mini-batch OGD , amongst others.
3.2.5 Other first-order algorithms
In literature, there are also some other ﬁrst-order online learning algorithms, such as Approximate Large Margin Algorithms (ALMA) which is a large margin variant
of the p-norm Perceptron algorithm, and the Relaxed Online Maximum Margin Algorithm
(ROMMA) . Many of these algorithms often follow the principle of large
margin learning. The metaGrad algorithm tries to adapt the
learning rate automatically for faster convergence.
3.3 Second-Order Online Learning
Unlike the ﬁrst-order online learning algorithms that only exploit the ﬁrst order derivative
information of the gradient for the online optimization tasks, second-order online learning
algorithms exploit both ﬁrst-order and second-order information in order to accelerate the
optimization convergence.
Despite the better learning performance, second-order online
learning algorithms often fall short in higher computational complexity. In the following we
present a family of popular second-order online learning algorithms.
3.3.1 Second Order Perceptron (SOP)
SOP algorithm is able to exploit certain geometrical properties
of the data which are missed by the ﬁrst-order algorithms.
For better understanding, we ﬁrst introduce the whitened Perceptron algorithm, which
strictly speaking, is not an online learning method. Assuming that the instances x1, ..., xT
Online Learning: A Comprehensive Survey
are preliminarily available, we can get the correlation matrix M = PT
whitened Perceptron algorithm is simply the standard Perceptron run on the transformed
sequence (M−1/2x1, y1), ..., (M−1/2xT , yT ). By reducing the correlation matrix of the transformed instances, the whitened Perceptron algorithm can achieve signiﬁcantly better mistake bound.
SOP can be viewed as an online variant of the whitened Perceptron algorithm. In online setting the correlation matrix M can be approximated by the previously seen instances.
SOP is outlined in Algorithm 6
Algorithm 6: SOP
INIT: w1 = 0, X0=[], v0 = 0, k = 1
for t = 1, 2, . . . , T do
Given an incoming instance xt, set St = [Xk−1 xt],
predict ˆyt = ft(xt) = sign(wt · xt), where wt = (aIn + StS⊤
Receive the true class label yt ∈{+1, −1};
if ˆyt ̸= yt then
vk = vk−1 + ytxt, Xk = St, k = k + 1.
Here a ∈R+ is a parameter that guarantees the existence of the matrix inverse.
3.3.2 Confidence Weighted Learning (CW)
The CW algorithm is motivated by the following observation: the
frequency of occurrence of diﬀerent features may diﬀer a lot in an online learning task.
(For example) The parameters of binary features are only updated when the features occur.
Thus, the frequent features typically receive more updates and are estimated more accurately compared to rare features. However, no distinction is made between these feature
types in most online algorithms. This indicates that the lack of second order information
about the frequency or conﬁdence of the features can hurt the learning.
In the CW setting, we model the linear classiﬁer with a Gaussian distribution, i.e.,
w ∼N(µ, Σ), where µ ∈Rd is the mean vector and Σ ∈Rd×d is the covariance matrix.
When making a prediction, the prediction conﬁdence M = w · x also follows a Gaussian
distribution: M ∼N(µM, ΣM), where µM = µ · x and ΣM = x⊤Σx.
Similar to the PA update strategy, the update rule in round t can be obtained by solving
the following convex optimization problem:
(µt+1, Σt+1) = arg min
µ∈Rd DKL (N(µ, Σ)||N(µt, Σt))
s.t. Pr[ytMt ≥0] ≥η
The objective function means that the new distribution should stay close to the previous
distribution so that the classiﬁer does not forget the information learnt from previous instances, where the distance between the two distributions is measured by the KL divergence.
The constraint means that the new classiﬁer should classify the new instance xt correctly
with probability higher than a predeﬁned threshold parameter η ∈(0, 1).
Hoi, Sahoo, Lu and Zhao
Note that this is only the basic form of conﬁdence weighted algorithms and has several
drawbacks. 1) Similar to the hard margin PA algorithm, the constraint forces the new
instance to be correctly classiﬁed, which makes this algorithm very sensitive to noise. 2)
The constraint is in a probability form. It is easy to solve a problem with the constraint
g(µM, ΣM) < 0. However, a problem with a probability form constraint is only solvable
when the distribution is known. Thus, this method faces diﬃculty in generalizing to other
online learning tasks where the constraint does not follow a Gaussian distribution.
3.3.3 Adaptive Regularization of Weight Vectors (AROW)
AROW is a variant of CW that is designed for non-separable data.
This algorithm adopts the same Gaussian distribution assumption on classiﬁer vector w
while the optimization problem is diﬀerent. By recasting the CW constraint as regularizers,
the optimization problem can be formulated as:
C(µ, Σ) = DKL (N(µ, Σ)||N(µt, Σt)) + λ1ℓ(yt, µ · xt) + λ2x⊤
where ℓ(yt, µ · xt) = (max(0, 1 −ytµ · xt))2 is the squared-hinge loss. During each iteration,
the update rule is obtained by solving the optimization problem:
(µt+1, Σt+1) = arg min
µ∈Rd(C(µ, Σ))
which balances the three desires. First, the parameters should not change radically on each
round, since the current parameters contain information about previous examples (ﬁrst
term). Second, the new mean parameters should predict the current example with low loss
(second term). Finally, as we see more examples, our conﬁdence in the parameters should
generally grow (third term). λ1 and λ2 are two positive parameters that control the weight
of the three desires.
Besides the robustness to noisy data, another important advantage of AROW is its
ability to be easily generalized to other online learning tasks, such as Conﬁdence Weighted
Online Collaborative Filtering algorithm and Second-Order Online Feature
Selection .
3.3.4 Soft Confidence weighted Learning (SCW)
This is a variant of CW learning in order to deal with non-separable data . Diﬀerent from AROW which directly adds loss and conﬁdence regularization,
and thus loses the adaptive margin property, SCW exploits adaptive margin by assigning
diﬀerent margins for diﬀerent instances via a probability formulation. Consequently, SCW
tends to be more eﬃcient and eﬀective.
Speciﬁcally, the constraint of CW can be rewritten as yt(µ · xt) ≥φ
t Σxt. Thus, the
loss function can be deﬁned as:ℓ(N(µ, Σ); (xt, yt)) = max(0, φ
t Σxt −yt(µ · xt)). The
original CW optimization can be rewritten as:
(µt+1, Σt+1) = arg min
µ∈Rd DKL (N(µ, Σ)||N(µt, Σt))
subject to ℓ(N(µ, Σ); (xt, yt)) = 0
Online Learning: A Comprehensive Survey
Inspired by soft-margin PA variants, SCW generalized CW into two soft-margin formulations:
(µt+1, Σt+1) = arg min
µ∈Rd DKL (N(µ, Σ)||N(µt, Σt)) + Cℓ(N(µ, Σ); (xt, yt))
(µt+1, Σt+1) = arg min
µ∈Rd DKL (N(µ, Σ)||N(µt, Σt)) + Cℓ2(N(µ, Σ); (xt, yt))
where C ∈R+ is a parameter controls the aggressiveness of this algorithm, similar to the
C in PA algorithm. The two algorithms are termed “SCW-I” and “SCW-II”.
3.3.5 Other second-order algorithms
The conﬁdence weighted idea also works for other online learning tasks such as multi-class
classiﬁcation , active learning and
structured-prediction . There are many other online learning
algorithms that adopt second order information: IELLIP assumes the
objective classiﬁer w lies in an ellipsoid and incrementally updates the ellipsoid based on the
current received instance. Other approaches include New variant of Adaptive Regularization
(NAROW) and the Normal Herding method via Gaussian
Herding (NHERD) .
Recently, Sketched Online Newton made signiﬁcant improvements to speed-up second order online learning.
3.4 Prediction with Expert Advice
This is an important online learning subject with many
applications.
A general setting is as follows.
A learner has N experts to choose from,
denoted by integers 1, . . . , N. At each time step t, the learner decides on a distribution pt
over the experts, where pt,i ≥0 is the weight of each expert i, and PN
i=1 pt,i = 1. Each
expert i then suﬀers some loss ℓt,i according to the environment. The overall loss suﬀered
by the learner is PN
i=1 pt,iℓt,i = p⊤
t ℓt, i.e., the weighted average loss of the experts with
respect to the distribution chosen by the learner.
Typically we assume that the loss suﬀered by any expert is bounded. Speciﬁcally, we
assume ℓt,i ∈ without loss of generality. Besides this condition, no assumption is made
on the form of the loss, or about how they are generated. Suppose the cumulative losses
experienced by each expert and the forecaster are calculated respectively as follows:
The loss diﬀerence between the forecaster and the expert is known as the “regret”, i.e.,
Rt,i = Lt −Lt,i,
i = 1, . . . , N.
The goal of learning the forecaster is to make the regret with respect to each expert as small
as possible, which is equivalent to minimizing the overall regret, i.e.,
1≤i≤N RT,i = LT −min
1≤i≤N LT,i
Hoi, Sahoo, Lu and Zhao
In general, online prediction with expert advice aims to ﬁnd an ideal forecaster to achieve a
vanishing per-round regret, a property known as the Hannan-consistency ,i.e.,
RT = o(T) ⇔lim
1≤i≤N LT,i
An online learner satisfying the above is called a Hannan-consistent forecaster . Next we review some representative algorithms for prediction with expert
3.4.1 Weighted Majority Algorithms
The weighted majority algorithm (WM) is a simple but widely studied algorithm that
makes a binary prediction based on a series of expert advices .
The simplest version is shown in Algorithm 7, where β ∈(0, 1) is a user
speciﬁed discount rate parameter.
Algorithm 7: Weighted Majority
INIT: Initialize the weights p1, p2, ...pN of all experts to 1/N.
for t = 1, 2, . . . , T do
Get the prediction x1, ..., xN from N experts.
Output 1 if P
i:xi=1 pi ≥P
i:xi=0 pi; otherwise output 0.
Receive the true value; if the i-th expert made a mistake, then pi = pi ∗β
3.4.2 Randomized Multiplicative Weights Algorithms
This algorithm works under the same assumption that the expert advices are all binary
 . While the prediction is random, the algorithm gives the prediction 1
with probability of γ =
and 0 with probability of 1 −γ.
3.4.3 Hedge Algorithm
The Hedge algorithm is perhaps the most well-known approach
for online prediction with expert advice, which can be viewed as a direct generalization of
Littlestone and Warmuth’s weighted majority algorithm . The working of Hedge algorithm is shown in Algorithm 8. The algorithm maintains
a weight vector whose value at time t is denoted wt = (wt,1, . . . , wt,N).
At all times,
all weights are nonnegative. All of the weights of the initial weight vector w1 must be
nonnegative and sum to one, which can be considered as a prior over the set of experts. If
it is believed that one expert performs the best, it is better to assign it more weight. If no
prior is known, it is better to set all the initial weights equally, i.e., w1,i = 1/N for all i.
The algorithm uses the normalized distribution to make prediction, i.e., pt = wt/PN
After the loss ℓt is disclosed, the weight vector wt is updated using a multiplicative rule
wt+1,i = wt,iβℓt,i,
β ∈ , which implies that the weight of expert i will exponentially
decrease with the loss ℓt,i. In theory, the Hedge algorithm is proved to be Hannan consistent.
Online Learning: A Comprehensive Survey
Algorithm 8: Hedge Algorithm
INIT: β ∈ , initial weight vector w1 ∈ N with PN
i=1 w1,i = 1
for t = 1, 2, . . . , T do
set distribution pt =
i=1 wt,i ;
Receive loss ℓt ∈ N from environment;
Suﬀer loss p⊤
Update the new weight vector to wt+1,i = wt,iβℓt,i
3.4.4 EWAF Algorithms
Besides Hedge, there are some other algorithms for online prediction with expert advice under more challenging settings, including exponentially weighted average forecaster (EWAF)
and Greedy Forecaster (GF) .
We will mainly discuss
EWAF, which is shown in Algorithm 9
Algorithm 9: EWAF
INIT: a poll of experts fi,
i = 1, . . . , N and L0,i = 0, i = 1, . . . , N, and learning rate η
for t = 1, 2, . . . , T do
The environment chooses the next outcome yt and the expert advice {ft,i};
The expert advice is revealed to the forecaster
The forecaster chooses the prediction ˆpt =
i=1 exp(−ηLt−1,i)ft,i
i=1 exp(−ηLt−1,i)
The environment reveals the outcome yt;
The forecaster incurs loss ℓ(ˆpt, yt) and;
Each expert incurs loss ℓ(ft,i, yt)
The forecaster update the cumulative loss Lt,i = Lt−1,i + ℓ(ft,i, yt)
The diﬀerence between EWAF and Hedge is that the loss in Hedge is the inner product
between the distribution and the loss suﬀered by each expert, while for EWAF, the loss is
between the prediction and the true label, which can be much more complex.
3.4.5 Parameter-free Online Learning
A category of prediction with expert advice deals with learning without user speciﬁed
learning rate. It is a diﬃcult task to set a learning rate prior to the learning procedure.
To address this issue, parameter-free online algorithms were proposed. Among the early
eﬀorts, Chaudhuri et al. proposed a variant of Hedge Algorithm without the use of a
learning rate. The proposed method achieved optimal regret matching the best bounds of
all the previous algorithms (with optimally-tuned parameters). Chernov and Vovk 
improved this bound, but did not have a closed-form solution. There were further extensions
which derived data-dependent bounds too + φs(w)
where φs is a sparsity-inducing regularizer. For example, when choosing φs = λ||w||0, it is
equivalent to imposing a hard constraint on the number of nonzero elements in w. Instead
of choosing ℓ0-norm which is hard to be optimized, a more commonly used regularizer is
ℓ1-norm, i.e., φs = λ||w||1, which can induce sparsity of the weight vector but does not
explicitly constrain the number of nonzero elements. The following reviews some popular
sparse online learning methods.
3.5.1 Truncated Gradient Descent
A straightforward idea to sparse online learning is to modify Online Gradient Descent and
round small coeﬃcients of the weight vector to 0 after every K iterations:
wt+1 = T0(wt −η∇ℓt(wt), θ)
where the function T0(v, θ) performs an element-wise rounding on the input vector: if the jth element vj is smaller than the threshold θ, set vj = 0. Despite its simplicity, this method
struggles to provide satisfactory performance because the aggressive rounding strategy may
ignore many useful weights which may be very small due to low frequency of appearance.
Motivated by addressing the above limitation, the Truncated Gradient Descent (TGD)
method explores a less aggressive version of the truncation function:
wt+1 = T1(wt −η∇ℓt(wt), ηgi, θ)
where T1(vj, α, θ) =
max(0, vj −α)
if vj ∈[0, θ]
min(0, vj + α)
if vj ∈[−θ, 0]
where gi > 0 is a parameter that controls the level of aggressiveness of the truncation. By
exploiting sparsity, TGD achieves eﬃcient time and space complexity that is linear with
respect to the number of nonzero features and independent of the dimensionality d. In
addition, it is proven to enjoy a regret bound of O(
T) for convex loss functions when
setting η = O(1/
3.5.2 Forward Looking Subgradients (FOBOS)
Consider the objective function in the t-th iteration of a sparse online learning task as
ℓt(w) + r(w), FOBOS assumes ft is a convex loss function (dif-
Online Learning: A Comprehensive Survey
ferentiable), and r is a sparsity-inducing regularizer (non-diﬀerentiable). FOBOS updates
the classiﬁer in the following two steps:
(1) Perform Online Gradient Descent: wt+ 1
2 = wt −ηt∇ℓt(wt)
(2) Project the solution in (i) such that the projection stays close to the interim vector
2 and (ii) has a low complexity due to r:
wt+1 = arg min
2 ||2 + ηt+ 1
When choosing ℓ1-norm as the regularizer, the above optimization can be solved with the
closed-form solution for each coordinate:
t+1 = sgn(wj
2 | −ηt+ 1
The FOBOS algorithm with ℓ1-norm regularizer can be viewed as a special case of TGD,
where the truncation threshold θ = ∞, and the truncation frequency K = 1.
2 = ηt+1 and ηt = O(1/
t), this algorithm also achieves O(
T) regret bound.
3.5.3 Regularized Dual Averaging (RDA)
Motivated by the theory of dual-averaging techniques , the RDA algorithm
 updates the classiﬁer by:
wt+1 = arg min
¯gt⊤w + Ψ(w) + βt
where Ψ(w) is the original sparsity-inducing regularizer, i.e., Ψ(w) = λ||w||1; h(w) =
2||w||2 is an auxiliary strongly convex function and ¯gt is the averaged gradients of all
previous iterations, i.e., ¯g = 1
τ=1 ∇ℓτ(wτ). Setting the step size βt = γ
t, one can
derive the closed-form solution:
if | ¯gtj| < λ
γ ( ¯gtj −λsgn( ¯gtj))
To further pinpoint the diﬀerences between RDA and FOBOS, we rewrite FOBOS in the
same notation as RDA:
wt+1 = arg min
t w + Ψ(w) +
||w −wt||2
Speciﬁcally, RDA diﬀers from FOBOS in several aspects. First, RDA uses the averaged
gradient instead of the current gradient. Second, h(w) is a global proximal function instead
of its local Bregman divergence.
Third, the coeﬃcient for h(w) is βt/t = γ/
is 1/αt = O(
t) in FOBOS. Fourth, the truncation of RDA is a constant λ, while the
truncation in FOBOS ηt+ 1
2 decrease with a factor
t. Clearly, RDA uses a more aggressive
truncation threshold, thus usually generates signiﬁcantly more sparse solutions. RDA also
ensures the O(
T) regret bound.
Hoi, Sahoo, Lu and Zhao
3.5.4 Adaptive Regularization
One major issue with both FOBOS and RDA is that the auxiliary strongly convex function h(w) may not fully exploit the geometry information of underlying data distribution.
Instead of choosing h(w) as an ℓ2-norm 1
2||w||2 in RDA or a Mahalanobis norm || · ||Ht in
FOBOS, proposed a data-driven adaptive regularization for h(w), i.e.,
where Ht = (Pt
2 accumulates the second order info from the previous instances
over time. Replacing the previous h(w) in both RDA and FOBOS by the temporal adaptation function ht(w), derived two generalized algorithms (Ada-RDA
and Ada-FOBOS) with the solutions as follows respectively.
if | ¯gtj| < λ
βHt,jj ( ¯gtj −λsgn( ¯gtj))
Ada-FOBOS:
t+1 = sgn is closely related to sparse online learning in that they both aim to learn an eﬃcient
classiﬁer for very high dimensional data.
However, the sparse learning algorithms aim
to minimize the ℓ-1 regularized loss, while the feature selection algorithms are motivated
to explicitly address the feature selection issue and thus impose a hard constraint on the
number of non-zero elements in classiﬁer. Because of these similarities, they share some
common strategies such as truncation and projection.
3.5.6 Others
Two stochastic methods were proposed in for ℓ1-regularized
loss minimization. The Stochastic Coordinate Descent (SCD) algorithm randomly selects
one coordinate from d dimensions and update this single coordinate with the gradient of
the total loss of all instances. The Stochastic Mirror Descent Made Sparse (SMIDAS) algorithm combines the idea of truncating the gradient with mirror descent algorithm, i.e.,
truncation is performed on the vector in dual space. The disadvantage of the two algorithms
is that their computational complexity depends on the dimensionality d. Besides, the two
algorithms are designed in batch learning setting, i.e., they assume all instances are known
prior to the learning task. Besides, there are also some recent sparse online learning algorithms proposed , which combine the ideas of sparse learning,
second order online learning, and cost-sensitive classiﬁcation together to make the online
algorithms scalable for high-dimensional class-imbalanced learning tasks.
Online Learning: A Comprehensive Survey
3.6 Online Learning with Kernels
We now survey a family of “Kernel-based Online Learning” algorithms for learning a nonlinear target function, where the nonlinearity is induced by kernels. We take the typical
nonlinear binary classiﬁcation task as an example. Our goal is to learn a nonlinear classi-
ﬁer f : Rd →R from a sequence of labeled instances (xt, yt), t = 1, ..., T, where xt ∈Rd
and yt ∈{+1, −1}. We build the classiﬁcation rule as: byt = sgn(f(xt)), where byt is the
predicted class label. We measure the classiﬁcation conﬁdence of certain instance xt by
|f(xt)|. Similar to the linear case, for an online classiﬁcation task, one can deﬁne the hinge
loss function ℓ(·) for the t-th instance using the classiﬁer at the t-th iteration:
ℓ((xt, yt); ft) = max(0, 1 −ytft(xt))
Formally speaking, an online nonlinear learner aims to achieve the lowest regret R(T) after
time T, where the regret function R(T) is deﬁned as follows:
ℓt(ft) −inf
where ℓt(·) is the loss for the classiﬁcation of instance (xt, yt), which is short for ℓ((xt, yt); ·).
We denote by f∗the optimal solution of the second term, i.e., f∗= arg minf
In the following, we ﬁrst introduce online kernel methods and then survey a family of
scalable online kernel learning algorithms organized into two major categories: (i) budget
online kernel learning using budget maintenance strategies and (ii) budget online kernel
learning using functional approximation strategies.
Then we brieﬂy introduce some approaches for online learning with multiple kernels. Without loss of generality, we will adopt
the above online binary classiﬁcation setting for the discussions in this section.
3.6.1 Online Kernel Methods
We refer to the output f of the learning algorithm as a hypothesis and denote the set of
all possible hypotheses by H = {f|f : Rd →R}. Here H a Reproducing Kernel Hilbert
Space (RKHS) endowed with a kernel function κ(·, ·) : Rd × Rd →R implementing the inner product⟨·, ·⟩such that: 1) κ has the reproducing property
⟨f, κ(x, ·)⟩= f(x) for x ∈Rd; 2) H is the closure of the span of all κ(x, ·) with x ∈Rd,
that is, κ(x, ·) ∈H ∀x ∈X. The inner product ⟨·, ·⟩induces a norm on f ∈H in the usual
way: ∥f∥H := ⟨f, f⟩
2 . We denote by Hκ an RKHS with explicit dependence on kernel κ.
Throughout the analysis, we assume κ(x, x) ≤X2, ∀x ∈Rd, X ∈R+ is a constant.
The task of training the model of SVM f(x) in batch is formulated as the optimization:
ℓ(f(xt); yt)
where λ > 0 is a regularization parameter used to control model complexity. According to
the Representer Theorem , the optimal solution of the above convex
optimization problem lies in the span of T kernels, i.e., those centered on the training
Consequently, the goal of a typical online kernel learning algorithm is to learn
Hoi, Sahoo, Lu and Zhao
the kernel-based predictive model f(x) for classifying a new instance x ∈Rd as follows:
t=1 αtκ(xt, x), where T is the number of processed instances, αt denotes the
coeﬃcient of the t-th instance, and κ(·, ·) denotes the kernel function. We deﬁne support
vector (SV) as the instance whose coeﬃcient α is nonzero. Thus, we rewrite the previous
classiﬁer as f(x) = P
i∈SV αiκ(xi, x), where SV is the set of SV’s and i is its index. We use
the notation |SV| to denote the SV set size. In literature, diﬀerent online kernel methods
have been proposed.
We begin by introducing the simplest one, that is, the kernelized
Perceptron algorithm.
Kernelized Perceptron.
This extends the Perceptron algorithm using the kernel trick.
Algorithm 10 outlines the Kernelized Perceptron algorithm .
Algorithm 10: Kernelized Perceptron
INIT: f0 = 0
for t = 1, 2, . . . , T do
Given an incoming instance xt, predict ˆyt = sgn(ft(xt));
Receive the true class label yt ∈{+1, −1};
if ˆyt ̸= yt then
SVt+1 = SVt ∪(xt, yt), ft+1 = ft + ytκ(xt, ·);
The algorithm works similar to the standard Perceptron algorithm, except that the inner
product, i.e., ft(xt) = P
i xt, is replaced by a kernel function in the kernel Percetron,
i.e., ft(xt) = P
Kernelized OGD.
This extends the OGD algorithm with kernels ,
as shown in Algorithm 11. Here, ηt > 0 is the learning rate parameter, and ℓ′
t is used to
denote the derivative of loss function with respect to the classiﬁcation score ft(xt).
Algorithm 11: Kernelized OGD
INIT: f0 = 0
for t = 1, 2, . . . , T do
Given an incoming instance xt, predict ˆyt = sgn(ft(xt));
Receive the true class label yt ∈{+1, −1};
if ℓt(ft) > 0 then
SVt+1 = SVt ∪(xt, yt), ft+1 = ft −ηt∇ℓt(ft(xt)) = ft −ηtℓ′
tκ(xt, ·);
Other Related Work.
The kernel trick implies that the inner product between any two
instances can be replaced by a kernel function, i.e., κ(xi, xj) = Φ(xi)⊤Φ(xj), ∀i, j, where
Φ(xt) ∈RD denotes the feature mapping from the original space to a new D-dimensional
space which can be inﬁnite. Using the kernel trick, many existing linear online learning algorithms can be easily extended to their kernelized variants, such as the kernelized Perceptron
Online Learning: A Comprehensive Survey
and kernelized OGD as well as kernel PA variants . However, some
algorithms that use complex update rules are non-trivial to be converted into kernelized versions, such as Conﬁdence-Weighted algorithms . Moreover, some online
kernel learning methods also attempt to make more eﬀective updates at each iteration. For
example, Double Updating Online Learning (DUOL) improves the eﬃcacy of traditional online kernel learning methods by not only
updating the weight of the newly added SV, but also the weight for one existing SV. Finally,
we note one major challenge of online kernel method is the computational eﬃciency and
scalability due to the curse of kernelization . In the following, we will
discuss two types of techniques to scale up kernel-based online learning methods.
3.6.2 Scalable Online Kernel Learning via Budget Maintenance
Despite enjoying the clear advantage of accuracy performance over linear models, online
kernel learning falls short in some critical drawbacks, in which one critical issue is the
growing unbounded number of support vectors with increasing computational and space
complexity over time. To address this challenge, a family of algorithms, termed “budget
online kernel learning”, have been proposed to bound the number of SV’s with a ﬁxed budget
B = |SV| using diverse budget maintenance strategies whenever the budget overﬂows. The
general framework for budgeting strategies is shown in Algorithm 12. Most existing budget
online kernel methods maintain the budget by three strategies: (i) SV Removal, (ii) SV
Projection, and (iii) SV Merging. We brieﬂy review each of them below.
Algorithm 12: Budget Online Kernel Learning
INIT: f0 = 0
for t = 1, 2, . . . , T do
Given an incoming instance xt, predict ˆyt = sgn(ft(xt));
Receive the true class label yt ∈{+1, −1};
if update is needed then
update the classiﬁer from ft to ft+ 1
2 and SVt+ 1
2 = SVt ∪(xt, yt)
if |SVt+ 1
2 | > B then
Update Support Vector Set from SVt+ 1
2 to SVt+1 such that |SVt+1| = B
Update the classiﬁer from ft+ 1
SV Removal.
This strategy maintains the budget by a simple and eﬃcient way: 1)
update the classiﬁer by adding a new SV whenever necessary (depending on the prediction
mistake/loss); 2) if the SV size exceeds the budget, discard one of existing SV’s and update
the classiﬁer accordingly. To achieve this, we need to address the following concerns: (i)
how to update the classiﬁer and (ii) how to choose one of existing SV’s for removal.
The ﬁrst step of updating classiﬁers depends on which online learning method is used.
For example, the Perceptron algorithm has been used in RBP ,
Hoi, Sahoo, Lu and Zhao
Forgetron , and Budget Perceptron . The OGD
algorithm has been adopted by BOGD and BSGD+ removal , while PA has been used by BPA-S .
The second step of SV removal is to ﬁnd one of existing SV’s, denoted as (xdel, yt), to be
removed by minimizing the impact of the resulting classiﬁer. One simple way is to randomly
discard one of existing SV’s uniformly with probability 1
B, as adopted by RBP and BOGD . Instead of choosing randomly, another way as
used in “Forgetron” is to discard the oldest SV by assuming an older SV
is less representative for the distribution of fresh training data streams. Despite enjoying
the merits of simplicity and high eﬃciency, these methods are often too simple to achieve
satisfactory learning results.
To optimize the performance, some approaches have tried to perform exhaustive search
in deciding the best SV for removal. For instance, the Budget Perceptron algorithm searches for one SV that is classiﬁed with high conﬁdence by the classiﬁer:
ydel(ft+ 1
2 (xdel) −αdelκ(xdel, xdel)) > β
where β > 0 is a ﬁxed tolerance parameter. BPA-S shares the similar idea of exhaustive
search. For every r ∈[B], a candidate classiﬁer fr = ft+ 1
2 −αrκ(xr, ·) is generated by
discarding the r-th SV from ft+ 1
2 . By comparing the B candidate classiﬁers, the algorithm
selects the one that minimizes the current objective function of PA:
ft+1 = argmin
2||fr −ft||2
H + Cℓt(fr)
where C > 0 is the regularization parameter of PA to balance aggressiveness and passiveness.
Comparing the principles of diﬀerent SV removal strategies, we observe that a simple rule
may not always generate satisfactory accuracy, while an exhaustive search often incurs
non-trivial computational overhead, which again may limit the application to large-scale
problems. When deploying a solution in practice, one would need to balance the trade-oﬀ
between eﬀectiveness and eﬃciency.
SV Projection.
SV Projection strategy ﬁrst appeared in where
two new algorithms, Projectron and Projectron++, were proposed, which signiﬁcantly outperformed the previous SV removal based algorithms such RBP and Forgetron. The SV
projection method follows the setting of SV removal and identiﬁes a support vector for removal during the update of the model. It then chooses a subset of SV as the projection base,
which will be denoted by P. Following this, a linear combination of kernels in P is used to
approximate the removed SV. The procedure of ﬁnding the optimal linear combination can
be formulated as a convex optimization of minimizing the projection error:
β = argmin
Eproj = argmin
||αdelκ(xdel, ·) −
βiκ(xi, ·)||2
Finally, the classiﬁer is updated by combining this result with the original classiﬁer:
ft+1 = ft+ 1
2 −αdelκ(xdel, ·) +
βiκ(xi, ·)
Online Learning: A Comprehensive Survey
There are several algorithms adopting the projection strategy, for example Projectron,
Projectron++, BPA-P, BPA-NN and BSGD+Project . These methods diﬀer in a few aspects. First, the update rules are based on
diﬀerent online learning algorithms. Generally speaking, PA based and OGD based tend
to outperform Perceptron based algorithms because of their eﬀective update. Second, the
choice of discarded SV is diﬀerent. Since projection itself is relative slow, exhaustive search
based algorithms (BPA-NN, BPA-P) are extremely time consuming. Thus algorithms with
simple selecting rules are prefered (Projectron, Projectron++, BSGD+Project).
the choice of projection base set P is diﬀerent.
In Projectron, Projectron++, BPA-P
and BSGD+Project, the discarded SV is projected onto the whole SV set, i.e. P = SV.
While in BPA-NN, P is only a small subset of SV, made up of the nearest neighbors of the
discarded SV (xdel, ydel). In general, a larger projection base set implies a more complicated
optimization problem and thus more time costs. The research direction of SV projection
based budget learning is to ﬁnd a proper way of selecting P so that the algorithm achieves
the minimized projection error with a relative small projection base set.
SV Merging.
Wang et al. proposed a SV merging method called “BSGD+Merge”
which replaces the sum of two SV’s αmκ(xm, ·)+αnκ(xn, ·) by a newly created SV αzκ(z, ·),
where αm, αn and αz are the corresponding coeﬃcients of xm, xn and z. Following the
previous discussion, the goal of online budget learning through SV merging strategy is to
ﬁnd the optimal αz ∈R and z ∈Rd that minimizes the gap between ft+1 and ft+ 1
As it is relatively complicated to optimize the two terms simultaneously, the optimization
is divided into two steps. First, assuming the coeﬃcient of z is αm + αn, this algorithm
tries to create the optimal SV that minimizes the merging error as follows
||(αm + αn)κ(z, ·) −(αmκ(xm, ·) + αnκ(xn, ·))||
The solution is z = hxm + (1 −h)xn, where 0 < h < 1 is a real number that can be found
by a line search method. This solution indicates that the optimal created SV lies on the
line connecting xm and xn. After obtaining the optimal created SV z, the next step is to
ﬁnd the optimal coeﬃcient αz, which can be formulated as
αz ||(αzκ(z, ·) −(αmκ(xm, ·) + αnκ(xn, ·))||.
The solution becomes αz = αmκ(xm, z) + αnκ(xn, z). The remaining problem is which two
SV’s xm and xn should be merged. The ideal solution is to ﬁnd the optimal pair with
the minimal merging error through exhaustive search, which however requires O(B2) time
complexity. How to ﬁnd the optimal pair eﬃciently remains an open challenge.
Among various algorithms of budget online kernel learning using budget maintenance, the key diﬀerences are their updating rules and budget maintenance strategies.
Table 1 gives a summary of diﬀerent algorithms and their properties. In addition to the
previous budget kernel learning algorithms, there are also some other works in online kernel
learning. For example, some studies introduce the
idea of sparse kernel learning to reduce the number of SV’s in the online-to-batch-conversion
problem, where an online algorithm can be used to train a kernel model eﬃciently for the
batch setting (See Section 3.7).
Hoi, Sahoo, Lu and Zhao
Table 1: Comparisons of diﬀerent budget online kernel learning algorithms.
Algorithms
Update Strategy
Budget Strategy
Update Time
Stoptron 
Perceptron
Tighter Perceptron 
Perceptron
Tightest Perceptron 
Perceptron
Budget Perceptron 
Perceptron
RBP 
Perceptron
Forgetron 
Perceptron
BOGD 
BPA-S 
BSGD+removal 
Projectron 
Perceptron
Projection
Projectron++ 
Perceptron
Projection
BPA-P 
Projection
BPA-NN 
Projection
BSGD+projection 
Projection
BSGD+merging 
3.6.3 Scalable Online Kernel Learning via Functional Approximation
In contrast to the previous budget online kernel learning methods using budget maintenance
strategies to guarantee eﬃciency and scalability, another emerging and promising strategy is
to explore functional approximation techniques for achieving scalable online kernel learning
 .
The key idea is to construct a kernel-induced feature representation z(x) ∈RD such
that the inner product of instances in the new feature space can eﬀectively approximate the
kernel function:
κ(xi, xj) ≈z(xi)⊤z(xj)
Using the above approximation, the predictive model with kernels can be rewritten as
αiκ(xi, x) ≈
αiz(xi)⊤z(x) = w⊤z(x)
where w = PB
i=1 αiz(xi) denotes the weight vector to be learned in the new feature space.
As a consequence, solving a regular online kernel classiﬁcation task can be turned into
a linear online classiﬁcation task on the new feature space derived from the kernel approximation. For example, the methods of online kernel learning with kernel approximation
in integrate some existing online learning algorithms
(e.g., OGD) with kernel approximation techniques to derive scalable online kernel learning algorithms, including Fourier Online Gradient Descent (FOGD) that explores random Fourier features
for kernel functional approximation , and Nystr¨om Online Gradient Descent (NOGD) that explores Nystr¨om low-rank matrix approximation methods for
approximating large-scale kernel matrix . A recent work, Dual
Space Gradient Descent updates the model as the RBP
algorithm, but also builds an FOGD model using the discarded SV’s. The ﬁnal prediction
is the combination of the two models.
Online Learning: A Comprehensive Survey
3.6.4 Online Multiple Kernel Learning
Traditional online kernel methods usually assume a predeﬁned good kernel is given prior to
the online learning task. Such approaches could be restricted since it is often hard to choose
a good kernel prior to the learning task. To overcome the drawback, Online Multiple Kernel
Learning (OMKL) aims to combine multiple kernels automatically for online learning tasks
without ﬁxing any predeﬁned kernel. In the following, we begin by introducing some basics
of batch Multiple Kernel Learning (MKL) .
Given a training set D = {(xt, yt), t = 1, . . . , T} where xt ∈Rd, yt ∈{−1, +1}, and a
set of m kernel functions K = {κi : X × X →R, i = 1, . . . , m}. MKL learns a kernel-based
prediction function by identifying an optimal combination of the m kernels, denoted by
θ = (θ1, . . . , θm), to minimize the margin-based classiﬁcation error, which can be cast into
the optimization below:
ℓ(f(xt), yt)
where ∆= {θ ∈Rm
+|θ⊤1m = 1}, K(θ)(·, ·) = Pm
i=1 θiκi(·, ·), ℓ(f(xt), yt) = max(0, 1 −
ytf(xt)). In the above, we use notation 1T to represent a vector of T dimensions with all
its elements being 1. It can also be cast into the following mini-max optimization problem:
where Ki ∈RT×T with Ki
j,l = κi(xj, xl), Ξ = {α|α ∈[0, C]T }, and ◦deﬁnes the elementwise product between two vectors. The above batch MKL optimization has been extensively
studied , but an eﬃcient solution remains an
open challenge.
Some eﬀorts of online MKL studies have attempted
to solve batch MKL optimization via online learning. Unlike these approaches that are
mainly concerned in optimizing the optimal kernel combination as regular MKL, another
framework of Online Multiple Kernel Learning (OMKL) is focused on exploring eﬀective online combination of multiple kernel
classiﬁers via a signiﬁcantly more eﬃcient and scalable way. Speciﬁcally, the OMKL in
 learns a kernel-based prediction function by selecting a
subset of predeﬁned kernel functions in an online learning fashion, which is in general more
challenging than typical online learning because both the kernel classiﬁers and the subset of
selected kernels are unknown, and more importantly the solutions to the kernel classiﬁers
and their combination weights are correlated. proposed novel algorithms
based on the fusion of two types of online learning algorithms, i.e., the Perceptron algorithm
that learns a classiﬁer for a given kernel, and the Hedge algorithm that combines classiﬁers by linear weights. Some stochastic selection strategies were
also proposed by randomly selecting a subset of kernels for combination and model updating
to further improve the eﬃciency. These methods were later extended for regression , learning from data with time-sensitive patterns and
imbalanced data streams .
In addition, there have been budgeting
approaches to make OMKL scalable .
Hoi, Sahoo, Lu and Zhao
3.7 Online to Batch Conversion
Let us denote by A an online learning algorithm for the purpose of training a binary
classiﬁer from a sequence of training examples. On each round, the algorithm receives an
instance xt ∈Rd, the algorithm chooses a vector wt ∈S ⊆Rd to predict the class label
of the instance, i.e., ˆyt = sgn(w⊤
t xt). After that, the environment responds by disclosing
the true label yt and some convex loss function ℓ(w; (xt, yt), and the algorithm suﬀers a
loss ℓt(wt) at the end of this round. For such a setting, consider a sequence of T rounds
(x1, y1), . . . , (xT , yT ), the online algorithm aims to minimize the following regret
ℓ(wt; (xt, yt)) −min
ℓ(w; (xt, yt))
However, for batch training setting, we are more interested in ﬁnding a model ˆw with good
generalization ability, i.e., we want to achieve a small excess risk deﬁned as
R( ˆw) −min
where the generalization risk is R(w) = E(x,y)[ℓ(w; (x, y))], and (x, y) satisﬁes a ﬁxed unknown distribution. Therefore, we would like to study the generalization performance of
online algorithms through the Online to Batch Conversion , where the conversion relates the regret
of the online algorithm to its generalization performance.
3.7.1 A General Conversion Theory
We now consider the generalization ability of online learning under the assumption that the
loss ℓ(w; (x, y)) is strongly convex, which is reasonable as many loss functions (e.g., square
loss) are strongly convex, and even if some loss (e.g., hinge loss) is not strongly convex, we
can impose some regularization term (e.g., 1
2∥· ∥) to achieve strong convexity. We denote
the dual norm of ∥· ∥as ∥· ∥∗, where ∥v∥∗= sup∥w∥≤1 v⊤w. Let Z = (x, y) be a random
variable taking values in some space Z. Our goal is to minimize R(w) = EZ[ℓ(w; Z)] over
w ∈S. Speciﬁcally, we assume the loss ℓ: S ×Z →[0, B] satisﬁes the following assumption:
Assumption LIST(LIpschitz and STrongly convex assumption) For all z ∈Z, the
function ℓz(w) = ℓ(w; z) is convex in w and satisﬁes:
1. ℓz has Lipschitz constant L w.r.t. the norm ∥· ∥, i.e., |ℓz(w) −ℓz(w′) ≤L∥w −w′∥.
2. ℓz is λ-strongly convex w.r.t. ∥· ∥, i.e., ∀θ ∈ , ∀w, w′ ∈S,
ℓz(θw + (1 −θ)w′) ≤
θℓz(w) + (1 −θ)ℓz(w′) −λ
2 θ(1 −θ)∥w −w′∥2.
For this kind of loss function, we consider an online learning setting where Z1, . . . , ZT are
given sequentially in i.i.d. We then have
E[ℓ(w; Zt)] = E[ℓ(w, (xt, yt))] := R(w), ∀t, w ∈S.
Online Learning: A Comprehensive Survey
Now consider an online learning algorithm A, which is initialized as w1.
Whenever Zt
is given, model wt is updated to wt+1.
Let Et[·] denote conditional expectation w.r.t.
Z1, . . . , Zt, we have Et[ℓ(wt; Zt)] = R(wt). Using the above assumptions and the Freedman’s
inequality leads to the following theorem for the generalization ability of online learning
Theorem 1 ) Under the assumption LIST, we have the following inequality, with probability at least 1 −4δ ln T,
R(wt) −R(w∗) ≤RegA(T)
where w∗= arg minw∈S R(w). Further, using Jensen’s inequality,
t R(wt) can be replaced by R( ¯wT ) where ¯wT = 1
If the assumption LIST is satisﬁed by ℓz(w), then the Online Gradient Descent (OGD)
algorithm that generates w1, . . . , wT has the following regret RegA(T) ≤
2λ(1 + ln T).
Plugging this inequality back into the theorem and using (1+ln T)/(2T) ≤ln T/T, ∀T ≥3
gives the following Corollary.
Corollary 2 Suppose assumption LIST holds for ℓz(w). Then the Online Gradient Descent (OGD) algorithm that generates w1, . . . , wT and ﬁnally outputs ¯wT = 1
t wt, satis-
ﬁes the following inequality for its generalization ability, with probability at least 1−4δ ln T,
R( ¯wT ) −R(w∗) ≤L2 ln T
for any T ≥3, where w∗= arg minw∈S R(w).
3.7.2 Other Conversion Theories
Online to batch conversion has been studied in literature . For general
convex loss functions, Cesa-Bianchi et al. proved the following generalization ability
of online learning algorithm with probability at least 1 −δ
R( ¯wT ) ≤1
ℓ(wt; zt) +
δ = RegA(T)
ℓ(w; zt) +
where the loss ℓ≤1. Zhang is another work that explicitly goes by the exponential
moment method to drive sharper concentration results.
In addition, Cesa-Bianchi and
Gentile improved their initial generalization bounds using Bernstein’s inequality by
assuming ℓ(·) ≤1, and proves the following inequality with probability at least 1 −δ
ℓ(wt; zt) + O
ln(T 2/δ)
ℓ(wt; zt)ln(T 2/δ)
where ˆw is selected from w1, . . . , wT , by minimizing a speciﬁcally designed penalized empirical risk.
In particular, the generalization risk converges to
t=1 ℓ(wt; zt) at rate
ln T 2/T) and vanishes at rate O(ln T 2/T) whenever the loss PT
t=1 ℓ(wt; zt) is O(1).
Hoi, Sahoo, Lu and Zhao
4. Applied Online Learning for Supervised Learning
4.1 Overview
In this section, we survey the most representative algorithms for a group of non-traditional
online learning tasks, wherein the supervised online algorithms cannot be used directly.
These algorithms are motivated by new problem settings and applications which follow the
traditional online setting, where the data arrives in a sequential manner. However, there
was a need to develop new algorithms which were suited to these scenarios. Our review
includes cost-sensitive online learning, online multi-task learning, online multi-view learning,
online transfer learning, online metric learning, online collaborative ﬁltering, online learning
structured prediction, distributed online learning, online learning with neural networks, and
online portfolio selection.
4.2 Cost-Sensitive Online Learning
In a supervised classiﬁcation task, traditional online learning methods are often designed
to optimize mistake rate or equivalently classiﬁcation accuracy. However, it is well-known
that classiﬁcation accuracy becomes a misleading metric when dealing with class-imbalanced
data which is common for many real-world applications, such as anomaly detection, fraud
detection, intrusion detection, etc. To address this issue, cost-sensitive online learning represents a family of online learning algorithms that are designed to take care
of diﬀerent misclassiﬁcation costs of diﬀerent classes in a class-imbalanced classiﬁcation
task. Next, we brieﬂy survey these algorithms.
Perceptron Algorithms with Uneven Margin (PAUM)
PAUM is a
cost-sensitive extension of Perceptron and the Perceptron with Margins
(PAM) algorithms . Perceptron makes an update only when
there is a mistake, while PAM tends to make more aggressive updates by checking the
margin instead of mistake. PAM makes an update whenever ytw⊤
t xt ≤τ, where τ ∈R+
is a ﬁxed parameter controlling the aggressiveness. To deal with class imbalance, PAUM
extends PAM via an uneven margin setting, i.e., employing diﬀerent margin parameters
for the two classes: τ+ and τ−.
Consequently, the update becomes ytw⊤
t xt ≤τyt.
properly adjusting the two parameters, PAUM achieves cost-sensitive updating eﬀects for
diﬀerent classes. One of major limitations with PAUM is that it does not directly optimize a
predeﬁned cost-sensitive measure, thus, it does not fully resolve the cost-sensitive challenge.
Cost-sensitive Passive Aggressive (CPA)
CPA was proposed
as a cost-sensitive variant of the PA algorithms. It was originally designed for multi-class
classiﬁcation by the following prediction rule: byt = arg maxy(wtΦ(xt, y)), where Φ is a
feature mapping function that maps xt to a new feature according to the class y.
simplicity, we restrict the discussion on the binary classiﬁcation setting. Using Φ(x, y) =
2yx, we will map the formulas to our setting. The prediction rule is: byt = sgn(w⊤
deﬁne the cost-sensitive loss as
ℓ(w, x, y) = w · Φ(x, by) −w · Φ(x, y) +
where ρ(y1, y2) is the function deﬁne to distinguish the diﬀerent cost of diﬀerent kind misclassiﬁcations and we have assumed ρ(y, y) = 0. When being converted to binary setting,
Online Learning: A Comprehensive Survey
the loss becomes
ℓ(w, x, y) =
The mistake depends on the prediction conﬁdence and the loss type. We omit the detailed
update steps since it follows the similar optimization as PA learning as discussed before.
Similar to PAUM, this algorithm also is limited in that it does not optimize a cost-sensitive
measure directly.
Cost-Sensitive Online Gradient Descent (CSOGD)
Unlike traditional OGD algorithms that often optimize accuracy, CSOGD 
applies OGD to directly optimize two cost-sensitive measures:
(1) maximizing the weighted sum of sensitivity and specificity, i.e, sum = ηp×sensitivity+
ηn × specificity, where the two weights satisfy 0 ≤ηp, ηn ≤1 and ηp + ηn = 1.
(2) minimizing the weighted misclassification cost, i.e., cost = cp × Mp + cn × Mn,
where Mp and Mn are the number of false negatives and false positives respectively,
0 ≤cp, cn ≤1 are the cost parameters for positive and negative classes, respectively,
and we assume cp + cn = 1.
The objectives can be equivalently reformulated into the following objective:
ρI(ytw·xt<0) +
I(ytw·xt<0)
where we set ρ = ηpTn
ηnTp when maximizing the weighted sum, Tp and Tn are the number of
positive and negative instances respectively; when minimizing the weighted misclassiﬁcation
cost, we instead set ρ = cp
cn . The objective is however non-convex, making it hard to optimize
directly. Instead of directly optimizing the non-convex objective, we attempt to optimize a
convex surrogate. Speciﬁcally, we replace the indicator function I(·) by a convex surrogate,
and attempt to optimize either one of the following modiﬁed hinge-loss functions at each
online iteration:
ℓI(w; (x, y)) = max(0, ρ ∗I(y=1) + I(y=−1) −y(w · x))
ℓII(w; (x, y)) = (ρ ∗I(y=1) + I(y=−1)) ∗max(0, 1 −y(w · x))
One can then derive cost-sensitive ODG (CSOGD) algorithms by applying OGD to optimize
either one of the above loss functions. The detailed algorithms can be found in . Two recent works extend the problem setting to cost-sensitive classiﬁcation
of multi-class problem . Further there are eﬀorts
to do cost-sensitive online learning with kernels .
Online AUC Maximization
Instead of optimizing accuracy, some online learning studies have attempted to directly optimize the Area Under the ROC curve (AUC), i.e.,
Hoi, Sahoo, Lu and Zhao
where x+ is a positive instance, x−is a negative instance, T+ is the total number of positive
instances and T−is the total number of negative instances. AUC measures the probability
for a randomly drawn positive instance to have a higher decision value than a randomly
sampled negative instance, and it is widely used in many applications. Optimizing AUC
online is however very challenging.
First of all, in the objective, the term PT+
j is non-convex. A common
way is to replace the indicator function by a convex surrogate, e.g., a hinge loss function
j ) = max{0, 1 −w(x+
Consequently, the goal of online AUC maximization is equivalent to minimizing the accumulated loss Lt(w) over all previous iterations, where the loss at the t-th iteration is
Lt(w) = Iyt=1
Iyτ=−1ℓ(w, xt −xτ) + Iyt=−1
Iyτ=1ℓ(w, xτ −xt)
The above takes the sum of the pairwise hinge loss between the current instance (xt, yt) and
all the received instances with the opposite class −yt. Despite being convex, it is however
impractical to directly optimize the above objective in online setting since one would need
to store all the received instances and thus lead to growing computation and memory cost.
The Online AUC Maximization method in proposed a novel idea of
exploring reservoir sampling techniques to maintain two buﬀers, B+ and B−of size N+ and
N−, which aim to store a sketch of historical instances. Speciﬁcally, when receiving instance
(xt, yt), it will be added to buﬀer Byt whenever it is not full, i.e. |Byt| < Nyt. Otherwise, xt
randomly replaces one instance in the buﬀer with probability
yt , where Nt+1
is the total
number of instances with class yt received so far. Reservoir sampling is able to guarantee
the instances in the buﬀers maintain an unbiased sampling of the original full dataset. As a
result, the loss Lt(w) can be approximated by only considering the instances in the buﬀers,
and the classiﬁer w can be updated by either OGD or PA algorithms.
Others. To improve the study in , a number of following studies
have attempted to make improvements from diﬀerent aspects . For example,
the study in generalized online AUC maximization as online learning
with general pairwise loss functions, and oﬀered new generalization bounds for online AUC
maximization algorithms similar to . The bounds were further improved
by which employs a generic decoupling technique to provide Rademacher
complexity-based generalization bounds. In addition, the work in overcomes the buﬀering storage cost by developing a regression-based algorithm which only
needs to maintain the ﬁrst and second-order statistics of training data in memory, making the resulting storage requirement independent from the training size. The very recent
work in presented a new second-order AUC maximization method by
improving the convergence using the adaptive gradient algorithm. The stochastic online
AUC maximization (SOLAM) algorithm formulates the online AUC
maximization as a stochastic saddle point problem and greatly reduces the memory cost.
Online Learning: A Comprehensive Survey
4.3 Online Multi-task Learning
Multi-task Learning is an approach that learns a group of related machine
learning tasks together. By considering the relationship between diﬀerent tasks, multi-task
learning algorithms are expected to achieve better performance than algorithms that learn
each task individually. Batch multi-task learning problems are usually solved by transfer
learning methods which transfer the knowledge learnt from one task
to another similar tasks.
In Online Multi-task Learning (OML) , however, the tasks have to be solved in parallel with instances arriving
sequentially, which makes the problem more challenging.
During time t, each of the task i ∈{1, ...K} receives an instance xi,t ∈Rdi, where di
is the feature dimension of task i. The algorithm then makes a prediction for each task i
based on the current model wi,t as ˆyi,t = sign(w⊤
i,txi,t). After making the prediction, the
true labels yi,t are revealed and we get a loss function vector ℓi,t ∈RK
+. Finally, the models
are updated by considering the loss vector and task relationship.
A straightforward baseline algorithm is to parallel update all the classiﬁers wi, i ∈
{1, ..., K}. OML algorithm should utilize the relationships between tasks to achieve higher
accuracy compared with the baseline. The multitask Perceptron algorithm is a pioneering work of OML that considers the inter-task relationship. Assuming that
a matrix A ∈RK∗K is known and ﬁxed, we can update the modePrasanthi Nairl i when an
instance xj,t for task j is received as follows:
wi,t+1 = wi,t + yj,tA−1
Other approaches in learned to optimize the relationship matrix, which also oﬀers the ﬂexibility of using a time-varying relationship.
Apart from learning the relationship explicitly, another widely used approach in OML
ﬁeld is to add some structure regularization terms to the original objective function . For example, we
may assume that each model is made up of two parts, a shared part across all tasks w0 and
an individual part vi, i.e., wi = w0 + vi where the common part helps to take advantage
of the task similarity. Now the regularized loss becomes
(ℓi,t + ||vi||2
2) + λ||w0||2
It was also improved using more complex inter-task relationship .
4.4 Online Multi-view Learning
Multi-view learning deals with problems where data are collected from diverse domains or
obtained from various feature extractors. By exploring features from diﬀerent views, multiview learning algorithms are usually more eﬀective than single-view learning. In literature,
there many surveys that oﬀer comprehensive summary of state-of-the-art methods in multiview learning in batch setting , while few works
tried to address this problem in online setting.
Hoi, Sahoo, Lu and Zhao
Two-view PA
We ﬁrst introduce a seminal work, the two-view online passive aggressive learning (Two-view PA) algorithm , which is motivated by the
famous single-view PA algorithm and the two-view SVM algorithm
 in batch setting.
During each iteration t, the algorithm receives an instance (xA
t , yt), where xA
is the feature vector in the ﬁrst view, xB
t ∈Rm is for the second view and yt ∈{1, −1} is
the label. The goal is to learn two classiﬁers wA ∈Rn and wB ∈Rm, each for one view,
and make accuracy prediction with their combination
ˆyt = sign(wA · xA + wB · xB).
Thus the hinge loss at iteration t is redeﬁned as
t ) = max(0, 1 −1
2yt(wA · xA
t + wB · xB
In the single-view PA algorithm, the objective function in each iteration is a balance between
two desires: minimizing the loss function at the current instance and minimizing the change
made to the classiﬁer.
While to utilize the special information in the multi-view data,
an additional term that measures the agreement between two terms is added. Thus, the
optimization is as follows,
t+1) = arg minwA,wB 1
t ) + γ|ytwA · xA
t −ytwB · xB
where γ and C are weight parameters. Fortunately, this optimization problem has a closed
form solution.
Other related works:
Other than solving classiﬁcation tasks, online multi-view learning
has been explored for solving similarity learning or distance metric learning, such as Online
multimodal deep similarity learning and online multi-modal distance
metric learning .
4.5 Online Transfer Learning
Transfer learning aims to address the machine learning tasks of building models in a new
target domain by taking advantage of information from another existing source domain
through knowledge transfer. Transfer learning is important for many applications where
training data in a new domain may be limited or too expensive to collect. There are two
diﬀerent problem settings, homogeneous setting where the target domain shares the same
feature space as the old/source one, and heterogeneous setting where the feature space of
the target domain is diﬀerent from that of the source domain. Although several surveys
on transfer learning are available , most of the referred
algorithms are in batch setting.
Online Transfer Learning (OTL) algorithms aim to learn a classiﬁer f : Rd →R from
a well-trained classiﬁer h : Rd′ →R in the source domain and a group of sequentially
arriving instances xt ∈Rd, t = 1, ..., T in the target domain. For conciseness, we will use
the previous notations for the online classiﬁcation task. We ﬁrst introduce a pioneer work
of OTL .
Online Learning: A Comprehensive Survey
Homogeneous Setting
One key challenge of this task is to address the concept drifting
issue that often occurs in this scenario. The algorithm in homogeneous setting (d = d′)
is based on the ensemble learning approach. At time t, an instance xt is received. The
algorithm makes a prediction based on the weighted average of the classiﬁer in the source
domain h(xt) and the current classiﬁer in the target domain ft(xt),
ˆyt = sgn(w1,tΠ(h(xt)) + w2,tΠ(ft(xt)) −1
where w1,t > 0, w2,t > 0 are the weights for the two functions and need to be updated
during each iteration. Π is a normalization function, i.e. Π(a) = max(0, min(1, a+1
In addition to updating the function ft by using some online learning algorithms, the
weights w1,t and w2,t should also be updated. One suggested scheme is
w1,t+1 = Ctw1,t exp(−ηℓ∗(h)),
w2,t+1 = Ctw2,t exp(−ηℓ∗(ft))
where Ct is a normalization term to keep w1,t+1+w2,t+1 = 1 and ℓ∗(g) = (Π(g(xt))−Π(yt))2.
Heterogeneous Setting
Since heterogeneous OTL is generally very challenging, we consider one simpler case where the feature space of the source domain is a subset of that of
the target domain. Without loss of generality, we assume the ﬁrst d′ dimensions of xt represent the old features, denoted as x(1)
∈Rd′. The other dimensions form a feature vector
∈Rd−d′. The key idea is to adopt a co-regularization principle of online learning two
classiﬁers f(1)
simultaneously from the two views, and predict an unseen example
on the target domain by
The function from source domain h(x(1)) is used to initialize f(1)
. The update strategy at
2 ||f(1) −f(1)
2 ||f(2) −f(2)
where γ1, γ2 and C are positive parameters and ℓt is the hinge loss.
Other Related Work
Multi-source Online Transfer Learning (MSOTL) solves a more challenging problem where k classiﬁers h1, ...hk are provided by k sources. The goal is to learn the optimal combination of the k classiﬁers and the
online updated classiﬁer ft. A naive solution is to construct a new d + k dimensional feature representation x′
t = [xt, h1(xt), ..., hk(xt)] and the online classiﬁer in this new feature
space. An extension of MSOTL aims to deal with transfer learning problem
under two disadvantageous assumptions, negative transfer where instead of improving performance, transfer learning from highly irrelevant sources degrades the performance on the
target domain, and imbalanced distributions where examples in one class dominate. The
Co-transfer Learning algorithm considers the transfer learning
problem not only in multi-source setting but also in the scenario where a large group of
instances are unlabeled.
Hoi, Sahoo, Lu and Zhao
4.6 Online Metric Learning
Distance metric learning (DML) or similarity learning is an important problem in machine learning, which enjoys many real-world applications, such as image retrieval, classiﬁcation and clustering. The goal of classic DML is to
seek a distance matrix A ∈Rd×d that deﬁnes the Mahalanobis distance between any two
instances xi ∈Rd and xj ∈Rd
dA(xi, xj) = (xi −xj)⊤A(xi −xj) = ||Wxi −Wxj||2
Typically, matrix A ⪰0 is required to be symmetric positive semi-deﬁnite, i.e., there exist
a matrix W ∈Rd×d such that A = W ⊤W. It is often hard to collect training data with
the exact true values of distances. Therefore, there are two types of problem settings for
online DML: 1) Pairwise data, where at each round t the learner receives a pair of instances
t ) and a label yt which is +1 if the pair is similar and −1 otherwise; 2) Triple data,
where at each round t the learner receives a triple (xt, x+
t ), with the feedback that
t ) > dA(xt, x−
t ). The goal of online learning is to minimize the accumulated loss
during the whole learning process PT
t=1 ℓt(A), where ℓt is the loss suﬀered from imperfect
prediction at round t. When evaluating the output model for online-to-batch-conversion,
we may use the metric in information retrieval to evaluate the actual performance, such as
mean average precision (mAP) or precision-at-top-k.
Below, we brieﬂy introduce a few representative work for DML in online setting.
Pseudo-metric Online Learning (POLA)
The POLA algorithm learns the distance matrix A from a stream of pairwise data. The loss at time t is an
adaptation of the hinge loss
ℓt(A, b) = max{0, 1 −yt(b −dA(x1
where b is the adaptive threshold value for similarity and will be updated incrementally
along with matrix A. We denote (A, b) ∈Rd2+1 as the new variable to learn. The update
strategy mainly follows the PA approach
2 ) = arg min
(A,b) ||(A, b) −(At, bt)||2
s.t. ℓt(A, b) = 0
The solution (At+ 1
2 ) ensures correct prediction to current pair while makes the minimal
change to the previous model. Then, the algorithm projects this solution to the feasible
space {(A, b) : A ⪰0, b ≥1} to obtain the updated model (At+1, bt+1). Like PA, one can
generalize POLA to soft-margin variants to be robust to noise.
Another similar work named Online Regularized Metric Learning is
simpler due to the adoption of ﬁxed threshold, and adopts the following loss function
ℓt(A) = max(0, b −yt(1 −dA(x1
whose gradient is
∇ℓt(A) = yt(x1
At time t, if the prediction is incorrect, the algorithm updates the matrix A by projecting
the OGD updated matrix into the positive deﬁnite space.
Online Learning: A Comprehensive Survey
Information Theoretic Metric Learning (ITML).
In the above algorithms, distances
between two matrices At and A are usually deﬁned using the Frobenius norm, i.e. ||At−A||2
The ITML algorithm adopts a diﬀerent deﬁnition from
an information theoretic perspective. Given a Mahalanobis distance parameterized by A,
its corresponding multivariate Gaussian distribution is p(x, A) = 1
2dA(x, µ)). The
diﬀerence between matrices is deﬁned as the KL divergence between two distributions.
Assuming all distributions have the same mean, the KL divergence can be calculated as
KL(p(x; A), p(x, At)) = tr(AA−1
t ) −log det(AA−1
Similar to the PA update strategy, during time t the matrix is updated by optimizing
At+1 = arg min
A⪰0 KL(p(x; A), p(x, At)) + ηℓt(A)
where η > 0 is a regularization parameter. This optimization has a closed-form solution.
Online Algorithm for Scalable Image Similarity Learning (OASIS)
algorithm learns a bilinear similarity matrix W ∈Rd from a stream of triplet data, where
the bilinear similarity measure between two instances is deﬁned as
SW (xi, xj) = x⊤
During time t, one triplet (xt, x+
t ) is received. Ideally, we expect the xt is more similar
t than to x−
t , i.e. SW (xt, x+
t ) > SW (xt, x−
t ). Similar to the PA algorithm, for a large
margin, the loss function is deﬁned as the hinge loss
ℓt(W) = max{0, 1 −SW (xt, x+
t ) + SW (xt, x−
The optimization problem to solve for updating Wt is
Wt+1 = arg min
2||W −Wt||2
F + Cξs.t.
ℓt(W) ≤ξ and ξ ≥0
where C is the parameter controlling the trade-oﬀ. The OASIS algorithm diﬀers from the
previous work in several aspects. First, it does not require the similarity matrix W to be
positive semi-deﬁnite and thus saves computational cost for the projection step. The bilinear
similarity matrix may be better than the Mahalanobis distance for some applications. Third,
the triplet data may be easier to collect in some applications.
Most of the previous methods assume a linear proximity function, 
overcomes the limitaiton using a kernelized approach for metric learning. Another approach
in performs sparse online metric learning for very high dimensional
data. There is also some work that solves the online similarity learning in an active learning
setting , which signiﬁcantly reduces the cost of collecting labeled data.
Some work in also applied techniques of the online similarity
learning for real-world applications of mobile application recommendation and tagging.
The series of work in proposed the online multi-modal
distance metric learning algorithms which learn distance metrics in multiple modalities,
enabling multimedia information retrieval applications.
Hoi, Sahoo, Lu and Zhao
4.7 Online Collaborative Filtering
Collaborative Filtering (CF) is an important learning
technique for recommender systems. Diﬀerent from content-based ﬁltering techniques, CF
algorithms usually require minimal knowledge about the features of items or users apart
from the previous preferences.
The fundamental assumption of CF is that if two users
rate many items similarly, they expect to share common preference on some other items.
Several survey papers in gave detailed reviews
of regular CF techniques. However, most of them assume batch settings. Below we introduce
basics of CF and then review several popular online algorithms for CF tasks.
An online CF algorithm works on a sequence of observed ratings given by n users to m
items. At time t ∈{1, 2, ..., T}, the algorithm receives the index of a user u(t) ∈{1, 2, ...n}
and the index of an item i(t) ∈{1, 2, ...m} and makes a prediction of the rating ˆr(t)
based on the knowledge of the previous ratings. Then the real rating r(t)
u,i ∈R is revealed and
the algorithm updates the model based on the loss suﬀered from the imperfect prediction,
denoted as ℓ(ˆr(t)
u,i). The goal of online CF is to minimize the Root Mean Square Error
(RMSE) or Mean Absolute Error(MAE) along the whole learning process, deﬁned as follows:
u,i −ˆr(t)
u,i −ˆr(t)
CF techniques are generally categorized into two types: memory-based methods and
model-based methods. Below brieﬂy introduces some popular algorithms in each category.
Memory-Based CF Methods.
This follows the instance-based learning paradigm:
1. Calculate the similarity score between any pairs of items. For example, the cosine
similarity between item i and item j is deﬁned as,
u∈Ui∩Uj rui · ru,j
where Ui denotes the set of users that have rated item i.
2. For each item i, ﬁnd its k nearest neighbor set Ni based on the similarity score.
3. Predict the rating ru,i as the weighted average of ratings from user u to the neighbors
of item j, where the weight is proportional to the similarity.
We name the above described algorithm as item-based CF, while similarly, the predictions
may also be calculated as the weighted average of ratings from similar users, which is called
user-based CF method. Memory-based CF methods were used for some early generation
recommendation systems, but very few is online learning approach. One reason is because of
data sparsity, as the similarity score Si,j is only available when there is at least one common
user that rates the two items i and j, which might be unrealistic during the beginning
stage. Another challenge is the large time consumption when updating the large number
of similarity scores incrementally with the arrival of new ratings. The Online Evolutionary
Collaborative Filtering algorithm provides an eﬃcient similarity score
updating method to address this problem.
Online Learning: A Comprehensive Survey
Model-Based CF Methods
Memory-based online CF methods suﬀer two limitations,
i.e., sensitivity due to data sparsity and ineﬃciency for similarity score update. To address
these issues, extensive work hasbeen focused on model-based CF algorithms. One of the
most successful approaches is the matrix factorization methodology ,
which assumes the rating by a user to an item is determined by k potential features, k ≪
Thus each user u can be represented by a vector uu ∈Rk, and each item i can
be represented by a vector vi ∈Rk.
The rating ru,i can then be approximated by the
dot product of the corresponding user vector and item vector, i.e., ˆru,i = u⊤
u vi. The CF
problem can then be represented by the following optimization problem:
U∈Rk×n,V ∈Rk×m
where the loss function is deﬁned to optimize certain evaluation metric:
ℓrmse(ˆru,i, ru,i) = (ru,i −ˆru,i)2
ℓmae(ˆru,i, ru,i) = |ru,i −ˆru,i|
The regularized loss at time t is
Lt = λ||u(t)
2 + λ||v(t)
2 + ℓ(u(t)
where λ > 0 is the regularization parameter. A straightforward CF approach is to apply
OGD on the regularized loss function ,
= (1 −2ηλ)u(t)
= (1 −2ηλ)v(t)
where η > 0 is the learning rate. Later, several improved algorithms are proposed, such as
Online Multi-Task Collaborative Filtering algorithm , Dual-Averaging
Online Probabilistic Matrix Factorization algorithm , Adaptive
Gradient Online Probabilistic Matrix Factorization algorithm and
Second-order Online Collaborative Filtering algorithm .
These algorithms adopt more advanced update strategies beyond OGD and thus can achieve
faster adaptation for rapid user preference changes in real-world recommendation tasks.
Besides the algorithms introduced, there are many online CF methods that explore
other challenging tasks. First, in many applications, both features of users and items are
available and thus need to be considered for better prediction. This generalized CF problem
can be solved by using tensor product kernel functions. For example, the Online Low-rank
with Features algorithm addresses this problem in online setting.
However, it only adopts the linear kernel for eﬃciency. Perhaps, better performance might
be achieved if online budget learning algorithms are adopted. Second, most CF algorithms
are based on a regression model, which is mainly concerned with the accuracy of rating
prediction, while there are some applications where ranking prediction might be much more
important. Two algorithms based on OGD and Dual Averaging approaches are proposed
to address this problem by replacing the regression-based loss with the ranking-based loss
 .
Third, for very large-scale applications, when the model has to be
Hoi, Sahoo, Lu and Zhao
learnt using parallel computing, conventional OGD update is not suitable because of the
possible conﬂict in updating the user/item vectors. The Streaming Distributed Stochastic
Gradient Descent algorithm provides an operable approach to addresses
this problem. Finally, the CF methods for Google News recommendation 
is a combination of memory-based and model-based algorithms.
Last but not least, to
address the sparsity problem and imbalance of rating data, incorporate
content information via latent dirichlet allocation into online CF.
4.8 Online Learning to Rank
Learning to rank is an important family of machine learning techniques for information
retrieval .
Diﬀerent from classiﬁcation problems where instances are classiﬁed as either “relevant”
or “not relevant”, learning to rank aims to produce a permutation of a group of unseen
instances which is similar to the knowledge acquired from the previously seen rankings. To
evaluate the performance of ranking algorithms, metrics for information retrieval such as
Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG) and
Precision-At-Top-k are most popular.
Unlike traditional learning to rank methods which are often based on batch learning
 , we mainly focus on reviewing existing learning to rank methods in online
settings , where instances are observed sequentially.
Learning to rank techniques are generally categorized into two approaches: pointwise and
pairwise. We will introduce some of the most representative algorithms in each category.
Pointwise Approach:
We ﬁrst introduce a simple Perceptron-based algorithm, the Prank
 , which provides a straightforward view
of the commonly used problem setting for pointwise learning to rank approaches.
To deﬁne the online learning to rank problem setting formally, we have a ﬁnite set of
ranks Y = {1, ..., k} from which a rank y ∈Y is assigned to an instance x ∈Rd. During
time t, an instance xt is received and the algorithm makes a prediction ˆyt based on the
current model Ht : Rd →Y. Then the true rank yt is revealed and the model is updated
based on the loss ℓ(ˆyt, yt). The loss, for instance, can be deﬁned as ℓ(ˆyt, yt) = |ˆyt, yt|. The
goal of the online learning to rank task is to minimize the accumulated loss along the whole
learning process PT
t=1 ℓ(ˆyt, yt).
The ranking rule of Prank algorithm consists of the combination of Perceptron weight
w ∈Rd and a threshold vector c ∈{R, ∞}d, whose elements are in nondecreasing order i.e.,
c1 ≤c2 ≤, ..., ≤ck = ∞. Like the Perceptron algorithm, the rank prediction is determined
by the value of the inner product w⊤
r∈{1,...,k}{r : w⊤
We can expand the target rank yt to a vector yt = {+1, ..., +1, −1, ..., −1} ∈Rk.
r = 1, ...k, yr
t = −1 if yt < r, and yr
t = 1 otherwise.
Thus, for a correct prediction,
t) > 0 holds for all r ∈Y. When a mistake appears ˆyt ̸= yt, there is subset M
of Y where yr
t) > 0 does not hold. The update rule is to move the corresponding
Online Learning: A Comprehensive Survey
thresholds for ranks in M and the weight vector toward each other:
wt+1 = wt + (
In theory, the elements in threshold vector c are always in nondecreasing order and the
total number of mistakes made during the learning process is bounded.
Online Aggregate Prank-Bayes Point Machine (OAP-BPM) is an
extension of the Prank algorithm by approximating the Bayes point. Speciﬁcally, the OAP-
BPM algorithm generates N diverse solutions of w and c during each iteration and combines
them for a better ﬁnal solution.
We denote Hj,t as the j-th solution at time t.
algorithm samples N Bernoulli variables bj,t ∈{0, 1}, j = {1, ..., N} independently.
bi,t = 1, The j-th solution is updated using the Prank algorithm according to the current
instance, Hj,t+1 = Prank(Hj,t, (xt, yt)). Otherwise, no update is conducted to the j-th
solution. The solution wt+1 and ct+1 is the average over N solutions. This work shows
better generalization performance than the basic Prank algorithm.
Pairwise Approach:
One simple method is to address the ranking problem by transforming it to a classiﬁcation problem . In a more challenging problem
setting, where no accurate rank y is available when collecting the data, only pairwise instances are provided. At time t, a pair of instances (x1
t ) are received with the knowledge
t is ranked before x2
t or the inverse case, and the aim is to ﬁnd a function f : Rd →R
that ﬁts the instance pairs, i.e., f(x1) > f(x2) when it is known x1
t is ranked before x2
or otherwise f(x1) < f(x2). When the function is linear, the problem can be rewritten as
w⊤(x1 −x2) > 0 when x1 is in front and otherwise w⊤(x1 −x2) < 0, where w ∈Rd is the
weight vector. This problem can easily be solved by using a variety of online classiﬁcation
algorithms for example).
4.9 Distributed Online Learning
Distributed online learning has become increasing popular due to the
explosion in size and complexity of datasets. Similar to the mini-batch online learning,
during each iteration, K instances are received and processed simultaneously. Usually, each
node processes one of the instances and updates its local model. These nodes communicate
with each other to make their local model consistent. When designing a distributed algorithm, besides computational time cost and accuracy, another important issue to consider is
the communication load between nodes. This is because in real world systems with limited
network capacity and large communication burden result in long latency.
Based on the network structure, distributed online learning algorithms can be classiﬁed
into two groups: centralized and decentralized algorithms. A centralized network is made
up of 1 master node and K −1 worker nodes, where the workers can only communicate
with the master node. By gathering and distributing information across the network, it is
not diﬃcult for distributed algorithms to reach a global consensus . In
decentralized networks, however, there is no master and each node can only communicate
with its neighbors . Although the algorithms are
more complex, decentralized learning is more popular because of the robustness of network
structure.
Hoi, Sahoo, Lu and Zhao
We can also group the distributed learning algorithms by synchronized and asynchronized
working modes. Synchronized algorithms are easy to design and enjoy better theoretical
bounds but the speed of the whole network is limited by the slowest node. Asynchronized
learning algorithms, on the other hand, are complex and usually have worse theoretical
bounds. The advantage is its faster processing speed .
4.10 Online Learning with Neural Networks
In addition to kernel-based online learning approaches, another rich family of nonlinear online learning algorithms follows the general idea of neural network based learning approaches
 . For example, the Perceptron algorithm could be viewed
as the simplest form of online learning with neural networks (but it is not nonlinear due
to its trivial network). Despite many extensive studies for online learning (or incremental
learning) with neural networks, many of existing studies in this ﬁeld fall short due to some
critical drawbacks, including the lack of theoretical analysis for performance guarantee,
heuristic algorithms without solid justiﬁcation, and computational too expense to achieve
eﬃcient and scalable online learning. Due to the large body of related work, it is impossible
to examine every piece of work in this area. In the following, we review several of the most
popularly cited related papers and discuss their key ideas for online learning with neural
A series of related work has explored online convex optimization methods for training
classical neural network models , such as the Multi-layer Perceptron (MLP).
For example, online/stochastic gradient descent has been extensively studied for training
neural networks in sequential/online learning settings, such as the eﬃcient back-propagation
algorithm using SGD . These works are mainly motivated to accelerate
the training of batch learning tasks instead of solving online learning tasks directly and
seldom give theoretical analysis.
In addition to the above, we also brieﬂy review other highly cited works that address
online/incremental learning with neural networks. For example, the study in presented a novel learning algorithm for training fully recurrent neural networks for temporal supervised learning which can continually run over time. However, the
work is limited in lacking theoretical analysis and performance guarantee, and the solution
could be quite computationally expensive. The work in presented a Resource-
Allocating Network (RAN) that learns a two-layer network by a strategy for allocating new
units whenever an unusual pattern occurs and a learning rule for reﬁning the network using
gradient descent. Although the algorithm was claimed to run in online learning settings, it
may suﬀer poor scalability as the model complexity would grow over time. The study in
 proposed a new neural network architecture called “ARTMAP” that
autonomously learns to classify arbitrarily many, arbitrarily ordered vectors into recognition categories based on predictive success. This supervised learning system was built from
a pair of ART modules that are capable of self organizing stable recognition categories in
response to arbitrary sequences of input patterns. Although an online learning simulation
has been done with ART (adaptive resonance theory), the solution is not optimized directly
for online learning tasks and there is also no theoretical analysis. The work in proposed an online sequential extreme learning machine (OS-ELM) which explores an
online/sequential learning algorithm for training single hidden layer feedforward networks
(SLFNs) with additive or radial basis function (RBF) hidden nodes in a uniﬁed framework.
The limitation also falls short in some heuristic approaches and lacking theoretical analysis.
Last but not least, there are also quite many studies in the ﬁeld which claim that they design
neural network solutions to work online, but essentially they are not truly online learning.
They just adapt some batch learning algorithms to work eﬃciently for seqential learning
environments, such as the series of Learning++ algorithms and their variants . Recently, Hedge Backpropagation was
proposed to learn deep neural networks in the online setting with the aim to address slow
convergence of deep networks through dynamic depth adaptation.
4.11 Online Portfolio Selection
On-line Portfolio Selection (OLPS) is a natural application of online learning for sequential
decisions of selecting a portfolio of stocks for optimizing certain metrics, e.g. cumulative
wealth, risk adjusted returns, etc. . Consider a ﬁnancial market with m assets, in which we have to allocate
our wealth. At every time period (or iteration), the price of the m stocks changes by a
factor of xt ∈Rm
+. This vector is also called the price relative vector. xt,i denotes the ratio
of the closing price of asset i at time t to the last closing price at time t −1. Thus, an
investment in asset i changes by a factor of xt,i in period t. At the beginning of time period
t the investment is speciﬁed by a portfolio vector bt ∈δm where δm = b : b ⪰0, b⊤1 = 1.
The portfolio is updated in every time-period based on a speciﬁc strategy, and produces a
sequence of mappings:
bt : Rm(t−1)
t = 2, 3, . . . , T
where T is the maximum length of the investment horizon. To make a decision for constructing a portfolio at time t, the entire historical information from x1, . . . , xt−1 is available. The
theoretical framework starts with a wealth of S0 = 1, and at the end of every time period,
the wealth changes as St = St−1 × (b⊤
Most eﬀorts in OLPS make a few (possibly unrealistic) assumptions, including no transaction costs, perfectly liquid market, and no impact cost (the portfolio selection strategy
does not aﬀect the market).
Besides the traditional benchmarking approaches, the approaches for OLPS can be categorized into Follow-the-winner, Follow-the-loser, Pattern
Matching, and Meta-Learning approaches .
The benchmark approaches, as the name suggests, are simple baseline methods whose
performance can be used to benchmark the performance of proposed algorithms. Common
baselines are Buy and Hold (BAH) strategy, Best Stock and Constant Rebalanced Portfolio
(CRP). The idea of BAH is to start with a portfolio with equal investment in each asset,
and never rebalance it. Best Stock is the performance of the asset with the highest returns
at the end of the investment horizon. CRP is a ﬁxed portfolio allocation to
which the portfolio is rebalanced to at the end of every period, and Best-CRP is the CRP
which obtains the highest returns at the end of the investment horizon. It should be noted
that Best Stock and Best-CRP strategies can only be executed in hindsight.
Hoi, Sahoo, Lu and Zhao
Follow-the-winner approaches adhere to the principle of increasing the relative portfolio
allocation weight of the stocks that have performed well in the past. Many of the approaches
are directly inspired by Convex Optimization theory, including Universal Portfolios , Exponential Gradient , Follow the Leader and Follow the Regularized Leader . In contrast to followthe-winner, there is a set of approaches that aim to follow-the-loser, with the belief that
asset prices have a tendency to revert back to a mean, i.e., if the asset price falls, it is likely to
rise up in the next time-period. These are also called mean-reversion strategies. The early
eﬀorts in this category included Anti Correlation which designed
a strategy by betting making statistical bets on positive-lagged correlation and negative
auto-correlation; and Passive-Aggressive Mean Reversion (PAMR) , which
extended the Online Passive Aggressive Algorithms to update the
portfolio to an optimal ”loser” portfolio - by selecting a portfolio that would have made an
optimal loss in the last observed time-period. A similar idea was used to extend conﬁdenceweighted online learning to develop Conﬁdence-Weighted Mean Reversion . The idea of PAMR was extended to consider multi-period asset returns, which led
to the development of Online Moving Average Reversion (OLMAR) and Robust Median Reversion (RMR) strategies.
Another popular set of approaches is the Pattern-Matching approaches, which aim to
ﬁnd patterns (they may be able to exploit both follow-the-winner and follow-the-loser) for
optimal sequential decision making. Most of these approaches are non-parametric. Exemplar approaches include .
Finally, meta-learning algorithms for portfolio selection aim to rebalance the portfolio on
the basis of the expert advice. There are a set of experts that output a portfolio vector,
and the meta-learner uses this information to obtain the optimal portfolio.
In general,
the meta-learner adheres to the follow-the-winner principle to identify the best performing experts. Popular approaches in this category include Aggregating Algorithms , Fast Universalization Algorithm and Follow the
Leading History . Besides these approaches, there are also
eﬀorts in portfolio selection with aims to optimize the returns accounting for transaction
costs. The idea is to incorporate the given transaction cost into the optimization objective
 .
A closely related area to Online Portfolio Selection is Online Learning for Time Series
Prediction. Time series analysis and prediction is a classical problem in machine learning,
statistics, and data mining.
The typical problem setting of time series prediction is as
follows: a learner receives a temporal sequence of observations, x1, . . . , xt, and the goal of the
learner is to predict the future observations (e.g., xt+1 or onwards) as accurately as possible.
In general, machine learning methods for time series prediction may also be divided into
linear and non-linear, univariate and multivariate, and batch and online. Some time series
prediction tasks may be resolved by adapting an existing batch learning algorithm using
sliding window strategies. Recently there have been some emerging studies for exploring
online learning algorithms for time series prediction .
Online Learning: A Comprehensive Survey
5. Bandit Online Learning
5.1 Overview
Bandit online learning, a.k.a. the “Multi-armed Bandit (MAB) problem , is an important branch of online learning where a learner makes
sequential decisions by receiving only partial feedback from the environment each time.
MAB problems are online learning tasks for sequential decisions with a trade-oﬀbetween
exploration and exploitation. Speciﬁcally, on each round, a player chooses one out of K
actions, the environment then reveals the payoﬀof the player’s action, and the goal of
the learner is to maximize the total payoﬀobtained during online learning process.
fundamental challenge of MAB is to address the exploration-exploitation tradeoﬀ , i.e., balancing between the exploitation of actions that gave highest payoﬀs in
the past and the exploration of new actions that might give higher payoﬀs in the future.
MAB problems are collectively called “bandit” problems. Historically, the name “bandit” is referred to the scenario of playing slot machines in a casino, where a player faces
a number of slot machines to insert coins (one slot machine is called one-armed bandit in
American slang), the expected reward of each machine might be diﬀerent, and the player’s
goal is to maximize the reward by repeatedly choosing where to insert the next coin.
MAB problems can be roughly divided into two major categories: stochastic MAB
and adversarial MAB. The former assumes a stochastic environment where rewards (or
“losses” equivalently) are i.i.d. and independent from each other, while the later removes
the stochastic assumption where the rewards (or losses) can be arbitrary or more formally
“adversarial” by adapting to the past decisions. Note that the notion of “reward” and “loss”
are symmetric and can be translated from one to the other equivalently. To be consistent,
if not mentioned speciﬁcally, we will use “loss” instead of “reward” for the rest discussion.
We now introduce the formal procedure of the MAB problem. Formally, a K-armed
bandit problem takes place in a sequence of rounds with length T ∈N, where T is often
unknown at the beginning (typically a learner is called an “anytime” algorithm if T is
unknown in advance). At the t-th round, the player chooses one out of K actions It ∈
[K] = {1, . . . , K} using some strategy. After that, the environment reveals the loss ℓt(It) of
the action to the forecaster. The goal is to minimize the total loss over the T rounds. In
theory, we are interested in analyzing the behavior of the learner, typically by comparing
the performance of its actions against with some optimal strategy. More formally, we can
deﬁne the “regret” as the diﬀerence between the cumulative loss of the best ﬁxed arm by
an optimal strategy and that of the player after playing T rounds I1, . . . , IT as
ℓt(It) −min
As both loss ℓt(i) and action It could be stochastic, we can deﬁne the expected regret as
E[RT ] = E
ℓt(It) −min
where the expectation is taken with respect to the random draw of both losses and learner’s
Hoi, Sahoo, Lu and Zhao
5.2 Stochastic Bandits
For stochastic MAB problem, each arm i ∈[k] corresponds to an unknown distribution Pi
on , and the losses ℓt(i) are independently drawn from the distribution Pi corresponding
to the selected arm. Let us denote by µi the mean of the distribution Pi, and deﬁne
i∗∈arg min
The expected regret can be rewritten as
E[RT ] = E[
µIt] −T min
i∈[K] µi = E[
µIt] −Tµ∗= E[
(µIt −µ∗)]
Further let Ni(s) = Ps
t=1 I(It = i) denote the number of times the player selected arm i
on the ﬁrst s rounds and ∆i = µi −µ∗be the suboptimality parameter of arm i, we can
simplify the expected regret as follows:
E[RT ] = E[
∆iE[Ni(T)].
5.2.1 Stochastic Multi-armed Bandit
In this section we mainly introduce two well-known algorithms for stochastic MAB.
The ﬁrst simplest algorithm for stochastic MAB is called the ϵ-Greedy rule
 . The idea is to with probability 1−ϵ play the the current best arm
of the highest average reward, and with probability ϵ play a random arm, where parameter
ϵ > 0 is a constant value in (0, 1). Algorithm 13 gives a summary of this algorithm. However,
Algorithm 13: ϵ-Greedy
INPUT: parameter ϵ > 0
INIT: empirical means µi = 0, ∀i ∈[K]
for t = 1, 2, . . . , T do
with probability 1 −ϵ play the current best arm it = arg mini∈[K] µi
with probability ϵ play a random arm
receive ℓt(it)
update the empirical means µit = (µit + ℓt(it))/(Nit(t) + 1)
the constant exploration probability ϵ results in a linear growth in the regret. One way to
ﬁx it is to decrease the value of ϵ over time and let it go to zero at a certain rate. For
example, an improved ϵt-greedy algorithm is to follow the epsilon-decreasing strategy by
deﬁning ϵt at round t as
where c > 0 and d ∈(0, 1). When 0 < d ≤mini:µi<µ∗∆i < 1 and T > cK
d , this improved
ϵt-greedy algorithm can achieve the logarithm regret O(ln T).
Online Learning: A Comprehensive Survey
Another well-known algorithm for stochastic MAB is the Upper Conﬁdence Bound
(UCB) algorithm , a strategy that simultaneously performs exploration
and exploitation using a heuristic principle of optimism in face of uncertainty. The intuition
is that, despite lacking knowledge about what actions are best, we will try to construct an
optimistic guess as to how good the expected payoﬀ/loss of each action is, and choose the
action with the best guess. If our guess is correct, we will be able to exploit that action and
incur little regret; but if our guess is wrong, then our optimistic guess will quickly decrease
and we will then be compelled to switch to a diﬀerent action, which therefore is able to
balance the exploration-exploitation tradeoﬀ.
Formally, the “optimism” comes in the form of Upper Conﬁdence Bound (UCB). In
particular, the idea is to calculate the conﬁdence intervals of the averages, which is a region around our estimates such that the true value falls within with high probability, and
repeatedly shrink the conﬁdence bounds such that the average will become more reliable.
Algorithm 14 gives a summary of the UCB algorithm.
Algorithm 14: UCB
INPUT: parameter ϵ > 0
INIT: empirical means µi = 0, ∀i ∈[K]
for t = 1, 2, . . . , T do
play the arm it = arg mini(µi −
receive ℓt(it)
update the empirical means µit = (µit + ℓt(it))/(Nit(t) + 1)
In theory, by running the UCB algorithm over T rounds, the expected regret is
The above is a speciﬁc worst-case bound on the expected regret . More
concisely, one can show that the expected regret of UCB is at most O(
KT ln T). Auer
et al. also gave some variants of improved UCB algorithms. Improved algorithms
and regret bound were also given in .
5.2.2 Bayesian Bandits
Bayesian methods have been explored for studying bandit problems from the beginning
of this ﬁeld.
One of the most well-known and classic algorithm in Bayesian Bandits is
Thompson Sampling , which is considered one of the oldest algorithm
to address the exploration-exploitation trade-oﬀfor bandit problems. Recent years have
seen a lot of interests in analyzing both empirical performance 
and theoretical properties of Thompson Sampling for bandit problems (Agrawal and Goyal,
In the following, we introduce a Bayesian setting of Bernoulli bandit and then
discusses the Thompson Sampling algorithm.
Consider a standard K-armed Bernoulli bandit, each action corresponds to the choice of
an arm, and the reward of the i-th arm is either 0 or 1 which follows a Bernoulli distribution
Hoi, Sahoo, Lu and Zhao
with mean µi, i.e., the probability of success for arm i (reward=1) is µi. The algorithm
maintains Bayesian priors on the Bernoulli means µi’s. At the beginning, the Thompson
Sampling algorithm initializes each arm i to have prior Beta(1, 1) on µi, since Beta(1, 1) is
the uniform distribution on (0, 1). On round t, after observing Si(t) successes (reward=1)
and Fi(t) failures (reward=0) on ki(t) = Si(t) + Fi(t) times of playing arm i, the algorithm
updates the distribution on θi as Beta(Si(t)+1, Fi(t)+1). The algorithm then samples from
these posterior distributions of the θi’s, and plays an arm according to the probability of its
mean being the largest. Algorithm 15 gives a summary of Thompson Sampling algorithm
for K-armed Bernoulli bandits problems.
Algorithm 15: Thompson Sampling
INIT: Si(1) = 0, Fi(1) = 0, ∀i = 1, . . . , K
for t = 1, 2, . . . , T do
For each arm i ∈[K], sample θi(t) from the Beta(Si + 1, Fi + 1) distribution.
Play arm i(t) = arg maxi θi(t), and observe reward rt
If rt = 1, then Si(t) = Si(t) + 1; else Fi(t) = Fi(t) + 1.
The above Thompson sampling algorithm can be easily extended to the general stochastic bandits setting, i.e., when the rewards for arm i are generated from an arbitrary unknown
distribution with support and mean µi. It has also been extensively used for contextual
bandit settings .
In theory, for the K-armed stochastic bandit problem, denoting ∆i = µ1 −µi, the
Thompson Sampling algorithm has the expected regret given in 
Other Related Work. In addition to the classic Thompson Sampling algorithm, another
notable variant of Bayesian Bandit is called the Bayes-UCB algorithm , which is a UCB-like algorithm, where the upper conﬁdence bounds are based on the
quantiles of Beta posterior distributions, and is able to achieve the lower bound of Lai and
Robbins for Bernoulli rewards. More other extensive and recent studies of Bayesian
Bandits can be found in .
5.3 Adversarial Bandits
In the previous setting of stochastic bandits, we generally assume that the rewards are i.i.d.,
which are drawn independently from some unknown but ﬁxed distribution. We now relax
such stochastic assumption on the rewards.
We assume the reward distribution can be
aﬀected by the previous actions taken by the player, which is termed as “Adversarial Bandits” problems Auer et al. . In the following, we review several classes of adversarial
bandits, including some fundamentals of adversarial MAB and other active topics such as
linear bandits and combinatorial bandits.
Online Learning: A Comprehensive Survey
5.3.1 Adversarial Multi-armed Bandit
We consider a K-armed adversarial bandit problem where K > 1 and the learner receives
an arbitrary sequence of loss vectors (ℓ1, . . . , ℓT ) where ℓt ∈ K∀t ∈[T].
round, the learner plays an action It ∈[K] and observes the loss ℓt(It). For adversarial
bandits, a randomized policy is commonly used. In particular, given some policy π, the
conditional distribution over the actions having observed Ωt−1 = {(I1, ℓ1), . . . , (It−1, ℓt−1)}
is Pt = π(·|Ωt−1) ∈PK−1.
The performance of a policy π on the environment can be
measured by the expected regret which is the expected loss of the policy relative to the best
ﬁxed action in hindsight:
E[RT ] = E
The Exponential-weights for Exploration and Exploitation algorithm (Exp3) is a popular algorithm for adversarial MAB. It follows the similar idea of
prediction with expert advice and applies the Hedge (or Weighted-Majority) algorithm to
the tradeoﬀof exploration and exploitation. Speciﬁcally, we ﬁrst deﬁne a probability vector
pt ∈RK in which the i−th element pt(i) indicates the probability of drawing arm i at time
t. This vector is initialized uniformly and updated at each round. On each round, the
learner plays an action by drawing It ∼pt where pt is set as follows
pt(i) = (1 −γ)
K , ∀i ∈[K]
where wt(i) is the importance weight of each arm i learned by the Hedge algorithm, and γ is
a parameter for weighting the exploration term. Algorithm 16 gives a summary of the Exp3
algorithm. By tuning the optimal parameter of γ, the Exp3 algorithm is able to achieve
the regret O(
TK ln K) in the adversarial setting.
Algorithm 16: Exp3
INPUT: parameter γ ∈(0, 1]
INIT: w1(i) = 1, ∀i ∈[K]
for t = 1, 2, . . . , T do
Set pt(i) = (1 −γ)
j=1 wt(j) + γ
K , ∀i ∈[K]
Play action by drawing It ∼pt
Receive ℓt(It) ∈ ,
Update wt(i) = wt(i)e−γ ℓt(i)
pt(i) , if i = It.
Other Related Works. This is generally more challenging than the stochastic setting.
A variety of algorithms have been explored in literature .
For example, the Exp3.P algorithm in improves the loss estimation and
probability update strategies to get a high probability bound. The Exp3.M algorithm in
 explores the new problem setting of multiple plays.
Hoi, Sahoo, Lu and Zhao
5.3.2 Linear Bandit and Combinatorial Bandit
We ﬁrst introduce the problem setting of the Linear Bandit optimization problem . During each iteration, the
player makes its decision by choosing a vector from a ﬁnite set S ⊆Rd of elements v(i) for
i = 1, ..., k. The chosen action at iteration t is indexed as It. The environment chooses a
loss vector ℓt ∈Rd and returns the linear loss as ct(It) = ℓ⊤
t v(It). Note that the player
has no access to the full knowledge of loss vector and the only information revealed to the
player is the loss of its own decision ct(It). Obviously, when setting d = k and v(i) is the
standard basis vector, this problem is identical to that in the previous section.
Combinatorial Bandit.
Combinatorial bandit is a special case of Linear Bandits, where S is a subset of binary hypercube {0, 1}d. The loss vector
ℓt may be generated from an unknown but ﬁxed distribution, which is termed as stochastic
combinatorial bandit, or chosen from some adversarial environment, which is termed as
adversarial combinatorial bandit. The goal is to minimize the expected regret
ct(It)] −min
i∈[K] LT (i)
where LT (i) = E PT
t=1 ct(i) is the expected sum of loss for choosing action i in all T
iterations, not a random variable in stochastic setting. One example algorithm for Combinatorial Bandit is the COMBAND algorithm . It ﬁrst
deﬁnes a sampling probability vector pt ∈Rk for sampling v(It) from S
pt = (1 −γ)qt + γµ
where qt ∈Rk is the exploitation probability vector that is updated during all iterations to
follow the best action, µ ∈Rd is a ﬁxed exploration probability, and γ ∈ is the weight to
control the exploitation and exploration trade-oﬀ. The algorithm draws the action It based
on distribution pt and gets the loss ct(It) from the environment. Second, an estimation of
the loss vector ℓt is calculated with the new information,
eℓt = ct(It)P +
is the pseudo-inverse of the expected correlation matrix Ept[vv⊤]. Finally, the
exploitation weights are scaled based on the estimated loss vector,
qt+1(i) ∝qt(i) exp(−η eℓt
where η > 0 is a learning rate parameter and ∝indicates that this scaling step is followed by
a normalization step so that Pk
i=1 q(i) = 1. The COMBAND algorithm achieves a regret
bound better than O(
Td ln |S|) for a variety of concrete choices of S.
Other Related Works.
Recently, many studies also address linear bandits and combinatorial bandits in diﬀerent settings. The ESCB algorithm eﬃciently
exploits the structure of the problem and gets a better regret bound of O(ln(T)). The
CUCB algorithm addresses the problem where the loss may be nonlinear. provided a useful survey for closely related works and gave a novel algorithm with promising bounds.
Online Learning: A Comprehensive Survey
5.4 Contextual Bandits
Contextual Bandit is a widely used extension of MAB by associating contextual information
with each arm . For example, in personalized recommendation problem, the task is to select products that are most likely to be purchased by
a user. In this case, each product corresponds to an arm and the features of each product
are easy to acquire .
In a contextual bandits problem, there is a set of policies F, which may be ﬁnite or
inﬁnite. Each f ∈F maps a context x ∈X ⊆Rd to an arm i ∈[k]. Diﬀerent from the
previous setting where the regret is deﬁned by competing with the arm with the highest
expected reward, the regret here is deﬁned by comparing the decision It with the best policy
f∗= arg inff∈F ℓD(f), where D is the data distribution.
[ℓIt,t −ℓt(f∗)]
In literature, there are comprehensive surveys on contextual bandit algorithms in both
stochastic and adversarial settings . Below
we focus on two settings of contextual bandits: multiclass classiﬁcation and expert advice.
5.4.1 The Multiclass Setting.
In this setting, contextual bandit is regarded as a special case of online multi-class classi-
ﬁcation tasks in bandit setting. The goal is to learn a mapping from context space Rd to
label space {1, ..., k} from a sequence of instances xt ∈Rd. Diﬀerent from classic online
multi-class classiﬁcation problems where a class label yt ∈{1, ..., k} is revealed at the end
of each iteration, in bandit setting, the learner only gets a partial feedback on whether ˆyt
equals to yt. In the following, we brieﬂy review some representative works of contextual
bandits for multi-class classiﬁcation.
Banditron is the ﬁrst bandit algorithm for online multiclass prediction , which is a variant of the Perceptron. To eﬃciently make prediction and update the
model, the Banditron algorithm keep a linear model W t, which is initialized as W 1 = 0 ∈
Rk×d. At the t-th iteration, after receiving the instance xt ∈Rd, it will ﬁrst set
ˆyt = arg max
r∈[k](W txt)r
where (z)r denotes the r-th element of z. Then the algorithm will deﬁne a distribution as
Pr(r) = (1 −γ)I(r = ˆyt) + γ/k, ∀r ∈[k]
which roughly implies that the algorithm exploits with probability 1 −γ and explores with
the remaining probability by uniformly predicting a random label from [k]. The parameter
γ controls the exploration-exploitation tradeoﬀ. The algorithm then randomly sample ˜yt
according to the probability Pr and predicts it as the label of xt. After the prediction,
the algorithm then receives the bandit feedback I(˜yt = yt). Then the algorithm uses this
feedback to construct a matrix
r,j = xt,j
I(ˆyt = r) −I(˜yt = yt)I(˜yt = r)
Hoi, Sahoo, Lu and Zhao
since its expectation satisﬁes E ˜Ut
r,j = xt,j (I(ˆyt = r) −I(yt = r)), where Ut is actually
a (sub)-gradient of the following hinge loss
ℓ(W; (xt, yt)) =
r∈[k]/ {yt}[1 −(Wxt)yt + (Wxt)r]+
where [z]+ = max(0, z). Then the algorithm will update the model by W t+1 = W t −˜Ut.
The Banditron algorithm is summarized in Algorithm 17.
Algorithm 17: Banditron
INIT: w1,1 = 0, .., wk,1 = 0
for t = 1, 2, . . . , T do
Receive an incoming instance xt
P(r) = (1 −γ)1[r = arg maxi w⊤
i,txt] + γ
Sample ˆyt according to P(r), r ∈{1, ..., k}
ur = xt(1[yt=ˆyt=r]
−1[r = arg maxi w⊤
wr,t+1 = wr,t + ur
This algorithm achieves O(
T) in linear separable case and O(T
3 ) in inseparable case.
Other Related Work. Following the Banditron, many algorithms have been proposed.
For example, Bandit Passive Aggressive (Bandit PA) follows the PA learning principle
and adopts the framework of one against all others to make prediction and update the
model .
In general, some update principles are based on ﬁrst-order
gradient descent , while others adopt second order learning .
Most of these algorithms explore the k classes uniformly with probability γ, while sample the classes based on the Upper Conﬁdence Bound.
5.4.2 The Expert Setting.
We now introduce a well-known algorithm called Exp4 for contextual
bandits in the expert settings.
The Exp4 algorithm assumes that there are N experts
who will give advice on the distribution over arms during all iterations. ξn
i,t indicates the
probability of picking arm i ∈[K] recommended by expert n ∈[N] during time t ∈[T].
Obviously, Pk
i,t = 1. During time t, the true reward vector is denoted by rt ∈ K.
Thus the expected reward of expert n is ξn
t · rt. The regret is deﬁned by comparing with
the expert with the highest expected cumulative reward.
The Exp4 algorithm ﬁrst deﬁnes a weight vector wt ∈RN that indicates the weights for
the N experts. We set the weight as w0 = 1 and update it during each iteration. During
iteration t, we calculate the probability of picking arm i as the weighted sum of advices
Online Learning: A Comprehensive Survey
from all N experts,
pi,t = (1 −γ)
n=1 wn,tξn
where γ ∈ is a parameter to balance exploitation and exploration. We then draw an
arm It according to probability pi,t and calculate an unbiased estimator of ˆri,t = ri,t
pi,t Ii=It,
which will be used to calculate the expected reward. Finally the weight wt is updated
according to the expected reward of each arm. The Exp4 algorithm is able to achieve the
regret bound O(
TK ln N) as shown in and tighter bounds were also
given in .
Other Related Work. Another general contextual bandit algorithm is the epoch-greedy
algorithm in that is similar to ϵ-greedy with shrinking ϵ. This
algorithm is computationally eﬃcient given an oracle optimizer but has the weaker regret
guarantee of O(T 2/3). LinUCB is an extension of UCB to contextual
bandit problem, by assuming that there is a feature vector xt,i ∈Rd at time t for each
arm i. Similar to the UCB algorithm, a model is learnt to estimate the upper conﬁdence
bound of each arm i ∈[k] given the input of xt,i. The algorithm simply chooses the arm
with the highest UCB. The LinREL algorithm is similar to LinUCB in that
it adopts the same problem setting and same maximizing UCB strategy. While, a diﬀerent
regularization term is used which leads to a diﬀerent calculation of the UCB.
5.5 Other Bandit Variants
In literature, there are many other studies addressing on various types of bandit variants.
We refer readers for more comprehensive studies on bandit topics in . Below we brieﬂy introduce a few other major variants.
Other than stochastic bandits and adversarial bandits, another fundamental topic of
multi-armed bandits is called “Markovian bandits”, which generally assumes the reward
processes are neither i.i.d. (like in stochastic MAB) nor adversarial. Speciﬁcally, arms are
associated with K Markov processes, each with its own state space. On each round, an arm
is chosen in some state, a stochastic reward is drawn from some probability distribution,
and the state of the reward process for the arm changes in a Markovian fashion, based
on an underlying stochastic transition matrix. Both reward and new state are revealed to
the player. The seminal work of Gittins gives an optimal greedy policy that can be
computed eﬃciently. A special case of Markovian bandits is Bayesian bandits , which are parametric stochastic bandits where
the parameters of the reward distributions are assumed to be drawn from known priors,
and the regret is computed by also averaging over the draw of parameters from the prior.
Another topic is to study inﬁnitely many-armed bandits problems where the number of
arms can be larger than the possible number of experiments or even inﬁnite . Among these studies, one niche sub-topic
is continuum-armed bandits , where the
arms lie in some Euclidean (or metric) space and their mean-reward is a deterministic and
smooth (e.g., Lipschitz) function of the arms, a.k.a. Lipschitz Bandit (Magureanu et al.,
Hoi, Sahoo, Lu and Zhao
6. Online Active Learning
6.1 Overview
In a standard online learning task (e.g., online binary classiﬁcation), the learner receives and
makes prediction for a sequence of instances generated from some unknown distribution. At
the end of every round, it always assumes the learner will receive the true label (feedback)
from the environment. For many real-world applications, obtaining the labels could be very
expensive, and sometimes it is not always necessary/informative to query the true labels of
every instance, e.g., if an instance is correctly classiﬁed with a high conﬁdence. Motivated
to address this challenge, online active learning is a special class of online learner that
observes a sequence of unlabeled instances each time deciding whether to query the label of
the incoming instance; if the label is queried, then the learner can use the labelled instance
to update the prediction model; otherwise, the model will be kept unchanged.
In literature, there are two major kinds of settings for online active learning. One is
called the “selective sampling” setting by adapting classical online learning for active learning. The other is online
active learning with expert advice by adapting the setting of prediction with expert advice
for active learning . Both operate in the
similar problem settings where true label of an instance is only queried when some condition
is satisﬁed, e.g., predictive conﬁdence is below some threshold.
6.2 Selective Sampling Algorithms
In this section we review a family of popular Selective Sampling (SS) algorithms for online
active learning tasks. In the following discussions, we use a typical online binary classiﬁcation task as a running example. For notation, an example is a pair (x, y), where x ∈Rd is
an instance vector and y ∈{−1, +1} is the binary class label. Assume the learning proceeds
in a sequence of T rounds, where T may not be known in advance. On each round t, a
learner observes an instance xt, then outputs a prediction ˆyt ∈{−1, +1} as the label for
the instance, and then decides whether or not to query the label yt. Whenever ˆyt ̸= yt, the
learner’s prediction outcome is considered as a mistake, no matter if it has decided to query
the label or not. For notation, we denote Mt = I(ˆyt ̸= yt) ∈{0, 1} as an indicator whether
the learner makes a mistake at round t. For most cases, we also assume the leaner adopts
a linear model to predict the class label using ˆyt = sign(ˆpt), where ˆpt = w⊤
6.2.1 First-order Selective Sampling Algorithms
Selective-sampling Perceptron.
This algorithm decides
whether or not to query the label yt through a simple randomized rule: drawing a Bernoulli
random variable Zt ∈{0, 1} with probability
Pr(Zt = 1) =
where δ > 0 is a smooth parameter that can be used to control the number of labels
queried during the online active learning process. If δ increases, the number of queried
labels increases. If Zt = 1, then the label yt of xt will be queried, and the model will be
Online Learning: A Comprehensive Survey
Algorithm 18: Selective-sampling Perceptron
INPUT: parameter δ > 0
INIT: w0 = (0, . . . , 0)⊤
for t = 1, 2, . . . , T do
Observe an input instance xt ∈Rd
Predict ˆyt = sign(ˆpt), where ˆpt = w⊤
Draw a Bernoulli random variable Zt ∈{0, 1} of probability
IF Zt = 1 THEN
Query label yt ∈{−1, +1} and Update wt by Perceptron: wt+1 = wt + Mtytxt.
updated using the Perceptron rule. Algorithm 18 gives a summary of the Selective-sampling
Perceptron algorithm.
In theory, assuming ∥xt∥≤R, for any w ∈Rd, the expected number of mistakes of the
Selective-sampling Perceptron algorithm can be bounded as:
Mt] ≤(1 + R2
+ ∥w∥2(2δ + R2)2
where ¯Lγ,T (w) = E[PT
t=1 ZtMtℓγ,t(w)], and ℓγ,t(w) = max(0, γ −ytw⊤xt). Furthermore,
the expected number of labels queried by the algorithm equals PT
δ+|ˆpt|]. This bound
depends on the value of the parameter δ. By choosing the optimal value of δ as
the expected number of mistakes can be bounded
This is an expectation version of the mistake bound for the standard Perceptron Algorithm.
Especially, in the special case when the data is linearly separable, the optimal value of δ is
R2/2 and this bound becomes the familiar Perceptron bound (∥w∥R)2/γ2. Instead of using
a ﬁxed constant parameter, also proposed an adaptive parameter
version of the selective sampling Perceptron algorithm as follows:
Pr(Zt = 1) =
δt + |ˆpt|,
δt = β(R′)2
where β > 0 is a predeﬁned parameter, R′ = max Rt−1, ∥xt∥, Rt−1 = max{∥xi∥|ZiMi = 1}.
Hoi, Sahoo, Lu and Zhao
Other ﬁrst-order approaches.
Instead of using Perceptron, the Passive-Aggressive Active learning algorithms in are selective sampling algorithms by
extending the framework of PA online learning algorithms. They also extended their algorithms for multi-class classiﬁcation and cost-sensitive classiﬁcation tasks. proposed a cost-sensitive online active learning approach that directly optimizes costsensitive measures using PA-like algorithms for class-imbalanced classiﬁcation tasks.
6.2.2 Second-order Selective Sampling Algorithms
Selective-sampling Second-order Perceptron.
Instead of using the standard Perceptron algorithm, also proposed a selective-sampling algorithm
based on the Second-order Perceptron.
Algorithm 19: Selective-sampling Second-order Perceptron
INPUT: parameter δ > 0
INIT: A0 = I, w0 = (0, . . . , 0)⊤
for t = 1, 2, . . . , T do
Observe an input instance xt ∈Rd
Computer ˆpt = [(At + xtx⊤
2 ut]⊤[(At + xtx⊤
2 xt] = u⊤
t (At + xtx⊤
Predict ˆyt = sign(ˆpt)
Draw a Bernoulli random variable Zt ∈{0, 1} of probability
IF Zt = 1 THEN
Query label yt ∈{−1, +1} and Update wt by Second-order Perceptron:
ut+1 = ut + Mtytxt, and At+1 = At + Mtxtx⊤
Let ut denote the weight vector computed by standard Perceptron, and At = I +
i≤t−1,ZiMi=1 xix⊤
i denote the correlation matrix over the mistaken trials plus an identity
matrix I, then the second-order Perceptron predicts the label of current instance xt as
ˆyt = sign(ˆpt), where ˆpt = [(At + xtx⊤
2 ut]⊤[(At + xtx⊤
2 xt] = u⊤
t (At + xtx⊤
The second-order algorithm diﬀers from standard Perceptron in that, before each prediction,
a linear transformation (At + xtx⊤
t )−1/2 is applied to both current Perceptron weight ut
and current instance xt. After prediction, the query strategy of this algorithm is the same
with the previous selective sampling: draw a Bernoulli random variable Zt ∈{0, 1} with
Pr(Zt = 1) =
δ + |ˆpt|.
Algorithm 19 gives a summary of the Selective-sampling Second-order Perceptron algorithm.
In theory, if the algorithm runs on a sequence of T rounds, for any w, the expected number
of mistakes made by the algorithm is bounded:
2γ2 w⊤E[AT ]w + 1
E ln(1 + λi)
Online Learning: A Comprehensive Survey
where ¯Lγ,T (w) = E[PT
t=1 ZtMtℓγ,t(w)] with ℓγ,t(w) = max(0, γ −ytw⊤xt), λ1, . . . , λd
are the eigenvalues of the random correlation matrix PT
t=1 ZtMtxtx⊤
and AT = I +
t=1 MtZtxtx⊤
t . Moreover, the expected number of queries by the algorithm equals PT
Furthermore, by setting δ = γ
i=1 E ln(1+λi)
, it leads to the optimal bound
t(w⊤E[AT ]w)
E ln(1 + λi)
 also proposed an improved selective-sampling algorithm based
on second-order Perceptron, which modiﬁes the sampling probability by incorporating the
second-order information, i.e., with
Pr(Zt = 1) =
δ + |ˆpt| + 1
Other second-order approaches.
Cesa-Bianchi et al. proposed a margin-based
selective sampling algorithm which also exploits second-order information in the model:
ˆyt = sign(pt),
But the query strategy is a margin-based sampling approach without explicitly exploiting
the second-order information:
Pr(Zt+1 = 1) = I proposed second-order online active learning algorithms by fully
exploiting both the ﬁrst-order and second-order information for online active learning tasks
and also gave cost-sensitive extensions for class-imbalanced tasks.
6.2.3 Other Selective Sampling Approaches
There are also a few other selective sampling approaches in which the base classiﬁer is
based on the Regularized Least Squares (RLS). In particular, on each round t, the linear
classiﬁcation model can be updated by the RLS estimate
wt = (I + St−1S⊤
t−1 + xtx⊤)−1St−1Yt−1
where matrix St−1 = [x′
1, . . . , x′
Nt−1] is the collection of Nt−1 instances queried up to time
t −1, and the vector Yt−1 = (Y ′
1, . . . , Y ′
Nt−1) is the set of queried labels for the instances.
The selective sampling algorithms that follow this paradigm include the Bound on Bias
Query (BBQ) algorithms and
their improved variants Dekel et al. ; Orabona and Cesa-Bianchi . A major
drawback of these methods is that the RLS-based base learner is more like a fashion of
batch learner instead of truly online learning, and thus the overall learning scheme might
be ineﬃcient or non-scalable if the number of queried labeled examples can be large.
Hoi, Sahoo, Lu and Zhao
6.3 Online Active Learning with Expert Advice
The idea of online active learning with expert advice dates back to classical Query by
Committee (QBC) in , where the idea is to query the
label of an instance based on the principle of maximal disagreement among a set of experts,
i.e., the conﬁdence criteria in this case is how much the expert hypotheses disagree on their
evaluation of instance predictions. QBC bounds from below the average information gain
provided by each requested label. Baram et al. considers the setting of how to online
combine an ensemble of active learners, which is executed based on a maximum entropy
criterion. Another perhaps more dominating line of studies in explore the exponentiated weighted average
forecaster for online active learning tasks, where an instance is stochastically queried based
on the available feedback on the importance of each expert in the pool.
Next we describe in detail one of the most recent approaches for online active learning
with expert advice in .
Consider an unknown sequence of instances
x1, . . . , xT ∈Rd, a “forecaster” aims to predict the class labels of every incoming instance
The forecaster sequentially computes its predictions based on the predictions from
a set of N “experts”. Speciﬁcally, at the t-th round, after receiving an instance xt, the
forecaster ﬁrst accesses the predictions of the experts {fi,t : Rd → |i = 1, . . . , N}, and
then computes its own prediction pt ∈ based on the predictions of the N experts.
After pt is computed, the true outcome yt ∈{0, 1} is disclosed. To solve this problem, the
“Exponentially Weighted Average Forecaster” (EWAF) makes the following prediction:
i=1 exp(−ηLi,t−1)fi(xt)
i=1 exp(−ηLi,t−1)
where η is a learning rate, Li,t = Pt
j=1 ℓ(fi(xj), yj),
j=1 ℓ(pj, yj) with ℓ(pt, yt) =
|pt −yt|. Unlike the above regular learning, in an active learning with expert advice task,
the outcome of an incoming instance is only revealed whenever the learner requests the label
from the environment/oracle. To solve this problem, binary variables zs ∈{0, 1}, s = 1, . . . , t
are introduced to indicate if an active forecaster has requested the label of an instance at
s-th trial. bLi,t is used to denote the loss function experienced by the active learner w.r.t.
the ith expert, i.e., bLi,t = Pt
s=1 ℓ(fi(xs), ys)zs. For this problem setting, Zhao et al. 
proposed a general framework of active forecasters, as shown in Algorithm 20.
Algorithm 20: Online Active Learning with Expert Advice
INPUT: a pool of experts fi, i = 1, . . . , N.
INIT: tolerance threshold δ and bLi,t = 0, i ∈[N].
for t = 1, 2, . . . , T do
Receive xt and compute fi(xt), i ∈[N];
Compute ˆpt =
i=1 exp(−ηbLi,t−1)fi(xt))
i=1 exp(−ηbLi,t−1)
If a conﬁdence condition is not satisﬁed
request label yt and update bLi,t = bLi,t−1 + ℓ(fi(xt), yt), i ∈[N];
Online Learning: A Comprehensive Survey
At each round, after receiving an instance xt, we compute the prediction of class label for
the instance by aggregating the prediction of each expert in the pool, i.e., fi(xt). Then, we
examine if a conﬁdence condition is satisﬁed. If so, we will skip the label request; otherwise,
the learner will request the class label for this instance from the environment. To decide
when to request the class label or not, the key idea is to seek a conﬁdence condition by
estimating the diﬀerence between pt and ˆpt. Intuitively, the smaller the diﬀerence, the more
conﬁdent we have for the prediction made by the forecaster. More speciﬁcally, the work in
 proved that for a small constant δ > 0, max1≤i,j≤N |fi(xt) −fj(xt)| ≤δ
implies |pt −bpt| ≤δ. This roughly means that, if any two experts do not disagree with each
other too much on the instance, then we can skip requiring its label.
In addition to the above work, there are also a few other active learning strategies for
online learning with expert advices, for example the active greedy forecaster . Online active learning with expert advice can be applied in some real-world applications, e.g., crowdsourcing tasks where the learner attempts to address
both the diverse quality of annotators’ performance with expert learning and eﬃcient annotation in seeking informative data using active learning.
7. Online Semi-supervised Learning
7.1 Overview
Semi-Supervised Learning (SSL) has been an important class of machine learning tasks
and techniques, which aims to make use of unlabeled data for learning tasks. It has been
extensively studied mostly in the settings of batch learning and some comprehensive surveys
can be found in . When online learning meets semisupervised learning, there are two major branches of research. One major branch of studies
is to turn traditional batch semi-supervised learning methods into online algorithms such
that they can work from data streams of both labeled and unlabeled data, which we call
this setting as “Online Semi-supervised Learning” and we will review a popular framework
of “online manifold regularization”. The other branch of studies is to study classical online
learning tasks in transductive learning settings (e.g., by assuming unlabeled data can be
made available before online learning tasks), which we call this setting as “Transductive
Online Learning”. We note that online active learning as introduced previously can be
generally viewed as a special type of online semi-supervised learning where an online learner
has to deal with both labeled and unlabeled data.
7.2 Online Manifold Regularization
In the area of semi-supervised learning, one major framework for semi-supervised learning is
based on manifold regularization , where the learner not only minimizes
the loss on the labeled data, but also minimizes the diﬀerence of predictions on the unlabeled
instances which are similar on the manifold. Speciﬁcally, consider instances (xt, yt), t ∈
{1, ...T}, the idea is to minimize the following objective function
δ(yt)ℓ(f(xt), yt) + λ1
2 ||f||2 + λ2
(f(xs) −f(xt))wst
Hoi, Sahoo, Lu and Zhao
where the ﬁrst term is the loss on labeled instances where δ(yt) = 1 if and only if yt exists
and l is the number of labeled data, the second term is a classic regularization term for
supervised learning, and the last term is the manifold regularization on unlabeled data.
In literature, online manifold regularization has been explored ,
which attempts to turn batch manifold regularization algorithms into online/incremental
algorithms. Speciﬁcally, the above objective can be returned online for each instance:
l δ(yt)ℓ(f(xt), yt) + λ1
2 ||f||2 + λ2
(f(xi) −f(xt))wit
It can be solved using Online Gradient Descent in O(T 2) time. Unfortunately, such straightforward solution is expensive in both time and space, since the calculation of the last term
requires to store all instances and measure the similarity wit between the incoming instances
and all existing ones.
To address this problem, the authors oﬀer two sparse approximations of the objective
The ﬁrst solution is not to keep all instances but to keep only the newest τ
ones, where τ is the buﬀer size. This strategy is simple but not very eﬃcient since the
discarded old instances may contain important information. The second solution adopts a
random projection tree to ﬁnd s cluster centers during online learning. Finally, instead of
calculating the similarity between xt and all existing instances, the algorithm only consider
the s cluster centers as the most representative instances.
In addition to the above work, proposed a fast approximate algorithm
for online semi-supervised learning. which leverages the incremental k-center quantization
method to group neighboring points so as to yield a set of reduced representative points,
and as a result an approximate similarity graph can be constructed to ﬁnd the harmonic
solution in semi-supervised learning .
Finally, there were some related eﬀorts on online active semi-supervised learning , which extends active learning in the online semi-supervised learning
settings. For example, following such kind of setting, developed the
OASIS algorithm by using a general online Bayesian learning framework.
7.3 Transductive Online Learning
Transductive online learning is a
niche class of online learning tasks, where we want to learn from an arbitrary sequence of
labeled examples (x1, y1), . . . , (xT , yT ) by making the assumption that the set of unlabeled
instances (x1, . . . , xT ) can be given in advance to the learner before an online learning
task begins. In particular, the work proposed an eﬃcient
algorithm based on the principle of prediction with expert advice by combining “random
playout” and “randomized rounding” of loss subgradients. We note that this niche topic
has received very few attention, possibly because of their assumption of obtaining unlabeled
data in advance, which may be unrealistic in many applications. However, the studies in
this niche family of studies may provide some theory insights about the linkage between
online learning and batch learning as demonstrated in .
Online Learning: A Comprehensive Survey
8. Online Unsupervised Learning
8.1 Overview
In this section we brieﬂy review some key work in the literature of online unsupervised
learning, where models are learned from unlabeled data streams where no explicit feedback is available. Broadly, we categorize the existing work into four major groups: Online
Clustering, Online Dimension Reduction, Online Anomaly Detection, and Online Density
Estimation. Due to the vast number of ways in which unsupervised learning in online settings have been explored in literature, and numerous applications for which algorithms are
designed, it is almost impossible to make a comprehensive treatment on this topic in this
survey. Instead, we try to focus on the key areas and give a general overview of the main
ideas in each area which are closely related to online learning.
8.2 Online Clustering
Clustering is an unsupervised learning process of grouping unlabeled data instances such
that instances in the same group are similar, and instances between groups are dissimilar. It gives an eﬀective mechanism to summarize the data, and does not require labels
of the instances in order to perform the clustering. For batch learning settings, clustering is usually classiﬁed into following categories: partition based clustering, hierarchical
clustering, density-based clustering, and grid-based clustering . For online
settings , partition-based and density-based clustering have been studied
more extensively. In the following we brieﬂy review some of online learning approaches for
clustering on streaming data especially for these two categories.
Partitioning Based clustering methods split the instances into partitions where each
partition represents a cluster. The partitions are designed on the basis of some distance
measures (e.g. Euclidean distance). The number of clusters is usually pre-deﬁned by the
The most popular algorithms in this category are those based on k-MEANS and
k-MEDOIDS algorithm. The k-MEANS algorithm is one of the oldest and most popular
clustering methods, where the idea is to identify k centroids, where each centroid corresponds to one out of k clusters by minimizing the sum of square errors between each
instance to their corresponding centroids. Sequential algorithms performing k-MEDOID or
k-MEDIAN clustering usually try to break the stream of instances into chunks where the
size of each chunk is set based on some pre-speciﬁed memory budget. Given a data stream
D, it is broken into several chunks denoted by D1, D2, . . . , Dt, . . . where each chunk contains
at most m instances, where m is the budget of the chunks. In such a case, k-MEDIANS
can be directly applied to each chunk. This framework is called the STREAM framework.
 . There are also sampling approaches designed for clustering when the data streams are extremely large . Another method is the StreamKM++ . In this
approach, ﬁrst, an adaptive non-uniform sampling approach is used to obtain small coresets
from the data streams. The coreset construction is done by the utilization of coreset tree
proposed in this paper which helps in signiﬁcant speed up.
Density-based clustering.
Most clustering techniques suﬀer from several drawbacks.
First, many of them (e.g.
k-MEANS) are designed for only spherical clusters and can
Hoi, Sahoo, Lu and Zhao
not adapt to arbitrary cluster shapes. In addition, the value of k, or the number of clusters
has to be known a priori. Lastly, these methods are susceptible to outliers. Density based
clustering algorithms (most popularly DBSCAN and its variants) are able to address all
these challenges. Density based approaches cluster dense regions which are separated by
sparse regions. A cluster based on density can take on arbitrary shapes, does not require
prior knowledge of the number of clusters, and is robust to outliers in the data. However,
performing density based clustering on streaming data in an online manner is plagued with
several challenges including dynamic evolution of the clusters, limited memory space, etc.
Following , we categorize the online density-based clustering algorithms into Micro-clustering Algorithms and Grid-based clustering Algorithms.
The micro-clustering algorithms aim to summarize a data in an online manner, and the
clustering is performed using these summaries . Grid-based
methods, divide the entire instance space into grids, and each instance upon arrival is assigned a grid, and then the clustering is then done based on the density of the grids .
Other Clustering Methods.
Hierarchical Clustering is a paradigm in which either a
bottom-up approach or a top-down approach is used to gradually agglomerate the data
points together. This results in a tree of clusters, which is also called a dendogram. Among
the earliest approaches to incremental hierarchical clustering was CobWeb ,
which determines how to insert a new data point into the tree structure based on a category utility criteria. Recent hierarchical clustering algorithms include the ClusTree , which oﬀers a compact self adapting index structure for storing stream summaries in addition to giving more importance to recent data, and Perch , which allows the clustering to scale to a large number of data points and clusters.
 propose an incremental approach to do Hierarchical clustering of the data,
in addition to accounting for variance and density of the data. Some techniques have been
developed for online clustering for very high dimensional data where the data sparsity makes
it very hard to perform clustering as many instances tend to be equidistant from one another. HPStream introduces a concept of projected clustering to
data streams. There are online clustering algorithms for other speciﬁc scenarios such as
clustering of discrete and categorical streams, text streams, uncertain data streams, graph
streams as well as distributed clustering .
8.3 Online Dimension Reduction
When the feature dimensions are very large, Dimension Reduction (DR) techniques can
be used to improve learning eﬃciency, compress original data, visualize data better, and
improve its applicability to real-world applications. Consider instance xt ∈Rd, the goal
of dimension reduction is to learn a new instance ˆxt ∈Rk where k ≤d by following
some principle of unsupervised learning. There have been several approaches to unsupervised dimensional reduction. We broadly categorize them into two major groups of studies:
subspace learning and manifold learning More comprehensive surveys of classic dimension
reduction techniques can be found in .
Online Learning: A Comprehensive Survey
Subspace Learning.
This class of DR methods aims to ﬁnd an optimal linear mapping
of input data in high-dimensional space to a lower-dimensional space. In general, there are
two major types of approaches: linear methods and nonlinear methods. Popular linear subspace methods include Principal Component Analysis (PCA) and Independent Component
Analysis (ICA), etc. Nonlinear methods often extend the linear subspace learning methods
using kernel tricks. Examples include Kernel PCA, Kernel ICA, etc.
For online dimension reduction tasks, more popular eﬀorts have been focused on addressing online PCA for unsupervised learning on streaming data settings in literature
 , while there are also a few studies for online ICA . For nonlinear space learning methods, online Kernel-PCA has also received some
research interests .
Manifold learning.
This class of DR methods generally belongs to nonlinear DR techniques. Manifold learning assumes that input data lie on an embedded non-linear manifold
within the high-dimensional space. DR by manifold learning aims to ﬁnd a low-dimensional
representation by preserving some properties of the manifold. For example, some methods
preserving global properties include Multi-dimensional scaling (MDS), and IsoMap , while some preserving local properties including Locally Linear Embedding (LLE) and Laplacian Eigenmaps.
For online manifold learning settings, the are some eﬀorts for achieving the incremental
approaches of manifold learning in literature. For example, Law and Jain proposed
an incremental learning algorithm for ISOMAP and Schuon et al. presented an online
approach for LLE.
8.4 Online Density Estimation
Online density estimation refers to constructing an estimate of an underlying unobservable probability density function based on observed data streams . In
literature, there are many diﬀerent approaches to perform density estimation, e.g., histograms, naive estimator, nearest neighbour methods, Parzen windows, etc. Among various approaches, Kernel Density Estimation (KDE) is probably one of the most extensively
explored topics in density estimation, which is a non-parametric way to estimate the probability density function of a target random variable . Here, we brieﬂy review and
categorize some of the commonly used approaches for kernel density estimation in onlinelearning settings. Given a sequence of instances D = {x1, . . . , xT }, where xt ∈Rd, KDE
estimates the density at a point x as
κ(x, xt) = 1
where the kernel κ(x, xt) is a radially symmetric unimodal function that integrates to 1
and h is a smoothing parameter called the bandwidth. Like in the case of Online Learning
with Kernels , this problem suﬀers from the curse of
kernelization, which means to estimate the density at any point x, it requires computing
the kernel function with respect to all the data points observed in the data stream so far.
Hoi, Sahoo, Lu and Zhao
There have been several attempts to overcome this curse of kernelization, and can be
grouped into Merging and Sampling approaches. Merging approaches require a pre-speciﬁed
budget on how many instances or kernels can be stored in memory. A newly arriving sample
will (typically) be stored in memory as a kernel, unless the budget is exceeded. If the budget
is exceeded, two or more similar kernels get merged. The merging criteria depends on some
objective function. Some eﬀorts in this direction include , which usually diﬀer in how they select the bandwidth
values. Another approach in performs clustering using self-organizing
maps, and leverages this to perform kernel merging. Sampling approaches randomly select
points to be kept in memory, but attempts to maintain a certain level of accuracy . Recent approaches try to perform online density estimation with eﬃcient methods for bandwidth selection and also to capture changes in the data
distribution. Finally, online KDE techniques can be applied and integrated with real-world
applications, such as real-time visual tracking .
8.5 Online Anomaly Detection
Anomaly Detection (AD), also known as “outlier detection” or “novelty detection”, is the
process of detecting abnormal behavior in the data. The deﬁnition of abnormal behaviors can be very subjective, and the notion of “anomaly” varies from domain to domain.
Anomaly detection research is abundant in literature due to its wide applications. Example applications include but not limited to intrusion detection, fraud detection, medical
anomaly detection, industrial damage detection, amongst others. Anomaly detection has
been extensively studied by many communities in a wide range of diverse settings, ranging
from supervised to unsupervised and semi-supervised learning, and batch learning to online
learning settings. More comprehensive surveys of classic anomaly detection studies can be
found in . In this survey, we will focus online
anomaly detection in unsupervised learning settings, which we believe it is one of the most
popular and dominating scenarios in many real-world applications.
According to the literature surveys, unsupervised anomaly detection can be grouped
into several major categories, including Distance based, Density based, Clustering based,
Statistical methods, and others (such as subspace and one-class learning, etc).
following, we brieﬂy review some of popular work by focusing on online learning settings.
In literature, distance based online AD algorithms have been extensively studied in the
context of unsupervised learning over data streams using distance-based methods . Some
typical strategies of distance-based online AD approaches is to apply the sliding window
model where distance-based anomalies/outliers can be detected in the current window. In
addition to distance-based methods, there are also some other studies that explore diﬀerent
methods for online AD in data streams, such as using one-class anomaly detector or online clustering based approaches . We note that,
despite extensive and diverse studies in the ﬁeld of online anomaly detection, from a machine
learning perspective, many of theses approaches (e.g., sliding windows based) not purely
learn in an online-learning fashion and many are not designed in machine learning based
manners. We therefore keep the review of this part brief and concise.
Online Learning: A Comprehensive Survey
9. Related Areas and Other Terminologies
9.1 Overview
In this section, we discuss the relationship of online learning with other related areas and terminologies which sometimes may be confused. We note that some of the following remarks
may be somewhat subjective, and their meanings may vary in diverse contexts whereas
some terms and notions may be used interchangeably.
9.2 Incremental Learning
Incremental learning, or decremental learning, represents a family of machine learning techniques , which are particularly
suitable for learning from data streams. There are various deﬁnitions of incremental learning/decremental learning. The basic idea of incremental learning is to learn some models from a stream of training instances with limited space and computational costs, often
attempting to approximate a traditional batch machine learning counterpart as much as
possible. For example, incremental SVM aims to train an SVM classiﬁer
the same as a batch SVM in an incremental manner where one training instance is added
for updating the model each time (and similarly a training instance can be removed by
updating the model decrementally).
Incremental learning can work either in online learning or batch learning manners . For the incremental online learning , only one example is presented for updating the model at one time, while for the incremental batch learning , a batch of multiple training examples are used for updating the model each
time. Incremental learning (or decremental learning) methods are often natural extensions
of existing supervised learning or unsupervised learning techniques for addressing eﬃciency
and scalability when dealing with real-world data particularly arriving in stream-based settings. Generally speaking, incremental learning can be viewed as a branch of online learning
and extensions for adapting traditional oﬄine learning counterparts in data-stream settings.
9.3 Sequential Learning
Sequential learning is mainly concerned with learning from sequential training data , formulated as follows: a learner trains a model from a collection of N training
data pairs {(x(i), y(i)), i = 1, . . . , N} where x(i) = (xi
2, . . . , xi
Ni) is an Ni-dimensional instance vector and y(i) = (yi
2, . . . , yi
Ni) is an Ni-dimensional label vector. It can be viewed
as a special type of supervised learning, known as structured prediction or structured (output) learning , where the goal is to predict structured objects (e.g., sequence
or graphs), rather than simple scalar discrete (“classiﬁcation”) or real values (“regression”).
Unlike traditional supervised learning that often assume data is independently and identically distributed, sequential learning attempts to exploit signiﬁcant sequential correlation of
sequential data when training the predictive models. Some classical methods of sequential
learning include sliding window methods, recurrent sliding windows, hidden Markov models,
conditional random ﬁelds, and graph transformer networks, etc. There are also many recent
studies for structured prediction with application to sequential learning . In general, sequential learning can be solved by either batch or online learning
Hoi, Sahoo, Lu and Zhao
algorithms. Finally, it is worth mentioning another closely related learning, i.e., “sequence
classiﬁcation”, whose goal is to predict a single class output for a whole input “sequence”
instance. Sequence classiﬁcation is a special case of sequential learning with the target class
vector reduced to a single variable. It is generally simpler than regular sequential learning,
and can be solved by either batch or online learning algorithms.
9.4 Stochastic Learning
Stochastic learning refers to a family of machine learning algorithms by following the theory and principles of stochastic optimization ,
which have achieved great successes for solving large-scale machine learning tasks in practice . Stochastic learning is closely related to online learning.
Typically, stochastic learning algorithms are motivated to accelerate the training speed of
some existing batch machine learning methods for large-scale machine learning tasks, which
may be often solved by batch gradient descent algorithms. Stochastic learning algorithms,
e.g., Stochastic Gradient Descent (SGD) or a.k.a Online Gradient Descent (OGD) in online learning terminology, often operate sequentially by processing one training instance
(randomly chosen) each time in an online learning manner, which thus are computationally more eﬃcient and scalable than the batch GD algorithms for large-scale applications.
Rather than processing a single training instance each time, a more commonly used stochastic learning technique in practice is the mini-batch SGD algorithm , which processes a small batch of training instances each
time. Thus, stochastic learning can be viewed as a special family of online learning algorithms and extensions, while online learning may explore more other topics and challenges
beyond stochastic learning/optimizations.
9.5 Adaptive Learning
This term is occasionally used in the machine learning and neural networks ﬁelds. There
is no a very formal deﬁnition about what exactly is adaptive learning in literature.
literature, there are quite a lot of diﬀerent studies more or less concerned with adaptive
learning , which attempt to adapt a learning model/system
(e.g., neural networks) for dynamically changing environments over time. In general, these
existing works are similar to online learning in that the environment is often changing
and evolving dynamically. But they are diﬀerent in that they are not necessarily purely
based on online learning theory and algorithms. Some of these works are based on heuristic
adaptation/modiﬁcation of existing batch learning algorithms for updating the models with
respect to the environment changes. Last but not least, most of these existing works are
motivated by diﬀerent kinds of heuristics, generally lack solid theoretical analysis and thus
can seldom give performance guarantee in theory.
9.6 Interactive Learning
Traditional machine learning mostly works in a fully automated process where training
data are collected and prepared typically with the aid of domain experts. By contrast,
interactive (machine) learning aims to make the machine learning procedure interactive by
Online Learning: A Comprehensive Survey
engaging human (users or domain experts) in the loop . The advantages of interactive learning include the natural integration of domain
knowledge in the learning process, eﬀective communication and continuous improvements
for learning eﬃcacy through the interaction between learning systems and users/experts.
Online learning often plays an important role in an interactive learning system, in which
active (online) learning can be used in ﬁnding the most informative instances to save labeling
costs, incremental (online) learning algorithms could be applied for updating the models
sequentially, and/or bandit online learning algorithms may be used for decision-making to
trade oﬀexploration and exploitation in some scenarios.
9.7 Reinforcement Learning
Reinforcement Learning (RL) is a branch
of machine learning inspired by behaviorist psychology, which is often concerned with how
software agents should take actions in an environment to maximize cumulative rewards. The
goal of an agent in RL is to ﬁnd a good policy and state-update function by attempting
to maximize the the expected sum of discounted rewards. RL is diﬀerent from supervised
learning in that the goal of supervised learning is to
reconstruct an unknown function f that can assign the desired output values y to input data
x; while the goal of RL is to ﬁnd the input (policy/action) x that gives the maximum reward
R(x). In general, RL can work either in batch or online learning manners. In practice, RL
methods are commonly applied to problems involving sequential dynamics and optimization
of some objectives, typically with online exploration of the eﬀects of actions. RL is closely
related to bandit online learning with the similar goal of ﬁnding a good policy that has
to balance the tradeoﬀbetween exploration (of uncertainty) and exploitation (of known
knowledge). Many RL solutions follow the same ideas of multi-armed bandits, and some
bandit algorithms were also inspired by the ﬁeld of RL studies too. However, RL can be
more general when learning to interact with more complex scenarios and environments.
9.8 Continual Learning
Continual Learning, also called “Lifelong Learning” is a ﬁeld of machine learning inspired by human ability to learn
new tasks throughout their lifespan. When new tasks arrive, humans are able to leverage
existing knowledge, and more eﬀectively learn the new tasks, and at the same time, they
do not forget how to perform the old tasks. In formal settings, the tasks arrive sequentially,
but instances for each task arrive as a batch, and thus each task is still learned in batch
settings. While older methods used linear models for lifelong learning , recent eﬀorts have been focused on continual learning with neural networks , in which one of key challenges is to address the catastrophic forgetting, a
problem which traditional machine learning including neural networks is often susceptible
to, but humans are immune to. When new tasks are learned, traditional machine learning
tends to forget how to perform older tasks, and a major research direction in continual
learning is to develop algorithms that can address this catastrophic forgetting. Although
continual learning is closely related to online learning, most existing studies still follow the
paradigm of batch training, which are not considered as online learning algorithms.
Hoi, Sahoo, Lu and Zhao
10. Conclusions
10.1 Concluding Remarks
This paper gave a comprehensive survey of existing online learning works and reviewed
ongoing trends of online learning research. In theory, online learning methodologies are
founded primarily based on learning theory, optimization theory, and game theory. According to the type of feedback to the learner, the existing online learning methods can be
roughly grouped into the following three major categories:
• Supervised online learning is concerned with the online learning tasks where full
feedback information is always revealed to the learner, which can be further divided
into three groups: (i) “‘linear online learning” that aims to learn a linear predictive
model, (ii) “nonlinear online learning” that aims to learn a nonlinear predictive model,
and (iii) non-traditional online learning that addresses a variety of supervised online
learning tasks which are diﬀerent from traditional supervised prediction models for
classiﬁcation and regression.
• Online learning with limited feedback is concerned with the online learning tasks
where the online learner receives partial feedback information from the environment
during the learning process.
The learner often has to make online predictions or
decisions by achieving a tradeoﬀbetween the exploitation of disclosed knowledge and
the exploration of unknown information.
• Unsupervised online learning is concerned with the online learning tasks where
the online learner only receives the sequence of data instances without any additional
feedback (e.g., true class label) during the online learning tasks. Examples of unsupervised online learning include online clustering, online representation learning, and
online anomaly detection tasks, etc.
In this survey, we have focused more on the ﬁrst category of work because it has received
more research attention than the other two categories in literature. This is mainly because
supervised online learning is a natural extension of traditional batch supervised learning,
and thus an online supervised learning technique could be directly applied to a wide range
of real-world applications especially for real-time machine learning from data streams where
conventional batch supervised learning techniques may suﬀer critical limitations. However,
we do note that in contrast to supervised online learning, the problems of online learning
with limited feedback or unsupervised online learning are generally much more challenging,
and thus should attract more research attentions and eﬀorts in the future.
10.2 Future Directions
Despite the extensive studies in literature, there are still many open issues and challenges,
which have not been fully solved by the existing works and need to be further explored by
the community eﬀorts in the future work. In the following, we highlight a few important
and emerging research directions for researchers who are interested in online learning.
First of all, although supervised online learning has been extensively studied, learning
from non-stationary data streams remain an open challenge.
In particular, one critical
Online Learning: A Comprehensive Survey
challenge with supervised online learning is to address “concept drifting” where the target
concepts to be predicted may change over time in unforeseeable ways. Although many online
learning studies have attempted to address concept drifting by a variety of approaches,
they are fairly limited in that they often make some restricted assumptions for addressing
certain types of concept drifting patterns. In general, there still lacks of formal theoretical
frameworks or principled ways for resolving all types of concept drifting issues, particularly
for non-stationary settings where target concepts may drift over time in arbitrary ways.
Second, an important growing trend of online learning research is to explore large-scale
online learning for real-time big data analytics. Although online learning has huge advantages over batch learning in eﬃciency and scalability, it remains a non-trivial task when
dealing with real-world big data analytics with extremely high volume and high velocity.
Despite extensive research in large-scale batch machine learning, more future research eﬀorts
should address parallel online learning and distributed online learning by exploiting various
computational resources, such as high-performance computing machines, cloud computing
infrastructures, and perhaps low-cost IoT computing environments.
Third, another challenge of online learning is to address the “variety” in online data
analytics tasks. Most existing online learning studies are often focused on handling singlesource structured data typically by vector space representations. In many real-world data
analytis applications, data may come from multiple diverse sources and could contain different types of data (such as structured, semi-structured, and unstructured data). Some
existing studies, such as the series of online multiple kernel learning works, have attempted
to address some of these issues, but certainly have not yet fully resolved all the challenges
of variety. In the future, more research eﬀorts should address the “variety” challenges, such
as multi-source online learning, multi-modal online learning, etc.
Fourth, existing online learning works seldom address the data “veracity” issue, that is,
the quality of data, which can considerably aﬀect the eﬃcacy of online learning. Conventional online learning studies often implicitly assume data and feedback are given in perfect
quality, which is not always true for many real-world applications, particularly for real-time
data analytics tasks where data arriving on-the-ﬂy may be contaminated with noise or may
have missing values or incomplete data without applying advanced pre-processing. More
future research eﬀorts should address the data veracity issue by improving the robustness
of online learning algorithms particularly when dealing with real data of poor quality.
Fifth, due to the remarkable successes and impact of deep learning techniques for various
applications in recent years, another emerging and increasingly important topic is “online
deep learning” , i.e., learning deep neural networks from data streams
on the ﬂy in an online fashion. Despite some preliminary research, we note there are still
many research challenges in this ﬁeld, e.g., how to balance the tradeoﬀbetween learning
accuracy, computational eﬃciency, learning scalability and model complexity.
Last but not least, we believe it can be valuable to explore “online continual learning” by
extending traditional continual learning methods for pure online-learning settings, which is
more natural for many real-world applications where data for either existing and novel tasks
are often arriving in a streaming and continuous fashion. Some research progress in online
deep learning might be applied here, but the key challenge of continual online learning is to
resolve the catastrophic forgetting problem across tasks during the online learning process.
Hoi, Sahoo, Lu and Zhao