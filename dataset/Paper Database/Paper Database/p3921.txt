Model Compression with Adversarial Robustness:
A Uniﬁed Optimization Framework
Shupeng Gui⋄,∗, Haotao Wang†,∗, Haichuan Yang⋄, Chen Yu⋄,
Zhangyang Wang† and Ji Liu‡
⋄Department of Computer Science, University of Rochester
†Department of Computer Science and Engineering, Texas A&M University
‡Ytech Seattle AI lab, FeDA lab, AI platform, Kwai Inc
†{htwang, atlaswang}@tamu.edu
⋄{sgui2, hyang36, cyu28}@ur.rochester.edu
‡ 
Deep model compression has been extensively studied, and state-of-the-art methods
can now achieve high compression ratios with minimal accuracy loss. This paper
studies model compression through a different lens: could we compress models
without hurting their robustness to adversarial attacks, in addition to maintaining
accuracy? Previous literature suggested that the goals of robustness and compactness might sometimes contradict. We propose a novel Adversarially Trained Model
Compression (ATMC) framework. ATMC constructs a uniﬁed constrained optimization formulation, where existing compression means (pruning, factorization,
quantization) are all integrated into the constraints. An efﬁcient algorithm is then
developed. An extensive group of experiments are presented, demonstrating that
ATMC obtains remarkably more favorable trade-off among model size, accuracy
and robustness, over currently available alternatives in various settings. The codes
are publicly available at: 
Introduction
Background: CNN Model Compression
As more Internet-of-Things (IoT) devices come online,
they are equipped with the ability to ingest and analyze information from their ambient environments
via sensor inputs. Over the past few years, convolutional neural networks (CNNs) have led to rapid
advances in the predictive performance in a large variety of tasks . It is appealing to deploy CNNs
onto IoT devices to interpret big data and intelligently react to both user and environmental events.
However, the model size, together with inference latency and energy cost, have become critical
hurdles . The enormous complexity of CNNs remains a major inhibitor for their more extensive
applications in resource-constrained IoT systems. Therefore, model compression is becoming
increasingly demanded and studied . We next brieﬂy review three mainstream compression
methods: pruning, factorization, and quantization.
Pruning refers to sparsifying the CNN by zeroing out non-signiﬁcant weights, e.g., by thresholding
the weights magnitudes . Various forms of sparsity regularization were explicitly incorporated in
the training process , including structured sparsity, e.g., through channel pruning .
Most CNN layers consist of large tensors to store their parameters , in which large redundancy
exists due to the highly-structured ﬁlters or columns . Matrix factorization was thus adopted
∗The ﬁrst two authors Gui and Wang contributed equally and are listed alphabetically.
33rd Conference on Neural Information Processing Systems , Vancouver, Canada.
 
to (approximately) decompose large weight matrices into several much smaller matrix factors . Combining low-rank factorization and sparse pruning showed further effectiveness .
Quantization saves model size and computation by reducing ﬂoat-number elements to lower numerical
precision, e.g., from 32 bits to 8 bits or less . The model could even consist of only binary
weights in the extreme case . Beyond scalar quantization, vector quantization was widely
adopted too in model compression for parameter sharing . also integrated pruning
and quantization in one ADMM optimization framework.
Adversarial Robustness: Connecting to Model Compression?
On a separate note, the prevailing deployment of CNNs also calls for attention to their robustness.
Despite their impressive predictive powers, the state-of-the-art CNNs remain to commonly suffer from
fragility to adversarial attacks, i.e., a well-trained CNN-based image classiﬁer could be easily fooled to
make unreasonably wrong predictions, by perturbing the input image with a small, often unnoticeable
variation . Other tasks, such as image segmentation and graph classiﬁcation , were
all shown to be vulnerable to adversarial attacks. Apparently, such ﬁndings put CNN models in
jeopardy for security- and trust-sensitive IoT applications, such as mobile bio-metric veriﬁcation.
There are a magnitude of adversarial defense methods proposed, ranging from hiding gradients ,
to adding stochasticity , to label smoothening/defensive distillation , to feature squeezing , among many more . A handful of recent works pointed out that those empirical
defenses could still be easily compromised , and a few certiﬁed defenses were introduced .
To our best knowledge, there have been few existing studies on examining the robustness of compressed models: most CNN compression methods are evaluated only in terms of accuracy on the
(clean) test set. Despite their satisfactory accuracies, it becomes curious to us: did they sacriﬁce
robustness as a “hidden price” paid? We ask the question: could we possibly have a compression
algorithm, that can lead to compressed models that are not only accurate, but also robust?
The answer yet seems to highly non-straightforward and contextually varying, at least w.r.t. different
means of compression. For example, showed that sparse algorithms are not stable: if an
algorithm promotes sparsity, then its sensitivity to small perturbations of the input data remains
bounded away from zero (i.e., no uniform stability properties). But for other forms of compression,
e.g., quantization, it seems to reduce the Minimum Description Length and might potentially
make the algorithm more robust. In deep learning literature, argued that the tradeoff between
robustness and accuracy may be inevitable for the classiﬁcation task. This was questioned by ,
whose theoretical examples implied that a both accurate and robust classiﬁer might exist, given
that classiﬁer has sufﬁciently large model capacity (perhaps much larger than standard classiﬁers).
Consequently, different compression algorithms might lead to different trade-offs between robustness
and accuracy. empirically discovered that an appropriately higher CNN model sparsity led to
better robustness, whereas over-sparsiﬁcation (e.g., less than 5% non-zero parameters) could in turn
cause more fragility. Although sparsiﬁcation (i.e., pruning) is only one speciﬁc case of compression,
the observation supports a non-monotonic relationship between mode size and robustness.
A few parallel efforts discussed activation pruning or quantization as defense ways. While
potentially leading to the speedup of model inference, they have no direct effect on reducing model
size and therefore are not directly “apple-to-apple” comparable to us. We also notice one concurrent
work combining adversarial training and weight pruning. Sharing a similar purpose, our method
appears to solve a more general problem, by jointly optimizing three means of pruning, factorization
quantization wr.t. adversarial robustness. Another recent work studied the transferability of
adversarial examples between compressed models and their non-compressed baseline counterparts.
Our Contribution
As far as we know, this paper describes one of the ﬁrst algorithmic frameworks that connects model
compression with the robustness goal. We propose a uniﬁed constrained optimization form for
compressing large-scale CNNs into both compact and adversarially robust models. The framework, dubbed adversarially trained model compression (ATMC), features a seamless integration
of adversarial training (formulated as the optimization objective), as well as a novel structured
compression constraint that jointly integrates three compression mainstreams: pruning, factorization
and quantization. An efﬁcient algorithm is derived to solve this challenging constrained problem.
While we focus our discussion on reducing model size only in this paper, we note that ATMC could be
easily extended to inference speedup or energy efﬁciency, with drop-in replacements of the constraint
(e.g., based on FLOPs) in the optimization framework.
We then conduct an extensive set of experiments, comparing ATMC with various baselines and
off-the-shelf solutions. ATMC consistently shows signiﬁcant advantages in achieving competitive
robustness-model size trade-offs. As an interesting observation, the models compressed by ATMC
can achieve very high compression ratios, while still maintaining appealing robustness, manifesting
the value of optimizing the model compactness through the robustness goal.
Adversarially Trained Model Compression
In this section, we deﬁne and solve the ATMC problem. ATMC is formulated as a constrained minmax optimization problem: the adversarial training makes the min-max objective (Section 2.1), while
the model compression by enforcing certain weight structures constitutes the constraint (Section 2.2).
We then derive the optimization algorithm to solve the ATMC formulation (Section 2.3).
Formulating the ATMC Objective: Adversarial Robustness
We consider a common white-box attack setting . The white box attack allows an adversary to
eavesdrop the optimization and gradients of the learning model. Each time, when an “clean" image x
comes to a target model, the attacker is allowed to “perturb” the image into x′ with an adversarial
perturbation with bounded magnitudes. Speciﬁcally, let ∆≥0 denote the predeﬁned bound for the
attack magnitude, x′ must be from the following set:
∞(x) := {x′ : ∥x′ −x∥∞≤∆} .
The objective for the attacker is to perturb x′ within B∆
∞(x), such as the target model performance is
maximally deteriorated. Formally, let f(θ; x, y) be the loss function that the target model aims to
minimize, where θ denotes the model parameters and (x, y) the training pairs. The adversarial loss,
i.e., the training objective for the attacker, is deﬁned by
f adv(θ; x, y) =
∞(x) f(θ; x′, y)
It could be understood that the maximum (worst) target model loss attainable at any point within
∞(x). Next, since the target model needs to defend against the attacker, it requires to suppress the
worst risk. Therefore, the overall objective for the target model to gain adversarial robustness could
be expressed as Z denotes the training data set:
f adv(θ; x, y).
Integrating Pruning, Factorization and Quantization for the ATMC Constraint
As we reviewed previously, typical CNN model compression strategies include pruning (element-level
 , or channel-level ), low-rank factorization , and quantization . In this
work, we aim to integrate all three into a uniﬁed, ﬂexible structural constraint.
Without loss of generality, we denote the major operation of a CNN layer (either convolutional
or fully-connected) as xout = W xin, W ∈Rm×n, m ≥n; computing the non-linearity (neuron)
parts takes minor resources compared to the large-scale matrix-vector multiplication. The basic
pruning encourages the elements of W to be zero. On the other hand, the factorization-based
methods decomposes W = W1W2. Looking at the two options, we propose to enforce the following
structure to W (k is a hyperparameter):
W = UV + C,
∥U∥0 + ∥V ∥0 + ∥C∥0 ≤k,
where ∥· ∥0 denotes the number of nonzeros of the augment matrix. The above enforces a novel,
compound (including both multiplicative and additive) sparsity structure on W , compared to existing
sparsity structures directly on the elements of W . Decomposing a matrix into sparse factors
was studied before , but not in a model compression context. We further allow for a sparse
error C for more ﬂexibility, as inspired from robust optimization . By default, we choose
U ∈Rm×m, V ∈Rm×n in (3).
Many extensions are clearly available for equation 3. For example, the channel pruning enforces
rows of W to be zero, which could be considered as a specially structured case of basic element-level
pruning. It could be achieved by a drop-in replacement of group-sparsity norms. We choose ℓ0 norm
here both for simplicity, and due to our goal here being focused on reducing model size only. We
recognize that using group sparsity norms in (3) might potentially be a preferred option if ATMC will
be adapted for model acceleration.
Quantization is another powerful strategy for model compression . To maximize the
representation capability after quantization, we choose to use the nonuniform quantization strategy
to represent the nonzero elements in DNN parameter, that is, each nonzero element of the DNN
parameter can only be chosen from a set of a few values and these values are not necessarily evenly
distributed and need to be optimized. We use the notation | · |0 to denote the number of different
values except 0 in the augment matrix, that is,
|M|0 := |{Mi,j : Mi,j ̸= 0 ∀i ∀j}|
For example, for M = [0, 1; 4; 1], ∥M∥0 = 3 and |M|0 = 2. To answer the all nonzero elements
of {U (l), V (l), C(l)}, we introduce the non-uniform quantization strategy (i.e., the quantization
intervals or thresholds are not evenly distributed). We also do not pre-choose those thresholds, but
instead learn them directly with ATMC, by only constraining the number of unique nonzero values
through predeﬁning the number of representation bits b in each matrix, such as
|U (l)|0 ≤2b, |V (l)|0 ≤2b, |C(l)|0 ≤2b
ATMC: Formulation
Let us use θ to denote the (re-parameterized) weights in all L layers:
θ := {U (l), V (l), C(l)}L
We are now ready to present the overall constrained optimization formulation of the proposed ATMC
framework, combining all compression strategies or constraints:
f adv(θ; x, y)
∥U (l)∥0 + ∥V (l)∥0 + ∥C(l)∥0
≤k, (sparsity constraint)
θ ∈Qb := {θ : |U (l)|0 ≤2b, |V (l)|0 ≤2b, |C(l)|0 ≤2b ∀l ∈[L]}.(quantization constraint)
Both k and b are hyper-parameters in ATMC: k controls the overall sparsity of θ, and b controls the
quantization bit precision per nonzero element. They are both “global” for the entire model rather
than layer-wise, i.e., setting only the two hyper-parameters will determine the ﬁnal compression. We
note that it is possible to achieve similar compression ratios using different combinations of k and b
(but likely leading to different accuracy/robustness), and the two can indeed collaborate or trade-off
with each other to achieve more effective compression.
Optimization
The optimization in equation 4 is a constrained optimization with two constraints. The typical method
to solve the constrained optimization is using projected gradient descent or projected stochastic
gradient descent, if the projection operation (onto the feasible set deﬁned by constraints) is simple
enough. Unfortunately, in our case, this projection is quite complicated, since the intersection of
the sparsity constraint and the quantization constraint is complicated. However, we notice that the
projection onto the feasible set deﬁned by each individual constraint is doable (the projection onto the
sparsity constraint is quite standard, how to do efﬁcient projection onto the quantization constraint
deﬁned set will be clear soon). Therefore, we apply the ADMM optimization framework to
split these two constraints by duplicating the optimization variable θ. First the original optimization
formulation equation 4 can be rewritten as by introducing one more constraint
∥θ∥0≤k, θ′∈Qb
f adv(θ; x, y)
It can be further cast into a minimax problem by removing the equality constraint θ = θ′:
∥θ∥0≤k, θ′∈Qb max
f adv(θ; x, y) + ρ⟨u, θ −θ′⟩+ ρ
where ρ > 0 is a predeﬁned positive number in ADMM. Plug the form of f adv, we obtain a complete
minimax optimization
∥θ∥0≤k, θ′∈Qb
∞(x)}(x,y)∈Z
f(θ; x′, y) + ρ⟨u, θ −θ′⟩+ ρ
ADMM essentially iteratively minimizes variables θ and θ′, and maximizes u and all xadv.
We update the dual variable as ut+1 = ut + (θ −θ′), which can be considered to be a
gradient ascent step with learning rate 1/ρ.
Update xadv
We update xadv for sampled data (x, y) by
xadv ←Proj{x′:∥x′−x∥∞≤∆} {x + α∇xf(θ; x, y)}
The ﬁrst step is to optimize θ in equation 7 (ﬁxing other variables) which is only related
to the sparsity constraint. Therefore, we are essentially solving
f(θ; xadv, y) + ρ
2∥θ −θ′ + u∥2
s.t. ∥θ∥0 ≤k.
Since the projection onto the sparsity constraint is simply enough, we can use the projected stochastic
gradient descent method by iteratively updating θ as
θ ←Proj{θ′′:∥θ′′∥0≤k}
f(θ; xadv, y) + ρ
2∥θ −θ′ + u∥2
{θ′′ : ∥θ′′∥0 ≤k} denotes the feasible domain of the sparsity constraint. γt is the learning rate.
The second step is to optimize equation 7 with respect to θ′ (ﬁxing other variables),
which is essentially solving the following projection problem
∥θ′ −(θ + u)∥2
s.t. θ′ ∈Qb.
To take a close look at this formulation, we are essentially solving the following particular one
dimensional clustering problem with 2b + 1 clusters on θ + u (for each U (l), V (l), and C(l))
Ui,j ∈{0, a1, a2, · · · , a2b}.
The major difference from the standard clustering problem is that there is a constant cluster 0. Take
U ′(l) as an example, the update rule of θ′ is U ′(l)
= ZeroKmeans2b(U (l) + uU (l)), where uU (l)
is the dual variable with respect to U (l) in θ. Here we use a modiﬁed Lloyd’s algorithm to
solve equation 8. The detail of this algorithm is shown in Algorithm 1,
We ﬁnally summarize the full ATMC algorithm in Algorithm 2.
Experiments
To demonstrate that ATMC achieves remarkably favorable trade-offs between robustness and model
compactness, we carefully design experiments on a variety of popular datasets and models as summarized in Section 3.1. Speciﬁcally, since no algorithm with exactly the same goal (adversarially robust
compression) exists off-the-shelf, we craft various ablation baselines, by sequentially composing
different compression strategies with the state-of-the-art adversarial training . Besides, we show
that the robustness of ATMC compressed models generalizes to different attackers.
Table 1: The datasets and CNN models used in the experiments.
#Parameters
Model Size (bits)
Dataset & Accuracy
13,776,000
MNIST: 99.32%
680,482,816
CIFAR-10: 93.67%
681,957,376
CIFAR-100: 73.16%
WideResNet
350,533,120
SVHN: 95.25%
Algorithm 1 ZeroKmeansB( ¯U)
1: Input: a set of real numbers ¯U, number
of clusters B.
2: Output: quantized tensor U.
3: Initialize a1, a2, · · · , aB by randomly
picked nonzero elements from ¯U.
4: Q := {0, a1, a2, · · · , aB}
for i = 0 to | ¯U| −1 do
δi ←arg minj( ¯Ui −Qj)2
Fix Q0 = 0
for j = 1 to B do
i I(δi=j) ¯
13: until Convergence
14: for i = 0 to | ¯U| −1 do
16: end for
Algorithm 2 ATMC
dataset Z, stepsize sequence
{γt > 0}T −1
t=0 , update steps n and T,
hyper-parameter ρ, k, and b, ∆
2: Output: model θ
3: α ←1.25 × ∆/n
4: Initialize θ, let θ′ = θ and u = 0
5: for t = 0 to T −1 do
Sample (x, y) from Z
for i = 0 to n −1 do
xadv ←Proj{x′:∥x′−x∥∞≤∆}
α∇xf(θ; x, y)
Proj{θ′′:∥θ′′∥0≤k}
γt∇θ[f(θ; xadv, y)+ ρ
2∥θ−θ′+u∥2
θ′ ←ZeroKmeans2b(θ + u)
u ←u + (θ −θ′)
13: end for
Experimental Setup
Datasets and Benchmark Models
As in Table 1, we select four popular image classiﬁcation
datasets and pick one top-performer CNN model on each: LeNet on the MNIST dataset ;
ResNet34 on CIFAR-10 and CIFAR-100 ; and WideResNet on SVHN .
Evaluation Metrics
The classiﬁcation accuracies on both benign and on attacked testing sets are
reported, the latter being widely used to quantify adversarial robustness, e.g., in . The model
size is computed via multiplying the quantization bit per element with the total number of non-zero
elements, added with the storage size for the quantization thresholds ( equation 8). The compression
ratio is deﬁned by the ratio between the compressed and original model sizes. A desired model
compression is then expected to achieve strong adversarial robustness (accuracy on the attacked
testing set), in addition to high benign testing accuracy, at compression ratios from low to high .
ATMC Hyper-parameters
For ATMC, there are two hyper-parameters in equation 4 to control
compression ratios: k in the sparsity constraint, and b in the quantization constraint. In our experiments, we try 32-bit (b = 32) full precision, and 8-bit (b = 8) quantization; and then vary different k
values under either bit precision, to navigate different compression ratios. We recognize that a better
compression-robustness trade-off is possible via ﬁne-tuning, or perhaps to jointly search for, k and b.
Training Settings
For adversarial training, we apply the PGD attack to ﬁnd adversarial
samples. Unless otherwise speciﬁed, we set the perturbation magnitude ∆to be 76 for MNIST and 4
for the other three datasets. (The color scale of each channel is between 0 and 255.) Following the
settings in , we set PGD attack iteration numbers n to be 16 for MNIST and 7 for the other three
datasets. We follow to set PGD attack step size α to be min(∆+ 4, 1.25∆)/n. We train ATMC
for 50, 150, 150, 80 epochs on MNIST, CIFAR10, CIFAR100 and SVHN respectively.
Adversarial Attack Settings
Without further notice, we use PGD attack with the same settings
as used in adversarial training on testing sets to evaluate model robustness. In section 3.3, we also
evaluate model robustness on PGD attack, FGSM attack and WRM attack with varying
attack parameter settings to show the robustness of our method across different attack settings.
Comparison to Pure Compression, Pure Defense, and Their Mixtures
Since no existing work directly pursues our same goal, we start from two straightforward baselines to
be compared with ATMC: standard compression (without defense), and standard defense (without
compression). Furthermore, we could craft “mixture” baselines to achieve the goal: ﬁrst applying a
defense method on a dense model, then compressing it, and eventually ﬁne-tuning the compressed
model (with parameter number unchanged, e.g., by ﬁxing zero elements) using the defense method
again. We design the following seven comparison methods (the default bit precision is 32 unless
otherwise speciﬁed):
• Non-Adversarial Pruning (NAP): we train a dense state-of-the-art CNN and then compress
it by the pruning method proposed in : only keeping the largest-magnitudes weight
elements while setting others to zero, and then ﬁne-tune the nonzero weights (with zero
weights ﬁxed) on the training set again until convergence, . NAP can thus explicitly control
the compressed model size in the same way as ATMC. There is no defense in NAP.
• Dense Adversarial Training (DA): we apply adversarial training to defend a dense
CNN, with no compression performed.
• Adversarial Pruning (AP): we ﬁrst apply the defensive method to pre-train a defense
CNN. We then prune the dense model into a sparse one , and ﬁne-tune the non-zero
weights of pruned model until convergence, similarly to NAP.
• Adversarial ℓ0 Pruning (Aℓ0): we start from the same pre-trained dense defensive CNN used
by AP and then apply ℓ0 projected gradient descent to solve the constrained optimization
problem with an adversarial training objective and a constraint on number of non-zero
parameters in the CNN. Note that this is in essence a combination of one state-of-the-art
compression method and PGD adversarial training.
• Adversarial Low-Rank Decomposition (ALR): it is all similar to the AP routine, except that
we use low rank factorization in place of pruning to achieve the compression step.
• ATMC (8 bits, 32 bits): two ATMC models with different quantization bit precisions are
chosen. For either one, we will vary k for different compression ratios.
Model Size Compression Ratio
ATMC-32bits
ATMC-8bits
Model Size Compression Ratio
ATMC-32bits
ATMC-8bits
Model Size Compression Ratio
ATMC-32bits
ATMC-8bits
Model Size Compression Ratio
ATMC-32bits
ATMC-8bits
Model Size Compression Ratio
PGD Attack
ATMC-32bits
ATMC-8bits
Model Size Compression Ratio
PGD Attack
ATMC-32bits
ATMC-8bits
(b) CIFAR-10
Model Size Compression Ratio
PGD Attack
ATMC-32bits
ATMC-8bits
(c) CIFAR-100
Model Size Compression Ratio
PGD Attack
ATMC-32bits
ATMC-8bits
Figure 1: Comparison among NAP, AP, Aℓ0, ALR and ATMC (32 bits & 8 bits) on four models/datasets. Top row: accuracy on benign testing images versus compression ratio. Bottom row:
robustness (accuracy on PGD attacked testing images) versus compression ratio. The black dashed
lines mark the the uncompressed model results.
Fig 1 compares the accuracy on benign (top row) and PGD-attack (bottom row) testing images
respectively, w.r.t. the compression ratios, from which a number of observations can be drawn.
First, our results empirically support the existence of inherent trade-off between robustness and
accuracy at different compression ratios; although the practically achievable trade-off differs by
method. For example, while NAP (a standard CNN compression) obtains decent accuracy results on
benign testing sets (e.g., the best on CIFAR-10 and CIFAR-100), it becomes very deteriorated in terms
of robustness under adversarial attacks. That veriﬁes our motivating intuition: naive compression,
while still maintaining high standard accuracy, can signiﬁcantly compromise robustness – the “hidden
price” has indeed been charged. The observation also raises a red ﬂag for current evaluation ways of
CNN compression, where the robustness of compressed models is (almost completely) overlooked.
Second, while both AP and ALR consider compression and defense in ad-hoc sequential ways, Aℓ0
and ATMC-32 bits further gain notably advantages over them via “joint optimization” type methods,
in achieving superior trade-offs between benign test accuracy, robustness, and compression ratios.
Furthermore, ATMC-32 bits outperforms Aℓ0 especially at the low end of compression ratios. That is
owing to the the new decomposition structure that we introduced in ATMC.
Third, ATMC achieves comparable test accuracy and robustness to DA, with only minimal amounts
of parameters after compression. Meanwhile, ATMC also achieves very close, sometimes better
accuracy-compression ratio trade-offs on benign testing sets than NAP, with much enhanced robustness. Therefore, it has indeed combined the best of both worlds. It also comes to our attention that
for ATMC-compressed models, the gaps between their accuracies on benign and attacked testing sets
are smaller than those of the uncompressed original models. That seems to potentially suggest that
compression (when done right) in turn has positive performance regularization effects.
Lastly, we compare between ATMC-32bits and ATMC-8bits. While ATMC-32bits already outperforms other baselines in terms of robustness-accuracy trade-off, more aggressive compression can be
achieved by ATMC-8bits (with further around four-time compression at the same sparsity level), with
still competitive performance. The incorporation of quantization and weight pruning/decomposition
in one framework allows us to ﬂexibly explore and optimize their different combinations.
Model Size Compression Ratio
ATMC-32bits
ATMC-8bits
(a) PGD, perturbation=2
Model Size Compression Ratio
ATMC-32bits
ATMC-8bits
(b) PGD, perturbation=8
Model Size Compression Ratio
FGSM-eps:4
ATMC-32bits
ATMC-8bits
(c) FGSM, perturbation=4
Model Size Compression Ratio
WRM-iter:7
ATMC-32bits
ATMC-8bits
(d) WRM, penalty=1.3, iteration=7
Figure 2: Robustness-model size trade-off under different attacks and perturbation levels. Note that
the accuracies here are all measured by attacked images, i.e., indicating robustness.
Generalized Robustness Against Other Attackers
In all previous experiments, we test ATMC and other baselines against the PGD attacker at certain
ﬁxed perturbation levels. We will now show that the superiority of ATMC persists under different
attackers and perturbation levels. On CIFAR-10 (whose default perturbation level is 4), we show the
results against the PGD attack with perturbation levels 2 and 8, in Fig 2a and Fig 2b, respectively.
We also try the FGSM attack with perturbation 4, and the WRM attack with penalty
parameter 1.3 and iteration 7, with results displayed in Fig 2c and Fig 2d, respectively. As we can see,
ATMC-32bit outperforms its strongest competitor AP in the full compression spectrum. ATMC-8bit
can get more aggressively compressed model sizes while maintaining similar or better robustness
to ATMC-32bit at low compression ratios. In all, the robustness gained by ATMC compression is
observed to be sustainable and generalizable.
Conclusion
This paper aims to address the new problem of simultaneously achieving high robustness and
compactness in CNN models. We propose the ATMC framework, by integrating the two goals in one
uniﬁed constrained optimization framework. Our extensive experiments endorse the effectiveness of
ATMC by observing: i) naive model compression may hurt robustness, if the latter is not explicitly
taken into account; ii) a proper joint optimization could achieve both well: a properly compressed
model could even maintain almost the same accuracy and robustness compared to the original one.