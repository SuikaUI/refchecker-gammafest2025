Quantum machine learning in feature Hilbert spaces
Maria Schuld∗and Nathan Killoran
Xanadu, 372 Richmond St W, Toronto, M5V 2L7, Canada
 
The basic idea of quantum computing is surprisingly similar to that of kernel methods in machine
learning, namely to eﬃciently perform computations in an intractably large Hilbert space. In this
paper we explore some theoretical foundations of this link and show how it opens up a new avenue
for the design of quantum machine learning algorithms. We interpret the process of encoding inputs
in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space.
quantum computer can now analyse the input data in this feature space. Based on this link, we
discuss two approaches for building a quantum model for classiﬁcation. In the ﬁrst approach, the
quantum device estimates inner products of quantum states to compute a classically intractable
kernel. This kernel can be fed into any classical kernel method such as a support vector machine. In
the second approach, we can use a variational quantum circuit as a linear model that classiﬁes data
explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a
continuous-variable system, and visualise the working principle with 2-dimensional mini-benchmark
INTRODUCTION
The goal of many quantum algorithms is to perform
eﬃcient computations in a Hilbert space that grows
rapidly with the size of a quantum system.
means that the number of operations applied to the
system grows at most polynomially with the system size.
An illustration is the famous quantum Fourier transform
applied to an n-qubit system, which uses O(poly(n))
operations to perform a discrete Fourier transform on
2n amplitudes.
In continuous-variable systems this is
pushed to the extreme, as a single operation – for example, squeezing – applied to a mode formally manipulates
a quantum state in an inﬁnite-dimensional Hilbert space.
In this sense, quantum computing can be understood
as a technique to perform “implicit” computations in
an intractably large Hilbert space through the eﬃcient
manipulation of a quantum system.
In machine learning, so-called kernel methods are a
well-established ﬁeld with a surprisingly similar logic.
In a nutshell, the idea of kernel methods is to formally
embed data into a higher- (and sometimes inﬁnite-)
dimensional feature space in which it becomes easier to
analyse (see Figure 1). A popular example is a support
vector machine that draws a decision boundary between
two classes of datapoints by mapping the data into a
feature space where it becomes linearly separable. The
trick is that the algorithm never explicitly performs
computations with vectors in feature space, but uses a
so-called kernel function that is deﬁned on the domain
of the original input data. Just like quantum computing,
kernel methods therefore perform implicit computations
in a possibly intractably large Hilbert space through the
eﬃcient manipulation of data inputs.
∗ 
Besides this apparent link,
kernel methods have
been hardly studied in the quantum machine learning
literature, a ﬁeld that (in the deﬁnition we employ here)
investigates the use of quantum computing as a resource
for machine learning.
Across the approaches in this
young ﬁeld, which vary from sampling to quantum
optimisation , linear algebra solvers and
using quantum circuits as trainable models for inference
 , a lot of attention has been paid to recent trends
in machine learning such as deep learning and neural
networks. Kernel methods, which were most successful
in the 1990s, are only mentioned in a few references
 . Besides a single study on the connection between
coherent states and Gaussian kernels , their potential
for quantum computing remains widely unexplored.
The aim of this paper is to investigate the relationship
between feature maps, kernel methods and quantum
computing. We interpret the process of encoding inputs
into a quantum state as a feature map which maps
data into a potentially vastly higher-dimensional feature
space, the Hilbert space of the quantum system. Data
can now be analysed in this ‘feature Hilbert space’,
where simple classiﬁers such as linear models may
gain enormous power.
Furthermore, it is well known
that the inner product of two data inputs that have
been mapped into feature space gives rise to a kernel
function that measures the distance between the data
Kernel methods use these kernel functions to
create models that have been very successful in pattern
recognition.
By switching between kernels one eﬀectively switches between diﬀerent models, which is known
as the kernel trick.
In the quantum case, the kernel
trick corresponds to changing the data encoding strategy.
These two perspectives, namely of kernels on the one
hand and feature spaces one the other hand, naturally
lead to two ways of building quantum classiﬁers for
supervised learning.
The implicit approach takes a
 
original space
feature space
FIG. 1. While in the original space of training inputs, data
from the two classes ‘blue squares’ and ‘red circles’ are not
separable by a simple linear model (left), we can map them
to a higher dimensional feature space where a linear model is
indeed suﬃcient to deﬁne a separating hyperplane that acts
as a decision boundary (right).
classical model that depends on a kernel function, but
uses the quantum device to evaluate the kernel, which
is computed as the inner products of quantum states
in ‘feature Hilbert space’.
The explicit approach uses
the quantum device to directly learn a linear decision
boundary in feature space by optimising a variational
quantum circuit.
A central result of this paper is that the idea of embedding data into a quantum Hilbert space opens up
a promising avenue to quantum machine learning, in
which we can generically use quantum devices for pattern recognition. The implicit and explicit approaches
are not only hardware-independent, but also suitable for
intermediate-term quantum technologies, which allows us
to test them with the generation of quantum computers that is currently being developed. Nonlinear feature
maps also circumvent the need to implement nonlinear
transformations on amplitude-encoded data, and thereby
solve an outstanding problem in quantum machine learning which we will come back to in the conclusion.
FEATURE MAPS, KERNELS AND
QUANTUM COMPUTING
In machine learning we are typically given a dataset
of inputs D = {x1, ..., xM} from a certain input set X,
and have to recognise patterns to evaluate or produce
previously unseen data. Kernel methods use a distance
measure κ(x, x′) between any two inputs x, x′ ∈X in order to construct models that capture the properties of a
data distribution. This distance measure is connected to
inner products in a certain space, the feature space. Besides many practical applications, the most famous being
the support vector machine, these methods have a rich
theoretical foundation from which we want to highlight some relevant points.
Feature maps and kernels
Let us start with the deﬁnition of a feature map.
Deﬁnition 1. Let F be a Hilbert space, called the feature
space, X an input set and x a sample from the input
set. A feature map is a map φ : X →F from inputs to
vectors in the Hilbert space. The vectors φ(x) ∈F are
called feature vectors.
Feature maps play an important role in machine learning,
since they map any type of input data into a space with a
well-deﬁned metric. This space is usually of much higher
dimension. If the feature map is a nonlinear function it
changes the relative position between data points (as in
the example of Figure 1), and a dataset can become a
lot easier to classify in feature space. Feature maps are
intimitely connected to kernels .
Deﬁnition 2. Let X be a nonempty set, called the input
A function κ : X × X →C is called a kernel if
the Gram matrix K with entries Km,m′ = κ(xm, xm′)
is positive semideﬁnite, in other words, if for any ﬁnite
subset {x1, ..., xM} ⊆X with M ≥2 and c1, ..., cM ∈C,
m′κ(xm, xm′) ≥0.
By deﬁnition of the inner product, every feature map
gives rise to a kernel.
Theorem 1. Let φ : X →F be a feature map. The inner
product of two inputs mapped to feature space deﬁnes a
kernel via
κ(x, x′) := ⟨φ(x), φ(x′)⟩F,
where ⟨·, ·⟩F is the inner product deﬁned on F.
Proof. We must show that the Gram matrix of this kernel
is positive deﬁnite. For arbitrary cm, cm′ ∈C and any
{x1, ..., xM} ⊆X with M ≥2, we ﬁnd that
m′κ(xm, xm′) = ⟨
cm′φ(xm′)⟩
cmφ(xm)||2 ≥0
The connection between feature maps and kernels
means that every feature map corresponds to a distance
measure in input space by means of the inner product
of feature vectors. It also means that we can compute
inner products of vectors mapped to much higher dimensional spaces by computing a kernel function, which may
be computationally a lot easier.
feature map
FIG. 2. Relationships between the concepts of a feature map,
kernel and reproducing kernel Hilbert space.
Reproducing kernel Hilbert spaces
Kernel theory goes further and deﬁnes a unique Hilbert
space associated with each kernel, the reproducing kernel
Hilbert space or RKHS .
Although rather abstract, this concept is useful in order to understand the
signiﬁcance of kernels for machine learning, as well as
their connection to linear models in feature space.
Deﬁnition 3. Let X be a non-empty input set and R a
Hilbert space of functions f : X →C that map inputs to
the real numbers. Let ⟨·, ·⟩be an inner product deﬁned
on R (which gives rise to a norm via ||f|| =
R is a reproducing kernel Hilbert space if every point
evaluation is a continuous functional F : f →f(x) for
all x ∈X. This is equivalent to the condition that there
exists a function κ : X × X →C for which
⟨f, κ(x, ·)⟩= f(x)
with κ(x, ·) ∈R and for all f ∈H, x ∈X.
The function κ is the unique reproducing kernel of R,
(2) is the reproducing property.
Note that a
diﬀerent, but isometrically isomorphic Hilbert space can
be derived for a so-called Mercer kernel .
Since a feature map gives rise to a kernel and a kernel
gives rise to a reproducing kernel Hilbert space, we can
construct a unique reproducing kernel Hilbert space for
any given feature map (see Figure 2).
Theorem 2. Let φ : X →F be a feature map over an
input set X, giving rise to a complex kernel κ(x, x′) =
⟨φ(x), φ(x′)⟩F.
The corresponding reproducing kernel
Hilbert space has the form
Rκ = {f : X →C|
f(x) = ⟨w, φ(x)⟩F, ∀x ∈X, w ∈F}
The functions ⟨w, ·⟩in the RKHS associated with
feature map φ can be interpreted as linear models, for
which w ∈F deﬁnes a hyperplane in feature space.
In machine learning these rather formal concepts gain
relevance because of the (no less formal) representer theorem :
Theorem 3. Let X be an input set, κ : X × X →R a
kernel, D a data set consisting of data pairs (xm, ym) ∈
X × R and f : X →R a class of model functions that
live in the reproducing kernel Hilbert space Rκ of κ. Furthermore, assume we have a cost function C that quantiﬁes the quality of a model by comparing predicted outputs f(xm) with targets ym, and which has a regularisation term of the form g(||f||) where g : [0, ∞) →R is
a strictly monotonically increasing function. Then any
function f ∗∈Rκ that minimises the cost function C can
be written as
αmκ(x, xm),
for some parameters αm ∈R.
The representer theorem implies that for a common
family of machine learning optimisation problems over
functions in an RKHS R, the solution can be represented
as an expansion of kernel functions as in Eq. (4). Consequently, instead of explicitly optimising over an inﬁnitedimensional RKHS we can directly start with the implicit
ansatz of Eq. (4) and solve the convex optimisation problem of ﬁnding the parameters αm. The combination of
Theorem 2 and Theorem 3 shows another facet of the
link of kernels and feature maps. A model that deﬁnes
a hyperplane in feature space can often be written as a
model that depends on kernel evaluations. In Section III
we will translate these two viewpoints into two ways of
designing quantum machine learning algorithms.
Input encoding as a feature map
The immediate approach to combine quantum mechanics and the theory of kernels is to associate the
Hilbert space of a quantum system with a reproducing
kernel Hilbert space and ﬁnd the reproducing kernel of
the system.
We show in Appendix A that for Hilbert
spaces with discrete bases, as well as for the special
‘continuous-basis’ case of the Hilbert space of coherent
states, the reproducing kernel is given by inner products
of basis vectors.
This insight can lead to interesting
results. For example, Chatterjee et al. show that
the inner product of an optical coherent state can
be turned into a Gaussian kernel (also called radial
basis function kernel) which is widely used in machine
learning. However, to widen the framework we choose
another route here.
Instead of asking what kernel is
associated with a quantum Hilbert space, we associate
a quantum Hilbert space with a feature space and
derive a kernel that is given by the inner product of
quantum states.
As seen in the previous section, this
will automatically give rise to an RKHS, and the entire
apparatus of kernel theory can be applied.
Assume we want to encode some input x from an input
set X into a quantum state that is described by a vector
|φ(x)⟩and which lives in Hilbert space F. This procedure
of ‘input encoding’ fulﬁlls the deﬁnition of a feature map
φ : X →F, which we call a quantum feature map here.
According to Theorem 1 we can derive a kernel κ from
this feature map via Eq.
By virtue of Theorem
2, the kernel is the reproducing kernel of an RKHS Rκ
as deﬁned in Eq. (3). The functions in Rκ are the inner
products of the ‘feature-mapped’ input data and a vector
|w⟩∈F, which deﬁnes a linear model
f(x; w) = ⟨w|φ(x)⟩
Note that we use Dirac brackets ⟨·|·⟩instead of the inner
product ⟨·, ·⟩to signify that we are calculating inner products in a quantum Hilbert space. Finally, the representer
theorem 3 guarantees that the minimiser minw C(w, D)
of the empirical risk
|f(xm; w) −ym|2 + ||f||Rκ
can be expressed by Equation (4). The simple idea of interpreting x →|φ(x)⟩as a feature map therefore allows
us to make use of the rich theory of kernel methods and
gives rise to machine learning models whose trained candidates can be expressed by inner products of quantum
states. Note that if the state |φ(x)⟩has complex amplitudes, we can always construct a real kernel by taking
the absolute square of the inner product.
QUANTUM MACHINE LEARNING IN
FEATURE HILBERT SPACE
Now let us enter the realm of quantum computing and
quantum machine learning. We show how to use the ideas
of Section II C to design two types of quantum machine
learning algorithms and illustrate both approaches with
an example from continuous-variable systems.
Feature-encoding circuits
perspective
computing,
quantum feature map x
|φ(x)⟩corresponds to
preparation
ground or vacuum state |0...0⟩of a Hilbert space F as
Uφ(x)|0...0⟩= |φ(x)⟩.
We will call Uφ(x) the featureembedding circuit.
The models from Eq.
(5) in the
reproducing Hilbert space from Deﬁnition 2 are inner
products between |φ(x)⟩and a general quantum state
We therefore consider a second circuit W
with W|0...0⟩= |w⟩, which we call the model circuit.
The model circuit speciﬁes the hyperplane of a linear
model in feature Hilbert space.
If the feature state
|φ(x)⟩is orthogonal to |w⟩, then x lies on the decision boundary, whereas states with a positive [negative]
inner product lie on the left [right] side of the hyperplane.
To show some examples of feature-embedding circuits
and their associated kernels, let us have a look at
popular input encoding techniques in quantum machine
Basis encoding.
Many quantum machine learning
algorithms assume that the inputs x to the computation
are encoded as binary strings represented by a computational basis state of the qubits .
For example, x = 01001 is represented by the 5-qubit basis state
|01001⟩. The computational basis state corresponds to a
standard basis vector |i⟩(with i being the integer representation of the bitstring) in a 2n-dimensional Hilbert
space F, and the eﬀect of the feature-embedding circuit
is given by
Uφ : x ∈{0, 1}n →|i⟩.
This feature map maps each data input to a state from an
orthonormal basis and is equivalent to the generic ﬁnitedimensional case discussed in Appendix A. As shown
there, the generic kernel is the Kronecker delta
κ(x, x′) = ⟨i|j⟩= δij,
which is a binary similarity measure that is only nonzero
for two identical inputs.
Amplitude encoding.
Another approach to information encoding is to associate normalised input vectors
x = (x0, ..., xN−1)T ∈RN of dimension N = 2n with the
amplitudes of a n qubit state |ψx⟩ ,
Uφ : x ∈RN →|ψx⟩=
As above, |i⟩denotes the i’th computational basis state.
This choice corresponds to the linear kernel,
κ(x, x′) = ⟨ψx|ψx′⟩= xT x′.
Copies of quantum states.
With a slight variation
of amplitude encoding we can implement polynomial kernels . Taking d copies of an amplitude encoded quantum state,
Uφ : x ∈RN →|ψx⟩⊗· · · ⊗|ψx⟩,
corresponds to the kernel
κ(x, x′) = ⟨ψx|ψx′⟩· · · ⟨ψx|ψx′⟩= (xT x′)d.
Product encoding.
One can also use a (tensor)
product encoding, in which each feature of the input
x = (x1, .., xN)T ∈RN is encoded in the amplitudes
of one separate qubit.
An example is to encode xi as
|φ(xi)⟩= cos(xi)|0⟩+ sin(xi)|1⟩for i = 1, ..., N .
This corresponds to a feature-embedding circuit with the
Uφ : x ∈RN →
and implies a cosine kernel,
κ(x, x′) =
cos(xi −x′
implicit approach
kernel( , )
explicit approach
prediction
prediction
training inputs
FIG. 3. Illustration of the two approaches to use quantum feature maps for supervised learning. The implicit approach uses
the quantum device to evaluate the kernel function as part of
a hybrid or quantum-assisted model which can be trained by
classical methods. In the explicit approach, the model is solely
computed by the quantum device, which consists of a variational circuit trained by hybrid quantum-classical methods.
Building a quantum classiﬁer
Having formulated the ideas from Section II C in the
language of quantum computing, we can identify two
diﬀerent strategies of designing a quantum machine
learning algorithm (see Figure 3). On the one hand, we
can use the quantum computer to estimate the inner
products κ(x, x′) = ⟨φ(x)|φ(x′)⟩from a kernel-dependent
model as in Eq. (4), which we call the implicit approach,
since we use the quantum system to estimate distance
measures on input space. This strategy requires a quantum computer that can do two things: to implement
Uφ(x) for any x ∈X and to estimate inner products
between quantum states (for example using a SWAP
test routine). The computation of the model from those
kernel estimates, as well as the training algorithm is left
to a classical device. This is an excellent strategy in the
context of intermediate-term quantum technologies ,
where we are interested in using a quantum computer
only for small routines of limited gate count,
compute as much as possible on the classical hardware.
Note that in the long term, quantum computers could
also be used to learn the parameters αm by computing
the inverse of the kernel Gram matrix, which has been
investigated in Refs. .
On the other hand, and as motivated in the introduction, one can bypass the representer theorem
and explicitly perform the classiﬁcation in the ‘feature
Hilbert space’ of the quantum system.
We call this
the explicit approach.
For example, this can mean to
ﬁnd a |w⟩that deﬁnes a model 5.
To do so, we can
make the model circuit trainable, W = W(θ), so that
quantum-classical hybrid training of θ can learn
the optimal model |w(θ)⟩= W(θ)|0⟩.
The ansatz for
FIG. 4. Shape of the squeezing kernel function κsq(x, x′) from
Equation (7) for diﬀerent squeezing strength hyperparameters
c. The input x is ﬁxed at (0, 0) and x′ is varied. The plots
show the interval [−1, 1] on both horizontal axes.
the model circuit’s architecture deﬁnes the space of
possible models and can act as regularisation (see also
Below we will follow a slightly more general
strategy and compute a state W(θ)Uφ|0...0⟩, from which
measurements determine the output of the model.
Depending on the measurement, this is not necessarily a
linear model in feature Hilbert space. We could even go
further and include postselection in the model circuit,
which might give the classiﬁer in feature Hilbert space
even more power.
Using quantum computers for learning tasks with
these two approaches is desirable in various settings.
For example, the implicit approach may be interesting
in cases where the quantum device evaluates kernels
or models faster in terms of absolute runtime speed.
Another interesting example is a setting in which the
kernel one wants to use is classically intractable because
the runtime grows exponentially or even faster with
the input dimension.
The explicit approach may be
useful when we want to leave the limits of the RKHS
framework and construct classiﬁers directly on Hilbert
In the remainder of this work we want to explore these
two approaches with several examples. We use squeezing in continuous-variable quantum systems as a feature map, for which the Hilbert space F is an inﬁnitedimensional Fock space.
This constructs a squeezingbased quantum machine learning classiﬁer which can for
example be implemented by optical quantum computers.
Squeezing as a feature map
A squeezed vacuum state is deﬁned as
2nn! (−eiϕ tanh(r))n|2n⟩,
where {|n⟩} denotes the Fock basis and z = reiϕ is the
complex squeezing factor with absolute value r and phase
It will be useful to introduce the notation |z⟩=
|(r, ϕ)⟩. We can interpret x →|φ(x)⟩= |(c, x)⟩as a feature map from a one-dimensional real input space x ∈R
FIG. 5. Decision boundary of a support vector machine with
the custom kernel from Eq. (7). The shaded areas show the
decision regions for Class 0 (blue) and Class 1 (red), and each
plot shows the rate of correct classiﬁcations on the training
set/test set. The ﬁrst row plots three standard 2-dimensional
datasets: ‘circles’, ‘moons’ and ‘blobs’, each with 150 test and
50 training samples. The second row illustrates that increasing the squeezing hyperparameter c changes the classiﬁcation
performance. Here we use a dataset of 500 training and 100
test samples. Training was performed with python’s scikitlearn SVC classiﬁer using a custom kernel which implements
the overlap of Eq. (8).
to the Hilbert space of Fock states, in short, the Fock
space. Here, c is a constant hyperparameter that determines the strength of the squeezing, and x is associated
with the phase. Moreover, when given multi-dimensional
inputs in a dataset of vectors x = (x1, ..., xN)T ∈RN, we
can deﬁne the joint state of N squeezed vacuum modes,
φ : x →|(c, x)⟩,
|(c, x)⟩= |(c, x1)⟩⊗. . . ⊗|(c, xN)⟩∈F,
as a feature map, where F is now a multimode Fock
We call this feature map the squeezing feature
map with phase encoding.
The kernel
κ(x, x′; c) =
⟨(c, xi)|(c, x′
⟨(c, xi)|(c, x′
sech c sech c
i−xi) tanh c tanh c,
derived from this feature map is easy to compute on
a classical computer. It is plotted in Figure 4, where we
see that the hyperparameter c determines the variance
of the kernel function.
Note that we can also encode
epoch 5000
FIG. 6. Decision boundary of a perceptron classiﬁer in Fock
space after mapping the 2-dimensional data points via the
squeezing feature map with phase encoding from Eq.
(with c = 1.5). The perceptron only acts on the real subspace and without regularisation.
The ‘blobs’ dataset has
now only 70 training and 20 test samples. The perceptron
achieves a training accuracy of 1 after less than 5000 epochs,
which means that the data is linearly separable in Fock space.
Interestingly, in this example the test performance remains
exactly the same. The simulations were performed with the
Strawberry Fields simulator as well as a scikit-learn out-ofthe-box perceptron classiﬁer.
features in the absolute value of the squeezing and de-
ﬁne a squeezing feature map with absolute value encoding,
x →|φ(x)⟩= |(x, c)⟩. However, in this version we cannot
vary the variance of the kernel function, which is why we
use the phase encoding in the following investiagtions.
An implicit quantum-assisted classiﬁer
In the implicit approach, we evaluate the kernel in Eq.
(7) with a quantum computer and feed it into a classical
kernel method. Instead of using a real quantum device,
we exploit the fact that, in the case of squeezing, the
kernel can be eﬃciently computed classically, and use it
as a custom kernel in a support vector machine. Figure
5 shows that such a model easily learns the decision
boundary of 2-dimensional mini-benchmark datasets.
Since the idea of a support vector machine is to ﬁnd
the maximum-margin hyperplane in feature space, we
want to know whether we can always ﬁnd a hyperplane
for which the training accuracy is 1.
In other words,
we ask if the data becomes linearly separable in Fock
space by the squeezing feature map.
An easy way to
do this is to apply a perceptron classiﬁer to the data in
feature space. The perceptron is guaranteed to ﬁnd such
a separating hyperplane if it exists. Figure 6 shows the
performance of a perceptron classiﬁer in the Fock space
for the ‘blobs’ data. The data was mapped to this space
by the squeezing feature map with phase encoding. As
one can see, after 5000 epochs (runs through the dataset)
the decision boundary perfectly ﬁts the training data,
achieving an accuracy of 1. The number of iterations to
train the perceptron is known to increase with O(1/γ2)
where γ is the margin between the two classes , and
indeed we ﬁnd in other simulations that the ‘moons’ and
‘circles’ data only take a few epochs until reaching full
accuracy. Although the perfect ﬁt to the training data is
of course not useful for machine learning (as can be seen
by the non-increasing accuracy on the test set) these
results are a clue to the fact that the squeezing feature
map makes data linearly separable in feature space, a
fact that we prove in Appendix B.
While the results of the simulations are promising,
a goal is to ﬁnd more sophisticated kernels. Although
quantum computers could oﬀer constant speed advantages, they become indispensable if the feature map circuit is classically intractable. However, squeezed states
are an example of so-called Gaussian states, and it is
well known that Gaussian states (although living in an
inﬁnite-dimensional Hilbert space) can be eﬃciently simulated by a classical computer , which we used in the
simulations. In order to do something more interesting,
one needs non-Gaussian elements to the circuit. For example, one can extend a standard linear optical network
of beamsplitters by a cubic phase gate or use photon number measurements . To this end, let Vφ(x)
be a non-Gaussian feature map circuit, i.e. a quantum
algorithm that takes a vacuum state and prepares an xdependent non-Gaussian state. The kernel
κ(x, x′) = ⟨0...0|V †
φ (x)Vφ(x′)|0...0⟩
can in general not be simulated by a classical computer
any more. It is therefore an interesting open question
what type of feature map circuits Vφ are classically intractable, but at the same time lead to powerful kernels
for classical models such as support vector machines.
An explicit quantum classiﬁer
In the explicit approach deﬁned above, we use a
parametrised continuous-variable circuit W(θ) to build
a “Fock-space” classiﬁer. For our squeezing example this
can be done as follows. We start with two vacuum modes
|0⟩⊗|0⟩. To classify a data input x, ﬁrst map the input
to a quantum state |c, x⟩= |c, x1⟩⊗|c, x2⟩by performing
a squeezing operation on each of the modes. Second, apply the model circuit W(θ) to |c, x⟩. Third, interpret the
probability p(n1, n2) of measuring a certain Fock state
|n1, n2⟩as the output of the machine learning model.
Since this probability depends on the displacement and
squeezing intensity, it is better to deﬁne two probabilities, say p(n1 = 2, n2 = 0) and p(n1 = 0, n2 = 2), as
a one-hot encoded output vector (o0, o1). This output
vector can be normalised to a new vector
where p(y = 0), p(y = 1) can now be interpreted as the
probability for the model to predict class y = 0 and
y = 1, respectively. The ﬁnal label is the class with the
feature map
circuit W(θ)
BS(θ1, θ2)
Representation of the Fock-space-classiﬁer in
the graphical language of quantum neural networks. A vector
(x1, x2)T from the input space X gets mapped into the feature
space F which is the inﬁnite-dimensional 2-mode Fock space
of the quantum system. The model circuit, including photon
detection measurement, implements a linear model in feature
space and reduces the “inﬁnite hidden layer” to two outputs.
b.) The model circuit of the explicit classiﬁer described in the
text uses only 2 modes to instantiate this inﬁnite-dimensional
hidden layer. The variational circuit W(θ) consists of repetitions of a gate block. We use the gate block shown in c.)
with the beamsplitter (BS), displacement (D), quadratic (P)
and cubic phase gates (C) described in the text.
higher probability. We can interpret this circuit in the
graphical representation of neural networks as shown at
the top in Figure 7.
Let us assume we could represent any possible quantum circuit in the feature Hilbert space with the circuit
W(θ). Since the data in F is linearly separable, there is
a W for which we obtain 100% accuracy on the training
set, as we saw in Figure 6. However, the goal of machine
learning is not to perfectly ﬁt data, but to generalise from
it. It is therefore not desirable to ﬁnd the optimal decision boundary for the training data in F, but to ﬁnd a
good candidate from a class of decision boundaries that
captures the structure in the data well. Such a restricted
class of decision boundaries can be deﬁned by using an
ansatz for the model circuit W(θ) which cannot represent
any circuit, yet still ﬂexible enough to reach interesting
candidates. Figure 7 c.) shows such a model circuit for
the 2 input modes in our continuous-variable example.
The architecture consists of repetitions of a general gate
iterations
FIG. 8. Fock space classiﬁer presented in Figure 7 and the
text for the ‘moons’ dataset. The shaded areas show the probability p(y = 1) of predicting class 1. The datasets consist of
150 training and 50 test samples, and has been trained for
5000 steps with stochastic gradient descent of batch-size 5,
an adaptive learning rate and a square-loss cost function with
a gentle l2 regularisation applied to all weights. The loss drops
predominantly in the ﬁrst 200 steps (left).
block. We denote by ˆa1,2, ˆa†
1,2 the creation and annihilation operators of mode 1 and 2, and with ˆx1,2, ˆp1,2 the
corresponding quadrature operators (see ). After an
entangling beam splitter gate,
BS(u, v) = eu(eivˆa†
1ˆa2−e−ivˆa1ˆa†
with u, v ∈R, the circuit consists of single-mode gates
that are ﬁrst, second and third order in the quadratures.
The ﬁrst-order gate is implemented by a displacement
2i(Im(z)ˆx−Re(z)ˆp),
with the complex displacement factor z.
quadratic phase gate for the second order,
P(u) = ei u
and a cubic phase gate for the third order operator,
V (u) = ei u
We can in principle construct any continuous-variable
quantum circuit from this gate set.
This basic circuit
block can easily be generalised to circuits of more modes
by replacing the single beam splitter by a full optical
network of beam splitters .
To show that the Fock space classiﬁer works, we plot
the decision boundary for the ‘moons’ data in Figure 8,
using 4 repetitions of the gate block from Figure 7 c.)
and 32 parameters in total. The training loss shows that
after about 200 iterations of a stochastic gradient descent
algorithm, the loss converges to almost zero.
CONCLUSION
In this paper we introduced a number of new ideas
for the area of quantum machine learning based on the
theory of feature spaces and kernels.
Interpreting the
encoding of inputs into quantum states as a feature map,
we associate a quantum Hilbert space with a feature
space. Inner products of quantum states in this feature
space can be used to evaluate a kernel function.
can alternatively train a variational quantum circuit as
an explicit classiﬁer in feature space to learn a decision
We introduced a squeezing feature map as
an example and motivated with small-scale simulations
that these two approach can lead to interesting results.
From this work there are many further avenues of
research. For example, we raised the question whether
there are interesting kernel functions that can be
computed by estimating the inner products of quantum states, for which state preparation is classically
intractable.
Another open question are the details
in the design and training of variational circuits, and
how learning algorithms can be tailormade for the use
in hybrid training schemes.
This is a topic that has
just begun to be investigated by the quantum machine
learning community .
Last but not least, we want to come back to a point
we raised in the introduction. In quantum machine learning, a lot of models use amplitude encoding, which means
that a data vector is represented by the amplitudes of
a quantum state. Especially when trying to reproduce
neural network-like dynamics one would like to perform
nonlinear transformations on the data. But while linear
transformations are natural for quantum theory, nonlinearities are diﬃcult to design in this context. Interesting workarounds based on postselection or repeat-untilsuccess circuits were proposed in , but at the considerable costs of making the circuit non-deterministic,
and with a probability of failure that grows with the
size of the architecture. The feature map approach ‘outsources’ the nonlinearity into the procedure of encoding
inputs into a quantum state and therefore oﬀers an elegant solution to the problem of nonlinearities in amplitude encoding.
 G. Verdon, M. Broughton,
and J. Biamonte, arXiv
 
 M. H. Amin, Physical Review A 92, 1 .
 M. Benedetti,
J. Realpe-G´omez,
R. Biswas,
Perdomo-Ortiz,
 
 T. J. Y. Guang Hao Low and I. L. Chuang, Physical
Review A 89, 062315 .
 P. Wittek and C. Gogolin, Scientiﬁc Reports 7 .
 V. Denchev, N. Ding, H. Neven, and S. Vishwanathan,
in Proceedings of the 29th International Conference on
Machine Learning (ICML-12) pp. 863–870.
Perdomo-Ortiz,
A. Aspuru-Guzik,
and V. Smelyanskiy, The European Physical Journal Special Topics 224, 163 .
 N. Wiebe, D. Braun,
and S. Lloyd, Physical Review
Letters 109, 050505 .
 P. Rebentrost, M. Mohseni, and S. Lloyd, Physcial Review Letters 113, 130503 .
 M. Schuld, I. Sinayskiy,
and F. Petruccione, Physical
Review A 94, 022342 .
 K. H. Wan, O. Dahlsten, H. Kristj´ansson, R. Gardner,
and M. Kim, npj Quantum Information 3, 36 .
 E. Farhi and H. Neven, arXiv preprint arXiv:1802.06002
 M. Schuld, M. Fingerhuth,
and F. Petruccione, Europhysics Letters 119, 60002 .
 R. Chatterjee and T. Yu, Quantum Information and
Communication 17, 1292 .
 B. Sch¨olkopf and A. J. Smola, Learning with kernels:
Support vector machines, regularization, optimization,
and beyond .
 C. Berg, J. P. R. Christensen, and P. Ressel, Harmonic
analysis on semigroups .
 T. Hofmann, B. Sch¨olkopf, and A. J. Smola, The Annals
of Statistics , 1171 .
 N. Aronszajn, Transactions of the American Mathematical Society 68, 337 .
 B. J Mercer, Phil. Trans. R. Soc. Lond. A 209, 415
 B. Sch¨olkopf, R. Herbrich, and A. Smola, in Computational learning theory pp. 416–426.
 S. Wang, Journal of Mathematics Research 7, 175 .
 E. Stoudenmire and D. J. Schwab, in Advances In Neural
Information Processing Systems pp. 4799–4807.
 G. G. Guerreschi and M. Smelyanskiy, arXiv preprint
 
 J. Preskill, arXiv preprint arXiv:1801.00862 .
 M. Schuld, I. Sinayskiy,
and F. Petruccione, Physical
Review A 94, 022342 .
 J. R. McClean, J. Romero, R. Babbush, and A. Aspuru-
Guzik, New Journal of Physics 18, 023023 .
 S. M. Barnett and P. M. Radmore, Methods in theoretical
quantum optics, Vol. 15 .
 A. B. Novikoﬀ, On convergence proofs for perceptrons,
Tech. Rep. .
 S. D. Bartlett, B. C. Sanders, S. L. Braunstein,
K. Nemoto, in Quantum Information with Continuous
Variables pp. 47–55.
 D. Gottesman, A. Kitaev, and J. Preskill, Physical Review A 64, 012310 .
 S. Lloyd and S. L. Braunstein, in Quantum Information
with Continuous Variables pp. 9–17.
 S. D. Bartlett and B. C. Sanders, Physical Review A 65,
042304 .
 In contrast to a standard technique in machine learning,
it is not advisable to use a softmax layer for this purpose,
since o0, o1 can be very small, which leads to almost uniform probabilities.
 C. Weedbrook, S. Pirandola, R. Garc´ıa-Patr´on, N. J.
Cerf, T. C. Ralph, J. H. Shapiro, and S. Lloyd, Reviews
of Modern Physics 84, 621 .
 F. Flamini, N. Spagnolo, N. Viggianiello, A. Crespi,
R. Osellame,
and F. Sciarrino, Scientiﬁc Reports 7,
15133 .
 
 A. Berlinet and C. Thomas-Agnan, Reproducing kernel
Hilbert spaces in probability and statistics .
 T. Griﬃths and A. Yuille, The probabilistic mind:
Prospects for Bayesian cognitive science , 33 .
 R. D. la Madrid, European Journal of Physics 26, 287
 J. R. Klauder and B.-S. Skagerstam, Coherent states: applications in physics and mathematical physics .
 L. Hogben, Handbook of linear algebra (CRC Press,
Appendix A: Reproducing kernels of quantum
In this section of the appendix we will try to ﬁnd
an answer to the question of which reproducing kernels
the Hilbert space of generic quantum systems gives
rise to. Quantum theory prescribes that the state of a
quantum system is modelled by a vector in a Hilbert
In a typical setting, the Hilbert space is
constructed from a complete basis of eigenvectors {|s⟩}
of a complete set of commuting Hermitian operators
which corresponds to physical observables. Due to the
hermiticity of the observables, the basis is orthogonal,
and it can be continuous (i.e., if the observable is the
position operator describing the location of a particle),
countably inﬁnite (i.e., observing the number of photons
in an electric ﬁeld), or ﬁnite (i.e., observing the spin of
an electron). Vectors in the Hilbert space are abstractly
referred to as |ψ⟩∈H in Dirac notation.
every such Hilbert space has a functional representation.
In the case of a discrete basis of dimension
N ∈N ∪∞, the functional representation Hf
is given by the (Hilbert) space l2 of square summable
sequences {ψ(si) = ⟨si|ψ⟩}N
i=1 with the inner product
si ψ(si)∗ϕ(si). In the continuous case this is
the space L2 of square summable (equivalence classes
of) functions ψ(s) = ⟨s|ψ⟩with the inner product
dsψ(s)∗ϕ(s).
The preceding formulation of
quantum theory therefore associates every quantum
system with a Hilbert space of functions mapping from
a set S = {s} to the complex numbers. The question is
if these Hilbert spaces give rise to a reproducing kernel
that makes them a RKHS with respect to the input set S.
With the resolution of identity 1 =
ds |s⟩⟨s| for the
continuous and 1 = P
i |si⟩⟨si| for the discrete case, we
can immediately “create” the reproducing property from
Eq. (2). Consider ﬁrst the discrete case:
ψ(si) = ⟨si|ψ⟩=
⟨si|sj⟩⟨sj|ψ⟩= ⟨⟨s| · ⟩|ψ(·)⟩.
We can identify ⟨si|sj⟩with the reproducing kernel.
Since the basis is orthonormal, we have κ(si, sj) = δi,j.
The continuous case is more subtle. Inserting the identity, we get
ds′⟨s|s′⟩⟨s′|ψ⟩= ⟨⟨s| · ⟩, ψ(·)⟩,
which is the reproducing kernel property with the reproducing kernel κ(s, s′) = ⟨s|s′⟩. However, the “function”
s′(s) = δ(s −s′) is not square integrable, which means it
is itself not part of Hf
s , and the properties of Deﬁnition
3 are not fulﬁlled. This is no surprise, as the space of
square integrable functions L2 is a frequent example of a
Hilbert space that is not a RKHS . The inconsistency
between Dirac’s formalism and functional analysis is
also a well-known issue in quantum theory, but usually
glossed over in physical contexts . If mathematical
rigour is needed, physicists usually refer to the theory of
rigged Hilbert spaces .
There are quantum systems with an inﬁnite basis
which naturally give rise to a reproducing kernel that is
not the delta function. These systems are described by
so-called generalised coherent states . In the context
of quantum machine learning, this has been discussed in
Ref. . Generalised coherent states are vectors |l⟩in
a Hilbert space Hc of ﬁnite or countably inﬁnite dimension, and where the index l is from some topological space
L (allowing us to deﬁne a norm |||l⟩|| =
⟨l|l⟩). They
have two fundamental properties. First, |l⟩is a strongly
continuous function of l,
l→l′ || |l′⟩−|l⟩|| = 0, |l⟩̸= 0.
Note that this excludes for example the discrete Fock
basis {|n⟩}, but also any orthonormal set of states {|z⟩}
with a continuous label z ∈C, since 1
2|||z′⟩−|z⟩|| = 1 for
z′ ̸= z. Second, there exists a measure µ on L so that
we have a resolution of identity 1 =
L |l⟩⟨l| dµ(l). This
leads to a functional representation of the Hilbert space
where a vector |ψ⟩∈Hc is expressed via |ψ⟩= P
with ψ(l) = ⟨l|ψ⟩. Inserting the resolution of identity to
the right hand side of this expression yields
⟨l|l′⟩⟨l′|ψ⟩dµ(l′),
which is exactly the reproducing property in Deﬁnition
3 with the reproducing kernel κ(l, l′) = ⟨l|l′⟩.
there is a ﬁnite overlap between any two states from
the basis,
the kernel is not the Dirac delta function, and we do not run into the same problem as
for continuous orthogonal bases.
Hence, the Hilbert
space of coherent states is an RKHS for the input set {l}.
The most well-known type of coherent state are optical
coherent states
|α⟩= e−|α|2
which are the eigenstates of the non-Hermitian bosonic
creation operator ˆa, with the associated kernel
κ(α, β) = ⟨α|β⟩= e
whose square is a radial basis function or Gaussian kernel
as remarked in .
Appendix B: Linear separability in Fock space
If we map the inputs of a dataset D to a new dataset
D′ = {|(c, x1)⟩, ..., |(c, xM)⟩},
using the squeezing feature map with phase encoding
from Eq. (6), the feature mapped data vectors in D′ are
always linearly separable, which means any assignment
of two classes of labels to the data can be separated by
a hyperplane in F (see Figure 1).
To show this, ﬁrst
consider the following:
Proposition 1. A set of M vectors in RN are linearly
separable if M −1 of them are linear independent.
The proof can be found in Appendix C. Proposition
1 tells us that if our data is linearly independent,
it is linearly separable.
This result is in fact known
from statistical learning theory:
The VC dimension
– a measure of ﬂexibility or expressive power – of
linear models in K dimensions is K + 1, which means
that a linear model can separate or “shatter” K + 1
data points if we can choose the strategy of how to arrange them, but not the strategy of how they are labelled.
If we can show that the squeezing feature map maps
vectors to linearly independent states in Fock space, we
know that any dataset becomes linearly separable in Fock
space. To simplify, lets ﬁrst see look at the squeezing map
of a single mode.
Proposition
{ϕ1, ..., ϕM} with ϕm ̸= ϕm′ for m = 1, ..., M, m ̸= m′
and a hyperparameter c ∈R, the squeezed vacuum Fock
states |(c, ϕ1)⟩, ..., |(c, ϕM)⟩are linearly independent.
The proof is found in Appendix D. A very similar
proof conﬁrms that the proposition also holds true for
the sueezing map with absolute value encoding described
in Section III C. Symbolic computation of the rank of the
design matrix in feature space in Mathematica conﬁrms
this result for randomly selected squeezing factors up
to M = 10 and a cutoﬀdimension that truncates Fock
space to 40 dimensions.
For the multimode feature map dealing with input data
of dimension higher than one,
|(c, ϕm)⟩= |(c, ϕm
1 )⟩⊗. . . ⊗|(c, ϕm))⟩,
|(c, ϕm′)⟩= |(c, ϕm′
1 )⟩⊗. . . ⊗|(c, ϕm′))⟩.
⟨(c, ϕm)|(c, ϕm′)⟩=
i )|(c, ϕm′
which is 1 if ϕm
for all i = 1, ..., N and a value
other than zero else. The linear independence therefore
carries over to multi-dimensional feature maps.
Appendix C: Proof of Proposition 1
D = {(x1, y1), · · · , (xM, yM)}
be a dataset of M vectors with xm ∈RN for all m =
1, · · · , M, and y ∈{−1, 1}. The vectors are guaranteed
to be linearly separable if for any assignment of classes
{−1, 1} to labels y1, ..., yM there is a hyperplane deﬁned
by parameters w1, ..., wN, b so that
i + b) = ym ∀m = 1, ..., M.
The sign function is a bit tricky, but if we can instead
show that the stronger condition
i + b = ym ∀m = 1, ..., M
holds for some parameters, Eq. C1 must automatically
be satisﬁed.
Equation C2 deﬁnes a system of M linear equations
with N + 1 unknowns (namely the variables). From the
theorey of linear algebra we know that there is at
least one solution if and only if the rank of the ‘coeﬃcient
is equal to the rank of its augmented matrix
Remember that the rank of a matrix is the number of
linearly independent row (and column) vectors.
If the data vectors are all linearly independent we have
that N ≥M (if N < M there would be some vectors
that depend on others, because we have more vectors
than dimensions), and the rank of X is min(M, N) = M.
Augmenting X by stacking any number of column
vectors simply increases N, which means that it does
not change the rank of the matrix. It follows that for
M linearly independent data points embedded in a N
dimensional space the system has a solution. The data
is therefore linearly separable.
With this argument we can add more vectors that are
linearly dependent until M = N. After this, we can in
fact add one (but only one) more data point that linearly
depends on the others, and still guarantee linear separability. That is because adding one data point makes the
row number equal to the column number in [X|1], and
adding more columns does not change the rank. In contrast, adding two data points means that we have more
columns than rows in [X|1], and adding the column for
[X|1|y] can indeed change the rank.
Appendix D: Proof of Proposition 2
Let’s consider a matrix M where the squeezed states
in Fock basis form the rows:
 −eiφj tanh(rj)
We introduce two auxiliary diagonal matrices:
D1 := diag{
D2 := diag
Multiplying, we ﬁnd that the matrix V := D1MD2 has
matrix elements
2eiφj tanh(rj)
Importantly, V has the structure of a Vandermonde matrix. In particular, it has determinant
det(V ) = 1
 −eiφi tanh(ri) + eiφj tanh(rj)
The only way that det(V ) = 0 is if
ei(φi−φj) tanh(ri) = tanh(rj)
for some i = j. The squeezing feature map with phase
encoding prescribes that ri = rj = c (and we assume that
c > 0). Thus, the only solution to the above equation is
when ϕi = ϕj, which can only be true if the two feature
vectors describe the same datapoint, which we excluded
in Proposition 2. Thus, det(V ) > 0, which means that
det(M) > 0, and hence M is full rank. This means that
the columns of M, which are our feature vectors, are
linearly independent. Note that the same proof also prescribes that squeezing feature maps with absolute value
encoding makes distinct data points linearly independent
in Fock space.