HybridSN: Exploring 3D-2D CNN Feature
Hierarchy for Hyperspectral Image Classiﬁcation
Swalpa Kumar Roy, Student Member, IEEE, Gopal Krishna, Shiv Ram Dubey, Member, IEEE, and Bidyut B.
Chaudhuri, Life Fellow, IEEE
This paper is a preprint. IEEE copyright notice. “ c⃝2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all
other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective
works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.”
Abstract—Hyperspectral image (HSI) classiﬁcation is widely
used for the analysis of remotely sensed images. Hyperspectral
imagery includes varying bands of images. Convolutional Neural
Network (CNN) is one of the most frequently used deep learning
based methods for visual data processing. The use of CNN for
HSI classiﬁcation is also visible in recent works. These approaches
are mostly based on 2D CNN. Whereas, the HSI classiﬁcation
performance is highly dependent on both spatial and spectral
information. Very few methods have utilized the 3D CNN because
of increased computational complexity. This letter proposes a
Hybrid Spectral Convolutional Neural Network (HybridSN) for
HSI classiﬁcation. Basically, the HybridSN is a spectral-spatial
3D-CNN followed by spatial 2D-CNN. The 3D-CNN facilitates
the joint spatial-spectral feature representation from a stack of
spectral bands. The 2D-CNN on top of the 3D-CNN further learns
more abstract level spatial representation. Moreover, the use of
hybrid CNNs reduces the complexity of the model compared to
3D-CNN alone. To test the performance of this hybrid approach,
very rigorous HSI classiﬁcation experiments are performed over
Indian Pines, Pavia University and Salinas Scene remote sensing
datasets. The results are compared with the state-of-the-art handcrafted as well as end-to-end deep learning based methods. A
very satisfactory performance is obtained using the proposed
HybridSN for HSI classiﬁcation. The source code can be found
at 
Index Terms—Deep Learning, Convolutional Neural Networks,
Spectral-Spatial, 3D-CNN, 2D-CNN, Remote Sensing, Hyperspectral Image Classiﬁcation, HybridSN.
I. INTRODUCTION
HE research in hyperspectral image analysis is important
due to its potential applications in real life . Hyperspectral imaging results in multiple bands of images which
makes the analysis challenging due to increased volume of
data. The spectral, as well as the spatial correlation between
different bands convey useful information regarding the scene
of interest. Recently, Camps-Valls et al. have surveyed the
advances in hyperspectral image (HSI) classiﬁcation . The
HSI classiﬁcation is tackled in two ways, one with handdesigned feature extraction technique and another with learning based feature extraction technique.
Several HSI classiﬁcation approaches have been developed
using the hand-designed feature description , . Yang and
S.K. Roy and G. Krishna are with Computer Science and Engineering
Department at Jalpaiguri Government Engineering College, Jalpaiguri, West
Bengal-735102, India (email: ; ).
S.R. Dubey is with Computer Vision Group, Indian Institute of Information Technology, Sri City, Chittoor, Andhra Pradesh-517646, India (e-mail:
 ).
B.B. Chaudhuri is with Computer Vision and Pattern Recognition Unit at
Indian Statistical Institute, Kolkata-700108, India (email: ).
Qian have proposed a joint collaborative representation by
using the locally adaptive dictionary . It reduces the adverse
impact of useless pixels and improves the HSI classiﬁcation
performance. Fang et al. have utilized the local covariance
matrix to encode the relationship between different spectral
bands . They used these matrices for HSI training and classiﬁcation using Support Vector Machine (SVM). A composite
kernel is used to combine spatial and spectral information
for HSI classiﬁcation . Li et al. have applied the learning
over the combination of multiple features for the classiﬁcation
of hyperspectral scenes . Some other hand crafted approaches are Joint Sparse Model and Discontinuity Preserving
Relaxation , Boltzmann Entropy-Based Band Selection ,
Sparse Self-Representation , Fusing Correlation Coefﬁcient
and Sparse Representation , Multiscale Superpixels and
Guided Filter , and etc.
Recently, the Convolutional Neural Network (CNN) has
become very popular due to drastic performance gain over
the hand-designed features . The CNN has shown very
promising performance in many applications where visual
information processing is required, such as image classiﬁcation
 , , object detection , semantic segmentation ,
colon cancer classiﬁcation , depth estimation , face
anti-spooﬁng , etc. In recent years, a huge progress is also
made in deep learning for hyperspectral image analysis. A
dual-path network (DPN) by combining the residual network
and dense convolutional network is proposed for the HSI
classiﬁcation . Yu et al. have proposed a greedy layer-wise
approach for unsupervised training to represent the remote
sensing images . Li et al. introduced a pixel-block pair
(PBP) based data augmentation technique to generalize the
deep learning for HSI classiﬁcation . Song et al. have proposed deep feature fusion network and Cheng et al. have
used the off-the-shelf CNN models for HSI classiﬁcation .
Basically, they extracted the hierarchical deep spatial features
and used with SVM for training and classiﬁcation. Recently,
the low power consuming hardwares for deep learning based
HSI classiﬁcation is also explored . Chen et al. have used
the deep feature extraction of 3D-CNN for HSI classiﬁcation
 . Zhong et al. have proposed the spectral-spatial residual
network (SSRN) . The residual blocks in SSRN use the
identity mapping to connect every other 3-D convolutional
layer. Mou et al. have investigated the residual conv-deconv
network, an unsupervised model, for HSI classiﬁcation .
Recently, Paoletti et al. have proposed the Deep Pyramidal
Residual Networks (DPRN) specially for the HSI data .
Very recently, Paoletti et al. have also proposed spectral-spatial
 
Neighbourhood
Extraction
Hyperspectral Image
Classiﬁcation
Spectral-Spatial Feature Learning
Spatial Feature Learning
Fig. 1: Proposed HybridSpectralNet (HybridSN) Model which integrates 3D and 2D convolutions for hyperspectral image (HSI) classiﬁcation.
capsule networks to learn the hyperspectral features ,
whereas Fang et al. introduced deep hashing neural networks
for hyperspectral image feature extraction .
It is evident from the literature that using just 2D-CNN or
3D-CNN had a few shortcomings such as missing channel
relationship information or very complex model, respectively.
It also prevented these methods from achieving a better
accuracy on hyperspectral images. The main reason is due
to the fact that hyperspectral images are volumetric data
and have a spectral dimension as well. The 2D-CNN alone
isn’t able to extract good discriminating feature maps from
the spectral dimensions. Similarly, a deep 3D-CNN is more
computationally complex and alone seems to perform worse
for classes having similar textures over many spectral bands.
This is the motivation for us to propose a hybrid-CNN model
which overcomes these shortcomings of the previous models.
The 3D-CNN and 2D-CNN layers are assembled for the
proposed model in such a way that they utilise both the spectral
as well as spatial feature maps to their full extent to achieve
maximum possible accuracy.
This letter proposes the HybridSN in Section 2; presents
the experiments and analysis in Section 3; and highlights the
concluding remarks in section 4.
II. PROPOSED HYBRIDSN MODEL
Let the spectral-spatial hyperspectral data cube be denoted
by I ∈RM×N×D, where I is the original input, M is the
width, N is the height, and D is the number of spectral
bands/depth. Every HSI pixel in I contains D spectral measures and forms a one-hot label vector Y = (y1, y2, . . . yC) ∈
R1×1×C, where C represents the land-cover categories. However, the hyperspectral pixels exhibit the mixed land-cover
classes, introducing the high intra-class variability and interclass similarity into I. It is of great challenge for any model to
tackle this problem. To remove the spectral redundancy ﬁrst
the traditional principal component analysis (PCA) is applied
over the original HSI data (I) along spectral bands. The PCA
reduces the number of spectral bands from D to B while
maintaining the same spatial dimensions (i.e., width M and
height N). We have reduced only spectral bands such that it
preserves the spatial information which is very important for
recognising any object. We represent the PCA reduced data
cube by X ∈RM×N×B, where X is the modiﬁed input after
PCA, M is the width, N is the height, and B is the number
of spectral bands after PCA.
TABLE I: The layer wise summary of the proposed HybridSN architecture with
window size 25×25. The last layer is based on the Indian Pines dataset.
Layer (type)
Output Shape
# Parameter
input 1 (InputLayer)
(25, 25, 30, 1)
conv3d 1 (Conv3D)
(23, 23, 24, 8)
conv3d 2 (Conv3D)
(21, 21, 20, 16)
conv3d 3 (Conv3D)
(19, 19, 18, 32)
reshape 1 (Reshape)
(19, 19, 576)
conv2d 1 (Conv2D)
(17, 17, 64)
ﬂatten 1 (Flatten)
dense 1 (Dense)
dropout 1 (Dropout)
dense 2 (Dense)
dropout 2 (Dropout)
dense 3 (Dense)
Total Trainable Parameters: 5, 122, 176
In order to utilize the image classiﬁcation techniques, the
HSI data cube is divided into small overlapping 3D-patches,
the truth labels of which are decided by the label of the
centred pixel. We have created the 3D neighboring patches
P ∈RS×S×B from X, centered at the spatial location (α, β),
covering the S×S window or spatial extent and all B spectral
bands. The total number of generated 3D-patches (n) from X
is given by (M −S + 1) × (N −S + 1). Thus, the 3D-patch
at location (α, β), denoted by Pα,β, covers the width from
α −(S −1)/2 to α + (S −1)/2, height from β −(S −1)/2
to β + (S −1)/2 and all B spectral bands of PCA reduced
data cube X.
In 2D-CNN, the input data are convolved with 2D kernels.
The convolution happens by computing the sum of the dot
product between input data and kernel. The kernel is strided
over the input data to cover full spatial dimension. The
convolved features are passed through the activation function
to introduce the non-linearity in the model. In 2D convolution,
the activation value at spatial position (x, y) in the jth feature
map of the ith layer, denoted as vx,y
i,j , is generated using the
following equation,
i,j = φ(bi,j +
i,j,τ × vx+σ,y+ρ
where φ is the activation function, bi,j is the bias parameter
for the jth feature map of the ith layer, dl−1 is the number
of feature map in (l −1)th layer and the depth of kernel wi,j
for the jth feature map of the ith layer, 2γ + 1 is the width
 
TABLE II: The classiﬁcation accuracies (in percentages) on Indian Pines, University of Pavia, and Salinas Scene datasets using proposed and state-of-the-art methods.
Indian Pines Dataset
University of Pavia Dataset
Salinas Scene Dataset
85.30 ± 2.8
83.10 ± 3.2
79.03 ± 2.7
94.34 ± 0.2
92.50 ± 0.7
92.98 ± 0.4
92.95 ± 0.3
92.11 ± 0.2
94.60 ± 2.3
89.48 ± 0.2
87.96 ± 0.5
86.14 ± 0.8
97.86 ± 0.2
97.16 ± 0.5
96.55 ± 0.0
97.38 ± 0.0
97.08 ± 0.1
98.84 ± 0.1
91.10 ± 0.4
89.98 ± 0.5
91.58 ± 0.2
96.53 ± 0.1
95.51 ± 0.2
97.57 ± 1.3
93.96 ± 0.2
93.32 ± 0.5
97.01 ± 0.6
95.32 ± 0.1
94.70 ± 0.2
96.41 ± 0.7
95.76 ± 0.2
94.50 ± 0.2
95.08 ± 1.2
94.79 ± 0.3
94.20 ± 0.2
96.25 ± 0.6
99.19 ± 0.3
99.07 ± 0.3
98.93 ± 0.6
99.90 ± 0.0
99.87 ± 0.0
99.91 ± 0.0
99.98 ± 0.1
99.97 ± 0.1
99.97 ± 0.0
99.75 ± 0.1
99.71 ± 0.1
99.63 ± 0.2
99.98 ± 0.0
99.98 ± 0.0
99.97 ± 0.0
of kernel, 2δ + 1 is the height of kernel, and wi,j is the value
of weight parameter for the jth feature map of the ith layer.
The 3D convolution is done by convolving a 3D kernel
with the 3D-data. In the proposed model for HSI data, the
feature maps of convolution layer are generated using the
3D kernel over multiple contiguous bands in the input layer;
this captures the spectral information. In 3D convolution, the
activation value at spatial position (x, y, z) in the jth feature
map of the ith layer, denoted as vx,y,z
, is generated as follows,
= φ(bi,j +
i,j,τ × vx+σ,y+ρ,z+λ
where 2η + 1 is the depth of kernel along spectral dimension
and other parameters are the same as in (Eqn. 1).
The parameters of CNN, such as the bias b and the kernel
weight w, are usually trained using supervised approaches 
with the help of a gradient descent optimization technique. In
conventional 2D CNNs, the convolutions are applied over the
spatial dimensions only, covering all the feature maps of the
previous layer, to compute the 2D discriminative feature maps.
Whereas, for the HSI classiﬁcation problem, it is desirable to
capture the spectral information, encoded in multiple bands
along with the spatial information. The 2D-CNNs are not able
to handle the spectral information. On the other hand, the
3D-CNN kernel can extract the spectral and spatial feature
representation simultaneously from HSI data, but at the cost
of increased computational complexity. In order to take the
advantages of the automatic feature learning capability of
both 2D and 3D CNN, we propose a hybrid feature learning
framework called HybridSN for HSI classiﬁcation. The ﬂow
diagram of the proposed HybridSN network is shown in
Fig. 1. It comprises of three 3D convolutions (Eqn. 2), one
2D convolution (Eqn. 1) and three fully connected layers.
In HybridSN framework, the dimensions of 3D convolution kernels are 8 × 3 × 3 × 7 × 1 (i.e., K1
2 = 3, and
3 = 7 in Fig. 1), 16×3×3×5×8 (i.e., K2
2 = 3, and
3 = 5 in Fig. 1) and 32×3×3×3×16 (i.e., K3
3 = 3 in Fig. 1) in the subsequent 1st, 2nd and 3rd
convolution layers, respectively, where 16×3×3×5×8 means
16 3D-kernels of dimension 3×3×5 (i.e., two spatial and one
spectral dimension) for all 8 3D input feature maps. Whereas,
the dimension of 2D convolution kernel is 64 × 3 × 3 × 576
1 = 3 and K4
2 = 3 in Fig. 1), where 64 is the
number of 2D-kernels, 3 × 3 represents the spatial dimension
of 2D-kernel, and 576 is the number of 2D input feature
maps. To increase the number of spectral-spatial feature maps
simultaneously, 3D convolutions are applied thrice and can
preserve the spectral information of input HSI data in the
output volume. The 2D convolution is applied once before the
flatten layer by keeping in mind that it strongly discriminates
the spatial information within the different spectral bands
without substantial loss of spectral information, which is very
important for HSI data. A detailed summary of the proposed
model in terms of the layer types, output map dimensions
and number of parameters is given in Table I. It can be seen
that the highest number of parameters are present in the 1st
dense layer. The number of node in the last dense layer is
16, which is same as the number of classes in Indian Pines
dataset. Thus, the total number of parameters in the proposed
model depends on the number of classes in a dataset. The
total number of trainable weight parameters in HybridSN is
5, 122, 176 for Indian Pines dataset. All weights are randomly
initialised and trained using back-propagation algorithm with
the Adam optimiser by using the softmax loss. We use minibatches of size 256 and train the network for 100 epochs with
no batch normalization and data augmentation.
III. EXPERIMENTS AND DISCUSSION
A. Dataset Description and Training Details
We have used three publicly available hyperspectral image
datasets1, namely Indian Pines, University of Pavia and Salinas
Scene. The Indian Pines (IP) dataset has images with
145 × 145 spatial dimension and 224 spectral bands in the
wavelength range of 400 to 2500 nm, out of which 24 spectral
bands covering the region of water absorption have been
discarded. The ground truth available is designated into 16
classes of vegetation. The University of Pavia (UP) dataset
consists of 610×340 spatial dimension pixels with 103 spectral
bands in the wavelength range of 430 to 860 nm. The ground
truth is divided into 9 urban land-cover classes. The Salinas
Scene (SA) dataset contains the images with 512×217 spatial
dimension and 224 spectral bands in the wavelength range of
360 to 2500 nm. The 20 water absorbing spectral bands have
been discarded. In total 16 classes are present in this dataset.
All experiments are conducted on an Acer Predator-Helios
laptop with the GTX 1060 Graphical Processing Unit (GPU)
and 16 GB of RAM. We have chosen the optimal learning
rate of 0.001, based on the classiﬁcation outcomes. In order
to make the fair comparison, we have extracted the same
spatial dimension in 3D-patches of input volume for different
datasets, such as 25 × 25 × 30 for IP and 25 × 25 × 15 for UP
and SA, respectively.
1www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing Scenes
 
Fig. 2: The classiﬁcation map for Indian Pines, (a) False color image (b) Ground truth (c)-(h) Predicted classiﬁcation maps for SVM, 2D-CNN, 3D-CNN, M3D-CNN, SSRN, and
proposed HybridSN, respectively.
True Class
Predicted Class
True Class
Predicted Class
True Class
Predicted Class
Fig. 3: The confusion matrix using proposed method over Indian Pines, University of
Pavia, and Salinas Scene datasets in 1st, 2nd, and 3rd matrix, respectively.
Fig. 4: The accuracy and loss convergence vs epochs over Indian Pines dataset.
B. Classiﬁcation Results
In this letter, we have used the Overall Accuracy (OA),
Average Accuracy (AA) and Kappa Coefﬁcient (Kappa) evaluation measures to judge the HSI classiﬁcation performance.
Here, OA represents the number of correctly classiﬁed samples
out of the total test samples; AA represents the average of
class-wise classiﬁcation accuracies; and Kappa is a metric
of statistical measurement which provides mutual information
regarding a strong agreement between the ground truth map
and classiﬁcation map. The results of the proposed HybridSN
model are compared with the most widely used supervised
methods, such as SVM , 2D-CNN , 3D-CNN ,
M3D-CNN , and SSRN . The 30% and 70% of the
data are randomly divided into training and testing groups,
respectively2. We have used the publicly available code3 of
the compared methods to compute the results.
Table II shows the results in terms of the OA, AA,
and Kappa coefﬁcient for different methods4. It can be observed from Table II that the HybridSN outperforms all
the compared methods over each dataset while maintaining
the minimum standard deviation. The proposed HybridSN
is based on the hierarchical representation of spectral-spatial
3D CNN followed by a spatial 2D CNN, which are complementary to each other. It is also observed from these
results that the performance of 3D-CNN is poor than 2D-
CNN over Salinas Scene dataset. To the best of the our
2More details of dataset are provided in the supplementary material
3 
4The class-wise accuracy is provided in the supplementary material.
TABLE III: The training time in minutes (m) and test time in seconds (s) over IP, UP,
and SA datasets using 2D-CNN, 3D-CNN and HybridSN architectures.
TABLE IV: The impact of spatial window size over the performance of HybridSN.
Window IP(%)
Window IP(%)
TABLE V: The classiﬁcation accuracies (in percentages) using proposed and state-ofthe-art methods on less amount of training data, i.e., 10% only.
Indian Pines
Univ. of Pavia
Salinas Scene
80.27 78.26 68.32 96.63 95.53 94.84 96.34 95.93 94.36
82.62 79.25 76.51 96.34 94.90 97.03 85.00 83.20 89.63
81.39 81.20 75.22 95.95 93.40 97.52 94.20 93.61 96.66
98.45 98.23 86.19 99.62 99.50 99.49 99.64 99.60 99.76
98.39 98.16 98.01 99.72 99.64 99.20 99.98 99.98 99.98
knowledge, this is probably due to the presence of two classes
in the Salinas dataset (namely Grapes-untrained and Vinyarduntrained) which have very similar textures over most spectral
bands. Hence, due to the increased redundancy among the
spectral bands, the 2D-CNN outperforms the 3D-CNN over
Salinas Scene dataset. Moreover, the performance of SSRN
and HybridSN is always far better than M3D-CNN. It is
evident that 3D or 2D convolution alone is not able to represent
the highly discriminative feature compared to hybrid 3D and
2D convolutions.
The classiﬁcation map for an example hyperspectral image is illustrated in Fig. 2 using SVM, 2D-CNN, 3D-CNN,
M3D-CNN, SSRN and HybridSN methods. The quality of
classiﬁcation map of SSRN and HybridSN is far better than
other methods. Among SSRN and HybridSN, the maps generated by HybridSN in small segment are better than SSRN.
Fig. 3 shows the confusion matrix for the HSI classiﬁcation
performance of the proposed HybridSN over IP, UP and SA
datasets, respectively. The accuracy and loss convergence for
100 epochs of training and validation sets are portrayed in
Fig. 4 for the proposed method. It can be seen that the convergence is achieved in approximately 50 epochs which points
out the fast convergence of our method. The computational
efﬁciency of HybridSN model appears in term of training
and testing times in Table III. The proposed model is more
 
efﬁcient than 3D-CNN. The impact of spatial dimension over
the performance of HybridSN model is reported in Table IV.
It has been found that the used 25 × 25 spatial dimension is
most suitable for the proposed method. We have also computed
the results with an even less training data, i.e., only 10% of
total samples and have summarized the results in Table V. It
is observed from this experiment that the performance of each
model decreases slightly, whereas the proposed method is still
able to outperform other methods in almost all cases.
IV. CONCLUSION
This letter has introduced a hybrid 3D and 2D model for
hyperspectral image classiﬁcation. The proposed HybridSN
model basically combines the complementary information of
spatio-spectral and spectral in the form of 3D and 2D convolutions, respectively. The experiments over three benchmark
datasets compared with recent state-of-the-art methods conﬁrm
the superiority of the proposed method. The proposed model
is computationally efﬁcient than the 3D-CNN model. It also
shows the superior performance for small training data.