DeepGCNs: Can GCNs Go as Deep as CNNs?
 
Guohao Li∗Matthias M¨uller∗Ali Thabet
Bernard Ghanem
Visual Computing Center, KAUST, Thuwal, Saudi Arabia
{guohao.li, matthias.mueller.2, ali.thabet, bernard.ghanem}@kaust.edu.sa
Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of ﬁelds. Their success
beneﬁted from a massive boost when very deep CNN models
were able to be reliably trained. Despite their merits, CNNs
fail to properly address problems with non-Euclidean data.
To overcome this challenge, Graph Convolutional Networks
(GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs
show promising results, but they are usually limited to very
shallow models due to the vanishing gradient problem (see
Figure 1). As a result, most state-of-the-art GCN models are
no deeper than 3 or 4 layers. In this work, we present new
ways to successfully train very deep GCNs. We do this by
borrowing concepts from CNNs, speciﬁcally residual/dense
connections and dilated convolutions, and adapting them to
GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use
these new concepts to build a very deep 56-layer GCN, and
show how it signiﬁcantly boosts performance (+3.7% mIoU
over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly beneﬁt from this work, as it opens up many opportunities for
advancing GCN-based research.
1. Introduction
GCNs have been gaining a lot of momentum in the last
few years. This increased interest is attributed to two main
factors: the increasing proliferation of non-Euclidean data
in real-world applications, and the limited performance of
CNNs when dealing with such data. GCNs operate directly
on non-Euclidean data and are very promising for applications that depend on this information modality. GCNs are
currently used to predict individual relations in social networks , model proteins for drug discovery , enhance predictions of recommendation engines , efﬁciently segment large point clouds , among other ﬁelds.
∗equal contribution
7 layers w/o residual
14 layers w/o residual
28 layers w/o residual
56 layers w/o residual
7 layers w/ residual
14 layers w/ residual
28 layers w/ residual
56 layers w/ residual
Figure 1. Training Deep GCNs. (left) We show the training loss
for GCNs with 7, 14, 28, and 56 layers, with and without residual
connections. We note how adding more layers without residual
connections translates to substantially higher loss. (right) In contrast, training GCNs with residual connections results in consistent
stability across all depths.
A key reason behind the success of CNNs is the ability to design and reliably train very deep CNN models. In
contrast, it is not yet clear how to properly train deep GCN
architectures, where several works have studied their limitations . Stacking more layers into a GCN leads
to the common vanishing gradient problem. This means
that back-propagating through these networks causes oversmoothing, eventually leading to features of graph vertices
converging to the same value . Due to these limitations,
most state-of-the-art GCNs are no deeper than 4 layers .
Vanishing gradients is not a foreign phenomenon in the
world of CNNs.
It also posed limitations on the depth
growth of these types of networks. ResNet provided
a big step forward in the pursuit of very deep CNNs when
it introduced residual connections between input and output
layers. These connections massively alleviated the vanishing gradient problem. Today, ResNets can reach 152 layers
and beyond. Further extension came with DenseNet ,
where more connections are introduced across layers. More
layers could potentially mean more spatial information loss
due to pooling. This issue was also addressed, with Dilated
Convolutions . The introductions of these key concepts
had substantial impact on the progress of CNNs, and we believe they can have a similar effect if well adapted to GCNs.
 
In this work, we present an extensive study of methodologies that allow for training very deep GCNs.
adapt concepts that were successful in training deep CNNs,
mainly residual connections, dense connections, and dilated
convolutions. We show how we can incorporate these layers
into a graph framework, and present an extensive analysis
of the effect of these additions to the accuracy and stability of deep GCNs. To showcase these layer adaptations, we
apply them to the popular task of point cloud semantic segmentation. We show that adding a combination of residual
and dense connections, and dilated convolutions, enables
successful training of GCNs up to 56 layers deep (refer to
Figure 1). This very deep GCN improves the state-of-the-art
on the challenging S3DIS point cloud dataset by 3.7%.
Contributions. We summarize our contributions as three
fold. (1) We adapt residual/dense connections, and dilated
convolutions to GCNs. (2) We present extensive experiments on point cloud data, showing the effect of each of
these new layers to the stability and performance of training deep GCNs. We use point cloud semantic segmentation
as our experimental testbed. (3) We show how these new
concepts help build a 56-layer GCN, the deepest GCN architecture by a large margin, and achieve close to 4% boost
in state-of-the-art performance on the S3DIS dataset .
2. Related Work
A large number of real-world applications deal with non-
Euclidean data, which cannot be systematically and reliably
processed by CNNs in general. To overcome the shortcomings of CNNs, GCNs provide well-suited solutions for non-
Euclidean data processing, leading to greatly increasing interest in using GCNs for a variety of applications. In social
networks , graphs represent connections between individuals based on mutual interests/relations. These connections are non-Euclidean and highly irregular. GCNs help
better estimate edge strengths between the vertices of social
network graphs, thus leading to more accurate connections
between individuals. Graphs are also used to model chemical molecule structures . Understanding the bioactivities of these molecules can have substantial impact on
drug discovery. Another popular use of graphs is in recommendation engines , where accurate modelling
of user interactions leads to improved product recommendations. Graphs are also popular modes of representation in
natural language processing , where they are used to
represent complex relations between large text units.
GCNs also ﬁnd many applications in computer vision.
In scene graph generation, semantic relations between objects are modelled using a graph. This graph is used to
detect and segment objects in images, and also to predict
semantic relations between object pairs .
Scene graphs also facilitate the inverse process, where an
image is reconstructed given a graph representation of the
scene . Graphs are also used to model human joints
for action recognition in video . GCNs are a perfect candidate for 3D point cloud processing, especially
since the unstructured nature of point clouds poses a representational challenge for systematic research. Several attempts in creating structure from 3D data exist by either
representing it with multiple 2D views , or by
voxelization . More recent work focuses on
directly processing unordered point cloud representations
 . The recent EdgeConv method by Wang
et al. applies GCNs to point clouds. In particular, they
propose a dynamic edge convolution algorithm for semantic
segmentation of point clouds. The algorithm dynamically
computes node adjacency at each graph layer using the distance between point features. This work demonstrates the
potential of GCNs for point cloud related applications and
beats the state-of-the-art in the task of point cloud segmentation. Unlike most other works, EdgeConv does not rely on
RNNs or complex point aggregation methods.
Current GCN algorithms including EdgeConv are limited to shallow depths. Recent works attempt to train deeper
GCNs. For instance, Kipf et al. trained a semi-supervised
GCN model for node classiﬁcation and showed how performance degrades when using more than 3 layers . Pham
et al. proposed Column Network (CLN) for collective
classiﬁcation in relational learning and showed peak performance with 10 layers with the performance degrading for
deeper graphs. Rahimi et al. developed a Highway
GCN for user geo-location in social media graphs, where
they add “highway” gates between layers to facilitate gradient ﬂow. Even with these gates, the authors demonstrate
performance degradation after 6 layers of depth. Xu et al.
 developed a Jump Knowledge Network for representation learning and devised an alternative strategy to select
graph neighbors for each node based on graph structure. As
with other works, their network is limited to a small number of layers (6). Recently, Li et al. studied the depth
limitations of GCNs and showed that deep GCNs can cause
over-smoothing, which results in features at vertices within
each connected component converging to the same value.
Other works also show the limitations of stacking
multiple GCN layers, which lead to highly complex backpropagation and the common vanishing gradient problem.
Many difﬁculties facing GCNs nowadays (e.g. vanishing
gradients and limited receptive ﬁeld) were also present in
the early days of CNNs . We bridge this gap and
show that the majority of these drawbacks can be remedied
by borrowing several orthogonal tricks from CNNs. Deep
CNNs achieved a huge boost in performance with the introduction of ResNet . By adding residual connections
between inputs and outputs of layers, ResNet tends to alleviate the vanishing gradient problem. DenseNet takes
this idea a step further and adds connections across layers as
well. Dilated Convolutions are a more recent approach
that has lead to signiﬁcant performance gains, speciﬁcally
in image-to-image translation tasks such as semantic segmentation , by increasing the receptive ﬁeld without
loss of resolution.
In this work, we show how one can
beneﬁt from concepts introduced for CNNs, mainly residual/dense connections and dilated convolutions, to train
very deep GCNs. We support our claim by extending the
work of Wang et al. to a much deeper GCN, and therefore signiﬁcantly increasing its performance. Extensive experiments on the task of point cloud semantic segmentation
validate these ideas for general graph scenarios.
3. Methodology
3.1. Representation Learning on Graphs
Graph Deﬁnition. A graph G is represented by a tuple G =
(V, E) where V is the set of unordered vertices and E is the
set of edges representing the connectivity between vertices
v ∈V. If ei,j ∈E, then vertices vi and vj are connected to
each other with an edge ei,j.
Graph Convolution Networks. Inspired by CNNs, GCNs
intend to extract richer features at a vertex by aggregating
features of vertices from its neighborhood. GCNs represent
vertices by associating each vertex v with a feature vector hv ∈RD, where D is the feature dimension. Therefore, the graph G as a whole can be represented by concatenating the features of all the unordered vertices, i.e.
hG = [hv1, hv2, ..., hvN ]⊤∈RN×D, where N is the cardinality of set V. A general graph convolution operation F
at the l-th layer can be formulated as the following aggregation and update operations,
Gl+1 = F(Gl, Wl)
= Update(Aggregate(Gl, Wagg
), Wupdate
Gl = (Vl, El) and Gl+1 = (Vl+1, El+1) are the input and
output graphs at the l-th layer, respectively.
are the learnable weights of the aggregation and
update functions respectively, and they are the essential
components of GCNs. In most GCN frameworks, aggregation functions are used to compile information from the
neighborhood of vertices, while update functions perform a
non-linear transform on the aggregated information to compute new vertex representations. There are different variants
of those two functions. For example, the aggregation function can be a mean aggregator , a max-pooling aggregator , an attention aggregator or an LSTM
aggregator . The update function can be a multi-layer
perceptron , a gated network , etc. More concretely, the representation of vertices is computed at each
layer by aggregating features of neighbor vertices for all
vl+1 ∈Vl+1 as follows,
hvl+1 = φ (hvl, ρ({hul|ul ∈N(vl)}, hvl, Wρ), Wφ), (2)
where ρ is a vertex feature aggregation function and φ is a
vertex feature update function, hvl and hvl+1 are the vertex features at the l-th layer and l + 1-th layer respectively.
N(vl) is the set of neighbor vertices of v at the
l-th layer, and hul is the feature of those neighbor vertices parametrized by Wρ. Wφ contains the learnable parameters of these functions.
For simplicity and without
loss of generality, we use a max-pooling vertex feature aggregator, without learnable parameters, to pool the difference of features between vertex vl and all of its neighbors:
ρ(.) = max(hul −hvl| ul ∈N(vl)). We then model the
vertex feature updater φ as a multi-layer perceptron (MLP)
with batch normalization and a ReLU as an activation
function. This MLP concatenates hvl with its aggregate features from ρ(.) to form its input.
Dynamic Edges. As mentioned earlier, most GCNs have
ﬁxed graph structures and only update the vertex features
at each iteration. Recent work demonstrates
that dynamic graph convolution, where the graph structure
is allowed to change in each layer, can learn better graph
representations compared to GCNs with ﬁxed graph structure. For instance, ECC (Edge-Conditioned Convolution)
 uses dynamic edge-conditional ﬁlters to learn an edgespeciﬁc weight matrix.
Moreover, EdgeConv ﬁnds
the nearest neighbors in the current feature space to reconstruct the graph after every EdgeConv layer. In order
to learn to generate point clouds, Graph-Convolution GAN
(Generative Adversarial Network) also applies k-NN
graphs to construct the neighbourhood of each vertex in every layer. We ﬁnd that dynamically changing neighbors in
GCNs helps alleviate the over-smoothing problem and results in an effectively larger receptive ﬁeld, when deeper
GCNs are considered. In our framework, we propose to recompute edges between vertices via a Dilated k-NN function in the feature space of each layer to further increase
the receptive ﬁeld. In what follows, we provide detailed description of three operations that can enable much deeper
GCNs to be trained: residual connections, dense connections, and dilated aggregation.
3.2. Residual Learning for GCNs
Designing deep GCN architectures is an open
problem in the graph learning space.
Recent work suggests that GCNs do not scale well to deep architectures, since stacking multiple layers of graph convolutions leads to high complexity in back-propagation. As
such, most state-of-the-art GCN models are usually no more
than 3 layers deep . Inspired by the huge success of
ResNet , DenseNet and Dilated Convolutions ,
Max Pooling
Fusion Block
Prediction
k = # of nearest neighbors
f = # of filters or hidden units
d = dilation rate
k=16 f=64 d=1
k=16 f=64 d=1
k=16 f=64 d=2
k=16 f=64 d=26
k=16 f=64 d=27
k=16 f=64 d=1
k=16 f=32 d=1
k=16 f=32 d=2
k=16 f=32 d=26
k=16 f=32 d=27
Figure 2. Proposed GCN architecture for point cloud semantic segmentation. (left) Our framework consists of three blocks: a GCN
Backbone Block (feature transformation of input point cloud), a Fusion Block (global feature generation and fusion), and an MLP Prediction Block (point-wise label prediction). (right) We study three types of GCN Backbone Block (PlainGCN, ResGCN and DenseGCN) and
use two kinds of layer connection (vertex-wise addition used in ResGCN or vertex-wise concatenation used in DenseGCN).
we transfer these ideas to GCNs to unleash their full potential. This enables much deeper GCNs that reliably converge
in training and achieve superior performance in inference.
In the original graph learning framework, the underlying
mapping F, which takes a graph as an input and outputs
a new graph representation (see Equation (1)), is learned.
Here, we propose a graph residual learning framework that
learns an underlying mapping H by ﬁtting another mapping
F. After Gl is transformed by F, vertex-wise addition is
performed to obtain Gl+1. The residual mapping F learns
to take a graph as input and outputs a residual graph representation Gres
l+1 for the next layer. Wl is the set of learnable
parameters at layer l. In our experiments, we refer to our
residual model as ResGCN.
Gl+1 = H(Gl, Wl)
= F(Gl, Wl) + Gl = Gres
3.3. Dense Connections in GCNs
DenseNet was proposed to exploit dense connectivity among layers, which improves information ﬂow in the
network and enables efﬁcient reuse of features among layers. Inspired by DenseNet, we adapt a similar idea to GCNs
so as to exploit information ﬂow from different GCN layers.
In particular, we have:
Gl+1 = H(Gl, Wl)
= T (F(Gl, Wl), Gl)
= T (F(Gl, Wl), ..., F(G0, W0), G0).
The operator T is a vertex-wise concatenation function that
densely fuses the input graph G0 with all the intermediate GCN layer outputs. To this end, Gl+1 consists of all
the GCN transitions from previous layers. Since we fuse
GCN representations densely, we refer to our dense model
as DenseGCN. The growth rate of DenseGCN is equal to
the dimension D of the output graph (similar to DenseNet
for CNNs ). For example, if F produces a D dimensional vertex feature, where the vertices of the input graph
G0 are D0 dimensional, the dimension of each vertex feature of Gl+1 is D0 + D × (l + 1).
3.4. Dilated Aggregation in GCNs
Dilated wavelet convolution is an algorithm originating
from the wavelet processing domain . To alleviate spatial information loss caused by pooling operations,
Yu et al. propose dilated convolutions as an alternative
to applying consecutive pooling layers for dense prediction
tasks, e.g. semantic image segmentation. Their experiments
demonstrate that aggregating multi-scale contextual information using dilated convolutions can signiﬁcantly increase
the accuracy of semantic segmentation tasks. The reason
behind this is the fact that dilation enlarges the receptive
ﬁeld without loss of resolution. We believe that dilation can
also help with the receptive ﬁelds of deep GCNs. Therefore,
we introduce dilated aggregation to GCNs. There are many
possible ways to construct a dilated neighborhood. We use
a Dilated k-NN to ﬁnd dilated neighbors after every GCN
layer and construct a Dilated Graph. In particular, for an
input graph G = (V, E) with Dilated k-NN and d as the dilation rate, the Dilated k-NN returns the k nearest neighbors
within the k × d neighborhood region by skipping every d
neighbors. The nearest neighbors are determined based on
a pre-deﬁned distance metric. In our experiments, we use
the ℓ2 distance in the feature space of the current layer.
Let N (d)(v) denote the d-dilated neighborhood of vertex
v. If (u1, u2, ..., uk×d) are the ﬁrst sorted k × d nearest
neighbors, vertices (u1, u1+d, u1+2d, ..., u1+(k−1)d) are the
d-dilated neighbors of vertex v (see Figure 3), i.e.
N (d)(v) = {u1, u1+d, u1+2d, ..., u1+(k−1)d}.
Figure 3. Dilated Convolution in GCNs. Visualization of dilated
convolution on a structured graph arranged in a grid (e.g. 2D image) and on a general structured graph. (top) 2D convolution with
kernel size 3 and dilation rate 1, 2, 4 (left to right). (bottom) Dynamic graph convolution with dilation rate 1, 2, 4 (left to right).
Therefore, the edges E(d) of the output graph are deﬁned
on the set of d-dilated vertex neighbors N (d)(v). Speciﬁcally, there exists a directed edge e ∈E(d) from vertex v to
every vertex u ∈N (d)(v). The GCN aggregation and update functions are applied, as in Equation (1), by using the
edges E(d) created by the Dilated k-NN, so as to generate
the feature h(d)
of each output vertex in V(d). We denote
this layer operation as a dilated graph convolution with dilation rate d, or more formally: G(d) = (V(d), E(d)). To
improve generalization, we use stochastic dilation in practice. During training, we perform the aforementioned dilated aggregations with a high probability (1 −ϵ) leaving a
small probability ϵ to perform random aggregation by uniformly sampling k neighbors from the set of k×d neighbors
{u1, u2, ..., uk×d}. At inference time, we perform deterministic dilated aggregation without stochasticity.
4. Experiments
We propose ResGCN and DenseGCN to handle the vanishing gradient problem of GCNs. To enlarge the receptive
ﬁeld, we deﬁne a dilated graph convolution operator for
GCNs. To evaluate our framework, we conduct extensive
experiments on the task of large-scale point cloud segmentation and demonstrate that our methods signiﬁcantly improve performance. In addition, we also perform a comprehensive ablation study to show the effect of different components of our framework.
4.1. Graph Learning on 3D Point Clouds
Point cloud segmentation is a challenging task because
of the unordered and irregular structure of 3D point clouds.
Normally, each point in a point cloud is represented by its
3D spatial coordinates and possibly auxiliary features such
as color and surface normal. We treat each point as a vertex
v in a directed graph G and we use k-NN to construct the
directed dynamic edges between points at every GCN layer
(refer to Section 3.1). In the ﬁrst layer, we construct the
input graph G0 by executing a dilated k-NN search to ﬁnd
the nearest neighbor in 3D coordinate space. At subsequent
layers, we dynamically build the edges using dilated k-NN
in feature space. For the segmentation task, we predict the
categories of all the vertices at the output layer.
4.2. Experimental Setup
We use the overall accuracy (OA) and mean intersection
over union (mIoU) across all classes as evaluation metrics.
For each class, the IoU is computed as
T P +T −P , where
TP is the number of true positive points, T is the number of
ground truth points of that class, and P is the number of predicted positive points. To motivate the use of deep GCNs,
we do a thorough ablation study on area 5 to analyze each
component and provide insights. We then evaluate our proposed reference model (backbone of 28 layers with residual graph connections and stochastic dilated graph convolutions) on all 6 areas and compare it to the shallow DGCNN
baseline and other state-of-the-art methods.
4.3. Network Architectures
As shown in Figure 2, all the network architectures in
our experiments have three blocks: a GCN backbone block,
a fusion block and an MLP prediction block. The GCN
backbone block is the only part that differs between experiments. For example, the only difference between PlainGCN
and ResGCN is the use of residual skip connections for
all GCN layers in ResGCN. Both have the same number
of parameters. We linearly increase the dilation rate d of
dilated k-NN with network depth.
For fair comparison,
we keep the fusion and MLP prediction blocks the same
for all architectures. In the S3DIS semantic segmentation
task, the GCN backbone block takes as input a point cloud
with 4096 points, extracts features by applying consecutive
GCN layers to aggregate local information, and outputs a
learned graph representation with 4096 vertices. The fusion
and MLP prediction blocks follow a similar architecture as
PointNet and DGCNN . The fusion block is used
to fuse the global and multi-scale local features. It takes as
input the extracted vertex features from the GCN backbone
block at every GCN layer and concatenates those features,
then passes them through a 1×1 convolution layer followed
by max pooling. The latter layer aggregates the vertex features of the whole graph into a single global feature vector,
which in return is concatenated with the feature of each vertex from all previous GCN layers (fusion of global and local
information). The MLP prediction block applies three MLP
layers to the fused features of each vertex/point to predict
its category. In practice, these layers are 1×1 convolutions.
PlainGCN. This baseline model consists of a PlainGCN
backbone block, a fusion block, and a MLP prediction
block. The backbone stacks 28 EdgeConv layers with
dynamic k-NN, each of which is similar to the one used in
DGCNN . No skip connections are used here.
ResGCN. We construct ResGCN by adding dynamic dilated k-NN and residual graph connections to PlainGCN.
These connections between all GCN layers in the GCN
backbone block do not increase the number of parameters.
DenseGCN. Similarly, DenseGCN is built by adding dynamic dilated k-NN and dense graph connections to the
PlainGCN. As described in Section 3.3, dense graph connections are created by concatenating all the intermediate
graph representations from previous layers. The dilation
rate schedule of our DenseGCN is the same as ResGCN.
4.4. Implementation
We implement all our models using Tensorﬂow. For fair
comparison, we use the Adam optimizer with the same initial learning rate 0.001 and the same learning rate schedule;
the learning rate decays 50% every 3 × 105 gradient decent
steps. The networks are trained with two NVIDIA Tesla
V100 GPUs using data parallelism. The batch size is set
to 8 for each GPU. Batch Normalization is applied to every
layer. Dropout with a rate of 0.3 is used at the second MLP
layer of the MLP prediction block. As mentioned in Section
3.4, we use dilated k-NN with a random uniform sampling
probability ϵ = 0.2 for GCNs with dilations. In order to isolate the effect of the proposed deep GCN architectures, we
do not use any data augmentation or post processing techniques. We train our models end-to-end from scratch.
4.5. Results
For convenient referencing, we use the naming convention BackboneBlock-#Layers to denote the key models in
our analysis and we provide all names in Table 1. We focus on residual graph connections for our analysis, since
ResGCN-28 is easier and faster to train, but we expect that
our observations also hold for dense graph connections.
We investigate the performance of different ResGCN architectures, e.g. with dynamic dilated k-NN, with regular
dynamic k-NN (without dilation), and with ﬁxed edges. We
also study the effect of different parameters, e.g. number of
k-NN neighbors (4, 8, 16, 32), number of ﬁlters (32, 64,
128), and number of layers (7, 14, 28, 56). Overall, we
conduct 20 experiments and show their results in Table 1.
Effect of residual graph connections. Our experiments in
Table 1 (Reference) show that residual graph connections
play an essential role in training deeper networks, as they
tend to result in more stable gradients. This is analogous
to the insight from CNNs . When the residual graph
connections between layers are removed (i.e. in PlainGCN-
28), performance dramatically degrades (-12% mIoU). In
Appendices A and B, we show similar performance gains
by combining residual graph connections and dilated graph
convolutions with other types of GCN layers.
Effect of dilation. Results in Table 1 (Dilation) show
that dilated graph convolutions account for a 2.85% improvement in mean IoU (row 3), motivated primarily by
the expansion of the network’s receptive ﬁeld. We ﬁnd that
adding stochasticity to the dilated k-NN does help performance but not to a signiﬁcant extent. Interestingly, our results in Table 1 also indicate that dilation especially helps
deep networks when combined with residual graph connections (rows 1,8). Without such connections, performance
can actually degrade with dilated graph convolutions. The
reason for this is probably that these varying neighbors result in ‘worse’ gradients, which further hinder convergence
when residual graph connections are not used.
Effect of dynamic k-NN. While we observe an improvement when updating the k nearest neighbors after every
layer, we would also like to point out that it comes at a relatively high computational cost. We show different variants
without dynamic edges in Table 1 (Fixed k-NN).
Effect of dense graph connections.
We observe similar performance gains with dense graph connections
(DenseGCN-28) in Table 1 (Connections). However, with
a naive implementation, the memory cost is prohibitive.
Hence, the largest model we can ﬁt into GPU memory uses
only 32 ﬁlters and 8 nearest neighbors, as compared to 64
ﬁlters and 16 neighbors in the case of its residual counterpart ResGCN-28. Since the performance of these two deep
GCN variants is similar, residual connections are more practical for most use cases and, hence we focus on them in our
ablation study. Yet, we do expect the same insights to transfer to the case of dense graph connections.
Effect of nearest neighbors. Results in Table 1 (Neighbors) show that a larger number of neighbors helps in general. As the number of neighbors is decreased by a factor of
2 and 4, the performance drops by 2.5% and 3.3% respectively. However, a large number of neighbors only results in
a performance boost, if the network capacity is sufﬁciently
large. This becomes apparent when we increase the number
of neighbors by a factor of 2 and decrease the number of
ﬁlters by a factor of 2.
Effect of network depth. Table 1 (Depth) shows that increasing the number of layers improves network performance, but only if residual graph connections and dilated
graph convolutions are used, as in Table 1 (Connections).
Effect of network width. Results in Table 1 (Width) show
that increasing the number of ﬁlters leads to a similar increase in performance as increasing the number of layers.
In general, a higher network capacity enables learning nuances necessary for succeeding in corner cases.
connection
stochastic
PlainGCN-28
Fixed k-NN
Connections
DenseGCN-28
ResGCN-28W
Table 1. Ablation study on area 5 of S3DIS. We compare our reference network (ResGCN-28) with 28 layers, residual graph connections,
and dilated graph convolutions to several ablated variants. All models were trained with the same hyper-parameters for 100 epochs on all
areas except for area 5, which is used for evaluation. We denote residual and dense connections with the ⊕and ▷◁symbols respectively.
We highlight the most important results in bold. ∆mIoU denotes the difference in mIoU with respect to the reference model ResGCN-28.
Qualitative Results. Figure 4 shows qualitative results on
area 5 of S3DIS . As expected from the results in Table 1, our ResGCN-28 and DenseGCN-28 perform particularly well on difﬁcult classes such as board, beam, bookcase and door. Rows 1-4 clearly show how ResGCN-28 and
DenseGCN-28 are able to segment the board, beam, bookcase and door respectively, while PlainGCN-28 completely
fails. Please refer to Appendices C, D and E for more qualitative results and other ablation studies.
Comparison to state-of-the-art. Finally, we compare our
reference network (ResGCN-28), which incorporates the
ideas put forward in the methodology, to several state-ofthe-art baselines in Table 2. The results clearly show the
effectiveness of deeper models with residual graph connections and dilated graph convolutions. ResGCN-28 outperforms DGCNN by 3.9% (absolute) in mean IoU, even
though DGCNN has the same fusion and MLP prediction
blocks as ResGCN-28 but with a shallower PlainGCN backbone block. Furthermore, we outperform all baselines in 9
out of 13 classes. We perform particularly well in the difﬁcult object classes such as board, where we achieve 51.1%,
and sofa, where we improve state-of-the-art by about 10%.
This signiﬁcant performance improvement on the difﬁcult classes is probably due to the increased network capacity, which allows the network to learn subtle details necessary to distinguish between a board and a wall for example. The ﬁrst row in Figure 4 is a representative example
for this occurrence. Our performance gains are solely due
to our innovation in the network architecture, since we use
the same hyper-parameters and even learning rate schedule
as the baseline DGCNN and only decrease the number of nearest neighbors from 20 to 16 and the batch size
from 24 to 16 due to memory constraints. We outperform
state-of-the art methods by a signiﬁcant margin and expect
further improvement from tweaking the hyper-parameters,
especially the learning schedule.
5. Conclusion and Future Work
In this work, we investigate how to bring proven useful concepts (residual connections, dense connections and
dilated convolutions) from CNNs to GCNs and answer the
question: how can GCNs be made deeper? Extensive experiments show that by adding skip connections to GCNs, we
can alleviate the difﬁculty of training, which is the primary
problem impeding GCNs to go deeper. Moreover, dilated
graph convolutions help to gain a larger receptive ﬁeld without loss of resolution. Even with a small amount of nearest neighbors, deep GCNs can achieve high performance on
point cloud semantic segmentation. ResGCN-56 performs
very well on this task, although it uses only 8 nearest neighbors compared to 16 for ResGCN-28. We were also able
to train ResGCN-151 for 80 epochs; the network converged
very well and achieved similar results as ResGCN-28 and
ResGCN-56 but with only 3 nearest neighbors. Due to com-
Ground Truth
PlainGCN-28
DenseGCN-28
Figure 4. Qualitative Results on S3DIS Semantic Segmentation. We show here the effect of adding residual and dense graph connections
to deep GCNs. PlainGCN-28, ResGCN-28, and DenseGCN-28 are identical except for the presence of residual graph connections in
ResGCN-28 and dense graph connections in DenseGCN-28. We note how both residual and dense graph connections have a substantial
effect on hard classes like board, bookcase, and sofa. These are lost in the results of PlainGCN-28.
PointNet 
PointNet++ 
3DRNN+CF 
DGCNN 
ResGCN-28 (Ours)
Table 2. Comparison of ResGCN-28 with state-of-the-art on S3DIS Semantic Segmentation. We report average per-class results across
all areas for our reference model ResGCN-28, which has 28 GCN layers, residual graph connections, and dilated graph convolutions, and
state-of-the-art baselines. ResGCN-28 outperforms state-of-the-art by almost 4%. It also outperforms all baselines in 9 out of 13 classes.
The metrics shown are overall point accuracy (OA) and mean IoU (mIoU). ’-’ denotes not reported and bold denotes best performance.
putational constraints, we were unable to investigate such
deep architectures in detail and leave it for future work.
Our results show that after solving the vanishing gradient
problem plaguing deep GCNs, we can either make GCNs
deeper or wider (e.g. ResGCN-28W) to get better performance. We expect GCNs to become a powerful tool for
processing non-Euclidean data in computer vision, natural
language processing, and data mining. We show successful cases for adapting concepts from CNNs to GCNs. In
the future, it will be worthwhile to explore how to transfer
other operators, e.g. deformable convolutions , other architectures, e.g. feature pyramid architectures , etc. It
will also be interesting to study different distance measures
to compute dilated k-NN, constructing graphs with different k at each layer, better dilation rate schedules for
GCNs, and combining residual and dense connections.
We also point out that, for the speciﬁc task of point cloud
semantic segmentation, the common approach of processing the data in 1m × 1m columns is sub-optimal for graph
representation. A more suitable sampling approach should
lead to further performance gains on this task.
Acknowledgments.
The authors thank Adel Bibi and
Guocheng Qian for their help with the project. This work
was supported by the King Abdullah University of Science
and Technology (KAUST) Ofﬁce of Sponsored Research
through the Visual Computing Center (VCC) funding.