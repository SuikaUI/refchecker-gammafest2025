The Dark Universe
Matthias Bartelmann∗
Zentrum f¨ur Astronomie der Universit¨at Heidelberg, Institut f¨ur Theoretische Astrophysik, Albert- ¨Uberle-Str. 2,
69120 Heidelberg, Germany
 
For a few years now, cosmology has a standard model. By this term, we mean a consistent theoretical background
which is at the same time simple and broad enough to oﬀer coherent explanations for the vast majority of cosmological phenomena. This review will brieﬂy summarise the cosmological model, then proceed to discuss what
we know from observations about the evolution of the Universe and its contents, and what we conclude about the
origin and the future of the Universe and the structures it contains.
I. THE COSMOLOGICAL STANDARD MODEL
A. Historical outline
This review will not follow the historical sequence of discoveries but rather present the arguments in a perhaps more
logical order than history has allowed. Yet, let us begin with a
brief and necessarily fragmental historical outline.
Modern cosmology began in 1915 with Einstein’s theory
of General Relativity, from which Friedmann in 1922 derived
the homogeneous and isotropic class of world models which
to this date form the foundation of the cosmological standard
model. His discovery that static world models were impossible conﬁrmed Einstein in his introduction of the cosmological
constant, which he abandoned in the 1930s after Slipher, Hubble and Humason had discovered the cosmic expansion. Earlier, Lemaˆıtre had speculated about a primordial ﬁreball which
seemed the natural starting point of an expanding universe.
First evidence for dark matter was found by Zwicky in
galaxy clusters, and by Babcock and Oort in galaxies, between
1933 and 1940. In 1946, Lifshitz worked out relativistic perturbation theory and started applying it to the linear growth
of cosmic structures. In the late 1940s, Gamow, Alpher and
Herman worked out how nuclear fusion may have proceeded
in the early universe and predicted a cosmic radiation background with a temperature of a few Kelvin.
The cosmic microwave background (CMB) was discovered
by Penzias and Wilson and explained by Dicke and collaborators in 1965. In 1970, Peebles and Yu as well as Sunyaev and Zel’dovich independently predicted structures in the
CMB and found that they should have relative amplitudes near
10−4, where subsequent searches did not ﬁnd them. Peebles
suggested in 1982 that a lower CMB ﬂuctuation level could
be explained if the dark matter could not interact with light.
It was quickly realised by means of the numerical simulations by Davis, Efstathiou, Frenk and White that cosmological structure formation could be explained within the emerging paradigm of cold dark matter. This paradigm experienced
further strong support when temperature ﬂuctuations at the revised level were ﬁnally found in the CMB with the COBE
∗Electronic address: 
satellite in 1992.
It was suggested by Guth in 1981 that a phase of inﬂationary expansion could solve the horizon and ﬂatness problems
of Friedmann cosmology. Mukhanov and Chibisov immediately saw that inﬂated quantum ﬂuctuations could be the origin of cosmic structures. At least in simple scenarios, inﬂation
requires a spatially ﬂat universe and thus a total energy density equal to its critical value, but it became obvious that matter alone cannot be as dense. The diﬀerence was tentatively
attributed to the cosmological constant, which was conﬁrmed
by several groups in the late 1990s through the accelerated
cosmic expansion inferred from type-Ia supernovae. The cosmological standard model could be considered complete when
the recent measurements of the CMB temperature ﬂuctuations
conﬁrmed that the universe is spatially ﬂat.
Modern cosmology is thoroughly described in many textbooks. To mention only a few, see and the textbook on the CMB
 .
This review is structured as follows. The introduction proceeds by summarising the framework of the Friedmann models in I.B and the growth of cosmic structures in I.C. Section II
describes what we know about the age of the Universe from
its ingredients through nuclear cosmo-chronology (II.A), stars
(II.B) and the cooling of white dwarfs (II.C). In Sect. III, measurements of the Hubble constant from Hubble’s law (III.A)
and gravitational lensing (III.B) are reviewed. Big-bang nucleosynthesis is the subject of Sect. IV, with subsections on
the production of light elements (IV.A) and the observed element abundances (IV.B). Section V gives an overview of
the matter density in the Universe, as constrained by galaxies (V.A), galaxy clusters as individual objects (V.B) and as
an evolving population (V.C). Section VI on the cosmic microwave background begins with the isotropic CMB (VI.A)
and continues with its ﬂuctuations (VI.B). Cosmic structures
are reviewed in Sect. VII, which starts with the quantiﬁcation of structures (VII.A) and continues with their measurement and results obtained (VII.B). Section VIII on cosmological weak lensing follows, in which gravitational light de-
ﬂection is ﬁrst summarised (VIII.A) before measurements
are described (VIII.B). Supernovae of type Ia are the subject of Sect.IX, where the principles of their cosmological
application (IX.A), the reasons thereof and the observational
 
results are described (IX.B. Section X on the normalisation
of the matter-ﬂuctuation power spectrum begins with an introduction (X.A) and continues describing constraints from
CMB ﬂuctuations (X.B), cosmological weak lensing (X.C),
galaxy clusters (X.D) and the Lyman-α forest (X.E). The ﬁnal
Sect. XI discusses the motivation and the evidence for cosmological inﬂation (XI.A) and ends with remarks on dark energy
B. Friedmann models
1. The metric
Cosmology studies the physical properties of the Universe
as a whole. The only of the four known interactions which
can play a role on cosmic length scales is gravity. Electromagnetism, the only other interaction with inﬁnite range, has
sources of opposite charge which tend to shield each other on
comparatively very small scales. Cosmic magnetic ﬁelds can
perhaps reach coherence lengths on the order of ≳10 Mpc, but
they are far too weak for them to be important for the cosmic
evolution. The weak and strong interactions, of course, have
microscopic range and must thus be unimportant for cosmology as a whole.
The best current theory of gravity is Einstein’s theory
of general relativity, which relates the geometry of a fourdimensional space-time manifold to its material and energy
content. Cosmological models must thus be constructed as
solutions of Einstein’s ﬁeld equations. Symmetry assumptions greatly simplify this process. Guided by observations
to be speciﬁed later, we assume that the Universe appears approximately identical in all directions of observation; in other
words, it is assumed to be isotropic on average. While this
assumption is obviously incorrect in our cosmological neighbourhood, it holds with increasing precision if observations
are averaged on increasingly large scales. The assumption
of isotropy can only be valid in a preferred reference frame
which is at rest with respect to the mean cosmic motion. The
motion of the Earth within this rest frame must be subtracted
before any observation can be expected to appear isotropic.
The second assumption asserts that the Universe should
appear equally isotropic about any of its points.
is homogeneous. Isotropic and homogeneous solutions for
Einstein’s ﬁeld equations admit the Robertson-Walker metric
whose line element is
ds2 = −c2dt2 + a2(t)
dθ2 + sin2 θ dφ2i
with the radial coordinate w and the radial function

K−1/2 sin(K1/2w)
|K|−1/2 sinh(|K|1/2w)
The curvature parameter K, which can be positive, negative or zero, has the dimension of an inverse squared length.
The scale factor a(t) isotropically stretches or shrinks the
three-dimensional spatial sections of the four-dimensional
space-time.
The scale factor is commonly normalised to
a0 = 1 at the present time. As usual, the line element ds
gives the proper time measured by an observer moving by
(dw, fK(w)dθ, fK(w) sin θdφ) within the coordinate time interval dt. For light, ds = 0.
Observers attached to the coordinate grid (w, θ, φ) on the
spatial hypersurfaces for constant cosmological time t are
called comoving. The cosmic expansion does not change the
comoving coordinates ⃗x, but the physical coordinates ⃗r = a⃗x.
2. Redshift and expansion
The changing scale of the Universe causes the cosmological redshift z. The wavelength of light from a distant comoving source seen by a comoving observer changes by the same
amount as the Universe changes its scale while the light is
travelling. Thus, the emitted and observed wavelengths λ and
λ0, respectively, are related by
where a is the scale factor at the time of emission. The relative
wavelength change is the redshift,
1 + z = a−1 ,
a = (1 + z)−1 .
When the metric is inserted into Einstein’s ﬁeld equations,
two ordinary diﬀerential equations result,
relating ˙a and ¨a to the density ρ, the pressure p, the cosmological constant Λ and the curvature K. Equation (7) can be
eliminated and replaced by
dt(ρc2a3) + pd(a3)
from which the change of ρ with a can be inferred once p is
given. Relativistic matter (“r”, “radiation”) has p = ρc2/3
while nonrelativistic matter (“m”, “dust”) is characterised by
ρc2 ≫p ≈0. Thus
For a model universe containing only matter, radiation and
the cosmological constant, Eq. (6) can be brought into the
a3 + ΩΛ0 + 1 −Ωm0 −Ωr0 −ΩΛ0
This is commonly called Friedmann’s equation, in which the
relative expansion rate ˙a/a is replaced by the Hubble function H(a) whose present value is the Hubble constant H0 ≡
H(a0) = H(1), and the matter-energy content is described by
the three density parameters Ωr0, Ωm0 and ΩΛ0.
H0 is often expressed in dimension-less form by
100 km s−1 Mpc−1
Since lengths in the Universe are typically measured with respect to the Hubble length, they are often given in units of
Similarly, luminosities are typically obtained by
multiplying ﬂuxes with squared luminosity distances and are
thus often given in units of h−2L⊙. We avoid this notation
where possible in the following and insert h = 0.7 where appropriate.
The dimension-less parameters Ωm0 and Ωr0 describe the
densities of matter and radiation today in units of the critical
A Robertson-Walker metric whose scale factor satis-
ﬁes Friedmann’s equation is called a Friedmann-Lemaˆıtre-
Robertson-Walker metric. The cosmological standard model
asserts that the Universe at large is described by such a metric,
and is thus characterised by the four parameters Ωm0, Ωr0, ΩΛ0
and H0. Since the critical density and the densities themselves
evolve in time, so do the density parameters.
The remainder of this review article is devoted to answering
two essential questions: (1) What are the values of the parameters characterising Friedmann’s equation? (2) How can we
understand the deviations of the real universe from a purely
homogeneous and isotropic space-time?
Table I, adapted from , summarises
the most important cosmological parameters and the values
adopted below, unless stated otherwise.
3. The radiation-dominated phase
It is an empirical fact that the Universe is expanding. Earlier
in time, therefore, the scale factor must have been smaller than
today, a < 1. In principle, it is possible for Friedmann models
to never reach a vanishing scale, a = 0, within ﬁnite time
into the past. However, a few crucial observational results
suﬃce to rule out such “bouncing” models. This implies that a
Universe like ours which is expanding today must have started
from a = 0 a ﬁnite time ago; thus, there must have been a Big
Equation (10) shows that the radiation density increases like
a−4 as the scale factor decreases, while the matter density increases with one power of a less. Even though the radiation
density is very much smaller today than the matter density,
there must have been a period in the early evolution of the
Universe when radiation dominated the energy density. The
scale factor when both densities were equal is called aeq. This
radiation-dominated era is very important for several observational aspects of the cosmological standard model. Since
the radiation retains the Planckian spectrum which it acquired
in the very early Universe in the intense interactions with
charged particles, its energy density is fully characterised by
its temperature T. As the energy density is both proportional
to T 4 and a−4, its temperature falls like T ∝a−1.
At the times relevant for our purposes, only photons and
neutrinos need to be considered as relativistic species. Since
photons are heated by electron-proton annihilation after neutrinos decoupled from the cosmic ﬂuid, the photons are heated
above the neutrinos by a factor (11/4)1/3, which follows directly from entropy conservation. If T is the temperature of
the cosmic microwave background after electron-positron annihilation, the energy densities in the photons and the three
neutrino species are
ρr,CMB = π2
ρr,ν = 3 × 7
or, as long as all neutrino species are relativistic,
ρr = ρr,CMB
1 + 21
!4/3≈1.68 ρr,CMB .
4. Age, distances and horizons
Since Friedmann’s equation gives the relative expansion
rate ˙a/a, we can use it to infer the cosmic time,
which illustrates that the age scale is the inverse Hubble constant H−1
0 . A simple example is given by the Einstein-de Sitter
model, which (unrealistically) assumes Ωm0 = 1, Ωr0 = 0 and
ΩΛ = 0. Then, from Eq. (10), E(a) = a−3/2 and
The cosmic age is t0 = t(a0) = t(1). The cosmic time and the
lookback time t0 −t are shown in Fig. 1 as functions of the
Distances can be deﬁned in many ways which typically lead
to diﬀerent expressions. The proper distance Dprop is the distance measured by the light-travel time, thus
dDprop = cdt
where the integral has to be evaluated between the scale factors of emission and observation of the light signal. The comoving radial coordinate w is the comoving distance measured along a radial light ray. Since light rays propagate with
zero proper time, ds = 0, this gives
+ BAO + SNe
(reference)
CMB temperature
2.728 ± 0.004 K
from 
total energy density
1.0052 ± 0.0064
matter density
0.258 ± 0.03
0.279 ± 0.015
assuming spatial ﬂatness
baryon density
0.0441 ± 0.0030
0.0462 ± 0.0015 here and below
cosmological constant
0.742 ± 0.03
0.721 ± 0.015
Hubble constant
0.701 ± 0.013
power-spectrum normalisation σ8
0.796 ± 0.036
0.817 ± 0.026
age of the Universe in Gyr
13.69 ± 0.13
13.73 ± 0.12
decoupling redshift
1087.9 ± 1.2
1088.2 ± 1.1
reionisation optical depth
0.087 ± 0.017
0.084 ± 0.016
spectral index
TABLE I Cosmological parameters obtained from the 5-year data release of WMAP , without and with the additional
constraints imposed by baryonic acoustic oscillations (BAO, § VII.A.6) and type-Ia supernovae (SNe, § IX). The dimension-less Hubble
constant h is deﬁned in (11), the normalisation parameter σ8 in (266) and the spectral index in (28). Reionisation is brieﬂy discussed in
§ X.B.2. Note that spatial ﬂatness (K = 0) was assumed in deriving most of these values, as mentioned in last column.
time in Gyr
redshift z
cosmic time
lookback time
FIG. 1 Cosmic time (solid curve) and lookback time (present age of
the universe minus cosmic time) in Gyr as functions of redshift. Here
and below, the parameters labelled as “reference” in Tab. I are used.
The angular-diameter distance Dang is deﬁned such that the
same relation as in Euclidean space holds between the physical size of an object and its angular size. It turns out to be
Dang(a) = afK[w(a)] ,
with fK(w) given by (2). The luminosity distance Dlum is analogously deﬁned to reproduce the Euclidean relation between
the bolometric luminosity of an object and its observed ﬂux.
This results in
Dlum(a) = Dang(a)
= fK[w(a)]
Figure 2 shows the angular-diameter and the proper distance
as functions of redshift.
distance in Mpc
redshift z
angular-diameter distance
proper distance
FIG. 2 Angular-diameter distance (solid curve) and proper distance
in Mpc as functions of redshift.
These distance measures rapidly diverge for scale factors
a < 1. For small distances, i.e. for a ≈1, they all reproduce
the linear relation
Since time is ﬁnite in a universe with a Big Bang, any particle can only be inﬂuenced by, and can only inﬂuence, events
within ﬁnite regions, called horizons. Several diﬀerent definitions of horizons exist. They are typically characterised
by some speed, e.g. the light speed, times the inverse Hubble function which sets the time scale.
C. Structures
1. Structure growth
The hierarchy of cosmic structures is assumed to have
grown from primordial seed ﬂuctuations in the process of
gravitational collapse: overdense regions attract material and
grow. They are described by the density contrast δ(⃗x, t) as a
function of the comoving coordinates ⃗x, which is the density
ﬂuctuation relative to the mean density ¯ρ(t),
δ(⃗x, t) ≡ρ(⃗x, t) −¯ρ(t)
Linear perturbation theory shows that, during the matterdominated era, the density contrast δ of sub-horizon perturbations is described by the second-order diﬀerential equation
¨δ + 2H˙δ −4πG¯ρδ = 0
if the dark matter is cold, i.e. if dark-matter ﬂows have negligible velocity dispersion. Equation (23) has two solutions,
one growing and one decaying. While the latter is irrelevant
for structure growth, the growing mode is described by the
growth factor D+(a), deﬁned such that the density contrast at
the scale factor a is related to the density contrast today δ0
by δ(a) = δ0D+(a). In most cases of practical relevance, the
growth factor is accurately described by
D+(a) = G(a)
with the ﬁtting formula
where the density parameters have to be evaluated at the scale
factor a. For a standard cosmological model, (25) deviates
from the accurate solution by ≤0.2 % for a ∈[0.01, 1]. The
linear growth factor is shown in Fig. 3 together with the fractional cosmic age t/t0.
A very important length scale for cosmic structure growth
is set by the horizon size at the end of the radiation-dominated
phase. Structures smaller than that became causally connected
while radiation was still dominating. The fast expansion due
to the radiation density inhibited further growth of such structures until the matter density started dominating. Small structures are thus suppressed compared to large structures, which
became causally connected only after radiation domination.
The horizon radius weq at the end of the radiation-dominated
era thus separates larger structures which could grow without inhibition, from smaller structures which were suppressed
during radiation domination. In comoving coordinates,
0 0.1 0.25
fractional time, growth factor
redshift z
fractional time
growth factor
FIG. 3 Fractional cosmic age t/t0 (solid curve) and structure growth
as functions of redshift.
2. The power spectrum
It is physically plausible that the density contrast in the Universe is a Gaussian random ﬁeld, i.e. that the probability for
ﬁnding a density contrast between δ and δ + dδ is given by
a Gaussian distribution. The principal reason for this is the
central limit theorem. A Gaussian random process is characterised by two numbers, the mean and the variance. By construction, the mean of the density contrast vanishes, such that
the variance deﬁnes it completely.
In linear approximation, density perturbations grow in
place, as Eq. (23) shows because the density contrast at one
comoving position ⃗x does not depend on the density contrast
at another. As long as structures evolve linearly, their scale
will be preserved, which implies that it is advantageous to
study structure growth in Fourier rather than real space. The
variance of the density contrast ˆδ(⃗k) at the comoving wave
vector ⃗k in Fourier space is called the power spectrum
Dˆδ(⃗k)ˆδ∗(⃗k′)
≡(2π)3Pδ(k) δD(⃗k −⃗k′) ,
where the Dirac function δD ensures that modes with diﬀerent
wave vectors are independent.
Once the power spectrum is known, the statistical properties of the linear density contrast are completely speciﬁed. It
is a remarkable fact that two simple assumptions about the
nature of the cosmic structures and the dark matter constrain
the shape of Pδ(k) completely. If the rms mass ﬂuctuation enclosed by the horizon is independent of time, and if the dark
matter is cold, the power spectrum will behave as
(k ≫keq) ,
with the spectral index ns = 1 . The
comoving wave number keq = 0.01 Mpc−1 of the peak location
in the power spectrum is set by the comoving horizon radius
at matter-radiation equality, weq, given in (26). The steep decline for structures smaller than weq reﬂects the suppression of
structure growth during radiation domination. Figure 4 shows
the linearly and the nonlinearly evolved power spectrum of
cold dark matter (CDM).
CDM power spectrum
wave number k in Mpc-1
nonlinear, z=0
nonlinear, z=1
nonlinear, z=2
FIG. 4 Linearly and non-linearly evolved cold dark matter (CDM)
power spectra. The linear CDM spectrum is shown for a = 1 and
scales like D2
3. Non-linear evolution
As the density contrast approaches unity, its evolution becomes non-linear.
In the course of non-linear evolution,
overdensities contract, causing matter to ﬂow from larger to
smaller scales. Power in the density-ﬂuctuation ﬁeld is thus
transported towards smaller modes, or towards larger wave
numbers k. This mode coupling process deforms the power
spectrum on small scales, i.e. for large k. Detailed studies
of the non-linear evolution of cosmic structures require numerical simulations, which need to cover large scales and to
resolve small scales well at the same time. Much progress has
been achieved in this ﬁeld within the last two decades due to
the fortunate combination of increasing computer power with
highly sophisticated numerical algorithms, such as particlemesh and tree codes, and adaptive mesh reﬁnement techniques
 . Although the
Zel’dovich approximation breaks down as the non-linear evolution proceeds, it is remarkable for two applications. First, it
allows a computation of the shapes of collapsing dark-matter
structures and arrives at the conclusion that the collapse must
be anisotropic, leading to the formation of sheets and ﬁlaments . Filamentary structures thus appear as a natural consequence of gravitational collapse in a
Gaussian random ﬁeld. Second, it provides an explanation for
the origin of the angular momentum of cosmic structures.
II. THE AGE OF THE UNIVERSE
A. Nuclear cosmo-chronology
1. The age of the Earth
How old is the Universe? We have no direct way to measure how long ago the Big Bang happened, but there are various ways to set lower limits to the age of the Universe. They
are all based on the same principle: since the Universe cannot
be younger than any of its parts, it must be older than the oldest objects it contains. Three methods for age determination
have been developed. One is based on the radioactive decay
of long-lived isotopes, another constrains the age of globular
clusters, and the third is based on the age of white dwarfs.
Nuclear cosmo-chronology compares the measured abundances of certain long-lived radioactive isotopes with their
initial abundances, which are eliminated by comparing abundances in diﬀerent probes.
To give a speciﬁc example, consider the two uranium isotopes 235U and 238U. They both decay into stable lead isotopes, 235U →207Pb through the actinium series and 238U →
206Pb through the radium series. The abundance of any of
these two lead isotopes is the sum of the initial abundance,
plus the amount produced by the uranium decay, e.g.
N207 = N207,0 + N235
for 207Pb, where N235 is the abundance of 235U nuclei today.
A similar equation with 235 replaced by 238 and 207 replaced
by 206 holds for the decay of 238U to 206Pb. The decay constants for the two uranium isotopes are measured as
λ235 = (1.015 Gyr)−1 ,
λ238 = (6.45 Gyr)−1 .
The idea is now that ores on Earth or meteorites formed during a period very short compared to the age of the Earth te, so
that their abundances can be assumed to have been locked up
instantaneously and simultaneously a time te ago. Chemical
fractionation has given diﬀerent abundances to diﬀerent samples, but could not distinguish between diﬀerent isotopes of
the same element. Thus, we expect diﬀerent samples to show
diﬀerent isotope abundances, but identical abundance ratios
between diﬀerent isotopes.
The unstable lead isotope 204Pb has no long-lived parents
and is therefore a measure for the primordial lead abundance.
Thus, the abundance ratios between 206Pb and 207Pb to 204Pb
calibrate the abundances in diﬀerent samples. Suppose we
have two independent samples a and b, in which the abundance ratios
R206 ≡N206
R207 ≡N207
are measured. They are given by
Ri = Ri,0 + N j
with (i, j) = (206, 238) or (207, 235). The present lead abundance ratios R206,0 and R207,0 should be the same in the two
samples and cancel when the diﬀerence between the ratios in
the two samples is taken. Then, the ratio of diﬀerences can be
written as
eλ235te −1
eλ238te −1 .
Once the lead abundance ratios have been measured in the two
samples, and the present uranium isotope ratio
is known, the age of the Earth te is the only unknown in (33).
This method yields 
te = (4.6 ± 0.1) Gyr .
2. The age of the Galaxy
A variant of this method can be used to estimate the age
of the Galaxy, but this requires a model for how the radioactive elements were formed during the lifetime of the Galaxy
until they were locked up in samples where we can measure
their abundances today. Again, we can assume that the Galaxy
formed quickly compared to its age tg.
Suppose there was an instantaneous burst of star formation
and subsequent supernova explosions a time tg ago and no
further production thereafter. Then, the radioactive elements
found on Earth today decayed for the time tg −te until they
were locked up when the Solar System formed. If we can
infer from supernova theory the initially produced abundance
ratio N235/N238, we can conclude from its present value (34)
and the age of the Earth what the age of the Galaxy must be.
The situation is slightly more complicated because element
production did not stop after the initial burst. Suppose that a
fraction f of the total number Np of the heavy nuclei locked
up in the Solar System was produced in a burst at t = 0, and
the remaining fraction 1 −f was added at a steady rate until
t = tg −te when the Earth was formed. Given the constant
production rate p, the abundance of a radioactive element with
decay constant λ is
N = Ce−λt + p
before tg −te, with a constant C to be suitably chosen, and
N = N0e−λ[t−(tg−te)]
thereafter, where N0 is the abundance of the element locked
up in the Solar System, as before. The initial conditions then
N(0) = C + p
λ = f Np ,
after the burst at t = 0, thus
N(tg −te) = e−λ(tg−te) 
eλ(tg−te) −1
when the Earth formed, and
N(tg) = e−λtg
eλ(tg−te) −1
today, when the galaxy reaches its age tg. Since the production
rate must be
p = (1 −f)Np
the present abundance is
N = Npe−λtg
f + (1 −f)
eλ(tg−te) −1
in terms of the produced abundance Np. Supernova theory
says that the produced abundance ratio of the isotopes 235U
and 238U is 
= 1.4 ± 0.2 .
Taking the ratio of (42) for the present abundances of 235U and
238U, inserting the decay constants from (30), the abundance
ratios from (34) and (43), and the age of the Earth te from (35)
yields an equation which contains only the age of the Galaxy
tg in terms of the assumed fraction f. This gives

(6.3 ± 0.2) Gyr
(8.0 ± 0.6) Gyr
(12 ± 2) Gyr
More detailed models yield comparable results.
Common assumptions and results from galaxy-formation theory assert that at least 1 Gyr is necessary before galactic
disks could have been assembled.
Therefore, nuclear cosmochronology constrains the age of the Galaxy to fall approximately within
7 Gyr ≲tg ≲13 Gyr .
Interestingly, nuclear cosmochronology has also been applied to metal-poor and therefore presumably very old stars,
in which heavy elements are nonetheless overabundant. For
example, constrain the age of such a star
to (12.5 ± 3) Gyr.
B. Stellar ages
Another method for measuring the age of the Universe
caused much trouble for cosmologists for a long time.
is based on stellar evolution and exploits the fact that the
time spent by stars on the main sequence of the Hertzsprung-
Russell or colour-magnitude diagram (see Fig. 5) depends
sensitively on their mass and thus on their colour.
Stars are described by the stellar-structure equations, which
relate the mass M, the density ρ and the pressure p to the
radius r and specify the temperature T and the luminosity L.
dr = 4πr2ρ ,
which simply state hydrostatic equilibrium and mass conservation, and
4πr2aSBcT 3 ,
dr = 4πr2ρϵ ,
which describe radiative energy transport and energy production.
κ is the opacity of the stellar material, ϵ is the energy production rate per unit mass, and aSB is the Stefan-
Boltzmann constant. Assuming κ is independent of temperature, the energy-transport equation, mass conservation, hydrostatic equilibrium and the equation-of-state for an ideal gas
yield the scaling relations
The second pair of equations gives T ∼M/R, which yields
L ∼M3 when inserted into the ﬁrst pair.
The total lifetime τ of a star on the main sequence must
scale as Lτ ∼M because the total energy radiated, Lτ, must
be a fraction of the total rest-mass energy. Together with the
earlier result, we ﬁnd
L ∼M−2 ∼L−2/3 .
According to the Stefan-Boltzmann law, the star’s luminosity
but we also know from above that T ∼M/R. Thus
and the lifetime τ on the main sequence turns out to scale as
τ ∼T −1, by Eq. (49). Afterwards, stars move away from the
main sequence towards the giant branch. Thus, as a coeval
stellar population ages, the point in its Hertzsprung-Russell
diagram up to which the main sequence remains populated
moves towards lower luminosities and temperatures along
(L, T) ∼(τ−3/2, τ−1). Old, coeval stellar populations exist:
they are the globular clusters which surround the centre of
the Galaxy in an approximately spherical halo. Therefore, the
main-sequence turn-oﬀpoints can be used to derive lower limits to the age of the Galaxy and the Universe.
In practice, such age determinations proceed by adapting
simulated stellar-evolution tracks to the Hertzsprung-Russell
diagrams of globular clusters and assigning the age of the best-
ﬁtting stellar-evolution model to the cluster .
The simulated stellar-evolution tracks depend on the assumed
FIG. 5 Hertzsprung-Russell or colour-magnitude diagram of the
globular cluster M 68. The magnitude is a logarithmic measure of the
luminosity, colours are deﬁned as magnitude diﬀerences. Stars populate the main sequence while burning hydrogen, then turn away from
it towards the giant branch. The main-sequence turn-oﬀis clearly
visible (courtesy of Achim Weiß).
metalicity of the stellar material, which changes the opacity
and thus the energy transport through the stars.
Since observations cannot tell the luminosity of the turnoﬀpoint on the main sequence, but only its apparent brightness, age determinations from globular clusters require observations measuring both the apparent brightness of the main sequence turn-oﬀ, and the distance to the globular cluster. There
are several ways for estimating cluster distances. One uses
the period-luminosity relation of a class of variable stars, the
RR Lyrae stars, which are similar to the Cepheids that will
play a central role in the next section. Another method exploits that stars on the horizontal branch have a typical luminosity and can thus be used to calibrate the cluster distance.
Therefore, uncertainties in the distance determinations directly translate to uncertainties in the age determinations. If
the distance is overestimated, so is the luminosity, which implies that the age is underestimated, and vice versa. Globular
clusters typically appeared to have ages well above estimates
based on the cosmological parameters assumed . In the past decade or so,
this has changed because improvements in stellar-evolution
theory and direct distance measurements with the Hipparcos
satellite have lowered the globular-cluster ages, while recently
determined cosmological parameters yield a higher age for the
Universe as assumed before . Now, globular-cluster ages imply
t0 ≳(12.5 ± 1.3) Gyr
for the age of the Universe, allowing for a Gyr between the Big
Bang and the formation of the globular clusters . The uncertainty in this result may still be considerable,
though .
C. Cooling of white dwarfs
A third method for cosmic age determinations is based on
the cooling of white dwarfs . They are the end products in the evolution of lowmass stars and form when the nuclear fuel in the stellar cores
is exhausted. This happens typically when a core of carbon
and oxygen has formed and temperature necessary for further
fusion cannot be reached. The C-O core then shrinks until it is
stabilised by electron degeneracy pressure. At this point, the
mean free path of the electrons becomes eﬀectively inﬁnite
and the core turns isothermal.
The pressure of a degenerate electron gas is independent
of temperature and depends only on the density. In the nonrelativistic limit, p ∝ρ5/3 or
where the latter two equations express hydrostatic equilibrium. Together, these relations imply R ∝M−1/3, showing
that more massive white dwarfs have smaller radii. Their surface gravity,
R2 ∝M5/3 ,
which can be determined from their spectra, is thus a direct
measure for their mass. It turns out that the vast majority of
white dwarfs are born with similar masses, Mwd ≈(0.55 ±
0.05) M⊙ , which is a conspiracy of stellar
evolution: more massive stars lose more mass through stellar
winds before they turn into white dwarfs.
Except for the latent heat of crystallisation, white dwarfs
cool passively by radiating the thermal energy stored by their
mass. Being born with essentially the same mass, they start
with very similar amounts of thermal energy.
Their temperatures and luminosities are determined by the radiation
processes and the opacities of their atmospheres, which in
turn depend on their metalicities. Age inferences from white
dwarfs thus require that their metalicities be known and the
energy transport through their atmospheres be modelled.
After an initial short phase of rapid neutrino cooling, the
energy loss slows down. This change in cooling time causes
the white dwarfs to pile up at the temperature and the luminosity where the energy loss due to neutrinos falls below that due
to radiative cooling. Thus, as time proceeds, their luminosity distribution develops a peak which is later slowly depopulated. Models for the white-dwarf cooling sequence allow the
construction of time-dependent, theoretical luminosity distributions from which, by comparison with the observed luminosity distribution, the age of a white-dwarf population can be
determined. This methods yields
twd ≈(9.5 ± 1) Gyr
for the age of white dwarfs in the Galactic disk . If we assume
that massive spiral disks form at redshifts below z ≲3, the
implied age of the Universe is approximately
t0 ≈(11 ± 1.4) Gyr .
The age of the white-dwarf population in the globular cluster
M 4 has been measured to be ∼12.1 Gyr .
D. Summary
Combining results, we see that the age of the Universe, as
measured by its supposedly oldest parts, is at least ≳11 Gyr,
and this places serious cosmological constraints. In the framework of the Friedmann-Lemaˆıtre models, they can be interpreted as limits on the cosmological parameters.
we lived in an Einstein-de Sitter universe with Ωm0 = 1 and
ΩΛ0 = 0. Then, we know from (16) that
H0 ≲2 × 10−18 s ,
or H0 ≲61 km s−1 Mpc−1 in conventional units. The Hubble
constant is measured to be larger than this, which can immediately be interpreted as an indication that we are not living in
an Einstein-de Sitter universe.
III. THE HUBBLE CONSTANT
A. Hubble constant from Hubble’s law
1. Hubble’s law
Slipher discovered preceding 1920 that distant galaxies typically move away from us . Hubble found that
their recession velocity grows with distance,
v = H0 D ,
and determined the constant of proportionality as H0
559 km s−1 Mpc−1 . We had seen
in (21) that all distance measures in a Friedmann-Lemaˆıtre
universe follow the linear relation
to ﬁrst order in z ≪1. Since cz = v is the velocity according
to the linearised relation between redshift and velocity, (59) is
exactly the relation Hubble found.
The problem with any measurement of the Hubble constant
from (58) is that, while (59) holds for an idealised, homogeneous and isotropic universe, real galaxies have peculiar motions on top of their Hubble velocity which are caused by the
attraction from local density inhomogeneities. For instance,
galaxies in our neighbourhood feel the gravitational pull of a
cosmologically nearby supercluster called the Great Attractor
and accelerate towards it. The galaxy M 31 in Andromeda is
moving within the Local Group of galaxies and approaches
the Milky Way.
Thus, the peculiar velocities of the galaxies must either be
known well enough, for which a model for the velocity ﬁeld is
necessary, or they must be observed at so large distances that
any peculiar motion is unimportant compared to their Hubble velocity. Requiring that the typical peculiar velocities, of
order (300 . . . 600) km s−1, be less than 10% of the Hubble velocity, galaxies with redshifts
z ≳10 × (300 . . . 600) km s−1
≈0.01 . . . 0.02
need to be observed. This is already so distant that it is hard or
impossible to apply simple geometric distant estimators. This
illustrates why accurate measurements of the Hubble constant
are so diﬃcult: nearby galaxies, whose distances are more accurately measurable, do not follow the Hubble expansion well,
but the distances to galaxies far enough to follow the Hubble
law are very hard to measure for an interesting account of the diﬃculties).
2. The distance ladder
Measurements of the Hubble constant from Hubble’s law
thus require accurate distance measurements out to cosmologically relevant distance scales. Since this is impossible in one
step, the so-called distance ladder must be applied, in which
each step calibrates the next.
The most fundamental distance measurement in the astrophysical toolbox is the trigonometric parallax caused by the
annual motion of the Earth around the Sun. By deﬁnition, a
star at a distance of a parsec perpendicular to the Earth’s orbital plane has a parallax of one arc second. Astrometric measurement accuracies of order 10−5 ′′ are thus necessary to measure distances of order 10 kpc. Although the astrometric satellite Hipparcos could measure distances only to ≤1 kpc, its
results were used to calibrate the distance ladder with nearby
stars. In this way, it was possible to determine the distance
to the Large Magellanic Cloud as DLMC = (54.0 ± 2.5) kpc
 . An independent distance estimate based on the elliptical ring that appeared around the supernova 1987A gave DLMC = (46.77±0.04) kpc .
The next important step in the distance ladder is formed
by the Cepheids. These are stars in late evolutionary stages
which undergo periodic variability. The underlying instability
is driven by the temperature dependence of the atmospheric
opacity in these stars, which is caused by the transition between singly and doubly ionised Helium. The cosmologically
important aspect of the Cepheids is that their variability period
τ and their luminosity L are related,
in the V band , hence their luminosity can
be inferred from their variability period, and their distance
from the ratio of their luminosity to the ﬂux S observed from
At the relevant distances, any distinction between diﬀerently
deﬁned distance measures is irrelevant.
It is crucially important here that the calibration of the
period-luminosity relation depends on the metalicity of the
pulsating variables used, and thus on the stellar population
they belong to. Hubble’s originally much too high result for
H0 was corrected when Baade realised that he had confused a
metal-poor class of variable stars with the metal-rich Cepheids
 .
Measuring the periods of Cepheids and comparing their apparent brightnesses in diﬀerent galaxies thus allows the determination of the relative distances to the galaxies. For example, comparisons between Cepheids in the LMC and the
Andromeda galaxy M 31 show 
= 16.4 ± 1.1 ,
while Cepheids in the member galaxies of the Virgo cluster
yield 
= 305 ± 16 .
Of course, for the Cepheid method to be applicable, it must
be possible to resolve at least the outer parts of distant galaxies
into individual stars and to reliably identify Cepheids among
them. This was one reason why the Hubble Space Telescope
was proposed , to apply the superb
resolution of an orbiting telescope to the measurement of H0.
Cepheid distance measurements are possible to distances ≲
Scaling relations within classes of galaxies provide additional distance indicators. In the three-dimensional parameter
space spanned by the velocity dispersion σv of their stars, the
radius Re enclosing half the luminosity, and the surface brightness Ie within Re, elliptical galaxies populate the tight fundamental plane deﬁned by
Since the luminosity must be
the fundamental-plane relation implies
Such a relation follows directly from the virial theorem if the
mass-to-light ratio in elliptical galaxies increases gently with
Thus, if it is possible to measure the surface brightness Ie
and the velocity dispersion σv of an elliptical galaxy, the
fundamental-plane relation gives the luminosity, which can be
compared to the ﬂux to ﬁnd the distance.
An interesting way for determining distances to galaxies
uses the ﬂuctuations in their surface brightness . The idea behind
this method is that the ﬂuctuations in the surface brightness
will be dominated by the rare brightest stars, and that the optical luminosity of the entire galaxy will be proportional to the
number N of such stars. Assuming Poisson statistics, the ﬂuctuation level will be proportional to
N, from which N and
L ∝N can be determined once the method has been calibrated
with galaxies whose distance is known otherwise. Again, the
distance is then found by comparing the ﬂux to the luminosity.
Planetary nebulae, which are late stages in the evolution
of massive stars, have a luminosity function with a steep upper cut-oﬀ . Moreover, their
spectra are dominated by sharp nebular emission lines which
facilitate their detection even at large distances because they
appear as bright objects in narrow-band ﬁlters tuned to the
emission lines. Since the cut-oﬀluminosity is known, it can
be converted to a distance.
An important class of distance indicators are supernovae of
type Ia. As will be described in § IX.B, their luminosities can
be standardised, allowing their distances to be inferred from
their ﬂuxes.
Although they are not standard (or standardisable) candles,
core-collapse supernovae of type II can also be used as distance indicators through the Baade-Wesselink method .
Suppose the spectrum of the supernova photosphere can be approximated by a Planck curve
whose temperature can be determined from the spectral lines.
Then, the Stefan-Boltzmann law ﬁxes the total luminosity,
L = σSBR2T 4 .
The photospheric radius, however, can be inferred from the
expansion velocity of the photosphere, which is measurable
by the Doppler shift in the emission lines, times the time after the explosion. When applied to the supernova SN 1987A
in the Large Magellanic Cloud, the Baade-Wesselink method
yields a distance of
DLMC = (49 ± 3) kpc
based on infrared photometry which
agrees with the direct distance measurements .
Note that the Bade-Wesselink method gives a one-step geometrical distance indicator bypassing the distance ladder.
3. The HST Key Project
All these distance indicators were used in the HST Key
Project to determine accurate distances to 26 galaxies between (3 . . . 25) Mpc, and ﬁve very
nearby galaxies for testing and calibration.
Double-blind photometry was applied to the identiﬁed distance indicators. Since Cepheids tend to lie in star-forming
regions and are thus attenuated by dust, and since their periodluminosity relation depends on metalicity, respective corrections had to be carefully applied. Then, the measured velocities had to be corrected by the peculiar velocities, which were
estimated by a model for the ﬂow ﬁeld. The estimated luminosities of the distance indicators could then be compared
with the extinction-corrected ﬂuxes to determine distances,
whose proportionality with the velocities corrected by the peculiar motions ﬁnally gave the Hubble constant. A weighed
average over all distance indicators is 
H0 = (72 ± 8) km s−1 Mpc−1 ,
where the error is the square root of the systematic and statistical errors summed in quadrature. An alternative interpretation
of the data is summarised in , who ﬁnd
a 14 % smaller value for H0 because of a diﬀerent calibration.
B. Gravitational Lensing
A completely diﬀerent method for determining the Hubble
constant uses gravitational lensing. Masses bend passing light
paths towards themselves and therefore act similarly to convex
glass lenses. As in ordinary geometrical optics, this eﬀect can
be described applying Fermat’s principle to a medium with an
index of refraction 
where Φ is the Newtonian gravitational potential.
If it is strong enough, the bending of the light paths causes
multiple images to appear from single sources. Compared to
the straight light paths in absence of the deﬂecting mass distribution, the curved paths are geometrically longer, and they
have to additionally propagate through a medium whose index of refraction is n > 1. This gives rise to a time delay
 which has a geometrical and
a gravitational component,
τ = 1 + zd
where ⃗θ are angular coordinates on the sky and ⃗β is the angular position of the source. ψ is the appropriately scaled Newtonian gravitational potential of the deﬂector, projected along
the line-of-sight. According to Fermat’s principle, images occur where τ is extremal, i.e. ⃗∇θτ = 0. The prefactor contains
the distances Dd,s,ds from the observer to the deﬂector, to the
source, and from the deﬂector to the source, respectively, and
the redshift zd of the lens.
The projected lensing potential ψ is related to the surfacemass density Σ of the deﬂector by
⃗∇2ψ = 2 Σ
where the critical surface-mass density is
The dimension-less time delay τ from (73) is related to the
true physical time delay t by
where the proportionality constant is a dimension-less combination of the distances Dd,s,ds with the Hubble radius cH−1
and the deﬂector redshift 1 + zd. Equation (76) shows that the
true time delay is proportional to the Hubble time, as it can
intuitively be expected .
Time delays are measurable in multiple images of a variable source. The variable signal arrives after diﬀerent times in
the images seen by the observer. If the deﬂector is a galaxy,
time delays are typically of order days to months and therefore
observable with a reasonable monitoring eﬀort.
Interestingly, it can be shown in an elegant, but lengthy
calculation that measured time delays can
be inverted to ﬁnd the Hubble constant from the approximate
H0 ≈A(1 −⟨κ⟩) + B⟨κ⟩(η −1) ,
where A and B are constants depending on the measured image positions and time delays, ⟨κ⟩is the mean scaled surfacemass density of the deﬂector averaged within an annulus
bounded by the image positions, and η ≈2 is the logarithmic slope of the deﬂector’s density proﬁle.
Therefore, if a model exists for the gravitationally-lensing
galaxy, the Hubble constant can be found from the positions
and time delays of the images. Applying this technique to ﬁve
diﬀerent lens systems, found
H0 = (73 ± 8) km s−1
assuming that the lensing galaxies have radially constant
mass-to-light ratios.
This result is remarkable because it was obtained in one
step without any reference to the extragalactic distance ladder. On the other hand, it is also problematic because it is obtained only if signiﬁcantly more concentrated density proﬁles
of the lensing galaxies are assumed than obtained e.g. from
the kinematics of the stars in galaxies. If less concentrated
lenses are adopted which agree with the kinematic constraints,
H0 derived from gravitational time delays drops substantially
to values near 50 km s−1 . This hints at an
as yet unexplained discrepancy between measurements of H0
and the measured time delays within the CDM framework.
C. Summary
The results given so far are in excellent agreement with the
H0 = (70.1 ± 1.3) km s−1 Mpc−1 ,
derived from CMB measurements (see § VI and Tab. I) assuming spatial ﬂatness, K = 0. adopting it, we can calibrate
several important numbers that scale with some power of the
Hubble constant. First, in cgs units, the Hubble constant is
H0 = (2.26 ± 0.04) × 10−18 s ,
which implies the Hubble time
= (14.01 ± 0.26) Gyr
and the Hubble radius
= (1.327 ± 0.025) × 1028 cm = (4.299 ± 0.08) Gpc . (82)
The critical density of the Universe turns out to be
ρcr0 = 3H2
8πG = (9.15 ± 0.3) × 10−30 g cm−3 .
It corresponds to ﬁve protons per m3, or approximately one
galaxy per Mpc3.
IV. BIG-BANG NUCLEOSYNTHESIS
A. The origin and abundance of Helium-4
1. Elementary considerations
Stellar spectra show that the abundance of 4He in stellar
atmospheres ranges between 0.2 ≲Y ≲0.3 by mass (the Sun
has Y = 0.263), i.e. about a quarter of the baryonic mass in
the Universe is composed of 4He. It is produced in stars in the
course of hydrogen burning. Per 4He nucleus, the amount of
energy released corresponds to 0.7% of the masses involved,
∆E = ∆mc2 = 0.007 (2mp + 2mn)c2 ≈0.028 mpc2
≈26 MeV ≈4.2 × 10−5erg .
Suppose a galaxy such as ours, the Milky Way, shines with
a luminosity of L ≈1010 L⊙≈3.8 × 1043 erg s−1 for a good
fraction of the age of the Universe, say for τ = 5 × 109 yr ≈
1.5 × 1017 s. Then, it releases a total energy of
Etot ≈Lτ ≈5.7 × 1060 erg .
The number of 4He nuclei required to produce this energy is
∆E ≈5.7 × 1060
4.2 × 10−5 ≈1.4 × 1065 ,
which amounts to a 4He mass of
MHe ≈4mp∆N ≈9.3 × 1041 g .
Assume further that the galaxy’s stars were all composed of
pure hydrogen initially, and that they are all more or less
similar to the Sun. Then, the mass in hydrogen was MH ≈
1010 M⊙≈2 × 1043 g initially, and the ﬁnal 4He abundance by
mass expected from the energy production is
Y∗≈9.3 × 1041
much less than the 4He abundance actually observed. This
discrepancy is exacerbated by the fact that 4He is destroyed
in later stages of the evolution of massive stars, a process affected also by mixing in stellar interiors.
We thus see that the amount of 4He observed in stars was
highly unlikely produced by these stars themselves under reasonable assumptions during the lifetime of the galaxies. We
must therefore consider that most of the 4He which is now
observed may have existed already well before the galaxies
Nuclear fusion of 4He and similar light nuclei in the early
Universe is possible only if the Universe was hot enough for a
suﬃciently long period during its early evolution. The nuclear
binding energies of order ∼MeV imply that temperatures had
to have fallen below T ∼106 MeV × 1.16 × 104 K/MeV =
1.16 × 1010 K before nucleosynthesis could begin.
other hand, temperatures needed to be still high enough for
charged nuclei to surmount the Coulomb barrier.
temperatures during primordial nucleosynthesis were of order
kT ≲100 keV. Since the temperature of the (photon background in the) Universe is now T0 ∼3 K as we shall see later,
this corresponds to times when the scale factor of the Universe
1.16 × 109 ≈2.59 × 10−9 .
At times so early, the actual mass density and a possible
cosmological constant are entirely irrelevant for the expansion
of the Universe, which is only driven by the radiation density.
Thus, the expansion function can be simpliﬁed to read E(a) =
r0 a−2, and we ﬁnd for the cosmic time according to (15)
≈2.40 × 1019a2 s ,
where we have inserted the Hubble constant from (79) and the
radiation-density parameter today, Ωr0 ≈8.51 × 10−5, which
will be justiﬁed later [see (157)]. It contains the contributions
from photons and three neutrino species, which are the particles relativistic at the relevant time.
Inserting anuc from (89) into (90) yields a time scale for nucleosynthesis of order a few minutes. It is instructive for later
purposes to establish a relation between time and temperature
based on (90). Since T = T0/a,
t = 2.40 × 1019 T0
2. The Gamow criterion
A crucially important step in the fusion of 4He is the fusion
of deuterium d,
p + n →d + γ
because the direct fusion of 4He from two neutrons and two
protons is extremely unlikely. The conditions in the early universe must have been such that deuterium could be formed
eﬃciently enough for the subsequent fusion of 4He, but not
too eﬃciently because otherwise too much deuterium would
be left over after the end of nucleosynthesis. Realising this,
 suggested that the amount of deuterium produced had to be “just right”, which he translated into the intuitive criterion
nB⟨σv⟩t ≈1 ,
where nB is the baryon number density, ⟨σv⟩is the velocityaveraged cross section for the reaction (92), and t is the time
available for the fusion, which we have seen in (91) to be
set by the present temperature of the cosmic radiation background, T0, and the temperature T required for deuterium fusion.
Thus, from an estimate of the baryon density nB in the Universe, from the known velocity-averaged cross section ⟨σv⟩,
and from the known temperature required for deuterium fusion, Gamow’s criterion enables an estimate of the present
temperature T0 of the cosmic radiation background. Based
on this, were able to predict
T0 ≈(1 . . . 5) K!
After these remarkably simple and farreaching conclusions, we shall now study primordial nucleosynthesis and consequences thereof in more detail.
3. Elements produced
The fusion of deuterium (92) is the crucial ﬁrst step. Since
the photodissociation cross section of d is large, destruction of
d is very likely because of the intense photon background until
the temperature has dropped way below the binding energy of
d, which is only 2.2 MeV, corresponding to 2.6 × 1010 K. In
fact, substantial d fusion is delayed until the temperature falls
to T = 9×108 K or kT ≈78 keV! As (91) shows, this happens
t ≈150 s after the Big Bang.
From deuterium, 3He and tritium t can be built, which can
both be processed to 4He. These reactions are now fast, immediately converting the newly formed d. In detail, these reactions are
d + d →t + p ,
3He + n →t + p ,
followed by
Fusion reactions with neutrons are irrelevant because free
neutrons are immediately locked up in deuterons once deuterium fusion begins, and passed on to t, 3He and 4He in the
further fusion steps.
Since there are no stable elements with atomic weight A =
5, addition of protons to 4He is unimportant. Fusion with d
is unimportant because its abundance is very low due to the
eﬃcient follow-up reactions. We can therefore proceed only
by fusing 4He with t and 3He to build up elements with A = 7,
3He + 4He →
followed by
7Li + νe .
Some 7Li is destroyed by
7Li + p →2 4He .
The fusion of two 4He nuclei leads to 8Be, which is unstable.
Further fusion of 8Be in the reaction
8Be + 4He →12C + γ
is virtually impossible because the low density of the reaction
partners essentially excludes that a 8Be nucleus meets a 4He
nucleus during its lifetime. While the reaction (98) is possible and extremely important in stars, it is suppressed below
any importance in the early Universe. The absence of stable
elements with A = 8 thus prohibits any primordial element
fusion beyond 7Li.
4. Helium abundance
Once stable hadrons can form from the quark-gluon plasma
in the very early universe, neutrons and protons are kept in
thermal equilibrium by the weak interactions
p + e−↔n + νe ,
n + e+ ↔p + ¯νe
until the interaction rate falls below the expansion rate of the
Universe. While equilibrium is maintained, the abundances
nn and np are controlled by the Boltzmann factor
where Q = 1.3 MeV is the energy equivalent of the mass difference between the neutron and the proton.
The weak interaction freezes out when T ≈1010 K or kT ≈
0.87 MeV, which is reached t ≈2 s after the Big Bang. At this
time, the n abundance by mass is
nnmn + npmp
Afterwards, the free neutrons undergo β decay with a half life1
of τn = (885.7 ± 0.8) s, thus
Xn = Xn(0)e−t/τn .
When d fusion ﬁnally sets in at td ≈150 s after the Big Bang,
the neutron abundance has dropped to
Xn(td) ≈Xn(0)e−td/τn ≈0.14 .
1 Particle Data Group, 
Now, essentially all these neutrons are collected into 4He because the abundances of the other elements can be neglected
to ﬁrst order. This simple estimate yields a 4He abundance by
Y ≈2Xn(td) ≈0.28
because the neutrons are locked up in pairs to form 4He nuclei. The Big-Bang model thus allows the prediction that 4He
must have been produced such that its abundance is approximately 28% by mass, which is in remarkable agreement with
the observed abundance and thus a strong conﬁrmation of the
Big-Bang model.
5. Expected abundances and abundance trends
Precise abundances of the light elements as produced by the
primordial fusion must be calculated solving rate equations
based on the respective fusion cross sections. Uncertainties
involved concern the exact values of the cross sections and
their energy dependence, and the precise life time of the free
neutrons. Since primordial nucleosynthesis happens during
the radiation-dominated era, the expansion rate is exclusively
set by the radiation density. Then, the only other parameter
controlling the primordial fusion processes is the baryon density, if the neutron lifetime is taken for granted.
An excellent recent review of these matters is , to which we refer for further detail.
Ignoring possible modiﬁcations by non-standard temperature evolution, the only relevant parameter deﬁning the primordial abundances is the ratio between the number densities
of baryons and photons. Since both densities scale like a−3 or,
equivalently, like T 3, their ratio η is constant. Anticipating the
photon number density to be determined from the temperature
of the CMB,
= 10−10η10 ,
η10 ≡273ΩBh2 .
Thus, once we know the photon number density, and once
we can determine the parameter η from the primodial element
abundances, we can infer the baryon number density. Typical 2-σ uncertainties at a ﬁducial η parameter of η10 = 5 are
0.4% for 4He, 15% for d and 3He, and 42% for 7Li. The 4He
abundance depends only very weakly on η because the largest
fraction of free neutrons is swept up into 4He without strong
sensitivity to the detailed conditions.
The principal eﬀects determining the abundances of d, 3He
and 7Li are the following: with increasing η, they can more
easily be burned to 4He, and so their abundances drop as η increases. At low η, an increase in the proton density causes 7Li
destruction in the reaction (97), while the precursor nucleus
7Be is more easily produced if the baryon density increases
further. This creates a characteristic “valley” of the predicted
7Li abundance near η ≈(2 . . . 3) × 10−10.
B. Observed element abundances
1. Principles
Of course, the main problem with any comparison between
light-element abundances predicted by primordial nucleosynthesis and their observed values is that much time has passed
since the primordial fusion ceased, and further fusion processes have happened since. Seeking to determine the primordial abundances, observers must therefore either select objects
in which little or no contamination by later nucleosynthesis
can reasonably be expected, in which the primordial element
abundance may have been locked up and separated from the
surroundings, or whose observed element abundances can be
corrected for their enrichment during cosmic history in some
Deuterium can be observed in cool, neutral hydrogen gas
(HI regions) via resonant UV absorption from the ground
state, or in radio wavebands via the hyperﬁne spin-ﬂip transition, or in the sub-millimetre regime via dH molecule lines.
These methods all employ the fact that the heavier d nucleus
causes small changes in the energy levels of electrons bound
3He is observed through the hyperﬁne transition in its
ion 3He+ in radio wavebands, or through its emission and absorption lines in HII regions. 4He is of course most abundant
in stars, but the fusion of 4He in stars is virtually impossible
to be corrected precisely. Rather, 4He is probed via the emission from optical recombination lines in HII regions . Measurements of 7Li must be performed
in old, local stellar populations. This restricts observations
to cool, low-mass stars because of their long lifetime, and to
stars in the Galactic halo to allow precise spectra to be taken
despite the low 7Li abundance.
2. Evolutionary corrections
Stars brooded heavy elements as early as z ∼6 or even
higher . Any attempts at measuring
primordial element abundances must therefore concentrate on
gas with a metal abundance as low as possible. The dependence of the element abundances on metalicity allows extrapolations to zero enrichment.
Such evolutionary corrections are low for d because it is observed in the Lyman-α forest lines, which arise from absorption in low-density, cool gas clouds at high redshift. Likewise,
they are low for the measurements of 4He because it is observed in low-metalicity, extragalactic HII regions. Probably,
little or no correction is required for the 7Li abundances determined from the spectra of very metal-poor halo stars , but a lively discussion is going on
 .
Inferences from 3He are diﬀerent because 3He is produced
from d in stars during the pre-main sequence evolution. It
is burnt to 4He during the later phases of stellar evolution in
stellar cores, but conserved in stellar exteriors. Observations
indicate that a net destruction of 3He must happen, possibly
due to extra mixing in stellar interiors. For these uncertainties, 3He is commonly excluded from primordial abundance
measurements.
3. Speciﬁc results
Due to the absence of strong evolutionary eﬀects and its
steep monotonic abundance decrease with increasing η, d is
perhaps the most trustworthy baryometer. Since it is produced
in the early Universe and destroyed by later fusion in stars, all
d abundance determinations are lower bounds to its primordial
abundance. Measurements of the d abundance at high redshift
are possible through absorption lines in QSO spectra, which
are likely to probe gas with primordial element composition
or close to it. Such measurements are challenging in detail
because the tiny isotope shift in the d lines needs to be distinguished from velocity-shifted hydrogen lines, H abundances
from saturated H lines need to be corrected by comparison
with higher-order lines, and high-resolution spectroscopy is
required for accurate continuum subtraction.
A deuterium abundance of
in high-redshift QSOs relative to hydrogen appears consistent
with most relevant QSO spectra, although substantially lower
values have been derived . A
substantial depletion from the primordial value is unlikely because any depletion should be caused by d fusion and thus
be accompanied by an increase in metal abundances, which
should be measurable.
Some spectra which were interpreted as having ≲10 times
the d abundance from (107) may be due to lack of spectral
resolution. The d abundance in the local interstellar medium
is typically lower,
∼1.5 × 10−5 ,
which is consistent with d consumption due to fusion processes. Conversely, the d abundance in the Solar System is
higher because d is locked up in the ice on the giant planets.
In low-metalicity systems, 4He should be near its primordial abundance, and a metalicity correction can be applied.
Possible systematic uncertainties are due to modiﬁcations by
underlying stellar absorption, collisional excitation of observed recombination lines, and the exact regression towards
zero metalicity.
Values between Yp = 0.240 ± 0.006 and
Yp = 0.2477 ± 0.0029 are considered realistic and likely .
Observations of the 7Li abundance aim at the oldest stars
in the Galaxy, which are halo (Pop-II) stars with very low
metalicity. They should have locked up very nearly primordial gas, but may have processed it . Cool stellar atmospheres are diﬃcult to model, and 7Li may have been produced by cosmic-ray
spallation on the interstellar medium ), which is asymptotically
independent of metalicity,
A(7Li) = 12 + log(nLi/nH) = 2.2 ± 0.1 ,
and shows very little dispersion. Stellar rotation is important
here because it enhances mixing in stellar interiors .
The Spite plateau is unlikely to reﬂect the primordial 7Li
abundance, but corrections are probably moderate. A possible
increase of 7Li with the iron abundance indicates low production of 7Li, but the probable net eﬀect is a depletion with respect to the primordial abundance by no more than ∼0.2 dex.
A conservative estimate yields
2.1 ≤A(7Li) ≤2.5 .
In absence of depletion, this value falls into the valley expected in the primordial 7Li at the boundary between destruction by protons and production from 8Be. However, if 7Li was
in fact depleted, its primordial abundance was higher than the
value (109), and then two values for η10 are possible.
4. Summary of results
Through the relation (105), the density of visible baryons
alone implies η10 ≥1.5. The deuterium abundance derived
from absorption systems in the spectra of high-redshift QSOs
indicates η10 = 5.65 . . . 6.38. The 7Li abundance predicted
from this value of η is then A(7Li)p = 3.81 . . . 4.86, which is
at least in mild conﬂict with the observed value A(7Li) = 2.1−
2.3, even if a depletion by 0.2 dex due to stellar destruction is
The predicted primordial abundance of 4He is then Yp =
0.2477 . . . 0.2489, which overlaps with the measured value
YP = 0.228 . . . 0.248.
Thus, the light-element abundances
draw a consistent picture starting from the observed deuterium
abundance.
We thus ﬁnd that Big-Bang nucleosynthesis alone implies
0.0207 ≤ΩBh2 ≤0.0234
0.0399 ≤ΩB ≤0.045
if conclusions are predominantly based on the deuterium
abundance in high-redshift absorption systems. We shall later
see that this result is in fantastic agreement with independent
estimates of the baryon density obtained from the analysis of
structures in the CMB.
At this point, it is important to note that primordial nucleosynthesis depends on the number of relativistic particle
species, and thus on the number of light neutrino families. The
theoretical abundances given here assume three lepton families in the standard model of the electroweak interaction. In
fact, light-element abundances had been used to constrain the
number of lepton families before it was measured in particle
detectors .
abundances
105(3He/H)
108(7Li/H)
FIG. 6 Theoretically expected abundances of the light elements deuterium, 3He, 4He and 7Li as functions of η10 ). The observed deuterium abundance is marked by the two
horizontal solid lines.
V. THE MATTER DENSITY IN THE UNIVERSE
A. Mass in galaxies
Given the luminosity of a stellar population, what is its
mass? If all stars were like the Sun, the answer was trivial,
but this is not the case. We shall focus the discussion here on
stars which fall on the main sequence of the colour-magnitude
diagram. Stars are formed with an initial mass distribution, the
“initial mass function”, which is for our purposes reasonably
approximated by the Salpeter form 
d ln M ∝M−1.35 .
Expressing the mass M in solar units, m ≡M/M⊙, and normalising the mass distribution to unity in the mass range
m0 ≤m < ∞yields
The accepted lower mass limit for a star is m0 = 0.08 because nuclear fusion cannot set in in objects of lower mass.
However, we are interested in stars that are able to produce
visible or near-infrared light so that we can translate the respective measured luminosities into mass. For a simple estimate, assume that stars have approximate Planck spectra, for
which Wien’s law holds, relating the wavelength λmax of the
peak in the Planck curve to the temperature T,
λmax = 0.29
The eﬀective Solar temperature T⊙= 5780 K implies λmax =
5.0 × 10−5 cm. Stars releasing the majority of their energy
in the optical and near-infrared regime should have λmax ≲
1µm = 10−4 cm and thus T ≳2900 K ≈0.5 T⊙. We saw in
(51) that the temperature scales as T ∼M1/2; thus, T ≳0.5 T⊙
implies m0 ≈0.25.
We also saw following (48) that the luminosity scales as
L ∼M3. With l ≡L/L⊙, the mean mass-to-light ratio of the
visible stellar population is thus expected to be
This shows that an average stellar population visible in the optical and near-infrared spectral ranges is expected to require
≈6.4 solar masses for one solar luminosity. In order to produce, say, 1010 L⊙, a galaxy thus needs to have a mass of at
least ≈6.4 × 1010 M⊙. This estimate assumes a homogeneous
stellar population at the beginning of its evolution, neglects
corrections from the exact spectral-energy distribution of the
stars and the emission of giant stars, but ⟨m/l⟩≳6.4 still
seems appropriate for evolved populations.
2. Galaxies
The rotation velocities of stars orbiting in spiral galaxies
are observed to rise quickly with radius and then to remain
roughly constant . If measurements
are continued with neutral hydrogen beyond the radii out to
which stars can be seen, these rotation curves are observed to
continue at an approximately constant level .
In a spherically-symmetric mass distribution, test particles
on circular orbits have orbital velocities of
rot(r) = GM(r)
Flat rotation curves thus imply that M(r) ∝r, requiring the
density to drop like ρ(r) ∝r−2. This is much ﬂatter than the
light distribution, which shows that spiral galaxies are characterised by an increasing amount of dark matter as the radius
increases.
A mass distribution with ρ ∝r−2 has formally inﬁnite mass,
which is physically implausible. However, at ﬁnite radius, the
density of the galaxy falls below the mean density of the surrounding universe. The spherical collapse model often invoked in cosmology shows that a spherical
mass distribution can be considered in dynamical equilibrium
if its mean overdensity is approximately 200 times higher than
the mean density ¯ρ. Let R be the radius enclosing this overdensity, and M the mass enclosed, then
4πR3 = 200¯ρ
R = 800π¯ρR2
At the same time, (115) needs to be satisﬁed, hence
Inserting typical numbers yields
R = 290 kpc
200 km s−1
With (115), this implies
= 2.7 × 1012 M⊙
200 km s−1
Typical luminosities of spiral galaxies are given by the
Tully-Fisher relation ,
220 km s−1
with L∗≈2.4 × 1010 L⊙. Thus, the mass-to-light ratio of a
massive spiral galaxy is found to be
in solar units, where it is assumed that the galaxy extends out
to the virial radius of ≈290 kpc with the same density proﬁle
r−2. Evidently, this exceeds the stellar mass-to-light ratio by
far. Of course, the mass-to-light ratio of galaxies depends on
the limiting radius assumed. Values of m/l ≈30 are often
quoted, which are typically based on the largest radii out to
which rotation curves can be measured.
3. The galaxy population
Galaxy luminosities are observed to be distributed according to the Schechter function ,
where the normalising factor is Φ∗≈3.7 × 10−3 Mpc−3, the
scale luminosity is L∗given under (120) above, and the powerlaw exponent is α ≈1 . Irrespective
of which physical processes this distribution originates from,
its functional form turns out to characterise mixed galaxy populations very well, even in galaxy clusters.
The luminosity density in galaxies is easily found to be
dL dL = Φ∗L∗
l1−αe−l dl
= Γ(2 −α)Φ∗L∗≈Φ∗L∗≈7.4 × 107
Mpc3 . (123)
The average mass-to-light ratio (121) then allows converting
this number into a mass density,
Lg ≈1.1 × 10−4
Mpc3 ≈7.5 × 10−31 g cm−3
and thus, with ρcr0 = 9.15 × 10−30 g cm−3 from Eq. (83),
Ωg0 ≈0.08 .
Of course, estimates based on the more conservative mass-tolight ratio m/l ≈30 ﬁnd values which are lower by a factor
of ∼5. In summary, this shows that the total mass expected
to be contained in the dark-matter halos hosting galaxies contributes of order 8% to the critical density in the Universe.
B. Mass in galaxy clusters
1. Kinematic masses
The next step upward in the cosmic hierarchy are galaxy
clusters, which were ﬁrst identiﬁed as signiﬁcant galaxy overdensities in relatively small areas of the sky (Herschel, 1786,
Although the majority of galaxies is not in galaxy
clusters, rich galaxy clusters contain several hundred galaxies, which by themselves contain a total amount of mass
≈3 × 1014 M⊙.
The mass in stars is of course considerably lower. With the
mean stellar mass-to-light ratio of m/l ≈6.4 from (114), the
same luminosity implies
M∗≲1.3 × 1013 M⊙.
The stellar mass of the Coma cluster, for instance, is inferred
to be M∗,Coma ≈(1.4 ± 0.3) × 1013 M⊙ .
The galaxies in rich galaxy clusters move with typical velocities of order ≲103 km s−1, measured from the redshifts in
their spectra. Therefore, only one component of the galaxy
velocity is observed. Its distribution is characterised by the
velocity dispersion σv.
If these galaxies were not gravitationally bound to the clusters, the clusters would disperse within ≲1 Gyr. Since they
exist over cosmological time scales, clusters need to be (at
least in some sense) gravitationally stable. Assuming virial
equilibrium, the potential energy of the cluster galaxies should
be minus two times the kinetic energy, or
where M and R are the total mass and the virial radius of the
cluster, and the factor three arises because the velocity dispersion represents only one of three velocity components. We
combine (128) with (116) to ﬁnd
1000 km s−1
and, with (128),
M ≈1.7 × 1015 M⊙
1000 km s−1
Hence, the mass required to keep cluster galaxies bound despite their high velocities exceeds the mass in galaxies by
about an order of magnitude, even if the entire virial mass of
the galactic halos is accounted for . The
stellar mass apparently contributes just about one per cent to
the total cluster mass.
2. Mass in the hot intracluster gas
Galaxy clusters are diﬀuse sources of thermal X-ray
Their X-ray continuum is caused by thermal
bremsstrahlung, whose bolometric volume emissivity is
jX = Z2gﬀCX n2 √
in cgs units, where Z is the ion charge, gﬀis the Gaunt factor,
n is the ion number density, T is the gas temperature, and
CX = 2.68 × 10−24
in cgs units, if T is measured in keV.
A common simple, axisymmetric model for the gas-density
distribution in clusters, adapted to X-ray observations, is
(1 + x2)3β/2 ,
where rc and θc are the physical and angular core radii. The
line-of-sight projection of the X-ray emissivity yields the Xray surface brightness as a function of the angular radius θ,
√πΓ(3β −1/2)
Z2gﬀCXrcn2
1 + x23β−1/2
where we have combined (131) and (133) and assumed for
simplicity that the cluster is isothermal, so T does not change
with radius. The latter equation shows that two parameters
of the density proﬁle (133), namely the slope β and the (angular) core radius θc, can be read oﬀthe observable surfacebrightness proﬁle.
The missing normalisation constant can be obtained from
the X-ray luminosity,
jXr2dr = 4πr3
√πΓ(3β −3/2)
and a spectral determination of the temperature T. Finally, the
total mass of the X-ray gas enclosed in spheres of radius R is
MX(R) = 4π
n(r)r2dr ,
which is a complicated function for general β. For β = 2/3,
which is a commonly measured value,
MX(R) = 4πr3
which is of course formally divergent for R →∞because the
density falls oﬀ∝r−2 for β = 2/3 and r →∞.
Inserting typical numbers, we set Z = 1 = gﬀ, β = 2/3 and
assume a hypothetic cluster with LX = 1045 erg s−1, a temperature of kT = 10 keV and a core radius of rc = 250 kpc =
7.75 × 1023 cm. Then, (135) yields the central ion density
n0 = 5 × 10−3 cm−3
and thus the central gas mass density
ρ0 = mpn0 = 8.5 × 10−27 g cm−3 .
Based on the virial radius (129) and on the mass (137), we
ﬁnd the total gas mass
MX ≈1.0 × 1014 M⊙.
This is of the same order as the cluster mass in galaxies, and
approximately one order of magnitude less than the total cluster mass.
It is reasonable to believe that clusters are closed systems
in the sense that there cannot have been much material exchange between their interior and their surroundings . If this is indeed the case, and the mixture between dark matter and baryons in clusters is typical for the
entire universe, the matter-density parameter should be
≈10Ωb0 ≈0.4 ,
given the determination (110) of Ωb0 from primordial nucleosynthesis. More precise estimates based on detailed investigations of individual clusters yield
Ωm0 ≈0.3 .
3. Alternative cluster mass estimates
Cluster masses can be estimated in several other ways. One
of them is directly related to the X-ray emission discussed
above. The hydrostatic Euler equation for a gas sphere in equilibrium with the spherically-symmetric gravitational potential
of a mass M(r) requires
dr = −GM(r)
where ρ and p are the gas density and pressure, respectively.
Assuming an ideal gas, the equation of state is p = nkT, where
n = ρ/(µmp) is the particle density and T the temperature. µ
is the mean particle mass in units of the proton mass mp. A
mixture of hydrogen and helium with helium fraction Y has
µ = 4/(8 −5Y), or µ ≈0.59 for Y = 0.25. If we further
simplify the problem assuming an isothermal gas distribution,
we can write
dr = −GM(r)
or, solving for the mass
M(r) = −rkT
For the β model introduced in (133), the logarithmic density
d ln r = d ln n
d ln r = −3β
thus the cluster mass is determined from the slope of the X-ray
surface brightness and the cluster temperature,
M(r) = 3βrkT
With the typical numbers used before, i.e. R ≈2.5 Mpc, β ≈
2/3 and kT = 10 keV, the X-ray mass estimate gives
M(R) ≈1.8 × 1015 M⊙,
in reassuring agreement with the mass estimate (130) from
galaxy kinematics, and again an order of magnitude more than
either the gas mass or the stellar mass.
A third, completely independent way of measuring cluster
masses is provided by gravitational lensing. Without going
into any detail, we mention here that it can generate image
distortions from which the projected lensing mass distribution
can be reconstructed. Mass estimates obtained in this way
by and large conﬁrm those from X-ray emission and galaxy
kinematics, although interesting discrepancies exist in detail
 .
None of the cluster mass estimates is particularly reliable
because they are all to some degree based on stability and
symmetry assumptions. For mass estimates based on galaxy
kinematics, for instance, assumptions have to be made on
the shape of the galaxy orbits, the symmetry of the gravitational potential and the mechanical equilibrium between orbiting galaxies and the body of the cluster. Numerous assumptions also enter X-ray based mass determinations, such
as hydrostatic equilibrium, spherical symmetry and, in some
cases, isothermality of the intracluster gas. Gravitational lensing does not need any equilibrium assumption, but inferences
from strongly distorted images depend very sensitively on the
assumed symmetry of the mass distribution, and measure only
the mass enclosed by the lensed images.
C. Mass density from cluster evolution
A conceptually very interesting constraint on the cosmic
mass density is based on the evolution of cosmic structures.
Abell’s cluster catalogue covers the redshift range 0.02 ≲z ≲0.2, which encloses a volume of
≈9×108 Mpc3. There are 1894 clusters in that volume, which
yields an estimate for the spatial cluster density of
nc ≈2 × 10−6 Mpc−3 .
It is a central assumption in cosmology that structures
formed from Gaussian random density ﬂuctuations.
spherical collapse model then asserts that gravitationally
bound objects of mass M form where the linear density contrast, smoothed within spheres of radius R enclosing the average mass M, exceeds a critical threshold of δc ≈1.686, quite
independent of cosmology. The probability for this to happen
in a Gaussian random ﬁeld with a standard deviation σR(z) is
σR(z) = σR0D+(z)
because the linear growth of the density contrast is determined
by the growth factor, for which a ﬁtting formula was given in
Now, the present-day standard deviation σR0 must be chosen such as to reproduce the observed number density of clusters given in (149). The fraction of cosmic matter contained
in clusters is approximated by
≈3 × 10−3Ω−1
The standard deviation σ in (150) must now be chosen such
that this number is reproduced, which yields
Ωm0 = 0.3 .
Equations (150) and (151) can now be used to estimate how
the cluster abundance should change with redshift. Simple
evaluation reveals that the comoving cluster abundance is expected to drop very rapidly with increasing redshift if Ωm0 is
high, and much more slowly if Ωm0 is low. Qualitatively, this
behaviour is easily understood. If, in a low-density universe,
clusters do not form early, they cannot form at all because the
rapid expansion due to the low matter density prevents them
from growing late in the cosmic evolution. From the observed
slow evolution of the cluster population as a whole, it can be
concluded that the matter density must be low. Estimates arrive at
Ωm0 ≈0.3 ,
in good agreement with the preceding determinations.
D. Conclusions
What does it all mean? The preceding discussion should
have demonstrated that the matter density in the Universe is
considerably less than its critical value, approximately one
third of it. Since only a small fraction of this matter is visible, we call the invisible majority dark matter.
What is this dark matter composed of? Can it be baryons?
Tight limits are set by primordial nucleosynthesis, which predicts that the density in baryonic matter should be ΩB ≈0.04,
cf. (110). In the framework of the Friedmann-Lemaˆıtre models, the baryon density in the Universe can be higher than this
only if baryons are locked up in some way before nucleosynthesis commences. They could form black holes before, but
their mass is limited by the mass enclosed within the horizon
at, say, up to one minute after the Big Bang. According to
(90), the scale factor at this time was a ≈10−10, and thus the
matter density was of order ρm ≈1030ρcr0 ≈10 g cm−3. The
horizon size is rH ≈ct ≈1.8 × 1012 cm, thus the mass enclosed by the horizon is ≈3 × 104M⊙, which limits possible
black-hole masses from above.
Gravitational microlensing was used to constrain the
amount of dark, compact objects of subsolar mass in the halo
of the Milky Way. Although they were found to contribute
part of the mass, they can certainly not account for all of
it . Black holes
with masses of the order 106 M⊙should have been found
through their microlensing eﬀects . The abundance of lower-mass black holes is limited by stellar-dynamics arguments, in particular by the presence of cold disks . Their correspondingly high velocities require objects more massive than
galaxies to keep them bound. The formation of galaxies would
be much delayed in this case until larger-scale objects could
fragment into smaller pieces. This is the opposite of the observed hierarchy of cosmic structure formation, which clearly
shows that galaxies appear substantially earlier than galaxy
clusters. Therefore, the conclusion seems inevitable that the
dark matter must be cold, i.e. consisting of particles moving
non-relativistically . A sequence of numerical
simulations has shown that structure formation in a universe
ﬁlled with cold dark matter can be brought into agreement
with the observed large-scale cosmic structures .
VI. THE COSMIC MICROWAVE BACKGROUND
A. The isotropic CMB
1. Thermal history of the Universe
How does the Universe evolve thermally? We have seen
that the abundance of 4He shows that the Universe must have
gone through an early phase which was hot enough for the
nuclear fusion of light elements. But was there thermal equilibrium? Thus, can we speak of the “temperature of the Universe”?
From isotropy, we must conclude that the Universe expanded adiathermally: no net heat can have ﬂowed between
any two volume elements in the Universe because any ﬂow
2 Particle Data Group, 
would have singled out a direction, which is forbidden by
isotropy. An adiathermal process is adiabatic if it proceeds
slow enough for equilibrium to be maintained. Then, it is
also reversible and isentropic. Of course, irreversible processes must have occurred during the evolution of the Universe. However, as we shall see later, the entropy in the Universe is so overwhelmingly dominated by the photons of the
microwave background radiation that no entropy production
by irreversible processes can have added a signiﬁcant amount
of entropy. Thus, we assume that the Universe has in fact expanded adiabatically.
As the Universe expands and cools, particles are diluted
and interaction rates drop, so thermal equilibrium must break
down at some point for any particle species because collisions
become too rare. Very early in the Universe, however, the
expansion rate was very high, and it is important to check
whether thermal equilibrium can have been maintained despite the rapid expansion. The collision probability between
any two particle species will be proportional to their number
densities squared, ∝n2, because collisions are dominated by
two-body encounters. The collision rate, i.e. the number of
collisions experienced by an individual particle with others
will be ∝n, which is ∝a−3 for non-relativistic particles. Thus,
the collision time scale was τcoll ∝a3.
According to Friedmann’s equation, the expansion rate in
the very early Universe was determined by the radiation density, and thus proportional to ∝˙a/a ∝a−2, and the expansion
time scale was τexp ∝a2. Equilibrium could be maintained as
long as the collision time scale was suﬃciently shorter than
the expansion time scale,
τcoll ≪τexp ,
which turns out to be well satisﬁed in the early Universe when
a ≪1. Thus, even though the expansion rate was very high
in the early Universe, the collision rates were even higher, and
thermal equilibrium could indeed be maintained.
The ﬁnal assumption is that the components of the cosmic ﬂuid behave as ideal gases. By deﬁnition, this requires
that their particles interact only via short-ranged forces, which
implies that partition sums can be written as powers of oneparticle partition sums and that the internal energy of the ﬂuids
does not depend on the volume occupied. This is a natural assumption which holds even for charged particles because they
shield opposite charges on length scales small compared to
the size of the observable universe.
It is thus well justiﬁed to assume that there was thermal
equilibrium between all particle species in the early universe,
that the constituents of the cosmic “ﬂuid” can be described as
ideal gases, and that the expansion of the universe can be seen
as an adiabatic process. In later stages of the cosmic evolution, particle species will drop out of equilibrium when their
interaction rates fall below the expansion rate of the Universe.
As long as all species in the Universe are kept in thermodynamic equilibrium, there is a single temperature characterising the cosmic ﬂuid. Once particle species drop out of thermal equilibrium because their interaction rates decrease, their
temperatures, if deﬁned, may begin deviating. Even then, we
characterise the thermal evolution of the Universe by the temperature of the photon background.
2. Mean properties of the CMB
As discussed before, the CMB had been predicted in order
to explain the abundance of the light elements, in particular
of 4He . It was
serendipitously discovered by Penzias and Wilson in 1965
 .
Measurements of the energy
density in this radiation were mostly undertaken at long (radio) wavelengths, i.e. in the Rayleigh-Jeans part of the CMB
spectrum. To ﬁrmly establish that the spectrum is indeed the
Planck spectrum expected for thermal black-body radiation,
the FIRAS experiment was placed on-board the COBE satellite, where it measured the best realisation of a Planck spectrum ever observed , the density parameter in radiation is
Ωr0 = 8.51 × 10−5 ,
which shows that the scale factor at matter-radiation equality
3280 ≈3.0 × 10−4 .
This calculation includes three neutrino species besides the
photons, which were relativistic at the time of matter-radiation
The number density of baryons in the Universe is approximately
nB ≈ΩB0ρcr
≈2.3 × 10−7 cm−3 ,
conﬁrming that the photon-to-baryon ratio is extremely high,
2.3 × 10−7 ≈1.8 × 109 .
3. Decoupling of the CMB
When and how was the CMB set free? While the Universe
was suﬃciently hot to keep electrons and protons separated
(we neglect heavier elements here for simplicity), the photons
scattered oﬀthe charged particles, their mean free path was
short, and the photons could not propagate. When the Universe cooled below a certain temperature, electrons and protons combined to form hydrogen, free charges disappeared,
the mean free path became virtually inﬁnite and photons could
freely propagate.
The recombination reaction
p + e−↔H + γ
can thermodynamically be described by Saha’s equation for
the ionisation fraction x,
e−χ/kT ≈0.26
where χ is the ionisation energy of hydrogen, χ = 13.6 eV,
and ζ is the Riemann Zeta function.
Notice that Saha’s equation contains the inverse of the η parameter (160), which is a huge number due to the high photonto-baryon ratio in the Universe. This counteracts the exponential which would otherwise guarantee that recombination
happens when kT ≈χ, i.e. at T ≈1.6 × 105 K. Recombination is thus delayed by the high photon number, which
illustrates that newly formed hydrogen atoms are eﬀectively
reionised by suﬃciently energetic photons until the temperature has dropped well below the ionsation energy. Putting
x ≈0.5 in (162) yields a recombination temperature of
kTrec ≈0.3 eV ,
Trec ≈3000 K
and thus a recombination redshift of zrec ≈1100. Since this is
in the early matter-dominated era, the age of the Universe was
1 + α(1 −2α) + 2α3/2i
≈374 kyr ,
cf. Eq. (15), where α := aeq/arec ≈0.33.
Recombination does not proceed instantaneously. The ionisation fraction x drops from 0.9 to 0.1 within a temperature
range of approximately 200 K, corresponding to a redshift
or a time interval of
√Ωm0(1 + z)5/2 ≈50 kyr .
We are thus led to conclude that the CMB was released when
the Universe was approximately 374,000 years old, during a
phase that lasted approximately 50,000 years. We have derived this result merely using the present temperature of the
CMB, the photon-to-baryon ratio, the Hubble constant and the
matter density parameter Ωm0. The cosmological constant or
a possible curvature of the Universe do not matter here.
B. Structures in the CMB
1. The dipole
The Earth is moving around the Sun, the Sun is orbiting
around the Galactic centre, the Galaxy is moving within the
Local Group, which is falling towards the Virgo cluster of
galaxies. We can thus not expect the Earth to be at rest with
respect to the CMB. We denote the net velocity of the Earth
with respect to the CMB rest frame by v⊕. Lorentz transformation shows that, to lowest order in v⊕/c, the Earth’s motion
imprints a dipolar intensity pattern on the CMB with an amplitude of
The dipole’s amplitude has been measured to be ≈1.24 mK,
from which the Earth’s velocity is inferred to be (Fixsen et al.,
v⊕≈(371 ± 1.5) km s−1 .
This is the highest-order deviation from isotropy in the CMB,
but it is irrelevant for our purposes since it does not allow any
conclusions on the Universe at large.
2. Expected amplitude of CMB ﬂuctuations
It is reasonable to expect that density ﬂuctuations in the
CMB should match density ﬂuctuations in the matter because
photons were tightly coupled to baryons by Thomson scattering before recombination. For adiabatic ﬂuctuations, the
density contrast δ in matter is 3/4 that in radiation. Since the
radiation density is ∝T 4, a density contrast δ is thus expected
to produce relative temperature ﬂuctuations of order
Obviously, there are large-scale structures in the Universe today whose density contrast reaches or even substantially exceeds unity. Assuming linear structure growth on scales where
δ0 ≈1, and knowing the scale factor at recombination, we can
thus infer that relative temperature ﬂuctuations of order
should be seen in the CMB, i.e. ﬂuctuations of order mK,
similar to the dipole. Such ﬂuctuations, however, were not
observed, although cosmologists kept searching increasingly
desperately for decades after 1965 for an example).
Why do they not exist? The estimate above is valid only
under the assumption that matter and radiation were tightly
coupled. Should this not have been the case, density ﬂuctuations did not need to leave such a pronounced imprint on the
CMB. In order to avoid the tight coupling, the majority of
matter must not interact electromagnetically. Thus, we conclude from the absence of mK ﬂuctuations in the CMB that
matter in the Universe must be dominated by something that
does not interact with light . This is perhaps
the strongest argument in favour of not-electromagnetically
interacting dark matter.
3. Expected CMB ﬂuctuations
Before we come to the results of CMB observations and
their signiﬁcance for cosmology, let us summarise which
physical eﬀects we expect to imprint structures on the CMB
 . The basic hypothesis is that the cosmic
structures that we see today formed via gravitational instability from seed ﬂuctuations in the early Universe, whose inﬂationary origin is likely, but yet unclear. This implies that there
had to be density ﬂuctuations at the time when the CMB was
released. Via Poisson’s equation, these density ﬂuctuations
were related to ﬂuctuations in the Newtonian potential. Photons released in a potential ﬂuctuation δΦ lost energy if the
ﬂuctuation was negative, and gained energy when the ﬂuctuation was positive. This energy change can be translated to the
temperature change
which is called the Sachs-Wolfe eﬀect . The factor 1/3 in front is caused by the gravitational
redshift being oﬀset by time retardation in the gravitational
ﬁeld of the perturbation.
Let us brieﬂy look into the expected statistics of the Sachs-
Wolfe eﬀect. We introduced the power spectrum of the density
ﬂuctuations in (27) as the variance of the density contrast in
Fourier space. Poisson’s equation implies
for the Fourier modes of the gravitational potential.
the power spectrum of the temperature ﬂuctuations due to the
Sachs-Wolfe eﬀect is
PT ∝PΦ ∝⟨ˆδˆδ∗⟩
according to (28) with ns = 1. This shows that the Sachs-
Wolfe eﬀect can only be important at small k, i.e. on large
scales, and dies oﬀquickly towards smaller scales.
The main constituents of the cosmic ﬂuid were dark matter, baryons, electrons and photons. Overdensities in the dark
matter compressed the ﬂuid due to their gravity until the rising
pressure in the tightly coupled baryon-electron-photon ﬂuid
was able to counteract gravity and drive the ﬂuctuations apart.
In due course, the pressure sank and gravity won again, and
so forth: the baryon-electron-photon ﬂuid underwent acoustic oscillations, but not the dark matter, which was decoupled.
Since the pressure was dominated by the photons, whose pressure is a third of their energy density, a good approximation
to the sound speed of the tightly coupled photon-baryon ﬂuid
Only such density ﬂuctuations could undergo acoustic oscillations which were small enough to be crossed by sound waves
in the available time. The largest comoving length that could
be travelled by sound waves was the comoving sound horizon
= 2cs √arec
= 163.3 Mpc .
Larger-scale density ﬂuctuations could not oscillate. We saw
in (18) and (19) that the comoving angular-diameter distance
from today to scale factor arec is fK[w(arec)] = w(arec) if we
assume spatial ﬂatness, K = 0. In excellent approximation,
fK[w(arec)] = w(arec) = 3.195 c
Thus, the sound horizon sets an angular scale of
w(arec) = 0.66◦.
This angular scale can be read oﬀthe ﬁrst acoustic peak in
the CMB power spectrum (see § VI.B.5 below). Its relation
to the physical sound horizon (175) depends almost precisely
on the sum of Ωm0 and ΩΛ0 if all else remains ﬁxed. Thus,
the location of the ﬁrst acoustic peak determines the spatial
curvature of the cosmological model. When combined with
measurements of H0, latest data (cf. Tab. I) show that K = 0
to high precision.
A third eﬀect inﬂuencing structures in the CMB is caused
by the fact that, as recombination proceeds, the mean-free
path of the photons increases. If ne = xnB is the electron
number density and σT is the Thomson cross section, the comoving mean-free path is
λ ≈(xnBσT)−1 .
As the ionisation fraction x drops towards zero, the mean-free
path approaches inﬁnity. After N scatterings, the photons will
have diﬀused by
on average. The number of scatterings per unit time is
dN ≈xnBσTcdt ,
and thus the diﬀusion scale is given by
Inserting x ≈1/2 as a typical value into the latter integrand,
we can approximate
Numerical evaluation returns a comoving damping length of
order 50 Mpc, corresponding to angular scales of θD ≈10′
on the sky. The eﬀect of this diﬀusion process is called Silk
damping . We thus expect three principal mechanisms to shape the appearance of the microwave sky: the
Sachs-Wolfe eﬀect on the largest scales, the acoustic oscillations on scales smaller than the sound horizon, and Silk damping on scales smaller than a few arc minutes.
4. CMB polarisation
If the CMB does indeed arise from Thomson scattering,
interesting eﬀects must occur because the Thomson scattering cross section is polarisation sensitive, and can thus convert unpolarised into linearly polarised radiation. Suppose an
electron is illuminated by unpolarised radiation from the left,
then the radiation scattered towards the observer will be linearly polarised in the perpendicular direction. Likewise, unpolarised radiation incoming from the top will be linearly polarised horizontally after being scattered towards the observer.
Thus, if the electron is irradiated by a quadrupolar intensity
pattern, the scattered radiation will be partially linearly polarised. The polarised intensity is expected to be of order 10%
of the total intensity. The polarised radiation must reﬂect the
same physical eﬀects as the unpolarised radiation, and the two
must be cross-correlated. Much additional information on the
physical state of the early Universe is thus contained in the
polarised component of the CMB, apart from the fact that the
mere detection of the polarisation supports the physical picture of the CMB’s origin.
5. The CMB power spectrum
Fourier transformation is impossible on the sphere, but the
analysis of the CMB proceeds in a completely analogous
way by decomposing the relative temperature ﬂuctuations into
spherical harmonics, ﬁnding the spherical-harmonic coeﬃcients
T Ylm(⃗θ) ,
and from them the power spectrum
which is the equivalent on the sphere to the three-dimensional
power spectra deﬁned in (27). The average over m expresses
the expectation of statistical isotropy. If parts of the observed
sky need to be masked, care has to be taken to orthonormalise
the spherical harmonics on the remaining domain.
The shape of the CMB power spectrum reﬂects the three
physical mechanisms identiﬁed above: at small l (on large
scales), the Sachs-Wolfe eﬀect causes a feature-less plateau,
followed by pronounced maxima and minima due to the
acoustic oscillations, damped on the smallest scales (largest
l) by Silk damping.
l(l+1)Cl/(2π) in µK2
multipole order l
Sachs-Wolfe effect
Acoustic peaks
Silk damping
FIG. 7 Schematic appearance of the CMB power spectrum due to
the three dominating physical eﬀects deﬁning its shape: the Sachs-
Wolfe eﬀect on largest scales, the Silk damping on smallest scales,
and the acoustic oscillations in between.
The detailed shape of the CMB power spectrum depends
sensitively on the cosmological parameters, which can in turn
be determined by ﬁtting the theoretically expected to the measured Cl. This is the main reason for the detailed and sensitive CMB measurements pioneered by COBE, continued by
ground-based and balloon experiments, and culminating recently in the spectacular results obtained by the WMAP satellite.
6. Secondary anisotropies and foregrounds
By deﬁnition, the CMB is the oldest visible source of
photons because all possible earlier sources could not shine
through the hot cosmic plasma. Therefore, every source that
produced microwave photons since, or that produced photons
which became redshifted into the microwave regime by now,
must appear superposed on the CMB. The CMB is thus covered by layers of foreground emission that have to be unveiled
before the CMB can be observed.
Broadly, the CMB foregrounds can be grouped into point
sources and diﬀuse sources. The most important among the
point sources are infrared galaxies at high redshift and bodies
in the Solar System such as the major planets, but even some
of the asteroids.
The population of infrared sources at high redshift is poorly
known, but the angular resolution of CMB measurements has
so far been too low to be signiﬁcantly contaminated by them.
Future CMB observations will have to remove them carefully
 .
Microwave radiation from bodies in the Solar System has
so far been used to calibrate microwave detectors. CMB observations at an angular resolution below ∼10′ are expected
to detect hundreds of minor planets.
Diﬀuse CMB foregrounds are mainly caused by our Galaxy
itself. There are three main components: synchrotron emission, emission from warm dust, and bremsstrahlung .
Synchrotron radiation is emitted by relativistic electrons in
the Galaxy’s magnetic ﬁeld. It is highly linearly polarised and
has a power-law spectrum falling steeply from radio towards
microwave frequencies. It is centred on the Galactic plane,
but shows ﬁlamentary extensions from the Galactic centre towards the Galactic poles.
The dust in the Milky Way is also concentrated in the
Galactic plane. It is between (10 . . . 20) K warm and thus substantially warmer than the CMB itself. It has a thermal spectrum modiﬁed by frequency-dependent self absorption. Due
to its higher temperature, the warm dust has a spectrum rising
with increasing frequency in the frequency window where the
CMB is usually observed.
Bremsstrahlung radiation is emitted by ionised hydrogen
clouds (HII regions) in the Galactic plane. Its spectrum has
the shape typical for thermal free-free radiation, falling exponentially at photon energies near and above the gas temperature, but is ﬂat at CMB frequencies. Further sources of microwave radiation in the Galaxy are less prominent. Among
them are line emission from CO molecules embedded in cool
gas clouds.
The falling synchrotron spectrum, the ﬂat spectrum of
the free-free radiation, and the rising spectrum of the warm
dust create a window for CMB observations between ∼
(100 . . . 200) GHz. The diﬀerent spectra of the foregrounds,
and their non-Planckian character, are crucial for their proper
removal from the CMB data. Therefore, CMB measurements
have to be carried out in multiple frequency bands.
Secondary anisotropies are caused by propagation eﬀects
rather than photon emission.
The most important are the
integrated Sachs-Wolfe eﬀect and the (thermal and kinetic)
Sunyaev-Zel’dovich eﬀects in galaxy clusters.
The integrated Sachs-Wolfe eﬀect is caused by gravitational
potential wells deepening while crossed by CMB photons. It
is determined by the line-of-sight integral of the time derivative of the potential ﬂuctuations caused by the density ﬂuctuations between us and the CMB. By cross-correlating CMB
temperature ﬂuctuations with structures in the galaxy distribution, the integrated Sachs-Wolfe eﬀect has indeed been detected . It
constrains the ratio D+(a)/a.
The Sunyaev-Zel’dovich eﬀect is due to inverse Compton scattering of CMB photons
oﬀhot electrons in the intracluster plasma. On average, the
photons gain energy and are thus moved from the low- to the
high-frequency part of the spectrum. When observed through
a galaxy cluster, the CMB therefore appears fainter at low and
brighter at high frequencies, with the transition at 217 GHz.
This gives galaxy clusters a peculiar spectral signature in the
microwave regime as they cast shadows below, emit above,
and vanish at 217 GHz. Once the angular resolution of CMB
detectors will drop towards a few arc minutes, a large number
of galaxy clusters are expected to show up in this way. Besides
this thermal Sunyaev-Zel’dovich eﬀect, there is the kinetic effect caused by the bulk motion of the cluster as a whole, which
causes CMB radiation to be scattered by the electrons moving
with the cluster. Very few clusters have so far been detected
in CMB data, but thousands are expected to be found in future
7. Measurements of the CMB
Wien’s law (113) shows that the CMB spectrum peaks at
λmax ≈0.11 cm, or at a frequency of νmax ≈282 GHz. As we
saw, Silk damping sets in below ∼10′ arc minutes, thus most
of the structures in the CMB are resolvable for rather small
telescopes. According to the formula
∆θ ≈1.44 λ
relating the diﬀraction-limited angular resolution ∆θ to the ratio between wavelength and telescope diameter D, we ﬁnd that
mirrors with
D ≲1.44 λmax
are suﬃcient to achieve suﬃcient angular resolution up to the
Silk-damping scale θD. Detectors are needed which are sensitive to millimetre and sub-mm radiation and reach µK sensitivity, while the telescope optics can be kept rather small and
Two types of detector are commonly used. The ﬁrst are
bolometers, which measure the energy of the absorbed radiation by the temperature increase it causes.
to be cooled to very low temperatures, typically in the mK
regime. The second are so-called high electron mobility detectors (HEMTs), in which the currents caused by the incoming electromagnetic ﬁeld are measured directly. They measure
amplitude and phase of the waves and are thus polarisationsensitive by construction, which bolometers are not. Polarisation measurements with bolometers are possible with suitably
shaped so-called feed horns guiding the radiation into the detectors.
Since water vapour in the atmosphere both absorbs and
emits microwave radiation through molecular lines, CMB observations need to be carried out either at high, dry and cold
sites on the ground (e.g. in the Chilean Andes or at the South
Pole), or from balloons rising above the troposphere, or from
satellites in space.
After the breakthrough achieved with COBE , progress was made with balloon experiments
and with ground-based interferometers. The balloons covered
a small fraction of the sky (typically ∼1%) at frequencies
between 90 and 400 GHz, while the interferometers observe
even smaller ﬁelds at somewhat lower frequencies (typically
around 30 GHz).
The ﬁrst discovery of the CMB polarisation and its crosscorrelation with the CMB temperature was achieved with the
DASI interferometer . The existence, location and height of the ﬁrst acoustic peak had been ﬁrmly
established before the NASA satellite
Wilkinson Microwave Anisotropy Probe (WMAP for short)
was launched, but the increased sensitivity and the full-sky
coverage of WMAP produced breath-taking results . WMAP is still operating, measuring the CMB
temperature at frequencies between 23 and 94 GHz with an
angular resolution of ≳15′.
The sensitivity of WMAP is
barely high enough for polarisation measurements.
By now, data from the ﬁrst ﬁve years of operation have been
published, and cosmological parameters have been obtained
ﬁtting theoretically expected to the measured temperature-
ﬂuctuation power spectrum and the temperature-polarisation
power spectrum. Results were given in Tab. I, adapted from
 .
FIG. 8 Power spectrum of CMB temperature ﬂuctuations as measured from the 5-year data of WMAP and several additional groundbased experiments. (Courtesy of the WMAP team)
Although the CMB provides insight into the cosmological
parameters on its own, it is most powerful when combined
with other data sets. In particular, the Hubble constant is not
an independent measurement from the CMB alone. Only by
assuming a ﬂat ΛCDM universe, it can be inferred from the
location of the ﬁrst acoustic peak in the CMB power spectrum
to be H0 = 70.1 ± 1.3 km s−1 Mpc−1, which agrees perfectly
with the results of the Hubble Key Project and at least one
interpretation of the gravitational-lens time delays.
A European CMB satellite mission is under way: ESA’s
Planck satellite has been launched in May 2009. It will observe the microwave sky in nine frequency bands between
30 and 857 GHz with about ten times higher sensitivity than
WMAP, and an angular resolution of ≳5′ . Its
wide frequency coverage will be very important for substantially improved foreground subtraction. It will also have suf-
ﬁcient sensitivity to precisely measure the CMB polarisation
in some of its frequency bands . Moreover, it is expected that Planck will
detect a large number of galaxy clusters through their thermal
Sunyaev-Zel’dovich eﬀect.
VII. COSMIC STRUCTURES
A. Quantifying structures
1. Introduction
We have seen before that there is a very speciﬁc prediction
for the power spectrum of cold dark matter density ﬂuctuations in the Universe, characterised by (28). Recall that its
shape was inferred from the simple assumption that the rms
mass of density ﬂuctuations entering the horizon should be independent of the time when they enter the horizon, and from
the fact that perturbation modes entering during the radiation
era are suppressed until matter begins dominating. Given the
simplicity of the argument, and the corresponding strength of
the prediction, it is very important for cosmology to ﬁnd out
whether the actual power spectrum of matter density ﬂuctuations does in fact have the expected shape, and if it has, to
determine the only remaining parameter, i.e. the normalisation of the power spectrum.
Since the comoving wave number keq of the peak in the
power spectrum is set by the comoving horizon radius at
matter-radiation equality (26),
it is proportional to the matter-density parameter Ωm0. A measurement of keq would thus provide an independent and very
elegant determination of Ωm0.
Since the power spectrum is deﬁned in Fourier space, it is
not obvious how it should be measured. The correlation function is related to the power spectrum by
P(k)sin kx
whose inverse transform is
ξ(x)sin kx
The power spectrum can thus in principle be determined from
a measured correlation function ξ(x).
2. Measuring the correlation function
The correlation function of the three-dimensional density
ﬁeld cannot be measured directly. Gravitational lensing by
large-scale structures comes close by measuring the weighed
line-of-sight projection of the matter power spectrum; see
§ VIII below. Starting from the assumption that galaxies may
be tracers of the underlying density ﬁeld, we can use their correlation function as an estimate for that of the matter.
Suppose we divide space into cells of volume dV small
enough to contain at most a single galaxy. Then, the probability of ﬁnding one galaxy in dV1 and another in dV2 is
dP = ⟨n(⃗x1)n(⃗x2)⟩dV1dV2 ,
where n is the number density of the galaxies as a function of
position. If we introduce a density contrast for the galaxies in
analogy to the density contrast for the matter,
and assume for now that δn = δ, we ﬁnd from (190) with
n = ¯n(1 + δ)
dP = ¯n2⟨(1+δ1)(1+δ2)⟩dV1dV2 = ¯n2[1+ξ(x)]dV1dV2 , (192)
where x is the comoving distance between the two volume
elements. This shows that the correlation function quantiﬁes
the excess probability above random for ﬁnding galaxy pairs
at a given distance.
Thus, the correlation function can be measured by counting galaxy pairs and comparing the result to the Poisson expectation, i.e. to the pair counts expected in a random point
distribution. Symbolically,
1 + ξ = ⟨DD⟩
where D and R represent the data and the random point set, respectively. Several other estimators for ξ have been proposed
which are all equivalent in the ideal situation of an inﬁnitely
extended point distribution. For ﬁnite point sets, their noise
properties diﬀer .
The recipe for measuring ξ(x) is thus to count pairs separated
by x in the data D and in the random point set R, or between
the data and the random point set, and to use one of the estimators proposed.
The obvious question is then how accurately ξ can be determined. The simple expectation in the absence of clustering
where Np is the number of pairs found. Thus, the Poisson
error on the correlation function is
This is a lower limit to the actual error, however, because
the galaxies are in fact correlated. It turns out that the result (195) should be multiplied with 1 + 4π¯nJ3, where J3 is
the volume integral over ξ within the galaxy-survey volume
 . The true error bars on ξ are therefore hard to
Having measured the correlation function, it would in principle suﬃce to carry out the Fourier transform (189) to ﬁnd
P(k), but this is diﬃcult in reality because of the inevitable
sample limitations. Consider (188) and an underlying power
spectrum of CDM shape, falling oﬀ∝k−3 for large k, i.e. on
small scales. For ﬁxed x, the integrand in (188) falls oﬀvery
slowly, which means that a considerable amount of smallscale power is mixed into the correlation function. Since ξ
at large x is small and most aﬀected by measurement errors,
any uncertainty in the large-scale correlation function is propagated to the power spectrum even on small scales.
A further problem is the uncertainty in the mean galaxy
number density ¯n. Since 1 + ξ ∝¯n−1 according to (192), the
uncertainty in ξ due to an uncertainty in ¯n is
1 + ξ ≈∆ξ = ∆¯n
showing that ξ cannot be measured with an accuracy better
than the relative accuracy of the mean galaxy density.
3. Measuring the power spectrum
Given these problems with real data, it seems appropriate to estimate the power spectrum directly. The function to
be transformed is the density ﬁeld sampled by the galaxies,
which can be represented by a sum of Dirac delta functions
centred on the locations of the N galaxies,
δD(⃗x −⃗xi) .
The Fourier transform of the galaxy density contrast is then
ˆδg(⃗k) = 1
In the absence of correlations, the Fourier phases of the individual terms are independent, and the variance of the Fourier
amplitude for a single mode becomes
⟨ˆδg(⃗k)ˆδ∗
ei⃗k⃗xie−i⃗k⃗xi = 1
This is the so-called shot noise present in the power spectrum due to the discrete sampling of the density ﬁeld. The
shot-noise contribution needs to be subtracted from the power
spectrum of the real, correlated galaxy distribution,
|ˆδg(⃗k)|2 −1
where the sum extends over all m Fourier modes with wave
number k contained in the survey.
This is not the ﬁnal result yet, because any real survey typically covers an irregularly shaped volume from which parts
need to be excised because they are overshone by stars or unusable for any other reason. The combined eﬀect of mask and
irregular survey volume is described by a window function
f(⃗x) which multiplies the galaxy density,
n(⃗x) →f(⃗x)n(⃗x) ,
(1 + δg) →f(⃗x)(1 + δg) ,
implying that the Fourier transform of the mask needs to be
subtracted.
Moreover, the Fourier convolution theorem says that the
Fourier transform of the product f(⃗x)δg(⃗x) is the convolution
of the Fourier transforms ˆf(⃗k) and ˆδg(⃗k),
bfδ = ˆf ∗ˆδg ≡
ˆf(⃗k′)ˆδg(⃗k′ −⃗k)d3k′ .
If the Fourier phases of ˆf and ˆδg are uncorrelated, which is
the case if the dimensions of the survey are large enough compared to the size 2π/k of the density mode, this translates to a
convolution of the power spectrum,
Pobs = Ptrue ∗| ˆf(⃗k)|2 .
This convolution smoothes the observed compared to the true
power spectrum and changes its amplitude.
If the Poisson error dominates in the survey, the diﬀerent
modes ˆδg(⃗k) can be shown to be uncorrelated, and the standard
deviation after summing over the m modes with wave number
2m/N, which yields the minimal error bar to be attached
to the power spectrum.
Thus, the shot noise contribution and the Fourier transform
of the window function need to be subtracted, and the window
function needs to be deconvolved, and the amplitude needs to
be corrected for the eﬀective volume covered by the window
function before the measured power spectrum can be compared to the theoretical expectation.
4. Biasing
What we have determined so far is the power spectrum of
the galaxy number-density contrast δn rather than that of the
matter density contrast δ. Both are related by a possibly scaledependent bias factor b(k), such that
δn(⃗k) = b(k)ˆδg(⃗k) .
Clearly, diﬀerent types of objects sample the underlying matter density ﬁeld in diﬀerent ways. Galaxy clusters, for instance, are much more rare than galaxies and are thus expected
to have a substantially higher bias factor than galaxies. Obviously, the bias factor enters the power spectrum squared, e.g.
gal(k) P(k) .
It constitutes a major and possibly systematic uncertainty in
the determination of the matter power spectrum from the
galaxy power spectrum.
5. Redshift-space distortions
For any quantiﬁcation of three-dimensional structures in
galaxy surveys, the three-dimensional positions ⃗xi of the
galaxies in the survey need to be known. Distances can be
inferred only from the galaxy redshifts and thus from galaxy
velocities. These, however, are composed of the Hubble velocities, from which the distances can be determined, and the
peculiar velocities,
v = vHubble + vpec ,
which are caused by local density perturbations and are unrelated to the galaxy densities. Since observations of individual
galaxies do not allow any separation between the two velocity
components, distances are inferred from the total velocity v
rather than the Hubble velocity as they should be,
= vHubble + vpec
= Dtrue + ∆D ,
giving rise to a distance error δD = vpec/H0, the so-called
redshift-space distortion.
The redshift-space distortions create a peculiar pattern
through which they can be corrected . Consider a matter overdensity such as a galaxy
cluster, containing galaxies moving with random virial velocities in it. The virial velocities of order 1000 km s−1 scatter
around the systemic cluster velocity and thus broaden the redshift distribution of the cluster galaxies. In redshift space,
therefore, the cluster appears stretched along the line-of-sight,
which is called the ﬁnger-of-god eﬀect.
In addition, the cluster is surrounded by an infall region
where the galaxies are not virialised yet, but move in an ordered, radial pattern towards the cluster. Galaxies in front of
the cluster thus have higher, and galaxies behind the cluster
have lower recession velocities compared to the Hubble velocity, leading to a ﬂattening of the infall region in redshift
A detailed analysis shows that the redshift-space power
spectrum Pz is related to the real-space power spectrum P by
Pz(k) = P(k)
1 + βµ22 ,
where µ is the direction cosine between the line-of-sight and
the wave vector ⃗k, and β is related to the bias parameter b
and f(Ωm) is the logarithmic derivative of the growth factor
f(Ωm) ≡d ln D+(a)
Thus, the characteristic pattern of the redshift-space distortions around overdensities allows a measurement of the bias
factor . Another way of measuring b is
based upon gravitational lensing . Measurements of b show that it is in fact
only weakly scale-dependent, near unity for “ordinary” galaxies, but depends mildly on galaxy luminosity and type .
6. Baryonic acoustic oscillations
As we have seen in the discussion of the CMB, the cosmic ﬂuid underwent acoustic oscillations on comoving scales
smaller than the sound horizon (175) ws = 163.3 Mpc, corresponding to a comoving wave number ks
0.038 Mpc−1.
When the CMB decoupled, the oscillations
ceased, leaving structures in the cosmic matter distribution
with a fundamental wavelength of ws and its overtones. The
other scale characterising the cosmic structures is the horizon
radius at matter-radiation equality (26), which was responsible to set the peak location of the matter-ﬂuctuation power
spectrum at keq = 0.01 Mpc−1.
Thus, at wave numbers ≈3.8 times the peak scale and
above, we expect the wave-like imprint of these baryonic
acoustic oscillations (BAOs) on top of the otherwise smooth
dark-matter power spectrum.
Mode coupling due to nonlinear evolution of cosmic structures must have stretched and
distorted this pattern to some degree. As we shall see below,
the BAOs have indeed been discovered in the largest galaxy
surveys. They play an important role in attempts to recover the
history of the cosmic expansion rate 
for a recent review).
B. Measurements and results
1. The power spectrum
Spectacularly successful measurements of the galaxy
power spectrum became recently possible with the two largest
galaxy surveys to date, the Two-Degree Field Galaxy Redshift Survey ) and the Sloan Digital
Sky Survey ). As anticipated in the
preceding discussion, an enormous eﬀort had to be made to
identify galaxies, measure their redshifts, select homogeneous
galaxy subsamples by luminosity and colour as a function of
redshift so as not to compare and correlate apples with oranges, estimate the window function of the survey, determine
the average galaxy number density, correct for the convolution
with the window function and for the bias, and so forth.
Moreover, numerical calibration experiments were carried
out in which all measurement and correction techniques were
applied to simulated data in the same way as to the real data
to determine reliable error estimates and to test whether the
full sequence of analysis steps ultimately yields an unbiased
Based on 221, 414 galaxies, the 2dFGRS consortium derived a power spectrum of superb quality .
First and foremost, it agrees well with the power-spectrum
shape expected for cold dark matter on the small-scale side of
the peak. This is a highly remarkable result on its own. The
2dFGRS power spectrum also clearly shows a turn-over towards larger scales, signalling the presence of the peak. The
survey is still not quite large enough to clearly show the peak,
but the peak location can be estimated from the ﬂattening of
the power spectrum. Its proportionality to Ωm0 allows an independent determination of the matter density parameter. Finally, and most spectacularly, the power spectrum shows the
baryonic acoustic oscillations, whose amplitude allows an independent determination of the ratio between the density parameters of baryons and dark matter.
Apart from the fact that the galaxy power spectrum supports
the CDM shape on small scales, the results obtained from the
2dFGRS can be summarised as follows:
Ωm0 = 0.233 ± 0.022
Ωb0/Ωm0 = 0.185 ± 0.046 .
A Hubble constant of h = 0.72 is assumed here. Indirectly,
the baryon density is constrained to be Ωb0 ≈0.04, which is
in perfect agreement with the value derived from primordial
nucleosynthesis and the measured abundances of the light elements.
Based on 522, 280 galaxies, the power spectrum inferred
from the SDSS is also well compatible
with the CDM shape. The estimate for the matter density parameter overlaps with that from the 2dFGRS on large scales,
Ωm0 = 0.22 ± 0.04, but increases when small scales are included.
FIG. 9 Galaxy power spectra obtained from the 2dFGRS and two releases of the SDSS . The expected shape of the CDM power spectrum is
well reproduced, and the diﬀerence in amplitudes may be attributed
to scale-dependent galaxy biasing. )
It should be kept in mind, however, that the assumption of
linear biasing may turn out to be too simplistic for precise
cosmological inferences to be drawn from galaxy power spectra. Nonetheless, power spectra for diﬀerent types of galaxy
will provide invaluable information on the physics of galaxy
formation.
VIII. COSMOLOGICAL WEAK LENSING
A. Cosmological light deﬂection
1. Deﬂection angle, convergence and shear
Gravitational lensing was mentioned two times before: ﬁrst
in § III.B as a means for measuring the Hubble constant
through the time delay caused by gravitational light deﬂection, and second as a means for measuring cluster masses
in § V.B.3. For cosmology as a whole, gravitational lensing
has also developed into an increasingly important tool for reviews).
Matter inhomogeneities deﬂect light. Working out this effect in the limit of a small Newtonian gravitational potential,
Φ ≪c2, leads to the deﬂection angle
⃗α(⃗θ) = 2
dw′ fK(w −w′)
⃗∇⊥Φ[ fK(w′)⃗θ] .
It is determined by a weighted integral over the gradient of
the Newtonian gravitational potential Φ perpendicular to the
line-of-sight into direction ⃗θ on the observer’s sky, and the
weight is given by the comoving angular-diameter distance
fK(w) deﬁned in (2). The integral extends along the comoving
radial distance w′ along the line-of-sight to the distance w of
the source.
Equation (212) can be intuitively understood. Light is de-
ﬂected due to the pull of the dimension-less Newtonian gravitational ﬁeld ⃗∇⊥Φ/c2 perpendicular to the otherwise unperturbed line-of-sight, and the eﬀect is weighed by the ratio between the angular-diameter distances from the deﬂecting potential to the source, fK(w −w′), and from the observer to the
source, fK(w). Thus, a lensing mass distribution very close
to the observer gives rise to a large deﬂection, while a lens
near the source, w′ ≈w, has very little eﬀect. The factor of
two is a relic from general relativity and is due to space-time
curvature, which is missing in Newtonian gravity. Assuming
K = 0, we can replace fK(w) by w.
It is important to realise that the deﬂection itself is not observable. If all light rays emerging from a source were de-
ﬂected by the same angle on their way to the observer, no noticeable eﬀect remained. What is important, therefore, is not
the deﬂection angle itself, but its change from one light ray to
the next. This is quantiﬁed by the derivative of the deﬂection
angle with respect to the direction ⃗θ,
dw′ (w −w′)w′
The additional factor w′ in the weight function arises because
the derivative of the potential is taken with respect to comoving coordinates xi rather than the angular components θi. Obviously, the complete weight function
W(w′, w) ≡(w −w′)w′
vanishes at the observer, w′ = 0, and at the source, w′ = w,
and peaks approximately half-way in between.
For applications of gravitational lensing, it is important to
distinguish the trace-free part of the matrix (213) from its
dw′ W(w′, w)
(w′⃗θ) . (215)
The derivatives of Φ can be combined to the two-dimensional
Laplacian, which can be replaced by the three-dimensional
Laplacian because the derivatives parallel to the line-of-sight
do not contribute to the integral (215). Thus, we ﬁnd
dw′ W(w′, w) ∆Φ .
Next, we can use Poisson’s equation to replace the Laplacian of Φ by the density. In fact, we have to take into account
that light deﬂection is caused by density perturbations, and
that we need the Laplacian in terms of comoving rather than
physical coordinates. Thus,
a−2∆Φ = 4πG¯ρδ ,
where δ is the density contrast and
¯ρ = ¯ρ0a−3 = ρcrΩm0a−3 = 3H2
is the mean matter density. Thus, Poisson’s equation reads
and (216) becomes
dw′ W(w′, w) δ
where we have introduced the (eﬀective) convergence κ.
The trace-free part of the matrix (213) is
which deﬁnes the so-called shear components γi. Speciﬁcally,
dw′ W(w′, w)
dw′ W(w′, w)
Combining results, we can write the matrix of deﬂectionangle derivatives as
κ + γ1
This matrix contains the important information on how an image is magniﬁed and distorted. In the limit of weak gravitational lensing, the size of a lensed image is changed by the
relative magniﬁcation
while the image distortion is given by the shear components.
In fact, an originally circular source with radius r will appear
as an ellipse with major and minor axes
1 −κ + γ ,
where γ ≡(γ2
2)1/2. The ellipticity of the observed image
of a circular source thus provides an estimate for the shear,
2. Power spectra
Of course, the exact light deﬂection expected along a particular line-of-sight cannot be predicted because the mass distribution along that light path is unknown. However, we can
predict the statistical properties of weak lensing from those of
the density-perturbation ﬁeld. We are thus led to the following
problem: Suppose the power spectrum P(k) of a Gaussian random density-perturbation ﬁeld δ is known, what is the power
spectrum of any weighed projection of δ along the line-ofsight? The answer is given by Limber’s equation. Suppose
the weight function is q(w) and the projection is
dw′ q(w′)δ(w′⃗θ) .
If q(w) is smooth compared to δ, i.e. if the weight function
changes on scales much larger than typical scales in the density contrast, then the power spectrum of g is
dw′ q2(w′)
where ⃗l is a two-dimensional wave vector which is the Fourier
conjugate variable to the two-dimensional position ⃗θ on the
Strictly speaking, Fourier transforms are inappropriate because the sky is not an inﬁnite, two-dimensional plane. Instead, the appropriate set of orthonormal base functions are
the spherical harmonics. However, lensing eﬀects are usually
observed in areas whose solid angle is very small compared
to the full sky. If this is so, the survey area can be approximated by a section of the plane locally tangent to the sky,
and Fourier transforms can be used in this so-called ﬂat-sky
approximation.
Equation (220) is clearly of the form (227) with the weight
thus the power spectrum of the convergence is, according to
Limber’s equation,
Pκ(l) = 9Ω2
dw′ ¯W2(w′, w) P
with a new weight function
¯W(w′, w) ≡W(w′, w)
While it requires huge data sets and extremely careful data
analysis to observe the diﬀerential magniﬁcation δµ or the
convergence κ , image distortions can
in principle be measured in a more straightforward way. With
a brief excursion through Fourier space, it can easily be shown
that the power spectrum of the shear is exactly identical to that
of the convergence,
Pγ(l) = Pκ(l) .
Thus, the statistics of the image distortions caused by cosmological weak lensing contains integral information on the
power spectrum of the matter ﬂuctuations.
Since the shear is deﬁned on the two-sphere (the observer’s
sky), its power spectrum is related to its correlation function
ξγ through the two-dimensional Fourier transform
(2π)2 Pγ(l)ei⃗φ⃗l =
2π Pγ(l)J0(lφ) ,
where Jν is the ordinary Bessel function of order ν.
3. Correlation functions
In principle, shear correlation functions are measured by
comparing the ellipticity of one galaxy with the ellipticity of
other galaxies at an angular distance φ from the ﬁrst. Ellipticities are oriented, of course, and one has to specify against
what other direction the direction of, say, the major axis of a
given ellipse is to be compared with. Since correlation functions are measured comparing the shear of galaxy pairs, a
preferred direction is deﬁned by the line connecting the two
galaxies of the pair under consideration.
Let α be the angle between this direction and the major axis
of the ellipse, then the tangential and cross components of the
shear are deﬁned by
γ+ ≡γ cos 2α ,
γ× ≡γ sin 2α .
The factor two is important because it accounts for the fact
that an ellipse is mapped onto itself when rotated by an angle
π. This illustrates that the shear is a spin-2 ﬁeld: It returns into
its original orientation when rotated by π rather than 2π.
The correlation functions of the tangential and cross components of the shear are
ξ++(φ) = ⟨γ+(θ)γ+(θ + φ)⟩= 1
2π Pκ(l) J0(lφ) + J4(lφ)
ξ××(φ) = ⟨γ×(θ)γ×(θ+φ)⟩= 1
2π Pκ(l) J0(lφ) −J4(lφ) ,
while the cross-correlation between the tangential and cross
components must vanish, ξ+×(φ) = 0. This suggests to deﬁne
the correlation functions ξ± = ξ++ ± ξ××, which are related to
the power spectrum through ξ+ = ξγ and
2π Pκ(l)J4(lφ) .
The principle of all measures for cosmic shear is the same:
They are integrals of the weak-lensing power spectrum times
ﬁlter functions describing the speciﬁc response of the measurement to the underlying power spectrum of density ﬂuctuations. The width of the ﬁlter functions controls the range
of density-perturbation modes ⃗k contributing to one speciﬁc
mode ⃗l of weak-lensing on the sky.
We can now estimate typical numbers for the cosmological
weak-lensing eﬀect. The power ∆κ in the weak-lensing quantities such as the cosmic shear is given by the power spectrum
Pκ(l) found in (230), times the volume in l-space,
∆κ(l) ≈l2Pκ(l) .
Assuming a cosmological model with Ωm0 = 0.3 and ΩΛ0 =
0.7, the CDM power spectrum and a reasonable source redshift distribution, ∆1/2
κ (l) is found to peak on scales l corresponding to angular scales 2π/l of 2′ . . . 3′, and the peak
reaches values of 0.04 . . . 0.05. This shows that cosmological
weak lensing will typically cause source ellipticities of a few
per cent, and correlations have a typical angular scale of a few
arc minutes. Details depend on the measure chosen through
the ﬁlter function.
B. Cosmic-shear measurements
1. Typical scales and requirements
How can cosmic gravitational lensing eﬀects be measured?
As shown in (226), the ellipticity of a hypothetic circular
source is an unbiased estimator for the shear. However, typical sources are not circular, but to ﬁrst approximation elliptical themselves. Thus, measuring their ellipticities yields their
intrinsic ellipticities in the ﬁrst place.
Let ϵ(s) be the intrinsic source ellipticity.
It is a twocomponent, spin-2 quantity. The cosmic shear adds to that
ellipticity, such that the observed ellipticity is
ϵ ≈ϵ(s) + γ
in the weak-lensing approximation. What is observed is therefore the sum of the signal, γ, and the intrinsic noise component
On suﬃciently deep observations, ≲20 galaxies per square
arc minute are routinely detected. Since the full moon has
half a degree diameter, it covers a solid angle of 152π = 700
square arc minutes, or ≲14, 000 of such distant, faint galaxies! From this point of view, the sky is covered by densely
patterned “wall paper” of distant galaxies. Thus, it is possible to average observed galaxy ellipticities. Assuming their
shapes are intrinsically independent, the intrinsic ellipticities
will average out, and the shear will remain,
⟨ϵ⟩≈⟨ϵ(s)⟩+ ⟨γ⟩≈⟨γ⟩.
It is a fortunate coincidence that the typical angular scale of
cosmic lensing, which we found to be of order a few arc minutes, is large compared to the mean distance between background galaxies, which is of order √1/20 ≈0.2′. This enables averages over background galaxies without cancelling
the cosmic shear signal. If γ varied on scales comparable to
or smaller than the mean galaxy separation, any average over
galaxies would remove the lensing signal as well.
The intrinsic ellipticities of the faint background galaxies
have a distribution with a standard deviation of σϵ ≈0.3.
Averaging over N of them, and assuming Poisson statistics,
yields expectation values of
⟨ϵ(s)⟩= 0 ,
δϵ = ⟨(ϵ(s))2⟩1/2 = σϵ
for the mean and its intrinsic ﬂuctuation.
A rough estimate for the signal-to-noise ratio of a cosmic
shear measurement can proceed as follows. Suppose the correlation function ξ is measured by counting pairs of galaxies
with a separation within δθ of θ. As long as θ is small compared to the side length of the survey area A, the number of
pairs will be
2 × n2A × 2πθ δθ ,
and thus the Poisson noise due to the intrinsic ellipticities will
where the factor of two arises because of the two galaxies involved in each pair. The signal is the square root of the correlation function ξ, which we can approximate as
ξ ≈l2Pκ(l)δ ln l ≈l2Pκ(l)δl
l ≈l2Pκ(l)δθ
where we have used in the last step that θ = 2π/l. Thus, the
signal-to-noise ratio is estimated to be
noise ≈ln δθ √πAPκ
Evidently, the signal-to-noise ratio, and thus the signiﬁcance
of any cosmic-lensing detection, grows with the survey area
and decreases with the intrinsic ellipticity of the source galaxies. In evaluating (245) numerically, we have to take into account that l2Pκ(l) must be a dimension-less number, which
implies that the power spectrum Pκ must have the dimension
steradian. Therefore, either the survey area A and the number
density n in (245) must be converted to steradians, or Pκ must
be converted to square arc minutes ﬁrst.
The signal-to-noise ratio increases approximately linearly
with scale. Assuming δθ/θ = 0.1, it is S/N ≈5 on a scale
of 0.5′ for a survey of one square degree area. This shows
that, if the cosmic shear should be measured on such small
scales with an accuracy of, say, ﬁve per cent, a survey area of
A ≈(20/5)2 ≈16 square degrees is needed since the signalto-noise ratio scales like the square root of the survey area.
On such an area, the ellipticities of 16 × 3600 × 30 ≈2 × 106
background galaxies have to be accurately measured.
Matters are more complicated in reality, but the ordersof-magnitude are well represented by this rough estimate.
Bearing in mind that typical ﬁelds-of-view of telescopes
large enough for detecting suﬃciently many faint background
galaxies reach one tenth of a square degree up to one square
degree, and that typical exposure times are of order one hour
for that purpose, the total amount of telescope time for a weaklensing survey of about 100 square degrees is estimated to be
several hundred telescope hours. With perhaps ﬁve hours of
telescope time per night, and perhaps half of the nights per
year usable, it is easy to see that the time needed for such
surveys is measured in months.
Since typical sizes of the faint background galaxies measure fractions of arc seconds, shape measurements require a
pixel resolution of, say, 0.1′′.
A total survey area of 100
square degrees must therefore be resolved into 100 × 3600 ×
3600/0.12 ≈1.3 × 1011 pixels.
These estimates neglect all sources of noise other than the
shot noise caused by the ﬁnite number of galaxies. On angular scales below a few arc minutes, the cosmic variance caused
by ﬁeld-to-ﬁeld variations in the shear signal due to the largescale cosmic structures must be added .
2. Ellipticity measurements
The determination of image ellipticities is straightforward
in principle, but diﬃcult in practice . Often, the surface-brightness quadrupole
I(⃗x)xix jd2x
is measured, from whose principal axes the ellipticity can be
read oﬀ. Real galaxy images, however, are typically far from
ideally elliptical. They are structured or otherwise irregular.
In addition, if they are small, they are coarsely resolved into
just a few pixels, so that only a crude approximation to the
integral in (246) can be found.
How the image of a point-like source, such as a star, appears on the detector is described by the point-spread function
(PSF). The PSF may be anisotropic if the telescope optics is
slightly astigmatic, and this anisotropy may, and will in general, depend on the location in the focal plane. The image is a
convolution of the ideal image shape prior to any distortion by
the atmosphere and the telescope optics. Any accurate measurement of image ellipticities requires a PSF correction or
deconvolution, for which the PSF must of course be known. It
is commonly measured oﬀthe images of stars in the ﬁeld.
Many other eﬀects may distort the PSF and thus the images
in systematic ways. For instance, if the CCD chips are not
exactly perpendicular to the optical axis of the telescope, or if
the individual chips of a CCD mosaic are not exactly coplanar, or if the telescope is slightly out of focus, systematic image deformations may result which typically vary across the
focal plane. They have to be measured and corrected. This
is commonly achieved by ﬁtting the measured PSF by loworder, two-dimensional polynomials on the focal plane. Since
part of the image distortions may depend on time due to thermal deformation, changing atmospheric conditions and such,
PSF corrections will also depend on time and have to be determined and applied with much care.
Even if the surface-brightness quadrupole of the image on
the detector can be accurately determined, the image appears
aﬀected by imperfections of the telescope optics and by the
turbulence in the atmosphere, the so-called seeing. Due to
the wave nature of light and the ﬁnite size of the telescope
mirror, the telescope will have ﬁnite resolution. The angular
resolution limit is given by (185). With λ ≈6 × 10−5 cm and
D = 400 cm, the angular resolution is ∆θ ≈0.04′′, much
smaller than needed for our purposes.
The turbulence of the Earth’s atmosphere eﬀectively convolves images with a Gaussian whose width depends on the
site, the weather and other conditions. Typical seeing ranges
around 1′′.
Under very good conditions, it can shrink to
∼0.5′′ or even less. Clearly, if an image of sub-arc second
size is convolved with a Gaussian of similar width, any ellipticity is substantially reduced.
Systematic eﬀects may remain which need to be detected
and quantiﬁed.
Any coherent image distortions caused by
gravitational lensing must be describable by the tidal gravitational ﬁeld, i.e. by second-order derivatives of a scalar potential. In analogy to the ⃗E-ﬁeld in electromagnetism, such
distortion patterns are called E-modes. Similarly, distortion
patterns which are described as the curl of a vector ﬁeld are
called B-modes. They cannot be due to gravitational lensing
and thus signal systematic eﬀects remaining in the data. Such
B-mode contaminations could recently be strongly reduced or
suppressed by improved algorithms for PSF correction . Absence of B-mode contamination does not allow the implication that the results are free of systematics,
though, because optical distortions also tend to create spurious E-modes.
3. Results
Despite the smallness of the eﬀect and the many diﬃculties in measuring it, much progress in cosmic-shear observations has been achieved in the past few years . Current and ongoing surveys,
in particular the Canada-France-Hawaii Legacy Survey , combined with well-
developed, largely automatic data-analysis pipelines, have
succeeded in producing cosmic-shear correlation functions
with very small error bars covering angular scales from below
an arc minute to several degrees. The best correlation functions could be shown to be at most negligibly contaminated
by B-modes.
The power spectrum Pκ(l) depends crucially on the nonlinear evolution of the dark-matter power spectrum. This, and
the exact redshift distribution of the background galaxies, are
the major uncertainties now remaining in the interpretation of
cosmic-shear surveys. Apart from that, the measured cosmicshear correlation functions agree very well with the theoretical
expectation from CDM density ﬂuctuations in a spatially-ﬂat,
low-density universe.
As (230) shows, the weak-lensing power spectrum Pκ(l)
depends on the product of a factor Ω2
m0 due to the Poisson
equation, times the amplitude A of the matter power spectrum. An additional weak dependence on cosmological parameters is caused by the geometric weight function ¯W(w′, w).
The cosmic-shear correlation function thus measures approximately the product AΩ2α
m0, α ≲1, which means that the amplitude of the power spectrum is nearly degenerate with the
matter density parameter. Only if it is possible to constrain
Ωm0 or A in any other way can the degeneracy be broken.
We shall see later how this may work. The amplitude of the
power spectrum A is conventionally described by a parameter σ2
8 which will be deﬁned and described in more detail in
§ X. Weak lensing thus constrains the product σ8Ωα
m0, and latest measurements ﬁnd σ8(Ωm0/0.25)0.64 ≈0.784 ± 0.043 .
Weak gravitational lensing is a fairly new ﬁeld of cosmological research.
Within a few years, it has considerably
matured and returned cosmologically interesting constraints.
Considerable potential is attributed to weak lensing in widearea surveys in particular when combined with photometric
redshift information, because this is expected to allow constraints on the growth of cosmic structures.
IX. SUPERNOVAE OF TYPE IA
A. Standard candles and distances
1. The principle
Before discussing supernovae of type Ia and their cosmological relevance, let us set the stage with a few illustrative
considerations. Suppose we had a standard candle whose luminosity, L, we knew precisely. Then, according to the deﬁnition of the luminosity distance in (20), the distance can be
inferred from the measured ﬂux, S , through
Besides the redshift z, the luminosity distance will depend on
the cosmological parameters,
Dlum = Dlum(z; Ωm0, ΩΛ0, H0, . . .) ,
All angular scales
Correlation function
Aperture−mass
FIG. 10 Constraints in the Ωm0−σ8 plane from weak lensing on large
angular scales in the CFHTLS. The Universe is assumed spatially ﬂat
here. )
which can in principle be used to determine cosmological parameters from a set of distance measurements from a class of
standard candles.
For this to work, the standard candles must be at a suitably high redshift for the luminosity distance to depend on the
cosmological model. As we have seen in (21), all distance
measures share the low-z limit
and lose their sensitivity to all cosmological parameters except
In reality, we rarely know the absolute luminosity L even of
cosmological standard candles. The problem is that they need
to be calibrated ﬁrst, which is only possible from a ﬂux measurement once the distance is known by other means, such as
from parallaxes in case of the Cepheids. Supernovae, however, which are the subject of this chapter, are typically found
at distances which are way too large to allow direct distance
measurements. Therefore, the only way out is to combine distant supernovae with local ones, for which the approximate
distance relation (249) holds.
Any measurement of ﬂux S i and redshift zi of the i-th standard candle in a sample then yields an estimate for the luminosity L in terms of the squared inverse Hubble constant,
Since all cosmological distance measures are proportional to
the Hubble length c/H0, the dependence on H0 on both sides
of (247) cancels, and the determination of cosmological parameters other than the Hubble constant becomes possible.
Thus, the ﬁrst lesson to learn is that cosmology from distant
supernovae requires a sample of nearby supernovae for calibration.
Of course, this nearby sample must satisfy the same criterion as the distance indicators used for the determination of
the Hubble constant: their redshifts must be high enough for
the peculiar velocities to be negligible, thus z ≳0.02. If the
redshifts are low enough for the linear approximation (249) to
hold, the interpretation of the nearby sample is independent of
the cosmological model.
It is important to note that it is not necessary to know the
absolute luminosity L even up to the uncertainty in H0. If
L is truly independent of redshift, cosmological parameters
could still be determined through (247) from the shape of the
measured relation between ﬂux and redshift even though its
precise amplitude may be unknown. It is only important that
the objects used are standard candles, but not how bright they
2. Requirements and degeneracies
Let us now collect several facts about cosmological inference from standard candles. Since we aim at the determination of cosmological parameters, say Ωm0, it is important to
estimate the accuracy that we can achieve from measurements
of the luminosity distance. Suppose we restrict the attention to
spatially ﬂat cosmological models, for which ΩΛ0 = 1 −Ωm0.
Then, because the dependence on the Hubble constant was
cancelled, Ωm0 is the only remaining relevant parameter. We
estimate the accuracy through ﬁrst-order Taylor expansion,
∆Dlum ≈dDlum
about a ﬁducial model, such as a ΛCDM model with Ωm0 =
At a ﬁducial redshift of z ≈0.8, we ﬁnd numerically
which shows that a relative distance accuracy of
is required to achieve an absolute accuracy of ∆Ωm0.
∆Ωm0 ≈0.02, say, distances thus need to be known to ≈1%.
This accuracy requires suﬃciently large supernova samples. Assuming Poisson statistics for simplicity and distance
measurements to N supernovae, the combined accuracy is
That is, an accuracy of ∆Ωm0 ≈0.02 can be achieved from
≈100 supernovae whose individual distances are known to
Anticipating physical properties of type-Ia supernovae,
their intrinsic peak luminosities in blue light are L ≈3.3 ×
1043 erg s−1, with a relative scatter of order 10%. 3 Given uncertainties ∆L in the luminosity L and ∆S in the ﬂux measurement S , error propagation on (247) yields the relative distance
uncertainty
Even if the ﬂux could be measured precisely, the intrinsic luminosity scatter currently forbids distance determinations to
better than 10%.
Fluxes have to be inferred from photon counts. For various reasons to be clariﬁed later, supernova light curves should
be determined until ∼35 days after the peak, when the luminosity has typically dropped to ≈2.5 × 1042 erg s−1. The
luminosity distance to z ≈0.8 is ≈5 Gpc, which implies
ﬂuxes S ≈1.1 × 10−14 erg s−1 cm−2 at peak and S ≈8.7 ×
10−16 erg s−1 cm−2 35 days later.
Dividing by an average photon energy of 5×10−12 erg, multiplying with the area of a typical telescope mirror with 4 m
diameter, and assuming a total quantum eﬃciency of 30%,
we ﬁnd detected photon ﬂuxes of S γ ≈85 s−1 at peak and
S γ ≈7 s−1 35 days afterwards. These ﬂuxes are typically distributed over a few CCD pixels.
Supernovae need to be identiﬁed against the background of
their host galaxies and the sky brightness. For distant supernovae, the sky brightness on the area of the supernova image
is typically 10–1000 times higher than the supernova itself,
dominating the noise budget. Let NSN and Nsky the number
of photons received from the supernova and the sky, respectively, on the supernovae image, and assume for simplicity
Nsky ≈100 NSN. Then, an estimate for the signal-to-noise ratio for the detection is
pNSN + Nsky
Signal-to-noise ratios of ≳10 up to 35 days after the maximum thus require NSN ≈104 photons from the supernova.
This implies exposure times of order 104/7 ≈1400 s, or
about 20 minutes.
Typical exposure times are thus of order (15 . . . 30) minutes to capture supernovae out to redshifts
z ∼1. Then, the photometric error around peak luminosity
is certainly less than the remaining scatter in the intrinsic luminosity, and relative distance accuracies of order 10% are
within reach.
However, a major diﬃculty is the fact that the identiﬁcation of type-Ia supernovae requires spectroscopy. Suﬃciently
accurate spectra typically require exposures lasting 1–3 hours
per spectrum on the world’s largest telescopes, such as ESO’s
Very Large Telescope which consists of four individual mirrors with 8 m diameter each.
3 As we shall see later, type-Ia supernovae are standardisable rather than
standard candles, and the standardising procedure is currently not able to
reduce the scatter further.
In order to see what we can hope to constrain by measuring
luminosity distances, we form the gradient of Dlum in the Ωm0-
ΩΛ0 plane,
at a ﬁducial ΛCDM model with Ωm0 = 0.3. When normalised
to unit length, it turns out to point into the direction
−0.76
This vector rotated by 90◦then points into the direction in
the Ωm0-ΩΛ0 plane along which the luminosity distance does
not change. Thus, near the ﬁducial ΛCDM model, the parameter combination
= −0.76 Ωm0 + 0.65 ΩΛ0
is degenerate.
The degeneracy direction, characterised by
the vector R(π/2)⃗g = (0.65, 0.76)T, encloses an angle of
arctan(0.76/0.65) = 49.5◦with the Ωm0 axis, almost along
the diagonal from the lower left to the upper right corner of
the parameter plane. Thus, it is almost perpendicular to the
degeneracy direction obtained from the curvature constraint
due to the CMB. This illustrates how parameter degeneracies
can very eﬃciently be broken by combining suitably diﬀerent
types of measurement. Moreover, combining supernova data
from a wide redshift range partially lifts the parameter degeneracy obtained from them already.
B. Supernovae
1. Types and classiﬁcation
Supernovae are “eruptively variable” stars. A sudden rise
in brightness is followed by a gentle decline.
unique events which at peak brightness reach luminosities
comparable to those of an entire galaxy, or (1010 . . . 1011) L⊙.
They reach their maxima within days and fade within several
Supernovae are traditionally characterised by their early
spectra . If hydrogen lines are missing, they
are of type I, otherwise of type II. Type-Ia supernovae show
silicon lines, unlike type-Ib/c supernovae, which are distinguished by the prominence of helium lines. Normal type-II
supernovae have spectra dominated by hydrogen. They are
subdivided according to their lightcurve shape into type-IIL
and type-IIP. Type-IIb supernova spectra are dominated by helium instead.
Except for type-Ia, supernovae arise due to the collapse of
a massive stellar core, followed by a thermonuclear explosion which disrupts the star by driving a shock wave through
it. Core-collapse supernovae of type-II arise from stars with
masses between (8 . . . 30) M⊙, those of type-I (i.e. types Ib/c)
from more massive stars .
Type-Ia supernovae, which we are dealing with here, arise
when a white dwarf is driven towards the Chandrasekhar mass
limit by mass overﬂowing from a companion star. In a binary
system, the more massive star evolves faster and can reach its
white-dwarf stage before its companion leaves the main sequence and becomes a red giant. When this happens, and the
stars are close enough, matter will ﬂow from the expanding
red giant on the white dwarf.
Electron degeneracy pressure can stabilise white dwarfs up
to the Chandrasekhar mass limit of ∼1.4 M⊙ . Because the core material is degenerate, its pressure is independent of its temperature. The mass accreted from the companion star increases
the pressure until nuclear burning can begin in isolated places
somewhere in the core.
The electron degeneracy is lifted,
the temperature rises dramatically, and the thermonuclear runaway sets in. Neutrinos produced in inverse beta decays carry
away much of the explosion energy unnoticed because they
can leave the supernova essentially without further interaction.
This thermonuclear runaway destroys the white dwarf.
Since this type of explosion (more precisely, deﬂagration) involves an approximately ﬁxed amount of mass, it is physically
plausible that the explosion releases a ﬁxed amount of energy.
Thus, the Chandrasekhar mass limit is the main responsible
for type-Ia supernovae to be approximate standard candles.
The nuclear fusion processes in type-Ia supernovae converts the carbon and oxygen in the core of the white dwarf into
56Ni, which later decays through 56Co into stable 56Fe. According to detailed numerical explosion models, the nuclear
fusion is started at random points near the centre of the white
dwarf .
The presence of silicon lines in the type-Ia spectra indicates
that not all of the white dwarf’s material is converted into 56Ni.
This shows that there is no explosion, but a deﬂagration, in
which the ﬂame front propagates at velocities below the sound
speed. The deﬂagration can burn the material fast enough if
it is turbulent, because the turbulence dramatically increases
the surface of the ﬂame front and thus the amount of material
burnt per unit time. Theoretical models predict that ∼0.5 M⊙
of 56Ni are typically produced.
The peak brightness is reached when the deﬂagration front
reaches the former white dwarf’s surface and drives it as a
rapidly expanding envelope into the surrounding space. The γ
photons released in the nuclear fusion processes are redshifted
by scattering oﬀthe expanding material and ﬁnally leave the
explosion site as X-ray, UV, optical and infrared photons.
Once the thermonuclear fusion has ended, additional energy is released by the β decay of 56Co into 56Fe with a half
life of 77.12 days. The exponential nature of the radioactive
decay causes the typical exponential decline phase in supernova light curves . Since the supernova light has to propagate through the
expanding envelope before we can see it, the opacity of the envelope and thus its metalicity are important for the appearance
of the supernova .
2. Observations
Since supernovae are transient phenomena, they can only
be detected by suﬃciently frequent monitoring of selected areas in the sky. Typically, ﬁelds are selected by their accessibility for the telescope to be used and the least degree of absorption by the Galaxy. Since a type-Ia supernova event lasts
for about a month, monitoring is required every few days.
Supernovae are then detected by diﬀerential photometry, in
which the average of all preceding images is subtracted from
the last image taken. Since the seeing varies, the images appear convolved with point-spread functions of variable width
even if they are taken with identical optics, thus the objects on
them appear more or less blurred. Before they can be meaningfully subtracted, they therefore have to be convolved with
the same eﬀective point-spread function.
This causes several complications in the later analysis procedure, in particular
with the photometry.
Of course, this detection procedure returns many variable
stars and supernovae of other types, which are not standard
candles and have to be removed from the sample.
Preselection of type-Ia candidates is done by colour and the lightcurve shape, but the identiﬁcation of type-Ia supernovae requires spectroscopy in order to identify the decisive silicon
lines at 6347 Å and 6371 Å. Since these lines move out of the
optical spectrum at redshifts z ≳0.5, near-infrared observations are crucially important for the high-redshift supernovae
relevant for cosmology.
Nearby supernovae, which are needed for calibration, reveal that type-Ia supernovae are not standard candles, but
show a substantial scatter in luminosity. It turned out that
there is an empirical relation between the duration of the supernova event and its peak brightness in that brighter supernovae last longer . This relation between
the light-curve shape and the brightness can be used to standardise type-Ia supernovae. It was seen as a major problem
for their cosmological interpretation that the origin for this relation was unknown, and that its application to high-redshift
supernovae was based on the untested assumption that the relation found and calibrated with local supernovae would also
hold there. Recent simulations indicate that the relation is
an opacity eﬀect : brighter supernovae produce more 56Ni and thus have a higher metalicity, which causes the envelope to be more opaque, the energy
transport through it to be slower, and therefore the supernova
to last longer.
Thus, before a type-Ia supernova can be used as a standard
candle, its duration must be determined, which requires the
light-curve to be observed over suﬃciently long time. It must
be taken into account here that the cosmic expansion leads
to a time dilation, due to which supernovae at redshift z appear longer by a factor of (1 + z). We note in passing that
the conﬁrmation of this time dilation eﬀect indirectly supports
the cosmic expansion. After the standardisation, the scatter
in the peak brightnesses of nearby supernovae is substantially
reduced. This encourages (and justiﬁes) their use as standardisable candles for cosmology.
The remaining relative uncertainty is now typically between
(10 . . . 15)% for individual supernovae.
Since, as we have
seen following (253), we require relative distance uncertainties at the per cent level, of order a hundred distant supernovae
are needed before meaningful cosmological constraints can be
placed, which justiﬁes the remark after (254).
An example for the several currently ongoing supernova
surveys is the Supernova Legacy Survey ) in the framework of the Canada-France-Hawaii Legacy
Survey (CFHTLS), which is carried out with the 4-m Canada-
France-Hawaii telescope on Mauna Kea.
It monitors four
ﬁelds of one square degree each ﬁve times during the 18 days
of dark time between two full moons (lunations).
Diﬀerential photometry is performed to ﬁnd out variables,
and candidate type-Ia supernovae are selected by light-curve
ﬁtting after removing known variable stars. Spectroscopy on
the largest telescopes (mostly ESO’s VLT, but also the Keck
and Gemini telescopes) is then needed to identify type-Ia supernovae. To give a few characteristic numbers, the SNLS has
taken 142 spectra of type-Ia candidates during its ﬁrst year of
operation, of which 91 were identiﬁed as type-Ia supernovae.
The light curves of these objects are observed in several
diﬀerent ﬁlter bands. This is important to correct for interstellar absorption. Any dimming by intervening material makes
supernovae appear fainter, and thus more distant, and will
bias the cosmological results towards faster expansion. Since
the intrinsic colours of type-Ia supernovae are characteristic,
any deviation of the observed from the intrinsic colours signals interstellar absorption which is corrected by adapting the
amount of absorption such that the observed is transformed
back into the intrinsic colour.
This correction procedure is expected to work well unless
there is material on the way which absorbs equally at all wavelengths, so-called “grey dust” . This could
happen if the absorbing dust grains are large compared to the
wavelength. Currently, it is quite diﬃcult to conclusively rule
out grey dust, although it is implausible based on the interstellar absorption observed in the Galaxy .
After applying the corrections for absorption and duration,
each supernova yields an estimate for the luminosity distance
to its redshift. Together, the supernovae in the observed sample constrain the evolution of the luminosity distance with
redshift, which is then ﬁt varying the cosmological parameters except for H0, i.e. typically Ωm0 and ΩΛ0. This yields an
“allowed” region in the Ωm0-ΩΛ0 plane compatible with the
measurements which is degenerate in the direction calculated
More information or further assumptions are necessary to
break the degeneracy. The most common assumption, justi-
ﬁed by the CMB measurements, is that the Universe is spatially ﬂat. Based upon it, the SNLS data yield a matter density
parameter of
Ωm0 = 0.263 ± 0.037 .
This is a remarkable result. First of all, it conﬁrms the other
independent measurements we have already discussed, which
were based on kinematics, cluster evolution and the CMB.
Second, it shows that, in the assumed spatially ﬂat universe,
SNLS 1st Year
Accelerating
Decelerating
No Big Bang
FIG. 11 Constraints in the Ωm0-ΩΛ0 plane obtained from type-Ia supernovae in the SNLS. ).
the dominant contribution to the total energy density must
come from something else than matter, possibly the cosmological constant.
It is important for the later discussion to realise in what way
the parameter constraints from supernovae diﬀer from those
from the CMB. The ﬂuctuations in the latter show that the
Universe is at least nearly spatially ﬂat, and the density parameters in dark and baryonic matter are near 0.24 and 0.04,
respectively. The rest must be the cosmological constant, or
the dark energy. Arising early in the cosmic history, the CMB
itself is almost insensitive to the cosmological constant, and
thus it can only constrain it indirectly.
supernovae,
angulardiameter distance during the late cosmic evolution, when the
cosmological constant is much more important.
shows, the luminosity distance constrains the diﬀerence between the two parameters,
ΩΛ0 = 1.17 Ωm0 + P ,
where the degenerate parameter P is determined by the measurement. Assuming ΩΛ0 = 1 −Ωm0 as in a spatially-ﬂat
universe yields
P = 1 −2.17 Ωm0 ≈0.43
from the SNLS ﬁrst-year result (260), illustrating that the survey has constrained the density parameters to follow the relation
ΩΛ0 ≈1.17 Ωm0 + 0.43 .
The relative acceleration of the universe, ¨a/a, is given by
(7), which can be simpliﬁed to read
if matter is pressure-less. Thus, the expansion of the universe
accelerates today (a = 1) if ¨a = H2
0(ΩΛ0 −Ωm0/2) > 0, or
ΩΛ0 > Ωm0/2. Given the measurement (263), the conclusion
seems inevitable that the Universe’s expansion does indeed
accelerate today .
If the Universe is indeed spatially ﬂat, then the transition
between decelerated and accelerated expansion happened at
1 −0.263 ≈0.263
a = 0.56 ,
or at redshift z ≈0.78. Luminosity distances to supernovae at
higher redshifts should show this transition, and in fact they
do .
3. Potential problems
The problem with possible grey dust has already been mentioned: While the typical colours of type-Ia supernovae allow the detection and correction of the reddening coming with
typical interstellar absorption, grey dust would leave no trace
in the colours and remain undetectable. However, grey dust
would re-emit the absorbed radiation in the infrared and add
to the infrared background, which is quite well constrained. It
thus seems that grey dust is not an important contaminant, if
it exists.
Gravitational lensing is inevitable for distant supernovae
 . Depending on the line-of-sight, they
are either magniﬁed or demagniﬁed. Since high magniﬁcations due to non-linear structures may occasionally happen,
the magniﬁcation distribution must be skewed towards demagniﬁcation to keep the mean of unit magniﬁcation. Thus,
the most probable magniﬁcation experienced by supernova is
below unity. In other words, lensing may lead to a slight demagniﬁcation if lines-of-sight towards type-Ia supernovae are
random with respect to the matter distribution. In any case,
the rms cosmic magniﬁcation adds to the intrinsic scatter of
the supernova luminosities. It may become signiﬁcant for redshifts z ≳1.
It is a diﬃcult and debated question whether supernovae
at high redshifts are intrinsically the same as at low redshifts
where they are calibrated. Should there be undetected systematic diﬀerences, cosmological inferences could be wrong.
In particular, it may be natural to assume that metalicities at
high redshifts are lower than at low redshifts. Since supernovae last longer if their atmospheres are more opaque, lower
metalicity may imply shorter supernova events, leading to underestimated luminosities and overestimated distances. Simulations of type-Ia supernovae, however, seem to show that
such an eﬀect is probably not signiﬁcant. For this and other
systematic eﬀects, see .
It was also speculated that distant supernovae may be intrinsically bluer than nearby ones due to their possibly lower
metalicity. Should this be so, the extinction correction, which
is derived from reddening, would be underestimated, causing
intrinsic luminosities to be under- and luminosity distances to
be overestimated. Thus, this eﬀect would lead to an underestimate of the expansion rate and counteract the cosmological
constant. There is currently no indication of such a colour
Supernovae of types Ib/c may be mistaken for those of type
Ia if the identiﬁcation of the characteristic silicon lines fails
for some reason. Since they are typically fainter than type-
Ia supernovae, they would contaminate the sample and bias
results towards higher luminosity distances, and thus towards
a higher cosmological constant. It seems, however, that the
possible contamination by non-type-Ia supernovae is so small
that it has no noticeable eﬀect.
Several more potential problems exist. It has been argued
for a while that, if the evidence for a cosmological constant
was based exclusively on type-Ia supernovae, it would probably not be considered entirely convincing. However, since
the supernova observations come to conclusions compatible
with virtually all independent cosmological measurements,
they add substantially to the persuasiveness of the cosmological standard model. Moreover, recent supernova simulations
reveal good physical reasons why they should in fact be reliable, standardisable candles.
X. THE NORMALISATION OF THE POWER SPECTRUM
A. Introduction
We saw in § VII.B that the measured power spectrum of the
galaxy distribution follows the CDM expectation in the range
of wave numbers where current large surveys allow its measurement. This range can be extended to some degree towards
smaller scales by measuring the autocorrelation of hydrogen
absorption lines in the spectra of distant quasars. Such observations of the power spectrum of the so-called Lyman-α forest
lines show that the power spectrum does indeed turn towards
the asymptotic behaviour ∝k−3 . In addition, we have seen that the peak location
agrees with the expectation for a universe with Ωm0 ≈0.3 and
h ≈0.72. This indicates that the CDM expectation for the
dark-matter power spectrum is indeed at least very close to its
real shape, which is a remarkable ﬁnding.
Although the shape of the power spectrum could thus be
quite well established, its amplitude still poses a surprisingly
obstinate problem. We shall see in this section why it is so
diﬃcult to measure. For this purpose, we shall discuss four
ways of measuring σ8; the amplitude of large-scale temperature ﬂuctuations in the CMB, the cosmic-shear autocorrelation
function, the abundance and evolution of the galaxy-cluster
population, and the statistics of Lyman-α forest lines.
For historical reasons, the amplitude of the dark-matter
power spectrum is characterised by the variance of the density
ﬂuctuations within spheres of 8 h−1Mpc radius. More generally, one imagines placing spheres of radius R randomly and
measuring the density-contrast variance within them. Since
the variance in Fourier space is characterised by the power
spectrum, it can be written as
(2π)3 Pδ(k)W2
where WR(k) is a window function selecting the k modes contributing to the variance within the spheres.
Imagining spheres of radius R in real space, the window
function should be the Fourier transform of a step function,
which is inconvenient because it extends to inﬁnite wave
numbers. It is thus more common to use either Gaussians,
since they Fourier transform to Gaussians, or step functions
in Fourier space. For simplicity of the following illustrative
calculations, we use the latter choice, thus
WR(k) = Θ(kR −k) = Θ
This is a step function dropping to zero at k = 2π/R. Inserting
this into (266) gives
2π2 Pδ(k) .
All modes larger than R contribute to the density ﬂuctuations
in spheres of radius R because all smaller modes average to
The normalisation of the power spectrum is usually
expressed in terms of σ8, ﬁxing R to its historical value of
8 h−1 Mpc.
B. Fluctuations in the CMB
1. The large-scale ﬂuctuation amplitude
We saw in § VI.B.3 that the long-wavelength (low-k) tail
of the CMB power spectrum is caused by the Sachs-Wolfe
eﬀect, giving rise to relative temperature ﬂuctuations (171) in
terms of the Newtonian potential ﬂuctuations δΦ. The threedimensional temperature-ﬂuctuation power spectrum is then
Pτ(k) = PΦ(k)
The Poisson equation in its form (219) implies that the
power spectra of potential- and density ﬂuctuations are related
PΦ(k) = 9H4
where the linear growth factor D+(arec) was introduced to relate the potential-ﬂuctuation power spectrum at the time of
decoupling to the present density-ﬂuctuation power spectrum
Now we need to account for projection eﬀects. A threedimensional mode with comoving wave number k and comoving wavelength λ = 2π/k appears under an angle θ = λ/wrec,
where wrec ≡w(arec) is the comoving angular-diameter distance (176) to the CMB. The angular wave number under
which the mode appears is thus
θ ≈wreck .
Expressing now the power spectrum (269) in terms of the
angular wave number l yields, with (270)
where the factor w−2
rec arises because of the transformation
from spatial to angular wave numbers l [cf. Limber’s equation
(228)], and the factor w4
rec/l4 expresses the factor k−4 from the
squared Laplacian. This shows that the angular power spectrum Pτ(l) of the large-scale CMB temperature ﬂuctuations
can only be translated into the amplitude of the dark-matter
power spectrum A if the cosmological parameters are already
known well enough.
A further complication is added by the integrated Sachs-
Wolfe eﬀect introduced in § VI.B.6. It depends on D+(a)/a
and adds secondary anisotropies to the CMB unless D+(a) =
a. The primordial CMB ﬂuctuations are then lower than measured and need to be corrected by subtracting the integrated
Sachs-Wolfe contribution, which adds a further dependence
on the cosmological parameters.
2. Translation to σ8
Two more complications arise in the translation of the largescale amplitude A to σ8. Since structures with wave numbers
larger than the peak location keq in the power spectrum contribute to σ8, the dependence of keq on the cosmological parameters comes in. Finally, the spectral index ns may aﬀect
the extrapolation from large to small scales substantially because of the long lever arm between the scales involved.
Of course, one could also use the small-scale part of the
CMB power spectrum for normalising the dark-matter power
spectrum. Due to the acoustic oscillations, however, this part
depends in a much more complicated way on additional cosmological parameters, such as the baryon density. Reading
σ8 oﬀthe low-order multipoles is thus a safer, albeit intricate
procedure.
Even if the cosmological parameters are now known well
enough to translate the low-order CMB multipoles to σ8,
an additional uncertainty remains. We know that, although
the Universe became neutral ∼400, 000 years after the Big
Bang, it must have been reionised after the ﬁrst stars and
other sources of UV radiation formed. Since then, CMB photons are travelling through ionised material again and experience Thomson (or Compton) scattering. The optical depth for
Thomson scattering is
neσT cdt ,
where ne is the number density of free electrons and σT is the
Thomson scattering cross section. After propagating through
the optical depth τ, the CMB ﬂuctuation amplitude is reduced
by exp(−τ).
Of course, the CMB photons cannot disappear through
Thomson scattering, thus the CMB’s overall intensity cannot
change in this way, but the ﬂuctuation amplitudes are lowered
in this diﬀusion process. The optical depth τ depends on the
path length through ionised material. In view of the CMB,
this means that the degree of ﬂuctuation damping depends on
the reionisation redshift, i.e. the redshift after which the cosmic baryons were transformed back into a plasma. Unless the
reionisation redshift is known, we cannot know by how much
the CMB ﬂuctuations were suppressed.
So far, the reionisation redshift can be estimated in two
ways. First, as discussed in § VI.B.4, Thomson scattering
creates linear polarisation. Of course, the polarisation due
to reionised material appears superposed on the primordial
polarisation, but on diﬀerent angular scales. The characteristic scale for secondary polarisation is the horizon size at
the reionisation redshift, which is much larger than the typical scales of the primordial polarisation. Thus, the reionisation redshift can be inferred from large-scale features in the
CMB polarisation, provided the cosmological parameters are
known well enough to translate angular scales into physical
scales . Modern cosmological parameter
estimates aim at ﬁtting the available data simultaneously for
all relevant parameters mostly by Monte-Carlo Markov chain
techniques, as explained in detail e.g. in the textbook by Durrer 2008.
Unfortunately, this is aggravated by the polarised microwave radiation from the Milky Way. Synchrotron and dust
emission can be substantially polarised and mask the CMB
polarisation, which can only be measured reliably if the foregrounds of Galactic origin can be accurately subtracted . Thus, the degree to which the foreground polarisation is known directly determines the accuracy of the σ8 parameter derived from the CMB ﬂuctuations. This contributes
considerably to the remaining uncertainty in the σ8 derived
from the 5-year WMAP data given in Tab. I.
The other way to constrain the reionisation redshift uses
the spectra of distant quasars. Light with wavelengths shorter
than the Lyman-α wavelength cannot propagate through neutral hydrogen because it is immediately absorbed.
Therefore, quasar spectra released before the reionisation redshift
must be completely absorbed blueward of the Lyman-α emission line. The appearance of this so-called Gunn-Peterson effect at high redshift thus signals
the transition from ionised into neutral material. Using this
technique, the reionisation was estimated to end at redshifts
∼6 . . . 8 , while the secondary polarisation of
the CMB implies a reionisation redshift of 11.0±1.4 if instantaneous reionisation is assumed . These
apparently discrepant values do not contradict each other because a small admixture of neutral hydrogen is enough to produce the Gunn-Peterson eﬀect, which therefore persists until
reionisation has completed.
C. Cosmological weak lensing
Compared to the outlined procedure to obtain σ8 from the
CMB, it appears completely straightforward to derive it from
the cosmic-shear measurements.
As we have discussed in
(VIII.B.3), the cosmic-shear power spectrum is proportional
m0 times the amplitude A of the dark-matter power spectrum, which leads to the approximate degeneracy Ωα
const. between σ8 and the matter-density parameter Ωm0.
A more subtle dependence on Ωm0 and to some degree also
on other cosmological parameters is introduced by the geometrical weight function ¯W(w′, w) shown in (231), and by the
growth of the power spectrum along the line-of-sight. This
slightly modiﬁes the form of the σ8-Ωm0 degeneracy, but does
not lift it. However, knowing Ωm0 well enough, we should
be able to read σ8 oﬀthe cosmic-shear correlation function.
However, there are three problems associated with that.
First, the cosmic shear measured on angular scales below
∼10′ is heavily inﬂuenced by the onset of non-linear structure
growth and the eﬀect this has on the dark-matter power spectrum. While the linear growth factor can be straightforwardly
calculated analytically, non-linear growth can only be quantiﬁed by means of large numerical simulations and recipes
derived from them . Insuﬃcient knowledge of the non-linear dark-matter
power spectrum is a major uncertainty in the cosmological interpretation of cosmic shear.
Second, the amplitude of cosmological weak-lensing effects depends on the redshift distribution of the sources used
for measuring ellipticities. Since these background galaxies
are typically very faint, it is demanding to measure their redshifts. Two methods have typically been used. One adapts the
known redshift distribution of sources in narrow, very deep
observations such as the Hubble Deep Field to the characteristics of the observation to be analysed. The other relies on
photometric redshifts, i.e. redshift estimates based on multiband photometry. Yet, the precise redshift distribution of the
background sources adds additional uncertainty to estimates
Third, it is possible that systematic eﬀects remain in weaklensing measurements because the eﬀect is so small, and many
corrections have to be applied to measured ellipticities before
the cosmic shear can be extracted. Advanced correction methods have been developed which made the B-mode contamination almost or completely disappear. This is good news, but it
does not yet guarantee the absence of other systematic eﬀects
in the data.
Nonetheless, cosmic lensing, combined with estimates of
the matter-density parameter, is perhaps the most promising
method for precisely determining σ8. Table II lists values of
σ8 derived from some cosmic-shear measurements under the
assumption of Ωm0 = 0.30 in a spatially-ﬂat universe.
 
 
0.72 ± 0.09
 
0.97 ± 0.13
 
1.02 ± 0.16
 
0.83 ± 0.07
Virmos-Descart
 
0.85 ± 0.06
CFHTLS-wide
 
0.80 ± 0.1
 
0.74 ± 0.04
100 sq. deg. combined
 
0.70 ± 0.04
CFHTLS-wide
 
TABLE II Values for σ8 derived from cosmic-shear measurements
under the assumption of a spatially-ﬂat universe with Ωm0 = 0.3.
The systematic deviations between them are probably due
to three diﬀerent uncertainties. First, the redshift distribution
of the background sources needs to be known; second, nonlinear evolution of the matter power spectrum need to be accurately modelled; and third, all kinds of PSF distortions need
to be corrected.
D. Galaxy clusters
1. The mass function
Based on the assumption that the density contrast is a Gaussian random ﬁeld and the spherical-collapse model, derived a mass function for dark-matter halos. It compares the standard deviation σR of the density-
ﬂuctuation ﬁeld to the linear density-contrast threshold δc ≈
1.686 for collapse in the spherical-collapse model. The mean
mass contained in spheres of radius R sets the halo mass,
which brings the mean (dark-) matter density ¯ρ into the game.
The standard deviation σR is related to the power spectrum.
For convenience, we introduce an eﬀective slope
n = d ln P(k)
for the power spectrum, which will of course be scaledependent. On large scales, n ≈1, while n →−3 on small
scales, i.e. for small halo masses. For galaxy clusters, n ≈−1.
We introduce the non-linear mass scale M∗as the mass contained in spheres of radius R chosen such that σR = 1. Since
σR grows with the linear growth factor D+(a), the non-linear
mass grows with time. It is convenient here to express the amplitude of the power spectrum, and thus σ8, in terms of M∗. In
suﬃcient approximation,
In terms of the dimensionless mass m ≡M/M∗, the Press-
Schechter mass function can be cast into the form
N(m, a)dm =
M2∗D+(a)αmα−2 exp
The Press-Schechter mass function, and some improved
variants of it ,
have been spectacularly conﬁrmed by numerical simulations
 . It shows that the mass function is
a power law with an exponential cut-oﬀnear the non-linear
mass scale M∗. For galaxy clusters, n ≈−1, thus α ≈1/3, and
N(m, a)dm ∝m−5/3 exp
with an amplitude characterised by M∗, the mean dark-matter
density ¯ρ, and the growth factor D+(a).
This opens a way to constrain cosmological parameters as
well as σ8 with galaxy clusters: if the abundance and evolution of the cluster mass function can be measured, they can
be determined from the mass scale of the exponential cut-oﬀ
and the amplitude of the power-law end. Today, the non-linear
mass scale is a few times 1013 M⊙. Therefore, the exponential
cut-oﬀin the halo mass will not be seen in the galaxy mass
function. Clusters, however, show a pronounced exponential
cut-oﬀ , and thus their population is very
sensitive to changes in σ8. In principle, therefore, σ8 should
be very well constrained by the cluster population.
2. What is a cluster’s mass?
The main problem here is how observable cluster properties should be related to quantities used in theory. Cluster
masses, as used in the theoretical mass function (277), are not
observables. Cluster observables are the X-ray temperature
and ﬂux, the optical luminosity and the velocity distribution
of their galaxies, and their gravitational-lensing eﬀects. Before we discuss their relation to mass, let us ﬁrst see what the
“mass of a galaxy cluster” could be.
It is easy to deﬁne masses of gravitationally bound, well
localised objects, such as planets or stars. They have a welldeﬁned boundary, e.g. the planetary surfaces or the stellar
photospheres.
This is markedly diﬀerent for objects like
galaxies and galaxy clusters. As far as we know, their densities drop smoothly towards zero like power laws, ∝r−(2...3).
Thus, although they are gravitationally bound, it is less obvious what should be seen as their outer boundary. Strictly
speaking, there is none.
The only way out is then to deﬁne an outer boundary in
such a way that it is well-deﬁned in theory and identiﬁable
in observational data. A common choice was introduced in
§ V.A.2: it deﬁnes the boundary by the mean overdensity it
encloses. Although this is problematic as well, it may be as
good as it gets. Three immediately obvious problems created
by this deﬁnition are that objects like galaxy clusters are often
irregularly shaped rather than spherical, that the overdensity
of 200 is as arbitrary as any other, even if it is inspired by
virial equilibrium in the spherical-collapse model, and that its
measurement requires a suﬃciently accurate density proﬁle to
be known or assumed.
How could standardised radii such as R200 be measured?
This could for instance be achieved applying equations such
as (147) after measuring the slope β and the core radius of
the X-ray surface brightness proﬁle together with the X-ray
temperature, by calibrating an assumed density proﬁle with
galaxy kinematics based on the virial theorem, or by constraining the cluster mass proﬁle with gravitational lensing.
Obviously, all these measurements have their own problems. Being sensitive to all mass along the line-of-sight, gravitational lensing cannot distinguish between mass bound to a
cluster or just projected onto it. Any measurement based on
the virial theorem must of course rely on virial equilibrium,
which takes time to be established and is often perturbed in
real clusters because of merging and accretion. The common
interpretation of X-ray measurements requires the assumption
that the X-ray gas be in hydrostatic equilibrium with the host
cluster’s gravitational potential.
This illustrates that it may be fair to say that there is no such
thing as the mass of a galaxy cluster. Even if measurements of
cluster “radii” were less dubious, it remained unclear whether
they mean the same as those assumed in theory, which are related to the spherical-collapse model. Interestingly, but not
surprisingly, cluster masses obtained from numerical simulations suﬀer from the same poor deﬁnition of the concept of a
“cluster radius”.
How can we make progress then? Observables such as the
cluster temperature TX or its X-ray luminosity LX should be
related to the depth of the gravitational potential they are embedded in, which should in turn be related to some measure of
the total mass. If we can calibrate such expected temperaturemass or luminosity-mass relations, e.g. using numerical simulations of galaxy clusters, a direct comparison between theory
and observations seems possible. This is sometimes called an
external calibration of the required relations.
Internal or self-calibrations, i.e. calibrations based on cluster data alone, have become increasingly fashionable over the
past years. Here, empirical temperature-mass and luminositymass relations are obtained based on one or more of the mass
estimates sketched above.
1.02 ± 0.07
M-T relation
 
0.77 ± 0.07
M-T relation
 
M-L relation
B¨ohringer, 2002)
0.75 ± 0.16
lensing masses
 
luminosity function
 
temperature function
 
0.69 ± 0.03
lensing masses
 
0.78 ± 0.17
optical richness
 
lensing masses
 
TABLE III Values of σ8 derived from the galaxy-cluster population
based on diﬀerent observational data.
The result of both procedures is qualitatively the same. It
allows the conversion of observables to mass, and thus of the
observed cluster temperature or luminosity functions to mass
functions, which can then be compared to theory. The shape
and amplitude of the power spectrum and the growth factor
can then be adapted to optimise the agreement between observed and expected mass functions. Clusters at moderate or
high redshift constrain the evolution of the mass function and
allow an independent estimate of the matter-density parameter
Ωm0, as sketched in § V.C before.
In view of the many diﬃculties listed, it is an astonishing
fact that, when applied to cluster samples rather than individual clusters, the determination of the cluster mass function
and its evolution seems to work quite well. Values for σ8
derived therefrom are given in Tab. III. Systematic diﬀerences
between these values are most likely due to the uncertainties
of the calibration procedures applied to the relations between
cluster masses and observables.
E. The Lyman-α forest
The Lyman-α forest lines mentioned before arise from absorption in neutral-hydrogen clouds. It is reasonable to assume that they are located where the dark matter is overdense,
such that ﬂuctuations in the neutral-hydrogen density are proportional to the dark-matter density contrast δ. Then, observations of the Lyman-α forest lines, in particular their number
per unit redshift, equivalent-width distribution and correlation
properties, must contain information on the shape and the normalisation of the dark-matter power spectrum.
Retrieving this information would be straightforward if
the biasing relation between the neutral-hydrogen and darkmatter densities was known and simple, the gas temperature
was known, redshifts were unaﬀected by peculiar motions,
and non-linear structure evolution could be neglected. While
this is not the case in reality, these perturbing eﬀects can be
calibrated with numerical simulations and corrected. Two different methods have been proposed; one inverts the Lyman-α
forest directly , while the other
adapts power spectra of numerical simulations to those recovered from Lyman-α forest observations .
Both methods have allowed interesting constraints which are
summarised in Tab. IV.
0.73 ± 0.04
Keck spectra
 
0.94 ± 0.08
LUQAS sample, Keck
 
0.91 ± 0.07
SDSS spectra
0.92 ± 0.09
LUQAS sample
 
TABLE IV Selection of σ8 values obtained from Lyman-α forest
data alone.
The method is young, but promising. Values of σ8 obtained
so far seem to be typically systematically higher than those derived from other observables. Possible sources of systematic
error are the onset of nonlinear structure growth and, perhaps
most importantly, the temperature of the hydrogen gas and its
relation to the gas density (often called its equation of state).
XI. INFLATION AND DARK ENERGY
A. Cosmological inﬂation
1. Motivation
In the preceding chapters, we have seen the remarkable
success of the cosmological standard model, which is built
upon the two symmetry assumptions underlying the class of
Friedmann-Lemaˆıtre-Robertson-Walker models which experienced a Big Bang a ﬁnite time ago. We shall now discuss a
fundamental problem of these models, and a possible way out.
Historically, the problem was raised in a diﬀerent way, but it
intrudes with the very straightforward realisation that it is by
no means obvious why the CMB should appear as isotropic as
it is, and why there should be large coherent structures in it.
Let us begin with the so-called comoving particle horizon,
which is the comoving distance that light can travel between
the Big Bang and time t. It is given by (175) with the sound
speed cs replaced by the light speed c,
a = 2c √arec
= 282.8 Mpc ,
with α ≈0.33 as deﬁned below (164). On the other hand, we
have seen in (176) that the comoving angular-diameter distance to the CMB is
w(arec) = 3.195 c
which implies that the angular size of the particle horizon is
w(arec) ≈1.14◦
The physical meaning of the particle horizon is that no
event between the Big Bang and recombination can exert
any inﬂuence on a given particle if it is more than the horizon length away. Our simple calculation thus shows that we
can understand how causal processes could establish identical physical conditions in patches of the sky with about one
degree radius. Points on the CMB separated by larger angles
were never causally connected before the CMB was released.
It is therefore not at all plausible how the CMB could have attained almost the same temperature across the entire sky! The
simple fact that the CMB is almost entirely isotropic across
the sky thus poses a problem which the standard cosmological
model is apparently unable to solve. Moreover, the formation
of coherent structures larger than the particle horizon remains
mysterious. This is one way to state the horizon problem. It
is sometimes called the causality problem: How can coherent
structures in the CMB be larger than the particle horizon was
at recombination?
Another uncomfortable problem of the standard cosmological model is the ﬂatness, or at least the near-ﬂatness, of spatial
hypersurfaces of our Universe. To see this, we write Friedmann’s equation (6) in the form
H2(a) = H2(a)
Ωtotal(a) −Kc2
which is equivalent to
Ωtotal(a) −1 = Kc2
The right-hand side typically grows as some power of the time
(a2H2)−1 = ˙a−2 ∝t β ;
e.g. β = 2/3 in a matter-dominated universe without cosmological constant. Then,
Ωtotal(a) −1 ∝t β .
This shows that any deviation of the total density parameter
Ωtotal from unity tends to grow with time. Thus, (spatial) ﬂatness is an unstable property. If it is not very precisely ﬂat in
the beginning, the Universe will develop away from ﬂatness.
Since we know that spatial hypersurfaces are now almost ﬂat,
|Ωtotal(a) −1| ≲1% (cf. Tab. I), the deviation from ﬂatness
must have been of order
|Ωtotal(arec) −1| ≲1% ×
or ten parts per million at the time of recombination. Clearly,
this requires enormous ﬁne-tuning. This is called the ﬂatness
problem: How can we understand ﬂatness in the late universe
without assuming an extreme degree of ﬁne-tuning at early
2. The idea of inﬂation
Since c/H is the Hubble radius, the quantity rH ≡c/(aH) is
the comoving Hubble radius. According to (283), it typically
grows with time as rH ∝t β/2. Since we can write (282) as
Ωtotal(a) −1 = Kr2
this is equivalent to the ﬂatness problem.
This motivates the idea that at least the ﬂatness problem
may be solved if the comoving Hubble radius could, at least
for some suﬃciently long period, shrink with time. If that
could be arranged, any deviation of Ωtotal(a) from unity would
be driven towards zero.
Conveniently, such an arrangement would also remove or
at least alleviate the causality problem. Possibly, the Hubble
radius, characterising the radius of the observable universe,
could be driven inside the horizon and thus move the entire observable universe into a causally-connected region. When the
hypothesised epoch of a shrinking comoving Hubble radius is
over, it starts expanding again, but if the reduction was suf-
ﬁciently large, it could remain within the causally-connected
region at least until the present.
How could such a shrinking comoving Hubble radius be
arranged? Obviously, we require
(aH)2 (˙aH + a ˙H) = −
(aH)2 < 0 ,
which is possible if and only if ¨a > 0, in other words, if the
expansion of the Universe accelerates. This appears counterintuitive because the cosmic expansion is dominated by gravity, which should be attractive and thus necessarily decelerate
the expansion. Friedmann’s equation (7) implies the matter
ρc2 + 3p < 0
In other words, cosmic acceleration is possible if and only if
the dominant ingredient of the cosmic ﬂuid has suﬃciently
negative pressure. Equation (8) implies the density evolution
for a cosmic ﬂuid satisfying (288), showing that its density
would fall oﬀﬂatter than a−2, and thus less steeply than the
matter or radiation densities.
Thus, once a component of
the cosmic ﬂuid with suﬃciently negative pressure reaches a
density comparable to the densities of matter or radiation, it
quickly starts dominating the cosmic expansion. In the limiting case ˙ρ = 0 or p = −ρc2, (7) reduces to
3 ρ = const. > 0 ,
which implies the exponential expansion or inﬂation
3. Slow roll, structure formation, and observational constraints
We have seen that we need inﬂation to solve the ﬂatness and
causality problems, and inﬂation needs a form of matter with
negative pressure. What could that be? Consider a scalar ﬁeld
φ with a self-interaction potential V(φ). Then, ﬁeld theory
shows that pressure and density of the scalar ﬁeld are related
by the equation of state
pφ = wρφc2
Evidently, negative pressure is possible if the kinetic energy of
the scalar ﬁeld is suﬃciently smaller than its potential energy.
For the cosmological-constant case, ˙φ = 0, we have w = −1
or p = −ρc2, in agreement with the conclusion from (289).
In other words, a suitably strongly self-interacting scalar ﬁeld
has exactly the properties we need.
Inﬂation, i.e. accelerated expansion, broadly requires ˙φ2 to
be suﬃciently smaller than V. Moreover, we need inﬂation
to operate long enough to drive the total matter density parameter suﬃciently close to unity for it to remain there to the
present day. These two conditions are conventionally cast into
 ). They are called the slow-roll conditions. The ﬁrst
assures that inﬂation can set in, because if it is satisﬁed, the
potential has a small gradient and cannot drive rapid changes
of the scalar ﬁeld. The second restricts the curvature of the
potential and thus assures that the inﬂationary condition is satisﬁed long enough.
Estimates show that inﬂation needs to expand the Universe
by ∼50 . . . 60 e-foldings (i.e. by a
factor of e50...60) for it to solve the causality and ﬂatness problems. Inﬂation ends once the slow-roll conditions are violated. By then, the Universe will have become extremely cold.
While the density of the inﬂaton ﬁeld φ will be approximately
the same as at the onset of inﬂation (as for the cosmological
constant, this is a consequence of the negative pressure), all
other matter and radiation ﬁelds will have their energy densities lowered by factors of a−3...4, i.e. by ≲100 orders of magnitude.
Once ϵ approaches unity, the kinetic term ˙φ2 will dominate
the potential, and the scalar ﬁeld will start oscillating rapidly.
It is assumed that the scalar ﬁeld then decays into ordinary
matter which ﬁlls or reheats the Universe after inﬂation is
It is an extremely interesting aspect of inﬂation that it
also provides a mechanism for seeding structure formation
 . As any other quantum ﬁeld,
the inﬂaton ﬁeld φ must have undergone vacuum oscillations
because the zero-point energy of a quantum harmonic oscillator cannot vanish due to Heisenberg’s uncertainty principle.
These vacuum oscillations cause the spontaneous creation and
annihilation of particle-antiparticle pairs. Once inﬂation sets
in, vacuum ﬂuctuation modes are quickly driven out of the
horizon and lose causal connection. Then, they cannot decay any more and “freeze in”. Thus, inﬂation introduces the
breath-taking notion that density ﬂuctuations in our Universe
today may have been seeded by vacuum ﬂuctuations of the
inﬂaton ﬁeld before inﬂation set in and enlarged them to cosmological scales.
This idea has precisely quantiﬁable consequences . First, linear density ﬂuctuations created
by the inﬂaton ﬁeld must be adiabatic and, by the central limit
theorem, they should form a Gaussian random ﬁeld. This is
because they arise from incoherent superposition of extremely
many independent ﬂuctuation modes whose amplitude and
wave number are all drawn from the same probability distribution. Under these circumstances, the central limit theorem
shows that the result, i.e. the superposition of all these modes,
must be a Gaussian random ﬁeld.
Second, it implies that the statistics of density ﬂuctuations
in the Universe today must be explicable by the statistics of
vacuum ﬂuctuations in a scalar quantum ﬁeld.
This is indeed the case. The power spectrum resulting from this consideration is very close to the scale-free Harrison-Zel’dovich-
Peebles shape introduced in § I.C.2,
Pδ(k) ∝kns ,
with ns ≈1. The spectral index ns would be precisely unity
if inﬂation lasted forever. Since this was obviously not so,
ns must deviate slightly from unity, and detailed calculations
show that it must be slightly smaller ,
ns = 1 + 2η −6ϵ .
The latest WMAP measurements do in
fact show that
ns = 0.960+0.014
cf. Tab. I. The completely scale-invariant spectrum, ns = 1, is
thus excluded at more than 3σ. The measured deviation of ns
from unity also restricts the number N of e-foldings completed
by inﬂation. Under fairly general assumptions,
N = 54 ± 7
 based on the three-year WMAP data.
Another prediction of inﬂation is that it may excite not only
scalar, but also tensor perturbations .
Scalar perturbations lead to the density ﬂuctuations, tensor
perturbations correspond to gravitational waves. Vector perturbations do not play any role because they decay quickly as
the universe expands. Simple models of inﬂation predict that
the ratio r between the amplitudes of tensor and scalar perturbations, taken in the limit of small wave numbers, is
An inﬂationary background of gravitational waves is in principle detectable through the polarisation of the CMB . Limits of order r ≲0.05 are expected
from the upcoming Planck satellite . Together with the result ns , 1 from
WMAP, it will then be possible to constrain viable inﬂation
models, i.e. to constrain the shape of the inﬂaton potential.
B. Dark energy
1. Motivation
The CMB shows us that the Universe is at least nearly spatially ﬂat. Constraints from kinematics, from cluster evolution
and from the CMB show that the matter density alone cannot
be responsible for ﬂattening space, and primordial nucleosynthesis and the CMB show that baryons contribute at a very
low level only. Something is missing, and it even dominates
today’s cosmic ﬂuid.
From structure formation, we know that this remaining constituent cannot clump on the scales covered by the galaxy surveys and below. It is thus diﬀerent from dark matter. The
terms dark energy, kosmon or quintessence have been coined
for it . The
type-Ia supernovae tell us that it behaves at least very similar
to a cosmological constant in the recent cosmic past. Maybe
the dark energy is a cosmological constant?
Nothing currently indicates any deviation from this “simplest” assumption . So far, the cosmological constant is a perfectly viable description for all observational evidence we have. However, this is deeply dissatisfactory from
the point of view of theoretical physics. The problem is the
value of ΩΛ0. As we have seen above, a self-interacting scalar
ﬁeld with negligible kinetic energy behaves like a cosmological constant. Then, its density should simply be given by its
potential V. Simple arguments suggest that V should be the
Planck mass divided by the third power of the Planck length,
which turns out to be 120 orders of magnitude larger than the
cosmological constant derived from observations. Since this
fails, it seems natural to expect that the cosmological constant
should vanish, but it does not. The main problem with the
cosmological constant is therefore, why is it not zero if it is so
Anthropic arguments suggest that we observe the Universe
as it is because it needs to be that way for us to exist. The
Universe needs to be old enough for stars to have produced
carbon, on which life must be based to the best of our knowledge. Yet, it must not be too old for the majority of stars
to have evolved past their main-sequence life for the stability of benign conditions for life. If the cosmological constant
was much larger, inﬂationary expansion would have prohibited structure formation and thus also the formation of life. It
may thus be that the cosmological parameters have the values
we measure because otherwise we would not exist to measure
The explanation of inﬂation by means of an inﬂaton ﬁeld
suggests another way out. As we have seen there, accelerated
expansion can be driven by a self-interacting scalar ﬁeld while
its potential energy dominates. Moreover, it can be shown that
if the potential V has an appropriate shape, the dark energy has
attractor properties in the sense that a vast range of initial density values can evolve towards the same value today . At this point at the latest, we leave the realm
of what may be considered the established cosmological standard model, and enter into the very controversial discussion
of dark energy.
2. Observational constraints?
If the dark energy is indeed dynamical and provided by a
self-interacting scalar ﬁeld, how can we ﬁnd out more about
it? Reviewing the cosmological measurements we have discussed so far, it becomes evident that they are all derived from
constraints on
• cosmic time, as in the age of the Galaxy or of globular
clusters, or in primordial nucleosynthesis;
• distances, as in the spatial ﬂatness derived from the
CMB, the type-Ia supernovae or the geometry of cosmological weak lensing; or
• the growth of cosmic structures, as in the acoustic oscillations in the CMB, the evolution of the cluster population, the structures in the galaxy distribution or the
source of cosmological weak-lensing eﬀects.
We must therefore seek to constrain the dark energy by
measurements of distances, times, and structure growth. Since
they can all be traced back to the expansion behaviour of the
universe as described by Friedmann’s equation, we must see
how the dark energy enters there, and what eﬀects it can seed
through it.
Let us therefore assume that the dark energy is a suitably
self-interacting, homogeneous scalar ﬁeld. Then, its pressure
can be described by
p = w(a)ρc2 ,
where the equation-of-state parameter w is some function of a.
According to (288), accelerated expansion needs w < −1/3,
and the cosmological constant corresponds to w = −1. Since
all cosmological measurements to date are in agreement with
the assumption of a cosmological constant, we need to arrange things such that w →−1 today. Suppose we had some
function w(a), which could either be obtained from a phenomenological choice, a model for the self-interaction potential V(φ) through (292) or from a suitable ad-hoc parameterisation. Then, (289) implies
ρ = −3(1 + w) ˙a
ρ(a) = ρ0 exp
[1 + w(a′)]da′
≡ρ0 f(a) .
For constant w, this simpliﬁes to
ρ(a) = ρ0 exp [−3(1 + w) ln a] = ρ0a−3(1+w) .
If w = −1, we recover the cosmological-constant case ρ =
ρ0 = const., for pressure-less material, w = 0 and ρ ∝a−3,
and for radiation, w = 1/3 and ρ ∝a−4.
Therefore, we can take account of the dynamical dark energy by replacing the term ΩΛ0 in the Friedmann equation (10)
by ΩDE0 f(a), and the expansion function E(a) turns into
Ωr0a−4 + Ωm0a−3 + ΩDE0 f(a) + ΩK0a−2i1/2 , (303)
where ΩK0 = 1 −Ωr0 −Ωm0 −ΩDE0 is a density parameter
assigned to the spatial curvature.
We thus see that the equation-of-state parameter enters the
expansion function in integrated form. Since all cosmological
observables are integrals over the expansion function, including the growth factor D+(a), this implies that cosmological
observables measure integrals over the integrated equation-ofstate function w(a). Note that in cosmological models with
dynamical dark energy, the growth factor is more complicated than described by Eq. (23) because dark-energy clustering needs to be taken into account . Needless to say, the dependence of cosmological measurements on the exact form of w(a) will be extremely weak,
which in turn implies that extremely accurate measurements
will be necessary for constraining the nature of the dark energy.
d ln D+(z)/d w, d ln Dlum(z)/d w
redshift z
d ln D+(z)/d w
d ln Dlum(z)/d w
FIG. 12 Relative changes of the growth factor D+ and the luminosity
distance Dlum with w as functions of redshift, assuming constant w.
In order to illustrate the required accuracies, let us consider
by how much the angular-diameter distance and the growth
factor change compared to ΛCDM upon changes in w away
d ln Dang(z)
d ln D+(z)
as a function of redshift z. Assuming Ωm0 = 0.3 and ΩΛ0 =
0.7, we ﬁnd typical values between −0.1 and −0.2 at most.
Since we currently expect deviations of w from −1 at most
at the ∼10% level, accurate constraints on the dark energy
require relative accuracies of distances and the growth factor
at the per-cent level.
Clearly, all models for dynamical dark energy proposed so
far are purely phenomenological. The most straightforward
question to be addressed in this situation may be whether a
cosmological constant can be ruled out in some way. All suitable cosmological information will need to be combined in
order to progress.
Currently, the largest hope is put on the BAOs (see
§ VII.A.6) and on so-called tomographic measurements.
BAOs have a characteristic physical scale whose angular scale
depends on the angular-diameter distance, which is in turn
sensitive to the dark energy. Tomography attempts to trace
the evolution of structures throughout cosmic history . An example is given by weak gravitational lensing:
Since its geometrical sensitivity peaks approximately halfway between the sources and the observer, sources at higher
redshift also probe more distant, and thus less evolved, cosmic
structures. If lensing eﬀects can be measured for sub-samples
of sources in diﬀerent redshift shells, the growth factor can be
probed diﬀerentially. First examples for this technique have
been published. They give rise to the expectation that clarifying the nature of the dark energy may indeed be feasible in the
near future.
Acknowledgements
I am most grateful to many colleagues for inspiring and
clarifying discussions, in particular to Peter Schneider, Achim
Weiß and Simon White, whose detailed comments helped improving this review substantially.
Three anonymous referees contributed numerous thoughtful and constructive comments. This work was supported in part by the German Science Foundation (DFG) through the Collaborative Research
Centres SFB 439 and TRR 33.