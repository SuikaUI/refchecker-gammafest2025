HAL Id: hal-02369818
 
Submitted on 19 Nov 2019
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
DeepCog: Optimizing Resource Provisioning in Network
Slicing with AI-based Capacity Forecasting
Dario Bega, Marco Gramaglia, Marco Fiore, Albert Banchs, Xavier
Costa-Perez
To cite this version:
Dario Bega, Marco Gramaglia, Marco Fiore, Albert Banchs, Xavier Costa-Perez. DeepCog: Optimizing Resource Provisioning in Network Slicing with AI-based Capacity Forecasting. IEEE Journal on
Selected Areas in Communications, 2019. ￿hal-02369818￿
DeepCog: Optimizing Resource Provisioning
in Network Slicing with AI-based Capacity
Forecasting
Dario Bega, Marco Gramaglia, Marco Fiore, Senior Member, IEEE, Albert Banchs Senior Member, IEEE, and
Xavier Costa-Perez Senior Member, IEEE,
Abstract—The dynamic management of network resources is
both a critical and challenging task in upcoming multi-tenant
mobile networks, which requires allocating capacity to individual network slices so as to accommodate future time-varying
service demands. Such an anticipatory resource conﬁguration
process must be driven by suitable predictors that take into
account the monetary cost associated to overprovisioning or
underprovisioning of networking capacity, computational power,
memory, or storage. Legacy models that aim at forecasting
trafﬁc demands fail to capture these key economic aspects of
network operation. To close this gap, we present DeepCog,
a deep neural network architecture inspired by advances in
image processing and trained via a dedicated loss function.
Unlike traditional trafﬁc volume predictors, DeepCog returns
a cost-aware capacity forecast, which can be directly used by
operators to take short- and long-term reallocation decisions that
maximize their revenues. Extensive performance evaluations with
real-world measurement data collected in a metropolitan-scale
operational mobile network demonstrate the effectiveness of our
proposed solution, which can reduce resource management costs
by over 50% in practical case studies.
Index Terms—Mobile networks, Network Slicing, 5G networks,
Artiﬁcial Intelligence, Deep Learning.
I. INTRODUCTION
Network slicing is an emerging paradigm that is expected
to characterize 5G and beyond-5G mobile systems. Network
slicing allows operators to tailor Virtual Network Functions
(VNFs) to the precise requirements of individual mobile
services , and will be key in enabling the unprecedented
heterogeneity of future mobile applications . Managing
sliced networks will represent a major challenge for operators,
since this new paradigm represents a shift from the rather
limited reconﬁguration possibilities offered by current Operations and Business Support System (OSS/BSS) to a complex,
software-deﬁned control layer that must dynamically organize
thousands of slices belonging to hundreds of tenants on the
same infrastructure .
D. Bega and A. Banchs are with the IMDEA Networks Institute,
28918 Legan´es, Spain, and also with the Telematic Engineering Department, University Carlos III of Madrid, 28911 Legan´es, Spain (email: ; ).
M. Gramaglia is with the Telematic Engineering Department, University
Carlos III of Madrid, 28911 Legan´es, Spain(e-mail: ).
M. Fiore is with CNR, 10129 Torino, Italy. (e-mail: )
X. Costa-P´erez are with NEC Laboratories Europe, 69115 Heidelberg,
Germany (e-mail: ).
Manuscript received June 22, 2019; revised September 3, 2019; accepted
November 6, 2019. Date of publication xxxx xx, xxxx; date of current version
xxxx xx, xxxx.(Corresponding author: Dario Bega.)
A. Network management and capacity forecast.
To rise to the challenge, network operators must introduce
substantial automation in the presently human-driven management and orchestration (MANO) processes, ultimately realizing the 5G principle of cognitive network management .
Achieving this objective requires advances in two complementary technologies: (i) technical solutions that enable endto-end Network Function Virtualization (NFV), providing the
ﬂexibility necessary for resource reallocation; and, (ii) a network intelligence that automatically identiﬁes and anticipates
demand patterns from streaming network measurement data,
and then takes decisions on how to conﬁgure VNFs and
allocate resources so as to maximize the system efﬁciency.
From a technical standpoint, solutions that implement NFV
at different network levels are well established, and start to be
tested and deployed. Examples include current MANO platforms architectures like ETSI NFV , and implementations
of MANO controllers such as OSM or ONAP that support reconﬁguring resources to VNFs on the ﬂy. By contrast,
the integration of intelligence in cognitive mobile networks is
still at an early stage. Nowadays, resource assignment to VNFs
is a reactive process, mostly based on hysteresis thresholding
and aimed at self-healing or fault tolerance. There is a need for
proactive, data-driven, automated solutions that enable costefﬁcient network resource utilization, by anticipating future
needs for capacity and timely reallocating resources just where
and when they are required.
The aim of our work is precisely to contribute to the
deﬁnition of a network intelligence that is adapted to a network
slicing environment. More speciﬁcally, the focus of this paper
is on the design of data analytics that enable the anticipatory
allocation of resources in cognitive mobile networks. To this
end, we investigate a machine learning solution that runs
on trafﬁc measurement data and provides operators with
information about the capacity needed to accommodate future
demands at each network slice – a critical knowledge for datadriven resource orchestration. We take a pragmatic approach,
and duly account for the economic costs associated to the
operation above.
Indeed, resource orchestration decisions have a direct monetary impact for the network operator in terms of operating
expenses, which can be divided into two macroscopic categories of cost.
• Overprovisioning – when providing excess capacity with
respect to the actual resource demand, the operator incurs
a cost due to the fact that it is reserving more resources
than those needed to a network entity (e.g., a network
slice, a network function, or a virtual machine). As
resources are typically isolated across slices, this seizes
the excess resources from other network entities that
may have possibly used them. At a global system level,
continued overprovisioning implies that the operator will
have to deploy more resources than those required to
accommodate the user demand, limiting the advantage of
a virtualized infrastructure and of cognitive networking
solutions in general.
• SLA violation – if insufﬁcient resources are allocated
to a network entity, users will suffer low Quality of
Service (QoS), or even discontinued service. This has
an indirect price for the operator, in terms of customer
dissatisfaction and increased churning rates, which is not
simple to quantify. However, in emerging contexts such
as those promoted by network slicing, underprovisioning
also entails a different, more direct and quantiﬁable
economic penalty for the operator. Under slicing, operators will sign Service Level Agreements (SLAs) with
the mobile service providers, which need to be strictly
enforced. Underprovisioning means violating such SLAs,
which results in substantial monetary fees for the network
Clearly, the cost is not the same in the two cases, and it may
also vary depending on the speciﬁc settings, including the
nature of the concerned resources, the technologies deployed
in the network infrastructure, or the market strategies of the
operator. In all cases, we posit that, once suitably modeled,
such costs shall be at the core of the orchestrating decisions.
Legacy techniques for the prediction of mobile network
trafﬁc, such as the one reviewed in Section II, fall short in this
respect. Such models aim at perfectly matching the temporal
behavior of trafﬁc, independently of whether the anticipated
demand is above or below the target, and are thus agnostic
of the aforementioned costs. As a result, they return forecasts
as that depicted in Fig. 1a, which refers to a real-world case
study of YouTube video streaming trafﬁc at a core network
datacenter. Note that no distinction is made between positive
and negative errors, which leads to substantial SLA violations
covering roughly half of the observation time. The operator
may then attempt to apply overprovisioning to the output
provided by such a trafﬁc predictor. Unfortunately, legacy
forecast models do not offer any insight on how large the
excess resource allocated on top of the forecast demand should
be. As we will demonstrate later in the paper, this makes such
a strategy highly inefﬁcient.
B. Paper contributions and data-driven setup
In this paper, we present DeepCog, a new mobile trafﬁc data
analytics tool that is explicitly tailored to solve the capacity
forecast problem exposed above. The design of DeepCog
yields multiple novelties, summarized as follows:
• It hinges on a deep learning architecture inspired by
recent advances in image and video processing, which
Normalized trafﬁc
Service demand
Trafﬁc prediction
(a) Legacy trafﬁc prediction
Service demand
Capacity forecast
(b) DeepCog capacity forecast
Fig. 1. Top: actual and predicted weekly demands for YouTube at a datacenter
controlling 470 4G eNodeBs. Bottom: levels of overprovisioning (blue) and
capacity violations (red) over time. (a) Output of a recent deep learning
predictor of mobile trafﬁc . (b) Output of DeepCog, tailored to anticipatory
network resource allocation. The best view of this ﬁgure is in colors.
exploits space- and time-independent correlations typical
of mobile trafﬁc and computes outputs at a datacenter
• It leverages a customized loss function that targets capacity forecast rather than plain mobile trafﬁc prediction,
letting the operator tune the balance between overprovisioning and demand violations;
• It provides long-term forecasts over conﬁgurable prediction horizons, operating on a per-service basis in
accordance with network slicing requirements.
Overall, these design principles jointly solve the problem of
capacity forecast in network slicing. This is illustrated by
Fig.1b, which shows an example of the required capacity
forecast by DeepCog in a real-world case study. We remark
that DeepCog is one of the very ﬁrst examples of rigorous
integration of machine learning into a cognitive network management process, and marks a difference from the common
practice of embedding vanilla deep learning structures into
network operation .
Extensive performance evaluations with substantial measurement data collected in an operational metropolitan-scale
mobile network demonstrate the superiority of our approach
over a wide range of benchmarks based on traditional and
state-of-the-art mobile trafﬁc predictors. Even though our
performance evaluation focuses on cellular network environments, it is worth noting that the data-driven approach adopted
is highly ﬂexible and can be applied to forecast capacity
in different scenarios, including, e.g., other types of access
The document is organized as follows. We ﬁrst provide
a review of related works, and highlight the novelty of our
proposed method, in Section II. We then outline the overall
framework of DeepCog in Section III, and detail the design
of its most critical component, i.e., the loss function, in
Section IV. The quality of the solution is then assessed in
realistic scenarios in Section V. Finally, we draw conclusions
in Section VI.
II. RELATED WORK
Applications to networking problems of machine learning
in general, and of deep learning in particular, are starting to
become popular. Artiﬁcial intelligence can indeed be applied
to solve many different problems that emerge in computer
tensor descrip,on of
network slice traﬃc
Neural network
adapted from image
and video processing,
with 3D-CNN encoding
tunable traﬃc aggrega,on level
targeted by the capacity forecast
slice traﬃc recorded
at the antenna level
error cost
Loss func4on
conﬁgurable balance of
resource overprovisioning
and unserviced demands
0.1 1 -0.3
0.5 -1 0.8
0.6 -0.4 0.2
0.1 1 -0.3
0.5 -1 0.8
0.6 -0.4 0.2
0.1 1 -0.3
0.5 -1 0.8
0.6 -0.4 0.2
0.2 -1.1 0.3
-0.5 0.9 0.1
0.1 1 -0.3
0.5 -1 0.8
0.6 -0.4 0.2
0.1 1.1 0.5
-0.3 -1 0.6
-0.5 0.8 -0.1
0.1 1 -0.3
0.5 -1 0.8
0.6 -0.4 0.2
0.3 -1 0.5
-0.1 1.4 0.7
-0.8 0.2 0.1
0.1 1 -0.3
0.5 -1 0.8
0.6 -0.4 0.2
-0.1 0.9 0.7
-0.6-0.3 0.3
0.4 0.7 0.2
0.1 1 -0.3
0.5 -1 0.8
0.6 -0.4 0.2
0.2 -1.1 0.3
-0.5 0.9 0.1
high correlation
Fig. 2. The overall DeepCog architecture. (a) Outline and interaction of the DeepCog components. (b) DeepCog neural network encoder-decoder structure.
networks, as highlighted in recent comprehensive surveys on
the topic , .
In the context of network management, emerging paradigms
like slicing increase substantially the complexity of orchestrating network functions and resources, at all levels. For
instance, intelligence is needed for the admission control
of new slices: as resources are limited and slicing entails
their strong isolation, this is critical to ensure that the system operates efﬁciently. With potentially hundreds of slices
allocated simultaneously, and a need to anticipate highly
proﬁtable future requests, the decision space for admission
control becomes so large that traditional approaches become
impractical. Solutions based on deep learning architectures
represent here a viable approach . Similar considerations
apply to other aspects of sliced network management, e.g.,
the allocation of computational resources to slices at the radio
access, based on transmission (e.g., modulation and coding
scheme, channel load) and environmental (e.g., signal quality,
hardware technology) conditions , or the anticipatory
reservation of Physical Resource Blocks (PRBs) to user trafﬁc
to be served in target network slices .
Our speciﬁc problem relates to the orchestration of generic
resources (e.g., CPU time, memory, storage, spectrum) to
slices at different network entities, which is tightly linked to
mobile trafﬁc prediction. The literature on forecasting network
trafﬁc is in fact vast , . Solutions to anticipate future
offered loads in mobile networks have employed a variety
of tools, from autoregressive models – to information
theoretical tools , passing by Markovian models and
deep learning , , , , .
However, we identify the following major limitations of
current predictors when it comes to supporting resource orchestration in mobile networks.
First, predictors of mobile trafﬁc invariably focus on providing forecasts of the future demands that minimize some
absolute error , . This approach leads to predicted
time series that deviate as little as possible from the actual
trafﬁc time series, as exempliﬁed in Fig. 1a for a real-world
case study. While reasonable for many applications, such an
output is not appropriate for network resource orchestration.
As explained in Section I, the operator aims at provisioning
sufﬁcient capacity to accommodate the offered load at all
times, since failing to do so implies high costs in terms of
high subscribers’ churn rates, as well as signiﬁcant fees for
violating SLAs signed with tenants. Yet, if an operator decided
to allocate resources based on a legacy prediction like that in
Fig. 1a, it would incur into capacity violations most of the time
(as illustrated in the bottom subplot).
Second, with the adoption of network slicing, forecasts must
occur at the slice level, i.e., for speciﬁc mobile services in
isolation. However, most trafﬁc predictors, including recent
ones, are evaluated with demands aggregated over all services , , . This is an easier problem, since aggregate
trafﬁc yields smoother and more regular dynamics, hence
previous solutions may not handle well the bursty, diversiﬁed
trafﬁc exhibited by each service. The only attempts at anticipating the demands generated by speciﬁc mobile services
have been made by using multiple-input single-output (MISO)
autoregressive models , and hybrid prediction methods
that incorporate α-stable models and sparsity with dictionary
learning .
Third, existing machine learning predictors for mobile traf-
ﬁc typically operate at base station level , . However, NFV operations mainly occur at datacenters controlling
tens (e.g., at the mobile edge) to thousands (e.g., in the
network core) of base stations. Here, prediction should be
more efﬁcient when performed on the aggregate trafﬁc at each
datacenter, where orchestration decisions are taken, rather than
combining independent forecasts from each base station.
Our proposed solution, DeepCog, addresses all of the open
problems above, by implementing a ﬁrst-of-its-kind predictor
that anticipates the minimum provisioned capacity needed to
cut down SLA violations. This closes the present gap between
trafﬁc prediction and practical orchestration, as it provides
the operator with an explicit capacity forecast that mitigates
underprovisioning in Fig. 1b while minimizing unnecessary
resource reservation. We remark that early versions of the
DeepCog framework were presented in and . Those
preliminary variants of our solution could achieve short-term
capacity forecasting over the next time step, whereas the
complete version presented in this paper supports long-term
capacity prediction over a conﬁgurable number of future time
III. A DEEP LEARNING FRAMEWORK FOR RESOURCE
ORCHESTRATION
The design of DeepCog is outlined in Fig. 2a. Its organization is that typical of deep learning systems, and it stems from
(i) properly formatted input data used to build the forecast,
which, in our case, represents the current and past trafﬁc
associated to a speciﬁc network slice as a tensor. Such input is
fed to (ii) a deep neural network architecture that extrapolates
and processes input features to provide (iii) an output value:
the capacity forecast. During the training phase, the output is
used to evaluate (iv) a loss function that quantiﬁes the error
with respect to the ground truth, and, in DeepCog, accounts for
the costs associated to resource overprovisioning and service
request denial.
In our network model, we consider that time is divided in
slots, which we denote by t. Let δi
s(t) be the trafﬁc associated
with slice s that is observed at base station i ∈N and time t.
A snapshot of the demand of slice s ∈S at time t is given by
a set δs(t) = {δ1
s(t), . . . , δN
s (t)}, and provides a global view
of the trafﬁc for that slice at time t across the whole network.
We let N denote the set of N base stations in the network,
and M the set of M < N datacenters. For each slice s ∈
S, base stations are associated to datacenters via a surjective
mapping fs : N →M, such that a datacenter j ∈M serves
the aggregated load of slice s for all of the associated bases
stations1. With this mapping, the trafﬁc for slice s processed
by datacenter j at time t is given by dj
i|fs(i)=j δi
Then, the set of demands across all datacenters is given by
ds(t) = {d1
s(t), . . . , dM
Let us denote the allocated capacity for slice s at datacenter j and time t as cj
s(t), and the set of capacities at all
j ∈M as cs(t) = {c1
s(t), . . . , cM
s (t)}. Then, the capacity
forecast problem is that of computing a constant capacity
¯cs(t, Th) = {¯c1
s(t, Th), . . . , ¯cM
s (t, Th)} that is allocated in
the network datacenters over a time horizon Th, i.e., through
an interval between the present time t and a future time
t + Th. In practice, this models the typical situation where
the resource reconﬁguration frequency is limited (e.g., by the
NFV technology), and the operator must decide in advance
the amount of resources that will stay assigned to a slice until
the next reallocation takes place. The time horizon Th thus
corresponds to the reconﬁguration period, and the allocated
capacity is such that cj
s(t) = ¯cj
s(t, Th) ∀j ∈M, t ∈[t, t+Th].
The forecast builds on knowledge of the previous Tp trafﬁc
snapshots δs(t−1), . . . , δs(t−Tp). The quality of the capacity
forecast ¯cs(t, Th) is measured by means of a suitable loss function ℓ(¯cs(t, Th), ds(t), . . . , ds(t + Th)). This function ℓ(·)
determines the compound cost of overprovisioning and underprovisioning network resources at the target datacenters, as
produced by allocating a constant capacity ¯cs(t, Th) when the
actual time-varying demand is in fact ds(t), . . . , ds(t + Th).
Below, we present each of the components of the framework, and discuss its mapping to the elements of a 5G network
architecture running cognitive resource management.
1We remark that DeepCog works for any arbitrary mapping, including, e.g.,
ﬂows from a slice in the same base station being split across datacenters, or
associations among base stations and datacenters varying over time. As a
matter of fact, DeepCog’s learning process is based exclusively on the trafﬁc
load at each individual datacenter and is thus independent of the actual sources
generating such trafﬁc.
A. The Neural Network
DeepCog leverages a deep neural network structure composed of suitably designed encoding and decoding phases,
performing a capacity forecasting prediction over a given
time horizon. The structure is general enough that it can be
trained to solve the capacity forecast problem for (i) network
slices dedicated to different services with signiﬁcantly diverse
demand patterns, (ii) any datacenter conﬁguration, and (iii)
any time horizon Th. The hyperparameters of the neural
network have been tuned through extensive simulation and
The design of the neural network structure in DeepCog is
inspired by recent breakthroughs in deep learning for
image and video processing. As summarized in Fig. 2b, the
network is composed of an encoder that receives an input
representing the mobile trafﬁc data δs(t −1), . . . , δs(t −Tp)
and maps important spatial and temporal patterns in such
data onto a low-dimensional representation. Intuitively, the
encoder extracts the relevant features from the input trafﬁc
tensors δs(t −1), . . . , δs(t −Tp) and the decoder leverages
such features to generate a capacity forecast that is tailored
to a given combination of slice, prediction time horizon, and
datacenter class; e.g., datacenters deployed close to the radio
access will show different features from those co-located with
the Internet gateways.
The result of the encoder undergoes a ﬂattening process
that converts the 3D (space and time) tensor data into a
unidimensional vector format. This is the input format required
by the fully connected layers that form the decoder, which then
generates the ﬁnal capacity forecast ¯cs(t, Th) at the target set
of datacenters M. Below, we detail the encoder and decoder
implementations, and discuss the training procedure.
1) The Encoder: The encoder is composed by a stack of
three three-dimensional Convolutional Neural Network (3D-
CNN) layers . Generic Convolutional Neural Networks
(CNNs) are a specialized kind of deep learning structure that
can infer local patterns in the feature space of a matrix input.
In particular, two-dimensional CNNs (2D-CNNs) have been
extensively utilized in image processing, where they can complete complex tasks on pixel matrices such as face recognition
or image quality assessment . 3D-CNNs extend 2D-CNNs
to the case where the features to be learned are spatiotemporal
in nature, which adds the time dimension to the problem and
transforms the input into a 3D-tensor. Since mobile network
trafﬁc exhibits correlated patterns in both space and time, our
encoder employs 3D-CNN layers2.
Formally, the 3D-CNN layers receive a tensor input
T (δs(t −1)) , . . . , T (δs(t −Tp)), where T (·) is a transformation of the argument snapshot into a matrix. This input is
processed by three subsequent 3D-CNN layers. Each neuron
of these layers runs a ﬁlter H (P
τ I(τ) ∗K(τ) + b) where
I(τ) is the input matrix passed to the neuron (e.g., I(τ) =
2We have employed CNNs instead of Recurrent Neural Networks (RNNs)
(typically used for forecasting application) because the mobile load at a time
instant t mainly depends on previous Tp instants and not on all the past values
(as conﬁrmed by our analysis). For this reason, CNNs provide us enough
temporal memory while being cheaper to train in terms of computational cost
compared with RNNs.
T (δs(τ)) at the very ﬁrst layer, for slice s and generic time
τ), ∗denotes the 3D convolution operator, K(t) is the kernel
of ﬁlters, H(·) is a non-linear activation function, and b
is a bias vector. We use two different kernel conﬁgurations
K(τ), as shown in Fig. 2b: a 3 × 3 × 3 kernel for the ﬁrst
3D-CNN layer, and a 6 × 6 × 6 kernel for the second and
third layers. These settings allow limiting the receptive ﬁeld,
i.e., the portion of input analyzed by each neuron, to small
regions: in presence of strong local correlation of the input
data, this approach is known to yield good performance with
fairly limited training, in particular compared to Recurrent
Neural Networks. As for the choice of the activation function,
many different options have been proposed in the literature,
spanning from linear functions to tanh, sigmoid or Rectiﬁed
Linear Unit (ReLU). Among these, we select ReLU, and set
H(x) = max (0, x), which provides advantages in terms of
discriminating performance and faster learning . Finally,
b is randomly set at the beginning of each training phase.
The second and third 3D-CNN layers are interleaved with
Dropout layers: such layers regularize the neural network and
reduce overﬁtting by randomly setting to zero a number
of output features from the preceding layer during the training
phase. The dropout rate deﬁnes the probability with which
output features undergo this effect. During training, we employ
two Dropout layers with dropout rate equal to 0.3.
2) The decoder: The decoder uses Multi-Layer Perceptrons
(MLPs) , a kind of fully-connected neural layers, where
every neuron of one layer is connected to every neuron of the
next layer. This provides the ability to solve complex function
approximation problems. In particular, MLPs are able to learn
global patterns in their input feature space , allowing the
neural network structure to forecast the targeted load value
leveraging the local features extracted by the encoder. In our
structure, each layer performs an operation H′(x · W + b),
where x is the MLP layer input vector, W a weight matrix
related to the neurons of each layer, and b the bias vector.
W plays a similar role to K(t) in the encoder part: its values
drive the prediction through the layers of the decoding part.
As for the activation functions H, we employ ReLU for all
MLP layers except for the last one, where a linear activation
function is used since the desired output takes real values.
The last linear layer can be conﬁgured to produce multiple
predictions in parallel, each matching the aggregate capacity
required by a subset of base stations, thus allowing to forecast
the needed capacity for different datacenters comprising a
subset of base stations. Ultimately, this organization makes
the DeepCog neural network capable of predicting per-slice
capacity requirements at datacenter level, in a way that can
adapt to any conﬁguration of M and to any time horizon Th.
3) The training procedure: We leverage the popular Adam
optimizer, which is a Stochastic Gradient Descent (SGD)
method that provides faster convergence compared to other
techniques . SGD trains the neural network model, evaluating at each iteration the loss function ℓ(·) between the
forecast and the ground truth, and tuning the model parameters
in order to minimize ℓ(·). For the conﬁguration of the Adam
optimizer, we use the default conﬁguration with a learning rate
of 5 × 10−4.
An important element that concerns the training of the
DeepCog architecture is that the encoder and the decoder
described in Section III-A1 and Section III-A2 have independent roles. Therefore, while the decoder heavily depends
on the forecast speciﬁcations, the encoder does not, and is
agnostic to the ﬁnal usage of the extracted features. This fact
allows adopting a transfer learning approach during training:
instead of treating the two blocks as a whole (and performing
the training over the full system for all the possible slices,
datacenter classes and horizons), we can train them separately.
Speciﬁcally, an horizon-independent encoder can be trained
on past trafﬁc tensors at maximum time granularity, and then
reused in combination with dedicated decoders tailored to
each Th value. Beside reducing the training time, this strategy
reduces the need for neural-network-wide training to different
settings of slice and datacenter only.
B. Arrangement of input data
The input is composed by measurement data generated in
a speciﬁc network slice, and recorded by dedicated probes
deployed within the network infrastructure. Depending on the
type and location of the probe, the nature of the measurement
data may vary, describing the demands in terms of, e.g.,
signal quality, occupied resource blocks, bytes of trafﬁc, or
computational load on VNFs. DeepCog leverages a set of
transformations to map any type of slice trafﬁc measurements
into a tensor format that can be processed by the learning
algorithm.
The 3D-CNN layer adopted as the ﬁrst stage of the decoder
requires a multidimensional tensor input. We thus need to
deﬁne the transformation T (·) of each trafﬁc snapshot into
a matrix. Note that 3D-CNN layers best perform in presence
of a tensor input that features a high level of local correlation,
so that neurons operate on similar values. In image processing,
where close-by pixels typically have high correlation, this is
easily solved by treating the pixel grid as a matrix. In line
with this strategy, the current common practice in mobile
network trafﬁc prediction is to leverage the geographical
locations of the base stations, and assign them to the matrix
elements so that their spatial proximity is preserved as much
as possible , . However, this approach does not consider
that correlations in mobile service demands at a base station
level do not depend on space, rather on land use : base
stations exhibiting strongly correlated network slice trafﬁc may
be far apart, e.g., covering the different train stations within
a same large city. Thus, we aim at creating a tensor input
whose neighboring elements correspond to base stations with
strongly correlated mobile service demands. To this end, we
construct the mapping of base stations into a matrix structure
as follows.
• For each base station i, we deﬁne its historical time
series of total trafﬁc as τ i = {δi(1), . . . , δi(t −1)},
where δi(t) = P
s(t). Then, for each pair i and j,
we determine the similarity of their recorded demands
by computing SBDij = fSBD(τ i, τ j), where fSBD(·) is the
shape-based distance, a state-of-the-art similarity measure
for time series . All pairwise distances are then stored
in a distance matrix D = (SBDij) ∈RM×M.
Capacity Forecast
x∗+ 1 x∗+ (1 + ϵ)αsj
Capacity Forecast
Fig. 3. Graphical representations of the α-OMC loss function. (a) Ideal model. (b) Actual implementation in (1).
• We compute virtual bidimensional coordinates pi for
each base station i so that the values in the distance
matrix D are respected as much as possible. Formally,
this maps to an optimization problem whose objective is
minx1,...,xM
i<j(∥pi−pj∥−SBDij)2, efﬁciently solved
via Multi-Dimensional Scaling (MDS) .
• We match each point pi to an element e of the input
matrix I, again minimizing the total displacement. To this
end, we: (i) quantize the virtual surface encompassing all
points pi so that it results into a regular grid of N cells;
(ii) assume that each cell is an element of the input matrix; (iii) compute the cost kie of assigning a point pi to
element e as the Euclidean distance between the point and
the cell corresponding to e. We then formalize an assignment problem with objective mina
e∈I kiexie,
where xie ∈ is a decision variable that takes value
1 if point pi is assigned to element e, and must fulﬁll
i∈N xie = 1 and P
e∈I xie = 1. The problem is solved
in polynomial time by the Hungarian algorithm .
The solution of the assignment problem is the transformation
T (·) of the original base stations into elements of the matrix
I. The mapping function T (·) allows translating a trafﬁc
snapshot δs(t) into matricial form. Applying this to snapshots
at different times, δs(t −1), . . . , δs(t −T), we can thus build
the tensor required by the entry encoder layer in Fig. 2b.
C. The Output function
DeepCog is designed for ﬂexibility, and can be used for
different orchestration scenarios. This is achieved thanks to
an adaptable last layer of the deep neural network, and a
conﬁgurable loss function. In general, the learning algorithm
returns a forecast of the capacity required to accommodate the
future demands for services associated to a speciﬁc network
slice. This generic deﬁnition of output can then be applied to
different orchestration use cases that may differ in the trafﬁc
aggregation level at which the resource conﬁguration takes
place, and/or in the frequency at which resource reallocation
can be realized.
For instance, the anticipatory assignment of baseband processing units to network slices in a Cloud Radio Access Network (C-RAN) datacenter requires a prediction of the capacity
needed to accommodate the trafﬁc of a few tens of base
stations; instead, reserving memory resources for a speciﬁc
network slice at a core network datacenter implies forecasting
capacity for the data sessions of subscribers associated to
hundreds of base stations. The output format of DeepCog can
accommodate any datacenter layout, by tailoring the last linear
layer of the neural networks to the speciﬁc requirements of the
layout (as discussed in Section III-A2).
Also, as discussed previously, the time horizon over which
the forecast is performed is another relevant system parameter,
which depends on NFV technology limitations and current
trends in commoditization of softwarized mobile network.
When the technology limitations do not allow frequent recon-
ﬁguration opportunities, resources need to be allocated over
long periods, e.g., of tens of minutes or even hours. In this
case, forecasting over long-term horizons provides the operator
with information on the constant capacity to be allocated
during long intervals. To realize this, DeepCog operates on
conﬁgurable time horizons, thanks to the ﬂexible loss function
that we will discuss next.
One of the key components of the system proposed in
the previous section is the loss function, denoted by ℓ(·).
This function determines the penalty incurred when making a
prediction error. In this paper, we propose a novel loss function
that is tailored to the speciﬁc requirements of the capacity
forecast problem. Our design of ℓ(·) accounts for the costs
resulting from (i) forecasting a lower value than the actual
offered load, which leads to an SLA violation due to the provisioning of insufﬁcient resources, (ii) predicting a higher value
than the actual one, which leads to overprovisioning, allocating
more resources than those needed to meet the demand. In order
to ensure that we drive the system towards an optimal trade-off
between overprovisioning and SLA violations, over a generic
time horizon Th, ℓ(·) must account for the penalty inﬂicted
in each case. In what follows, we describe the design of α-
OMC (Operator Monetary Cost), a loss functions that provides
DeepCog with the capability of optimizing the overall running
costs of the system.
A. Loss function design
In DeepCog, the loss function steers the behavior of the
neural network by adjusting the weights of the neurons according to the error between the estimated value and the real
one. To achieve the objective of minimizing the overall cost,
a custom loss function for the capacity forecasting problem
is composed by a term f(x, x∗) that deals with the resource
overprovisioning penalty, and a term g(x, x∗) that models
the cost of resource violations. The variable x represents the
allocated resources at a given time interval, while x∗is the
real demanded load for the same period. So the overall cost is
due by the discrepancy between x and x∗in any time horizon.
The shape of overall cost function f(x, x∗) + g(x, x∗) is
depicted in Fig. 3a. A perfect algorithm (i.e., an oracle) always
keeps the system in the optimal operation point x = x∗where
no penalty is introduced, i.e., f(x∗, x∗) = g(x∗, x∗) = 0. Of
course, errors are inherent to predictions, and it is very unlikely
that the forecast perfectly matches the real demand: hence, a
penalty value is back-propagated depending on whether x is
above or below the target operation point x∗.
1) g(x, x∗), a reactive approach to SLA violations: When
the orchestrated resources are less than those needed in reality (i.e., x < x∗) the network operator pays a monetary
compensation to the tenant. We assume an SLA that enforces
a proportional compensation depending on the number of time
intervals in which an operator fails to meet the requirements
set by a tenant due to insufﬁcient capacity allocated to the
slice. Thus, SLA violations determine a ﬁxed cost for the
operator at every time interval where the tenant demand is
not satisﬁed. Accordingly, we let the system learn that the
operation point x∗is actually higher than the estimated one
by applying a penalty βs as soon as the estimation falls below
the real value. The parameter βs can be customized to the
needs of the slice s ∈S: higher values may be used for cases
where reliability is paramount like, e.g., in URLLC network
slices; instead, lower values can be applied for slices where
KPI commitments are provided over longer time intervals.
Note that higher βs values are likely to bring the system
toward x > x∗, incurring hence in higher deployment costs,
as discussed next.
2) f(x, x∗), a monotonically increasing cost for resource
overprovisioning: While SLA violations depend on the agreements between the tenants and the operator, the overprovisioning cost solely depends on the network operator, and more
speciﬁcally on the deployment costs associated with excess
allocated capacity. We assume that such a cost grows with
the amount of unused capacity at each time interval, and
model it as a positive monotonic function that is only applied
when x > x∗: the higher the resource provisioning error, the
more (unnecessarily) expensive is the deployment. The exact
expression of f(x, x∗) may vary, and one could consider, e.g.,
linear, super-linear, or exponential shapes. For DeepCog, we
design α-OMC to use a linear function, as shown in Fig. 3a.
The linear scaling factor γj is conﬁgurable by the operator, and
represents the monetary cost of the excess resource allocation.
The cost depends on the speciﬁc datacenter j ∈M at which
the capacity forecasting takes place: for instance, spectrum
resources at the edge are typically scarcer and more expensive
to deploy than computational resources in a network core
datacenter. In case of expensive resources (characterized by
a large γj), a positive forecasting error will have a higher
impact, favoring a capacity forecast with a lower level of
overprovisioning.
3) Balancing the two cost contributions.:
Overall, the
amount of resources that a network operator is willing to
allocate depends on the cost that it has to pay when failing
to meet the demands for a given slice (given by βs) and the
cost associated with adding extra resources at a speciﬁc datacenter (given by γj). These two parameters, βs and γj, push
the capacity allocations towards opposite directions, namely
overdimensioning and underdimensioning, respectively. Rather
than their absolute values, what really matter for the resulting
allocation is the ratio between the two parameters, which
determines the trade-off between overdimensioning and underdimensioning. Accordingly, in the following we express the
custom loss as a function of a single parameter αsj .=
We remark that αsj indicates the monetary costs of SLA
violations with respect to the overprovisioning: failing to
meet the slice requirements once costs as much as allocating
αsj units of excess capacity. Thus, a higher αsj implies
higher SLA violation costs relative to the deployment (i.e.,
overprovisioning) cost. A mobile network operator can easily
set this parameter based on its deployment costs, SLA fees,
and market strategies.
Another important remark is that the SGD method used to
train the neural network does not work with constant or step
functions, and requires that the loss function be differentiable
in all its domain. We solve this problem by introducing
minimum slopes of very small intensity ϵ for x < x∗and
at x = x∗. We name the resulting loss function Operator
Monetary Cost, which has a single conﬁgurable parameter αsj.
The ﬁnal expression of α-OMC is
α-OMC(x, x∗) =
αsj −ϵ (x −x∗)
if x∗< x ≤x∗+ ϵαsj
x −x∗−ϵαsj
if x > x∗+ ϵαsj.
Fig. 3b provides a sample illustration of (1) above.
The ﬁnal loss function ℓ(·) then measures the quality of the
forecast over the time horizon Th, by applying the α-OMC
expression over multiple time intervals as follows:
ℓ(¯cs(t, Th), ds(t), . . . , ds(t + Th)) =
s(t, Th), dj
For the sake of readability and without loss of generality,
in the remainder of the paper we will employ a constant
βs = β across slices and a constant γj = γ across datacenter
deployment, leading to αsj = α for all s ∈S, j ∈M.
B. Correctness and convergence
We now analyze the proposed loss function in terms of (i)
correctness, i.e., its capability of achieving a performance that
is close to the optimal, and (ii) convergence, i.e., the time it
requires to learn such a correct strategy.
In Fig. 4, we run DeepCog in the representative network
resource management case studies that are later detailed in
Section V, where a slice is dedicated to one particular mobile
service and runs in a speciﬁc class of network datacenter. For
each case study, DeepCog forecasts a given level of capacity
to be allocated which leads to an associated monetary cost. In
order to investigate the correctness of the solution, we vary
the provisioned capacity by adding to or subtracting a ﬁxed
offset from the capacity indicated by DeepCog.
The curves of Fig. 4 illustrate the variation of the monetary
cost (in the y axis) as the offset is shifted (in the x axis),
where increasingly positive (respectively, negative) values on
the x axis correspond to a higher (respectively, lower) level
of capacity provisioning with respect that suggested by our
solution. The results prove that DeepCog always identiﬁes the
capacity allocation that minimizes the monetary cost for the
operator under the inherently inaccurate prediction, as both a
higher and a lower level of overprovisioning leads to a greater
cost. This holds under any combination3 of target mobile
service, datacenter class, and system settings α or Th, which
demonstrates the high consistency of our solution in balancing
costs caused by SLA violations and overprovisioning.
We next assess the convergence properties of the loss
function that drives DeepCog, by observing its behavior over
time. Speciﬁcally, we measure the normalized cost of the
solution identiﬁed by our learning algorithm, and compare it
against that returned by the same neural network trained with
legacy loss functions.
Fig. 5 shows how the average normalized cost of network
operation varies during the training phase for different α, services and datacenter classes. While the α-OMC loss function
minimizes the monetary cost of the operator in less than 20
epochs, both MAE and MSE converge to a ﬁxed fee that grows
as α increases. This conﬁrms that classical loss functions are
not effective when dealing with capacity forecasting, resulting
in high penalties for operators. The results are consistent
across all of the different conﬁguration scenarios we tested.
V. PERFORMANCE EVALUATION
In order to evaluate the performance of DeepCog in realistic
settings, we consider the mobile network infrastructure of a
major operator in a large metropolitan region. The area under
study covers around 100 km2 with a resident population of
more than 2 millions, and is surrounded by a conurbation
of 11 millions inhabitants who often commute to it. We run
DeepCog on real-world measurement data of an operator with
a market share of 35% in the target region that captures
the trafﬁc generated by millions of users. The data were
collected by monitoring the GPRS Tunneling Protocol (GTP)
via dedicated probes deployed at the network gateway and the
classiﬁcation of IP ﬂows into services was performed via Deep
Packet Inspection (DPI) with proprietary models developed
by the network operator. The trafﬁc demands, expressed in
bytes, refer to individual mobile services; they are aggregated
at the antenna sector level and over intervals of 5 minutes.
The demands capture the highly heterogeneous and timevarying loads that characterize real-world mobile network
deployments, with differences in the offered trafﬁc volume
of up to two orders of magnitude between antenna sectors.
We consider services that belong to different categories,
including video streaming, messaging and social networks.
3Fig. 4 shows results for α in the range [0.5, 3], and two exemplary Th
values, 5 and 30 minutes. Similar curves characterize all α values and
prediction horizons (up to 8 hours) we tested.
These services impose a broad range of requirements into the
network. For instance, video streaming services are consumed
ubiquitously, they have signiﬁcant bandwidth requirements
(e.g., up to 6 Mbps for a 1080p video in YouTube ) as
well as latency constraints to avoid interruptions while playing
the video , and they also require signiﬁcant data center
resources from the server side . Messaging services, instead, have a large component of uplink trafﬁc; overall, it has a
rather relaxed requirements in terms of bandwidth and latency,
as it does not involve any interactive communications. Social
networks need considerable bandwidth and with some latency
requirements; according to , a bandwidth of 8 Mbps and
an access delay not exceeding 100 ms are required to achieve
a high service quality. Beyond the speciﬁc requirements of
each service, what really matters in the context of this paper
is the fact that these are services of a very different nature with
highly diverse requirements, and therefore they are likely to
be served by different slices in a mobile network supporting
network slicing.
We employ the measurement data to design three case studies combining several popular mobile services and different
classes of network datacenters4. Each class is deﬁned by the
network location and number of served eNodeBs, ranging from
centralized datacenters located in the core and serving many
eNodeBs to more distributed ones located in the edge and
serving a smaller number of eNodeBs. By selecting a diverse
set of case studies, we can assess the DeepCog ﬂexibility
serving heterogeneous NFV scenarios, comprising different
services and datacenter classes (C-RAN, MEC and core). In
a ﬁrst case study, we consider that a slice is instantiated for
the incumbent video streaming service, i.e., YouTube, at C-
RAN datacenters in the target metropolitan area, each located
in proximity of the radio access and performing baseband
processing and scheduling for around ten eNodeBs. In the second case study, we look into Mobile Edge Computing (MEC)
datacenters that handle the trafﬁc of around 70 eNodeBs each,
where a dedicated slice accommodates the trafﬁc generated
by Snapchat, a favored messaging app. The third case study
focuses on a network slice dedicated to social network services
provided by Facebook that are run at a core network datacenter
controlling all 470 4G eNodeBs in the target metropolitan area.
The three case studies cover applications with diverse requirements in terms of bandwidth and latency; also, they entail
very different spatiotemporal dynamics of the mobile trafﬁc,
as the considered services feature different loads and activity
peaks . In addition, the datacenter classes we consider
have dissimilar geographical coverage and aggregated trafﬁc
volumes, as they serve the demands associated to a variable
number of antennas, from ten to several hundreds. Overall, the
three case studies considered for the DeepCog’s performance
evaluation are very useful to understand the effect of network
slicing on the network operation costs. In fact, our results
illustrate for the ﬁrst time the impact of the slice isolation
requirements –critical to future softwarized networks– on
services that have a dominant role in today’s trafﬁc and are
4The internal organization of the mobile network – hence the demand
recorded at each datacenter – is inferred by adopting the methodology
proposed in .
Normalized monetary cost
(a) Facebook, core datacenter
Normalized monetary cost
(b) Snapchat, MEC datacenter
Normalized monetary cost
(c) YouTube, C-RAN datacenter
Normalized monetary cost
(d) Facebook, core datacenter
Normalized monetary cost
(e) Snapchat, MEC datacenter
Normalized monetary cost
(f) YouTube, C-RAN datacenter
Fig. 4. Monetary cost (aggregated over time and normalized by the cost of one capacity unit) incurred when the overprovisioning level is shifted from that
selected by DeepCog (at the abscissa origin). Each plot refers to one case study, i.e., a combination of (i) mobile service associated to a dedicated slice and
(ii) datacenter type. Top row: Th = 5 minutes, bottom row: Th = 30 minutes.
Training Epochs
Normalized Cost
(a) Facebook, Core, α = 0.5
Training Epochs
Normalized Cost
(b) Snapchat, MEC, α = 2
Training Epochs
Normalized Cost
(c) YouTube, C-RAN, α = 3
Fig. 5. Average cost versus the learning epochs, when the DeepCog neural network architecture is trained with α-OMC, MSE and MAE loss functions.
expected to keep playing a very relevant role in future mobile
networks. In our tests, we do not parametrize different excess
resource costs for each datacenter nor different SLA violation
penalties for diverse services, leading to homogeneous α
settings (i.e., αsj = α for all s ∈S, j ∈M). However,
since DeepCog provisions resources for each slice and each
datacenter independently, by evaluating different α values our
results provide insights on the behavior of heterogeneous α
settings as well.
As discussed in Section III, DeepCog outputs a capacity
forecast within a variable time-horizon Th. We measure this
time in the number of steps it comprises, where each step
corresponds to the 5 mins granularity of our measurement
data. In our evaluation Th ranges from 5 minutes (which maps
to a next-step prediction) to 8 hours (which corresponds to a
forecast with a 96 time steps look-ahead). These are reasonable
values in our context, since resource reallocation updates
in the order of minutes are typical for computational and
memory resources in architectures implementing NFV ,
and are in line with those supported by any state-of-the-art
Virtual Infrastructure Manager (VIM) . Conversely, larger
intervals are more suitable for operations involving manual
intervention, e.g., spectrum leasing.
In all cases, we use the previous 30 minutes of trafﬁc (i.e.,
Tp = 6) as the DeepCog input, arranged in a 47 × 10 matrix
as an input. This conﬁguration proved to yield the best results
when confronted to a number of other design strategies for
the input that we explored, including longer, shorter, or noncontinuous historical data time intervals. Capacity is predicted
in terms of bytes of trafﬁc, which is a reasonable metric to
capture for resource utilization in actual virtual network functions , and is independent of the exact type of resources
relevant for the mobile operator in each case study. We employ
two months of mobile trafﬁc data for training, two weeks of
data for validation and another two for the actual experiments.
This setting is also used for all benchmark approaches. All
results are derived with a high level of conﬁdence and low
standard deviation.
A. Gain over state-of-the-art trafﬁc predictors
We ﬁrst focus on the particular case of next-step prediction,
i.e., Th = 5 minutes, as this benchmark lets us compare
our framework against state-of-the-art solutions that can only
perform a forecast for the following time interval. We compare
DeepCog against four benchmarks: (i) a naive technique that
forecasts the future offered load by replicating the demand
Youtube/C-RAN
Snapchat/MEC
Facebook/Core
Normalized
Monetary Cost
Youtube/C-RAN
Snapchat/MEC
Facebook/Core
Normalized
Monetary Cost
Overprovisioning
SLA violations
Fig. 6. Comparative evaluation of DeepCog with four benchmarks in three representative case studies. The monetary cost (normalized by the cost of one
capacity unit) incurred by the operator is split into costs due to overprovisioning (dark) and SLA violations (light). Left: α = 2. Right: α = 0.5.
recorded at the same time during the previous week; (ii) the
ﬁrst approach proposed to predict mobile trafﬁc based on a
deep learning structure, referred to as Infocom17 ; (iii)
a recent solution for mobile network demand prediction that
leverages a more complex deep neural network, referred to as
MobiHoc18 ; (iv) a reduced version of DeepCog, which
replaces α-OMC with a legacy Mean Absolute Error (MAE)
loss function5.
The results achieved in our three reference case studies
by DeepCog and by the four benchmarks above are shown
in Fig. 6. The plots report the normalized monetary cost for
the operator, broken down into the expenses for unnecessary resource allocation (i.e., overprovisioning) and fees for
unserviced demands (i.e., SLA violations). We observe that
DeepCog yields substantially lower costs than all other solutions. Indeed, the cost incurred by DeepCog for α = 2 ranges
between 15% (Facebook/Core) and 27% (Youtube/C-RAN) of
the cost provided by the best competitor, depending on the case
study. Infocom17, as all other benchmarks, targets mobile
network trafﬁc prediction, whereas DeepCog aims at forecasting capacity. As a result, DeepCog balances overprovisioning
and SLA violations so as to minimize operation expenses,
while Infocom17 is oblivious to such practical resource
management considerations. In other words, legacy predictors
follow as closely as possible the general trend of the time
series and allocate resources based on their prediction, which
leads to systematic SLA violations that are not acceptable
from a market viewpoint and determine huge fees for the
operator. Instead, DeepCog selects the appropriate level of
overprovisioning that, by suitably overestimating the offered
load, minimizes monetary penalties (see Fig. 4). Indeed, even
when choosing a low value such as α = 0.5, which inﬂicts
a small penalty for a SLA violation, the cost incurred by
DeepCog is 64% of that incurred by the best performing
benchmark.
B. Comparison with overprovisioned trafﬁc prediction
In the light of the above results, a more reasonable approach
to resource allocation could be to consider a traditional mobile
trafﬁc prediction as a basis, and adding some overprovisioning
offset on top of it. In order to explore the effectiveness of such
an approach, we design and implement several variants to MAE,
as follows.
5We also experimented with other popular loss functions, e.g., Mean
Squared Error (MSE), with comparable results, omitted for space reasons.
A ﬁrst variant adds an a-posteriori constant overprovisioning offset to the MAE output. This strategy, referred to as
MAE-post, requires selecting a value of the static offset,
which is then added to the predicted trafﬁc. We dimension
the offset as a certain percentage of the peak trafﬁc activity
observed in the whole historical data, and set it at 5%,
which we deem a reasonable value in presence of a decently
accurate prediction. Alternatively, we also consider a bestcase version of this solution, named MAE-post-best, where
an a-posteriori overprovisioning is chosen by performing an
exhaustive search over all possible offset values and selecting
the one that minimizes the loss function ℓ(·).
A second variant accounts for some level of overprovisioning in a preemptive fashion, by introducing the offset during
the deep neural network training. To this end, the MAE-pre
solution replaces the MAE loss function with a new loss
function O+ 1
s(t)|, where O denotes the apriori overprovisioning offset. Also in this case, we set O equal
to 5% of the peak trafﬁc in the historical data. To compare
against the best possible operation of this scheme, we also
consider a MAE-pre-best variant where O is set equal to the
average overprovisioning level provided by DeepCog for the
test period.
We remark that the MAE-post-best and MAE-pre-best
approaches are oracles and not feasible in practice, since
they require knowledge of the future to determine the best aposteriori values for the offset and the value of O, respectively.
Yet, they provide a benchmark for comparing the performance
of DeepCog against optimal solutions that rely on traditional
mobile network trafﬁc prediction.
Fig. 7 shows the relative performance of the four variants
above with respect to that attained by DeepCog, for Th = 5 min
(left) and Th = 45 min (right). The ﬁgure shows the oveprovisioned capacity, unserviced trafﬁc, and total economic cost
incurred by the operator relative to the performance offered by
DeepCog (in percentage). For Th = 5 min, the results highlight
how using a static overprovisioning in combination with a
traditional trafﬁc prediction is largely suboptimal, both when
the additional offset is considered preemptively or a-posteriori.
Indeed, the two practical solutions considered, i.e., MAE-post
and MAE-pre, cause SLA violations that are two- to threefold more frequent than that incurred into by DeepCog,
resulting in an economic cost that is 140% to 400% higher.
Interestingly, even when parametrized with the best possible
offsets, the approaches based on legacy trafﬁc prediction
Youtube/C-RAN
Snapchat/MEC
Facebook/Core
Rel. Overprovisioning (%)
Th = 5 mins
Rel. Violations (%)
Youtube/C-RAN
Snapchat/MEC
Facebook/Core
Rel. Overprovisioning (%)
Th = 45 mins
Rel. Violations (%)
Youtube/C-RAN
Snapchat/MEC
Facebook/Core
Rel. Norm. Monetary Cost (%)
179.6165.7
145.6139.5
(a) α = 2, Th = 5 minutes
Youtube/C-RAN
Snapchat/MEC
Facebook/Core
Rel. Norm. Monetary Cost (%)
188.4179.6
188.4158.0
236.0228.6
-30.8 -19.6
(b) α = 2, Th = 45 minutes
MAE-post-best
MAE-pre-best
Overprovisioning
Violations
Fig. 7. Relative performance of overprovisioned trafﬁc predictors, expressed as a percent of the cost attained by DeepCog. Top: relative overprovisioning and
SLA violations. Bottom: relative monetary cost. Results refer to α = 2 and prediction horizons of 5 (left) and 45 (right) minutes.
cannot match the performance of DeepCog: MAE-post-best
and MAE-pre-best dramatically reduce the penalties of their
viable counterparts, yet lead to monetary costs that are up to
60% higher than those of DeepCog.
The results for Th = 45 min6, show that the above considerations hold across different values of the prediction horizon.The
advantage over feasible overprovisioned trafﬁc predictors such
as MAE-pre or MAE-post is aligned with that observed under
a next-step prediction, as such solutions increase the overall
cost by 188% to 236%. When considering a long horizon of
45 minutes, oracle methods based on overprovisioning like
(i.e., MAE-pre-best and MAE-post-best) can outperform
DeepCog, further reducing the operator cost by 19% to 30%.
This is due to the fact that, when prediction must be performed
with signiﬁcant time advance, the accuracy of DeepCog cannot
be as high as an oracle that knows future demand and has
hence a signiﬁcant advantage. However, even under such
conditions DeepCog performs almost as good as the oracles
or better.
We conclude that trafﬁc predictors – no matter how they
are enhanced – are not appropriate for the capacity forecast
problem, for the simple reason that they are designed for a
different purpose. Indeed, they ignore the economic penalties
incurred by SLA violations, and this limits drastically their
ability to address this problem. Strategies that rely on integrating such costs into the solution after the trafﬁc prediction
is performed are largely suboptimal.
6Note that, in order to perform a fair comparison, we had to extended the
MAE policy to compute the average absolute error on each time slot in the
[t, . . . , t + Th] interval.
C. Controlling resource allocation trade-offs with the α parameter of DeepCog
As discussed in Section IV, DeepCog addresses a fundamental trade-off between overprovisioning and SLA violations,
aiming to ﬁnd the best possible compromise between the two.
An operator is given the ﬂexibility of choosing the desired
operation point within this trade-off, by suitably setting the α
parameter. In the following, we carry out an extensive analysis
of the trade-off between overprovisioning of resources and
failing to meet service demands. This study is conducted for a
large number of practical scenarios that extend the original
three case studies considered in the comparative analysis.
Speciﬁcally, we select ﬁve different network slices, dedicated
to the same number of popular mobile services: the three we
already studied, i.e., YouTube, Facebook, and Snapchat, plus
iTunes and Instagram. We then investigate the performance of
DeepCog when such slices are deployed at the three classes of
datacenters introduced before, i.e., at the C-RAN, MEC, and
network core. Overall, this leads to 15 distinct scenarios.
Fig. 8a shows results in all of the above settings under different economic strategies that are reﬂected by the α parameter
of the loss function ℓ(·). Conﬁgurations range from policies
that prioritize minimizing overprovisioning over avoiding SLA
violations (α = 0.5) to others that strictly enforce the SLAs
at the price of allocating additional resources (α = 5). The
plots tell apart the contribution of the two components that
contribute to the total monetary cost: overprovisioning is
expressed as a percentage of the actual demand, and SLA
violations are measured as a percentage of the time slots in the
test period. As expected, higher α values reduce the number
SLA violations, as they become increasingly expensive; this
occurs at the cost of provisioning additional capacity, which
a) Youtube, Core datacenter
b) iTunes, Core datacenter
c) Facebook, Core datacenter
d) Instagram, Core datacenter
e) Snapchat, Core datacenter
f) Youtube, MEC datacenter
g) iTunes, MEC datacenter
h) Facebook, MEC datacenter
i) Instagram, MEC datacenter
j) Snapchat, MEC datacenter
k) Youtube, C-RAN datacenter
l) iTunes, C-RAN datacenter
m) Facebook, C-RAN datacenter
n) Instagram, C-RAN datacenter
o) Snapchat, C-RAN datacenter
SLA violations [%]
Overprovisioning [%]
Overprovisioning
SLA violations
(a) Th = 5 minutes
a) Youtube, Core datacenter
b) iTunes, Core datacenter
c) Facebook, Core datacenter
d) Instagram, Core datacenter
e) Snapchat, Core datacenter
f) Youtube, MEC datacenter
g) iTunes, MEC datacenter
h) Facebook, MEC datacenter
i) Instagram, MEC datacenter
j) Snapchat, MEC datacenter
k) Youtube, C-RAN datacenter
l) iTunes, C-RAN datacenter
m) Facebook, C-RAN datacenter
n) Instagram, C-RAN datacenter
o) Snapchat, C-RAN datacenter
Overprovisioning [%]
SLA violations [%]
Overprovisioning
SLA violations
(b) Th = 120 minutes
Fig. 8. Tradeoff between resource overprovisioning (expressed as a percentage of the actual demand) and SLA violation (expressed as a percentage of time
slots), as a function of the α parameter. Results refer to 15 different scenarios, and two values of the prediction horizon Th, i.e., 5 minutes (a) and 120
minutes (b).
becomes instead cheaper in proportion7. The trend is consistent across all scenarios, conﬁrming that α effectively drives
resource orchestration towards the desired operation point.
Our analysis also reveals that the level of overprovisioning
grows in most cases as one moves from datacenters in the
network core outwards. This trend applies across slices, and
is due to the fact that more centralized datacenters serve an
increasingly aggregate trafﬁc that is generally less noisy and
easier to predict. Under such conditions, DeepCog needs a
reduced additional capacity to limit unserviced demands: as a
result, SLA violations are often lower at core datacenters.
Fig. 8a refers to a short-term prediction for Th = 5 minutes,
however the same trends discussed above are conﬁrmed for
larger prediction horizons. For instance, Fig. 8b reports the
same results for Th = 120 minutes. The only remarkable
difference is that overprovisioning and SLA violations are
higher than in the case of a 5-minute prediction, as forecasting
on larger time horizons is obviously harder. Yet, the impact
of α is equivalent to that observed for Th = 5 minutes. We
analyze in more detail DeepCog’s performance as a function
of Th in the next section.
Overall, the results presented above show that DeepCog
ﬁnds good trade-offs between resource overprovisioning and
7Sporadic disruptions in the monotonicity of the cost curves are due to the
inherent randomness of the measurement data; indeed, the data correspond
to a speciﬁc time period and it may show some biases that would not be
observed over different (or longer) time periods.
SLA violations in very different cost settings across slices and
datacenter types. Since each DeepCog instance for a slice at a
datacenter runs independently, this shows that DeepCog will
grant good performance also in scenarios where resource costs
may differ across datacenters, e.g., due to diverse operation
and management costs in urban and rural facilities.
D. Long-term capacity prediction with DeepCog
DeepCog aims at forecasting the (constant) capacity that
should be allocated over a long-term horizon, so as to minimize the monetary cost incurred by the operator. As discussed
in Section III, this is particularly useful in practical settings
where the NFV technology imposes limits on the frequency
upon which resources can be reallocated. In this section, we
thoroughly study how the performance of DeepCog varies with
the prediction horizon.
Fig. 9 summarizes the overall trend of the monetary cost
incurred by DeepCog, as the periodicity of the reconﬁguration
opportunities ranges from 5 minutes to 8 hours. The plots
outline a diversity of scenarios, combining different datacenter
classes (C-RAN, MEC, and core) and relative expenses of
overprovisioning and SLA violations (α equal to 0.5, 2, and 5).
The results correspond to the case where one slice is dedicated
to the trafﬁc generated by YouTube, but equivalent behaviors
were observed for the other services. In all settings, the cost
grows with the prediction horizon, which, as already mentioned, is largely expected. What is less expected, however, is
a) Youtube, Core datacenter Alpha = 0.5
b) Youtube, Core datacenter Alpha = 2
c) Youtube, Core datacenter Alpha = 5
d) Youtube, MEC datacenter Alpha = 0.5
e) Youtube, MEC datacenter Alpha = 2
f) Youtube, MEC datacenter Alpha = 5
g) Youtube, C-RAN datacenter Alpha = 0.5
h) Youtube, C-RAN datacenter Alpha = 2
i) Youtube, C-RAN datacenter Alpha = 5
Normalized Monetary Cost [x103]
Overprovisioning
SLA violations
Monetary cost (normalized by the cost of one capacity unit) incurred by the operator, versus the prediction time horizon Th. The plots refer to
different combinations of datacenter class and economic strategies modeled by α, for a slice dedicated to the YouTube mobile service.
the quasi-linear relationship between the cost and Th. This is a
very important result, as it shows that even if we increase the
intervals for resource reallocation (i.e., the time horizon), the
economic expenses of the operator remain bounded and do not
skyrocket (as they would if the growth was, e.g., exponential).
The result thus demonstrates the efﬁciency of DeepCog in
limiting the unavoidable increased penalty associated to forecasting long-term capacity: as an indicative ﬁgure, the cost
is roughly increased by two when moving from a 5-minute
prediction to one that spans the following 8 hours which is a
very reasonable factor.
The impact of the other system parameters is in line with
our previous analysis: higher monetary fees for SLA violations
(i.e., higher α values) lead to increased costs, whereas the
performance is comparable across resource allocations over
different classes of datacenter (C-RAN, MEC and core), each
corresponding to different trafﬁc volumes. It is nonetheless
interesting to note that the property of a linear growth of
the cost over Th is preserved under any combination of such
parameters.
Fig. 9 also offers a breakdown of the overall monetary
costs into the two contributions (overprovisioning and SLA
violations). Violations of SLAs yield substantially higher
absolute costs and dominate the increase of total cost with
Th; the effect is clearly stronger for higher values of α. A
more detailed view that highlights the exact evolution of the
two cost components as a function of Th is provided in Fig. 10,
showing that both contribute to increasing costs over longerterm forecasts. However, and interestingly, the dynamics of
the two components with Th are diverse depending on the
system settings such as the datacenter class and the value of
α. The common trend here is that the penalty associated with
both overprovisioning and SLA violations is fairly stable when
the horizon is increased from 5 minutes up to two hours. For
forecasts beyond two hours, however, these fees (one of the
two or both) tend to increase substantially with Th.
We ascribe these behaviors to (i) the relationship between
Th and the timescale of temporal ﬂuctuations in the input
demand, and (ii) the way DeepCog reacts to the problem
of devising a capacity forecast, which becomes harder for
larger Th values. The ﬁrst point relates to the characteristics
of the input data (see Fig. 11 for illustrative examples of
the temporal oscillation of the service demand). For very
large Th (above 120 minutes) the prediction task performed
by DeepCog resorts to an “envelope” of the demand that
accommodates the peak over the Th period. This means that
for those times where demand is below the peak, we incur a
high level of overprovisioning that increases the resulting cost.
In contrast, smaller Th allow to adapt the capacity forecasting
to the actual demand at each point in time, providing an
advantage in terms of cost. The second point relates to the
behavior and performance of DeepCog under large Th values.
DeepCog aims at providing a similar level of overprovisioning
over time, as exempliﬁed by the top three plots of Fig. 11. For
large Th values this yields increased SLA violations, since the
oscillations make it more likely that the constant capacity falls
below the demand curve at some point during Th. Additionally,
larger Th values make the prediction task inherently harder,
which further contributes to increasing the SLA violations
VI. CONCLUSIONS
In this paper we presented and evaluated DeepCog, an
original data analytics tool for the cognitive management of
resources in sliced 5G networks. DeepCog tackles the novel
problem of capacity forecasting, whose solution is key to the
sustainable operation of future multi-tenant mobile networks.
Inspired by recent advances in deep learning for image and
video processing, DeepCog hinges upon a deep neural network
structure, which analyzes antenna-level demand snapshots for
different services in order to provide a prediction of the
resources that the operator has to allocate to accommodate the
future load. The operation is performed for individual mobile
services separately, and over a conﬁgurable time horizon. At
the core of DeepCog there is α-OMC, a new and customized
loss function that drives the deep neural network training so
as to minimize the monetary cost contributed by two main deployment fees, i.e., overprovisioning and SLA violation. Ours
a) Youtube, Core datacenter Alpha = 0.5
b) Youtube, Core datacenter Alpha = 2
c) Youtube, Core datacenter Alpha = 5
d) Youtube, MEC datacenter Alpha = 0.5
e) Youtube, MEC datacenter Alpha = 2
f) Youtube, MEC datacenter Alpha = 5
g) Youtube, C-RAN datacenter Alpha = 0.5
h) Youtube, C-RAN datacenter Alpha = 2
i) Youtube, C-RAN datacenter Alpha = 5
Overprovisioning [%]
SLA violations [%]
Th [slots]
Overprovisioning
SLA violations
Fig. 10. Breakdown of monetary costs into two contributions: (i) overprovisioning (expressed as a percentage of the actual demand) and (ii) SLA violations
(expressed as a percentage of time slots), in the scenarios of Fig. 9.
Normalized trafﬁc
Service demand
(a) α = 0.5
Normalized trafﬁc
Service demand
Normalized trafﬁc
Service demand
Normalized trafﬁc
Service demand
(d) α = 0.5
Normalized trafﬁc
Service demand
Normalized trafﬁc
Service demand
Fig. 11. Illustrative examples of the capacity forecast returned by DeepCog behavior under different prediction time horizons. The scenario refers to a network
slice dedicated to the YouTube mobile service that is deployed at a core datacenter, under α = 2.
is, to the best of our knowledge, the only work to date where a
deep learning architecture is explicitly tailored to the problem
of anticipatory resource orchestration in mobile networks.
The solution presented in this paper thus represents a ﬁrst
attempt to integrate data analytics based on machine learning
into an overall cognitive management framework. Thorough
empirical evaluations with real-world metropolitan-scale data
show the substantial advantages granted by DeepCog over
state-of-the-art predictors and other automated orchestration
strategies, providing a ﬁrst analysis of the practical costs of
heterogeneous network slice management across a variety of
case studies.
ACKNOWLEDGMENTS
The work of University Carlos III of Madrid was supported by H2020 5G-TOURS project (grant agreement no.
856950). The work of NEC Laboratories Europe was supported by H2020 5G-TRANSFORMER project (grant agreement no. 761536) and 5GROWTH project (grant agreement
no. 856709). The work of CNR-IEIIT was partially supported
by the ANR CANCAN project (ANR-18-CE25-0011).