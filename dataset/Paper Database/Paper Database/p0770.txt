The Annals of Statistics
2004, Vol. 32, No. 6, 2385â€“2411
DOI 10.1214/009053604000000698
Â© Institute of Mathematical Statistics, 2004
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO
METHODS AND ITS APPLICATION TO BAYESIAN INFERENCE
BY NICOLAS CHOPIN
Bristol University
The term â€œsequential Monte Carlo methodsâ€ or, equivalently, â€œparticle
ï¬lters,â€ refers to a general class of iterative algorithms that performs Monte
Carlo approximations of a given sequence of distributions of interest (Ï€t). We
establish in this paper a central limit theorem for the Monte Carlo estimates
produced by these computational methods. This result holds under minimal
assumptions on the distributions Ï€t, and applies in a general framework
which encompasses most of the sequential Monte Carlo methods that have
been considered in the literature, including the resample-move algorithm of
Gilks and Berzuini [J. R. Stat. Soc. Ser. B Stat. Methodol. 63 127â€“146]
and the residual resampling scheme. The corresponding asymptotic variances
provide a convenient measurement of the precision of a given particle ï¬lter.
We study, in particular, in some typical examples of Bayesian applications,
whether and at which rate these asymptotic variances diverge in time, in order
to assess the long term reliability of the considered algorithm.
1. Introduction.
Sequential Monte Carlo methods form an emerging, yet
already very active branch of the Monte Carlo paradigm. Their growing popularity
comes in part from the fact that they are often the only viable computing
techniques in those situations where data must be processed sequentially. Their
range of applicability is consequently very wide, and includes nonexclusively
signal processing, ï¬nancial modeling, speech recognition, computer vision, neural
networks, molecular biology and genetics, target tracking and geophysics, among
others. A very good introduction to the ï¬eld has been written by KÃ¼nsch ,
while the edited volume of Doucet, de Freitas and Gordon provides an
interesting coverage of recent developments in theory and applications.
Speciï¬cally, sequential Monte Carlo methods (alternatively termed â€œparticle
ï¬ltersâ€ or â€œrecursive Monte Carlo ï¬ltersâ€) are iterative algorithms that produce
and update recursively a set of weighted simulations (the â€œparticlesâ€) in order
to provide a Monte Carlo approximation of an evolving distribution of interest
Ï€t(dÎ¸t), t being an integer index. In a sequential Bayesian framework, Ï€t(dÎ¸t)
will usually represent the posterior distribution of parameter Î¸t given the t ï¬rst
observations. The term â€œparameterâ€ must be understood here in a broad sense, in
Received November 2002; revised March 2004.
AMS 2000 subject classiï¬cations. Primary 65C05, 62F15, 60F05; secondary 82C80, 62L10.
Key words and phrases. Markov chain Monte Carlo, particle ï¬lter, recursive Monte Carlo ï¬lter,
resample-move algorithms, residual resampling, state-space model.
that Î¸t may include any unknown quantity which may be inferred from the t ï¬rst
observations, and is not necessarily of constant dimension. We denote by t the
support of Ï€t(dÎ¸t).
The study of the asymptotic properties of sequential Monte Carlo methods is
admittedly a difï¬cult problem, and some methodological papers [Liu and Chen
 , e.g.] simply state some form of the law of large numbers for the most
elaborate algorithms, that is, the Monte Carlo estimates are shown to converge
almost surely to the quantity of interest as H, the number of particles, tends toward
inï¬nity. More reï¬ned convergence results have been obtained, such as the central
limit theorem of Del Moral and Guionnet , later completed by Del Moral
and Miclo , or upper bounds for the Monte Carlo error expressed in various
norms [Crisan and Lyons , Crisan, Gaines and Lyons , Crisan
and Doucet , Del Moral and Guionnet , KÃ¼nsch and Le Gland
and Oudjane ]. Unfortunately, it has been, in general, at the expense of
generality [with the exception of Crisan and Doucet ], whether in terms of
computational implementation (only basic algorithms are considered, which may
not be optimal) or of applicability (the sequence Ï€t has to be generated from some
speciï¬c dynamical model that fulï¬lls various conditions).
In this paper we derive a central limit theorem that applies to most of the
sequential Monte Carlo techniques developed recently in the methodological
literature, including the resample-move algorithm of Gilks and Berzuini ,
the auxiliary particle ï¬lter of Pitt and Shephard and the stochastic remainder
resampling scheme [Baker ], also known as the residual resampling
scheme [Liu and Chen ]. No assumption is made on the model that generates
the sequence of distributions of interest (Ï€t), so that our theorem equally applies to
those recent algorithms [Chopin , Del Moral and Doucet and CappÃ©,
Guillin, Marin and Robert ] that have been developed for contexts that
widely differ from the standard application of sequential Monte Carlo methods,
namely, the sequential analysis of state space models.
The appeal of a central limit theorem is that it provides an (asymptotically) exact
measure of the Monte Carlo error, through the asymptotic variance. This allows for
a rigorous comparison of the relative efï¬ciency of given algorithms. In this way, we
show in this paper, again by comparing the appropriate asymptotic variances, that
the residual resampling scheme always outperforms the multinomial resampling
scheme, and that the Raoâ€“Blackwell variance reduction technique of Doucet,
Godsill and Andrieu is, indeed, effective.
The most promising application of our central limit theorem is the possibility to
assess the stability of a given particle ï¬lter (in terms of precision of the computed
estimates) through the time behavior of the corresponding asymptotic variances.
This is a critical issue since it is well known that sequential Monte Carlo methods
tend to degenerate in a number of cases, sometimes at a very fast rate. We consider
in this paper some typical Bayesian problems, such as the sequential analysis of
state-space models. We will show that under some conditions stability can be
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2387
achieved at least for â€œï¬lteringâ€ the states, that is, for approximating the marginal
posterior density Ï€t(xt), where xt stands for the current state at iteration t.
The paper is organized as follows. Section 2 proposes a generic description
of particle ï¬lters, establishes a central limit theorem for computed estimates in
a general framework and draws some conclusions from this result. Section 3
discusses the stability of particle ï¬lters through the time behavior of the asymptotic
variances provided by the central limit theorem. Proofs of theorems are put in the
2. Central limit theorem for particle ï¬lters.
2.1. General formulation of particle ï¬lters.
In full generality, a particle
system is a triangular array of random variables in  Ã— R+,
Î¸(j,H),w(j,H)
where  is some space of interest. The variables Î¸(j,H) are usually called
â€œparticles,â€ and their contribution to the sample may vary according to their
weights w(j,H). We will say that this particle system targets a given distribution Ï€
deï¬ned on  if and only if
j=1 w(j,H)Ï•(Î¸(j,H))
j=1 w(j,H)
holds almost surely as H â†’+âˆfor any measurable function Ï• such that the
expectation above exists. A ï¬rst example of a particle system is a denumerable set
of independent draws from Ï€, with unit weights, which obviously targets Ï€. In this
simple case, particles and weights do not depend on H, and the particle system is
a sequence rather than a triangular array. This is not the case in general, however,
and, while cumbersome, the dependence in H will be maintained in notation to
allow for a rigorous mathematical treatment.
Now assume a sequence (Ï€t)tâˆˆN of distributions deï¬ned on a sequence of
probabilized spaces (t). In most, if not all, applications, t will be a power
of the real line or some subset of it, and, henceforth, Ï€t(Â·) will also denote the
density of Ï€t with respect to an appropriate version of the Lebesgue measure.
A sequential Monte Carlo algorithm (or particle ï¬lter) is a method for producing
a particle system whose target evolves in time: at iteration t of the algorithm, the
particle system targets Ï€t, and therefore allows for Monte Carlo approximations
of the distribution of (current) interest Ï€t. Clearly, particle ï¬lters do not operate in
practice on inï¬nite triangular arrays but rather manipulate particle vectors of ï¬xed
size H. One must keep in mind, however, that the justiï¬cation of such methods is
essentially asymptotic.
The structure of a particle ï¬lter can be decomposed into three basic iterative
operations, that will be referred to hereafter as mutation, correction and selection
steps. At the beginning of iteration t, consider a particle system ( Ë†Î¸(j,H)
tâˆ’1 ,1)jâ‰¤H,
that is, with unit weights, which targets Ï€tâˆ’1. The mutation step consists in
producing new particles drawn from
where kt is a transition kernel which maps tâˆ’1 into P (t), the set of probability
measures on t. The â€œmutatedâ€ particles (with unit weights) target the new
distribution ËœÏ€t(Â·) =
 Ï€tâˆ’1(Î¸tâˆ’1)kt(Î¸tâˆ’1,Â·)dÎ¸tâˆ’1. This distribution ËœÏ€t is usually
not relevant to the considered application, but rather serves as an intermediary
stage for practical reasons. To shift the target to the distribution of interest Ï€t,
particles are assigned weights
with Ï…t(Â·) = Ï€t(Â·)/ ËœÏ€t(Â·).
This is the correction step. The particle system (Î¸(j,H)
)jâ‰¤H targets Ï€t.
The function Ï…t is referred to as the weight function. Note that the normalizing
constants of the densities Ï€t and ËœÏ€t are intractable in most applications. This is why
weights are deï¬ned up to a multiplicative constant, which has no bearing anyway
on the estimates produced by the algorithm, since they are weighted averages.
Finally, the selection step consists in replacing the current vector of particles by
a new, uniformly weighted vector ( Ë†Î¸(j,H)
,1)jâ‰¤H , which contains a number n(j,H)
of replicates of particle Î¸(j,H)
, n(j,H) â‰¥0. The n(j,H)â€™s are random variables such
j n(j,H) = H and E(n(j,H)) = HÏj, where the normalized weights are given
Ïj = w(j,H)
and where dependencies in H and t are omitted for convenience. In this way, particles whose weights are too small are discarded, while particles with important
weights serve as multiple starting points for the next mutation step. There are various ways of generating the n(j,H)â€™s. Multinomial resampling [Gordon, Salmond
and Smith ] amounts to drawing independently the H new particles from
the multinomial distribution which produces Î¸(j,H)
with probability Ïj. Residual
resampling [originally termed â€œstochastic remainder samplingâ€ in the genetic algorithm literature, Baker , then rediscovered by Liu and Chen ]
consists in reproducing âŒŠHÏjâŒ‹times each particle Î¸(j,H)
, where âŒŠÂ·âŒ‹stands for the
integer part. The particle vector is completed by H r = H âˆ’
jâŒŠHÏjâŒ‹independent
draws from the multinomial distribution which produces Î¸(j,H)
with probability
(HÏj âˆ’âŒŠHÏjâŒ‹)/H r. Systematic resampling [another method initially proposed
in the genetic algorithm ï¬eld, Whitley , then rediscovered by Carpenter,
Clifford and Fearnhead ; see also Crisan and Lyons for a slightly
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2389
different algorithm] is another interesting selection scheme, which is such that the
number of replicates n(j,H) is ensured to differ from HÏj by at most one. We
failed, however, to extend our results to this third selection scheme.
The structure of a particle ï¬lter can be summarized as follows:
1. Mutation: Draw for j = 1,...,H,
where kt :tâˆ’1 â†’P (t) is a given probability kernel.
2. Correction: Assign weights to particles so that, for j = 1,...,H,
where ËœÏ€t(Â·) =
 Ï€tâˆ’1(Î¸tâˆ’1)kt(Î¸tâˆ’1,Â·)dÎ¸tâˆ’1.
3. Selection: Resample, according to a given selection scheme,
The ï¬rst mutation step, t = 0, is assumed to draw independent and identically
distributed particles from some instrumental distribution ËœÏ€0.
It is shown without difï¬culty that the particle system produced by this generic
algorithm does iteratively target the distributions of interest, that is, the following
convergences hold almost surely:
 â†’E ËœÏ€t(Ï•),
j=1 w(j,H)
j=1 w(j,H)
 â†’EÏ€t(Ï•),
as H â†’+âˆ, provided these expectations exist. These convergences will be
referred to as the law of large numbers for particle ï¬lters.
2.2. Some examples of particle ï¬lters.
The general formulation given in the
previous section encompasses most of the sequential Monte Carlo algorithms described in the literature. By way of illustration, assume ï¬rst that the distributions Ï€t
are deï¬ned on a common space, t = . In a Bayesian framework, Ï€t will usually be the posterior density of Î¸, given the t ï¬rst observations, Ï€t(Î¸) = Ï€(Î¸|y1: t),
where y1: t denotes the sequence of observations y1,...,yt. If particles are not
mutated, kt being the â€œidentity kernelâ€ kt(Î¸,Â·) = Î´Î¸, we have ËœÏ€t = Ï€tâˆ’1 for t > 0,
and our generic particle ï¬lter becomes one of the variations of the sequential importance resampling algorithm [Rubin , Gordon, Salmond and Smith 
and Liu and Chen ]. The weight function simpliï¬es to
Ï…t(Î¸) = Ï€(Î¸|y1: t)/Ï€(Î¸|y1: tâˆ’1) âˆp(yt|y1: tâˆ’1,Î¸)
in a Bayesian model, where p(yt|y1: tâˆ’1,Î¸) is the conditional likelihood of yt,
given the parameter Î¸ and previous observations.
Gilks and Berzuini propose a variant of this algorithm, namely, the
resample-move algorithm, in which particles are mutated according to an MCMC
[Markov chain Monte Carlo; see, e.g., Robert and Casella ] kernel kt, which
admits Ï€tâˆ’1 as an invariant density. In that case, we still have ËœÏ€t = Ï€tâˆ’1, and the
expression for the weight function Ï…t is unchanged. The motivation of this strategy
is to add new particle values along iterations so as to limit the depletion of the
particle system.
Now consider the case where Ï€t is deï¬ned on a space of increasing dimension
of the form t = Xt. A typical application is the sequential inference of a
dynamical model which involves a latent process (xt), and Ï€t stands then for
density Ï€(x1: t|y1: t). Assume kt can be decomposed as
1: tâˆ’1,dx1: t) = Îºt(xâˆ—
1: tâˆ’1,dx1: tâˆ’1)qt(xt|x1: tâˆ’1)dxt,
where Îºt :Xtâˆ’1 â†’P (Xtâˆ’1) is a transition kernel, and qt(Â·|Â·) is some conditional
probability density. If Îºt admits Ï€tâˆ’1 as an invariant density, the weight function
is given by
Ï…t(x1: t) =
Ï€tâˆ’1(x1: tâˆ’1)qt(xt|x1: tâˆ’1).
Again, the case where Îºt is the identity kernel corresponds to some version of the
sequential importance resampling algorithm, while setting Îºt to a given MCMC
transition kernel with invariant density Ï€tâˆ’1 leads to the resample-move algorithm
of Gilks and Berzuini . The standard choice for qt(Â·|Â·) is the conditional
prior density of xt, given x1: tâˆ’1, as suggested originally by Gordon, Salmond
and Smith , but this is not always optimal, as pointed out by Pitt and
Shephard and Doucet, Godsill and Andrieu . In fact, it is generally
more efï¬cient to build some conditional density qt which takes into account the
information carried by yt in some way, in order to simulate more values compatible
with the observations.
These two previous cases can be combined into one, by considering a dynamical
model which features at the same time a ï¬xed parameter Î¸ and a sequence of latent
variables (xt), so that t =  Ã— Xt, and Ï€t stands for the joint posterior density
Ï€(Î¸,x1: t|y1: t).
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2391
2.3. Central limit theorem.
The following quantities will play the role of
asymptotic variances in our central limit theorem. Let, for any measurable
Ï• :0 â†’Rd, V0(Ï•) = Var ËœÏ€0(Ï•), and by induction, for any measurable Ï• :
Vt(Ï•) = Vtâˆ’1
Vt(Ï•) = Vt
Vt(Ï•) = Vt(Ï•) + VarÏ€t(Ï•),
The notation Ekt (Ï•) and Varkt (Ï•) is shorthand for the functions Âµ(Î¸tâˆ’1) =
Ekt(Î¸tâˆ’1,Â·){Ï•(Â·)} and (Î¸tâˆ’1) = Varkt(Î¸tâˆ’1,Â·){Ï•(Â·)}, respectively. Note that these
equations do not necessarily produce ï¬nite variances for any Ï•. We now specify the
classes of functions for which the central limit theorem enunciated below will hold,
and, in particular, for which these asymptotic variances exist. Denoting by âˆ¥Â· âˆ¥the
Euclidean norm in Rd, we deï¬ne recursively
to be the set of measurable
functions Ï• :t â†’Rd such that for some Î´ > 0,
E ËœÏ€tâˆ¥Ï…t Â· Ï•âˆ¥2+Î´ < +âˆ,
and that the function Î¸tâˆ’1
â†’Ekt(Î¸tâˆ’1,Â·){Ï…t(Â·)Ï•(Â·)} is in
tâˆ’1. The initial set
contains all the measurable functions whose moments of order two with respect to
ËœÏ€0 are ï¬nite.
THEOREM 1.
If the selection step consists of multinomial resampling, and
provided that the unit function Î¸t
â†’1 belongs to
for every t, then for
, EÏ€t(Ï•), Vt(Ï•) and Vt(Ï•) are ï¬nite quantities, and the following
convergences in distribution hold as H â†’+âˆ:
j=1 w(j,H)
j=1 w(j,H)
â†’N {0,Vt(Ï•)},
 Ë†Î¸ (j,H)
â†’N {0, Vt(Ï•)}.
A proof is given in the Appendix. In the course of the proof an additional
central limit theorem is established for the unweighted particle system (Î¸(j,H)
produced by the mutation step, which targets ËœÏ€t. This result is not given here,
however, for it holds for a slightly different class of functions, and is of less
practical interest. The assumption that the function Î¸t
â†’1 belongs to
deserves further comment. Qualitatively, it implies that the weight function Ï…t has
ï¬nite moment of order 2 + Î´ with respect to ËœÏ€t, for some Î´ > 0, and, therefore,
restricts somehow the dispersion of the particle weights. It also implies that
contains all bounded functions Ï•. In practice this assumption will be fulï¬lled, for
instance, whenever each weight function Ï…t is bounded from above, which occurs
in many practical settings.
A central limit theorem also holds when the selection step follows the residual
sampling scheme of Liu and Chen , but this imposes some change in the
expression for the asymptotic variances. The new expression for Vt(Ï•) is
Vt(Ï•) = Vt(Ï•) + Rt(Ï•),
Rt(Ï•) = E ËœÏ€t{r(Ï…t)Ï•Ï•â€²} âˆ’
E ËœÏ€t{r(Ï…t)}
E ËœÏ€t{r(Ï…t)Ï•}
E ËœÏ€t{r(Ï…t)Ï•}
and r(x) is x minus its integer part.
THEOREM 2.
The results of Theorem 1 still hold when the selection steps
consists of residual resampling, except that the asymptotic variances are now
deï¬ned by equations (3), (4) and (7).
The proofs of Theorems 1 and 2 (given in the Appendix) rely on an induction
argument: conditional on past iterations, each step generates independent (but not
identically distributed) particles, which follow some (conditional) central limit
theorem. In contrast, the systematic resampling scheme is such that, given the
previous particles, the new particle system is entirely determined by a single draw
from a uniform distribution; see Whitley . This is why extending our results
to this third selection scheme seems not straightforward, and possibly requires an
entirely different approach.
The appeal of the recursive formulae (3)â€“(5) and (7) is that they put forward
the impact of each new step on the asymptotic variance, particularly the additive
effect of the selection and mutation steps. In the multinomial case, an alternative
expression for the asymptotic variance is
where Et is the functional operator which associates to Ï• the function
Et(Ï•):Î¸tâˆ’1
â†’Ekt(Î¸tâˆ’1,Â·){Ï…t(Â·)Ï•(Â·)},
and Ek+1: t(Ï•) = Ek+1 â—¦Â·Â·Â· â—¦Et(Ï•) for k + 1 â‰¤t, Et+1: t(Ï•) = Ï•. This closed
form expression is more convenient when studying the stability of the asymptotic
variance over time, as we will illustrate in the next section. A similar formula for
the residual case can be obtained indirectly by deriving the difference between the
multinomial and the residual cases, that is, for t > 0,
t (Ï•) âˆ’Vt(Ï•) =
Rk{Ek+1: t(Ï•)} âˆ’VarÏ€k{Ek+1: t(Ï•)}
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2393
where Vt(Ï•), V r
t (Ï•) are deï¬ned through the recursions (3)â€“(5) and (3), (4) and (7),
respectively. In the following, we will similarly distinguish the residual case
through an r-sufï¬x in notation.
2.4. First conclusions.
A ï¬rst application of this central limit theorem is to
provide a rigorous justiï¬cation for some heuristic principles that have been stated
in the literature, see, for instance, Liu and Chen . Inequalities in this section
refer to the canonical order for symmetric matrices, that is to say A > B (resp.
A â‰¥B) if and only if A âˆ’B is positive deï¬nite (resp. positive semideï¬nite).
First, it is preferable to compute any estimate before the selection step, since
the immediate effect of the latter is a net increase in asymptotic variance: Vt(Ï•) >
Vt(Ï•) for any nonconstant function Ï•. In this respect one may wonder why
selection steps should be performed. We will see that the immediate degradation
of the particle system is often largely compensated for by gains in precision in the
future iterations.
Second, residual sampling always outperforms multinomial resampling. Let
Ï• :t â†’Rd and Â¯Ï• = Ï• âˆ’EÏ€t(Ï•). Then
Rt(Ï•) = Rt( Â¯Ï•) â‰¤E ËœÏ€t{r(Ï…t) Â¯Ï• Â¯Ï•â€²} â‰¤VarÏ€t(Ï•),
since r(x) â‰¤x. It follows from this inequality and (11) that V r
t (Ï•) â‰¤Vt(Ï•).
Actually, a substantial gain should be expected when using the residual scheme
since the inequality above is clearly not sharp.
Our central limit theorem also provides a formal justiï¬cation for resorting to
â€œmarginalizedâ€ particle ï¬lters, as explained in the following section.
2.5. Marginalized particle ï¬lters.
In some speciï¬c cases it is possible to
decompose the density Ï€t(Î¸t) into Ï€m
t (Î»t|Î¾t), with Î¸t = (Î¾t,Î»t) lying in
t Ã— t, in such a way that it is possible to implement a particle ï¬lter
that targets the marginal densities Ï€m
rather than the Ï€tâ€™s. When this occurs,
this second algorithm usually produces more precise estimators (in a sense that
we explain below) in the Î¾t-dimension. The idea of resorting to â€œmarginalizedâ€
particle ï¬lters has been formalized by Doucet, Godsill and Andrieu , and
implemented in various settings by Chen and Liu , Chopin and
Andrieu and Doucet , among others.
Doucet, Godsill and Andrieuâ€™s justiï¬cation for resorting to â€œmarginalizedâ€ particle ï¬lters is that they yield importance weights with a smaller variance than their â€œunmarginalizedâ€ counterpart, which suggests that the produced
estimates are also less variable. This is proven by a Raoâ€“Blackwell decomposition, and, consequently, â€œmarginalizedâ€ particle ï¬lters are sometimes referred to
as â€œRaoâ€“Blackwellizedâ€ particle ï¬lters. We now extend the argument of these authors by proving that the asymptotic variance of any estimator is, indeed, smaller
in the â€œmarginalizedâ€ case. Assume decompositions of Ï€t and ËœÏ€t of the form
Ï€t(Î¸t) = Ï€m
t (Î»t|Î¾t),
ËœÏ€t(Î¸t) = ËœÏ€m
t (Î¾t) ËœÏ€c
t (Î»t|Î¾t),
where (Î¾t,Î»t) identiï¬es to Î¸t, and Ï€m
t , are, respectively, marginal
and conditional densities of Î¾t and Î»t. Consider two particle ï¬lters, tracking,
respectively, (Ï€t) and (Ï€m
t ). It is assumed that both ï¬lters implement the same
selection scheme (whether multinomial or residual), and that their mutation steps
consist in drawing, respectively, from kernels kt and km
t , which are such that the
following probability measures coincide on t =
tâˆ’1(Î»tâˆ’1|Î¾tâˆ’1)kt{(Î¾tâˆ’1,Î»tâˆ’1),(dÎ¾t,dÎ»t)}dÎ»tâˆ’1
t (Î¾tâˆ’1,dÎ¾t) ËœÏ€c
t (Î»t|Î¾t)dÎ»t,
for almost every Î¾tâˆ’1 in
tâˆ’1. Note that in full generality it is not always possible
to build a kernel km
from a given kt which satisï¬es this relation. As illustrated
by the aforementioned references, however, it is feasible in some cases of interest.
This equality implies, in particular, that
tâˆ’1(Î¾tâˆ’1)km
t (Î¾tâˆ’1,Â·)dÎ¾tâˆ’1 = ËœÏ€m
Asymptotic variances and other quantities are distinguished similarly through the
m-sufï¬x for the marginal case, that is, Vt(Ï•) and V m
t (Ï•), and so on.
THEOREM 3.
For any Ï• :
t â†’Rd such that Ï• âˆˆ
, we have V m
Vt(Ï•) and V m,r
t (Ï•). These inequalities are attained for a nonconstant Ï•
if and only if Ï€c
t (Â·|Î¾t) = ËœÏ€c
t (Â·|Î¾t) for almost every Î¾t âˆˆ
t, for any t â‰¥0.
As suggested by the condition for equality above or more clearly exhibited in the
proof in the Appendix, marginalizing allows for canceling the weight dispersion
due to the discrepancy between conditional densities ËœÏ€c
t , while the part due
to the discrepancy between marginal densities Ï€m
t remains identical.
Beyond the small number of cases where this marginalization technique can be
effectively carried out, this result has also strong qualitative implications. In the
following sections we will study the behavior of the time sequence Vt(Ï•) in order
to measure whether and at which rate a given particle ï¬lter â€œdiverges.â€ In this
respect, we will be able in some cases to build a marginalized particle ï¬lter whose
rate of divergence is theoretically known, thus providing a lower bound for the
actual rate of divergence of the considered particle ï¬lter.
3. Stability of particle ï¬lters.
3.1. Sequential importance sampling.
The sequential importance sampling
algorithm is a particle ï¬lter that alternates mutation and correction steps, but does
not perform any selection step. Weights are consequently not initialized to one at
each iteration, and are rather updated through
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2395
We suppress any notational dependence on H since it is meaningless in such a
case. Due to its speciï¬c nature, this algorithm needs to be treated separately. Since
particles are not resampled, they remain independent through iterations. It follows
via the standard central limit theorem that
â†’N {0,V sis
where the corresponding asymptotic variance is
(Ï•) = E ËœÏ€t
and ËœÏ€t denotes this time the generating distribution of particles Î¸(j)
obtained by
the recursion of mutation kernels kt(Â·,Â·), that is,
ËœÏ€tâˆ’1(Î¸tâˆ’1)kt(Î¸tâˆ’1,Â·)dÎ¸tâˆ’1,
the distribution ËœÏ€0 being arbitrary. Sequential importance sampling is rarely an
efï¬cient algorithm, but the value of V sis
(Ï•) can serve as a benchmark in some
occasions, as we will see in the following.
3.2. Sequential importance sampling and resampling in the ï¬xed parameter
In the ï¬xed parameter case, that is, t =  and Ï€t(Î¸) = Ï€(Î¸|y1: t), Ï€t is
expected to become more and more informative on Î¸, and to eventually converge
to a Dirac mass at some point Î¸0. Sequential importance sampling and resampling
algorithms typically diverge in such a situation, since they generate once and for all
the set of particle values from ËœÏ€0, a majority of which are presumably far from Î¸0.
The following result quantiï¬es this degeneracy effect.
THEOREM 4.
Let Ï• : â†’Rd, Ï• âˆˆ
. Then under regularity conditions
given in the Appendix, there exist positive constants c1, c2 and c3 such that
(Ï•)âˆ¥â‰c1tp/2âˆ’1,
t (Ï•)âˆ¥â‰c2tp/2,
âˆ¥Vt(Ï•)âˆ¥â‰c3tp/2,
as t goes toward inï¬nity, where âˆ¥Â· âˆ¥denotes the Euclidean norm, p is the
dimension of  and V r
t (Ï•), Vt(Ï•) refer here to the sequential importance
resampling case, that is, kt(Î¸,Â·) = Î´Î¸.
The conditions mentioned above amount to assuming that Ï€t is the posterior
density of a model regular enough to ensure the existence and asymptotic
normality of the maximum likelihood estimator. Under such conditions, Ï€t can be
approximated at ï¬rst order as a Gaussian distribution centered at Î¸0 with variance
I (Î¸0)âˆ’1/t, where I (Î¸0) is the Fisher information matrix evaluated at Î¸0. The
results above are then derived through the Laplace approximation of integrals;
see the Appendix. At ï¬rst glance, it seems paradoxical that V sis
(Ï•) converges to
zero when p = 1. Note, however, that the ratio Vt(Ï•)/VarÏ€t(Ï•), which measures
the precision of the algorithm relative to the variation of the considered function,
is likely to diverge even when p = 1, since typically VarÏ€t(Ï•) â‰I (Î¸0)âˆ’1/t as
That the sequential importance resampling algorithm diverges more quickly
than the sequential importance sampling algorithm in this context is unsurprising:
when particles are not mutated, the only effect of a selection step is to deplete the
particle system. In this respect, we have for any nonconstant function Ï•,
t (Ï•) â‰¤Vt(Ï•).
The proof of this inequality is straightforward.
Due to its facility of implementation and the results above, it may be
recommended to use the sequential importance sampling algorithm for studying
short series of observations, provided that the dimension of  is low. But,
in general, one should rather implement a more elaborate particle ï¬lter which
includes mutation steps in order to counter the particle depletion. A further
implication of these results is the following. Consider a dynamical model
which involves a ï¬xed parameter Î¸, and assume that the marginal posterior
distributions Ï€(Î¸|y1: t), obtained by marginalizing out latent variables x1: t, satisfy
the regularity conditions of Theorem 4. Then, following the argument developed
in Section 2.5, we get that the rate of divergence of the sequential importance
resampling algorithm for this kind of model is at least of order O(tp/2), where
p is the dimension of this ï¬xed parameter.
3.3. Sequential importance sampling and resampling for Bayesian ï¬ltering
and smoothing.
For simplicity we assume that Ï€t(x1: t) = Ï€(x1: t|y1: t) is the
posterior density of a state space model with latent Markov process (xt), xt âˆˆX,
and observed process (yt), yt âˆˆY, which satisï¬es the equations
yt|xt âˆ¼f (yt|xt)dyt,
xt|xtâˆ’1 âˆ¼g(xt|xtâˆ’1)dxt.
We distinguish two types of functions: those which are deï¬ned on common
dimensions of the spaces t = Xt, say, Ï• :x1: t â†’Ï•(xk), for t â‰¥k, and those
which are evaluated on the â€œlastâ€ dimension of t, that is, Ï• :x1: t â†’Ï•(xt).
Evaluating these two types of functions amounts to, respectively, â€œsmoothingâ€ or
â€œï¬lteringâ€ the states.
The sequential importance sampling algorithm is usually very inefï¬cient in
such a context, whether for smoothing or ï¬ltering the states. We illustrate this
phenomenon by a simple example. Assume the tth mutation step consists of
drawing xt from the prior conditional density g(xt|xtâˆ’1), which is usually easy
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2397
to implement. Consider two evolving particles Î¸(j)
1: t with weights w(j)
j = 1,2. We have
log f (yk|x(1)
f (yk|x(2)
Assuming that the joint process (yt,x(1)
) is stationary, the sum above
typically satisï¬es some central limit theorem of the form
log f (yk|x(1)
f (yk|x(2)
â†’N (0,Ïƒ 2),
where the limiting distribution is centered for symmetry reasons. Note that
this convergence is with respect to the joint probability space of the simulated
processes x(j)
, j = 1,2 and the observation process (yt), while all our previous
results were for a given sequence of observations. In this way, (13) yields that
the ratio of weights of the two particles either converges or diverges exponentially
fast. More generally, when H particles are generated initially, very few of them
will have a prominent weight after some iterations, thus leading to very unreliable
estimates, whether for smoothing or ï¬ltering the states. The algorithm suffers from
the curse of dimensionality, in that its degeneracy grows exponentially with the
dimension of the space of interest t.
We now turn to the sequential importance resampling algorithm, and remark
ï¬rst that, for Ï• :x1: t â†’Ï•(x1) and t > 0,
Vt(Ï•) â‰¥V r
t (Ï•) > V sis
provided Ï• is not constant. The proof of this inequality is straightforward. The
sequential importance resampling algorithm is even more inefï¬cient than the
sequential importance sampling algorithm in smoothing the ï¬rst state x1, because
the successive selection steps only worsen the deterioration of the particle system
in the x1 dimension. This is consistent with our claim in Section 2.4 that a selection
step always degrades the inference on past and current states, but may possibly
improve the inference on future states. In this respect, the algorithm is expected to
show more capability in ï¬ltering the states, and we now turn to the study of the
ï¬ltering stability.
The functional operator Et which appears in the expression for Vt(Ï•), see (9),
summarizes two antagonistic effects: on one hand, the weight distortion due to
the correction step, and, on the other hand, the rejuvenation of particles due to
the application of the kernel kt. Stability will be achieved provided that these two
effects compensate in some way.
For simplicity, we assume that the state space X is included in the real line and
that the studied ï¬ltering function Ï• :x1: t â†’Ï•(xt) is real-valued. Recall that for
the sequential importance resampling algorithm, kt is given by
1: tâˆ’1,dx1: t) = Î´xâˆ—
1: tâˆ’1qt(xt|xâˆ—
1: tâˆ’1)dxt,
for some given conditional probability density qt(Â·|Â·). We assume that qt only
depends on the previous state xtâˆ’1, and, therefore, deï¬nes a Markov transition.
The ability of qt to â€œforget the pastâ€ is usually expressed through its contraction
coefï¬cient [see Dobrushin ]
âˆ¥qt(Â·|xâ€²) âˆ’qt(Â·|xâ€²â€²)âˆ¥1,
where âˆ¥Â· âˆ¥1 stands for the L1-norm. Note Ït â‰¤1, and if Ït < 1, qt is said to be
strictly contractive. Deï¬ne the variation of a given function Ï• by
|Ï•(x) âˆ’Ï•(xâ€²)|.
Then the coefï¬cient Ït measures the extent to which the application qt â€œcontractsâ€
the variation of the considered function, that is, for any xâ€²,xâ€²â€² âˆˆX,
qt(x|xâ€²)Ï•(x)dx âˆ’
qt(x|xâ€²â€²)Ï•(x)dx
 â‰¤ÏtÏ•.
Furthermore, it is known [Dobrushin ] that if qt is such that, for all
x,xâ€²,xâ€²â€² âˆˆX,
qt(x|xâ€²â€²) â‰¤C,
then its contraction coefï¬cient satisï¬es Ït â‰¤1 âˆ’Câˆ’1. We therefore make such
assumptions in order to prove the stability of the sequential importance resampling
algorithm.
THEOREM 5.
Assume that Ï• < +âˆand there exist constants C, f and Â¯f
such that, for any t â‰¥0, x,xâ€²,xâ€²â€² âˆˆX, y âˆˆY,
g(x|xâ€²â€²) â‰¤C,
qt(x|xâ€²â€²) â‰¤C,
0 < f â‰¤f (y|x) â‰¤Â¯f .
Then Vt(Ï•) is bounded from above in t , Le Gland and Oudjane and most especially, KÃ¼nsch
 ], except that these authors rather consider the stability of some
distance (such as the total variation norm of the difference) between the â€œtrueâ€
ï¬ltering density Ï€t(xt) and the empirical density computed from the particle
system. In fact, Del Moral and Miclo [ , page 36] proved that the actual
variance of the Monte Carlo error is bounded from above over time under
similar conditions. Unfortunately, all these results, including ours, require strong
assumptions, such as (15), that are unrealistic when X is not compact. Further
research will hopefully provide weaker assumptions, but this may prove an
especially arduous problem.
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2399
3.4. Resample-move algorithms, variance estimation.
Following Gilks and
Berzuini , we term â€œresample-move algorithmâ€ any particle ï¬lter algorithm
which includes an MCMC step in order to reduce degeneracy, as described in
Section 2.2. It seems difï¬cult to make general statements about such algorithms
and we will rather make informal comments.
The ï¬xed parameter case is especially well behaved. Basic particle ï¬lters
diverge only at a polynomial rate, as seen in Section 3.2, in contrast with the
exponential rate for state-space models. Adding (well-calibrated) MCMC mutation
steps should, consequently, lead to stable algorithms in many cases of interest. In
fact, it is doubtful that a mutation step must be performed at each iteration to
achieve stability. Chopin argues and provides some experimental evidence
that it may be sufï¬cient to perform move steps at a logarithmic rate, that is, the nth
move step should occur at iteration tn âˆ¼exp(Î±n).
Situations where a latent process intervenes seem less promising. Smoothing
the states is especially a difï¬cult problem, and we do not think that there is
any solution for circumventing the curse of dimensionality that we have pointed
out in the previous section. Even if mutation steps are performed at every
iteration, the MCMC transition kernels should themselves suffer from the curse of
dimensionality, in that their ability to rejuvenate particles of dimension t is likely
to decrease with t.
Resample-move algorithms remain an interesting alternative when the considered dynamical model includes a ï¬xed parameter Î¸. MCMC mutation steps should
avoid depletion in simulated values of Î¸, and make it possible at least to ï¬lter the
states and estimate the parameter under reasonable periods of time. Unfortunately,
the corresponding MCMC transition kernels will often depend on the whole past
trajectory, so that long term stability remains uncertain.
In such complicated setups it is necessary to monitor at least numerically the
degeneracy of the considered particle ï¬lter algorithm. We propose the following
method. Run k, say k = 10, parallel independent particle ï¬lters of size H. For any
quantity to be estimated, compute the average of the k corresponding estimates.
This new estimator is clearly consistent and asymptotically normal. Moreover, the
computational cost of this strategy is identical to that of a single particle ï¬lter of
size kH, while the obtained precision will be also of the same order of magnitude
in both cases, that is to say {Vt(Ï•)/(kH)}1/2. This method does not, therefore,
incur an unnecessary computational load, and allows for assessing the stability of
the algorithm through the evolution of the empirical variance of these k estimates.
A.1. Proofs of Theorems 1 and 2.
We start by outlining some basic
properties of the sets
with respect to linearity. The set
is stable through
linear transformations, that is, Ï• âˆˆ
if M is a dâ€² Ã— d matrix
of real numbers. In particular, if the vector function Ï• = (Ï•1,...,Ï•d)â€² belongs to
, then each of its coordinates belongs to
t . The converse proposition is also
true. Finally, we have Vt(MÏ• + Î») = MVt(Ï•)Mâ€² for any constant Î» âˆˆRd, and this
relation also holds for the operators Vt and Vt. Proving these statements is not
difï¬cult and is left to the reader.
The proof works by induction with Lemmas A.1â€“A.3 for Theorem 1, and
Lemmas A.1, A.2 and A.4 for Theorem 2. The inductive hypothesis is the
following. For a given t > 0, it is assumed that for all Ï• âˆˆ
 Ë†Î¸ (j,H)
 âˆ’EÏ€tâˆ’1(Ï•)
â†’N {0, Vtâˆ’1(Ï•)}.
LEMMA A.1 (Mutation).
Under the inductive hypothesis, we have
 âˆ’E ËœÏ€t(Ïˆ)
â†’N {0, Vt(Ïˆ)}
for any measurable Ïˆ :t â†’Rd such that the function Âµ:Î¸tâˆ’1
â†’Ekt(Î¸tâˆ’1,Â·){Ïˆ(Â·)âˆ’
E ËœÏ€t(Ïˆ)} belongs to
tâˆ’1 and there exists Î´ > 0 such that E ËœÏ€tâˆ¥Ïˆâˆ¥2+Î´ < +âˆ.
We assume that Ïˆ is real-valued (d = 1). The generalization to d > 1
follows directly from the CramÃ©râ€“Wold theorem and the linearity properties stated
Let Â¯Ïˆ = Ïˆ âˆ’E ËœÏ€t(Ïˆ), Âµ(Î¸tâˆ’1) = Ekt(Î¸tâˆ’1,Â·){ Â¯Ïˆ(Â·)}, Ïƒ 2(Î¸tâˆ’1) = Varkt(Î¸tâˆ’1,Â·){ Â¯Ïˆ(Â·)}
0 = EÏ€tâˆ’1(Ïƒ 2). We have EÏ€tâˆ’1(Âµ) = 0, and by Jensenâ€™s inequality,
Varkt(Î¸tâˆ’1,Â·){Ïˆ(Â·)}
Ekt(Î¸tâˆ’1,Â·){Ïˆ(Â·)2}
E ËœÏ€t|Ïˆ|(2+Î´)
2/(2+Î´) < +âˆ,
which makes it possible to apply the law of large numbers for particle ï¬lters to Ïƒ 2,
Ïƒ 2Î¸(j,H)
almost surely.
Î½(Î¸tâˆ’1) = Ekt(Î¸tâˆ’1,Â·){| Â¯Ïˆ(Â·) âˆ’Âµ(Î¸tâˆ’1)|2+Î´}
â‰¤21+Î´ Ektâˆ’1(Î¸tâˆ’1,Â·)| Â¯Ïˆ(Â·)|2+Î´ +
Ektâˆ’1(Î¸tâˆ’1,Â·) Â¯Ïˆ(Â·)
â‰¤22+Î´ Ektâˆ’1(Î¸tâˆ’1,Â·)| Â¯Ïˆ(Â·)|2+Î´
where (19) comes from the Cr inequality and (20) from Jensenâ€™s inequality, we
deduce that
EÏ€tâˆ’1(Î½) â‰¤22+Î´E ËœÏ€t|Ïˆ|2+Î´ < +âˆ.
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2401
This inequality ensures that the expectations deï¬ning Î½ in (18) (and, similarly,
those deï¬ning Âµ and Ïƒ 2) are ï¬nite for almost every Î¸tâˆ’1. It follows that
 â†’EÏ€tâˆ’1(Î½)
almost surely,
and combining this result with (17), we obtain the almost sure convergence of
j=1 Î½(Î¸(j,H)
j=1 Ïƒ 2(Î¸(j,H)
tâˆ’1 )}(2+Î´)/2
j=1 Î½(Î¸(j,H)
j=1 Ïƒ 2(Î¸(j,H)
tâˆ’1 )}(2+Î´)/2 â†’0.
Let TH = H âˆ’1/2 H
j=1 Â¯Ïˆ(Î¸ (j,H)
), Stâˆ’1 denote the sigma-ï¬eld generated by the
random variables forming the triangular array ( Ë†Î¸(j,H)
tâˆ’1 )jâ‰¤H, that is, the particle
system at time t âˆ’1, and ÂµH = E(TH |Stâˆ’1). Conditional on Stâˆ’1, the Â¯Ïˆ(Î¸(j,H)
form a triangular array of independent variables which satisfy the Liapunov
condition, see (21), and have variances whose mean converges to Ïƒ 2
0 , see (17).
Therefore [Billingsley , page 362], the following central limit theorem for
triangular arrays of independent variables holds:
(TH âˆ’ÂµH)|Stâˆ’1
Since EÏ€tâˆ’1(Âµ) = 0 and Âµ âˆˆ
tâˆ’1, we have also, by applying (16) to the
function Âµ,
ÂµH = H âˆ’1/2
 Ë†Î¸ (j,H)
â†’N {0, Vtâˆ’1(Âµ)}.
The characteristic function of TH is
TH (u) = E{exp(iuTH )}
= E[exp(iuÂµH)E{exp(iuTH âˆ’iuÂµH)|Stâˆ’1}],
where E{exp(iuTH âˆ’iuÂµH)|Stâˆ’1} is the characteristic function of TH âˆ’ÂµH
conditional on Stâˆ’1, which according to (22) converges to exp(âˆ’Ïƒ 2
0 u2/2). It
follows from (23) that
exp(iuÂµH)E{exp(iuTH âˆ’iuÂµH)|Stâˆ’1} D
0 u2/2 + iuZ),
where Z is a random variable distributed according to N {0, Vtâˆ’1(Âµ)}. The
expectation of the left-hand side term converges to the expectation of the righthand side term following the dominated convergence theorem, and this completes
the proof.
LEMMA A.2 (Correction).
, assume the inductive hypothesis
holds and the function Î¸t
â†’1 belongs to
j=1 w(j,H)
j=1 w(j,H)
â†’N {0,Vt(Ï•)}.
Let Â¯Ï• = Ï• âˆ’EÏ€t(Ï•). For notational convenience we assume that
d = 1, but the generalization to d â‰¥1 is straightforward. It is clear that the vector
function Ïˆ = (Ï…t Â· Â¯Ï•,Ï…t)â€² fulï¬lls the conditions mentioned in Lemma A.1, and as
such satisï¬es
â†’N {0, Vt(Ïˆ)}.
Then, resorting to the Î´-method with function g(x,y) = x/y, we obtain
j=1 Ï…t(Î¸(j,H)
) Â¯Ï•(Î¸(j,H)
j=1 Ï…t(Î¸(j,H)
where V = {(âˆ‚g/âˆ‚x,âˆ‚g/âˆ‚y)(0,1)}Vt(Ïˆ){(âˆ‚g/âˆ‚x,âˆ‚g/âˆ‚y)(0,1)}â€² = Vt{Ï…t Â· (Ï• âˆ’
EÏ€tÏ•)}. The left-hand side term is unchanged if we replace the Ï…t(Î¸(j,H)
)â€™s by the
weights w(j,H)
, since they are proportional.
LEMMA A.3 (Selection, multinomial resampling).
Let Vt(Ï•) = Vt(Ï•) +
VarÏ€t(Ï•) and assume the particle system is resampled according to the multinomial
scheme. Then, under the same conditions as in Lemma A.2,
 Ë†Î¸ (j,H)
â†’N {0, Vt(Ï•)}.
The proof is similar to that of Lemma A.1. Assume d = 1, denote
by St the sigma-ï¬eld generated by the random variables (Î¸(j,H)
and let Â¯Ï• = Ï• âˆ’EÏ€t(Ï•), TH = H âˆ’1/2 H
j=1 Â¯Ï•( Ë†Î¸(j,H)
) and ÂµH = E(TH |St).
Conditional on St, TH is, up to a factor H âˆ’1/2, a sum of independent draws
from the multinomial distribution which produces Â¯Ï•(Î¸(j,H)
) with probability
j=1 w(j,H)
. Then, as in Lemma A.1, we have
(TH âˆ’ÂµH)|St
where this time Ïƒ 2
0 = VarÏ€t(Ï•), which is the limit as H â†’+âˆof the variance of
the multinomial distribution mentioned above. The proof is completed along the
same lines as in Lemma A.1.
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2403
LEMMA A.4 (Selection, residual resampling).
Let Vt(Ï•) take the value given
by (7) and assume the particle system is resampled according to the residual
resampling scheme. Then, under the same conditions as in Lemma A.2,
 Ë†Î¸ (j,H)
â†’N {0, Vt(Ï•)}.
The proof is identical to that of Lemma A.2, except that conditional
on St, TH is H âˆ’1/2 times a constant, plus a sum of independent draws from the
multinomial distribution described in Section 2.1. This yields a different value
0 = E ËœÏ€t{r(Ï…t) Â· Ï•2} âˆ’
E ËœÏ€t{r(Ï…t)}
E ËœÏ€t{r(Ï…t) Â· Ï•}
In addition, we also have to make sure that the number of these independent
draws H r tends toward inï¬nity. In fact, H r/H â†’E ËœÏ€t[r(Î½t)]. To see this, consider
H r/H âˆ’H âˆ’1
where HÏj = Ï…t(Î¸(j,H)
j Ï…t(Î¸(j,H)
)}, see Section 2.1, so that the
difference above should eventually be zero as H âˆ’1 
j Ï…t(Î¸(j,H)
) â†’1. More
precisely, we have |r(x)âˆ’r(y)| â‰¤1, in general, and r(x)âˆ’r(y) = x âˆ’y provided
|x âˆ’y| < Îµ and r(x) âˆˆ[Îµ,1 âˆ’Îµ] for any Îµ < 1/2. Therefore, assuming that
j Ï…t(Î¸(j,H)
)}âˆ’1 âˆˆ[1 âˆ’Îµâ€²,1 + Îµâ€²] for some Îµâ€² > 0 and H large enough, we
get that the sum above should be zero plus something bounded from above by the
proportion of particles such that Îµâ€²Ï…t(Â·) > 1/2 or r{Ï…t(Â·)} /âˆˆ[Îµâ€²Ï…t(Â·),1 âˆ’Îµâ€²Ï…t(Â·)].
This proportion can be made as small as necessary.
A.2. Proof of Theorem 3.
t â†’Rd and Â¯Ï• = Ï• âˆ’EÏ€t(Ï•) = Ï• âˆ’
t (Ï•) for a given t â‰¥0. To simplify notation, it is assumed that d = 1, but
the adaptation to the general case is straightforward. All quantities related to
the â€œmarginalizedâ€ particle ï¬lter are distinguished by the m-sufï¬x. For instance,
t (Ï•) stands for the function Î¾t
t (Î¾t,Â·){Ï…m
t (Â·)Ï•(Â·)}, in agreement with the
deï¬nition of Et(Ï•) in (10). In this respect, the marginal weight function Ï…m
t (Â·), and if we deï¬ne the â€œconditionalâ€ weight function Ï…c
t (Î»t|Î¾t) =
t (Î»t|Î¾t)/ ËœÏ€c
t (Î»t|Î¾t), we have the identity
Ï…t(Î¸t) = Ï…m
t (Î»t|Î¾t).
It follows from (12) that
tâˆ’1{Et( Â¯Ï•)} = Ekm
t Â¯Ï•E ËœÏ€ct (Ï…c
since E ËœÏ€ct (Ï…c
t ) = 1, and by induction, we show similarly, for k â‰¤t, that
k {Ek+1: t( Â¯Ï•)} = Em
k+1: t( Â¯Ï•).
Hence, for k â‰¤t,
E ËœÏ€k[{Ï…kEk+1: t( Â¯Ï•)}2] = E ËœÏ€m
kEk+1: t Â¯Ï•}2
k+1: t( Â¯Ï•)}2],
by Jensenâ€™s inequality. From the closed form (9) of Vt(Ï•), we deduce the inequality
t (Ï•) â‰¤Vt(Ï•) for the case when the selection step follows the multinomial
scheme. Alternatively, if the selection step consists of residual resampling, let
Ï• = Ï• âˆ’E ËœÏ€t{r(Ï…t)Ï•}/E ËœÏ€t{r(Ï…t)}. Then
t (Ï•) = E ËœÏ€t{r(Ï…t)Ï•2} âˆ’E ËœÏ€m
t )Ï•2} + {E ËœÏ€m
E ËœÏ€ct r(Ï…t) âˆ’r(Ï…m
and since E ËœÏ€ct (Ï…t) = Ï…m
t , we have E ËœÏ€ct âŒŠÏ…tâŒ‹â‰¤âŒŠÏ…m
t âŒ‹, hence E ËœÏ€ct r(Ï…t) â‰¥r(Ï…m
consequently, Rt(Ï•) â‰¥Rm
t (Ï•) for any Ï•. It is then easy to show by induction that
the desired inequality is also veriï¬ed in the residual case.
A.3. Regularity conditions and proof of Theorem 4.
Let Ï€0(Î¸) denote the
prior density and p(y1: t|Î¸) the likelihood of the t ï¬rst observations, so that through
Bayes formula,
Ï€t(Î¸) = Ï€(Î¸|y1: t) âˆÏ€0(Î¸)p(y1: t|Î¸).
Let lt(Î¸) = logp(y1: t|Î¸). The following statements are assumed to hold almost
1. The maximum Ë†Î¸t of lt(Î¸) exists and converges as t â†’+âˆto Î¸0 such that
Ï€0(Î¸0) > 0 and ËœÏ€0(Î¸0) > 0.
2. The matrix
is positive deï¬nite and converges to I (Î¸0), the Fisher information matrix at Î¸0.
3. There exists  > 0 such that
{lt(Î¸) âˆ’lt( Ë†Î¸t)}
4. The functions Ï€0(Î¸) and lt(Î¸) are six-times continuously differentiable, the
partial derivatives of order six of lt(Î¸)/t are bounded on any compact set
â€² âŠ‚, and the bound does not depend on t and the observations.
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2405
5. Ï• : â†’Rd is six-times continuously differentiable, Ï•â€²(Î¸0) Ì¸= 0.
For convenience, we start with the one-dimensional case (p = 1). The Laplace
approximation of an integral [see, e.g., Tierney, Kass and Kadane ] is
Ïˆ(Î¸)exp{âˆ’th(Î¸)}dÎ¸
= (2Ï€/t)1/2Ïƒ exp{âˆ’t Ë†h}
Ïƒ 2 Ë†Ïˆâ€²â€² âˆ’Ïƒ 4 Ë†Ïˆâ€² Ë†hâ€²â€²â€² + 5
12Ïƒ 6 Ë†Ïˆ Ë†hâ€²â€²â€² âˆ’1
4Ïƒ 4 Ë†Ïˆ Ë†hiv
tâˆ’1 + O(tâˆ’2)
where hats on Ïˆ, h and their derivatives indicate evaluation at the point which
minimizes h, and Ïƒ = âˆ’(1/Ë†hâ€²â€²)1/2. This approximation remains valid for a
function ht depending on t, provided that the ï¬‚uctuations of ht or its derivatives
can be controlled in some way. Conditions above allow, for instance, for applying
this approximation to the functions ht,1(Î¸) = âˆ’lt(Î¸)/t and ht,2(Î¸) = âˆ’2lt(Î¸)/t;
see Schervish [ , page 446] for technical details. It is necessary, however,
to assume that Ïˆ(Î¸0) Ì¸= 0, so that Ïˆ is either strictly positive or strictly negative
at least in a neighborhood of Î¸0. Since V sis
(Ï•) = V sis
(Ï• + Î») for any Î» âˆˆR, we
assume without loss of generality that Ï•(Î¸0) Ì¸= 0. V sis
(Ï•) equals
 Ïˆ1(Î¸)p(y1: t|Î¸)2 dÎ¸ âˆ’2EÏ€t(Ï•)
 Ïˆ2(Î¸)p(y1: t|Î¸)2 dÎ¸
 Ï€(Î¸)p(y1: t|Î¸)dÎ¸}2
+ {EÏ€t(Ï•)}2  Ïˆ3(Î¸)p(y1: t|Î¸)2 dÎ¸
 Ï€(Î¸)p(y1: t|Î¸)dÎ¸}2
where Ïˆ1 = Ï€0(Î¸)2Ï•(Î¸)2/ ËœÏ€0(Î¸), Ïˆ2 = Ï€0(Î¸)2Ï•(Î¸)/ ËœÏ€0(Î¸) and Ïˆ3 = Ï€0(Î¸)2/
ËœÏ€0(Î¸). Combining the appropriate Laplace approximations, we get that
Ã— [Ïˆ1( Ë†Î¸t) âˆ’2EÏ€t(Ï•)Ïˆ2( Ë†Î¸t) + {EÏ€t(Ï•)}2Ïˆ3( Ë†Î¸t) + Atâˆ’1 + O(tâˆ’2)]
{Ï€0( Ë†Î¸t) + Btâˆ’1 + O(tâˆ’2)}2
{Ï•( Ë†Î¸t) âˆ’EÏ€t(Ï•)}2 + A ËœÏ€0( Ë†Î¸t)Ï€0( Ë†Î¸t)âˆ’2tâˆ’1 + O(tâˆ’2)
ËœÏ€0( Ë†Î¸t){1 + BÏ€0( Ë†Î¸t)âˆ’1tâˆ’1 + O(tâˆ’2)}2
where A is the sum of O(tâˆ’1) terms corresponding to the three Laplace expansions
of the numerator, and B is the O(tâˆ’1) term of the denominator. Since Ï•( Ë†Î¸t) âˆ’
EÏ€t(Ï•) = O(tâˆ’1), t = I (Î¸0) + O(tâˆ’1) and Ïˆ( Ë†Î¸t) = Ïˆ(Î¸0) + O(tâˆ’1) for any
continuous function Ïˆ, we get through appropriate derivations that
(Ï•) = I (Î¸0)1/2Ï•â€²(Î¸0)2
2Ï€1/2 ËœÏ€0(Î¸0) tâˆ’1/2 + O(tâˆ’3/2).
Derivations in multidimensional cases are much the same, except that notation
is more cumbersome. When p > 1, the factor tâˆ’1/2 in the Laplace expansion is
replaced by tâˆ’p/2, so that in the ratio (24) we get a factor tp/2, and since the tp/2
terms cancel as in the one-dimensional case, the actual rate of divergence is tp/2âˆ’1,
and this completes the ï¬rst part of the proof.
In the sequential importance resampling case (multinomial scheme), qt(Î¸,Â·) =
Î´Î¸ and ËœÏ€t = Ï€tâˆ’1, and according to (9),
Vt(Ï•) = V sis
Then through a direct adaptation of expansions above we obtain a divergence rate
for Vt(Ï•) of order (t
k=0(t âˆ’k)p/2âˆ’1) = O(tp/2). For the residual case, it follows
from (11) and (25) that
t (Ï•) = V sis
The difï¬culty in this case is that the noncontinuous function r(Â·) takes part in
the expression for Rk(Â·), see (8). It is clear, however, that the Laplace expansion
can be generalized to cases where regularity conditions for the likelihood and
other functions are fulï¬lled only locally around Î¸0. The additional assumption
that Ï€t(Î¸0)/Ï€tâˆ’1(Î¸0) is not an integer for any t > 0 allows r(Ï…t) to be six-times
continuously differentiable in a neighborhood around Î¸0, and, therefore, makes it
possible to expand the terms of the sum above, which leads to a rate of divergence
of order O(tp/2) in the same way as in the multinomial case.
A.4. Proof of Theorem 5.
As a preliminary, we state without proof the
following inequality. Let Ï•,Ïˆ :R â†’R such that Ï• â‰¥0, supÏˆ â‰¥0 and infÏˆ â‰¤0.
(Ï•Ïˆ) â‰¤supÏ• Â· Ïˆ.
Due to particular cancelations, the weight function Ï…t(x1: t) only depends on
xtâˆ’1 and xt in the state space case
Ï…t(x1: t) = Ï…t(xtâˆ’1,xt) âˆf (yt|xt)g(xt|xtâˆ’1)
qt(xt|xtâˆ’1)
Straightforward consequences of this expression are the identities
Ï€t(xt|xtâˆ’1) =
qt(xt|xtâˆ’1)Ï…t(xtâˆ’1,xt)
 qt(x|xtâˆ’1)Ï…t(xtâˆ’1,x)dx ,
Ï€t+1(xt+1|xk) =
 Ï€t(xt|xk)qt+1(xt+1|xt)Ï…t+1(xt,xt+1)dxt
 Ï€t(xt|xk)qt+1(x|xt)Ï…t+1(xt,x)dxt dx ,
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2407
for k < t, where Ï€t(xt|xk) denotes the conditional posterior density of xt given xk
and the t ï¬rst observations, that is, Ï€t(xt|xk) = Ï€(xt|xk,y1: t) = Ï€(xt|xk,yk+1: t).
We start by proving some useful lemmas.
LEMMA A.5.
The conditional posterior density Ï€t(xt|xk), k < t, deï¬nes a
Markov transition from xk to xt whose contraction coefï¬cient is less than or equal
to (1 âˆ’Câˆ’2)tâˆ’k.
This is adapted from KÃ¼nsch . For xk,xâ€²
k,xk+1 âˆˆX, k < t,
Ï€t(xk+1|xk)
Ï€t(xk+1|xâ€²
k) = g(xk+1|xk)p(yk+1: t|xâ€²
k)p(yk+1: t|xk) â‰¤C2,
since g(xk+1|xk) â‰¤Cg(xk+1|xâ€²
p(yk+1: t|xâ€²
k)p(yk+1: t|xk+1)dxk+1
g(xk+1|xk)p(yk+1: t|xk+1)dxk+1.
Therefore, the contraction coefï¬cients of Markov transitions Ï€t(xk+1|xk) and
Ï€t(xt|xk) are less than or equal to, respectively, (1 âˆ’Câˆ’2) and (1 âˆ’Câˆ’2)tâˆ’k.
LEMMA A.6.
Let Î» be a probability density on X and h(x|xâ€²) a conditional
probability density deï¬ning a Markov transition on X. Then for any xâ€² âˆˆX, y âˆˆY,
 f (y|x)h(x|xâ€²)dx
 f (y|x)h(x|xâ€²â€²)dx} â‰¤1 + ÏhCf ,
where Ïh is the contraction coefï¬cient of h(Â·|Â·), and Cf = Â¯f /f âˆ’1.
It follows from the deï¬nition of Ïh [see (14)] that for xâ€²,xâ€²â€² âˆˆX,
f (y|x)h(x|xâ€²)dx âˆ’
f (y|x)h(x|xâ€²â€²)dx
 â‰¤Ïh( Â¯f âˆ’f )
and therefore,
f (y|x)h(x|xâ€²)dx
f (y|x)h(x|xâ€²â€²)dx
+ Ïh( Â¯f âˆ’f ),
 f (y|x)h(x|xâ€²)dx}
 f (y|x)h(x|xâ€²â€²)dx} â‰¤1 + Ïh
 f (y|x)h(x|xâ€²â€²)dx}
LEMMA A.7.
Let Ï = 1 âˆ’Câˆ’1 and Ï2 = 1 âˆ’Câˆ’2. Then for k < t,
(1 + ÏÏiâˆ’1
for any real-valued ï¬ltering function, Ï• :x1: t â†’Ï•(xt).
Let Â¯Ï• = Ï• âˆ’EÏ€t(Ï•). Note the arguments of Ek+1: t( Â¯Ï•) are x1: k in
general, but in the case considered in Section 3.3 it only depends on xk and is
therefore treated as a function X â†’X. For the sake of clarity, we treat the case
k = t âˆ’2, but the reasoning is easily generalized. The following decomposition is
deduced from identity (28):
Etâˆ’1: t( Â¯Ï•)(xtâˆ’2)
= Eqtâˆ’1(xtâˆ’1|xtâˆ’2){Ï…tâˆ’1(xtâˆ’2,xtâˆ’1)Et( Â¯Ï•)(xtâˆ’1)}
= Eqtâˆ’1(xtâˆ’1|xtâˆ’2){Ï…tâˆ’1(xtâˆ’2,xtâˆ’1)}EÏ€tâˆ’1(xtâˆ’1|xtâˆ’2){Et( Â¯Ï•)(xtâˆ’1)}.
It follows from (27) that the ï¬rst term satisï¬es
Eqtâˆ’1(xtâˆ’1|xtâˆ’2){Ï…tâˆ’1(xtâˆ’2,xtâˆ’1)} âˆ
f (ytâˆ’1|xtâˆ’1)g(xtâˆ’1|xtâˆ’2)dxtâˆ’1,
where the proportionality constant can be retrieved by remarking that the
expectation of this term with respect to Ï€tâˆ’2 equals one and, therefore,
Eqtâˆ’1(xtâˆ’1|xtâˆ’2){Ï…tâˆ’1(xtâˆ’2,xtâˆ’1)}
 f (ytâˆ’1|xtâˆ’1)g(xtâˆ’1|xtâˆ’2)dxtâˆ’1
EÏ€tâˆ’2(xtâˆ’2){
 f (ytâˆ’1|xtâˆ’1)g(xtâˆ’1|xtâˆ’2)dxtâˆ’1}
according to Lemma A.6. Note Ï€tâˆ’2(xtâˆ’2) denotes the Ï€tâˆ’2-marginal density
of xtâˆ’2. It follows from the decomposition above and the inequality in (26) that
Etâˆ’1: t( Â¯Ï•) â‰¤(1 + ÏCf )Ïˆ,
where Ïˆ is the function
Ïˆ(xtâˆ’2) = EÏ€tâˆ’1(xtâˆ’1|xtâˆ’2){Et( Â¯Ï•)(xtâˆ’1)}
= EÏ€tâˆ’1(xtâˆ’1|xtâˆ’2)
Eqt(xt|xtâˆ’1){Ï…t(xtâˆ’1,xt) Â¯Ï•(xt)}
Note that Ïˆ does take positive and negative values, since the expectation of
Etâˆ’1: t( Â¯Ï•) with respect to Ï€tâˆ’2 is null. We now decompose Ïˆ in the same way,
Ïˆ(xtâˆ’2) = EÏ€tâˆ’1(xtâˆ’1|xtâˆ’2)
Eqt(xt|xtâˆ’1){Ï…t(xtâˆ’1,xt)}
EÏ€t(xt|xtâˆ’2){ Â¯Ï•(xt)},
CENTRAL LIMIT THEOREM FOR SEQUENTIAL MONTE CARLO METHODS 2409
by consequence of the identity (29). The expectation of the ï¬rst term with respect
to Ï€tâˆ’1(xtâˆ’2) equals one, so that
EÏ€tâˆ’1(xtâˆ’1|xtâˆ’2)
Eqt(xt|xtâˆ’1){Ï…t(xtâˆ’1,xt)}
 Ï€tâˆ’1(xtâˆ’1|xtâˆ’2)f (yt|xt)g(xt|xtâˆ’1)dxtâˆ’1 dxt
EÏ€tâˆ’1(xtâˆ’2){
 Ï€tâˆ’1(xtâˆ’1|xtâˆ’2)f (yt|xt)g(xt|xtâˆ’1)dxtâˆ’1 dxt}
â‰¤1 + ÏÏ2Cf ,
according to Lemmas A.5 and A.6. Resorting again to inequality (26), we get
Ïˆ â‰¤(1 + ÏÏ2Cf )Ï2
which leads to the desired inequality, and this completes the proof of Lemma A.7.
To conclude the proof of Theorem 5, remark that E ËœÏ€k(Ï…k) = 1. Therefore,
Ï…k(xkâˆ’1,xk) =
f (yk|xk)g(xk|xkâˆ’1)/qk(xk|xkâˆ’1)
E ËœÏ€k(x1: k){f (yk|xk)g(xk|xkâˆ’1)/qk(xk|xkâˆ’1)}
â‰¤C2 Â¯f /f ,
and since the expectation of the function Ek+1: t{Ï• âˆ’EÏ€t(Ï•)} with respect to Ï€k
is null, the function Ek+1: t{Ï• âˆ’EÏ€t(Ï•)} is ensured to take positive and negative
values, so that
 â‰¤Ek+1: t
and, ï¬nally,
â‰¤C4( Â¯f /f )2
(1 + ÏÏiâˆ’1
Cf )2Ï2(tâˆ’k)
â‰¤C4( Â¯f /f )2 exp
â‰¤C4( Â¯f /f )2 exp{2ÏCf /(1 âˆ’Ï2)}Ï2(tâˆ’k)
It follows from (9) that Vt(Ï•) is bounded from above by a convergent series.
Acknowledgments.
This paper is the fourth part of my Ph.D. thesis, defended
on March 2003 at UniversitÃ© Pierre et Marie Curie, Paris. It was inspired in
part by Hans KÃ¼nschâ€™s lectures on particle ï¬lters at the â€œSummer School on
Advanced Computational Methods for Statistical Inference,â€ Luminy, September
2001, and beneï¬ted from helpful comments by Christian Robert, Hans KÃ¼nsch,
Eric Moulines, Pierre Del Moral, one anonymous referee and an Associate Editor.