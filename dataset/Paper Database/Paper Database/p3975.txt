Adaptive Lightweight Text Filtering
Gabriel L. Somlo and Adele E. Howe
Colorado State University
Computer Science Dept.
Fort Collins, CO 80523, U.S.A.
{howe, somlo}@cs.colostate.edu
tel.: +1 (970) 491-7589
fax: +1 (970) 491-2466
Adaptive Lightweight Text Filtering
Gabriel L. Somlo and Adele E. Howe
Colorado State University
Computer Science Dept.
Fort Collins, CO 80523, U.S.A.
{howe, somlo}@cs.colostate.edu
Abstract. We present a lightweight text ﬁltering algorithm intended
for use with personal Web information agents. Fast response and low resource usage were the key design criteria, in order to allow the algorithm
to run on the client side. The algorithm learns adaptive queries and dissemination thresholds for each topic of interest in its user proﬁle. We
describe a factorial experiment used to test the robustness of the algorithm under diﬀerent learning parameters and more importantly, under
limited training feedback. The experiment borrows from standard practice in TREC by using TREC-5 data to simulate a user reading and
categorizing documents. Results indicate that the algorithm is capable
of achieving good ﬁltering performance, even with little user feedback.
Introduction
Text ﬁltering makes binary decisions about whether to disseminate documents
that arrive from a dynamic incoming stream. Adaptive ﬁltering systems start
out with little or no information about the user’s needs, and the decision of
whether to disseminate a document must be made when the document becomes
available. The system is given feedback on each disseminated document to update
its user proﬁle and improve its ﬁltering performance.
In this paper, we present a lightweight ﬁltering system designed for use in
personal Web information agents. We assess how well the algorithm works with
little feedback, a requirement for its application as a personal information gathering agent. Also, we assess the eﬀect of algorithm parameters on robustness.
We make two key contributions in our research. First, our algorithm adapts
standard ﬁltering techniques to the needs of personalized web applications:
lightweight, privacy protecting and responsive to user provided examples. The
algorithm learns a proﬁle of user information interests. Second, we adapt a rigorous evaluation method to web systems by using text ﬁltering benchmarks to
simulate user behavior. Traditionally, Web systems have been evaluated with
user studies, with the disadvantages of slow data collection, little experimental
control and decreased objectivity of conclusions. Relying on simulated user feedback allows us to test many alternative design decisions before subjecting the
system to a user study, which means we are less likely to waste subjects’ time
and are more likely to produce a well tuned system.
Adaptive Lightweight Text Filtering
Filtering Algorithm
At its core, our ﬁltering algorithm uses TF-IDF vectors to represent documents and topic queries. As documents arrive from the incoming stream, they
are ﬁrst transformed into TF (term frequency) vectors, by associating each distinct word in the document with its frequency count, and then are weighted by
each word’s IDF (inverse document frequency). The components of the vectors
are computed using the well-known formula: TFIDFd,t = TFd,t · IDFt where
TFd,t is the term frequency of t in document d. For any given term, its IDF is
computed based on the fraction of the total documents that contain this term at
least once: IDFt = log
DFt where D is the number of documents in the collection, and DFt is the number of documents containing t (document frequency).
Traditionally, D and DF values assume the existence of a ﬁxed corpus, in
which documents are known a priori. In ﬁltering, however, we do not assume
random access to all documents. Thus, D and DF are approximated: every time
a new incoming document is processed, we increment D and the entries into DF
that correspond to the terms contained in the new document.
The similarity between two TF-IDF vectors (e.g., between a query and a
document vector) is computed using the cosine metric , i.e., the normalized
dot product between the two vectors. This measures the cosine of the “angle”
between the two vectors, and ranges from 1 (when the vectors have the same
direction) to 0 (when the vectors have nothing in common).
For ﬁltering, topic queries generalize subjects of interest to a user. In addition
to a TF-IDF vector, each topic query in the proﬁle maintains a dissemination
threshold, the minimum similarity between the vector of an incoming document
and the topic’s query vector required for disseminating the document. Dissemination thresholds range between . When a new document arrives, its vector
is compared to each topic query vector; if the similarity exceeds the dissemination threshold for at least one topic, the document is shown to the user.
Filtering algorithms start with proﬁle queries created by converting a description of each topic into a TF-IDF vector; the vectors are updated based only
on feedback from documents that have been disseminated . Our algorithm
starts with an empty proﬁle. Learning occurs from positive examples of relevant
document-topic pairs provided by the user. For our application, new topics are
created in the user proﬁle when the user submits a relevant document for a nonexistent topic; the document’s TF-IDF vector forms the initial query vector for
the topic and the dissemination threshold is initialized to 0.5. Earlier, pilot experiments showed that the initial value did not matter; feedback quickly moves
the value within a short distance of its apparent ideal.
For existing topics, the query vector is learned using a mechanism inspired by
relevance feedback . Both the original query vector and the document vector
are normalized to eliminate the inﬂuence of document length, after which the
document vector is weighted by a document weight w and added to the query
vector. The document weight controls how much inﬂuence the original query
vector and the new document vector will each have on the updated query.
Somlo and Howe
feedback document
radius = (1− threshold)
raise threshold for more precision
feedback document
radius = (1− threshold)
lower threshold for more recall
Fig. 1. Learning the dissemination threshold
AddDocToTopic(doc, topic)
s = similarity(doc.vector, topic.vector);
normalize(doc.vector);
normalize(topic.vector);
topic.vector + = w · doc.vector;
normalize(topic.vector);
topic.threshold + = α · (s −topic.threshold);
Fig. 2. Topic learning mechanism
Like the query vector, the dissemination threshold also needs to be adapted.
Callan proposes a dissemination threshold that starts low and grows in time to
improve precision. The system presented in features dissemination thresholds that can both increase and decrease, but the algorithm heavily relies on
numerous cached documents to optimize the dissemination threshold.
To keep our ﬁltering system lightweight, we prohibit the algorithm from
caching past document vectors. Instead, the threshold tracks the similarity between the original query vector and the latest feedback document, with a learning
rate α. We can imagine the original query vector to be at the center of a “bubble”, with a radius given by the dissemination threshold. If the new feedback
document is inside this bubble, we can raise the threshold (shrink the bubble),
to improve precision. For feedback documents outside the bubble, we must lower
the threshold to improve recall. This process is illustrated graphically in Figure
1. Unlike Callan’s method, ours permits non-monotonic changes to the threshold
vector. The learning algorithm, for both the query vector and the dissemination
threshold, is presented in pseudocode in Figure 2. The pseudocode includes two
critical parameters: w (document weight) and α (learning rate).
Personal Web Information Agents
Personal Web information agents, such as Letizia , WebWatcher and Web-
Mate , can be tailored, over time, to help their users ﬁnd information that
matches their own interests. The helper agent intercedes between the user and
their Web access, learning a model of user requests to be used in modifying requests and/or ﬁltering results. Letizia and WebWatcher recommend links that
Adaptive Lightweight Text Filtering
are likely to lead to interesting documents by pre-fetching pages (Letizia) or
comparing the proﬁle to a ﬁxed collection (WebWatcher). WebMate uses greedy
incremental clustering to build its user proﬁle.
Web information agents are similar to text ﬁltering systems. Both types of
systems examine an incoming document stream; agents often generate the stream
for themselves by crawling the Web or by extracting links from a Web page during browsing. Both types of systems maintain user proﬁles. Both systems make
binary decisions about the documents in the stream: whether to recommend
them to the user based on their similarity to the user proﬁle. Thus, we propose
that ﬁltering algorithms such as the one presented in Section 2 be used with
Web helper agents, and that performance evaluation of these agents be done
with rigorous methods borrowed from IR, and speciﬁcally from text ﬁltering.
We simulate the behavior of the user with data from a text ﬁltering benchmark test. The agent starts with an empty proﬁle. The user can create new
topics by showing the agent a relevant document and providing a topic name.
The agent will compare all new documents from its incoming stream against
each topic in the proﬁle, and decide whether to disseminate them to the user.
How the incoming stream is generated is beyond the scope of this paper, but
any method used by the Web agents mentioned earlier in this section will do.
Usually, in text ﬁltering, only disseminated documents are used for feedback
 . Our scenario relaxes this requirement by allowing users to present documents
they encountered independently when surﬁng the Web. At any time, a user can
create a new topic or reinforce an existing one. It is likely though that users will
stop providing feedback on a given topic after a certain number of documents,
and expect the agent to continue its ﬁltering task without further instruction.
Performance Evaluation
The purpose of our study is to determine the robustness of our ﬁltering system,
in particular its sensitivity to the amount of feedback and to parameter settings.
The experiment proceeds as follows: For each new incoming document, the system predicts relevance based on the current user proﬁle. The new document is
compared to each internal proﬁle topic vector, and a record is made when any of
the dissemination thresholds have been passed. Then, the “user” provides feedback for relevant documents, which is simulated with the relevance ratings in
the datasets. The new document is added to all topics for which it is known to
be relevant. Our independent variables are:
w document weight that controls how much inﬂuence the new document vector
exerts on the updated query. Values below one favor the original query vector.
Our experiments used: 0.1, 0.15, 0.2, 0.25, 0.5, 1.0, 2.0, and 4.0.
α the learning rate for the dissemination threshold, with values of 0.1, 0.3, 0.5,
0.7, and 0.9.
Umax the maximum number of updates (documents provided as feedback) for
each topic, with values of 5, 10, 20, 30, 40, 50, 60, 70, 100, 200, and ∞(as
many as possible).
Somlo and Howe
We use two datasets: the Foreign Broadcasting Information Service (FBIS)
collection, and the Los Angeles Times (LATIMES) collection from TREC1 disk
# 5. The FBIS and the LATIMES collections consist of 48527 and 36330 news
articles, classiﬁed into 194 topics. Because the ﬁrst document that is relevant
to a topic is always used to create an entry for the topic in the user proﬁle, we
only use topics with more than one relevant document, which reduces the total
number of topics to 186.
For each combination of independent variables, we record:
RF number of relevant documents found correctly as relevant;
RM number of relevant documents missed (incorrectly identiﬁed as non-relevant);
NF number of non-relevant documents found as relevant;
These values are used to compute the following performance metrics:
number of relevant documents
correctly disseminated
number of relevant disseminated
precision =
performance metric proposed by
Lewis and Gale , assume equal
weights for recall and precision.
LGF = 2·precision·recall
precision+recall
TREC-8 evaluation 
LF2 = 3RF −NF
First, we test the inﬂuence of algorithm parameters under ideal conditions, when
all possible learning information is used (Umax = ∞). Then, we evaluate robustness by reducing the amount of feedback to the algorithm, i.e., smaller values of
Umax. A single user building their own web proﬁle (our application) cannot be
expected to provide thousands of relevance judgments for every topic.
Impact of Parameters α and w when Umax = ∞
A two-way Analysis of Variance indicates that both α and w, individually and
jointly, strongly inﬂuence the value of recall (p < 0.00001). Recall is consistently
good for values of α ≥0.7 and w ≤0.5, but strongly decreases outside this
region. A surface plot as well as an ANOVA table for each dataset are shown in
Figure 3. Performance depends primarily on α and is close to optimal as long as
w is within a range of values. The best parameter values are in Table 1.
Precision depends strongly on α (p < 0.00001). As one would expect, precision is highest for the smallest tested value of α = 0.1, the most conservative
update. The document weight w also has a signiﬁcant eﬀect on precision, albeit
much smaller. The best values for precision seem to occur for w ∈[0.25, 1.0]. The
interaction eﬀects are negligible. 0.5104 at α = 0.1 and w = 1.0 for LATIMES.
1 TREC (Text REtrieval Conference) document collections can be ordered from
 
Adaptive Lightweight Text Filtering
Metric: Rec. Prec. LGF LF1 LF2 Rec. Prec. LGF LF1 LF2
0.511 0.516 0.453 4533 8441 0.493 0.510 0.386 1012 2097
Table 1. Best parameter values for each metric
403.36 0.00001
142.44 0.00001
27.59 0.00001
435.12 0.00001
96.07 0.00001
8.43 0.00626
a) Dataset: FBIS
b) Dataset: LATIMES
Fig. 3. Eﬀects of learning rate α and document weight w on recall
613.63 0.00001
9.52 0.00389
0.99 0.32598
427.54 0.00001
3.53 0.06824
2.41 0.12896
a) Dataset: FBIS
b) Dataset: LATIMES
Fig. 4. Eﬀects of learning rate α and document weight w on precision
The previous analysis considers recall and precision in isolation, which is
clearly unrealistic for our application. The measures that encompass both recall and precision (LGF, LF1, LF2) show strong eﬀects of w and α (see Figures 5,6,7). LGF and LF1 show interaction eﬀects. LGF shows almost best
performance within the interval, α ≤0.3 and w ≤0.5.
In conclusion, with unlimited training, the best learning parameters for our
algorithm are w ∈[0.2, 1.0], and α ≤0.3 for everything except recall, where val-
Somlo and Howe
348.49 0.00001
209.67 0.00001
17.58 0.00017
127.47 0.00001
99.41 0.00001
6.42 0.01574
a) Dataset: FBIS
b) Dataset: LATIMES
Fig. 5. Eﬀects of learning rate α and document weight w on LGF
1736.64 0.00001
36.18 0.00001
6.23 0.01726
1070.65 0.00001
10.36 0.00272
5.06 0.03063
a) Dataset: FBIS
b) Dataset: LATIMES
Fig. 6. Eﬀects of learning rate α and document weight w on TREC8 LF1
ues of α ≥0.9 give the best results. Fortunately for our application, performance
is always pretty good within the same interval on both datasets; thus, we would
expect our best parameter settings to generalize to other data sets.
Robustness with Less Feedback (Umax)
The two datasets vary wildly in the number of relevant documents per topic.
Histograms of the distribution of topics according to the number of known relevant documents they contain are given in Figure 8. To determine the eﬀects
of limiting training, we had to focus on topics for which many documents were
available. We used the topics in the fourth quartile of the above distributions:
Adaptive Lightweight Text Filtering
643.15 0.00001
109.24 0.00001
0.11 0.73996
426.38 0.00001
29.97 0.00001
0.99 0.32714
a) Dataset: FBIS
b) Dataset: LATIMES
Fig. 7. Eﬀects of learning rate α and document weight w on TREC8 LF2
Known Hits in Topic
Known Hits in Topic
a) Dataset: FBIS
b) Dataset: LATIMES
Fig. 8. Distribution of known relevant documents across topics
this includes 36 topics from the FBIS dataset, with 75 or more known relevant
documents, and 36 topics from the LATIMES dataset, with 31 or more known
relevant documents. These topics include the majority of known relevant documents in our datasets: 7,678 for FBIS, and 2,368 for LATIMES.
We plotted recall, precision, and LGF against Umax for each combination of
α and w in Figure 9. These plots show that the interaction eﬀects between Umax
and the two parameters are small for both datasets. Additionally, the eﬀect of
Umax is non-monotonic, and that while no parameter combination dominates all
values of Umax, some are consistently among the top few.
ANOVA tests indicate that Umax strongly inﬂuences recall: F = 128.34, p <
0.00001 for FBIS, and F = 36.30, p < 0.00001 for LATIMES. In comparison, the inﬂuence on precision is signiﬁcant only on the FBIS dataset (F =
47.36, p < 0.00001 for FBIS, F = 1.02, p < 0.31 for LATIMES). The Umax on
LGF is signiﬁcant for both datasets: F = 100.59, p < 0.00001 for FBIS, and
F = 31.07, p < 0.00001 for LATIMES. In contrast to the unlimited updates,
Somlo and Howe
10 20 30 40 50 60 70 100 200 INF
10 20 30 40 50 60 70 100 200 INF
10 20 30 40 50 60 70 100 200 INF
a) Dataset: FBIS
10 20 30 40 50 60 70 100 200 INF
10 20 30 40 50 60 70 100 200 INF
10 20 30 40 50 60 70 100 200 INF
b) Dataset: LATIMES
Fig. 9. Inﬂuence of Umax on recall, precision, and LGF for combinations of α and w
Umax = INF
Umax = INF
a) Dataset: FBIS
b) Dataset: LATIMES
Fig. 10. Inﬂuence of Umax on LGF for the FBIS dataset
limited updates favor high values of α, while w remains in the same range. This
result follows from the need to make the most out of the available data.
Interestingly, our plots show that very good results can be obtained after as
few as 10 updates per topic. After 10 updates per topic for FBIS, we reached
0.4489 recall and 0.4501 precision, resulting in 0.4495 LGF at α = 0.9 and
w = 0.5; LATIMES showed 0.4055 recall, 0.3818 precision, and LGF of 0.3933,
at α = 0.7 and w = 1.0. As an example, Figure 10 displays the surface plots of
LGF at Umax = 10 and Umax = ∞.
Conclusions
We presented a lightweight ﬁltering algorithm intended for use with Web information agents, and an evaluation method for such algorithms based on TREC
Adaptive Lightweight Text Filtering
benchmarks. We ran a factorial experiment to test the algorithm’s robustness.
We found that performance is robust within a relatively wide interval of parameter values. Importantly for our target application, we found that the algorithm
is robust against limited training information; we observed a slow degradation
in performance as the amount of feedback was reduced down to 10.
When used in a Web information agent, our algorithm will need to train with
up to 10 or 20 positive examples per topic. In consequence, the algorithm must
start out with a large learning rate for the dissemination threshold. An idea we
intend to test in the future is to lower α as more documents are used to train a
particular topic.
Using benchmarks instead of user studies has allowed us to collect large
amounts of information in a short period of time. First, we intend to test our
algorithm on more datasets, starting with the Financial Times document collection used in the ﬁltering track at recent TREC conferences. Ultimately, when
these algorithms have been incorporated into the web agent, we will test the
eﬃcacy of the design decisions derived here in a user study.