Low-Shot Learning from Imaginary Data
Yu-Xiong Wang1,2
Ross Girshick1
Martial Hebert2
Bharath Hariharan1,3
1Facebook AI Research (FAIR)
2Carnegie Mellon University
3Cornell University
Humans can quickly learn new visual concepts, perhaps
because they can easily visualize or imagine what novel
objects look like from different views. Incorporating this
ability to hallucinate novel instances of new concepts might
help machine vision systems perform better low-shot learning, i.e., learning concepts from few examples. We present
a novel approach to low-shot learning that uses this idea.
Our approach builds on recent progress in meta-learning
(“learning to learn”) by combining a meta-learner with a
“hallucinator” that produces additional training examples,
and optimizing both models jointly. Our hallucinator can
be incorporated into a variety of meta-learners and provides signiﬁcant gains: up to a 6 point boost in classiﬁcation accuracy when only a single training example is available, yielding state-of-the-art performance on the challenging ImageNet low-shot classiﬁcation benchmark.
1. Introduction
The accuracy of visual recognition systems has grown
dramatically.
But modern recognition systems still need
thousands of examples of each class to saturate performance. This is impractical in cases where one does not
have enough resources to collect large training sets or that
involve rare visual concepts. It is also unlike the human visual system, which can learn a novel visual concept from
even a single example . This challenge of learning new
concepts from very few labeled examples, often called lowshot or few-shot learning, is the focus of this work.
Many recently proposed approaches to this problem fall
under the umbrella of meta-learning . Meta-learning
methods train a learner, which is a parametrized function
that maps labeled training sets to classiﬁers. Meta-learners
are trained by sampling small training sets and test sets
from a large universe of labeled examples, feeding the sampled training set to the learner to get a classiﬁer, and then
computing the loss of the classiﬁer on the sampled test set.
These methods directly frame low-shot learning as an optimization problem.
However, generic meta-learning methods treat images as
blue heron
Figure 1. Given a single image of a novel visual concept, such as a
blue heron, a person can visualize what the heron would look like
in other poses and different surroundings. If computer recognition
systems could do such hallucination, they might be able to learn
novel visual concepts from less data.
black boxes, ignoring the structure of the visual world. In
particular, many modes of variation (for example camera
pose, translation, lighting changes, and even articulation)
are shared across categories. As humans, our knowledge of
these shared modes of variation may allow us to visualize
what a novel object might look like in other poses or surroundings (Figure 1). If machine vision systems could do
such “hallucination” or “imagination”, then the hallucinated
examples could be used as additional training data to build
better classiﬁers.
Unfortunately, building models that can perform such
hallucination is hard, except for simple domains like handwritten characters . For general images, while considerable progress has been made recently in producing realistic
samples, most current generative modeling approaches suffer from the problem of mode collapse : they are only
able to capture some modes of the data. This may be insufﬁcient for low-shot learning since one needs to capture many
modes of variation to be able to build good classiﬁers. Furthermore, the modes that are useful for classiﬁcation may
be different from those that are found by training an image generator. Prior work has tried to avoid this limitation
 
by explicitly using pose annotations to generate samples in
novel poses , or by using carefully designed, but brittle,
heuristics to ensure diversity .
Our key insight is that the criterion that we should aim
for when hallucinating additional examples is neither diversity nor realism. Instead, the aim should be to hallucinate
examples that are useful for learning classiﬁers. Therefore,
we propose a new method for low-shot learning that directly
learns to hallucinate examples that are useful for classiﬁcation by the end-to-end optimization of a classiﬁcation objective that includes data hallucination in the model.
We achieve this goal by unifying meta-learning with hallucination. Our approach trains not just the meta-learner,
but also a hallucinator: a model that maps real examples
to hallucinated examples. The few-shot training set is ﬁrst
fed to the hallucinator; it produces an expanded training set,
which is then used by the learner. Compared to plain metalearning, our approach uses the rich structure of shared
modes of variation in the visual world. We show empirically
that such hallucination adds a signiﬁcant performance boost
to two different meta-learning methods , providing
up to a 6 point improvement when only a single training example is available. Our method is also agnostic to the choice
of the meta-learning method, and provides signiﬁcant gains
irrespective of this choice. It is precisely the ability to leverage standard meta-learning approaches without any modiﬁcations that makes our model simple, general, and very easy
to reproduce. Compared to prior work on hallucinating examples, we use no extra annotation and signiﬁcantly outperform hallucination based on brittle heuristics . We also
present a novel meta-learning method and discover and ﬁx
ﬂaws in previously proposed benchmarks.
2. Related Work
Low-shot learning is a classic problem . One class
of approaches builds generative models that can share priors across categories . Often, these generative
models have to be hand-designed for the domain, such as
strokes or parts for handwritten characters. For
more unconstrained domains, while there has been signiﬁcant recent progress , modern generative models
still cannot capture the entirety of the distribution .
Different classes might not share parts or strokes, but
may still share modes of variation, since these often correspond to camera pose, articulation, etc.
If one has a
probability density on transformations, then one can generate additional examples for a novel class by applying sampled transformations to the provided examples .
Learning such a density is easier for handwritten characters that only undergo 2D transformations , but much
harder for generic image categories. Dixit et al. tackle
this problem by leveraging an additional dataset of images
labeled with pose and attributes; this allows them to learn
how images transform when the pose or the attributes are
altered. To avoid annotation, Hariharan and Girshick 
try to transfer transformations from a pair of examples from
a known category to a “seed” example of a novel class.
However, learning to do this transfer requires a carefully
designed pipeline with many heuristic steps. Our approach
follows this line of work, but learns to do such transformations in an end-to-end manner, avoiding both brittle heuristics and expensive annotations.
Another class of approaches to low-shot learning has focused on building feature representations that are invariant to intra-class variation. Some work tries to share features between seen and novel classes or incrementally learn them as new classes are encountered . Contrastive loss functions and variants of the triplet
loss have been used for learning feature representations suitable for low-shot learning; the idea is to push examples from the same class closer together, and farther from
other classes. Hariharan and Girshick show that one
can encourage classiﬁers trained on small datasets to match
those trained on large datasets by a carefully designed loss
function. These representation improvements are orthogonal to our approach, which works with any features.
More generally, a recent class of methods tries to
frame low-shot learning itself as a “learning to learn”
task, called meta-learning .
The idea is to directly
train a parametrized mapping from training sets to classi-
ﬁers. Often, the learner embeds examples into a feature
space. It might then accumulate statistics over the training set using recurrent neural networks (RNNs) ,
memory-augmented networks , or multilayer perceptrons (MLPs) , perform gradient descent steps to ﬁnetune
the representation , and/or collapse each class into prototypes . An alternative is to directly predict the classiﬁer
weights that would be learned from a large dataset using
few novel class examples or from a small dataset classiﬁer . We present a uniﬁed view of meta-learning
and show that our hallucination strategy can be adopted in
any of these methods.
3. Meta-Learning
Let X be the space of inputs (e.g., images) and Y be a
discrete label space. Let D be a distribution over X × Y.
Supervised machine learning typically aims to capture the
conditional distribution p(y|x) by applying a learning algorithm to a parameterized model and a training set Strain =
{(xi, yi) ∼D}N
i=1. At inference time, the model is evaluated on test inputs x to estimate p(y|x). The composition
of the inference and learning algorithms can be written as
a function h (a classiﬁcation algorithm) that takes as input
the training set and a test input x, and outputs an estimated
probability distribution ˆp over the labels:
ˆp(x) = h(x, Strain).
In low-shot learning, we want functions h that have high
classiﬁcation accuracy even when Strain is small. Metalearning is an umbrella term that covers a number of recently proposed empirical risk minimization approaches to
this problem .
Concretely, they consider parametrized classiﬁcation algorithms h(·, ·; w) and
attempt to estimate a “good” parameter vector w, namely
one that corresponds to a classiﬁcation algorithm that can
learn well from small datasets. Thus, estimating this parameter vector can be construed as meta-learning .
Meta-learning algorithms have two stages.
stage is meta-training in which the parameter vector w
of the classiﬁcation algorithm is estimated. During metatraining, the meta-learner has access to a large labeled
dataset Smeta that typically contains thousands of images
for a large number of classes C. In each iteration of metatraining, the meta-learner samples a classiﬁcation problem out of Smeta.
That is, the meta-learner ﬁrst samples a subset of m classes from C.
Then it samples a
small “training” set Strain and a small “test” set Stest. It
then uses its current weight vector w to compute conditional probabilities h(x, Strain; w) for every point (x, y)
in the test set Stest.
Note that in this process h may
perform internal computations that amount to “training”
on Strain.
Based on these predictions, h incurs a loss
L(h(x, Strain; w), y) for each point in the current Stest.
The meta-learner then back-propagates the gradient of the
total loss P
(x,y)∈Stest L(h(x, Strain; w), y). The number
of classes in each iteration, m, and the maximum number
of training examples per class, n, are hyperparameters.
The second stage is meta-testing in which the resulting
classiﬁcation algorithm is used to solve novel classiﬁcation
tasks: for each novel task, the labeled training set and unlabeled test examples are given to the classiﬁcation algorithm
and the algorithm outputs class probabilities.
Different meta-learning approaches differ in the form of
h. The data hallucination method introduced in this paper
is general and applies to any meta-learning algorithm of the
form described above. Concretely, we will consider the following three meta-learning approaches:
Prototypical networks: Snell et al. propose an architecture for h that assigns class probabilities based on distances from class means µk in a learned feature space:
h(x, Strain;w) = ˆp(x)
e−d(φ(x;wφ),µk)
j e−d(φ(x;wφ),µj)
(xi,yi)∈Strain φ(xi; wφ)I[yi = k]
(xi,yi)∈Strain I[yi = k]
Here ˆpk are the components of the probability vector ˆp
and d is a distance metric (Euclidean distance in ). The
only parameters to be learned here are the parameters of the
feature extractor wφ. The estimation of the class means µk
can be seen as a simple form of “learning” from Strain that
takes place internal to h.
Matching networks: Vinyals et al. argue that when
faced with a classiﬁcation problem and an associated training set, one wants to focus on the features that are useful for
those particular class distinctions. Therefore, after embedding all training and test points independently using a feature extractor, they propose to create a contextual embedding of the training and test examples using bi-directional
long short-term memory networks (LSTMs) and attention
LSTMs, respectively. These contextual embeddings can be
seen as emphasizing features that are relevant for the particular classes in question. The ﬁnal class probabilities are
computed using a soft nearest-neighbor mechanism. More
speciﬁcally,
h(x, Strain;w) = ˆp(x)
(xi,yi)∈Strain e−d(f(x),g(xi))I[yi = k]
(xi,yi)∈Strain e−d(f(x),g(xi))
f(x) =AttLSTM(φ(x; wφ), {g(xi)}N
i=1; wf) (7)
i=1 =BiLSTM({φ(xi; wφ)}N
Here, again d is a distance metric. Vinyals et al. used
the cosine distance. There are three sets of parameters to be
learned: wφ, wg, and wf.
Prototype matching networks: One issue with matching
networks is that the attention LSTM might ﬁnd it harder to
“attend” to rare classes (they are swamped by examples of
common classes), and therefore might introduce heavy bias
against them. Prototypical networks do not have this problem since they collapse every class to a single class mean.
We want to combine the beneﬁts of the contextual embedding in matching networks with the resilience to class imbalance provided by prototypical networks.
To do so, we collapse every class to its class mean before creating the contextual embeddings of the test examples. Then, the ﬁnal class probabilities are based on distances to the contextually embedded class means instead of
individual examples:
h(x, Strain;w) = ˆp(x)
e−d(f(x),νk)
j e−d(f(x),νj)
f(x) =AttLSTM(φ(x; wφ), {νk}|Y|
(xi,yi)∈Strain g(xi)I[yi = k]
(xi,yi)∈Strain I[yi = k]
i=1 =BiLSTM({φ(xi; wφ)}N
Figure 2. Meta-learning with hallucination. Given an initial training set Strain, we create an augmented training set Saug
adding a set of generated examples SG
train is obtained by
sampling real seed examples and noise vectors z and passing them
to a parametric hallucinator G. The hallucinator is trained end-toend along with the classiﬁcation algorithm h. Dotted red arrows
indicate the ﬂow of gradients during back-propagation.
The parameters to be learned are wφ, wg, and wf. We
call this novel modiﬁcation to matching networks prototype
matching networks.
4. Meta-Learning with Learned Hallucination
We now present our approach to low-shot learning by
learning to hallucinate additional examples. Given an initial
training set Strain, we want a way of sampling additional
hallucinated examples. Following recent work on generative modeling , we will model this stochastic process by way of a deterministic function operating on a noise
vector as input. Intuitively, we want our hallucinator to take
a single example of an object category and produce other
examples in different poses or different surroundings. We
therefore write this hallucinator as a function G(x, z; wG)
that takes a seed example x and a noise vector z as input,
and produces a hallucinated example as output. The parameters of this hallucinator are wG.
We ﬁrst describe how this hallucinator is used in metatesting, and then discuss how we train the hallucinator.
Hallucination during meta-testing: During meta-testing,
we are given an initial training set Strain. We then hallucinate ngen new examples using the hallucinator. Each hallucinated example is obtained by sampling a real example
(x, y) from Strain, sampling a noise vector z, and passing
x and z to G to obtain a generated example (x′, y) where
x′ = G(x, z; wG). We take the set of generated examples
train and add it to the set of real examples to produce an
augmented training set Saug
train = Strain ∪SG
train. We can
now simply use this augmented training set to produce conditional probability estimates using h. Note that the hallucinator parameters are kept ﬁxed here; any learning that
happens, happens within the classiﬁcation algorithm h.
Meta-training the hallucinator: The goal of the hallucinator is to produce examples that help the classiﬁcation algorithm learn a better classiﬁer. This goal differs from realism: realistic examples might still fail to capture the many
modes of variation of visual concepts, while unrealistic hallucinations can still lead to a good decision boundary .
We therefore propose to directly train the hallucinator to
support the classiﬁcation algorithm by using meta-learning.
As before, in each meta-training iteration, we sample
m classes from the set of all classes, and at most n examples per class.
Then, for each class, we use G to
generate ngen additional examples till there are exactly
naug examples per class.
Again, each hallucinated example is of the form (x′, y), where x′ = G(x, z; wG),
(x, y) is a sampled example from Strain and z is a sampled noise vector.
These additional examples are added
to the training set Strain to produce an augmented training set Saug
Then this augmented training set is fed
to the classiﬁcation algorithm h, to produce the ﬁnal loss
(x,y)∈Stest L(h(x, Saug
train), y), where Saug
train = Strain ∪
train and SG
train = {(G(xi, zi; wG), yi)ngen
i=1 : (xi, yi) ∈
To train the hallucinator G, we require that the classi-
ﬁcation algorithm h(x, Saug
train; w) is differentiable with respect to the elements in Saug
train. This is true for many metalearning algorithms. For example, in prototypical networks,
h will pass every example in the training set through a feature extractor, compute the class means in this feature space,
and use the distances between the test point and the class
means to estimate class probabilities. If the feature extractor is differentiable, then the classiﬁcation algorithm itself
is differentiable with respect to the examples in the training
set. This allows us to back-propagate the ﬁnal loss and update not just the parameters of the classiﬁcation algorithm
h, but also the parameters wG of the hallucinator. Figure 2
shows a schematic of the entire process.
Using meta-learning to train the hallucinator and the
classiﬁcation algorithm has two beneﬁts.
First, the hallucinator is directly trained to produce the kinds of hallucinations that are useful for class distinctions, removing
the need to precisely tune realism or diversity, or the right
modes of variation to hallucinate. Second, the classiﬁcation algorithm is trained jointly with the hallucinator, which
enables it to make allowances for any errors in the hallucination. Conversely, the hallucinator can spend its capacity
on suppressing precisely those errors which throw the classiﬁcation algorithm off.
Note that the training process is completely agnostic to
the speciﬁc meta-learning algorithm used. We will show in
our experiments that our hallucinator provides signiﬁcant
gains irrespective of the meta-learner.
5. Experimental Protocol
We use the benchmark proposed by Hariharan and Girshick . This benchmark captures more realistic scenarios than others based on handwritten characters or lowresolution images . The benchmark is based on ImageNet images and subsets of ImageNet classes. First, in the
representation learning phase, a convolutional neural network (ConvNet) based feature extractor is trained on one
set of classes with thousands of examples per class; this set
is called the “base” classes Cbase. Then, in the low-shot
learning phase, the recognition system encounters an additional set of “novel” classes Cnovel with a small number of
examples n per class. It also has access to the base class
training set. The system has to now learn to recognize both
the base and the novel classes. It is tested on a test set containing examples from both sets of classes, and it needs to
output labels in the joint label space Cbase ∪Cnovel. Hariharan and Girshick report the top-5 accuracy averaged over
all classes, and also the top-5 accuracy averaged over just
base-class examples, and the top-5 accuracy averaged over
just novel-class examples.
Tradeoffs between base and novel classes: We observed
that in this kind of joint evaluation, different methods had
very different performance tradeoffs between the novel and
base class examples and yet achieved similar performance
on average. This makes it hard to meaningfully compare the
performance of different methods on just the novel or just
the base classes. Further, we found that by changing hyperparameter values of some meta-learners it was possible to
achieve substantially different tradeoff points without substantively changing average performance. This means that
hyperparameters can be tweaked to make novel class performance look better at the expense of base class performance
(or vice versa).
One way to concretize this tradeoff is by incorporating
a prior over base and novel classes. Consider a classiﬁer
that gives a score sk(x) for every class k given an image
x. Typically, one would convert these into probabilities by
applying a softmax function:
pk(x) = p(y = k|x) =
However, we may have some prior knowledge about the
probability that an image belongs to the base classes Cbase
or the novel classes Cnovel. Suppose that the prior probability that an image belongs to one of the novel classes is µ.
Then, we can update Equation (14) as follows:
pk(x) = p(y = k|x)
= p(y = k|y ∈Cbase, x)p(y ∈Cbase|x)
+ p(y = k|y ∈Cnovel, x)p(y ∈Cnovel|x)
eskI[k ∈Cbase]
j esjI[j ∈Cbase](1 −µ)
eskI[k ∈Cnovel]
j esjI[j ∈Cnovel]µ.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Novel class prior
Top-5 accuracy (%)
All classes
Novel classes
Base classes
Figure 3. The variation of the overall, novel class, and base class
accuracy for MN in the evaluation proposed by Hariharan and Girshick as the novel class prior µ is varied.
The prior probability µ might be known beforehand, but can
also be cross-validated to correct for inherent biases in the
scores sk. However, note that in some practical settings, one
may not have a held-out set of categories to cross-validate.
Thus resilience to this prior is important.
Figure 3 shows the impact of this prior on matching networks in the evaluation proposed by Hariharan and Girshick . Note that the overall accuracy remains fairly stable, even as novel class accuracy rises and base class accuracy falls. Such prior probabilities for calibration were proposed for the zero-shot learning setting by Chao et al. .
A new evaluation: The existence of this tunable tradeoff
between base and novel classes makes it hard to make
apples-to-apples comparisons of novel class performance if
the model is tasked with making predictions in the joint label space. Instead, we use a new evaluation protocol that
evaluates four sets of numbers:
1. The model is given test examples from the novel
classes, and is only supposed to pick a label from the
novel classes.
That is, the label space is restricted
to Cnovel (note that doing so is equivalent to setting
µ = 1 for prototypical networks but not for matching
networks and prototype matching networks because of
the contextual embeddings). We report the top-5 accuracy on the novel classes in this setting.
2. Next, the model is given test examples from the base
classes, and the label space is restricted to the base
classes. We report the top-5 accuracy in this setting.
3. The model is given test examples from both the base
and novel classes in equal proportion, and the model
has to predict labels from the joint label space. We
report the top-5 accuracy averaged across all examples.
We present numbers both with and without a
novel class prior µ; the former set cross-validates µ
to achieve the highest average top-5 accuracy.
Note that, following , we use a disjoint set of classes for
cross-validation and testing. This prevents hyperparameter
choices for the hallucinator, meta-learner, and novel class
prior from becoming overﬁt to the novel classes that are
seen for the ﬁrst time at test time.
6. Experiments
6.1. Implementation Details
Unlike prior work on meta-learning which experiments
with small images and few classes , we use
high resolution images and our benchmark involves hundreds of classes. This leads to some implementation challenges. Each iteration of meta-learning at the very least has
to compute features for the training set Strain and the test
set Stest. If there are 100 classes with 10 examples each,
then this amounts to 1000 images, which no longer ﬁts in
memory. Training a modern deep convolutional network
with tens of layers from scratch on a meta-learning objective may also lead to a hard learning problem.
Instead, we ﬁrst train a convolutional network based feature extractor on a simple classiﬁcation objective on the
base classes Cbase. Then we extract and save these features to disk, and use these pre-computed features as inputs.
For most experiments, consistent with , we use a small
ResNet-10 architecture . Later, we show some experiments using the deeper ResNet-50 architecture .
Meta-learner architectures: We focus on state-of-the-art
meta-learning approaches, including prototypical networks
(PN) , matching networks (MN) , and our improvement over MN — prototype matching networks (PMN). For
PN, the embedding architecture consists of two MLP layers
with ReLU as the activation function. We use Euclidean
distance as in . For MN, following , the embedding
architecture consists of a one layer bi-directional LSTM that
embeds training examples and attention LSTM that embeds
test samples. We use cosine distance as in . For our
PMN, we collapse every class to its class mean before the
contextual embeddings of the test examples, and we keep
other design choices the same as those in MN.
Hallucinator architecture and initialization: For our hallucinator G, we use a three layer MLP with ReLU as the
activation function. We add a ReLU at the end since the
pre-trained features are known to be non-negative. All hidden layers have a dimensionality of 512 for ResNet-10 features and 2048 for ResNet-50 features. Inspired by , we
initialize the weights of our hallucinator network as block
diagonal identity matrices. This signiﬁcantly outperformed
standard initialization methods like random Gaussian, since
the hallucinator can “copy” its seed examples to produce a
reasonable generation immediately from initialization.
6.2. Results
As in , we run ﬁve trials for each setting of n (the
number of examples per novel class) and present the av-
# of examples / novel class
Improvement in top-5
accuracy (absolute)
Novel classes
(PN w/ G) - (PN)
(PMN w/ G) - (PMN)
# of examples / novel class
Improvement in top-5
accuracy (absolute)
All classes
(PN w/ G) - (PN)
(PMN w/ G) - (PMN)
(PN w/ G) - (PN) w/ prior
(PMN w/ G) - (PMN) w/ prior
Figure 4. Improvement in accuracy by learned hallucination for
different meta-learners as a function of the number of examples
available per novel class.
erage performance. Different approaches are comparably
good for base classes, achieving 92% top-5 accuracy. We
focus more on novel classes since they are more important
in low-shot learning. Table 1 contains a summary of the
top-5 accuracy for novel classes and for the joint space both
with and without a cross-validated prior. Standard deviations for all numbers are of the order of 0.2%. We discuss
speciﬁc results, baselines, and ablations below.
Impact of hallucination: We ﬁrst compare meta-learners
with and without hallucination to judge the impact of hallucination. We look at prototypical networks (PN) and prototype matching networks (PMN) for this comparison. Figure 4 shows the improvement in top-5 accuracy we get
from hallucination on top of the original meta-learner performance. The actual numbers are shown in Table 1.
We ﬁnd that our hallucination strategy improves novel
class accuracy signiﬁcantly, by up to 6 points for prototypical networks and 2 points for prototype matching networks.
This suggests that our approach is general and can work
with different meta-learners. While the improvement drops
when more novel category training examples become available, the gains remain signiﬁcant until n = 20 for prototypical networks and n = 5 for prototype matching networks.
Accuracy in the joint label space (right half of Figure 4)
shows the same trend. However, note that the gains from
hallucination decrease signiﬁcantly when we cross-validate
for an appropriate novel-class prior µ (shown in dotted
lines). This suggests that part of the effect of hallucination
is to provide resilience to mis-calibration. This is important
in practice where it might not be possible to do extensive
cross-validation; in this case, meta-learners with hallucination demonstrate signiﬁcantly higher accuracy than their
counterparts without hallucination.
Comparison to prior work: Figure 5 and Table 1 compare
our best approach (prototype matching networks with hallucination) with previously published approaches in low-shot
learning. These include prototypical networks , matching networks , and the following baselines:
1. Logistic regression: This baseline simply trains a linear classiﬁer on top of a pre-trained ConvNet-based
feature extractor that was trained on the base classes.
All with prior
45.8 57.8 69.0 74.3 77.4
57.6 64.7 71.9 75.2 77.5
56.4 63.3 70.6 74.0 76.2
43.3 55.7 68.4 74.0 77.0
55.8 63.1 71.1 75.0 77.1
54.7 62.0 70.2 73.9 75.9
45.0 55.9 67.3 73.0 76.5
56.9 63.2 70.6 74.5 76.5
55.6 62.1 69.3 73.1 75.4
39.3 54.4 66.3 71.2 73.9
49.5 61.0 69.7 72.9 74.6
53.6 61.4 68.8 72.0 73.8
43.6 54.0 66.0 72.5 76.9
54.4 61.0 69.0 73.7 76.5
54.5 60.7 68.2 72.6 75.6
38.4 51.1 64.8 71.6 76.6
40.8 49.9 64.2 71.9 76.9
52.9 60.4 68.6 72.9 76.3
LogReg w/ Analogies 40.7 50.8 62.0 69.3 76.5
52.2 59.4 67.6 72.8 76.9
53.2 59.1 66.8 71.7 76.3
54.7 66.8 77.4 81.4 83.8
65.7 73.5 80.2 82.8 84.5
64.4 71.8 78.7 81.5 83.3
53.3 65.2 75.9 80.1 82.6
64.8 72.1 78.8 81.7 83.3
63.4 70.8 77.9 80.9 82.7
53.9 65.2 75.7 80.2 82.8
65.2 72.0 78.9 81.7 83.1
63.9 70.5 77.5 80.6 82.4
49.6 64.0 74.4 78.1 80.0
61.4 71.4 78.0 80.0 81.1
62.9 70.5 77.1 79.5 80.8
53.5 63.5 72.7 77.4 81.2
64.9 71.0 77.0 80.2 82.7
63.8 69.9 75.9 79.3 81.9
Table 1. Top-5 accuracy on the novel classes and on all classes (with and without priors) for different values of n. ∗Our methods. PN: Prototypical networks, MN: Matching networks, PMN: Prototype matching networks, LogReg: Logistic regression. Methods with “w/ G” use
a meta-learned hallucinator.
# of examples / novel class
Top-5 accuracy (%)
Novel classes
LogReg w/ Analogies
# of examples / novel class
Top-5 accuracy (%)
Novel classes (zoom)
# of examples / novel class
Top-5 accuracy (%)
All classes (with prior)
Figure 5. Our best approach compared to previously published methods. From left to right: just the novel classes, zoomed in performance
for the case when the number of examples per novel class n ≤2, performance on the joint label space with a cross-validated prior.
2. Logistic regression with analogies:
This baseline
uses the procedure described by Hariharan and Girshick to hallucinate additional examples. These
additional examples are added to the training set and
used to train the linear classiﬁer.
Our approach easily outperforms all baselines, providing almost a 2 point improvement across the board on the
novel classes, and similar improvements in the joint label
space even after allowing for cross-validation of the novel
category prior. Our approach is thus state-of-the-art.
Another intriguing ﬁnding is that our proposed prototype
matching network outperforms matching networks on novel
classes as more novel class examples become available (Table 1). On the joint label space, prototype matching networks are better across the board.
Interestingly, the method proposed by Hariharan and
Girshick underperforms the standard logistic regression baseline (although it does show gains when the novel
class prior is not cross-validated, as shown in Table 1, indicating that its main impact is resilience to mis-calibration).
Unpacking the performance gain: To unpack where our
performance gain is coming from, we perform a series of
ablations to answer the following questions.
Are sophisticated hallucination architectures necessary?
In the semantic feature space learned by a convolutional network, a simple jittering of the training examples might be
enough. We created several baseline hallucinators that did
such jittering by: (a) adding Gaussian noise with a diagonal
covariance matrix estimated from feature vectors from the
base classes, (b) using dropout (PN/PMN w/ Dropout), and
(c) generating new examples through a weighted average of
real ones (PN/PMN w/ Weighted). For the Gaussian hallucinator, we evaluated both a covariance matrix shared across
classes and class-speciﬁc covariances. We found that the
shared covariance outperformed class-speciﬁc covariances
by 0.7 point and reported the best results. We tried both
retraining the meta-learner with this Gaussian hallucinator,
and using a pre-trained meta-learner: PN/PMN w/ Gaussian uses a pre-trained meta-learner and PN/PMN w/ Gaussian(tr) retrains the meta-learner. As shown in Figure 6,
# of examples / novel class
Improvement in top-5
accuracy (absolute)
Novel classes
(PN w/ G) - (PN)
(PN w/ Gaussian) - (PN)
(PN w/ Gaussian(tr)) - (PN)
(PN w/ init G) - (PN)
(PN w/ det. G) - (PN)
(PN w/ det. G(tr)) - (PN)
(PN w/ Dropout) - (PN)
(PN w/ Weighted) - (PN)
(PN w/ Analogies) - (PN)
# of examples / novel class
Improvement in top-5
accuracy (absolute)
Novel classes
(PMN w/ G) - (PMN)
(PMN w/ Gaussian) - (PMN)
(PMN w/ Gaussian(tr)) - (PMN)
(PMN w/ init G) - (PMN)
(PMN w/ det. G) - (PMN)
(PMN w/ det. G(tr)) - (PMN)
(PMN w/ Dropout) - (PMN)
(PMN w/ Weighted) - (PMN)
(PMN w/ Analogies) - (PMN)
Figure 6. Comparison of our learned hallucination with several
ablations for both PN (left) and PMN (right). Our approach signiﬁcantly outperforms the baselines, showing that a meta-learned
hallucinator is important. Best viewed in color with zoom.
while such hallucinations help a little, they often hurt signiﬁcantly, and lag the accuracy of our approach by at least 3
points. This shows that generating useful hallucinations is
not easy and requires sophisticated architectures.
Is meta-learning the hallucinator necessary?
Simply passing Gaussian noise through an untrained convolutional network can produce complex distributions. In
particular, ReLU activations might ensure the hallucinations
are non-negative, like the real examples. We compared hallucinations with (a) an untrained G and (b) a pre-trained
and ﬁxed G based on analogies from with our metatrained version to see the impact of our training. Figure 6
shows the impact of these baseline hallucinators (labeled
PN/PMN w/ init G and PN/PMN w/ Analogies, respectively). These baselines hurt accuracy signiﬁcantly, suggesting that meta-training the hallucinator is important.
Does the hallucinator produce diverse outputs?
A persistent problem with generative models is that they
fail to capture multiple modes . If this is the case, then
any one hallucination should look very much like the others, and simply replicating a single hallucination should be
enough. We compared our approach with: (a) a deterministic baseline that uses our trained hallucinator, but simply
uses a ﬁxed noise vector z = 0 (PN/PMN w/ det. G) and
(b) a baseline that uses replicated hallucinations during both
training and testing (PN/PMN w/ det. G(tr)). These baselines had a very small, but negative effect. This suggests
that our hallucinator produces useful, diverse samples.
Visualizing the learned hallucinations: Figure 7 shows t-
SNE visualizations of hallucinated examples for novel
classes from our learned hallucinator and a baseline Gaussian hallucinator for prototypical networks. As before, we
used statistics from the base class distribution for the Gaussian hallucinator.
Note that t-SNE tends to expand out
parts of the space where examples are heavily clustered together. Thus, the fact that the cloud of hallucinations for the
Gaussian hallucinator is pulled away from the class distributions suggests that these hallucinations are very close to
each other and far away from the rest of the class. In contrast, our hallucinator matches the class distributions more
(a) Gaussian baseline
(b) G with 1 seed
(c) 2 seeds
(d) 4 seeds
Figure 7. t-SNE visualizations of hallucinated examples. Seeds
are shown as stars, real examples as crosses, hallucinations as triangles. (a) Gaussian, single seed. (b,c,d) Our approach, 1, 2, and
4 seeds respectively. Best viewed in color with zoom.
closely, and with different seed examples captures different
parts of the space. Interestingly, our generated examples
tend to cluster around the class boundaries. This might be
an artifact of t-SNE, or perhaps a consequence of discriminative training of the hallucinator. However, our hallucinations are still fairly clustered; increasing the diversity of
these hallucinations is an avenue for future work.
Representations from deeper models: All experiments till
now used a feature representation trained using the ResNet-
10 architecture . The bottom half of Table 1 shows the
results on features from a ResNet-50 architecture. As expected, all accuracies are higher, but our hallucination strategy still provides gains on top of both prototypical networks
and prototype matching networks.
7. Conclusion
In this paper, we have presented an approach to lowshot learning that uses a trained hallucinator to generate additional examples. Our hallucinator is trained end-to-end
with meta-learning, and we show signiﬁcant gains on top of
multiple meta-learning methods. Our best proposed model
achieves state-of-the-art performance on a realistic benchmark by a comfortable margin. Future work involves pinning down exactly the effect of the hallucinated examples.
Acknowledgments: We thank Liangyan Gui, Larry Zitnick, Piotr
Doll´ar, Kaiming He, and Georgia Gkioxari for valuable and insightful discussions.
This work was supported in part by ONR MURI
N000141612007 and U.S. Army Research Laboratory (ARL) under the
Collaborative Technology Alliance Program, Cooperative Agreement
W911NF-10-2-0016. We also thank NVIDIA for donating GPUs and AWS
Cloud Credits for Research program.