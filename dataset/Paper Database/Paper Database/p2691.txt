Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 31–39,
Gothenburg, Sweden, April 26-30 2014. c⃝2014 Association for Computational Linguistics
Extractive Summarization using Continuous Vector Space Models
Mikael K˚ageb¨ack, Olof Mogren, Nina Tahmasebi, Devdatt Dubhashi
Computer Science & Engineering
Chalmers University of Technology
SE-412 96, G¨oteborg
{kageback, mogren, ninat, dubhashi}@chalmers.se
Automatic summarization can help users
extract the most important pieces of information from the vast amount of text digitized into electronic form everyday. Central to automatic summarization is the notion of similarity between sentences in
text. In this paper we propose the use of
continuous vector representations for semantically aware representations of sentences as a basis for measuring similarity.
We evaluate different compositions
for sentence representation on a standard
dataset using the ROUGE evaluation measures. Our experiments show that the evaluated methods improve the performance
of a state-of-the-art summarization framework and strongly indicate the beneﬁts
of continuous word vector representations
for automatic summarization.
Introduction
The goal of summarization is to capture the important information contained in large volumes of
text, and present it in a brief, representative, and
consistent summary. A well written summary can
signiﬁcantly reduce the amount of work needed to
digest large amounts of text on a given topic. The
creation of summaries is currently a task best handled by humans. However, with the explosion of
available textual data, it is no longer ﬁnancially
possible, or feasible, to produce all types of summaries by hand. This is especially true if the subject matter has a narrow base of interest, either due
to the number of potential readers or the duration
during which it is of general interest. A summary
describing the events of World War II might for
instance be justiﬁed to create manually, while a
summary of all reviews and comments regarding
a certain version of Windows might not. In such
cases, automatic summarization is a way forward.
In this paper we introduce a novel application
of continuous vector representations to the problem of multi-document summarization. We evaluate different compositions for producing sentence
representations based on two different word embeddings on a standard dataset using the ROUGE
evaluation measures. Our experiments show that
the evaluated methods improve the performance of
a state-of-the-art summarization framework which
strongly indicate the beneﬁts of continuous word
vector representations for this tasks.
Summarization
There are two major types of automatic summarization techniques, extractive and abstractive. Extractive summarization systems create summaries
using representative sentences chosen from the input while abstractive summarization creates new
sentences and is generally considered a more dif-
ﬁcult problem.
Illustration
Extractive
Document Summarization.
For this paper we consider extractive multidocument summarization, that is, sentences are
chosen for inclusion in a summary from a set of
documents D. Typically, extractive summarization techniques can be divided into two components, the summarization framework and the similarity measures used to compare sentences. Next
we present the algorithm used for the framework
and in Sec. 2.2 we discuss a typical sentence similarity measure, later to be used as a baseline.
Submodular Optimization
Lin and Bilmes formulated the problem of
extractive summarization as an optimization problem using monotone nondecreasing submodular
set functions. A submodular function F on the
set of sentences V satisﬁes the following property:
for any A ⊆B ⊆V \{v}, F(A + {v}) −F(A) ≥
F(B + {v}) −F(B) where v ∈V . This is called
the diminishing returns property and captures the
intuition that adding a sentence to a small set of
sentences (i.e., summary) makes a greater contribution than adding a sentence to a larger set. The
aim is then to ﬁnd a summary that maximizes diversity of the sentences and the coverage of the input text. This objective function can be formulated
as follows:
F(S) = L(S) + λR(S)
where S is the summary, L(S) is the coverage of
the input text, R(S) is a diversity reward function.
The λ is a trade-off coefﬁcient that allows us to
deﬁne the importance of coverage versus diversity
of the summary. In general, this kind of optimization problem is NP-hard, however, if the objective
function is submodular there is a fast scalable algorithm that returns an approximation with a guarantee. In the work of Lin and Bilmes a simple submodular function is chosen:
Sim(i, j), α
Sim(i, j)}
The ﬁrst argument measures similarity between
sentence i and the summary S, while the second argument measures similarity between sentence i and the rest of the input V . Sim(i, j) is
the similarity between sentence i and sentence j
and 0 ≤α ≤1 is a threshold coefﬁcient. The diversity reward function R(S) can be found in .
Traditional Similarity Measure
Central to most extractive summarization systems is the use of sentence similarity measures
(Sim(i, j) in Eq.
Lin and Bilmes measure
similarity between sentences by representing each
sentence using tf-idf 
vectors and measuring the cosine angle between
vectors. Each sentence is represented by a word
vector w = (w1, . . . , wN) where N is the size of
the vocabulary. Weights wki correspond to the tfidf value of word k in the sentence i. The weights
Sim(i, j) used in the L function in Eq. 1 are found
using the following similarity measure.
Sim(i, j) =
tfw,i × tfw,j × idf2
w,i × idf2
w,j × idf2
where tfw,i and tfw,j are the number of occurrences of w in sentence i and j, and idfw is the
inverse document frequency (idf) of w.
In order to have a high similarity between sentences using the above measure, two sentences
must have an overlap of highly scored tf-idf words.
The overlap must be exact to count towards the
similarity, e.g, the terms The US President and
Barack Obama in different sentences will not add
towards the similarity of the sentences. To capture deeper similarity, in this paper we will investigate the use of continuous vector representations
for measuring similarity between sentences. In the
next sections we will describe the basics needed
for creating continuous vector representations and
methods used to create sentence representations
that can be used to measure sentence similarity.
Background on Deep Learning
Deep learning 
is a modern interpretation of artiﬁcial neural networks (ANN), with an emphasis on deep network
architectures. Deep learning can be used for challenging problems like image and speech recognition ,
as well as language modeling , and in all cases, able to achieve state-ofthe-art results.
Inspired by the brain, ANNs use a neuron-like
construction as their primary computational unit.
The behavior of a neuron is entirely controlled by
its input weights. Hence, the weights are where
the information learned by the neuron is stored.
More precisely the output of a neuron is computed
as the weighted sum of its inputs, and squeezed
into the interval using a sigmoid function:
Figure 2: FFNN with four input neurons, one hidden layer, and 1 output neuron. This type of architecture is appropriate for binary classiﬁcation
of some data x ∈R4, however depending on the
complexity of the input, the number and size of the
hidden layers should be scaled accordingly.
where θi are the weights associated with neuron i
and x is the input. Here the sigmoid function (g) is
chosen to be the logistic function, but it may also
be modeled using other sigmoid shaped functions,
e.g. the hyperbolic tangent function.
The neurons can be organized in many different ways. In some architectures, loops are permitted. These are referred to as recurrent neural networks. However, all networks considered here are
non-cyclic topologies. In the rest of this section
we discuss a few general architectures in more detail, which will later be employed in the evaluated
Feed Forward Neural Network
A feed forward neural network (FFNN) is a type of ANN where the neurons are
structured in layers, and only connections to subsequent layers are allowed, see Fig 2. The algorithm is similar to logistic regression using nonlinear terms.
However, it does not rely on the
user to choose the non-linear terms needed to ﬁt
the data, making it more adaptable to changing
datasets. The ﬁrst layer in a FFNN is called the
input layer, the last layer is called the output layer,
and the interim layers are called hidden layers.
The hidden layers are optional but necessary to ﬁt
complex patterns.
Training is achieved by minimizing the network
error (E). How E is deﬁned differs between different network architectures, but is in general a
differentiable function of the produced output and
Reconstruction
Figure 3: The ﬁgure shows an auto-encoder that
compresses four dimensional data into a two dimensional code. This is achieved by using a bottleneck layer, referred to as a coding layer.
the expected output.
In order to minimize this
function the gradient ∂E
∂Θ ﬁrst needs to be calculated, where Θ is a matrix of all parameters, or
weights, in the network. This is achieved using
backpropagation . Secondly, these gradients are used to minimize E using e.g. gradient descent. The result of this processes is a set of weights that enables the network
to do the desired input-output mapping, as deﬁned
by the training data.
Auto-Encoder
An auto-encoder (AE) , see Fig. 3, is a type of FFNN with a topology designed for dimensionality reduction. The
input and the output layers in an AE are identical,
and there is at least one hidden bottleneck layer
that is referred to as the coding layer. The network is trained to reconstruct the input data, and
if it succeeds this implies that all information in
the data is necessarily contained in the compressed
representation of the coding layer.
A shallow AE, i.e. an AE with no extra hidden layers, will produce a similar code as principal component analysis. However, if more layers
are added, before and after the coding layer, nonlinear manifolds can be found. This enables the
network to compress complex data, with minimal
loss of information.
Recursive Neural Network
A recursive neural network (RvNN), see Fig. 4,
ﬁrst presented by Socher et al. , is a type of
feed forward neural network that can process data
through an arbitrary binary tree structure, e.g. a
Figure 4: The recursive neural network architecture makes it possible to handle variable length input data. By using the same dimensionality for all
layers, arbitrary binary tree structures can be recursively processed.
binary parse tree produced by linguistic parsing of
a sentence. This is achieved by enforcing weight
constraints across all nodes and restricting the output of each node to have the same dimensionality
as its children.
The input data is placed in the leaf nodes of
the tree, and the structure of this tree is used to
guide the recursion up to the root node. A compressed representation is calculated recursively at
each non-terminal node in the tree, using the same
weight matrix at each node. More precisely, the
following formulas can be used:
p [xl; xr]
yp = g(zp)
where yp is the computed parent state of neuron
p, and zp the induced ﬁeld for the same neuron.
[xl; xr] is the concatenation of the state belonging
to the right and left sibling nodes. This process results in a ﬁxed length representation for hierarchical data of arbitrary length. Training of the model
is done using backpropagation through structure,
introduced by Goller and Kuchler .
Word Embeddings
Continuous distributed vector representation of
words, also referred to as word embeddings, was
ﬁrst introduced by Bengio et al. . A word
embedding is a continuous vector representation
that captures semantic and syntactic information
about a word. These representations can be used
to unveil dimensions of similarity between words,
e.g. singular or plural.
Collobert & Weston
Collobert and Weston introduce an efﬁcient
method for computing word embeddings, in this
work referred to as CW vectors. This is achieved
ﬁrstly, by scoring a valid n-gram (x) and a corrupted n-gram (¯x) (where the center word has been
randomly chosen), and secondly, by training the
network to distinguish between these two n-grams.
This is done by minimizing the hinge loss
max(0, 1 −s(x) + s(¯x))
where s is the scoring function, i.e. the output of
a FFNN that maps between the word embeddings
of an n-gram to a real valued score. Both the parameters of the scoring function and the word embeddings are learned in parallel using backpropagation.
Continuous Skip-gram
A second method for computing word embeddings
is the Continuous Skip-gram model, see Fig. 5, introduced by Mikolov et al. . This model is
used in the implementation of their word embeddings tool Word2Vec. The model is trained to predict the context surrounding a given word. This is
accomplished by maximizing the objective function
−c≤j≤c,j̸=0
log p(wt+j|wt)
where T is the number of words in the training
set, and c is the length of the training context.
The probability p(wt+j|wt) is approximated using
the hierarchical softmax introduced by Bengio et
al. and evaluated in a paper by Morin and
Bengio .
Phrase Embeddings
Word embeddings have proven useful in many natural language processing (NLP) tasks. For summarization, however, sentences need to be compared.
In this section we present two different
methods for deriving phrase embeddings, which
in Section 5.3 will be used to compute sentence to
sentence similarities.
Vector addition
The simplest way to represent a sentence is to
consider it as the sum of all words without regarding word orders.
This was considered by
projection
Figure 5: The continuous Skip-gram model. Using the input word (wt) the model tries to predict
which words will be in its context (wt±c).
Mikolov et al. for representing short
phrases. The model is expressed by the following
xw∈{sentence}
where xp is a phrase embedding, and xw is a word
embedding. We use this method for computing
phrase embeddings as a baseline in our experiments.
Unfolding Recursive Auto-encoder
The second model is more sophisticated, taking into account also the order of the words
and the grammar used.
An unfolding recursive
auto-encoder (RAE) is used to derive the phrase
embedding on the basis of a binary parse tree.
The unfolding RAE was introduced by Socher et
al. and uses two RvNNs, one for encoding
the compressed representations, and one for decoding them to recover the original sentence, see
Figure 6. The network is subsequently trained by
minimizing the reconstruction error.
Forward propagation in the network is done by
recursively applying Eq. 5a and 5b for each triplet
in the tree in two phases. First, starting at the center node (root of the tree) and recursively pulling
the data from the input. Second, again starting
at the center node, recursively pushing the data
towards the output. Backpropagation is done in
a similar manner using backpropagation through
structure .
Figure 6: The structure of an unfolding RAE, on
a three word phrase ([x1, x2, x3]). The weight matrix θe is used to encode the compressed representations, while θd is used to decode the representations and reconstruct the sentence.
Measuring Similarity
Phrase embeddings provide semantically aware
representations for sentences. For summarization,
we need to measure the similarity between two
representations and will make use of the following
two vector similarity measures. The ﬁrst similarity measure is the cosine similarity, transformed to
the interval of 
Sim(i, j) =
∥xj∥∥xj∥+ 1
where x denotes a phrase embedding The second
similarity is based on the complement of the Euclidean distance and computed as:
Sim(i, j) = 1−
∥xk −xn ∥2
∥xj −xi ∥2
Experiments
In order to evaluate phrase embeddings for summarization we conduct several experiments and
compare different phrase embeddings with tf-idf
based vectors.
Experimental Settings
Seven different conﬁguration were evaluated. The
ﬁrst conﬁguration provides us with a baseline and
is denoted Original for the Lin-Bilmes method
described in Sec. 2.1. The remaining conﬁgurations comprise selected combinations of word embeddings, phrase embeddings, and similarity measures.
The ﬁrst group of conﬁgurations are based on
vector addition using both Word2Vec and CW vectors. These vectors are subsequently compared using both cosine similarity and Euclidean distance.
The second group of conﬁgurations are built upon
recursive auto-encoders using CW vectors and are
also compared using cosine similarity as well as
Euclidean distance.
VectorType EmbeddingMethodSimilarityMethod,
e.g. W2V_AddCos for Word2Vec vectors combined using vector addition and compared using
cosine similarity.
To get an upper bound for each ROUGE score
an exhaustive search were performed, where each
possible pair of sentences were evaluated, and
maximized w.r.t the ROUGE score.
Dataset and Evaluation
The Opinosis dataset consists of short user reviews in 51 different topics. Each of these topics contains between 50 and
575 sentences and are a collection of user reviews
made by different authors about a certain characteristic of a hotel, car or a product (e.g. ”Location of Holiday Inn, London” and ”Fonts, Amazon Kindle”). The dataset is well suited for multidocument summarization (each sentence is considered its own document), and includes between
4 and 5 gold-standard summaries (not sentences
chosen from the documents) created by human authors for each topic.
Each summary is evaluated with ROUGE, that
works by counting word overlaps between generated summaries and gold standard summaries. Our
results include R-1, R-2, and R-SU4, which counts
matches in unigrams, bigrams, and skip-bigrams
respectively. The skip-bigrams allow four words
in between .
The measures reported are recall (R), precision
(P), and F-score (F), computed for each topic individually and averaged. Recall measures what fraction of a human created gold standard summary
that is captured, and precision measures what fraction of the generated summary that is in the gold
standard. F-score is a standard way to combine
recall and precision, computed as F = 2 P∗R
Implementation
All results were obtained by running an implementation of Lin-Bilmes submodular optimization
summarizer, as described in Sec. 2.1. Also, we
have chosen to ﬁx the length of the summaries
to two sentences because the length of the goldstandard summaries are typically around two sentences.
The CW vectors used were trained by
Turian et al. 1, and the Word2Vec vectors
by Mikolov et al. 2. The unfolding RAE
used is based on the implementation by Socher
et al. 3, and the parse trees for guiding
the recursion was generated using the Stanford
Parser 4.
The results from the ROUGE evaluation are compiled in Table 1. We ﬁnd for all measures (recall,
precision, and F-score), that the phrase embeddings outperform the original Lin-Bilmes. For recall, we ﬁnd that CW_AddCos achieves the highest result, while for precision and F-score the
CW_AddEuc perform best. These results are consistent for all versions of ROUGE scores reported
(1, 2 and SU4), providing a strong indication for
phrase embeddings in the context of automatic
summarization.
Unfolding RAE on CW vectors and vector addition on W2V vectors gave comparable results
w.r.t. each other, generally performing better than
original Linn-Bilmes but not performing as well as
vector addition of CW vectors.
The results denoted OPT in Table 1 describe
the upper bound score, where each row represents optimal recall and F-score respectively. The
best results are achieved for R-1 with a maximum recall of 57.86%. This is a consequence of
hand created gold standard summaries used in the
evaluation, that is, we cannot achieve full recall
or F-score when the sentences in the gold standard summaries are not taken from the underlying documents and thus, they can never be fully
matched using extractive summarization. R-2 and
SU4 have lower maximum recall and F-score, with
22.9% and 29.5% respectively.
Discussion
The results of this paper show great potential for
employing word and phrase embeddings in summarization. We believe that by using embeddings
we move towards more semantically aware summarization systems. In the future, we anticipate
1 
2 
3 socherr/codeRAEVectorsNIPS2011.zip
4 
Table 1: ROUGE scores for summaries using different similarity measures.
OPT constitutes the
optimal ROUGE scores on this dataset.
W2V_AddCos
W2V_AddEuc
W2V_AddCos
W2V_AddEuc
W2V_AddCos
W2V_AddEuc
improvements for the ﬁeld of automatic summarization as the quality of the word vectors improve and we ﬁnd enhanced ways of composing
and comparing the vectors.
It is interesting to compare the results of different composition techniques on the CW vectors, where vector addition surprisingly outperforms the considerably more sophisticated unfolding RAE. However, since the unfolding RAE uses
syntactic information, this may be a result of using
a dataset consisting of low quality text.
In the interest of comparing word embeddings,
results using vector addition and cosine similarity
were computed based on both CW and Word2Vec
vectors. Supported by the achieved results CW
vectors seems better suited for sentence similarities in this setting.
An issue we encountered with using precomputed word embeddings was their limited vocabulary, in particular missing uncommon (or common incorrect) spellings.
This problem is particularly pronounced on the evaluated Opinosis
dataset, since the text is of low quality. Future
work is to train word embeddings on a dataset used
for summarization to better capture the speciﬁc semantics and vocabulary.
The optimal R-1 scores are higher than R-2 and
SU4 (see Table 1) most likely because the score ignores word order and considers each sentence as a
set of words. We come closest to the optimal score
for R-1, where we achieve 60% of maximal recall
and 49% of F-score. Future work is to investigate
why we achieve a much lower recall and F-score
for the other ROUGE scores.
Our results suggest that the phrase embeddings
capture the kind of information that is needed for
the summarization task. The embeddings are the
underpinnings of the decisions on which sentences
that are representative of the whole input text, and
which sentences that would be redundant when
combined in a summary. However, the fact that
we at most achieve 60% of maximal recall suggests that the phrase embeddings are not complete
w.r.t summarization and might beneﬁt from being
combined with other similarity measures that can
capture complementary information, for example
using multiple kernel learning.
Related Work
To the best of our knowledge, continuous vector
space models have not previously been used in
summarization tasks. Therefore, we split this section in two, handling summarization and continuous vector space models separately.
Continuous Vector Space Models
Continuous distributed vector representation of
words was ﬁrst introduced by Bengio et al. .
They employ a FFNN, using a window of words
as input, and train the model to predict the next
word. This is computed using a big softmax layer
that calculate the probabilities for each word in the
vocabulary. This type of exhaustive estimation is
necessary in some NLP applications, but makes
the model heavy to train.
If the sole purpose of the model is to derive
word embeddings this can be exploited by using
a much lighter output layer. This was suggested
by Collobert and Weston , which swapped
the heavy softmax against a hinge loss function.
The model works by scoring a set of consecutive
words, distorting one of the words, scoring the distorted set, and ﬁnally training the network to give
the correct set a higher score.
Mikolov et al. introduced a model called
Continuous Skip-gram.
This model is trained
to predict the context surrounding a given word
using a shallow neural network. The model is less
aware of the order of words, than the previously
mentioned models, but can be trained efﬁciently
on considerably larger datasets.
An early attempt at merging word representations into representations for phrases and sentences is introduced by Socher et al. . The
authors present a recursive neural network architecture (RvNN) that is able to jointly learn parsing
and phrase/sentence representation. Though not
able to achieve state-of-the-art results, the method
provides an interesting path forward. The model
uses one neural network to derive all merged representations, applied recursively in a binary parse
tree. This makes the model fast and easy to train
but requires labeled data for training.
Summarization Techniques
Radev et al. pioneered the use of cluster
centroids in their work with the idea to group, in
the same cluster, those sentences which are highly
similar to each other, thus generating a number
of clusters. To measure the similarity between a
pair of sentences, the authors use the cosine similarity measure where sentences are represented as
weighted vectors of tf-idf terms. Once sentences
are clustered, sentence selection is performed by
selecting a subset of sentences from each cluster.
In TextRank , a document is represented
as a graph where each sentence is denoted by a
vertex and pairwise similarities between sentences
are represented by edges with a weight corresponding to the similarity between the sentences.
The Google PageRank ranking algorithm is used
to estimate the importance of different sentences
and the most important sentences are chosen for
inclusion in the summary.
Bonzanini, Martinez, Roelleke presented an algorithm that starts with the set of
all sentences in the summary and then iteratively
chooses sentences that are unimportant and removes them. The sentence removal algorithm obtained good results on the Opinosis dataset, in particular w.r.t F-scores.
We have chosen to compare our work with that
of Lin and Bilmes , described in Sec. 2.1.
Future work is to make an exhaustive comparison
using a larger set similarity measures and summarization frameworks.
Conclusions
We investigated the effects of using phrase embeddings for summarization, and showed that these
can signiﬁcantly improve the performance of the
state-of-the-art summarization method introduced
by Lin and Bilmes in . Two implementations of word vectors and two different approaches
for composition where evaluated.
All investigated combinations improved the original Lin-
Bilmes approach (using tf-idf representations of
sentences) for at least two ROUGE scores, and top
results where found using vector addition on CW
In order to further investigate the applicability
of continuous vector representations for summarization, in future work we plan to try other summarization methods. In particular we will use a
method based on multiple kernel learning were
phrase embeddings can be combined with other
similarity measures. Furthermore, we aim to use
a novel method for sentence representation similar
to the RAE using multiplicative connections controlled by the local context in the sentence.
Acknowledgments
The authors would like to acknowledge the project
Towards a knowledge-based culturomics supported by a framework grant from the Swedish
Research Council ,
and the project Data-driven secure business intelligence grant IIS11-0089 from the Swedish Foundation for Strategic Research (SSF).