Restoring Vision in Adverse Weather Conditions
with Patch-Based Denoising Diffusion Models
Ozan ¨Ozdenizci and Robert Legenstein
Abstract—Image restoration under adverse weather conditions has been of signiﬁcant interest for various computer vision
applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision
transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel
patch-based image restoration algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling
approach enables size-agnostic image restoration by using a guided denoising process with smoothed noise estimates across
overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined
deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both
weather-speciﬁc and multi-weather image restoration, and experimentally show strong generalization to real-world test images.
Index Terms—denoising diffusion models, patch-based image restoration, deraining, desnowing, dehazing, raindrop removal.
INTRODUCTION
HE restoration of images under adverse impacts of
weather conditions such as heavy rain or snow is
of wide interest to computer vision research. At the extreme, observed images to be restored may contain severe
weather related obstructions of the true background (e.g.,
snow ﬂakes, dense hazing effects), causing a well known
ill-posed inverse problem where various solutions can be
obtained for the unknown ground truth background. Deep
neural networks (DNNs) are shown to excel at such image
restoration tasks compared to traditional approaches ,
 , , and this success extends with the current progress
in DNN architectural designs, e.g., with vision transformers , . State-of-the-art designs have recently shown
its effectiveness in low-level weather restoration problems
with transformers , and multi-layer perceptron based
models . Beyond task-specialized solutions, recent work
also proposed to tackle this problem for multiple weather
corruptions in uniﬁed architectures , , , .
Earlier deep learning based solutions to adverse weather
restoration have extensively explored task-speciﬁc generative modeling methods, mainly with generative adversarial
networks (GANs) , , . In this setting generative
models aim to learn the underlying data distribution for
cleared image backgrounds, given weather-degraded examples from a training set. Due to their stronger expressiveness in that sense, generative approaches further accommodate the potential of better generalization to multi-task
vision restoration problems. Along this line, we introduce
a novel solution to this problem by using a state-of-the-art
conditional generative modeling approach, with denoising
diffusion probabilistic models , .
O. ¨Ozdenizci and R. Legenstein are with the Institute of Theoretical
Computer Science, Graz University of Technology, Graz, Austria.
E-mail: {ozan.ozdenizci,robert.legenstein}@igi.tugraz.at
O. ¨Ozdenizci is also afﬁliated with TU Graz - SAL Dependable Embedded
Systems Lab, Silicon Austria Labs, Graz, Austria.
demonstrated remarkable success in various generative modeling tasks , , , . These architectures were
however not yet considered for image restoration under
adverse weather conditions, or demonstrated to generalize
across multiple image restoration problems. A major obstacle for their usage in image restoration is their architectural constraint that prohibits size-agnostic image restoration, whereas image restoration benchmarks and real-world
problems consist of images with various sizes.
We present a novel perspective to the problem of improving vision in adverse weather conditions using denoising diffusion models. Particularly for image restoration,
we introduce a novel patch-based diffusive restoration approach to enable size-agnostic processing. Our method uses
a guided denoising process for diffusion models by steering
the sampling process based on smoothed noise estimates
for overlapping patches. Proposed patch-based image processing scheme further introduces a light-weight diffusion
modeling approach, and extends practicality of state-ofthe-art diffusion models with extensive computational resource demands. We experimentally use extreme weather
degradation benchmarks on removing snow, combined rain
with haze, and removal of raindrops obstructing the camera
sensor. We demonstrate our diffusion modeling perspective
to excel at several associated problems.
Our contributions are summarized as follows:
We present a novel patch-based diffusive image
restoration algorithm for arbitrary sized image processing with denoising diffusion models.
We empirically demonstrate our approach to achieve
state-of-the-art performance on both weather-speciﬁc
and multi-weather restoration tasks.
We experimentally present strong generalization
from synthetic to real-world multi-weather restoration with our generative modeling perspective.
 
RELATED WORK
Diffusion-based Generative Models
Diffusion based and score-matching based , 
generative models recently regained interest with improvements adopted in denoising diffusion probabilistic models ,
 and noise-conditional score networks , , reaching exceptional image synthesis capabilities . Both approaches relate to a class of generative models that are based
on learning to reverse the process of sequentially corrupting
data samples with increasing additive noise, until the perturbed distribution matches a standard normal prior. This
is achieved either by optimizing a time-conditional additive
noise estimator or a noise conditional score function
(i.e., gradient of log-likelihood) parameterized by a
DNN. These models are then used for step-wise denoising
of samples from a noise distribution, to obtain samples from
the data distribution via Langevin dynamics . Denoising
diffusion models were shown to also implicitly learn these
score functions at each noise scale, and both methods were
later reframed in a uniﬁed continuous-time formulation
based on stochastic differential equations .
Another resembling perspective links energy-based models to this class of generative methods , . Energybased models estimate an unnormalized probability density
deﬁned via the Boltzmann distribution, by optimizing a
DNN that represents the energy function. At test time
one can similarly perform Langevin sampling starting from
pure noise towards the learned distribution, however this
time using the gradient of the energy function. Notably,
energy-based models differ in its training approach which
relies on contrastive divergence methods , , whereas
diffusion- and score-based models exploit the sequential forward noising (diffusion) scheme to cover a smoother density
across isolated modes of the training data distribution.
Recently, diffusion-based conditional generative models
have shown state-of-the-art performance in various tasks
such as class-conditional data synthesis with classiﬁer guidance , image super-resolution , , image deblurring , text-based image synthesis and editing , ,
and general image-to-image translation tasks (e.g., inpainting, colorization) , , . Similar conditional generative modeling applications also exist from a score-based
modeling perspective , . Notably, Kawar et al. 
recently proposed denoising diffusion restoration models for
general linear inverse image restoration problems, which
exploits pre-trained denoising diffusion models for unsupervised posterior sampling. In contrast to our model, this
approach does not perform conditional generative modeling
and does not consider image size agnostic restoration. More
generally, diffusion models were so far not considered for
image restoration under adverse weather conditions.
Image Restoration in Adverse Weather Conditions
The inverse problem of restoring single images by estimating the background scene under weather related foreground
degradations is ill-posed. In this scenario the observed image only contains a mixture of pixel intensities from the
weather distortion (e.g., rain streaks) and the background,
which can even be fully occluded. Traditional model-based
restoration methods explored various weather distortion
characteristic priors to address this problem .
Image Deraining & Dehazing: Earliest deep learning
era breakthroughs extensively studied the problem of image
deraining with convolutional neural networks (CNN), see
e.g. the deep detail network , , and the joint rain
detection and removal (JORDER) method . Following
works explored novel mechanisms such as recurrent context
aggregation proposed in RESCAN , or spatial attention
maps in SPANet . Concurrently popularized GAN based
image-to-image translation models (e.g., pix2pix , CycleGAN , perceptual adversarial networks ) were
found successful in modeling underlying image background
structures when simply applied to these problems. This
subsequently led to dedicated generative models tailored
for weather restoration tasks, such as image deraining conditional GANs , or conditional variational image deraining based on VAEs. There has been an independent
line of work focusing solely on image dehazing , ,
 , where also similar GAN based generative solutions
were adopted . Recently, more challenging natural extensions to this problem were explored, such as heavy
rain removal combined with dehazing tasks in a realistic
setting by Li et al. via the heavy rain GAN (HRGAN).
Novel solutions introduced hierarchical multi-scale feature
extraction and fusion , as well as its extension progressive coupled networks (PCNet) which were shown to
outperform several methods on combined deraining and
dehazing tasks. Most recently Zamir et al. proposed
multi-stage progressive image restoration networks with
supervised attention modules (MPRNet), which was shown
to excel across several general image restoration tasks.
Removing Raindrops: Beyond removal of rain streaks,
another natural extension considers removing raindrops
that introduce artifacts on the camera sensor. Originally
Qian et al. presented a dataset on this phenomena,
and proposed an Attentive GAN for raindrop removal.
Concurrently Quan et al. proposed an image-to-image
CNN with an attention mechanism (RaindropAttn) for the
same problem, and Liu et al. demonstrated the effectiveness of dual residual networks (DuRN), a general
purpose image restoration model, on this particular task.
Subsequent work focused on restoring multiple degradation
effects such as simultaneous removal of raindrops and rain
streaks . Most recently Xiao et al. proposed an image
deraining transformer (IDT) with state-of-the-art results
on generating rain-free images for rain streak removal tasks
at various severities, and for raindrop removal.
Image Desnowing: One of the earliest deep learning
methods for removing snow artifacts from images was
proposed by DesnowNet with a CNN-based architecture. Several existing image deraining solutions were later
also shown to perform relatively well on this task (e.g.,
SPANet , RESCAN ). Later Chen et al. proposed
JSTASR which is speciﬁcally designed for size and transparency aware snow removal in a uniﬁed framework. Most
recently Zhang et al. proposed a deep dense multi-scale
network (DDMSNet) which exploits simultaneous semantic
image segmentation and depth estimation mechanisms to
improve image desnowing performance, being one of the
most effective solutions presented so far.
Multi-Weather Restoration: There have been recent attempts in unifying multiple restoration tasks within single
deep learning frameworks, including generative modeling
solutions to restore superimposed noise types , restoring test-time unknown mixtures of noise or weather corruptions , or speciﬁcally adverse multi-weather image
degradations , , . Seminal work by Li et al. in this
context proposed the All-in-One uniﬁed weather restoration
method which utilizes a multi-encoder and decoder architecture and neural architecture search across task-speciﬁc
optimized encoders. Most recently Valanarasu et al. proposed an alternative state-of-the-art solution to this problem
with TransWeather, as an end-to-end vision transformer
based multi-weather image restoration model. Notably, to
our interest, these two studies , use the same combination of weather degradation benchmark datasets , ,
 , hence constructing an accumulated line of comparable
progress for this research problem.
ADVERSE WEATHER IMAGE RESTORATION WITH
PATCH-BASED DENOISING DIFFUSION MODELS
Denoising Diffusion Probabilistic Models
Denoising diffusion models , are a class of generative models that learn a Markov Chain which gradually converts a Gaussian noise distribution into the data distribution
that the model is trained on. The diffusion process (i.e., forward
process) is a ﬁxed Markov Chain that sequentially corrupts
the data x0 ∼q(x0) at T diffusion time steps, by injecting
Gaussian noise according to a variance schedule β1, . . . , βT :
q(xt | xt−1) = N(xt;
1 −βt xt−1, βt I),
q(x1:T | x0) =
q(xt | xt−1).
Diffusion models learn to reverse this predeﬁned forward
process in (1) utilizing the same functional form. The reverse
process deﬁned by the joint distribution pθ(x0:T ) is a Markov
Chain with learned Gaussian denoising transitions starting
at a standard normal prior p(xT ) = N(xT ; 0, I):
pθ(x0:T ) = p(xT )
pθ(xt−1 | xt),
pθ(xt−1 | xt) = N(xt−1; µθ(xt, t), Σθ(xt, t)).
Here the reverse process is parameterized by a neural
network that estimates µθ(xt, t) and Σθ(xt, t). The forward
process variance schedule βt can be learned jointly with the
model or kept constant , ensuring that xT approximately
follows a standard normal distribution.
The model is trained by optimizing a variational bound
on negative data log likelihood Eq(x0)[−log pθ(x0)] ≤Lθ,
which can be expanded into , :
DKL(q(xT | x0) || p(xT ))
−log pθ(x0 | x1)
DKL(q(xt−1 | xt, x0) || pθ(xt−1 | xt))
This loss was shown to be efﬁciently optimized via
stochastic gradient descent over randomly sampled Lt−1
terms , taking into consideration that we can marginalize
the Gaussian diffusion process to sample intermediate xt
terms directly from clean data x0 through:
q(xt | x0) = N(xt; √¯αt x0, (1 −¯αt) I),
which also can be expressed in closed form:
xt = √¯αt x0 +
where αt = 1 −βt, ¯αt = Qt
i=1 αi, and ϵt ∼N(0, I) has the
same dimensionality as data x0 and latent variables xt.
Here the Lt−1 terms in (5) compare the KL divergence between two Gaussians, pθ(xt−1 | xt) from (4) and
q(xt−1 | xt, x0). The latter is the true unknown generative
process posterior conditioned on x0, denoted by:
q(xt−1 | xt, x0) = N(xt−1; ˜µt(xt, x0), ˜βt I),
where the distribution parameters can be written as:
˜βt = (1 −¯αt−1)
(1 −¯αt) βt,
by incorporating the property (7) into ˜µt(xt, x0) . One
can either consider ﬁxed reverse process variances for a simple training objective Σθ(xt, t) = σ2
t I (e.g., σ2
t = ˜βt) , or
optimize Σθ(xt, t) with a hybrid learning objective .
The overall training objective for the former, when
pθ(xt−1 | xt)
N(xt−1; µθ(xt, t), σ2
t I), corresponds to
training a network µθ(xt, t) that predicts ˜µt. Using an
alternative reparameterization of the reverse process by:
µθ(xt, t) =
the model can instead be trained to predict the noise vector
ϵθ(xt, t) by optimizing the re-weighted simpliﬁed objective:
Ex0,t,ϵt∼N(0,I)
||ϵt −ϵθ(√¯αt x0 +
1 −¯αtϵt, t)||2i
In this setting we optimize a network that predicts the
noise ϵt at time t, from xt. Sampling with the learned
parameterized Gaussian transitions pθ(xt−1 | xt) can then
be performed starting from xT ∼N(0, I) by:
where z ∼N(0, I), which resembles one step of sampling
via Langevin dynamics .
A large T and small βt for the forward steps allows
the assumption that the reverse process becomes close to
a Gaussian, which however leads to costly sampling, e.g.,
when T = 1000. The variance schedule is generally chosen
to be β1 < β2 < . . . < βT , leading to larger updates to be
performed for noisier samples. We focus on using a ﬁxed,
linearly increasing variance schedule as originally found
sufﬁcient in , whereas learning this schedule based on
e.g., signal-to-noise ratio estimates is also possible.
Deterministic Implicit Sampling
Denoising diffusion implicit models present an accelerated deterministic sampling approach for pre-trained
diffusion models, which were shown to yield consistent and
better quality image samples. Implicit sampling exploits a
generalized non-Markovian forward process formulation:
qλ(x1:T | x0) = qλ(xT | x0)
qλ(xt−1 | xt, x0),
where we will rewrite the distribution in (8) in terms of a
particular choice of its standard deviation λt as:
qλ(xt−1 | xt, x0) = N(xt−1; ˜µt(xt, x0), λ2
and the mean denoted in terms of the variance as:
˜µt = √¯αt−1 x0 +
1 −¯αt−1 −λ2
by incorporating the property (7) into ˜µt(xt, x0). Here,
by setting λ2
t = ˜βt the forward process becomes Markov
and one recovers the original diffusion model formulation
described earlier. Importantly, the training objective (11)
remains the same, but only embedded non-Markov forward
processes are exploited for inference .
A deterministic implicit sampling behavior sets λ2
hence after generating an initial xT from the marginal noise
distribution sampling becomes deterministic. We will similarly use our models by setting λ2
t = 0. Implicit sampling
using a noise estimator network can then be performed by:
xt−1 =√¯αt−1
xt −√1 −¯αt · ϵθ(xt, t)
1 −¯αt−1 · ϵθ(xt, t).
During accelerated sampling one only needs a subsequence τ1, τ2, . . . , τS of the complete {1, . . . , T} timestep
indices. This helps reducing the number of sampling
timesteps up to two orders of magnitude. We determine this
sub-sequence by uniformly interleaving from {1, . . . , T}:
τi = (i −1) · T/S + 1 ,
which sets τ1 = 1 at the ﬁnal step of reverse sampling.
Conditional Diffusion Models
Conditional diffusion models have shown state-of-theart image-conditional data synthesis and editing capabilities. The core idea is to learn a conditional reverse process pθ(x0:T | ˜x) without modifying the diffusion process
q(x1:T | x0) for x, such that the sampled x has high ﬁdelity
to the data distribution conditioned on ˜x (see Figure 1).
During training we sample (x0, ˜x) ∼q(x0, ˜x) from a
paired data distribution (e.g., a clean image x0 and weather
degraded image ˜x), and learn a conditional diffusion model
where we provide ˜x as input to the reverse process:
pθ(x0:T | ˜x) = p(xT )
pθ(xt−1 | xt, ˜x).
Our previous formulation of optimizing a noise estimator network via (11) then uses ϵθ(xt, ˜x, t). For image-based
conditioning, inputs x and ˜x are concatenated channel-wise,
resulting in six dimensional input image channels.
pθ(xt−1 | xt, ˜x)
q(xt | xt−1)
Fig. 1. An overview of the forward diffusion (dashed line) and reverse
denoising (solid line) processes for a conditional diffusion model.
Note that conditioning the reverse process on ˜x maintains its compatibility with implicit sampling. In this formulation one samples from xt−1 ∼pθ(xt−1 | xt, ˜x) with:
xt−1 =√¯αt−1
xt −√1 −¯αt · ϵθ(xt, ˜x, t)
1 −¯αt−1 · ϵθ(xt, ˜x, t),
which follows a deterministic reverse path towards x0 with
ﬁdelity to the condition ˜x, starting from xT ∼N(0, I).
Patch-based Diffusive Image Restoration
Image restoration benchmarks, as well as real world pictures, consist of images with various sizes. Contrarily, existing generative architectures are mostly tailored for ﬁxedsize image processing. There has been one recent diffusion
modeling work which studied size-agnostic blurred image
restoration . Their model is optimized using ﬁxed-size
patches and then used for deblurring by simply providing
arbitrary sized inputs to the model, hence strictly relying on
a modiﬁed fully-convolutional network architecture. This
also leads to high test time computation demands such that
the whole image can be processed within memory. Differently, we decompose images into overlapping ﬁxed-sized
patches also at test time and blend them during sampling.
The general idea of patch-based restoration is to operate
locally on patches extracted from the image and optimally
merge the results. An important drawback of this approach
so far has been that the resulting image can contain merging
artifacts from independently restored intermediate results,
which was extensively studied in traditional restoration
methods , , . We will tackle this problem by guiding the reverse sampling process towards smoothness across
neighboring patches, without emerging edge artifacts.
We deﬁne the unknown ground truth image of arbitrary
size as X0, the weather-degraded observation as ˜X, and Pi
to be a binary mask matrix of same dimensionality as X0
and ˜X, indicating the i-th p × p patch location from the
image. Our training approach is outlined in Algorithm 1, in
which we learn the conditional reverse process:
0:T | ˜x(i)) = p(x(i)
t−1 | x(i)
t , ˜x(i)),
= Crop(Pi◦X0) and ˜x(i) = Crop(Pi◦˜X) denoting
p × p patches from a training set image pair (X0, ˜X),
Restoration
Weather-degraded
observation: ˜X
Noise estimator network
t , ˜x(d), t)
(a) Patch-based diffusive image restoration
Mean estimated noise
based sampling
updates for the
overlapping pixels:
d=1 ϵθ(x(d)
t , ˜x(d), t)
(b) Illustrating sampling for overlapping patches
Fig. 2. (a) Illustration of the patch-based diffusive image restoration pipeline detailed in Algorithm 2. (b) Illustrating mean estimated noise guided
sampling updates for overlapping pixels across patches. We demonstrate a simpliﬁed example where r = p/2, and there are only four overlapping
patches sharing the grid cell marked with the white border and gratings. In this case, we would perform sampling updates for the pixels in this region
based on the mean estimated noise over the four overlapping patches, at each denoising time step t.
where Crop(.) operation extracts the patch from the location
indicated by Pi. During training we randomly sample (with
uniform probability) the p × p patch location for Pi within
the complete range of image dimensions.
Our test time patch-based diffusive image restoration
method is illustrated in Figure 2a and outlined in Algorithm 2. Firstly, we decompose the image ˜X of arbitrary
size by extracting all overlapping p × p patches from a
grid-like arranged parsing scheme. We consider a grid-like
arrangement over the complete image where each grid cell
contains r × r pixels (r < p), and extract all p × p patches by
moving over this grid with a step size of r in both horizontal
and vertical dimensions (see Figure 2b for an illustration).
We deﬁne D as the total number of extracted patches,
deﬁning a dictionary of overlapping patch locations.
Due to the ill-posed nature of the problem, different
restoration estimates for overlapping grid cells will be obtained when performing conditional reverse sampling based
on neighboring overlapping patches. We alleviate this by
performing reverse sampling based on the mean estimated
noise for each pixel in overlapping patch regions, at any
given denoising time step t (see Figure 2b). Our approach
effectively steers the reverse sampling process to ensure
higher ﬁdelity across all contributing neighboring patches.
More speciﬁcally at each time step t of sampling, (1) we
estimate the additive noise for all overlapping patch locations d ∈{1, . . . , D} using ϵθ(x(d)
t , ˜x(d), t), (2) accumulate
these overlapping noise estimates at their respective patch
locations in a matrix ˆΩt of same size as the whole image
(line 8 in Alg. 2), (3) normalize ˆΩt based on the number
of received estimates for each pixel (line 11 in Alg. 2), (4)
perform an implicit sampling update using the smoothed
whole-image noise estimate ˆΩt (line 12 in Alg. 2).
Our method is different from a naive baseline of averaging overlapping ﬁnal reconstructions after sampling. Such
an approach destroys the local patch distribution ﬁdelity
to the learned posterior if applied post-sampling. (see Section 1.3 of Supplementary Materials for both quantitative
evaluations and visual comparisons on this). Differently
from our overlapping patch based guided sampling principle, however in a similar spirit, there are also recently
successful image editing methods based on steering the
Algorithm 1 Diffusive weather restoration model training
Input: Clean and weather-degraded image pairs (X0, ˜X)
Randomly sample a binary patch mask Pi
= Crop(Pi ◦X0) and ˜x(i) = Crop(Pi ◦˜X)
t ∼Uniform{1, . . . , T}
ϵt ∼N(0, I)
Perform a single gradient descent step for
∇θ||ϵt −ϵθ(√¯αt x(i)
0 +√1 −¯αtϵt , ˜x(i), t)||2
7: until converged
8: return θ
Algorithm 2 Patch-based diffusive image restoration
Input: Weather-degraded image ˜X, conditional diffusion
model ϵθ(xt, ˜x, t), number of implicit sampling steps S,
dictionary of D overlapping patch locations.
1: Xt ∼N(0, I)
2: for i = S, . . . , 1 do
t = (i −1) · T/S + 1
tnext = (i −2) · T/S + 1 if i > 1 else 0
ˆΩt = 0 and M = 0
for d = 1, . . . , D do
= Crop(Pd ◦Xt) and ˜x(d) = Crop(Pd ◦˜X)
ˆΩt = ˆΩt + Pd · ϵθ(x(d)
t , ˜x(d), t)
M = M + Pd
ˆΩt = ˆΩt ⊘M
// ⊘: element-wise division
Xt ←√¯αtnext
Xt −√1−¯αt · ˆΩt
+ √1 −¯αtnext · ˆΩt
13: end for
14: return Xt
reverse process in the latent space to achieve sampling from
a condensed subspace of the learned density , .
Note that a smaller r increases overlap between patches
and hence smoothness, however also the computational
burden. We used p = 64 or 128 pixels for Pi, and r = 16
pixels. Before processing, we resized whole image dimensions to be multiples of 16 as also conventionally done
with vision transformers . Here, choosing r = p would
construct a set of non-overlapping patches for processing,
hence would assume independency across patches during
restoration. However such neighboring patches are clearly
not independent in images, and this would lead to a suboptimal approximation with edge artifacts in restored images
(see Section 1.3 of Supplementary Materials for ablations).
Our proposed patch-based conditional diffusion modeling approach is task-agnostic, and further extends to simultaneously handling multiple weather corruptions when example image pairs from a mixture of weather degradations
are observed at training time, which we will experimentally
demonstrate in Section 4. The model is then effectively optimized to estimate the background while restoring images
(e.g., approximating the background behind the occlusions
from large snowﬂakes or raindrops) based on a learned mixture of conditional distributions. Note that while doing so,
our model does not require any additional input regarding
which task (weather) to consider at training or test time.
EXPERIMENTAL RESULTS
We used three standard benchmark image restoration
datasets considering adverse weather conditions of snow,
heavy rain with haze, and raindrops on the camera sensor.
Snow100K is a dataset for evaluation of image
desnowing models. It consists of 50,000 training and 50,000
test images split into approximately equal sizes of three
Snow100K-S/M/L sub-test sets (16,611/16,588/16,801), indicating the synthetic snow strength imposed via snowﬂake
sizes (light/mid/heavy). It also contains additional 1,329
real snowy images (Snow100K-Real) to evaluate real-world
generalization of models trained with synthetic data.
Outdoor-Rain is a dataset of simultaneous rain and
fog which exploits a physics-based generative model to simulate not only dense synthetic rain streaks, but also incorporating more realistic scene views, constructing an inverse
problem of simultaneous image deraining and dehazing.
The Outdoor-Rain training set consists of 9,000 images, and
the test set we used, denoted in as Test1, is of size 750
for quantitative evaluations.
RainDrop is a dataset of images with raindrops
introducing artifacts on the camera sensor and obstructing
the view. It consists of 861 training images with synthetic
raindrops, and a test set of 58 images dedicated for quantitative evaluations, denoted in as RainDrop-A.
Diffusion Model Implementations
We performed experiments both in weather-speciﬁc and
multi-weather
restoration
our weather-speciﬁc restoration models as SnowDiffp,
RainHazeDiffp and RainDropDiffp, and our multi-weather
restoration model as WeatherDiffp, with the subscripts denoting the input patch size of the model. We trained both
64x64 and 128x128 patch size versions of all models.
We used the same diffusion process conﬁguration for
all trained models. We grounded our model selection and
hyper-parameters via the deﬁnitions used in previous seminal work by , . The network had a U-Net architecture based on WideResNet , which uses group
normalization and self-attention blocks at 16x16 feature
map resolution , . We used input time step embedding for t through sinusoidal positional encoding 
and provided these embeddings as input to each residual
block, enabling the model to share parameters across time.
For input image conditioning we channel-wise concatenate the patches xt and ˜x, resulting in six dimensional
input image channels (i.e., RGB for both images). We did
not perform task-speciﬁc parameter tuning or modiﬁcations to the neural network architecture. Further speciﬁcations on the model conﬁgurations are provided in Table 1 of Supplementary Materials. Our code is available at:
 
Training Speciﬁcations
At each training iteration of 64x64 patch diffusion models,
we initially sampled 16 images from the training set and
randomly cropped 16 patches of size 64x64 from each,
resulting in mini-batches of size 256 patches. For 128x128
patch diffusion models, we randomly cropped 8 patches
from each of the 8 sampled training images per iteration,
resulting in mini-batches of size 64. We used all training
set images per epoch for weather-speciﬁc restoration. For
WeatherDiffp we used the curated AllWeather dataset from
 , which has 18,069 samples composed of subsets of training images from Snow100K, Outdoor-Rain and RainDrop, in
order to create a balanced training set across three weather
conditions with a similar approach to . Our multi-weather
models are effectively conditioned to generate the most
likely background for any of the three conditions, as we use
a mixture of weather degradations in training batches.
We trained all models for 2,000,000 iterations, except for
WeatherDiff128 which was trained for 2,500,000 iterations
due to complexity of this task (see Section 1.1 of Supplementary Materials for an empirical analysis). We used
an Adam optimizer with a ﬁxed learning rate of 0.00002
without weight decay. An exponential moving average with
a weight of 0.999 was applied during parameter updates, as
it was shown to facilitate more stable learning , .
Comparison Methods and Evaluation Metrics
We perform comparisons of our weather-speciﬁc models
with several state-of-the-art methods discussed in Section 2.2 for image desnowing , , , , ,
combined image deraining and dehazing , , ,
 , , and removing raindrops , , , , .
We compare WeatherDiffp with two state-of-the-art multiweather image restoration methods: All-in-One , which
utilizes a multi-encoder and decoder pipeline with a neural architecture search mechanism, and TransWeather ,
which exploits an end-to-end vision transformer. Notably,
both of these works were presented for multi-weather image
restoration using the same three benchmark datasets.
Our comparison method choices were mainly driven in
accordance with the baselines from , , as well as methods that are in directly comparable setting since they either
reported identical test set evaluations with the datasets we
used, or publicly provided their pretrained models.
Quantitative evaluations between ground truth and restored images were performed via the conventional peak
Snow100K-S 
Snow100K-L 
SPANet 
JSTASR 
RESCAN 
DesnowNet 
DDMSNet 
SnowDiff64
SnowDiff128
All-in-One 
TransWeather 
WeatherDiff64
WeatherDiff128
(a) Image Desnowing
Outdoor-Rain 
CycleGAN 
pix2pix 
HRGAN 
PCNet 
MPRNet 
RainHazeDiff64
RainHazeDiff128
All-in-One 
TransWeather 
WeatherDiff64
WeatherDiff128
(b) Image Deraining & Dehazing
RainDrop 
pix2pix 
RaindropAttn 
AttentiveGAN 
RainDropDiff64
RainDropDiff128
All-in-One 
TransWeather 
WeatherDiff64
WeatherDiff128
(c) Removing Raindrops
Fig. 3. Quantitative comparisons in terms of PSNR and SSIM (higher is better) with state-of-the-art image desnowing and deraining methods. Above
half of the tables show comparisons of our weather-speciﬁc SnowDiffp, RainHazeDiffp and RainDropDiffp models individually evaluated for each
task. Bottom half of the tables show evaluations of our uniﬁed multi-weather model WeatherDiffp on all three test sets with respect to All-in-One 
and TransWeather multi-weather restoration methods. Best and second best values are indicated with bold text and underlined text respectively.
(b) DesnowNet 
(c) DDMSNet 
(d) Ours (SnowDiff64)
(e) Ground truth
Fig. 4. Qualitative reconstruction comparisons of our best model on SnowTest100K test samples with DesnowNet and DDMSNet .
signal-to-noise ratio (PSNR) and structural similarity
(SSIM) metrics. We evaluated PSNR and SSIM based
on the luminance channel Y of the YCbCr color space in
accordance with the previous convention , , , .
We also used two other metrics for reference-free quality
assessment of real-world restoration performance, namely
the Naturalness Image Quality Evaluator (NIQE) , and
Integrated Local NIQE (IL-NIQE) scores. Better perceptual image quality leads to lower NIQE and IL-NIQE scores.
Weather-Speciﬁc Image Restoration Results
Figure 3 presents our quantitative evaluations. The top
half of the tables contain results from weather-speciﬁc
image restoration, where we show S
10 sampling
time steps for p = 64, and S = 50 for p = 128 (see
Section 1.2 of Supplementary Materials for other choices,
where sometimes better results can be achieved by tuning
S for each task individually). Our models achieve performances superior to all compared existing methods on
all tasks. For image desnowing and combined deraining
and dehazing tasks, our 64x64 patch models yield the best
results (i.e., 36.59/0.9626 on Snow100K-S, 30.43/0.9145 on
Snow100K-L and 28.38/0.9320 on Outdoor-Rain). For removing raindrops, with both input patch resolutions we
outperform the recent image de-raining transformers 
with RainDropDiff128 having the best PSNR of 32.52.
Figure 4 depicts some visualizations of image desnowing reconstructions for sample test images, comparing our
method with DesnowNet and DDMSNet . As illustrated, while DDMSNet appears to achieve noticable
higher visual quality than DeSnowNet in reconstructions,
our method SnowDiff64 shows remarkable restoration quality in ﬁne details (enlarged in red and blue bounding boxes).
Figure 5 depicts visualizations on sample Outdoor-Rain
test images, demonstrating the superiority of our model
RainHazeDiff64 over HRGAN and MPRNet . In
particular, smoothing effects from dehazing results in loss
of details in reconstructions with other methods, while our
model can recover these (e.g., second example in Figure 5,
metal railing lines enlarged in the bounding boxes).
(b) HRGAN 
(c) MPRNet 
(d) Ours (RainHazeDiff64)
(e) Ground truth
Fig. 5. Qualitative reconstruction comparisons of our best model on Outdoor-Rain test samples with HRGAN and MPRNet .
(b) RaindropAttn 
(c) AttentiveGAN 
(d) Ours (RainDropDiff128)
(e) Ground truth
Fig. 6. Qualitative reconstruction comparisons of our best model on Raindrop test samples with RaindropAttn and AttentiveGAN .
Figure 6 visualizes raindrop removal examples, comparing our best model RainDropDiff128 with Attentive-
GAN and RaindropAttn . Note that we particularly
illustrate HRGAN on Outdoor-Rain, and AttentiveGAN on
RainDrop test sets, since these approaches are earlier generative modeling based applications to this problem with
GANs. Our models generate more resembling reconstructions to the ground truths in all comparisons, and diffusion generative modeling signiﬁcantly outperforms the ones
based on GANs. We could not present visual comparisons
to IDT due to publicly unavailable implementations.
Multi-Weather Image Restoration Results
The bottom half of Figure 3 presents quantitative evaluations for multi-weather image restoration in comparison to
All-in-One and TransWeather, where we show S = 25 for
p = 64, and S = 50 for p = 128 (see Section 1.2 of Supplementary Materials for other choices, where sometimes
better results can be achieved by tuning S for each task
individually). We present PSNR/SSIM for publicly available TransWeather predictions with our deﬁnitions from
Section 4.4, which gave different results than reported in .
Generally our method yields exceptional image quality
and ground truth similarity on all three test sets. For the
image desnowing task, WeatherDiff64 achieves the best
PSNR/SSIM metrics with 35.83/0.9566 and 30.09/0.9041
for Snow100K-S and Snow100K-L respectively. Notably,
on the combined image deraining and dehazing task,
WeatherDiff64 and WeatherDiff128 yields better PSNR values of 29.64 and 29.72 respectively, which also outperforms
all dedicated weather-speciﬁc models at the above half of
Figure 3b. This is particularly important as WeatherDiffp
signiﬁcantly outperforms our RainHazeDiffp models on this
task. This indicates an improvement of the background
generative capability when combined with other tasks and
datasets. None of the existing multi-weather restoration
methods showed a similar knowledge transfer in comparison to their weather-speciﬁc counterparts.
(b) TransWeather 
(c) Ours (WeatherDiff64)
Fig. 7. Comparison of real-world snowy image restoration examples using TransWeather and WeatherDiff64. In the above example TransWeather
mistakenly removes the side view mirrors from both cars, however yields cleaner restorations than our method around the black car. In the below
example our method obtains better removal of tiny snowﬂakes from images when viewed in detail.
Our models are only outperformed in a single metric,
by All-in-One for PSNR on the RainDrop task (All-in-One:
31.12, Ours: 30.71). Nevertheless our results show better
ground truth similarity for this case (All-in-One: 0.9268,
Ours: 0.9312). These results demonstrate that WeatherDiff
models can successfully learn the underlying data distribution under several adverse weather corruption tasks.
Weather Restoration Generalization from Synthetic
to Real-World Images
We evaluate our models trained on synthetic data with realworld image restoration test cases. For visual illustrations
we compare our best performing WeatherDiff64 model with
the recent TransWeather network, which are both specialized on multi-weather restoration. Figure 7 presents qualitative image desnowing comparisons for selected images with
light snow from the miscellaneous realistic snowy images
set Snow100K-Real . First example in Figure 7 shows a
case where reconstructions by TransWeather removes the
side view mirrors of cars, whereas our model preserves this
detail (enlarged in the bounding boxes). On the other hand,
TransWeather gave cleaner restorations than our method
around the black car overall. In the second example, clearer
reconstructions with our model can be observed for a detailed image with light snow artifacts.
We also included additional real-world restoration test
cases from the raindrop removal test set of the RainDS
Quantitative NIQE and IL-NIQE score comparisons on real-world
image datasets with multi-weather restoration models. Best and second
best values are indicated with bold text and underlined text respectively.
Snow100K-Real 
RainDS 
TransWeather 
WeatherDiff64
WeatherDiff128
dataset presented by which consisted of 97 real test
images. Figure 8 presents qualitative comparisons for removing raindrops from real images using the same multiweather restoration models. First example in Figure 8 depicts a detailed image where TransWeather reconstructions
removes partly obstructed background components (i.e.,
leaves and stones), whereas our generative model completes
these details during restoration. Second example shows a
case with very bright raindrop artifacts on the camera sensor which were not completely restored by TransWeather,
whereas our model is comparably better. We provide more
visual examples in Section 2 of Supplementary Materials.
Finally in Table 1 we present our quantitative comparisons on these two real-world image restoration test sets
based on reference-free image quality metrics. Results show
(b) TransWeather 
(c) Ours (WeatherDiff64)
Fig. 8. Comparison of real-world raindrop image restoration examples using TransWeather and WeatherDiff64. Our method generates creative
reconstructions in the above example with stones on the grass and sharper leaves on branches, whereas TransWeather smoothes out many details.
In the below example, very bright raindrop artifacts could not be restored by TransWeather while our model recovers these.
that our WeatherDiff64 (S = 25) and WeatherDiff128 (S =
50) models yield better perceptual image quality scores on
both test sets, and signiﬁcantly outperforms the state-of-theart multi-weather restoration model TransWeather .
DISCUSSION
We present a novel patch-based image restoration approach
based on conditional denoising diffusion probabilistic models, to improve vision under adverse weather conditions.
Our solution is shown to yield state-of-the-art performance
on weather-speciﬁc and multi-weather image restoration
tasks on benchmark datasets. Notably, our method is general to any conditional diffusive generative modeling task
with arbitrary sized images.
Importantly, the proposed patch-based processing makes
our model input size-agnostic, and also introduces a lightweight generative diffusion modeling capability since the
architecture can be based on a simpler backbone network
for restoration at lower patch resolutions. This way we
extend the practicality of state-of-the-art diffusion model
architectures with large computational resource demands in
terms of the number of parameters and memory requirements during training and inference. Our novel patch-based
processing technique currently enables restoration of images
on a single GPU with as little as 12GB memory. Our approach also eliminates the restriction for the diffusion model
backbone to have a fully-convolutional structure to be able
to perform arbitrary sized image processing, and therefore
our model can beneﬁt from widely used resolution-speciﬁc
attention mechanisms , .
Our empirical analyses are mainly grounded on default
architectural choices and minimal parameter settings used
in seminal diffusion modeling works , . By incorporating novel methods that improve diffusion models in
terms of better sample quality or faster sampling mechanisms , we argue that quantitative results can further
be improved on particular weather restoration problems.
Limitations
The main limitation of our approach is its comparably
longer inference duration, with respect to the end-to-end
image restoration networks which only require a single forward pass for processing. To illustrate an empirical example,
our WeatherDiff64 model requires 20.52 seconds (wall-clock
time) to restore an image of size 640 × 432 with S = 10 on
a single NVIDIA A40 GPU, whereas TransWeather requires
0.88 seconds. Such timing speciﬁcations of our method also
directly rely on the choice of algorithm hyper-parameters
(e.g., a lower value of r slightly increases image quality but
also the inference times), and implementation efﬁciency.
Another natural limitation of our model is its inherently
limited capacity to only generalize to the restoration tasks
observed at training time. While we effortlessly enable
multi-weather restoration by using image pairs from multiple corruptions at training time, this still does not qualify
our generative model to conditionally generalize to unseen
corruptions (e.g., poor lighting conditions). Nevertheless,
this natural limitation is also present in all recent studies
that aim to tackle the problem of multi-weather image
restoration , , .
ACKNOWLEDGMENTS
This work has been supported by the “University SAL Labs”
initiative of Silicon Austria Labs (SAL).