This is a postprint version of the following published document:
Bega, D., et al. AI-Based Autonomous Control, Management, and
Orchestration in 5G: From Standards to Algorithms. In, IEEE Network,
34(6), Nov./Dec. 2020, Pp. 14-20
DOI: 
© 2020 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or
promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component
of this work in other works.
AI-based Autonomous Control, Management, and
Orchestration in 5G: from Standards to Algorithms
Dario Bega, Marco Gramaglia, Ramon Perez, Marco Fiore, Albert Banchs, and Xavier Costa-P´erez
Abstract—While the application of Artiﬁcial Intelligence (AI)
to 5G networks has raised a strong interest, standard solutions
to bring AI into 5G systems are still in their infancy and have a
long way to go before they can be used to build an operational
system. In this paper, we contribute to bridging the gap between
standards and a working solution, by deﬁning a framework
that brings together the relevant standard speciﬁcations and
complements them with additional building blocks. We populate
this framework with concrete AI-based algorithms that serve
different purposes towards developing a fully operational system. We evaluate the performance resulting from applying our
framework to control, management and orchestration functions,
showing the beneﬁts that AI can bring to 5G systems.
INTRODUCTION
Network control, management, and orchestration entail the
dynamic placement, conﬁguration, and resource provisioning
of Virtual Network Functions (VNFs) within the Network
Function Virtualization (NFV) infrastructure. The complexity
of these operations exceeds substantially that of equivalent
tasks in legacy 4G LTE networks. There, the relatively limited
amount of variables in one-size-ﬁts-all core and radio access
network domains accommodates management models that
mainly rely on expert monitoring and intervention. Instead,
the traditional human-based approach is hardly viable in virtualized 5G networks where the coexistence of heterogeneous
mobile services, diversiﬁed network requirements, and tenantdeﬁned management policies create a need for specialized
and time-varying infrastructure deployments. This calls, in
turn, for automated solutions in the control, management, and
orchestration of the network.
Artiﬁcial Intelligence (AI) is a natural choice to support the
emerging need for autonomous network operation and management. 3GPP and other Standard Developing Organizations
(SDOs) have started delineating the road for the integration
of AI into the mobile network architecture. Such a process
starts with an efﬁcient collection of data in the network
infrastructure and knowledge inference from these data, which
are paramount to effective AI-assisted decision-making. In this
sense, SDOs are pushing efforts towards deﬁning AI-based
Data Analytics frameworks that are suitable for autonomous
and efﬁcient control, management and orchestration of mobile
networks. For instance, 3GPP has incorporated the following
modules into its standardized architecture: (i) Network Data
This work was supported by the H2020 5G-TOURS European project
(Grant Agreement No. 856950).
Dario Bega and Albert Banchs are with IMDEA Networks Institute and
University Carlos III of Madrid; Marco Gramaglia is with University Carlos
III of Madrid; Ramon Perez is with Telcaria Ideas; Marco Fiore is with with
IMDEA Networks Institute; Xavier Costa-P´erez is with NEC Laboratories
Analytics Function (NWDAF) , and (ii) Management Data
Analytics Function (MDAF) . Other organizations, such
as the O-RAN alliance, envision similar entities in their
architectures . ETSI has also deﬁned comparable assisting
elements within the Industry Speciﬁcation Groups (ISGs) on
Experiential Networked Intelligence (ENI) and Zero touch
network & Service Management (ZSM) . Furthermore,
open-source initiatives such as ONAP are also including
data analytics into their architecture.
All these ongoing efforts are, however, at an early stage.
The frameworks they propose and the solution designs they
foster are preliminary and mainly aim at introducing several
key building blocks at a very high level of abstraction. They
are still far from detailed, full-blown network data analytics
that are ready for deployment.
In this context, the goal of this paper is to complement
and support ongoing standardization activities by (i) proposing a comprehensive framework that leverages data analytics
for network control, management and orchestration, bringing
together the corresponding efforts at relevant initiatives such
as 3GPP, ETSI and O-RAN; and (ii) populating the proposed
framework with practical algorithms that build on AI and
machine learning (ML) solutions.
AI-DRIVEN DATA ANALYTICS FRAMEWORK
Figure 1 depicts the network data analytics framework we
propose. The framework design encompasses the Management
and Orchestration plane as well as the Control plane functionalities, as AI can indeed improve the performance at all levels.
Within each plane, we take as reference architecture the one
proposed by 3GPP, integrating it with an ETSI NFV MANO
architecture and expanding it with O-RAN modules.
Management and Orchestration plane
In the Management and Orchestration plane, the MDAF
module is responsible for the so-called Management Data
Analytics Service (MDAS) for all network slice instances, subinstances and network functions hosted within the network
infrastructure. This involves the centralized collection of network data for subsequent publishing to other network management and orchestration modules. In the proposed framework,
we speciﬁcally employ this service to collect mobile data
trafﬁc loads generated in the radio access domain by the
individual slices; in particular, the MDAS comprises the
load level at both Network Function (NF) and network slice
levels, provided as a periodic notiﬁcation and expressed either
in absolute terms or relative to the provisioned capacity. As
Management & Orchestration domain
Control plane
short‐term
optimization
3GPP modules
New algorithms
ETSI modules
O‐RAN modules
short‐term
Fig. 1: Proposed framework with standard functions (from
3GPP, ETSI and O-RAN) and the new AI-based algorithms
(AI-LTF, AI-MTF and AI-STF).
a result, the MDAF allows building historical databases of
the network demands for each base station and slice. These
data are then exposed to the AI-based prediction algorithms
for (i) long-term forecasting (AI-LTF), and (ii) mid-term
forecasting (AI-MTF).
The AI-LTF algorithm aims at assisting the VNF placement decisions taken by the orchestration system. To this
end, AI-LTF leverages the network demand history to predict
the future aggregate load across the different infrastructure
locations. Then, the NFV Orchestrator (NFVO) compares
such a prediction against the current available capacity in
each infrastructure location and anticipates potential overload
conditions. The NFVO can react, e.g., by moving VNFs out of
the congested infrastructure (while meeting the requirements
of the corresponding network slice). The AI-LTF algorithm
operates on long timescales, typically in the order of hours:
indeed, VNFs repositioning is quite a drastic action that
involves substantial overhead, and consequently it is only
performed infrequently and as an answer to substantial trafﬁc
ﬂuctuations.
The second algorithm, AI-MTF, has a different purpose:
it fuels the resource scaling decisions taken by the VNF
Manager (VNFM). The VNFM has an interface with the Virtual Infrastructure Managers (VIMs) to monitor the resource
usage of the VNFs of each slice, and it also leverages data
collected and published by the MDAF to determine the level
of unsatisﬁed demand and the amount of unused resources.
Based on all this information, the AI-MTF algorithm assists
the orchestration framework on the decision (i) to provide
more resources to the VNFs of a slice when the predicted load
exceeds the current resources, an operation typically referred
to as upscaling, or (ii) to downscale resources to save cost
when VNFs are leaving a signiﬁcant fraction of the resources
unused. Such decisions must be taken over faster timescales
than those affecting the VNF placement, and generally occur
over intervals in the order of tens of minutes, which is the
typical frequency for the execution of new VNF instances
involving up- and downscaling.
Note that AI-LTF and AI-MTF only take as input the load
history from MDAF and do not interact between themselves or
with any other module. The forecasts of AI-LTF and AI-MTF
are fed into the NFVO and VNFM engines, which may instead
also leverage information obtained from other modules to take
their decisions.
Control plane
On the control plane, the NWDAF module is responsible
for collecting data on the load level of a NF or a network
slice , playing a very similar role to that of the MDAF in
the management domain. In our framework, these data are fed
to the AI-based short-term forecasting algorithm (AI-STF),
which predicts the future trafﬁc load. The forecast is leveraged
by the Policy Control Function (PCF) module, which provides
a uniﬁed policy framework to govern the network behavior.
PCF can use the forecast provided by AI-STF to optimize its
policies, such as (i) the QoS parameters (for those services
that can be provided at different QoS levels), (ii) the access
and mobility policies, or (iii) the UE Route Selection Policy
(URSP). In contrast to the previous modules, these updates
are performed at rather fast timescales, down to hundreds of
milliseconds.
While the NWDAF module has been designed for the
network core, a similar approach can be applied to the radio
access network (RAN). Although 3GPP has not yet proposed
modules equivalent to NWDAF in the RAN, other initiatives
such as the O-RAN alliance have taken this path. In the
ORAN architecture , the Radio Network Information Base
(RNIB) collects load information of ﬂows or ﬂow aggregates at
the RAN level, the RAN Intelligent Controller (RIC) enables
near real-time control of RAN elements/resources, and the
RAN resource orchestrator handles the overall resources at the
base station level. In this case, the AI-STF forecasts can be
leveraged by the RIC to perform the optimization of the radio
resources at a ﬁne time granularity (in the order of hundreds
of ms) and by the RAN resource orchestration to update the
resource and bandwidth allocation at larger timescales (up to
the order of minutes).
AI-BASED ALGORITHMS DESIGN
The above framework introduces three new AI-based algorithms: AI-LTF, AI-MTF and AI-STF. The three algorithms
follow the same design guidelines, as all of them aim at
providing network capacity forecasts. The main difference
between them is that they work at different granularity in terms
of trafﬁc volume (at global, slice, or ﬂow levels) and timescale
(intervals of hours, tens of minutes, minutes or shorter). In
the following, we present the uniﬁed design of these three
algorithms.
Capacity forecasting
In contrast to the majority of the literature in the area
of forecasting, our algorithm design addresses an original
problem of ‘capacity forecasting’. Capacity forecasting goes
beyond the typical estimation of future demands that is targeted by most trafﬁc predictors. Indeed, predictors in the
literature almost exclusively aim at matching the temporal
behavior of trafﬁc as closely as possible, giving the same
weight to positive and negative errors . While this approach
produces forecasts that reduce as much as possible the error
between the future and the anticipated demand, it is unsafe in a
capacity allocation context where the metric of interest is the
cost incurred by an operator when deploying the resources,
rather than the error between the real and the forecasted
demand. In this case, underestimating future demands causes
SLA violations that have a monetary penalty much higher than
the cost resulting from overprovisioning the resources, as long
as the level of overdimensioning is not excessive.
In contrast to the above legacy approaches, the aim of capacity forecasting is to ﬁnd the level of capacity that sufﬁces to
meet the expected load at (almost) all times, even if this comes
at the price of requiring a certain level of overprovisioning. To
perform such capacity forecasting, we rely on AI techniques,
which have been repeatedly shown to outperform traditional
statistical models in mobile trafﬁc prediction tasks that are
kin to the capacity forecasting problem at hand , . In
particular, our design takes advantage of recent advances in
supervised learning via Deep Neural Network (DNN) architectures, which –unlike other approaches– are well suited to
cope with the high dimensionality of the mobile data trafﬁc,
the complex spatial and temporal correlations it entails , and
the non-linear metric of interest that characterizes our problem.
Algorithm design overview
Our algorithm design builds on recent proposals that properly model the monetary costs incurred by the mobile network
operator . It is based on the following workﬂow. First,
current and past mobile trafﬁc information, collected at the
desired level of granularity, is properly formatted into an
input suitable for feeding the prediction algorithm. This input
is fed to a DNN architecture that processes input features
to provide an output value: the capacity forecast. During
the training phase, the output is used to evaluate a loss
function that quantiﬁes the error with respect to the ground
truth (i.e., the label), accounting for the costs of resource
overprovisioning (i.e., allocating more capacity than needed)
and underprovisioning (i.e., allotting insufﬁcient capacity to
meet the demand).
More precisely, time is divided into slots and data on the
actual trafﬁc load is collected by MDAF, NWDAF and RNIB
for each slot. Such load refers to the total load (for the
AI-LTF algorithm), the load of individual slices (for the
AI-MTF algorithm) and the load of ﬂows or ﬂow aggregates
(for the AI-STF algorithm). Base stations are associated to
datacenters such that a datacenter serves the aggregated load
of all its associated bases stations. Our framework aims at allocating the required capacity at each datacenter or associated
network functions.
Our goal is to compute a constant capacity to be allocated in
the network datacenters over a future time horizon Th, based
on knowledge of the previous Tp trafﬁc snapshots. The time
horizon models typical situations where the resource reconﬁguration frequency is limited (e.g., by the NFV technology) and
the operator must decide in advance the amount of resources
that will stay assigned to a slice until the next reallocation
takes place. As discussed before, AI-STF, AI-MTF and
AI-LTF target short, intermediate and long time horizons,
respectively.
To perform capacity forecasting, we leverage a DNN composed of suitably designed encoding and decoding phases,
which operate over an interval Th. The neural network architecture is general enough that it can be trained to solve
the capacity forecast problem for (i) trafﬁc loads with diverse
demand patterns, (ii) any datacenter, and (iii) any time horizon
Th. This allows leveraging the same DNN design to implement
all three algorithms. The design consists of the following three
components:
• Encoder: the historical mobile data trafﬁc provided as
input is high dimensional, as it comprises a large number
of base stations as well as several network slices or
ﬂows. The encoder projects this complex input space into
a latent low dimensional representation, which is then
analyzed to produce the needed prediction.
• Decoder: the decoder performs the actual forecast. The
decoder structure reﬂects the kind of output values that
shall be used to assist our framework, including the trafﬁc
granularity (i.e., the datacenter and the trafﬁc volume
level) and the time horizon.
• Loss function: the supervised learning strategy we adopt
requires that the algorithm can assess the goodness of
the outcome. To this end, we employ a dedicated loss
function to measure the quality of the capacity forecast
and steer the system during the training phase.
In the remainder of this section, we detail the implementation of the above three components. While the three algorithms
considered in this paper (AI-LTF, AI-MTF, and AI-STF)
share the same encoder structure, they output the forecasts over
different time horizons, which has an impact on the decoder
and the loss function computation.
Encoder and decoder structure
The neural network architecture used by the proposed
algorithms is summarized in Figure 2, and is composed of
an encoder-decoder sequence. The internal structures of the
encoder and decoder are inspired by recent breakthroughs in
deep learning for image and video processing . Their
design stems from the intuition that subsequent snapshots
of the spatial distribution of the network data trafﬁc can be
assimilated to frames in a video.
The encoder is composed of a stack of three threedimensional Convolutional Neural Network (3D-CNN) layers . Convolutional Neural Networks (CNNs) are a kind
Load History
Tensor representation
AI-LTF Decoder
AI-MTF Decoder
Fully Connected
AI-STF Decoder
Template Loss Function
Fig. 2: Neural network encoder-decoder structure.
of deep learning structure specialized to infer local patterns in
the feature space of a matrix input. Two-dimensional CNNs
(2D-CNNs) have been extensively utilized in image processing
to complete complex tasks on pixel matrices such as face
recognition or image quality assessment. 3D-CNNs extend 2D-
CNNs to address the case were the features to be learned are
spatiotemporal in nature, which adds the time dimension to
the problem and transforms the input into a 3D-tensor.
Since mobile network trafﬁc exhibits correlated patterns in
space and time, we design an encoder that employs 3D-CNN
layers. We use a 3×3×3 kernel for the ﬁrst 3D-CNN layer and
a 6×6×6 kernel for the second and third layers. This limits the
portion of input analyzed by each neuron to small regions – a
strategy known to perform well when the input has strong local
correlations. We employ ReLU activation functions, which
grant good performance and fast learning .
The decoder uses Multi-Layer Perceptrons (MLPs) ,
a class of fully-connected neural layers where every neuron
of one layer is connected to every neuron of the next layer.
MLPs are able to learn global patterns in the input feature
space, which allows forecasting the target capacity leveraging
the local features extracted by the encoder. For the decoder
activation functions, we employ ReLU in all MLP layers
except for the last one, where a linear activation function
returns real-valued outputs. The last linear layer is capable
of performing multiple capacity forecasts in parallel (e.g., for
different slices or different datacenters).
For the training procedure, we employ the popular Adam
optimizer, a Stochastic Gradient Descent (SGD) method with
fast convergence properties . This trains the neural network
model by evaluating at each iteration the loss function resulting
from the forecast and the ground truth, and back-propagating
it to tune the model parameters to minimize such loss.
Loss function design
The loss function drives the learning process and is thus
critical to the quality of the forecasting. To this end, it is
essential to ensure consistency between the target metric for
forecasting and the employed loss function. In mobile network
management, the relevant metric to assess the quality of the
capacity allocation is the cost incurred by the operator, referred
to as Operator Monetary Cost (OMC). This metric captures
the costs resulting from (i) forecasting a lower value than
the actual offered load (which leads to the provisioning of
insufﬁcient resources), and (ii) predicting a higher value than
the actual one (which leads to allocating more resources than
those needed to meet the demand).
General-purpose loss functions like MSE or MAE are
clearly inappropriate to optimize the OMC. Indeed, these loss
functions weigh equally all errors independent of whether the
forecasting falls above or below the real value, and hence
cannot learn the actual impact of different types of errors.
Instead, a customized loss function is required to determine
the actual penalty caused by a prediction error. In particular,
by setting the loss function equal to the penalty inﬂicted by a
given error in terms of OMC, the neural network is trained to
minimize the metric of interest. In line with this, we design
the loss function as follows:
• A constant penalty β is associated to each time slot where
the allocated resources are lower than those needed in
reality, leading to an SLA violation. Such penalty value
can be customized to the desired behavior, e.g., higher
values may be used for cases where reliability is needed,
such as for URLLC network slices; instead, lower values
can be applied for slices with more relaxed requirements.
• A monotonically increasing cost is attributed to resource
overprovisioning, with a ﬁxed rate of γ per overprovisioned byte. The more the resources (unnecessarily)
provisioned, the higher the deployment cost for the operator. This reﬂects the deployment expenditure associated
with excess allocated capacity, which we assume that
grows linearly with the amount of unused capacity. The
linear scaling factor γ is conﬁgurable and represents the
monetary cost of the excess resource allocation.
The conﬁguration of the two cost models above can, in
fact, be controlled by a single parameter α deﬁned as the
ratio between β and γ. Intuitively, α represents the amount of
overprovisioned capacity that the operator is willing to deploy
to avoid committing an SLA violation. Operators can use α
as a knob to steer the operational point of the system towards
higher expenses in resource deployments but reduced chances
of SLA violations, or vice-versa.
The resulting loss function is ﬂexible enough to accommodate different infrastructure locations (e.g., deploying resources at the network edge has a higher cost than at the
core), resource types (e.g., radio resources are sensibly more
expensive than CPU resources) and SLA strategies (e.g., slices
providing critical services may entail higher violation fees).
PERFORMANCE EVALUATION
We evaluate the proposed framework with real-world data
trafﬁc recorded in the mobile network of a major European
operator, providing coverage to a large metropolitan region.
Our dataset includes information about the exchanged trafﬁc
of the most popular services, which we classify into seven
categories (streaming, social network, web, cloud, gaming,
messaging and miscellaneous). It includes per-service trafﬁc
information provided as an aggregate over 5-minute intervals
at 470 base stations. The data spans 11 weeks, of which we
use 8 weeks for training, 2 for validation, and one for testing.
For the sole purpose of evaluating our algorithms with real
trafﬁc, we assume that each service category is assigned a
dedicated slice, and adopt the methodology proposed in to
build a network topology model that associates network trafﬁc
to NFs and datacenters. Our topology comprises different network levels ranging from the edge (the lowest level) to a fully
centralized node (the highest level), such that higher network
level nodes aggregate more trafﬁc and serve a larger load. We
refer to the highest level node as ‘core network datacenter’
and the lowest level ones as ‘edge network datacenters’.
Unless otherwise stated, we ﬁx Tp = 6 (which means that
the forecasting modules are fed with data of the previous 30
minutes of trafﬁc) and conﬁgure α = 1 (implying that one
SLA violation has the same monetary cost as provisioning an
excess capacity sufﬁcient to cover the trafﬁc peak).
AI-LTF: Long-term forecasting for VNF placement
The long-term forecasting capabilities provided by the
AI-LTF algorithm are useful to make decisions about the
suitable placement of the VNFs serving one or more slices.
To evaluate its performance, we consider a scenario where a
datacenter with processing capacity C serves the seven slices
and assume that the computational demand of a given slice is
proportional to the amount of transmitted bytes.
In this case study, we set Th = 8 hours to account for the
fact that VNF placement decisions are typically taken with a
coarse time granularity of hours due to the limitation of the
underlying NFV technology. We focus on an edge network
datacenter and employ AI-LTF to support the VNF placement
decisions taken by the NFVO module by anticipating the
overall trafﬁc load at the target datacenter. Then, the NFVO
can decide at every Th how many slices are served by the
datacenter of capacity C, and which slices shall instead be
placed elsewhere.
Figure 3 depicts the result obtained with AI-LTF against
that obtained with an oracle algorithm that assists the NFVO
with the knowledge of the real future demand (such an oracle
algorithm is unfeasible in practice but provides an optimal
benchmark to assess AI-LTF’s performance). We observe that
AI-LTF follows quite closely the oracle. The overall usage
of the deployed infrastructure remains high at all times. The
algorithm only moves more slices than needed away from
the datacenter on very limited occasions. In rare cases, it
places more slices than it should in the datacenter, leading
to an overload situation that results in computational outages
for the served slices; however, even when this happens, the
Occupation ratio
Fig. 3: VNF placement of slices at one target datacenter. Occupation ratio (top) and number of admitted slices (bottom) for
each 8-hour orchestration period. The algorithm implemented
by the AI-LTF module is compared against an optimal but
unfeasible oracle solution with perfect knowledge of the future
trafﬁc load.
actual overload levels are negligible. These results conﬁrm
that AI-LTF is a promising solution to assist effective VNF
placement decisions.
AI-MTF: mid-term forecasting for NFVI scaling
Once the VNFs serving various slices are placed at a
given datacenter, it is possible to dynamically reallocate the
resources assigned to each slice within the capacity C of
the datacenter by scaling up or down the resources assigned
to each slice. The time dynamics involved in such up- and
downscaling are faster than those analyzed in the previous
experiment for the VNF placement. Indeed, resource provisioning within the same datacenter (which involves booting
up a VNF and setting up the data plane) can be performed at
timescales of tens of minutes.
The AI-MTF algorithm can support such resource up- and
downscaling process. We investigate its performance in a
case study where the resources allotted to the slice serving
streaming trafﬁc at a core network datacenter are scaled every
30 minutes. Results, shown in Figure 4, conﬁrm that the
proposed algorithm yields remarkable accuracy. The allocated
capacity to the slice is scaled up and down to match closely
the demand generated by the service. As highlighted in
the bottom plot, the capacity allocated in excess is quite
small, which implies that limited resources are wasted due
to overprovisioning. Furthermore, the algorithm almost never
incurs underprovisioning, and thus it always serves the offered
demand and avoids violating the slice SLA.
AI-STF: short term forecasting for QoS policies
The optimization of policies and resource allocations for
individual ﬂows or aggregates at different levels (PCF, RIC,
RAN resource orchestration) can be performed at shorter
Normalized trafﬁc
Slice demand
AI-MTF capacity forecast
Fig. 4: NFVI scaling for a slice serving streaming trafﬁc at one
target datacenter. Allocated capacity versus service demand
(top) and excess capacity (bottom) of AI-MTF. Values are
normalized to the peak allocated capacity. Excess demand is
shown in blue and unserviced demand in red.
timescales than those considered before. In particular, depending on the speciﬁc operation, these updates can be performed
within intervals of a few minutes or less.
The AI-STF module is intended to back up this kind of
high-pace network management tasks. We provide an example
of application in Figure 5 for the case of resource allocation,
analyzing the network resources assigned to streaming ﬂows
in an edge network datacenter based on the prediction returned
by AI-STF over time periods of Th = 5 minutes (which is the
ﬁnest time granularity available in our dataset). Speciﬁcally,
the ﬁgure shows the distribution of the ratio of allocated
resources to the demand, where a value below 1 denotes that
the capacity forecast is not sufﬁcient to satisfy the demand,
while values above 1 mean that we allocated more capacity
than needed.
We observe that AI-STF is effective in provisioning suf-
ﬁcient resources to serve the aggregate demand for streaming
ﬂows while avoiding wasting too many resources in overprovisioning. We also observe that the parameter α can be tuned
to choose the desired trade-off between resource overprovisioning and SLA violations. Larger α values, corresponding
to higher penalties for SLA violations, reduce signiﬁcantly
the probability of underprovisioning, obviously at the cost of
increasing the amount of resources wasted in overprovisioning.
Overall performance
We next evaluate the overall performance of the three
algorithms when jointly running in a complete 5G system.
We consider the total load generated by the seven service categories at a core network datacenter where AI-LTF targets the
aggregate load at the datacenter, while AI-MTF and AI-STF
focus on the individual allocation for each service category.
The results, given in Table I, show (i) the percentage of
unserviced demand, and (ii) the cost gains provided by our AIbased algorithms over a traditional forecasting technique .
Capacity forecast / demand
Fig. 5: Distribution of the ratio of the allocated capacity with
AI-STF over the aggregate demand of the streaming ﬂows at
a target edge network datacenter. Different curves correspond
to diverse α ratios of the monetary penalty of SLA to the cost
of overprovisioning. The integral of the curve for values of
the abscissa below 1 corresponds to the probability of SLA
violation.
The results on unserviced demand conﬁrm the effectiveness
of α in controlling the level of reliability at the expense
of a larger resource deployment. Indeed, when selecting a
sufﬁciently large α, we can achieve practically zero outages,
which may be suitable to support, e.g., URLLC services. Even
for low values of α, the overall unserviced trafﬁc remains
reasonably low (below 1%). As expected, accuracy increases
when the predicted time horizon is shorter (which explains
why AI-STF outperforms AI-MTF for all α’s and AI-MTF
outperforms AI-LTF for α = 0.5 and α = 1) as well as when
the trafﬁc aggregate is larger (which explains why AI-LTF
outperforms AI-LTF and AI-STF for α = 2).
The results on cost gains show the advantage of our approach over a traditional forecasting technique for time series,
namely a seasonal autoregressive integrated moving average
(ARIMA) model . In order to better align the seasonal
ARIMA model with the requirements of the capacity forecasting problem, we augment it with a ﬁxed overprovisioning
on top of the predicted trafﬁc; in line with benchmarks in the
literature, we set an overprovisioning of 5% of the estimated
peak trafﬁc . The results conﬁrm that our algorithms attain
much smaller operator monetary costs than the traditional
technique, with gains of up to 80%.
CONCLUSIONS
In this paper, we presented some of the challenges and
opportunities that AI offers in the context of 5G networks.
By deﬁning a framework that joins contributions from various
initiatives and populating it with AI-based algorithms serving
different purposes, we showed how standards can be leveraged
to deploy AI-based 5G systems. Our performance evaluation
results illustrate the beneﬁts of a proper integration of AI
into 5G. Importantly, this work also provides a basis to apply
AI to other functions within the 5G system beyond the ones
addressed in the paper.
TABLE I: Unserviced demand and cost gains for AI-LTF, AI-MTF, and AI-STF and the overall system, for different α
values. The percentage of unserviced demand is given by the amount of trafﬁc exceeding the capacity forecasted by AI-LTF,
AI-MTF, and AI-STF. Cost gains are computed as the difference between the costs of the traditional and the AI-based
approaches over the cost of the traditional approach. The cost of the overall system is computed as the sum of the costs of
the three algorithms.
Unserviced demand (%)
Cost gains (%)
Overall system