MIDAS Regressions:
Further Results and New Directions∗
Eric Ghysels†
Arthur Sinko‡
Rossen Valkanov§
First Draft: February 2002
This Draft: February 7, 2006
We explore Mixed Data Sampling (henceforth MIDAS) regression models.
The regressions
involve time series data sampled at diﬀerent frequencies. Volatility and related processes are
our prime focus, though the regression method has wider applications in macroeconomics and
ﬁnance, among other areas. The regressions combine recent developments regarding estimation
of volatility and a not so recent literature on distributed lag models. We study various lag
structures to parameterize parsimoniously the regressions and relate them to existing models.
We also propose several new extensions of the MIDAS framework. The paper concludes with an
empirical section where we provide further evidence and new results on the risk-return tradeoﬀ.
We also report empirical evidence on microstructure noise and volatility forecasting.
∗We thank two Referees and an Associate Editor, Alberto Plazzi, Pedro Santa-Clara as well as seminar
participants at City University of Hong Kong, Emory University, the Federal Reserve Board, ITAM, Korea
University, New York University, Oxford University, Tsinghua University, University of Iowa, UNC, USC,
participants at the Symposium on New Frontiers in Financial Volatility Modelling, Florence, the Academia
Sinica Conference on Analysis of High-Frequency Financial Data and Market Microstructure, Taipei, the
CIREQ-CIRANO-MITACS conference on Financial Econometrics, Montreal and the Research Triangle
Conference, for helpful comments. All remaining errors are our own.
†Department of Finance, Kenan-Flagler School of Business and Department of Economics University of
North Carolina, Gardner Hall CB 3305,Gardner Hall CB 3305, Chapel Hill, NC 27599-3305, phone: (919)
966-5325, e-mail: .
‡Department of Economics, University of North Carolina, Gardner Hall CB 3305, Chapel Hill, NC 27599-
3305, e-mail: 
§Rady School of Management, UCSD, Pepper Canyon Hall, 3rd Floor 9500 Gilman Drive, MC 0093 La
Jolla, CA 92093-0093, phone: (858) 534-0898, e-mail: .
Introduction
The availability of data sampled at diﬀerent frequency always presents a dilemma for a
researcher working with time series data. On the one hand, the variables that are available
at high frequency contain potentially valuable information. On the other hand, the researcher
cannot use this high frequency information directly if some of the variables are available at
a lower frequency, because most time series regressions involve data sampled at the same
interval. The common solution in such cases is to “pre-ﬁlter” the data so that the left-hand
and right-hand side variables are available at the same frequency. In the process, a lot of
potentially useful information might be discarded, thus rendering the relation between the
variables diﬃcult to detect.1 As an alternative, Ghysels, Santa-Clara, and Valkanov ,
 and have recently proposed regressions that directly accommodate variables
sampled at diﬀerent frequencies. Their MIxed Data Sampling – or MIDAS – regressions
represent a simple, parsimonious, and ﬂexible class of time series models that allow the
left-hand and right-hand side variables of time series regressions to be sampled at diﬀerent
frequencies.
Since MIDAS regressions have only recently been introduced, there are a lot of unexplored
questions.
The goal of this paper is to explore some of the most pressing issues, to lay
out some new ideas about mixed-frequency regressions, and to present some new empirical
results. Before we start, it is useful to introduce a simple MIDAS regression. Suppose that
a variable yt is available once between t −1 and t (say, monthly), another variable x(m)
observed m times in the same period (say, daily or m = 22), and that we are interested in
the dynamic relation between yt and x(m)
. In other words, we want to project the left-hand
side variable yt onto a history of lagged observations of x(m)
t−j/m. The superscript on x(m)
denotes the higher sampling frequency and its exact timing lag is expressed as a fraction of
the unit interval between t −1 and t. A simple MIDAS model is
yt = β0 + β1B(L1/m; θ)x(m)
for t = 1, . . . , T and where B(L1/m; θ) = PK
k=0 B(k; θ)Lk/m and L1/m is a lag operator such
that L1/mx(m)
t−1/m, and the lag coeﬃcients in B(k; θ) of the corresponding lag operator
Lk/m are parameterized as a function of a small-dimensional vector of parameters θ.
1This situation is becoming more frequent now as dramatic improvements in information gathering have
produced new, high-frequency datasets, particularly in the area of ﬁnancial econometrics.
In the mixed-frequency framework (1), the number of lags of x(m)
is likely to be signiﬁcant.
For instance, if monthly observations of yt is aﬀected by six months’ worth of lagged daily
’s, we would need 132 lags (K = 132) of high-frequency lagged variables.
parameters of the lagged polynomial are left unrestricted (or B(k) does not depend on
θ), then there would be a lot of parameters to estimate. As a way of addressing parameter
proliferation, in a MIDAS regression the coeﬃcients of the polynomial in L1/m are captured
by a known function B(L1/m; θ) of a few parameters summarized in a vector θ. We will
discuss several alternative speciﬁcations of B(L1/m; θ) in the paper. Finally, the parameter
β1 captures the overall impact of lagged x(m)
’s on yt. We identify β1 by normalizing the
function B(L1/m; θ) to sum up to unity. While the normalization and the identiﬁcation of β1
are not strictly necessary in a MIDAS regression, they will be very useful for our applications
later in the paper.
In some speciﬁc cases, the results from the MIDAS regressions can be obtained using highfrequency regressions alone.
We work out one such example in the context of volatility
forecasting. While we are able to derive an explicit relation between the MIDAS parameters
and the purely high-frequency model, the relation is already quite complicated in this simple
case. For more interesting applications, such as these we conduct later in the paper, such
a relation is diﬃcult to derive. This ﬁnding illustrates another advantage of our approach:
the MIDAS speciﬁcation captures a very rich dynamic of the high-frequency process in a
very simple and parsimonious fashion.
The MIDAS models beneﬁt from several strands
of econometric models. The parameterization of the polynomial is similar in spirit to the
distributed lag models , Dhrymes and Sims for surveys
on distributed lag models).
Mixed data sampling regression models share some features
with distributed lag models but also have unique features. For instance, while we use a
parameterization of B(k; θ) that is common in distributed lag models, we also introduce a
new one called beta polynomial and that appears well suited in the applications that we
consider. We also discuss MIDAS regressions with stepfunctions introduced in Forsberg and
Ghysels . Their appeal is the use of OLS estimation methods, but this comes at a
cost, namely that parsimony may not be preserved.
A convenient parametric function of B(L1/m; θ) also allows us to directly deal with lag
selection. In an unrestricted case, we have to design a lag selection procedure which can be
particularly diﬃcult in this setup, where we will have to make the choice whether to include,
say, 66 or 67 daily lags in forecasting of a monthly observation yt. The parameterizations
of B(L1/m; θ) that we propose are quite ﬂexible.
For diﬀerent value of θ, they can take
various shapes.
In particular, the parameterized weights can decrease at diﬀerent rates
as the number of lags increases. Therefore, by estimating θ, we eﬀectively allow the data
to select the number of lags that are needed in the mixed-data relation between yt and xt.
Hence, once we choose the appropriate functional form of B(L1/m; θ), the lag length selection
in MIDAS is purely data-driven.
Variations of the MIDAS regression (1) have been used by Ghysels, Santa-Clara, and
Valkanov , Ghysels, Santa-Clara, and Valkanov . More complex speciﬁcations
are certainly possible and, in this paper, we propose several natural extensions of the basic
MIDAS regressions. First, on the right-hand side we can include variables sampled at various
frequencies. Second, non-linearities are easy to introduce as demonstrated by Ghysels, Santa-
Clara, and Valkanov who use one such model. In this paper, we discuss more general
non-linear MIDAS regressions. Third, MIDAS can accommodate tick-by-tick data that are
observed at unequally spaced intervals. Finally, multivariate MIDAS regressions are also
possible. All of these models are new and still unexplored. Some of them present unique
challenges, others are straightforward to estimate.
We revisit two empirical applications that related to prior studies, (1) the risk-return tradeoﬀand (2) volatility prediction. Regarding the risk-return trade-oﬀ, we present a variation
of the results in Ghysels, Santa-Clara, and Valkanov and Ghysels, Santa-Clara, and
Valkanov . The ﬁrst paper uses a MIDAS regression to show that there is a positive
relation between market volatility and return. Expected returns are proxied using monthly
averages while the variance is estimated using daily squared returns over the last year. The
second paper shows that while squared daily returns are good forecasts of future monthly
variances, there are predictors that clearly dominate. Here, we combine the insights from
both papers. First, we look at the risk-return relation at diﬀerent frequencies, one, two,
three, and four weeks.
Second, we use a diﬀerent polynomial speciﬁcation from the one
used in Ghysels, Santa-Clara, and Valkanov .2 Third, we use several predictors that
Ghysels, Santa-Clara, and Valkanov show are good at forecasting future volatility
in a MIDAS context. Finally, we use a diﬀerent dataset from Ghysels, Santa-Clara, and
Valkanov .
2For further evidence on the risk-return trade-oﬀusing MIDAS, see e.g. ´Angel, Nave, and Rubio ,
Wang and Charoenrook and Conrad . Models of idiosyncratic volatility using MIDAS appear
in e.g. Jiang and Lee and Brown and Ferreira .
We ﬁnd that there is a robustly positive and statistically signiﬁcant risk-return tradeoﬀ
across horizons and across predictors. Remarkably, the tradeoﬀis signiﬁcant even for weekly
returns, even though they are noisy proxies of expected returns. However, the relation is
clearer at the two to four week horizon. Surprisingly, we ﬁnd that variables that are better
at predicting the variance do not necessarily produce better forecasts of expected returns or
better estimates of the risk-return tradeoﬀ. Hence, they must be capturing a component of
the variance that is not priced by the market and consequently that is unrelated to expected
We also include empirical evidence on the impact of microstructure noise on volatility
prediction.
While using high frequency data has some clear advantages, there are some
costs. High frequency sampling may be plagued by microstructure noise. Several papers
have tried to shed light on this: A¨ıt-Sahalia, Mykland, and Zhang , Bandi and Russell
 , Bandi and Russell , Hansen and Lunde , Zhang, Mykland, and A¨ıt-
Sahalia , among others have suggested corrections for microstructure noise. We assess
how much these corrections improve forecasting.
The paper is structured as follows. Section two discusses various polynomial speciﬁcations.
Section three shows that the MIDAS framework is very ﬂexible and captures a rich set
of dynamics that would be diﬃcult to obtain using standard same-frequency regressions.
Section four presents various extensions of MIDAS models, such as a generalized MIDAS
regression, non-linear MIDAS regressions, tick-by-tick MIDAS regressions, and multivariate
MIDAS. In section ﬁve, we apply some of the generalizations to estimate the relation between
conditional expected return and risk using ten years of daily Dow Jones index return data.
Some of our results conﬁrm previous ﬁndings, others are quite surprising and oﬀer new
directions for research. In section six, we oﬀer concluding remarks.
Polynomial Speciﬁcations
The parameterization of the lagged coeﬃcients of B(k; θ) in a parsimonious fashion is one
of the key MIDAS features.
In this section, we discuss various speciﬁcations of MIDAS
regression polynomials. A ﬁrst subsection is devoted to ﬁnite polynomials and we discuss
in particular two parameterizations that were used in previous papers and that we will use
in the empirical section of this paper. A second subsection deals with inﬁnite polynomials
and introduces autoregressive augmentations and rational polynomials. A third subsection
deals with MIDAS regressions using stepfunctions. The ﬁnal subsection covers identiﬁcation
Finite Polynomials: Exponential Almon and Beta
In this section, we focus on the speciﬁcation (1) appearing. More speciﬁcally, we deal with
ﬁnite one-sided polynomials applied to a single regressor. This is one of the simplest MIDAS
speciﬁcations and it allows us focus on the parameterization of B(k; θ).
We focus on two parameterizations of B(k; θ). The ﬁrst one is:
eθ1k+...+θQkQ
k=1 eθ1k+...+θQkQ
which we call the ”Exponential Almon Lag,” since it is related to “Almon Lags” that are
popular in the distributed lag literature or Judge, Griﬃth, Hill, L¨utkepohl,
and Lee ). The function B(k; θ) is known to be quite ﬂexible and can take various
shapes with only a few parameters for
further discussion). Ghysels, Santa-Clara, and Valkanov use the functional form (2)
with two parameters, or θ = [θ1; θ2]. Figure 1 illustrates the ﬂexibility of the Exponential
Almon Lag even in this simple two-parameter case. First, it is easy to see that for θ1 = θ2 = 0,
we have equal weights (this case is not plotted). Second, the weights can decline slowly (top
panel) or fast (middle panel) with the lag. Finally, the exponential function (2) can produce
hump shapes as shown in the bottom panel of Figure 1. A declining weight is guaranteed as
long as θ2 ≤0. It is important to point out that the rate of decline determines how many
lags are included in regression (1). Since the parameters are estimated from the data, once
the functional form of B(k; θ) is speciﬁed, the lag length selection is purely data driven.
The second parameterization has also only two parameters, or: θ = [θ1; θ2]:
B(k; θ1, θ2) =
K, θ1; θ2)
K, θ1; θ2)
f(x, a, b) = xa−1(1 −x)b−1Γ(a + b)
Speciﬁcation (3) has, to the best of our knowledge, not been used in the literature. It is
based on the Beta function and we refer to it as the “Beta Lag.” Figure 6 displays various
shapes of (3) for several values of θ1 and θ2. The function can also take many shapes not
displayed in the ﬁgure. For instance, it is easy to show that for θ1 = θ2 = 1 we have equal
weights (this case is not shown). As in Figure 1, we only display parameter settings that
are relevant for the types of applications we have in mind. The top panel in Figure 6 shows
the case of slowly declining weight which corresponds to θ1 = 1 and θ2 > 1. As θ2 increases,
we obtain faster declining weights, as shown in the middle panel of the ﬁgure. Finally, the
bottom panel illustrates a hump-shaped pattern which emerges for θ1 = 1.6 and θ2 = 7.5.3
The ﬂexibility of the Beta function is well-known. It is often used in Bayesian econometrics
to impose ﬂexible, yet parsimonious prior distributions. As pointed out in the Exponential
Almon Lag case, the rate of weight decline determines how many lags are included in the
MIDAS regression.
The Exponential Almon and the Beta Lag speciﬁcations have two important characteristics,
namely, (i) they provide positive coeﬃcients, which is necessary for a.s. positive deﬁniteness
of estimated volatility, and (ii) they sum up to unity. We impose positive weights because
volatility modelling is the main application in this paper. The latter property allows us
to identify a scale parameter β1, that is, we run MIDAS regression models as speciﬁed
While MIDAS regression models are not limited to the two aforementioned
distributed lag schemes, for our purpose we focus our attention exclusively on these two
parameterizations. The speciﬁcation in (2) is theoretically more ﬂexible, since it depends
on Q parameters. However, for the stability of the solution additional restrictions should be
imposed: θi ≤0, ∀i = 1, .., Q ). On the
other hand, the weight speciﬁcation in (3) if ﬂexible enough to generate various shapes with
only two parameters.
3Convex shapes appear when θ1 > θ2. While those shapes are not of immediate interest in our volatility
applications, they might be very useful in other applications.
There is the obvious concern how to choose K in (1). Several papers have been written on
the eﬀects of misspecifying the lag length in Almon lag models, see the discussion in Judge,
Griﬃth, Hill, L¨utkepohl, and Lee (section 9.3.2), as well as on the subject of lag
selection, see Judge, Griﬃth, Hill, L¨utkepohl, and Lee (section 9.3.4). The existing
literature can be readily applied in the context of MIDAS regressions with m ﬁxed. There is,
however, a topic that requires special attention. Many papers were also written about ﬁnite
polynomial approximations to inﬁnite lags , Dhrymes
 , Sims , among others). Most revolve around rational fraction approximations.
In MIDAS regressions this raises issues that are not straightforward and to which we return
Inﬁnite Polynomials and Autoregressive Augmentations
The class of ARMA and GARCH models exploit the fact that a ratio of two ﬁnite polynomials
B(L)/A(L) implies an inﬁnite lag polynomial.
The same idea has been advanced in
distributed lag models, see e.g. Jorgenson . A geometric lag model ,
Nerlove , Cagan ) refers to the speciﬁc case where A(L) is a polynomial
of degree one.
In such a case, in a usual time series regression where yt and xt are
observed at the same frequency, we have yt+1 = β0 + λyt + B(L)xt + εt+1 and hence,
yt+1 = ˜β0 + (B(L)/(1 −λL))xt + ˜εt+1 so that a simple autoregressive augmentation of a
distributed lag model yields a parsimonious way of producing an inﬁnite lag polynomial.
Autoregressive augmentation can be introduced in MIDAS regressions in two alternative
ways. Indeed, we can write
yt+1 = β0 + λyt + β1B(L1/m; θ)x(m)
yt+1 = β0 + λyt+1−1/m + β1B(L1/m; θ)x(m)
It is immediately clear that these two speciﬁcations are not equivalent. They can be written
respectively as:
yt+1 = ˜β0 + β1B(L1/m; θ)/(1 −λL)x(m)
yt+1 = ˜β0 + β1B(L1/m; θ)/(1 −λL1/m)x(m)
Both speciﬁcation should be used with the following caveats. In the case of (5), we do not
obtain a geometric polynomial in L1/m but rather a polynomial B(L1/m; θ) P
j λjLj which is
a mixture with geometrically declining spikes at distance m. Hence, we obtain a “seasonal”
polynomial and this augmentation can be used only if there are seasonal patterns in x(m)
The second polynomial is geometric in L1/m and indeed yields B(L1/m; θ) P
However, it assumes that lagged yt+1−1/m are available.
This amounts to considering a
special case of a distributed lag model. Moreover, speciﬁcation (6) has some econometric
complications, since the appearance of y(m)
t+1−1/m implies that one has to deal with endogenous
regressors and with instrumental variable estimation in a MIDAS context. Ghysels, Santa-
Clara, and Valkanov discuss the econometric implications, in particular eﬃciency
losses that occur due to the fact that the introduction of lagged dependent variables is most
often not possible in MIDAS regressions.
Despite these diﬃculties, the use of ﬁnite polynomial ratios to accommodate inﬁnite lag
MIDAS speciﬁcations is still promising.
For instance, consider the following MIDAS
regression:
yt = β0 + β1[B1K(L1/m)/B2Q(L1/m)]x(m) + εt ≡β0 + β1
k=1 B1(k, θ)Lk/m
k=1 B2(k, θ)Lk/mx(m)
where K and Q are the respective orders of the polynomials in the numerator and
denominator. The speciﬁcation in (9) is a MIDAS version of the rational distributed lag
model discussed in Jorgenson . It should also be noted that Bollerslev and Wright
 suggest to use smoothed periodogram estimators to deal with parameter proliferation
in the context of high-frequency ﬁnancial data. Periodogram estimators are in essence inﬁnite
parameters settings and typically imprecise in applications that do not involve very large
data sets.
Stepfunctions
The advantage of the MIDAS framework is that we maintain a relatively simple parametric
format and are also able to extend it easily to non-linear and multivariate settings as
discussed later. The drawback is that we have to use non-linear estimation methods since
all the polynomial lag structures are constrained via non-linear functional speciﬁcations. We
conclude the section with some observations about MIDAS with stepfunctions, introduced
in Forsberg and Ghysels . These MIDAS regressions are inspired by the HAR model
of Corsi which was also used in Andersen, Bollerslev, and Diebold . To deﬁne
a MIDAS regression with stepfunctions, consider regressors Xt(K, m) ≡PK
t−j/m, which
are partial sums of high frequency x(m). Then the MIDAS regression with M steps is:
βiXt(Ki, m) + εt
where K1 < . . . < KM. The impact of x(m)
is measured by PM
i=1 βi, since it appears in all
the partial sums (or steps). The impact of x(m)
t−j for K1 < j K2 is measured by PM
Hence, the distributed lag patterns is approximated by a number of discrete steps. The
more steps appear in the regressions the less parsimonious, which deﬁes the purpose of the
MIDAS regression approach. Yet, stepfunction approximations can be very useful and their
ease to estimate can be very appealing. Besides Forsberg and Ghysels , MIDAS with
stepfunctions is also used in Ghysels, Sinko, and Valkanov to study the impact of
economic news on the cross-section of returns.
Identiﬁcation issues
The stepfunctions discussed in the previous subsection have another advantage compared
to the generic MIDAS regression appearing in equation (1). Suppose we want to test the
hypothesis that βi = 0 ∀i ≥1 in equation (10).
In equation (1) the same hypothesis,
namely that none of the regressors are signiﬁcant, comes with further complications since
the slope parameter β1 and the parameters in θ, governing the polynomial B(L1/m; θ), are
not separately identiﬁed under the null. Therefore testing such hypothesis involves testing
problems where nuisance parameters vanish under the null hypothesis and this in turn aﬀects
the asymptotic distribution of the resulting test statistic.
There is a considerable literature on how to deal with hypothesis testing in the presence of
parameters that are not identiﬁed under the null. If θ were known, then the testing problem
could be formulated easily, namely β1 can be estimated via OLS, given θ, and hence yield an
estimate ˆβ1(θ). Testing the null hypothesis that β1 = 0 would not be complicated, namely
under conventional regularity conditions a heteroskedasticity-robust t-test takes the form:
WT(θ) = T ˆβ1(θ)′R(R ˆV ∗
T R−1R′ ˆβ1(θ)
where R is the selection matrix ′, and ˆV ∗
T is a HAC covariance estimator of the parameters
associated with the OLS regression. Under standard regularity conditions this statistic has
point-optimal interpretation and χ2
1 limiting distribution as T →∞. Davies and
 suggested testing the null by supθ∈Θ WT (θ), where Θ is the parameter space assumed
to be a bounded subset of the reals. Hansen derive a distributional theory based on a
local-to-null reparameterization: β1 = c/
T, with the null hypothesis now being c = 0 and
the alternative c ̸= 0. To compute p-values, Hansen a simulation approach (see page
419). The solution of Davies and gives a straightforward, if conservative,
method for adjusting the testing statistic, whereas the Hansen simulation approach
should be less conservative.
Reverse Engineering the MIDAS Regression
One may still wonder whether it is necessary to use polynomials like the ones presented
in the previous section. In some cases, one can indeed formulate a time-series model for
the data sampled at frequency 1/m and compute the implied MIDAS regression which is
an exercise we shall call reverse engineering. The purpose of this section is to go through
such an exercise and to show that it is feasible only in some very special cases. However,
in general this approach appears to be an impractical alternative to MIDAS regressions.
The complexity of the reverse engineering will clarify the appeal of the route we advocate:
simplicity, ﬂexibility, and parsimony.
We consider an example drawn from the volatility literature.
To set the stage, let us
reconsider equation (1) where the right-hand side variable is y(m)
. In other words, yt is
observed at two frequencies. In addition, assume that both yt and y(m)
are generated by
a weak GARCH(1,1) process.4 More speciﬁcally, consider the so-called GARCH diﬀusion
which yields exact weak GARCH(1,1) discretization that are represented by the following
equations:
ln Pt −ln Pt−1/m = r(m)
= σ(m),tz(m)
(m),t = φ(m) + α(m)[r(m)
t−1/m]2 + β(m)σ2
4The terminology of weak GARCH originated with the work of Drost and Nijman and refers
to volatility predictions involving only linear functionals of past returns and squared returns. Obviously,
many ARCH-type models involve nonlinear functions of past (daily) returns. It would be possible to study
nonlinear functions involving distributed lags of high frequency returns. This possibility is explored later in
the paper.
where z(m)
is Normal i.i.d. (0, 1) and r(m)
is the returns process sampled at frequency 1/m.5
Suppose we run regression (1) between the (monthly) sum of squared returns and (daily)
squared returns, i.e., we estimate
t+j/m]2 = β0 + β1B(L1/m)[r(m)
then the resulting MIDAS regression would be:
(m + ρ(m))φ(m)
[mφ(m) + δ(m)]ρ(m)
[mφ(m) + δ(m)] P∞
k=0(β(m)/β1)kLk
where ρ(m) = 1/(1−β(m)) and δ(m) = (1−(α(m) +β(m))m)α(m)/(1−α(m) −β(m))(α(m) +β(m)).
Clearly, in this simple case, the MIDAS regression can be reverse engineered and would yield
estimates of the underlying weak GARCH(1,1) model or the GARCH diﬀusion.
The simplicity of this example may lead one to think that this path is promising. However,
as the following example shows, things become quite complicated when more realistic models
are used. In particular, many recent papers on volatility suggest that the process should be
modelled as a two-factor model. Ding and Granger and Engle and Lee suggest
a two-factor GARCH model. Two-factor stochastic volatility models have been proposed
by Alizadeh, Brandt, and Diebold , Chacko and Viceira , Gallant, Hsu, and
Tauchen and Chernov, Gallant, Ghysels, and Tauchen .
The latter study
provides a comprehensive comparison of various one- and two-factor continuous time models
and ﬁnds the log-linear two-factor model to be the most promising. Maheu shows
that the two-factor GARCH models can also take into account the long-range dependence
found in ﬁnancial market volatility. In light of this, let us consider a two-factor GARCH
model where each factor follows a GARCH(1,1) process as speciﬁed in equations (1) through
(4) appearing in Appendix 6). This model yields a restricted GARCH(2,2) representation
5The GARCH parameters of (11) are related to the GARCH diﬀusion via formulas appearing in
Corollary 3.2 of Drost and Werker . Likewise, Drost and Nijman derive the mappings between
GARCH parameters corresponding to processes with r(m)
sampled with diﬀerent values of m.
for (the observable process) h(m)
(1 −ρ2(m))ω(m) + (α1(m) + α2(m))[ϵ(m)
−(ρ1(m)α2(m) + ρ2(m)α1(m))[ϵ(m)
+(ρ1(m) + ρ2(m) −α1(m) −α2(m))h(m)
−(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m))h(m)
where ρi(m), ω(m), αi(m) determine the volatility components, for i = 1,2, and are explicitly
deﬁned in Appendix 6.
Using the computations in equations (5) through (8), which appears in Appendix 6, we can
derive the implied MIDAS regression, for a case where m = 4, applicable to a monthly/weekly
MIDAS regression setting. The intercept of the MIDAS regression is:
(1 −ρ2(m))ω(m)(4 −(ρ1(m) + ρ2(m)) −ρ1(m)ρ2(m) −(ρ1(m) + ρ2(m))2
−ρ1(m)ρ2(m) −(ρ1(m) + ρ2(m))ρ1(m)ρ2(m) −(ρ1(m) + ρ2(m))3 −2(ρ1(m) + ρ2(m))×
ρ1(m)ρ2(m) −(ρ1(m) + ρ2(m))2ρ1(m)ρ2(m) −(ρ1(m)ρ2(m))2 −(ρ1(m) + ρ2(m))4
−3(ρ1(m) + ρ2(m))2ρ1(m)ρ2(m) −(ρ1(m)ρ2(m))2 −(ρ1(m) + ρ2(m))3ρ1(m)ρ2(m)
−2(ρ1(m) + ρ2(m))(ρ1(m)ρ2(m))2)
Despite the simplicity of the model and the low value of m we ﬁnd that the implied MIDAS
polynomial is extremely complex and impractical. It appears in the Appendix as formula (9).
It is also worth noting that for stochastic volatility models the problem is even more diﬃcult
since the volatility factors are latent and therefore need to be extracted from observed past
returns. This is an extremely diﬃcult task to perform for which there are no analytical
closed-form solutions.6
The two examples in this section show that reverse engineering is not a practical solution,
except in some very limited circumstances.
It should also be noted that this analysis is
conﬁned to MIDAS regressions involving a pure autoregressive time-series setting without
additional regressors.
If additional regressors are introduced, then reverse engineering
becomes simply impractical.
6See for instance Chernov, Gallant, Ghysels, and Tauchen for further discussion. Meddahi 
derives a weak GARCH(2,2) representation of a two-factor SV model which could be used in this particular
case, but not in a more general setting.
Variations on the MIDAS Regression Theme
In this section we cover a number of issues that come to the forefront when volatility dynamics
and its stylized facts are considered.
In a ﬁrst subsection we discuss some alternative
choices of volatility measures in the context of MIDAS regressions. The subject of nonlinear
equations and multivariate MIDAS regression models is vast and the purpose of the second
subsection is not to be comprehensive. The same observation applies to the ﬁnal subsection
dealing with tick-by-tick applications.
More General Univariate MIDAS Linear Regression Models
A general univariate MIDAS linear regression model can be written as
yt+k = β0 +
Bij(L1/mi)x(mi)
where Bij(L1/mi) are polynomials parameterized by the vector θ which we suppress for
simplicity. We will also suppress the double index to Bij when its presence is redundant. For
the purpose of exposition we will most often consider yt+k with k = 1. Equation (15) is a
conventional distributed lag model when K = 1, L = 1 and m1 = 1 and a single polynomial
MIDAS model when K = 1, L = 1 and m1 > 1. Moreover, the MIDAS regression involves
a single time series process when x(m1)
. We run a MIDAS regression where at least
two diﬀerent sampling frequencies are combined when K > 1 and L = 1. A commonly
encountered case would be m1 = 1 and either one or more mi < 1. Such a MIDAS regression
would combine for instance monthly (daily) with daily (intra-daily) data to predict future
monthly (daily) series.
MIDAS regressions with L > 1 deserve some attention and to facilitate the discussion let us
assume that K = 1 with m1 > 1. This case corresponds to having two or more polynomials
with parameters θi = (θi
2), i = 1, . . . , L that involve the same operator L1/m1. To further
simplify the discussion, suppose that L = 2 and that θ1
1 > 1 and θ2
plot one such example in Figure 3 using a mixture of two Beta lag polynomials. The ﬁrst
polynomial, plotted in the top panel, is declining, whereas the second one, plotted in the
middle panel, is “hump shaped.” Mixing the two polynomials produces a third polynomial,
plotted in the bottom panel. From this example, it becomes clear that mixing polynomials
with the same high frequency lag operator would allow us to capture seasonal patterns or
rich non-monotone decay structures. However, the price for this ﬂexibility will be a less
parsimonious speciﬁcation as L increases.
Non-Linear MIDAS Regression Models
So far we carried out the analysis with the basic univariate MIDAS regression model. We
can further generalize the regression appearing in (15) to:
yt+k = β0 + f(
Bij(L1/mi)g(x(mi)
where the functions f and g can either be known functions or else parameter dependent.
For example, in many volatility applications one takes the log transformation, i.e. one tries
to predict future log volatility (yt+k) and therefore takes f equal to log, with g(x) = x. One
parametric choice for g of interest in the context of volatility is the following:
yt+k = β0 +
Bij(L1/mi)(r(m)
|)2 + εt+1
The above speciﬁcation is very much inspired by the EGARCH model of Nelson . We
reserve a particular parameter θL to test for leverage eﬀects, when zero we obtain the linear
MIDAS regression model. A non-zero θL entails a response for positive returns that diﬀers
from that of negative returns. The parameter θL is estimated jointly with the polynomial
parameters θ and any other parameters appearing in the MIDAS regression model.
Equation (17) could be viewed as a nonlinear MIDAS regression model that allows us to
investigate a particular issue, namely leverage. There are other models of this kind that
can be tailored to a speciﬁc question and we leave this topic for further research. It should
parenthetically be noted that the speciﬁcation in (17) also applies to the risk-return tradeoﬀequation and possibly other settings as well. Ghysels, Santa-Clara, and Valkanov 
indeed ﬁnd that θL is signiﬁcant with monthly/daily MIDAS regression regressions.
Another choice of a parameter dependent function g in (16) is the Box-Cox transformation,
which in the context of ARCH type models has been considered by Higgins and Bera
 , Ding, Granger, and Engle , Hentschel , and Duan . In general,
non-linearities in MIDAS regressions can be handled without complications using standard
econometric approaches.
Tick-by-Tick Applications
Unequally spaced data is a topic of interest in ﬁnance and other areas , Duﬃe and Glynn , Dufour and Engle , Engle ,
Ghysels and Jasiak , Renault and Werker for some recent examples and further
references).
The idea of a MIDAS regression where polynomial weights are governed by
hyperparameters is not necessarily limited to equal divisions of the reference interval. Hence,
instead of using the lag operator L1/m one can use an operator Lτ where τ is real-valued
instead of a rational number. When the MIDAS polynomial is for example of the Almon-type
then the weight for the τ th lag becomes:
eθ1k+...+θQkQ
k=1 eθ1k+...+θQkQ
where typically k is measured in time elapsed like a lag operator. Consequently, if we have
a data set of transactions data and are interested in predicting tomorrow’s volatility (t + 1)
using all the transactions data of the previous day or part of the previous day we can use,
say, [r(t,τi)]2, where the index (t, τi) refers to the time between to the close on day t and
transaction i on day t.
The unequally spaced applications have the virtue that one does not estimate the MIDAS
polynomial on a ﬁxed, equally-spaced grid, but rather using past random events. Obviously,
it is not clear that microstructure noise may prevent us from putting this idea to work in
the context of volatility applications. There are, however, other areas of interest pertaining
to the microstructure of the market, such as measuring the price impact of trades, where
following a MIDAS approach applied to unequally spaced data may be useful.
Multivariate MIDAS Regression Models
We turn now to multivariate speciﬁcations. If we consider a linear MIDAS, we can further
generalize the regression appearing in (15) to:
Yt+1 = B0 +
Bij(L1/mi)X(mi)
where Y, ε, and X are n-dimensional vector processes, B0 a n-dimensional vector and Bij
are n × n matrices of polynomials. The main issue of course is how to handle parameter
proliferation in multivariate settings. One approach would be to take all the oﬀ-diagonal
elements as controlled by one polynomial whereas the diagonal elements have a common
second polynomial.
Such restrictions may not always be appropriate.
Ultimately, the
restrictions that are needed to reduce the number of parameters will be dictated by the
application at hand.
Multivariate applications in the context of volatility would typically involve trading volume.
In principle, one can consider a MIDAS regression model explaining jointly future trading
volume and future volatility by past intra-daily trading volume and squared returns. This
application is very much in the spirit of univariate MIDAS regression volatility models.
Considering multivariate MIDAS regressions (18) allows us to address Granger causality
It is of particular interest, because the notion of Granger causality, as put forth
in Granger , is subject to temporal aggregation error that can disguise causality or
actually create spurious causality when a relevant process is omitted.7
While the MIDAS regression framework does not necessarily resolve all aggregation issues,
it might provide a convenient and powerful way of testing for Granger causality. Indeed, in
typical VAR models based on same-frequency regressions, Granger causality may be diﬃcult
to detect due to temporal aggregation on the right-hand side variables. The restrictions on
the polynomials to test for causality are very much the same as those in the regular Granger
causality tests. It is also worth noting that MIDAS regression polynomials, univariate or
multivariate, can be two-sided, i.e., they can involve future realizations of x(m). This allows
us to conduct Granger causality tests as suggested by Sims .
The multivariate speciﬁcations include systems of equations that can address ARCH-in-mean
7There is a considerable literature on the subject. See, e.g. Breitung and Swanson for a recent
discussion.
eﬀects. In particular, consider the system
rt+1 = b10 + b1B1(L1/m)[r(m)
]2 + ε1,t+1
Qt,t+1 = b20 + b2B2(L1/m)[r(m)
]2 + ε2,t+1
where the ﬁrst equation in (19) refers to the return-volatility tradeoﬀand the second is
a volatility predictor, i.e.
Qt,t+1 is next period’s realized volatility.
If we restrict the
polynomials in the two equations to be equal and estimate the system simultaneously then
we have a model like the ARCH-in-mean speciﬁcation. However, the ﬂexibility of MIDAS
regression models also allows us to estimate the ﬁrst and second equation in (19) separately,
and hence one can test the imposed polynomial restriction.
We can conclude this section with the observation that the MIDAS regressions are very
While we have attempted to be comprehensive in the variations of MIDAS
speciﬁcations, there are certainly interesting models that we have omitted. As with samefrequency regressions, the speciﬁcation of the model, be it multivariate or non-linear, will be
guided by the researchers’ agenda and ingenuity.
Two Empirical Examples
In this section we report on two empirical applications involving MIDAS regression models.
We revisit (1) the risk-return trade-oﬀand (2) volatility prediction. Regarding the riskreturn trade-oﬀ, we present a variation of the results in Ghysels, Santa-Clara, and Valkanov
 and Ghysels, Santa-Clara, and Valkanov . Regarding volatility, we study the
impact of microstructure noise on volatility prediction. A subsection is devoted to each topic.
Revisiting the Risk-Return Tradeoﬀ
In this subsection we revisit Merton’s ICAPM model, which suggests that the
conditional expected excess returns on the stock market should vary positively with the
market’s conditional variance:
Et[Rt+1] = µ + γV art[Rt+1],
where γ is the coeﬃcient of relative risk aversion of the representative agent. This relation
has received a lot of attention in empirical ﬁnance. The main diﬃculty in testing the ICAPM
resides in the fact that the conditional mean and variance of the market are not observable
and must be ﬁltered from past returns.
To quickly review the literature, Baillie and
DeGennaro , French, Schwert, and Stambaugh , Chou , and Campbell and
Hentschel ﬁnd a positive but insigniﬁcant relation between the conditional variance
and the conditional expected return. Using diﬀerent methods, Campbell and Nelson
 ﬁnd a signiﬁcantly negative relation, whereas Glosten, Jagannathan, and Runkle
 , Harvey , and Turner, Startz, and Nelson ﬁnd both a positive and a
negative relation depending on the method used. Other related papers are Chan, Karolyi,
and Stulz , Lettau and Ludvigson , Merton , and Pindyck .
In a recent paper, Ghysels, Santa-Clara, and Valkanov estimate equation (20) using
monthly returns as proxies for expected returns and daily squared returns in the estimation
of the conditional variance.
In the speciﬁcation of the MIDAS weights, they use the
Exponential Almon Lag (2) of second degree. Using CRSP value weighted returns from
January 1928 to December 2000, they ﬁnd a positive and statistically signiﬁcant risk-return
tradeoﬀ. The authors argue that their signiﬁcant and positive results obtain because their
MIDAS speciﬁcation allows them to use monthly returns in speciﬁcation of the mean and
daily squared returns in the estimation of the variance.
In another MIDAS paper, Ghysels, Santa-Clara, and Valkanov ﬁnd that volatility can
be forecasted using daily regressors other than squared returns. They use MIDAS regressions
to predict realized volatility at weekly, two-weeks, three-weeks, and monthly horizons. The
authors show that better in- and out-of-sample estimates of the volatility are obtained when
the predictors on the right-hand side are daily absolute returns, daily realized volatilities,
daily ranges, and daily realized powers. The exact deﬁnitions of these predictors are provided
below. The daily realized volatility, daily ranges, and daily realized powers are obtained from
intra-daily (5 −minute) data of the Dow Jones index returns over the period from April
1993 to October 2003. Ghysels, Santa-Clara, and Valkanov show that the best overall
predictor of conditional volatility is the realized power and that, not surprisingly, better
forecasts are obtained at shorter (weekly) horizons.
In this subsection, we address several outstanding questions that arise from the previously
cited papers. First, is it possible to uncover a positive risk-return relation at frequencies from
one week to one month, given that volatility is well forecasted at high frequencies, but also
that our measure of expected returns grows increasingly noisier as the horizon decreases?
Second, can we improve on the estimation of the tradeoﬀby using predictors other than
squared daily returns? Third, would the results change if the parameters are speciﬁed as a
Beta Lag (3) function instead of an Exponential Almon Lag? Finally, there is also a question
of whether the Ghysels, Santa-Clara, and Valkanov results can be replicated using a
diﬀerent dataset and a shorter sample period.
Methodology using MIDAS Regressions
We answer these questions by revisiting the risk-return equation (20) using the Dow Jones
index returns from April 1993 to October 2003. To estimate risk-return tradeoﬀparameter
γ using data at frequencies higher than a month, we obtain weekly, two-weeks, three-weeks,
and monthly returns from the 5 −minute price series. We denote the Dow Jones index
return over a horizon H as rt+H,t, similarly, we denote by rt day t return and rit the ith
5 −minute intra-daily return. We study horizons H of 5, 10, 15, and 22 days, respectively.
It is important to point out that returns are observed only once during a unit of time as
indicated by their subscript.
We consider the following regressions:
rt+H,t = µG
Expression (21) is a projection of rt+H,t onto lagged daily squared returns which corresponds
to the ARCH/GARCH-in-mean class of models (under some parameter restrictions). The
second equation determines the conditional volatility prediction, deﬁning the MIDAS
polynomial PK
k=0 B(k; θG
t−k as the prediction of volatility.
Next we study two similar
m)|rt−k| + εa
m)[hi −lo]t−k + εr
Equations (22) and (23) involve projecting rt+H,t onto past daily absolute returns and daily
ranges, respectively, which are two alternative measures of volatility. Therefore they are
natural candidate regressors in the MIDAS speciﬁcation , Ding, Granger, and Engle , Alizadeh, Brandt, and Diebold and Gallant,
Hsu, and Tauchen ).
In the next equation (24), past RVt are used to predict rt+H,t as well as future realized
volatility.
Examples of such models of volatility have been advocated by Andersen,
Bollerslev, and Diebold (and references cited therein).
rt+H,t = µQ
m)RVt−k + εQ
The last regression (25) involves “realized power” deﬁned as Pm
j,t, which has been
suggested by Barndorﬀ-Nielsen and Shephard and Barndorﬀ-Nielsen, Graversen, and
Shephard . More speciﬁcally, Barndorﬀ-Nielsen and Shephard suggest to consider the
sum of high-frequency absolute returns, or the realized power variation Pt, which is deﬁned
j=1 |rj,t|. Regression (25) projects future returns on past daily realized power.
rt+H,t = µp
t−k,t−k−1 + εp
We will estimate all ﬁve speciﬁcations under the alternative assumptions that the lag
coeﬃcients B(k; θ) follow the Beta (3) or the Exponential Almon (2) parameterization.8
The latter speciﬁcation has been used by Ghysels, Santa-Clara, and Valkanov .
By comparing the Beta and Exponential Almon results, we investigate whether the
parameterizations are ﬂexible enough to capture the dynamics of the underlying processes. If
the estimated coeﬃcients of risk aversion γ are similar across the two speciﬁcations, then this
a strong indication that they are both successful at capturing the shape of the polynomial
It is also important to point out that, while the parametric form of the lag coeﬃcients might
be the same across regressions, their shape will not be the same from predictor to predictor
and across horizons because the parameters θ will be diﬀerent. As discussed at length above,
the ﬂexible parametric speciﬁcation of the lag weights is one of the deﬁning characteristics
8Due to space limitations we will only report the Exponential Almon lag parameterization, the Beta one
is available on the web at 
of MIDAS regressions. For the estimates of γ to be directly comparable, all measures of
volatility are re-scaled to be in the same units for all horizons and across predictors.
Equations (21) - (25) are estimated at various frequencies using NLS. To correct for
heteroscedasticity we are using Newey-West corrected standard errors.
The correction
window is chosen using the covariance matrix of the parameter estimates as A−1
is an estimate of the Hessian matrix of the likelihood function and BT is an
estimate of the outer product of the gradient vector with itself applying the Bartlett kernel
window m = floor((4T/100)2/9).
Empirical results
The empirical results for equations (21) - (25) at one-, two-, three-, and four-week frequencies
appear in Table 1 (for Exponential Almon Lag weights (3)). The ﬁrst two columns of the
table reports the intercept coeﬃcient in expression (20) as well as the main parameter of
interest, γ.
Newey-West t-statistics of γ under the null of no risk-return tradeoﬀare
shown in the third column. We report the mean absolute deviation (MAD) as a measure
of goodness of ﬁt (fourth column), because it provides more robust results in the presence
of heteroskedasticity. The R2s are reported in the ﬁfth column. The estimates of θ are not
reported since they do not have an economic interpretation. However, they determine the
shape of the polynomial lags B(k; θ) which are of clear economic interest. Hence, given the
estimates of θ, we report what fraction of the polynomial lags is placed on the ﬁrst ﬁve
daily lag (column six), daily lags 6 to 20 (column seven), and lags beyond the ﬁrst twenty
days (column eight). The weights are immediately available as fractions, because they have
previously been normalized to sum up to one.
The results in the table provide interesting answers to the questions that we raise in the
previous sub-section.
First, at monthly frequency, there is a positive and statistically
signiﬁcant risk-return tradeoﬀin the Dow Jones data
for squared returns and absolute
returns only. The estimates of γ for the Exponential Almon and Beta polynomials (that
latter not reported in Table 1) are between 2.504 and 3.444 which is well within the bounds
of economically reasonable levels of risk aversion and references therein).
This result also conﬁrms the ﬁndings of Ghysels, Santa-Clara, and Valkanov who
ﬁnd a γ estimate of 2.6 using a diﬀerent dataset, shorter sample, and Exponential Almon
MIDAS weights. In addition, the γ estimated by the Exponential Polynomial is statistically
more signiﬁcant than the Beta Polynomial gamma. Surprisingly, the estimates of monthly γ
computed using other measures of volatility are not signiﬁcant but positive and within the
reasonable levels of risk aversion. This ﬁnding contradicts the evidence from Ghysels, Santa-
Clara, and Valkanov according to which the power variation and the realized volatility
predict future volatility better than the daily squared and absolute returns. For the Beta
Polynomial (not reported) the relation between conditional mean and conditional variance
is positive and statistically signiﬁcant for most of the volatility measures for one-, two-, and
three-week horizons except for the realized volatility measure for three-week horizon and for
the range measure for two-week horizon. The ﬁndings are better than for the Almon lag
polynomial as far as statistical signiﬁcance is concerned. It appears therefore that we recover
the results of Ghysels, Santa-Clara, and Valkanov with Dow-Jones data, yet ironically,
they are only obtained with the Beta Polynomial, whereas the Almon lag polynomial only
seems to work for the original setting of squared returns. In a sense, this is not so surprising
since Ghysels, Santa-Clara, and Valkanov advocate the use of the Beta Polynomial
when predicting volatility with high-frequency based volatility measures. It shows that there
are sometimes diﬀerences in the use of polynomials. We ﬁnd that the power variation predicts
worse the risk-return trade-oﬀthan other volatility measures. This also ﬁnding contradicts
the results in Ghysels, Santa-Clara, and Valkanov showing that daily realized power
is a signiﬁcantly better in- and out-of-sample predictor of future volatility. They also ﬁnd
that daily range and daily quadratic variation signiﬁcantly outperform squared daily returns
as predictors of future variance. There are at least two interpretations for this result. It
may appear that for the risk-return tradeoﬀthe superiority of volatility forecasts seems not
to matter that much for this sample. Or, it may also be true that these variables forecast
a component of the variance that does not command compensation in terms of expected
returns. The results in the tables are not a direct test of any particular hypotheses, but they
are suﬃciently robust across predictors and across horizons to lead us to believe that this
ﬁnding merits more careful analysis.
Overall, the direct comparison between the results Exponential Almon Beta polynomials
conveys a mixed message. On the one hand, the MAD goodness of ﬁt measure demonstrates
that there is no diﬀerence between volatility measures and polynomial speciﬁcations. On
the other hand, comparison between short-time horizons γ coeﬃcients demonstrates better
performance of the Beta polynomial speciﬁcation. We interpret this results as an indication
that the Beta polynomial could be a better choice for the higher frequency models, whereas
the Exponential lag polynomial could be a better choice for the lower frequency. Overall the
diﬀerences are not so spectacular and one may wonder whether there are beneﬁts using only
the relatively short samples of data for which intradaily data are available, as opposed to
the long span of daily data in the Ghysels, Santa-Clara, and Valkanov paper.
Next, we turn to Figures 4 and 5.
The top two panels show respectively the Beta and
Exponential polynomials for the one week horizon and four week horizon.
The top two
plots show the weights obtained with the empirical estimates of equations (21)- (25). In
Figure 4 we note that for realized power and absolute daily returns, the Beta and Almon
polynomials diﬀer slightly across the various other regressors. At the four week horizon we
observe a hump-shaped pattern for realized power and also to a certain degree for daily
squared returns. It is also worth noting that the most regressors, and in particular realized
power, involve polynomials putting hardly any weight on longer lags. The four week horizon
quadratic variation polynomial is also quite diﬀerent for the Beta and Almon polynomials.
The daily squared return decay pattern is similar to the one reported in Ghysels, Santa-
Clara, and Valkanov , although the latter used a diﬀerent estimator. Unlike the one
week horizon polynomials, we now observe more weight is put on the longer lags.
Finally, as expected, we also observe int Table 1) that the MAD increases steadily with the
horizon for all predictors. We also computed the MAD for a constant return prediction, see
Table 2. It is widely know that returns are hard to predict, and indeed with the regression
MAD results are compared with those unconditional return prediction MAD we do not see
much of an improvement. This is another way of saying that return predictions at such short
horizons usually lead to low R2s. Volatility predictions, in contrast yield much higher R2s
and this is the topic of the next subsection.
Volatility forecasting and microstructure noise
In this subsection we study forecasting future volatility using past volatility measures
unadjusted and adjusted for microstructure noise. The literature on the subject of market
microstructures and their impact on asset prices is considerable.
The area covers many
aspects, ranging from: (1) price discreteness issues , Harris
 ); (2) asymmetries in information , Easley and
O’Hara , Easley and O’Hara ); and (3) bid-ask spreads .)
Therefore, for a variety of reasons – including most prominently those mentioned above –
the eﬃciency price process is concealed by a veil of microstructure noise.9
The availability of high-frequency data in recent years led to extensive empirical research on
methods for studying the stylized facts and possibly correcting asset returns for the presences
microstructure noise. Since our focus is on predicting future volatility using the type of
regressions discussed in the previous subsection, we focus on corrections for microstructure
noise of RVt. There are many ways to approach the problem of adjusting increments in
quadratic variation for microstructure noise. A kernel-based correction was ﬁrst introduced
by Zhou and further developed by Hansen and Lunde , Barndorﬀ-Nelsen,
Hansen, Lunde, and Shephard among others. Corrections based on sub-sampling
were introduced in Zhou , Zhang, Mykland, and A¨ıt-Sahalia and Zhang
 . Bandi and Russell and Bandi and Russell studied optimal sampling
in the presence of microstructure noise. Filtering, as an approach to microstructure noise
correction, was applied in Ebens , Andersen, Bollerslev, Diebold, and Ebens ,
Maheu and McCurdy and Bollen and Inder . Except for the work of Bollen and
Inder which uses the autoregressive ﬁlter, all other studies have used the moving average
ﬁlter. We will primarily use the corrections suggested in Hansen and Lunde , who
present a comprehensive study of the recent developments.
Methods and data
To compare the performance of the diﬀerent volatility measures, we use two adjusted
(RV 5min, RV 30min) and two unadjusted (RV 5min
AC1 , RV 1tick
ACNW30) volatility measures.
subscripts 5min and 30min denote the sampling frequency of the returns used in the
construction of realized volatility. By deﬁnition, all returns used in these estimators are
equally spaced. Under the assumption that the microstructure noise is iid, RV 5min
by Zhou provides a consistent estimator of the daily variance. Adopting the Hansen
and Lunde modiﬁcation of the above estimator, we deﬁne (dropping the day t
subscript for notational convenience):
ri,yri−1,y
where m is the number of 5-minutes returns per day (for DJIA stocks this number is 79).
9For additional references see O’Hara , Hasbrouck .
Instead of calendar-time (equally spaced time intervals), the RV 1tick
ACNW30 estimator uses
transactions-based data which is also referred to as tick-time. Hence, the 1tick estimator
makes use of all the available high-frequency data. The subscript ACNW, reﬂects the fact
that it estimator uses Newey-West kernel. Hansen and Lunde deﬁne the 1tick estimator as
ACNWk = ˜γ1tick
ri,1tickri−1,1tick
where N is the number of observations available for the current day; N/(N −j) is a scaling
factor introduced to compensate for the ”missing” autocovariance terms.
To assess the forecasting performance, we follow the recent work of Ghysels, Santa-Clara,
and Valkanov who use MIDAS regressions to predict realized volatility at weekly,
two-weeks, three-weeks, and monthly horizons. In the context of forecasting the increments
in quadratic variation, denoted RV x
y,(t+H,t) for horizon H with x and y taking the values
above - for example x = 5min and y = AC1 for the Zhou corrected RV estimates. For the
various measures we consider the following regressors:
y,(t+H,t) = µQ
H(k, θ)RV x
y,(t−k) + εQ
Hence, we compare how correcting for microstructure noise improves the forecasts of future
corrected increments and consider H equal to one week. Note that we consider uncorrected
measures of quadratic variation on both sides of equation (27). We use a Beta polynomial
which is particularly suitable for the application at hand.
The AA (Alcoa Inc) and MSFT (Microsoft) stocks are used as empirical examples. Figure
4 displays the daily volatility dynamics using the RV 5min
volatility measures for the sample
considered by Hansen and Lunde . The summary statistics for these two stocks are
in Table 3. The time series and summary statistics clearly demonstrate that the volatility
dynamics of the ﬁrst part of the sample is quite diﬀerent from the dynamics of the second
one. There is evidence of a structural change or regime switch, and this leads us to study
not only the entire sample but also two subsamples, respectively three and two years long.
For example, the sample mean of the daily series for the ﬁrst three years of the AC1-corrected
AA stock (trades returns) is 5.98 whereas for the last two years is 2.54. For the MSFT stock
the corresponding numbers are 6.15 and 1.47.
Our analysis covers two sample sizes and two measures of stock returns for every stock. We
start with the entire sample, i.e. from January 3, 2000 – December 31, 2004. The returns are
computed using mid-quotes prices and trading prices. The results covering both deﬁnitions
of returns and covering both samples appear in Tables 4 and 5 where each row corresponds
to the same left hand side variable discussed above but with diﬀerent explanatory variables
and sample sizes.
The empirical results pertaining to equation (27) a one week prediction horizon appear in
Table 4 (for the AA) and in Table 5 (for MSFT).
For the AA stock, the main ﬁnding is that the unadjusted RV 5min measure has the best
explanatory power across all models and samples. The diﬀerence between the best and the
worst (RV 30min) predictors changes from 8.6% to 15.5% depending on the sample, returns,
construction method, and LHS variable. In addition, the RV 5min
and the RV 1tick
ACNW30 have
approximately the same explanatory power despite the fact that the former is corrected only
for independent noise, whereas the latter allows for noise dependence.
The results for MSFT stock (Table 5) are similar. The unadjusted RV 5min measure has the
best explanatory power across all models and samples except for the whole sample where the
model with RV 5min
does marginally better (the diﬀerence only being 1.1%). The diﬀerence
between the worst and the best forecast varies from .5% to 8% which is much smaller than
the respective diﬀerence for the AA stock. For the 2000 – 2002 subsample RV 30min is the
worst estimator. However, this is not true for the whole MSFT sample.
Therefore, for these two stocks, we ﬁnd that the noise-corrected volatility measures perform,
on average, worse than unadjusted ﬁve minutes volatility measures. We can speculate that
the noise for the ﬁve minutes data is negligible compared to the signal, and the gains from
the adjustment are lower than the costs (in terms of the MSE). Another explanation is that
the MIDAS regression is more eﬃcient in extracting the signal from the unadjusted daily
realized volatility measures compared to the noise-corrected schemes.
To conclude, we turn again to Figures 4 and 5. The lower panel show respectively the RV 5min
and RV 1tick
ACNW30 regression polynomial estimates with the various past realized volatility
measures. We note that the estimated polynomial weights do not diﬀer much across the
various regressors. This ﬁnding is in line with Ghysels, Santa-Clara, and Valkanov .
The implication is that the diﬀerences in prediction performance comes entirely from the
choice of regressors, not the weighting schemes.
Conclusions
MIDAS regression models were recently introduced Ghysels, Santa-Clara, and Valkanov
 , , . This paper complements the current MIDAS literature by oﬀerings
some new theoretical and empirical results. To explicitly demonstrate the need for mixeddata sampling regressions, we show that the MIDAS results can be obtained with the
usual same-frequency time series regressions only in very speciﬁc cases. For more general
models, the MIDAS regressions clearly dominate. We also introduce several new MIDAS
speciﬁcations that include more general mixed-data structures, non-linearities, unequally
spaced observations, and multiple equations. Some of these speciﬁcations are straightforward
to estimate, other present particular challenges.
One particularly attractive approach is
called MIDAS with stepfunctions. Although it sometimes deﬁes the idea of parsimony, it
has the advantage of only involving linear regression models.
While we discuss a large variety of issues, there are clearly some areas that remain unresolved.
These areas pertain to multivariate and tick-by-tick applications, as well as long memory,
seasonality and other common time series themes like (fractional) co-integration.
Appendix: Reverse Engineering MIDAS Regressions –
A Two-Factor Model Example
We consider a two factor GARCH model, namely:
with the components as follows:
= ω(m) + ρ1(m)h(m)
1t−1/m + α1(m)µ(m)
= ρ2(m)h(m)
2t−1/m + α2(m)µ(m)
where µ(m)
and returns are written as:
= a(m) + ϵ(m)
where a(m) is the conditional mean, ϵ(m)
is i.i.d. (0, 1) while h(m)
]2. The component GARCH model implies a restricted GARCH(2,2) representation for
(the observable process) h(m)
speciﬁed in 14. Using this representation we can compute the
following:
t+1/m|Ih(m)
(1 −ρ2(m))ω(m)(1 −(ρ1(m) + ρ2(m)) −ρ1(m)ρ2(m)) + (ρ1(m) + ρ2(m))h(m)
+ρ1(m)ρ2(m)h(m)
t−1/m −(ρ1(m) + ρ2(m) −α1(m) −α2(m))µt
+(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m))µt−1/m
t+2/m|Ih(m)
(1 −ρ2(m))ω(m)(1 −(ρ1(m) + ρ2(m))2 −ρ1(m)ρ2(m) −(ρ1(m) + ρ2(m))×
ρ1(m)ρ2(m)) + ((ρ1(m) + ρ2(m))2 + ρ1(m)ρ2(m))h(m)
+ (ρ1(m) + ρ2(m))×
ρ1(m)ρ2(m)h(m)
t−1/m −((ρ1(m) + ρ2(m))(ρ1(m) + ρ2(m) −α1(m) −α2(m))
+(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m)))µt
+ρ1(m)ρ2(m)(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m))µt−1/m
t+3/m|Ih(m)
(1 −ρ2(m))ω(m)(1 −(ρ1(m) + ρ2(m))3 −2(ρ1(m) + ρ2(m))ρ1(m)ρ2(m)
−(ρ1(m) + ρ2(m))2ρ1(m)ρ2(m) −(ρ1(m)ρ2(m))2) + ((ρ1(m) + ρ2(m))3
+2(ρ1(m) + ρ2(m))ρ1(m)ρ2(m))h(m)
+ ((ρ1(m) + ρ2(m))2ρ1(m)ρ2(m)
+(ρ1(m)ρ2(m))2)h(m)
t−1/m −((ρ1(m) + ρ2(m))2(ρ1(m) + ρ2(m) −α1(m) −α2(m))
−ρ1(m)ρ2(m)(ρ1(m) + ρ2(m) −α1(m) −α2(m))
+(ρ1(m) + ρ2(m))(ρ1(m)ρ2(m) −ρ1(m)α2(m)
−ρ2(m)α1(m)))µt + ((ρ1(m) + ρ2(m))2(ρ1(m)ρ2(m)
−ρ1(m)α2(m) −ρ2(m)α1(m)) + ρ1(m)ρ2(m)×
(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m)))µt−1/m
t+4/m|Ih(m)
(1 −ρ2(m))ω(m)(1 −(ρ1(m) + ρ2(m))4 −3(ρ1(m) + ρ2(m))2ρ1(m)ρ2(m)
−(ρ1(m)ρ2(m))2 −(ρ1(m) + ρ2(m))3ρ1(m)ρ2(m) −2(ρ1(m) + ρ2(m))(ρ1(m)ρ2(m))2)
+((ρ1(m) + ρ2(m))4 + 3(ρ1(m) + ρ2(m))2ρ1(m)ρ2(m) + (ρ1(m)ρ2(m))2)h(m)
+((ρ1(m) + ρ2(m))3ρ1(m)ρ2(m) + 2(ρ1(m) + ρ2(m))(ρ1(m)ρ2(m))2)h(m)
−((ρ1(m) + ρ2(m))3(ρ1(m) + ρ2(m) −α1(m) −α2(m))
+(ρ1(m) + ρ2(m))2(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m))
−2(ρ1(m) + ρ2(m))ρ1(m)ρ2(m)(ρ1(m) + ρ2(m) −α1(m)
−α2(m)) + ρ1(m)ρ2(m)(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m)))µt
+((ρ1(m) + ρ2(m))3 + 2(ρ1(m) + ρ2(m))ρ1(m)ρ2(m)(ρ1(m)ρ2(m)
−ρ1(m)α2(m) −ρ2(m)α1(m)))µt−1/m
Then the MIDAS projection equation has the following expression:
((ρ1(m) + ρ2(m)) + (ρ1(m) + ρ2(m))2 + ρ1(m)ρ2(m) + (ρ1(m)+
ρ2(m))3 + 2(ρ1(m) + ρ2(m))ρ1(m)ρ2(m)
+(ρ1(m) + ρ2(m))4 + 3(ρ1(m) + ρ2(m))2ρ1(m)ρ2(m) + (ρ1(m)ρ2(m))2)
+(ρ1(m)ρ2(m) + (ρ1(m) + ρ2(m))ρ1(m)ρ2(m) + (ρ1(m) + ρ2(m))2ρ1(m)ρ2(m)
+(ρ1(m)ρ2(m))2 + (ρ1(m) + ρ2(m))3ρ1(m)ρ2(m)
+2(ρ1(m) + ρ2(m))(ρ1(m)ρ2(m))2)L1/m
+((ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m)) −(ρ1(m) + ρ2(m))×
(ρ1(m) + ρ2(m) −α1(m) −α2(m)) −(ρ1(m) + ρ2(m) −α1(m) −α2(m))
−(ρ1(m) + ρ2(m))2(ρ1(m) + ρ2(m) −α1(m) −α2(m))
−ρ1(m)ρ2(m)(ρ1(m) + ρ2(m) −α1(m) −α2(m))
+(ρ1(m) + ρ2(m))(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m))
+(ρ1(m) −ρ2(m))3(ρ1(m) + ρ2(m) −α1(m) −α2(m)) + (ρ1(m)
+ρ2(m))2(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m))
−2(ρ1(m) + ρ2(m))ρ1(m)ρ2(m)(ρ1(m) + ρ2(m) −α1(m) −α2(m))
+ρ1(m)ρ2(m)(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m)))×
(1 −(ρ1(m) + ρ2(m))L1/m + ρ1(m)ρ2(m)L2/m)/
(1 −(ρ1(m) + ρ2(m) −α1(m) −α2(m))L1/m + (ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m))L2/m)
+((ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m)) + ρ1(m)ρ2(m)×
(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m))+
(ρ1(m) + ρ2(m))2(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m)) + ρ1(m)ρ2(m)(ρ1(m)ρ2(m)
−ρ1(m)α2(m) −ρ2(m)α1(m)) + (ρ1(m) + ρ2(m))3
+2(ρ1(m) + ρ2(m))ρ1(m)ρ2(m)(ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m)))×
(1 −(ρ1(m) + ρ2(m))L1/m + ρ1(m)ρ2(m)L2/m)L1/m/
(1 −(ρ1(m) + ρ2(m) −α1(m) −α2(m))L1/m + (ρ1(m)ρ2(m) −ρ1(m)α2(m) −ρ2(m)α1(m))L2/m)