User Model User-Adap Inter 22:441–504
DOI 10.1007/s11257-011-9118-4
ORIGINAL PAPER
Explaining the user experience of recommender systems
Bart P. Knijnenburg · Martijn C. Willemsen ·
Zeno Gantner · Hakan Soncu · Chris Newell
Received: 30 November 2010 / Accepted in revised form: 30 August 2011 /
Published online: 10 March 2012
© The Author(s) 2012. This article is published with open access at Springerlink.com
Research on recommender systems typically focuses on the accuracy of
prediction algorithms. Because accuracy only partially constitutes the user experience
of a recommender system, this paper proposes a framework that takes a user-centric
approach to recommender system evaluation. The framework links objective system
aspects to objective user behavior through a series of perceptual and evaluative constructs (called subjective system aspects and experience, respectively). Furthermore,
it incorporates the inﬂuence of personal and situational characteristics on the user
experience. This paper reviews how current literature maps to the framework and
identiﬁes several gaps in existing work. Consequently, the framework is validated
B. P. Knijnenburg (B)
Department of Informatics, Donald Bren School of Information and Computer Sciences,
University of California, Irvine, CA 92697, USA
e-mail: 
B. P. Knijnenburg · M. C. Willemsen
Human-Technology Interaction Group, School of Innovation Sciences, Eindhoven University
of Technology (TU/e), P.O. Box 513, 5600 MB, Eindhoven, The Netherlands
e-mail: 
Z. Gantner
Information Systems and Machine Learning Lab (ISMLL), University of Hildesheim,
Marienburger Platz 22, 31141 Hildesheim, Germany
e-mail: 
European Microsoft Innovation Center GmbH, Ritterstrasse 23, 52072 Aachen, Germany
e-mail: 
BBC Research & Development, Centre House, 56 Wood Lane, London, W12 7SB, UK
e-mail: 
B. P. Knijnenburg et al.
with four ﬁeld trials and two controlled experiments and analyzed using Structural
Equation Modeling. The results of these studies show that subjective system aspects
and experience variables are invaluable in explaining why and how the user experience of recommender systems comes about. In all studies we observe that perceptions
of recommendation quality and/or variety are important mediators in predicting the
effects of objective system aspects on the three components of user experience: process (e.g. perceived effort, difﬁculty), system (e.g. perceived system effectiveness) and
outcome (e.g. choice satisfaction). Furthermore, we ﬁnd that these subjective aspects
have strong and sometimes interesting behavioral correlates (e.g. reduced browsing
indicates higher system effectiveness). They also show several tradeoffs between system aspects and personal and situational characteristics (e.g. the amount of preference
feedback users provide is a tradeoff between perceived system usefulness and privacy
concerns). These results, as well as the validated framework itself, provide a platform
for future research on the user-centric evaluation of recommender systems.
Recommender systems · Decision support systems · User experience ·
User-centric evaluation · Decision-making · Human-computer interaction ·
User testing · Preference elicitation · Privacy
1 Introduction
Recommender systems are designed to help the user make better choices from large
content catalogs, containing items as distinct as books, movies, laptops, cameras,
jokes, and insurance policies .
Before the advent of recommender systems, such content-based systems would offer
users the entire catalog (possibly with a generic search/ﬁlter feature). Recommender
systems, on the other hand, offer each user a personalized subset of items, tailored
to the user’s preferences. The system derives these user preferences from implicit or
explicit feedback . Implicit feedback recommenders analyze
clicking/purchasing behavior . Explicit
feedback recommenders let users rate items, , critique items , assign weights to item attributes , or
indicate their speciﬁc needs .
Finally, the system calculates recommendations by comparing the user’s preferences
to the features of the catalog items (content-based recommender systems), or to other
users’ preferences (collaborative ﬁltering recommenders).
A typical interaction proceeds as follows: First, the user’s preferences are elicited.
Based on the collected preference data, the system tries to predict how much the user
would appreciate each of the available items in the catalog. Finally, the system presents
the user those items that have the highest predicted value to the user. In some recommender systems this terminates the interaction, in other systems the users continue to
indicate their preferences and receive recommendations continually.
An essential aspect of any recommender system is the algorithm that provides personalized recommendations based on the user’s preferences . The more
Experience of recommender systems
accurate the predictions of this algorithm, the more accurately the system can predict the best recommendations for the user. Not surprisingly, a significant part of the
research on recommender systems concerns creating and evaluating better prediction
algorithms . An excellent
overview of available algorithms can be found in Burke and in Adomavicius and
Tuzhilin ; more recent approaches were presented in Koren et al. , Koren
 and Hu et al. . Herlocker et al. provide a thorough discussion of
available evaluation metrics.
The premise of this algorithm research is that better algorithms lead to perceivably better recommendations, which in turn lead to better user experience in terms
of choice satisfaction and perceived system effectiveness. However, several researchers have argued that there are other factors that inﬂuence the user experience (users’
subjective evaluation of their interaction with the system), and that these factors have
not received the amount of attention they deserve . System aspects other than accuracy can inﬂuence satisfaction
and other evaluative measures . Furthermore, situational or personal aspects can also inﬂuence how people interact with and evaluate the system. Unfortunately, even studies that consider aspects other than accuracy look at a limited
set of variables that inﬂuence each other (e.g., how satisfaction changes due to a
diversiﬁcation, or how choices become more accurate with the inclusion of a recommender engine) without integrating these variables into a model of overall user
experience.
An integrated view on the user experience of recommender systems can be
obtained by means of user-centric development and evaluation
 . The current paper therefore extends and tests
our user-centric evaluation framework for recommender systems proposed in Knijnenburg et al. . To understand and improve the user experience of recommender systems, it is necessary to conduct empirical evaluations that consider the
entire process of how the user experience comes about. Therefore, our framework
describes how objective aspects of the system (e.g. the algorithms used) are subjectively perceived by the user (e.g. if they perceive differences in recommendation
quality for these different algorithms), and how these perceptions, together with personal and situational characteristics, result in speciﬁc user experience and interaction with the system (e.g. whether a higher perceived recommendation quality leads
to a more positive evaluation of the system, a higher satisfaction with the chosen
items, and a change in user behavior). Such a framework will provide a deeper understanding of how objective system aspects inﬂuence the user experience and behavior
through perceived system aspects. It thereby allows for a better understanding of why
and how certain aspects of the system result in a better user experience and others
do not, which helps further user-centric research and development of recommender
B. P. Knijnenburg et al.
2 Components of the framework
The main goal of our framework is to provide a set of structurally related concepts that
can be used in empirical studies to describe and measure the user experience of recommender systems. User experience is an ill-deﬁned concept, and lacks well-developed
assessment methods and metrics .
In our framework, we distinguish between objective system aspects, (e.g. algorithms,
user interface features), subjective system aspects (users’ perceptions of these objective system aspects), and user experience (users’ evaluations of their interaction with
the system) and interaction (users’ behaviors). We also consider the context of the
interaction in terms of personal and situational characteristics. Before we describe
the framework itself, we will discuss several theories that served as a basis for our
framework.
2.1 Existing theories
2.1.1 Normative and attitudinal models
At the core of many psychological models of human behavior is the Theory of Reasoned Action (TRA) by Fishbein and Ajzen . This theory claims that attitudinal
and normative factors inﬂuence behavioral intention, which in turn predicts actual
behavior. Davis et al. adopted the attitudinal part of this
theory in their Technology Acceptance Model (TAM). In the TAM, the attitude towards
using a technology is explained by the perceived usefulness and perceived ease of use
of the system. Venkatesh et al. created a similar theory called the Uniﬁed Theory of Acceptance and Use of Technology (UTAUT), based on the normative part of
TRA, showing how personal and situational characteristics can inﬂuence behavioral
intention. In the UTAUT, attitudinal concepts are entirely replaced by more experience-related evaluative concepts (performance expectancy, effort expectancy, social
inﬂuence, and facilitating conditions).
With respect to our framework, these theories make a distinction between behaviors (and behavioral intentions) and the attitudes that cause these behaviors. These
attitudes are in turn caused by experiential factors like perceived usefulness and ease
of use (TAM), and by personal and situational characteristics (UTAUT).
2.1.2 User experience models
Hassenzahl deﬁnes user experience (UX) as “a momentary, primarily evaluative feeling (good-bad) while interacting with a product or service. Good UX is the
consequence of fulﬁlling the human needs for autonomy, competence, stimulation
(self-oriented) through interacting with the product or service (i.e. hedonic quality).”
Hassenzahl’s model of user experience describes how certain objective aspects
of the system (e.g. its interaction and presentation style) are perceived in terms of
pragmatic attributes (i.e. does the system deliver high quality results in an effortless
Experience of recommender systems
way?) and hedonic attributes (i.e. does it stimulate, is it desirable?). These perceptions
in turn cause an experiential evaluation in terms of appeal, pleasure and satisfaction.
With respect to our framework, Hassenzahl’s model links objective system aspects
to evaluative experiential factors through subjective perceptions. The distinction
between perception and evaluation is subtle but important: Perception denotes whether
certain objective system aspects register with the user at all, while evaluation denotes
whether the perceived aspect has any personal relevance to the user. This may for
instance give us an insight in why users may perceive a change in recommendation
quality but at the same time do not show a change in experience or behavior.
Furthermore, whereas the TRA-related theories are restricted to pragmatic attributes, Hassenzahl also stresses the importance of hedonic attributes. Experimental
evidence shows that hedonic attributes like pleasure and ‘ﬂow’ indeed also determine the user
experience .
2.1.3 User experience models for recommender systems
Hayes et al. propose a framework for testing the user satisfaction of recommender algorithms in operational systems. Their approach is restricted to behavioral
measures of satisfaction, and their focus is primarily on the algorithm. Furthermore,
Hayes et al.’s work has limits because they advocate a setup in which several algorithms are tested at the same time for the same user, an approach which provides a
significant departure from the normal user experience of a recommender system which
generally employs only one algorithm at a time.
Zins and Bauernfeind constructed a model of the user experience of recommender systems based on a survey conducted among users of two travel recommenders
and a system for ﬁnding digital cameras. Their model shows how personal characteristics inﬂuence trust, ﬂow, and browsing behavior, and how these in turn inﬂuence
system satisfaction. A clear limitation of their model is that it does not explain how
objective system aspects may inﬂuence the user experience.
McNee et al. created an analytic model of Human-Recommender Interaction (HRI) for the development of recommender systems. The goals and tasks of
the users are analyzed and used to determine the appropriate recommender system
dialogue and ‘personality’. McNee et al.’s model clearly serves a different purpose
than our framework (development as opposed to evaluation). They do however suggest linking subjective HRI metrics to traditional objective performance metrics of
algorithm accuracy, and stress the importance of the context (e.g. users’ goals and
tasks) in which recommendations are made.
Xiao and Benbasat presented an extensive literature review of the marketingoriented research on recommender systems. Their overview, too, provides insight into
the mechanisms underlying the user experience of recommender systems, albeit from
many different studies (each focusing just on one part of the entire experience). Their
resulting framework shows how certain characteristics of recommender systems cause
changes in users’ evaluation and decision-making behaviors, and in their adoption of
the recommender system. It also includes personal and in situational characteristics
that moderate these effects. The framework we present below bears a lot of similarity
B. P. Knijnenburg et al.
to Xiao and Benbasat’s framework, but goes beyond it by including subjective
system aspects. Moreover, while their framework is constructed mainly for the purpose of summarizing existing research, we pose our framework as a starting-point for
the evaluation of recommender systems.
Pu and Chen provide an extensive questionnaire to test
several speciﬁc experience concepts of recommender systems. Their research model
also explicitly considers perceived system qualities as antecedents of user beliefs, attitudes and behavioral intentions, and in that way it is similar to our framework. Our
framework, however, takes a more abstract approach by providing a description of the
structural relationships between the general, higher level concepts that play a role in
user experience, without strictly specifying operational, lower level constructs and the
questions that measure them.1 To answer speciﬁc research questions, researchers need
to deﬁne and operationalize a set of speciﬁc, lower level constructs, and Pu and Chen’s
questionnaires can be used as a starting point for this operationalization; another option
is to use the pragmatic procedure for recommender system evaluation that is based on
our framework . However, since user experience is highly
contingent upon the purpose of the system under evaluation, the speciﬁc concepts and
speciﬁc questionnaire items to measure these concepts may differ from study to study.
Moreover, Pu and Chen’s model does not include context (personal and situational
characteristics), and it does not suggest how objective system aspects inﬂuence the
various constructs in their framework, making it more difﬁcult to select concepts from
their framework for a particular user study.
Ozok et al. provide a wide range of design guidelines based on a questionnaire of recommender system usability. Their results describe the effects of speciﬁc
system aspects on the usability of recommender systems. However, they employ a
descriptive approach, which relies on the users’ stated opinions about recommender
systems in general instead of experimental manipulations of a speciﬁc system.
2.2 The main structure of the framework explained
Figure 1 shows our framework. Like Hassenzahl and Xiao and Benbasat ,
we take objective system aspects (OSA) as a starting point for the evaluation. The
objective system aspects consist of the algorithms used by the system, the visual
and interaction design of the system, the way it presents the recommendations, and
additional features such as social networking. Like Hassenzahl , we link these
objective system aspects to subjective system aspects (SSA), which represent users’
perception of the objective system aspects. Also like in Hassenzahl model,
these subjective system aspects include both pragmatic characteristics (usability and
quality) and hedonic characteristics (appeal). The subjective system aspects, measured with questionnaires, are expected to mediate the inﬂuence of the objective system aspects on the user experience. The main reason for including subjective system
aspects as mediators is that recommender systems provide a personalized experience
1 In this respect, our framework resembles Fishbein and Ajzen TRA, which also instructs researchers
to elicit a different set of personal and normative beliefs to measure per experiment.
Experience of recommender systems
Fig. 1 Our framework for the user-centric evaluation of recommender systems
to the user, and that this personalization may not be equally apparent to all users. The
subjective system aspects can show whether the objective aspects are perceived at
Like all existing models, our framework carefully distinguishes between attitude
and behavior,2 although we use the more speciﬁc terms experience and interaction.
The experience (EXP) signiﬁes users’ evaluation of the system. In that way it is closely
related to attitudes in TAM (or rather the factors that cause them), with the addition
of hedonic aspects model). Experience is also measured
with questionnaires, and is conceptually divided into the evaluation of the system (system-EXP), the evaluation of the decision process (process-EXP), and the evaluation
of the ﬁnal decisions made (outcome-EXP). The interaction (INT) is the observable
behavior of the user. A complex interplay exists between interaction and experience:
a positive user experience changes the interaction, but the interaction is also what
initially caused the user experience.
Like the models of Xiao and Benbasat and Venkatesh et al. , our
model asserts that experience and interaction typically also depend on personal and
situational characteristics (referred to as PC and SC). Personal characteristics include
demographics, trust, domain knowledge, and perceived control (the latter two are
2 To emphasize this distinction we have slightly altered the labels in the framework since our previous
publications.
B. P. Knijnenburg et al.
prominent in TRA). Situational characteristics are dependent on the context of the
interaction; at different points in time, users may have different choice goals, trust and
privacy concerns, and familiarity with the system .
3 Expected beneﬁts of our framework
Our framework explicitly links the objective interaction (INT) to objective system
aspects (OSA) through a series of subjective constructs (SSA and EXP). The framework can be used as a guideline for controlled user experience tests. Specifically, by
manipulating a certain system aspect (OSA) in a controlled experiment (keeping all
other aspects the same), one can identify the effect of this aspect on the users’ perceptions (SSA), experience (EXP) and behaviors (INT). By careful manipulation of
speciﬁc objective system aspects, researchers can uncover generic truths about recommender systems. These can then inform the design and development of future versions
of the studied aspects.
Moreover, the framework allows one to conduct empirical evaluations in a more
integrative fashion than most existing recommender systems research: It allows
researchers to consider the interplay between multiple objective system aspects (e.g.
algorithm versus interface), as well as to investigate the trade-offs between several
aspects of user experience (e.g. satisfaction versus choice difﬁculty).
The framework provides insight into the relationships between the general concepts that play a role in the user experience of recommender systems. By tailoring the
operationalization of these general concepts to the speciﬁc system under evaluation,
the framework can be applied to a range of different types of consumer-facing recommender systems, including e-commerce recommenders (recommending products),
media recommenders (recommending, for example videos, music or news articles)
and social network recommenders (recommending users to befriend or follow). To
exemplify this point, in the remainder of this section we use our framework to map
a wide array of existing research. Most of this existing research (as well as our own
empirical work) specifically considers what in the realm of e-commerce recommenders has been called “experience products”, for which the quality is hard to determine
before purchase, as opposed to “search products”, which have attributes that can be
considered before the decision is made . Because it is hard to determine
the quality of experience items before the actual decision is made, decision-making
processes for such items typically rely more heavily on recommendations and other
external sources of information .
To conclude this section, we indicate speciﬁc gaps in current knowledge, and provideanoverviewoftheresearchopportunitiesthatthesegapspresent.Inthesubsequent
section, we present the results of several empirical evaluations that use parts of the
framework for their main hypotheses. After presenting the results of these evaluations
one by one, the ﬁndings will be integrated under the generic concepts of the framework. This will allow us to validate the framework, and to take a ﬁrst step towards
bridging the uncovered gaps. To effectively integrate our ﬁndings, we limited our
empirical evaluations to media recommenders.
Experience of recommender systems
3.1 The objects of user experience evaluation
User experience as deﬁned in our framework (EXP) is not a one-dimensional concept;
it may entail various aspects (broadly ranging from pragmatic to hedonic concepts) and
several different objects of evaluation. Especially for recommender systems, knowing
the object of evaluation is critical in understanding the dynamics of the user experience : When we say that the user
experience of recommender system X is better than that of system Y, are we evaluating
the system, the process of using the system to get to a decision, or the chosen item
itself? This distinction is important, as different system aspects may inﬂuence different
objects of user experience; a visually attractive interface may improve the evaluation
of the system (system-EXP), a good preference elicitation method may make decisions
easier (process-EXP), and an accurate algorithm may increase the quality of the ﬁnal
decision (outcome-EXP).
Furthermore, the evaluations of the different experience objects may inﬂuence each
other. For instance, a positive evaluation of the chosen item(s) may “rub off” on the
evaluation of the system. To capture the multi-faceted nature of user-experience, our
framework therefore considers each of these objects: the system, the process, and the
Incurrentresearch,however,thisisrarelydone;researchersindifferentdomainsuse
different objects of evaluation. Particularly, marketing and decision-making researchers mainly look at the outcome-EXP variables such as the quality of the choice, the
users’ conﬁdence in making the right choice, and the satisfaction with the chosen item
 . They rarely take the system or the choice process as
focal points of evaluation.
Human–computer interaction (HCI) researchers have traditionally been more comprehensive in the coverage of all objects of user experience of their research. However,
this ﬁeld has a tendency towards formative evaluations such as Think Aloud testing
and Heuristic Evaluation . The results of such evaluations
are limited in generalizability, and therefore not the focus of this paper. The available
summative evaluations in the HCI ﬁeld primarily report on system-EXP variables such
as system quality and user loyalty (also operationalized as the intention to return), on
process-EXP variables such as cognitive effort and competence in using the system,
and on outcome-EXP variables such as decision satisfaction .
According to our framework (Fig. 1), these user experience effects do not stand
alone, but are instead part of a larger chain of effects. Specifically, in the following
sections we will argue that the user experience (EXP) is caused by objective system
aspects (OSA, via SSA) and personal or situational characteristics (PC or SC). Moreover, the experience variables themselves may be structurally related to one another:
Bharati and Chaudhury , for instance, showed that the perceived quality of the
system (system-EXP) positively inﬂuences the decision satisfaction (outcome-EXP).
Although Bharati and Chaudhury investigate the objective system aspects in their study
B. P. Knijnenburg et al.
(by manipulating the recommender system under evaluation), they do not include the
objective system aspects in the analyzed chain of effects.
3.2 From accuracy to user experience
A large part of existing recommender systems research is focused on creating better
prediction algorithms, thereby implicitly assuming that better algorithms will lead
to a better user experience. Explicitly testing this assumption would require empirical evaluations with real users on real systems. Several researchers in marketing
and decision-making conducted such user-centric evaluations of their recommender
systems. For instance, they looked at the reduction in choice effort through a recommender system . However, they usually
compare a recommender system against a system without recommendation features
(or a recommender system against no system at all), rather than looking at the often
subtle differences between algorithms. The results of such unbalanced comparisons,
in which the “personalized” condition clearly has an advantage over the non-personalized condition, are usually unsurprising . However, Chin
 argues that this advantage is not always apparent and that a comparison with
a non-personalized system may very well be justiﬁed. Some researchers compare a
recommender system against human recommenders , but these studies provide little insight into the effect of algorithm accuracy
on the user experience; for that, several algorithms should be pitted against each other.
Surprisingly few studies compare algorithms in live experiments with real users.
Researchers who do compare the user experience effects of several algorithms ﬁnd surprising results. In a comparison of six recommender algorithms, McNee et al. 
found that although the “Item-Item CF” algorithm provided the best predictions, users
rated it the least helpful. Torres et al. found that although the “CBF-separated
CF” approach had the lowest predictive accuracy among ﬁve algorithms, this approach
resulted in the highest user satisfaction. In other words, the presumed link between
algorithm accuracy (an OSA) and user experience (EXP) is all but evident. Our framework allows researchers of recommender systems to take a step beyond algorithmic
accuracy (OSA) towards its effects on user experience (EXP).
3.3 Subjective system aspects as mediators
The presented framework indicates that the apparent missing link between algorithm
accuracy and user experience can be found in mediation through perception. The link
between algorithm accuracy (an OSA) and user experience (EXP) is often weak , and can then only be established by including the mediation through the users’
perception of the algorithm accuracy (an SSA). In other words, the framework hypothesizes that users can perceive algorithm accuracy, and that this perception inﬂuences
the experience (OSA →SSA →EXP).
In light of these hypotheses, existing research has established that users are, in
several instances, able to observe objective differences in recommendation quality . It is however not clear how these (typically subtle) differences in perceived recommendation quality affect the user experience (SSA →EXP), because
few researchers have tested the effect of their algorithms on the users’ perception,
behavior and experience.
This gap in existing research (i.e. not measuring the SSA as a mediator between
OSA and EXP) makes it hard to explain why in some experiments better algorithms
do not lead to a better experience. One possible reason might be that users were not
able to notice the quality differences (the OSA does not affect the SSA), e.g., the
quality differences between two algorithms may have been too small to notice, and
thus would not inﬂuence the user experience. Another possible explanation (which is
not mutually exclusive) might be that users may have observed the quality differences,
but may just not have been inﬂuenced by these differences in their experience (no link
between SSA and EXP), e.g., they may actually like to see good recommendations
accompanied with some bad ones, as it makes their ﬁnal decision easier to justify.
Finally, an effect of accuracy on experience may exist, but just be overshadowed by
individual differences of perception (this is not unlikely for recommender systems, as
their effect is not equally pronounced for each user). In such case one should measure
whether the user actually noticed the quality difference or not (SSA is needed as a
mediator between OSA and EXP).
The inclusion of SSAs may thus increase the robustness of the effects of the OSAs
on EXP. Moreover, SSAs provide a more thorough understanding of how and why
certain features of a recommender system affect the user experience. This does not
only hold for algorithm accuracy, but for any manipulated objective system aspects.
Chen and Pu have created a chain of effects from objective algorithm accuracy
(OSA) to user-perceived algorithm accuracy (SSA), from objective user effort (OSA)
to user-perceived effort (SSA), and from the perceived accuracy and effort (SSA) to
intention to purchase and intention to return (INT). They ﬁnd significant differences
between their two tested interfaces for each of these constructs. This path analysis is
an important step towards an integrated analysis of recommender system experience,
but in our approach we extend it in two directions: First of all, we include the manipulations that cause the objective differences in the model. In the Chen and Pu study
they cause significant differences in each construct individually, but an inclusion of
the manipulation as a dummy variable into the path model would allow for a mediation
analysis of the experimental effects. Secondly, we add constructs explicitly asking the
users about their experience (EXP).
3.4 Triangulation of (logged) behavioral data
Although user experience (EXP) is mainly a subjective phenomenon, its effects will
likely be reﬂected in the users’ observable behavior (INT). This idea is a fundamental
property of all theories based on Fishbein and Ajzen’s Theory of Reasoned
Action, although we do not take the direction of the effect to be merely one-way.
Whereas attitude causes behavior in TRA, our focus on experience (which is a much
more interactive concept than attitude) also considers the inverse effect. For example:
B. P. Knijnenburg et al.
users who are more satisﬁed may increase their usage of the system (EXP →INT),
while at the same time increased usage may cause an increase in satisfaction (INT →
Researchers in the ﬁeld of algorithm accuracy predominantly use behavioral data
for their evaluations: they use logged clicks (either item selections or ratings) to train
and test their algorithms . Researchers in marketing and decision-making also analyze behavioral data, but focus more on decision time, switching
behavior after the choice, or total consumption volume . A common problem with behavioral data, however, is that they are not always good indicators of users’ subjective
experience. For instance, Pu and Chen found that the actual time users spent
looking at items in their system did not correlate with users’ subjective perceptions,
And Spiekermann et al. found that stated willingness to provide feedback did
not correlate with actual feedback behavior.
Another problem with behavioral data is that their interpretation is often troublesome . For instance, if users stay on a video clip recommendation site for a longer time, does this mean that the efﬁciency of the system is low
(it takes longer for users to ﬁnd what they want), or that the users enjoy the site more
(to the point that they stay longer to watch more clips)? To solve this dilemma, Van
Velsen et al. suggests to “triangulate” the objective behavioral data (INT) with
the subjective experience data (EXP) gathered through other methods (e.g. questionnaires).
From a commercial perspective, inﬂuencing the users’ objective behavior may seem
to be the primary objective of recommender systems research, such as getting the user
to buy more products (in e-commerce recommenders) or watch more advertisements
(in media recommenders). However, experience concepts reﬂect and inﬂuence users’
attitudes towards a system, and research shows that positive attitudes are related to
increased adoption rates . To get an indication of the longer-term effects of the system, behavioral data
should thus be complemented with subjective experience measurements. In our framework, behavioral data is therefore correlated (triangulated) with subjectively measured
experience concepts.
3.5 Personal and situational characteristics in context
The user experience cannot be entirely attributed to the recommender system itself, it
may also depend on characteristics of the user (Personal Characteristics, or PC) and
the situation in which the user is using the system (Situational Characteristics, or SC)
 . These factors are typically beyond the inﬂuence of the recommender
system, but do inﬂuence the user experience.
Domain knowledge (or ‘expertise’) is an important PC variable in this respect:
Kamis and Davern show that users with a higher level of domain knowledge
perceive recommender systems as less useful and harder to use than novices. In a
music recommender experiment, Hu and Pu show that expert users perceive
Experience of recommender systems
recommendations as less accurate, and the system as less helpful. They also state that
they would use the system less. Overall, users with a moderate level of expertise rate
the system as most effective.
Users’ trust in the system may also inﬂuence their experience, and vice versa.
Komiak and Benbasat show that good recommendations can increase trust in
both the competence and the integrity of a recommender system, and that a higher
level of trust eventually leads to an increased intention to adopt the system. Wang and
Benbasat show that trust in recommender systems is furthermore caused by
disposition (the user’s initial level of trust), calculation (the user’s estimation of the
costs and beneﬁts for the system to be trustworthy), interaction (the user’s expectations
about the system, control over the system, and validation of the system results) and
knowledge (an inference based on what the user knows about the system). This makes
trust both a PC (depending on the user’s personality) and an SC variable (depending
on the user, the system and the situation).
Very few studies have investigated which personal and situational characteristics
exactly motivate and inhibit users to provide preference feedback to the system . This is an important issue, as many
recommender systems rely on explicit feedback (e.g. users’ ratings) to give good
recommendations. Privacy concerns may reduce users’ tendency to disclose personal
information . On the other hand, if it positively inﬂuences their user
experience (i.e. in terms of better recommendations), users may be more willing to
provide feedback . This makes it hard to evaluate the
impact of these characteristics on the user experience relative to other possible factors
that inﬂuence the user experience (e.g. is a certain PC →EXP more substantive than
a certain SSA →EXP?), and to prove the effectiveness of possible remedies for negative inﬂuences . Below we provide a brief overview of user interface
aspects (OSAs) inﬂuencing the user experience (EXP) of recommender systems.
B. P. Knijnenburg et al.
3.6.1 Preference elicitation method
The preference elicitation method is the way in which the recommender system discovers what the user likes and dislikes. In content-based recommender systems, users
may indicate their preference by assigning weights to attributes , prioritizing user needs or critiquing examples . The research on these different preference elicitation methods shows that they have a substantive impact on the user experience . Moreover, the optimal preference elicitation method may depend on
user characteristics such as domain knowledge .
In collaborative ﬁltering recommender systems, the two most common preference
elicitation methods are explicit and implicit elicitation. In explicit elicitation, users rate
theitemswith,forexample,onetoﬁvestars .
In implicit elicitation, preferences are derived from an analysis of the browsing and
selection behavior of users. Research shows that a combination of explicit and implicit
elicitation results in a higher recommendation accuracy , but no
research has investigated differences in user experience and behavior between explicit
and implicit elicitation.3 Our framework provides the opportunity to investigate the
effects of preference elicitation beyond accuracy.
3.6.2 Size and composition of recommendation sets
Most recommender systems provide an ordered list of recommendations. Whereas a
substantial amount of research considers the individual qualities of these recommendations, little research has considered the composition of the list . The composition may
play an important role in the user experience of a recommender system, because it
inﬂuences the users’ decision-making process through context effects .
It is also unclear how many recommendations the system should provide. In
conventional choice situations, too few items may restrict the users’ freedom of
choice, whereas too many items may lead to choice overload . In the
context of recommender systems, where all recommended items are highly relevant,
this choice overload effect may be even more prominent. When embedded in a user
interface, a longer list of recommendations may enjoy the added beneﬁt of attracting
more attention .
3 Algorithms are often designed to handle a speciﬁc type of data (e.g. binary, ﬁve star rating) and are therefore restricted to a speciﬁc preference elicitation method. In practice, they are therefore often treated as one
and the same thing. However, to get a more nuanced understanding of the speciﬁc effects of algorithms and
preference elicitation methods, we make an explicit distinction between these two system aspects.
Experience of recommender systems
The order in which recommendations are presented also seems to have an effect
on the users’ experience and interaction. The consequences of such serial positioning effects are however unclear: Lynch and Ariely ﬁnd that sorting by quality
reduces price sensitivity in an e-commerce recommender; Diehl et al. , however, ﬁnd that it increases price sensitivity. Tam and Ho ﬁnd that making one
recommendation stand out increases user attraction, elaboration, and choice likelihood
of that recommendation.
Hu and Pu show that putting a logical structure
on the list of recommendations (specifically, categorizing the recommendations to
reﬂect different trade-offs) leads to a higher perceived categorical diversity of the recommendations, a higher satisfaction and decision conﬁdence, and a higher intention
to reuse the system and purchase items with it.
Ziegler et al. found that, up to a certain point, sacriﬁcing (actual and perceived) individual recommendation quality in favor of recommendation set diversity
can lead to a more positive subjective evaluation of the recommendation set . This ﬁnding should be compared with other factors inﬂuencing the user experience to verify the robustness and extent of this effect.
For instance, Willemsen et al. ﬁnd that diversiﬁcation may reduce the choice
difﬁculty (SSA), which further improves the user experience (EXP). Our framework
provides a starting point for investigating the impact of the size and composition of
recommendation sets on the user experience.
Finally, one can also think about when to provide recommendations. Ho and Tam
 ﬁnd that users are most susceptible to be persuaded by recommendations in
the earlier stages of the decision process.
3.6.3 Explanations
Another part of the presentation of recommendations is the possibility to explain why
certain items are recommended. Herlocker et al. found that users like explanations in collaborative ﬁltering recommender systems. Studying knowledge-based
recommenders, Felfernig et al. and Cramer et al. show that explanations increase the users’ perception of the system’s competence (system-EXP) and
their trust in the quality of the recommendations (SSA). Tintarev and Masthoff 
show that users tend to like personalized explanations (i.e. explanations that highlight
facts related to their preferences), but that these may actually be less effective than
generic explanations.
3.7 Research opportunities
It is important to realize that our framework is not merely a classiﬁcation of the
important aspects of the user experience of recommender systems. Nor is it a prede-
ﬁned metric for standardized “performance” tests. Instead, it is a generic guideline
for in-depth empirical research on the user experience of recommender systems; it
conceptually deﬁnes a generic chain of effects that helps researchers to explain why
and how the user experience of recommender systems comes about. This explanation
B. P. Knijnenburg et al.
is the main value of the user-centric evaluation of recommender systems (McNee et al.
The remainder of this paper will describe the empirical evaluations of our own
recommender systems: a number of studies that together comprise a preliminary validation of parts of the evaluation framework. In these studies we have tried to repeatedly
test a limited set of core variables of our framework. Additionally, the reviewed literature reveals some gaps in existing research in terms of how well it covers the hypothesized relations in our framework. These gaps can be translated into requirements for
our studies; by covering them, our studies provide a more thorough validation of our
framework. Specifically, each empirical study will broadly adhere to a subset of the
following requirements:
3.7.1 Requirements regarding objective system aspects (manipulations)
Algorithm: Despite the predominant research interest in algorithms, there is an
apparent paucity of knowledge on how algorithm accuracy inﬂuences user experience. The main manipulation in most of our studies is the algorithm used for
providing recommendations.
Recommendation set composition: Another important manipulation is the composition of the set of recommendations presented to the user, as this aspect remains
largely untreated in existing research.
Preference input data: Algorithm and interface meet at the point of preference
elicitation. In content-based recommenders this topic has been researched extensively. In collaborative-ﬁltering recommenders, however, the topic remains largely
untreated, especially when it comes to explicit versus implicit preference elicitation. Several of our empirical evaluations therefore manipulate the type of input
(explicit or implicit) used for recommendation.
3.7.2 Requirements regarding subjective system aspects
Perceived aspects as mediators: Subjective system aspects such as perceived recommendation quality, accuracy and diversity are measured in our studies, as we
expect that these perceptions mediate the effect of the objective system aspects on
the user experience concepts.
3.7.3 Requirements regarding experience and interaction
5. User experience evaluation: System effectiveness (system-EXP), choice satisfaction (outcome-EXP) and usage effort and choice difﬁculty (process-EXP) will
measure the three objects of user experience evaluation where possible. Relations
between these EXP variables will be reported.
6. Providing feedback: Many recommender systems elicit the users’ preferences by
analyzing their preference feedback. We therefore extensively analyze the positive
and negative antecedents of the users’ intention to provide feedback, as well as
their actual feedback behavior.
Experience of recommender systems
7. Behavioral data: To link the attitudinal part of user experience to the users’
observable behavior, logging data will be triangulated with subjective concepts.
3.7.4 Requirements regarding personal and situational characteristics
PC and SC: Important and under-researched personal and situational characteristics such as domain knowledge and privacy concerns are included where possible
and useful.
4 Empirical evaluations: validation of the framework
4.1 Background
The framework proposed in this paper was originally developed as an analysis tool
for the MyMedia project , which is part of the European Commission 7th Framework Programme. The main goal of MyMedia was to improve the
state-of-the-art of multi-media recommender systems. A recommender system development framework, the MyMedia Software Framework ,4 was
created and deployed in real-world applications at four industrial partners. Each partner conducted a ﬁeld trial with their system, with several aims in mind: technical
feasibility, business opportunities, and user experience research. In the current paper
we discuss the results of four ﬁeld trials (FT1-FT4), two conducted using the MyMedia
version of ClipClub player, developed by the European Microsoft Innovation Center
(EMIC), and two conducted using a web-based TV catch-up service, developed by the
British Broadcasting Corporation (BBC). Each trial was designed and evaluated on
the basis of the proposed evaluation framework. We amended these trials by conducting two experiments (EX1 and EX2) with a comparatively more controlled but also
more artiﬁcial quality. The tight control over the users’ interaction with the system in
these experiments allowed us to consider in more detail the decision-making processes
underlying the user experience. The six studies are described in Table 1, and will be
discussed in more detail below.
The main goal of the studies is two-fold: To generate new knowledge that ﬁlls the
gaps in current research, and to validate the proposed evaluation framework. To assist
this latter goal, we repeatedly include the recommendation algorithm (an OSA), the
perceived recommendation quality (an SSA), and the system’s effectiveness (EXP) as
core components of our analysis. Furthermore, for each study we discuss the extent
to which its results ﬁt the framework.
We realize that the exclusive focus on collaborative ﬁltering recommender systems, all based on the same MyMedia development framework, and all with media
content, limits the scope of this validation. Despite this, we believe that the evaluation
framework itself has sufﬁciently generic qualities to apply to recommender systems
4 The recommender algorithms used in the studies described here are available in the MyMedia Software
Framework, which is available for non-commercial research purposes, 
com, as well as in the open source package MyMediaLite, 
B. P. Knijnenburg et al.
Table 1 Properties of the studies that were conducted based on the evaluation framework
FT1 Emic pre-trial
Adjusted Microsoft ClipClub
Continuously updated database of clips targeted at teenagers
Participants
43 EMIC colleagues and partners
Manipulations
Algorithms:
• Random recommendations
• Vector Space Model (VSM) algorithm based on explicit feedback
Main questions
Does a system that provides personalized recommendations provide a
better user experience than a system that provides random
recommendations?
What factors inﬂuence the users’ intention to provide feedback?
FT2 EMIC trial
Adjusted Microsoft Clipclub
Continuously updated database of clips targeted at teenagers
Participants
108 externally recruited “young” participants (targeted mean age of 25)
Manipulations
Algorithms:
• General most popular items
• Bayesian Personalized Ranking Matrix Factorization (BPR-MF)
algorithm based on implicit feedback
• VSM algorithm based on explicit feedback
• Users receive no speciﬁc information
• Users are told that their ratings are collected and that this data is used
to provide better recommendations
• Users are told that their behavior is monitored and that this data is used
to provide better recommendations
Main questions
What is the difference in subjective recommendation quality between the
different algorithms?
Does a system that provides personalized recommendations lead to a better
user experience than a system that recommends the “generally most
popular” items?
What is the difference between the implicit recommender and the explicit
recommender in terms of user experience?
What factors inﬂuence the users’ intention to provide feedback?
FT3 BBC pre-trial
BBC MyMedia player
BBC television programming (up to one week old)
Participants
59 externally recruited British participants, reﬂecting a balanced
representation of the UK television audience
Manipulations
For the rating trial, algorithms:
• General most popular items
• BPR-MF algorithm based on implicit feedback
• MF algorithm based on explicit feedback
For the rating trial, time:
• Day 1 … Day 9
For the experience trial, time:
Main questions
What is the difference in recommendation list quality between the different algorithms?
How does the quality of the recommendation lists generated by the
different algorithms evolve over time?
How does the user experience of the system evolve over time?
Experience of recommender systems
Table 1 continued
FT 4 BBC trial
BBC MyMedia player
BBC television programming (up to one week old)
Participants
58 externally recruited British participants, reﬂecting a balanced
representation of the UK television audience
Manipulations
Algorithms:
• General most popular items
• BPR-MF algorithm based on implicit feedback
• MF algorithm based on explicit feedback
Main questions
What is the difference in subjective recommendation quality between the
different algorithms?
Does a system that provides personalized recommendations lead to a better
user experience than a system that recommends the “generally most
popular” items?
What is the difference between the implicit recommender and the explicit
recommender in terms of user experience?
EX1 Choice overload experiment
Adjusted BBC MyMedia player
Movies (MovieLens 1M dataset)
Participants
174 participants invited from a panel with students or recently graduated
students from several Dutch universities
Manipulations
Recommendation set quality and size:
• Top-5 (5 best recommendations)
• Top-20 (20 best recommendations)
• Lin-20 (5 best recommendations, recommendation ranked 99, 199, … 1499)
Main questions
How do the objective quality and size of the recommendation set inﬂuence
the subjective recommendation set quality and diversity?
How do the objective quality and size of the recommendation set inﬂuence
choice difﬁculty and choice satisfaction?
EX2 Diversiﬁcation experiment
Adjusted BBC MyMedia player
Movies (MovieLens 1M dataset)
Participants
137 Amazon Turk workers
Manipulations
Algorithms:
• General most popular items
• MF algorithm based on explicit feedback
• K-Nearest Neighbor (kNN) algorithm based on explicit feedback
Recommendation set diversiﬁcation:
• No diversiﬁcation
• Some diversiﬁcation
• Lots of diversiﬁcation
Main questions
What is the difference in subjective recommendation quality between the
different algorithms?
Does a system that provides personalized recommendations lead to a better
user experience than a system that recommends the “generally most
popular” items?
What is the difference between the kNN recommender and the MF
recommender in terms of user experience?
Do users notice our manipulation of the variety of the recommendations?
Do users like (objective or subjective) variety in their recommendation
sets? If so, does the effect overshadow the effect of algorithm accuracy?
B. P. Knijnenburg et al.
in general. The framework is based on a broad range of existing research, and does
not assume speciﬁc operationalizations of the measured concepts. Furthermore, the
manipulations and main questions in the conducted studies are rather generic, and
hence the range of validity of our conclusions can be broadened from multi-media
recommender systems to recommender systems in general.
4.2 Empirical validation techniques
User experience research can be conducted both qualitatively and quantitatively
 . Qualitative research is more exploratory, but is usually less generalizable, and cannot be statistically validated. Quantitative analysis allows for statistical validation, but one has to have clear hypotheses
about theoretical constructs and their relations before conducting the study. In many
cases it is advisable to apply these techniques in tandem, but for the purpose of this
paper we will restrict our analysis to quantitative results (hypotheses for our studies
are conveniently provided by the framework). In order to prevent conﬁrmation bias in
our validation of the framework, we extensively use data analysis methods like exploratory factor analysis (EFA) and structural equation modeling (SEM), which allow for a
more exploratory analysis of quantitative data. Below we discuss the general procedure
of our research; Appendix A provides a more in-depth description. We recommend
researchers who want to use our framework to either follow a similar procedure, or to
opt for our pragmatic version described elsewhere .
4.2.1 Measurement
Experience, SSAs, PCs and SCs can be measured using questionnaires. To assure a
more robust measurement of all concepts, we typically use a minimum of seven statements (both positively and negatively phrased) that can be answered on a balanced
5- or 7-point scale (from “completely disagree” to “completely agree”) for each unidimensional concept. Exploratory factor analyses can be conducted to test the robustness
of the concepts, and to exclude any questions that do not contribute to the measurement
of the intended concepts.
4.2.2 Manipulation
Objective system aspects (OSA) can be manipulated, i.e., several versions of the
aspect (conditions) can be created, and assigned randomly to each participant. By
keeping everything else constant between conditions, one can single out the effect of
this manipulated aspect on the user experience (EXP). In our research, one condition
always serves as a baseline against which all other conditions are compared.5 Furthermore, by manipulating several system aspects independently, one can compare the
relative impact of these aspects on the user experience (e.g. “Does the effect of the
user interface overshadow the impact of different algorithms?”).
5 Similar to the use of dummy variables in standard linear regression.
Experience of recommender systems
4.2.3 Structure
The incorporation of user perceptions (SSA) increases the robustness and explanatory power of the evaluations, and different objects of user experience (system-EXP,
process-EXP, outcome-EXP) can be treated in parallel to gain further insight in the
workings of user experience. Logged behavioral data can be triangulated with the user
experience concepts to link the subjective experience (EXP) to objective user behavior
(INT). This creates a causal chain of effects from manipulated OSAs, via subjectively
measured SSAs and EXPs, to objectively measured INTs. Structural equation modeling can be used to conduct a mediation analysis on these effects.
From a statistical perspective, SEM concurrently tests the robustness of the measured
constructs and the relationships between them.
4.2.4 Graphical presentation of SEMs
We graphically present our structural equation models as diagrams containing the
constructs (boxes) and the relationships between them (arrows). Rectangular boxes
are latent constructs based on the questionnaire items. For the sake of clarity, we do
not include the questionnaire items themselves in the diagrams. Elliptical boxes are
behavioral metrics, extracted from the systems’ data logs. One-headed arrows represent regression coefﬁcients (which have a causal direction); double-headed arrows
represent correlation coefﬁcients.
The color of the boxes matches the colors in the framework (Fig. 1); each color
signiﬁes a speciﬁc type of construct (purple: OSA, green: SSA, orange: EXP, blue:
INT, red: PC, light blue: SC). The numbers on the arrows represent the regression
coefﬁcients (i.e. the strength of the structural relations, also represented by the thickness of the arrows), their standard deviation (between parentheses) and the statistical
significance. Non-significant relations are not included in the graphs.
4.3 FT1 EMIC pre-trial
The EMIC pre-trial was conducted to conﬁrm two basic premises for the success of
recommender systems in general: the premise that the user experience of a recommender is inﬂuenced by the recommendations, and the premise that users will provide
adequate preference feedback to train the recommender system.
4.3.1 Setup
The trial was conducted with 43 EMIC colleagues and partners who participated in
the trial on a voluntary basis (28 male, average age of 31, SD = 9.45). A detailed
treatment of this study can be found in Knijnenburg et al. . We therefore only
brieﬂy discuss the results here.
Participants all used a slightly modiﬁed version of the MSN Clipclub system (see
Fig. 2); the special section with recommendations was highlighted, the social networking features were disabled (as to not interfere with the study), an explanation of the
B. P. Knijnenburg et al.
Fig. 2 The modiﬁed ClipClub prototype
rating facilities was included, and participants were probed to provide at least one rating every ﬁve minutes (although this rating request could be denied). Participants were
randomly assigned to one of two conditions: 25 participants received random clips as
recommendations, and the remaining 18 participants received recommendations provided by a content-based Vector Space Modeling engine (i.e. we manipulated the OSA
“personalized vs. random recommendations”). Participants were told that providing
ratings would change the recommendations.6
After half an hour of interaction with the system, several questionnaires7 were
taken to measure the participants’ perceived recommendation quality (SSA), choice
satisfaction (outcome-EXP), perceived system effectiveness (system-EXP), intention
to provide feedback (INT), general trust in technology (PC), and system-speciﬁc privacy concerns (SC). The questions were factor-analyzed to produce metrics for these
concepts, and factor scores were included together with the manipulation in a regression path model (Fig. 3). Additionally, factor scores were correlated with behavioral
metrics obtained from click-stream logs.
6 Which is true for both conditions, but in the random condition these new recommendations would just
be another batch of random items.
7 A complete overview of the questionnaires used in the studies can be found in Appendix B.
Experience of recommender systems
perceived recommendation
perceived system
effectiveness
personalized
recommendations
clips watched
from beginning
viewing time
clips clicked
rating probes
.696 (.276)
.515 (.135)
.572 (.125)
.346 (.125)
.296 (.123)
-.255 (.113)
-.268 (.156)
15.17 vs 10.15
Objective System Aspects (OSA)
personalized
Subjective System Aspects (SSA)
Experience (EXP)
effectiveness, satisfaction
Personal Characteristics (PC)
Situational Characteristics (SC)
privacy concerns
Interaction (INT)
feedback, all ellipses
in technology
satisfaction
privacy concerns
intention to provide
Fig. 3 The path model constructed for FT1 - EMIC pre-trial. Please refer to Sect. 4.2.4 “Graphical presentation of SEMs” for interpretation of this ﬁgure. Note that the rectangle “personalized recommendations”
represents the difference between personalized and random recommendations, random being the baseline
4.3.2 Results
The results show that personalized recommendations (as compared to random recommendations) have a higher perceived quality (OSA →SSA), which leads to a higher
choice satisfaction (SSA →outcome-EXP) and system effectiveness (SSA →system-EXP). Behavioral data corroborates these hypotheses. Users of the system with
personalized recommendations watch a larger number of clips from beginning to end
(OSA →INT). Moreover, users who click fewer clips and have a lower total viewing
time rate the system as more effective (EXP ↔INT), which indicates that a higher
system effectiveness is related to reduced browsing activity (see discussion below).
The intention to provide feedback increases with choice satisfaction and system
effectiveness (EXP →INT) but decreases when users have a higher system-speciﬁc
privacy concern (SC →INT), which in turn increases when they have a lower trust in
technology (PC →SC).8 In terms of behavior, the number of canceled rating probes
(popping up after ﬁve minutes without rating) is significantly lower in the personalized condition than in the random condition (OSA →INT), and is also negatively
correlated with intention to provide feedback (INT ↔INT).
8 An effect of a personal characteristic on a situational characteristic is not explicitly predicted by our
framework, but in this case makes perfect sense.
B. P. Knijnenburg et al.
4.3.3 Discussion of results
The results show that a system with a recommender algorithm that provides personalized recommendations has a better user experience (in terms of both choice satisfaction
and system effectiveness) than a system that provides random recommendations. This
is not necessarily a surprising result; such a comparison between random and personalized recommendations is a bit unbalanced .
More interestingly, however, the path model indicates that this effect is indeed mediated by perceived recommendation quality (requirement 4 in Sect. 3.7). Furthermore,
there is no residual correlation between choice satisfaction and system effectiveness,
and a mediated variant provides a weaker model than the one described. In other
words, in this study there was no structural relation between these experience variables (requirement 5).
Users seem to base their intention to provide feedback on a trade-off between having
a better user experience and maintaining their privacy (requirement 6 and 8). However, the intention to provide feedback was not correlated with the total number of
ratings, indicating that the relation between intention and behavior can be very weak (a
well-known psychological phenomenon called the ‘intention-behavior gap’; Sheeran
User behavior is correlated with the experience variables (requirement 7), but at
times also directly with the manipulation of our study. Against our intuition, users
who rate the system as more effective have a lower total viewing time and a lower
number of watched clips. However, the results also show that at the same time, the
number of clips watched from beginning to end is higher in the personalized condition than in the non-personalized condition. The lower total viewing time and number
of clicked clips thus reﬂects a reduction in browsing, not consumption. This makes
sense, because recommendations are supposed to be an alternative to browsing in this
system. The outcomes clearly demonstrate the value of triangulating the behavioral
measures with subjective measures. Showing how behavioral measures are related
to experience variables allows researchers to assign meaning to these measurements,
which at times may even counter the researchers’ intuition. Moreover, it grounds our
subjective theory in observable behavior.
4.4 FT2 EMIC trial
The EMIC extended trial was conducted to reproduce and extend the effects of the
pre-trial. Taking a step beyond the unbalanced comparison in the pre-trail between
personalized and random recommendations, the extended trial looked at differences
between the non-personalized “generally most popular” items (GMP condition), the
VSM algorithm using explicit feedback as input (the same VSM condition as used in
the pre-trial) and the Bayesian Personalized Ranking Matrix Factorization algorithm
 , a state-of-the-art algorithm using all clicks to predict
recommendations (‘implicit feedback’; MF-I condition). Furthermore, we specifically controlled what the users were told about the systems’ use of their feedback:
nothing (none condition); that their rating behavior was being used to provide better
Experience of recommender systems
recommendations (rating behavior condition); or that all their behavior was being used
to provide better recommendations (all behavior condition). We hypothesized that this
manipulation would inﬂuence users’ privacy concerns.
4.4.1 Setup
An external company recruited German participants from a young demographic (targeted mean age of 25). Participants were instructed to use the system as many times
as they liked over the 20-day duration of the study, and they were asked to ﬁll out a
47-item user experience questionnaire after each session (which would comprise at
least 20 minutes of interaction). The external company paid the participants for their
cooperation. Participants were randomly assigned to a scenario (i.e. the OSA “scenario” [no story/rating behavior/all behavior] is a between subjects manipulation),
and after each questionnaire they would switch algorithms (i.e. the OSA “algorithm”
[GMP/VSM/MF-I] is a within subjects manipulation). Participants were informed of
this switch, but were not told which algorithm they would be using. Users used the
same system as in FT1 (see Fig. 2), but in contrast to the ﬁrst ﬁeld trial, there were no
explicit rating requests in this trial, and there was also no speciﬁc section with recommendations; instead, all categories that the user could navigate to (e.g. sport, gossip,
cars, news) showed a personalized subset of the items in that category. The behavior
of each participant was logged, allowing for an extensive analysis of the click-stream
of each user.
The trial yielded 430 questionnaires from 108 participants. After excluding all
questionnaires with fewer than 12 clicks (indicating insignificant usage), 258 questionnaires (60%) remained from 95 remaining participants (88%). These participants
had an average age of 27.8(SD = 4.70). 49 of them were male.
The questions in the questionnaires were ﬁrst submitted to an exploratory factor
analysis (EFA) to determine whether their covariances naturally reproduced the predicted constructs. This resulted in 7 factors:
Perceived recommendation quality (6 items, e.g. “I liked the items shown by the
system”, factor R2 = .009)9
Effort of using the system (3 items, e.g. “The system is convenient”, factor R2 =
Perceived system effectiveness and fun (10 items, e.g. “I have fun when I’m using
the system”, factor R2 = .694)10
Choice satisfaction (5 items, e.g. “I like the items I’ve chosen”, factor R2 = .715)
General trust in technology (4 items, e.g. “Technology never works”, no incoming
9 R2 values are taken from the ﬁnal SEM and not from the EFA. The R2 for perceived recommendation
quality was low because it was predicted by the algorithm condition only. We typically report the best ﬁtting
item as an example for the scale, full questionnaires can be found in Appendix B.
10 Fun was intended to be a separate construct, but the exploratory factor analysis could not distinguish this
construct from perceived system effectiveness. In other words, in answering our questionnaires participants
did not seem to conceptually distinguish these two constructs.
B. P. Knijnenburg et al.
System-speciﬁc privacy concerns (3 items, e.g. “The system invades my privacy”,
factor R2 = .333)
Intention to provide feedback (4 items, e.g. “I like to give feedback on the items
I’m watching”, factor R2 = .264).
12 items were deleted due to low communalities and/or unwanted cross-loadings. The
items were then analyzed using a conﬁrmatory structural equation modeling (SEM)
approach with repeated ordinal dependent variables and a weighted least squares estimator, in which the subjective constructs were structurally related to each other, to the
conditions (algorithm and scenario), and to several behavioral measures extracted from
the usage logs. The ﬁnal model had a reasonable model ﬁt (χ2(41) = 85.442, p <
.001, CFI = .977, TLI = .984, rMSEA = .065).11 Figure 4 displays the effects found
with this model.
4.4.2 Results
The model shows that the recommendations from the Matrix Factorization algorithm
(MF-I) have a higher perceived quality than the non-personalized “generally most
popular” (GMP) items (OSA →SSA). Higher perceived recommendation quality in
turn leads to lower effort (SSA →process-EXP), a higher perceived effectiveness and
fun (SSA →system-EXP), and a higher choice satisfaction (SSA →outcome-EXP).
The effect of the algorithm on choice satisfaction is however only partially mediated;
there is also a direct positive effect of the MF-I algorithm on choice satisfaction (OSA
→EXP). The vector space modeling algorithm (VSM) does not even affect perceived
quality at all; it only has a direct effect on choice satisfaction (increasing it marginally;
p < .10). Lower effort leads to more perceived effectiveness and fun , which in turn leads to
more satisfactory choices (system-EXP →outcome-EXP).
The effort required to use the system inﬂuences the intention to provide feedback:
the less effort users have to invest, the more they are willing to provide feedback (process-EXP →INT). Privacy concerns also increase when the system takes more effort
to use (process-EXP →SC).12 Figure 5 displays the tendency of marginal effects
of the different algorithms on recommendation quality, system effectiveness & fun,
and choice satisfaction. The graphs show the standardized difference between the
algorithms (MF-I and VSM) and the baseline condition (GMP).
Figure 4 shows a direct effect of trust in technology on users’ intention to provide
feedback (PC →INT). Trust in technology also reduces privacy concerns (PC →SC).
Telling users that their rating behavior is used to improve recommendations increases
their privacy concerns (OSA →SC).13 Notably, telling users that all their behavior is
being used to improve recommendations does not have this effect.
11 Agreed-upon model ﬁt requirements are described in more detail in Appendix A.
12 Our framework does not predict an effect of experience on situational characteristics. However, we
understand that privacy concerns may be higher when users have to put more effort into using the system,
because this usually means that they also have to provide more information to the system.
13 Our framework does not predict an effect from an objective system aspect on a situational characteristic
to exist. However, this particular OSA was specifically designed to change this particular SC.
Experience of recommender systems
in technology
satisfaction
system-speci c
privacy concerns
intention to provide
telling users that
their rating behavior
is used to improve recommendations
matrix factorization (MF-I)
using implict feedback
(vs. the generally most popular items; GMP)
clips clicked
percentage of
clips watched
from beginning
number of clips
watched at
full screen
number of times
the participant used the
system before
menu-items
log10 of total
viewing time
telling users that
all their behavior
is used to improve recommendations
-.510 (.114)
.200 (.087)
.464 (.064)
.847 (.086)
-.809 (.153)
.886 (.113)
-.474 (.097)
.458 (.101)
-.333 (.113)
.228 (.120)
.611 (.329)
1.328 (.788)
-1.123 (.507)
.982 (.286)
-.265 (.103)
.678 (.315)
-7.70 (2.70)
-.161 (.065)
.035 (.017)
-.116 (.049)
-1.571 (.664)
Objective System Aspects (OSA)
MF-I, VSM, all behavior, rating behavior
Subjective System Aspects (SSA)
Experience (EXP)
effort, effectiveness & fun, satisfaction
Personal Characteristics (PC)
Situational Characteristics (SC)
privacy concerns, number of times
Interaction (INT)
feedback, all ellipses
Vector Space Model (VSM)
using explicit feedback
(vs the generally most popular items; GMP)
to use the system
perceived recommendation
perceived system
effectiveness & fun
.446 (.133)
.267 (.154)
Fig. 4 The path model constructed for FT2 - EMIC trial. Please refer to Sect. 4.2.4 “Graphical presentation
of SEMs” for interpretation of this ﬁgure. Note that the “algorithm” manipulation is represented by the two
rectangles “Matrix Factorization (MF-I)” and “Vector Space Model (VSM)”, which are tested against the
non-personalized baseline “generally most popular (GMP)”. Likewise, the “scenario” manipulation is represented by the two rectangles “all behavior” and “rating behavior”, which are tested against the baseline
“no story”
In terms of behavioral measures, we ﬁnd that, like in the pre-trial (FT1), a high
choice satisfaction decreases the number of clips clicked (EXP →INT). Users who
perceive the recommendations to be of a higher quality also perform fewer searches
(SSA →INT), and users browse less between different categories when the MF-I
algorithm is used (OSA →INT). Users who are more satisﬁed with their choices
watch more clips from beginning to end (EXP →INT). Interestingly, fewer users
do so when the VSM algorithm is used (OSA →INT). Users who perceive the recommendations to be of a higher quality also watch more clips at full screen (SSA
→INT). Furthermore, intention to provide feedback increases the number of ratings
provided by the user, something we did not ﬁnd in FT1. Finally, in later sessions the
number of clips clicked and the viewing time increase (SC →INT). Participants also
provide fewer ratings in later sessions (SC →INT; this is in line with Harper et al.
B. P. Knijnenburg et al.
Fig. 5 Tendency of marginal (direct and indirect) effects of the “algorithm” condition on perceived recommendation quality, system effectiveness and fun, and choice satisfaction. Error bars indicate ±1 standard
error compared to the value of GMP, which is ﬁxed to zero. Scales of the vertical axes are in sample standard
deviations. The Matrix Factorization algorithm (MF) provides higher perceived recommendation quality
and a higher choice satisfaction than the non-personalized “most popular” algorithm (GMP); the Vector
Space Modeling algorithm (VSM) does not provide a significantly better experience
4.4.3 Discussion of the results
Ingeneralterms,thisstudyconﬁrmsthestructuralrelationsfoundinFT1,buttherearea
number of important differences and additional insights. Specifically, the study allows
us to compare two different algorithms with a non-personalized baseline (requirement
1, see Sect. 3.7). The results indicate that the BPR-MF recommendations of the MF-I
condition are most successful; these have a marginally higher perceived quality and
lead to a higher choice satisfaction (see Fig. 5). The VSM recommendations only
lead to a marginally higher choice satisfaction. A possible reason for these differences
is that the BPR-MF algorithm used implicit feedback, which can be gathered more
quickly than the explicit feedback used by the VSM algorithm (requirement 3).
The inclusion of the construct “effort of using the system” changes part of our path
model compared to FT1: Whereas in FT1 choice satisfaction and perceived system
effectiveness increased users’ intention to provide feedback, we now observe that it
is actually the effort of using the system that causes this effect (requirement 6). This
difference between FT1 and FT2 indicates the importance of measuring all aspects of
the user experience (requirement 5) as this allows us to better understand the nature
of the difference.
The trial conﬁrms that privacy and trust are important factors inﬂuencing the intention to provide feedback (requirement 6), although the effects are now structurally
different. Like in FT1, trust in technology reduces privacy concerns, but it now also
directly increases the intention to provide feedback (instead of a mediated effect via
privacy concerns). Due to the increased power of the study, FT2 enables us to measure
the link between feedback intention and behavior.
We also observe that telling users that their rating behavior is used (compared to
telling them nothing, or that all their behavior is used) increases their privacy concerns.
This is surprising, because using all behavior (including rating behavior) is by definition more intrusive than using only rating behavior. The wording in the two conditions
is almost the same: “In order to increase recommendation quality, we are going to
use [your ratings data]/[all your activities]”. One possible reason for the heightened
Experience of recommender systems
privacy concerns in the “rating behavior” scenario is that users in this scenario have
active control over the amount of information they provide to the system. They are
thus encouraged (or forced) to make an explicit trade-off between the potential usefulness of providing information and the effect this may have on their level of privacy.
In the “all behavior” condition, there is nothing the users can do to prevent the system
from analyzing their behavior, and it is also difﬁcult to anticipate the privacy effects
of exhibited and forborne behavior. Users may therefore be less sensitized with regard
to privacy concerns, or even forget that the system is continuously analyzing their
By triangulating our behavioral data with the subjective constructs (requirement
8), we are also able to revalidate the ﬁnding from FT1 that a good experience means:
“less browsing, but more time enjoying the content”. Finally, the extended period of
use allows us to look at the effect of familiarity with the system. Specifically, users
are exploring the system to a further extent in later sessions, which may indicate less
reliance on the recommendations.
4.5 FT3 BBC pre-trial
The BBC pre-trial used a special-purpose recommender system to present users with
recent programming of the BBC (see Fig. 6) and strove to compare three different algorithms side by side. As FT2 revealed that usage over time changes (i.e., we
observed less reliance on the recommendations over time), the pre-trial was conducted
to investigate how recommendation quality would evolve over time.
4.5.1 Setup
An external market research company recruited 59 British participants to participate
in both FT3 and FT4. The participants were selected to reﬂect a balanced mix of ages
(one third between 18 and 35, one third between 36 and 50, and one third between 51
and 66). Within each age group, the selection included equal numbers of males and
females. The selected sample also consisted of equal numbers of participants from
ﬁve regions across the UK. Finally, half of the participants were occasional users of
the existing TV/radio catch-up service, the other half were frequent users. Participants were paid a small fee for each daily task, and a bonus for each completed user
experience questionnaire. Over a period of two weeks (not counting weekends) participants performed two daily tasks. Specifically, the participants were asked every
day to rate a list of recommendations presented by three different algorithms (rating
task), and to use the system freely (free use task). The participants had an average age
of 41.5 (SD = 12.6). 27 of them were male.
The algorithms used in the rating task were the non-personalized ‘generally most
popular’ items (the GMP condition), and two personalized Matrix Factorization algorithms. One is described in Rendle and Schmidt-Thieme and relies on user
ratings to provide recommendations (‘explicit feedback’; the MF-E condition). The
other, BPR-MF , relies on all clicks in the interface (‘implicit
feedback’; the MF-I condition). Every day, each participant rated three lists (one from
B. P. Knijnenburg et al.
Fig. 6 The BBC MyMedia recommender prototype
each of the algorithms) of ﬁve recommended programs. The order in which the lists
were presented changed every day.
The system in the ‘free use’ task employed the MF-E algorithm for all participants
throughout the entire two-week period. After both weeks, participants ﬁlled out a
questionnaire asking about their user experience with the ‘free use’ system.
4.5.2 Results of the rating task
In the rating task users were asked to rate ﬁve recommended programs separately,
as well as the recommendation list as a whole. As can be seen in Fig. 7 below, the
average ratings decreased over time (contrast F(1, 15) = 12.7, p < .005,r = .68).
Either participants gradually became more critical about the recommendations (a psychological phenomenon called habituation), or they were actually confronted with
recommendations of a lower quality (e.g. because items previously recommended
were not included in the recommendations to prevent repetitiveness).
After an initial training period, at days four to seven (May 20–25), we observe
significantly higher ratings of the MF-E recommendation lists (MMF−E = 3.52 vs.
MGMP = 2.80, contrast F(1, 29) = 45.4, p < .001,r = .78) as well as the MF-I recommendation lists (MMF−I = 3.33 vs. MGMP = 2.80, contrast F(1, 29) = 19.1, p <
.001,r = .63). In the second week (May 24–27), the ratings for the lists provided by
MF-E and MF-I started to drop significantly (interaction of MF-E vs. GMP with linear
decreasing time: F(1, 32) = 6.90, p < .05,r = .42; interaction of MF-I vs. GMP
with linear decreasing time: F(1, 32) = 10.9, p < .005,r = .50), going down to the
Experience of recommender systems
Fig. 7 The means of the list
ratings, over time, of the
recommendation lists for the
generally most popular items
(GMP) and the Matrix
Factorization algorithm
recommendations based on
explicit (MF-E) and implicit
(MF-I) feedback. The
error bars present ±1 Standard
level of GMP. In other words, the quality of the MF-I and MF-E recommendations
improves over time, but later falls back to the level of the non-personalized GMP
recommendations.
4.5.3 Results of the free use task
To analyze the free use task, we compared the outcomes of the user experience questionnaires collected at the two different time points (i.e. the SC “time” [end of week
1/end of week 2], can be seen as a within subjects manipulation). All 59 users partook
in this part of the study, but 8 users only completed the ﬁrst week’s questionnaire,
resulting in 110 data points. The results of the experience questionnaire (17 questions) were ﬁrst submitted to an exploratory factor analysis (EFA) to see if their
covariances naturally reproduced the predicted constructs. This resulted in three factors:
Perceived recommendation variety (2 questions, “The recommendations contained
alotofvariety”and“Alltherecommendedprogrammesweresimilartoeachother”,
factor R2 = .052)
Perceived recommendation quality (7 questions, e.g. “The recommended items
were relevant” and “I liked the recommendations provided by the system”, factor
R2 = .148)
Perceived system effectiveness (6 questions, e.g. “I would recommend the MyMedia recommender to others” and “The MyMedia recommender is useful”, factor
R2 = .548).
Two questions were deleted due to low communalities. The items were then analyzed using a conﬁrmatory SEM approach with repeated ordinal dependent variables
and an unweighted least squares estimator, in which the subjective constructs were
structurally related to each other and to the dichotomous independent variable ‘time’
(week 1 vs. week 2). The ﬁnal model had a good model ﬁt (χ2(22) = 27.258, p =
.20, CFI = .982, TLI = .984, rMSEA = .047). Figure 8 displays the effects found
with this model.
The model shows that a higher perceived variety of the recommendations causes
users to perceive the recommendations as having a higher quality, and that higher
B. P. Knijnenburg et al.
-.468 (.233)
.406 (.123)
.389 (.196)
perceived recommendation
perceived system
effectiveness
.879 (.158)
Subjective System Aspects (SSA)
variety, quality
Experience (EXP)
effectiveness
Situational Characteristics (SC)
.305 (.154)
perceived recommendation
Fig. 8 The path model constructed for FT3 - BBC pre-trial. Please refer to Sect. 4.2.4 “Graphical presentation of SEMs” for interpretation of this ﬁgure
quality recommendations in turn cause an increased perceived system effectiveness
(SSA →SSA →EXP). There is also a direct effect of perceived variety on perceived
system effectiveness (SSA →EXP); the perceived quality does not fully mediate the
effect of variety on effectiveness.
Additionally, in this study the variety of the recommendations turned out to be
significantly lower in the second week (SC →SSA), which in turn led to a lower
perceived quality of the recommendations in the second week. This is in line with
the drop in recommendation list ratings as found in the rating task (see Fig. 7).
The lower perceived recommendation quality, in turn, decreased the perceived effectiveness of the system. However, time also had a positive direct effect on perceived
system effectiveness (SC →EXP), which cancelled out the negative indirect effect
(the mean system effectiveness did not differ significantly between week 1 and
4.5.4 Discussion of the results
The drop in perceived recommendation quality in the second week was consistent
between the rating and free use task. The SEM model explains that this happened
because the set of recommended programs was less varied in the second week. Arguably, perceived variety drops because TV programs repeat after the ﬁrst week, at
which point they resurface among the recommendations. Interestingly, due to a direct
positive effect of time on perceived system effectiveness, these effects in the end did not
reduce the user experience. An ad hoc explanation of this effect could be that although
users may have felt that recommending new episodes of previously recommended
shows is not a sign of high quality recommendations, they may at the same time have
appreciated the reminder to watch the new episode. The study thus clearly shows the
merit of measuring subjective concepts in addition to user ratings in understanding
the underlying causes of rating differences.
Experience of recommender systems
4.6 FT4 BBC trial
The BBC trial was primarily conducted to test the effect of explicit versus implicit
preference elicitation methods on the user experience of the system.
4.6.1 Setup
The trial used the same system as the ‘free use’ system in the pre-trial, but now the
algorithm that provided the recommendations in this system was changed every three
days between GMP, MF-I and MF-E until each algorithm was used once by each user
(see Sect. 4.5.1 for more details on these conditions). In each condition users were
given the same background on the system, specifically, no information was given on
the way recommendations were computed or on what type of information they were
based. Participants were randomly assigned to one of three groups, for which the order
of the algorithms was manipulated in a Latin square design, so that each algorithm
was being used by one third of the users at any time. 58 users (the same as those
in the pre-trial) completed this study, resulting in 174 data points. The user experience questionnaire was similar to the pre-trial, but two questions were added to gain
a more robust measurement of recommendation variety (now measured by 4 items).
The exploratory factor analysis resulted in the same three factors (see Sect. 4.5.3).
The behavior of each participant was logged, allowing for an extensive analysis of the
click-stream of each user.
The questionnaire items were then analyzed using a conﬁrmatory structural equation modeling (SEM) approach with repeated ordinal dependent variables and a
weighted least squares estimator, in which the subjective constructs were structurally
related to each other, to the conditions (algorithm/preference input method), and to six
behavioral measures extracted from the usage logs. The ﬁnal model (Fig. 9) had a reasonably good ﬁt (χ2(29) = 49.672, p = .0098, CFI = .976, TLI = .982, rMSEA =
4.6.2 Results
As in FT3 (see Fig. 8), we observe that a higher perceived recommendation variety
leads to a higher perceived recommendation quality, which in turn leads to a higher
perceived system effectiveness (SSA →SSA →system-EXP). Interestingly, the two
variants of the matrix factorization algorithm affect different SSAs: compared to GMP,
MF-E recommendations (which are based on explicit feedback) have a significantly
higher perceived variety, while MF-I recommendations (which are based on implicit
feedback) have a significantly higher perceived quality (OSA →SSA). Despite their
different trajectories, the net effect of these algorithms is that their increased perceived quality positively affects the user experience (in terms of perceived system
effectiveness) compared to the GMP condition (see also Fig. 10).
In terms of behavioral measures, perceived variety is correlated with the number
of items clicked in the player (as opposed to the catalog; INT →SSA). Recommendation quality is correlated with the rating users give to clicked items, and the number
B. P. Knijnenburg et al.
Number of items
clicked in the
of clicked items
Number of items
rated higher than
the predicted rating
items rated
1.250 (.183)
.310 (.148)
.356 (.175)
1.352 (.222)
.856 (.132)
20.094 (7.600)
.314 (.111)
1.865 (.729)
participant's
Objective System Aspects (OSA)
MF-I, MF-E
Subjective System Aspects (SSA)
variety, quality
Experience (EXP)
effectiveness
Personal Characteristics (PC)
Interaction (INT)
all ellipses
perceived system
effectiveness
.846 (.127)
Matrix Factorization recommender with
explicit feedback (MF-E)
(versus generally most popular; GMP)
Matrix Factorization recommender with
implicit feedback (MF-I)
(versus most popular; GMP)
perceived recommendation
perceived recommendation
Fig. 9 The path model constructed for FT4 - BBC trial. Please refer to Sect. 4.2.4 “Graphical presentation
of SEMs” for interpretation of this ﬁgure. Note that the “preference elicitation method” manipulation is
represented by the two rectangles “explicit feedback (MF-E)” and “implicit feedback (MF-I)”, which are
tested against the non-personalized baseline “generally most popular (GMP)”
Fig. 10 Tendency of marginal (direct and indirect) effects of the “preference elicitation method” condition on perceived recommendation variety, perceived recommendation quality, and system effectiveness.
Error bars indicate ±1 Standard Error. The value of GMP is ﬁxed to zero; scales are in sample standard
deviations
of items that are rated higher than the predicted rating (SSA →INT).14 Finally, the
perceived system effectiveness is correlated with the total number of items that participants rate (EXP ↔INT). We furthermore note that older participants rate significantly
more items (PC →INT).
14 SSA →INT is not predicted, but these behaviors are a direct expression of the SSA.
Experience of recommender systems
4.6.3 Discussion of the results
Thisstudyconﬁrmsthebasicstructuralrelationsbetweenmanipulation,subjectivesystem aspects, and experience as also found in FT3 (OSA →SSA →EXP; requirement
4, see Sect. 3.7). In this case, the OSA is the preference elicitation method (requirement
3). Two variants of the matrix factorization algorithm using different types of input
(MF-E and MF-I) are tested against a non-personalized baseline (GMP). The fact that
both MF-E and MF-I resulted in a better user experience than GMP is not very surprising, but interestingly the model shows that the cause for this effect is different for each
algorithm. MF-E (which makes recommendations based on explicit feedback) seems
to result in a higher variety in recommendations than GMP, which in turn leads to a
higher recommendation quality and a higher perceived effectiveness (OSA →SSA
→SSA →EXP); MF-I (which makes recommendations based on implicit feedback)
on the other hand seems to have a direct inﬂuence on recommendation quality, which
also leads to a higher perceived effectiveness (OSA →SSA →EXP).
The lack of an increased diversity for MF-I recommendations could be caused by a
“pigeonholing effect”: when the MF-I algorithm uses clicks to predict implicit preferences, and gives recommendations based on these preferences, most subsequent clicks
will be in line with these predicted preferences, and the algorithm will increasingly
home in on a very speciﬁc set of recommendations. In MF-E, negative explicit feedback (low ratings) can prevent such pigeonholing. Like Jones et al. , our results
show that explicit control over the recommendations leads to a higher recommendation
quality through an increase of perceived variety, but that beyond the effect through
variety, there is no increase in recommendation quality (e.g. by means of a higher
accuracy). The marginal effect on the perceived quality of the recommendations for
MF-E and MF-I is about equal, and higher than the quality of the non-personalized
recommendations (GMP). The same holds for perceived system effectiveness (see
An interesting way to exploit the difference between MF-I and MF-E would be to
combine this result with the result of FT3, which found that diversity decreases over
time (see Fig. 8). In this respect, a recommender system could start out with the MF-I
algorithm, which can provide high-quality recommendations from the start (as it does
not need any ratings). As the diversity of the recommendations starts decreasing, the
algorithm can put more weight on the users’ ratings in determining their preferences,
or even switch to the MF-E algorithm altogether. This would give the system a boost in
recommendation variety, which should provide a better user experience in the long run.
Several behavioral correlates corroborate our subjective measures (requirement
7). The perceived variety of the recommendations is related to the number of items
clicked in the player. These items are related to the item that is currently playing,
and are therefore arguably more specialized (and thus over time more varied) than
other recommendations. The results further show that users reward good recommendations with higher ratings, and system effectiveness is correlated with the number of
ratings the user provides (see also requirement 6). The causal effect here is unclear:
participants that rate more items may end up getting a better experience because the
algorithms gain more knowledge about the user (INT →EXP). On the other hand, as
B. P. Knijnenburg et al.
found in FT1 (see Fig. 3), users may intend to rate more items when they notice that
this improves their experience (EXP →INT).
4.7 EX1 choice overload experiment
The four studies described above are all ﬁeld trials, testing real-life user-behavior in
real-world systems. These studies have a high ecological validity, but it is hard to analyze decision processes in detail, because users are unrestricted in their needs, goals,
and actions. To get more in-depth knowledge about users’ decision processes, we conducted two controlled lab experiments. The tasks in these experiments are ﬁxed, as
is the interaction with the system: Users ﬁrst rate a set of test-items and then make a
single decision, each considering the same choice goal. The ﬁrst of these experiments
looked at the effect of the size and quality of the recommendation set on the user experience to study a phenomenon called choice overload. The experiment is described in
detail in , so here we will only brieﬂy discuss the results and the
merit of using the framework in this experiment.
4.7.1 Setup
The experiment was conducted with 174 Dutch participants (invited from a panel with
students or recently graduated students from several Dutch universities) with an average age of 26.8(SD = 8.6). 89 of them were male. Participants were paid e3 upon
successful completion of the experiment. They were asked to use the system (loaded
with the 1M MovieLens dataset,15 and using the Matrix Factorization algorithm implementation from the MyMedia Software Framework) to ﬁnd a good movie to watch. To
train the recommender system, they ﬁrst rated at least 10 movies they knew, and were
subsequently presented with a list of recommendations from which to make a choice.
Users were randomly assigned to receive either one of three recommendation sets:
Top-5 (the best ﬁve recommendations, according to our MF algorithm), Top-20 (the
best twenty recommendations), and Lin-20 (the best ﬁve recommendations, supplemented with the recommendations with rank 99, 199, 299, …, 1499). After making a
choice, users completed a set of questionnaires to measure perceived recommendation
set variety, recommendation set attractiveness, choice difﬁculty, satisfaction with the
chosen item, and movie expertise. A Structural Equation Model was ﬁtted on these
questionnaire constructs and our two dichotomous manipulation variables (“Top-20
vs. Top-5” and “Lin-20 vs. Top-5”); Fig. 11 shows the resulting path model; for details
on the data analysis, refer to .
4.7.2 Results
The model shows that satisfaction with the chosen item (outcome-EXP) is the result
of two opposing forces: a positive effect of the quality of the recommendations (SSA
15 The MovieLens datasets are freely available at The 1M MovieLens dataset contains
one million ratings for 3952 movies and 6040 users.
Experience of recommender systems
.455 (.211)
.181 (.075)
.503 (.090)
1.151 (.161)
.336 (.089)
-.417 (.125)
.205 (.083)
.879 (.265)
.612 (.220)
-.804 (.230)
.894 (.287)
perceived recommendation
perceived recommendation
vs Top-5 recommendations
satisfaction
vs Top-5 recommendations
Objective System Aspects (OSA)
Top-20, Lin-20
Subjective System Aspects (SSA)
variety, quality
Experience (EXP)
culty, satisfaction
Personal Characteristics (PC)
Fig. 11 The path model constructed for EX1 - Choice overload experiment. Please refer to Sect. 4.2.4
“Graphical presentation of SEMs” for interpretation of this ﬁgure. Note that the manipulation of recommendation set size and quality is represented by the two rectangles “Top-20” (a larger set than Top-5) and
“Lin-20” (a larger set than Top-5, but with lower ranked additional items)
→outcome-EXP) and a negative effect of choice difﬁculty (process-EXP →outcome-EXP). Furthermore, high-quality recommendations increase choice difﬁculty
(SSA →process-EXP), and recommendation quality itself depends strongly on the
perceived recommendation variety (SSA →SSA; consistent with FT3 and FT4).
RelativetoTop-5, theTop-20conditionincreases theperceivedvarietyof therecommendation set (OSA →SSA) and the difﬁculty of the choice (OSA →process-EXP):
Top20 is more varied and more difﬁcult to choose from. The Lin-20 set is also more
varied than Top-5 (OSA →SSA), and negatively affects the recommendation set
attractiveness (OSA →SSA). Lin-20 furthermore has a positive residual effect (i.e.
controlling for recommendation set quality and choice difﬁculty) on satisfaction (OSA
→outcome-EXP), relative to the Top-5 condition. Finally, in contrast to the ﬁndings
of Kamis and Davern and Hu and Pu , increased movie expertise in our
study positively affects recommendation set attractiveness and perceived variety (PC
4.7.3 Discussion of results
The marginal effect of our manipulation of the recommendation set composition on
choice satisfaction is zero (requirement 2, see Sect. 3.7); there is no significant marginal difference between the Top-5, Top-20 and Lin-20 sets (see Fig. 12). Thanks
to our mediating SSAs and EXPs (requirement 4 and 5), the model is able to show
exactly why this is the case. The Top-20 set is more varied (SSA) than Top-5 (and
thereby more attractive), but it is also more difﬁcult (process-EXP) to make a choice
from this set, and these effects level out to eventually show no difference in choice
B. P. Knijnenburg et al.
Fig. 12 Tendency of marginal
(direct and indirect) effects of
the “recommendation set
composition” condition on
choice satisfaction. Error bars
indicate ±1 Standard Error. The
value of Top-5 is ﬁxed to zero;
scales are in sample standard
deviations
satisfaction (outcome-EXP). The Lin-20 set is less attractive than the Top-5 (SSA),
but there is a positive residual effect on choice satisfaction (outcome-EXP). Arguably,
this can be interpreted as a context effect: participants in this condition contrasted their
choice against the inferior items that are in the tail of the Lin-20 distribution, making
the choice more satisfying because it was easier to justify. In the end, however, these
effects too cancel out, so the resulting net effect on choice satisfaction is approximately
Finally, the results show that domain knowledge (PC) inﬂuences the perception
of recommendation set variety and quality. Although requirement 8 speciﬁes that PC
should be related to EXP or INT, we have now at several occasions shown a relationship between PC/SC and SSA. Apparently, situational and personal characteristics can
inﬂuence users’ perceptions as well (e.g. “an expert’s eye”), something not recognized
in the earlier version of our framework .
4.8 EX2 diversiﬁcation experiment
As a ﬁnal experiment, we set out to investigate the relative effect of algorithm accuracy and recommendation set diversity on user experience. Ziegler et al. already
showed that diversifying the output of a recommender algorithm, although detrimental
for the accuracy of the recommended list, results in an increase in overall satisfaction (OSA →EXP). However, their experiment used single-question measures and
a restricted set of algorithms (only item-based and user-based k-Nearest Neighbors).
Furthermore, as we have shown in our previous experiments, perceived variety and
perceived quality of the recommendations (SSAs) are important mediators between
objective aspects (OSA) and user experience (EXP).
To better understand the intricate relation between accuracy, diversity and satisfaction, we therefore conducted an experiment in which we manipulated algorithm
and recommendation set variety (OSA) and measured perceived accuracy and diversity (SSAs) as well as several user experience variables (process-EXP, system-EXP
and outcome-EXP). The experiment compares k-Nearest Neighbors (kNN) with a
non-personalized “generally most popular” (GMP) algorithm (a baseline) and a stateof-the-art Matrix Factorization algorithm (MF). Like EX1, it uses the MovieLens 1M
Experience of recommender systems
4.8.1 Setup
The experiment was conducted online, using Amazon’s Mechanical Turk service to
gather 137 participants (78 male, mean age of 30.2, SD = 10.0), mainly from the US
and India. Participants were paid US $4 upon successful completion of the experiment.
They were asked to rate at least ten items from the MovieLens 1M set, after which
they would receive a list of ten recommendations. The composition of the list was
manipulated in 3 × 3 conditions, independently varying the algorithm (GMP, kNN,
MF) and the diversiﬁcation (none, little, lot) between subjects. Participants were asked
to choose one movie. Finally, they ﬁlled out the user experience questionnaires.
Diversiﬁcation was based on movie genre, and implemented using the greedy heuristic of Ziegler et al. . Specifically, the heuristic starts with the item with the
highest predicted rating, calculates a diversity score for each of the remaining items
in the top 100 (based on how many movies in the current recommendation list were
of the same genre), creates a compound score S weighing the predicted rating R and
the diversity score D according to the formula S = aD + (1 −a)R, and then chooses
from the top 100 the item with the highest compound score. This process is repeated
until the list contains ten items. For none, little and lot, the values of ‘a’ were 0, 0.5
and 0.9 respectively. Diversity scores were calculated using cosine similarity between
the genres of the movies.
Participants answered 40 questionnaire items on a 7-point scale. The answers were
factor analyzed and produced 6 conceptual factors:
Perceived recommendation set diversity (5 items, e.g. “Several movies in the list
of recommended movies were very different from each other”, factor R2 = .087)
Perceived recommendation set accuracy16 (6 items, e.g. “The recommended movies ﬁtted my preferences”, factor R2 = .256)
Perceived choice difﬁculty (4 items, e.g. “Selecting the best movie was easy/difﬁcult”, factor R2 = .312)
Perceived system effectiveness (7 items, e.g. “The recommender system gives me
valuable recommendations”, factor R2 = .793)
Choice satisfaction (6 items, e.g. “My chosen movie could become one of my
favorites”, factor R2 = .647)
Expertise (3 items, e.g. “Compared to my peers I watch a lot of movies”, no incoming arrows)
8 items were deleted due to low communalities or excessive cross-loadings. The discovered constructs were then causally related with each other and with the 3 × 3 conditions in a Structural Equation Model. The resulting model (Fig. 13) has a reasonably
good ﬁt (χ2(68) = 132.19, p < .001, CFI = .954, TLI = .976, rMSEA = .083).
The interactions between diversiﬁcation and algorithm were included in the model,
but not in the graph. This means that the effect of diversiﬁcation in the graph holds
16 In the previous studies we used the concept “perceived recommendation quality” instead of “perceived
recommendation accuracy”. The concepts of “recommendation quality” and “recommendation accuracy”
are slightly different: perceived accuracy merely looks at how well the recommendations ﬁt ones preferences, while recommendation quality allows for other possible sources of quality.
B. P. Knijnenburg et al.
-.567 (.112)
.401 (.089)
.434 (.064)
1.690 (.202)
.151 (.072)
-.469 (.094)
.234 (.078)
.254 (.080)
.309 (.083)
-.657 (.216)
.410 (.206)
-.872 (.246)
.859 (.403)
-1.229 (.518)
.979 (.412)
-1.036 (.470)
perceived recommendation
satisfaction
perceived recommendation
recommendation
k-Nearest Neighbors (kNN)
(versus most popular)
Matrix Factorization (MF)
(versus most popular)
participants' movie
participants'
perceived system
effectiveness
Objective System Aspects (OSA)
cation, kNN, MF
Subjective System Aspects (SSA)
variety, accuracy
Experience (EXP)
culty, effectiveness, satisfaction
Personal Characteristics (PC)
expertise, gender
Fig. 13 The path model constructed for EX2 - Diversiﬁcation experiment. Please refer to Sect. 4.2.4
“Graphical presentation of SEMs” for interpretation of this ﬁgure. The manipulation “diversiﬁcation” gives
values 0, 1, and 2 to levels “none”, “little” and “lot” respectively. The manipulation of algorithm is represented by the two rectangles “k-Nearest Neighbors” and “Matrix Factorization”, each tested against the
“generally most popular” recommendations. Figure 14 gives a more insightful presentation of the effects
of our conditions
only for the “generally most popular” algorithm condition, and that the effects of kNN
and MF hold only for the non-diversiﬁed condition.
4.8.2 Results
The model shows that non-diversiﬁed recommendations provided by the kNN and MF
algorithms are perceived as more accurate than the non-diversiﬁed “generally most
popular” (GMP) recommendations (OSA →SSA). Diversiﬁed GMP recommendations are also perceived as more accurate than non-diversiﬁed GMP recommendations
(OSA →SSA), even though their predicted rating has been traded off with diversity
(i.e., the predicted rating of the diversiﬁed item set is lower). Accurate recommendations are generally more difﬁcult to choose from (SSA →process-EXP), however,
there is also a direct negative effect of kNN, MF and diversiﬁcation on choice dif-
ﬁculty (OSA →process-EXP). Looking at the marginal effects in Fig. 14 (“Choice
difﬁculty”) we observe that it is less difﬁcult to choose from the recommendations
produced by the high diversiﬁcation settings and the kNN and MF algorithms (or in
other words: it is mainly the non-diversiﬁed generally most popular recommendations
that are difﬁcult to choose from).
Surprisingly, the lack of a direct effect of diversiﬁcation on perceived diversity suggests that diversiﬁed recommendations do not seem to be perceived as more diverse.
Figure 14 (“Perceived recommendation diversity”) gives a possible explanation for
this lack of effect. The relation between manipulated diversity and perceived diversity
is not consistent between the three algorithms. Most notably, two observations differ
strongly from our initial expectations: the kNN algorithm with low diversiﬁcation
Experience of recommender systems
Fig. 14 Tendency of marginal (direct and indirect) effects of algorithm and diversiﬁcation on our subjective
constructs. Error bars indicate ±1 Standard Error. The value of GMP-none is ﬁxed to zero; scales are in
sample standard deviations
provides surprisingly diverse recommendations, and the most diversiﬁed “generally most popular” recommendations are not perceived to be as diverse as they
should be.
In general, more diverse recommendations (in this case perceived diversity) are also
perceived as more accurate (SSA →SSA) and less difﬁcult to choose from . Users who rate the recommendations
as accurate also perceive the system to be more effective (SSA →system-EXP). The
more effective the system and the easier the choice, the more satisﬁed participants are
B. P. Knijnenburg et al.
with their choice (process-EXP →outcome-EXP and system-EXP →outcome-EXP,
respectively).
Finally, we observe that males ﬁnd the recommendations generally less accurate
(PC →SSA), and that in contrast to ﬁndings by Kamis and Davern and Hu
and Pu expertise increases the perceived accuracy and diversity of the recommendations (PC →SSA) and also increases the choice satisfaction , our results show a positive effect of diversiﬁcation on
the user experience (requirement 2, see Sect. 3.7). The inclusion of several user
experience constructs in our experiment enables us to show that this effect is
mainly due to a lower choice difﬁculty and a higher perceived system effectiveness (requirement 5). Interestingly, this effect is partially mediated by perceived
accuracy, but not by perceived diversity (requirement 4). Arguably, users do not
necessarily see the diversiﬁed recommendations as more diverse. The higher perceived accuracy may have resulted from the fact that users evaluate the accuracy
in our questionnaire not per item, but for the list of recommendations as a whole.
Users may have noticed that more diverse lists more accurately reﬂect their diverse
Figure 14 shows that the “GMP-none” condition (i.e. the non-diversiﬁed condition
without diversiﬁcation) stands out; it is the only condition that results in a significantly
worse user experience. Also, Fig. 13 shows that the effects of diversiﬁcation and algorithm are similar: they both increase recommendation accuracy and decrease choice
difﬁculty. In other words: diversiﬁcation of GMP recommendations might be just as
effective in improving the user experience as introducing a recommendation algorithm.
Moreover, despite the absence of an interaction effect in the model, Fig. 14 suggests
that these effects are not additive: introducing diversiﬁcation and personalization does
not improve the user experience beyond introducing either of them separately. Possibly, there is a ceiling-effect to the improvement of the perceived recommendation
quality, at least in the between subjects design we employed in this study in which
participants do not compare between conditions themselves directly. The result seems
to suggest that providing a diversiﬁed but non-personalized list of recommendations
may result in an equally good user experience as providing personalized recommendations, but we would suggest a replication with other datasets to conﬁrm this surprising
observation.
We are puzzled by the fact that our diversiﬁcation manipulation did not result in
a higher perceived diversity of the recommendations. Our diversiﬁcation algorithm
reduces the similarity between movies in terms of genre. This may not ﬁt the definition of similarity as the users of the system judge it. Alternatively, our measurement
of this concept could have been too weak to pick up on the subtle differences between
the recommendation lists. Finally, the effects of expertise and gender show support
for requirement 8: users’ personal characteristics do indeed inﬂuence their user experience.
Experience of recommender systems
5 Validation of the framework
The goal of this paper is to describe a generic framework for the user-centric evaluation
of recommender systems. Such a framework should have merits beyond the scope of
a single research project. Despite their limited scopes, the ﬁeld trials and experiments
described in this paper can provide an initial test of the general applicability of the
framework; any (lack of) consistency in our results gives valuable cues concerning
the external validity of the framework. Furthermore, the framework was designed to
allow for ad-hoc ﬁndings that can be elaborated in future work. Taken together, the
expected and unexpected ﬁndings of our ﬁeld trials and experiments allow us to reﬂect
on the merit of our framework as a guiding tool for user-centric recommender systems
Below, we consolidate the ﬁndings of our ﬁeld trials and experiments by validating parts of the framework separately and discussing the merit of the framework as
a whole. We also assess whether parts of the framework need to be reconsidered in
future investigations.
The ﬁndings are summarized in Table 2. The table shows that most requirements
(as deﬁned in Sect. 3.7) are covered by at least two studies. As the requirements arise
from gaps in the existing literature, the results of our studies bridge these gaps, and
at the same time provide a thorough validation of the framework itself. Moreover,
most of the speciﬁc results are conﬁrmed in more than one study, thereby providing
evidence for the robustness of the results as well as the general applicability of the
framework.
Based on the current ﬁndings and the relation of the framework to a broad range
of existing literature, we believe that the general requirements are extensible beyond
the scope of the current studies. However, speciﬁc results may very well only hold
for collaborative ﬁltering media recommenders. Further research is needed to extend
these speciﬁc ﬁndings to other types of recommender systems.
5.1 Objective system aspects (OSA) and subjective system aspects (SSA)
We deﬁned Objective System Aspects (OSAs) as the features of a recommender system that may inﬂuence the user experience, and as things to manipulate in our studies.
Most work in the ﬁeld of recommender systems is focused on algorithms. In our
studies we therefore considered algorithms as a manipulation (requirement 1), but we
also considered the type of preference input data (requirement 2) and the composition
of the recommendation list (requirement 3). We reasoned that users would notice a
change in an OSA: these subjective observations are the Subjective System Aspects
(SSAs). Specifically, we argued that SSAs would mediate the effect of the OSAs on
the user’s experience (EXP; requirement 4).
Taken together, the manipulations in FT1, FT2, FT4 and EX2 show that users are
able to perceive higher quality recommendations (OSA →SSA) and that this perception mediated the effect of the recommendation quality on the user experience (SSA
→EXP), even though this mediation is in some cases only partial (i.e. in FT2, EX1,
B. P. Knijnenburg et al.
Table 2 A summary of the results of our ﬁeld trials and experiments, listed per requirement (as deﬁned in
Sect. 3.7)
Requirement
1. Algorithm
Turning recommendations on or off has a noticeable
effect on user experience
FT1, FT2, FT4, EX2
Experience differences between algorithms are less
pronounced
2. Recommendation
set composition
Recommendation set size, quality and diversity have a
signiﬁcant impact on the user experience
Sets of different sizes and quality may end up having the
same choice satisfaction due to choice overload
Users do not perceive diversiﬁed recommendation sets
as more diverse, but they do perceive them as are more
3. Preference
input data
Explicit feedback leads to more diverse
recommendations, which subsequently leads to
increased perceived quality; implicit feedback
increases the perceived recommendation quality
4. Perceived
aspects as
Perceived aspects, particularly perceived
recommendation quality and variety, provide a better
understanding of the results
experience
evaluation
Usage effort and choice difﬁculty are measured as
process-related experience
FT2, EX1, EX2
Perceived system effectiveness is measured as
system-related experience
All except EX1
Choice satisfaction is measured as outcome-related
experience
FT1, FT2, EX1, EX2
Process-related experience causes system- or
outcome-related experience
FT2, EX1, EX2
System-related experience causes outcome-related
experience
6. Providing
Intention to provide feedback is a trade-off between
trust/privacy and experience
FT1, FT2, FT4
Users’ feedback behavior may not always be correlated
with their intentions to provide feedback
7. Behavioral data
A positive personalized user experience is characterized
by reduced browsing behavior and increased
consumption
8. Personal
situational
characteristics
Users’ experience and behaviors change over time
Age, gender and domain knowledge have an inﬂuence
on users’ perceptions, experience and behaviors
FT4, EX1, EX2
EX1 and EX2 show similar results for the recommendation set composition. Size,
quality and diversiﬁcation of the recommendation set each inﬂuence the user experience via subjective perceptions (OSA →SSA →EXP), even though the effect on
choice difﬁculty is mainly direct. Surprisingly, our diversiﬁcation algorithm increased
Experience of recommender systems
perceived accuracy but not perceived diversity. Despite this, diversiﬁcation was as
effective in improving the user experience as the introduction of a good algorithm.
In terms of preference input, FT4 shows that even though implicit feedback recommendations are based on more data than explicit feedback recommendations (all user
behavior versus ratings only), they are not always better from a user’s perspective,
especially if one takes into account the variety of the recommendations.
Concluding, the core component of our framework—the link from algorithm or
preference input data (OSA) to subjective recommendation quality (SSA) to experience (EXP)—is upheld throughout every conducted study. Some direct effects (OSA
→EXP) occur, which could indicate that not all possible SSAs were measured. In
general the SSAs mediate the effects of OSA on EXP, thereby explaining the effects of
the OSAs in more detail (i.e. the why and how of improved user experience). The capability to explain both surprising and straightforward ﬁndings makes SSAs an essential
part of our evaluation.
5.2 The different objects of experience (EXP)
We reasoned that user experience could be process-related, system-related and outcome-related. We argued that different recommender system aspects could inﬂuence
different types of experience, and that one type of experience could inﬂuence another.
Previous research typically includes just one type of user experience in their evaluation. In most of our studies we therefore considered more than one type of experience
(requirement 5).
Our research conﬁrms that these different types of user experience indeed exist; the
studies discern the following user experience constructs: perceived usage effort (process-EXP; FT2), choice difﬁculty (process-EXP; EX1, EX2), perceived system effectiveness (system-EXP; all studies, except EX1) and choice satisfaction (outcome-EXP;
FT1, FT2, EX1, EX2). In all cases, the main mediator that causes these experience
variables is the perceived recommendation quality. Structurally, process-related experiences often cause system-related experiences, which in turn cause outcome-related
experiences (process-EXP →system-EXP →outcome-EXP).
5.3 Behavioral data and feedback intentions
We indicated that behavioral data could be triangulated with subjective experience
data to improve the interpretation of behavioral results, and to ground self-report measures in actual behavior (requirement 7). We also reasoned that the speciﬁc behavior
of providing preference feedback is of particular interest to recommender systems
researchers, because in many systems this feedback is needed to provide good recommendations (requirement 6).
FT1, FT2 and FT4 show that feedback behavior is a trade-off between the users’
trust or privacy concerns, and the user experience. The actual feedback behavior is not
always highly correlated with the intention to provide feedback.
Furthermore, all ﬁeld trials show several significant triangulations. Consistently,
reduced browsing and increased consumption are indicators of effective systems.
B. P. Knijnenburg et al.
In some cases behaviors are directly related to the OSAs. This probably means that
we did not measure the speciﬁc SSAs that would mediate the effect.
5.4 Personal and situational characteristics (PC and SC)
We argued that personal and situational characteristics may inﬂuence the users’ experience and interaction with the system (requirement 8). Our research (FT1 and FT2)
addresses trust in technology (PC) and system-speciﬁc privacy concerns (SC), and
shows how these concepts inﬂuence users’ feedback intentions (see requirement
6). Furthermore, the experience and interaction of the recommender systems in our
research changes over time (SC; see FT2 and FT3). Concluding, several contextual
effects seem to inﬂuence the users’ interaction and experience with the recommender
system. Our research addresses trust, privacy, time, age, gender and expertise (domain
knowledge).
Our initial conception of the effect of personal and situational characteristics seems
to have been too restrictive: We only allowed these characteristics to inﬂuence the
experience (EXP) and interaction (INT). Based on the results of our research, we
acknowledge that these characteristics can sometimes inﬂuence not only the evaluation, but also the perception of the system (i.e. inﬂuence the subjective system aspects,
SSAs). We suggest including an additional arrow from personal and situational characteristics to subjective system aspects (PC →SSA and SC →SSA), and we encourage
researchers to investigate this connection in more detail in future experiments.
Our research on the users’ intention to provide feedback also alludes to possible
changes in the framework. The results of FT1 and FT2 are not entirely consistent
in terms of the factors that inﬂuence the intention to provide feedback, and further
research is needed to specifically delineate what causes and inhibits users to provide
preference feedback to the system .
6 Conclusion
The framework provides clear guidance for the construction and analysis of new recommender system experiments. It allows for an in-depth analysis that goes beyond
algorithmic performance: it can explain why users like a certain recommender system
and how this user experience comes about.
The framework also puts emphasis on the integration of research. When testing
with real users one cannot study algorithms in isolation; several system aspects (and
personal and situational characteristics) have to be combined in a single experiment
to gain a full understanding of the user experience.
For industry researchers, the user-centric focus of the framework provides a step
closer to the customers, who may not consider the accuracy of the algorithm the most
important aspect of their experience. Questionnaire-taking and A/B testing (the industry term for testing several versions of a certain system aspect) are an accepted form
of research in web technology. For academic researchers, the framework provides an
opportunity to check the real-world impact of the latest algorithmic improvements.
Moreover, interesting effects of situational and personal characteristics, as well as
Experience of recommender systems
behavioral correlates can be used as input for context-aware recommender engines.
Even more so, when evaluating the relative merit of novel recommendation approaches
such as context-aware algorithms and recommenders using
social networks , one has to rely on more sophisticated ways of
measuring the full user experience, and our framework could serve as a guideline for
such evaluations.
7 Future research
This paper has argued that measuring algorithmic accuracy is an insufﬁcient method
to analyze the user experience of recommender systems. We have therefore introduced
and validated a user-centric evaluation framework that explains how and why the user
experience of a recommender system comes about. With its mediating variables and
its integrative approach, the framework provides a structurally inclusive foundation
for future work. Our research validates the framework and, beyond that, produced
some unanticipated results.
Still, our work represents merely the tip of the iceberg of user-centric recommender
systems research. Our research is limited in scope: we only tested a small number of
media-oriented recommender systems. To determine the scope of applicability of our
framework, further validation of the framework should consider other content types,
specifically “search products”. Moreover, some of our results are inconclusive and
require further investigation.
The link between algorithmic accuracy and user experience is a fundamental question that currently still functions as the untested premise of a considerable part of the
recommender systems research. The same holds for users’ intention to provide feedback. And whereas our research shows some interesting results concerning the use
of explicit versus implicit feedback, it is by no means exhaustive in this respect. We
furthermore believe that future work could investigate the effects of other personal and
situational characteristics, and the results of these studies could be used to personalize
not only the recommendations of the system, but also the system itself .
Because of the pioneering nature of our work, we took a very thorough approach
in our evaluation. The proposed methodology of measuring constructs with multiple
questionnaire items and analyzing the results with exploratory factor analyses and
structural equation models improves the external validity of our results. We realize,
however, that this methodology may be infeasible when testing recommender systems
in a fully operational industry setting. We therefore created a pragmatic procedure that
allows the measurement of speciﬁc concepts of recommender system user experience
with just a few key questionnaire items and a simpliﬁed (possibly automated) statistical evaluation . Our current results provided useful input
for the development of this procedure.
Concluding, our framework provides a platform for future work on recommender
systems, and allows researchers and developers in industry and academia to consistently evaluate the user experience of their systems. The future of user-centric recommender systems research is full of exciting opportunities.
B. P. Knijnenburg et al.
Acknowledgements
We would like to thank Mark Graus for programming the recommender systems
used in EX1 and EX2, Steffen Rendle for implementing the explicit feedback MF algorithm, Niels Reijmer,
Yunan Chen and Alfred Kobsa for their commentsat several stagesof thispaper,and Dirk Bollen for allowing
us to incorporate the results of his choice overload experiment (EX1) in this paper. We also thank the three
anonymous reviewers for their extensive comments on the initial submission. We gratefully acknowledge
the funding of our work through the European Commission FP7 project MyMedia (www.mymediaproject.
org) under the grant agreement no. 215006. For inquiries please contact .
Open Access
This article is distributed under the terms of the Creative Commons Attribution License
which permits any use, distribution, and reproduction in any medium, provided the original author(s) and
the source are credited.
Appendix A: Methodology
Statistical analysis of our ﬁeld trials and experiments involved two main steps: validating the measured latent concepts using exploratory factor analysis (EFA) and testing
the structural relations between the manipulations, latent concepts and behavioral
measurements using structural equation modeling (SEM). In this section we provide
a detailed description of our methodological approach by means of a hypothetical
example. See Knijnenburg et al. for a more pragmatic procedure to evaluate
recommender systems.
In the example, we test two algorithms (A1 and A2) against a non-personalized
baseline (A0). We measure perceived recommendation quality (Q) with 5 statements
(Q1 . . . Q5) to which participants can agree or disagree on a 5-point scale. Satisfaction
with the system (S) is measured with 6 items (S1 . . . S6). Finally, we measure user
behavior (B) in terms of the number of clips watched from beginning to end.
Exploratory factor analysis (EFA)
The ﬁrst step is to conﬁrm whether the 13 items indeed measure the predicted two
latent concepts. This is done using exploratory factor analysis. This technique extracts
common variance between the measured items and distributes this variance over a
number of latent factors. We use the software package Mplus17 to do this analysis
with the command “analysis: type = efa 1 3; estimator = wlsmv;”. This runs the
exploratory factor analysis with 1, 2 and 3 factors, and uses a weighted least squares
estimation procedure with mean- and variance-adjusted chi-square tests.
The factor analytical model can be represented as Fig. 15. Each item is essentially
a regression outcome,18 predicted by two unobserved latent variables. I1,1 to I2,11 are
called the loadings of the items Q1 . . . S6 on the factors F1 and F2. The model tries
to estimate these loadings so that the paths match the covariance matrix of the items
Q1 to S6 as closely as possible (e.g. cov1,2 ≈I1,1 ∗I1,2 + I2,1 ∗I2,2 + I1,1 ∗w1,2 ∗
I2,2+I1,2∗w1,2∗I2,1). Intuitively, factor analysis tries to model the “overlap” between
items. The part of the variance that does not overlap is excluded (and represented by
17 
18 Since these outcomes are measured on a 5-point scale, this regression model is an ordinal response
Experience of recommender systems
I2,10 I2,11
Fig. 15 Representation of the exploratory factor analysis example. Two factors, F1 and F2, are extracted
from the questionnaire data (items Q1. . .Q5 and S1 . . . S6). The arrows from the factors to the questions
(I1,1 . . . I2,11) represent the factor loadings. The arrows at the bottom represent the portion of the variance
not accounted for by the analysis. Factors have a certain reliability (represented by the arrows at the top),
and may be correlated (represented by w1,2). If the grey arrows are close to zero, F1 effectively measures
recommendation quality (Q) and F2 effectively measures satisfaction with the system (S)
the arrows at the bottom of Fig. 15). The more overlap extracted, the more reliable the
factor (the arrows at the top). The factors may be correlated with each other (w1,2).
The solution has no standard coordinate system, so it is rotated so that each question
loads on only one factor as much as possible.
If the measurement tool was speciﬁed correctly, then the model has a good leastsquares ﬁt, and only the paths from F1 to Q1 . . . Q5 and from F2 to S1 . . . S6 are significantly larger than zero (i.e. only the darker paths in Fig. 15). In that case F1 measures
recommendation quality, and F2 measures satisfaction with the system. However, the
following problems may arise:
A one-factor solution ﬁts almost as well as a two-factor solution: In this case, we
must conclude that Q and S are essentially the same concept.
A three-factor solution ﬁts much better than a two-factor solution: In this case, this
questionnaire measures three concepts (usually because either S or Q is in fact a
combination of two factors).
A certain item does not have enough in common with the other items to load on any
of the factors significantly (p < .05): In this case the item has “low communality”
and should be removed from the analysis.
A certain item loads on the wrong factor, or on both factors: The item has a high
“cross-loading”, and unless we can come up with a good reason for this to occur,
it should be removed from the analysis.
Once an adequate factor solution is established, the remaining items are used in the
second step of the analysis.
B. P. Knijnenburg et al.
Fig. 16 Representation of the structural equation modeling example. The algorithms (A1 and A2) inﬂuence
the perceived recommendation quality (Q), which in turn inﬂuences the satisfaction with the system (S).
The satisfaction (S) is in turn correlated with the number of clips watched from beginning to end (B). Q is
measured by (Q1 . . . Q5) and S is measured by (S1 . . . S6). In the models in the main text the questionnaire
items (Q1 . . . S6) are hidden in order to get a less cluttered representation
Structural equation modeling (SEM)
The second step is to test the structural relations between our manipulation (A), the
latent concepts (Q and S) and the behavior measurement (B). Our manipulation has
three conditions (A0, A1, and A2), so we create two dummy variables (A1 and A2),
which are tested against the baseline. The dummies are coded in such a way that they
represent the different conditions: For participants in the baseline A1 = 0, and A2 = 0;
for participants with algorithm 1, A1 = 1, and A2 = 0; for participants with algorithm
1, A1 = 0, and A2 = 1. As a result of this coding, the effect of A1 is therefore the
effect of algorithm 1 compared to the baseline, and the effect of A2 is the effect of
algorithm 2 compared to the baseline.
The resulting model is tested using structural equation modeling. The speciﬁcation
of the model deﬁnes the factors and the items with which they are measured (in Mplus:
“Q by Q1–Q5; S by S1–S6;”), and the structural relations among the factors and other
variables (in Mplus: “S on Q A1 A2; Q on A1 A2;”). This creates a model that can be
represented as Fig. 16 (but, as of yet, without B).
Like any regression model, structural equation models make assumptions about
the direction of causality in the model. From a modeling perspective, an effect
(Q →S) and its reverse (S →Q) are equally plausible . By including the manipulation(s) in our model, we are able to “ground”
the causal effects: participants are randomly assigned to a condition, so condition assignment cannot be caused by anything in the model (i.e. A1 →Q
is possible, but not Q →A1). Furthermore, the framework provides hypotheses for the directionality of causal effects .
Experience of recommender systems
For each regression path in the model, a regression coefﬁcient is estimated. As
the values of the latent constructs are standardized (by means of the factor analysis),
the regression coefﬁcient between the two latent constructs (Q →S) shows that a 1
standard deviation difference in Q causes a 0.60 standard deviation difference S. The
standard error of the coefﬁcient (0.15) can be used in a z-test to test whether the path
is significant (p = z[.60/.15] = .00003 < .001). The dummy variables A1 and A2
are not standardized, but represent the presence (1) or absence (0) of a certain condition. Therefore, the coefﬁcient on the arrow A1 →Q shows that Q is 0.50 standard
deviations higher for participants in A1 than for those in A0.
In a typical model, not all initially speciﬁed paths are significant. This means that
some effects will be fully mediated. For example: if the paths from A1 and A2 to
S (the lighter paths in Fig. 16) are not significant, the effect of A1 and A2 on S is
fully mediated. Otherwise, there is only partial mediation. Non-significant effects are
removed from the model and the model is ran again to create a more parsimonious
Variables that have no hypothesized effect (such as B in this model) are initially
included in the model without specifying any structural relation for it. Mplus can provide a “modiﬁcation index” for this variable, thereby showing where it best ﬁts in the
model. Since such effect is ad-hoc, it is only to be included if it is highly significant.
Variables for which we have no hypothesized direction of effect (again, variable B
in this model, for which we don’t know whether B →S or S →B) are included as a
correlation. This means that no assumption about the direction of causality is made.
The ﬁnal model (after excluding non-significant effects and including ad-hoc
effects) can be tested as a whole. The Chi-square statistic tests the difference in
explained variance between the proposed model and a fully speciﬁed model. A good
model is not statistically different from the fully speciﬁed model (p > .05). However, this statistic is commonly regarded as too sensitive, and researchers have therefore proposed other ﬁt indices . Based on extensive simulations, Hu and Bentler propose cut-off values for these ﬁt indices to be:
CFI > .96, TLI > .95, and rMSEA < .05. Moreover, a good model makes sense
from a theoretical perspective. The model shown in Fig. 16 has this quality: The algorithms (A1 and A2) inﬂuence the perceived recommendation quality (Q), which in turn
inﬂuences the satisfaction with the system (S). The satisfaction (S) is in turn correlated
with the number of clips watched from beginning to end (B).
Appendix B: Questionnaire items for each construct
This appendix lists all the questionnaire items shown to the participants of the ﬁeld
trials and experiments. It makes a distinction between those items that were included
19 Not all non-significant effects are excluded from the models. For instance, when two conditions are
tested against a baseline, and one of the conditions does not significantly differ from the baseline but the
other does, then the non-significant effect is retained to allow a valid interpretation of the significant effect.
Furthermore, in EX2, the interactions between diversity and algorithm, though not significant, are retained,
as they allow a valid interpretation of the significant conditional main effects.
B. P. Knijnenburg et al.
in the ﬁnal analysis, and those items that did not contribute to stable constructs and
were therefore deleted. Included questions are in order of decreasing factor loading.
FT1 EMIC pre-trial
Perceived recommendation quality
I liked the items recommended by the system.
The recommended items ﬁtted my preference.
The recommended items were well-chosen.
The recommended items were relevant.
The system recommended too many bad items.
I didn’t like any of the recommended items.
The items I selected were “the best among the worst”.
Perceived system effectiveness
I would recommend the system to others.
The system is useless.
The system makes me more aware of my choice options.
I make better choices with the system.
I can ﬁnd better items without the help of the system.
I can ﬁnd better items using the recommender system.
Not included:
The system showed useful items.
Choice satisfaction
I like the items I’ve chosen.
I was excited about my chosen items.
I enjoyed watching my chosen items.
The items I watched were a waste of my time.
The chosen items ﬁt my preference.
I know several items that are better than the ones I selected.
Some of my chosen items could become part of my favorites.
I would recommend some of the chosen items to others/friends.
Intention to provide feedback
I like to give feedback on the items I’m watching.
Normally I wouldn’t rate any items.
I only sparingly give feedback.
Experience of recommender systems
I didn’t mind rating items.
In total, rating items is not beneﬁcial for me.
General trust in technology
Technology never works.
I’m less conﬁdent when I use technology.
The usefulness of technology is highly overrated.
Technology may cause harm to people.
Not included:
I prefer to do things by hand.
I have no problem trusting my life to technology.
I always double-check computer results.
System-speciﬁc privacy concern
I’m afraid the system discloses private information about me.
The system invades my privacy.
I feel conﬁdent that the system respects my privacy.
I’m uncomfortable providing private data to the system.
I think the system respects the conﬁdentiality of my data.
FT2 EMIC trial
Perceived recommendation quality
I liked the items shown by the system.
The shown items ﬁtted my preference.
The shown items were well-chosen.
The shown items were relevant.
The system showed too many bad items.
I didn’t like any of the shown items.
Not included:
The system showed useful items.
The items I selected were “the best among the worst”.
Effort to use the system
The system is convenient.
I have to invest a lot of effort in the system.
It takes many mouse-clicks to use the system.
B. P. Knijnenburg et al.
Using the system takes little time.
It takes too much time before the system provides adequate recommendations.
Perceived system effectiveness and fun
I have fun when I’m using the system.
I would recommend the system to others.
Using the system is a pleasant experience.
The system is useless.
Using the system is invigorating.
The system makes me more aware of my choice options.
Using the system makes me happy.
I make better choices with the system.
I use the system to unwind.
I can ﬁnd better items using the recommender system.
Not included:
I can ﬁnd better items without the help of the system.
I feel bored when I’m using the system.
Choice satisfaction
I like the items I’ve chosen.
I was excited about my chosen items.
I enjoyed watching my chosen items.
The items I watched were a waste of my time.
The chosen items ﬁt my preference.
Not included:
I know several items that are better than the ones I selected.
Some of my chosen items could become part of my favorites.
I would recommend some of the chosen items to others/friends.
Intention to provide feedback
I like to give feedback on the items I’m watching.
Normally I wouldn’t rate any items.
I only sparingly give feedback.
I didn’t mind rating items.
Not included:
In total, rating items is not beneﬁcial for me.
Experience of recommender systems
General trust in technology
Technology never works.
I’m less conﬁdent when I use technology.
The usefulness of technology is highly overrated.
Technology may cause harm to people.
System-speciﬁc privacy concern
I’m afraid the system discloses private information about me.
The system invades my privacy.
I feel conﬁdent that the system respects my privacy.
Not included:
I’m uncomfortable providing private data to the system.
I think the system respects the conﬁdentiality of my data.
FT3 BBC pre-trial
Perceived recommendation quality
The recommended items were relevant.
I liked the recommendations provided by the system.
The recommended items ﬁtted my preference.
The MyMedia recommender is providing good recommendations.
I didn’t like any of the recommended items.
The MyMedia recommender is not predicting my ratings accurately.
The recommendations did not include my favorite programmes.
Perceived recommendation variety
The recommendations contained a lot of variety.
All the recommended programmes were similar to each other.
Perceived system effectiveness
The MyMedia recommender is useful.
I would recommend the MyMedia recommender to others.
The MyMedia recommender has no real beneﬁt for me.
I can save time using the MyMedia recommender.
I can ﬁnd better programmes without the help of the MyMedia recommender.
B. P. Knijnenburg et al.
The MyMedia recommender is recommending interesting content I hadn’t previously considered.
Not included:
The system gave too many recommendations.
FT4 BBC trial
Perceived recommendation quality
The MyMedia recommender is providing good recommendations.
I liked the recommendations provided by the system.
The recommended items ﬁtted my preference.
The recommended items were relevant.
I didn’t like any of the recommended items.
The MyMedia recommender is not predicting my ratings accurately.
The recommendations did not include my favorite programmes.
Perceived recommendation variety
The recommendations contained a lot of variety.
The MyMedia recommender is recommending interesting content I hadn’t previously considered.
The recommendations covered many programme genres.
All the recommended programmes were similar to each other.
Most programmes were from the same genre.
Perceived system effectiveness
The MyMedia recommender has no real beneﬁt for me.
I would recommend the MyMedia recommender to others.
The MyMedia recommender is useful.
I can save time using the MyMedia recommender.
I can ﬁnd better programmes without the help of the MyMedia recommender.
Not included:
The system gave too many recommendations.
EX1 choice overload experiment
Perceived recommendation variety
The list of recommendations was varied.
The list of recommendations included movies of many different genres.
Experience of recommender systems
Many of the movies in the list differed from other movies in the list.
All recommendations seemed similar.
Not included:
No two movies in the list seemed alike.
The list of recommendations was very similar/very varied.
Perceived recommendation quality
The list of recommendations was appealing.
How many of the recommendations would you care to watch?
The list of recommendations matched my preferences.
I did not like any of the recommendations in the list.
Choice difﬁculty
Eventually I was in doubt between … items.
I changed my mind several times before making a decision.
I think I chose the best movie from the options.
The task of making a decision was overwhelming.
Not included:
How easy/difﬁcult was it to make a decision?
How frustrating was the decision process?
Choice satisfaction
My chosen movie could become one of my favorites.
How satisﬁed are you with the chosen movie?
I would recommend the chosen movie to others.
I think I would enjoy watching the chosen movie.
I would rather rent a different movie from the one I chose.
I think I chose the best movie from the options.
Not included:
The list of recommendations had at least one movie I liked.
I am a movie lover.
Compared to my peers I watch a lot of movies.
Compared to my peers I am an expert on movies.
B. P. Knijnenburg et al.
EX2 diversiﬁcation experiment
Perceived recommendation accuracy
The recommended movies ﬁtted my preference.
Each of the recommended movies was well-chosen.
I would give the recommended movies a high rating.
The provided recommended movies were interesting.
I liked each of the recommended movies provided by the system.
Each of the recommended movies was relevant.
Not included:
I did not like any of the recommended movies.
Perceived recommendation variety
Several movies in the list of recommended movies were very different from each
The list of recommended movies covered many genres.
The list of recommended movies had a high variety.
Most movies were from the same type.
The list of recommended movies was very similar/ very varied.
Not included:
All recommended movies were similar to each other.
Choice difﬁculty
Selecting the best movie was very easy/very difﬁcult.
Comparing the recommended movies was very easy/very difﬁcult.
Making a choice was overwhelming.
I changed my mind several times before choosing a movie.
Not included:
Making a choice was exhausting.
Making a choice was fun.
Making a choice was frustrating.
Eventually I was in doubt between … movies.
Perceived system effectiveness
The recommender system gave me valuable recommendations.
I would recommend the recommender system to others.
I can ﬁnd better movies using the recommender system.
Experience of recommender systems
I make better choices with the recommender system.
The recommender system is useless.
The recommender system makes me more aware of my choice options.
I don’t need the recommender system to ﬁnd good movies.
Choice satisfaction
My chosen movie could become one of my favorites.
The chosen movie ﬁts my preference.
I will enjoy watching my chosen movie.
I like the movie I have chosen.
I will recommend the movie to others/friends.
Watching my chosen movie will be a waste of my time.
Not included:
I am excited about my chosen movie.
Compared to my peers I watch a lot of movies.
Compared to my peers I am an expert on movies.
I only know a few movies.
Not included:
I am a movie lover.