Catastrophic forgetting and mode collapse in GANs
1st Hoang Thanh-Tung
Applied Artiﬁcial Intelligence Institute
Deakin University
 
2nd Truyen Tran
Applied Artiﬁcial Intelligence Institute
Deakin University
 
Abstract—In this paper, we show that Generative Adversarial
Networks (GANs) suffer from catastrophic forgetting even when
they are trained to approximate a single target distribution.
We show that GAN training is a continual learning problem
in which the sequence of changing model distributions is the
sequence of tasks to the discriminator. The level of mismatch
between tasks in the sequence determines the level of forgetting.
Catastrophic forgetting is interrelated to mode collapse and can
make the training of GANs non-convergent. We investigate the
landscape of the discriminator’s output in different variants
of GANs and ﬁnd that when a GAN converges to a good
equilibrium, real training datapoints are wide local maxima of the
discriminator. We empirically show the relationship between the
sharpness of local maxima and mode collapse and generalization
in GANs. We show how catastrophic forgetting prevents the
discriminator from making real datapoints local maxima, and
thus causes non-convergence. Finally, we study methods for
preventing catastrophic forgetting in GANs.
Index Terms—GANs, generative, catastrophic forgetting, mode
I. INTRODUCTION
GANs are a powerful tool for modeling complex
distributions. Training a GAN to approximate a single target
distribution is often considered as a single task. In this paper,
we introduce a novel view of GAN training as a continual
learning problem in which the sequence of changing model
distributions are considered as the sequence of tasks. We
discover a surprising result that GANs suffer from catastrophic
forgetting, a problem often observed in continual learning
settings . Catastrophic forgetting (CF) in artiﬁcial neural
networks is the problem where the knowledge
of previously learned tasks is abruptly destroyed by the
learning of the current task. When a GAN suffers from CF,
it exhibits undesired behaviors such as mode collapse and
non-convergence.
In section III, we show that GAN training is actually a
continual learning problem and demonstrate the CF problem
on a number of datasets. We show that catastrophic forgetting
and mode collapse are two different but interrelated
problems and together, they can make the training of GANs
non-convergent (section III-B, IV-B). To avoid mode collapse
and improve convergence, it is important to address the CF
problem. We identify 2 factors that causes CF in GANs: 1)
Information from previous tasks is not used in the current task,
2) Knowledge from previous tasks is not usable for the current
task and vice versa. Our ﬁndings shed light on how to avoid
catastrophic forgetting to learn the target distribution properly
(Section V).
In section IV, we investigate the effect of CF and mode
collapse on the landscape of the discriminator’s output. We ﬁnd
that when a GAN converge to a good local equilibrium without
mode collapse, real datapoints are wide local maxima of the
discriminator. We show that the sharper the local maxima are,
the more severe mode collapse is. Section IV-B shows that
when CF happen, the discriminator is directionally monotonic.
A GAN with a directionally monotonic discriminator does not
converge to an equilibrium. The fact conﬁrms that CF is a
cause of non-convergence.
Section V explains how state-of-the-art methods for stabilizing GANs such as Wasserstein GAN , zero-centered
gradient penalty on training examples (GAN-R1) , zerocentered gradient penalty on interpolated samples (GAN-0GP)
 , and optimizers with momentum, can prevent CF and
mode collapse. Finally, we introduce a new loss function that
helps preventing CF while adding zero computational overhead.
Contributions:
1) We detect the CF problem in GANs.
2) We show the relationship between CF, mode collapse,
and non-convergence.
3) We study the relationship between the sharpness of local
maxima and mode collapse.
4) We show that CF tends to make the discriminator
directionally monotonic around real datapoints.
5) We identify the causes of CF and explain the effectiveness
of methods for preventing CF in GANs.
II. RELATED WORKS
Convergence. Prior works on the convergence of GANs
usually consider the convergence in parameter space . However, convergence in parameter space tells little
about the quality of the equilibrium that a GAN converge to.
For example, Thanh-Tung et al. demonstrated that TTUR 
can make GAN converge to collapsed equilibrium. Consensus
Optimization can introduce spurious local equilibria with
unknown properties to the game.
We directly study the behaviors of GANs in the data space.
By analyzing the discriminator’s output landscape, we ﬁnd that
when a GAN converges, real datapoints are local maxima of
the discriminator. We discover the relationship between the
sharpness of local maxima and mode collapse, generalization.
 
Catastrophic forgetting. Seff et al. studied the standard
continual learning setting in which a GAN is trained to generate
samples from a set of distributions introduced sequentially. The
problem is solved by the direct application of continual learning
algorithms such as Elastic Weight Consolidation (EWC) 
to GANs. Liang et al. independently came up with a
similar intuition that GAN training is a continual learning
problem.1 The paper, however, did not study the causes and
effects of the problem and focused on applying continual
learning algorithms to address catastrophic forgetting in GANs.
We focus on explaining the causes and effect of the problem
and its relationship to mode collapse and non-convergence.
III. CATASTROPHIC FORGETTING PROBLEM IN GANS
A. GANs training as continual learning problems
Let us consider a GAN with generator G(·; θ) : Rdz →Rd, a
continuous function with parameter θ ∈Rm; and discriminator
D(·; ψ) : Rd →R, a continuous function with parameter
ψ ∈Rn. G transforms a dz-dimensional noise distribution pz
to a d-dimensional model distribution pg that approximates a
d-dimensional target distribution pr. D maps d-dimensional
inputs to 1-dimensional outputs. Let LD be the loss function
for D, LG be the loss function for G (Table I). In practice, G
and D are neural networks trained by alternating SGD .
At each iteration of the training process, G is updated to
better fool D. pt
g, the model distribution at iteration t, is
different from the model distribution at the previous iteration
and the next iteration pt+1
. The knowledge required to
separate pt
g from pr is different from that for the pair {pt−1
, pr} and {pt
g, pr} are two different classiﬁcation tasks
to the discriminator.2 The sequence of changing model distributions
i=1 and the target distribution pr form a sequence
i=1 to the discriminator. Because
the generator at iteration t, Gt, can only generate samples
g, Dt, the discriminator at iteration t, cannot access
samples from previous model distributions p<t
g . That makes the
learning process of D a continual learning problem. Similarly,
the generator has to fool a sequence of changing discriminators
i=1. The training process of a GAN poses a different
continual learning problem to each of the players. In this paper,
we focus on the continual learning problem in the discriminator
as many prior works have showed that the quality of a GAN
mainly depends on its discriminator .
If the sequence pt
g converges to a distribution p∗
the sequence of tasks
i=1 converges to a single task
of separating 2 distributions p∗
g and pr. In practice, however,
the sequence of model distributions does not always converge.
Nagarajan and Kolter formally proved that the players in
1Liang et al. came up with the idea a few months after us. They agreed
that we are the ﬁrst to consider the catastrophic forgetting problem in a single
GAN. Their preprint has not been published at any conferences or journals.
2In the original theoretical formulation of GAN, at every GAN iteration, the
discriminator and the generator are trained until convergence . That means
g can be arbitrarily different from pt−1
. In practice, at each iteration, only a
limited number of gradient updates are applied to the players. We can consider
a chunk of consecutive model distributions as a task to the discriminator.
Wasserstein GAN do not converge to an equilibrium but
oscillate in a small cycle around the equilibrium. Although nonsaturating GAN (GAN-NS) was proven to be convergent
under strong assumptions , Fedus et al. observed
that on many real world datasets, the distance between pt
and pr (measured in KL-divergence and Jensen-Shannon
divergence) does not decrease as t increases. The authors
suggested that pg can approach pr in many different and
unpredictable ways. These results imply that in the most
common variants of GANs, pt
g can be arbitrarily different from
for large n. If the knowledge used for separating pt
pr cannot be used for separating pt−n
and pr, a discriminator
trained on T t could forget T t−n, i.e. it classiﬁes samples in
T t−n wrongly (Fig. 11b). When this happens, we say that the
discriminator exhibits catastrophic forgetting behaviors.
B. Catastrophic forgetting in GANs
1) Catastrophic forgetting on synthetic dataset: We begin
by analyzing the problem on the 8 Gaussian dataset, a dataset
generated by a mixture of 8 Gaussians placed on a circle. In
Fig. 1, red datapoints are generated samples, blue datapoints
are real samples. The discriminator and generator are 2 hidden
layer MLP with 64 hidden neurons. ReLU activation function
was used. pz is a 2-dimensional standard normal distribution.
SGD with constant learning rate of α = 3 × 10−3 was used
for both networks. The vector at a datapoint x shows the
negative gradient −∂LG/∂x. The vector shows the direction
in which LG decreases the fastest. The length of the vector
corresponds to the speed of change in LG. Because the gradient
ﬁeld is conservative, the the difference between the loss of two
datapoints x0 and x1 is:
LG(x0) −LG(x1) =
where v = −∂LG/∂x and C is a path from x0 to x1. For the
variants in Table I, ∂LG/∂x only depends on x and D. Because
decreasing LG in these GANs corresponds to increasing D(x),
going in the direction of −∂LG/∂x increases the score D(x).
Let y0 = G(z0), z0 ∼pz be a fake datapoint. Updating y0
with SGD with a small enough learning rate will move y0
in the direction of −∂LG/∂y0 by a distance proportional to
∥−∂LG/∂y0∥. If the discriminator is ﬁxed, then SGD updates
will move y0 along its integral curve, in the direction of
increasing D(y0).3
Fig. 1a - 1d show the evolution of a GAN-NS on 8 Gaussian
dataset. In Fig. 1a - 1c, the discriminator assigns higher score
to datapoints that are further away from the fake datapoints,
regardless of the true labels of these points. This is shown by
the gradient vectors pointing away from the fake datapoints.
The integral curves do not converge to any real datapoints.
If D is ﬁxed, updating G with gradient descent makes pg
diverges. Because gradients w.r.t. different fake datapoints have
3 In practice, gradient updates are not applied to y0 but to the generator’s
parameters. Because the generator also minimizes LG, gradient updates to the
generator move y0 in a direction that approximates −∂LG/∂y0. −∂LG/∂y0 is
a good approximation of the direction that y0 will move in the next iteration.
−Ex∼pr[D(x)] +Ez∼pz[D(G(z))] + λEu[(∥(∇D)u∥−1)2]
−Ez∼pz[D(G(z))]
where u = αx + (1 −α)y; x ∼px, y ∼pg, α ∼U(0, 1)
Ex∼pr[−log(D(x))] + Ez∼pz[−log(1 −D(G(z)))]
Ez∼pz[−log(D(G(z)))]
Ex∼pr[−log(D(x))] + Ez∼pz[−log(1 −D(G(z)))] + λEx∼pr[∥(∇D)x∥2]
Ez∼pz[−log(D(G(z)))]
Ex∼pr[−log(D(x))] + Ez∼pz[−log(1 −D(G(z)))] + λEu[∥(∇D)u∥2]
Ez∼pz[−log(D(G(z)))]
where u = αx + (1 −α)y; x ∼px, y ∼pg, α ∼U(0, 1)
TABLE I: Loss functions of GAN variants considered in this paper.
(a) Iteration 3000
(b) Iteration 3500
(c) Iteration 3600
(d) Iteration 20000
(e) Iteration 1000
(f) Iteration 2500
(g) Iteration 5000
(h) Adam. Iteration 1500
Fig. 1: (a) - (d) catastrophic forgetting in GAN-NS trained on the 8 Gaussian dataset. (e) - (g) GAN-R1 with λ = 10.
GAN-0GP and WGAN-GP exhibit similar behaviors on this dataset. (h) GAN-NS trained with Adam. Viewing on computer is
recommended.
the same direction, almost all of fake datapoints move in the
same direction and do not spread out over the space. Because
of CF, the generator is unable to break out of mode collapse.
Inside the green box (Fig. 1a), gradients at all datapoints
have approximately the same direction. The loss LG decreases
(the score D(·) increases) monotonically along the direction
of the green vector u, a random vector that points away from
the fake datapoints.4 We have the following observation:
Observation 1. In a large neighborhood around a real
datapoint, LG (and therefore, D(·)) is directionally monotonic.
A theoretical explanation to this phenomenon is given in Sec.
IV-B. Because fake samples in Fig. 1a-1d are concentrated in
4 Graphically, we see that the angles between the green vector u and
v = −∂LD/∂x are less than 90° for all x in the box. Thus, the dot product
v · du is positive. The line integral in Eqn. 1 is positive for x0, x1 in the
box that satisfy x1 = x0 + ku, k > 0. LG monotonically decreases along
the direction of u. We say that LG is monotonic in direction u.
a small region (i.e. mode collapse), D can easily separate them
from distant real samples and does not learn useful features
of the real data. We say that D catastrophically forgets real
samples that are far away from the current fake samples. Mode
collapse and CF are interrelated, one problem makes the other
more severe.
In Fig. 1b, fake datapoints on the right of the red box have
higher scores than real datapoints on the left, although in Fig.
1a, these real datapoints have higher scores than these fake
datapoints. Going from Fig. 1a to 1d, we observe that the
vectors’ directions change as soon as fake datapoints move.
The phenomenon suggests that information about previous
model distributions is not preserved in the discriminator. As
Dt tries to separate pt
g from pr, it assigns low scores to regions
with fake samples and higher scores to other regions. Because
Dt does not ’remember’ p<t
g , it could assign high scores to
regions previously occupied by pt
g, i.e. Dt could classify old
fake samples as real. Fake samples at iteration 3000 (Fig. 1a)
are classiﬁed as real by D3500 (Fig. 1b). Similar behaviors
are observed on MNIST (Fig. 11b). Because of forgetting, D
could direct G to move to a region which G has visited before.
That could cause G and D to fall in a learning loop and do
not converge to an equilibrium. In Fig. 1a - 1d, the model
distribution rotates around the circle indeﬁnitely. CF is a cause
of non-convergence.
2) Catastrophic forgetting on image datasets: We performed
experiments on real world datasets to conﬁrm the existence
of CF in GANs. We visualize the landscape around a real
datapoint x by plotting the output of the discriminator along
a random line through x. We choose a random unit vector
ˆu ∈Rd, ∥ˆu∥= 1 and plot the value of the function
f(k) = D(x + k ˆu)
for k ∈[−100, 100]. We use the same ˆu for all images in
Fig. 2. We choose to visualize D(·) instead of LG because
LG explodes if D(·) ≪1. The quality of the image x + k ˆu
decreases as |k| increases. A good discriminator D∗should
assign lower scores to samples with lower quality. D∗(x)
should be higher than D∗(x + k ˆu), k > 0, i.e. x is a local
maximum of D∗. If x is a local maximum of D∗, f ∗(k) must
have a local maximum at k = 0 (the center of each subplot).
The result reported below was observed in all 10 different runs
of the experiment.
Fig. 2 demonstrates the problem on MNIST. The generator
and discriminator are 3 hidden layer MLPs with 512 hidden
neurons. SGD with constant learning rate α = 3 × 10−4 was
ued in training.
As shown in Fig. 2, the generated images keep changing
from one shape to another, implying that the game does not
converge to an equilibrium. In a large neighborhood around
every real image, the discriminator’s output is monotonic in
the sampled direction. At iteration 100000, for every image, f
is a decreasing function (Fig. 2f), while at iteration 200000,
f is an increasing function (Fig. 2g). More conretely, let
∇ˆuDt(x0) be the discriminator’s directional derivative along
direction ˆu at x0 at iteration t. Then Fig. 2f and 2g shows that
∇ˆuD100000(x0) and ∇ˆuD200000(x0) for some x0 near the
real datapoint x, have opposite directions. The knowledge of
D200000 (what D200000 learned on {p200000
, pr}) is not usable
for {p100000
We trained DCGAN on CelebA and CIFAR-10
 to study the effect of network architecture and dataset
complexity on the level of forgetting. Network architecture and
hyper parameters are given in Table II.
On CelebA, Fig. 9a - 9g show that CNN suffers less from CF
than MLP. The discriminator in DCGAN-NS is not directional
monotonic and it successfully makes many real datapoints its
local maxima (see Sec. IV for more). The discriminator can
effectively discriminate real images from neighboring noisy
images. The generator moves fake datapoints toward these
local maxima and produces recognizable faces.
On CIFAR-10 (Fig. 10a - 10g), the discriminator cannot
discriminate real images from noisy images. The function f(k)
in Fig. 10b is almost an increasing function while in Fig. 10d it
is almost a decreasing function. The training does not converge
as fake images change signiﬁcantly as the learning progresses.
Conclusion: GAN-NS trained on high dimensional datasets
exhibits the same catastrophic forgetting behaviors as on toy
datasets: (1) real datapoints are not local maxima of the
discriminator or in more extreme cases, the discriminator is
directionally monotonic in the neighborhoods of real datapoints;
(2) the gradients w.r.t. datapoints in the neighborhood of a real
datapoint change their directions signiﬁcantly as fake datapoints
3) The causes of Catastrophic Forgetting: Based on the
above experiments, we identiﬁed two reasons for CF:
1) Information from previous tasks is not carried to/used
for the current task. SGD does not use information from
previous model distributions, p<t
g . At iteration t, SGD
update for the discriminator is computed from samples
g and pr only. Because information from p<t
not used in training, the discriminator forgets p<t
g , i.e. it
does not assign low score to samples from p<t
2) The current task is signiﬁcantly different from previous
tasks so the knowledge of the current task cannot
be used for previous tasks and vice versa. As old
knowledge is overwritten by new knowledge, optimizing
the discriminator on the current task will degrade its
performance on older tasks.
Methods for preventing CF is studied in Section V.
IV. THE OUTPUT LANDSCAPE
A. The evolution of the landscape
We apply the visualization technique in Section III-B2 to
other variants of GAN. We reuse the network architecture and
learning rate from the experiment in Fig. 2. We replace SGD
with Adam with β1 = 0.5, β2 = 0.99. We run each experiment
10 times with different random seeds and report results that
are consistent between different runs. The evolution of the
landscape and generated samples of GAN-NS, GAN-0GP with
λ = 100, GAN-R1 with λ = 100, and WGAN-GP with λ = 10
are shown in Fig. 3, 4, 5, and 6 respectively.
GAN-0GP, GAN-R1, and WGAN-GP have signiﬁcantly
better sample quality and diversity than GAN-NS. GAN-NS
does not exhibit good convergence behavior: the digit in a image
changes from one digit to another as the training progresses
(Fig. 3).5 GAN-0GP, GAN-R1, and WGAN-GP exhibit better
convergence behaviors: for many images, the digits stay the
same during training.
We observe that throughout the training process of GAN-0GP,
GAN-R1, and WGAN-GP, for every real datapoint, the function
f(k) always has a local maximum at k = 0, implying that real
datapoints are local maxima of the discriminator. This can also
be seen in GAN-R1 trained on the 8 Gaussian dataset (Fig. 1e
- 1g): the gradients w.r.t. datapoints in the neighborhood of a
real datapoint point toward that real datapoint (GAN-0GP and
5Note that this does not contradict the statement in that GAN-NS
converge to an equilibrium. Many of the assumptions in that paper is not
satisﬁed in practice, e.g. the learning rate is not decayed toward 0.
(b) Landscape 50000
(c) Landscape 100000
(d) Landscape 200000
(e) Generated 50000
(f) Generated 100000
(g) Generated 200000
Fig. 2: Catastrophic forgetting problem in GAN-NS trained with SGD. (a) real datapoints from MNIST dataset. (b) - (d) the
landscape around these real datapoints at different training iterations. In each subplot, the X-axis represent k, the Y -axis
represent D(·). (e) - (g) generated data at different iterations. The same noise inputs were used for all iterations.
(b) Landscape 50000
(c) Landscape 100000
(d) Landscape 200000
(e) Generated 50000
(f) Generated 100000
(g) Generated 200000
Fig. 3: Output landscape and generated samples from GAN-NS + Adam.
(b) Landscape 50000
(c) Landscape 100000
(d) Landscape 200000
(e) Generated 50000
(f) Generated 100000
(g) Generated 200000
Fig. 4: Output landscape and generated samples from GAN-0GP with λ = 100.
(b) Landscape 50000
(c) Landscape 100000
(d) Landscape 200000
(e) Generated 50000
(f) Generated 100000
(g) Generated 200000
Fig. 5: Output landscape and generated samples from GAN-R1, λ = 100.
(b) Landscape 50000
(c) Landscape 100000
(d) Landscape 200000
(e) Generated 50000
(f) Generated 100000
(g) Generated 200000
Fig. 6: Output landscape and generated samples from WGAN-GP, λ = 10, 5 discriminator updates per 1 generator update.
WGANGP exhibit the same behaviors). If a fake datapoint is in
the basin of attraction of a real datapoint and gradient updates
are applied directly on the fake datapoint, it will be attracted
toward the real datapoint. Different attractors (local maxima)
at different regions of the data space attract different fake
datapoints toward different directions, spreading fake datapoints
over the space, effectively reducing mode collapse.
Fig. 7 shows that GAN-0GP with λ = 10 suffers from
mild mode collapse.6 The maxima in Fig. 7 are much sharper
than those in Fig. 6. The discriminator overﬁts to the real
training datapoints and forces the scores of near by datapoints
to be close to 0. That creates many ﬂat regions where the
gradients of the discriminator w.r.t. datapoints in these regions
are vanishingly small. A fake datapoint located in a ﬂat region
cannot move toward the real datapoint because the gradient is
vanishingly small. Real datapoints in Fig. 7 have small basin
of attraction and cannot effectively spread fake samples over
the space. The diversity of generated samples is thus reduced,
making mode collapse visible. In order to attract fake datapoints
toward different directions, local maxima should be wide, i.e.
they should have large basin of attraction.
The landscapes of GAN-NS in Fig. 2 and 3 contain many
ﬂat regions where the scores D(·) are very close to 1 or 0. The
same problem is seen on the 8 Gaussian dataset (datapoints
in the orange and blue boxes in Fig. 1a-1d have scores close
to 1 and 0, respectively). However, unlike Fig. 7, the real
datapoints in Fig. 1a - 1d, 2, and 3 are not local maxima. The
discriminator in GAN-NS underﬁts the data.
CNN based discriminators do not create ﬂat regions in the
output landscape (Fig. 9b-9d and 10b-10d). However, when the
dataset is more complicated, DCGAN-NS discriminator fails
to make real datapoints local maxima and the training does not
converge (Fig. 10a-10g). The discriminator underﬁts the data
because it is not powerful enough to learn features that separate
real and fake/noisy samples. More powerful discriminators
based on ResNet signiﬁcantly improve the quality of
GANs (e.g. ). We make the following observation:
Observation 2. For a GAN to converge to a good local
equilibrium, real datapoints should be wide local maxima
of the discriminator.
6This is consistent with the analysis by the authors of GAN-0GP. Thanh-
Tung et al. claimed that larger λ leads to better generalization but may slow
down the training.
(a) Generated 100000
(b) Landscape 100000
Fig. 7: Mode collapse without CF in GAN-0GP, λ = 10.
B. The effect of catastrophic forgetting on the landscape
(a) Iter. 0
(b) Iter. 10
(c) Iter. 100
(d) Iter. 200
(e) Iter. 300
(f) Optimal
(g) Iter. 0
(h) Iter. 10
(i) Iter. 125
(j) Iter. 250
Fig. 8: High capacity Dirac GAN with n = 2. The blue
line represents the discriminator’s function. The real and fake
datapoints are shown by the blue and red dots, respectively.
(a) - (e): Dirac GAN trained on the current fake example
only. (f): empirically optimal Dirac discriminator trained on
the current fake example only. (g) - (j): Dirac GAN trained on
two fake examples: old fake example on the left and current
fake example on the right.
We investigate the effect of CF on Dirac GAN , a GAN
that learns a 1 dimensional Dirac distribution located at the
origin, pr = δ0. In the original Dirac GAN, the discriminator
is a linear function with 1 parameter, D(x) = ψx, ψ ∈[−1, 1]
and the model distribution is a Dirac distribution located at
θ, pg = δθ. θ is the generator’s parameter. Initially, θ ̸= 0. At
each iteration, the training dataset of Dirac GAN contains two
training examples: a real training example x0 = 0, and a fake
training example y0 = θ. Gradient updates are applied directly
on the fake training example.
= −D(0) + D(x)
The unique equilibrium is ψ = θ = 0. Mescheder et al.
showed that the players in Dirac GAN do not converge to
an equilibrium (see Fig. 1 in ). To make the game converge
to the above equilibrium, the authors proposed R1 gradient
penalty which pushes the gradient w.r.t. the real datapoint to 0
(Table I). A high dimensional GAN can be narrowed to a Dirac
GAN by considering a pair of real and fake sample and the
discriminator’s output along the line connecting these samples
(similar to the landscape in Fig. 2-6).
Because the discriminator in the original Dirac GAN is a
linear function with a single parameter, the output of Dirac
discriminator is always a monotonic function. We consider a
generic discriminator which is a 1 hidden layer neural network:
ˆD(x) = Ψ⊤
1 σ(Ψ0x) where Ψ0, Ψ1 ∈[−1, 1]n×1, and σ is
a monotonically increasing activation function such as Leaky
ReLU (Fig. 8). At equilibrium, θ = 0 and ˆD(x) is any function
with a global maximum at x = 0. Although ˆD can have global
maxima (see Fig. 8h), optimizing ˆD only on the current task
makes ˆD a monotonic function (Fig. 8f).
Proposition 1. The optimal Dirac discriminator ˆD∗(x) that
minimizes Ldirac
in Eqn. 3 is a monotonic function.
Proof. Let ˆD(x) = Ψ⊤
1 σ(Ψ0x) where Ψ0, Ψ1 ∈[−1, 1]n×1
be the discriminator and σ be a non-decreasing activation
function such as ReLU, Leaky ReLU, Sigmoid, or Tanh. Let
x0 = 0 be the real datapoint, y0 = θ ̸= 0 be the fake datapoint.
The empirically optimal discriminator D∗must maximize the
difference D∗(x0) −D∗(y0).
1 σ(Ψ0 × 0)
1 σ(Ψ0 × y0)
Ψ1,iσ(Ψ0,iy0)
ˆD(x0) −ˆD(y0)
Ψ1,i × (σ(0) −σ(Ψ0,iy0))
and σ is non-decreasing
σ(0) −σ(−|y0|) ≥σ(0) −σ(Ψ0,iy0) ≥σ(0) −σ(|y0|)
If σ is ReLU or Leaky ReLU or Tanh, then σ(0) = 0,
|σ(|y0|)| ≥|σ(−|y0|)|, thus
|σ(0) −σ(|y0|)| > |σ(0) −σ(−|y0|)|
Architecture
DCGAN Pytorch example
Learning rate
Batch size
Adam, β1 = 0.5, β2 = 0.99
No. ﬁlters at 1st layer
TABLE II: DCGAN model architecture & hyper parameters.
If σ is Sigmoid, then σ(0) = 0.5 and |σ(0) −σ(|y0|)| =
|σ(0) −σ(−|y0|)|. For both cases, we have
|σ(0) −σ(Ψ0,iy0)| ≤|σ(0) −σ(|y0|)|
Ψ1,i(σ(0) −σ(Ψ0,iy0)) ≤1 × |σ(0) −σ(|y0|)|
The equality for both Eqn. 1 and 2 is achieved for all cases
when Ψ1,i = −1 and σ(Ψ0,iy0) = σ(|y0|) ⇒Ψ0,iy0 = |y0| ⇒
Ψ0,i = sign(y0). The optimal discriminator’s parameters are
0 = sign(y0) × 1, Ψ∗
D(x) = −1⊤σ(x × sign(y0) × 1)
Without loss of generality, assume sign(y0) = 1.
D(x) = −1⊤σ(x × 1) = −nσ(x)
Because σ is monotonic, D(x) is monotonic.
Optimizing the performance of ˆD pushes it toward ˆD∗,
ˆD monotonic (Fig. 8a - 8e). This explains the
directional monotonicity of discriminators in Fig. 1a-1d, 2.
Although the discriminator in Fig. 8f minimizes the score
of the current fake datapoint, it assigns high scores to (old)
fake datapoints on the left of the real datapoint, i.e. it forgets
these datapoints. If the discriminator is ﬁxed, then minimizing
corresponds to moving θ to −∞. Dirac GAN with a
monotonic discriminator does not converge. When the generator
and discriminator are trained with alternating SGD, the two
players oscillate around the equilibrium (Fig. 8a - 8e).
The problem can be alleviated if one old fake datapoint is
added to the training dataset. Fig. 8g - 8j shows that when
old fake example is added, Dirac GAN has better convergence
behavior (the small ﬂuctuation is due to the large constant
learning rate of 0.1). The discriminator at iteration 10 has a
global maximum at the origin. If the discriminator is ﬁxed,
then θ will converge to 0. The experiment suggests that
information about previous model distributions helps GANs
converge. used a buffer of recent old fake samples to
reﬁne reasonably good fake samples. Recent old fake samples
reduce the oscillation around the equilibrium, helping GANs to
converge faster and produce sharper images. However, because
the number of samples needed to capture the statistics of a
distribution grows exponentially with it dimensionality, storing
old fake datapoints is not efﬁcient for high dimensional data. In
the next section, we study more efﬁcient methods for preserving
information about old distributions.
(b) Land. 5000
(c) Land. 10000
(d) Land. 20000
(e) Gen. 5000
(f) Gen. 10000
(g) Gen. 20000
(i) Land. 5000
(j) Land. 10000
(k) Land. 20000
(l) Gen. 5000
(m) Gen. 10000
(n) Gen. 20000
Fig. 9: Result on CelebA. (a) - (g) DCGAN-NS. (h) - (n)
(b) Land. 5000
(c) Land. 10000
(d) Land. 20000
(e) Gen. 5000
(f) Gen. 10000
(g) Gen. 20000
(i) Land. 5000
(j) Land. 10000
0.00000025
0.00000050
0.00000075
0.00000100
0.00000125
0.00000150
0.00000175
(k) Land. 20000
(l) Gen. 5000
(m) Gen. 10000
(n) Gen. 20000
Fig. 10: Result on CIFAR-10. (a) - (g) DCGAN-NS. (h) - (n)
DCGAN-imba, γ = 10.
V. PREVENTING CATASTROPHIC FORGETTING
Based on the reasons identiﬁed in Section III-B, we propose
the following ways to address CF problem:
1) Preserve and use information from previous tasks in the
current task.
2) Introduce prior knowledge to the game in a way such
that old knowledge is useful for the new task and is not
erased by the new task.
2.054/0.913
DCGAN-imba, γ = 10
3.381/0.078
DCGAN-0GP, λ = 100
2.705/0.901
DCGAN-0GP-imba, λ = 100, γ = 10
3.038/0.342
TABLE III: Inception scores of models at iteration 50k. The
result is averaged over 10 different runs.
Fig. 11: Score of ﬁxed fake images during training from
iteration 10000 to 200000. The same MLP in Fig. 2 was trained
with SGD with learning rate 3e −4. (a) - (b) GAN-NS. (c) -
(d) GAN-0GP with λ = 100. GAN-NS assigns random scores
to the same fake image, implying that it does not remember
information about this fake sample. GAN-0GP is much more
stable and consistently assigns scores lower than 0.5 to old
fake samples.
A. Preserving and using old information
Optimizers with momentum. The update rule of SGD with
γgt−1 + η∇t
The momentum term γgt−1 is a simple form of memory that
carries gradient information from previous training iterations
to the current iteration. When the discriminator/generator is
updated with gt, the performance of the network on previous
tasks is also improved. The effectiveness of momentum in
preventing CF is demonstrated in Fig. 1h: the discriminator’s
gradient pattern is more stable and similar to those of GAN-
0GP and GAN-R1.
Continual learning algorithms such as EWC and online
EWC prevent important knowledge of previous tasks from
being overwritten by the new task. At the end of a task T t,
online EWC computes the importance ˆωt
i of each parameter θt
to the task and adds a regularization term to the loss function
of task T t+1:
i + (1 −α)ωt−1
i is the value of θi at the end of task T t, α balances
the importance of the current task and previous tasks, ωt
accumulates the importance of θi throughout the training
process. Because consecutive model distributions are similar,
we consider a chunk of τ distributions as a task to the
discriminator. The importance ωi is computed every τ GAN
training iteration. The regularizer prevents important weights
from deviating too far from the values that are optimal to
previous tasks while allowing less important weights to change
more freely. It helps the discriminator preserves important
information about old distributions. Liang et al. independently
proposed a similar way of adapting continual learning methods
to GANs. Experiments in the paper showed that continual
learning methods improve the quality of GANs.
B. Introducing prior knowledge to the game
In Dirac GAN, if the discriminator has a local maximum at
the real datapoint then it can always classify the real and
the fake datapoint correctly, regardless of location of the
fake datapoint. Because separating different fake distributions
from the target distribution requires the same knowledge,
that knowledge will not be erased from the discriminator.
We want to introduce to the game the knowledge that real
datapoints should be local maxima. R1 and 0GP are two ways
to implement that.
R1 regularizer (the third row in Table I) forces the gradients
w.r.t. a real datapoint to be 0, making it a local extremum of
the discriminator. As the discriminator maximizes the score of
real datapoints, real datapoints become local maxima of the
discriminator. Fig. 1e - 1g shows that real datapoints are always
local maxima and the gradient pattern of the discriminator stay
unchanged as pg moves toward pr. Fig. 5 demonstrates the
same effect of R1 on MNIST. Note that noisy images that are
far away from the real images (e.g. x+k ˆu for k < −50) have
higher scores than real images. This is because no regularizer
is applied to these noisy images.
0GP regularizer (the forth row in Table I) pushes gradients
w.r.t. datapoints on the line connecting a real datapoint x
and a fake datapoint y toward 0. 0GP forces the score to
increase gradually as we move from y to x. During training,
x is paired with different yi. Thus, the score D(x) is greater
than the scores of fake datapoints in a wider neighborhood.
That ﬁxes the problem of R1 and creates wider local maxima
(Fig. 4, 9). Thanh-Tung et al. showed that GAN-0GP
generalizes better than GAN-R1. Although generalization is
beyond the scope of this paper, we believe that the sharpness
of the discriminator’s landscape is related to its generalization
capability. Prior works on generalization of neural networks
 showed ﬂat (wide) minima of the loss surface generalize
better than sharp minima. Creating discriminators with wide
local maxima is a good way to improve GANs’ generalizability.
WGAN-GP (the ﬁrst row in Table I) uses 1-centered gradient
penalty (1GP) which pushes gradients w.r.t. datapoints on the
line connecting a real datapoint x and a fake datapoint y
toward 1, forcing the score to increase gradually from y to
x. Fig. 6 shows that real datapoints are local maxima of the
discriminator. Wu et al. showed that WGAN-0GP performs
slightly better than WGAN-1GP. Our hypothesis is that 0GP
creates wider maxima than 1GP as it make the score on the
line from y to x to change more slowly.
Imbalanced weights for real and fake samples. To prevent
the discriminator from forgetting distant real datapoints, we
propose to increase the weight of the loss for real datapoints:
LD = γLreal + Lfake
where γ > 1 is an empirically chosen hyper parameter,
Lreal, Lfake are the losses for real and fake samples, respectively. When γ > 1, the discriminator is penalized more
if it assigns a low score to a real datapoint. The situation
where real datapoints are local minima like in Fig. 10b or have
low scores like in the blue boxes in Fig. 1a - 1b will less
likely to happen. Fig. 10k shows that the new loss successfully
helps the discriminator to make more real datapoints local
maxima and thus improve fake samples’ quality. Table III shows
the effectiveness of imbalanced loss on CIFAR-10 dataset: it
signiﬁcantly improves Inception Score and reduces the
score’s variance. The imbalanced loss is orthogonal to gradient
penalties and can be used to improve gradient penalties (the
last two rows in Table III).
VI. CONCLUSION
Catastrophic forgetting is a important problem in GANs.
It is directly related to mode collapse and non-convergence.
Addressing catastrophic forgetting leads to better convergence
and less mode collapse. Methods such as imbalanced loss,
zero centered gradient penalties, optimizers with momentum,
and continual learning are effective at preventing catastrophic
forgetting in GANs. 0GP helps GANs to converge to good
local equilibria where real datapoints are wide local maxima of
the discriminator. The gradient penalty is a promising method
for improving generalizability of GANs.