TransFuse: Fusing Transformers and CNNs for
Medical Image Segmentation
Yundong Zhang‚ãÜ1, Huiye Liu‚ãÜ1,2 (B), and Qiang Hu1
1 Rayicer, Suzhou, China
 
2 Georgia Institute of Technology, Atlanta, GA, USA
Abstract. Medical image segmentation - the prerequisite of numerous
clinical needs - has been signiÔ¨Åcantly prospered by recent advances in
convolutional neural networks (CNNs). However, it exhibits general limitations on modeling explicit long-range relation, and existing cures, resorting to building deep encoders along with aggressive downsampling
operations, leads to redundant deepened networks and loss of localized
details. Hence, the segmentation task awaits a better solution to improve
the eÔ¨Éciency of modeling global contexts while maintaining a strong
grasp of low-level details. In this paper, we propose a novel parallelin-branch architecture, TransFuse, to address this challenge. TransFuse
combines Transformers and CNNs in a parallel style, where both global
dependency and low-level spatial details can be eÔ¨Éciently captured in
a much shallower manner. Besides, a novel fusion technique - BiFusion
module is created to eÔ¨Éciently fuse the multi-level features from both
branches. Extensive experiments demonstrate that TransFuse achieves
the newest state-of-the-art results on both 2D and 3D medical image
sets including polyp, skin lesion, hip, and prostate segmentation, with
signiÔ¨Åcant parameter decrease and inference speed improvement.
Keywords: Medical Image Segmentation ¬∑ Transformers ¬∑ Convolutional
Neural Networks ¬∑ Fusion
Introduction
Convolutional neural networks (CNNs) have attained unparalleled performance
in numerous medical image segmentation tasks , such as multi-organ segmentation, liver lesion segmentation, brain 3D MRI, etc., as it is proved to be
powerful at building hierarchical task-speciÔ¨Åc feature representation by training
the networks end-to-end. Despite the immense success of CNN-based methodologies, its lack of eÔ¨Éciency in capturing global context information remains a
challenge. The chance of sensing global information is equaled by the risk of
eÔ¨Éciency, because existing works obtain global information by generating very
large receptive Ô¨Åelds, which requires consecutively down-sampling and stacking
‚ãÜThese authors contributed equally to this work.
 
Y. Zhang et al.
convolutional layers until deep enough. This brings several drawbacks: 1) training of very deep nets is aÔ¨Äected by the diminishing feature reuse problem ,
where low-level features are washed out by consecutive multiplications; 2) local
information crucial to dense prediction tasks, e.g., pixel-wise segmentation, is
discarded, as the spatial resolution is reduced gradually; 3) training parameterheavy deep nets with small medical image datasets tends to be unstable and
easily overÔ¨Åtting. Some studies use the non-local self-attention mechanism
to model global context; however, the computational complexity of these modules typically grows quadratically with respect to spatial size, thus they may
only be appropriately applied to low-resolution maps.
Transformer, originally used to model sequence-to-sequence predictions in
NLP tasks , has recently attracted tremendous interests in the computer
vision community. The Ô¨Årst purely self-attention based vision transformers (ViT)
for image recognition is proposed in , which obtained competitive results on
ImageNet with the prerequisite of being pretrained on a large external dataset.
SETR replaces the encoders with transformers in the conventional encoderdecoder based networks to successfully achieve state-of-the-art (SOTA) results
on the natural image segmentation task. While Transformer is good at modeling
global context, it shows limitations in capturing Ô¨Åne-grained details, especially
for medical images. We independently Ô¨Ånd that SETR-like pure transformerbased segmentation network produces unsatisfactory performance, due to lack
of spatial inductive-bias in modelling local information (also reported in ).
To enjoy the beneÔ¨Åt of both, eÔ¨Äorts have been made on combining CNNs with
Transformers, e.g., TransUnet , which Ô¨Årst utilizes CNNs to extract low-level
features and then passed through transformers to model global interaction. With
skip-connection incorporated, TransUnet sets new records in the CT multi-organ
segmentation task. However, past works mainly focus on replacing convolution
with transformer layers or stacking the two in a sequential manner. To further
unleash the power of CNNs plus Transformers in medical image segmentation, in
this paper, we propose a diÔ¨Äerent architecture‚ÄîTransFuse, which runs shallow
CNN-based encoder and transformer-based segmentation network in parallel, followed by our proposed BiFusion module where features from the two branches
are fused together to jointly make predictions. TransFuse possesses several advantages: 1) both low-level spatial features and high-level semantic context can
be eÔ¨Äectively captured; 2) it does not require very deep nets, which alleviates
gradient vanishing and feature diminishing reuse problems; 3) it largely improves eÔ¨Éciency on model sizes and inference speed, enabling the deployment
at not only cloud but also edge. To the best of our knowledge, TransFuse is the
Ô¨Årst parallel-in-branch model synthesizing CNN and Transformer. Experiments
demonstrate the superior performance against other competing SOTA works.
Proposed Method
As shown in Fig. 1, TransFuse consists of two parallel branches processing information diÔ¨Äerently: 1) CNN branch, which gradually increases the receptive Ô¨Åeld
TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation
and encodes features from local to global; 2) Transformer branch, where it starts
with global self-attention and recovers the local details at the end. Features with
same resolution extracted from both branches are fed into our proposed BiFusion Module, where self-attention and bilinear Hadamard product are applied
to selectively fuse the information. Then, the multi-level fused feature maps are
combined to generate the segmentation using gated skip-connection . There
Linear Projection
Transformer
Transformer
Transformer Branch
(H/4, W/4)
(H/8, W/8)
(H/4, W/4)
(H/8, W/8)
(H/16, W/16)
CNN Branch
ùêøùê∫, head(ùê≠2)
ùêøùê∫, head(ùêü0)
ùêøùê∫, head(·àòùêü2)
Pred. Head
AG Attn. Gate
Fig. 1: Overview of TransFuse (best viewed in color): two parallel branches - CNN
(bottom right) and transformer (left) fused by our proposed BiFusion module.
are two main beneÔ¨Åts of the proposed branch-in-parallel approach: Ô¨Årstly, by
leveraging the merits of CNNs and Transformers, we argue that TransFuse can
capture global information without building very deep nets while preserving
sensitivity on low-level context; secondly, our proposed BiFusion module may simultaneously exploit diÔ¨Äerent characteristics of CNNs and Transformers during
feature extraction, thus making the fused representation powerful and compact.
Transformer Branch. The design of Transformer branch follows the typical encoder-decoder architecture. SpeciÔ¨Åcally, the input image x ‚ààRH√óW √ó3 is
Ô¨Årst evenly divided into N = H
S patches, where S is typically set to 16. The
patches are then Ô¨Çattened and passed into a linear embedding layer with output
dimension D0, obtaining the raw embedding sequence e ‚ààRN√óD0. To utilize
the spatial prior, a learnable positional embeddings of the same demension is
added to e. The resulting embeddings z0 ‚ààRN√óD0 is the input to Transformer
encoder, which contains L layers of multiheaded self-attention (MSA) and Multilayer Perceptron (MLP). We highlight that the self-attention (SA) mechanism,
which is the core principal of Transformer, updates the states of each embedded
patch by aggregating information globally in every layer:
SA(zi) = softmax
Y. Zhang et al.
where [q, k, v] = zWqkv, Wqkv ‚ààRD0√ó3Dh is the projection matrix and vector
zi ‚ààR1√óD0, qi ‚ààR1√óDh are the ith row of z and q, respectively. MSA is an extension of SA that concatenates multiple SAs and projects the latent dimension
back to RD0, and MLP is a stack of dense layers (refer to for details of MSA
and MLP). Layer normalization is applied to the output of the last transformer
layer to obtain the encoded sequence zL ‚ààRN√óD0. For the decoder part, we use
progressive upsampling (PUP) method, as in SETR . SpeciÔ¨Åcally, we Ô¨Årst
reshape zL back to t0 ‚ààR
16 √óD0, which could be viewed as a 2D feature map
with D0 channels. We then use two consecutive standard upsampling-convolution
layers to recover the spatial resolution, where we obtain t1 ‚ààR
4 √óD2, respectively. The feature maps of diÔ¨Äerent scales t0, t1 and t2
are saved for late fusion with corresponding feature maps of the CNN branch.
CNN Branch. Traditionally, features are progressively downsampled to
32 and hundreds of layers are employed in deep CNNs to obtain global
context of features, which results in very deep models draining out resources.
Considering the beneÔ¨Åts brought by Transformers, we remove the last block
from the original CNNs pipeline and take advantage of the Transformer branch
to obtain global context information instead. This gives us not only a shallower
model but also retaining richer local information. For example, ResNet-based
models typically have Ô¨Åve blocks, each of which downsamples the feature maps
by a factor of two. We take the outputs from the 4th (g0 ‚ààR
16 √óC0), 3rd
8 √óC1) and 2nd (g2 ‚ààR
4 √óC2) blocks to fuse with the results from
Transformer (Fig. 1). Moreover, our CNN branch is Ô¨Çexible that any oÔ¨Ä-the-shelf
convolutional network can be applied.
BiFusion Module. To eÔ¨Äectively combine the encoded features from CNNs
and Transformers, we propose a new BiFusion module (refer to Fig. 1) that incorporates both self-attention and multi-modal fusion mechanisms. SpeciÔ¨Åcally, we
obtain the fused feature representation f i, i = 0, 1, 2 by the following operations:
ÀÜti = ChannelAttn(ti)
ÀÜbi = Conv(tiWi
ÀÜgi = SpatialAttn(gi)
f i = Residual([ÀÜbi,ÀÜti, ÀÜgi])
1 ‚ààRDi√óLi, W i
2 ‚ààRCi√óLi, | ‚äô| is the Hadamard product and Conv
is a 3x3 convolution layer. The channel attention is implemented as SE-Block
proposed in to promote global information from the Transformer branch.
The spatial attention is adopted from CBAM block as spatial Ô¨Ålters to
enhance local details and suppress irrelevant regions, as low-level CNN features
could be noisy. The Hadamard product then models the Ô¨Åne-grained interaction
between features from the two branches. Finally, the interaction features ÀÜbi
and attended features ÀÜti, ÀÜgi are concatenated and passed through a Residual
block. The resulting feature f i eÔ¨Äectively captures both the global and local
context for the current spatial resolution. To generate Ô¨Ånal segmentation, f is
are combined using the attention-gated (AG) skip-connection , where we
have ÀÜf i+1 = Conv([Up(ÀÜf i), AG(f i+1, Up(ÀÜf i))]) and ÀÜf 0 = f 0, as in Fig. 1.
Loss Function. The full network is trained end-to-end with the weighted
IoU loss and binary cross entropy loss L = Lw
bce, where boundary pix-
TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation
els receive larger weights . Segmentation prediction is generated by a simple head, which directly resizes the input feature maps to the original resolution and applies convolution layers to generate M maps, where M is the
number of classes. Following , We use deep supervision to improve the gradient Ô¨Çow by additionally supervising the transformer branch and the Ô¨Årst
fusion branch. The Ô¨Ånal training loss is given by L = Œ±L
G, head(ÀÜf 2)
 G, head(t2)
 G, head(f 0)
, where Œ±, Œ≥, Œ≤ are tunnable hyperparameters and G is groundtruth.
Experiments and Results
Data Acquisition. To better evaluate the eÔ¨Äectiveness of TransFuse, four
segmentation tasks with diÔ¨Äerent imaging modalities, disease types, target objects, target sizes, etc. are considered: 1) Polyp Segmentation, where Ô¨Åve public
polyp datasets are used: Kvasir , CVC-ClinicDB , CVC-ColonDB ,
EndoScene and ETIS . The same split and training setting as described
in are adopted, i.e. 1450 training images are solely selected from Kvasir
and CVC-ClinicDB while 798 testing images are from all Ô¨Åve datasets. Before
processing, the resolution of each image is resized into 352√ó352 as . 2)
Skin Lesion Segmentation, where the publicly available 2017 International Skin
Imaging Collaboration skin lesion segmentation dataset (ISIC2017) is used3.
ISIC2017 provides 2000 images for training, 150 images for validation and 600
images for testing. Following the setting in , we resize all images to 192√ó256.
3) Hip Segmentation, where a total of 641 cases are collected from a hospital
with average size of 2942√ó2449 and pixel spacing as 0.143mm4. Each image is
annotated by a clinical expert and double-blind reviewed by two specialists. We
resized all images into 352√ó352, and randomly split images with a ratio of 7:1:2
for training, validation and testing. 4)Prostate Segmentation, where volumetric
Prostate Multi-modality MRIs from the Medical Segmentation Decathlon 
are used. The dataset contains multi-modal MRIs from 32 patients, with a median volume shape of 20√ó320√ó319. Following the setting in , we reshape all
MRI slices to 320 √ó 320, and independently normalize each volume using z-score
normalization.
Implementation Details. TransFuse was built in PyTorch framework and
trained using a single NVIDIA-A100 GPU. The values of Œ±, Œ≤ and Œ≥ were set to
0.5, 0.3, 0.2 empirically. Adam optimizer with learning rate of 1e-4 was adopted
3 Another similar dataset ISIC2018 was not used because of the missing test set annotation, which makes fair comparison between existing works can be hardly achieved.
4 All data are from diÔ¨Äerent patients and with ethics approval, which consists of 267
patients of Avascular Necrosis, 182 patients of Osteoarthritis, 71 patients of Femur
Neck Fracture, 33 patients of Pelvis Fracture, 26 patients of Developmental Dysplasia
of the Hip and 62 patients of other dieases.
Y. Zhang et al.
and all models were trained for 30 epochs as well as batch size of 16, unless
otherwise speciÔ¨Åed.
In polyp segmentation experiments, no data augmentation was used except
for multi-scale training, as in . For skin lesion and hip segmentation, data
augmentation including random rotation, horizontal Ô¨Çip, color jittering, etc. were
applied during training. A smaller learning rate of 7e-5 was found useful for skin
lesion segmentation. Finally, we follow the nnU-Net framework to train and
evaluate our model on Prostate Segmentation, using the same data augmentation and post-processing scheme. As selected pretrained datasets and branch
backbones may aÔ¨Äect the performance diÔ¨Äerently, three variants of TransFuse
are provided to 1) better demonstrate the eÔ¨Äectiveness as well as Ô¨Çexibility of
our approach; 2) conduct fair comparisons with other methods. TransFuse-S
is implemented with ResNet-34 (R34) and 8-layer DeiT-Small (DeiT-S) as
backbones of the CNN branch and Transformer branch respectively. Similarly,
TransFuse-L is built based on Res2Net-50 and 10-layer DeiT-Base (DeiT-B),
while TransFuse-L* uses ResNetV2-50 and ViT-B . Note that ViTs and DeiTs
have the same backbone architecture and they mainly diÔ¨Äer in the pre-trained
strategy and dataset: the former is trained on ImageNet21k while the latter is
trained on ImageNet1k with heavier data augmentation.
Evaluation Results TransFuse is evaluated on both 2D and 3D datasets to
demonstrate the eÔ¨Äectiveness. As diÔ¨Äerent medical image segmentation tasks
serve diÔ¨Äerent diagnosis or operative purposes, we follow the commonly used
evaluation metrics for each of the segmentation tasks to quantitatively analyze
the results. Selected visualization results of TransFuse-S are shown in Fig. 2.
Results of Polyp Segmentation. We Ô¨Årst evaluate the performance of
our proposed method on polyp segmentation against a variety of SOTA methods, in terms of mean Dice (mDice) and mean Intersection-Over-Union (mIoU).
As in Tab. 1, our TransFuse-S/L outperform CNN-based SOTA methods by a
large margin. SpeciÔ¨Åcally, TransFuse-S achieves 5.2% average mDice improvement on the unseen datasets (ColonDB, EndoSene and ETIS). Comparing to
other transformer-based methods, TransFuse-L* also shows superior learning
ability on Kvasir and ClinicDB, observing an increase of 1.3% in mIoU compared to TransUnet. Besides, the eÔ¨Éciency in terms of the number of parameters as well as inference speed is evaluated on an RTX2080Ti with Xeon(R) Gold
5218 CPU. Comparing to prior CNN-based arts, TransFuse-S achieves the best
performance while using only 26.3M parameters, about 20% reduction with respect to HarDNet-MSEG (33.3M) and PraNet (32.5M). Moreover, TransFuse-S
is able to run at 98.7 FPS, much faster than HarDNet-MSEG (85.3 FPS) and
PraNet (63.4 FPS), thanks to our proposed parallel-in-branch design. Similarly,
TransFuse-L* not only achieves the best results compared to other Transformerbased methods, but also runs at 45.3 FPS, about 12% faster than TransUnet.
Results of Skin Lesion Segmentation. The ISBI 2017 challenge ranked
methods according to Jaccard Index on the ISIC 2017 test set. Here, we use
Jaccard Index, Dice score and pixel-wise accuracy as evaluation metrics. The
TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation
Table 1: Quantitative results on polyp segmentation datasets compared to previous SOTAs. The results of is obtained by running the released code and we
implement SETR-PUP. ‚Äò-‚Äô means results not available.
mDice mIoU mDice mIoU mDice mIoU mDice mIoU mDice mIoU
U-Net 
0.818 0.746 0.823 0.750 0.512 0.444 0.710 0.627 0.398 0.335
U-Net++ 
0.821 0.743 0.794 0.729 0.483 0.410 0.707 0.624 0.401 0.344
ResUNet++ 
0.813 0.793 0.796 0.796
PraNet 
0.898 0.840 0.899 0.849 0.709 0.640 0.871 0.797 0.628 0.567
HarDNet-MSEG 0.912 0.857 0.932 0.882 0.731 0.660 0.887 0.821 0.677 0.613
TransFuse-S
0.918 0.868 0.918 0.868 0.773 0.696 0.902 0.833 0.733 0.659
TransFuse-L
0.918 0.868 0.934 0.886 0.744 0.676 0.904 0.838 0.737 0.661
SETR-PUP 
0.911 0.854 0.934 0.885 0.773 0.690 0.889 0.814 0.726 0.646
TransUnet 
0.913 0.857 0.935 0.887 0.781 0.699 0.893 0.824 0.731 0.660
TransFuse-L*
0.920 0.870 0.942 0.897 0.781 0.706 0.894 0.826 0.737 0.663
Table 2: Quantitative results on ISIC
2017 test set. Results with backbones use
weights pretrained on ImageNet.
Epochs Jaccard Dice Accuracy
DCL-PSI 
ResNet-101
SLSDeep 
Unet++ 
TransFuse-S R34+DeiT-S
Results on in-house hip
dataset. All models use pretrained
backbones from ImageNet and are
of similar size (‚àº26M). HD and
ASD are measured in mm.
Unet++ 
14.4 1.21 9.33 0.932 5.04 0.813
HRNetV2 14.2 1.13 6.36 0.769 5.98 0.762
TransFuse-S
9.81 1.09 4.44 0.767 4.19 0.676
comparison results against leading methods are presented in Tab. 2. TransFuse-
S is about 1.7% better than the previous SOTA SLSDeep in Jaccard score,
without any pre- or post-processing and converges in less than 1/3 epochs. Besides, our results outperform Unet++ that employs pretrained R34 as backbone and has comparable number of parameters with TransFuse-S (26.1M vs
26.3M). Again, the results prove the superiority of our proposed architecture.
Results of Hip Segmentation. Tab. 3 shows our results on hip segmentation task, which involves three human body parts: Pelvis, Left Femur (L-Femur)
and Right Femur (R-Femur). Since the contour is more important in dianosis
and THA preoperative planning, we use HausdorÔ¨ÄDistance (HD) and Average
Surface Distance (ASD) to evaluate the prediction quality. Compared to the
two advanced segmentation methods , TransFuse-S performs the best on
both metrics and reduces HD signiÔ¨Åcantly (30% compared to HRNetV2 as well
as 34% compared to Unet++ on average), indicating that our proposed method
is able to capture Ô¨Åner structure and generates more precise contour.
Results of Prostate Segmentation. We compare TransFuse-S with nnU-
Net , which ranked 1st in the prostate segmentation challenge . We follow
the same preprocessing, training as well as evaluation schemes of the publicly
Y. Zhang et al.
Table 4: Quantitative results on prostate MRI segmentation. PZ, TZ stand for
the two labeled classes (peripheral and transition zone) and performance (PZ,
TZ and mean) is measure by dice score.
Params Throughput
nnUnet-2d 
0.6285 0.8380 0.7333 29.97M
0.209s/vol
nnUnet-3d full 0.6663 0.8410 0.7537 44.80M
0.381s/vol
TransFuse-S
0.6738 0.8539 0.7639 26.30M 0.192s/vol
Ablation study on parallel-inbranch design. Res: Residual.
Composition
Kvasir ColonDB
Sequential
Sequential
R34+DeiT-S
Sequential
R34+DeiT-S
Concat+Res 0.912
R34+DeiT-S
Table 6: Ablation study on BiFusion module. Res: Residual; TFM:
Transformer; Attn: Attention.
Jaccard Dice Accuracy
Concat+Res
+CNN Spatial Attn
+TFM Channel Attn
+Dot Product
available nnU-Net framework5 and report the 5-fold cross validation results in
Tab. 4. We can Ô¨Ånd that TransFuse-S surpasses nnUNet-2d by a large margin
(+4.2%) in terms of the mean dice score. Compared to nnUNet-3d, TransFuse-S
not only achieves better performance, but also reduces the number of parameters
by ‚àº41% and increases the throughput by ‚àº50% (on GTX1080).
Ablation Study. An ablation study is conducted to evaluate the eÔ¨Äectiveness of the parallel-in-branch design as well as BiFusion module by varying design
choices of diÔ¨Äerent backbones, compositions and fusion schemes. A seen (Kvasir)
and an unseen (ColonDB) datasets from polyp are used, and results are recorded
in mean Dice. In Tab. 5, by comparing E.3 against E.1 and E.2, we can see that
combining CNN and Transformer leads to better performance. Further, by comparing E.3 against E.5, E.6, we observe that the parallel models perform better
than the sequential counterpart. Moreover, we evaluate the performance of a
double branch CNN model (E.4) using the same parallel structure and fusion
settings with our proposed E.6. We observe that E.6 outperforms E.4 by 2.2%
in Kvasir and 18.7% in ColonDB, suggesting that the CNN branch and transformer branch are complementary to each other, leading to better fusion results.
Lastly, performance comparison is conducted between another fusion module
comprising concatenation followed by a residual block and our proposed BiFusion module (E.5 and E.6). Given the same backbone and composition setting,
E.6 with BiFusion achieves better results. Additional experiments conducted on
ISIC2017 are presented in Tab. 6 to verify the design choice of BiFusion module,
from which we Ô¨Ånd that each component shows its unique beneÔ¨Åt.
5 
TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation
Polyp Segmentation
Skin Lesion Segmentation
Hip Segmentation
Prostate Segmentation
Fig. 2: Results visualization on all three tasks (best viewed in color). Each row
follows the repeating sequence of ground truth (GT) and predictions (Pred).
Conclusion
In this paper, we present a novel strategy to combine Transformers and CNNs
with late fusion for medical image segmentation. The resulting architecture,
TransFuse, leverages the inductive bias of CNNs on modeling spatial correlation
and the powerful capability of Transformers on modelling global relationship.
TransFuse achieves SOTA performance on a variety of segmentation tasks whilst
being highly eÔ¨Écient on both the parameters and inference speed. We hope that
this work can bring a new perspective on using transformer-based architecture.
In the future, we plan to improve the eÔ¨Éciency of the vanilla transformer layer as
well as test TransFuse on other medical-related tasks such as landmark detection
and disease classiÔ¨Åcation.
Acknowledgement. We gratefully thank Weijun Wang, MD, Zhefeng Chen,
MD, Chuan He, MD, Zhengyu Xu, Huaikun Xu for serving as our medical advisors on hip segmentation project.
Y. Zhang et al.