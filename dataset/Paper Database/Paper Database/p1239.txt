DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation
Xiaojuan Qi
Pheng-Ann Heng
Department of Computer Science and Engineering
The Chinese University of Hong Kong
{hchen, xjqi, lqyu, pheng}@cse.cuhk.edu.hk
The morphology of glands has been used routinely by
pathologists to assess the malignancy degree of adenocarcinomas. Accurate segmentation of glands from histology
images is a crucial step to obtain reliable morphological
statistics for quantitative diagnosis. In this paper, we proposed an efﬁcient deep contour-aware network (DCAN) to
solve this challenging problem under a uniﬁed multi-task
learning framework. In the proposed network, multi-level
contextual features from the hierarchical architecture are
explored with auxiliary supervision for accurate gland segmentation. When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved. Moreover, our
network can not only output accurate probability maps of
glands, but also depict clear contours simultaneously for
separating clustered objects, which further boosts the gland
segmentation performance. This uniﬁed framework can be
efﬁcient when applied to large-scale histopathological data
without resorting to additional steps to generate contours
based on low-level cues for post-separating. Our method
won the 2015 MICCAI Gland Segmentation Challenge out
of 13 competitive teams, surpassing all the other methods
by a signiﬁcant margin.
1. Introduction
Normally, a typical gland is composed of a lumen area
forming the interior tubular structure and epithelial cell nuclei surrounding the cytoplasm, as illustrated in Figure 1
(top left). Malignant tumours arising from glandular epithelium, also known as adenocarcinomas, are the most prevalent form of cancer. In the routine of histopathological examination, the morphology of glands has been widely used
for assessing the malignancy degree of several adenocarcinomas, including breast , prostate , and colon .
Accurate segmentation of glands is one crucial pre-requisite
step to obtain reliable morphological statistics that indicate
the aggressiveness of tumors. Conventionally, this is perlumen
Figure 1: Examples of gland segmentation in benign (top
row) and malignant (bottom row) cases.
From left to
right columns show the original images (stained with hematoxylin and eosin) and annotations by pathologists (individual objects are denoted by different colors), respectively.
formed by expert pathologists who evaluate the structure of
glands in the biopsy samples. However, manual annotation
suffers from issues such as limited reproducibility, considerable efforts, and time-consuming. With the advent of whole
slide imaging, large-scale histopathological data need to be
analyzed. Therefore, automatic segmentation methods are
highly demanded in clinical practice to improve the efﬁciency as well as reliability and reduce the workload on
pathologists.
Nevertheless, this task is quite challenging for several
reasons. First, there is a huge variation of glandular morphology depending on the different histologic grades as
well as from one disease to another. Figure 1 (left column)
shows the large difference of glandular structures between
benign and malignant cases from colon tissues. Second,
the existence of touching glands in tissue samples makes
it quite hard for automated methods to separate objects individually. Third, in the malignant cases such as moderately and poorly differentiated adenocarcinomas, the glandular structures are seriously degenerated, as shown in Fig-
 
ure 1 (bottom left). Therefore, methods utilizing the prior
knowledge with glandular regularity are prone to fail in such
cases . In addition, the variation of tissue preparation
procedures such as sectioning and staining can cause deformation, artifacts and inconsistency of tissue appearance,
which can impede the segmentation process as well.
In the last few years, many researchers have devoted their
efforts to addressing this challenging problem and achieved
a considerable progress.
However, obvious performance
gap is still observed between the results given by the algorithms and annotations from pathologists. Broadly speaking, previous studies in the literature can be categorized
into two classes: (1) pixel based methods. For this kind
of method, various hand-crafted features including texture,
color, morphological cues and Haar-like features were utilized to detect the glandular structure from histology images ; (2) structure based
methods. Most of approaches in this category take advantage of prior knowledge about the glandular structure, such
as graph based methods , glandular boundary delineation with geodesic distance transform , polar space
random ﬁeld model , stochastic polygons model ,
Although these methods achieved promising results
in cases of adenoma and well differentiated (low grade)
adenocarcinoma, they may fail to achieve satisfying performance in malignant subjects, where the glandular structures
are seriously deformed. Recently, deep neural networks are
driving advances in image recognition related tasks in computer vision and medical image computing . The most relevant study to our
work is the U-net that designed a U-shaped deep convolutional network for biomedical image segmentation and won
several grand challenges recently .
In this paper, we propose a novel deep contour-aware
network to solve this challenging problem.
Our method
tackles three critical issues for gland segmentation. First,
our method harnesses multi-level contextual feature representations in an end-to-end way for effective gland segmentation. Leveraging the fully convolutional networks, it can
take an image as input and output the probability map directly with one single forward propagation. Hence, it’s very
efﬁcient when applied to large-scale histopathological image analysis. Second, because our method doesn’t make an
assumption about glandular structure, it can be easily generalized to biopsy samples with different histopathological
grades including benign and malignant cases. Furthermore,
instead of treating the segmentation task independently,
our method investigates the complementary information,
i.e., gland objects and contours, under a multi-task learning framework. Therefore, it can simultaneously segment
the gland and separate the clustered objects into individual
ones, especially in benign cases with existence of touching glands. Extensive experimental results on the bench-
classifier
Figure 2: The schematic illustration of FCN with multilevel contextual features.
mark dataset of 2015 MICCAI Gland Segmentation Challenge corroborated the effectiveness of our method, yielding
much better performance than other advanced methods.
In this section, we describe in detail the formulation
of our proposed deep contour-aware network for accurate
gland segmentation. We start by introducing the fully convolutional network (FCN) for end-to-end training. Furthermore, we propose to harness the multi-level contextual features with auxiliary supervision for generating good likelihood maps of glands. Then we elaborate the deep contouraware network drawn from FCN for effective gland segmentation by fusing the complementary information of objects and contours. In order to mitigate the challenge of
insufﬁcient training data, we employ the transfer learning
approach by exploiting the knowledge learned from cross
domains to further improve the performance.
2.1. FCN with multi-level contextual features
Fully convolutional networks achieved the state-of-theart performance on image segmentation related tasks [7,
Such great success is mostly attributed to the outstanding capability in feature representation for dense classiﬁcation. The whole network can be trained in an endto-end (image-to-image) way, which takes an image as input and outputs the probability map directly. The architecture basically contains two modules including downsampling path and upsampling path. The downsampling path
contains convolutional and max-pooling layers, which are
extensively used in the convolutional neural networks for
image classiﬁcation tasks . The upsampling path contains convolutional and deconvolutional layers (backwards
strided convolution ), which upsample the feature maps
and output the score masks. The motivation behind this is
that the downsampling path aims at extracting the high level
abstraction information, while the upsampling path predicting the score masks in a pixel-wise way.
The classiﬁcation scores from FCN are established based
on the intensity information from the given receptive ﬁeld.
However, the network with single receptive ﬁeld size can-
݌௢ ሺݔ; ܹ௢, ܹ௦ሻ
݌௖ ሺݔ; ܹ௖, ܹ௦ሻ
Figure 3: The overview of the proposed deep contour-aware network.
not handle the large variation of gland shape properly. For
example, as shown in Figure 1, a small receptive ﬁeld (e.g.,
150 × 150) is suitable for normal glands in benign cases,
while malignant cases usually need a large receptive ﬁeld
since the gland shape in adenocarcinomas are degenerated
and elongated, hence enclosing larger contextual information can help to eliminate ambiguity, suppress the interior
tubular structure, and improve the recognition performance.
Therefore, based on the FCN, we push it further by harnessing multi-level contextual feature representations, which include different levels of contextual information, i.e., intensities appearing in various sizes of receptive ﬁeld.
schematic illustration of FCN with multi-level contextual
feature representations can be seen in Figure 2. Speciﬁcally, the architecture of neural network contains a number
of convolutional layers, 5 max-pooling layers for downsampling and 3 deconvolutional layers for upsampling. With
the network going deeper, the size of receptive ﬁeld is becoming larger. Derived from this, the upsampling layers
are designed deliberately by considering the requirement of
different receptive ﬁeld sizes. They upsample the feature
maps and make predictions based on the contextual cues
from given receptive ﬁeld. Then these predictions are fused
together with a summing operation and ﬁnal segmentation
results based on multi-level contextual features are generated after softmax classiﬁcation.
Direct training a network with such a large depth may
fall into a local minima. Inspired by previous studies on
training neural networks with deep supervision ,
weighted auxiliary classiﬁers C1-C3 are added into the network to further strengthen the training process, as shown
in Figure 2. This can alleviate the problem of vanishing
gradients with auxiliary supervision for encouraging the
back-propagation of gradient ﬂow. Finally, the FCN with
multi-level contextual features extracted from input I can
be trained by minimizing the overall loss L, i.e., a combination of auxiliary loss La(I; W) with corresponding discount weights wa and data error loss Le(I; W) between the
predicted results and ground truth annotation, as shown following:
L(I; W) = λψ(W) +
waLa(I; W) + Le(I; W) (1)
where W denotes the parameters of neural network and
ψ(W) is the regularization term with hyperparameter λ for
balancing the tradeoff with other terms.
2.2. Deep contour-aware network
By harnessing the multi-level contextual features with
auxiliary supervision, the network can produce good probability maps of gland objects. However, it’s still quite hard
to separate the touching glands by leveraging only on the
likelihood of gland objects due to the essential ambiguity
in touching regions. This is rooted in the downsampling
path causing spatial information loss along with feature abstraction. The boundary information formed by epithelial
cell nuclei provides good complementary cues for splitting
objects. To this end, we propose a deep contour-aware network to segment the glands and separate clustered objects
into individual ones.
The overview of the proposed deep contour-aware network can be seen in Figure 3. Instead of treating the gland
segmentation task as a single and independent problem, we
formulate it as a multi-task learning framework by exploring the complementary information, which can infer the results of gland objects and contours simultaneously. Specifically, the feature maps are upsampled with two different
branches (green and blue arrows shown in the ﬁgure) in order to output the segmentation masks of gland objects and
contours, respectively.
In each branch, the mask is predicted by FCN with multi-level contextual features as illustrated in Section 2.1. During the training process, the
parameters of downsampling path Ws are shared and updated for these two tasks jointly, while the parameters of upsampling layers for two individual branches (denoted as Wo
and Wc) are updated independently for inferring the probability of gland objects and contours, respectively. Therefore, the feature representations through the hierarchical
structure can encode the information of segmented objects
and contours at the meantime. Note that the network with
multiple tasks is optimized together in an end-to-end way.
This joint multi-task learning process has several advantages. First, it can increase the discriminative capability of
intermediate feature representations with multiple regularizations on disentangling subtly correlated tasks , hence
improve the robustness of segmentation performance. Second, in the application of gland segmentation, the multitask learning framework can also provide the complementary contour information that serves well to separate the
clustered objects. This can improve the object-level segmentation performance signiﬁcantly, especially in benign
histology images where touching gland objects often exist.
When dealing with large-scale histopathological data, this
uniﬁed framework can be quite efﬁcient. With one forward
propagation, it can generate the results of gland objects and
contours simultaneously instead of resorting to additional
post-separating steps by generating contours based on lowlevel cues .
In the training process, the discount weights wa from
auxiliary classiﬁers are decreased until marginal values with
the number of iterations increasing, therefore we dropped
these terms in the ﬁnal loss for simplicity. Finally the training of network is formulated as a per-pixel classiﬁcation
problem regarding the ground truth segmentation masks including gland objects and contours, as shown following:
Ltotal(x; θ) = λψ(θ) −
log po(x, ℓo(x); Wo, Ws)
log pc(x, ℓc(x); Wc, Ws)
where the ﬁrst part is the L2 regularization term and latter
two are the data error loss terms. x is the pixel position in
image space X, po(x, ℓo(x); Wo, Ws) denotes the predicted
probability for true label ℓo(x) (i.e., the index of 1 in one
hot vector) of gland objects after softmax classiﬁcation, and
similarly pc(x, ℓc(x); Wc, Ws) is the predicted probability
for true label ℓc(x) of gland contours. The parameters θ =
{Ws, Wo, Wc} of network are optimized by minimizing the
total loss function Ltotal with standard back-propagation.
With the predicted probability maps of gland object
po(x; Wc, Ws) and contour pc(x; Wc, Ws) from the deep
contour-aware network, these complementary information
are fused together to generate the ﬁnal segmentation masks
m(x), deﬁned as:
if po(x; Wo, Ws) ≥to and pc(x; Wc, Ws) < tc
where to and tc are the thresholds (set as 0.5 in our experiments empirically). Then, post-processing steps including
smoothing with a disk ﬁlter (radius 3), ﬁlling holes and removing small areas are performed on the fused segmentation results. Finally, each connected component is labeled
with a unique value for representing one segmented gland.
2.3. Transfer learning with rich feature hierarchies
There is a scarcity of medical training data along with
accurate annotations in most situations due to the expensive cost and complicated acquisition procedures.
Compared with the limited data in medical domain, much more
training data can be obtained in the ﬁeld of computer vision. Previous studies have evidenced that transfer learning
in deep convolutional networks can alleviate the problem
of insufﬁcient training data . The learned parameters (convolutional ﬁlters) in the lower layers of network
are general while those in higher layers are more speciﬁc
to different tasks . Thus, transfer the rich feature hierarchies with embedded knowledge learned from plausibly
related datasets could help to reduce overﬁtting on limited
medical dataset and further boost the performance.
Therefore, we utilized an off-the-shelf model from
DeepLab , which was trained on the PASCAL VOC 2012
dataset . Compared to the small scale dataset (a few
hundred images) in gland segmentation, the PASCAL VOC
dataset contains more than ten thousand images with pixellevel annotations. Leveraging the effective generalization
ability of transfer learning in deep neural networks, we initialized the layers in downsampling path with pre-trained
weights from the DeepLab model while the rest layers randomly with Gaussian distribution. Then we ﬁne tuned the
whole network on our medical task in an end-to-end way
with stochastic gradient descent. In our experiments, we
observed the training process converged much faster (about
four hours) by virtue of the prior knowledge learned from
rich dataset than random initialization setting.
3. Experiments and results
3.1. Dataset and pre-processing
We evaluated our method on the public benchmark
dataset of Gland Segmentation Challenge Contest in MIC-
CAI 2015 (also named as Warwick-QU dataset) . The
images were acquired by a Zeiss MIRAX MIDI slide scanner from colorectal cancer tissues with a resolution of
0.62µm/pixel. They consist of a wide range of histologic
grades from benign to malignant subjects. It’s worth noting that poorly-differentiated cases are included to evaluate
the performance of algorithms. The training dataset is composed of 85 images (benign/malignant=37/48) with ground
truth annotations provided by expert pathologists. The testing data contains two sections: Part A (60 images) for of-
ﬂine evaluation and Part B (20 images) for on-site evaluation. For the on-site contest, participants must submit their
results to the organizers within an hour after data release.
The ground truths of testing data are held out by the challenge organizers for independent evaluation. The ﬁnal ranking is based on the evaluation results from testing data Part
A and Part B with an equal weight1. To increase the robustness and reduce overﬁtting, we utilized the strategy of
data augmentation to enlarge the training dataset. The augmentation transformations include translation, rotation, and
elastic distortion (e.g., pincushion and barrel distortions).
3.2. Implementation details
Our framework was implemented under the open-source
deep learning library Caffe . The network randomly
crops a 480 × 480 region from the original image as input
and output the prediction masks of gland objects and contours. The score masks of whole testing image are produced
with an overlap-tile strategy. For the label of contours, we
extracted the boundaries of connected components based on
the gland annotations from pathologists, then dilated them
with a disk ﬁlter (radius 3). In the training phase, the learning rate was set as 0.001 initially and decreased by a factor
of 10 when the loss stopped decreasing till 10−7. The discount weight wa was set as 1 initially and decreased by a
factor of 10 every ten thousand iterations until a marginal
value 10−3. In addition, dropout layers (dropout rate
0.5) were incorporated in the convolutional layers with kernel size 1×1 for preventing the co-adaption of intermediate
3.3. Qualitative evaluation
In order to illustrate the efﬁcacy of our method qualitatively, some segmentation results of testing data are shown
in Figure 4 (benign cases) and Figure 5 (malignant cases),
respectively.
For diagnosing the role of complementary
contour information (i.e., contour-aware component), we
also performed an ablation study and compared the performance of network relying only on the prediction of gland
objects. Qualitative results are shown in Figure 4 and Figure 5 (middle row). From the segmentation results we can
see that the method leveraging the multi-level contextual
features without contour-aware can accurately segment the
gland objects in both benign and malignant cases. However, there are some touching gland objects that cannot be
separated.
The situation is deteriorated when the touching objects are clustered together, as the case shown in
the ﬁrst column of Figure 4.
In comparison, the deep
contour-aware network is capable of separating these touching gland objects clearly. This highlights the superiority of
 
combi/research/bic/glascontest/
deep contour-aware network by exploring the complementary information under a uniﬁed multi-task learning framework qualitatively.
3.4. Quantitative evaluation and comparison
The evaluation criteria in the grand challenge includes
F1 score, object-level Dice index and Hausdorff distance,
which consider the performance of gland detection, segmentation and shape similarity, respectively. Due to limited
submissions in this challenge, we only submitted two entries to probe the performance of our method quantitatively.
They were generated from the deep contour-aware network
illustrated in Figure 3 without and with fusing the contouraware results, denoted as CUMedVision1 and CUMedVision2, respectively.
Detection For the gland detection evaluation, the metric F1
score is utilized, which is the harmonic mean of precision
P and recall R, deﬁned as:
P + R, P =
where Ntp, Nfp, and Nfn denote the number of true positives, false positives, and false negatives, respectively. According to the challenge evaluation, the ground truth for
each segmented object is the object in the manual annotation that has maximum overlap with that segmented object.
A segmented gland object that intersects with at least 50%
of its ground truth is considered as a true positive, otherwise
it’s considered as a false positive. A ground truth gland object that has no corresponding segmented object or has less
than 50% of its area overlapped by its corresponding segmented object is considered as a false negative.
The detection results of different methods in this challenge are shown in Table 1. Note that all the top 5 entries
utilized methods based on the deep convolutional neural
networks. Specially, the method from Freiburg designed
a very deep U-shaped network and achieved the best results
in several grand challenges . This method also explored
the multi-level feature representations by concatenating feature maps from hierarchical layers and weighted loss was
utilized to separate the touching objects.
Our submitted entry CUMedVision1 without fusing the
contour-aware results surpassed all the other methods by a
signiﬁcant margin on testing data Part B, highlighting the
strength of FCN with multi-level contextual feature representations for image segmentation. Our second submitted entry CUMedVision2 with contour-aware component
achieved the best results on testing data Part A and competitive performance on Part B, which demonstrated the effectiveness of deep contour-aware network on this challenging
problem. From Table 1, we noticed that all methods yielded
relatively lower performance on testing data Part B than Part
A. This mainly comes from the different data distributions.
Figure 4: Segmentation results of benign cases (from top to bottom): original images, segmentation results without contouraware, and segmentation results with contour-aware (different colors denote individual gland objects).
Figure 5: Segmentation results of malignant cases (from top to bottom): original images, segmentation results without
contour-aware, and segmentation results with contour-aware (different colors denote individual gland objects).
We observed that benign cases make up about 55% in Part
A while most of Part B are malignant cases. CUMedVision2 achieved inferior performance (but still competitive
compared to other methods) than CUMedVision1 on Part B.
This arises from the fact that irregular structures in malignant cases can make the gland segmentation more challeng-
CUMedVision2
CUMedVision1
Freiburg2 
CVIP Dundee
Freiburg1 
Ching-Wei Wang1
Table 1: The detection results of different methods in 2015
MICCAI Gland Segmentation Challenge (only top 10 entries are shown here and the ranking from top to bottom is
made according to the standard competition ranking ).
ing. For example, the low-contrast between interior tubular
structure and stroma as a result of tissue degeneration may
make methods relying on epithelial boundary cues more
likely fail in such cases. Nevertheless, our deep contouraware network ranked ﬁrst regarding the detection results
on all testing data.
Segmentation
Given a set of pixels G annotated as a
ground truth object and a set of pixels S segmented as a
gland object, Dice index is often employed for segmentation
evaluation D(G, S) = 2(|G ∩S|)/(|G| + |S|). However,
this is not suitable for segmentation evaluation on individual objects. Instead, an object-level Dice index is utilized
and deﬁned as:
Dobject(G, S) = 1
ωiD(Gi, Si) +
˜ωjD( ˜Gj, ˜Sj)
where Si denotes the ith segmented object, Gi denotes a
ground truth object that maximally overlaps Si, ˜Gj denotes
the jth ground truth object, ˜Sj denotes a segmented object
that maximally overlaps ˜Gj, ωi = |Si|/ PnS
m=1 |Sm|, ˜ωj =
| ˜Gj|/ PnG
n=1 | ˜Gn|, nS and nG are the total number of segmented objects and ground truth objects, respectively.
The segmentation results of different methods are shown
in Table 2.
We can see that our results CUMedVision2
achieved the best performance on testing data Part A
and CUMedVision1 outperformed all the other advanced
methods on Part B. Similarly, there is around 3% improvement in Part A and 2% decrement on Part B in terms of
object-level Dice index comparing our method with and
without fusing contour-aware results. By examining some
malignant cases, we observed that some inaccurate contours in interior structures may cause the deformed glands
fragmented.
One failure example is shown in Figure 5
CUMedVision2
Freiburg2 
CUMedVision1
Freiburg1 
CVIP Dundee
Table 2: The segmentation results of different methods in
2015 MICCAI Gland Segmentation Challenge.
(fourth column), which indicates that contours may oversplit the object in some seriously degenerated cases.
summary, the deep contour-aware network achieved the best
segmentation results regarding the object-level Dice index
on all testing data, which evidenced the efﬁcacy of our
method consistently. Shape similarity
The shape similarity is measured by using the Hausdorff distance between
the shape of segmented object and that of the ground truth
object, deﬁned as:
H(G, S) = max{sup
y∈S ∥x −y∥, sup
x∈G ∥x −y∥}
Likewise, an object-level Hausdorff is employed:
Hobject(G, S) = 1
ωiH(Gi, Si) +
˜ωjH( ˜Gj, ˜Sj)
The shape similarity results of different methods are
shown in Table 3. Our results CUMedVision2 from deep
contour aware network achieved the smallest Hausdorff distance (the only one less than 50 pixels), outperforming other
methods by a signiﬁcant margin on testing data Part A. In
addition, the results of CUMedVision1 is comparable to the
best results from ExB1 regarding the shape similarity on
Overall results
For the overall results, each team is assigned three ranking numbers for each part of testing data
based on the three criteria mentioned above, one ranking
number per criterion, using a standard competition ranking . The sum score of these numbers is used for the
ﬁnal ranking, i.e., a smaller score stands for better overall
segmentation results. The ﬁnal ranking can be seen in Table 4 (only top 10 entries are shown). Although there is
a side-effect with contour-aware component in some malignant cases, our deep contour-aware network yielded the best
Ranking score
Final ranking
Hausdorff A
Hausdorff B
CUMedVision2
Freiburg2 
CUMedVision1
Freiburg1 
Table 4: The ﬁnal ranking of different methods in 2015 MICCAI Gland Segmentation Challenge (A and B denote the part of
testing data, only top 10 entries are shown here).
Freiburg2 
Freiburg1 
CUMedVision2
CUMedVision1
CVIP Dundee
Table 3: The shape similarity results of different methods in
2015 MICCAI Gland Segmentation Challenge.
performance in terms of overall results out of 13 teams, outperforming all the other advanced methods by a signiﬁcant
margin. One straightforward way to refrain from the sideeffect is to classify the histopathological images into benign and malignant cases ﬁrst, then segment the image with
contour-aware component or not depending on the classi-
ﬁcation results. This may enlighten other researchers for
more advanced fusion algorithms.
3.5. Computation cost
It took about four hours to train the deep contour-aware
network on a workstation with 2.50 GHz Intel(R) Xeon(R)
E5-1620 CPU and a NVIDIA GeForce GTX Titan X GPU.
Leveraging the efﬁcient inference of fully convolutional architecture, the average time for processing one testing image with size 755 × 522 was about 1.5 seconds, which was
much faster than other methods in the literature.
Considering large-scale histology images are demanded for
prompt analysis with the advent of whole slide imaging, the
fast speed implies the possibility of our method in clinical
4. Conclusions
In this paper, we have presented a deep contour-aware
network that integrates multi-level contextual features to accurately segment glands from histology images.
of learning gland segmentation in isolation, we formulated it as a uniﬁed multi-task learning process by harnessing the complementary information, which helps to further separate the clustered gland objects efﬁciently.
Extensive experimental results on the benchmark dataset with
rich comparison results demonstrated the outstanding performance of our method. In the future work, we will optimize the method and investigate its capability on large-scale
histopathological dataset.
Acknowledgements
This work is supported by Hong Kong Research Grants
Council General Research Fund (Project No.
412412 and Project No.
CUHK 412513), a grant from
the National Natural Science Foundation of China (Project
No. 61233012) and a grant from Ministry of Science and
Technology of the People’s Republic of China under the
Singapore-China 9th Joint Research Programme (Project
No. 2013DFG12900). The authors also gratefully thank
the challenge organizers for helping the evaluation.