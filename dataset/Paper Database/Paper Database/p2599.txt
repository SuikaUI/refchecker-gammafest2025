Larger Norm More Transferable: An Adaptive Feature Norm Approach for
Unsupervised Domain Adaptation
Ruijia Xu1
Guanbin Li1∗
Jihan Yang1
Liang Lin1,2
1School of Data and Computer Science, Sun Yat-sen University, China
2DarkMatter AI Research
 
 
 
 
Domain adaptation enables the learner to safely generalize into novel environments by mitigating domain shifts
across distributions. Previous works may not effectively uncover the underlying reasons that would lead to the drastic model degradation on the target task.
In this paper,
we empirically reveal that the erratic discrimination of the
target domain mainly stems from its much smaller feature
norms with respect to that of the source domain. To this
end, we propose a novel parameter-free Adaptive Feature
Norm approach. We demonstrate that progressively adapting the feature norms of the two domains to a large range
of values can result in signiﬁcant transfer gains, implying
that those task-speciﬁc features with larger norms are more
transferable. Our method successfully uniﬁes the computation of both standard and partial domain adaptation with
more robustness against the negative transfer issue. Without bells and whistles but a few lines of code, our method
substantially lifts the performance on the target task and exceeds state-of-the-arts by a large margin (11.5% on Ofﬁce-
Home and 17.1% on VisDA2017 ). We hope our
simple yet effective approach will shed some light on the
future research of transfer learning.
Code is available
at 
1. Introduction
Deep neural networks, driven by numerous labeled samples, have made remarkable progress in a wide range of
computer vision tasks. However, those models are very vulnerable to generalize into new application scenarios. Even a
subtle deviation from the training regime can lead to a drastic degradation of the model . Therefore, with the
strong motivation to safely transfer knowledge from a labelrich source domain to an unlabeled target domain, Unsupervised Domain Adaptation (UDA) attempts to train a classi-
∗Corresponding author is Guanbin Li.
Figure 1: Feature visualization of source and target samples
on the Source Only model. This technique is widely used to
characterize the feature embeddings under the softmax-related
objectives . Speciﬁcally, we set the task-speciﬁc
features to be two-dimensional and retrain the model. Unlike
t-SNE whose size of empty space does not account for
the similarity between the two data points, this visualization
map enables us to interpret the size of feature norms as well
as inter-class and intra-class variances. As illustrated, target
samples tend to collide in the small-norm (i.e., low-radius) regions which are vulnerable to slight angular variations of the
decision boundaries and lead to erratic discrimination.
ﬁer using source samples that can generalize well to the target domain while mitigating the domain shift between the
two underlying distributions.
Under the guidance of the theoretical upper bound in ,
the key idea of most existing DA algorithms is to capture not
only the task-discriminative but the domain-invariant representations by simultaneously minimizing the source error
and some speciﬁc statistical discrepancy across the two domains, e.g., H-divergence , H∆H-divergence , Maximum Mean Discrepancy (MMD) , correlation distance and etc.
Adversarial domain adaptation , which seeks to minimize an approximate domain discrepancy with an adversarial objective, has recently evolved
into a dominant method in this ﬁeld. To the best of our
knowledge, RevGrad is the pioneer to empirically measure the H-divergence by a parametric domain discriminator and adversarially align the features via reverse gradient
backpropagation. ADDA instead facilitates the adversarial alignment with GAN-based objective in an asymmetric manner. MCD places a min-max game between the
feature generator and the two-branch classiﬁers to reduce
the H∆H-divergence. On par with the feature-level alignment, generative pixel-level adaptation 
utilizes Image-to-Image translation techniques to capture
the low-level domain shifts.
While the notion of model degradation has been well recognized within the DA community , little research
work has been published to analyze the underlying cause of
this phenomenon. Thus, existing statistic divergences may
fail to precisely depict the domain shift and bridging such
discrepancies may not guarantee the safe transfer across domains. For example, Shu et al. verify that bridging
JensenShannon divergence between the two domains does
not imply high accuracy on the target task. In this paper,
we take a step towards unveiling the nature of model degradation from a solid empirical observation, which is highlighted by Fig. 1. This visualization map suggests that the
excessively smaller norms of target-domain features with
respect to that of the source domain account for their erratic
discrimination. However, there remain two hypothetical interpretations from the current observation:
1) Misaligned-Feature-Norm Hypothesis: The domain
shift between the source and target domains relies on their
misaligned feature-norm expectations. Matching the mean
feature norms of the two domains to an arbitrary shared
scalar is supposed to yield similar transfer gains.
2) Smaller-Feature-Norm Hypothesis: The domain shift
substantially relies on the excessively less-informative features with smaller norms for the target task. Despite nonrigorous alignment, adapting the target features far away
from the small-norm regions can lead to safe transfer.
With these points in mind, we introduce our parameterfree Adaptive Feature Norm (AFN) approach. First, we propose a simple yet effective statistic distance to characterize
the mean-feature-norm discrepancy across domains. Second, we design the Hard AFN to bridge this domain gap by
restricting the expected feature norms of the two domains to
approximate a shared scalar. It suggests that norm-aligned
features can bring effective transfer yet the results can be
further improved with a larger scalar. To explore a more
sufﬁcient large feature norm in a stable way, we propose
the Stepwise AFN to encourage a progressive feature-norm
enlargement for each individual sample across domains. As
stepwise AFN reveals, the key to achieving successful transfer is to properly lift the target samples towards the largenorm regions while the rigorous alignment is superﬂuous.
This innovative discovery inspires us to revisit what features are transferable. We recognize that those task-speciﬁc
features with larger norms imply more informative transferability. Similar ﬁndings are explored in the ﬁeld of model
compression in terms of the smaller-norm-less-informative
assumption , which suggests that parameters or features
with smaller norms play a less informative role during the
inference. Like the two sides of a coin, in contrast to the
model compression that prunes unnecessary computational
elements or paths, we place the larger-norm constraint upon
the task-speciﬁc features to facilitate the more informative
and transferable computation on the target domain.
It is noteworthy that under the partial DA, the negative transfer is caused not only from the unrelated samples
within the shared categories but also from the unrelated data
from the source outlier categories. To this end, we propose
meaningful protocols to evaluate the robustness w.r.t a speciﬁc algorithm to defense against these potential risks of
negative transfer. With thorough evaluation, it reveals that
our fairly novel feature-norm-adaptive manner is more robust to safely transfer knowledge from the source domain.
We summarize our contributions as follows:
i) We empirically unveil the nature of model degradation from a solid observation that the excessively smaller
norms of the target-domain features with respect to that of
the source domain account for their erratic discrimination.
ii) We propose a novel AFN approach for UDA by progressively adapting the feature norms of the two domains to
a large range of scalars. Our approach is fairly simple yet
effective and is translated into a few lines of code.
iii) We succeed in unifying the computation for both
vanilla and partial DA and the feature-norm-adaptive manner is more robust to defense against the negative transfer.
iv) Extensive experimental results have demonstrated
the promise of our approach by exceeding state-of-the-arts
across a wide range of visual DA benchmarks.
2. Related Work
Domain adaptation generalizes the learner
across different domains by mitigating the domain shift
problem. Supervised DA exploits a few labeled
data in the target domain while unsupervised DA has no access to that. We focus on the latter scenario in our paper.
Under the guidance of the theoretical upper bound proposed in , existing methods explore domain-invariant
structures by minimizing some speciﬁc statistic distances
between the two domains. For example, Maximum Mean
Discrepancy (MMD) based methods learn
transferable features by minimizing the MMD of their kernel embeddings.
Deep correlation alignment proposes to match the mean and covariance of the two distributions. introduces H- and H∆H-divergence to characterize the domain discrepancy, which are further developed into matching the corresponding deep representations
by and respectively. Regarding the methodology, kernel-based DA and adversarial
DA are widely-used in the ﬁeld.
Inspired by GANs , adversarial DA involves a subnetwork as the domain classiﬁer to discriminate features of
different domains while the deep learner tries to generate
the features that deceive the domain classiﬁer. For example,
RevGrad utilize a parametric subnetwork as the domain
discriminator and adversarially align the features via reverse
gradient backpropagation. ADDA instead facilitates
the adversarial alignment with GAN-based objectives in an
asymmetric manner. MCD conducts a min-max game
between the feature generator and the two-branch classiﬁers
in order to reduce the H∆H-divergence. On par with the
feature-level adversarial alignment, generative pixel-level
adaptation utilizes Image-to-Image translation techniques to capture the low-level domain shifts.
In addition, other methods are proposed to learn targetspeciﬁc structures. DRCN involves a reconstruction
penalty on target samples. utilizes tri-training to obtain
target pseudo labels. reﬁnes the target decision boundary based on the cluster assumption. iCAN iteratively
applies sample selection on pseudo-labeled target samples
and retrains the network.
Standard domain adaptation assumes that the two domains share the identical label space. further open up
the partial setting where source label space subsumes the
target one, However, it is not trivial to directly migrate the
current models in the standard DA as they are prone to suffer from the negative transfer effect. PADA attempts to
alleviate this issue by detecting and down-weighting samples belonging to the source outlier classes.
3.1. Preliminaries
Given a source domain Ds = {(xs
i=1 of ns labeled samples associated with |Cs| categories and a target
domain Dt = {xt
i=1 of nt unlabeled samples associated
with |Ct| categories. DA occurs when the underlying distributions corresponding to the source and target domains in
the shared label space are different but similar to make
sense the transfer. Unsupervised DA considers the scenario
that we have no access to any labeled target examples.
Vanilla Setting Under this setting, the source and target
domains share the identical label space, i.e., Cs = Ct.
Partial Setting The source label space subsumes the target one, i.e., Cs ⊃Ct. The source labeled data belonging to
the outlier categories Cs\Ct are unrelated to the target task.
Input: Xs + Xt
Xs: Source samples
Xt: Target samples
Ys: Source labels
G: Backbone Network
F: Classifier
Classification Loss
FC-BN-ReLU-Dropout
Shared Lmax
Figure 2: The overall framework of our proposed Adaptive
Feature Norm approach. The backbone network G denotes the
general feature extraction module. F is employed as the taskspeciﬁc classiﬁer with l layers, each of which is organized in
the FC-BN-ReLU-Dropout order. During each iteration, we
apply the feature norm adaptation upon the task-speciﬁc features along with the source classiﬁcation loss as our optimization objective. For the Hard variant of AFN, the mean feature
norms of source and target samples are constrained to a shared
scalar. For the Stepwise variant, we encourage a progressive
feature-norm enlargement with respect to each individual example at the step size of ∆r. To this end, far away from the
small-norm regions after the adaptation, the target samples can
be correctly classiﬁed without any supervision.
3.2. L2-preserved Dropout
In this part, we ﬁrst prove that the standard Dropout operator is L1-preserved. As our algorithm is computed based
on the L2 norms of the hidden features, we introduce the
following L2-preserved Dropout operation to meet our goal.
Dropout is a widely-used regularization technique in
deep neural networks . Given a d-dimensional input vector x, in the training phase, we randomly zero the
element xk, k = 1, 2,..., d with probability p by samples
ak ∼P that are generated from the Bernoulli distribution:
To compute an identity function in the evaluation stage, the
outputs are further scaled by a factor of
1−p and thus
which implicitly preserves the L1-norm in both of the training and evaluation phases since xk and ak are independent:
E[|ˆxk|] = E[|ak
1 −pxk|] =
1 −pE[ak]E[|xk|] = E[|xk|] .
However, as we are in pursuit of adaptive L2 feature norm,
we instead scale the output by a factor of
√1−p and obtain
which satisﬁes
E[|ˆxk|2] = E[|ak
√1 −pxk|2] =
k]E[|xk|2] = E[|xk|2] .
3.3. Framework
As indicated in Fig. 2, our framework consists of a backbone network G and a classiﬁer F. Existing ﬁndings reveal
that deep features eventually transit from general to speciﬁc
along the network and feature transferability signiﬁcantly
drops in higher layers . In our case, G is regarded as
the general feature extraction module that inherits from the
prevailing network architecture such as ResNet . F represents the task-speciﬁc classiﬁer that has l fully-connected
layers. We denotes the ﬁrst l −1 layers of the classiﬁer
as Ff, which results in the so-called bottleneck feature embeddings f . Those features computed by Ff depend
greatly on the speciﬁc domain and are not safely transferable to a novel domain. Eventually, we calculate the class
probabilities along the last layer Fy, which is followed by a
softmax operator. We denote the parameters of G, Ff, Fy
with θg, θf and θy respectively. Our intention is to explore
an adaptive algorithm to compute the domain-transferable
features f = Ff(G(·)) using only source domain supervision. On the other side, as we are unifying the computation
with respect to both vanilla and partial DA, it raises an interleaving challenge to defense against the negative transfer
effect caused by the outlier categories in the source domain.
3.4. Hard Adaptive Feature Norm
Based on our Misaligned-Feature-Norm Hypothesis, we
propose the Maximum Mean Feature Norm Discrepancy
(MMFND) to characterize the mean-feature-norm distance
between the two distributions and verify whether bridging
this statistical domain gap can result in appreciable transfer gains. MMFND is deﬁned by Eq. (6), where the function class H is the combination of all the possible functions
composited by the L2-norm operator with the deep representation module, i.e., h(x) = (∥·∥2 ◦Ff ◦G)(x).
MMFND[H, Ds, Dt] := sup
Intuitively, the functions class H are rich enough to contain substantial positive real valued functions on the input x
and the upper bound will greatly deviate from zero if there
is no restriction on the function type. In order to avoid this
happening, we place a restrictive scalar R to match the corresponding mean feature norms.
By restricting both the
mean feature norms of the two domains respectively converging towards the shared equilibrium R, the domain gap
in terms of MMFND will vanish to zero. We implement this
via the Hard Adaptive Feature Norm (HAFN) algorithm,
which is illustrated by Eq. (7).
C1(θg, θf, θy) = 1
(xi,yi)∈Ds
Ly(xi, yi)
h(xi), R) + Ld( 1
h(xi), R)) .
The optimization objective consists of two terms: the
source classiﬁcation loss Ly in order to obtain the taskdiscriminative features by minimizing the softmax cross
entropy on the source labeled samples, which is indicated by Eq. (8), where p =
p1, . . . , p|Cs|
is the softmax of the activations predicted by the classiﬁer, i.e.,
p = softmax(F(G(x))); the feature-norm penalty in order to obtain the domain-transferable features by minimizing the feature-norm discrepancy between the two domains,
where Ld(·, ·) is taken as the L2-distance and λ is a hyperparameter to trade off the two objectives.
i ; θg, θf, θy) = −
i ] log pk .
Simple yet effective, MMFND appears to be a novel and
superior statistical distance to characterize the cross-domain
shift. And by bridging this feature-norm discrepancy with
only source-domain supervision through executing HAFN,
we can ﬁnally achieve the task-discriminative as well as
domain-transferable features.
However, the preference setting of R still remains unsettled. As the Misaligned-Feature-Norm Hypothesis suggests, matching feature-norm expectations of the two domains to an arbitrary shared positive real value is supposed
to yield similar transfer gains. But this assertion is found not
to be true by our empirical results. Speciﬁcally, although restricting the mean feature norms of the two domains to even
a fairly small value (e.g., R = 1, that is, feature normalization) has shown effective results, however, with R gradually increases, the obtained models are still prone to achieve
higher accuracies on the target task. To this end, it is natural
to explore a sufﬁciently large R and verify whether the rigorous alignment between the feature-norm expectations is
necessary, which is revealed by our Smaller-Feature-Norm
Hypothesis. In fact, it is unfortunate that HAFN fails to
set an extremely large R as the gradients generated by the
feature-norm penalty may eventually lead to an explosion.
3.5. Stepwise Adaptive Feature Norm
To break the aforementioned bottleneck, we introduce an
improved variant called Stepwise Adaptive Feature Norm
(SAFN) in order to encourage the model to learn taskspeciﬁc features with larger norms in a progressive manner,
Table 1: Accuracy (%) on Ofﬁce-Home under vanilla setting (ResNet-50)
Ar→Pr Ar→Rw Cl→Ar
Cl→Rw Pr→Ar
Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg
ResNet 
CDAN* 
50.2±0.2 70.1±0.2 76.6±0.3 61.1±0.4 68.0±0.1 70.7±0.2 59.5±0.2 48.4±0.3 77.3±0.2 69.4±0.0 53.0±0.6 80.2±0.3 65.4
52.0±0.1 71.7±0.6 76.3±0.3 64.2±0.3 69.9±0.6 71.9±0.6 63.7±0.4 51.4±0.2 77.1±0.0 70.9±0.4 57.1±0.1 81.5±0.0 67.3
Table 2: Accuracy (%) on VisDA2017 under vanilla setting (ResNet-101)
ResNet 
92.7±0.7 55.4±4.1 82.4±2.6 70.9±1.2 93.2±0.9 71.2±3.9 90.8±0.5 78.2±1.3 89.1±0.7 50.2±2.4 88.9±0.8 24.5±0.5
93.6±0.2 61.3±4.0 84.1±0.5 70.6±2.2 94.1±0.5 79.0±4.1 91.8±0.5 79.6±1.3 89.9±0.7 55.6±3.4 89.0±0.3 24.4±2.9
which is indicated by Eq. (9) as follows:
C2(θg, θf, θy) = 1
(xi,yi)∈Ds
Ly(xi, yi)
Ld(h(xi; θ0) + ∆r, h(xi; θ)) ,
where θ = θg ∪θf. θ0 and θ represent the updated and
updating model parameters in the last and current iterations
respectively. ∆r denotes the positive residual scalar to control the feature-norm enlargement. During each iteration,
the second penalty in SAFN encourages a feature-norm enlargement at the step size of ∆r with respect to individual examples, based on their feature norms calculated by
the past model parameters in the last iteration. Instead of
assigning a hard value, SAFN enables the optimization process more stable and fairly easy to trade off between the two
objectives. To this end, executing SAFN can lead to higher
accuracies on the target task by generating more informative
features with larger norms. It is noteworthy that SAFN does
not rigorously bridge the mean-feature-norm discrepancy,
yet one can alternatively place a terminal R to restrict the
endless enlargement, which is indicated by Eq. (10). However, our empirical results revealed that Eq. (10) has slightly
different result as to replace the second term in Eq. (9).
As the Smaller-Feature-Norm hypothesis suggests, once we
properly adapt the target samples towards the large-norm
regions, the rigorous alignment becomes superﬂuous.
Ld(max(h(xi; θ0) + ∆r, R), h(xi; θ)) .
3.6. Model Robustness Evaluation
Though the notion of negative transfer has been well recognized within the DA community , its rigorous deﬁnition is still unclear . A widely accepted description of
negative transfer is stated as transferring knowledge
from the source can have a negative impact on the target
learner. While intuitive, how to evaluate it still remains
open. Inspired by , we propose meaningful protocols
to evaluate the robustness of a given algorithm especially
under the more general partial setting. It is noteworthy that
in this setting, the negative transfer is caused not only from
the unrelated samples within the shared categories but also
from the unrelated data from the source outlier classes. Let
T|Ct|, AS|Ct|→T|Ct| and AS|Cs|→T|Ct| denote the accuracies
by using just l% target labeled data, transferring without
and with source outlier classes w.r.t an identical algorithm.
We deﬁne i) Al%
T|Ct| −AS|Ct|→T|Ct| (Closed Negative Gap,
CNG): the negative impact occurs if the algorithm cannot
obtain more transfer gains over the negative inﬂuences from
another domain than even just labeling a few (e.g., 1%) target data, which is valueless when deployed in the wild. ii)
AS|Ct|→T|Ct| −AS|Cs|→T|Ct| (Outlier Negative Gap, ONG):
especially measures the negative inﬂuences that are caused
by the source unrelated categories. iii) Al%
T|Ct|−AS|Cs|→T|Ct|
(Partial Negative Gap, PNG): reveals whether it is valuable
for an algorithm to access and transfer from those available
large domains with the potential risks of CNG and ONG.
We say that the negative effect exceeds the positive gains
once the gap value is positive and vice versa. The larger absolute value suggests more desperate negative inﬂuences or
more encouraging positive gains.
4. Experiment
4.1. Setup
VisDA2017 is the challenging large-scale benchmark that attempts to bridge the signiﬁcant synthetic-to-
Table 3: Accuracy (%) on ImageCLEF-DA in vanilla setting
P→I I→C C→I C→P P→C Avg
ResNet-50 
CDAN* 
±0.4 ±0.4 ±0.1 ±0.6
±0.4 ±0.5 ±0.1 ±0.3
±0.1 ±0.4 ±0.4 ±0.0
Table 4: Accuracy (%) on Ofﬁce-31 under vanilla setting
A→W D→W W→D A→D D→A W→A Avg
ResNet-50 68.4
CDAN* 
real domain gap with over 280K images across 12 object
categories. The source domain has 152,397 synthetic images generated by rendering from 3D models. The target
domain has 55,388 real object images collected from Microsoft COCO . Under the partial setting, we follow 
to choose (in alphabetic order) the ﬁrst 6 categories as target
categories and conduct the Synthetic-12 →Real-6 task.
Ofﬁce-Home is another challenging dataset that
collects images of everyday objects to form four domains:
Artistic images (Ar), Clipart images (Cl), Product images
(Pr) and Real-World images (Rw). Each domain contains
65 object categories and they amount to around 15,500 images. Under the partial setting, we follow to choose (in
alphabetic order) the ﬁrst 25 categories as target categories.
Ofﬁce-31 is a widely-used benchmark for visual
DA. It contains 4,652 images of 31 ofﬁce environment categories from three domains: Amazon (A), DSLR (D) and
Webcam (W), which correspond to online website, digital
SLR camera and web camera images respectively.
ImageCLEF-DA is built for ImageCLEF 2014 domain
adaptation challenge1 and consists of 12 common categories
shared by three public datasets: Caltech-256 (C), ImageNet
1 
ILSVRC2012 (I) and Pascal VOC 2012 (P). There are 50
images in each category and 600 images in each domain.
Implementation Details We follow the standard protocol to utilize all labeled source data
and unlabeled target data that belongs to their own label
spaces. We implement our experiments on the widely-used
PyTorch2 platform. For fair comparison, our backbone network is identical to the competitive approaches and is also
ﬁne-tuned from the ImageNet pre-trained model. We
adopted a uniﬁed set of hyper-parameters throughout the
Ofﬁce-Home, Ofﬁce-31 and ImageCLEF-DA datasets under
both settings, where λ = 0.05, R = 25 in HAFN and ∆r =
1.0 in SAFN. Since the synthetic domain on VisDA2017 is
easy to converge, we applied a slightly smaller λ and ∆r
that equal to 0.01 and 0.3 respectively. We used mini-batch
SGD optimizer with learning rate 1.0 × 10−3 on all benchmarks. We used center-crop images for the reported results. For each transfer task, we reported the average accuracy over three random repeats. For fairer comparison
with those methods which used ten-crop images at
the evaluation phase with the best-performing models, we
also included our corresponding results with the notion of
{method}* to beneﬁt the future comparisons.
4.2. Result Analysis
Results on Ofﬁce-Home, VisDA2017, ImageCLEF-DA
and Ofﬁce-31 under the vanilla setting are reported in Table 1, 2, 3, 4 respectively.
Results on Ofﬁce-Home and
VisDA2017 under the partial setting are reported in Table 5, 7. Robustness evaluations in terms of CNG, ONG
and PNG are shown in Table 6. As illustrated, our methods
signiﬁcantly outperform the state-of-the-arts throughout all
experiments, where SAFN is the top-performing variant.
Results on VisDA2017 reveal some interesting observations: i) Adversarial based models such as DANN may not
effectively learn a diverse transformation across domains
on this extremely large-scale transfer task and is prone to
suffer from the risk of mode mismatch. However, our encouraging results prove the efﬁcacy of AFN to work reasonably on this large-scale dataset and bridge the signiﬁcant
synthetic-to-real domain gap. ii) Note that existing methods
usually mix and optimize multiple learning objectives, and
it is not always easy to get an optimal solution. For example, MCD incorporates another class balance objective
to align target samples in a balanced way. Nevertheless, our
method yields superior performance on most categories, revealing that it is robust to the unbalanced issue without any
other auxiliary constraint. iii) Our model is parameter-free
thus is more lightweight than the compared methods.
As indicated in Table 1, 3 and 4, our methods achieve
new state-of-the-arts on these three benchmarks, and with
larger rooms of improvement for those hard transfer tasks,
2 
Table 5: Accuracy (%) on Ofﬁce-Home under partial setting (ResNet-50)
Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg
ResNet 
Table 6: Evaluation on the Robustness
Ar →Rw (5%)
Cl →Rw (5%)
Pr →Rw (5%)
VisDA2017 (1%)
Table 7: Accuracy (%) on VisDA2017 under partial setting
Synthetic-12→Real-6
ResNet-50 
65.06±0.90
67.65±0.51
e.g., D →A, A →D, Cl →Pr, Cl →Rw and etc, where the
source and target domains are substantially different.
As illustrated in Table 5 and 7, our models obtain substantial improvements for partial DA, with 11.5% gain on
Ofﬁce-Home and 17.1% gain on VisDA2017. Plain domainadversarial networks, e.g., DANN, seriously suffer from
the mismatch from the source outlier classes and perform
even worse than the Source Only variant. An intuitive solution, e.g., PADA , is to detect and down-weight the
outlier categories during the domain alignment. However,
without any heuristic reweighting mechanism, our featurenorm-adaptive manner exhibits stronger robustness against
the unrelated data from the source domain. We testify this
point via more thorough evaluation in Table 6. Besides, our
method works stably and does not require to adjust different hyper-parameters for different subtasks within the same
dataset as was done in PADA.
We carefully conduct robustness evaluation for the most
challenging transfer tasks, e.g., Ar65 →Rw25, Synthetic-
12 →Real-6 and etc. As described in Section 3.6, the positive gap implies more negative impacts over the positive
gains and vice versa. The target labeled ratio is 5% and
1% for the two benchmarks. Results in Table 6 reveal some
interesting observations: i) Throughout all evaluation metrics on all transfer tasks, we can either achieve the largest
transfer gains or smallest negative inﬂuences. ii) All the
methods, including ours, are inevitable to the positive ONG
under the more challenging partial setting, while SAFN alleviates the outlier negative impact to the utmost extent. iii)
For the Cl →Rw transfer task, the comparison methods all
have positive PNG, suggesting that they are unable to obtain more transfer gains from the Cl65 domain than using
only 5% Rw25 labeled samples. However, we still derive
encouraging result in this task. iv) It is noteworthy that on
the most challenging VisDA2017 dataset with the signiﬁcant
synthetic-to-real gap, current approaches, including ours,
all fail to distill more positive knowledge from the synthetic
domain than just labeling 1% real samples. It remains a big
challenge for the future development of DA community.
5. Ablation Study
Feature Visualization: Although testifying the efﬁcacy
of a DA algorithm via t-SNE embeddings is considered over-interpreted,3 we still follow the de facto practice
to provide the intuitive understanding. We randomly select
2000 samples across 12 categories from the source and target domains on VisDA2017 and visualize their task-speciﬁc
features by t-SNE. As shown in Fig. 4(a), the ResNet features of target samples collide into a mess because of extremely large synthetic-to-real domain gap. After adaptation, as illustrated in Fig. 4(b), our method succeeded in
separating target domain samples and better aligning them
to the corresponding source domain clusters.
Sample Size of Target Domain: In this part, we empirically demonstrate that our approach is scalable and datadriven with respect to the increase of unlabeled target sam-
3 
Sample size
SAFN on VisDA2017
(a) Sample Size
Value of R
Office31(A->W)
(b) Sensitivity of R
Office31(A->W)
Office-31 r
(c) Sensitivity of ∆r
Embedding Size
Office31(A->W)
(d) Embedding Size
Figure 3: Analysis of (a) varying unlabeled target sample size; (b)(c) parameter sensitivity of R and ∆r; (d) varying embedding size.
VisDA2017_Source_Only
(a) Source Only
VisDA2017_SAFN
Figure 4: (a) and (b) correspond to the t-SNE embedding visualization of the Source Only and SAFN models on VisDA2017.
The triangle and star markers denote the source and target samples respectively. Different colors indicate different categories.
ples, which exposes the appealing capacity in practice. It
is not necessarily intuitive for adversarial learning based
methods to optimize and obtain a diverse transformation
upon large volumes of unlabeled target samples. Speciﬁcally, we shufﬂe the target domain on VisDA2017 and sequentially access the top 25%, 50%, 75% and 100% of the
dataset. We train and evaluate our approach on these four
subsets. As illustrated in Fig. 3(a), with the sample size
gradually increases, the classiﬁcation accuracy of the corresponding target domain grows accordingly. It shows that the
more unlabeled target data are involved in the feature norm
adaptation, the more transferable classiﬁer can be obtained.
Complementary with Other Methods: In this part, we
demonstrate that our approach can be used in combination
with other DA techniques. Because of limited space, we
particularly exploit ENTropy minimization (ENT) , a
low-density separation technique, for demonstration. ENT
is widely applied in DA community to encourage the decision boundary to pass through the target lowdensity regions by minimizing the conditional entropy of
target samples. We conduct this case study on ImageCLEF-
DA and Ofﬁce-31 datasets and report the accuracies in Table 3 and 4 respectively. As indicated, with ENT to ﬁt the
target-speciﬁc structure, we further boost the recognition
performance by 0.8% and 1.4% on these two datasets.
Sensitivity of R and ∆r: We conduct case studies to
investigate the sensitivity of parameter R in HAFN and parameter ∆r in SAFN. We select VisDA2017 and task A→W
for demonstration. The results are shown in Fig. 3(b) and
Fig. 3(c), by varying R ∈{5, 10, 15, 20, 25, 30, 35} on
both datasets, ∆r ∈{0.2, 0.3, 0.4, 0.5} on VisDA2017 and
∆r ∈{0.5, 1.0, 1.5, 2.0} on task A→W. For parameter R,
the accuracy ﬁrst gradually increases with larger values of
R and then begins to decrease as the feature-norm penalty
in HAFN may explode. As shown in Fig. 3(c), the accuracies stay almost the same as parameter ∆r varies, revealing
that SAFN works reasonably stable on these two tasks.
Sensitivity of Embedding Size: We investigate the sensitivity of embedding size of the task-speciﬁc features as
it plays a signiﬁcant role in norm computation. We conduct this case study on both VisDA2017 and A→W transfer
tasks. We report the average accuracy over three random
repeats for those embedding sizes varying in {500, 1000,
1500, 2000}. As illustrated in Fig. 3(d), the accuracy stays
almost the same and achieves slightly higher when the embedding size is set to 1000, indicating that our approach is
robust to a wide range of feature space dimensions.
6. Conclusion
We have presented an innovative discovery for UDA,
revealing that the model degradation on the target domain
mainly stems from its much smaller feature norms with respect to that of the source domain. To this end, we demonstrated that progressively adapting the feature norms of the
two domains to a large range of values can result in significant transfer gains, implying that those task-speciﬁc features with larger norms are more transferable. Our method
is parameter-free, easy to implement and performs stably. In addition, we successfully unify the computation of
both standard and partial DA, and thorough evaluations revealed that our feature-norm-adaptive manner is more robust against the negative transfer. Extensive experimental
results have validated the virtue of our proposed approach.
Acknowledgements
This work was supported in part by the State Key Development Program under Grant 2016YFB1001004, in part
by the National Natural Science Foundation of China under
Grant No.U1811463 and No.61702565, in part by Guangdong Climbing Program Special Funds (pdjhb0009). This
work was also sponsored by SenseTime Research Fund.