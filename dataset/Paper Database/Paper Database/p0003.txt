Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1–10,
Beijing, China, July 26-31, 2015. c⃝2015 Association for Computational Linguistics
On Using Very Large Target Vocabulary for
Neural Machine Translation
S´ebastien Jean
Kyunghyun Cho
Roland Memisevic
Universit´e de Montr´eal
Yoshua Bengio
Universit´e de Montr´eal
CIFAR Senior Fellow
Neural machine translation, a recently
proposed approach to machine translation based purely on neural networks,
has shown promising results compared to
the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine
translation has its limitation in handling
a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words.
In this paper, we propose
a method based on importance sampling
that allows us to use a very large target vocabulary without increasing training complexity.
We show that decoding can be
efﬁciently done even with the model having a very large target vocabulary by selecting only a small subset of the whole
target vocabulary.
The models trained
by the proposed approach are empirically
found to match, and in some cases outperform, the baseline models with a small
vocabulary as well as the LSTM-based
neural machine translation models. Furthermore, when we use an ensemble of
a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured
by BLEU) on both the English→German
and English→French translation tasks of
Introduction
Neural machine translation (NMT) is a recently
introduced approach to solving machine translation . In neural machine translation, one builds a single neural network that reads a source sentence and generates
its translation. The whole neural network is jointly
trained to maximize the conditional probability of
a correct translation given a source sentence, using the bilingual corpus. The NMT models have
shown to perform as well as the most widely used
conventional translation systems .
Neural machine translation has a number of
advantages over the existing statistical machine
translation system, speciﬁcally, the phrase-based
system . First, NMT requires
a minimal set of domain knowledge. For instance,
all of the models proposed in , or do not assume any linguistic property in both source and target sentences
except that they are sequences of words.
Second, the whole system is jointly trained to maximize the translation performance, unlike the existing phrase-based system which consists of many
separately trained features whose weights are then
tuned jointly. Lastly, the memory footprint of the
NMT model is often much smaller than the existing system which relies on maintaining large tables of phrase pairs.
Despite these advantages and promising results,
there is a major limitation in NMT compared to
the existing phrase-based approach. That is, the
number of target words must be limited. This is
mainly because the complexity of training and using an NMT model increases as the number of target words increases.
A usual practice is to construct a target vocabulary of the K most frequent words (a socalled shortlist), where K is often in the range of
30k to 80k . Any word not included in this vocabulary is mapped to a special token representing
an unknown word [UNK]. This approach works
well when there are only a few unknown words
in the target sentence, but it has been observed
that the translation performance degrades rapidly
as the number of unknown words increases .
In this paper, we propose an approximate training algorithm based on (biased) importance sampling that allows us to train an NMT model with
a much larger target vocabulary. The proposed algorithm effectively keeps the computational complexity during training at the level of using only
a small subset of the full vocabulary.
the model with a very large target vocabulary is
trained, one can choose to use either all the target
words or only a subset of them.
We compare the proposed algorithm against the
baseline shortlist-based approach in the tasks of
English→French and English→German translation using the NMT model introduced in . The empirical results demonstrate that we can potentially achieve better translation performance using larger vocabularies, and
that our approach does not sacriﬁce too much
speed for both training and decoding.
Furthermore, we show that the model trained with this algorithm gets the best translation performance yet
achieved by single NMT models on the WMT’14
English→French translation task.
Neural Machine Translation and
Limited Vocabulary Problem
In this section, we brieﬂy describe an approach
to neural machine translation proposed recently in
 . Based on this description we explain the issue of limited vocabularies
in neural machine translation.
Neural Machine Translation
Neural machine translation is a recently proposed
approach to machine translation, which uses a single neural network trained jointly to maximize
the translation performance 
and encodes it into a sequence of hidden states
h = (h1, · · · , hT ):
ht = f (xt, ht−1) .
Then, the decoder, another recurrent neural network, generates a corresponding translation y =
(y1, · · · , yT ′) based on the encoded sequence of
hidden states h:
p(yt | y<t, x) ∝exp {q (yt−1, zt, ct)} ,
zt = g (yt−1, zt−1, ct) ,
ct = r (zt−1, h1, . . . , hT ) ,
and y<t = (y1, . . . , yt−1).
The whole model is jointly trained to maximize
the conditional log-probability of the correct translation given a source sentence with respect to the
parameters θ of the model:
θ∗= arg max
where (xn, yn) is the n-th training pair of sentences, and Tn is the length of the n-th target sentence (yn).
Detailed Description
In this paper, we use a speciﬁc implementation of
neural machine translation that uses an attention
mechanism, as recently proposed in .
In , the encoder in
Eq. (1) is implemented by a bi-directional recurrent neural network such that
h←−h t; −→h t
xt, ←−h t+1
, −→h t = f
xt, −→h t−1
They used a gated recurrent unit for f ).
The decoder, at each time, computes the context vector ct as a convex sum of the hidden states
(h1, . . . , hT ) with the coefﬁcients α1, . . . , αT
computed by
exp {a (ht, zt−1)}
k exp {a (hk, zt−1)},
where a is a feedforward neural network with a
single hidden layer.
A new hidden state zt of the decoder in Eq. (3) is
computed based on the previous hidden state zt−1,
previous generated symbol yt−1 and the computed
context vector ct. The decoder also uses the gated
recurrent unit, as the encoder does.
The probability of the next target word in
Eq. (2) is then computed by
p(yt | y<t, x) = 1
t φ (yt−1, zt, ct) + bt
where φ is an afﬁne transformation followed by
a nonlinear activation, and wt and bt are respectively the target word vector and the target word
bias. Z is the normalization constant computed by
k φ (yt−1, zt, ct) + bk
where V is the set of all the target words.
For the detailed description of the implementation, we refer the reader to the appendix of .
Limited Vocabulary Issue and
Conventional Solutions
One of the main difﬁculties in training this neural machine translation model is the computational
complexity involved in computing the target word
probability (Eq. (6)). More speciﬁcally, we need
to compute the dot product between the feature
φ (yt−1, zt, ct) and the word vector wt as many
times as there are words in a target vocabulary in
order to compute the normalization constant (the
denominator in Eq. (6)). This has to be done for,
on average, 20–30 words per sentence, which easily becomes prohibitively expensive even with a
moderate number of possible target words. Furthermore, the memory requirement grows linearly
with respect to the number of target words. This
has been a major hurdle for neural machine translation, compared to the existing non-parametric
approaches such as phrase-based translation systems.
Recently proposed neural machine translation
models, hence, use a shortlist of 30k to 80k most
frequent words . This makes training more feasible,
but comes with a number of problems. First of all,
the performance of the model degrades heavily if
the translation of a source sentence requires many
words that are not included in the shortlist . This also affects the performance
evaluation of the system which is often measured
by BLEU. Second, the ﬁrst issue becomes more
problematic with languages that have a rich set of
words such as German or other highly inﬂected
languages.
There are two model-speciﬁc approaches to this
issue of large target vocabulary. The ﬁrst approach
is to stochastically approximate the target word
probability. This has been proposed recently in
 based on noise-contrastive estimation . In the second approach, the target words are clustered into multiple classes, or hierarchical classes, and the target
probability p(yt|y<t, x) is factorized as a product
of the class probability p(ct|y<t, x) and the intraclass word probability p(yt|ct, y<t, x).
This reduces the number of required dot-products into the
sum of the number of classes and the words in a
class. These approaches mainly aim at reducing
the computational complexity during training, but
do not often result in speed-up when decoding a
translation during test time.1
Other than these model-speciﬁc approaches,
there exist translation-speciﬁc approaches.
translation-speciﬁc approach exploits the properties of the rare target words. For instance, Luong
et al. proposed such an approach for neural machine translation . They replace rare words (the words that are not included
in the shortlist) in both source and target sentences
into corresponding ⟨OOVn⟩tokens using the word
alignment model. Once a source sentence is translated, each ⟨OOVn⟩in the translation will be replaced based on the source word marked by the
corresponding ⟨OOVn⟩.
modelspeciﬁc approaches and the translation-speciﬁc
approaches are often complementary and can be
used together to further improve the translation
performance and reduce the computational complexity.
Approximate Learning Approach to
Very Large Target Vocabulary
Description
In this paper, we propose a model-speciﬁc approach that allows us to train a neural machine
translation model with a very large target vocabulary. With the proposed approach, the compu-
1This is due to the fact that the beam search requires the
conditional probability of every target word at each time step
regardless of the parametrization of the output probability.
tational complexity of training becomes constant
with respect to the size of the target vocabulary.
Furthermore, the proposed approach allows us to
efﬁciently use a fast computing device with limited memory, such as a GPU, to train a neural machine translation model with a much larger target
vocabulary.
As mentioned earlier, the computational inef-
ﬁciency of training a neural machine translation
model arises from the normalization constant in
Eq. (6). In order to avoid the growing complexity of computing the normalization constant, we
propose here to use only a small subset V ′ of the
target vocabulary at each update. The proposed
approach is based on the earlier work of .
Let us consider the gradient of the logprobability of the output in Eq. (6). The gradient
is composed of a positive and negative part:
∇log p(yt | y<t, x)
p(yk | y<t, x)∇E(yk),
where we deﬁne the energy E as
E(yj) = w⊤
j φ (yj−1, zj, cj) + bj.
The second, or negative, term of the gradient is in
essence the expected gradient of the energy:
EP [∇E(y)] ,
where P denotes p(y | y<t, x).
The main idea of the proposed approach is to
approximate this expectation, or the negative term
of the gradient, by importance sampling with a
small number of samples. Given a predeﬁned proposal distribution Q and a set V ′ of samples from
Q, we approximate the expectation in Eq. (9) with
EP [∇E(y)] ≈
k′:yk′∈V ′ ωk′ ∇E(yk),
ωk = exp {E(yk) −log Q(yk)} .
This approach allows us to compute the normalization constant during training using only a small
subset of the target vocabulary, resulting in much
lower computational complexity for each parameter update. Intuitively, at each parameter update,
we update only the vectors associated with the correct word wt and with the sampled words in V ′.
Once training is over, we can use the full target vocabulary to compute the output probability of each
target word.
Although the proposed approach naturally addresses the computational complexity, using this
approach naively does not guarantee that the number of parameters being updated for each sentence pair, which includes multiple target words,
is bounded nor can be controlled. This becomes
problematic when training is done, for instance,
on a GPU with limited memory.
In practice, hence, we partition the training corpus and deﬁne a subset V ′ of the target vocabulary for each partition prior to training. Before
training begins, we sequentially examine each target sentence in the training corpus and accumulate
unique target words until the number of unique target words reaches the predeﬁned threshold τ. The
accumulated vocabulary will be used for this partition of the corpus during training. We repeat this
until the end of the training set is reached. Let us
refer to the subset of target words used for the i-th
partition by V ′
This may be understood as having a separate
proposal distribution Qi for each partition of the
training corpus. The distribution Qi assigns equal
probability mass to all the target words included in
the subset V ′
i , and zero probability mass to all the
other words, i.e.,
if yt ∈V ′
otherwise.
This choice of proposal distribution cancels out
the correction term −log Q(yk) from the importance weight in Eqs. (10)–(11), which makes the
proposed approach equivalent to approximating
the exact output probability in Eq. (6) with
p(yt | y<t, x)
t φ (yt−1, zt, ct) + bt
k:yk∈V ′ exp
k φ (yt−1, zt, ct) + bk
It should be noted that this choice of Q makes the
estimator biased.
The proposed procedure results in speed up
against usual importance sampling, as it exploits
the advantage of modern computers in doing
matrix-matrix vs matrix-vector multiplications.
Informal Discussion on Consequence
The parametrization of the output probability in
Eq. (6) can be understood as arranging the vectors
associated with the target words such that the dot
product between the most likely, or correct, target
word’s vector and the current hidden state is maximized. The exponentiation followed by normalization is simply a process in which the dot products are converted into proper probabilities.
As learning continues, therefore, the vectors of
all the likely target words tend to align with each
other but not with the others. This is achieved exactly by moving the vector of the correct word in
the direction of φ (yt−1, zt, ct), while pushing all
the other vectors away, which happens when the
gradient of the logarithm of the exact output probability in Eq. (6) is maximized. Our approximate
approach, instead, moves the word vectors of the
correct words and of only a subset of sampled target words (those included in V ′).
Once the model is trained using the proposed approximation, we can use the full target vocabulary
when decoding a translation given a new source
sentence. Although this is advantageous as it allows the trained model to utilize the whole vocabulary when generating a translation, doing so may
be too computationally expensive, e.g., for realtime applications.
Since training puts the target word vectors in the
space so that they align well with the hidden state
of the decoder only when they are likely to be a
correct word, we can use only a subset of candidate target words during decoding. This is similar
to what we do during training, except that at test
time, we do not have access to a set of correct target words.
The most na¨ıve way to select a subset of candidate target words is to take only the top-K most
frequent target words, where K can be adjusted to
meet the computational requirement. This, however, effectively cancels out the whole purpose of
training a model with a very large target vocabulary. Instead, we can use an existing word alignment model to align the source and target words in
the training corpus and build a dictionary. With the
dictionary, for each source sentence, we construct
a target word set consisting of the K-most frequent words (according to the estimated unigram
probability) and, using the dictionary, at most K′
likely target words for each source word. K and
K′ may be chosen either to meet the computational requirement or to maximize the translation
performance on the development set. We call a
subset constructed in either of these ways a candidate list.
Source Words for Unknown Words
In the experiments, we evaluate the proposed approach with the neural machine translation model
called RNNsearch (see
Sec. 2.1.1). In this model, as a part of decoding
process, we obtain the alignments between the target words and source locations via the alignment
model in Eq. (5).
We can use this feature to infer the source word
to which each target word was most aligned (indicated by the largest αt in Eq. (5)).
especially useful when the model generated an
[UNK] token.
Once a translation is generated
given a source sentence, each [UNK] may be replaced using a translation-speciﬁc technique based
on the aligned source word. For instance, in the
experiment, we try replacing each [UNK] token
with the aligned source word or its most likely
translation determined by another word alignment
Other techniques such as transliteration
may also be used to further improve the performance .
Experiments
English→French and English→German translation tasks.
We trained the neural machine
translation models using only the bilingual, parallel corpora made available as a part of WMT’14.
For each pair, the datasets we used are:
• English→French:2
– Common Crawl
– News Commentary
– Gigaword
– Europarl v7
• English→German:
– Common Crawl
– News Commentary
– Europarl v7
2The preprocessed data can be found and downloaded from 
˜schwenk/nnmt-shared-task/README.
English-French
English-German
Table 1: Data coverage (in %) on target-side corpora for different vocabulary sizes. ”All” refers to
all the tokens in the training set.
To ensure fair comparison, the English→French
corpus, which comprises approximately 12 million sentences, is identical to the one used in
 .
English→German, the corpus was preprocessed,
in a manner similar to , in order to remove many poorly translated
sentences.
We evaluate the models on the WMT’14 test
set ,3 while the concatenation
of news-test-2012 and news-test-2013 is used
for model selection (development set).
presents data coverage w.r.t. the vocabulary size,
on the target side.
Unless mentioned otherwise, all reported BLEU
scores are computed with
the multi-bleu.perl script4 on the cased tokenized
translations.
As a baseline for English→French translation, we
use the RNNsearch model proposed by , with 30k source and target
words.5 Another RNNsearch model is trained for
English→German translation with 50k source and
target words.
For each language pair, we train another set
of RNNsearch models with much larger vocabularies of 500k source and target words, using
the proposed approach.
We call these models
RNNsearch-LV. We vary the size of the shortlist used during training (τ in Sec. 3.1). We tried
3To compare with previous submissions, we use the ﬁltered test sets.
4 
mosesdecoder/blob/master/scripts/
generic/multi-bleu.perl
5The authors of gave us access to
their trained models. We chose the best one on the validation
set and resumed training.
15k and 30k for English→French, and 15k and
50k for English→German. We later report the results for the best performance on the development
set, with models generally evaluated every twelve
The training speed is approximately the
same as for RNNsearch. Using a 780 Ti or Titan
Black GPU, we could process 100k mini-batches
of 80 sentences in about 29 and 39 hours respectively for τ = 15k and τ = 50k.
For both language pairs, we also trained new
models, with τ = 15k and τ = 50k, by reshufﬂing
the dataset at the beginning of each epoch. While
this causes a non-negligible amount of overhead,
such a change allows words to be contrasted with
different sets of other words each epoch.
To stabilize parameters other than the word embeddings, at the end of the training stage, we
freeze the word embeddings and tune only the
other parameters for approximately two more days
after the peak performance on the development set
is observed. This helped increase BLEU scores on
the development set.
We use beam search to generate a translation
given a source.
During beam search, we keep
a set of 12 hypotheses and normalize probabilities by the length of the candidate sentences, as in
 .6 The candidate list is chosen
to maximize the performance on the development
set, for K ∈{15k, 30k, 50k} and K′ ∈{10, 20}.
As explained in Sec. 3.2, we test using a bilingual dictionary to accelerate decoding and to replace unknown words in translations. The bilingual dictionary is built using fast align . We use the dictionary only if a word
starts with a lowercase letter, and otherwise, we
copy the source word directly. This led to better
performance on the development sets.
Note on ensembles
For each language pair, we
began training four models from each of which
two points corresponding to the best and secondbest performance on the development set were collected.
We continued training from each point,
while keeping the word embeddings ﬁxed, until
the best development performance was reached,
and took the model at this point as a single model
in an ensemble. This procedure resulted in a total of eight models from which we averaged the
length-normalized log-probabilities. Since much
of training had been shared, the composition of
6These experimental details differ from (Bahdanau et al.,
RNNsearch-LV
Phrase-based SMT
29.97 (26.58)
32.68 (28.76)
+Candidate List
33.36 (29.32)
+UNK Replace
33.08 (29.08)
34.11 (29.98)
+Reshufﬂe (τ=50k)
34.60 (30.53)
37.19 (31.98)
(a) English→French
RNNsearch-LV
Phrase-based SMT
16.46 (17.13)
16.95 (17.85)
+Candidate List
17.46 (18.00)
+UNK Replace
18.97 (19.16)
18.89 (19.03)
19.40 (19.37)
21.59 (21.06)
(b) English→German
Table 2: The translation performances in BLEU obtained by different models on (a) English→French and
(b) English→German translation tasks. RNNsearch is the model proposed in ,
RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the
LSTM-based model proposed in . Unless mentioned otherwise, we report singlemodel RNNsearch-LV scores using τ = 30k (English→French) and τ = 50k (English→German).
For the experiments we have run ourselves, we show the scores on the development set as well in the
brackets. (⋆) , (◦) , (•) , (∗) Standard
Moses Setting , (⋄) .
such ensembles may be sub-optimal. This is supported by the fact that higher cross-model BLEU
scores are observed for models that were partially trained together.
Translation Performance
In Table 2, we present the results obtained by the
trained models with very large target vocabularies, and alongside them, the previous results reported in , , and . Without translation-speciﬁc strategies, we
can clearly see that the RNNsearch-LV outperforms the baseline RNNsearch.
In the case of the English→French task,
RNNsearch-LV approached the performance level
of the previous best single neural machine translation (NMT) model, even without any translationspeciﬁc techniques (Sec. 3.2–3.3).
With these,
however, the RNNsearch-LV outperformed it. The
performance of the RNNsearch-LV is also better
than that of a standard phrase-based translation
system . Furthermore, by combining 8 models, we were able to achieve a translation performance comparable to the state of the
art, measured in BLEU.
For English→German, the RNNsearch-LV outperformed the baseline before unknown word replacement, but after doing so, the two systems performed similarly. We could reach higher largevocabulary single-model performance by reshuf-
ﬂing the dataset, but this step could potentially
also help the baseline. In this case, we were able
to surpass the previously reported best translation
result on this task by building an ensemble of 8
With τ = 15k, the RNNsearch-LV performance
worsened a little, with best BLEU scores, without reshufﬂing, of 33.76 and 18.59 respectively for
English→French and English→German.
The English→German ensemble described in
this paper has also been used for the shared translation task of the 10th Workshop on Statistical Machine Translation (WMT’15), where it was ranked
ﬁrst in terms of BLEU score. The translations by
this ensemble can be found online.7
Decoding Speed
In Table 3, we present the timing information of
decoding for different models. Clearly, decoding
from RNNsearch-LV with the full target vocab-
7 
output/1774?run_id=4079
RNNsearch-LV
RNNsearch-LV
+Candidate list
Table 3: The average per-word decoding time.
Decoding here does not include parameter loading and unknown word replacement. The baseline
uses 30k words. The candidate list is built with
K = 30k and K′ = 10. (⋆) i7-4820K (single
thread), (◦) GTX TITAN Black
ulary is slowest.
If we use a candidate list for
decoding each translation, the speed of decoding
substantially improves and becomes close to the
baseline RNNsearch.
A potential issue with using a candidate list is
that for each source sentence, we must re-build a
target vocabulary and subsequently replace a part
of the parameters, which may easily become timeconsuming.
We can address this issue, for instance, by building a common candidate list for
multiple source sentences. By doing so, we were
able to match the decoding speed of the baseline
RNNsearch model.
Decoding Target Vocabulary
For English→French (τ = 30k), we evaluate the
inﬂuence of the target vocabulary when translating the test sentences by using the union of a ﬁxed
set of 30k common words and (at most) K′ likely
candidates for each source word according to the
dictionary. Results are presented in Figure 1. With
K′ = 0 (not shown), the performance of the system is comparable to the baseline when not replacing the unknown words (30.12), but there is not as
much improvement when doing so (31.14). As the
large vocabulary model does not predict [UNK] as
much during training, it is less likely to generate
it when decoding, limiting the effectiveness of the
post-processing step in this case. With K′ = 1,
which limits the diversity of allowed uncommon
words, BLEU is not as good as with moderately
larger K′, which indicates that our models can, to
some degree, correctly choose between rare alternatives. If we rather use K = 50k, as we did
for testing based on validation performance, the
improvement over K′ = 1 is approximately 0.2
When validating the choice of K, we found it
to be correlated with the value of τ used during
BLEU score
With UNK replacement
Without UNK replacement
Single-model
(English→French) with respect to the number of
dictionary entries K′ allowed for each source
For example, on the English→French
validation set, with τ = 15k (and K′ = 10), the
BLEU score is 29.44 with K = 15k, but drops
to 29.19 and 28.84 respectively for K = 30k and
50k. For τ = 30k, the score increases moderately from K = 15k to K = 50k. A similar
effect was observed for English→German and on
the test sets. As our implementation of importance
sampling does not apply the usual correction to the
gradient, it seems beneﬁcial for the test vocabularies to resemble those used during training.
Conclusion
In this paper, we proposed a way to extend the size
of the target vocabulary for neural machine translation. The proposed approach allows us to train
a model with much larger target vocabulary without any substantial increase in computational complexity. It is based on the earlier work in which used importance sampling to reduce the complexity of computing the
normalization constant of the output word probability in neural language models.
On English→French and English→German
translation tasks, we observed that the neural machine translation models trained using the proposed method performed as well as, or better
than, those using only limited sets of target words,
even when replacing unknown words.
As performance of the RNNsearch-LV models increased
when only a selected subset of the target vocabulary was used during decoding, this makes the
proposed learning algorithm more practical.
When measured by BLEU, our models showed
translation
performance
comparable
state-of-the-art translation systems on both the
English→French task and English→German task.
On the English→French task, a model trained
with the proposed approach outperformed the best
single neural machine translation (NMT) model
from by approximately 1
BLEU point. The performance of the ensemble
of multiple models, despite its relatively less
diverse composition, is approximately 0.3 BLEU
points away from the best system (Luong et al.,
On the English→German task, the best
performance of 21.59 BLEU by our model is
higher than that of the previous state of the art
(20.67) reported in .
Finally, we release the source code used in our
experiments to encourage progress in neural machine translation.8
Acknowledgments
The authors would like to thank the developers
of Theano . We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Qu´ebec, Compute
Canada, the Canada Research Chairs, CIFAR and