Practical Detection of Trojan Neural Networks:
Data-Limited and Data-Free Cases
Ren Wang1, Gaoyuan Zhang2, Sijia Liu2, Pin-Yu Chen2, Jinjun Xiong2, and
Meng Wang1
1 Rensselaer Polytechnic Institute
2 IBM Research
 , , ,
 , , 
Abstract. When the training data are maliciously tampered, the predictions of the acquired deep neural network (DNN) can be manipulated
by an adversary known as the Trojan attack (or poisoning backdoor attack). The lack of robustness of DNNs against Trojan attacks could signiﬁcantly harm real-life machine learning (ML) systems in downstream
applications, therefore posing widespread concern to their trustworthiness. In this paper, we study the problem of the Trojan network (TrojanNet) detection in the data-scarce regime, where only the weights of
a trained DNN are accessed by the detector. We ﬁrst propose a datalimited TrojanNet detector (TND), when only a few data samples are
available for TrojanNet detection. We show that an eﬀective data-limited
TND can be established by exploring connections between Trojan attack
and prediction-evasion adversarial attacks including per-sample attack as
well as all-sample universal attack. In addition, we propose a data-free
TND, which can detect a TrojanNet without accessing any data samples. We show that such a TND can be built by leveraging the internal
response of hidden neurons, which exhibits the Trojan behavior even at
random noise inputs. The eﬀectiveness of our proposals is evaluated by
extensive experiments under diﬀerent model architectures and datasets
including CIFAR-10, GTSRB, and ImageNet.
Keywords: Trojan attack, adversarial perturbation, interpretability, neuron activation
Introduction
DNNs, in terms of convolutional neural networks (CNNs) in particular, have
achieved state-of-the-art performances in various applications such as image
classiﬁcation , object detection , and modelling sentences . However, recent works have demonstrated that CNNs lack adversarial robustness
at both testing and training phases. The vulnerability of a learnt CNN against
prediction-evasion (inference-phase) adversarial examples, known as adversarial
This paper has been accepted by ECCV 2020.
 
R. Wang et al.
attacks (or adversarial examples), has attracted a great deal of attention .
Eﬀective solutions to defend these attacks have been widely studied, e.g., adversarial training , randomized smoothing , and their variants .
At the training phase, CNNs could also suﬀer from Trojan attacks (known as
poisoning backdoor attacks) , causing erroneous behavior of CNNs
when polluting a small portion of training data. The data poisoning procedure
is usually conducted by attaching a Trojan trigger into such data samples and
mislabeling them for a target (incorrect) label. Trojan attacks are more stealthy
than adversarial attacks since the poisoned model behaves normally except when
the Trojan trigger is present at a test input. Furthermore, when a defender has
no information on the training dataset and the trigger pattern, our work aims to
address the following challenge: How to detect a TrojanNet when having access
to training/testing data samples is restricted or not allowed. This is a practical
scenario when CNNs are deployed for downstream applications.
Some works have started to defend Trojan attacks but have to use a large
number of training data . When training data are inaccessible, a
few recent works attempted to solve the problem of TrojanNet detection in the
absence of training data . However, the existing solutions
are still far from satisfactory due to the following disadvantages: a) intensive
cost to train a detection model, b) restrictions on CNN model architectures, c)
accessing to knowledge of Trojan trigger, d) lack of ﬂexibility to detect various
types of Trojan attacks, e.g., clean-label attack . In this paper, we aim to
develop a uniﬁed framework to detect Trojan CNNs with milder assumptions on
data availability, trigger pattern, CNN architecture, and attack type.
Contributions. We summarize our contributions as below.
– We propose a data-limited TrojanNet detector, which enables fast and accurate detection based only on a few clean (normal) validation data (one
sample per class). We build the data-limited TrojanNet detector (DL-TND)
by exploring connections between Trojan attack and two types of adversarial
attacks, per-sample adversarial attack and universal attack .
– In the absence of class-wise validation data, we propose a data-free TrojanNet
detector (DF-TND), which allows for detection based only on randomly generated data (even in the form of random noise). We build the DF-TND by
analyzing how neurons respond to Trojan attacks.
– We develop a uniﬁed optimization framework for the design of both DL-TND
and DF-TND by leveraging proximal algorithm .
– We demonstrate the eﬀectiveness of our approaches in detecting Trojan-
Nets with various trigger patterns (including clean-label attack) under different network architectures (VGG16, ResNet-50, and AlexNet) and diﬀerent
datasets (CIFAR-10, GTSRB, and ImageNet). We show that both DL-TND
and DF-TND yield 0.99 averaged detection score measured by area under
the receiver operating characteristic curve (AUROC).
Related work. Trojan attacks are often divided into two main categories: triggerdriven attack and clean-label attack . The ﬁrst threat model
Practical Detection of Trojan Neural Networks
Table 1. Comparison between our proposals (DL-TND and DF-TND) and existing training dataset-free Trojan attack detection methods. The comparison is conducted from the following perspectives: Trojan attack type, necessity of validation data
(Dvalid), construction of a new training dataset (Mtrain), dependence on (recovered)
trigger size for detection, demand for training new models (e.g., GAN), and necessity
of searching all neurons.
Applied attack type
Detection conditions
Trigger Clean-label Dvalid New Mtrain Trigger size New models Neuron search
TABOR 
DeepInspect 
stamps a subset of training data with a Trojan trigger and maliciously label
them to a target class. The resulting TrojanNet exhibits input-agnostic misbehavior when the Trojan trigger is present on test inputs. That is, an arbitrary
input stamped with the Trojan trigger would be misclassiﬁed as the target class.
Diﬀerent from trigger-driven attack, the second threat model keeps poisoned
training data correctly labeled. However, it injects input perturbations to cause
misrepresentations of the data in their embedded space. Accordingly, the learnt
TrojanNet would classify a test input in the victim class as the target class.
Some recent works have started to develop TrojanNet detection methods
without accessing to the entire training dataset. References attempted
to identify the Trojan characteristics by reverse engineering Trojan triggers.
Speciﬁcally, neural cleanse (NC) identiﬁed the target label of Trojan attacks
by calculating perturbations of a validation example that causes misclassiﬁcation
toward every incorrect label. It was shown that the corresponding perturbation
is signiﬁcantly smaller for the target label than the perturbations for other labels. The other works considered the similar formulation as NC and
detected a Trojan attack through the strength of the recovered perturbation.
Our data-limited TND is also spurred by NC, but we build a more eﬀective
detection (independent of perturbation size) rule by generating both per-image
and universal perturbations. A meta neural Trojan detection (MNTD) method
is proposed by , which trained a detector using Trojan and clean networks
as training data. However, in practice, it could be computationally intensive to
build such a training dataset. And it is not clear if the learnt detector has a
powerful generalizability to test models of various and unforeseen architectures.
The very recent works made an eﬀort towards detecting TrojanNets
in the absence of validation/test data. In , a generative model was built to
reconstruct trigger-stamped data, and detect the model using the size of the
R. Wang et al.
trigger. In , the concept of universal litmus patterns (ULPs) was proposed
to learn the trigger pattern and the Trojan detector simoutaneously based on
a training dataset consisting of clean/Trojan networks. In , artiﬁcial brain
stimulation (ABS) was used in TrojanNet detection by identifying the compromised neurons responding to the Trojan trigger. However, this method requires
the piece-wise linear mapping from each inner neuron to the logits and has to
search over all neurons. Diﬀerent from the aforementioned works, we propose a
simpler and more eﬃcient detection method without the requirements of building additional models, reconstructing trigger-stamped inputs, and accessing the
test set. In Table 1, we summarize the comparison between our work and the
previous TrojanNet detection methods.
Preliminary and Motivation
In this section, we ﬁrst provide an overview of Trojan attacks and the detector’s
capabilities in our setup. We then motivate the problem of TrojanNet detection.
Trojan attacks
Fig. 1. Examples of poisoned images. (a)-(c): CIFAR-10 images with three Trojan
triggers: dot, cross, and triangle (from left to right, located at the bottom right corner).
(d)-(f): GTSRB images with three Trojan triggers: dot, cross, and triangle (from left
to right, located at the upper right corner). (g)-(i): ImageNet images with watermarkbased Trojan triggers. (j)-(l): Clean-label poisoned images on CIFAR-10 dataset (The
images look like deer and thus will be labeled as ‘deer’ by human. However, the latent
representations are close to the class ‘plane’).
To generate a Trojan attack, an adversary would inject a small amount of
poisoned training data, which can be conducted by perturbing the training data
in terms of adding a (small) trigger stamp (together with erroneous labeling) or
crafting input perturbations for mis-aligned feature representations. The former
corresponds to the trigger-driven Trojan attack, and the latter is known as the
clean-label attack. Fig. 1 (a)-(i) present examples of poisoned images under
Practical Detection of Trojan Neural Networks
diﬀerent types of Trojan triggers, and Fig. 1 (j)-(l) present examples of cleanlabel poisoned images. In this paper, we consider CNNs as victim models in
TrojanNet detection. A well-poisoned CNN contains two features: (1) It is able
to misclassify test images as the target class only if the trigger stamps or images
from the clean-label class are present; (2) It performs as a normal image classiﬁer
during testing when the trigger stamps or images from the clean-label class are
Detector’s capabilities
Once a TrojanNet is learnt over the poisoned training dataset, a desired Trojan-
Net detector should have no need to access the Trojan trigger pattern and the
training dataset. Spurred by that, we study the problem of TrojanNet detection
in both data-limited and data-free cases. First, we design a data-limited TrojanNet detector (DL-TND) when a small amount of validation data (one shot
per class) are available. Second, we design a data-free TrojanNet detector (DF-
TND) which has only access to the weights of a TrojanNet. The aforementioned
two scenarios are not only practical, e.g., when inspecting the trustworthiness
of released models in the online model zoo , but also beneﬁcial to achieve a
faster detection speed compared to existing works which require building a new
training dataset and training a new model for detection (see Table 1).
Motivation from input-agnostic misclassiﬁcation of TrojanNet
Since arbitrary images can be misclassiﬁed as the same target label by Trojan-
Net when these inputs consist of the Trojan trigger used in data poisoning, we
hypothesize that there exists a shortcut in TrojanNet, leading to input-agnostic
misclassiﬁcation. Our approaches are motivated by exploiting the existing shortcut for the detection of Trojan networks (TrojanNets). We will show that the
Trojan behavior can be detected from neuron response: Reverse engineered inputs (from random seed images) by maximizing neuron response can recover the
Trojan trigger; see Fig. 2 for an illustrative example.
Detection of Trojan Networks with Scarce Data
In this section, we begin by examining the Trojan backdoor through the lens of
predictions’ sensitivity to per-image and universal input perturbations. We show
that a small set of validation data (one sample per class) are suﬃcient to detect
TrojanNets. Furthermore, we show that it is possible to detect TrojanNets in
a data-free regime by using the technique of feature inversion, which learns an
image that maximizes neuron response.
Trojan perturbation
Given a CNN model M, let f(·) ∈RK be the mapping from the input space to
the logits of K classes. Let fy denote the logits value corresponding to class y.
R. Wang et al.
Seed Images
Perturbation
Clean Network
Perturbation
Visualization of recovered
trigger-driven images by using DF-
TND given random seed images, including 3 randomly selected CIFAR-
10 images (cells at columns 1-3
and row 1) and 1 random noise
image (cell at column 4 and row
1). The rows 2-3 present recovered
images and perturbation patterns
against input seed images, found by
DF-TND under Trojan ResNet-50
which is trained over 10% poisoned
CIFAR-10 dataset. Here the original
trigger is given by Fig. 1 (b). The
rows 4-5 present results in the same
format as rows 2-3 but obtained by
our apporach under the clean network, which is normally trained over
The ﬁnal prediction is then given by arg maxy fy. Let r(·) ∈Rd be the mapping
from the input space to neuron’s representation, deﬁned by the output of the
penultimate layer (namely, prior to the fully connected block of the CNN model).
Given a clean data x ∈Rn, the poisoned data through Trojan perturbation δ is
then formulated as 
ˆx(m, δ) = (1 −m) · x + m · δ,
where δ ∈Rn denotes pixel-wise perturbations, m ∈{0, 1}n is a binary mask to
encode the position where a Trojan stamp is placed, and · denotes element-wise
product. In trigger-driven Trojan attacks , the poisoned training data
ˆx(m, δ) is mislabeled to a target class to enforce a backdoor during model training. In clean-label Trojan attacks , the variables (m, δ) are designed to
misalign the feature representation r(ˆx(m, δ)) with r(x) but without perturbing
the label of the poisoned training data. We call M a TrojanNet if it is trained
over poisoned training data given by (1).
Data-limited TrojanNet detector: A solution from adversarial
example generation
We next address the problem of TrojanNet detection with the prior knowledge
on model weights and a few clean test images, at least one sample per class.
Let Dk denote the set of data within the (predicted) class k, and Dk−denote
the set of data with prediction labels diﬀerent from k. We propose to design
a detector by exploring how the per-image adversarial perturbation is coupled
Practical Detection of Trojan Neural Networks
Convolution
perturbation
generator:
Neuron activation
vs. universal perturbation
Neuron activation vs.
per-image perturbation
Trojan model
detection: similarity
of  neuron
activations
perturbation
generator:
Convolution
Perturbation
generator by
activation
maximization:
activation
Outlier detection
for Trojan Model
Fig. 3. Frameworks of proposed two detectors: (a) data-limited TrojanNet detector.
(b) data-free TrojanNet detector.
with the universal perturbation due to the presence of backdoor in TrojanNets.
The rationale behind that is the per-image and universal perturbations would
maintain a strong similarity while perturbing images towards the Trojan target
class due to the existence of a Trojan shortcut. The framework is illustrated in
Fig. 3 (a), and the details are provided in the rest of this subsection.
Untargeted universal perturbation. Given images {xi ∈Dk−}, our goal is to ﬁnd
a universal perturbation tuple u(k) = (m(k), δ(k)) such that the predictions of
these images in Dk−are altered given the current model. However, we require
u(k) not to alter the prediction of images belonging to class k, namely, {xi ∈Dk}.
Spurred by that, the design of u(k) = (m(k), δ(k)) can be cast as the following
optimization problem:
ℓatk(ˆx(m, δ); Dk−) + ¯ℓatk(ˆx(m, δ); Dk) + λ∥m∥1
subject to {δ, m} ∈C,
where ˆx(m, δ) was deﬁned in (1), λ > 0 is a regularization parameter that
strikes a balance between the loss term ℓuatk + ¯ℓatk and the sparsity of the
trigger pattern ∥m∥1, and C denotes the constraint set of optimization variables
m and δ, C = {0 ≤δ ≤255, m ∈{0, 1}n}.
We next elaborate on the loss terms ℓatk and ¯ℓatk in problem (2). First, the
loss ℓatk enforces to alter the prediction labels of images in Dk−, and is deﬁned
R. Wang et al.
as the C&W untargeted attack loss 
ℓatk(ˆx(m, δ); Dk−) =
max {fyi(ˆxi(m, δ)) −max
t̸=yi ft(ˆxi(m, δ)), −τ}, (3)
where yi denotes the prediction label of xi, recall that ft(ˆxi(m, δ)) denotes
the logit value of the class t with respect to the input ˆxi(m, δ), and τ ≥0 is
a given constant which characterizes the attack conﬁdence. The rationale behind max {fyi(ˆxi(m, δ)) −maxt̸=yi ft(ˆxi(m, δ)), −τ} is that it reaches a negative value (with minimum −τ) if the perturbed input ˆxi(m, δ) is able to change
the original label yi. Thus, the minimization of ℓatk enforces the ensemble of
successful label change of images in Dk−. Second, the loss ¯ℓatk in (2) is proposed
to enforce the universal perturbation not to change the prediction of images in
Dk. This yields
¯ℓatk(ˆx(m, δ); Dk) =
t̸=k ft(ˆxi(m, δ)) −fyi(ˆxi(m, δ)), −τ},
where recall that yi = k for xi ∈Dk. We present the rationale behind (3) and
(4) as below. Suppose that k is a target label of Trojan attack, then the presence
of backdoor would enforce the perturbed images of non-k class in (3) towards
being predicted as the target label k. However, the universal perturbation (performed like a Trojan trigger) would not aﬀect images within the target class k,
as characterized by (4).
Targeted per-image perturbation. If a label k is the target label speciﬁed by the
Trojan adversary, we hypothesize that perturbing each image in Dk−towards
the target class k could go through the similar Trojan shortcut as the universal
adversarial examples found in (2). Spurred by that, we generate the following
targeted per-image adversarial perturbation for xi ∈Dk,
atk(ˆx(m, δ); xi) + λ∥m∥1
subject to {δ, m} ∈C,
atk(ˆx(m, δ); xi) is the targeted C&W attack loss 
atk(ˆx(m, δ); xi) =
t̸=k ft(ˆxi(m, δ)) −fk(ˆxi(m, δ)), −τ}.
For each pair of label k and data xi, we can obtain a per-image perturbation
tuple s(k,i) = (m(k,i), δ(k,i)).
For solving both problems of universal perturbation generation (2) and perimage perturbation generation (5), the promotion of λ enforces a sparse perturbation mask m. This is desired when the Trojan trigger is of small size, e.g.,
Fig.1-(a) to (f). When the Trojan trigger might not be sparse, e.g., Fig.1-(g) to
(i), multiple values of λ can also be used to generate diﬀerent sets of adversarial perturbations. Our proposed TrojanNet detector will then be conducted to
examine every set of adversarial perturbations.
Practical Detection of Trojan Neural Networks
Detection rule. Let ˆxi(u(k)) and ˆxi(s(k,i)) denote the adversarial example of xi
under the the universal perturbation u(k) and the image-wise perturbation s(k,i),
respectively. If k is the target label of the Trojan attack, then based on our similarity hypothesis, u(k) and s(k,i) would share a strong similarity in fooling the
decision of the CNN model due to the presence of backdoor. We evaluate such a
similarity from the neuron representation against ˆxi(u(k)) and ˆxi(s(k,i)), given by
 r(ˆxi(u(k))), r(ˆxi(s(k,i)))
, cos(·, ·) represents cosine similarity. Here
recall that r(·) denotes the mapping from the input image to the neuron representation in CNN. For any xi ∈Dk−, we form the vector of similarity scores
sim = {v(k)
}i. Fig. 7 shows the neuron activation of ﬁve data samples with
the universal perturbation and per-image perturbation under a target label, a
non-target label, and a label under the clean network (cleanNet). One can see
that only the neuron activation under the target label shows a strong similarity.
Fig. 4 also provides a visualization of v(k)
sim for each label k.
Given the similarity scores v(k)
sim for each label k, we detect whether or not
the model is a TrojanNet (and thus k is the target class) by calculating the socalled detection index I(k), given by the q%-percentile of v(k)
sim. In experiments,
we choose q = 25, 50, 70. The decision for TrojanNet is then made by I(k)≥T1 for
a given threshold T1, and accordingly k is the target label. We can also employ
the median absolute deviation (MAD) method to v(k)
sim to mitigate the manual
speciﬁcation of T1. The details are shown in the Appendix.
Detection of Trojan networks for free: A solution from feature
inversion against random inputs
The previously introduced data-limited TrojanNet detector requires to access
clean data of all K classes. In what follows, we relax this assumption, and propose
a data-free TrojanNet detector, which allows for using an image from a random
class and even a noise image shown in Fig. 2. The framework is summarized in
Fig. 3 (b), and details are provided in what follows.
It was previously shown in that a TrojanNet exhibits an unexpectedly
high neuron activation at certain coordinates. That is because the TrojanNet
produces robust representation towards the input-agnostic misclassiﬁcation induced by the backdoor. Given a clean data x, let ri(x) denote the ith coordinate
of neuron activation vector. Motivated by , we study whether or not an
inverted image that maximizes neuron activation is able to reveal the characteristics of the Trojan signature from model weights. We formulate the inverted
image as ˆx(m, δ) in (1), parameterized by the pixel-level perturbations δ and
the binary mask m with respect to x. To ﬁnd ˆx(m, δ), we solve the problem of
activation maximization
i=1 [wiri(ˆx(m, δ))] −λ∥m∥1
subject to {δ, m} ∈C, 0 ≤w ≤1, 1T w = 1,
where the notations follow (2) except the newly introduced variables w, which
adjust the importance of neuron coordinates. Note that if w = 1/d, then the
R. Wang et al.
ﬁrst loss term in (7) becomes the average of coordinate-wise neuron activation.
However, since the Trojan-relevant coordinates are expected to make larger impacts, the corresponding variables wi are desired for more penalization. In this
sense, the introduction of self-adjusted variables w helps us to avoid the manual
selection of neuron coordinates that are most relevant to the backdoor.
Detection rule. Let the vector tuple p(i) = (m(i), δ(i)) be a solution of problem
(7) given at a random input xi for i ∈{1, 2, . . . , N}. Here N denotes the number
of random images used in TrojanNet detection. We then detect if a model is
TrojanNet by investigating the change of logits outputs with respect to xi and
ˆxi(p(i)), respectively. For each label k ∈[K], we obtain
[fk(ˆxi(p(i))) −fk(xi)].
The decision of TrojanNet with the target label k is then made according to
Lk ≥T2 for a given threshold T2. We ﬁnd that there exists a wide range of the
proper choice of T2 since Lk becomes an evident outlier if the model contains
a backdoor with respect to the target class k; see Figs. 8 and 9 for additional
justiﬁcations.
A uniﬁed optimization framework in TrojanNet detection
In order to build TrojanNet detectors in both data-limited and data-free settings,
we need to solve a sparsity-promoting optimization problem, in the speciﬁc forms
of (2), (5), and (7), subject to a set of box and equality constraints. In what
follows, we propose a general optimization method by leveraging the idea of
proximal gradient .
Consider a problem with the generic form of problems (2), (5), and (7),
m,δ,w F(δ, m, w) + λ∥m∥1 + I(δ) + I(m) + I′(w),
where F(δ, m, w) denotes the smooth loss term, and I(x), I′(w) denote the
indicator functions to encode the hard constraints
0 x ∈[0, α]n
∞otherwise,
0 w ∈ n, 1T w = 1
∞otherwise.
In I(x), α = 1 for m and α = 255 for δ. We remark that the binary constraint
m ∈{0, 1}n is relaxed to a continuous probabilistic box m ∈ n.
To solve problem (9), we adopt the alternative proximal gradient algorithm
 , which splits the smooth-nonsmooth composite structure into a sequence of
easier problems that can be solved more eﬃciently or even analytically. To be
more speciﬁc, we alternatively perform
m(t+1) = Proxµt(I+λ∥·∥1)(m(t) −µt∇mF(δ(t), m(t)))
δ(t+1) = ProxµtI(δ(t) −µt∇δF(δ(t), m(t+1)))
w(t+1) = ProxµtI′(w(t) + µt∇wF(δ(t+1), m(t+1), w(t))),
Practical Detection of Trojan Neural Networks
where µt denotes the learning rate at iteration t, and Proxµg(a) denotes the
proximal operator of function g with respect to the parameter µ at an input a.
We next elaborate on the proximal operators used in (11)-(13). The proximal
operator Proxµt(I+λ∥·∥1)(a) is given by the solution to the problem
I(m) + λ∥m∥1 +
where a := m(t) −µt∇mF(δ(t), m(t)). The solution to problem (14), namely,
m(t+1) is given by 
= Clip (sign(ai) max {|ai| −λµt, 0}), ∀i,
where m(t+1)
dentoes the ith entry of m(t+1), and Clip is a clip function
that clip the variable to 1 if it is larger than 1 and to 0 if it is smaller than 0.
Similarly, δ(t+1) in (12) is obtained by
= Clip (bi),
where b := δ(t) −µt∇δF(δ(t), m(t+1)).
The proximal operator ProxµtI′(c) in (13) is given by the solution to the
which is equivalent to
s.t. 0 ≤w ≤1, 1T w = 1.
Here c := w(t) + µt∇wF(δ(t+1), m(t+1), w(t)). The solution to problem (18) is
given by 
w(t+1) = [c −µ1]+ ,
where [a]+ denotes the operation of max{0, a}, and µ is the root of the equation
1T [c −µ1]+ = P
i max{0, ci −µ} = 1.
Substituting (15), (16) and (19) into (11)-(13), we then obtain the complete
algorithm, in which each step has a closed-form.
Experimental Results
In this section, We validate the DL-TND and DF-TND by using diﬀerent CNN
model architectures, datasets, and various trigger patterns3.
3 The code is available at: 
R. Wang et al.
Data-limited TrojanNet detection (DL-TND)
Trojan settings. Testing models include VGG16 , ResNet-50 , and
AlexNet . Datasets include CIFAR-10 , GTSRB , and Restricted ImageNet (R-ImgNet) (restricting ImageNet to 9 classes). We trained 85 TrojanNets and 85 clean networks, respectively. The numbers of diﬀerent models
are shown in Table 6. Fig. 1 (a)-(f) show the CIFAR-10 and GTSRB dataset
with triggers of dot, cross, and triangle, respectively. One of these triggers is
used for poisoning the model. We also test models poisoned for two target labels simultaneously: the dot trigger is used for one target label, and the cross
trigger corresponds to the other target label. Fig. 1 (g)-(i) show poisoned ImageNet samples with the watermark as the trigger. The TrojanNets are various by
specifying triggers with diﬀerent shapes, colors, and positions. The data poisoning ratio also varies from 10% −12%. The cleanNets are trained with diﬀerent
batches, iterations, and initialization. Table 7 summarizes test accuracies and attack success rates of our generated Trojan and cleanNets. We compare DL-TND
with the baseline Neural Cleanse (NC) for detecting TrojanNets.
Visualization of similarity scores’ distribution. Fig. 4 shows the distribution of our detection statistics, namely, representation similarity scores v(k)
deﬁned in Sec. 3.2, for diﬀerent class labels. As we can see, the distribution
corresponding to the target label 0 in the TrojanNet concentrates near 1, while
the other labels in the TrojanNet and all the labels in the cleanNets have more
dispersed distributions around 0. Thus, we can distinguish the TrojanNet from
the cleanNets and further ﬁnd the target label.
Fig. 4. Distribution of similarity scores for clean-
Net versus TrojanNet under diﬀerent classes.
Fig. 5. ROC curve for TrojanNet
detection using DL-TND.
Detection performance. To build DL-TND, we use 5 validation data points for
each class of CIFAR-10 and R-ImgNet, and 2 validation data points for each class
of GTSRB. Following Sec. 3.2, we set I(k) to quantile-0.25, median, quantile-
0.75 and vary T1. Let the true positive rate be the detection success rate for
TrojanNets and the false negative rate be the detection error rate for cleanNets.
Then the area under the curve (AUC) of receiver operating characteristics (ROC)
Practical Detection of Trojan Neural Networks
can be used to measure the performance of the detection. Table 2 shows the AUC
values, where “Total” refers to the collection of all models from diﬀerent datasets.
Table 2. AUC values for TrojanNet detection and
target label detection, given in the format (·, ·).
The detection index for each class is selected as
Quantile (Q) = 0.25, Q = 0.5, and Q = 0.75 of the
similarity scores (illustrated in Fig. 4).
(0.99, 0.99)
Q = 0.75 (1, 0.98)
(0.99, 0.97) (0.99, 0.98)
We plot the ROC curve of
the “Total” in Fig. 5. The results show that DL-TND can
perform well across diﬀerent
datasets and model architectures. Moreover, ﬁxing I(k)
as median, T1
0.896 could provide a detection success rate over 76.5%
for TrojanNets and a detection success rate over 82% for cleanNets. Table 3 shows the comparisons of
DL-TND to Neural Cleanse (NC) on TrojanNets and cleanNets (T1 = 0.7).
Even using the MAD method as the detection rule, we ﬁnd that DL-TND greatly
outperforms NC in detection tasks of both TrojanNets and cleanNets (Note that
NC also uses MAD). The results are shown in Table 8.
Table 3. Comparisons between DL-TND and NC on TrojanNets and cleanNets
using T1 = 0.7. The results are reported in the format (number of correctly detected
models)/(total number of models)
DL-TND (clean) DL-TND (Trojan) NC (clean) NC (Trojan)
CIFAR-10 ResNet-50
ImageNet ResNet-50
Data-free TrojanNet detector (DF-TND)
Trojan settings. The DF-TND is tested on cleanNets and TrojanNets that
are trained under CIFAR-10 and R-ImgNet (with 10% poisoning ratio unless
otherwise stated). We perform the customized proximal gradient method shown
in Sec. 3.4 to solve problem (7), where the number of iterations is set as 5000.
Revealing Trojan trigger. Recall from Fig. 2 that the trigger pattern can be
revealed by input perturbations that maximize neuron response of a TrojanNet.
By contrast, the perturbations under the cleanNets behave like random noises.
Fig. 6 provides visualizations of recovered inputs by neuron maximization at a
R. Wang et al.
TrojanNet versus a cleanNet on CIFAR-10 and ImageNet datasets. The key insight is that for a TrojanNet, it is easy to ﬁnd an inverted image (namely, feature
inversion) by maximizing neurons activation via (7) to reveal the Trojan characteristics (e.g., the shape of a Trojan trigger) compared to the activation from
a cleanNet. Fig. 6 shows such results are robust to the choice of inputs (even for
a noise input). We observe that the recovered triggers may have diﬀerent colors
and locations diﬀerent from the original trigger. This is possibly because the
trigger space has been shifted and enlarged by using convolution operations. In
Figs. 12 and Fig. 13, we also provide additional experimental results for the sensitivity of trigger locations and sizes. Furthermore, we show some improvements
of using the reﬁne method in Fig. 14.
CIFAR-10 input (32 × 32)
ImageNet input (224 × 224)
Random noise input (32 × 32)
Random noise input (224 × 224)
Seed Images
(cleanNet)
Perturbation
(cleanNet)
(TrojanNet)
Perturbation
(TrojanNet)
True trigger:
Triangle in Fig. 1 (e)
Cross trigger in
Fig. 1 (f)
Watermark trigger
in Fig. 1 (h)
Watermark trigger
in Fig. 1 (j)
Triangle trigger in
Fig. 1 (e)
Cross trigger in
Fig. 1 (f)
Watermark trigger
in Fig. 1 (h)
Watermark trigger
in Fig. 1 (j)
Fig. 6. Visualization of recovered input images by using our proposed DF-TND method
under random seed images. Here the Trojan ResNet-50 models are trained by 10% poisoned data (by adding the trigger patterns shown as Fig. 1) and clean data, respectively.
First row: Seed input images (from left to right: 2 randomly selected CIFAR-10 images, 2 randomly selected ImageNet images, 2 random noise images in CIFAR-10 size,
2 random noise images in ImageNet size). Second row: Recovered images under clean-
Nets. Third row: Perturbation patterns given by the diﬀerence between the recovered
images in the second row and the original seed image. Fourth row: Recovered images
under TrojanNets. Fifth row: Perturbation patterns given by the diﬀerence between
the recovered images in the fourth row and the original seed images. Trigger patterns
can be revealed using our method under the TrojanNet, and such a Trojan signature
is not contained in the cleanNet. The trigger information is listed in the last row and
triggers are visualized in Fig. 1
Detection performance. We now test 1000 seed images on 10 TrojanNets and
10 cleanNets using DF-TND deﬁned in Sec. 3.3. We compute AUC values of
Practical Detection of Trojan Neural Networks
DF-TND by choosing seed images as clean validation inputs and random noise
inputs, respectively.
Table 4. AUC for DF-TND over CIFAR-
10 and R-ImgNet classiﬁcation models using clean validation images and random
noise images, respectively
clean validation data
random noise inputs
Results are summarized in Table
4, and the ROC curves are shown in
Additional results on
DL-TND and DF-TND
First, we apply DL-TND and DF-TND
on detecting TrojanNets with diﬀerent levels attack success rate (ASR). We control ASR by choosing diﬀerent data poisoning ratios when generating a Trojan-
Net. The results are summarized in Table 5. As we can see, our detectors can
still achieve competitive performance when the attack likelihood becomes small,
and DL-TND is better than DF-TND when ASR reaches 30%.
Table 5. Comparison between DL-TND
and DF-TND on models at diﬀerent attack
success rate
poisoning ratio
0.5% 0.7% 1% 10%
average attack success rate 30% 65% 82% 99%
AUC for DL-TND
0.82 0.91 0.95 0.99
AUC for DF-TND
0.91 0.94 0.99
Moreover, we conduct experiments
when the number of TrojanNets is
much less than the total number of
models, e.g., only 5 out of 55 models
are poisoned. We ﬁnd that the AUC
value of the precision-recall curves are
0.97 and 0.96 for DL-TND and DF-
TND, respectively. Similarly, the average AUC value of the ROC curves is
0.99 for both detectors.
Third, we evaluate our proposed DF-TND to detect TrojanNets generalized
by clean-label Trojan attacks . We ﬁnd that even in the least information
case, DF-TND can still yield 0.92 AUC score when detecting 20 TrojanNets from
40 models.
Conclusion
Trojan attack injects a backdoor into DNNs during the training process, therefore leading to unreliable learning systems. Considering the practical scenarios
where a detector is only capable of accessing to limited data information, this
paper proposes two practical approaches to detect TrojanNets. We ﬁrst propose
a data-limited TrojanNet detector (DL-TND) that can detect TrojanNets with
only a few data samples. The eﬀectiveness of the DL-TND is achieved by drawing
a connection between Trojan attack and prediction-evasion adversarial attacks
including per-sample attack as well as all-sample universal attack. We ﬁnd that
both input perturbations obtained from per-sample attack and from universal
attack exhibit Trojan behavior, and can thus be used to build a detection metric. We then propose a data-free TrojanNet detector (DF-TND), which leverages
neuron response to detect Trojan attack, and can be implemented using random
data samples and even random noise. We use the proximal gradient algorithm
R. Wang et al.
as a general optimization framework to learn DL-TND and DF-TND. The effectiveness of our proposals has been demonstrated by extensive experiments
conducted under various datasets, Trojan attacks, and model architectures.
Acknowledgement
This work was supported by the Rensselaer-IBM AI Research Collaboration
( part of the IBM AI Horizons Network ( 
biz/AIHorizons). We would also like to extend our gratitude to the MIT-IBM
Watson AI Lab ( for the general support of computing resources.
Practical Detection of Trojan Neural Networks