Received: 5 December 2007,
Revised: 24 March 2008,
Accepted: 29 March 2008,
Published online in Wiley InterScience: 22 May 2008
Predicting linear B-cell epitopes using string
Yasser EL-Manzalawya,b,d,e*, Drena Dobbs c,d,e and Vasant Honavara,b,c,e
The identiﬁcation and characterization of B-cell epitopes play an important role in vaccine design, immunodiagnostic
tests, and antibody production. Therefore, computational tools for reliably predicting linear B-cell epitopes are highly
desirable. We evaluated Support Vector Machine (SVM) classiﬁers trained utilizing ﬁve different kernel methods using
ﬁvefold cross-validation on a homology-reduced data set of 701 linear B-cell epitopes, extracted from Bcipep database,
and 701 non-epitopes, randomly extracted from SwissProt sequences. Based on the results of our computational
experiments, we propose BCPred, a novel method for predicting linear B-cell epitopes using the subsequence kernel.
We show that the predictive performance of BCPred (AUC ¼ 0.758) outperforms 11 SVM-based classiﬁers developed
and evaluated in our experiments as well as our implementation of AAP (AUC ¼ 0.7), a recently proposed method for
predicting linear B-cell epitopes using amino acid pair antigenicity. Furthermore, we compared BCPred with AAP and
ABCPred, a method that uses recurrent neural networks, using two data sets of unique B-cell epitopes that had been
previously used to evaluate ABCPred. Analysis of the data sets used and the results of this comparison show that
conclusions about the relative performance of different B-cell epitope prediction methods drawn on the basis of
experiments using data sets of unique B-cell epitopes are likely to yield overly optimistic estimates of performance of
evaluated methods. This argues for the use of carefully homology-reduced data sets in comparing B-cell epitope
prediction methods to avoid misleading conclusions about how different methods compare to each other. Our
homology-reduced data set and implementations of BCPred as well as the APP method are publicly available through
our web-based server, BCPREDS, at: Copyright # 2008 John Wiley & Sons, Ltd.
Keywords: linear B-cell epitope; epitope mapping; epitope prediction
INTRODUCTION
B-cell epitopes are antigenic determinants that are recognized
and bound by receptors (membrane-bound antibodies) on the
surface of B lymphocytes . There are many
different types of B-cell receptors, but each B-cell produces only
one type. When a B-cell receptor binds its cognate antigen, the
B-cell is stimulated to undergo proliferation. This involves the
generation of two types of cells, effector or plasma B-cells, which
produce and secrete soluble antibodies, and memory B-cells,
which remain in the organism and can proliferate rapidly if
re-exposed to antigen. Hence, understanding the sequence and
structural features of B-cell epitopes is critical both for the design
of effective vaccines and for the development of sensitive
diagnostic tests.
B-cell epitopes can be classiﬁed into two types: linear
(continuous) epitopes and conformational (discontinuous) epitopes. Linear epitopes are short peptides, corresponding to a
contiguous amino acid sequence fragment of a protein . In contrast, conformational
epitopes are composed of amino acids that are not contiguous in
primary sequence, but are brought into close proximity within the
folded protein structure. Although it is believed that a large majority
of B-cell epitopes are discontinuous , experimental
epitope identiﬁcation has focused primarily on linear B-cell
epitopes . Even in the case of linear B-cell epitopes,
however, antibody–antigen interactions are often conformationdependent. The conformation-dependent aspect of antibody
binding complicates the problem of B-cell epitope prediction,
making it less tractable than T-cell epitope prediction. Therefore,
the development of reliable computational methods for predicting
linear B-cell epitopes is an important challenge in bioinformatics
and computational biology .
Several studies have reported correlations between certain
physicochemical properties of amino acids and the locations of
linear B-cell epitopes within protein sequences , and several epitope prediction methods
based on physicochemical properties of amino acids have been
proposed. For example, hydrophilicity, ﬂexibility, turns, or solvent
accessibility propensity scales were used in the methods of Parker
(www.interscience.wiley.com) DOI:10.1002/jmr.893
Research Article
Correspondence to: Y. EL-Manzalawy, 226 Atanasoff Hall, Iowa State University, Ames, IA 50010, USA.
E-mail: 
Y. EL-Manzalawy, V. Honavar
Artificial Intelligence Laboratory, Iowa State University, Ames, IA 50010, USA
b Y. EL-Manzalawy, V. Honavar
Department of Computer Science, Iowa State University, Ames, IA 50010, USA
Department of Genetics, Development and Cell Biology, Iowa State University,
Ames, IA 50010, USA
Y. EL-Manzalawy, D. Dobbs, V. Honavar
Bioinformatics and Computational Biology Graduate Program, Iowa State
University, Ames, IA 50010, USA
Y. EL-Manzalawy, D. Dobbs, V. Honavar
Center for Computational Intelligence, Learning and Discovery, lowa State
University, Ames, IA 50010, USA
J. Mol. Recognit. 2008; 21: 243–255
Copyright # 2008 John Wiley & Sons, Ltd.
et al., , Karplus and Schulz , Pellequer et al. 
and Emini et al. , respectively. PREDITOP , PEOPLE , BEPITOPE , and BcePred predict
linear B-cell epitopes based on groups of physicochemical
properties instead of a single property.
Recently, Blythe and Flower performed an exhaustive
assessment of 484 amino acid propensity scales, combined with
ranges of proﬁle parameters, to examine the correlation between
propensity scale-based proﬁles and the location of linear B-cell
epitopes in a set of 50 proteins. They reported that for predicting
B-cell epitopes based on amino acid sequence information, even
the best combinations of amino acid propensities performed only
marginally better than random. They concluded that the
reported performance of such methods in the literature is likely
to have been overly optimistic, in part due to the small size of the
data sets on which the methods had been evaluated.
Motivated by Blythe and Flower results and the
increasing availability of experimentally identiﬁed linear B-cell
epitopes, several studies have attempted to improve the accuracy
of linear B-cell epitope prediction methods using machine
learning approaches. BepiPred combines two
amino acid propensity scales and a Hidden Markov Model (HMM)
trained on linear epitopes to yield a slight improvement in
prediction accuracy relative to techniques that rely on analysis of
amino acid physicochemical properties. ABCPred uses artiﬁcial neural networks for predicting
linear B-cell epitopes. Both feed-forward and recurrent neural
networks were evaluated on a non-redundant data set of 700
B-cell epitopes and 700 non-epitope peptides, using ﬁvefold
cross-validation tests. Input sequence windows ranging from 10
to 20 amino acids, were tested and the best performance, 66%
accuracy, was obtained using a recurrent neural network trained
on peptides 16 amino acids in length. In the method of So¨llner
and Mayer , each epitope is represented using a set of 1487
features extracted from a variety of propensity scales, neighborhood matrices, and respective probability and likelihood values.
Of two machine learning methods tested, decision trees and a
nearest-neighbor method combined with feature selection, the
latter was reported to attain an accuracy of 72% on a data set of
1211 B-cell epitopes and 1211 non-epitopes, using a ﬁvefold
cross-validation test . Chen et al. 
observed that certain amino acid pairs (AAPs) tend to occur more
frequently in B-cell epitopes than in non-epitope peptides. Using
propensity
observation,
combination with a support vector machine (SVM) classiﬁer,
they reported prediction accuracy of 71% on a data set of 872
B-cell epitopes and 872 non-B-cell epitopes, estimated using
ﬁvefold cross-validation. In addition, Chen et al. demonstrated an improvement in the prediction accuracy, 72.5%, when
the APP propensity scale is combined with turns, accessibility,
antigenicity, hydrophilicity, and ﬂexibility propensity scales.
In this report, we present BCPred, a method for predicting
linear B-cell epitopes using an SVM machine learning method.
Although the performance of SVM-based classiﬁers largely
depends on the selection of the kernel function, there are no
theoretical foundations for choosing good kernel functions in a
data-dependent way. Therefore, one objective of this study was
to explore a class of kernel methods, namely string kernels, in
addition to the widely used radial bias function (RBF) kernel. Our
choice of string kernels was motivated by their successful
application in a number of bioinformatics classiﬁcation tasks,
including protein remote homology detection , protein structure prediction , protein binding site prediction , and
histocompatibility
prediction . In addition, we introduce
the subsequence kernel (SSK), which has been successfully used
classiﬁcation
under-explored
macromolecular
classiﬁcation
applications. Our empirical results demonstrate superior performance of SSK over other string kernels and the RBF kernel.
Hence, we employed the SSK in building SVM classiﬁers for our
proposed linear B-cell epitope prediction method, BCPred.
A second goal of this study was to determine how existing
methods for linear B-cell epitope prediction compare with each
other and with BCPred. At present, little is known about the
relative performance of different methods, due to the lack of
published direct comparisons using standard benchmark data
sets. Unfortunately, neither the data set used by So¨llner and
Mayer nor the code used for generating and selecting the
features used to represent epitope peptides as input to the
classiﬁers is publicly available. The code for the AAP method
 is also not publicly available; however, in
contrast to the other methods, it is relatively straightforward for
implementation. Fortunately, although the code used to train the
neural network classiﬁer used in ABCPred is not publicly available,
Saha and Raghava have made available the data set used
for developing and evaluating the ABCPred server, as well as a
blind test set . Thus, although we are
unable to include direct comparisons with results of So¨llner and
Mayer , in this paper we report direct
comparisons of the ABCPred method ,
our implementation of the AAP method of Chen et al. , and
our proposed BCPred method, using the ABCPred data sets made
publicly available by Saha and Raghava .
Homology-reduced data sets
Bcipep database contains 1230 unique linear
B-cell epitopes. We retrieved a set of 947 unique epitopes with
each epitope satisfying one of the following two conditions:
(i) the epitope length is at least 20 amino acids; or (ii) the epitope
is less than 20 amino acids in length and the accession number of
the source antigen is provided.
A set of 20-mer peptides was derived from the 947 unique
epitopes by: (i) truncating epitopes longer than 20 residues by
removing amino acids from both ends to yield a 20-mer from the
middle, and (ii) extending epitopes shorter than 20 residues by
adding amino acids on both ends, based on the corresponding
complete antigen sequences retrieved from SwissProt . Because the resulting data set of 947 20-mer
peptides was no longer non-redundant, we removed duplicated
and highly homologous peptides by ﬁltering the data set based
on an 80% sequence identity cutoff using the CD-HIT program
 to obtain a homology-reduced data set of 701
peptides (positive instances of B-cell epitopes). A total of 701
non-epitope peptides were generated by randomly extracting
20-mer peptides from sequences in SwissProt database while ensuring that none of the negative
instances so obtained also occur in the positive instances.
www.interscience.wiley.com/journal/jmr
Copyright # 2008 John Wiley & Sons, Ltd.
J. Mol. Recognit. 2008; 21: 243–255
Y. EL-MANZALAWY ET AL.
Because there is no evidence that 20 amino acids is the optimal
length for B-cell epitopes, we decided to experiment with
different epitope lengths. Variants of the 20-mer data set were
generated by repeating the above procedure for peptide lengths
of 18,16, 14, or 12 residues. For the sake of brevity, we will refer to
these data sets by BCPnn, where nn is a two digit number
representing the length of the peptides in the data set (e.g.,
BCP16 refers to the homology-reduced data set where each
peptide is composed of 16 residues).
It should be noted that deriving a data set of shorter peptides
from the 20-mer data set by trimming amino acids from both
termini of each peptide is not guaranteed to produce a data
set with <80% sequence identity because such trimming
could increase the similarity between two peptides in the data
set. Therefore, to ensure that the resulting data sets are
homology-reduced, we reapplied the 80% sequence identity
cutoff ﬁlter in generating each data set of epitopes less than 20
residues in length. The resulting homology-reduced data sets
(BCP20, BCP18, BCP16, BCP14, and BCP12) are available at http://
ailab.cs.iastate.edu/bcpreds/.
ABCPred data set
Saha and Raghava have made available the data sets
used to train and evaluate ABCPred. Because the best reported
performance of ABCPred was obtained using a 16-mer peptide
data set (ABCP16), we chose this data set for directly comparing
ABCPred with BCPred and AAP using ﬁvefold
cross-validation.
Blind test set
Saha and Raghava have made available a blind test set
comprising 187 epitopes, none of which were used in training the
ABCPred method, and a set of 200 16-mer non-epitope peptides
extracted from the non-allergen data set of Bjo¨rklund et al. .
B-cell epitopes less than 16 amino acids in length were extended
to 16-mer peptides by adding an equal number of residues to
both ends based on the protein sequence of the source antigen.
In the remaining text, we will use the abbreviation (SBT16) to refer
to Saha 16-mer blind test set.
Support vector machines and kernel methods
Support vector machines (SVMs) are a class of
supervised machine learning methods used for classiﬁcation and
regression. Given a set of labeled training data (xi,yi), where xi 2 Rd
and yi 2 fþ1; 1g, training an SVM classiﬁer involves ﬁnding a
hyperplane that maximizes the geometric margin between
positive and negative training data samples. The hyperplane is
described as fðxÞ ¼ w; x
i þ b, where w is a normal vector and b
is a bias term. A test instance, x, is assigned a positive label if f
(x) > 0, and a negative label otherwise. When the training data are
not linearly separable, a kernel function is used to map
nonlinearly separable data from the input space into a feature
space. Given any two data samples xi and xj in an input space
X 2 Rd, the kernel function K returns Kðxi; xjÞ ¼ FðxiÞ; FðxjÞ
where F is a nonlinear map from the input space X to the
corresponding feature space. The kernel function K has the
property that K(xi,xj) can be computed without explicitly mapping
xi and xj into the feature space, but instead, using their dot
in the input space. Therefore, the kernel trick
allows us to train a linear classiﬁer, e.g., SVM, in a highdimensional feature space where the data are assumed to be
linearly separable without explicitly mapping each training
example from the input space into the feature space. This
approach relies implicitly on the selection of a feature space in
which the training data are likely to be linearly separable (or
nearly so) and explicitly on the selection of the kernel function
to achieve such separability. Unfortunately, there is no single
kernel that is guaranteed to perform well on every data set.
Consequently, the SVM approach requires some care in selecting
a suitable kernel and tuning the kernel parameters (if any).
String kernels
String kernels are a class of kernel methods that have
been successfully used in many sequence classiﬁcation tasks
 . In these applications, a
protein sequence is viewed as a string deﬁned on a ﬁnite
alphabet of 20 amino acids. In this work, we explore four string
kernels: spectrum , mismatch , local alignment , and subsequence
 , in predicting linear B-cell epitopes. The
subsequence kernel has proven useful in text
classiﬁcation and natural language processing
 . However, to the best of our knowledge, this
kernel has not been previously explored in the context of
macromolecular
classiﬁcation
description of the four kernels follows.
Spectrum kernel
Let A denote a ﬁnite alphabet, e.g., 20 amino acids. x and y denote
two strings deﬁned on the alphabet A. For k  1, the k-spectrum
is deﬁned as :
Fk ¼ ðfaðxÞÞa2Ak
where fa is the number of occurrences of the k-length substring
a in the sequence x. The k-spectrum kernel of the two sequences
x and y is obtained by taking the dot product of the
corresponding k spectra:
ðx; yÞ ¼ FkðxÞ; FkðyÞ
Intuitively, this kernel captures a simple notion of string
similarity: two strings are deemed similar (i.e., have a high
k-spectrum kernel value) if they share many of the same k-length
substrings.
Mismatch kernel
The mismatch kernel is a variant of the
Speciﬁcally, the (k, m)-mismatch kernel allows up to m  k
mismatches to occur when comparing two k-length substrings.
Let a be a k-length substring, the (k, m)-mismatch feature map is
deﬁned on a as:
Fðk;mÞðaÞ ¼ ðfbðaÞÞb2Ak
where fbðaÞ ¼ 1 if b 2 Nðk;mÞðaÞ, where b is the set of k-mer
substrings that differs from a by at most m mismatches. Then, the
feature map of an input sequence x is the sum of the feature
J. Mol. Recognit. 2008; 21: 243–255
Copyright # 2008 John Wiley & Sons, Ltd.
www.interscience.wiley.com/journal/jmr
B-CELL EPITOPES PREDICTION METHODS
vectors for k-mer substrings in x:
Fðk;mÞðxÞ ¼
kmers a in x
The (k, m)-mismatch kernel is deﬁned as the dot product of the
corresponding feature maps in the feature space:
ðx; yÞ ¼ Fðk;mÞðxÞ; Fðk;mÞðyÞ
It should be noted that the (k, 0)-mismatch kernel results in a
feature space that is identical to that of the k-spectrum kernel. An
efﬁcient data structure for computing the spectrum and
O(|x|þ|y|)
Oðkmþ1jAjmðjxj þ jyjÞÞ,
respectively, has been provided by Leslie et al. .
Local alignment kernel
Local alignment (LA) kernel is a string kernel
adapted for biological sequences. The LA kernel measures the
similarity between two sequences by summing up scores
obtained from gapped local alignments of the sequences. This
kernel has several parameters: the gap opening and extension
penalty parameters, d and e, the amino acid mutation matrix s,
and the factor b, which controls the inﬂuence of suboptimal
alignments on the kernel value. Detailed formulation of the LA
kernel and a dynamic programming implementation of the
kernel with running time complexity in O(jxjjyj) have been
provided by Saigo et al. .
Subsequence kernel
The subsequence kernel generalizes the
k-spectrum kernel by considering a feature space generated by
the set of all (contiguous and non-contiguous) k-mer subsequences. For example, if we consider the two strings ‘‘act’’ and
‘‘acctct’’, the value returned by the spectrum kernel with k ¼ 3 is 0.
On the other hand, the (3, 1)-mismatch kernel will return 3
because the 3-mer substrings ‘‘acc’’‘, ‘‘cct’’, and ‘‘tct’’ have at most
one mismatch when compared with ‘‘act’’. The subsequence
kernel considers the set (‘‘ac-t’’, ‘‘a-ct’’, ‘‘ac—t’’, ‘‘a-c–t’’, ‘‘a—ct’’) of
non-contiguous substrings and returns a similarity score that is
weighted by the length of each non-contiguous substring.
Speciﬁcally, it uses a decay factor, l  1, to penalize noncontiguous substring matches. Therefore, the subsequence
kernel with k ¼ 3 will return 2l4 þ 3l6 when applied to ‘‘act’’
and ‘‘acctct’’ strings. More precisely, the feature map Fk of a string
x is given by
Fðk;lÞðxÞ ¼
where u ¼ x(i) denotes a substring in x where 1  i1 <    <
ijuj  jxj such that uj ¼ sij, for j ¼ 1,. . ., juj and l(i) ¼ ijuj – i1 þ l is the
length of the subsequence in x. The subsequence kernel for two
strings x and y is determined as the dot product of the
corresponding feature maps:
Kðx; yÞsub
¼ Fðk;lÞðxÞ; Fðk;lÞðyÞ
llðjÞþlðjÞ
This kernel can be computed using a recursive algorithm based
on dynamic programming in O(kjxjjyj) time and space. The
running time and memory requirements can be further reduced
using techniques described by Seewald and Kleedorfer .
Amino acid pairs propensity scale
Amino acid pairs (AAPs) are obtained by decomposing a protein/
peptide sequence into its 2-mer subsequences. Chen et al. 
observed that some particular AAPs tend to occur more
frequently in B-cell epitopes than in non-epitope peptides.
Based on this observation, they developed an AAP propensity
scale deﬁned by:
uðaÞ ¼ log f þ
a are the occurrence frequencies of AAP a in the
epitope and non-epitope peptide sequences, respectively. These
frequencies have been derived from Bcipep and
Swissprot databases, respectively. To
avoid the dominance of an individual AAP propensity value, the
scale in equation (8) has been normalized to a (1, þ1) interval
through the following conversion:
uðaÞ ¼ 2 uðaÞ  min
where max and min are the maximum and minimum values of
the propensity scale before the normalization.
Chen et al. explored SVMs using two kernels: a dot
product kernel applied to the average of the AAP scale values for
all the AAPs in a peptide and an RBF kernel deﬁned in a
400-dimensional feature space as follows:
FAAPðxÞ ¼ ðfaðxÞ  uðaÞÞa2A2
where faðxÞ is the number of occurrences of the 2-mer a in the
peptide x. The optimal performance was obtained using the RBF
kernel and a window of 20 amino acids .
Fivefold cross-validation
In our experiments, we used stratiﬁed ﬁvefold cross-validation
tests in which the data set is randomly partitioned into ﬁve equal
subsets such that the relative proportion of epitopes to
non-epitopes in each subset is 1:1. Four of the ﬁve subsets are
used for training the classiﬁer and the ﬁfth subset is used for
testing the classiﬁer. This procedure is repeated ﬁve times, each
time choosing different subsets of the data for training and
testing. The estimated performance of the classiﬁer corresponds
to an average of the results from the ﬁve cross-validation runs.
Implementation and SVM parameter optimization
We used Weka machine learning
workbench for implementing the spectrum, mismatch, and LA
kernels (RBF and the subsequence kernel are already implemented in Weka). We evaluated the k-spectrum kernel, Kspct
k ¼ 1, 2, and 3. The (k, m)-mismatch kernel was evaluated at (k, m)
equals (3, 1), (4, 1), (5, 1), and (5, 2). The subsequence kernel, Ksub
was evaluated at k ¼ 2, 3, and 4 and the default value for l, 0.5.
The LA kernel was evaluated using the BLOSUM62 substitution
matrix, gap opening and extension parameters equal to 10 and 1,
respectively, and b ¼ 0.5. For the SVM classiﬁer, we used the Weka
implementation of the SMO algorithm. For the string
www.interscience.wiley.com/journal/jmr
Copyright # 2008 John Wiley & Sons, Ltd.
J. Mol. Recognit. 2008; 21: 243–255
Y. EL-MANZALAWY ET AL.
kernels, the default value of the C parameter, C ¼ 1, was used for
the SMO classiﬁer. For methods that use the RBF kernel, we found
that tuning the SMO cost parameter C and the RBF kernel
parameter g is necessary to obtain satisfactory performance. We
tuned these parameters using a two-dimensional grid search
over the range C ¼ 25, 23, . . ., 23, g ¼ 215, 213,. . ., 23. It should
be noted that the parameter optimization was performed using only
the training data.
Performance evaluation
The prediction accuracy (ACC), sensitivity (Sn), speciﬁcity (Sp), and
correlation coefﬁcient (CC) are often used to evaluate prediction
algorithms . The CC measure has a value in the
range from 1 to þ1, and the closer the value to þ1, the better
the predictor. ACC, Sn, Sp, and CC are deﬁned as follows:
TP þ FP þ TN þ FN
TP þ FN and Sp ¼
TP  TN  FP  FN
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðTN þ FNÞðTN þ FPÞðTP þ FNÞðTP þ FPÞ
where TP, FP, TN, and FN are the numbers of true positives, false
positives, true negatives, and false negatives, respectively.
Although these metrics are widely used to assess the
performance of machine learning methods, they all suffer from
the important limitation of being threshold-dependent. Threshold-dependent metrics describe the classiﬁer performance at a
speciﬁc threshold value. It is often possible to increase the
number of true positives (equivalently, sensitivity) of the classiﬁer
at the expense of an increase in false positives (equivalently,
false alarm rate). The receiver operating characteristic (ROC)
curve describes the performance of the classiﬁer over all possible
thresholds. The ROC curve is obtained by plotting the true
positive rate as a function of the false positive rate or,
equivalently, sensitivity versus (1-speciﬁcity) as the discrimination
threshold of the binary classiﬁer is varied. Each point on the ROC
curve describes the classiﬁer at a certain threshold value, i.e., at a
particular choice of tradeoff between true positive rate and false
positive rate. The area under ROC curve (AUC) is a useful summary
statistic for comparing two ROC curves. AUC is deﬁned as the
probability that a randomly chosen positive example will be
ranked higher than a randomly chosen negative example. An
ideal classiﬁer will have an AUC ¼ 1, while a classiﬁer assigning
labels at random will have an AUC ¼ 0.5, any classiﬁer performing
better than random will have an AUC value that lies between
these two extremes.
SVM using the subsequence kernel outperforms other
kernel methods and the AAP method
In the ﬁrst set of experiments, we used our homology-reduced
data sets to evaluate SVMs trained using the spectrum kernel at
k ¼ 1, 2, and 3, the (k, m)-mismatch kernel at (k, m) ¼ (3, 1), (4, 1),
(5, 1), and (5, 2), the LA kernel, and the subsequence kernel at
k ¼ 2, 3, and 4. We compared the performance of the four string
kernels to that of the RBF kernel trained using a binary
representation of the data in which each amino acid is
represented by a 20-bit binary string. In addition, we evaluated
our implementation of the AAP method on our
data sets. For all methods, the performance was evaluated using
ﬁvefold cross-validation. Because it is not feasible to include the
complete set of results in this paper, we report only the results on
the 20-mer peptides data set, BCP20, and provide the results on
data sets BCP18, BCP16, BCP14, and BCP12 in the Supplementary
Materials.
Table 1 compares the performance of different kernel-based
SVM classiﬁers on BCP20 data set. The subsequence kernel has
the best overall performance, in terms of AUC. The (5,
1)-mismatch kernel performs slightly better than the k-spectrum
kernel, and the performance of k-spectrum kernel with k ¼ 1 and
k ¼ 3 is much better than its performance with k ¼ 2. The
performance of both the k-spectrum and (k, m)-mismatch kernels
appears to be very sensitive to the choice of k and m parameters,
because for some choices of k and m, the classiﬁer performance
deteriorates to that expected for random assignment of labels to
test instances. In contrast, the performance of the subsequence
kernel appears to be much less sensitive to the choice of
parameter k.
Our implementation of the AAP method has
the second best overall performance and demonstrates the
highest speciﬁcity. The LA kernel is very competitive in
performance with AAP. Interestingly, the AAP signiﬁcantly
outperforms the RBF kernel trained using data in its binary
representation. The AAP method is essentially an RBF kernel
trained on the same data but using a different representation in
which each peptide is represented by a vector of 400 numeric
values computed based on the AAP propensity scale. The
signiﬁcant difference observed in performance of these two
RBF-based methods highlights the importance of the data
representation in kernel methods. All of these observations hold
not only for the BCP20 data set but also for the homology-reduced
data sets of peptides with different lengths (see Supplementary
Materials). Most of the methods have their best performance on
BCP20 data set and show slight decreases in performance on data
sets with decreasing peptide length.
Table 1. Performance of different methods on our BCP20
homology-reduced data set using ﬁvefold cross-validation.
BCPred method denotes Ksub
0.023 0.468
J. Mol. Recognit. 2008; 21: 243–255
Copyright # 2008 John Wiley & Sons, Ltd.
www.interscience.wiley.com/journal/jmr
B-CELL EPITOPES PREDICTION METHODS
Figure 1 shows the ROC curves for all methods evaluated in this
experiment. The ROC curve for the subsequence kernel, Ksub
dominates the other ROC curves over a broad range of choices for
the tradeoff between true positive and false positive rates. For
any user-selected threshold corresponding to speciﬁcity in the
range 100 to 20%, Ksub
ð4;0:5Þ has the best corresponding sensitivity.
We conclude that BCPred, SVM-based classiﬁer trained using the
subsequence kernel Ksub
ð4;0:5Þ, outperforms all other methods tested
in predicting linear B-cell epitopes.
Statistical analysis
We summarize statistical analysis of the results and conclusions
presented in the preceding subsection. Speciﬁcally, we attempt
to answer, from a statistical prospective, the following questions: is the performance of BCPred signiﬁcantly different
from those of other methods? Or more generally, how do the
different B-cell epitope prediction methods compare with each
To answer these questions, we utilized multiple hypothesis
comparisons for comparing a set of
classiﬁers on multiple data sets. We chose to use the AUC as the
performance metric in these tests. Table 2 shows the AUC values
of 13 classiﬁers on the ﬁve homology-reduced data sets.
One approach for performing multiple hypothesis comparisons over the results in Table 2, is to perform paired t-tests
between each pair of classiﬁers at p-value equal to 0.05. However,
when the number of classiﬁers being compared is large
Figure 1. ROC curves for different prediction methods on BCP20 homology-reduced data set. BCPred method denotes Ksub
ð4;0:5Þ. The BCPred ROC curve
dominates all other ROC curves for any user-selected threshold corresponding to speciﬁcity in the range of 100 to 20%.
Table 2. AUC values for different methods evaluated on homology-reduced data sets. For each data set, the rank of each classiﬁer is
shown in parentheses
0.613(8.8)
0.658(6.2)
0.581(10.6)
0.691(4.5)
0.657(6.1)
0.691(4.5)
0.681(4.1)
0.661(6.2)
0.607(8.6)
0.683(3.4)
www.interscience.wiley.com/journal/jmr
Copyright # 2008 John Wiley & Sons, Ltd.
J. Mol. Recognit. 2008; 21: 243–255
Y. EL-MANZALAWY ET AL.
compared to the number of datasets, paired t-tests are
susceptible to type I error, i.e., falsely concluding that the two
methods signiﬁcantly differ from each other in terms of
performance when in fact they do not. To reduce the chance
of type I errors, we used Bonferroni adjustments in performing multiple comparisons. Speciﬁcally, two
classiﬁers are considered different at 0.05 signiﬁcance level, if the
null hypothesis (that they are not different) is rejected by a paired
t-test at 0.05/12 ¼ 0.0042 conﬁdence level (12 denotes the
number of comparisons). Table 3 summarizes the results of
Bonferroni-corrected tests comparing the performance of the
classiﬁers. Signiﬁcantly different pairs of classiﬁers are indicated
with a . The results in Table 3 show that the reported
performance of BCPred is signiﬁcantly different from the
performance of other classiﬁers. On the other hand, the
differences between the performance of Kspct
ð3;0:5Þ classiﬁers and the performance of AAP are not statistically
signiﬁcant.
A second approach for performing multiple hypothesis
comparisons over the results in Table 2 is to use non-parametric
tests. Demsˇar has suggested that non-parametric
tests should be preferred over parametric tests for comparing
machine learning algorithms because the non-parametric tests,
unlike parametric tests, do not assume normal distribution of the
samples (e.g., the data sets). Demsˇar suggested a three-step
procedure for performing multiple hypothesis comparisons using
non-parametric tests. First, the classiﬁers being compared are
ranked on the basis of their observed performance on each data
set (see Table 2). Second, Friedman test is applied to determine
whether the measured average ranks are signiﬁcantly different
from the mean rank under the null hypothesis. Third, if the null
hypothesis can be rejected at 0.05 signiﬁcance level, the Nemenyi
test is used to determine whether signiﬁcant differences exist
between any given pair of classiﬁers. Unfortunately, this
procedure requires the number of data sets to be greater than
10 and the number of methods to be greater than 5 . Because we have 13 classiﬁers to compare and only 5 data
sets, we cannot use this procedure. However, as noted by Demsˇar
 , the average ranks by themselves provide a
reasonably fair comparison of classiﬁers. Hence, we use average
ranks to compare BCPred with the other methods. As shown in
Table 2, BCPred and Ksub
ð3;0:5Þ have average ranks 1 and 2,
respectively, followed by AAP and LA kernel with average ranks
3.4 and 4.1.
In summary, the results reported in Table 2 along with the
statistical analysis of the results lend support to the conclusion
summarized in the preceding subsection that the performance of
BCPred is superior to that of the other 12 methods.
Effect of epitope length on BCPred performance
Our choice of an epitope length of 20 amino acids in the
experiments summarized above was motivated by the previous
works . Figure 2
shows the distribution of unique epitope lengths in Bcipep
database . The Bcipep database contains 1230
unique B-cell epitopes with 99.4% of the epitopes having lengths
ranging from 3 to 38 amino acids. It turns out that 86.7% of the
unique B-cell epitopes are at most 20 amino acids in length.
However, it is natural to ask as to how the performance of BCPred
varies with the choice of epitope length. We now proceed to
examine the effect of epitope length on the performance of
In order to study the effect of epitope length we compared the
performance of BCPred and other methods trained and tested on
data sets with epitope lengths of 20, 18, 16, 14, and 12 amino
acids. Our results show that BCPred and ﬁve other methods reach
their best performance (in terms of AUC) on data set BCP20
(corresponding to epitope length of 20) (see Table 2). This
observation raises an obvious question: can we improve the
predictive performance of BCPred if we increase the epitope
length to beyond 20? To explore this question, we generated
ﬁve additional homology-reduced data sets, BCP22, BCP24, BCB26,
BCP28, and BCP30 (corresponding to epitope lengths of 22, 24,
26, 28, and 30, respectively) and compared the performance of
BCPred on the resulting data sets using ﬁvefold cross-validation.
The performance of BCPred on the ﬁve data sets is summarized in
Table 4. It is interesting to note that the measured AUC on BCP22
Table 3. Results of Bonferroni adjustments using p-value ¼ 0.0042. ‘‘’’ indicates that the corresponding pair of methods is
signiﬁcantly different
ð2;0:5Þ (M9)
ð3;0:5Þ (M10)
BCPred (M11)
J. Mol. Recognit. 2008; 21: 243–255
Copyright # 2008 John Wiley & Sons, Ltd.
www.interscience.wiley.com/journal/jmr
B-CELL EPITOPES PREDICTION METHODS
is 0.788 compared to 0.758 on BCP20. A slightly better AUC, 0.804,
was observed on BCP28.
Why does BCPred have a better performance on BCP28
compared with its performance on BCP20? There are at least
three possible explanations: (i) BCP28 includes longer segments
of epitope sequences of length greater than 20 amino acids in the
data set than BCP20; (ii) Some of the hard-to-predict epitopes in
BCP20 are eliminated from BCP28 because these epitopes are
located very close to the ends of the antigen sequence and so
extending these epitopes to 28 amino acids in length by adding
an equal number of amino acids from both ends is not possible;
(iii) amino acid neighbors of the epitopes carry some useful signal
that helps the classiﬁer to better discriminate epitopes from
non-epitopes.
To test these hypotheses, we constructed a modiﬁed version of
BCP20 data set, MBCP20. MBCP20 was derived from BCP28 by
trimming 4 amino acids from both ends of each peptide in BCP28.
Therefore, BCP28 and MBCP20 can be viewed as two different
representations of the same set of epitope/non-epitope data.
However, the sequence similarity between any pair of epitopes in
BCP28 is guaranteed to be less than 80% but this is not
necessarily the case for epitopes in MBCP20. The performance of
BCPred on MBCP20 data set is shown in Table 4. The results show
that the performance of BCPred on MBCP20, the trimmed version
of BCP28, is worse than that on BCP28. This observation provides
some evidence against the second of the three possible
explanations for observed improvements in performance with
epitope length chosen to construct the data sets used to train
and test BCPred. It also lends some credence to the suggestion
that the amino acid neighbors of the B-cell epitopes may help
the classiﬁer to better discriminate between epitopes and
non-epitope sequences. As noted earlier, another possibility is
that increasing the length results in covering a larger fraction of
epitope sequence in the data set in the case of epitope sequences
that are longer than 20 amino acid in length (about 13% of the
epitopes).
Comparing BCPred with existing linear B-cell epitope
prediction methods
Although a number of machine learning based methods for
predicting linear B-cell epitopes have been proposed , little
is known about how these methods directly compare with one
another due to the lack of published comparisons using standard
benchmark data sets. Unfortunately, because the code and
precise parameters used to train several of these methods are not
available, we were unable to make direct comparisons of these
methods using the homology-reduced data sets we used in our
ﬁrst set of experiments (summarized in Tables 1 and 2). However,
we were able to compare BCPred with our implementation of
APP and ABCPred, using the publicly available benchmark data
sets that were used to evaluate
ABCPred. Because the best reported performance of ABCPred
was obtained using a data set of 16-mer peptides, comprising 700
epitopes and 700 non-epitopes peptides, we used the same data
set, ABCP16, to compare ABCPred with BCPred and AAP. In
addition, a blind test set, SBT16, consisting of 187 epitopes and
200 16-mer non-epitopes, also made available by Saha and
Raghava , was used to compare the three methods.
Table 5 compares the performance of BCPred, AAP, and
ABCPred on ABCP16 data set , using
ﬁvefold cross-validation. In terms of overall accuracy, both BCPred
Table 4. Performance of BCPred on homology-reduced data
sets containing longer epitopes (22–30 residues) and modi-
ﬁed BCP20, MBCP20, data set
Table 5. Performance of BCPred, AAP, and ABCpred evaluated on ABCP16 data set using ﬁvefold cross-validation. ‘‘—’’
denotes unavailable information
Figure 2. Length distribution of unique linear B-cell epitopes in Bcipep database . 86.7% of the epitopes are at most 20 amino acids in
www.interscience.wiley.com/journal/jmr
Copyright # 2008 John Wiley & Sons, Ltd.
J. Mol. Recognit. 2008; 21: 243–255
Y. EL-MANZALAWY ET AL.
and AAP outperformed ABCPred on this data set, with BCPred
showing the best performance (74.57%). Interestingly, the
performance of BCPred and AAP on ABCP16 data set was better
than their performance on the homology-reduced data set used in
the ﬁrst set of experiments described above. The performance of
the three classiﬁers trained on ABCP16 data set, but tested on the
blind test set SBT16 is summarized in Table 6. In this case, the
performance of ABCPred was slightly better than that of BCPred
What explains the discrepancy between the performance
estimated on ABCP16 data set and the performance on
SBT16 blind test set?
Based on the empirical results summarized above, it is natural to
ask: How can we explain the differences in relative performance
of BCPred and AAP on our homology-reduced data sets versus the
performance of these methods on ABCP16 data set ? How can we explain the observation that
BCPred and AAP outperform ABCPred in ﬁvefold cross validation
experiments using ABCP16 data set but not on the blind test set,
SBT16 data set?
Could the observed differences in relative performance be
explained by differences in the two data sets, BCP16 and ABCP16?
To explore this possibility, we considered the procedures used to
create the data sets. Recall that Saha and Raghava started
with a data set of 20-mer peptides (after extending the length of
shorter B-cell epitopes based on the corresponding antigen
sequences). [As noted above, there is a possibility that the
resulting data set of 20-mer peptides includes several highly
similar peptides (e.g., peptides that differ from each other in only
one or two amino acids). More importantly, the 16-mer data set,
ABCP16, was derived from the 20-mer data set, ABCP20, by
trimming two amino acids from the ends of each 20-mer peptide;
as a result, two 20-mers that were not duplicates of each other
might yield 16-mers that are highly similar after the ends are
trimmed off. In summary, the ABCP20 data set reported by Saha
and Raghava was constructed from unique epitopes
without applying any homology reduction ﬁlters. Moreover, the
procedure used by Saha and Raghava to derive ABCP16
from ABCP20 can be expected to increase the pair-wise similarity
between sequences in ABCP16 relative to the pairwise sequence
similarity within ABCP20.
Indeed, when we scanned the positive peptides in ABCP16
data set for duplicate peptides, we
found 37 cases in which a 16-mer peptide has at least one exact
duplicate in the 16-mer data set and several of these have
multiple copies in the 16-mer data set (see Table 7). Consequently, ﬁvefold cross validation using ABCP16 data set is likely
to yield overly optimistic performance estimates, especially for
methods that rely on sequence features such as those identiﬁed
by the subsequence kernel and AAP.
To determine exactly how redundant are the positive peptides
in ABCP16 data set, we ﬁltered them using an 80% sequence
identity cutoff. We found that applying the 80% sequence
identity cutoff resulted in the number of positive peptides in
the ABCP16 data being reduced from 696 to 532. Thus, 23.5% of
the positive peptides in ABCP16 data set have more than 80%
sequence identity. This observation leads us to conclude that the
observed differences in the performance of BCPred and AAP on
the homology-reduced data set (BCP16) relative to that on the
ABCP16 data set, as well as the results of comparisons of AAP,
ABCPred, and BCPred on the blind test set (SBT16), are explained
by the presence of a relatively large number of highly similar
peptides in ABCP16 data set.
The preceding analysis highlights an important issue in
evaluating linear B-cell prediction tools, which, to our knowledge,
has not been addressed in previous studies. Previously published
linear B-cell epitope prediction methods 
have been evaluated using data sets of unique epitopes without
considering any sequence similarities that may exist among
epitopes. In reality, unique epitopes may share a high degree of
similarity (e.g., a shorter epitope may be included within a longer
one, or two epitopes may differ in only one or two amino acids). In
this work, we demonstrated that cross-validation performance
estimated on such data sets can be overly optimistic. Moreover,
such data sets can lead to false conclusions when used to
prediction
comparison of ABCPred, AAP, and BCPred using ﬁvefold crossvalidation on ABCP16 data set suggested that AAP and BCPred
signiﬁcantly outperform ABCPred. Such a conclusion may not be
valid because evaluation of the three methods on a blind test set,
Table 6. Performance comparison of BCPred, AAP, and
ABCPred. The three classiﬁers were trained using ABCP16 data
set and evaluated using SBT16 blind test set
Table 7. List of 37 duplicated 16-mer peptides and number
of occurrence of each (N) in ABCP16 data set 
RGPGRAFVTIGKIGNM
RKRIHIGPGRAFYTTK
GPQGLAGQRGIVGLPG
KSIRIQRGPGRAFVTI
QEVGKAMYAPPISGQI
SIRIQRGPGRAFVTIG
SEGATPQDLNTMLNTV
RQGPKEPFRDYVDRFY
LGIWGCSGKLICTTAV
AKATYEAALKQYEADL
GDRADGQPAGDRADGQ
NYNKRKRIHIGPGRAF
DGVGAASRDLEKHGAI
NNNTRKSITKGPGRVI
RLIEDNEYTARQGAKF
LQARILAVERYLKDQQ
EQELLELDKWASLWNW
KMQTLWDEIMDINKRK
NVTENFDMWKNDMVEQ
TRKSIRIQRGPGRAFV
NVTENFNMWKNDMVEQ
DPNPQEVVLVNVTENF
GIWGCSGKLICTTAVP
YLKDQQLLGIWGCSGK
QLLGIWGCSGKLICTT
TRKSITKGPGRVIYAT
QVTPGRGPGRAPCSAG
SFNISTSIRGKVQKEY
IRIQRGPGRAFVTIGK
RPVVSTQLLLNGSLAE
RIQRGPGRAFVTIGKI
RKDPVVYPWMKKVHVN
KRIHIGPGRAFYTTKN
RNRWEWRPDFESEKVK
AGTVGENVPDDLYIKG
FLQIYKQGGFLGLSNI
KRKRIHIGPGRAFYTT
J. Mol. Recognit. 2008; 21: 243–255
Copyright # 2008 John Wiley & Sons, Ltd.
www.interscience.wiley.com/journal/jmr
B-CELL EPITOPES PREDICTION METHODS
SBT16, suggests that the three methods are comparable to each
BCPREDS web server
We have implemented BCPREDS, an online web server for B-cell
epitope prediction, using classiﬁers trained on the homologyreduced data sets of B-cell epitopes developed in this work. The
server can be accessed at 
Because it is often valuable to compare predictions of multiple
methods, and consensus predictions are more reliable than
individual predictions, the BCPREDS server allows users to choose
the method for predicting B-cell epitopes, either BCPred or AAP
(and in the future, additional methods). Users provide an antigen
sequence and optionally can specify desired epitope length and
speciﬁcity threshold. Results are returned in several user-friendly
formats. In what follows, we illustrate the use of the BCPREDS
server in a representative application of B-cell epitope prediction.
Identifying B-cell epitopes in the receptor-binding domain
of SARS-CoV spike protein
Since its outbreak in 2002, the development of an effective and
Respiratory
Coronavirus (SARS-CoV) has become an urgent need for
preventing future worldwide outbreak of SARS, a life threatening
disease . Infection by SARS-CoV is initiated by the
binding of its spike (S) protein to its functional receptor,
angiotensin-converting enzyme 2 (ACE2), which is expressed on
the surface of host cells . The S
protein comprises 1255 amino acids and consists of two
functional domains: S1 (residues 1–667) and S2 (residues
668–1255) . The S1 domain is responsible for
binding to receptors on target cells and the S2
domain contributes to the subsequence fusion between viral
envelope and cellular membrane . In addition,
the S2 domain contains two highly conserved heptad repeat (HR)
regions, HR1 and HR2 correspond to amino acid residues
915–949 and 1150–1184, respectively . Several
studies reported that the receptor-binding domain (RBD),
residues 318–510, is an attractive target for developing SARS-CoV
vaccine because blocking the binding of S1 domain to cellular
receptors can prevent envelope fusion and virus entry mediated
by the S2 domain . Based
on these ﬁndings, we surveyed the literature to collect previously
identiﬁed epitopes within the RBD fragment of the SARS-CoV S
protein. The collected epitopes are summarized in Table 8. None
of these epitopes appears in our training data sets. Because
epitope SP3 is included within the epitope (434–467) (GNYNY-
KYRYLKHGKLRPFERDISNVPFSPDGKPC) reported by Lien et al.
 , we omitted the longer epitope.
We submitted 193 residues comprising the RBD region of
SARS-CoV S protein (residues 318–510 according to accession
AAT74874) to the BCPREDS, ABCPred, and Bepipred servers. For
BCPREDS, we used the default speciﬁcity threshold (75%) and set
the epitope length to 16 residues. For the other two servers, we
used the default settings. Figure 3 shows the BCPred (top) and
AAP (bottom) predictions returned by BCPREDS. Four of the B-cell
epitopes predicted by BCPred overlap with epitopes that have
been identiﬁed in the antigenic regions of RBD of SARS-CoV S
Table 8. B-cell epitopes previously identiﬁed in the RBD of
SARS-CoV S protein
Amino acid sequence
SP1 (336–352)
SVYAWERKKISNCVADY
SP2 (424–435)
NTRNIDATSTGN
SP3 (442–458)
YLKHGKLRPFERDISNV
SP4 (459–470)
PFSPDGKPCTPP
SP5 (483–494)
FYTTTGIGYQPY
Figure 3. BCPREDS server predictions of epitopes within the RBD of SARS-CoV S protein, made using BCPred (top) and AAP (bottom). Experimentally
identiﬁed epitopes are underlined. ‘‘E’’ indicates that the corresponding amino acid residue lies in a predicted linear B-cell epitope.
www.interscience.wiley.com/journal/jmr
Copyright # 2008 John Wiley & Sons, Ltd.
J. Mol. Recognit. 2008; 21: 243–255
Y. EL-MANZALAWY ET AL.
protein through experiments. Three of the ﬁve epitopes
predicted by AAP have substantial overlap with the SP1, SP2,
and SP5 epitopes, and the fourth partially overlaps epitopes SP2
and SP3; the ﬁfth does not overlap with any experimentally
reported epitopes. In contrast, the ABCPred server, using default
parameters, returned 22 predictions covering almost the entire
query sequence. BepiPred returned nine predicted variablelength epitopes, but only three of them are longer than four
residues in length. Two out of these four epitopes overlap with
experimentally reported epitopes. The complete ABCPred and
predictions
Supplementary
Materials. In evaluating these results, it is worth noting that a
high false positive rate is more problematic than an occasional
false negative prediction in the B-cell epitope prediction task
 , because a major goal of B-cell epitope
prediction tools is to reduce the time and expense of wet lab
experiments.
The B-cell epitopes predicted over the entire SARS-CoV S
protein using BCPred is given in Figure S3. Interestingly, the
predictions of BCPred over the RBD region are identical
regardless of whether the predictions are made over only the
RBD sequence fragment or over the entire S protein sequence of
DISCUSSION
In this paper, we explored a family of SVM-based machine
learning methods for predicting linear B-cell epitopes from
primary amino acid sequence. We explored four string kernel
methods and compared them to the widely used RBF kernel. Our
results demonstrate the usefulness of the four string kernels in
predicting linear B-cell epitopes, with the subsequence kernel
showing a superior performance over other kernels. In addition,
we observed that the subsequence kernel is less sensitive to
the choice of the parameter k than the k-spectrum and
(k,m)-mismatch kernels. Our experiments using ﬁvefold crossvalidation on a homology-reduced data set of 701 linear B-cell
epitopes and 701 non-epitopes demonstrated that the subsequence kernel signiﬁcantly outperforms other kernel methods in
addition to APP method . To the best of our
knowledge, the subsequence kernel although
previously used in text classiﬁcation and natural language
processing applications, have not been widely exploited in
the context of macromolecular sequence classiﬁcation tasks. The
superior performance of the subsequence kernel on B-cell
epitope prediction task suggests that it might ﬁnd use in other
related macromolecular sequence classiﬁcation tasks, e.g., MHC
binding peptide prediction and protein subcellular localization prediction .
One of the challenges for developing reliable linear B-cell
epitope predictors is how to deal with the large variability in the
length of the epitopes which ranges from 3 to 30 amino acids in
length. Many standard machine learning methods require
training and testing the classiﬁer using sequences of ﬁxed
length. For example, the AAP method was
evaluated on a data set where the length of the input sequences
was ﬁxed to 20 amino acids. Saha and Raghava 
experimented with data sets consisting of peptide sequences
of length 20 and shorter, and reported optimal performance of
ABCPred classiﬁer on a data set consisting of 16-mer peptides. In
BepiPred and propensity scale based
methods , the training examples are windows of ﬁve or
seven amino acids labeled according to whether the amino acid
at the center of the window is included in a linear B-cell epitope
or not. Here, we evaluated BCPred on several data sets consisting
of ﬁxed length peptides with lengths ranging from 12 to 30
amino acids with incremental step equal to 2. Our results suggest
that amino acid neighbors of the B-cell epitope carry some useful
information that can help the classiﬁer to discriminate better
between epitopes and non-epitopes. This is especially interesting
in light of the observation that adding a single amino acid to a
linear B-cell epitope may affect binding to the antibody.
A similar situation arises in predicting the major histocompatibility complex class II (MHC-II) binding peptides. The length of
MHC-II binding peptides typically varies from 11 to 30 amino
acids in length. Most of the currently available MHC-II binding
peptide prediction methods focus on identifying a putative 9-mer
binding core region. Therefore, classiﬁers are trained using 9-mer
peptides instead of variable length ones. Recently, two methods
 for predicting variable
length MHC-II peptides have been proposed. Both methods use
the entire sequences of MHC-II binding peptides (as opposed to
only the 9-mer cores) for training MHC-II binding peptide
predictors. The ﬁrst method maps a variable
length peptide into a ﬁxed length feature vector obtained from
sequence-derived structural and physicochemical properties of
the peptide. The second method 
uses the local alignment (LA) kernel that we used in this study. It
would be interesting to apply these methods to the problem of
learning to identify variable length linear B-cell epitopes. Our
ongoing work aims at exploring the application of string kernels
for learning from ﬂexible length linear B-cell epitopes.
signiﬁcant
improvement
performance of B-cell epitope prediction methods reported in
the literature, it is important to understand the strengths and
limitations of different methods through direct comparisons on
standard benchmark data sets. Hence, we compared the BCPred
method using the subsequence kernel-based SVM developed in
this paper with two published methods: AAP 
and ABCPred . In our experiments
using the Saha and Raghava 16-mer peptide data set
(containing
approximately
non-epitope peptides) on which ABCPred had the best reported
performance of 66%, both BCPred and AAP outperformed
ABCPred, based on ﬁvefold cross-validation. However, when the
classiﬁers were tested on a separate blind test set instead, no
signiﬁcant difference was observed in their performance. Careful
examination of the ABCPred 16-mer data set revealed that the
data set has a high degree of sequence redundancy among the
epitope peptides, leading to overly optimistic estimates of
performance in some cases.
Our demonstration that the only publicly available data set of
linear B-cell epitopes is, in fact, highly
redundant (with almost 25% of individual 16-mer epitopes
having at least one other epitope with >80% sequence identity)
is signiﬁcant. We showed that the redundancy in such a data set
can be reﬂected in overly-optimistic performance estimates,
especially for certain types of machine learning classiﬁers.
Consequently, using such a data set can also lead to false
conclusions
prediction
J. Mol. Recognit. 2008; 21: 243–255
Copyright # 2008 John Wiley & Sons, Ltd.
www.interscience.wiley.com/journal/jmr
B-CELL EPITOPES PREDICTION METHODS
methods. Therefore, it is very important to evaluate and compare
different linear B-cell epitope prediction methods on data sets
that are truly non-redundant or homology-reduced with respect to
their constituent epitope sequences, i.e., in which the level of
pair-wise sequence identity shared between individual epitopes
is known. Towards this goal, we have made our homology-reduced
data set of linear B-cell epitopes (with <80% sequence identity)
publicly available as a benchmarking data set for comparing
existing and future linear B-cell epitope prediction methods.
Based on the results of this study, we developed BCPREDS, an
online web server for predicting linear B-cell epitopes using
either the BCPred method, which implements the subsequence
kernel introduced in this paper, or the AAP method of Chen et al.
 . A case study in which BCPREDS was used to predict linear
demonstrates the potential value of this server in guiding clinical
investigations.
Work in progress is aimed at further development and
empirical comparisons of different methods for B-cell epitope
prediction, in particular, addressing the more challenging
problem of predicting discontinuous or conformational B-cell
Acknowledgements
We thank the anonymous reviewers for their comments and
suggestions, Dr. Janez Demsˇar for discussing the applicability of
non-parametric tests, Dr. Douglas Bonett for suggesting the
Bonferroni adjustment test. This research was supported in part
by a doctoral fellowship from the Egyptian Government to Yasser
El-Manzalawy and a grant from the National Institutes of Health
(GM066387) to Vasant Honavar and Drena Dobbs.