Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection
Debidatta Dwibedi
Ishan Misra
Martial Hebert
The Robotics Institute, Carnegie Mellon University
 , {imisra, hebert}@cs.cmu.edu
A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets.
For example, ﬁnding a large labeled
dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires
expensive data collection and annotation. In this paper, we
propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that
ensuring only patch-level realism provides enough training
signal for current object detector models. We automatically
‘cut’ object instances and ‘paste’ them on random backgrounds. A naive way to do this results in pixel artifacts
which result in poor performance for trained models. We
show how to make detectors ignore these artifacts during
training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21% on benchmark datasets.
In a cross-domain setting, our synthetic
data combined with just 10% real data outperforms models trained on all real data.
1. Introduction
Imagine using an object detection system for an environment like your kitchen. Such a system needs to not only
recognize different kinds of objects but also distinguish between many different instances of the same object category,
e.g., your cup vs. my cup. With the tremendous progress
that has been made in visual recognition, as documented on
benchmark detection datasets, one may expect to easily take
a state-of-the-art system and deploy it for such a setting.
However, one of the biggest drawbacks of using a stateof-the-art detection system is the amount of annotations
needed to train it. For a new environment with new objects, we would likely need to curate thousands of diverse
images with varied backgrounds and viewpoints, and annotate them with boxes. Traditionally, vision researchers have
undertaken such a mammoth task for a few commonly occurring categories like man, cow, sheep etc., but
Generated Scenes (Training Data)
Object Instances
Background Scenes
Detections on Real Images
Figure 1: We present a simple way to rapidly generate training images for instance detection with minimal human effort. We automatically extract object instance masks and
render it on random background images to create realistic
training images with bounding box labels. Our results show
that such data is competitive with human curated datasets,
and contains complementary information.
this approach is unlikely to scale to all possible categories,
especially the instances in your kitchen. In a personalized
setting we need annotations for instances like your cup. We
believe that collecting such annotations is a major impediment for rapid deployment of detection systems in robotics
or other personalized applications.
Recently, a successful research direction to overcome
this annotation barrier, is to use synthetically rendered
scenes and objects to train a detection system.
This approach requires a lot of effort to make the scenes and
objects realistic, ensuring high quality global and local consistency. Moreover, models trained on such synthetic data
 
Randomly Sample Objects
Randomly Sample Negatives
Randomly Sample Scenes
Out of plane
1. Collect Images of Objects and Scenes
2. Predict Object Mask
Segmented Objects
3. Data Augmentation
Augmentations
4. Synthesize Same Scene with Different Blending Modes
Truncations
Occlusions
Different Blending Modes
Invariant to Local Artifacts
Model real world scenarios
Figure 2: We present a simple approach to rapidly synthesize datasets for instance detection. We start with a set of images
of the instances and background scenes. We then automatically extract the object mask and segment the object. We paste
the objects on the scenes with different blending to ensure that local artifacts are ignored by the detection model. Our results
show that this synthesized data is both competitive with real data and contains complementary information.
have trouble generalizing to real data because of the change
in image statistics . To address this, an emerging
theme of work moves away from graphics based renderings to composing real images. The underlying theme is
to ‘paste’ real object masks in real images, thus reducing the
dependence on graphics renderings. Concurrent work 
estimates scene geometry and layout and then synthetically
places object masks in the scene to create realistic training images. However, the scene layout estimation step may
not generalize to unseen scenes. In our paper, we show a
simpler approach that does not require such scene geometry
estimation to create training images.
Our key insight is that state-of-the art detection methods like Faster-RCNN and even older approaches like
DPM etc. care more about local region-based features
for detection than the global scene layout. As an example, a
cup detector mostly cares about the visual appearance of the
cup and its blending with the background, and not so much
about where the cup occurs in the scene: the table-top or
the ground. We believe that while global consistency is important, only ensuring patch-level realism while composing
synthetic datasets should go a long way to train these detectors. We use the term patch-level realism to refer to the
observation that the bounding box containing the pasted object looks realistic to the human eye.
However, naively placing object masks in scenes creates
subtle pixel artifacts in the images. As these minor imperfections in the pixel space feed forward deeper into the layers of a ConvNet , they lead to noticeably different features and the training algorithm focuses on these discrepancies to detect objects, often ignoring to model their complex visual appearance. As our results show (Table 1), such
models give reduced detection performance.
Since our main goal is to create training data that is useful for training detectors, we resolve these local imperfections and maintain patch level realism. Inspired from methods in data augmentation and denoising auto encoders ,
we generate data that forces the training algorithm to ignore
these artifacts and focus only on the object appearance. We
show how rendering the same scene with the same object
placement and only varying the blending parameter settings
(Section 5.2) makes the detector robust to these subtle pixel
artifacts and improves training. Although these images do
not respect global consistency or even obey scene factors
such as lighting etc., training on them leads to high performance detectors with little effort. Our method is also complementary to existing work that ensures global
consistency and can be combined with them.
Data generated using our approach is surprisingly effective at training detection models. Our results suggest that
curated instance recognition datasets suffer from poor coverage of the visual appearances of the objects. With our
method, we are able to generate many such images with
different viewpoints/scales, and get a good coverage of the
visual appearance of the object with minimal effort. Thus,
our performance gain is particularly noticeable when the
test scenes are different from the training scenes, and thus
the objects occur in different viewpoints/scales.
2. Related Work
Instance detection is a well studied problem in computer vision. provides a comprehensive overview of
the popular methods in this ﬁeld. Early approaches, such
as , heavily depend on extracting local features such as
SIFT , SURF , MSER and matching them to
retrieve instances . These approaches do not work
well for objects which are not ‘feature-rich’, where shapebased methods are more successful.
Modern detection methods based on learned
ConvNet features generalize across feature
rich and feature poor objects .
With the availability of powerful commodity GPUs, and fast detection algorithms , these methods are suitable for realtime object detection required in robotics. More recently,
deep learning based approaches in computer vision are being adopted for the task of pose estimation of speciﬁc
objects . Improving instance detection and pose
estimation in warehouses will be signifcantly useful for the
perception pipeline in systems trying to solve the Amazon
Picking Challenge .
The use of these powerful methods for object and instance detection requires large amounts of annotated data.
This requirement is both impractical and expensive for
rapidly deploying detection systems. Sythesizing data is
one way to address this issue.
 use rendered images of objects to do both object detection and pose estimation. They render 3D models of objects from different
viewpoints and place them against randomly sampled backgrounds. also highlight the importance of using photorealsitic models in training CNNs.
There is a wide spectrum of work where rendered
datasets are used for computer vision tasks. At one end, we
have datasets with images of single objects on random backgrounds . On the other end, there are datasets
where the entire scene is rendered . On that
spectrum our work lies in between as we do not render the
whole world but use real images of both objects and backgrounds to compose new scenes. In this sense, our work
closely related to contemporary work from which generates synthetic data for localizing text in scenes.
Sedaghat et al. show how an annotated dataset can
be created for the task of object pose estimation by taking
videos by walking around the object. uses synthetic
data from for multi-view instance recognition. use
real and synthetic images for 2D-3D alignment.
Similarly, render 3D humans in scenes and use
this data for pose estimation. Tasks requiring dense annotation, such as segmentation, tracking etc. have also shown
to beneﬁt by using such approaches . 
shows a novel approach for collecting data of objects in a
closed domain setting. annotate 3D points belonging to an object in the point cloud reconstruction of a
scene and propagate the label to all frames where the object is visible. As synthetic data can be signiﬁcantly different from real images, shows a domain adaptation approach to overcome this issue. In contrast, our work composes training scenes using real object images as well as
real background images.
The existing approaches to sythesizing datasets focus
largely on ensuring global consistency and realism . While global consistency is important, we believe
that local features matter more for training detection systems. Our approach ensures that when we train our detection model it is invariant to local discrepancies.
Object Detection
Instance Detection
Granola Bar 1
Granola Bar 2
Granola Bars
Object vs Instance Detection.
Instance detection involves ﬁne-grained recognition within the same
‘object category’(as shown by the visually similar cups)
while also detecting the same instance from different viewpoints(depicted by the different views of the granola bars).
In this example, instance recognition must distinguish
amongst 6 classes: 2 types of granola bars and 4 types
of coffee cups.
Object detection would distinguish only
amongst 2 classes: coffee cups and granola bars.
3. Background
Instance Detection: Instance detection requires accurate
localization of a particular object, e.g. a particular brand of
cereal, a particular cup etc. In contrast, generic object detection detects an entire generic category like a cereal box
or a cup (see Figure 3). In fact, in the instance detection scenario correctly localizing a cereal box of some other brand
is counted as a mistake. Instance detection occurs commonly in robotics, AR/VR etc., and can also be viewed as
ﬁne-grained recognition.
Traditional
Collection:
datasets involves a data curation step and an annotation step.
Typically, data curation involves collecting internet images
for object detection datasets . However, this fails for
instance datasets as ﬁnding internet images of particular instances is not easy. For instance detection data curation involves placing the instances in varied backgrounds
and manually collecting the images. Manually collecting
these images requires one to pay attention to ensure diversity in images by placing the object in different backgrounds
and collecting different viewpoints. The annotation step is
generally crowd sourced. Depending on the type of data,
human annotations can be augmented with object tracking
or 3D sensor information .
Unfortunately, both these steps are not suitable for
rapidly gathering instance annotations. Firstly, as we show
in our experiments, even if we limit ourselves to the same
type of scene, e.g., kitchens, the curation step can lack diversity and create biases that do not hold in the test setting.
Secondly, as the number of images and instances increase,
manual annotation requires additional time and expense.
4. Approach Overview
We propose a simple approach to rapidly collect data for
instance detection. Our results show that our approach is
competitive with the manual curation process, while requiring little time and no human annotation.
Ideally, we want to capture all of the visual diversity of
an instance. Figures 1 and 3 show how a single instance appears different when seen from different views, scales, orientation and lighting conditions. Thus, distinguishing between such instances requires the dataset to have good coverage of viewpoints and scales of the object. Also, as the
number of classes increases rapidly with newer instances,
the long-tail distribution of data affects instance recognition
problems. With synthetic data, we can ensure that the data
has good coverage of both instances and viewpoints. Figure 2 shows the main steps of our method:
1. Collect object instance images: Our approach is agnostic to the way the data is collected. We assume that we
have access to object images which cover diverse viewpoints and have a modest background.
2. Collect scene images: These images will serve as background images in our training dataset. If the test scenes
are known beforehand (like in the case of a smarthome or a warehouse) one can collect images from those
scenes. As we do not compute any scene statistics like
geometry or layout, our approach can readily deal with
new scenes.
3. Predict foreground mask for the object: We predict
a foreground mask which separates the instance pixels
from the background pixels. This gives us the object
mask which can be placed in the scenes.
4. Paste object instances in scenes: Paste the extracted
objects on a randomly chosen background image. We
ensure invariance to local artifacts while placing the
objects so that the training algorithm does not focus on
subpixel discrepancies at the boundaries. We add various modes of blending and synthesize the exact same
scene with different blending to make the algorithm robust to these artifacts. We also add data augmentation
to ensure a diverse viewpoint/scale coverage.
5. Approach Details and Analysis
We now present additional details of our approach and
provide empirical analysis of our design choices.
5.1. Collecting images
We ﬁrst describe how we collect object/background images, and extract object masks without human effort.
Images of objects from different viewpoints: We choose
the objects present in Big Berkeley Instance Recognition
Dataset (BigBIRD) to conduct our experiments. Each
object has 600 images, captured by ﬁve cameras with different viewpoints. Each image also has a corresponding depth
image captured by an IR camera.
Background images of indoor scenes: We place the extracted objects from the BigBIRD images on randomly sampled background images from the UW Scenes dataset .
There are 1548 images in the backgrounds dataset.
Foreground/Background segmentation: Once we have
collected images of the instances, we need to determine
the pixels that belong to the instance vs.
the background. We automate this by training a model for foreground/background classiﬁcation.
We train a FCN network (based on VGG-16 pre-trained on PAS-
CAL VOC image segmentation) to classify each image
pixel into foreground/background. The object masks from
the depth sensor are used as ground truth for training this
model. We train this model using images of instances which
are not present in our ﬁnal detection evaluation. We use 
as a post-processing step to clean these results and obtain
an object mask. Figure 5 shows some of these results. In
practice, we found this combination to generalize to images
of unseen objects with modest backgrounds and give good
quality object masks from input images. It also generalizes
to transparent objects, e.g., coca cola bottle, where the
depth sensor does not work well.
5.2. Adding Objects to Images
After automatically extracting the object masks from input images, we paste them on real background images.
Na¨ıvely pasting objects on scenes results in artifacts which
the training algorithm focuses on, ignoring the object’s visual appearance. In this section, we present steps to generate data that forces the training algorithm to ignore these
artifacts and focus only on the object appearance. To evaluate these steps empirically, we train a detection model on
our synthesized images and evaluate it on a benchmark instance detection dataset (real images).
Detection Model: We use the Faster R-CNN method
and initialize the model from a VGG-16 model pretrained on object detection on the MSCOCO dataset.
Benchmarking Dataset:
After training the detection
model on our synthetic images, we use the GMU Kitchen
dataset for evaluation.
There are 9 scenes in this
dataset. Three dataset splits with 6 scenes for training and
3 for testing have been provided in to conduct experiments on the GMU Kitchen dataset. We follow these splits
for train/test and report the average over them. No images
or statistics from this dataset are used for either dataset synthesis or training the detector. We report Mean Average Precision (mAP) at IOU of 0.5 in all our experiments.
Directly pasting objects on background images creates
boundary artifacts. Figure 6 shows some examples of such
Figure 4: A few randomly chosen samples from our synthesized images. We describe the details of our approach in Section 5.
Table 1: We analyze the effect of various factors in synthesizing data by generating data with different settings and training
a detector . We evaluate the trained model on the GMU Dataset . As we describe in Section 5, these factors greatly
improve the quality of the synthesized data.
2D Rot. 3D Rot. Trunc. Occl. coca coffee
hunt’s mahatma nature nature palmolive
Blending (Sec 5.2.1)
No blending
Gaussian Blurring
Poisson 
All Blend + same image
Data Aug. (Sec 5.2.2)
No 2D Rotation
No 3D Rotation
No Occlusion
All + Distractor
Honey Bunches of Oats
Mahatma Rice
Coca Cola Glass Bottle
Palmolive Orange
Figure 5: Given an image of a new unseen object instance,
we use a ConvNet to predict foreground/background pixels. Using these predictions we automatically obtain an object mask. This method generalizes to transparent surfaces
where traditional methods relying on depth sensors for segmentation fail (second row).
artifacts. Although these artifacts seem subtle, when such
images are used to train detection algorithms, they give poor
performance as seen in Table 1. As current detection methods strongly depend on local region-based features,
boundary artifacts substantially degrade their performance.
The blending step ‘smoothens’ out the boundary artifacts
between the pasted object and the background. Figure 6
shows some examples of blending. Each of these modes
add different image variations, e.g., Poisson blending 
smooths edges and adds lighting variations. Although these
blending methods do not yield visually ‘perfect’ results,
they improve performance of the trained detectors. Table 1
lists these blending methods and shows the improvement in
performance after training on blended images.
To make the training algorithm further ignore the effects
of blending, we synthesize the exact same scene with the
same object placement, and only vary the type of blending used.
We denote this by ‘All Blend + same image’
in Table 1. Training on multiple such images where only
the blending factor changes makes the training algorithm
invariant to these blending factors and improves performance by 8 AP points over not using any form of blending.
Data Augmentation
While pasting the objects on background, we also add the
following modes of data augmentation:
2D Rotation: The objects are rotated at uniformly sampled
No Blending
Gaussian Blurring
Poisson Blending
Figure 6: Different blending modes used while generating
datasets. These modes help the model in ignoring artifacts
arising from pasting objects on background scenes. More
details in Section 5.2.1
random angles in between 30 to −30 degrees to account for
camera/object rotation changes. Table 1 shows a gain of 3
AP points by adding this augmentation.
3D Rotation: As we can control this step, we have many
images containing atypical 3D rotations of the instances
which is hard to ﬁnd in real data. Table 1 shows a gain
of more than 4 AP points because of this augmentation. In
Section 6.2 and Figure 7, we show examples of how a model
trained on human collected data consistently fails to detect
instances from certain viewpoints because the training data
has poor viewpoint coverage and different biases from the
test set. This result shows the value of being able to synthesize data with diverse viewpoints.
Occlusion and Truncation: Occlusion and truncation naturally appear in images. They refer to partially visible objects (such as those in Figure 2). We place objects at the
boundaries of the images to model truncation, ensuring at
least 0.25 of the object box is in the image. To add occlusion, we paste the objects with partial overlap with each
other (max IOU of 0.75). Like other modes of augmentation, we can easily vary the amount of truncation/occlusion.
As Table 1 shows, adding truncation/occlusion improves the
result by as much as 10 AP points.
Distractor Objects:
We add distractor objects in the
scenes. This models real-world scenarios with multiple distractor objects. We use additional objects from the Big-
BIRD dataset as distractors. Presence of synthetic distractors also encourages the learning algorithm to not only latch
on to boundary artifacts when detecting objects but also improves performance by 3 AP points.
6. Experiments
We now compare the effectiveness of our synthesized
data against human annotated data on two benchmark
datasets. We ﬁrst describe our common experimental setup.
Synthesized Data: We analyze our design choices in Section 5 to pick the best performing ones. We use a total of
33 object instances from the BigBIRD Dataset overlap-
Missed detections on the Active Vision
Dataset for a model trained on the hand-annotated GMU
Dataset . The model consistently fails to detect certain
viewpoints as the training data has poor viewpoint coverage
and has biases different from the test set. Each row shows a
single instance.
Ground Truth Images
Corresponding False Positives
Figure 8: Examples of false positives from the UNC dataset
by the detector trained on the hand-annotated bounding
boxes from the GMU dataset. Object detectors trained on
hand annotated scenes also need new negatives to be able to
perform well in newer scenes.
ping with the 11 instances from GMU Kitchen Dataset 
and the 33 instances from Active Vision Dataset . We
use a foreground/background ConvNet (Section 5.1) to extract the foreground masks from the images.
The foreground/background ConvNet is not trained on instances we
Table 2: We compute the performance of training a model on synthetic data and compare it against training on real data. We
evaluate on the test split of the GMU Kitchen Dataset .
coca coffee
hunt’s mahatma nature nature palmolive
Real Images from GMU
SP-BL-SS 
(Ours) Synthetic Images
SP-BL-SS + Real Images 
(Ours) Synthetic + Real Images 88.5
use to evaluate detection. As in Section 5, we use backgrounds from the UW Scenes Dataset We generate
a synthetic dataset with approximately 6000 images using
all modes of data augmentation from Section 5. We sample scale, rotation, position and the background randomly.
Each background appears roughly 4 times in the generated
dataset with different objects. To model occlusions we allow a maximum IOU of 0.75 between objects. For truncations, we allow at least 25% of the object box to be in
the image.
For each scene we have three versions produced with different blending modes as described in Section 5.2.1. Figure 4 shows samples of generated images.
We use this synthetic data for all our experiments.
code used for generating scenes is available at: https:
//goo.gl/imXRt7.
Model: We use a Faster R-CNN model based on the
VGG-16 pre-trained weights on the MSCOCO detection task. We initialize both the RPN trunk and the object
classiﬁer trunk of the network in this way. We ﬁne-tune
on different datasets (both real and synthetic) and evaluate the model’s performance. We ﬁne-tune all models for
25K iterations using SGD+momentum with a learning rate
of 0.001, momentum 0.9, and reduce the learning rate by a
factor of 10 after 15K iterations. We also use weight decay
of 0.0005 and dropout of 0.5 on the fully-connected layers. We set the value of all the loss weights (both RPN and
classiﬁcation) as 1.0 in our experiments. We ensure that
the model hyperparameters and random seed do not change
across datasets/experiments for consistency.
Evaluation: We report Average Precision (AP) at IOU of
0.5 in all our experiments for the task of instance localization. Following , we consider boxes of size at least
50 × 30 pixels in the images for evaluation.
6.1. Training and Evaluation on the GMU Dataset
Similar to Section 5,
we use the GMU Kitchen
Dataset which contains 9 kitchen scenes with 6, 728 images. We evaluate on the 11 objects present in the dataset
overlapping with the BigBIRD objects. We additionally report results from . Their method synthesizes images by accounting for global scene structure when placing
Table 3: Evaluation on the entire Active Vision dataset by
varying the amount of real data from the GMU Kitchen
Scenes train dataset
hunt’s mahatma nature red mAP
cola bunches sauce
Real Images
24.6 46.6 41.9
49.0 23.0 36.5
Synthetic + Real Images 69.9
48.7 50.9 51.1
10% Real + Syn
40% Real + Syn
52.8 47.0 50.8
70% Real + Syn
48.5 51.8 50.6
objects in scenes, e.g., ensure that cups lie on ﬂat surfaces
like table tops. In contrast, our method does not use take
into account such global structure, but focuses on patchlevel realism. We note that their method uses a different background scenes dataset for their synthesis.
Table 2 shows the evaluation results. We see that training on the synthetic data is competitive with training on real
images (rows 1 vs 3) and also outperforms the synthetic
dataset from (rows 2 vs 3). Combining synthetic data
with the real data shows a further improvement for all synthetic image datasets (rows 4, 5). These results show that
the data generated by our approach is not only competitive
with both real data and existing synthetic data, but also provides complementary information. Figure 9 shows qualitative examples illustrating this point.
6.2. Evaluation on the Active Vision Dataset
To test generalization across datasets, we now present
experiments where we train on either our synthetic data or
the GMU Dataset , and evaluate on the Active Vision
Dataset . The Active Vision Dataset has 9 scenes and
17,556 images. It has 33 objects in total and 6 objects in
overlap with the GMU Kitchen Scenes. We use these 6 objects for our analysis. We do not use this dataset for training.
We train a model trained on all the images from the
GMU Dataset (Section 6.1). This model serves as a base-
Synthetic + Real Data
Synthetic + Real Data
Synthetic Data
Synthetic Data
Figure 9: We show qualitative detection results and mark true positives in green, false positives in red and arrows to highlight regions. The top two rows are from the GMU Kitchen Scenes and the bottom two rows from the Active Vision
Dataset . (a), (b): Model trained on real data misses objects which are heavily occluded (a) or stops detecting objects as
viewpoint changes from a to b. (c), (d): Model trained on synthetic data detects occluded and truncated objects. (e): Combining synthetic data removes false positives due to training only on real data. (g), (h): Combining real data removes false
positives due to training only on synthetic data. (f), (g): Viewpoint changes cause false negatives. (Best viewed electronically)
line for our model trained on synthetic data. As Table 3
shows, by collecting just 10% images and adding our synthetically generated images, we are able to get more MAP
than using the real images in the dataset without the synthetic images. This highlights how useful our approach of
dataset generation is in scenarios where there is a dearth of
labeled images. Also, the performance gap between these
datasets is smaller than in Section 6.1.
Failure modes of real data:
Upon inspecting the errors made by the GMU model, we see that a common
error mode of the detector is its failure to recognize certain views in the test-set (see Figure 7). These viewpoints
were sparsely present in the human annotated training data.
In contrast, our synthetic training data has a diverse viewpoint coverage. The model trained on the synthesized images drastically reduces these errors. Combining the synthesized images with the real images from GMU gives a
further improvement of 10 AP points suggesting that synthesized images do provide complementary information.
Varying Real Data: We investigate the effect of varying the
number of real images combined with the synthesized data.
We randomly sample different amounts of real images from
the GMU Dataset and combine them with the synthetic data
to train the detector. As a baseline we also train the model
on varying fractions of the real data. Table 3 shows that
by adding synthetic images to just 10% of the real images
we get a boost of 10 AP points over just using real images.
This performance is also tantalizingly close to the performance of combining larger fractions of real data. This result reinforces the effectiveness and complementary nature
of our approach. In the supplementary material, we present
additional such results.
7. Discussion and Future Work
We presented a simple technique to synthesize annotated
training images for instance detection.
Our key insights
were to leverage randomization for blending objects into
scenes and to ensure a diverse coverage of instance viewpoints and scales. We showed that patch-based realism is
sufﬁcient for training region-proposal based object detectors. Our method performs favorably to existing hand curated datasets and captures complementary information. In
a realistic cross-domain setting we show that by combining
just 10% of the available real annotations with our synthesized data, our model performs better than using all the real
annotations. From a practical standpoint our technique affords the possibility of generating scenes with non-uniform
distributions over object viewpoints and scales without additional data collection effort.
We believe our work can be combined with existing approaches that focus on global consistency for placing
objects and which model realism. Future work should
focus on a combination of such approaches.
Acknowledgements: The authors are grateful to Georgios Georgakis and
Phil Ammirato for their help with the datasets and discussions. This work
was supported in part by NSF Grant CNS1518865.