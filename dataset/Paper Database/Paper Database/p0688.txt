Task-Agnostic Meta-Learning for Few-shot Learning
Muhammad Abdullah Jamal†, Guo-Jun Qi†∗, and Mubarak Shah♯
† Laboratory for MAchine Perception and LEarning
 
♯Center for Research in Computer Vision
University of Central Florida
Meta-learning approaches have been proposed to tackle the few-shot learning problem.
Typically, a meta-learner is trained on a variety of tasks in the hopes of being generalizable to
new tasks. However, the generalizability on new tasks of a meta-learner could be fragile when it
is over-trained on existing tasks during meta-training phase. In other words, the initial model
of a meta-learner could be too biased towards existing tasks to adapt to new tasks, especially
when only very few examples are available to update the model. To avoid a biased meta-learner
and improve its generalizability, we propose a novel paradigm of Task-Agnostic Meta-Learning
(TAML) algorithms. Speciﬁcally, we present an entropy-based approach that meta-learns an
unbiased initial model with the largest uncertainty over the output labels by preventing it from
over-performing in classiﬁcation tasks. Alternatively, a more general inequality-minimization
TAML is presented for more ubiquitous scenarios by directly minimizing the inequality of initial
losses beyond the classiﬁcation tasks wherever a suitable loss can be deﬁned. Experiments
on benchmarked datasets demonstrate that the proposed approaches outperform compared
meta-learning algorithms in both few-shot classiﬁcation and reinforcement learning tasks.
Introduction
The key to achieving human level intelligence is to learn from a few labeled examples. Human can
learn and adapt quickly from a few examples using prior experience. We want our learner to be able
to learn from a few examples and quickly adapt to a changing task. All these concerns motivate to
study the few-shot learning problem. The advantage of studying the few-shot problem is that it
only relies on few examples and it alleviates the need to collect large amount of labeled training set
which is a cumbersome process.
Recently, meta-learning approach is being used to tackle the problem of few-shot learning. A
meta-learning model usually contains two parts – an initial model, and an updating strategy (e.g., a
parameterized model) to train the initial model to a new task with few examples. Then the goal of
meta-learning is to automatically meta-learn the optimal parameters for both the initial model and
the updating strategy that are generalizable across a variety of tasks. There are many meta-learning
approaches that show promising results on few-shot learning problems. For example, Meta-LSTM 
uses LSTM meta-learner that not only learns initial model but also the updating rule. On the
contrary, MAML only learns an initial model since its updating rule is ﬁxed to a classic gradient
descent method as a meta-learner.
∗Corresponding author: G.-J. Qi, email: .
 
The problem with existing meta-learning approaches is that the initial model can be trained
biased towards some tasks, particularly those sampled in meta-training phase. Such a biased initial
model may not be well generalizable to an unseen task that has a large deviation from meta-training
tasks, especially when very few examples are available on the new task. This inspires us to meta-train
an unbiased initial model by preventing it from overperforming on some tasks or directly minimizing
the inequality of performances across diﬀerent tasks, in a hope to make it more generalizable to
unseen tasks. To this end, we propose a Task-Agnostic Meta-Learning (TAML) algorithms in this
Speciﬁcally, we propose two novel paradigms of TAML algorithms – an entropy-based TAML
and inequality-minimization measures based TAML. The idea of using entropy based approach is
to maximize the entropy of labels predicted by the initial model to prevent it from overperforming
on some tasks. However, the entropy-based approach is limited to discrete outputs from a model,
making it more amenable to classiﬁcation tasks.
The second paradigm is inspired by inequality measures used in Economics. The idea is to
meta-train an initial model in such a way that it directly minimizes the inequality of losses by the
initial model across a variety of tasks. This will force the meta-learner to learn a unbiased initial
model without over-performing on some particular tasks. Meanwhile, any form of losses can be
adopted for involved task without having to rely on discrete outputs. This makes this paradigm
more ubiquitous to many scenarios beyond classiﬁcation tasks.
The remainder of the paper is organized as follows. We elaborate the proposed TAML approach
to meta-learning in Section 2. It is followed by a review about the related work (Section 3). In
Section 4, we present extensive experimental studies on few-shot classiﬁcation and reinforcement
Our goal is to train a model that can be task-agnostic in a way that it prevents the initial model or
learner to over-perform on a particular task. In this section, we will ﬁrst describe our entropy based
and inequality-minimization measures based approach to the problem, and then we will discuss some
of the inequality measures that we used in the paper.
Task Agnostic Meta-Learning
In this section, we propose a task-agnostic approach for few-shot meta-learning. The goal of few-shot
meta-learning is to train a model in such a way that it can learn to adapt rapidly using few samples
for a new task. In this meta-learning approach, a learner is trained during a meta-learning phase on
variety of sampled tasks so that it can learn new tasks , while a meta-learner trains the learner and
is responsible for learning the update rule and initial model.
The problem with the current meta-learning approach is that the initial model or learner can be
biased towards some tasks sampled during the meta-training phase, particularly when future tasks in
the test phase may have discrepancy from those in the training tasks. In this case, we wish to avoid
an initial model over-performing on some tasks. Moreover, an over-performed initial model could
also prevent the meta-learner to learn a better update rule with consistent performance across tasks.
To address this problem, we impose an unbiased task-agnostic prior on the initial model by
preventing it from over-performing on some tasks so that a meta-learner can achieve a more
competitive update rule. There have been many meta-learning approaches to few-shot learning
problems that have been brieﬂy discussed in the section 3. While the task-agnostic prior is a widely
applicable principle for many meta-learning algorithms, we mainly choose Model-Agnostic Meta
Learning approach (MAML) as an example to present the idea, and it is not hard to extend to other
meta-learning approaches.
In the following, we will depict the idea by presenting two paradigms of task-agnostic meta-learning
(TAML) algorithms – the entropy-maximization/reduction TAML and inequality-minimization
Entropy-Maximization/Reduction TAML
For simplicity, we express the model as a function fθ that is parameterized by θ. For example, it
can be a classiﬁer that takes an input example and outputs its discrete label. During meta-training,
a batch of tasks are sampled from a task distribution p(T ), and each task is K-shot N-way problem
where K represents the number of training examples while N represent the number of classes
depending on the problem setting. In the MAML, a model is trained on a task T i using K examples
and then tested on a few new examples Dval for this task.
A model has an initial parameter θ and when it is trained on the task T i, its parameter is
updated from θ to θi by following an updating rule. For example, for K-shot classiﬁcation, stochastic
gradient descent can be used to update model parameter by θi ←θ −α∇θLTi(fθ) that attempts to
minimize the cross-entropy loss LTi(fθ) for the classiﬁcation task Ti over K examples.
To prevent the initial model fθ from over-performing on a task, we prefer it makes a random
guess over predicted labels with an equal probability so that it is not biased towards the task. This
can be expressed as a maximum-entropy prior over θ so that the initial model should have a large
entropy over the predicted labels over samples from task T i.
The entropy for task Ti is computed by sampling xi from PTi(x) over its output probabilities yi,n
over N predicted labels:
HTi(fθ) = −Exi∼PTi(x)
ˆyi,n log(ˆyi,n)
where [yi,1, · · · , yi,N] = fθ(xi) is the predictions by fθ, which are often an output from a softmax
layer in a classiﬁcation task. The above expectation is taken over xi’s sampled from task Ti.
Alternatively, one can not only maximize the entropy before the update of initial model’s
parameter, but also minimize the entropy after the update. So overall, we maximize the entropy
reduction for each task T i as HTi(fθ)−HTi(fθi). The minimization of HTi(fθi) means that the model
can become more certain about the labels with a higher conﬁdence after updating the parameter
θ to θi. This entropy term can be combined with the typical meta-training objective term as a
regularizer to ﬁnd the optimal θ, which is
ETi∼P(T )LTi(fθi) + λ[−HTi(fθ) + HTi(fθi)]
where λ is a positive balancing coeﬃcient, and the ﬁrst term is the expected loss for the updated
model fθi. The entropy-reduction algorithm is summarized in 1.
Unfortunately, the entropy-based TAML is subject to a critical limitation – it is only amenable
to discrete labels in classiﬁcation tasks to compute the entropy. In contrast, many other learning
problems, such as regression and reinforcement learning problems, it is often trained by minimizing
some loss or error functions directly without explicitly accessing a particular form of outputs like
discrete labels. To make the TAML widely applicable, we need to deﬁne an alternative metric to
measure and minimize the bias across tasks.
Algorithm 1 Entropy-Reduction TAML for Few-Shot Classiﬁcation
Require: p(T ): distribution over tasks.
Require: α, β: hyperparameters
Randomly Initialize θ
while not done do
Sample batch of tasks T i ∼p(T )
for all T i do
Sample K samples from T i
Evaluate ∇θLTi(fθ) and LTi(fθ) using K samples .
Compute adapted parameters using gradient descent
θi ←θ −α∇θLTi
Sample Dval from Ti for meta update.
Update θ ←θ −β∇θ{ETi∼P(T )LTi(fθi) + λ[−HTi(fθ) + HTi(fθi)]} using Dval,
LTi, and HTi.
Inequality-Minimization TAML
We wish to train a task-agnostic model in meta-learning such that its initial performance is unbiased
towards any particular task T i. Such a task-agnostic meta-learner would do so by minimizing the
inequality of its performances over diﬀerent tasks.
To this end, we propose an approach based on a large family of statistics used to measure the
"economic inequalities" to measure the "task bias". The idea is that the loss of an initial model on
each task Ti is viewed as an income for that task. Then for the TAML model, its loss inequality over
multiple tasks is minimized to make the meta-learner task-agnostic.
Speciﬁcally, the bias of the initial model towards any particular tasks is minimized during
meta-training by minimizing the inequality over the losses of sampled tasks in a batch. So, given an
unseen task during testing phase, a better generalization performance is expected on the new task
by updating from an unbiased initial model with few examples. The key diﬀerence between both
TAMLs lies that for entropy, we only consider one task at a time by computing the entropy of its
output labels. Moreover, entropy depends on a particular form or explanation of output function,
e.g., the SoftMax output. On the contrary, the inequality only depends on the loss, thus it is more
ubiquitous.
The complete algorithm is explained in 2. Formally, consider a batch of sampled tasks {Ti}
and their losses {LTi(fθ)} by the initial model fθ, one can compute the inequality measure by
IE({LTi(fθ)}) as discussed later. Then the initial model parameter θ is meta-learned by minimizing
the following objective
ETi∼p(T ) [LTi(fθi)] + λIE({LTi(fθ)})
through gradient descent as shown in Algorithm 2. It is worth noting that the inequality measure is
computed over a set of losses from sampled tasks. The ﬁrst term is the expected loss by the model
fθi after the update, while the second is the inequality of losses by the initial model fθ before the
update. Both terms are a function of the initial model parameter θ since θi is updated from θ. In
the following, we will elaborate on some choices on inequality measures IE.
Inequality Measures
Inequality measures are instrumental towards calculating the economic inequalities in the outcomes
that can be wealth, incomes, or health related metrics. In meta-learning context, we use ℓi = LTi(fθ)
to represent the loss of a task Ti, ¯ℓrepresents the mean of the losses over sampled tasks, and M is
the number of tasks in a single batch. The inequality measures used in TAML are brieﬂy described
Theil Index .
This inequality measure has been derived from redundancy in information theory,
which is deﬁned as the diﬀerence between the maximum entropy of the data and an observed entropy.
Suppose that we have M losses {ℓi|i = 1, · · · , M}, then Thiel Index is deﬁned as
Generalized Entropy Index .
The relation between information theory and information
distribution analysis has been exploited to derive a number of measures for inequality. Generalized
Entropy index has been proposed to measure the income inequality. It is not a single inequality
measure, but it is a family that includes many inequality measures like Thiel Index, Thiel L etc. For
some real value α, it is deﬁned as:
α ̸= 0, 1,
From the equation, we can see that it does represent a family of inequality measures. When α is zero,
it is called a mean log deviation of Thiel L, and when α is one, it is actually Thiel Index. A larger
GE α value makes this index more sensitive to diﬀerences at the upper part of the distribution, and
a smaller α value makes it more sensitive to diﬀerences at the bottom of the distribution.
Atkinson Index .
It is another measure for income inequality which is useful in determining
which end of the distribution contributed the most to the observed inequality. It is deﬁned as :
for 0 ≤ϵ ̸= 1,
for ϵ = 1, ,
where ϵ is called "inequality aversion parameter". When ϵ = 0 the index becomes more sensitive to
the changes in upper end of the distribution ,and when it approaches to 1, the index becomes more
sensitive to the changes in lower end of the distribution.
Gini-Coeﬃcient .
It is usually deﬁned as the half of the relative absolute mean diﬀerence. In
terms of meta-learning, if there are M tasks in a single batch and a task Ti loss is represented by ℓi,
then Gini-Coeﬃcient is deﬁned as:
j=1 |ℓi −ℓj|
Gini- coeﬃcient is more sensitive to deviation around the middle of the distribution than at the
upper or lower part of the distribution.
Variance of Logarithms .
It is another common inequality measure deﬁned as:
[ln ℓi −ln g(ℓ)]2
where g(ℓ) is the geometric mean of ℓwhich is deﬁned as (QM
i=1 ℓi)1/M . The geometric mean put
greater emphasis on the lower losses of the distribution.
Algorithm 2 Inequality Measures Based TAML for Few-Shot Classiﬁcation
Require: p(T ): distribution over tasks.
Require: α, β: hyperparameters
Randomly Initialize θ
while not done do
Sample batch of tasks T i ∼p(T )
for all T i do
Sample K-shot samples from T i
Evaluate ∇θLTi(fθ) and LTi using K samples .
Compute adapted parameters using gradient descent
θi = θ −α∇θLTi
Sample a dataset Dval,i from task Ti used below.
Update θ ←θ −β∇θ[ETi∼p(T )LTi(fθi) + λIE({LTi(fθ)})] using Dval,i, LTi, and IE
Related Work
The idea of meta-learning has been proposed more than a couple of decades ago . Most of
the approaches to meta-learning include learning a learner’s model by training a meta-learner. Recent
studies towards meta-learning for deep neural networks include learning a hand-designed optimizer like
SGD by parameterizing it through recurrent neural networks. Li , and Andrychowicz studied
a LSTM based meta-learner that takes the gradients from learner and performs an optimization
step. Recently, meta-learning framework has been used to solve few-shot classiﬁcation problems. 
used the same LSTM based meta-learner approach in which LSTM meta-learner takes the gradient
of a learner and proposed an update to the learner’s parameters. The approach learns both weight
initialization and an optimizer of the model weights. Finn proposed a more general approach
for meta-learning known as MAML by simply learning weight initialization for a learner through a
ﬁxed gradient descent. It trains a model on a variety of tasks to have a good initialization point
Table 1: Few Shot Classiﬁcation results on Omniglot dataset for fully connected network and convolutional network on 5-way setting, where * means re-run results as there is no general training/test
splitting available for Omniglot, thus we re-run compared models with the same splitting used in
running the TAML for a fair comparison. The ± shows 95% conﬁdence interval over tasks.
MANN, no conv 
MAML, no conv 
89.7 ± 1.1%
97.5 ± 0.6 %(96.1 ± 0.4)%*
TAML(Entropy), no conv
91.19 ± 1.03%
97.40 ± 0.34%
TAML(Theil), no conv
91.37 ± 0.97%
96.84 ± 0.36%
TAML(GE(2)), no conv
91.3 ± 1.0%
96.76 ± 0.4%
TAML(Atkinson), no conv
91.77 ± 0.97%
97.0 ± 0.4%
TAML (Gini-Coeﬃcient), no conv
93.17 ± 1.0%
Siamese Nets 
Matching Nets 
Neural Statistician 
Memory Mod. 
Prototypical Nets 
Meta Nets 
Snail 
99.07 ± 0.16%
99.78 ± 0.09%
98.7 ± 0.4%
99.9± 0.1%
TAML(Entropy)
99.23 ± 0.35%
99.71 ± 0.1%
TAML(Theil)
99.5 ± 0.3%
99.81 ± 0.1 %
TAML(GE(2))
99.47 ± 0.25 %
99.83 ± 0.09%
TAML(Atkinson)
99.37 ± 0.3%
99.77 ± 0.1%
TAML (Gini-Coeﬃcient)
99.3 ± 0.32%
99.70 ± 0.1%
TAML(GE(0))
99.33 ± 0.31%
99.75 ± 0.09%
99.1 ± 0.36%
99.6 ± 0.1%
that can be quickly adapted (few or one gradient steps) to a new task using few training examples.
Meta-SGD extends the MAML, which not only learns weight initialization but also the learner’s
update step size.
 proposes a temporal convolution and attention based meta-learner called
SNAIL that achieves state-of-the-art performance for few-shot classiﬁcation tasks and reinforcement
learning tasks.
Other paradigms of meta-learning approaches include training a memory augmented neural
network on existing tasks by coupling with LSTM or feed-forward neural network controller .
There are also several non-meta-learning approaches to few-shot classiﬁcation problem by designing
speciﬁc neural architectures. For example, trains a Siamese network to compare new examples
with existing ones in a learned metric space. Vinyals used a diﬀerentiable nearest neighbour
loss by utilizing the cosine similarities between the features produced by a convolutional neural
network. proposed a similar approach to matching net but used a square euclidean distance
metric instead. In this paper, we mainly focus on the meta-learning approaches and their applications
to few-shot classiciation and reinforcement tasks.
Experiments
We report experiment results in this section to evaluate the eﬃcacy of the proposed TAML approaches
on a variety of few-shot learning problems on classiﬁcation and reinforcement learning.
Classiﬁcation
We use two benchmark datasets Omniglot and MiniImagenet for few-shot classiﬁcation problem.
The Omniglot dataset has 1623 characters from 50 alphabets. Each character has 20 instances which
are drawn by diﬀerent individuals. We randomly select 1200 characters for training and remaining
for testing. From 1200 characters, we randomly sample 100 for validation. As proposed in , the
dataset is augmented with rotations by multiple of 90 degrees.
The Mini-Imagenet dataset was proposed by and it consists of 100 classes from Imagenet
dataset. We used the same split proposed by for fair comparison. It involves 64 training classes,
12 validation classes and 20 test classes. We consider 5-way and 20-way classiﬁcation for both 1-shot
and 5-shot.
For K-shot N-way classiﬁcation, we ﬁrst sample N unseen classes from training set and for
every N unseen class, we sample K diﬀerent instances. We follow the same model architecture used
by . The Omniglot dataset images are downsampled by 28x28 and we use a strided convolutions
instead of max-pooling. The MiniImagenet images are downsampled to 84x84 and we used 32 ﬁlters
in the convolutional layers. We also evaluate the proposed approach on non-convolutional neural
network. For a fair comparison with MANN and MAML , we follow the same architecture
used by MAML . We use Leaky-ReLU as non-linearity instead of ReLU non-linearity.
We train and evaluate the meta-models based on TAML that are unbiased and show they can
be adapted to new tasks in few iterations as how they are meta-trained. For Omniglot dataset, we
use a batch size of 32 and 16 for 5-way and 20-way classiﬁcation, respectively. We follow for
other training settings. For fair comparison with Meta-SGD on 20-way classiﬁcation, the model was
trained with 1 gradient step. For 5-way Mini-Imagenet, we use a batch size of 4 for both 1-shot and
5-shot settings. For 20-way classiﬁcation on MiniImagenet, the learning rate was set to 0.01 for both
1-shot and 5-shot, and each task is updated using one-gradient step. All the models are trained for
60000 iterations. We use the validation set to tune the hyper-parameter λ for both the approaches.
We report the results for 5-way Omniglot for both fully connected network and convolutional network.
The convolutional network learned by TAML outperforms all the state-of-the-art methods in Table 1.
For 20-way classiﬁcation, we re-ran the Meta-SGD algorithm with our own training/test splitting
for fair comparison since the Meta-SGD is not open-sourced and their training/test split is neither
available. The results are reported in the Table 2. It can be shown that TAML outperforms MAML
and Meta-SGD for both 1-shot and 5-shot settings.
For MiniImagenet, the proposed TAML approaches outperform the compared ones for 5-way
classiﬁcation problem. The entropy based TAML achieves the best performance compared with
inequality-minimization TAML for 5-shot problem. For 20-way setting, we use the reported results
from Meta-SGD for both MAML and Meta-SGD. We outperform both MAML and Meta-SGD for
both 1-shot and 5-shot settings. It is interesting to note that MAML performs poor compared with
matching nets and Meta-learner LSTM when it is trained using one gradient step as reported in
Reinforcement Learning
In reinforcement learning, the goal is to learn the optimal policy given fewer trajectories or experiences.
A reinforcement learning task T i is deﬁned as Markov Decision Process that consists of a state space
S, an action space A, the reward function R, and state-transition probabilities qi(xt+1|xt, at) where
at is the action at time step t. In our experiments, we are using the same settings as proposed in 
Table 2: Few Shot Classiﬁcation results on Omniglot dataset for CNN on 20-way setting. For a fair
comparison, * denotes re-run results by both meta-learning approaches on the same training/test split
used in TAML models. The proposed TAML approaches outperform both MAML and Meta-SGD.
90.81 ± 0.5%
97.49 ± 0.15%
Meta-SGD* 
93.98 ± 0.43%
98.42 ± 0.11%
TAML(Entropy + MAML)
95.62 ± 0.5%
98.64 ± 0.13%
TAML(Theil + Meta-SGD)
95.15 ± 0.39%
98.56 ± 0.1%
TAML(Atkinson + Meta-SGD)
94.91 ± 0.42%
98.50 ± 0.1%
TAML (VL + Meta-SGD)
95.12 ± 0.39%
98.58 ± 0.1%
TAML(Theil + MAML)
92.61 ± 0.46%
98.4 ± 0.1%
TAML(GE(2) + MAML)
91.78 ± 0.5%
97.93 ± 0.1%
TAML(Atkinson + MAML)
93.01 ± 0.47%
98.21 ± 0.1%
TAML(GE(0) + MAML)
92.95 ± 0.5%
98.2 ± 0.1%
TAML (VL + MAML)
93.38 ± 0.47%
98.54 ± 0.1%
Table 3: Few Shot Classiﬁcation results on Mini-Imagenet dataset on 5-way and 20-way setting.
The results for other methods on 5-way are reported from MAML, and for 20-way, the results are
reported from Meta-SGD. TAML approaches outperform MAML on both settings and Meta-SGD
on 20-way setting.
28.86 ± 0.54%
49.79 ± 0.79%
Nearest Neighbors
41.08 ± 0.70%
51.04 ± 0.65%
Matching Nets 
43.56 ± 0.84%
55.31 ± 0.73%
17.31 ± 0.22%
22.69 ± 0.20%
Meta-Learn LSTM 
43.44 ± 0.77%
60.60 ± 0.71%
16.70 ± 0.23%
26.06 ± 0.25%
MAML (ﬁrstorderapprox.) 
48.07 ± 1.75%
63.15 ± 0.91%
48.70 ± 1.84%
63.11 ± 0.92%
16.49 ± 0.58%
19.29 ± 0.29%
Meta-SGD 
50.47 ± 1.87%
64.03 ± 0.94%
17.56 ± 0.64%
28.92 ± 0.35%
TAML(Entropy + MAML)
49.33 ± 1.8%
66.05 ± 0.85%
TAML(Theil + MAML)
49.18 ± 1.8%
65.94 ± 0.9%
18.74 ± 0.65%
25.77 ± 0.33%
TAML(GE(2) + MAML)
49.13 ± 1.9%
65.18 ± 0.9%
18.22 ± 0.67%
24.89 ± 0.34%
TAML(Atkinson + MAML)
48.93 ± 1.9%
65.24 ± 0.91%
TAML(GE(0) + MAML)
48.73 ± 1.8%
65.71 ± 0.9%
18.95 ± 0.68%
24.53± 0.33%
TAML (VL + MAML)
49.4 ± 1.9%
66.0 ± 0.89%
18.13 ± 0.64%
25.33 ± 0.32%
TAML(GE(0) + Meta-SGD)
18.61 ± 0.64%
29.75± 0.34%
TAML (VL + Meta-SGD)
18.59 ± 0.65%
29.81 ± 0.35%
where we are sampling trajectories using policy fθ. The loss function used is the negative of the
expectation of the sum of the rewards,LTi = −Eat∼fθ,xt,qTi
t=1 Ri(xt, at)
Experiments were performed using rllab suite . Vanilla policy gradient is used to for
inner gradient updates while trust region policy optimizer (TRPO) is used as meta-optimizer.
The algorithm is the same as mentioned in algorithm 2 with the only diﬀerence bing that trajectories
were sampled instead of images.
For reinforcement learning experiment, we evaluate TAML on a 2D navigation task. The policy
network that was used in performing this task is identical to the policy network that was used in
 for a fair comparison, which is a three-layered network using ReLU while setting the step size
α = 0.1. The experiment consists an agent moving in two-dimensional environment and the goal of
the agent is to reach the goal state that is randomly sampled from a unit square. For evaluation
purposes, we compare the results of TAML with MAML, oracle policy, conventional pre-training and
random initialization. Our results have shown that GE(0), Theil, and GE(2) TAML perform on-par
with MAML after 2 gradient steps but start to outperform it afterwards as shown in ﬁgure 1.
Figure 1: Results on 2D Navigation task.
Conclusion
In this paper, we proposed a novel paradigm of Task-Agnostic Meta-Learning (TAML) algorithms to
train a meta-learner unbiased towards a variety of tasks before its initial model is adapted to unseen
tasks. Both an entropy-based TAML and a general inequality-minimization TAML applicable to
more ubiquitous scenarios are presented. We argue that the meta-learner with unbiased task-agnostic
prior could be more generalizable to handle new tasks compared with the conventional meta-learning
algorithms. The experiment results also demonstrate the TAML could consistently outperform
existing meta-learning algorithms on both few-shot classiﬁcation and reinforcement learning tasks.