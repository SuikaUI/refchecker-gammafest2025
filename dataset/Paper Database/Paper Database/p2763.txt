MIT Open Access Articles
Eyeriss: An Energy-Efficient Reconfigurable
Accelerator for Deep Convolutional Neural Networks
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Chen, Yu-Hsin, Tushar Krishna, Joel Emer, and Vivienne Sze. "Eyeriss: An Energy-
Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks." in ISSCC 2016,
IEEE International Solid-State Circuits Conference, Jan. 31-Feb. 4, 2016. San Francisco, CA.
As Published: 
ISSCC2016AdvanceProgram.pdf
Publisher: Institute of Electrical and Electronics Engineers (IEEE)
Persistent URL: 
Version: Author's final manuscript: final author's manuscript post peer review, without
publisher's formatting or copy editing
Terms of use: Creative Commons Attribution-Noncommercial-Share Alike
Eyeriss: An Energy-Efficient Reconfigurable
Accelerator for Deep Convolutional Neural Networks
Yu-Hsin Chen1, Tushar Krishna1, Joel Emer1,2, Vivienne Sze1
1Massachusetts Institute of Technology, Cambridge, MA,
Westford, MA
Deep learning using convolutional neural networks (CNN) gives state-of-the-art
accuracy on many computer vision tasks (e.g. object detection, recognition,
segmentation). Convolutions account for over 90% of the processing in CNNs
for both inference/testing and training, and fully convolutional networks are
increasingly being used. To achieve state-of-the-art accuracy requires CNNs
with not only a larger number of layers but also millions of filters weights, and
varying shapes (i.e. filter sizes, number of filters, number of channels) as shown
in Fig. 1. For instance, AlexNet uses 2.3 million weights (4.6MB of storage)
and requires 666 million MACs per 227x227 image (13 kMACs/pixel). VGG16
 uses 14.7 million weights (29.4 MB of storage) and requires 15.3 billion
MACs per 224x224 image (306 kMACs/pixel). The large number of filter
weights and channels results in substantial data movement, which consumes
significant energy.
Existing accelerators do not support the configurability necessary to efficiently
support large CNNs with different shapes , and using mobile GPUs can be
expensive . This paper describes an accelerator that can deliver state-ofthe-art accuracy with minimum energy consumption in the system (including
DRAM) in real-time, by using two key methods: (1) efficient dataflow and
supporting hardware (spatial array, memory hierarchy and on-chip network) that
minimize data movement by exploiting data reuse and support different shapes;
(2) exploit data statistics to minimize energy through zeros skipping/gating to
avoid unnecessary reads and computations; and data compression to reduce
off-chip memory bandwidth, which is the most expensive data movement.
Fig. 2 shows the top-level architecture and memory hierarchy of the
accelerator. Data movement is optimized by buffering input image data (Img),
filter weights (Filt) and partial sums (Psum) in a shared 108KB SRAM buffer,
which facilitates the temporal reuse of loaded data. Image data and filter
weights are read from DRAM to the buffer and streamed into the spatial
computation array allowing for overlap of memory traffic and computation. The
streaming and reuse allows the system to achieve high computational efficiency
even when running the memory link at a lower clock frequency than the spatial
array. The spatial array computes inner products between the image and filter
weights, generating partial sums that are returned from the array to the buffer
and then, optionally rectified (ReLU) and compressed, to the DRAM. Runlength-based compression reduces the average image bandwidth by 2x.
Configurable support for image and filter sizes that do not fit completely into the
spatial array is achieved by saving partial sums in the buffer and later restoring
them to the spatial array. The sizes of the spatial array and buffer determine the
number of such ‘passes’ needed to do the calculations for a specific layer.
Unused PEs are clock gated.
Fig. 3 shows the dataflow within the array for filter weights, image values and
partial sums. If the filter height (R) equals the number of rows in the array (in
our case 12), the logical dataflow would be as follows: (1) filter weights are fed
from the buffer into the left column of the array (one filter row per PE) and the
filter weights move from left to right within the array; (2) image values are fed
into the left column and bottom row of the array (one image row per PE) and
the image values move up diagonally; (3) partial sums for each output row
move up vertically, and can be read out of the top row at the end of the
computational pass. If the partial sums are used in the next pass, they are fed
into the bottom row of the array from the buffer at the beginning of the next
computational pass.
In order to maximize utilization of a fixed-size array for different shapes, the
mapping may require either folding or replication if the shape size is larger or
smaller than the array dimension, respectively. Replication results in increased
throughput as compared to the purely logical dataflow described above. Cases
II, III, IV, and V in Fig. 3 illustrate the replication and folding of image values for
various layers of AlexNet. The same data values are shown in the same color.
Across the six example cases, which include physical mapping of filter weights,
image values and partial sums onto the fixed-size spatial array, we see the
logical dataflow patterns translating to myriad physical dataflow patterns that
need to be supported. Furthermore, the same data value is often needed by
multiple PEs, whose physical location in the array depends on the data type
(filter, image or partial sum) and layer.
Since different layers have different shapes and hence different mappings, a
design-time fixed interconnect topology will not work. Every PE can potentially
be a destination for a piece of data in some particular configuration, and so a
Network-on-Chip (NoC) is needed to support address based data delivery.
However, traditional NoC designs with switches at every PE to buffer/forward
data to one or multiple targets would result in multi-cycle delays. A full-chip
broadcast to every PE could work, but would consume enormous power.
To optimize data movement, it is important to exploit spatial reuse, where a
single buffer read can be used by multiple PEs (i.e. multicast). Fig. 4 shows our
NoC that supports configurable data patterns, and provides an energy-efficient
multicast to a variable number of PEs within a single-cycle. The NoC comprises
one Global Y bus, and 12 Global X buses (one per row). Each PE is configured
with a (row, col) ID at the beginning of processing via a scan chain. Multicast to
any subset of PEs is achieved by assigning the same ID to multiple PEs. Data
from the buffer is tagged with the target PEs’ (row, col) ID, and multicast
controllers at the input of each X bus and each PE deliver data only to those X
buses and PEs, respectively, that match the target id to avoid unnecessary
switching. Data is sent on the buses only if all target PEs are ready (i.e., have
an empty buffer) to receive. To support high bandwidth, we use separate input
NoCs for filter, image, and partial sums. The partial sum NoC has a separate
set of output links to the buffer to write the final partial sums. The NoC data
delivery for four of the cases from Fig. 3 is shown in Fig. 4.
Each processing engine, shown in Fig. 5, is a three-stage pipeline responsible
for calculating the inner product of the input image and filter weights for a single
row of the filter. The sequence of partial sums for the sliding filter window is
computed sequentially. The partial sums for the row are passed on a local link
to the neighboring PE (see Fig. 4) where the cross-row partial sums are
computed. Local scratch pads allow for energy-efficient temporal reuse of input
image and filter weights by recirculating values needed by different windows. A
partial sum scratch pad allows for temporal reuse of partial sums being
generated for different images and/or channels and filters. Data gating is
achieved by recording the input image values of zero in a ‘zero buffer’ and
skipping filter reads and computation for those values resulting in a 45% power
savings in the PE.
The test chip is implemented in 65nm CMOS. It operates at 200MHz core clock
and 60MHz link clock, which results in a frame rate of 34.7fps on the five
convolutional layers in AlexNet and a measured power of 278mW at 1V. The
PE array, NoC and on-chip buffer consume 77.8%, 15.6% and 2.7% of the total
power, respectively. The core and link clocks can scale up to 250MHz and
90MHz, respectively. This enables us to achieve a throughput of 44.8fps at
1.17V. Fig. 6 shows the performance at each layer including compression ratio,
power consumption, PE utilization, and memory access to highlight the
reduction in DRAM bandwidth, efficiency of the reconfigurable mapping and
reduced data access due to data reuse, respectively. A die photo of the chip
and the range of the shapes it can support natively are shown in Fig. 7.
Acknowledgement: This work is funded by the DARPA YFA grant N66001-14-1-
4039, MIT Center for Integrated Circuits & Systems, and a gift from Intel. The
authors would also like to thank Mehul Tikekar and Michael Price for their
technical assistance.
 A. Krizhevsky, I. Sutskever, G. E. Hinton, “ImageNet Classification with
Deep Convolutional Neural Networks,” Neural Information Processing Systems,
 K. Simonyan, A. Zisserman, “Very Deep Convolutional Networks for Large-
Scale Image Recognition,” CoRR, abs/1409.1556, 2014.
 S. Park et al., “A 1.93 TOPS/W Scalable Deep Learning/Inference Processor
with Tetra-parallel MIMD Architecture for Big Data Applications,” ISSCC, 2015
 S. Chetlur et al, “cuDNN: Efficient Primitives for Deep Learning,” CoRR,
abs/1410.0759,
Figure 14.5.1: Deep CNNs are large with varying shapes
Figure 14.5.2: Top level architecture with 168 PEs
Figure 14.5.3: Logical and physical dataflows
Figure 14.5.4: Network-on-Chip (NoC) for multicasting
Figure 14.5.5: 3-stage pipelined processing engine
Figure 14.5.6: Performance of AlexNet conv layers
Figure 14.5.7: Chip spec and die photo