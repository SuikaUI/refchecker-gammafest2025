Multi-Scale Convolutional Neural Networks for Time Series
Classiﬁcation
Zhicheng Cui
Department of Computer
Science and Engineering
Washington University in St.
Louis, USA
 
Wenlin Chen
Department of Computer
Science and Engineering
Washington University in St.
Louis, USA
 
Yixin Chen
Department of Computer
Science and Engineering
Washington University in St.
Louis, USA
 
Time series classiﬁcation (TSC), the problem of predicting
class labels of time series, has been around for decades within
the community of data mining and machine learning, and
found many important applications such as biomedical engineering and clinical prediction. However, it still remains
challenging and falls short of classiﬁcation accuracy and ef-
ﬁciency. Traditional approaches typically involve extracting
discriminative features from the original time series using
dynamic time warping (DTW) or shapelet transformation,
based on which an oﬀ-the-shelf classiﬁer can be applied.
These methods are ad-hoc and separate the feature extraction part with the classiﬁcation part, which limits their accuracy performance. Plus, most existing methods fail to take
into account the fact that time series often have features at
diﬀerent time scales.
To address these problems, we propose a novel end-to-end neural network model, Multi-scale
Convolutional Neural Network (MCNN), which incorporates
feature extraction and classiﬁcation in a single framework.
Leveraging a novel multi-branch layer and learnable convolutional layers, MCNN automatically extracts features at
diﬀerent scales and frequencies, leading to superior feature
representation. MCNN is also computationally eﬃcient, as
it naturally leverages GPU computing. We conduct comprehensive empirical evaluation with various existing methods
on a large number of benchmark datasets, and show that
MCNN advances the state-of-the-art by achieving superior
accuracy performance than other leading methods.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications-
Data Mining; J.3 [Computer Applications]:
Medical Sciences
General Terms
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.
Time series classiﬁcation, deep learning, convolutional neural network
INTRODUCTION
Our daily lives constantly produce time series data, such
as stock prices, weather readings, biological observations,
health monitoring data, etc. In the era of big data, there
are increasing needs to extract knowledge from time series
data, among which a main task is time series classiﬁcation
(TSC), the problem of predicting class labels for time series. It has been a long standing problem with a large scope
of real-world applications. For example, there has been active research on clinical prediction, the task of predicting
whether a patient might be in danger of certain deterioration based on the patient’s clinical time series such as ECG
signals. A real-time deterioration warning system powered
by TSC has achieved unprecedented performance compared
with traditional clinical approaches and been applied in major hospitals .
Most existing TSC approaches fall into two categories :
distance-based methods and feature-based methods.
For distance-based methods, the key part is to measure
the similarity between any given two time series.
on the similarity metrics, the classiﬁcation can be done using algorithms such as k-nearest neighbors (kNN) or support vector machines (SVM) with similarity-based kernels.
The most notable similarity measurement is dynamic time
warping (DTW) which aligns two time series with dynamic
warping to get the best ﬁt. It could be easily done through
dynamic programming.
For feature-based methods, each time series is characterized with a feature vector and any feature-based classiﬁer
(e.g. SVM or logistic regression) can be applied to generate the classiﬁcation results. There have been many handcrafted feature extraction schemes across diﬀerent applications. For example, in a clinical prediction application, each
time series is divided into several consecutive windows and
features are extracted from each window. The ﬁnal feature
vector is a concatenation of feature vectors from all windows .
The features include simple statistics such as
mean and variance, as well as complex features from detrended ﬂuctuation analysis and spectral analysis. Another
approach extracts features based on shapelets which can be
regarded as a signature subsequence of the time series. Typically, potential candidate shapelets are generated in advance
and they can be used in diﬀerent ways. For example, they
 
can be considered as a dictionary and each shapelet is regarded as a word. The time series is then described by a
bag-of-word model. A more recent study constructs the
feature vector such that the value of each feature is the minimum distance between anywhere in the time series and the
corresponding shapelet. A drawback of the shapelet method
is that it requires extensive search for the discriminative
shapelets from a large space. To bypass the need of trying
out lots of shapelet candidates, Grabocka et al. propose
to jointly learn a number of shapelets of the same size along
with the classiﬁer. However, their method only oﬀers linear
separation ability.
In recent years, convolutional neural networks (CNNs)
have led to impressive results in object recognition , face
veriﬁcation , and audio classiﬁcation .
A key reason for the success of CNNs is its ability to automatically learn complex feature representations using its
convolutional layers. With the great recent success of deep
learning and the presence of so many various handcrafted
features in TSC, it is natural to ask a question: is it possible to automatically learn the feature representation from
time series? However, there have not been many research
eﬀorts in the area of time series to embrace deep learning approaches. In this paper, we advocate a novel neural
network architecture, Multi-scale Convolutional Neural Network (MCNN), a convolutional neural network speciﬁcally
designed for classifying time series.
A distinctive feature of MCNN is that its ﬁrst layer contains multiple branches that perform various transformations of the time series, including those in the frequency
and time domains, extracting features of diﬀerent types and
time scales.
Subsequently, convolutional layers apply dot
products between the transformed waves and 1-D learnable
ﬁlters, which is a general way to automatically recognize
various types of features from the input. As a single convolutional layer can detect local patterns similar to shapelets,
stacking multiple convolutional layers can construct more
complex patterns. As a result, MCNN is a powerful generalpurpose framework for TSC. Diﬀerent than traditional TSC
methods, MCNN is an end-to-end model without requiring any handcrafted features.
We conduct comprehensive
experiments and compare with many existing TSC models.
Strong empirical results show that MCNN elevates the stateof-the-art of TSC. It gives superior overall performance, surpassing most existing models by a large margin, especially
when enough training data is present.
MULTI-SCALE CONVOLUTIONAL NEU-
RAL NETWORK (MCNN) FOR TSC
In this section, we formally deﬁne the aforementioned time
series classiﬁcation (TSC) problem. Then we describe our
MCNN framework for solving TSC problems.
Notations and Problem Deﬁnition
A time series is a sequence of real-valued data points with
timestamps.
In this paper, we focus on time series with
identical interval length. We denote a time series as T =
{t1, t2, ..., tn}, where ti is the value at time stamp i and
there are n timestamps for each time series.
We denote a labelled time series dataset as D = {(Ti, yi)}N
which contains N time series and their associated labels. For
each i = 1, · · · , N, Ti represents the ith time series and its
label is yi. For ease of presentation, in this paper we consider classiﬁcation problems where yi is a categorical value in
C = {1, · · · , C} where C ∈Z+ is the number of labels. However, our framework can be easily extended to real-valued
regression tasks. The TSC problem is to build a predictive
model to predict a class label y ∈C given an input time
series T. Unlike some previous works, we do not require all
training and testing time series to have the same number of
timestamps in our framework.
MCNN framework
Time series classiﬁcation is a long standing problem that
has been studied for decades. However, it remains a very
challenging problem despite great advancement in data mining and machine learning. There are some key factors contributing to its diﬃculty.
First, diﬀerent time series may
require feature representations at diﬀerent time scales. For
example, it is found that certain long-range (over a few
hours involving hundreds of time stamps) patterns in body
temperature time series have predictive values in forecasting sepsis . Existing TSC features can rarely adapt to
the right scales. Second, in real-world time series data, discriminative patterns in the time series is often distorted
by high-frequency perturbations and random noises.
Automatic smoothing and de-noising procedures are needed to
make the overall trend of the time series more clear.
To address these problems for TSC, we propose a multiscale convolutional neural network (MCNN) framework in
which the input is the time series to be predicted and the
output is its label.
The overall architecture of MCNN is
depicted in Figure 1.
The MCNN framework has three sequential stages: transformation, local convolution, and full convolution.
1) The transformation stage applies various transformations on the input time series. We currently include identity mapping, down-sampling transformations in the time
domain, and spectral transformations in the frequency domain. Each part is called a branch, as it is a branch input
to the convolutional neural network.
2) In the local convolution stage, we use several convolutional layers to extract the features for each branch. In
this stage, the convolutions for diﬀerent branches are independent from each other. All the outputs will pass through
a max pooling procedure with multiple sizes.
3) In the full convolution stage, we concatenate all extracted features and apply several more convolutional layers
(each followed by max pooling), fully connected layers, and
a softmax layer to generate the ﬁnal output.
This is an
entirely end-to-end system and all parameters are trained
jointly through back propagation.
Transformation stage
Multi-scale branch.
A robust TSC model should be
able to capture temporal patterns at diﬀerent time scales.
Long-term features reﬂect overall trends and short-term features indicate subtle changes in local regions, both of which
can be potentially crucial to the prediction quality for certain tasks.
In the multi-scale branch of MCNN, we use down-sampling
to generate sketches of a time series at diﬀerent time scales.
Suppose we have a time series T = {t1, t2, ..., tn} and the
down-sampling rate is k, then we will only keep every kth
Multi-Frequency
Multi-Scale
Down-sampling
Identity mapping
Convolution
Concatenation
Convolution
Fully connected
label distribution
Transformation Stage
Local Convolution Stage
Full Convolution Stage
Figure 1: Overall architecture of MCNN.
data points in the new time series:
T k = {t1+k∗i}, i = 0, 1, ..., ⌊n −1
Using this method, we generate multiple new input time
series with diﬀerent down sampling rates, e.g. k = 2, 3, · · · .
Multi-frequency branch.
In real-world applications,
high-frequency perturbations and random noises widely exist in the time series data due to many reasons, which poses
another challenge to achieving high prediction accuracy. It
is often hard to extract useful information on raw time series
data with the presence of these noises. In MCNN, we adopt
low frequency ﬁlters with multiple degrees of smoothness to
address this problem.
A low frequency ﬁlter can reduce the variance of time series. In particular, we employ moving average to achieve this
goal. Given an input time series, we generate multiple new
time series with varying degrees of smoothness using moving
average with diﬀerent window sizes. This way, newly generated time series represent general low frequency information,
which make the trend of time series more clear. Suppose the
original time series is T = {t1, t2, ..., tn}, the moving average
works by converting this original time series into a new time
T ℓ= xi + xi+1 + ... + xi+ℓ−1
where ℓis the window size and i = 0, 1, ...n −ℓ+ 1. With
diﬀerent ℓ, MCNN generates multiple time series of diﬀerent
frequencies, all of which will be fed into the local convolutional layer for this branch. Diﬀerent from the multi-scale
branch, each time series in the multi-frequency branch has
the same length, which allows us to assemble them into multiple channels for the following convolutional layer.
Local convolution stage
Local convolution.
After down sampling, we obtain
multiple time series with diﬀerent lengths from a single input
time series. We apply independent 1-D local convolutions
on each of these newly generated time series. In particular,
the ﬁlter size of local convolution will be the same across
all these time series.
Note that, with a same ﬁlter size,
shorter time series would get larger local receptive ﬁeld in
the original time series. This way, each output from the local
convolution stage captures a diﬀerent scale of the original
time series. An advantage of this method is that, by down
sampling the time series instead of increasing the ﬁlter size,
we can greatly reduce the number of parameters in the local
convolutional layer.
Max pooling with multiple sizes.
Max pooling, a
form of non-linear down-sampling, is also performed between
successive convolutional layers in MCNN. This can reduce
feature maps’ size as well as the amount of following layers’
parameters to avoid overﬁtting and improve computation
eﬃciency. More importantly, the max pooling operation introduces invariance to spatial shifting, making MCNN more
Instead of using small pooling sizes like 2 or 5, in MCNN
we introduce a variable called the pooling factor, p, which
is the length after max pooling. Suppose the output time
series after convolution has a length of n, then both our
pooling size and stride in max pooling are n
p . The pooling
size is fairly large since p is often chosen from {2, 3, 5}. By
doing this, we can have more ﬁlters and enforce each ﬁlter
to learn only a local feature, since in the backpropogation
phase, ﬁlters will be updated based on those few activated
convolution parts.
Full convolution stage
After extracting feature maps from multiple branches, we
concatenate all these features and feed them into other convolutional layers as well as a fully connected layer followed
by a softmax transformation. Following , we adopt the
technique of deep concatenation to concatenate all the feature maps vertically.
The output of MCNN will be the predicted distribution
of each possible label for the input time series. To train the
neural network, MCNN uses the cross-entropy loss deﬁned
where o(i)
yi is the yth
output of instance i through the neural network, which is the probability of its true label. The
parameters W and bias b in MCNN are those in local and
full convolutional layers, as well as those in the fully connected layers, all of which are learned jointly through back
propagation.
Data augmentation
One advantage for our framework is the ability to deal
with large scale datasets. When dealing with smaller datasets,
convolutional nets tend to overﬁt. Currently, most publicly
available TSC datasets have limited sizes. To overcome this
problem, we propose a data augmentation technique on the
original datasets in order to avoid overﬁtting and improve
the generalization ability. For massive datasets with abundant training data, data augmentation may not be needed.
We propose window slicing for the data augmentation.
For a time series T = {t1, · · · , tn}, a slice is a snippet of
the original time series, deﬁned as Si:j = {ti, ti+1, ..., tj},
1 ≤i ≤j ≤n. Suppose a given time series T is of length
n, and the length of the slice is s, our slicing operation will
generate a set of n-s+1 sliced time series:
Slicing(T, s) = {S1:s, S2:s+1, · · · , Sn−s+1:n},
where all the time series in Slicing(T, s) have the same label
as their original time series T does.
We apply window slicing on all time series in a given training dataset. When doing training, all the training slices are
considered independent training instances. We also do window slicing when predicting the label of a testing time series.
We ﬁrst use the trained MCNN to predict the label of each
of its slices, and then use a majority vote among all these
slices to make the ﬁnal prediction. Another advantage of
slicing is that the time series are not required to have equal
length since we can always cut all the time series into the
same length using window slicing.
DISCUSSION
In this section, we discuss several properties of the MCNN
framework and its relations to some other important works.
Effectiveness of convolution ﬁlters
Convolution has been a well-established method for handling sequential signals . We advocate that it is also a
good ﬁt for capturing characteristics in time series. Suppose
f is a ﬁlter of length m and T is a time series. Let T · f
be the result of 1-dimensional discrete convolution. The ith
element of the result is given by
(T · f)[i] =
fm+1−j · ti+j−1
Depending on the ﬁlter, the convolution is capable of extracting many insightful information from the original time
series. For example, if f = [1, −1], the result of the convolution would be the gradient between any two neighboring
Before Convolution
After Convolution
Max value of
each curve
Figure 2: Three time Series on Gun Point dataset
before and after performing the convolution operation. The two blue curves belong to one class and
the red curve belongs to a diﬀerent class.
points. However, is MCNN able to learn such kind of ﬁlters? The answer is yes. To show this, we train MCNN on
a real-world dataset Gun Point with a ﬁlter whose size is 15
(m = 15). For illustration, we pick one of the ﬁlters learned
by MCNN as well as 3 time series from the dataset.
We show the shape of this selected ﬁlter and these 3 time
series on the left of Figure 2, and the shape after convolution
with the ﬁlter. Here, the two blue curves belong to one class
and the red curve belongs to a diﬀerent class. The learned
ﬁlter (shown in the left ﬁgure) may look random at the ﬁrst
glance. However, a closer examination shows that it makes
First, we can observe from the left ﬁgure that each time series has a upward part and a downward part no matter which
label it has. After convolution with the ﬁlter, all three new
signals form a valley at the location of the upward part and
a peak at the location of the downward part. Second, since
in MCNN we use max pooling right after convolution, the
learned ﬁlter correctly ﬁnds that the downward part is more
important, as max pooling only picks the maximum value
from each convolved signal. As a result, the convolution and
max pooling correctly diﬀerentiate the blue curves and red
curve, since the maximum values of the two blue curves after convolution is greater than that of the red curve. By
this visualization, MCNN also oﬀers certain degree of interpretability as it tells us the characteristic found by MCNN.
Third, these three time series have similar overall shapes but
diﬀerent time scales, as the “plateau” on the top have diﬀerent lengths. It is very challenging for other methods such as
DTW or shapelet to classify them. However, a single ﬁlter
learned by MCNN coupled with max pooling can classify
them well.
To further demonstrate the power of convolution ﬁlters
for TSC, we compute the max pooling result of all times
series in the train set convolving with the ﬁlter shown in
Figure 2, and show all of them in Figure 3. Here, each point
corresponds to a time series in the dataset. The blue and
red points correspond to two diﬀerent classes, respectively.
The x-axis is the max-pooling value of each point, and yaxis is the class label. We can see from Figure 3 that, if
we set the classiﬁcation threshold at around 0.25, one single
convolution ﬁlter can already achieve very high accuracy to
classify the dataset.
Relation to learning shapelets
A major class of TSC methods are based on shapelet analysis which assumes that time series are featured by some
Maxpooling Value on Train Set
Figure 3: After training MCNN, we picked one ﬁlter and perform the convolution operation and max
pooling on all training data.
common subsequences.
Shapelet can either be extracted
from existing time series, or learned from the data. A recent
study proposes a learning time series shapelet (LTS)
method which achieves unprecedented performance improvement over simple extraction.
In the LTS method, each
time series can be represented by a feature vector in which
each feature is the similarity between the time series and a
shapelet. A logistic regression is applied on this new representation of time series to get the ﬁnal prediction. Both the
shapelets and parameters in the logistic regression model are
jointly learned.
There is a strong relevance between the LTS method and
MCNN, as both learn the parameters of the shapelets or ﬁlters jointly with a classiﬁer. In fact, LTS can be viewed as a
special case of MCNN. To make this more clear, let us ﬁrst
consider a simpler architecture, a special case of MCNN,
where there is only one identity branch, and the input time
series is processed by a 1-D convolutional layer followed by
a softmax layer. The 1-D convolutional ﬁlter in the model
can be regarded as a shapelet. The second layer (after convolution) is the new representation of the input time series.
In this case, each neuron in the second layer is a inner product between the ﬁlter (or shapelet) and the corresponding
window of the input time series. From this, we can see that
MCNN model adopts inner product as the similarity measurement while LTS employs the Euclidean distance.
To further show the relationship between inner product in
convolution and Euclidean distance, we can actually express
the Euclidean distance in the form of convolution. Let T ⊖
f be the Euclidean distances between a time series T =
{t1, · · · , tn} and a ﬁlter f = {f1, · · · , fm}, its ith element is:
ti+j−1 −fm+1−j
ti+j−1fm+1−j
j −2(T · f)[i]
From Eq. (5), the Euclidean distance is nothing but the
combination of convolution T · f (after ﬂipping the sign of
f) and the ℓ2 norms of f and a part of T. The ﬁrst term
in Eq. (5) is a constant for each time series, and therefore
can be regarded as a bias which MCNN has incorporated
in the model.
We can thus see that learning shapelets is
a special case of learning convolution ﬁlters when the ﬁlters are restricted to have the same ℓ2 norm. Moreover, if
we consider the full MCNN framework, its multi-scale and
multi-frequency branches make it even more general to handle diﬀerent time scales and noises.
Eq. (5) also gives us a hint on how to use convolution neural networks to implement Euclidean distances. By doing
this, the Euclidean distance of between the time series and
the shapelets can be eﬃciently computed leveraging deep
learning packages and the speedups from their GPU implementation.
RELATED WORK
TSC has been studied for long time. A plethora of time
series classiﬁcation algorithms have been proposed.
traditional TSC methods fall into two categories: distance
based methods that use kNN classiﬁers on top of distance
measures between time series, and feature based classiﬁers
that extract or search for deterministic features in the time
or frequency domain and then apply traditional classiﬁcation
algorithms. In recent years, some ensemble methods that
collect many TSC classiﬁers together have also been studied.
A full review of these methods is out of the scope here but we
will do a comprehensive empirical comparison with leading
TSC methods in the next section. Below, we review some
works that are most related to MCNN.
In recent years, there have been active research on deep
neural networks that can combine hierarchical feature extraction and classiﬁcation together. Extensive comparison has shown that convolution operations in CNN have
better capability on extracting meaningful features than adhoc feature selection . However, applications of CNN to
TSC have not been studied until recently.
A multi-channel CNN has been proposed to deal with multivariate time series . Features are extracted by putting
each time series into diﬀerent CNNs. After that, they concatenate those features together and put them into a new
CNN framework. Large multivariate datasets are needed in
order to train this deep architecture. While for our method,
we focus on univariate time series and introduce two more
branches that can extract multi-scale and multi-frequency
information and further increase the prediction accuracy. 
feeds CNN with variables post-processed using an input variable selection (IVS) algorithm. The key diﬀerence compared
with MCNN is that they aim at reducing the input size with
diﬀerent IVS algorithms. In contrast, we are exploring more
raw information for CNN to discover.
In addition to classiﬁcation, CNN is also used for time
series metric learning. In , Zheng et al. proposed a model
called convolutional nonlinear neighbourhood components
analysis that preforms CNN based metric learning and uses
1-NN as the classiﬁer in the embedding space.
Shapelets attract lots of attention because people can detect shapes that are crucial to TSC, providing insights and
interpretability. However, searching shapelets from all the
time series segmentations is time consuming and some stoping methods are proposed to accelerate this procedure. In
 , Grabocka et al. proposed a model that can learn global
shapelets automatically instead of searching. As discussed
in Section 3.2, MCNN is general enough to be able to learn
shapelets.
CNN can achieve scale invariance to some extent by using
the pooling operation. Thus, it is beneﬁcial to introduce a
multi-scale branch to extract short term as well as long term
features. In image recognition, CNNs keep feature maps in
each stage and feed those feature maps altogether to the ﬁnal
fully connected layer . By doing this, both short term
and higher level features are preserved. For our model, we
down sample the raw data into diﬀerent time scales which
provides low level features of diﬀerent scales and higher level
features at the same time.
EXPERIMENTAL RESULTS
In this section, we conduct extensive experiments on various benchmark datasets to evaluate MCNN and compare
it against many leading TSC methods. We have made an
eﬀort to include the most recent works.
Experimental setup
We ﬁrst describe the setup for our experiments.
Baseline methods. For comprehensive evaluation, we
evaluate two classical baseline methods:
1-NN with Euclidean distance (ED) and 1-NN DTW .
select 11 existing methods with state-of-the-art results published within the recent three years, including: DTW with
a warping window constraint set through cross validation
(DTW CV) , Fast Shapelet (FS) , SAX with vector
space model (SV) , Bag-of-SFA-Symbols (BOSS) ,
Shotgun Classiﬁer (SC) , time series based on a bag-offeatures (TSBF) , Elastic Ensemble (PROP) , 1-NN
Bag-Of-SFA-Symbols in Vector Space (BOSSVS) , Learn
Shapelets Model(LTS) , and the Shapelet Ensemble (SE)
model .
We also test standard convolutional neural network with
the same number of parameters as in MCNN to show the
beneﬁt of using the proposed multi-scale transformations
and local convolution.
For reference, we also list the results of ﬂat-COTE (COTE), an ensemble model proposed
by Bagnall et al. , which uses the weighted votes over 35
diﬀerent classiﬁers. MCNN is orthogonal to ﬂat-COTE and
can be incorporated as a constituent classiﬁer.
Datasets. We evaluate all methods thoroughly on the
UCR time series classiﬁcation archive , which consists of
46 datasets selected from various real-world domains. We
omit Car and Plane because a large portion of baseline methods do not provide related results. All the datasets in the
archive are publicly available1. Following the suggestions in
 , we z-normalize the following datasets during preprocessing: Beef, Coﬀee, Fish, OSULeaf and OliveOil.
All the experiments use the default training and testing
set splits provided by UCR, and the results are rounded
to three decimal places. For authoritative comparison, we
adopt the experimental results collected by Bagnall et al. 
and Schafer for the baseline methods.
Conﬁguring MCNN. For MCNN, we conduct the experiments on all the datasets with the same network architecture as in Figure 1. Since most of the datasets in the UCR
archive are not large enough, we ﬁrst use window slicing to
1 
CNN better here
Scatter plot of test accuracies of standard CNN against MCNN on all 44 UCR datasets.
MCNN is better in all but 3 datasets.
increasing the size of the training size. For window slicing,
we set the length of slices to be 0.9n where n is the original
length of the time series. We set the number of ﬁlters to be
256 for the convolutional layers and include 256 neurons in
the fully connected layer.
We use mini-batch stochastic gradient with momentum to
update parameters in MCNN. We adopt the grid search for
hyper-parameter tuning based on cross validation. The hyper parameters MCNN include the ﬁlter size, pooling factor,
and batch size.
In particular, the search space for the ﬁlter size is {0.05, 0.1, 0.2},
which denotes the ratio of the ﬁlter length to the original
time series length; the search space for the pooling factor
is {2, 3, 5}, which denotes the number of outputs of maxpooling. Early stopping is applied for preventing overﬁtting.
Speciﬁcally, we use the error on the validation set to determine the best model. When the validation error does not
get reduced for a number of epochs, the training terminates.
MCNN is implemented based on theano and run on
NVIDIA GTX TITAN graphics cards with 2688 cores and 6
GB global memory. For full replicability of the experiments,
we will release our code and make it available in public2.
CNN vs. MCNN. Before comparing against other TSC
classiﬁers, we ﬁrst compare MCNN with standard CNN. We
test a CNN that has the same architecture and number of
parameters as our MCNN but does not have the multi-scale
transformation and local convolutions. Figure 4 shows the
scatter plot of the test accuracies of CNN and MCNN on the
44 datasets. We can see that MCNN achieves better results
on 41 out of 44 datasets.
A binomial test conﬁrms that
MCNN is signiﬁcantly better than CNN at the 1% level.
Comprehensive evaluation
Table 1 shows a comprehensive evaluation of all methods
on the UCR datasets. For each dataset, we rank all the 15
classiﬁers. The last row of Table 1 shows the mean rank for
each solver (lower is better). We see that MCNN is very
competitive, achieving the highest accuracy on 10 datasets.
MCNN has a mean rank of 3.95, lower than all the state-ofthe-art methods except for COTE, which is an ensemble of
2Source codes of the programs developed by our lab are
published at 
Table 1: Testing error and rank for 44 ucr time series dataset.
DTW ED DTWCV
TSBF TSF BOSSVS PROP
0.396 0.389
0.514 0.417 0.22
0.373 0.245 0.261
0.437 0.435 0.233
0.367 0.467
0.447 0.467
0.133 0.287
0.167 0.133
0.003 0.148
0.053 0.007
0.009 0.039
0.006 0.003 0.001
ChlorineCon
0.352 0.35
0.417 0.334
0.312 0.336
0.314 0.203
CinCECGTorso
0.349 0.103
0.174 0.344 0.125 0.021 0.262 0.069
0.167 0.154 0.064
0.004 0.071
0.246 0.423
0.527 0.308 0.259
0.297 0.278 0.287
0.209 0.218 0.154
0.256 0.433
0.505 0.318 0.208
0.326 0.259
0.249 0.236 0.167 0.154
0.246 0.413
0.547 0.297 0.246
0.277 0.263 0.239
0.228 0.128
DiatomSizeR
0.033 0.065
0.117 0.121 0.046
0.069 0.126 0.101
0.033 0.124 0.082 0.023
ECGFiveDays
0.232 0.203
0.004 0.003
0.055 0.183
0.192 0.286
0.411 0.244
0.247 0.234 0.231
0.217 0.263 0.105
0.17 0.216
0.09 0.114
0.034 0.051 0.034
0.048 0.057 0.091
0.095 0.231
0.042 0.079
0.059 0.087 0.057
0.31 0.369
0.489 0.374 0.301
0.288 0.209 0.277
0.232 0.281 0.191
0.177 0.217
0.197 0.017 0.011
0.066 0.023 0.029
0.093 0.087
0.061 0.013
0.011 0.047
0.623 0.63
0.616 0.575 0.536
0.607 0.488 0.565
0.532 0.523 0.488
InlineSkate
0.616 0.658
0.734 0.593 0.511 0.653 0.603 0.675
0.573 0.615 0.551
ItalyPower
0.05 0.045
0.095 0.089 0.053
0.053 0.096 0.033
0.048 0.036
Lightning2
0.131 0.246
0.295 0.23
0.148 0.098
0.177 0.344 0.164
Lightning7
0.274 0.425
0.403 0.342 0.342
0.274 0.262 0.263
0.233 0.197
0.066 0.086
0.033 0.199 0.058
0.092 0.037 0.072
MedicalImages
0.263 0.316
0.433 0.516 0.288
0.305 0.269 0.232
0.396 0.258
MoteStrain
0.165 0.121
0.217 0.117 0.073
0.113 0.135 0.118
0.087 0.109 0.085
NonInvThorax1
0.21 0.171
0.174 0.138 0.103
0.093 0.064
NonInvThorax2
0.135 0.12
0.089 0.097 0.073
0.167 0.133
0.213 0.133
0.409 0.483
0.359 0.153 0.012 0.273 0.329 0.426
0.182 0.285 0.145
SonyAIBORobot
0.275 0.305
0.314 0.306 0.321
0.238 0.175 0.235
0.103 0.067 0.146
SonyAIBORobotII 0.169 0.141
0.215 0.126 0.098 0.066
0.196 0.177
0.082 0.115 0.076
StarLightCurves
0.093 0.151
0.06 0.108 0.021 0.093 0.022 0.036
0.033 0.024 0.031
SwedishLeaf
0.208 0.213
0.269 0.275 0.072
0.075 0.109
0.087 0.093 0.046
0.068 0.089 0.032
0.083 0.034 0.121
0.036 0.114 0.046
SyntheticControl
0.007 0.12
0.081 0.013
0.033 0.008 0.023
0.007 0.017
TwoLeadECG
0.113 0.004 0.004
0.029 0.001 0.112
0.003 0.004 0.015
TwoPatterns
0.096 0.253
0.09 0.011 0.016
0.048 0.046 0.053
0.003 0.059
0.272 0.261
0.293 0.324 0.241
0.248 0.164 0.213
0.216 0.196
0.366 0.338
0.392 0.364 0.313
0.322 0.249 0.288
0.287 0.303 0.267
0.342 0.35
0.364 0.357 0.312
0.346 0.217 0.267
0.268 0.273 0.265
0.02 0.005
0.004 0.002 0.001 0.002 0.004 0.047
0.004 0.002 0.001
WordSynonyms
0.351 0.382
0.563 0.436 0.345
0.357 0.302 0.381
0.403 0.266
0.164 0.17
0.249 0.151 0.081
0.159 0.149 0.157
0.195 0.113
10.05 12.32
12.88 10.25
Table 2: Pairwise comparison with MCNN. p(BT)
and p(WSR) are the p-values of binomial test and
Wilcoxon signed rank test, respectively.
#better #tie #worse
9.43 × 10−7 4.24 × 10−7
2.15 × 10−10 1.54 × 10−8
8.96 × 10−9 3.24 × 10−6
5.12 × 10−12 1.11 × 10−8
3.35 × 10−7 2.48 × 10−6
4.43 × 10−7 1.60 × 10−5
1.73 × 10−2 1.60 × 10−3
3.10 × 10−8 5.08 × 10−7
2.94 × 10−4 6.77 × 10−5
8.96 × 10−6 8.90 × 10−5
9.50 × 10−3 1.20 × 10−2
2.20 × 10−3 1.60 × 10−2
2.68 × 10−1 7.38 × 10−2
8.78 × 10−1 5.95 × 10−1
35 classiﬁers.
To further analyze the performance, we make pairwise
comparison for each algorithm against MCNN. Binomial test
(BT) and the Wilcoxon signed rank test (WSR) are used
to measure the signiﬁcance of diﬀerence. Corresponding pvalues are listed in table 2, indicating that MCNN is significantly better than all the other methods except for BOSS
and COTE at the 1% level (p < 0.01). Moreover, it shows
that the diﬀerences between COTE, BOSS, and MCNN are
not signiﬁcant.
Figure 5 shows the critical diﬀerence diagram, as proposed in .
The values shown on the ﬁgure are the average rank of each classiﬁer. Bold lines indicate groups of
classiﬁers which are not signiﬁcantly diﬀerent.
The critical diﬀerence (CD) length is shown on the graph. Figure
5 is evaluated on MCNN, all baseline methods and COTE.
MCNN is among the most accurate classiﬁers and its performance is very close to COTE. It is quite remarkable that
MCNN, a single algorithm, obtains the same state-of-the-art
performance as an ensemble model consisting of 35 diﬀerent
classiﬁers. Note that MCNN is orthogonal to ﬂat-COTE as
MCNN can also be included as a predictor in ﬂat-COTE to
further improve the performance. There are obvious margins between MCNN and other baseline classiﬁers.
We now group these classiﬁers into three categories and
provide mode detailed analysis.
Distance based classiﬁers. These classiﬁers use nearest neighbor algorithms based on distance measures between
time series. The simplest distance measure is the Euclidean
distance (ED). Dynamic time warping (DTW) is proposed to
extract the global similarity while addressing the phase shift
problem. DTW with 1-NN classiﬁer has been hard to beat
for a long time and now become a benchmark method. DTW
with warping set through cross-validation (DTWCV) is also
a traditional bench mark. k-NN classiﬁers also uses transformed features.
Fast shapelet (FS) search shapelet on a
lower transformed space. Bag-of-SFA-Symbols (BOSS) 
proposes a distance based on histograms of symbolic Fourier
approximation words.
The BOSSVS model combines the
BOSS model with the vector space model to reduce the time
complexity. They are all combine with 1-NN for ﬁnal prediction.
By grouping distance based classiﬁers together, we can
compare their average performance with MCNN. In order
to illustrate the overall performance of diﬀerent algorithms,
we plot accumulated rank on all the tested datasets in Figure
6. We order all the datasets alphabetically by their name
and show the accumulated rank. For example, if a method
is always ranked #1, its accumulated rank is N for the N th
dataset. From Figure 6, we see that MCNN has the lowest accumulated rank, outperforming all the distance based
classiﬁers.
Feature based classiﬁers. For feature based classiﬁers,
we selected SAX-VSM, TSF, TSBF, LTS. Symbolic aggregate approximation (SAX) has become a classical method
to discretize time series based on piecewise mean value.,
SAX-VSM achieved state-of-the-art classiﬁcation accuracy
on UCR dataset by combining vector space Model (VSM)
with SAX. Time series forest (TSF) divide time series into
diﬀerent intervals and calculate the mean, standard deviation and slop as interval features. Instead of using traditional entropy gain, TSF proposed a new split criteria by
adding an addition term measuring the nearest distance between interval features and split threashold to the entropy
and achieved better results than traditional random forests.
The bag-of-features framework (TSBF) also extracts interval features with diﬀerent scales.
The features from each
interval form an instance, and each time series forms a bag.
Random forest is used to build a supervised codebook and
classify time series. Finally, learning time series shapelets
(LTS) provides not only competitive results, but also the
ability to learn shapelets directly.
Classiﬁcation is made
based on logistic regression.
The middle plot of ﬁgure 6 compares the performance of
MCNN against some on feature based classiﬁers, including
SV, TSBF, TSF, and LTS. It is clearly that MCNN is substantially better than these feature based classiﬁers, as its
accumulated rank is consistently the lowest by a large margin.
Ensemble based classiﬁers There is a growing trend
in ensembling diﬀerent classiﬁers together to achieve higher
accuracy. The Elastic Ensemble (PROP) combined 11 distinct classiﬁers based on elastic distance measures through a
weighted ensemble scheme. This was the ﬁrst classiﬁer that
signiﬁcantly outperformed DTW at that time. Shapelet ensemble (SE) combines shapelet transformations with a heterogeneous ensemble method. The weight of each classiﬁer
is assigned based on the cross validation accuracy. The ﬂat
collective of transform-based ensembles (ﬂat-COTE) is an
ensemble of 35 diﬀerent classiﬁers based on features from
time and frequency domains and has achieved state-of-theart accuracy performance. Despite its high testing accuracy,
ensemble methods suﬀer high complexity during the training
process as well as testing. From the third plot in ﬁgure 6, we
can observe that MCNN is very close to COTE and much
better than SE and PROP. Critical diﬀerence analysis in
Figure 5 also conﬁrms that there is no signiﬁcant diﬀerence
between COTE and MCNN. It is in fact quite remarkable
that a single algorithm in MCNN can match the performance
of the COTE ensemble. The performance of MCNN is likely
to improve further if it is trained with larger datasets, since
convolutional neural networks are known to be able to absorb huge training data and make improvements.
Learning Shapelets (LTS)
Shapeless Ensemble (SE)
Shotgun Classifier (SC)
SAX-VSM (SV)
Fast Shapelet (FS)
Figure 5: Critical Diﬀerence Diagram over the mean ranks of MCNN, 13 baseline methods and the COTE
ensemble. The critical diﬀerence is 3.01. COTE is the best classiﬁer which ensembles 35 classiﬁers. MCNN
performs equally well compared with COTE.
Accumulated Rank
Distance Based Classifiers
Accumulated Rank
Feature Based Classifiers
Accumulated Rank
Ensemble Classifiers
Figure 6: Comparison of MCNN against three groups of classiﬁers in terms of accumulated ranks.
CONCLUSIONS
We have presented Multi-scale Convolutional Neural Network(MCNN), a convolutional neural network tailored for
time series classiﬁcation. MCNN uniﬁes feature extraction
and classiﬁcation, and jointly learns the parameters through
back propagation. It leverages the strength of CNN to automatically learn good feature representations in both time
and frequency domains. In particular, MCNN contains multiple branches that perform various transformations of the
time series, which extract features of diﬀerent frequency
and time scales, addressing the limitation of many previous
works that they only extract features at a single time scale.
We have also discussed the insights that learning convolution ﬁlters in MCNN generalizes shapelet learning, which in
part explains the excellent performance of MCNN.
We have conducted comprehensive experiments and compared with leading time series classiﬁcation models. We have
demonstrated that MCNN achieves state-of-the-art performance and outperforms many existing models by a large
margin, especially when enough training data is present.
More importantly, an advantage of CNNs is that they can
absorb massive amount of data to learn good feature representations. Currently, all the TSC datasets we have access to
are not very large, ranging from a training size of around 50
to a few thousands. We envision that MCNN will show even
greater advantages in the future when trained with much
larger datasets. We hope MCNN will inspire more research
on integrating deep learning with time series data analysis.
For future work, we will investigate how to augment MCNN
for time series classiﬁcation by incorporating other side information from multiple sources, such as text, image and
ACKNOWLEDGMENTS
The authors are supported in part by the IIS-1343896,
DBI-1356669, and III-1526012 grants from the National Science Foundation of the United States, a Microsoft Research
New Faculty Fellowship, and a Barnes-Jewish Hospital Foundation grant.