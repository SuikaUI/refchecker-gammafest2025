ETH Library
How to become a Bayesian in
eight easy steps: An annotated
reading list
Journal Article
Author(s):
Etz, Alexander; Gronau, Quentin F.; Dablander, Fabian; Edelsbrunner, Peter A.; Baribault, Beth
Publication date:
Permanent link:
 
Rights / license:
In Copyright - Non-Commercial Use Permitted
Originally published in:
Psychonomic Bulletin & Review 25(1), 
This page was generated automatically upon download from the ETH Zurich Research Collection.
For more information, please consult the Terms of use.
Reference as: Etz, A., Gronau, Q. F., Dablander, F., Edelsbrunner, P. A., Baribault, B. (in press).
How to become a Bayesian in eight easy steps: An annotated reading list. Psychonomic Bulletin &
How to become a Bayesian in eight easy steps: An annotated
reading list
Alexander Etz
University of California, Irvine
Quentin F. Gronau
University of Amsterdam
Fabian Dablander
University of Tübingen
Peter A. Edelsbrunner
ETH Zürich
Beth Baribault
University of California, Irvine
In this guide, we present a reading list to serve as a concise introduction to
Bayesian data analysis. The introduction is geared toward reviewers, editors,
and interested researchers who are new to Bayesian statistics. We provide
commentary for eight recommended sources, which together cover the theoretical and practical cornerstones of Bayesian statistics in psychology and
related sciences. The resources are presented in an incremental order, starting with theoretical foundations and moving on to applied issues. In addition, we outline an additional 32 articles and books that can be consulted to
gain background knowledge about various theoretical speciﬁcs and Bayesian
approaches to frequently used models.
Our goal is to oﬀer researchers a
starting point for understanding the core tenets of Bayesian analysis, while
requiring a low level of time commitment. After consulting our guide, the
reader should understand how and why Bayesian methods work, and feel
able to evaluate their use in the behavioral and social sciences.
Introduction
In recent decades, signiﬁcant advances in computational software and hardware have
allowed Bayesian statistics to rise to greater prominence in psychology (Van de Schoot,
Winder, Ryan, Zondervan-Zwijnenburg, & Depaoli, in press). In the past few years, this rise
has accelerated as a result of increasingly vocal criticism of p-values in particular , and classical statistics in general .
When a formerly scarcely used statistical method rapidly becomes more common, editors
Psychonomic Bulletin & Review
and peer reviewers are expected to master it readily, and to adequately evaluate and judge
manuscripts in which the method is applied. However, many researchers, reviewers, and
editors in psychology are still unfamiliar with Bayesian methods.
We believe that this is at least partly due to the perception that a high level of
diﬃculty is associated with proper use and interpretation of Bayesian statistics.
seminal texts in Bayesian statistics are dense, mathematically demanding, and assume
some background in mathematical statistics . Even texts that
are geared toward psychologists , while
less mathematically diﬃcult, require a radically diﬀerent way of thinking than the classical
statistical methods most researchers are familiar with.
Furthermore, transitioning to a
Bayesian framework requires a level of time commitment that is not feasible for many
researchers. More approachable sources that survey the core tenets and reasons for using
Bayesian methods exist, yet identifying these sources can prove diﬃcult for researchers with
little or no previous exposure to Bayesian statistics.
In this guide, we provide a small number of primary sources that editors, reviewers,
and other interested researchers can study to gain a basic understanding of Bayesian statistics. Each of these sources was selected for their balance of accessibility with coverage of
essential Bayesian topics. By focusing on interpretation, rather than implementation, the
guide is able to provide an introduction to core concepts, from Bayes’ theorem through to
Bayesian cognitive models, without getting mired in secondary details.
This guide is divided into two primary sections. The ﬁrst, Theoretical sources, includes
commentaries on three articles and one book chapter that explain the core tenets of Bayesian
methods as well as their philosophical justiﬁcation. The second, Applied sources, includes
commentaries on four articles that cover the most commonly used methods in Bayesian data
analysis at a primarily conceptual level. This section emphasizes issues of particular interest
to reviewers, such as basic standards for conducting and reporting Bayesian analyses.
We suggest that for each source, readers ﬁrst review our commentary, then consult
the original source. The commentaries not only summarize the essential ideas discussed in
each source, but also give a sense of how those ideas ﬁt into the bigger picture of Bayesian
statistics. This guide is part of a larger special issue in Psychonomic Bulletin & Review on
the topic of Bayesian inference that contains articles which elaborate on many of the same
points we discuss here, so we will periodically point to these as potential next steps for the
interested reader. For those who would like to delve further into the theory and practice of
Bayesian methods, the Appendix provides a number of supplemental sources that would be
of interest to researchers and reviewers. To facilitate readers’ selection of additional sources,
each source is brieﬂy described and has been given a rating by the authors that reﬂects its
level of diﬃculty and general focus (i.e., theoretical versus applied; see Figure A1). It is
important to note that our reading list covers sources published up to the time of this
writing .
Overall, the guide is designed such that a researcher might be able to read all eight
of the highlighted articles1 and some supplemental readings within a week. After readers
acquaint themselves with these sources, they should be well-equipped both to interpret
existing research and to evaluate new research that relies on Bayesian methods.
1Links to freely available versions of each article are provided in the References section.
Psychonomic Bulletin & Review
Theoretical sources
In this section, we discuss the primary ideas underlying Bayesian inference in increasing levels of depth. Our ﬁrst source introduces Bayes’ theorem and demonstrates how
Bayesian statistics are based on a diﬀerent conceptualization of probability than classical,
or frequentist, statistics . These ideas are extended in our second source’s
discussion of Bayesian inference as a reallocation of credibility between
possible states of nature. The third source demonstrates how the concepts established in
the previous sources lead to many practical beneﬁts for experimental psychology . The section concludes with an in-depth review of Bayesian hypothesis testing using
Bayes factors with an emphasis on this technique’s theoretical beneﬁts .
1. Conceptual introduction: What is Bayesian inference?
Source: Lindley — The analysis of experimental data: The appreciation of tea and
Lindley leads with a story in which renowned statistician Ronald A. Fisher is having
his colleague, Dr. Muriel Bristol, over for tea. When Fisher prepared the tea—as the story
goes—Dr. Bristol protested that Fisher had made the tea all wrong. She claims that tea
tastes better when milk is added ﬁrst and infusion second,2 rather than the other way
around; she furthermore professes her ability to tell the diﬀerence. Fisher subsequently
challenged Dr. Bristol to prove her ability to discern the two methods of preparation in a
perceptual discrimination study. In Lindley’s telling of the story, which takes some liberties
with the actual design of the experiment in order to emphasize a point, Dr. Bristol correctly
identiﬁed 5 out of 6 cups where the tea was added either ﬁrst or second. This result left
Fisher faced with the question: Was his colleague merely guessing, or could she really tell
the diﬀerence? Fisher then proceeded to develop his now classic approach in a sequence of
steps, recognizing at various points that tests that seem intuitively appealing actually lead
to absurdities, until he arrived at a method that consists of calculating the total probability
of the observed result plus the probability of any more extreme results possible under the
null hypothesis (i.e., the probability that she would correctly identify 5 or 6 cups by sheer
guessing). This probability is the p-value. If it is less than .05, then Fisher would declare
the result signiﬁcant and reject the null hypothesis of guessing.
Lindley’s paper essentially continues Fisher’s work, showing that Fisher’s classic procedure is inadequate and itself leads to absurdities because it hinges upon the nonexistent
ability to deﬁne what other unobserved results would count as “more extreme” than the
actual observations. That is, if Fisher had set out to serve Dr. Bristol 6 cups (and only
6 cups) and she is correct 5 times, then we get a p-value of .1, which is not statistically
signiﬁcant. According to Fisher, in this case we should not reject the null hypothesis that
Dr. Bristol is guessing. But had he set out to keep giving her additional cups until she was
correct 5 times, which incidentally required 6 cups, we get a p-value of .03, which is statistically signiﬁcant. According to Fisher, we should now reject the null hypothesis. Even
2As a historical note: Distinguishing milk-ﬁrst from infusion-ﬁrst tea preparation was not a particular
aﬀectation of Dr. Bristol’s, but a cultural debate that has persisted for over three centuries .
The left bar indicates the
probability that Dr. Bristol is guessing prior to the study (.8), if 5 right and 1 wrong are
observed (.59), and if 6 right and 0 wrong are observed (.23). The lines represents Lindley’s
corresponding beliefs about Dr. Bristol’s accuracy if she is not guessing.
though the data observed in both cases are exactly the same, we reach diﬀerent conclusions
because our deﬁnition of “more extreme” results (that did not occur) changes depending
on which sampling plan we use. Absurdly, the p-value, and with it our conclusion about
Dr. Bristol’s ability, depends on how we think about results that might have occurred but
never actually did, and that in turn depends on how we planned the experiment (rather
than only on how it turned out).
Lindley’s Bayesian solution to this problem considers only the probability of observations actually obtained, avoiding the problem of deﬁning more extreme, unobserved results.
The observations are used to assign a probability to each possible value of Dr. Bristol’s
success rate. Lindley’s Bayesian approach to evaluating Dr. Bristol’s ability to discriminate between the diﬀerently made teas starts by assigning a priori probabilities across the
range of values of her success rate. If it is reasonable to consider that Dr. Bristol is simply
guessing the outcome at random (i.e., her rate of success is .5), then one must assign an a
priori probability to this null hypothesis (see our Figure 1, and note the separate amount of
probability assigned to p = .5). The remaining probability is distributed among the range
of other plausible values of Dr. Bristol’s success rate (i.e., rates that do not assume that
Psychonomic Bulletin & Review
she is guessing at random)3. Then the observations are used to update these probabilities
using Bayes’ rule (this is derived in detail in Etz & Vandekerckhove, this issue). If the observations better ﬁt with the null hypothesis (pure guessing), then the probability assigned
to the null hypothesis will increase; if the data better ﬁt the alternative hypothesis, then
the probability assigned to the alternative hypothesis will increase, and subsequently the
probability attached to the null hypothesis will decrease (note the decreasing probability
of the null hypothesis on the left axis of Figure 2). The factor by which the data shift the
balance of the hypotheses’ probabilities is the Bayes factor .
A key takeaway from this paper is that Lindley’s Bayesian approach depends only on
the observed data, so the results are interpretable regardless of whether the sampling plan
was rigid or ﬂexible or even known at all. Another key point is that the Bayesian approach
is inherently comparative: Hypotheses are tested against one another and never in isolation.
Lindley further concludes that, since the posterior probability that the null is true will often
be higher than the p-value, the latter metric will discount null hypotheses more easily in
2. Bayesian credibility assessments
Source: Kruschke — Introduction: Credibility, models, and parameters
“How often have I said to you that when all other θ yield P(x|θ) of 0, whatever
remains, however low its P(θ), must have P(θ|x) = 1?”
– Sherlock Holmes, paraphrased
In this book chapter, Kruschke explains the fundamental Bayesian principle of reallocation
of probability, or “credibility,” across possible states of nature. Kruschke uses an example featuring Sherlock Holmes to demonstrate that the famous detective essentially used
Bayesian reasoning to solve his cases. Suppose that Holmes has determined that there exist only four diﬀerent possible causes (A, B, C, and D) of a committed crime which, for
simplicity in the example, he holds to be equally credible at the outset. This translates to
equal prior probabilities for each of the four possible causes (i.e., a prior probability of 1/4
for each). Now suppose that Holmes gathers evidence that allows him to rule out cause
A with certainty. This development causes the probability assigned to A to drop to zero,
and the probability that used to be assigned to cause A to be then redistributed across the
other possible causes. Since the probabilities for the four alternatives need to sum to one,
the probability for each of the other causes is now equal to 1/3 (Figure 2.1, p. 17). What
Holmes has done is reallocate credibility across the diﬀerent possible causes based on the
evidence he has gathered. His new state of knowledge is that only one of the three remaining
alternatives can be the cause of the crime and that they are all equally plausible. Holmes,
3If the null hypothesis is not initially considered tenable, then we can proceed without assigning separate
probability to it and instead focus on estimating the parameters of interest (e.g., the taster’s accuracy in
distinguishing wines, as in Lindley’s second example; see Lindley’s Figure 1, and notice that the amount
of probability assigned to p = .5 is gone). Additionally, if a range of values of the parameter is considered
impossible—such as rates that are below chance—then this range may be given zero prior probability.
Psychonomic Bulletin & Review
being a man of great intellect, is eventually able to completely rule out two of the remaining
three causes, leaving him with only one possible explanation—which has to be the cause of
the crime (as it now must have probability equal to 1), no matter how improbable it might
have seemed at the beginning of his investigation.
The reader might object that it is rather unrealistic to assume that data can be
gathered that allow a researcher to completely rule out contending hypotheses.
applications, psychological data are noisy, and outcomes are only probabilistically linked
to the underlying causes. In terms of reallocation of credibility, this means that possible
hypotheses can rarely be ruled out completely (i.e., reduced to zero probability), however,
their credibility can be greatly diminished, leading to a substantial increase in the credibility
of other possible hypotheses. Although a hypothesis has not been eliminated, something
has been learned: Namely, that one or more of the candidate hypotheses has had their
probabilities reduced and are now less likely than the others.
In a statistical context, the possible hypotheses are parameter values in mathematical
models that serve to describe the observed data in a useful way. For example, a scientist
could assume that their observations are normally distributed and be interested in which
range of values for the mean is most credible. Sherlock Holmes only considered a set of
discrete possibilities, but in many cases it would be very restrictive to only allow a few alternatives (e.g., when estimating the mean of a normal distribution). In the Bayesian framework one can easily consider an inﬁnite continuum of possibilities, across which credibility
may still be reallocated. It is easy to extend this framework of reallocation of credibility to
hypothesis testing situations where one parameter value is seen as “special” and receives a
high amount of prior probability compared to the alternatives serves as a good ﬁrst introduction to Bayesian thinking, as it requires
only basic statistical knowledge (a natural follow-up is Kruschke & Liddell, this issue). In
this chapter, Kruschke also provides a concise introduction to mathematical models and
parameters, two core concepts which our other sources will build on. One ﬁnal key takeaway
from this chapter is the idea of sequential updating from prior to posterior (Figure 2.1, p. 17)
as data are collected. As Dennis Lindley famously said: “Today’s posterior is tomorrow’s
prior” .
3. Implications of Bayesian statistics for experimental psychology
Source: Dienes — Bayesian versus orthodox statistics: Which side are you on?
Dienes explains several diﬀerences between the frequentist (which Dienes calls orthodox and we have called classical; we use these terms interchangeably) and Bayesian
paradigm which have practical implications for how experimental psychologists conduct experiments, analyze data, and interpret results (a natural follow-up to the discussion in this
section is available in Dienes & McLatchie, this issue). Throughout the paper, Dienes also
discusses subjective (or context-dependent) Bayesian methods which allow for inclusion of
relevant problem-speciﬁc knowledge in to the formation of one’s statistical model.
The probabilities of data given theory and of theory given data.
testing a theory, both the frequentist and Bayesian approaches use probability theory as
the basis for inference, yet in each framework, the interpretation of probability is diﬀerent.
Psychonomic Bulletin & Review
It is important to be aware of the implications of this diﬀerence in order to correctly interpret frequentist and Bayesian analyses. One major contrast is a result of the fact that
frequentist statistics only allow for statements to be made about P(data | theory)4: Assuming the theory is correct, the probability of observing the obtained (or more extreme) data is
evaluated. Dienes argues that often the probability of the data assuming a theory is correct
is not the probability the researcher is interested in. What researchers typically want to
know is P(theory | data): Given that the data were those obtained, what is the probability
that the theory is correct? At ﬁrst glance, these two probabilities might appear similar, but
Dienes illustrates their fundamental diﬀerence with the following example: The probability
that a person is dead (i.e., data) given that a shark has bitten the person’s head oﬀ(i.e.,
theory) is 1. However, given that a person is dead, the probability that a shark has bitten
this person’s head oﬀis very close to zero . It is important to keep in mind that a p-value does not correspond to
P(theory | data); in fact, statements about this probability are only possible if one is willing
to attach prior probabilities (degrees of plausibility or credibility) to theories—which can
only be done in the Bayesian paradigm.
In the following sections, Dienes explains how the Bayesian approach is more liberating than the frequentist approach with regard to the following concepts: stopping rules,
planned versus post hoc comparisons, and multiple testing. For those new to the Bayesian
paradigm, these proposals may seem counterintuitive at ﬁrst, but Dienes provides clear and
accessible explanations for each.
Stopping rules.
In the classical statistical paradigm, it is necessary to specify
in advance how the data will be collected.
In practice, one usually has to specify how
many participants will be collected; stopping data collection early or continuing after the
pre-speciﬁed number of participants has been reached is not permitted. One reason why
collecting additional participants is not permitted in the typical frequentist paradigm is
that, given the null hypothesis is true, the p-value is not driven in a particular direction
as more observations are gathered. In fact, in many cases the distribution of the p-value
is uniform when the null hypothesis is true, meaning that every p-value is equally likely
under the null. This implies that even if there is no eﬀect, a researcher is guaranteed to
obtain a statistically signiﬁcant result if they simply continue to collect participants and
stop when the p-value is suﬃciently low. In contrast, the Bayes factor, the most common
Bayesian method of hypothesis testing, will approach inﬁnite support in favor of the null
hypothesis as more observations are collected if the null hypothesis is true. Furthermore,
since Bayesian inference obeys the likelihood principle, one is allowed to continue or stop
collecting participants at any time while maintaining the validity of one’s results of data given (|) theory.
Psychonomic Bulletin & Review
theory will be the same no matter its temporal relation to the data.
Multiple testing.
When conducting multiple tests in the classical approach, it is
important to correct for the number of tests performed . Dienes
points out that within the Bayesian approach, the number of hypotheses tested does not
matter—it is not the number of tests that is important, but the evaluation of how accurately
each hypothesis predicts the observed data. Nevertheless, it is crucial to consider all relevant
evidence, including so-called “outliers,” because “cherry picking is wrong on all statistical
approaches” .
Context-dependent Bayes factors.
The last part of the article addresses how
problem-speciﬁc knowledge may be incorporated in the calculation of the Bayes factor. As
is also explained in our next highlighted source , there are two main
schools of Bayesian thought: default (or objective) Bayes and context-dependent (or subjective) Bayes. In contrast to the default Bayes factors for general application that are
designed to have certain desirable mathematical properties , Dienes provides an online calculator5 that enables one
to obtain context-dependent Bayes factors that incorporate domain knowledge for several
commonly used statistical tests. In contrast to the default Bayes factors, which are typically designed to use standardized eﬀect sizes, the context-dependent Bayes factors specify
prior distributions in terms of the raw eﬀect size. Readers who are especially interested
in prior elicitation should see the appendix of Dienes’ article for a short review of how to
appropriately specify prior distributions that incorporate relevant theoretical information
 .
4. Structure and motivation of Bayes factors
Source: Rouder et al. — Bayesian t-tests for accepting and rejecting the null hypothesis
In many cases, a scientist’s primary interest is in showing evidence for an invariance,
rather than a diﬀerence. For example, researchers may want to conclude that experimental
and control groups do not diﬀer in performance on a task , that participants were performing at chance
 , or that two variables are unrelated .
In classical statistics this is generally not possible as signiﬁcance tests are asymmetric;
they can only serve to reject the null hypothesis and never to aﬃrm it. One beneﬁt of
Bayesian analysis is that inference is perfectly symmetric, meaning evidence can be obtained
that favors the null hypothesis as well as the alternative hypothesis . This is made possible by the use of Bayes
factors.6 The section covering the shortcomings of classical statistics (“Critiques of Inference
by Signiﬁcance Tests”) can safely be skipped, but readers particularly interested in the
motivation of Bayesian inference are advised to read it.
5 
6Readers for whom Rouder and colleagues’ treatment is too technical could focus on Dienes’
conceptual ideas and motivations underlying the Bayes factor.
Psychonomic Bulletin & Review
What is a Bayes factor?.
The Bayes factor is a representation of the relative predictive success of two or more models, and it is a fundamental measure of relative evidence.
The way Bayesians quantify predictive success of a model is to calculate the probability of
the data given that model—also called the marginal likelihood or sometimes simply the evidence. The ratio of two such probabilities is the Bayes factor. Rouder and colleagues 
denote the probability of the data given some model, represented by Hi, as f(data | Hi).7
The Bayes factor for H0 versus H1 is simply the ratio of f(data | H0) and f(data | H1)
written B01 (or BF01), where the B (or BF) indicates a Bayes factor, and the subscript
indicates which two models are being compared (see p. 228). If the result of a study is
B01 = 10 then the data are ten times more probable under H0 than under H1. Researchers
should report the exact value of the Bayes factor since it is a continuous measure of evidence, but various benchmarks have been suggested to help researchers interpret Bayes
factors, with values between 1 and 3, between 3 and 10, and greater than 10 generally
taken to indicate inconclusive, weak, and strong evidence, respectively , although diﬀerent researchers may set
diﬀerent benchmarks. Care is need when interpreting Bayes factors against these benchmarks, as they are not meant to be bright lines against which we judge a study’s success
(as opposed to how a statistical signiﬁcance criterion is sometimes treated); the diﬀerence
between a Bayes factor of, say, 8 and 12 is more a diﬀerence of degree than of category.
Furthermore, Bayes factors near 1 indicate the data are uninformative, and should not be
interpreted as even mild evidence for either of the hypotheses under consideration.
Readers who are less comfortable with reading mathematical notation may skip over
most of the equations without too much loss of clarity. The takeaway is that to evaluate
which model is better supported by the data, we need to ﬁnd out which model has done
the best job predicting the data we observe. To a Bayesian, the probability a model assigns
to the observed data constitutes its predictive success ; a model that assigns a high probability to the data relative to another model is
best supported by the data. The goal is then to ﬁnd the probability a given model assigns
the data, f(data | Hi). Usually the null hypothesis speciﬁes that the true parameter is a
particular value of interest (e.g., zero), so we can easily ﬁnd f(data | H0). However, we
generally do not know the value of the parameter if the null model is false, so we do not
know what probability it assigns the data. To represent our uncertainty with regard to
the true value of the parameter if the null hypothesis is false, Bayesians specify a range
of plausible values that the parameter might take under the alternative hypothesis. All
of these parameter values are subsequently used in computing an average probability of
the data given the alternative hypothesis, f(data | H1) . If the prior distribution gives
substantial weight to parameter values that assign high probability to the data, then the
average probability the alternative hypothesis assigns to the data will be relatively high—the
model is eﬀectively rewarded for its accurate predictions with a high value for f(data | H1).
The role of priors.
The form of the prior can have important consequences on
the resulting Bayes factor. As discussed in our third source , there are two
7The probability (f) of the observed data given (|) hypothesis i (Hi), where i indicates one of the
candidate hypotheses (e.g., 0, 1, A, etc.). The null hypothesis is usually denoted H0 and the alternative
hypothesis is usually denoted either H1 or HA.
Psychonomic Bulletin & Review
primary schools of Bayesian thought: default (objective) Bayes and contextdependent (subjective) Bayes . The default Bayesian tries to specify prior distributions that convey little information
while maintaining certain desirable properties. For example, one desirable property is that
changing the scale of measurement should not change the way the information is represented
in the prior, which is accomplished by using standardized eﬀect sizes. Context-dependent
prior distributions are often used because they more accurately encode our prior information
about the eﬀects under study, and can be represented with raw or standardized eﬀect sizes,
but they do not necessarily have the same desirable mathematical properties (although
sometimes they can).
Choosing a prior distribution for the standardized eﬀect size is relatively straightforward for the default Bayesian. One possibility is to use a normal distribution centered at 0
and with some standard deviation (i.e., spread) σ. If σ is too large, the Bayes factor will
always favor the null model, so such a choice would be unwise . This happens because such a prior distribution assigns weight to very extreme values of the eﬀect size, when in reality, the eﬀect is most often reasonably small (e.g.,
almost all psychological eﬀects are smaller than Cohen’s d = 2). The model is penalized
for low predictive success. Setting σ to 1 is reasonable and common—this is called the
unit information prior. However, using a Cauchy distribution (which resembles a normal
distribution but with less central mass and fatter tails) has some better properties than the
unit information prior, and is now a common default prior on the alternative hypothesis,
giving rise to what is now called the default Bayes factor . To use the Cauchy distribution, like the normal distribution, again one
must specify a scaling factor. If it is too large, the same problem as before occurs where
the null model will always be favored. Rouder and colleagues suggest a scale of 1, which
implies that the eﬀect size has a prior probability of 50% to be between d = −1 and d = 1.
For some areas, such as social psychology, this is not reasonable, and the scale should be
reduced. However, slight changes to the scale often do not make much diﬀerence in the
qualitative conclusions one draws.
Readers are advised to pay close attention to the sections “Subjectivity in priors”
and “Bayes factors with small eﬀects.” The former explains how one can tune the scale
of the default prior distribution to reﬂect more contextually relevant information while
maintaining the desirable properties attached to prior distributions of this form, a practice
that is a reasonable compromise between the default and context-dependent schools. The
latter shows why the Bayes factor will often show evidence in favor of the null hypothesis
if the observed eﬀect is small and the prior distribution is relatively diﬀuse.
Applied sources
At this point, the essential concepts of Bayesian probability, Bayes’ theorem, and the
Bayes factor have been discussed in depth. In the following four sources, these concepts
are applied to real data analysis situations. Our ﬁrst source provides a broad overview of
the most common methods of model comparison, including the Bayes factor, with a heavy
emphasis on its proper interpretation .
The next source begins by demonstrating Bayesian estimation techniques in the context
Psychonomic Bulletin & Review
of developmental research, then provides some guidelines for reporting Bayesian analyses
 . Our ﬁnal two sources discuss issues in Bayesian cognitive
modeling, such as the selection of appropriate priors (Lee & Vanpaemel, this issue), and
the use of cognitive models for theory testing .
Before moving on to our ﬁnal four highlighted sources, it will be useful if readers
consider some diﬀerences in perspective among practitioners of Bayesian statistics. The
application of Bayesian methods is very much an active ﬁeld of study, and as such, the
literature contains a multitude of deep, important, and diverse viewpoints on how data
analysis should be done, similar to the philosophical divides between Neyman–Pearson and
Fisher concerning proper application of classical statistics . The divide
between subjective Bayesians, who elect to use priors informed by theory, and objective
Bayesians, who instead prefer “uninformative” or default priors, has already been mentioned
throughout the Theoretical sources section above.
A second division of note exists between Bayesians who see a place for hypothesis
testing in science, and those who see statistical inference primarily as a problem of estimation. The former believe statistical models can stand as useful surrogates for theoretical
positions, whose relative merits are subsequently compared using Bayes factors and other
such “scoring” metrics . The latter would rather delve deeply into a single model or analysis and
use point estimates and credible intervals of parameters as the basis for their theoretical
conclusions .8
Novice Bayesians may feel surprised that such wide divisions exist, as statistics (of
any persuasion) is often thought of as a set of prescriptive, immutable procedures that can
be only right or wrong. We contend that debates such as these should be expected due
to the wide variety of research questions—and diversity of contexts—to which Bayesian
methods are applied. As such, we believe that the existence of these divisions speaks to the
intellectual vibrancy of the ﬁeld and its practitioners. We point out these diﬀerences here
so that readers might use this context to guide their continued reading.
5. Bayesian model comparison methods
Source: Vandekerckhove et al. — Model comparison and the principle of parsimony
John von Neumann famously said: “With four parameters I can ﬁt an elephant,
and with ﬁve I can make him wiggle his trunk” (as quoted in Mayer, Khairy, & Howard,
698), pointing to the natural tension between model parsimony and goodness
of ﬁt. The tension occurs because it is always possible to decrease the amount of error
between a model’s predictions and the observed data by simply adding more parameters
to the model.
In the extreme case, any data set of N observations can be reproduced
8This divide in Bayesian statistics may be seen as a parallel to the recent discussions about use of classical
statistics in psychology , where a greater push has been made to adopt an estimation
approach over null hypothesis signiﬁcance testing (NHST). Discussions on the merits of hypothesis testing
have been running through all of statistics for over a century, with no end in sight.
Psychonomic Bulletin & Review
perfectly by a model with N parameters.
Such practices, however, termed overﬁtting,
result in poor generalization and greatly reduce the accuracy of out-of-sample predictions.
Vandekerckhove and colleagues take this issue as a starting point to discuss various
criteria for model selection. How do we select a model that both ﬁts the data well and
generalizes adequately to new data?
Putting the problem in perspective, the authors discuss research on recognition memory that relies on multinomial processing trees, which are simple, but powerful, cognitive
Comparing these diﬀerent models using only the likelihood term is ill-advised,
because the model with the highest number of parameters will—all other things being
equal—yield the best ﬁt. As a ﬁrst step to addressing this problem, Vandekerckhove et al.
 discuss the popular Akaike information criterion (AIC) and Bayesian information
criterion (BIC).
Though derived from diﬀerent philosophies , both AIC and BIC try to solve the trade-oﬀbetween goodness-of-ﬁt and
parsimony by combining the likelihood with a penalty for model complexity. However, this
penalty is solely a function of the number of parameters and thus neglects the functional
form of the model, which can be informative in its own right. As an example, the authors
mention Fechner’s law and Steven’s law. The former is described by a simple logarithmic
function, which can only ever ﬁt negatively accelerated data.
Steven’s law, however, is
described by an exponential function, which can account for both positively and negatively
accelerated data. Additionally, both models feature just a single parameter, nullifying the
beneﬁt of the complexity penalty in each of the two aforementioned information criteria.
The Bayes factor yields a way out. It extends the simple likelihood ratio test by
integrating the likelihood with respect to the prior distribution, thus taking the predictive
success of the prior distribution into account . Essentially, the Bayes factor is a likelihood ratio test averaged over all possible
parameter values for the model, using the prior distributions as weights: It is the natural
extension of the likelihood ratio test to a Bayesian framework. The net eﬀect of this is to
penalize complex models. While a complex model can predict a wider range of possible
data points than a simple model can, each individual data point is less likely to be observed
under the complex model. This is reﬂected in the prior distribution being more spread
out in the complex model. By weighting the likelihood by the corresponding tiny prior
probabilities, the Bayes factor in favor of the complex model decreases. In this way, the
Bayes factor instantiates an automatic Ockham’s Razor .
However, the Bayes factor can be diﬃcult to compute because it often involves integration over very many dimensions at once. Vandekerckhove and colleagues advocate
two methods to ease the computational burden: importance sampling and the Savage-Dickey
density ratio ; additional common computational methods include the Laplace
approximation , bridge sampling , and the encompassing prior approach . They
also provide code to estimate parameters in multinomial processing tree models and to compute the Bayes factor to select among them. Overall, the chapter provides a good overview
of diﬀerent methods used to tackle the tension between goodness-of-ﬁt and parsimony in
Psychonomic Bulletin & Review
a Bayesian framework. While it is more technical then the sources reviewed above, this
article can greatly inﬂuence how one thinks about models and methods for selecting among
6. Bayesian estimation
Source: van de Schoot et al. — A gentle introduction to Bayesian analysis: Applications to developmental research
This source approaches practical issues related to parameter estimation in the context
of developmental research. This setting oﬀers a good basis for discussing the choice of priors
and how those choices inﬂuence the posterior estimates for parameters of interest. This is a
topic that matters to reviewers and editors alike: How does the choice of prior distributions
for focal parameters inﬂuence the statistical results and theoretical conclusions that are
obtained? The article discusses this issue on a basic and illustrative level.
At this point we feel it is important to note that the diﬀerence between hypothesis
testing and estimation in the Bayesian framework is much greater than it is in the frequentist
framework. In the frequentist framework there is often a one-to-one relationship between the
null hypothesis falling outside the sample estimate’s 95% conﬁdence interval and rejection
of the null hypothesis with a signiﬁcance test (e.g., when doing a t-test). This is not so
in the Bayesian framework; one cannot test a null hypothesis by simply checking if the
null value is inside or outside a credible interval. A detailed explanation of the reason for
this deserves more space than we can aﬀord to give it here, but in short: When testing
hypotheses in the Bayesian framework one should calculate a model comparison metric.
See Rouder and Vandekerckhove (this issue) for an intuitive introduction to (and synthesis
of) the distinction between Bayesian estimation and testing.
Van de Schoot and colleagues begin by reviewing the main diﬀerences between
frequentist and Bayesian approaches. Most of this part can be skipped by readers who are
comfortable with basic terminology at that point. The only newly introduced term is Markov
chain Monte Carlo (MCMC) methods, which refers to the practice of drawing samples from
the posterior distribution instead of deriving the distribution analytically (which may not
be feasible for many models; see also van Ravenzwaaij, Cassey, & Brown, this issue and
Matzke, Boehm, & Vandekerckhove, this issue). After explaining this alternative approach
(p. 848), Bayesian estimation of focal parameters and the speciﬁcation of prior distributions
is discussed with the aid of two case examples.
The ﬁrst example concerns estimation of an ordinary mean value and the variance
of reading scores and serves to illustrate how diﬀerent sources of information can be used
to inform the speciﬁcation of prior distributions. The authors discuss how expert domain
knowledge (e.g., reading scores usually fall within a certain range), statistical considerations (reading scores are normally distributed), and evidence from previous studies (results
obtained from samples from similar populations) may be jointly used to deﬁne adequate
priors for the mean and variance model parameters. The authors perform a prior sensitivity
analysis to show how using priors based on diﬀerent considerations inﬂuence the obtained
results. Thus, the authors examine and discuss how the posterior distributions of the mean
and variance parameters are dependent on the prior distributions used.
The second example focuses on a data set from research on the longitudinal reciprocal
Psychonomic Bulletin & Review
associations between personality and relationships.
The authors summarize a series of
previous studies and discuss how results from these studies may or may not inform prior
speciﬁcations for the latest obtained data set. Ultimately, strong theoretical considerations
are needed to decide whether data sets that were gathered using slightly diﬀerent age groups
can be used to inform inferences about one another.
The authors ﬁt a model with data across two time points and use it to discuss how
convergence of the MCMC estimator can be supported and checked. They then evaluate
overall model ﬁt via a posterior predictive check. In this type of model check, data simulated
from the speciﬁed model are compared to the observed data.
If the model is making
appropriate predictions, the simulated data and the observed data should appear similar.
The article concludes with a brief outline of guidelines for reporting Bayesian analyses and
results in a manuscript. Here, the authors emphasize the importance of the speciﬁcation
of prior distributions and of convergence checks (if MCMC sampling is used) and brieﬂy
outline how both might be reported. Finally, the authors discuss the use of default priors
and various options for conducting Bayesian analyses with common software packages (such
as Mplus and WinBUGS).
The examples in the article illustrate diﬀerent considerations that should be taken into
account for choosing prior speciﬁcations, the consequences they can have on the obtained
results, and how to check whether and how the choice of priors inﬂuenced the resulting
inferences.
7. Prior elicitation
Source: Lee and Vanpaemel (this issue) — Determining priors for cognitive models
Statistics does not operate in a vacuum, and often prior knowledge is available that
can inform one’s inferences. In contrast to classical statistics, Bayesian statistics allows one
to formalize and use this prior knowledge for analysis. The paper by Lee and Vanpaemel
(this issue) ﬁlls an important gap in the literature: What possibilities are there to formalize
and uncover prior knowledge?
The authors start by noting a fundamental point: Cognitive modeling is an extension
of general purpose statistical modeling (e.g., linear regression). Cognitive models are designed to instantiate theory, and thus may need to use richer information and assumptions
than general purpose models . A consequence of this is that the prior
distribution, just like the likelihood, should be seen as an integral part of the model. As
Jaynes put it: “If one fails to specify the prior information, a problem of inference
is just as ill-posed as if one had failed to specify the data” (p. 373).
What information can we use to specify a prior distribution? Because the parameters
in such a cognitive model usually have a direct psychological interpretation, theory may be
used to constrain parameter values. For example, a parameter interpreted as a probability
of correctly recalling a word must be between 0 and 1.
To make this point clear, the
authors discuss three cognitive models and show how the parameters instantiate relevant
information about psychological processes. Lee and Vanpaemel also discuss cases in which
all of the theoretical content is carried by the prior, while the likelihood does not make any
strong assumptions. They also discuss the principle of transformation invariance, that is,
prior distributions for parameters should be invariant to the scale they are measured on
Psychonomic Bulletin & Review
(e.g., measuring reaction time using seconds versus milliseconds).
Lee and Vanpaemel also discuss speciﬁc methods of prior speciﬁcation. These include
the maximum entropy principle, the prior predictive distribution, and hierarchical modeling.
The prior predictive distribution is the model-implied distribution of the data, weighted with
respect to the prior. Recently, iterated learning methods have been employed to uncover
an implicit prior held by a group of participants. These methods can also be used to elicit
information that is subsequently formalized as a prior distribution. 
In sum, the paper gives an excellent overview of why and how one can specify prior
distributions for cognitive models. Importantly, priors allow us to integrate domain-speciﬁc
knowledge, and thus build stronger theories .
information on specifying prior distributions for data-analytic statistical models rather than
cognitive models see Rouder, Morey, Verhagen, Swagman, and Wagenmakers (in press) and
Rouder, Engelhardt, McCabe, and Morey .
8. Bayesian cognitive modeling
Source: Lee — Three case studies in the Bayesian analysis of cognitive models
Our ﬁnal source further discusses cognitive modeling, a more tailored
approach within Bayesian methods. Often in psychology, a researcher will not only expect
to observe a particular eﬀect, but will also propose a verbal theory of the cognitive process
underlying the expected eﬀect. Cognitive models are used to formalize and test such verbal
theories in a precise, quantitative way. For instance, in a cognitive model, psychological
constructs, such as attention and bias, are expressed as model parameters. The proposed
psychological process is expressed as dependencies among parameters and observed data
(the “structure” of the model).
In peer-reviewed work, Bayesian cognitive models are often presented in visual form
as a graphical model. Model parameters are designated by nodes, where the shape, shading, and style of border of each node reﬂect various parameter characteristics. Dependencies
among parameters are depicted as arrows connecting the nodes. Lee gives an exceptionally
clear and concise description of how to read graphical models in his discussion of multidimensional scaling .
After a model is constructed, the observed data are used to update the priors and
generate a set of posterior distributions. Because cognitive models are typically complex,
posterior distributions are almost always obtained through sampling methods (i.e., MCMC;
see van Ravenzwaaij et al., this issue), rather than through direct, often intractable, analytic
calculations.
Lee demonstrates the construction and use of cognitive models through three case
studies. Speciﬁcally, he shows how three popular process models may be implemented in
a Bayesian framework. In each case, he begins by explaining the theoretical basis of each
model, then demonstrates how the verbal theory may be translated into a full set of prior
distributions and likelihoods. Finally, Lee discusses how results from each model may be
interpreted and used for inference.
Each case example showcases a unique advantage of implementing cognitive models
in a Bayesian framework (see also Bartlema, Voorspoels, Rutten, Tuerlinckx, & Vanpaemel,
Psychonomic Bulletin & Review
this issue). For example, in his discussion of signal detection theory, Lee highlights how
Bayesian methods are able to account for individual diﬀerences easily . Throughout, Lee emphasizes that Bayesian
cognitive models are useful because they allow the researcher to reach new theoretical
conclusions that would be diﬃcult to obtain with non-Bayesian methods.
Overall, this
source not only provides an approachable introduction to Bayesian cognitive models, but
also provides an excellent example of good reporting practices for research that employs
Bayesian cognitive models.
Conclusion
By focusing on interpretation, rather than implementation, we have sought to provide
a more accessible introduction to the core concepts and principles of Bayesian analysis than
may be found in introductions with a more applied focus. Ideally, readers who have read
through all eight of our highlighted sources, and perhaps some of the supplementary reading,
may now feel comfortable with the fundamental ideas in Bayesian data analysis, from basic
principles to prior distribution selection (Lee & Vanpaemel,
this issue), and with the interpretation of a variety of analyses, including Bayesian analogs
of classical statistical tests , estimation in a Bayesian
framework , Bayes factors and other methods for hypothesis
testing , and Bayesian cognitive models (Lee,
Reviewers and editors unfamiliar with Bayesian methods may initially feel hesitant to
evaluate empirical articles in which such methods are applied (Wagenmakers, Love, et al.,
this issue). Ideally, the present article should help ameliorate this apprehension by oﬀering
an accessible introduction to Bayesian methods that is focused on interpretation rather than
application. Thus, we hope to help minimize the amount of reviewer reticence caused by
authors’ choice of statistical framework.
Our overview was not aimed at comparing the advantages and disadvantages of
Bayesian and classical methods.
However, some conceptual conveniences and analytic
strategies that are only possible or valid in the Bayesian framework will have become evident. For example, Bayesian methods allow for the easy implementation of hierarchical
models for complex data structures , they allow multiple comparisons and ﬂexible sampling rules during data collection without correction of inferential statistics , and they allow
inferences that many researchers in psychology are interested in but are not able to answer
with classical statistics such as providing support for a null hypothesis . Thus, the inclusion of more research that uses Bayesian methods in
the psychological literature should be to the beneﬁt of the entire ﬁeld . In this article, we have provided an overview of sources that should allow a
novice to understand how Bayesian statistics allows for these beneﬁts, even without prior
knowledge of Bayesian methods.
Psychonomic Bulletin & Review
Acknowledgments
The authors would like to thank JeﬀRouder, E.-J. Wagenmakers, and Joachim Vandekerckhove for their helpful comments. AE and BB were supported by grant #1534472
from NSF’s Methods, Measurements, and Statistics panel. AE was further supported by the
National Science Foundation Graduate Research Fellowship Program (#DGE1321846).