IDAV Publications
Identification of Outliers in Multivariate Data
 
Journal of the American Statistical Association, 91
Rocke, David
Woodruff, David
Publication Date
Peer reviewed
eScholarship.org
Powered by the California Digital Library
University of California
David M. ROCKE and David L. WOODRUFF
New insights are given into why the problem of detecting multivariate outliers can be difficult and why the difficulty increases
with the dimension of the data. Significant improvements in methods for detecting outliers are described, and extensive simulation
experiments demonstrate that a hybrid method extends the practical boundaries of outlier detection capabilities. Based on simulation
results and examples from the literature, the question of what levels of contamination can be detected by this algorithm as a function
of dimension, computation time, sample size, contamination fraction, and distance of the contamination from the main body of
data is investigated. Software to implement the methods is available from the authors and STATLIB.
KEY WORDS: Heuristic search; M estimation; Minimum covariance detenninant; S estimation.
1. INTRODUCTION
Although methods of detecting sporadic outliers in multivariate data have existed for many years , the problem of detecting clusters of outliers can be
extremely difficult. This essentially requires robust estimation of multivariate location and shape, and most estimators are known to fail when the fraction of contamination
is greater than (1/(P + 1), where p is the dimension of the
data. Thus detecting outliers or a disparate population that
compose more than a small fraction of the data has been
impractical in high dimension.
In this article we give new insights into why the problem
of detecting multivariate outliers is so difficult and why the
difficulty increases with the dimension of the data. We then
describe significant improvements in methods for detecting
outliers and demonstrate, using extensive experiments, that
a hybrid method extends the practical boundaries of outlier
~~~~~~ti~n
of the e~_~~
is complicated by the fact the probability of detecting outliers depends on many factors, such as the computer time
expended, dimension, number of data points, fraction of
data contaminated, type of contamination, and algorithm
parameters. Nonetheless, we are able to specify approximately what levels of contamination can be detected by
this algorithm under a variety of conditions.
The estimation of multivariate location and shape is
one of the most difficult problems in robust statistics
 . For some statistical procedures, it is relatively straightforward to obtain estimates that are resistant to a reasonable fraction of outliers-for example, onetn(Ax + b) = Atn(X) + b.
A shape estimator Cn E PDS(p), the set of p x p positive
definite symmetric (PDS) matrices, is affine equivariant if
and only if for any vector b E IRP and any nonsingular p x p
This implies, for example, that stretching or rotating measurement scales will change the estimates appropriately.
Dropping the requirement of affine equivariance does increase the number of estimators that are available, and certainly there may be cases where a non-affine-equivariant
estimator provides superior performance, but it is also important to have robust, computable, affine-equivariant estimators available for use.
Methods have been reported in the literature for a number of approaches for finding robust estimates of multivariate location and shape (and thus for identifying outliers). Combinatorial estimators, such as the minimum
volume ellipsoid (MVE) and minimum covariance determinant (MCD) estimators of Rousseeuw , have been
addressed with random search , steepest descent with random restarts , and heuristic search optimization efforts
 . Smooth estimators such
as maximum likelihood and M estimators , E. I. du Pont de Nemours
and Company, and the National Institute of Environmental Health Sciences, National Institutes of Health (P42 ES04699). The authors are grateful for helpful comments by the referees that markedly improved both the
article and the underlying algorithm.
@ 1996 American Statistical Association
Journal of the American Statistical Association
September 1996, Vol. 91, No. 435, Theory and Methods
dimensional location and regression
with error-free predictors . The multivariate
location and shape problem is more difficult, because most
known methods will break down if the fraction of outliers is
larger than 1/ (p + 1), where p is the dimension of the data
 . This means
that in high dimension, a very small fraction of outliers can
result in very bad estimates.
We are particularly interested in obtaining estimates that
are affine equivariant. A location estimator tn E JRP is affine
equivariant if and only if for any vector b E JRP and any
nonsingular p x p matrix A,
Journal of the American Statistical Association, September 1996
cation that the bad data are necessarily errors-they may
just arise from a distinct subpopulation-but the locution
is convenient.
A second aspect of our viewpoint on this problem is that
we aspire to methods that are affine equivariant, so that
measurement scale changes or other linear transformations
do not alter the behavior of analysis methods. An implication of this viewpoint is that Mahalanobis distances become
very important, because these are among the few potentially
affine-invariant outlier identification criteria.
Definition 1.
Let n be a positive definite symmetric
p x p matrix. The Mahalanobis distance between points x
and y in JRP with respect to n is defined by
= (x -y)Tn-1 and
S estimators can be computed with a
straightforward iteration from a good starting point or using an ad hoc search for the global
minimum . Sequential point addition estimators (FORWARD) have been defined algorithmically by Atkinson and Hadi working separately. Hadi suggested using a non-affine-equivariant
starting point, but the point addition portion of the algorithm is affine equivariant and is nearly the same as the point
addition portion of Atkinson's completely affine-equivariant
algorithm. Maronna and Yohai reported some computational results for the Stahel-Donoho projection estimator ; however, the method appears suitable only for small data sets in low dimension
(their largest case is n = 30,p = 6). We have omitted any
further analysis of this estimator, due to the current lack of
a computational method suitable for higher dimension.
In the remainder of the article, we discuss that nature of
multivariate outliers, with a special view to what sorts of
outliers are worth studying. We show that outliers with the
same shape as the main data are in some sense the hardest
to find, and that the more compact the outliers, the harder
they are to find. We adopt shift outliers as a reasonable
target, being of the hardest shape but of a feasible size to
locate. We also study more briefly outliers that are more
compact as well as shifted, and also pure radial outliers.
We then analyze the comparative performance of the
new hybrid algorithm and previous methods. Our algorithm,
which uses search techniques from both FSA and FORWARDiA;tkiifsOif1Q93~::l~A~iuwC
and Mulira 1993), as well as from our own previous work
 , proves as a package to be superior to
other methods suggested for multivariate outlier identification. Finally, we investigate the question of what problems
can be practically tackled with our methods.
We refer to the distance and the matrix that defines it interchangeably as a metric.
For data like those we consider here, the true metric is the
covariance matrix of the population from which the good
data arise, and a good metric is one close to the true metric. In particular, when the covariance of the whole sample
differs greatly from the covariance of the good data, a good
metric is one that resembles the latter rather than the former. The term all-data metric refers to the metric induced
by the covariance matrix of the entire sample; this may be
"good" or "bad," depending on the amount and type of contamination.
We find it convenient to distinguish the size and shape of
a metric as follows.
Definition 2.
Let n be a matriX defining a metric. The
size of the metric is the determinant Inl. The shape of
the metric is the equivalence class of metrics E such that
= E/IEll/p. Equivalently, we may identify the
shape as the member of the equivalence class with determinant 1; that is, n/lnll/p.
This leads to similar definition of shape and size for samples.
THE NATURE OF MULTIVARIATE OUTLIERS
In this section we develop theory that leads to a characterization of classes of data with outliers that are, in a
well-defined sense, the hardest to find. Armed with this,
we are in a position to conduct experiments that support
claims about the worst-case performance of algorithms. To
create this characterization, we investigate the difficulties
of locating multivariate outliers.
First, to frame the problem as this article deals with it,
we assume that there is a fraction greater than one-half of
the data from a well-behaved multivariate population; for
example, multivariate normal. Of course, in practical cases,
data transformations may be required before this plausibly
holds. In addition to the well-behaved data, other data do
not fit the pattern of this well-behaved majority; these may
arise from a distinct population or may be measurement errors. We sometimes call the majority of the data that come
from that well-behaved population the good data, and the
remainder the bad data. There is supposed to be no impli-
Definition 3.
Let X be an n x p matrix representing a
sample of n points in JRP. Let 8 = n-l(X
be the sample covariance matrix. The size or scale of X is
the determinant 181 of its covariance matrix, and the shape
of X is 8/1811/p. By extension, we refer to the size and
shape of other covariance-like estimators, such as the robust
ones that are the subject of this article.
We now consider the question of what classes of outliers
are hard to find. We begin by examining the case in which
a good metric is available. This is the goal of most affineequivariant outlier identification methods-find a good metric so that the outliers will reveal themselves. The following
lemma is a routine application of multivariate computations.
Consider a sample of n points in JRP. Let the
"good" data have mean 1L0 and covariance ~o. Let the "bad"
data have mean 1L0 + IL and covariance matrix 0, and let
this comprise a fraction c of the overall data. Then the ex-
Rod<e and Woodruff: Outliers in Multivariate Data
2. If x is a bad point, then
pected sample mean and covariance matrix are
E(x) = #.to + e#.t
E(S) = (1 -e)Eo + en + e(l -e)1L1L
1 -c + >..c n-+oo c
1 -c + >..c
Theorem 1. Consider a sample of n points in JRP. Let
the "good" data have mean /.Lo and covariance Eo. Let the
"bad" data have mean /.Lo + /.L and covariance matrix o.
Consider the Mahalanobis square distance 40 (x, /.Lo) of a
point from the true mean using the true metric. Then for a
fixed location displacement /.L and size 101 of the outliers,
the expectation of the Mahalanobis square distance of a bad
point from the true mean is least when the shape of 0 is
the same as the shape of Eo. This is thus the worst case
from a detection viewpoint.
Theorem 1 suggests that the hardest kind of outliers to
find, when a good metric is available, is the kind that has
a covariance matrix with the same shape as the good data.
For this situation, this reduces the infinitely variable kinds
of outliers to a single kind. If this kind of outlier can then
be detected, then other kinds should be as well. Thus we
intend to focus on a situation in which there are good data
drawn from a multivariate normal distribution and bad data
drawn from the same distrib1,ltion and then displaced. These
are often called shift outliers .
Shift outliers may be contrasted with classes of outliers
that may be easy to detect, in the sense of appearing disparate even with"theJnetrlc obtained by ~
all th~~",
For easilyaetecTea'outliers.less e1ab(:irMe:-Iechniques:'"'«'
sufficient-examining the Mahalanobis distances from the
mean of the data using the covariance matrix of the data
will suffice. Although we have seen that the shape for bad
data that maximizes their masking is the shape of the good
data, we have not yet addressed the issue of size. The following theorem shows how easy detection is a consequence
of the number and size of the contamination.
Theorem 2. Consider a sample of n points in JR". Let
the "good" data be multivariate normal with mean /.Lo and
covariance Eo- Let the "bad" data be multivariate normal
with mean /.Lo + /.L, where I/.LI = 1J, and covariance matrix
0 = >'Eo, and let this comprise a fraction e of the overall
data. Let E be the expected covariance matrix of the mixed
sample as in Lemma 1 and consider 4(x,/.Lo + ell) the
Mahalanobis square distance in the all-data metric between
a data point x and the overall population mean. Then,
1. If x is a good point, then
Ao = (1 -e)(ep -_r!-=!!:))
Although pure shift outlIers IDlght seem to
be detectable, given that their mean Mahalanobis distance
from the sample mean is larger than that of the good points,
no method is known that can find the outliers with complete
assurance. This is because the overlap in the distributions of
distances can be very substantial if the amount of contamination is large. Although shift outliers are realistic, and are
sufficiently challenging to separate the performance of different methods, we also examined some intermediate cases
in which 0 < >. < 1, particularly what we call crossover
outliers, in which>' is chosen in accordance with Equation
(4). These proved even more difficult (as predicted) for all
methods examined.
E(d~(x, lLo + elL»
1 + c:2772
1 -c: + AC: + c:(l -C:)1]2
-~ + A~ n-oo ~+
3. The difference in the value of E(d},(x, ILo + elL)) for
a bad point and the value for a good point for large 1/ is an
increasing function of >., so that>. = 0 is the worst case.
4. If >. = 0, so that the outliers fonn a point mass, and
if 1/ is large, then the value of E(d~(x, ILo + elL)) for a
bad point is less than the value for a good point whenever
e> 1/(P+ 1).
5. If >. = 1 (pure shift outliers), and if 1/ is large, then
the value of E(d~(X,ILo + elL)) for a bad point is always
larger than the value for a good point. However, for large
p, the standardized distribution of the distance of a good
point and the standardized distribution of the distance of a
bad point converge.
6. For large 1/, the value of >. at which E(d},(x, ILo
+ elL)) has the same value for good points and bad points
e((1-e)p-e)
whenever this is positive.
If a good starting estimate for the shape of
the- ~~..-~;J?~-,~l~',"~
~h~_~o~oon:
taIn1natlon
to illscover
IS iliat-w1i1ch
same shape
as the good data. Because substantial contamination can be
found only by constructing a relatively good shape estimate,
this is the most difficult case for such search methods.
Although point-mass contamination is the
most difficult to detect by the Mahalanobis distance from
the sample mean, it is easy to detect in other ways, such
as pair-wise distances. Our hybrid algorithm has a preestimation phase that involves eliminating any exact duplicates. This will avoid problems with accidentally replicated
data, for example, and will prevent exact point-mass outliers from being troublesome.
Journal of the American Statistical Association, September 1996
As we see later, pure shift outliers are sufficient to baffle
some previously proposed methods like the random search
algorithm in the program MINVOL .
Others like those proposed by Atkinson turn out to
be better than random search. However, the hybrid method
proposed here dominates all other methods examined. Of
course, it should be kept in mind that the algorithm of
Hawkins has been incorporated into our hybrid algorithm and that of Atkinson is also used at one stage
in the computations. We also examined some other types of
outliers, including multiple clusters and outliers that have
been reduced in scale as well as shifted.
Because we are interested mainly in high dimension,
we rely primarily on extensive computational experiments to compare methods, rather than the standard, lowdimensional examples often used in the literature. However,
we did examine the performance of the method on some of
these standard examples, the results of which are reported in
Section 5. For the reasons outlined in this section, the experiments involve mainly shift outliers, although we examined
other cases to check for any sensitivity to this specification.
Dimensions as large as 40 were examined so that highdimensional cases would be represented, even though the
computation times can rise rapidly with the dimension. Previously, the literature has concentrated almost exclusively
on dimensions less than 10, and usually no larger than 5.
Methods that appear satisfactory for a problem with 3 dimensions and 20 data points can be completely impractical
for even somewhat larger problems . We examine a range of contamination fractions from
1/(P + 1), which is the smallest nontrivial amount of contamination to 40% or 45% which canc.beccalmo~i_i-
"'~ ..c'"',c'
"'~r".,:,ble to find. Therefiatheoretical"
limit Oft the humber of
contaminated points that can be found, even in principle;
the number of good points must be at least h = L(n + p
+ 1)/2J .
AFFINE-EQUIVARIANT METHODS FOR
OUTLIER DETECTION
All known methods for this problem consist of the following two phases:
.Phase I: Estimate a location and shape.
.Phase II: Scale the shape estimate so that it can be
used to suggest which points are far enough from the
location estimate to be considered possible outliers.
We now discuss these two phases and the steps within them
in reverse order.
The output from Phase I of a multivariate outlier identification procedure is a location and shape, and thus a set
of distances of points from the location using Definition 1.
From this it is clear which points are the most distant, but
not whether any of the distances is too large to be consistent
with the absence of outliers.
A first step in answering the latter question is to apply
some sort of consistency adjustment; for example, multiplying all the distances by the ratio of the median distance
and the square root of the 50% point of a x~ distribution
Because any affine-equivariant location and shape
estimation method gives an unbiased location estimator and
a shape estimator that has expectation a multiple of the
true shape for elliptically symmetric distributions , the square distances are asymptotically
some multiple of X~ for normal data. Thus standardization
is sufficient to ensure that the distances are asymptotically
x~. Alternatively, one can scale the shape matrix so that
it is consistent for the covariance matrix of a multivariate normal distribution, which has the same effect. These
equivalent forms of standardization were used by, for example, Maronna and Yohai and Rousseeuw and van
Zomeren . We prefer to standardize the distances by
scaling the hth order statistic of the distances to the h/n
quantile of~,
where h = l determined the 97.5% point of the distances by simulation, and Atkinson standardized by the total of
the square distances, which is well known to be p( n -1)
for the mean and covariance estimator. We follow a similar procedure; by simulation, we determine the empirical
point of the distances using the given estimation
method on multivariate normal samples. This gives a cutoff
that only~a !ractioo ~16fpoints~~~
erage-willnave-mstances above the cutoff point if the given
procedure is used on multivariate normal samples.
Although a standardization such as the foregoing can ensure that the type I error is maintained in multivariate normal samples, it can lead to insensitivity when large numbers of outliers exist. Suppose, for example, that a sample is
given in dimension 20 in which 600 points are multivariate
normal and 400 are outliers. Suppose that the estimation
method is capable of finding a shape matrix that is essentially that of the good data. If the median square distance is
scaled to the median of a X~o' then the 500th distance will
be set to ~
= 4.397. However, the 500th distance is
actually at the 5/6 point of the good data, so the distance
should be Jx~~
= 5.096, and thus all the distances of
the good points are too small by a factor of .86. Now a
cutoff for distances of ~~
= 6.732 would result in
an average of only I false outlier per 1,000 with normal
samples, but in the present case, the probability of a good
point exceeding it is roughly the chance that a X~o exceeds
[(6.732)(5.096)/4.397]2 = 60.87, which is about 5 x 10-6.
Thus many points that are unequivocally outliers would
not be declared discordant because of this bias induced by
the presence of large numbers of outliers. The appropriate cutoff for this sample is (6.732)(4.397)/5.096 = 5.801
(not 6.732), and any point with a distance in the interval
Rocke and Woodruff: Outliers in Multivariate Data
[5.801,6.732] probably should be declared an outlier, but
will not be so declared under the rule.
To alleviate this problem, we add a second step to Phase
II. We take the points not declared outliers in the first step
of Phase II and calculate the mean and covariance matrix
of those points. If the data were really free of outliers, then
this should comprise a fraction 1 -Ql of the sample; but if
outliers are present, then the fraction may be much smaller.
The covariance of the nearest 1 -Ql
fraction of a multivariate normal sample has as expectation a multiple of the
true covariance matrix, where the multiple is
k(p, (rl) = Fv2
so if we inflate the covariance matrix thus obtained, then
we asymptotically unbias the calculation, while retaining
sensitivity to outliers. Finally, we reject points beyond the
X;;l-Q~ point using this new shape estimator. The entire
Phase II process can be summarized as follows:
Phase u: Scaling and outlier determination.
some of which are embedded in the algorithm. We refer
to the complete method as the hybrid algorithm because
it uses both combinatorial and smooth features, as well as
incorporating several other useful heuristics.
Phase I: Hybrid robust estimation with roughly T seconds allowed (large T)
.Step O. On entry, we have data consisting of n points
in dimension p, a total available CPU time T in seconds, and a function ,(p) that detennines the size of
partition to be used for any dimension p.
.Step 1. Remove any exact duplicate points.
.Step 2. Randomize the order of the data points.
.Step 3. Partition the data into Ln/,(p)J cells indexed
.Step 4. For each cell:
a. Spend T/Ln/,(p)J
seconds on a search for the
MCD .
b. Use the MCD as a starting point for a sequential point addition algorithm , using the entire sample of size n starting
from the p + 1 points that have the smallest distance from the MCD location using the MCD
c. Use this result as the starting point for translated
biweight M estimation , using the entire sample of size n. This yields estimates fLj and
Ej of location and shape.
.Step 5. Select the index j for which IEj I is least, and
-set f.I. ~f!-j and ~ = Ei'~:~~~~~~~
Step O. On entry, we have data consisting of n points
in dimension p and Phase I estimates it and E, with
the shape matrix standardized so that the hth ordered
distance is equal to ~,
where h = l(n + p +
Step 1. Determine by simulation a cutoff point Lal
so that when multivariate normal samples of size n
in dimension p are submitted to the Phase I process, a fraction £Xl of the points on the average lie
~:;!J;:~Jffit~~~i~ lnatiii-;~5ig1is:
where S is the covariance matrix of all points whose
distance in the first step is less than Lal. The new
location estimator is the mean of those points.
Step 3. Reject as outliers any point whose distance
using the revised location and shape is larger than
M and S Estimation
An S estimate of multivariate location and shape is defined as that vector t and PDS matrix C that minimizes I C I
subject to
n-l LP([(Xi
-t)TCwhich we write as
(Xi -t)P/2)
We have generally used al = a2 = .01, but smaller numbers may be used at the cost of more simulation time if one
wishes to reject fewer good points.
n-l L p(di) = boo
It has been shown by Lopuhaa that S estimators are
in the class of M estimators with standardizing constraints
with weight functions VI (d) = W(d),V2(d) = pw(d), and
V3(d) = v(d), where 1/J(d) = p'(d),w(d) = 1/J(d)jd, and
v(d) = 1/J(d)d, with constraint (7) showed that S estimators in high dimension
can be sensitive to outliers even if the breakdown point is
set to be near 50%. We use the translated biweight (or tbiweight) M estimation method defined by Rocke ,
with a standardization step consisting of equating the median of p(di) with the median under normality. This is then
not an S estimate, but is instead a constrained M estimate.
The convergence criterion for the algorithm is subject
to choice. We use the maximum change in the weights to
decide on termination. The specifics of the iteration for
The established methods for this problem fall into two
classes: combinatorial and smooth. Combinatorial estimators construct estimates of location and shape from a subset of the data that itself is hoped to be at least mostly
outlier-free. Smooth estimators attempt to satisfy a continuous equation by iteration from a starting point. Unless iteration from the whole sample mean and covariance sufficesan easy case-this requires either a direct search or use of
a prior combinatorial estimator as a starting point.
Our proposed method is outlined as Phase I. Step 2 is
included so that the resulting algorithm is sure to be permutation invariant in expectation. As a practical matter, this
step is not important. The rest of this section is devoted to
describing the other steps in more detail in reverse order.
We also compare the method to methods in the literature,
Journal of the American Statistical Association, September 1996
Percentage of Successful Estimation Runs With
Biweight S Estimation and t-Biweight M Estimation
The last two columns are the fraction
of runs out 01 20 that the indicated method found
the "good" root of the estimating equations.
The experiments were in dimension
10 with outliers
at a distance
both M and S estimators was given by Rocke and Woodruff
In accord with the theory of Rocke , we have found
that using the t-biweight M estimator greatly improves the
performance of the hybrid algorithm compared to using biweight S estimation, at least when the outliers lie relatively
close in (d = 2, as defined in Sec. 4). When d = 4, the
smooth estimation method used made no important difference. Some detailed evidence is given in Table 1. The situation here is that twenty replicates of shift outliers at d = 2
in dimension 10 and with indicated sample size, fraction
of outliers, and computation time allowed. The response is
the percentage of replicates for which the indicated estimator achieved the good root. Note that the t-biweight performance exceeds that of the biweight S estimate by large
amounts in every case. A large number of additional experiments confirm this important difference in performance.
We use the t-bi~~i~!:M~!!!!!!:t~f!:-~~~~c~~!;
good starting point presents severe computational difficulties. Regardless of which algorithms are used to compute
them, combinatorial estimators such as the MCD search a
space that increases exponentially with the sample size and
the dimension. In fact, when using the MCD as a first stage
in a two-stage estimator, one can have the perverse situation of being made worse off by having more data. To cope
with this problem, the data must be partitioned so that the
search space for the MCD is kept in a reasonable range.
After some modest experimentation, we settled on a cell
size of l' = 5p. This possibly may be too small for high
dimension, but determining the optimal value was beyond
the scope of this article.
As shown by Woodruff and Rocke , using data partitioning in this fashion allows for acquisition of the good
root with high probability with a computational time increasing only linearly with n (instead of exponentially). We
use data partitioning in Step 3 of Phase I.
3.5 Sequential Point Addition
Working separately, Atkinson and Hadi 
have proposed algorithms that begin with an estimate of
shape and location based on (p + 1) points and then select
successively larger sets. The set with k + 1 points consists
of those points whose Mahalanobis distances from the mean
of the k set using the covariance of the k set as a metric
are smallest.
Hadi suggested using coordinatewise medians as a preliminary location estimator and the covariance of the whole
data with that as center for a preliminary shape estimator.
The initial set of p + 1 points consists of those whose Mais least, using the preliminary shape estimator-~-a'i11etric.
The algorithm proposed by Hadi breaks down if the contamination is extremely far away from the good data in the
correct metric. Also, the coordinatewise median is not affine
equivariant and consequently can work extremely well on
a suite of data sets, but then perform horribly on the same
data after an affine transformation. For example, with outliers clustered on a diagonal and not very far from the good
data, our experiments suggested near-perfect outlier detection. If the same data are transformed to have a covariance
that is the identity matrix, then the performance is degraded
significantly.
Atkinson's method is affine equivariant. He suggested
restarting the procedure many times with randomly selected
sets of p + 1 points. For each trial, sequential addition is
performed and for each stage in the sequential addition,
ihe covariance matrix is calculated, and the resulting shape
matrix is expanded (or contracted), so that half (or (n + p
+ 1)/2) of the points are included in the ellipsoid defined
by the current location and shape. The estimate over all
trials and over all stages of each trial in which the scaled
shape matrix has minimum determinant may be taken as
the robust estimate of the shape and location of the data.
As we see later, Atkinson's algorithm is a large improvement over Min Vol. In our tables and graphs, we refer to this
procedure, following Atkinson, as the forward algorithm, or
Forward for short.
Search and Partitioning
The simple iteration scheme for M estimation fails without a good starting point. An M estimator that begins iteration using an estimate based on all of the data breaks
down with 1/ (p + 1) of the data contaminated . Two methods of addressing this problem seem possible. One is to look directly for the global minimizer of the
S criterion. The other is to find a good starting point for the
iteration by using a preliminary combinatorial estimator.
Ruppert proposed an algorithm called Surreal for
direct search for the global minimizer of an S estimator
used in multiple regression. He reported computational experiments that demonstrated the effectiveness of the Surreal
for this purpose. In the same paper, he also proposed an extension of the method to robust estimation of multivariate
location and shape. It appears that Surreal is not as effective
for this problem as for regression. In dimension 10, Surreal
rarely found the good root when the fraction of contamination was greater than about 12%. Because this was not
competitive with other algorithms examined, detailed results are not presented.
We also have examined direct search as a method of finding the good root for S or M estimation and have found that
it seems better to use a preliminary combinatorial estimator such as the MCD . As pointed out
by Woodruff and Rocke , using the MCD to find a
We found that including a sequential addition step between the search for the MCD and the smooth estimator
improved the results in many cases. We ran more than 200
simulated data sets in dimension 20 with n values of 200,
400, and 800 and various fractions of "bad" data from .2-
.4. In these experiments, inclusion of Step 4b in the Phase
I algorithm resulted in an improved estimate in over 70%
of the data sets. In many cases the improvement was very
modest and did not affect Phase II results. Nonetheless, inclusion of the step seems well worth the small amount of
computer time required to execute it (small relative to the
time required for the MCD search). We use sequential point
addition in Step 4b of Phase I.
been thought possible. Our primary model is shift outliers,
in which the good data are defined to be multivariate standard normal and the bad data to be multivariate unit normal
with a shifted mean. We also take a more abbreviated look
at outliers in which the covariance matrix is multiplied by
AO of Equation (4) so that the expected distance is equalized
between a good point and a bad point in the metric of all
the data (crossover outliers).
We measure the amount of shift in terms of the unit of
measurement Qp = ~~,
which is more or less the radius of a sphere around the mean that contains almost all of
the good points. If the outliers are centered at a distance of
2Qp, then these spheres should not overlap. We implement
outliers at a distance of dQp by adding dQ; to each component, where Q; = ~~~.
This places the outliers at
the correct distance out on a diagonal. In the experiments
here with A = 1 and A = AO, we use d = 2, which we call
close outliers, and d = 4, which we call far outliers. To obtain radial outliers, we generate each outlier separately as a
multivariate normal with mean m, where m is in a different
random direction for each outlier and Iml = dQp.
This generation mechanism is sufficient for affineequivariant methods; but for non-affine-equivariant methods, the data should then be standardized so that the entire
sample has mean 0 and covariance I. This can be done
using the singular value decomposition as follows. Let S
be the covariance matrix of the whole sample of good and
bad data. This can be written as S = QTDQ, where Q
is an orthogonal matrix and D is the diagonal matrix of
eigenvalues. If X is the centered sample, then the sample~~;
--One---convertient-aspect -of~-using shift outliers (or
crossover outliers) in this problem is that in our experience, smooth methods such as M and S estimation usually
have at most two roots: one that can be found by iterating from the good data (the good root) and one that occurs
when iterating from all the data (the bad root). For small
amounts of contamination or very large amounts of contamination, these roots may not be distinct, but only when
they differ is the problem interesting. This leads naturally
to a fairly strict criterion of success for a Phase I algorithm.
If the method yields a location it and a metric E, then the
method is successful if the largest value of di:(x, it) for
a good point is smaller than the smallest value for a bad
point. F9r the overall performance of both phases, we use
the less-strict type I and type II error measurement.
3.6 Minimum Covariance Determinant
Faced with a subsample of contaminated data, our experiments indicate that the best way to find a good starting
point for sequential point addition (or for M iteration) is
to search for the MCD. It was originally thought that the
MVE would be preferable for computational reasons , even though the MCD
has greater asymptotic efficiency. This was based on the
notion that MVE algorithms would make use of elemental subsets. Woodruff and Rocke demonstrated that
heuristic search algorithms that use larger subsample sizes
perform better. Given this fact, there is no longer any reason to prefer the MVE to the MCD. Simulations done by
Woodruff and Rocke strongly support the contention
that the MCD is in fact the better estimator to use.
The MCD for any set of data is defined by the half sample whose covariance matrix has minimum determinant.
It is convenient tg s~~blQ~.M~GcQ,,_~~1!!~~o~~~
from half sample--t{}-half~ampl~bY-Temoving one point
in the current half sample and adding one point not currently in the half sample. NeighbOrhoods defined in this
way can form the basis of a steepest descent to a local
minimum. Hawkins suggested using steepest descent with random restarts, which he called FSA. Woodruff
and Rocke advocated using a steepest descent-based
meta-heuristic called tabu search (TS) .
Our experiments indicate that FSA can outperform the
simple TS algorithm given by Woodruff and Rocke, especially when not much time is allocated for the search.
A much more complicated ghost image processing algorithm performs better than FSA given
large amounts of time and data, but it does not perform
better with small amounts of time and, furthermore, in our
tests, the improved performance does not seem to be sufficient to make a major difference in Phase II performance
for the search durations of interest to us. Given the lack
of a qualitative difference in the performance envelope and
the relative elegance of pure steepest descent with random
restarts, we used the FSA algorithm in Step 3a of Phase I
in the tests described here.
Null Behavior
Table 2 gives some simulation results to support the good
behavior of the proposed two-phase method when the data
are multivariate normal. Each line of the table is based on 20
instances in which the entire algorithm, Phase I and Phase
ll, was applied to multivariate normal data. The third column is the fraction out of the total of 20np points that were
rejected as outliers. It is easily seen that these numbers are
all quite near the nominal rejection fraction of .01.
COMPUTATIONAL EXPERIMENTS
The results given in Section 2 allow the construction of a
less arbitrary set of simulations than might otherwise have
Journal of the American Statistical Association, September
Actual Type 1 E"ors When the Nominal Type 1 E"o, is .01
Each line of the table represents 20 replicates
the algorijhm
fo multivariate
normal data. The third column is the fraction of the 20np points that were iabeled as
outliers by the aigorijhm
search over elemental subsets at all levels of contamination. The hybrid algorithm in turn is noticeably more effective than Forward. Similar results obtain for other sample
sizes, times, distances, and dimensions. In higher dimensions, limited trials suggest that the superiority of the hybrid algorithm is even greater. However, given the finite
time available for computer simulations in high dimension,
most of the runs were devoted to determining the envelope
of feasible solution for the hybrid algorithm, rather than to
documenting the exact degree of superiority over competing algorithms.
Another feature of the plots is worth noting. A small
addition to the fraction of outliers converts a problem that
is easy to solve into one that is quite difficult. For example,
~c=~~~,"OJ
O.3;:~;~:O.4~c
of Outliers
Fraction of Outliers
A Comparison of Algorithms
In this section we compare results using three different
strategies for Phase I: the hybrid algorithm, random search
over elemental subsets , and the
forward algorithm . In all cases
Phase n is as given in Section 3.1. The steepest descent
algorithm (FSA) is not shown separately,
because it has been incorporated into the hybrid algorithm.
Surreal was also tried, but its performance was not competitive with the others, and so it has been omitted from the
summaries.
Given that some runs in high dimension may take up to
an hour of CPU time, and that there are many conditions
under which one should compare estimators, a fully comprehensive Monte Carlo study is impractical. The database
used in the comparison study comprises more than 10,000
runs. The dimension pW8Sc~i,2{}"""~~fwiths~ple
sizes of n = 50 to n=6,~th--largersample
sizes used
in higher dimensions. Several processing times t were tried
for each case, varying from a few seconds to several hours
in high-dimensional examples. The degree of contamination
c was varied from levels where the solution could almost
always be found by most methods to levels where none of
the methods could get them right.
To increase the utility of the number of runs that were
practical to perform, a generalized linear model was fit to
the outcomes of the experiments, each of which consisted
of 20 trials at each case. The logit of the probability that
a given estimator would succeed in identifying the outliers
was taken to be a linear function of n, c, and log(tjn). Different models were fit for each estimator, distance of outliers, and each dimension examined. We defined success to
consist of identifying at least 90% of the outliers correctly
as outliers. In almost every case, the hybrid algorithm had
no errors if it succeeded at all, but we used the more liberal
definition of success, because sometimes a method identified almost all of the outliers but missed a few, and it was
thought to be unfair to call that "failure."
Figure 1 shows plots of the fitted probability of success
as a function of the amount c of contamination for three
estimators in dimension 20, with n = 400 and t = 1,600
seconds. Figure la is for outliers set at a distance of
d = 2; Figure 1 b is for outliers at d = 4. The message is
clear. The Forward algorithm is greatly superior to random~
Figure 1. Predicted Performance
of Outlier Detection Methods
Dimension 20 with n = 400. The dotted line represents
MINVOL; the
dashed line. FORWARD; the solid line. HYBRID. (a) Outliers at a distance
of d = 2; (b) outliers at a distance of d = 4.
Rocke and Woodruff: Outliers in Multivariate Data
Table 3. Effect of Outliers in One or Multiple Clusters
Table 5. Effect of Dimension
Predicted success rate
Predicted success rate
Number of clusters
Time (sec)
t: (percent)
e (percent)
dimensions, sample sizes, outlier distances, fractions of out-
liers, type of outliers, and computation times is the hybrid
e t eoretlc
resu ts 0 ",,00
Rocke demonstrated that any amount of contami-
nation below 50% can be handled with sufficient data and
sufficient processing time. Here we ask a different question:
What amount of contamination can be practically detected
with the amount of data given and with practical processing
outlier, dimension, samp1eSize:and computation time on
the effectiveness of the algorithm. A series of tables show
some results, which are based on simulations. To produce
the predicted success rate, a generalized linear model was
fit as described previously.
Type of Outliers.
Most of our results are based
on the use of shift outliers, in which the outlying values
are generated from a distribution with the same covariance
matrix and a shifted mean. Theorem 1 implies that this is
the hardest shape that outliers can take. We tested this empirically by generating outliers that were in more than one
The experiments
in th~ table were performed with outliers at a distance of d = 2 The
last column represents the fitted value from a generalized
linear model fitted to the results of theexperimen
The experiments
in this table were performed
in dimension 20 with n = 400 and 1,600
seconds of processing time. The iast column represents the fitled value from a generalized
model fitled to the results of the experiment.
the hybrid algorithm presented with a data set of 800 points
in dimension 20 with 30% shift outliers at a distance of
d = 2 is predicted to succeed 85% of the time with 3,200
seconds of processing time. The success rate falls to 15%
if the fraction of outliers rises to 35%.
Estimating the Envelope
This section is devoted to the following question: For what
Table 6. Effect of Sample Size
Table 4. Effect of Outlier Shrinkage
The experiments
in this table were performed in dimension
20 with outliers
at a distance
The last column represents
the fitted value from a generalized
linear model fitted to
the results of the experiment.
The experiments
in this table were performed
in dimension
20 with n = 800 and 30%
outliers. The last column represents
the fitted value from a generalized linear model fitted to the
of the experiment
Journal of the American Statistical Association,
September 1996
of Computation
E: (percent)
Predicted success rate (percent)
NOTE: The experiments in this table were performed in dimension 20 with n = 800 and outiiers
at a distance of d = 2. The last column represents the fitted value frorrl a generalized linear
model fitted to the resu~ 01 the experiment.
"cluster. Each cluster had a mean the same distance from the
center as before (d = 2 or d = 4), but lying along one of
the 2P random diagonals (:j::l, :j::l, ..., :j::l). This provides a
different overall outlier shape. The ultimate in this is to use
a different random direction for each point, which amounts
to radial outliers.
Table 3 shows some results of simulations with multiple
clusters. These were done in dimension 20 with n = 400 and
1,600 seconds allowed for processing. It is apparent from
these results that if the outliers lie in more than one cluster,
then the process of identifying the good data becomes dramatically easier, culminating in radial outliers, for which
we were successful in every trial of the experiment.
Another issue involves outliers that have a distribution
with the same shape as the main data but of a different size.
Theorem 2 implies that the smaller the size, the harder the
problem. Point mass outliers have the smallest size but are
easily detected with our pairwise comparison front end. A
reasonable compromise as-an-.aItemative
to shift outliers is
what we call crossover outliers, in which the shiiriKagels
set just sufficiently large to make the mean distance of the
Table 8. Critical Contamination Level for 90% Success
With the Hybrid Algorithm
e: (perc:ent)
The last column .how.
the percentage
01 contamination
at which a predicted
the instances
could be successfully
completed.
The predictions
were trom a generalized
model fitted to the results of the experiment.
4.3.5 The Current Envelope. By "the envelope" we
mean the limits of the size and difficulty of problem that
can be handled with current technology. This is dependent
on the dimension, the sample size, the fraction and type of
outliers, the distance of the outliers from the main data, and
the available processing time.
Table 8 shows some results. For each indicated combination of dimension and outlier distance, a generalized linear
model was fit as described earliek Then the level of conoutliers the same as the mean distance of the good data
in the metric of the covariance of all the data; see Equation (4). Table 4 shows some example results for dimension
20 with n = 800 and 30% outliers. Shrinking the outliers
alwa'j~ make~ the ~earch harder, but the effect is lessened
if the search time is increased or if the outliers are relatively distant. Qualitatively, other comparisons remain the
same whether shift or crossover outliers are used as the test
bed, and so we have continued to use shift outliers in most
instances.
4.3.2 Dimension. Outlier problems in higher dimension are harder than those in lower dimension. More data
usually will be needed, if only because the number of parameters to be estimated is higher, and more processing
time will be needed. Even with some adjustments in this
direction, the greater difficulty of problems in high dimensions shows up. In Table 5 we have let the sample size
n grow with p2 and allowed the processing time to be 4n
seconds. Clearly, as the dimension rises, the amount of contamination that can be coped with falls, even after raising
the amount of data and the computational time. Nevertheless, many problems in dimension 40 and higher will still
be feasible, if the amount of contamination does not rise
4.3.3 Sample Size. The results of Woodruff and Rocke
 show that for large enough sample sizes, even the
hardest outlier problem can be tackled. Table 6 shows this
effect in practice. These experiments use shift outliers at
d = 2 in dimension 20. Whatever the amount of contamination, increasing the sample size (and the computational
~_rti~.!!ely)
in~s~~~_u~~:1:!~~.~~
to detect outliers at 35-40% in dimension 20 may require
very large samples and long computation times.
4.3.4 Time. Often, increasing the available data is not
a feasible option. Table 7 addresses the question of applying
increasing amounts of time without increasing the sample
size. These experiments involved shift outliers at a distance
of d = 2 in dimension 20 with 800 points. Increasing the
time does increase the success rate, although there is no
assurance that the limit of the success rate as the time increases is 100%. An important avenue of future research is
to make more effective use of large amounts of time with
a fixed sample. One possible approach is to use multiple
random partitions, with only a fixed amount of time allocated to each one. Thus as available CPU time increases,
the number of partition cells examined will increase, rather
than the intensiveness of t1:.:: search in each cell.
Rocke and Woodruff: Outliers in Multivariate Dslta
Log Expected Squ~lre Distance
Log Expected Square Distance
~~.""'~::;:~"!.::
Log Expected Square Distance
Mahalanobis
Square Distances fc)r the HBK Data Using Three Metrics Plotted Against ~
,Expected Order Statistics on a Log-Log
Scale. (a) The al/-data metric. The horizontallin9
represents the 99% point of a x~ random variable. The three highest points are 14, 12, and 13, in
that order. (b) The metric derived from the data after excluding points 12, 13, and 14. The horizontal line represents the 99% point of a x~ random
variable. (c) The metric derived from the hybria' algorithm. The horizontal line represents a 99% cutoff as estimated by Phase II of the algorithm.
The 14 points above the cutoff are the points introduced by Hawkins et al. as outliers (1-14).
small to be able to handle large amounts of contamination.
For assured success with high contamination, substantially
larger values of both than the ones that we used may be necessary. Multiple processor machines could also be used to
increase the effectiveness of the algorithm, which is parallelizable in a number of ways .
tamination was found that allowed a predicted 90% of the
data sets to be successfully completed. To avoid undue extrapolation, computation times and sample sizes were set to
within the bounds of what were used for problems of that
nature in our study.
The more data (and the more computation time), the
greater the fraction of outliers that can be handled. Within
our self-imposed bounds, we can say that outlier fractions
in the 30-40% range can be reliably solved in dimension
10,25-35% in dimension 20, and 20-25% in dimension 40.
Although these bounds are crude, it does give some feel
for what problems are feasible. It is likely that the sample
sizes and processing times for dimension 40 are actually too
In this section we examine the performance of the hybrid
algorithm on several examples used in the literature. The
first is the constructed data of Hawkins, Bradu, and Kass
 , which consists of 75 points in dimension 3 plus a
response. (We refer to these data as HBK for short.) We ex-
Journal of the American Statistic,al Association, September 1996
Log Expected Square Distance
Figure 4. Mahalanobis Square Distances for the Bushfire Data Using
the Metric Derived From the Hybrid Algorithm. The horizontal line represents a 99% cutoff as estimated by Phase II of the algorithm. The 13
points above the cutoff are the points identified by Maronna and Yohai
as outliers using the Stahel- Donoho method; from largest to smallest,
these are 35,38,33,37,34.36,32,9,8,
10, 11,31, and 7.
Log Expected Square Distance
amine just the three predictor variables. The first 14 points
were designed to be leverage points, but only point 14 and
possibly point 13 show up as such if ordinary Mahalanobis
distances are used . Figure 2a
shows the Mahalanobis square distances versus X~ order
statistics with a horizontal line at the .99 percentage point
of a X~ variable. Only point 14 appears out 01' line. If this
int is omitted then
int 13"~ows-u agcA:,,- ant and:"
if point 13 is also omitted, then point 12 is close to the
cutoff. Now no further points appear discrepant. Figure 2b
shows the distances with points 12, 13, and 14 omitted;
nothing appears out of the ordinary. Figure 2c shows the
distances obtained from the hybrid algorithm. There is a
clear separation of the 14 initial points from the remainder
of the points-they are well above the cutoft' derived by
simulation in Phase II. The hybrid algorithm has identified
the structure as specified by Hawkins et al. . , Rousseeuw and Leroy ,
and others have also identified this structure.)
The second data set is the modified wood gravity data
originally from Draper and Smith , consisting of
20 observations of 5 explanatory variables and a response
(wood specific gravity). We analyze only the explanatory
variables. Rousseeuw. and Rousseeuw and Leroy
 modified the data by replacing observations 4, 6, 8,
and 19 by outliers. Figure 3 shows the distances from the
hybrid algorithm. The four discrepant points are the observations that were perturbed by Rousseeuw. Again, these
do not show up using the all-data metric, but the plots have
been omitted because similar ones have appeared elsewhere.
The third example data set is the bushfire data used by
Maronna and Yohai as an example. This consists
of 38 observations (pixels) of satellite measurements on 5
frequency bands used to locate bushfire scars . Maronna and Yohai concluded that there were outliers in two groups: points 7-11, which are relatively easy to
detect by various robust methods, and points 32-38, which
are masked by the first group to estimators other than the
Stahel-Donoho (SD) projection estimator as implemented
by Maronna and Yohai. Figure 4 shows the distances by
the hybrid algorithm. The most extreme"8!~~.p-
of points is
""Cc-c.=="~=~"
;"'~~"~&;"'9,"and
32o.;.c38,wher~thenexfmostextteme
four is 7, 1O, 11, and 31. These are almost the same points
Figure 5, Mahalanobis Square Distances for the Milk Data Using the
Metric Derived From the Hybrid Algorithm. The horizontal line represents
a 99% cutoff as estimated by Phase II of the algorithm. The 15 points
above the cutoff are the points identified by Atkinson as outliers using
the Forward algorithm; from largest to smallest, these are 70, 2, 41, 1,
12, 13, 14,3,15,47,75,17,
Rocke and Woodruff: Outliers in Multivariate Dlata
Table 9. Specifications of Exam~l/e Data Sets
in dimension 20, and 20-25% in dimension 40. The ability of this algorithm to accurately characterize the outliers
in a multivariate distribution was also shown, using several
examples from the literature.
Note: Programs in the C language to implement
the hybrid algorithm and to produce the test data are
available from the authors and from STATLIB. Please
send requests bye-mail
to or
dl .
The HBK data are from Hawkins et al. . the modified wood gravity data are from
Rousseeuwand
Leroy .
the bushfire data are from Maronna
and Yohai . and the
milk data are from Daudin
et al. .
APPENDIX: PROOFS OF THEOREMS
Proof of Theorem 1
Because all of the statements are affine equivariant, we may
without loss of generality take JLo = 0 and ~o = I. Then, without loss of generality, we may rotate the coordinate until 51 is a
diagonal matrix with diagonal elements (U)l, U)2, ...,U)p). The constraint that the size (determinant) of 51 is fixed can be expressed as
nf=lU)i = D. Now d~o(x,O) = 2:::f=lX~, whose expectation is
+ 2:::f=l U)i. Thus the theorem asserts that the minimum value
of 2:::f=l U)i subject to nf=l U)i = D is attained at U)i = D1/p for
i ::; p. This can be shown by a straightforward application
of the method of Lagrange multipliers.
identified by the SD estimator (point 31 has been added)
but sorted into slightly different groups. Further clarification of the clustering issue awaits development of methods
for identifying clusters that may comprise less than half of
The final data set consists of 8 measurements on 86 samples of milk analyzed
by Atkinson . Two of the points (63 and 64) are exact
duplicates (which may explain the discrepancy between the
86 observations in Daudin et al.'s table 1 and the 85 observations said to be contained in it). We omitted the duplicate
point but preserved the original numbering of the 86 points
to facilitate comparison with Atkinson's results. Figure 5
shows the distances for this data set using the hybrid algorithm. The outlying points occur in three groups: point 70,
points 1,2, and 41; and the remainder. The small group of
four lying just below the cutoff consists of points 11,20,27,
and 77. The points lying above the cutoff coincide with the
points identified as outliers or possible outliers by Atkinson.
The appearance of clustering is even stronger here than in
--the bushftre~suggesting~ffiat
a m(]lreci>mPlei~ySis
might eventually be called for.
Table 9 summarizes the examples presented in this section. Two further features of the analysis of these data with
the hybrid algorithm are worthy of note. First, each analysis was successfully accomplished with only a minimal
amount of search time. Even though the fraction of contamination ranged up to 34%, in dimensions as low as 5-8, this
does not fall within the problematic range of the method.
Second, although there is a stochastic aspect to the MCD
search, multiple tries on the same data set always yielded
the same answer to many significant digits. The instability
of some other methods does not seem to apply to the hybrid
algorithm.
Proof of Theorem 2
As before, because all operations are affine equivariant, we may
set ILo = 0 and Eo = I without loss of generality. Also, without
loss of generality, by affine equivariance, we may take IL to be a
vector in which the first coordinate is 1/ and the other coordinates
By Lemma 1, in the indicated coordinate system, n-l,
diagonal matrix with elements ((1 -c + Ac + c(l-
c)1/2)-1, (1-
':;{Tf~+~.)-l.)...If
the c~~~t~Qf
are (Xl,X2,... ,Xp), then
~d~(x,cl') = (Xl -c17)2(1- c + Ac + c(l-
E: + .AE:)-1. (A.I)
Because the covariance matrix is I, the expectation of this quantity
is (1 + E:21]2)(1- E: +).E; + E:(1- E:)1]Z»)-l + (p -1)(1-
E: + .AE:)-1,
which for large 1] is E:(1- E:)-l + (P-1)(1-
E:+ >.E:)-l. Similarly,
for a bad point,
E(d~(x,E:JJ.)
= (.).2 + (1 -o:f1)2)(1
-0: + >.0: + 0:(1 -0:)1)2)'-
+ A(P -1)(1 -0: + Ao:)-1,
which for large 1/ is (1 -0:)0:-1 + A(p -1)(1 -0: + Ao:)-1.
For large 1], the difference in E(di:(x,o:J1.») between good and
bad points is
(A -1)(p -1)
(A -1)(p -1)
where a positive value indicates that the bad points have a larger
expected Mahalanobis distance. With regard to varying A, this is,
up to a constant, (p -1)Aj(1
-c + Ac). This has derivative with
respect to A,
-1»..£]/(1-
6. CONCLUSIONS
In this article we have investigated the nature of multivariate outliers and methods for their detection. We have
shown that shift outliers provide a reasonable test bed for
multivariate outlier detection, being difficult but not impossible to detect.
Using this test bed, we have shown a new hybrid algorithm to be superior to existing methods for this problem.
Given sufficient data and processing time, this algorithm
can deal with even heavily contaminated data in high dimensions. Roughly speaking, outlier fractions in the 30-
40% range can be reliably solved in dimension 10, 25-35%
= (p -l){l
c)/(l -c + ,\c);
Journal of the American Statistic:al Association,
The most difficult case is when this difference in distances is algebraically least, which is when>. = O.
When>. = 0 (point mass contamination). the difference in the
expected Mahalanobis distances is
Donoho, D. L. , "Breakdown Properties of Multivariate Location
Estimators," Ph.D. qualifying paper, Harvard University, Dept. of Statistics.
Draper, N. R., and Smith, H. (1%6), Applied Regression Analysis, New
York: John Wiley.
Glover, F. , "Tabu Search-Part I," ORSA Journal on Computing, I,
"Tabu Search-Part n," ORSA Journal on Computing, 2,
which reaches zero when e: = l/(p + 1). (Note that this is the
breakdown point of many multivariate estimators.)
When>.. = 1, the difference in the expected Mahalanobis distances is
which is always positive but is a small fraction of the mean distance of either type of point for high dimension. Specifically, under normality, the variance of the Mahalanobis distance of a good
point for large 1/ is 2(p -1), so the difference between the expected Mahalanobis distances in units of the standard deviation of
the Mahalanobis distance of a good point is
e (1 -e )J2 -c:p-=1) ,
which goes to zero as the dimension rises (for fixed e). Because
both aire asymptotically (in p) normal, the distributions converge.
, The condition for the good and bad expected distances to be
equal is (for large 7/)
.).0 = .(!-- c)(cp -(12ll
-c((1-c)p-c)
[Received August..1993. Revised December 1995.J