Domain Adaptation via Transfer Component Analysis
Sinno Jialin Pan1, Ivor W. Tsang2, James T. Kwok1 and Qiang Yang1
1Department of Computer Science and Engineering
Hong Kong University of Science and Technology, Hong Kong
2School of Computer Engineering, Nanyang Technological University, Singapore 639798
1{sinnopan,jamesk,qyang}@cse.ust.hk, 
Domain adaptation solves a learning problem in a
target domain by utilizing the training data in a different but related source domain. Intuitively, discovering a good feature representation across domains is crucial.
In this paper, we propose to
ﬁnd such a representation through a new learning method, transfer component analysis (TCA),
for domain adaptation. TCA tries to learn some
transfer components across domains in a Reproducing Kernel Hilbert Space (RKHS) using Maximum Mean Discrepancy (MMD). In the subspace
spanned by these transfer components, data distributions in different domains are close to each other.
As a result, with the new representations in this
subspace, we can apply standard machine learning
methods to train classiﬁers or regression models in
the source domain for use in the target domain. The
main contribution of our work is that we propose
a novel feature representation in which to perform
domain adaptation via a new parametric kernel using feature extraction methods, which can dramatically minimize the distance between domain distributions by projecting data onto the learned transfer
components. Furthermore, our approach can handle large datsets and naturally lead to out-of-sample
generalization. The effectiveness and efﬁciency of
our approach in are veriﬁed by experiments on two
real-world applications: cross-domain indoor WiFi
localization and cross-domain text classiﬁcation.
Introduction
Domain adaptation aims at adapting a classiﬁer or regression
model trained in a source domain for use in a target domain,
where the source and target domains may be different but related. This is particularly crucial when labeled data are in
short supply in the target domain. For example, in indoor
WiFi localization, it is very expensive to calibrate a localization model in a large-scale environment. However, the WiFi
signal strength may be a function of time, device or space,
depending on dynamic factors. To reduce the re-calibration
effort, we might want to adapt a localization model trained
in one time period (the source domain) for a new time period (the target domain), or to adapt the localization model
trained on one mobile device (the source domain) for a new
mobile device (the target domain). However, the distributions
of WiFi data collected over time or across devices may be
very different, hence domain adaptation is needed [Yang et
al., 2008]. Another example is sentiment classiﬁcation. To
reduce the effort of annotating reviews for various products,
we might want to adapt a learning system trained on some
types of products (the source domain) for a new type of product (the target domain). However, terms used in the reviews
of different types of products may be very different. As a result, distributions of the data over different types of products
may be different and thus domain adaptation is again needed
[Blitzer et al., 2007].
A major computational problem in domain adaptation is
how to reduce the difference between the distributions of
source and target domain data. Intuitively, discovering a good
feature representation across domains is crucial. A good feature representation should be able to reduce the difference in
distributions between domains as much as possible, while at
the same time preserving important (geometric or statistical)
properties of the original data.
Recently, several approaches have been proposed to learn
a common feature representation for domain adaptation
[Daum´e III, 2007; Blitzer et al., 2006]. Daum´e III 
proposed a simple heuristic nonlinear mapping function to
map the data from both source and target domains to a highdimensional feature space, where standard machine learning
methods are used to train classiﬁers. Blitzer et al. proposed the so-called structural correspondence learning (SCL)
algorithm to induce correspondencesamong features from the
different domains. This method depends on the heuristic selections of pivot features that appear frequently in both domains. Although it is experimentally shown that SCL can reduce the difference between domains based on the A-distance
measure [Ben-David et al., 2007], the heuristic criterion of
pivot feature selection may be sensitive to different applications. Pan et al. proposed a new dimensionality reduction method, Maximum Mean Discrepancy Embedding
(MMDE), for domain adaptation. The motivation of MMDE
is similar to our proposed work. It also aims at learning a
shared latent space underlying the domains where distance
between distributions can be reduced. However, MMDE suf-
fers from two major limitations: (1) MMDE is transductive,
and does not generalize to out-of-sample patterns; (2) MMDE
learns the latent space by solving a semi-deﬁnite program
(SDP), which is a very expensive optimization problem.
In this paper, we propose a new feature extraction approach, called transfer component analysis (TCA), for domain adaptation. It tries to learn a set of common transfer
components underlying both domains such that the difference in distributions of data in the different domains, when
projected onto this subspace, can be dramatically reduced.
Then, standard machine learning methods can be used in this
subspace to train classiﬁers or regression models across domains. More speciﬁcally, if two domains are related to each
other, there may exist several common components (or latent
variables) underlying them. Some of these components may
cause the data distributions between domains to be different,
while others may not. Some of these components may capture the intrinsic structure underlying the original data, while
others may not. Our goal is to discover those components
that do not cause distribution change across the domains and
capture the structure of the original data well. We will show
in this paper that, compared to MMDE, TCA is much more
efﬁcient and can handle the out-of-sample extension problem.
The rest of the paper is organized as follows. Section 2 ﬁrst
describes the problem statement and preliminaries of domain
adaptation. Our proposed method is presented in Section 3.
We then review some related works in Section 4. In Section
5, we conduct a series of experiments on indoor WiFi localization and text classiﬁcation. The last section gives some
conclusive discussions.
In the sequel, A ≻0 (resp. A ⪰0) means that the matrix A is symmetric and positive deﬁnite (pd) (resp. positive
semideﬁnite (psd)). Moreover, the transpose of vector / matrix (in both the input and feature spaces) is denoted by the
superscript ⊤, A† is the pseudo-inverse of the matrix A, and
tr(A) denotes the trace of A.
Preliminaries of Domain Adaptation
In this paper, we focus on the setting where the target domain
has no labeled training data, but has plenty of unlabeled data.
We also assume that some labeled data DS are available in
a source domain, while only unlabeled data DT are available
in the target domain. We denote the source domain data as
DS = {(xS1, yS1), . . . , (xSn1 , ySn1)}, where xSi ∈X is the
input and ySi ∈Y is the corresponding output. Similarly,
we denote the target domain data as DT = {xT1, . . . , xTn2 },
where the input xTi is also in X. Let P(XS) and Q(XT )
(or P and Q for short) be the marginal distributions of XS
and XT , respectively. In general, P and Q can be different.
Our task is then to predict the labels yTi’s corresponding to
the inputs xTi’s in the target domain. The key assumption
in a typical domain adaptation setting is that P ̸= Q, but
P(YS|XS) = P(YT |XT ) .
Maximum Mean Discrepancy
Many criteria, such as the Kullback-Leibler (KL) divergence,
can be used to estimate the distance between distributions.
However, many of these criteria are parametric, since an intermediate density estimate is usually required.
such a non-trivial task, a non-parametric distance estimate
between distributions is more desirable.
Recently, Borgwardt et al. proposed the Maximum Mean Discrepancy
(MMD) as a relevant criterion for comparing distributions
based on the Reproducing Kernel Hilbert Space (RKHS). Let
X = {x1, . . . , xn1} and Y = {y1, . . . , yn2} be random variable sets with distributions P and Q. The empirical estimate
of the distance between P and Q, as deﬁned by MMD, is
Dist(X,Y) =
where H is a universal RKHS [Steinwart, 2001], and φ :
Therefore, the distance between distributions of two samples can be well-estimated by the distance between the means
of the two samples mapped into a RKHS.
Transfer Component Analysis
Based on the inputs {xSi} and outputs {ySi} from the source
domain, and the inputs {xTi} from the target domain, our
task is to predict the unknown outputs {yTi} in the target domain. The general assumption in domain adaptation is that
the marginal densities, P(XS) and Q(XT ), are very different.
In this section, we attempt to ﬁnd a common latent
representation for both XS and XT that preserves the data
conﬁguration of the two domains after transformation. Let
the desired nonlinear transformation be φ : X →H. Let
Si} = {φ(xSi)}, X′
Ti} = {φ(xTi)} and
T be the transformed input sets from the source,
target and combined domains, respectively. Then, we desire
that P′(X′
S) = Q′(X′
Assuming that φ is the feature map induced by a universal
kernel. As shown in Section 2.1, the distance between two
distributions P and Q can be empirically measured by the
(squared) distance between the empirical means of the two
Therefore, a desired nonlinear mapping φ can be found by
minimizing this quantity. However, φ is usually highly nonlinear and a direct optimization of (2) can get stuck in poor
local minima. We thus need to ﬁnd a new approach, based on
the following assumption.
The key assumption in the proposed domain adaptation setting is that P ̸= Q, but P(YS|φ(XS)) = P(YT |φ(XT )) under a transformation mapping φ on the input.
In Section 3.1, we ﬁrst revisit Maximum Mean Discrepancy Embedding (MMDE) which proposed to learn the kernel
matrix K corresponding to the nonlinear mapping φ by solving a SDP optimization problem. In Section 3.2, we then propose a factorization of the kernel matrix for MMDE. An ef-
ﬁcient eigendecomposition algorithm for kernel learning and
computational issues are discussed in Sections 3.3 and 3.4.
Kernel Learning for Domain Adaptation
Instead of ﬁnding the nonlinear transformation φ explicitly,
Pan et al. proposed to transform this problem as a
kernel learning problem. By virtue of the kernel trick, (i.e.,
k(xi, xj) = φ(xi)′φ(xj)), the distance between the empirical
means of the two domains in (2) can be written as:
T ) = tr(KL),
is a (n1 + n2) × (n1 + n2) kernel matrix, KS,S, KT,T and
KS,T respectively are the kernel matrices deﬁned by k on the
data in the source domain, target domain, and cross domains;
and L = [Lij] ⪰0 with Lij =
1 if xi, xj ∈XS; Lij =
if xi, xj ∈XT ; otherwise, −
In the transductive setting, learning the kernel k(·, ·) can
be solved by learning the kernel matrix K instead. In [Pan
et al., 2008], the resultant kernel matrix learning problem is
formulated as a semi-deﬁnite program (SDP). Principal Component Analysis (PCA) is then applied on the learned kernel
matrix to ﬁnd a low-dimensional latent space across domains.
This is referred to as Maximum Mean Discrepancy Embedding (MMDE).
Parametric Kernel Map for Unseen Patterns
There are several limitations of MMDE. First, it is transductive and cannot generalize on unseen patterns. Second,
the criterion (3) requires K to be positive semi-deﬁnite and
the resultant kernel learning problem has to be solved by
expensive SDP solvers. Finally, in order to construct lowdimensional representations of X′
T , the obtained K
has to be further post-processed by PCA. This may potentially discard useful information in K.
In this paper, we propose an efﬁcient method to ﬁnd a nonlinear mapping φ based on kernel feature extraction. It avoids
the use of SDP and thus its high computational burden. Moreover, the learned kernel k can be generalized to out-of-sample
patterns directly. Besides, instead of using a two-step approach as in MMDE, we propose a uniﬁed kernel learning
method which utilizes an explicit low-rank representation.
First, recall that the kernel matrix K in (4) can be decomposed as K = (KK−1/2)(K−1/2K), which is often known
as the empirical kernel map [Sch¨olkopf et al., 1998]. Consider the use of a (n1 + n2) × m matrix 
W to transform the
corresponding feature vectors to a m-dimensional space. In
general, m ≪n1 + n2. The resultant kernel matrix1 is then
K = (KK−1/2
W ⊤K−1/2K) = KWW ⊤K,
where W = K−1/2
W ∈R(n1+n2)×m. In particular, the corresponding kernel evaluation of k between any two patterns
xi and xj is given by
k(xi, xj) = k⊤
xiWW ⊤kxj,
1As is common practice, one can ensure that the kernel matrix
K is positive deﬁnite by adding a small ϵ > 0 to its diagonal [Pan et
al., 2008].
[k(x1, x), . . . , k(xn1+n2, x)]⊤
Hence, the kernel k in (6) facilitates a readily parametric form
for out-of-sample kernel evaluations.
Moreover, using the deﬁnition of K in (5), the distance between the empirical means of the two domains can be rewritten as:
T ) = tr((KWW ⊤K)L)
= tr(W ⊤KLKW).
Transfer Components Extraction
In minimizing criterion (7), a regularization term tr(W ⊤W)
is usually needed to control the complexity of W. As will
be shown later in this section, this regularization term can
avoid the rank deﬁciency of the denominator in the generalized eigendecomposition. The kernel learning problem for
domain adaptation then reduces to:
tr(W ⊤W) + μ tr(W ⊤KLKW)
W ⊤KHKW = I,
where μ is a trade-off parameter, I ∈Rm×m is the identity
matrix, H = In1+n2 −
n1+n2 11⊤is the centering matrix,
where 1 ∈Rn1+n2 is the column vector with all ones, and
In1+n2 ∈R(n1+n2)×(n1+n2) is the identity matrix. Moreover, note that the constraint W ⊤KHKW = I is added in
(8) to avoid the trivial solution (W = 0), such that the transformed patterns do not collapse to one point, which can in-
ﬂate the learned kernel k such that the embedding of data x′
is preserved as in kernel PCA.
Though the optimization problem (8) involves a nonconvex norm constraint W ⊤KHKW = I, it can still be
solved efﬁciently by the following trace optimization problem:
Proposition 1 The optimization problem (8) can be reformulated as
tr((W ⊤KHKW)†W ⊤(I + μKLK)W),
tr((W ⊤(I + μKLK)W)−1W ⊤KHKW).
Proof. The Lagrangian of (8) is
tr(W ⊤(I + μKLK)W) −tr((W ⊤KHKW −I)Z), (11)
where Z is a symmetric matrix. Setting the derivative of (11)
w.r.t. W to zero, we have
(I + μKLK)W = KHKWZ.
Multiplying both sides on the left by W T , and then on
substituting it into (11), we obtain (9).
Since the matrix
I + μKLK is non-singular beneﬁted from the regularization
term tr(W ⊤W), we obtain an equivalent trace maximization
problem (10).
Similar to kernel Fisher discriminant (KFD), the solution
of W in (10) is the eigenvectors corresponding to the m leading eigenvalues of (I + μKLK)−1KHK, where at most
n1 + n2 −1 eigenvectors can be extracted. In the sequel,
the proposed method is referred to as Transfer Component
Analysis (TCA).
Computational Issues
The kernel learning algorithm in [Pan et al., 2008] relies on
SDPs. As there are O((n1 +n2)2) variables in K, the overall
training complexity is O((n1 + n2)6.5) [Nesterov and Nemirovskii, 1994]. This becomes computationally prohibitive
even for small-sized problems. Note that criterion (3) in this
kernel learning problem is similar to the recently proposed
supervised dimensionality reduction method colored MVU
[Song et al., 2008], in which low-rank approximation is used
to reduce the number of constraints and variables in the SDP.
However, gradient descent is required to reﬁne the embedding
space and thus the solution can get stuck in a local minimum.
On the other hand, our proposed kernel learning method requires only a simple and efﬁcient eigendecomposition. This
takes only O(m(n1 +n2)2) time when m non-zero eigenvectors are to be extracted [Sorensen, 1996].
Related Works
Domain adaptation, which can be considered as a special
setting of transfer learning [Pan and Yang, 2008], has been
widely studied in natural language processing (NLP) [Ando
and Zhang, 2005; Blitzer et al., 2006; Daum´e III, 2007].
Ando and Zhang and Blitzer proposed structural correspondence learning (SCL) algorithms to learn the
common feature representation across domains based on
some heuristic selection of pivot features. Daum´e III 
designed a heuristic kernel to augment features for solving
some speciﬁc domain adaptation problems in NLP. Besides,
domain adaptation has also been investigated in other application areas such as sentiment classiﬁcation [Blitzer et al.,
2007]. Theoretical analysis of domain adaptation has also
been studied in [Ben-David et al., 2007].
The problem of sample selection bias (also referred to as
co-variate shift) is also related to domain adaption. In sample selection bias, the basic assumption is that the sampling
processes between the training data Xtrn and test data Xtst
may be different.
As a result, P(Xtrn) ̸= P(Xtst), but
P(Ytrn|Xtrn) = P(Ytst|Xtst). Instance re-weighting is a
major technique for correcting sample selection bias [Huang
et al., 2007; Sugiyama et al., 2008]. Recently, a state-ofart method, called kernel mean matching (KMM), is proposed [Huang et al., 2007].
It re-weights instances in a
RKHS based on the MMD theory, which is different from our
proposed method. Sugiyama et al. proposed another
re-weighting algorithm, Kullback-Leibler Importance Estimation Procedure (KLIEP), which is integrated with crossvalidation to perform model selection automatically. Xing et
al. proposed to correct the labels predicted by a shiftunaware classiﬁer towards a target distribution based on the
mixture distribution of the training and test data. Matching
distributions by re-weighting instances is also used successfully in Multi-task Learning [Bickel et al., 2008]. However,
unlike instance re-weighting, the proposed TCA method can
cope with noisy features (as in image data and WiFi data) by
effectively denoising and ﬁnding a latent space for matching
distributions across different domains simultaneously. Thus,
TCA can be treated as an integration of unsupervised feature
extraction and distribution matching in a latent space.
Experiments
In this section, we apply the proposed domain adaptation algorithm TCA on two real-world problems: indoor WiFi localization and text classiﬁcation.
Cross-domain WiFi Localization
For cross-domain WiFi localization, we use a dataset published in the 2007 IEEE ICDM Contest [Yang et al., 2008].
This dataset contains some labeled WiFi data collected in
time period A (the source domain) and a large amount of unlabeled WiFi data collected in time period B (the target domain). Here, a label means the corresponding location where
the WiFi data are received. WiFi data collected from different
time periods are considered as different domains. The task is
to predict the labels of the WiFi data collected in time period B. More speciﬁcally, all the WiFi data are collected in
an indoor building around 145.5 × 37.5 m2, 621 labeled data
are collected in time period A and 3128 unlabeled data are
collected in time period B.
We conduct a series of experiments to compare TCA with
some baselines, including other feature extraction methods
such as KPCA, sample selection bias (or co-variate shift)
methods, KMM and KLIEP and a domain adaptation method,
SCL. For each experiment, all labeled data in the source
domain and some unlabeled data in the target domain are
used for training. Evaluation is then performed on the remaining unlabeled data (out-of-sample) in the target domain.
This is repeated 10 times and the average performance is
used to measure the generalization abilities of the methods.
In addition, to compare the performance between TCA and
MMDE, we conduct some experiments in the transductive
setting [Nigam et al., 2000]. The evaluation criterion is the
Average Error Distance (AED) on the test data, and the lower
the better. For determining parameters for each method, we
randomly select a very small subset of the target domain data
to tune parameters. The values of parameters are ﬁxed for all
the experiments.
Figure 1(a) compares the performance of Regularized
Least Square Regression (RLSR) model on different feature
representations learned by TCA, KPCA and SCL, and different re-weighted instances learned by KMM and KLIEP.
Here, we use μ = 0.1 for TCA and the Laplacian kernel.
As can be seen, the performance can be improved with the
new feature representations of TCA and KPCA. TCA can
achieve much higher performance because it aims at ﬁnding
the leading components that minimize the difference between
domains. Then, from the space spanned by these components,
the model trained in one domain can be used to perform accurate prediction in the other domain.
Figure 1(b) shows the results under a varying number of
unlabeled data in the target main. As can be seen, with only
a few unlabeled data in the target domain, TCA can still ﬁnd
a good feature representation to bridge between domains.
Since MMDE cannot generalize to out-of-sample patterns,
in order to compare TCA with MMDE, we conduct another
series of experiments in a transductive setting, which means
that the trained models are only evaluated on the unlabeled
data that are used for learning the latent space. In Figure 1(c),
we apply MMDE and TCA on 621 labeled data from the
# Dimensions
Average Error Distance (unit: m)
(a) Varying # features of domain adaptation
methods. (# unlabeled data is 300).
# Unlabeled Data for Training
Average Error Distance (unit: m)
(b) Varying # unlabeled data (# dimensions of
TCA and KPCA is 10).
# Dimensions
Average Error Distance (unit: m)
(c) Comparison of MMDE and TCA in the
transductive setting (# unlabeled data is 300).
Figure 1: Comparison of Average Error Distance (in m).
source domain and 300 unlabeled data from the target domain to learn new representations, respectively, and then train
RLSR on them. More comparison results in terms of ACE
with varying number of training data are shown in Table 1.
The experimental results show that TCA is slightly higher
(worse) than MMDE in terms of AED. This is due to the nonparametric kernel matrix learned by MMDE, which can ﬁt
the observed unlabeled data better. However, as mentioned in
Section 3.4, the cost of MMDE is expensive due to the computationally intensive SDP. The comparison results between
TCA and MMDE in terms of computational time on the WiFi
dataset are shown in Table 2.
Table 1: ACE (in m) of MMDE and TCA with 10 dimensions
and varying # training data (# labeled data in the source domain is ﬁxed to 621, # unlabeled data in the target domain
varies from 100 to 800.)
# unlabeled and labeled data used for training
2.413 2.378 2.313 2.285
MMDE 2.315 2.247 2.208 2.212 2.207
Table 2: CPU training time (in sec) of MMDE and TCA with
varying # training data.
# unlabeled and labeled data used for training
1,021 1,121
MMDE 3,209 3,539 4,168 4,940 10,093 14,165 18,094 33,004
Cross-domain Text Classiﬁcation
In this section, we perform cross-domain binary classiﬁcation experiments on a preprocessed dataset of Reuters-21578.
These data are categorized to a hierarchical structure. Data
from different sub-categories under the same parent category
are considered to be from different but related domains. The
task is to predict the labels of the parent category. By following this strategy, three datasets orgs vs people, orgs vs places
and people vs places are constructed. We randomly select
50% labeled data from the source domain, and 35% unlabeled data from the target domain. Evaluation is based on
the (out-of-sample) testing of the remaining 65% unlabeled
data in the target domain. This is repeated 10 times and the
average results reported.
Similar to the experimental setting on WiFi localization,
we conduct a series of experiments to compare TCA with
KPCA, KMM, KLIEP and SCL. Here, the support vector machine (SVM) is used as the classiﬁer. The evaluation criterion is the classiﬁcation accuracy (the higher the better). We
experiment with both the RBF kernel and linear kernel for
feature extraction or re-weighting used by KPCA, TCA and
KMM. The kernel used in the SVM for ﬁnal prediction is a
linear kernel, and the parameter μ in TCA is set to 0.1.
As can be seen from Table 3, different from experiments
on the WiFi data, sample selection bias methods, such as
KMM and KLIEP perform better than KPCA and PCA on the
text data. However, with the feature presentations learned by
TCA, SVM performs the best for cross-domain classiﬁcation.
This is because TCA not only discovers latent topics behind
the text data, but also matches distributions across domains in
the latent space spanned by the latent topics. Moreover, the
performance of TCA using the RBF kernel is more stable.
Conclusion and Future Work
Learning feature representations is of primarily an important
task for domain adaptation. In this paper, we propose a new
feature extraction method, called Transfer Component Analysis (TCA), to learn a set of transfer components which reduce the distance across domains in a RKHS. Compared to
the previously proposed MMDE for the same task, TCA is
much more efﬁcient and can be generalized to out-of-sample
patterns. Experiments on two real-world datasets verify the
effectiveness of the proposed method. In the future, we are
planning to take side information into account when learning
the transfer components across domains, which may be better
for the ﬁnal classiﬁcation or regression tasks.
Acknowledgement
Sinno Jialin Pan and Qiang Yang thank the support from Microsoft Research MRA07/08.EG01 and Hong Kong CERG
Project 621307. Ivor W. Tsang thanks the support from Singapore MOE AcRF Tier-1 Research Grant (RG15/08). James
T. Kwok thanks the support from CERG project 614508.
Table 3: Comparison between Different Methods (number inside parentheses is the standard deviation over 10 repetitions).
people vs places
orgs vs people
orgs vs places
0.5198 (.0252)
0.6696 (.0287)
0.6683 (.0221)
0.5564 (.0788)
0.5574 (.0760)
0.5653 (.0984)
0.5453 (.0911)
0.6470 (.0598)
0.6140 (.0534)
0.5424 (.0590)
0.6703 (.0334)
0.6491 (.0391)
0.5631 (.0346)
0.6652 (.0549)
0.6114 (.0564)
KPCA (RBF)
0.5900 (.0185)
0.5863 (0.0405)
0.5883 (.0185)
0.5934 (.0169)
0.5955 (0.0676)
0.6267 (.0814)
0.6032 (.0323)
0.5968 (0.0705)
0.6098 (.0315)
0.6000 (.0267)
0.5964 (0.0742)
0.6247 (.0438)
TCA (linear)
0.5804 (.0528)
0.6397 (.0897)
0.6403 (.0722)
0.5495 (.0764)
0.7308 (.0495)
0.7006 (.0527)
0.5600 (.0969)
0.7425 (.0579)
0.6720 (.0374)
0.5468 (.0635)
0.7330 (.0432)
0.5989 (.0700)
0.6129 (.0176)
0.6297 (.0302)
0.6899 (.0195)
0.5920 (.0148)
0.7088 (.0251)
0.7042 (.0218)
0.5954 (.0201)
0.7196 (.0235)
0.6942 (.0220)
0.5916 (.0166)
0.7217 (.0275)
0.6896 (.0203)
0.5267 (.0310)
0.6834 (.0327)
0.6733 (.0198)
KMM (linear)
0.5836 (.0159)
0.7006 (.0353)
0.6714 (.0263)
0.5836 (.0159)
0.6968 (.0224)
0.6655 (.0245)
0.5758 (.0241)
0.6946 (.0192)
0.6638 (.0112)