Diagnosis of thyroid cancer using deep convolutional neural
network models applied to sonographic images: a retrospective,
multicohort, diagnostic study
Xiangchun Li*,† [Prof],
Tianjin Cancer Institute, National Clinical Research Center for Cancer, Key Laboratory of Cancer
Prevention and Therapy of Tianjin, Tianjin Medical University Cancer Institute and Hospital,
Tianjin Medical University, Tianjin, China
Sheng Zhang*,† [Prof],
Department of Diagnostic and Therapeutic Ultrasonography, National Clinical Research Center
for Cancer, Key Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical
University Cancer Institute and Hospital, Tianjin Medical University, Tianjin, China
Qiang Zhang*,
Department of Maxillofacial and Otorhinolaryngology Oncology, National Clinical Research
Center for Cancer, Key Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical
University Cancer Institute and Hospital, Tianjin Medical University, Tianjin, China
Department of Diagnostic and Therapeutic Ultrasonography, National Clinical Research Center
for Cancer, Key Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical
University Cancer Institute and Hospital, Tianjin Medical University, Tianjin, China
Department of Pathology, National Clinical Research Center for Cancer, Key Laboratory of
Cancer Prevention and Therapy of Tianjin, Tianjin Medical University Cancer Institute and
Hospital, Tianjin Medical University, Tianjin, China
Jing Zhao,
Department of Diagnostic and Therapeutic Ultrasonography, National Clinical Research Center
for Cancer, Key Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical
University Cancer Institute and Hospital, Tianjin Medical University, Tianjin, China
Xiaojie Xin,
Correspondence to: Prof Kexin Chen, Department of Epidemiology and Biostatistics, Tianjin Medical University Cancer Institute and
Hospital, Tianjin 300060, China .
*Contributed equally and are joint first authors
†Contributed equally and are joint senior authors
Contributors
XL did the data analysis. KC, WZ, SZ, and MG supervised the project. XL, QZ, WZ, SZ, and KC designed the experiment. XL, QZ,
XiWe, WZ, and KC wrote the report. CTW, MNG, and BCP edited the report. YP curated pathological examinations. JZ, XX, XiWa,
XuWa, FY, MY, QW, LZ, ZZ, YZ, XZ, and XY gathered and annotated data. XiWe, SZ, CQ, JL, YZ, and XY interpreted the
validation set.
Declaration of interests
We declare no competing interests.
HHS Public Access
Author manuscript
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
 
Lancet Oncol. 2019 February ; 20(2): 193–201. doi:10.1016/S1470-2045(18)30762-9.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Department of Diagnostic and Therapeutic Ultrasonography, National Clinical Research Center
for Cancer, Key Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical
University Cancer Institute and Hospital, Tianjin Medical University, Tianjin, China
Chunxin Qin,
Department of Thyroid and Breast Surgery, Weihai Municipal Hospital, Shandong, China
Xiaoqing Wang,
Department of Diagnostic and Therapeutic Ultrasonography, National Clinical Research Center
for Cancer, Key Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical
University Cancer Institute and Hospital, Tianjin Medical University, Tianjin, China
Jianxin Li,
Department of Ultrasonography, Weihai Municipal Hospital, Shandong, China
Department of Diagnostic and Therapeutic Ultrasonography, National Clinical Research Center
for Cancer, Key Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical
University Cancer Institute and Hospital, Tianjin Medical University, Tianjin, China
Yanhui Zhao,
Department of Ultrasonography, Affiliated Hospital of Chifeng University, Inner Mongolia, China
Meng Yang,
Department of Epidemiology and Biostatistics, National Clinical Research Center for Cancer, Key
Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical University Cancer
Institute and Hospital, Tianjin Medical University, Tianjin, China
Qinghua Wang,
Department of Epidemiology and Biostatistics, National Clinical Research Center for Cancer, Key
Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical University Cancer
Institute and Hospital, Tianjin Medical University, Tianjin, China
Zhiming Zheng,
Department of Ultrasonography, Integrated Traditional Chinese and Western Medicine Hospital,
Jilin, China
Xiangqian Zheng [Prof],
Department of Thyroid and Neck Cancer, National Clinical Research Center for Cancer, Key
Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical University Cancer
Institute and Hospital, Tianjin Medical University, Tianjin, China
Xiangming Yang,
Department of Ultrasonography, Dezhou Municiple Hospital, Shandong, China
Christopher T Whitlow,
Departments of Radiology and Biomedical Engineering, Wake Forest School of Medicine,
Winston-Salem, NC, USA
Metin Nafi Gurcan [Prof],
Center for Biomedical Informatics Department of Internal Medicine, Wake Forest School of
Medicine, Winston-Salem, NC, USA
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Lun Zhang [Prof],
Department of Maxillofacial and Otorhinolaryngology Oncology, National Clinical Research
Center for Cancer, Key Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical
University Cancer Institute and Hospital, Tianjin Medical University, Tianjin, China
Xudong Wang,
Department of Maxillofacial and Otorhinolaryngology Oncology, National Clinical Research
Center for Cancer, Key Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical
University Cancer Institute and Hospital, Tianjin Medical University, Tianjin, China
Boris C Pasche [Prof],
Wake Forest Baptist Comprehensive Cancer Center, Wake Forest Baptist Medical Center,
Department of Cancer Biology, Wake Forest School of Medicine, Winston-Salem, NC, USA
Ming Gao [Prof],
Department of Thyroid and Neck Cancer, National Clinical Research Center for Cancer, Key
Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical University Cancer
Institute and Hospital, Tianjin Medical University, Tianjin, China
Wei Zhang† [Prof],
Wake Forest Baptist Comprehensive Cancer Center, Wake Forest Baptist Medical Center,
Department of Cancer Biology, Wake Forest School of Medicine, Winston-Salem, NC, USA
Kexin Chen† [Prof]
Department of Epidemiology and Biostatistics, National Clinical Research Center for Cancer, Key
Laboratory of Cancer Prevention and Therapy of Tianjin, Tianjin Medical University Cancer
Institute and Hospital, Tianjin Medical University, Tianjin, China
Background—The incidence of thyroid cancer is rising steadily because of overdiagnosis and
overtreatment conferred by widespread use of sensitive imaging techniques for screening. This
overall incidence growth is especially driven by increased diagnosis of indolent and welldifferentiated papillary subtype and early-stage thyroid cancer, whereas the incidence of advancedstage thyroid cancer has increased marginally. Thyroid ultrasound is frequently used to diagnose
thyroid cancer. The aim of this study was to use deep convolutional neural network (DCNN)
models to improve the diagnostic accuracy of thyroid cancer by analysing sonographic imaging
data from clinical ultrasounds.
Methods—We did a retrospective, multicohort, diagnostic study using ultrasound images sets
from three hospitals in China. We developed and trained the DCNN model on the training set, 131
731 ultrasound images from 17 627 patients with thyroid cancer and 180 668 images from 25 325
controls from the thyroid imaging database at Tianjin Cancer Hospital. Clinical diagnosis of the
training set was made by 16 radiologists from Tianjin Cancer Hospital. Images from anatomical
sites that were judged as not having cancer were excluded from the training set and only
individuals with suspected thyroid cancer underwent pathological examination to confirm
diagnosis. The model’s diagnostic performance was validated in an internal validation set from
Tianjin Cancer Hospital (8606 images from 1118 patients) and two external datasets in China . All
individuals with suspected thyroid cancer after clinical examination in the validation sets had
pathological examination. We also compared the specificity and sensitivity of the DCNN model
with the performance of six skilled thyroid ultrasound radiologists on the three validation sets.
Findings—Between Jan 1, 2012, and March 28, 2018, ultrasound images for the four study
cohorts were obtained. The model achieved high performance in identifying thyroid cancer
patients in the validation sets tested, with area under the curve values of 0·947 (95% CI 0·935–
0·959) for the Tianjin internal validation set, 0·912 (95% CI 0·865–0·958) for the Jilin external
validation set, and 0·908 (95% CI 0·891–0·925) for the Weihai external validation set. The DCNN
model also showed improved performance in identifying thyroid cancer patients versus skilled
radiologists. For the Tianjin internal validation set, sensitivity was 93·4% (95% CI 89·6–96·1)
versus 96·9% (93·9–98·6; p=0·003) and specificity was 86·1% (81·1–90·2) versus 59·4% (53·0–
65·6; p<0·0001). For the Jilin external validation set, sensitivity was 84·3% (95% CI 73·6–91·9)
versus 92·9% (84·1–97·6; p=0·048) and specificity was 86·9% (95% CI 77·8–93·3) versus 57·1%
(45·9–67·9; p<0·0001). For the Weihai external validation set, sensitivity was 84·7% (95% CI
77·0–90·7) versus 89·0% (81·9–94·0; p=0·25) and specificity was 87·8% (95% CI 81·6–92·5)
versus 68·6% (60·7–75·8; p<0·0001).
Interpretation—The DCNN model showed similar sensitivity and improved specificity in
identifying patients with thyroid cancer compared with a group of skilled radiologists. The
improved technical performance of the DCNN model warrants further investigation as part of
randomised clinical trials.
Funding—The Program for Changjiang Scholars and Innovative Research Team in University in
China, and National Natural Science Foundation of China.
Introduction
The incidence of thyroid cancer has been increasing worldwide over the past two decades,
including in the USA, where a decrease in the incidence of many other cancer types has
been reported.1 Thyroid cancer is three times more prevalent in women than in men1 and is
the most frequently diagnosed type of cancer in women younger than 30 years of age in
China.2 Patients who are suspected of thyroid disease undergo ultrasound imaging, the
results of which are interpreted by a radiologist for clinical diagnosis. A key aspect of a
radiologist’s interpretation of thyroid cancer is recognition of the malignant thyroid nodule,
according to the Thyroid Imaging, Reporting and Data System (TI-RADS) guide lines. The
American College of Radiology (ACR) TI-RADS,3 European TI-RADS,4 and American
Thyroid Association guidelines5 propose multiple criteria to interpret sonographic images.
Among these criteria, solid aspect, hypoechogenicity, taller-than-wide shape, irregular
margin, extrathyroidal extension, calcification, and punctate echogenic foci are clinically
relevant features associated with suspicion of malignant disease.3–8 Patients with suspected
thyroid cancer undergo fine-needle aspiration biopsy or surgical resection, which is assessed
by pathological examination (the gold standard for diagnosis). Therefore, diagnosis of
thyroid cancer is a time-consuming and often subjective process requiring substantial
experience and expertise of radiologists.
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
There are four main subtypes of thyroid cancer: papillary, follicular, medullary, and
anaplastic.7 The 5-year relative survival of patients with thyroid cancer is 99·7%,1 but this
value varies substantially for different subtypes when stratified by stages: near 100% for
stage I and II papillary, follicular, and medullary carcinoma; 71% for stage III follicular
carcinoma, 81% for stage III medullary carcinoma, and 93% for stage III papillary
carcinoma; and 7% for anaplastic, 28% for medullary, 50% for follicular, and 51% for
papillary carcinoma at stage IV.9 All anaplastic thyroid cancers are considered stage IV.9 In
view of the good prognostic outcome of early-stage thyroid cancer, analysis of thyroid
ultrasound imaging data by an artificial intelligence algorithm with high performance could
help differentiate patients at different risk and avoid unnecessary fine-needle aspiration
biopsy or thyroidectomy for those at lower risk, particularly for those patients with papillary
carcinomas.
The widespread use of sensitive imaging methods for screening has led to a steady increase
in incidence of thyroid cancer, causing overdiagnosis and overtreatment in this setting.10,11
Indolent and well-differentiated papillary carcinomas and other early-stage thyroid cancers
are the main reasons for the growth in incidence, since the incidence of advanced-stage
thyroid cancer is rising only marginally. Mortality from thyroid cancer has decreased
slightly during the past decade.10 The frequency of estimated age-standardised
thyroidectomy has risen annually by threefold to fourfold in both sexes over the same
period.10 Therefore, development of an artificial intelligence framework based on a precise
algorithm with high sensitivity and specificity could maintain a high recall rate for patients
with thyroid cancer and identify individuals at low risk for developing advanced disease,
thus avoiding unnecessary fine-needle aspiration biopsy. Recently, deep convolutional neural
network (DCNN) models have been shown to achieve dermatologist-level classification
accuracy in skin cancer diagnosis.12 Deep learning models have also shown improved
performance compared with human experts in detection of diabetic retinopathy and eyerelated diseases from raw input pixels of retinal fundus photographs.13–15
A traditional machine-learning algorithm for diagnosis of thyroid cancer has been previously
developed,16 but it used as inputs features that were identified explicitly by human experts.
Unlike traditional machine learning, deep learning does not require engineered features
designed by human experts. Rather, deep learning takes raw image pixels and corresponding
class labels from medical imaging data as inputs and automatically learns feature
representation with a general manner.17 Learned representations can be used for
classification and object detection. In this study, we aimed to ascertain the capability of deep
learning models for automated diagnosis of thyroid cancer using real-world sonographic
data from clinical thyroid ultrasound examinations. We compared results with pathological
examination reports (the diagnostic gold standard). This study encompassed model
development with a cohort of more than 300 000 images, and validation of the model in
three validation datasets.
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Study design and participants
We did a retrospective, multicohort, diagnostic study using ultrasound images sets from
three hospitals in China. We obtained ultrasound images for the training set (312 399 images
from 42 952 patients) from the thyroid imaging database at Tianjin Cancer Hospital, Tianjin,
China. We obtained images for validation sets from thyroid imaging databases at Tianjin
Cancer Hospital (internal validation set, 8606 images from 1118 patients), the Integrated
Traditional Chinese and Western Medicine Hospital, Jilin, China (Jilin external validation
set, 741 images from 154 patients), and Weihai Municipal Hospital, Shandong, China
(Weihai external validation set, 11 039 images from 1420 patients).
We included adult patients aged 18 years or older. Clinical diagnosis of the training set was
made by 16 radiologists from Tianjin Cancer Hospital, according to TI-RADS guidelines.3–5
All patients with thyroid cancer and 5651 negative control individuals in the training set, and
all individuals in the three validation sets, underwent pathological examination. Pathological
examination reports were provided by the pathology department at Tianjin Cancer Hospital.
All ultrasound images and pathological examination reports were deidentified before they
were transferred to investigators.
This study was approved by the institutional review board (IRB) of Tianjin Cancer Hospital
and undertaken according to the Declaration of Helsinki. Informed consent from patients
with thyroid cancer and controls was exempted by the IRB because of the retrospective
nature of this study.
Procedures
All thyroid ultrasound images extracted from the thyroid imaging database at all three
hospital sites were in jpeg format. Ultrasound equipment manufactured by Philips, Toshiba,
and GE Healthcare (various models) was used to generate ultrasound images.
Image quality control was performed for the training set; we removed images from thyroid
cancer patients if the anatomical sites did not have cancer as per the pathological review
report, according to the location sign on the image. For example, if the image available was
from the left lobes of the thyroid but pathology data were for the isthmus of the thyroid, the
image was considered not suitable for training. For the validation sets, all images were
included. Sonographic images with lymph nodes were also included in both training and
validation sets.
A DCNN classification model, in which image input features (eg, image pixels) are mapped
to the corresponding output label (eg, benignity or malignancy), was used to train the deep
learning algorithm. The DCNN algorithm can learn hierarchical representations from the
input imaging data. Such a trained model can make predictions on input data. We used the
ResNet model18 with 50 layers (ResNet-50) and the Darknet model19 with 19 layers
(Darknet-19) for image classification. Layers are functional units of neural network and can
have different functions in that they learn and store abstract features of the input image. The
ResNet-50 and Darknet-19 models were first trained iteratively for classification of patients
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
with thyroid cancer (using 131 731 images) and controls (using 180 668 images). We next
combined these two deep learning models by weighting their performance (measured by
area under the curve [AUC]) and assessed the ensemble DCNN model with the internal and
external validation sets.
Darknet-19 was proposed as the backbone for the object detection algorithm19 because it is
more computationally efficient than ResNet-50 (in that Darknet-19 has fewer arithmetic
operations compared with ResNet-50) and achieved performance metrics with ImageNet
data19 that were comparable with those obtained with ResNet-50 (appendix p 4). The
weights of ResNet-50 and Darknet-19 were initialised from the same network that had been
trained to classify 1000 objects in the ImageNet dataset,20 except the last layer. The weights
of last layer were randomly initialised and the output unit was changed to two for matching
the number of classes in our study (ie, thyroid cancer vs control). We trained the network
with stochastic gradient descent running on an NVIDIA graphic processing unit (GPU) with
a GTX 1080Ti graphics card (NVIDIA, Beijing, China). We also applied on-the-fly data
augmentation12,21 for each image during training to avoid overfitting. On-the-fly
augmentation generates more training images through image processing such as random
cropping, rotation, horizontal or vertical flipping, scaling, translations, and adjustment of the
saturation and exposure, which mimic the data diversity observed in the real world, avoiding
model overfitting. Image augmentation was not done for the validation sets. Additionally, a
weight decay rate of 0·0005 was also set to additionally combat for overfitting. Weight decay
can prevent the weights of neural network from growing too large.
See Online for appendix
For more on R see www.R-project.org
To quantify the contribution of the pixels that most influence the DCNN model’s prediction,
we generated a class activation map22 by using global average pooling in the ResNet model
(appendix p 4).
To derive individual-level prediction scores, we denoted n as the total number of images
available from that patient and let Pcancer=[P1, P2, …, Pn] denote the predicted probabilities
for these n images that were classified as cancer. The score θ assigned to an individual was
defined as the average value of log-transformed Pcancer.
θ = −[ In( 1 −P1) + In( 1 −P2) + … + In( 1 −Pn)] /n
The prediction scores obtained from ResNet-50 and Darknet-19 were combined, which is
weighted by their performance—ie, the area under the receiver operating characteristic
(ROC) curve (AUC) value of ResNet-50 (AUCResNet-50) and AUC value of Darknet-19
(AUCDarknet-19).
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
θcombined = w1 × θResNet−50 + w2 × θDarknet−19
Here, w1 = AUCResNet−50/( AUCResNet−50 + AUCDarknet−19) and w2=1·0 – w1
We compared the performance of the deep learning model predictions for thyroid cancer
diagnosis with those of six skilled thyroid ultrasound radiologists (XiWe, XX, XiWa, FY, JZ,
and SZ) with at least 6 years’ experience each. We asked the radiologists to read and
interpret subsets of thyroid ultrasound imaging data randomly selected from validation sets.
We made the random selections using the random sampling function implemented in R
software (sample). The entire image subset for selected patients was shown to the
radiologists, who interpreted the images according to the guidelines of ACR TI-RADS. Each
radiologist read image subsets from two validation sets. The performance of the radiologists
was assessed by comparing their predictions with pathological reports (which are the
diagnostic gold standard).
Prediction scores derived from DCNN models were compared with pathological
examination reports of formalin-fixed and paraffin-embedded samples of suspected cancers
removed surgically, which is the gold standard for diagnosis. Pathological examination was
done to confirm diagnosis for all individuals in the training set with thyroid nodules
displaying malignant characteristics at clinical examination (17 627 [76%] of 23 278
individuals). The remaining 19 674 individuals were used as negative controls. All
individuals in the three validation sets had pathological examination results. Pathological
assessment was done by board-certified pathologists at individual sites according to WHO
Classification of Tumors of Endocrine Organs. All pathological assessments were based on
haematoxylin and eosin-stained whole-slide images.
Statistical analysis
For classification purposes, we used the ROC curve to show the diagnostic ability of the
deep learning model in discriminating thyroid cancer patients from controls. The ROC curve
was created by plotting the true positive rate (sensitivity) against the false positive rate (1 –
sensitivity), by varying the predicted probability threshold, and we calculated AUC values.
We calculated 95% CIs for sensitivity and specificity with the Clopper-Pearson method.23
Sensitivity was calculated as the fraction of patients with cancer who were correctly
identified, and specificity was calculated as the fraction of patients without thyroid cancer
who were correctly identified. We calculated AUC values, accuracy, sensitivity, and
specificity using R software caret (version 6.0–78) and GenBinomApps (version 1.0–2). The
ROC curve was plotted by R software pROC (version 1.10.0).
We also calculated likelihood ratios for positive and negative results. We calculated the
likelihood ratio for positive results as sensitivity divided by 1 – specificity and the likelihood
ratio for negative results as 1–sensitivity divided by specificity. The confusion matrix in our
study is a 2 × 2 contingency table that reports the number of true positives, false positives,
false negatives, and true negatives. We used the average accuracy, sensitivity, and specificity
of the radiologists when comparing performance between the deep learning model and the
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
radiologists. The inter-radiologist agreement rate and Fleiss’ kappa value24 were calculated
for each validation set using R software irr (version 0.84). We used the binomial test to
statistically evaluate the difference in accuracy, sensitivity, and specificity between the deep
learning model and the radiologists. Statistical analyses were done with R software images that did not match with pathological reports in
terms of anatomical locations were removed from this set. The complete training set
consisted of 312 399 images from 42 952 individuals: 17 627 patients with thyroid cancer
(131 731 images) and 25 325 controls (180 668 images).
Between Jan 1, 2018, and Mar 28, 2018, 8606 images from 1118 individuals for the internal
validation set were obtained from Tianjin Cancer Hospital. Between Apr 1, 2016, and Feb
28, 2018, 741 images from 154 individuals for the first external validation set were obtained
from Integrated Traditional Chinese and Western Medicine Hospital (Jilin set). Between Jan
1, 2016, and Dec 29, 2017, 11 039 images from 1420 individuals for the second external
validation set were obtained from Weihai Municipal Hospital (Weihai set). Baseline
characteristics of the training set and three validation sets are shown in table 1.
Clinicopathological information related to tumour subtype and tumour size are provided in
the appendix (p 1).
A flowchart depicting processes during the study is shown in figure 1. The model achieved
high performance in identifying thyroid cancer patients in the validation sets tested (table 2),
with AUC values of 0·947 (95% CI 0·935–0·959) for the Tianjin internal validation set,
0·912 (0·865–0·958) for the Jilin external validation set, and 0·908 (0·891–0·925) for the
Weihai external validation set (figure 2). Likelihood ratios for positive and negative
diagnostic results were, respectively, 6·40 (95% CI 5·27–7·96) and 0·09 (0·07–0·12) for the
internal validation set, 6·43 (3·92–12·77) and 0·18 (0·09–0·29) for the Jilin set, and 6·74
(5·68–8·14) and 0·18 (0·14–0·21) for the Weihai set. The appendix (p 1) shows exemplified
class activation maps that identify the pixels on which the ResNet-50 model was fixating its
attention for prediction. Confusion matrices reporting the number of true-positive, falsepositive, false-negative, and true-negative results for ResNet-50, Darknet-19, and the
ensemble DCNN model are shown in the appendix (p 2).
500 (45%) of 1118 individuals from the Tianjin internal validation set (3734 [43%] of 8606
images), 274 (19%) of 1420 individuals from the Weihai external validation set (2233 [16%]
of 13 949 images), and all 154 (100%) individuals from the Jilin external validation set were selected, and these images were used to assess the performance of the
ensemble DCNN model versus the group of six skilled thyroid ultrasound radiologists (table
3). Radiologist 1 read 4483 images (n=654 individuals), radiologist 2 read 5967 images
(n=774), radiologist 3 read 3734 images (n=500), radiologist 4 read 2982 images (n=482),
radiologist 5 read 741 images (n=154), and radiologist 6 read 2233 images (n=274). The
entire image set for every selected patient was shown to and read by the radiologists.
Radiologists’ manual inter pretation results were aggregated and the classification accuracy,
sensitivity, and specificity were calculated and compared with that of deep learning models.
Among the radiologists, for the Tianjin internal validation set, accuracy ranged from 78·0%
(95% CI 74·1–81·6; 390 of 500 individuals) to 79·6% (75·8–83·0; 398 of 500 individuals),
sensitivity ranged from 94·1% (90·5–96·7; 241 of 256 individuals) to 98·4% (96·0–99·6; 252
of 256 individuals), and specificity from 57·0% (50·5–63·3; 139 of 244 individuals) to
62·3% (55·9–68·4; 152 of 244 individuals). For the Jilin external validation set, accuracy
ranged from 70·8% (95% CI 62·9–77·8; 109 of 154 individuals) to 74·7% (67·0–81·3; 115 of
154 individuals), sensitivity from 85·7% (75·3–92·9; 60 of 70 individuals) to 97·1% (90·1–
99·7; 68 of 70 individuals), and specificity from 51·2% (40·0–62·3; 43 of 84 individuals) to
63·1% (51·9–73·4; 53 of 84 individuals). For the Weihai external validation set, accuracy
ranged from 72·6% (66·9–77·8; 199 of 274 individuals) to 81·8% (76·7–86·1; 223 of 274
individuals), sensitivity from 85·6% (77·9–91·4; 101 of 118 individuals) to 94·1% (88·2–
97·6; 111 of 118 individuals), and specificity from 62·2% (54·1–69·8; 97 of 156 individuals)
to 78·8% (71·6–85·0; 123 of 156 individuals). The inter-radiologist agreement rate was
86·4% (95% CI 83·1–89·3; 432 of 500 individuals; Fleiss’ Kappa 0·79) in the Tianjin
internal validation set, 76·6% (69·1–83·1; 118 of 154 individuals; Fleiss’ Kappa 0·65) in the
Jilin external validation set, and 69·7% (63·9–75·1; 191 of 274 individuals; Fleiss’ Kappa
0·59) in the Weihai external validation set.
Compared with the skilled radiologists, the ensemble DCNN model achieved high
performance in identifying thyroid cancer patients. For the Tianjin internal validation set,
accuracy was 89·8% (95% CI 86·8–92·3; 994 of 1118 individuals) with the DCNN model
versus 78·8% (75·0–82·3; 394 of 500 individuals; p<0·0001) with the radiologists, sensitivity
was 93·4% (95% CI 89·6–96·1; 519 of 563 individuals) versus 96·9% (93·9–98·6; 248 of
256 individuals; p=0·003), and specificity was 86·1% (95% CI 81·1–90·2; 475 of 555
individuals) versus 59·4% (53·0–65·6; 145 of 244 individuals; p<0·0001). For the Jilin
external validation set, accuracy was 85·7% (95% CI 79·2–90·8; 132 of 154 individuals)
versus 72·7% (65·0–79·6%; 112 of 154 individuals; p<0·0001), sensitivity was 84·3% (95%
CI 73·6–91·9%; 59 of 70 individuals) versus 92·9% (84·1–97·6; 65 of 70 individuals;
p=0·048), and specificity was 86·9% (95% CI 77·8–93·3; 73 of 84 individuals) versus 57·1%
(45·9–67·9%; 48 of 84 individuals; p<0·0001). For the Weihai external validation set,
accuracy was 86·5% (95% CI 81·9–90·3; 1225 of 1420 individuals) versus 77·4% (72·0–
82·2; 212 of 274 individuals; p<0·0001), sensitivity was 84·7% (95% CI 77·0–90·7; 460 of
542 individuals) versus 89·0% (81·9–94·0%; 105 of 118 individuals; p=0·25), and specificity
was 87·8% (95% CI 81·6–92·5; 765 of 878 individuals) versus 68·6% (60·7–75·8; 107 of
156 individuals; p<0·0001). At the same specificity as the group of radiologists, the
ensemble DCNN model had higher or at least comparable sensitivity and specificity across
these three validation sets (figure 2, table 3). Additionally, the ensemble DCNN model had
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
higher kappa coefficient, positive predictive value, and F1 score compared with the
performance of the radiologists (table 3). Classification confusion matrices reporting the
number of true-positive, false-positive, false-negative, and true-negative results achieved by
the group of skilled ultrasound radiologists, the ResNet-50 model, the Darknet-19 model,
and the ensemble DCNN model are provided in the appendix (pp 3, 4).
Discussion
The findings of our retrospective study show that our DCNN model tested in three validation
sets can achieve high accuracy, sensitivity, and specificity in automated thyroid cancer
diagnosis in a real-world setting. The developed artificial intelligence system had
significantly higher accuracy and specificity in classifying thyroid cancer patients compared
with a group of skilled radiologists. The thyroid ultrasound images used in our study were
produced by several different types of ultrasound equipment, which contributed to increased
data diversity to train the algorithm and test interpretation subjectivity from radiologists.
Thyroid cancer diagnosis requires accurate recognition of malignant thyroid nodules.
However, thyroid nodules are characterised by heterogeneous appearances and vague
boundaries, leading to difficulties in accurate recognition and consistent interpretation of
malignant nodules by radiologists, as shown by varying agreement rates between
radiologists in the validation sets. Deep learning has advantages in overcoming the problem
of heterogeneity, because feature representation learned from thyroid ultrasound images is
not limited by engineered features used by radiologists. Instead, the DCNN model learned
feature representations with an automated procedure. Interpretation of thyroid cancer by
deep learning maintains consistency and, therefore, diagnostic reproducibility. Another
benefit offered by our artificial intelligence system is that it could report results instantly on
a graphical processing unit, and integration of the system into ultrasound equipment could
help radiologists accelerate the interpretation process. Integration of this system into a
portable ultrasound machine could enable flexible monitoring of disease development and
progression and, thus, augment the capability of radiologists to manage individuals who are
at high risk of thyroid cancer. Conferred by the high speed of a GPU, the developed DCNN
model has the advantage to assess all images of a lesion, whereas a radiologist sometimes
cannot do so because ultrasound image interpretation is labour-intensive. Implementation of
the DCNN model could lead to a reduction in overdiagnosis and overtreatment related to
thyroid cancer. However, the applicability of this proposed integration system needs to be
tested in prospective clinical studies.
To the best of our knowledge, our study included the largest number of images so far for
development and validation of a deep learning model. All patients in the validation sets
underwent thyroid surgery and pathological examination, whereas some controls in the
training set did not have surgery or a pathology report. The performance of the deep learning
model is presumably lower in the validation sets because they were enriched for nodules
with more typical features of malignancy and, thus, were more difficult to differentiate. The
improvement in accuracy and specificity reported with the DCNN model might lead to a
reduction in unnecessary fine-needle aspiration biopsy procedures. However, clinical
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
diagnostic validity needs to be assessed in future randomised clinical trials against current
standard procedures.
The trained DCNN model could correctly pinpoint malignant thyroid nodules in a weakly
supervised manner through class activation map analysis. DCNN models and machine
learning approaches based on conventional feature extraction have previously been
investigated for discrimination of malignancy of thyroid nodules from ultrasound images.
For example, Ma and colleagues25 used DCNN and analysed 8148 manually annotated
thyroid nodules and obtained an accuracy of 83·0% (95% CI 82·3–83·7) in thyroid nodule
diagnosis; however, data from this study are not available so we could not assess them with
our artificial intelligence system. Xia and colleagues26 achieved an accuracy of 87·7% in
differentiating malignant and benign nodules by applying extreme machine learning to
radiologist-collected features that were obtained from 203 ultrasound images of 187 patients
with thyroid cancer. Pereira and colleagues27 reported an accuracy of 83% achieved by a
DCNN model in distinguishing between malignant and benign thyroid nodules from 946
images of 165 patients, which was substantially higher than machine learning algorithms
based on conventional feature extraction. However, these studies were limited by small
sample sizes and no external validation sets. We do not know if the improvement in accuracy
we reported in our study relates to the machine learning method used or to the much larger
training dataset.
The website can be accessed at 
Our study has some limitations. We did not include training data from other hospitals, and
we did not do sensitivity analyses with respect to tumour size and subtypes of malignant
disease. 5651 (13%) of 42 952 individuals in the training set were true negatives, with the
assumption that patients who did not undergo surgery would be mainly negative diagnoses.
The performance of our artificial intelligence system is expected to increase by including
more data and expanding the sets to real-world data from other hospitals. Other limitations
were that a TI-RADS score of 5 was the only condition to score nodules as malignant, and
that in contrast to the algorithm, radiologists in our study did not analyse lymph node images
to support their diagnosis. In daily practice, a radiologist reviews approximately 300 images
(from about 30 individuals) under time constraints. In our study, radiologists were asked to
review images without time constraints; thus, the specificity of this group of skilled
radiologists is expected to decrease in daily practice. The features of benign nodules or
normal thyroid are less heterogeneous than are those of malignant nodules. Although thyroid
cancer subtypes with low incidence—such as follicular thyroid cancer—were not well
represented in our training set, the hierarchical features learned from papillary carcinoma
should be generalisable to other subtypes since features of thyroid nodules from images of
papillary carcinoma are shared with those from follicular carcinoma. Because the algorithm
was trained only with images from anatomical sites that did have cancer, and the probability
of cancer was calculated by averaging logarithmic transformation of one minus probabilities
from each image, the algorithm could report a lower score in a clinical trial, when noncancer site images would not be removed, leading to decreased sensitivity.
Lancet Oncol. Author manuscript; available in PMC 2020 March 20.
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Factors that limit generalisability of the DCNN model relate mainly to an absence of
multicentre training cohorts and removal of images from anatomical sites of cancer patients
who have no tumours. Additionally, most patients in the cohorts are northern Han Chinese.
Future multicentre investigations should mitigate this limiting factor and improve
generalisability. The current artificial intelligence system was not able to account for other
clinical parameters; therefore, it cannot replace manual diagnosis of thyroid cancer but could
augment the ability of thyroid ultra sound radiologists in thyroid cancer diagnosis.
We are building a website to provide free access to the developed DCNN model. In our
future work, we intend to link hierarchical features of thyroid ultrasound images learned by
DCNN models to features of thyroid nodules that are mostly used by radiologists in
interpreting thyroid cancer. Medical resources in urban and rural areas of China—and in
many other countries in the world—are unbalanced; the artificial intelligence system
developed in our study could contribute to reducing barriers and providing a convenient way
for community hospitals to improve thyroid cancer diagnosis.
The newly developed DCNN model showed improved accuracy, sensitivity, and specificity
in identifying patients with thyroid cancer at levels similar to or higher than a group of
skilled radiologists. The improved technical performance obtained by the DCNN model
indicates that this method is valuable to proceed with and to be tested in prospective clinical
Supplementary Material
Refer to Web version on PubMed Central for supplementary material.
Acknowledgments
This study was supported by the Program for Changjiang Scholars and Innovative Research Team in University in
China (grant IRT_14R40, to KC), and the National Natural Science Foundation of China (grant 31801117 to XL).
WZ is supported by a fellowship from the National Foundation for Cancer Research and a Hanes and Wills Family
endowed professorship in cancer at the Wake Forest Baptist Comprehensive Cancer Center. BCP and WZ are
supported by the Cancer Center Support Grant from the National Cancer Institute to the Comprehensive Cancer
Center of Wake Forest Baptist Medical Center (P30 CA012197).