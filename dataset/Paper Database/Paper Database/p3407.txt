Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1723–1732,
Beijing, China, July 26-31, 2015. c⃝2015 Association for Computational Linguistics
Multi-Task Learning for Multiple Language Translation
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu and Haifeng Wang
Baidu Inc, Beijing, China
{dongdaxiang, wu hua, hewei06, yudianhai, wanghaifeng}@baidu.com
In this paper, we investigate the problem of
learning a machine translation model that
can simultaneously translate sentences
from one source language to multiple
target languages. Our solution is inspired
by the recently proposed neural machine
translation
generalizes
translation
learning problem. We extend the neural
translation
multi-task
learning framework which shares source
representation
the modeling of different target language
translation. Our framework can be applied
to situations where either large amounts
of parallel data or limited parallel data
is available.
Experiments show that
our multi-task learning model is able to
achieve signiﬁcantly higher translation
quality over individually learned model in
both situations on the data sets publicly
available.
Introduction
Translation from one source language to multiple
target languages at the same time is a difﬁcult task
for humans. A person often needs to be familiar
translation
language pairs.
Machine translation systems
suffer from the same problems too.
current classic statistical machine translation
framework, it is hard to share information across
different phrase tables among different language
pairs. Translation quality decreases rapidly when
the size of training corpus for some minority
language pairs becomes smaller. To conquer the
multi-task
sequence learning model to conduct machine
translation from one source language to multiple
languages,
translation(NMT)
framework proposed by Bahdanau et al. .
Speciﬁcally,
we extend the recurrent neural
network based encoder-decoder framework to a
multi-task learning model that shares an encoder
across all language pairs and utilize a different
decoder for each target language.
The neural machine translation approach has
recently achieved promising results in improving
translation quality.
Different from conventional
statistical machine translation approaches, neural
machine translation approaches aim at learning
a radically end-to-end neural network model to
optimize translation performance by generalizing
translation
Based on the neural translation
framework, the lexical sparsity problem and the
long-range dependency problem in traditional
statistical machine translation can be alleviated
through neural networks such as long shortterm memory networks which provide great
lexical generalization and long-term sequence
memorization abilities.
assumption
framework is that many languages differ lexically
but are closely related on the semantic and/or the
syntactic levels.
We explore such correlation
across different target languages and realize it
under a multi-task learning framework. We treat a
separate translation direction as a sub RNN
encode-decoder task in this framework which
shares the same encoder (i.e.
the same source
representation)
translation directions, and use a different decoder
for each speciﬁc target language. In this way, this
proposed multi-task learning model can make full
different language pairs. Since the encoder part
shares the same source language representation
across all the translation tasks, it may learn
semantic and structured predictive representations
that can not be learned with only a small amount
Moreover, during training we jointly
model the alignment and the translation process
simultaneously for different language pairs under
the same framework.
For example, when we
simultaneously
Korean and Japanese, we can jointly learn latent
similar semantic and structure information across
Korea and Japanese because these two languages
share some common language structures.
The contribution of this work is three folds.
First, we propose a uniﬁed machine learning
framework to explore the problem of translating
languages.
To the best of our knowledge, this
problem has not been studied carefully in the
statistical
translation
Second, given large-scale training corpora for
different language pairs,
we show that our
framework can improve translation quality on
each target language as compared with the neural
translation model trained on a single language
pair. Finally, our framework is able to alleviate
the data scarcity problem, using language pairs
with large-scale parallel training corpora to
improve the translation quality of those with few
parallel training corpus.
The following sections will be organized as
in section 2, related work will be
described, and in section 3, we will describe our
multi-task learning method.
Experiments that
demonstrate the effectiveness of our framework
will be described in section 4.
Lastly, we will
conclude our work in section 5.
Related Work
Statistical machine translation systems often rely
on large-scale parallel and monolingual training
corpora to generate translations of high quality.
Unfortunately,
statistical
translation
system often suffers from data sparsity problem
due to the fact that phrase tables are extracted from
the limited bilingual corpus. Much work has been
done to address the data sparsity problem such
as the pivot language approach and deep learning
techniques .
On the problem of how to translate one source
language to many target languages within one
model, few work has been done in statistical
machine translation. A related work in SMT is
the pivot language approach for statistical machine
translation which uses a commonly used language
as a ”bridge” to generate source-target translation
for language pair with few training corpus. Pivot
based statistical machine translation is crucial in
machine translation for resource-poor language
pairs, such as Spanish to Chinese. Considering
the problem of translating one source language
to many target languages, pivot based SMT
approaches does work well given a large-scale
source language to pivot language bilingual corpus
and large-scale pivot language to target languages
However, in reality, language pairs
between English and many other target languages
may not be large enough, and pivot-based SMT
sometimes fails to handle this problem.
approach handles one to many target language
translation in a different way that we directly learn
an end to multi-end translation system that does
not need a pivot language based on the idea of
neural machine translation.
Neural Machine translation is a emerging
translation,
by several work recently , aiming at end-to-end machine
translation without phrase table extraction and
traditional statistical machine translation, neural
machine translation encodes a variable-length
source sentence with a recurrent neural network
into a ﬁxed-length vector representation and
decodes it with another recurrent neural network
from a ﬁxed-length vector into variable-length
target sentence.
A typical model is the RNN
encoder-decoder approach proposed by Bahdanau
et al. ,
which utilizes a bidirectional
recurrent neural network to compress the source
sentence information and ﬁts the conditional
probability of words in target languages with
a recurrent manner.
Moreover, soft alignment
parameters are considered in this model.
speciﬁc example model in this paper, we adopt a
RNN encoder-decoder neural machine translation
model for multi-task learning, though all neural
network based model can be adapted in our
framework.
In the natural language processing ﬁeld, a
notable work related with multi-task learning
was proposed by Collobert et al. which
shared common representation for input words
and solve different traditional NLP tasks such as
part-of-Speech tagging, name entity recognition
and semantic role labeling within one framework,
where the convolutional neural network model
Hatori et al. proposed to
jointly train word segmentation, POS tagging and
dependency parsing, which can also be seen as
a multi-task learning approach. Similar idea has
also been proposed by Li et al. in Chinese
dependency parsing. Most of multi-task learning
or joint training frameworks can be summarized
as parameter sharing approaches proposed by
Ando and Zhang where they jointly trained
models and shared center parameters in NLP
Researchers have also explored similar
approaches 
in statistical machine translation which are often
refered as domain adaption. Our work explores the
possibility of machine translation under the multitask framework by using the recurrent neural
networks. To the best of our knowledge, this is the
ﬁrst trial of end to end machine translation under
multi-task learning framework.
Multi-task Model for Multiple
Language Translation
Our model is a general framework for translating
from one source language to many targets. The
model we build in this section is a recurrent
neural network based encoder-decoder model with
multiple target tasks, and each task is a speciﬁc
translation direction.
Different tasks share the
same translation encoder across different language
We will describe model details in this
Objective Function
Given a pair of training sentence {x, y}, a
encoder-decoder machine translation model ﬁts a
parameterized model to maximize the conditional
probability of a target sentence y given a source
sentence x , i.e., argmax p(y|x). We extend this
into multiple languages setting.
In particular,
suppose we want to translate from English to
languages,
French(Fr), Dutch(Nl), Spanish(Es).
training data will be collected before training, i.e.
En-Fr, En-Nl, En-Es parallel sentences. Since the
English representation of the three language pairs
is shared in one encoder, the objective function
conditional probability terms conditioned on
representation generated from the same encoder.
L(Θ) = argmax
log p(yiTp|xiTp; Θ))
where Θ = {Θsrc, ΘtrgTp, Tp = 1, 2, · · · , Tm},
Θsrc is a collection of parameters for source
And ΘtrgTp
is the parameter set
of the Tpth target language.
Np is the size
of parallel training corpus of the pth language
For different target languages, the target
encoder parameters are seperated so we have Tm
decoders to optimize.
This parameter sharing
strategy makes different language pairs maintain
the same semantic and structure information of the
source language and learn to translate into target
languages in different decoders.
Model Details
(xTp, yTp) where Tp denotes the index of the Tpth
language pair. For a speciﬁc language pair, given
2 , · · · , xTp
maximize the conditional probability for each
generated target word.
The probability of
generating the tth target word is estimated as:
1 , · · · , yTp
t−1, xTp) = g(yTp
where the function g is parameterized by a
feedforward neural network with a softmax output
And g can be viewed as a probability
predictor with neural networks. sTp
is a recurrent
neural network hidden state at time t, which can
be estimated as:
the context vector cTp
depends on a sequence of
annotations (h1, · · · , hLx) to which an encoder
maps the input sentence,
number of tokens in x.
Each annotation hi
is a bidirectional recurrent representation with
forward and backward sequence information
around the ith word.
where the weight aTp
tj is a scalar computed by
k=1 exp(eTp
tj = φ(st−1Tp, hj)
tj is a normalized score of etj which is a soft
alignment model measuring how well the input
context around the jth word and the output word
in the tth position match. etj is modeled through a
perceptron-like function:
φ(x, y) = vT tanh(Wx + Uy)
To compute hj, a bidirectional recurrent neural
network is used.
In the bidirectional recurrent
neural network, the representation of a forward
sequence and a backward sequence of the input
sentence is estimated and concatenated to be a
single vector.
This concatenated vector can be
used to translate multiple languages during the test
From a probabilistic perspective, our model is
able to learn the conditional distribution of several
target languages given the same source corpus.
Thus, the recurrent encoder-decoders are jointly
trained with several conditional probabilities
added together. As for the bidirectional recurrent
neural network module, we adopt the recently
proposed gated recurrent neural network (Cho
The gated recurrent neural
network is shown to have promising results in
several sequence learning problem such as speech
recognition and machine translation where input
and output sequences are of variable length.
is also shown that the gated recurrent neural
network has the ability to address the gradient
vanishing problem compared with the traditional
recurrent neural network, and thus the long-range
dependency problem in machine translation can
be handled well.
In our multi-task learning
framework, the parameters of the gated recurrent
neural network in the encoder are shared, which is
formulated as follows.
ht = (I −zt) ⊙ht−1 + zt ⊙ˆht
zt = σ(Wzxt + Uzht−1)
ˆht = tanh(Wxt + U(rt ⊙ht−1))
rt = σ(Wrxt + Urht−1)
Where I is identity vector and ⊙denotes element
wise product between vectors. tanh(x) and σ(x)
are nonlinear transformation functions that can be
applied element-wise on vectors. The recurrent
computation procedure is illustrated in 1, where
xt denotes one-hot vector for the tth word in a
computation, where rt is a reset gate responsible
for memory unit elimination, and zt can be viewed
as a soft weight between current state information
and history information.
tanh(x) = ex −e−x
The overall model is illustrated in Figure 2
where the multi-task learning framework with
four target languages is demonstrated.
soft alignment parameters Ai for each encoderdecoder are different and only the bidirectional
recurrent neural network representation is shared.
Optimization
optimization
mini-batch stochastic gradient descent approach
 . The only difference between our
optimization and the commonly used stochastic
gradient descent is that we learn several minibatches within a ﬁxed language pair for several
mini-batch iterations and then move onto the next
language pair.
Our optimization procedure is
shown in Figure 3.
Figure 2: Multi-task learning framework for multiple-target language translation
Figure 3: Optimization for end to multi-end model
Translation with Beam Search
Although parallel corpora are available for the
encoder and the decoder modeling in the training
phrase, the ground truth is not available during test
time. During test time, translation is produced by
ﬁnding the most likely sequence via beam search.
ˆY = argmax
Y p(YTp|STp)
Given the target direction we want to translate to,
beam search is performed with the shared encoder
and a speciﬁc target decoder where search space
belongs to the decoder Tp. We adopt beam search
algorithm similar as it is used in SMT system
 except that we only utilize scores
produced by each decoder as features. The size
of beam is 10 in our experiments for speedup
consideration. Beam search is ended until the endof-sentence eos symbol is generated.
Experiments
We conducted two groups of experiments to
show the effectiveness of our framework.
goal of the ﬁrst experiment is to show that
multi-task learning helps to improve translation
performance given enough training corpora for all
language pairs.
In the second experiment, we
show that for some resource-poor language pairs
with a few parallel training data, their translation
performance could be improved as well.
The Europarl corpus is a multi-lingual corpus
including 21 European languages. Here we only
choose four language pairs for our experiments.
The source language is English for all language
And the target languages are Spanish
(Es), French (Fr), Portuguese (Pt) and Dutch
To demonstrate the validity of our
learning framework, we do some preprocessing
on the training set.
For the source language,
we use 30k of the most frequent words for
source language vocabulary which is shared
across different language pairs and 30k most
frequent words for each target language.
Outof-vocabulary words are denoted as unknown
words, and we maintain different unknown word
labels for different languages.
For test sets,
we also restrict all words in the test set to
be from our training vocabulary and mark the
OOV words as the corresponding labels as in
the training data. The size of training corpus in
experiment 1 and 2 is listed in Table 1 where
Training Data Information
Src tokens
49,158,635
50,263,003
49,533,217
49,283,373
Trg tokens
51,622,215
52,525,000
50,661,711
54,996,139
Table 1: Size of training corpus for different language pairs
En-Nl-sub and En-Pt-sub are sub-sampled data
set of the full corpus. The full parallel training
corpus is available from the EuroParl corpus,
downloaded from EuroParl public websites1. We
mimic the situation that there are only a smallscale parallel corpus available for some language
pairs by randomly sub-sampling the training data.
The parallel corpus of English-Portuguese and
English-Dutch are sub-sampled to approximately
15% of the full corpus size. We select two data
Language pair
Common test
Table 2: Size of test set in EuroParl Common
testset and WMT2013
sets as our test data. One is the EuroParl Common
test set2 in European Parliament Corpus, the other
is WMT 2013 data set3. For WMT 2013, only
En-Fr, En-Es are available and we evaluate the
translation performance only on these two test
sets. Information of test sets is shown in Table 2.
Training Details
Our model is trained on Graphic Processing Unit
K40. Our implementation is based on the open
source deep learning package Theano so that we do not need to take care
about gradient computations. During training, we
randomly shufﬂe our parallel training corpus for
each language pair at each epoch of our learning
process. The optimization algorithm and model
hyper parameters are listed below.
• Initialization of all parameters are from
uniform distribution between -0.01 and 0.01.
• We use stochastic gradient descent with
strategy Ada-Delta .
1http:www.statmt.orgeuroparl
2 
3 sets
• Mini batch size in our model is set to 50 so
that the convergence speed is fast.
• We train 1000 mini batches of data in one
language pair before we switch to the next
language pair.
• For word representation dimensionality, we
use 1000 for both source language and target
• The size of hidden layer is set to 1000.
We trained our multi-task model with a multi-
GPU implementation due to the limitation of
Graphic memory.
And each target decoder is
trained within one GPU card, and we synchronize
our source encoder every 1000 batches among all
GPU card. Our model costs about 72 hours on full
large parallel corpora training until convergence
and about 24 hours on partial parallel corpora
training. During decoding, our implementation on
GPU costs about 0.5 second per sentence.
Evaluation
We evaluate the effectiveness of our method with
EuroParl Common testset and WMT 2013 dataset.
BLEU-4 is used as the
evaluation metric. We evaluate BLEU scores on
EuroParl Common test set with multi-task NMT
models and single NMT models to demonstrate
the validity of our multi-task learning framework.
On the WMT 2013 data sets,
we compare
performance of separately trained NMT models,
multi-task NMT models and Moses. We use the
EuroParl Common test set as a development set in
both neural machine translation experiments and
Moses experiments. For single NMT models and
multi-task NMT models, we select the best model
with the highest BLEU score in the EuroParl
Common testset and apply it to the WMT 2013
dataset. Note that our experiment settings in NMT
is equivalent with Moses, considering the same
training corpus, development sets and test sets.
Experimental Results
We report our results of three experiments to
show the validity of our methods.
In the ﬁrst
experiment, we train multi-task learning model
jointly on all four parallel corpora and compare
BLEU scores with models trained separately on
each parallel corpora. In the second experiment,
we utilize the same training procedures as
Experiment 1, except that we mimic the situation
where some parallel corpora are resource-poor and
maintain only 15% data on two parallel training
In experiment 3, we test our learned
model from experiment 1 and experiment 2 on
WMT 2013 dataset.
Table 3 and 4 show the
case-insensitive BLEU scores on the Europarl
common test data. Models learned from the multitask learning framework signiﬁcantly outperform
the models trained separately.
Table 4 shows
that given only 15% of parallel training corpus
of English-Dutch and English-Portuguese, it is
possible to improve translation performance on all
the target languages as well. This result makes
sense because the correlated languages beneﬁt
from each other by sharing the same predictive
structure, e.g. French, Spanish and Portuguese, all
of which are from Latin. We also notice that even
though Dutch is from Germanic languages, it is
also possible to increase translation performance
under our multi-task learning framework which
demonstrates the generalization of our model to
multiple target languages.
Single NMT
Multi Task
Table 3: Multi-task neural translation v.s. single
model given large-scale corpus in all language
We tested our selected model on the WMT 2013
dataset. Our results are shown in Table 5 where
Multi-Full is the model with Experiment 1 setting
and the model of Multi-Partial uses the same
setting in Experiment 2.
The English-French
and English-Spanish translation performances are
improved signiﬁcantly compared with models
trained separately on each language pair.
Single NMT
Multi Task
Table 4: Multi-task neural translation v.s. single
model with a small-scale training corpus on some
language pairs. * means that the language pair is
sub-sampled.
that this result is not comparable with the result
reported in as we use
much less training corpus. We also compare our
trained models with Moses. On the WMT 2013
data set, we utilize parallel corpora for Moses
training without any extra resource such as largescale monolingual corpus.
From Table 5, it is
shown that neural machine translation models
have comparable BLEU scores with Moses. On
the WMT 2013 test set, multi-task learning model
outperforms both single model and Moses results
signiﬁcantly.
Model Analysis and Discussion
We try to make empirical analysis through
learning curves and qualitative results to explain
why multi-task learning framework works well in
multiple-target machine translation problem.
From the learning process, we observed that the
speed of model convergence under multi-task
learning is faster than models trained separately
especially when a model is trained for resourcepoor language pairs. The detailed learning curves
are shown in Figure 4.
Here we study the
learning curve for resource-poor language pairs,
i.e. English-Dutch and En-Portuguese, for which
only 15% of the bilingual data is sampled for
training. The BLEU scores are evaluated on the
Europarl common test set.
From Figure 4, it
can be seen that in the early stage of training,
given the same amount of training data for each
language pair, the translation performance of
the multi-task learning model is improved more
rapidly. And the multi-task models achieve better
translation quality than separately trained models
within three iterations of training.
The reason
of faster and better convergence in performance
is that the encoder parameters are shared across
different language pairs, which can make full use
of all the source language training data across the
language pairs and improve the source language
Nmt Baseline
Nmt Multi-Full
Nmt Multi-Partial
26.02(+2.13)
25.01(+1.12)
25.31(+2.03)
25.83(+2.55)
Table 5: Multi-task NMT v.s. single model v.s. moses on the WMT 2013 test set
Figure 4: Faster and Better convergence in Multi-task Learning in multiple language translation
representation.
The sharing of encoder parameters is useful
especially for the resource-poor language pairs.
In the multi-task learning framework, the amount
of the source language is not limited by the
resource-poor language pairs and we are able to
learn better representation for the source language.
Thus the representation of the source language
learned from the multi-task model is more stable,
and can be viewed as a constraint that leverages
translation performance of all language pairs.
Therefore, the overﬁtting problem and the data
scarcity problem can be alleviated for language
pairs with only a few training data. In Table 6,
we list the three nearest neighbors of some source
words whose similarity is computed by using
the cosine score of the embeddings both in the
multi-task learning framework (from Experiment
two ) and in the single model (the resourcepoor English-Portuguese model).
Although the
nearest neighbors of the high-frequent words such
as numbers can be learned both in the multi-task
model and the single model, the overall quality of
the nearest neighbors learned by the resource-poor
single model is much poorer compared with the
multi-task model.
The multi-task learning framework also generates
translations of higher quality. Some examples are
shown in Table 7.
The examples are from the
Nearest neighbors
deliver 0.78, providing 0.74,
terrorism 0.66, criminal 0.65,
homelessness 0.65
condense 0.74,
mutate 0.71,
evolve 0.70
eight 0.98,seven 0.96, 12 0.94
Single-Resource-Poor
Nearest Neighbors
0.67,extending
parliamentarians 0.44
care 0.75, remember 0.56, three
committing
0.30, longed-for 0.28
eight 0.87, three 0.69, thirteen
Source language nearest-neighbor comparison
between the multi-task model and the single model
WMT 2013 test set.
The French and Spanish
translations generated by the multi-task learning
model and the single model are shown in the table.
Conclusion
In this paper, we investigate the problem of how to
translate one source language into several different
target languages within a uniﬁed translation
Our proposed solution is based on the
Students, meanwhile, say the course is
one of the most interesting around.
Reference-Fr
Les ´etudiants, pour leur part, assurent
int´eressants.
´etudiants,
entre-temps,
entendu l’ une des plus int´eressantes.
Les ´etudiants, en attendant, disent qu’ il
est l’ un des sujets les plus int´eressants.
In addition,
they limited the right
of individuals and groups to provide
assistance to voters wishing to register.
Reference-Fr
De plus, ils ont limit´e le droit de
personnes et de groupes de fournir
une assistance aux ´electeurs d´esirant s’
ils limitent le droit des
particuliers et des groupes pour fournir
l’ assistance aux ´electeurs.
De plus, ils restreignent le droit des
individus et des groupes `a fournir une
assistance aux ´electeurs qui souhaitent
enregistrer.
Table 7: Translation of different target languages
given the same input in our multi-task model.
recently proposed recurrent neural network based
encoder-decoder framework. We train a uniﬁed
neural machine translation model under the multitask learning framework where the encoder is
shared across different language pairs and each
target language has a separate decoder.
best of our knowledge, the problem of learning
to translate from one source to multiple targets
has seldom been studied. Experiments show that
given large-scale parallel training data, the multitask neural machine translation model is able
to learn good predictive structures in translating
multiple targets. Signiﬁcant improvement can be
observed from our experiments on the data sets
publicly available. Moreover, our framework is
able to address the data scarcity problem of some
resource-poor language pairs by utilizing largescale parallel training corpora of other language
pairs to improve the translation quality. Our model
is efﬁcient and gets faster and better convergence
for both resource-rich and resource-poor language
pair under the multi-task learning.
In the future, we would like to extend our
learning framework to more practical setting. For
example, train a multi-task learning model with
the same target language from different domains
to improve multiple domain translation within
one model.
The correlation of different target
languages will also be considered in the future
Acknowledgement
This paper is supported by the 973 program
2014CB340505.
We would like to
thank anonymous reviewers for their insightful