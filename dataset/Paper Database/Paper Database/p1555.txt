Machine Learning, 37, 277–296 
c⃝1999 Kluwer Academic Publishers. Manufactured in The Netherlands.
Large Margin Classiﬁcation Using
the Perceptron Algorithm
YOAV FREUND
 
AT&T Labs, Shannon Laboratory, 180 Park Avenue, Room A205, Florham Park, NJ 07932-0971, USA
ROBERT E. SCHAPIRE
 
AT&T Labs, Shannon Laboratory, 180 Park Avenue, Room A279, Florham Park, NJ 07932-0971, USA
Editors: Jonathan Baxter and Nicol`o Cesa-Bianchi
We introduce and analyze a new algorithm for linear classiﬁcation which combines Rosenblatt’s perceptron algorithm with Helmbold and Warmuth’s leave-one-out method. Like Vapnik’s maximal-margin classiﬁer,
our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik’s algorithm, however, ours is much simpler to implement, and much more efﬁcient in terms of computation time. We
also show that our algorithm can be efﬁciently used in very high dimensional spaces using kernel functions. We
performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten
digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin
classiﬁers on the same problem, while saving signiﬁcantly on computation time and programming effort.
Introduction
One of the most inﬂuential developments in the theory of machine learning in the last
few years is Vapnik’s work on support vector machines (SVM) . Vapnik’s
analysis suggests the following simple method for learning complex binary classiﬁers. First,
use some ﬁxed mapping 8 to map the instances into some very high dimensional space
in which the two classes are linearly separable. Then use quadratic programming to ﬁnd
the vector that classiﬁes all the data correctly and maximizes the margin, i.e., the minimal
distance between the separating hyperplane and the instances.
There are two main contributions of his work. The ﬁrst is a proof of a new bound on the
difference between the training error and the test error of a linear classiﬁer that maximizes
the margin. The signiﬁcance of this bound is that it depends only on the size of the margin
(or the number of support vectors) and not on the dimension. It is superior to the bounds
that can be given for arbitrary consistent linear classiﬁers.
The second contribution is a method for computing the maximal-margin classiﬁer efﬁciently for some speciﬁc high dimensional mappings. This method is based on the idea of
kernel functions, which are described in detail in Section 4.
The main part of algorithms for ﬁnding the maximal-margin classiﬁer is a computation
of a solution for a large quadratic program. The constraints in the program correspond to
FREUND AND SCHAPIRE
the training examples so their number can be very large. Much of the recent practical work
on support vector machines is centered on ﬁnding efﬁcient ways of solving these quadratic
programming problems.
In this paper, we introduce a new and simpler algorithm for linear classiﬁcation which
takes advantage of data that are linearly separable with large margins. We named the new
algorithm the voted-perceptron algorithm. The algorithm is based on the well known perceptron algorithm of Rosenblatt and a transformation of online learning algorithms to batch learning algorithms developed by Helmbold and Warmuth . Moreover,
following the work of Aizerman, Braverman and Rozonoer , we show that kernel
functions can be used with our algorithm so that we can run our algorithm efﬁciently in very
high dimensional spaces. Our algorithm and its analysis involve little more than combining these three known methods. On the other hand, the resulting algorithm is very simple
and easy to implement, and the theoretical bounds on the expected generalization error
of the new algorithm are almost identical to the bounds for SVM’s given by Vapnik and
Chervonenkis in the linearly separable case.
We repeated some of the experiments performed by Cortes and Vapnik on the
use of SVM on the problem of classifying handwritten digits. We tested both the votedperceptron algorithm and a variant based on averaging rather than voting. These experiments
indicate that the use of kernel functions with the perceptron algorithm yields a dramatic
improvement in performance, both in test accuracy and in computation time. In addition, we
found that, when training time is limited, the voted-perceptron algorithm performs better
than the traditional way of using the perceptron algorithm (although all methods converge
eventually to roughly the same level of performance).
Recently, Friess, Cristianini and Campbell have experimented with a different
online learning algorithm called the adatron. This algorithm was suggested by Anlauf
and Biehl as a method for calculating the largest margin classiﬁer (also called the
“maximally stable perceptron”). They proved that their algorithm converges asymptotically
to the correct solution.
Our paper is organized as follows. In Section 2, we describe the voted perceptron algorithm. In Section 3, we derive upper bounds on the expected generalization error for
both the linearly separable and inseparable cases. In Section 4, we review the method of
kernels and describe how it is used in our algorithm. In Section 5, we summarize the results of our experiments on the handwritten digit recognition problem. We conclude with
Section 6 in which we summarize our observations on the relations between the theory and
the experiments and suggest some new open problems.
The algorithm
We assume that all instances are points x ∈Rn. We use ∥x∥to denote the Euclidean length
of x. For most of the paper, we assume that labels y are in {−1, +1}.
The basis of our study is the classical perceptron algorithm invented by Rosenblatt . This is a very simple algorithm most naturally studied in the online learning model.
The online perceptron algorithm starts with an initial zero prediction vector v = 0. It predicts
the label of a new instance x to be ˆy = sign(v · x). If this prediction differs from the label
LARGE MARGIN CLASSIFICATION USING THE PERCEPTRON ALGORITHM
y, it updates the prediction vector to v = v + yx. If the prediction is correct then v is not
changed. The process then repeats with the next example.
The most common way the perceptron algorithm is used for learning from a batch of
training examples is to run the algorithm repeatedly through the training set until it ﬁnds a
prediction vector which is correct on all of the training set. This prediction rule is then used
for predicting the labels on the test set.
Block , Novikoff and Minsky and Papert have shown that if the data
are linearly separable, then the perceptron algorithm will make a ﬁnite number of mistakes,
and therefore, if repeatedly cycled through the training set, will converge to a vector which
correctly classiﬁes all of the examples. Moreover, the number of mistakes is upper bounded
by a function of the gap between the positive and negative examples, a fact that will be
central to our analysis.
In this paper, we propose to use a more sophisticated method of applying the online
perceptron algorithm to batch learning, namely, a variation of the leave-one-out method of
Helmbold and Warmuth . In the voted-perceptron algorithm, we store more information during training and then use this elaborate information to generate better predictions
on the test data. The algorithm is detailed in ﬁgure 1. The information we maintain during
training is the list of all prediction vectors that were generated after each and every mistake.
For each such vector, we count the number of iterations it “survives” until the next mistake
is made; we refer to this count as the “weight” of the prediction vector.1 To calculate a prediction we compute the binary prediction of each one of the prediction vectors and combine
all these predictions by a weighted majority vote. The weights used are the survival times
described above. This makes intuitive sense as “good” prediction vectors tend to survive
for a long time and thus have larger weight in the majority vote.
a labeled training set ⟨(x1, y1), . . . , (xm, ym)⟩
number of epochs T
a list of weighted perceptrons ⟨(v1, c1), . . . , (vk, ck)⟩
Initialize: k := 0, v1 := 0, c1 := 0.
Repeat T times:
For i = 1, . . . , m:
Compute prediction: ˆy := sign(vk · xi)
If ˆy = y then ck := ck + 1.
else vk+1 := vk + yixi;
ck+1 := 1;
k := k + 1.
Prediction
the list of weighted perceptrons: ⟨(v1, c1), . . . , (vk, ck)⟩
an unlabeled instance: x
compute a predicted label ˆy as follows:
ci sign(vi · x);
ˆy = sign(s).
The voted-perceptron algorithm.
FREUND AND SCHAPIRE
In this section, we give an analysis of the voted-perceptron algorithm for the case T = 1 in
which the algorithm runs exactly once through the training data. We also quote a theorem
of Vapnik and Chervonenkis for the linearly separable case. This theorem bounds
the generalization error of the consistent perceptron found after the perceptron algorithm
is run to convergence. Interestingly, for the linearly separable case, the theorems yield very
similar bounds.
As we shall see in the experiments, the algorithm actually continues to improve performance after T = 1. We have no theoretical explanation for this improvement.
If the data are linearly separable, then the perceptron algorithm will eventually converge
on some consistent hypothesis (i.e., a prediction vector that is correct on all of the training
examples). As this prediction vector makes no further mistakes, it will eventually dominate
the weighted vote in the voted-perceptron algorithm. Thus, for linearly separable data, when
T →∞, the voted-perceptron algorithm converges to the regular use of the perceptron
algorithm, which is to predict using the ﬁnal prediction vector.
As we have recently learned, the performance of the ﬁnal prediction vector has been
analyzed by Vapnik and Chervonenkis . We discuss their bound at the end of this
We now give our analysis for the case T = 1. The analysis is in two parts and mostly
combines known material. First, we review the classical analysis of the online perceptron
algorithm in the linearly separable case, as well as an extension to the inseparable case. Second, we review an analysis of the leave-one-out conversion of an online learning algorithm
to a batch learning algorithm.
The online perceptron algorithm in the separable case
Our analysis is based on the following well known result ﬁrst proved by Block and
Novikoff . The signiﬁcance of this result is that the number of mistakes does not
depend on the dimension of the instances. This gives reason to believe that the perceptron
algorithm might perform well in high dimensional spaces.
Theorem 1 (Block, Novikoff).
Let ⟨(x1, y1), . . . , (xm, ym)⟩be a sequence of labeled
examples with ∥xi∥≤R. Suppose that there exists a vector u such that ∥u∥= 1 and
yi(u · xi) ≥γ for all examples in the sequence. Then the number of mistakes made by the
online perceptron algorithm on this sequence is at most (R/γ )2.
Although the proof is well known, we repeat it for completeness.
Let vk denote the prediction vector used prior to the kth mistake. Thus, v1 = 0 and, if
the kth mistake occurs on (xi, yi) then yi(vk · xi) ≤0 and vk+1 = vk + yixi.
vk+1 · u = vk · u + yi(u · xi) ≥vk · u + γ.
Therefore, vk+1 · u ≥kγ .
LARGE MARGIN CLASSIFICATION USING THE PERCEPTRON ALGORITHM
Similarly,
∥vk+1∥2 = ∥vk∥2 + 2yi(vk · xi) + ∥xi∥2 ≤∥vk∥2 + R2.
Therefore, ∥vk+1∥2 ≤kR2.
Combining, gives
kR ≥∥vk+1∥≥vk+1 · u ≥kγ
which implies k ≤(R/γ )2 proving the theorem.
Analysis for the inseparable case
If the data are not linearly separable then the Theorem 1 cannot be used directly. However,
we now give a generalized version of the theorem which allows for some mistakes in the
training set. As far as we know, this theorem is new, although the proof technique is very
similar to that of Klasner and Simon . See also the recent work of
Shawe-Taylor and Cristianini who used this technique to derive generalization error
bounds for any large margin classiﬁer.
Let⟨(x1, y1), . . . , (xm, ym)⟩beasequenceoflabeledexampleswith∥xi∥≤R.
Let u be any vector with ∥u∥= 1 and let γ > 0. Deﬁne the deviation of each example as
di = max{0, γ −yi(u · xi)},
and deﬁne D =
i . Then the number of mistakes of the online perceptron algorithm
on this sequence is bounded by
The case D = 0 follows from Theorem 1, so we can assume that D > 0.
The proof is based on a reduction of the inseparable case to a separable case in a higher
dimensional space. As we will see, the reduction does not change the algorithm.
We extend the instance space Rn to Rn+m by adding m new dimensions, one for each
example. Let x′
i ∈Rn+m denote the extension of the instance xi. We set the ﬁrst n coordinates
i equal to xi. We set the (n + i)’th coordinate to 1 where 1 is a positive real constant
whose value will be speciﬁed later. The rest of the coordinates of x′
i are set to zero.
Next we extend the comparison vector u ∈Rn to u′ ∈Rn+m. We use the constant Z,
which we calculate shortly, to ensure that the length of u′ is one. We set the ﬁrst n coordinates
of u′ equal to u/Z. We set the (n + i)’th coordinate to (yidi)/(Z1). It is easy to check that
the appropriate normalization is Z =
1 + D2/12.
FREUND AND SCHAPIRE
Consider the value of yi(u′ · x′
yi(u′ · x′
= yi(u · xi)
≥yi(u · xi)
+ γ −yi(u · xi)
Thustheextendedpredictionvectoru′ achievesamarginofγ/
1 + D2/12 ontheextended
In order to apply Theorem 1, we need a bound on the length of the instances. As R ≥∥xi∥
for all i, and the only additional non-zero coordinate has value 1, we get that ∥x′
R2 + 12. Using these values in Theorem 1 we get that the number of mistakes of the online
perceptron algorithm if run in the extended space is at most
(R2 + 12)(1 + D2/12)
Setting 1 =
RD minimizes the bound and yields the bound given in the statement of the
Toﬁnishtheproofweshowthatthepredictionsoftheperceptronalgorithmintheextended
space are equal to the predictions of the perceptron in the original space. We use vi to denote
the prediction vector used for predicting the instance xi in the original space and v′
i to denote
the prediction vector used for predicting the corresponding instance x′
i in the extended space.
The claim follows by induction over 1 ≤i ≤m of the following three claims:
1. The ﬁrst n coordinates of v′
i are equal to those of vi.
2. The (n + i)’th coordinate of v′
i is equal to zero.
3. sign(v′
i) = sign(vi · xi).
Converting online to batch
We now have an algorithm that will make few mistakes when presented with the examples
one by one. However, the setup we are interested in here is the batch setup in which we are
given a training set, according to which we generate a hypothesis, which is then tested on a
seperate test set. If the data are linearly separable then the perceptron algorithm eventually
converges and we can use this ﬁnal prediction rule as our hypothesis. However, the data
might not be separable or we might not want to wait till convergence is achieved. In this
case, we have to decide on the best prediction rule given the sequence of different classiﬁers
that the online algorithm genarates. One solution to this problem is to use the prediction
rule that has survived for the longest time before it was changed. A prediction rule that
LARGE MARGIN CLASSIFICATION USING THE PERCEPTRON ALGORITHM
has survived for a long time is likely to be better than one that has only survived for a few
iterations. This method was suggested by Gallant who called it the pocket method.
Littlestone , suggested a two-phase method in which the performance of all of the
rules is tested on a seperate test set and the rule with the least error is then used. Here we
use a different method for converting the online perceptron algorithm into a batch learning
algorithm; the method combines all of the rules generated by the online algorithm after it
was run for just a single time through the training data.
We now describe Helmbold and Warmuth’s very simple “leave-one-out” method
of converting an online learning algorithm into a batch learning algorithm. Our votedperceptron algorithm is a simple application of this general method. We start with the
randomized version. Given a training set ⟨(x1, y1), . . . , (xm, ym)⟩and an unlabeled instance
x, we do the following. We select a number r in {0, . . . , m} uniformly at random. We then
take the ﬁrst r examples in the training sequence and append the unlabeled instance to the
end of this subsequence. We run the online algorithm on this sequence of length r + 1, and
use the prediction of the online algorithm on the last unlabeled instance.
In the deterministic leave-one-out conversion, we modify the randomized leave-oneout conversion to make it deterministic in the obvious way by choosing the most likely
prediction. That is, we compute the prediction that would result for all possible choices of r
in {0, . . . , m}, and we take majority vote of these predictions. It is straightforward to show
that taking a majority vote runs the risk of doubling the probability of mistake while it has
the potential of signiﬁcantly decreasing it. In this work we decided to focus primarily on
deterministic voting rather than randomization.
The following theorem follows directly from Helmbold and Warmuth . and Cesa-Bianchi et al. .)
Theorem 3.
Assume all examples (x, y) are generated i.i.d. Let E be the expected number
of mistakes that the online algorithm A makes on a randomly generated sequence of m +
1 examples. Then given m random training examples, the expected probability that the
randomized leave-one-out conversion of A makes a mistake on a randomly generated test
instance is at most E/(m+1). For the deterministic leave-one-out conversion, this expected
probability is at most 2E/(m + 1).
Putting it all together
It can be veriﬁed that the deterministic leave-one-out conversion of the online perceptron
algorithm is exactly equivalent to the voted-perceptron algorithm of ﬁgure 1 with T = 1.
Thus, combining Theorems 2 and 3, we have:
Corollary 1.
Assume all examples are generated i.i.d. at random. Let ⟨(x1, y1), . . . ,
(xm, ym)⟩be a sequence of training examples and let (xm+1, ym+1) be a test example.
Let R = max1≤i≤m+1 ∥xi∥. For ∥u∥= 1 and γ > 0, let
(max{0, γ −yi(u · xi)})2.
FREUND AND SCHAPIRE
Then the probability (over the choice of all m + 1 examples) that the voted-perceptron
algorithm with T = 1 does not predict ym+1 on test instance xm+1 is at most
∥u∥=1;γ >0
µ R + Du,γ
(where the expectation is also over the choice of all m + 1 examples).
In fact, the same proof yields a slightly stronger statement which depends only on examples on which mistakes occur. Formally, this can be stated as follows:
Corollary 2.
Assume all examples are generated i.i.d. at random. Suppose that we run the
online perceptron algorithm once on the sequence ⟨(x1, y1), . . . , (xm+1, ym+1)⟩, and that
k mistakes occur on examples with indices i1, . . . , ik. Redeﬁne R = max1≤j≤k ∥xi j∥, and
0, γ −yi j
Now suppose that we run the voted-perceptron algorithm on training examples
⟨(x1, y1), . . . , (xm, ym)⟩for a single epoch. Then the probability (over the choice of all
m +1 examples) that the voted-perceptron algorithm does not predict ym+1 on test instance
xm+1 is at most
m + 1 E [k] ≤
∥u∥=1;γ >0
µ R + Du,γ
(where the expectation is also over the choice of all m + 1 examples).
A rather similar theorem was proved by Vapnik and Chervonenkis 
for training the perceptron algorithm to convergence and predicting with the ﬁnal perceptron
Theorem 4 (Vapnik and Chervonenkis).
Assume all examples are generated i.i.d. at
random. Suppose that we run the online perceptron algorithm on the sequence ⟨(x1, y1),
. . . , (xm+1, ym+1)⟩repeatedly until convergence, and that mistakes occur on a total of k
examples with indices i1, . . . , ik. Let R = max1≤j≤k ∥xi j∥, and let
1≤j≤k yi j
Assume γ > 0 with probability one.
LARGE MARGIN CLASSIFICATION USING THE PERCEPTRON ALGORITHM
Now suppose that we run the perceptron algorithm to convergence on training examples
⟨(x1, y1), . . . , (xm, ym)⟩. Then the probability (over the choice of all m + 1 examples) that
the ﬁnal perceptron does not predict ym+1 on test instance xm+1 is at most
(where the expectation is also over the choice of all m + 1 examples).
For the separable case (in which Du,γ can be set to zero), Corollary 2 is almost identical
to Theorem 4. One difference is that in Corolary 2, we lose a factor of 2. This is because we
use the deterministic algorithm, rather than the randomized one. The other, more important
difference is that k, the number of mistakes that the perceptron makes, will almost certainly
be larger when the perceptron is run to convergence than when it is run just for a single
epoch. This gives us some indication that running the voted-perceptron algorithm with
T = 1 might be better than running it to convergence; however, our experiments do not
support this prediction.
Vapnik (to appear) also gives a very similar bound for the expected error of support-vector
machines. There are two differences between the bounds. First, the set of vectors on which
the perceptron makes a mistake is replaced by the set of “essential support vectors.” Second,
the radius R is the maximal distance of any support vector from some optimally chosen
vector, rather than from the origin. (The support vectors are the training examples which
fall closest to the decision boundary.)
Kernel-based classiﬁcation
We have seen that the voted-perceptron algorithm has guaranteed performance bounds
when the data are (almost) linearly separable. However, linear separability is a rather strict
condition.Onewaytomakethemethodmorepowerfulisbyaddingdimensionsorfeaturesto
the input space. These new coordinates are nonlinear functions of the original coordinates.
Usually if we add enough coordinates we can make the data linearly separable. If the
separation is sufﬁciently good (in the senses of Theorems 1 and 2) then the expected
generalization error will be small (provided we do not increase the complexity of instances
too much by moving to the higher dimensional space).
However, from a computational point of view, computing the values of the additional
coordinates can become prohibitively hard. This problem can sometimes be solved by the
elegant method of kernel functions. The use of kernel functions for classiﬁcation problems
was proposed by suggested Aizerman, Braverman and Rozonoer who speciﬁcally described a method for combining kernel functions with the perceptron algorithm. Continuing
their work, Boser, Guyon and Vapnik suggested using kernel functions with SVM’s.
Kernel functions are functions of two variables K(x, y) which can be represented as an
inner product 8(x)·8(y) for some function 8 : Rn →RN and some N > 0. In other words,
we can calculate K(x, y) by mapping x and y to vectors 8(x) and 8(y) and then taking
their inner product.
FREUND AND SCHAPIRE
For instance, an important kernel function that we use in this paper is the polynomial
K(x, y) = (1 + x · y)d.
Thereexistgeneralconditionsforcheckingifafunctionisakernelfunction.Inthisparticular
case, however, it is straightforward to construct 8 witnessing that K is a kernel function.
For instance, for n = 3 and d = 2, we can choose
In general, for d > 2, we can deﬁne 8(x) to have one coordinate cM(x) for each monomial
M(x) of degree at most d over the variables x1, . . . , xn, and where c is an appropriately
chosen constant.
Aizerman, Braverman and Rozonoer observed that the perceptron algorithm can be formulated in such a way that all computations involving instances are in fact in terms of inner
products x · y between pairs of instances. Thus, if we want to map each instance x to a
vector 8(x) in a high dimensional space, we only need to be able to compute inner products
8(x) · 8(y), which is exactly what is computed by a kernel function. Conceptually, then,
with the kernel method, we can work with vectors in a very high dimensional space and
the algorithm’s performance only depends on linear separability in this expanded space.
Computationally, however, we only need to modify the algorithm by replacing each inner
product computation x·y with a kernel function computation K(x, y). Similar observations
were made by Boser, Guyon and Vapnik for Vapnik’s SVM algorithm.
In this paper, we observe that all the computations in the voted-perceptron learning
algorithm involving instances can also be written in terms of inner products, which means
that we can apply the kernel method to the voted-perceptron algorithm as well. Referring to
ﬁgure 1, we see that both training and prediction involve inner products between instances
x and prediction vectors vk. In order to perform this operation efﬁciently, we store each
prediction vector vk in an implicit form, as the sum of instances that were added or subtracted
in order to create it. That is, each vk can be written and stored as a sum
for appropriate indices i j. We can thus calculate the inner product with x as
To use a kernel function K, we would merely replace each xi j · x by K(xi j, x).
Computing the prediction of the ﬁnal vector vk on a test instance x requires k kernel
calculations where k is the number of mistakes made by the algorithm during training.
Naively, the prediction of the voted-perceptron would seem to require O(k2) kernel calculations since we need to compute v j · x for each j ≤k, and since v j itself involves a sum of
LARGE MARGIN CLASSIFICATION USING THE PERCEPTRON ALGORITHM
j −1 instances. However, taking advantage of the recurrence v j+1 · x = v j · x + yi j(xi j · x),
it is clear that we can compute the prediction of the voted-perceptron also using only k
kernel calculations.
Thus, calculating the prediction of the voted-perceptron when using kernels is only
marginally more expensive than calculating the prediction of the ﬁnal prediction vector,
assuming that both methods are trained for the same number of epochs.
Experiments
In our experiments, we followed closely the experimental setup used by Cortes and Vapnik
 in their experiments on the NIST OCR database.2 We chose to use this setup because
the dataset is widely available and because LeCun et al. have published a detailed
comparison of the performance of some of the best digit classiﬁcation systems in this
Examples in this NIST database consist of labeled digital images of individual handwritten digits. Each instance is a 28×28 matrix in which each entry is an 8-bit representation of
a grey value, and labels are from the set {0, . . . , 9}. The dataset consists of 60,000 training
examples and 10,000 test examples. We treat each image as a vector in R784, and, like Cortes
and Vapnik, we use the polynomial kernels of Eq. (1) to expand this vector into very high
dimensions.
To handle multiclass data, we essentially reduced to 10 binary problems. That is, we
trained the voted-perceptron algorithm once for each of the 10 classes. When training on
class ℓ, we replaced each labeled example (xi, yi) (where yi ∈{0, . . . , 9}) by the binarylabeled example (xi, +1) if yi = ℓand by (xi, −1) if yi ̸= ℓ. Let
1), . . . , (vℓ
be the sequence of weighted prediction vectors which result from training on class ℓ.
To make predictions on a new instance x, we tried four different methods. In each method,
we ﬁrst compute a score sℓfor each ℓ∈{0, . . . , 9} and then predict with the label receiving
the highest score:
ˆy = arg max
The ﬁrst method is to compute each score using the respective ﬁnal prediction vector:
This method is denoted “last (unnormalized)” in the results. A variant of this method is to
compute scores after ﬁrst normalizing the ﬁnal prediction vectors:
FREUND AND SCHAPIRE
This method is denoted “last (normalized)” in the results. Note that normalizing vectors has
no effect for binary problems, but can plausibly be important in the multiclass case.
The next method (denoted “vote”) uses the analog of the deterministic leave-one-out
conversion. Here we set
The third method (denoted “average (unnormalized)”) uses an average of the predictions
of the prediction vectors
As in the “last” method, we also tried a variant (denoted “average (normalized)”) using
normalized prediction vectors:
The ﬁnal method (denoted “random (unnormalized)”), is a possible analog of the randomized leave-one-out method in which we predict using the prediction vectors that exist
at a randomly chosen “time slice.” That is, let t be the number of rounds executed (i.e., the
number of examples processed by the inner loop of the algorithm) so that
for all ℓ. To classify x, we choose a “time slice” r ∈{0, . . . , t} uniformly at random. We
where rℓis the index of the ﬁnal vector which existed at time r for label ℓ. Formally, rℓis
the largest number in {0, . . . , kℓ} satisfying
The analogous normalized method (“Random (normalized)”) uses
LARGE MARGIN CLASSIFICATION USING THE PERCEPTRON ALGORITHM
Our analysis is applicable only for the cases of voted or randomly chosen predictions
and where T = 1. However, in the experiments, we ran the algorithm with T up to 30.
When using polynomial kernels of degree 5 or more, the data becomes linearly separable.
Thus, after several iterations, the perceptron algorithm converges to a consistent prediction
vector and makes no more mistakes. After this happens, the ﬁnal perceptron gains more and
more weight in both “vote” and “average.” This tends to have the effect of causing all of the
variants to converge eventually to the same solution. By reaching this limit we compare the
voted-perceptron algorithm to the standard way in which the perceptron algorithm is used,
which is to ﬁnd a consistent prediction rule.
We performed experiments with polynomial kernels for dimensions d = 1 (which corresponds to no expansion) up to d = 6. We preprocessed the data on each experiment by
randomly permuting the training sequence. Each experiment was repeated 10 times, each
time with a different random permutation of the training examples. For d = 1, we were
only able to run the experiment for ten epochs for reasons which are described below.
Figure 2 shows plots of the test error as a function of the number of epochs for four
of the prediction methods–“vote” and the unnormalized versions of “last,” “average” and
“random” (we omitted the normalized versions for the sake of readability). Test errors are
averaged over the multiple runs of the algorithm, and are plotted one point for every tenth
of an epoch.
Some of the results are also summarized numerically in Table 1 which show (average) test error for several values of T for the seven different methods in the rows marked
“Vote,” “Avg. (unnorm),” etc. The rows marked “SupVec” show the number of “support vectors,” that is, the total number of instances that actually are used in computing
scores as above. In other words, this is the size of the union of all instances on which a
Learning curves for algorithms tested on NIST data.
Results of experiments on NIST 10-class OCR data. The rows marked SupVec and Mistake give average
number of support vectors and average number of mistakes. All other rows give test error rate in percent for the
various methods.
(Continued on next page.)
FREUND AND SCHAPIRE
mistake occured during training. The rows marked “Mistake” show the total number of
mistakes made during training for the 10 different labels. In every case, we have averaged
over the multiple runs of the algorithm.
The column corresponding to T = 0.1 is helpful for getting an idea of how the algorithms
perform on smaller datasets since in this case, each algorithm has only used a tenth of the
available data (about 6000 training examples).
Ironically, the algorithm runs slowest with small values of d. For larger values of d, we
move to a much higher dimensional space in which the data becomes linearly separable. For
small values of d– especially for d = 1– the data are not linearly separable which means
that the perceptron algorithm tends to make many mistakes which slows down the algorithm
signiﬁcantly. This is why, for d = 1, we could not even complete a run out to 30 epochs but
had to stop at T = 10 (after about six days of computation). In comparison, for d = 2, we
can run 30 epochs in about 25 hours, and for d = 5 or 6, a complete run takes about 8 hours.
(All running times are on a single SGI MIPS R10000 processor running at 194 MHZ.)
The most signiﬁcant improvement in performance is clearly between d = 1 and d = 2.
The migration to a higher dimensional space makes a tremendous difference compared to
running the algorithm in the given space. The improvements for d > 2 are not nearly as
Our results indicate that voting and averaging perform better than using the last vector.
This is especially true prior to convergence of the perceptron updates. For d = 1, the data
are highly inseparable, so in this case the improvement persists for as long as we were able
to run the algorithm. For higher dimensions (d > 1), the data becomes more separable and
the perceptron update rule converges (or almost converges), in which case the performance
of all the prediction methods is very similar. Still, even in this case, there is an advantage
to using voting or averaging for a relatively small number of epochs.
There does not seem to be any signiﬁcant difference between voting and averaging in
terms of performance. However, using random vectors performs the worst in all cases. This
stands in contrast to our analysis, which applies only to random vectors and gives an upper
bound on the error of average vectors which is twice the error of the randomized vectors.
A more reﬁned analysis of the effect of averaging is required to better explain the observed
Using normalized vectors seems to sometimes help a bit for the “last” method, but can
help or hurt performance slightly for the “average” method; in any case, the differences in
performance between using normalized and unnormalized vectors are always minor.
LeCun et al. give a detailed comparison of algorithms on this dataset. The best of
the algorithms that they tested is (a rather old version of) boosting on top of the neural net
LeNet 4 which achieves an error rate of 0.7%. A version of the optimal margin classiﬁer
algorithm , using the same kernel function, performs signiﬁcantly
better than ours, achieving a test error rate of 1.1% for d = 4.
Table 2 shows how the variants of the perceptron algorithm perform on the ten binary
problems corresponding to the 10 class labels. For this table, we ﬁx d = 4, and we also
compare performance to that reported by Cortes and Vapnik for SVM’s. Table 3
gives more details of how the perceptron methods perform on the single binary problem
of distinguishing “9” from all other images. Note that these binary problems come closest
Results of experiments on individual classes using polynomial kernels with d = 4. The rows marked
SupVec and Mistake give average number of support vectors and average number of mistakes. All other rows give
test error rate in percent for the various methods.
Cortes & Vapnik
Results of experiments on NIST data when distinguishing “9” from all other digits. The rows marked
SupVec and Mistake give average number of support vectors and average number of mistakes. All other rows give
test error rate in percent for the various methods.
(Continued on next page.)
LARGE MARGIN CLASSIFICATION USING THE PERCEPTRON ALGORITHM
(Continued).
to the theory discussed earlier in the paper. It is interesting that the perceptron algorithm
generally ends up using fewer support vectors than with the SVM algorithm.
Conclusions and summary
The most signiﬁcant result of our experiments is that running the perceptron algorithm in
a higher dimensional space using kernel functions produces very signiﬁcant improvements
in performance, yielding accuracy levels that are comparable, though still inferior, to those
obtainable with support-vector machines. On the other hand, our algorithm is much faster
and easier to implement than the latter method. In addition, the theoretical analysis of the
expected error of the perceptron algorithm yields very similar bounds to those of supportvector machines. It is an open problem to develop a better theoretical understanding of the
empirical superiority of support-vector machines.
We also ﬁnd it signiﬁcant that voting and averaging work better than just using the ﬁnal
hypothesis. This indicates that the theoretical analysis, which suggests using voting, is
capturing some of the truth. On the other hand, we do not have a theoretical explanation for
the improvement in performance following the ﬁrst epoch.
FREUND AND SCHAPIRE
Acknowledgments
We thank Vladimir Vapnik for some helpful discussions and for pointing us to Theorem 4.
1. Storing all of these vectors might seem an excessive waste of memory. However, as we shall see, when perceptrons are used together with kernels, the excess in memory and computation is really quite minimal.
2. National Institute for Standards and Technology, Special Database 3. See 
ocr/ for information on obtaining this dataset and for a list of relevant publications.