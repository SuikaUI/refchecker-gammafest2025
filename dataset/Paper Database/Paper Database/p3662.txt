TabNet: Attentive Interpretable Tabular Learning
Sercan ¨O. Arık, Tomas Pﬁster
Google Cloud AI
Sunnyvale, CA
 , 
We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet
uses sequential attention to choose which features to reason
from at each decision step, enabling interpretability and more
efﬁcient learning as the learning capacity is used for the most
salient features. We demonstrate that TabNet outperforms
other variants on a wide range of non-performance-saturated
tabular datasets and yields interpretable feature attributions
plus insights into its global behavior. Finally, we demonstrate
self-supervised learning for tabular data, signiﬁcantly improving performance when unlabeled data is abundant.
Introduction
Deep neural networks (DNNs) have shown notable success
with images , text and audio
 . For these, canonical architectures that
efﬁciently encode the raw data into meaningful representations, fuel the rapid progress. One data type that has yet to
see such success with a canonical architecture is tabular data.
Despite being the most common data type in real-world AI
(as it is comprised of any categorical and numerical features),
 , deep learning for tabular data remains
under-explored, with variants of ensemble decision trees
(DTs) still dominating most applications .
Why? First, because DT-based approaches have certain bene-
ﬁts: (i) they are representionally efﬁcient for decision manifolds with approximately hyperplane boundaries which are
common in tabular data; and (ii) they are highly interpretable
in their basic form (e.g. by tracking decision nodes) and there
are popular post-hoc explainability methods for their ensemble form, e.g. – this is an
important concern in many real-world applications; (iii) they
are fast to train. Second, because previously-proposed DNN
architectures are not well-suited for tabular data: e.g. stacked
convolutional layers or multi-layer perceptrons (MLPs) are
vastly overparametrized – the lack of appropriate inductive
bias often causes them to fail to ﬁnd optimal solutions for tabular decision manifolds .
Why is deep learning worth exploring for tabular data?
One obvious motivation is expected performance improve-
Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
ments particularly for large datasets . In
addition, unlike tree learning, DNNs enable gradient descentbased end-to-end learning for tabular data which can have a
multitude of beneﬁts: (i) efﬁciently encoding multiple data
types like images along with tabular data; (ii) alleviating the
need for feature engineering, which is currently a key aspect
in tree-based tabular data learning methods; (iii) learning
from streaming data and perhaps most importantly (iv) endto-end models allow representation learning which enables
many valuable application scenarios including data-efﬁcient
domain adaptation ,
generative modeling and
semi-supervised learning .
We propose a new canonical DNN architecture for tabular
data, TabNet. The main contributions are summarized as:
1. TabNet inputs raw tabular data without any preprocessing
and is trained using gradient descent-based optimization,
enabling ﬂexible integration into end-to-end learning.
2. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and better learning as the learning capacity
is used for the most salient features (see Fig. 1). This
feature selection is instance-wise, e.g. it can be different
for each input, and unlike other instance-wise feature selection methods like or , TabNet employs a single deep
learning architecture for feature selection and reasoning.
3. Above design choices lead to two valuable properties: (i)
TabNet outperforms or is on par with other tabular learning models on various datasets for classiﬁcation and regression problems from different domains; and (ii) TabNet
enables two kinds of interpretability: local interpretability
that visualizes the importance of features and how they
are combined, and global interpretability which quantiﬁes
the contribution of each feature to the trained model.
4. Finally, for the ﬁrst time for tabular data, we show significant performance improvements by using unsupervised
pre-training to predict masked features (see Fig. 2).
Related Work
Feature selection: Feature selection broadly refers to judiciously picking a subset of features based on their usefulness for prediction. Commonly-used techniques such as forarXiv:1908.07442v5 [cs.LG] 9 Dec 2020
Feature selection
Input processing
Aggregate information
Feature selection
Input processing
Predicted output (whether the income level >$50k)
Professional occupation related
Investment related
Input features
Feedback from
previous step
Feedback to
Figure 1: TabNet’s sparse feature selection exempliﬁed for Adult Census Income prediction . Sparse feature
selection enables interpretability and better learning as the capacity is used for the most salient features. TabNet employs multiple
decision blocks that focus on processing a subset of input features for reasoning. Two decision blocks shown as examples process
features that are related to professional occupation and investments, respectively, in order to predict the income level.
Occupation
Relationship
Exec-managerial
Farming-fishing
Prof-specialty
Handlers-cleaners
High-school
Armed-Forces
Occupation
Relationship
High-school
High-school
Exec-managerial
Adm-clerical
Income > $50k
TabNet encoder
TabNet decoder
Decision making
Occupation
Relationship
Exec-managerial
High-school
Farming-fishing
Prof-specialty
High-school
Handlers-cleaners
Exec-managerial
Prof-specialty
High-school
Armed-Forces
Unsupervised pre-training
Supervised fine-tuning
TabNet encoder
Figure 2: Self-supervised tabular learning. Real-world tabular datasets have interdependent feature columns, e.g., the education
level can be guessed from the occupation, or the gender can be guessed from the relationship. Unsupervised representation
learning by masked self-supervised learning results in an improved encoder model for the supervised learning task.
ward selection and Lasso regularization attribute feature importance based on the entire training
data, and are referred as global methods. Instance-wise feature selection refers to picking features individually for each
input, studied in with an explainer model
to maximize the mutual information between the selected
features and the response variable, and in by using an actor-critic framework to
mimic a baseline while optimizing the selection. Unlike these,
TabNet employs soft feature selection with controllable sparsity in end-to-end learning – a single model jointly performs
feature selection and output mapping, resulting in superior
performance with compact representations.
Tree-based learning: DTs are commonly-used for tabular
data learning. Their prominent strength is efﬁcient picking
of global features with the most statistical information gain
 . To improve the performance of standard DTs, one common approach is ensembling
to reduce variance. Among ensembling methods, random
forests use random subsets of data with randomly
selected features to grow many trees. XGBoost and LightGBM are the two
recent ensemble DT approaches that dominate most of the
recent data science competitions. Our experimental results
W: [$", - $", 0, 0]
b: [-a $", a $", -1, -1]
−$"!" + $"%
W: [0, 0, $# , - $# ]
b: [-1, -1, -d $#, d $# ]
−$#!# + $#&
Figure 3: Illustration of DT-like classiﬁcation using conventional DNN blocks (left) and the corresponding decision manifold
(right). Relevant features are selected by using multiplicative sparse masks on inputs. The selected features are linearly
transformed, and after a bias addition (to represent boundaries) ReLU performs region selection by zeroing the regions.
Aggregation of multiple regions is based on addition. As C1 and C2 get larger, the decision boundary gets sharper.
for various datasets show that tree-based models can be outperformed when the representation capacity is improved with
deep learning while retaining their feature selecting property.
Integration of DNNs into DTs: Representing DTs with
DNN building blocks as in yields redundancy in representation and inefﬁcient learning. Soft (neural) DTs use differentiable decision
functions, instead of non-differentiable axis-aligned splits.
However, losing automatic feature selection often degrades
performance. In , a soft
binning function is proposed to simulate DTs in DNNs, by
inefﬁciently enumerating of all possible decisions. proposes a DNN architecture by explicitly leveraging
expressive feature combinations, however, learning is based
on transferring knowledge from gradient-boosted DT. proposes a DNN architecture by adaptively growing from primitive blocks while representation learning into
edges, routing functions and leaf nodes. TabNet differs from
these as it embeds soft feature selection with controllable
sparsity via sequential attention.
Self-supervised learning: Unsupervised representation
learning improves supervised learning especially in small
data regime . Recent work for text and image data
has shown signiﬁcant advances – driven by the judicious
choice of the unsupervised learning objective (masked input
prediction) and attention-based deep learning.
TabNet for Tabular Learning
DTs are successful for learning from real-world tabular
datasets. With a speciﬁc design, conventional DNN building
blocks can be used to implement DT-like output manifold,
e.g. see Fig. 3). In such a design, individual feature selection is key to obtain decision boundaries in hyperplane form,
which can be generalized to a linear combination of features
where coefﬁcients determine the proportion of each feature.
TabNet is based on such functionality and it outperforms DTs
while reaping their beneﬁts by careful design which: (i) uses
sparse instance-wise feature selection learned from data; (ii)
constructs a sequential multi-step architecture, where each
step contributes to a portion of the decision based on the
selected features; (iii) improves the learning capacity via nonlinear processing of the selected features; and (iv) mimics
ensembling via higher dimensions and more steps.
Fig. 4 shows the TabNet architecture for encoding tabular data. We use the raw numerical features and consider
mapping of categorical features with trainable embeddings.
We do not consider any global feature normalization, but
merely apply batch normalization (BN). We pass the same Ddimensional features f ∈ℜB×D to each decision step, where
B is the batch size. TabNet’s encoding is based on sequential
multi-step processing with Nsteps decision steps. The ith
step inputs the processed information from the (i −1)th step
to decide which features to use and outputs the processed
feature representation to be aggregated into the overall decision. The idea of top-down attention in the sequential form
is inspired by its applications in processing visual and text
data and reinforcement learning while searching for a small subset of
relevant information in high dimensional input.
Feature selection: We employ a learnable mask M[i] ∈
ℜB×D for soft selection of the salient features. Through
sparse selection of the most salient features, the learning
capacity of a decision step is not wasted on irrelevant
ones, and thus the model becomes more parameter efﬁcient. The masking is multiplicative, M[i] · f. We use an
attentive transformer (see Fig. 4) to obtain the masks using the processed features from the preceding step, a[i −1]:
M[i] = sparsemax(P[i −1] · hi(a[i −1])). Sparsemax normalization encourages sparsity
by mapping the Euclidean projection onto the probabilistic
simplex, which is observed to be superior in performance and
aligned with the goal of sparse feature selection for explainability. Note that PD
j=1 M[i]b,j = 1. hi is a trainable function, shown in Fig. 4 using a FC layer, followed by BN. P[i]
is the prior scale term, denoting how much a particular feature
has been used previously: P[i] = Qi
j=1(γ −M[j]), where γ
is a relaxation parameter – when γ = 1, a feature is enforced
transformer
transformer
transformer
transformer
transformer
attributes
(a) TabNet encoder architecture
transformer
Encoded representation
Reconstructed
transformer
(b) TabNet decoder architecture
transformer
Shared across decision steps
Decision step dependent
transformer
Prior scales
Figure 4: (a) TabNet encoder, composed of a feature transformer, an attentive transformer and feature masking. A split block
divides the processed representation to be used by the attentive transformer of the subsequent step as well as for the overall
output. For each step, the feature selection mask provides interpretable information about the model’s functionality, and the
masks can be aggregated to obtain global feature important attribution. (b) TabNet decoder, composed of a feature transformer
block at each step. (c) A feature transformer block example – 4-layer network is shown, where 2 are shared across all decision
steps and 2 are decision step-dependent. Each layer is composed of a fully-connected (FC) layer, BN and GLU nonlinearity. (d)
An attentive transformer block example – a single layer mapping is modulated with a prior scale information which aggregates
how much each feature has been used before the current decision step. sparsemax is used for
normalization of the coefﬁcients, resulting in sparse selection of the salient features.
to be used only at one decision step and as γ increases, more
ﬂexibility is provided to use a feature at multiple decision
steps. P is initialized as all ones, 1B×D, without any prior
on the masked features. If some features are unused (as in selfsupervised learning), corresponding P entries are made 0
to help model’s learning. To further control the sparsity of the
selected features, we propose sparsity regularization in the
form of entropy , Lsparse =
−Mb,j[i] log(Mb,j[i]+ϵ)
, where ϵ is a
small number for numerical stability. We add the sparsity regularization to the overall loss, with a coefﬁcient λsparse. Sparsity provides a favorable inductive bias for datasets where
most features are redundant.
Feature processing: We process the ﬁltered features using
a feature transformer (see Fig. 4) and then split for the
decision step output and information for the subsequent
step, [d[i], a[i]] = fi(M[i] · f), where d[i] ∈ℜB×Nd and
a[i] ∈ℜB×Na. For parameter-efﬁcient and robust learning
with high capacity, a feature transformer should comprise
layers that are shared across all decision steps (as the same
features are input across different decision steps), as well as
decision step-dependent layers. Fig. 4 shows the implementation as concatenation of two shared layers and two decision
step-dependent layers. Each FC layer is followed by BN and
gated linear unit (GLU) nonlinearity ,
eventually connected to a normalized residual connection
with normalization. Normalization with
0.5 helps to stabilize learning by ensuring that the variance throughout the
network does not change dramatically .
For faster training, we use large batch sizes with BN. Thus,
except the one applied to the input features, we use ghost BN
 form, using a virtual batch
size BV and momentum mB. For the input features, we observe the beneﬁt of low-variance averaging and hence avoid
ghost BN. Finally, inspired by decision-tree like aggregation
as in Fig. 3, we construct the overall decision embedding
as dout = PNsteps
ReLU(d[i]). We apply a linear mapping
Wﬁnaldout to get the output mapping.1
Interpretability: TabNet’s feature selection masks can shed
light on the selected features at each step. If Mb,j[i] = 0,
then jth feature of the bth sample should have no contribution
to the decision. If fi were a linear function, the coefﬁcient
Mb,j[i] would correspond to the feature importance of fb,j.
Although each decision step employs non-linear processing,
their outputs are combined later in a linear way. We aim
to quantify an aggregate feature importance in addition to
analysis of each step. Combining the masks at different steps
requires a coefﬁcient that can weigh the relative importance
of each step in the decision. We simply propose ηb[i] =
c=1 ReLU(db,c[i]) to denote the aggregate decision contribution at ith decision step for the bth sample. Intuitively, if
db,c[i] < 0, then all features at ith decision step should have
0 contribution to the overall decision. As its value increases,
it plays a higher role in the overall linear combination. Scaling the decision mask at each decision step with ηb[i], we
1For discrete outputs, we additionally employ softmax during
training (and argmax during inference).
propose the aggregate feature importance mask, Magg−b,j =
ηb[i]Mb,j[i]
ηb[i]Mb,j[i].2
Tabular self-supervised learning: We propose a decoder
architecture to reconstruct tabular features from the Tab-
Net encoded representations. The decoder is composed of
feature transformers, followed by FC layers at each decision step. The outputs are summed to obtain the reconstructed features. We propose the task of prediction of missing feature columns from the others. Consider a binary mask
S ∈{0, 1}B×D. The TabNet encoder inputs (1 −S) · ˆf
and the decoder outputs the reconstructed features, S · ˆf. We
initialize P = (1 −S) in encoder so that the model emphasizes merely on the known features, and the decoder’s last
FC layer is multiplied with S to output the unknown features.
We consider the reconstruction loss in self-supervised phase:
(ˆfb,j−fb,j)·Sb,j
b=1(fb,j−1/B PB
b=1 fb,j)2
. Normalization
with the population standard deviation of the ground truth
is beneﬁcial, as the features may have different ranges. We
sample Sb,j independently from a Bernoulli distribution with
parameter ps, at each iteration.
Experiments
We study TabNet in wide range of problems, that contain
regression or classiﬁcation tasks, particularly with published
benchmarks. For all datasets, categorical inputs are mapped
to a single-dimensional trainable scalar with a learnable embedding3 and numerical columns are input without and preprocessing.4 We use standard classiﬁcation (softmax cross
entropy) and regression (mean squared error) loss functions
and we train until convergence. Hyperparameters of the Tab-
Net models are optimized on a validation set and listed in
Appendix. TabNet performance is not very sensitive to most
hyperparameters as shown with ablation studies in Appendix.
In Appendix, we also present ablation studies on various design and guidelines on selection of the key hyperparameters.
For all experiments we cite, we use the same training, validation and testing data split with the original work. Adam
optimization algorithm and Glorot
uniform initialization are used for training of all models.5
Instance-wise feature selection
Selection of the salient features is crucial for high performance, especially for small datasets. We consider 6 tabular
datasets from (consisting 10k training
samples). The datasets are constructed in such a way that
only a subset of the features determine the output. For Syn1-
Syn3, salient features are same for all instances (e.g., the
2Normalization is used to ensure PD
j=1 Magg−b,j = 1.
3In some cases, higher dimensional embeddings may slightly improve the performance, but interpretation of individual dimensions
may become challenging.
4Specially-designed feature engineering, e.g. logarithmic transformation of variables highly-skewed distributions, may further
improve the results but we leave it out of the scope of this paper.
5An open-source implementation will be released.
Table 1: Mean and std. of test area under the receiving operating characteristic curve (AUC) on 6 synthetic datasets from , for TabNet vs. other feature selection-based DNN models: No sel.: using all features without any feature selection,
Global: using only globally-salient features, Tree Ensembles , Lasso-regularized model, L2X
 and INVASE . Bold numbers denote the best for each dataset.
No selection
.578 ± .004
.789 ± .003
.854 ± .004
.558 ± .021
.662 ± .013
.692 ± .015
.574 ± .101
.872 ± .003
.899 ± .001
.684 ± .017
.741 ± .004
.771 ± .031
Lasso-regularized
.498 ± .006
.555 ± .061
.886 ± .003
.512 ± .031
.691 ± .024
.727 ± .025
.498 ± .005
.823 ± .029
.862 ± .009
.678 ± .024
.709 ± .008
.827 ± .017
.690 ± .006
.877 ± .003
.902 ± .003
.787 ± .004
.784 ± .005
.877 ± .003
.686 ± .005
.873 ± .003
.900 ± .003
.774 ± .006
.784 ± .005
.858 ± .004
.682 ± .005
.892 ± .004
.897 ± .003
.776 ± .017
.789 ± .009
.878 ± .004
output of Syn2 depends on features X3-X6), and global feature selection, as if the salient features were known, would
give high performance. For Syn4-Syn6, salient features are
instance dependent (e.g., for Syn4, the output depends on either X1-X2 or X3-X6 depending on the value of X11), which
makes global feature selection suboptimal. Table 1 shows that
TabNet outperforms others , LASSO regularization, L2X ) and is on par with INVASE . For Syn1-Syn3, TabNet performance
is close to global feature selection - it can ﬁgure out what
features are globally important. For Syn4-Syn6, eliminating
instance-wise redundant features, TabNet improves global
feature selection. All other methods utilize a predictive model
with 43k parameters, and the total number of parameters is
101k for INVASE due to the two other models in the actorcritic framework. TabNet is a single architecture, and its size
is 26k for Syn1-Syn3 and 31k for Syn4-Syn6. The compact
representation is one of TabNet’s valuable properties.
Performance on real-world datasets
Table 2: Performance for Forest Cover Type dataset.
Test accuracy (%)
AutoML Tables
Forest Cover Type : The task is classiﬁcation of forest cover type from cartographic variables.
Table 2 shows that TabNet outperforms ensemble tree based
approaches that are known to achieve solid performance
 . We also consider AutoML Tables
 , an automated search framework based on
ensemble of models including DNN, gradient boosted DT,
AdaNet and ensembles 
with very thorough hyperparameter search. A single TabNet
without ﬁne-grained hyperparameter search outperforms it.
Table 3: Performance for Poker Hand induction dataset.
Test accuracy (%)
Deep neural DT
Rule-based
Poker Hand : The task is classiﬁcation of the poker hand from the raw suit and rank attributes of
the cards. The input-output relationship is deterministic and
hand-crafted rules can get 100% accuracy. Yet, conventional
DNNs, DTs, and even their hybrid variant of deep neural DTs
 severely suffer from
the imbalanced data and cannot learn the required sorting and
ranking operations .
Tuned XGBoost, CatBoost, and LightGBM show very slight
improvements over them. TabNet outperforms other methods,
as it can perform highly-nonlinear processing with its depth,
without overﬁtting thanks to instance-wise feature selection.
Table 4: Performance on Sarcos dataset. Three TabNet models of different sizes are considered.
Model size
Random forest
Stochastic DT
Adaptive neural tree
Gradient boosted tree
Sarcos : The task is regressing inverse dynamics of an anthropomorphic robot arm.
 shows that decent performance with a
very small model is possible with a random forest. In the very
small model size regime, TabNet’s performance is on par
with the best model from with 100x more
parameters. When the model size is not constrained, TabNet
achieves almost an order of magnitude lower test MSE.
Table 5: Performance on Higgs Boson dataset. Two TabNet
models are denoted with -S and -M.
Test acc. (%)
Model size
Sparse evolutionary MLP
Gradient boosted tree-S
Gradient boosted tree-M
Gradient boosted tree-L
Higgs Boson : The task is distinguishing between a Higgs bosons process vs. background. Due to
its much larger size (10.5M instances), DNNs outperform DT
variants even with very large ensembles. TabNet outperforms
MLPs with more compact representations. We also compare
to the state-of-the-art evolutionary sparsiﬁcation algorithm
 that integrates non-structured sparsity
into training. With its compact representation, TabNet yields
almost similar performance to sparse evolutionary training
for the same number of parameters. Unlike sparse evolutionary training, the sparsity of TabNet is structured – it does not
degrade the operational intensity and can
efﬁciently utilize modern multi-core processors.
Table 6: Performance for Rossmann Store Sales dataset.
Rossmann Store Sales : The task is forecasting the store sales from static and time-varying features.
We observe that TabNet outperforms commonly-used methods. The time features (e.g. day) obtain high importance, and
the beneﬁt of instance-wise feature selection is observed for
cases like holidays where the sales dynamics are different.
Interpretability
Synthetic datasets: Fig. 5 shows the aggregate feature importance masks for the synthetic datasets from Table 1.6 The
output on Syn2 only depends on X3-X6 and we observe that
6For better illustration here, the models are trained with 10M
samples rather than 10K as we obtain sharper selection masks.
the aggregate masks are almost all zero for irrelevant features
and TabNet merely focuses on the relevant ones. For Syn4,
the output depends on either X1-X2 or X3-X6 depending
on the value of X11. TabNet yields accurate instance-wise
feature selection – it allocates a mask to focus on the indicator X11, and assigns almost all-zero weights to irrelevant
features (the ones other than two feature groups).
Real-world datasets: We ﬁrst consider the simple task of
mushroom edibility prediction . Tab-
Net achieves 100% test accuracy on this dataset. It is indeed
known that “Odor” is the most discriminative feature – with “Odor” only, a model can get > 98.5%
test accuracy . Thus, a high feature
importance is expected for it. TabNet assigns an importance
score ratio of 43% for it, while other methods like LIME
 , Integrated Gradients
 and DeepLift assign less than 30% . Next, we consider Adult Census Income. TabNet
yields feature importance rankings consistent with the wellknown 
(see Appendix) For the same problem, Fig. 6 shows the clear
separation between age groups, as suggested by “Age” being
the most important feature by TabNet.
Self-supervised learning
Table 7: Mean and std. of accuracy (over 15 runs) on Higgs
with Tabnet-M model, varying the size of the training dataset
for supervised ﬁne-tuning.
Test accuracy (%)
dataset size
Supervised
With pre-training
57.47 ± 1.78
61.37 ± 0.88
66.66 ± 0.88
68.06 ± 0.39
72.92 ± 0.21
73.19 ± 0.15
Table 7 shows that unsupervised pre-training signiﬁcantly
improves performance on the supervised classiﬁcation task,
especially in the regime where the unlabeled dataset is much
larger than the labeled dataset. As exempliﬁed in Fig. 7 the
model convergence is much faster with unsupervised pretraining. Very fast convergence can be useful for continual
learning and domain adaptation.
Conclusions
We have proposed TabNet, a novel deep learning architecture for tabular learning. TabNet uses a sequential attention
mechanism to choose a subset of semantically meaningful
features to process at each decision step. Instance-wise feature selection enables efﬁcient learning as the model capacity
is fully used for the most salient features, and also yields
more interpretable decision making via visualization of selection masks. We demonstrate that TabNet outperforms previous work across tabular datasets from different domains.
Lastly, we demonstrate signiﬁcant beneﬁts of unsupervised
pre-training for fast adaptation and improved performance.
Syn2 dataset
X1 X2 X3 X4 X5
X7 X8 X9 X10
Test samples
Syn4 dataset
Figure 5: Feature importance masks M[i] (that indicate feature selection at ith step) and the aggregate feature importance mask
Magg showing the global instance-wise feature selection, on Syn2 and Syn4 . Brighter colors show a higher
value. E.g. for Syn2, only X3-X6 are used.
Figure 6: First two dimensions of the T-SNE of the decision
manifold for Adult and the impact of the top feature ‘Age’.
Figure 7: Training curves on Higgs dataset with 10k samples.
Acknowledgements
Discussions with Jinsung Yoon, Kihyuk Sohn, Long T. Le,
Ariel Kleiner, Zizhao Zhang, Andrei Kouznetsov, Chen Xing,
Ryan Takasugi and Andrew Moore are gratefully acknowledged.
Performance on KDD datasets
Table 8: Performance on KDD datasets.
Test accuracy (%)
Appetency, Churn and Upselling datasets are classiﬁcation tasks for customer relationship management, and KDD
Census Income dataset is for income
prediction from demographic and employment related variables. These datasets show saturated behavior in performance
(even simple models yield similar results). Table 8 shows that
TabNet achieves very similar or slightly worse performance
than XGBoost and CatBoost, that are known to be robust as
they contain high amount of ensembles.
Comparison of feature importance ranking of
Table 9: Importance ranking of features for Adult Census Income. TabNet yields feature importance rankings consistent
with the well-known methods.
Capital gain
Capital loss
Hours per week
Marital status
Native country
Occupation
Relationship
Work class
We observe the commonality of the most important features (“Age”, “Capital gain/loss”, “Education number”, “Relationship”) and the least important features (“Native country”, “Race”, “Gender”, “Work class”).
Self-supervised learning on Forest Cover Type
Experiment hyperparameters
For all datasets, we use a pre-deﬁned hyperparameter search
space. Nd and Na are chosen from {8, 16, 24, 32, 64, 128},
Table 10: Self-supervised tabular learning results. Mean and
std. of accuracy (over 15 runs) on Forest Cover Type, varying
the size of the training dataset for supervised ﬁne-tuning.
Test accuracy (%)
dataset size
Supervised
With pre-training
65.91 ± 1.02
67.86 ± 0.63
78.85 ± 1.24
79.22 ± 0.78
Nsteps is chosen from {3, 4, 5, 6, 7, 8, 9, 10}, γ is chosen from {1.0, 1.2, 1.5, 2.0}, λsparse is chosen from
{0, 0.000001, 0.0001, 0.001, 0.01, 0.1}, B is chosen from
{256, 512, 1024, 2048, 4096, 8192, 16384, 32768}, BV
chosen from {256, 512, 1024, 2048, 4096}, the learning rate
is chosen from {0.005, 0.01.0.02, 0.025}, the decay rate is
chosen from {0.4, 0.8, 0.9, 0.95} and the decay iterations
is chosen from {0.5k, 2k, 8k, 10k, 20k}, and mB is chosen
from {0.6, 0.7, 0.8, 0.9, 0.95, 0.98}. If the model size is not
under the desired cutoff, we decrease the value to satisfy
the size constraint. For all the comparison models, we run a
hyperparameter tuning with the same number of search steps.
Synthetic: All TabNet models use Nd=Na=16, B=3000,
BV =100, mB=0.7. For Syn1 we use λsparse=0.02,
Nsteps=4 and γ=2.0; for Syn2 and Syn3 we use
λsparse=0.01, Nsteps=4 and γ=2.0; and for Syn4, Syn5 and
Syn6 we use λsparse=0.005, Nsteps=5 and γ=1.5. Feature
transformers use two shared and two decision step-dependent
FC layer, ghost BN and GLU blocks. All models use Adam
with a learning rate of 0.02 (decayed 0.7 every 200 iterations
with an exponential decay) for 4k iterations. For visualizations, we also train TabNet models with datasets of size
10M samples. For this case, we choose Nd = Na = 32,
λsparse=0.001, B=10000, BV =100, mB=0.9. Adam is
used with a learning rate of 0.02 (decayed 0.9 every 2k iterations with an exponential decay) for 15k iterations. For
Syn2 and Syn3, Nsteps=4 and γ=2. For Syn4 and Syn6,
Nsteps=5 and γ=1.5.
Forest Cover Type: The dataset partition details, and the
hyperparameters of XGBoost, LigthGBM, and CatBoost are
from . We re-optimize AutoInt hyperparameters. TabNet model uses Nd=Na=64, λsparse=0.0001,
B=16384, BV =512, mB=0.7, Nsteps=5 and γ=1.5. Feature transformers use two shared and two decision stepdependent FC layer, ghost BN and GLU blocks. Adam is used
with a learning rate of 0.02 (decayed 0.95 every 0.5k iterations with an exponential decay) for 130k iterations. For unsupervised pre-training, the decoder model uses Nd=Na=64,
B=16384, BV =512, mB=0.7, and Nsteps=10. For supervised ﬁne-tuning, we use the batch size B=BV as the training
datasets are small.
Poker Hands: We split 6k samples for validation from
the training dataset, and after optimization of the hyperparameters, we retrain with the entire training dataset. DT,
MLP and deep neural DT models follow the same hyperparameters with . We
tune the hyperparameters of XGBoost, LigthGBM, and
CatBoost. TabNet uses Nd=Na=16, λsparse=0.000001,
B=4096, BV =1024, mB = 0.95, Nsteps=4 and γ=1.5.
Feature transformers use two shared and two decision stepdependent FC layer, ghost BN and GLU blocks. Adam is
used with a learning rate of 0.01 (decayed 0.95 every 500
iterations with an exponential decay) for 50k iterations.
Sarcos: We split 4.5k samples for validation from the training dataset, and after optimization of the hyperparameters,
we retrain with the entire training dataset. All comparison models follow the hyperparameters from . TabNet-S model uses Nd=Na=8, λsparse=0.0001,
B=4096, BV =256, mB=0.9, Nsteps=3 and γ=1.2. Each
feature transformer block uses one shared and two decision step-dependent FC layer, ghost BN and GLU blocks.
Adam is used with a learning rate of 0.01 (decayed 0.95
every 8k iterations with an exponential decay) for 600k iterations. TabNet-M model uses Nd=Na=64, λsparse=0.0001,
B=4096, BV =128, mB=0.8, Nsteps=7 and γ=1.5. Feature transformers use two shared and two decision stepdependent FC layer, ghost BN and GLU blocks. Adam is
used with a learning rate of 0.01 (decayed 0.95 every 8k
iterations with an exponential decay) for 600k iterations.
The TabNet-L model uses Nd=Na=128, λsparse=0.0001,
B=4096, BV =128, mB=0.8, Nsteps=5 and γ=1.5. Feature transformers use two shared and two decision stepdependent FC layer, ghost BN and GLU blocks. Adam is
used with a learning rate of 0.02 (decayed 0.9 every 8k iterations with an exponential decay) for 600k iterations.
Higgs: We split 500k samples for validation from the training dataset, and after optimization of the hyperparameters,
we retrain with the entire training dataset. MLP models are
from . For gradient boosted trees , we tune the learning rate and depth – the
gradient boosted tree-S, -M, and -L models use 50, 300
and 3000 trees respectively. TabNet-S model uses Nd=24,
Na=26, λsparse=0.000001, B=16384, BV =512, mB=0.6,
Nsteps=5 and γ=1.5. Feature transformers use two shared
and two decision step-dependent FC layer, ghost BN and
GLU blocks. Adam is used with a learning rate of 0.02
(decayed 0.9 every 20k iterations with an exponential decay) for 870k iterations. TabNet-M model uses Nd=96,
Na=32, λsparse=0.000001, B=8192, BV =256, mB=0.9,
Nsteps=8 and γ=2.0. Feature transformers use two shared
and two decision step-dependent FC layer, ghost BN and
GLU blocks. Adam is used with a learning rate of 0.025 (decayed 0.9 every 10k iterations with an exponential decay) for
370k iterations. For unsupervised pre-training, the decoder
model uses Nd=Na=128, B=8192, BV =256, mB=0.9,
and Nsteps=20. For supervised ﬁne-tuning, we use the batch
size B=BV as the training datasets are small.
Rossmann: We use the same preprocessing and data split
with – data from 2014 is used for training and
validation, whereas 2015 is used for testing. We split 100k
samples for validation from the training dataset, and after optimization of the hyperparameters, we retrain with the entire
training dataset. The performance of the comparison models
are from . Obtained with hyperparameter tuning, the MLP is composed of 5 layers of FC (with a hidden
unit size of 128), followed by BN and ReLU nonlinearity,
trained with a batch size of 512 and a learning rate of 0.001.
TabNet model uses Nd=Na=32, λsparse=0.001, B=4096,
BV =512, mB=0.8, Nsteps=5 and γ=1.2. Feature transformers use two shared and two decision step-dependent
FC layer, ghost BN and GLU blocks. Adam is used with a
learning rate of 0.002 for 15k iterations.
KDD: For Appetency, Churn and Upselling datasets, we apply the similar preprocessing and split as . The performance of the comparison models are from . TabNet models use Nd=Na=32, λsparse=0.001, B=8192, BV =256,
mB=0.9, Nsteps=7 and γ=1.2. Each feature transformer
block uses two shared and two decision step-dependent FC
layer, ghost BN and GLU blocks. Adam is used with a learning rate of 0.01 (decayed 0.9 every 1000 iterations with an
exponential decay) for 10k iterations. For Census Income,
the dataset and comparison model speciﬁcations follow . TabNet model uses Nd=Na=48, λsparse=0.001,
B=8192, BV =256, mB=0.9, Nsteps=5 and γ=1.5. Feature transformers use two shared and two decision stepdependent FC layer, ghost BN and GLU blocks. Adam is
used with a learning rate of 0.02 for 4k iterations.
Mushroom edibility: TabNet model uses Nd=Na=8,
λsparse=0.001, B=2048, BV =128, mB=0.9, Nsteps=3
and γ=1.5. Feature transformers use two shared and two
decision step-dependent FC layer, ghost BN and GLU blocks.
Adam is used with a learning rate of 0.01 (decayed 0.8 every
400 iterations with an exponential decay) for 10k iterations.
Adult Census Income: TabNet model uses Nd=Na=16,
Nsteps=5 and γ=1.5. Feature transformers use two shared
and two decision step-dependent layer, ghost BN and GLU
blocks. Adam is used with a learning rate of 0.02 (decayed
0.4 every 2.5k iterations with an exponential decay) for 7.7k
iterations. 85.7% test accuracy is achieved.
Ablation studies
Table 11 shows the impact of ablation cases. For all cases,
the number of iterations is optimized on the validation set.
Obtaining high performance necessitates appropriatelyadjusted model capacity based on the characteristics of the
dataset. Decreasing the number of units Nd, Na or the number of decision steps Nsteps are efﬁcient ways of gradually
decreasing the capacity without signiﬁcant degradation in
performance. On the other hand, increasing these parameters beyond some value causes optimization issues and do
not yield performance beneﬁts. Replacing the feature transformer block with a simpler alternative, such as a single
shared layer, can still give strong performance while yielding
a very compact model architecture. This shows the importance of the inductive bias introduced with feature selection
and sequential attention. To push the performance, increasing
the depth of the feature transformer is an effective approach.
While increasing the depth, parameter sharing between feature transformer blocks across decision steps is an efﬁcient
way to decrease model size without degradation in performance. We indeed observe the beneﬁt of partial parameter
sharing, compared to fully decision step-dependent blocks or
fully shared blocks. We also observe the empirical beneﬁt of
GLU, compared to conventional nonlinearities like ReLU.
Table 11: Ablation studies for the TabNet encoder model for the forest cover type dataset.
Ablation cases
accuracy %
(difference)
Base (Nd = Na = 64, γ = 1.5, Nsteps = 5, λsparse = 0.0001, feature
transformer block composed of two shared and two decision
step-dependent layers, B = 16384)
Decreasing capacity via number of units (with Nd = Na = 32)
Decreasing capacity via number of decision steps (with Nsteps = 3)
Increasing capacity via number of decision steps (with Nsteps = 9)
Decreasing capacity via all-shared feature transformer blocks
Increasing capacity via decision step-dependent feature transformer
Feature transformer block as a single shared layer
Feature transformer block as a single shared layer, with ReLU instead of
Feature transformer block as two shared layers
Feature transformer block as two shared layers and 1 decision
step-dependent layer
Feature transformer block as a single decision-step dependent layer
Feature transformer block as a single decision-step dependent layer,
with Nd=Na=128
Feature transformer block as a single decision-step dependent layer,
with Nd=Na=128 and replacing GLU with ReLU
Feature transformer block as a single decision-step dependent layer,
with Nd=Na=256 and replacing GLU with ReLU
Reducing the impact of prior scale (with γ = 3.0)
Increasing the impact of prior scale (with γ = 1.0)
No sparsity regularization (with λsparse = 0)
High sparsity regularization (with λsparse = 0.01)
Small batch size (B = 4096)
The strength of sparse feature selection depends on the
two parameters we introduce: γ and λsparse. We show that
optimal choice of these two is important for performance. A
γ close to 1, or a high λsparse may yield too tight constraints
on the strength of sparsity and may hurt performance. On the
other hand, there is still the beneﬁt of a sufﬁcient low γ and
sufﬁciently high λsparse, to aid learning of the model via a
favorable inductive bias.
Lastly, given the ﬁxed model architecture, we show the
beneﬁt of large-batch training, enabled by ghost BN . The optimal batch size for TabNet
seems considerably higher than the conventional batch sizes
used for other data types, such as images or speech.
Guidelines for hyperparameters
We consider datasets ranging from ∼10K to ∼10M samples,
with varying degrees of ﬁtting difﬁculty. TabNet obtains high
performance on all with a few general principles on hyperparameters:
• For most datasets, Nsteps ∈ is optimal. Typically,
when there are more information-bearing features, the optimal value of Nsteps tends to be higher. On the other hand,
increasing it beyond some value may adversely affect training dynamics as some paths in the network becomes deeper
and there are more potentially-problematic ill-conditioned
matrices. A very high value of Nsteps may suffer from
overﬁtting and yield poor generalization.
• Adjustment of Nd and Na is an efﬁcient way of obtaining a
trade-off between performance and complexity. Nd = Na
is a reasonable choice for most datasets. A very high value
of Nd and Na may suffer from overﬁtting and yield poor
generalization.
• An optimal choice of γ can have a major role on the performance. Typically a larger Nsteps value favors for a larger
• A large batch size is beneﬁcial – if the memory constraints
permit, as large as 1-10 % of the total training dataset size
can help performance. The virtual batch size is typically
much smaller.
• Initially large learning rate is important, which should be
gradually decayed until convergence.