Psychological Review
1994. Vol. 101. No. 4, 608-631
Copyright 1994 by the American Psychological Association. Inc.
0033-295X/94/S3.00
A Rational Analysis of the Selection Task as Optimal Data Selection
Mike Oaksford and Nick Chater
Human reasoning in hypothesis-testing tasks like Wason's selection task has been depicted as prone to systematic biases. However, performance on this task has been assessed against a
now outmoded falsificationist philosophy of science. Therefore, the experimental data is reassessed
in the light of a Bayesian model of optimal data selection in inductive hypothesis testing. The model
provides a rational analysis of the selection task that fits well with people's performance on both abstract and thematic versions of the task. The model suggests that reasoning in these
tasks may be rational rather than subject to systematic bias.
Over the past 30 years, results in the psychology of reasoning
have raised doubts about human rationality. The assumption of
human rationality has a long history. Aristotle took the capacity
for rational thought to be the defining characteristic of human
beings, the capacity that separated us from the animals. Descartes regarded the ability to use language and to reason as the
hallmarks of the mental that separated it from the merely physical. Many contemporary philosophers of mind also appeal to a
basic principle of rationality in accounting for everyday, folk
psychological explanation whereby we explain each other's behavior in terms of our beliefs and desires .
These philosophers, both ancient and modern, share a common
view of rationality: To be rational is to reason according to rules
 . Logic and mathematics provide the normative
rules that tell us how we should reason. Rationality therefore
seems to demand that the human cognitive system embodies
the rules of logic and mathematics. However, results in the psychology of reasoning appear to show that people do not reason
according to these rules. In both deductive 
and probabilistic reasoning , people's performance appears biased when compared with the standards of logic and probability theory.
Recently, however, some psychologists and philosophers have
offered a different account of what it is to be rational .
Correspondence concerning this article should be addressed to Mike
Oaksford, who is now at Department of Psychology, University of Warwick, Coventry CV4 7AL United Kingdom; or to Nick Chater, who is
now at Department of Experimental Psychology, University of Oxford,
South Parks Road, Oxford OX 1 3UD United Kingdom. Electronic mail
may be sent to .
1990; Evans, 1993; Stich, 1990). In particular, Anderson 
argued that we must distinguish normative from adaptive rationality. An organism's behavior is rational if it is optimally
adapted to its environment, even if reasoning according to logical rules had no causal role in producing the behavior. Such
optimality assumptions have become widespread in contemporary social and behavioral science, from economics to optimal foraging theory . Moreover, Anderson has extended this
approach to provide "rational analyses" of memory, categorization, and problem solving .
In this article, we apply this approach to Wason's selection task, which has raised more doubts over human
rationality than any other psychological task . In the selection
task, an experimenter presents subjects with four cards, each
with a number on one side and a letter on the other, and a rule
of the form ifp, then q, for example, if there is a vowel on one
side (p), then there is an even number on the other side (q). The
four cards show an A (p card), a K (not-p card), a 2 (q card)<
and a 7 (not-q card; see Figure 1). Subjects have to select those
cards that they must turn over to determine whether the rule is
true or false. Logically, subjects should select only the p and notq cards. However, only 4% of subjects make this response, other
responses being far more common (p and q cards, 46%; p card
only, 33%; p, q, and not-q cards (7%); and p and not-q cards, 4%;
Johnson-Laird & Wason, 1970a).
The selection task is a laboratory version of the problem of
choosing the best experiments to test scientific laws. Popper's
 method of falsification provides the standard normative
account of this situation. Popper argued that, logically, experiments can only falsify general laws, they cannot confirm general
laws. Hence, scientists should only conduct experiments that
can falsify a general law. The selection task provides an opportunity to see whether people spontaneously adopt Popper's falsificationist strategy . Logically,
the only way to falsify the conditional rule if p, then q in the
selection task is to look for cards with p on one side and not-q
on the other. Only two visible card faces are potentially of this
type: the p card and the not-q card. Hence, according to falsifi-
RATIONAL ANALYSIS OF THE SELECTION TASK
Figure 1. The four cards in the abstract version of Wason's selection task.
Rational Analysis
In this section, we first informally outline the problem of optimal data selection and how it applies to the selection task. We
then present the Bayesian approach to optimal data selection.
We then apply this account to derive a rational analysis of the
selection task. Finally, we explore some general properties of the
model's behavior.
cation, subjects should choose only these two cards. However, in
the selection task, as few as 4% of subjects make this card selection. This lack of fit between normative theory and behavior is
responsible for the widespread doubts over human rationality
we mentioned above.
Contemporary philosophers of science have rejected falsificationism as unfaithful to the history of science and to be anyway unworkable . More
recent accounts of scientific inference take a Bayesian probabilistic approach to confirmation . In particular, the Bayesian theory of
optimal data selection offers a
different account of how scientists should choose experiments
that do not place an exclusive emphasis on falsification. Using
this theory to develop a rational analysis of the selection task fits
well with other rational analyses that also
use Bayesian methods. Our rational analysis will show that we
can view behavior in the selection task as optimizing the expected amount of information gained by turning each card.
The purpose of a rational analysis is to show that behavior
is optimally adapted to the environment. Good fits between a
rational analysis and behavior indicate only that such an analysis provides an organizing framework for describing the behavior. Whether the behavior is rational depends on whether the
rational analysis adequately characterizes the environment.
Anderson used diffuse Bayesian prior distributions to
model the environment. Although we do not use such distributions, we do make some assumptions about the environment
that we do not justify until the Discussion section. In particular,
we assume that the properties that figure in causal relations are
rare in the environment. We call this the rarity assumption. We
show that we can organize the data on the selection task on the
assumption that subjects act as Bayesian optimal data selectors
with rarity. In the Discussion section, we argue that the environment respects rarity, and hence we can view peoples' behavior on the selection task as adaptively rational.
The organization of this article is as follows. In the first section, we develop our rational analysis. In the following sections,
we apply this analysis to a range of selection task data: the standard abstract results, the nonindependence of card selections
 , the negations paradigm , tasks that vary the probabilities of so-called fictional outcomes , the therapy experiments , the reduced array selection task , and the thematic selection
tasks . Finally, we discuss the assumptions and implications of our account.
Informal Outline
Optimal data selection involves choosing experiments to decide between rival hypotheses .
For example, suppose that a metallurgist has various competing
hypotheses about the underlying relationship between temperature and tensile strength. To decide between these hypotheses
the metallurgist must choose new temperatures at which to test
a metal's tensile strength. Intuitively, the most informative temperatures will be those where the hypotheses make divergent
predictions . The Bayesian theory of optimal data
selection formalizes these intuitions.
Everyday hypothesis testing also involves optimal data selection. Suppose that you are interested in the hypothesis that eating tripe makes people feel sick. In collecting evidence, should
you ask known tripe eaters or tripe avoiders whether they feel
sick? Should you ask people known to be, or not to be, sick
whether they have eaten tripe? This case is analogous to the selection task. Logically, the hypothesis can be written as a conditional sentence, if you eat tripe (p), then you feel sick (q). The
groups of people that you may investigate then correspond to
the various visible card options, p, not-p, q, and not-q. In practice, who is available will influence decisions about who to investigate. The selection task abstracts away from this practical
detail by presenting one example of each potential source of
data. In terms of our everyday example, it is like coming across
four people, one known to have eaten tripe, one known not to
have eaten tripe, one known to feel sick, and one known not to
feel sick. You must then judge which of these people you should
question about how they feel or what they have eaten.
Let us consider informally what seems to be a rational selection of data in this situation. First, asking a person who has
eaten tripe (p) is likely to be informative. If this person feels
sick, then the hypothesis gains some credence; if not, then this
evidence falsifies the hypothesis. Second, asking whether a tripe
avoider (not-p) feels sick is futile, because the rule says nothing
about how people feel if they have not eaten tripe. Third, asking
whether a person who feels sick (q) has eaten tripe is worthwhile.
The hypothesis will gain credence if they have eaten tripe, although if they have not, no conclusion appears to be forthcoming. Fourth, the person who is not feeling sick (not-q) is also
worth questioning. If they have eaten tripe, this evidence falsifies the hypothesis. If they have not, no conclusion appears to
be forthcoming. In this example, it seems that the p card is certain to be informative, the not-p card certainly will not be, and
the q and not-q cards may or may not be informative. We now
introduce the Bayesian approach to optimal data selection and
show how to justify and extend these intuitions.
MIKE OAKSFORD AND NICK CHATER
Bayesian Approach
We assume that the data that lead to the greatest expected
reduction in uncertainty are the optimal data to select. We formalize uncertainty using Shannon-Wiener information. Given n mutually exclusive and exhaustive hypotheses (//,-), the uncertainty, /(//,), is
After obtaining some data D from performing an experiment
(e.g., from turning a card in the selection task), we revise P(H/)
to P(Hj\D). Hence, the new uncertainty I(H,\D) is
-2 P(H,,\D)\og2P(H,\D).
We derive the P(H,\D) terms using Bayes' theorem:
P(D\Hj)P(Hj)
which specifies the posterior probability of a hypothesis //,
given some data D in terms of the priors of each hypothesis Ht
and the likelihoods of D given each Hj. The information gain,
Ig, produced by a particular piece of data, is the reduction in
uncertainty1:
Before collecting the data (i.e., turning a card), an inquirer
does not know what the data outcome (the value of the hidden
face) will be. Hence, it is not possible to calculate how informative the data will be. We therefore calculate the expected information gain, E(Ig), taking into account all possible data outcomes (i.e., the two possible hidden faces of a card):
£(/,) = E[I(H, D) - /(//,)].
Probability theory allows us to rewrite this as
P(Dk)I(H, | D) | -
P(Hj>P(Dk\Hj).
Thus, E(Ig) is the uncertainty after performing the experiment,
weighted by the probability of each possible experimental outcome, less the prior uncertainty. The greater E(Ig), the more
useful we predict an experiment is in deciding between rival
hypotheses, //,.
In this framework, hypotheses are not tested in isolation but
in comparison to one another. This is for two reasons. First,
uncertainty is only defined with respect to a set of alternative
hypotheses (Equations 1 and 2). Second, the Bayesian calculations also require that such alternatives are defined (Equation
3). This comparative aspect of hypothesis testing is central to
much recent philosophy of science .
Probabilities for the Dependence Model, MD and the
Independence Model, MI
(\-a)(\-b)
(\-a)(\-b)
Note, a corresponds to the probability of p, P(p), and b corresponds
to the probability of q in the absence ofp, P(q\ not-p).
Optimal Data Selection and the Selection Task
We now apply our Bayesian model of optimal data selection
to the selection task. This involves specifying the alternative
hypotheses that subjects must choose between and defining
them probabilistically.
Accounting for the selection task requires considering just
two hypotheses. One hypothesis, MD, is that the dependency //
p, then q holds. That is, if p occurs, then q must occur. We assume the simplest possible alternative hypothesis, MI, that p
and q are completely independent. Table 1 specifies the probabilities of each whole card, given each model. The left-hand side
of the table specifies the probability, given MD, of finding a p, q
card, P(p, q\MD); ap, not-q card, P(p, not-q\MD); a not-p, q card,
P(not-p, q\MD); and a not-p, not-q card, P(not-p, not-q\MD). Similarly, the right-hand side of Table 1 specifies the corresponding
probabilities for M,.
We set the probability of p, P(p), to be the same in both
models; that is, the row marginals are the same for MD and M,.
Otherwise, observing p or not-p instances alone could provide
evidence about whether the rule holds. In Table 1, we represent
P(p) by a. We also set the probability of q in the absence of p,
P(q\not-p), to be the same in both models. This reflects that a
conditional rule makes a claim about the consequent only if the
antecedent occurs but makes no claim otherwise .
In Table 1, we represent P(q\not-p) by b. The probability of q,
P(q), is not the same in MD and Af,; that is, the column marginals are not the same. This is because the probability of # depends
on whether the rule is true or false. If the rule holds, then the
number of <?s must be at least as great as the number of ps. There
is no reason to assume this if p and q are independent.
We calculate expected information gain, E(Ig), as follows.
Equations 1 to 6 show how £"(4) can be derived using the prior
probabilities of each hypothesis P(H,) and the conditional probabilities of each data outcome given each hypothesis, P(Dk\H,).
We calculate the expected information gain associated with
turning, say, the q card from the prior probabilities of the two
hypotheses MD and M, and the probabilities of the two possible
outcomes of turning the q card (i.e., a p or not-p).
We specify the priors by treating the probability of the inde-
1 Oaksford provided a qualitative account of the selection task
in terms of information gain in the situation-theoretic framework of
Barwise and Perry .
RATIONAL ANALYSIS OF THE SELECTION TASK
Figure 2. Model behavior. At each [P(p), P(Q), P(M,)] coordinate, the three boxes represent the E(Ig)
values for the three cards. The area of the box is proportional to the E(Ig) for the corresponding card.
Coordinates where there are three dots indicate regions where the inequality F\q) > P(q)P(MD) is violated,
and hence the probability values are inconsistent. MI = independent model.
pendence model, P(Mi), as a third parameter. The prior probability, of the dependence model P(MD), is therefore 1 — P(M,).
To explain how we calculate the conditional probabilities,
PiDklHj), let us first consider a slightly different set up from the
standard selection task. Suppose instead of turning cards with
one visible face you draw a card from a pack in ignorance of
both sides. You can draw four possible cards: p, q\ p, not-q\ notp, <?; or not-p, not-q. We can calculate E(Ig)s for this set up directly from the conditional probabilities of each of these cards
given each model. These conditional probabilities are the cell
values in Table 1. However, these are not the appropriate conditional probabilities to calculate E(Ig) in the selection task. Calculations of the conditional probabilities, P(Dk\H,), must themselves be conditional on the visible face because the visible face
is known. For example, if q is the visible face, then there are two
possible outcomes if the card is turned: p or not-p. The probabilities of these outcomes, conditional on the hypothesis MD and
the visible q face, are P(p\q, MD) and P(not-p\q, MD), and similarly for M,.2 We derived these probabilities from Table 1 using
identities such as
P(p\q,MD) =
P(p, q\MD) = a, by the top left-hand cell in Table 1. P(not-p,
q\MD) = (1 - a)b, by the bottom left-hand cell in Table 1.
2 There is an additional, although unimportant, complication, which
concerns whether subjects can use the visible faces to tell between MD
and M,. To see why this is possible, consider the following line of argument. If the experimenter drew the cards at random from a sample of
cards and oriented these cards randomly to produce a range of visible
faces, then in principle, the visible faces can contain useful information
about which hypothesis is true. In particular, a visible q card is more
likely if MD is true than if MI is true, and hence partially confirms MD.
Similarly, a visible not-q face is less likely if MD is true than if M, is
true, and hence partially confirms M,. Because the p and not-p cards are
equally probable given either MD or M,, they do not confirm or disconfirm either model. As noted above, this reasoning depends on the assumption that the visible faces are randomly selected. In practice, of
course, rather than making a random choice, the experimenter specifically chooses the cards so that there is precisely one visible face of each
kind. Accordingly, the visible faces do not carry any information about
which model is correct, and hence subjects cannot use the visible faces
as evidence to tell between MD and M,. For this reason, we assume that
subjects can derive no information from the visible faces. However, even
if subjects did assume that the visible faces were generated by a random
process, this would make little difference to our calculations. This is
because taking visible cards into account would amount to revising the
parameter that determines the prior probabilities o(MD and M,, and,
as we shall see, the predictions of our model turn out to be relatively
independent of this parameter.
MIKE OAKSFORD AND NICK CHATER
Hence, by Equation 7, P(p\q, MD) = a/[a + (I - a)b]. In this
way, we computed the conditional probabilities required to calculate the E(Ig)s in the selection task in terms of P(p) (i.e., a), b,
and P(MD). Finally, we calculated b in terms of P(MD), P(p),
and the probability of q, f\q):
In summary, we derive the E(Ig)s from the three probabilities
P(M,) (recall that P[MD] = 1 - P[M,]), P(p), and P(q), which
are the only free parameters in our model. The inequality P(q)
^ P(p)P(MD) must be respected if these parameters are to be
consistent. This restriction arises because, if MD holds, every p
is associated with a q, and hence P(q) must be at least as high as
Model Behavior
We illustrate the behavior of the model in Figure 2. The three
parameters, P(p), P(q), and P(Mj) define a three-dimensional
space. We calculated E(Ig)s for each card for five values (.1, .3,
.5, .7, and .9) of each parameter. The not-p card does not appear
in Figure 2 because its E(Ig) value is always zero. At each coordinate, the three boxes represent the E(Ig) values for the three
cards. The area of the box is proportional to the E(Ig) for the
corresponding card.3 Coordinates where there are three dots indicate regions where the inequality P(q) ^ P(q)P(MD) is violated, and hence the probability values are inconsistent. Figure
2 reveals the following pattern of expected informativeness for
_ the four cards:
p card: Is informative insofar as P(q) is low. It is largely independent ofP(p).4
q card: Is informative when P(p) and P(q) are both small.
not-q card: Is informative to the extent that P(p) is large. It is
independent of P(q).
not-p card: Is not informative.
We highlight three aspects of the model's behavior. First, variation in P(Mi) rescales the E(Ig) values but does not change their
order for the four cards. In consequence, the relative informational value of each card is insensitive to the priors (but see
Footnote 4). Second, E[Ig(not-p)] is always zero. This is consistent with the intuition that conditional rules make no assertions
about what occurs if the antecedent is not satisfied. So, as we
suggested above, consulting nontripe eaters cannot tell you
whether eating tripe causes sickness. Third, when P(p) and P(q)
are small (in the bottom right-hand corner of Figure 2), E[Ig(q)}
is greater than E[I£riot-q)]. Figure 3 shows the entire region, R,
(shown in black) where E[Ig(q)] > E[Ig(not-q)], in more detail.
Here, as in all subsequent analyses (unless explicitly stated otherwise), P(Mj) is set to .5.5 At all other values of P(p) and P(q),
either E[I£not-q)} is greater than E\I£q)\ or these values are
undefined.
In modeling experimental data, we assume by default that
P(p) and P(q) lie within R; that is, subjects treat p and q as rare.
We refer to this assumption as the rarity assumption. Optimal
data selection, together with the rarity assumption, will allow us
to capture a wide range of experimental results. In the Discussion section, we consider whether the rarity assumption can it-
Figure 3. Plot of P(p) against P(q) with P(M,) = .5, showing the region
R (in black) where E[I£q)\ > E[Ig(not-q)}.
self be rationally justified. But first, we use this framework to
organize the data on the selection task.
Standard Abstract Results
We described the standard abstract selection task in the introduction. Abstract tasks use unfamiliar content and contrast
with thematic tasks that use familiar everyday contents. We discuss the thematic tasks later.
We conducted a meta-analysis of the abstract
data that revealed the following ordering in individual card selection frequencies: p > q > not-q > not-p. Table 2 shows the
results of the studies included in our meta-analysis.
We included all studies reporting individual card selection frequencies. We found 13 such studies reporting 34 standard abstract
selection tasks, involving 845 subjects.6 Table 2 shows the frequency
3 We thank Mike Malloch for suggesting this method of visualization
and for writing the software that generated this figure.
4 However, when P(M,) is low (i.e., P[MD\ is high), Equation 8 reveals
that b can still be low. This leads to high values of E[Ij(p)] when P(Mj)
is low, and P(p) and P(q) are high. See the top right-hand corner of Figure 3.
5 Much theoretical debate concerning the validity of Bayesian statistics centers on how to assign prior probabilities, and there is a complex
literature that considers how to do this . When we have just two discrete hypotheses, that ifp,
then q holds and that p and q are completely independent, the most
obvious prior, that each model has prior probability .5, is also the most
theoretically justified . We shall
assume this prior in all subsequent analyses except when it is explicitly
6 By "affirmative" we mean that the task rule contained no negations.
Below, we look at Evans's "negations paradigm," where negations are
included in the task rules.
RATIONAL ANALYSIS OF THE SELECTION TASK
Studies Reporting an Affirmative Abstract Version of the Selection Task
Card selection frequencies
Expt./condition
Wason 
Evans & Lynch 
Manktelow & Evans ( 1 979)
Origgs& Cox 
Griggs 
Chrostowski &Griggs 
Hoch&Tschirgi a
Valentine 
Yachanin 
Beattie& Baron 
Cosmides 
Girottoetal. 
Oaksford & Stenning ( 1 992)
M proportion cards selected
Expt. 1 /experimental
Expt. 1 /control
Single expt.
Expt. 1 /abstract
Expt. 2/abstract
Expt. 3/abstract
Expt. 4/abstract
Expt. 5/abstract
Expt. 1 /Trial 1
Expt. 1 /Trial 2
Expt. 3/Trial 1
Expt. 3/Trial 2
Single expt./nonmem./T-F
Single expt./nonmem./vio.
Single expt./nonmem./vio.
Single expt./nonmem./T-F
Single expt./bachelor's
Single expt./AA
Expt. 2/widgit/vio.
Expt. 2/widgit/test
Expt. 1 /four-card, +ve
Expt. 2/four-card, +ve
Expt. 3/four-card, +ve
Expts. 1 & 2
Expts. 3 & 4
Expt. 1 /arbitrary rule
Expt. 2/arbitrary rule
Expt. 3/arbitrary rule
Expt. 4/arbitrary rule
Expt. 2/abstract
Expt. 3/colored shape
Expt. 3/vowel-even
Expt. 3/control
Note. For all studies using Evans's negations paradigm , only the data for the
affirmative rule are included. Studies were only included where individual card selection frequencies were
reported or could be inferred from exhaustive reporting of card combinations. Expt. = Experiment; nonmem. = no memory cueing; T = true; F = false; vio. = violation condition; AA = affirmative antecedent
and affirmative consequent condition; +ve = affirmative consequent condition.
* Only the bachelor's condition is included because the other two conditions (high school and master's) were
not comparable to the subjects used in the remaining studies.
of individual card selections for each of these tasks and the average
across studies. We performed a one-way analysis of variance taking
task instance as the unit of analysis (see Glass, McGaw, & Smith
 for rationale), card type as the independent variable, and
proportion of cards selected as the dependent variable.7 This was
highly significant, P(3, 99) = 271.01, p< .0001. Post hoc Tukey
honestly significant difference tests revealed that each pairwise comparison between cards was significant at at least the .05 level. This
provides strong evidence for the p > q > not-q > not-p ordering in
card selection frequencies.
To model this ordering, we assume that by default subjects
are operating in region R of Figure 3. For every such point in R
the expected information gain is ordered such that
£['*(<?)] > E[f^not-q)] > E[I£not-p)}. Mean E(Ig) values sampled across R were E[I^p)} = .76; E(I£q)} = .20; E[I£not-q)} =
.09; and E[I^not-p)] = O.8 This order mirrors the/? > q > not-q
> not-p ordering in card selection frequencies. Therefore, card
selection frequencies are monotonically related to expected information gain. This relationship suggests that subjects base
7 While advocating the use of Bayesian statistics in our model, we
continue to use standard statistical tests in this article.
8 We selected the points by laying a grid over Figure 3 with a mesh
size of .025 on both axes. We then took the mean over all points that fell
MIKE OAKSFORD AND NICK CHATER
their card selections on the expected information gain of each
card. We tested the prediction that the cards are ordered in this
way using Page's L test for ordered alternatives . As expected, this proved to be
highly significant, L(N = 34, k = 4) = 1,007.5, ZL = 9.36, p <
Nonindependence of Card Selections
Some studies have investigated whether card selections are
statistically associated. In an analysis of just the q and not-q
cards, Evans found that selection of these two cards was
statistically independent. However, in a more detailed metaanalysis of three experiments, Pollard found consistent
associations between card selections. He found that similarly
valenced cards, that is, the p and q cards and the not-p and notq cards, are positively associated, whereas selections of dissimilarly valenced cards—that is, p and not-p, p and not-q, q and
not-p, and q and not-q—are negatively associated. Although
these associations are not always statistically significant, their
direction, positive or negative, is consistent across experiments.
We confirmed and extended these findings in a further metaanalysis. We took data from the studies analyzed by Pollard
 and five further studies, three from Oaksford and Stenning and two
further unpublished control experiments (O & SI and O & S3).
All these experiments used task rules with negations varied in
their antecedents and consequents, a manipulation that we discuss in the next section, Negations Paradigm Selection Task.
Table 3 shows the results of our meta-analysis (see the rows labeled "AA" in Table 3). For the AA rule (affirmative antecendent, affirmative consequent) the following identities should be
borne in mind: TA = p card; FA = not-p card; TC = q card; FC
= not-q card (we explain the TA, FA, TC, and FC categories in
the next section).
We performed the meta-analysis following Pollard . We
tested all six possible pairwise associations using Fisher's exact
tests in the direction of the association present. We assigned a
positive or negative z score to each result, setting z to 0 if the
test yielded p > .5 in either direction (because this reveals a
two-tailed probability of 1.0). We then calculated combined z
estimates for each comparison and rule form using Stouffer's
method . Concentrating on the AA rule form, the
combined estimates (see Table 3) were all significant (onetailed) apart from the positive association between p (TA) and q
(TC). The signs of the associations never reversed for any of the
six pairs across all eight experiments. This was significant in
one-tailed binomial tests for each of the six associations (p < .005).
To model these associations we make three assumptions
about how E(Ig)s map onto card selections. First, we assume
that every card has some probability of being chosen because
some subjects will simply not perform any, or an appropriate,
analysis. In particular, subjects will sometimes choose the not-p
card, with E[I^not-p)] = 0. We account for this by adding a
small fixed constant (.1) to the E(Ig)s for each card. Second,
because of the four cards present in the selection task, we assume that card choice is a competitive matter. A card should
have a greater chance of being chosen if it is less distinguishable
from alternatives. One way to ensure that this happens for all
four cards, including the not-p card, is to scale the E(Ig)s by the
mean information available. We do this by dividing the derived
score, E(Ig) + . 1, for each card by the mean of this quantity
for all four cards. We refer to this value as the scaled expected
information gain, SE(Ig). We assume that subjects choose cards
as a monotonic function of their SE(Ig) value. Third, a reasonable constraint on values for F\p) and P(q) is that P(q) ^ f\p),
otherwise the dependency model could not hold.
We sampled a variety of points corresponding to pairs of values for P(p) and P(q) at intervals of .025. The points satisfied
the inequalities, P(p) < .2, P(q) < .2, P(q) > P(p), and P(q) <
f\p) + .025. The first two inequalities enforce the rarity assumption. The third inequality ensures that the dependency
model can hold. The last inequality corresponds to the reasonable constraint that although the probability of q is greater than
the probability of p, it is only marginally greater . In the Discussion section, we discuss the justification
of this constraint. We calculated SE(Ig)s for all four cards for
each pair of f\p) and P(q) values (mean SE[Ig]s for each card
appear in the AA row in Table 4). We then computed Spearman
rank order correlation coefficients between the SE(Ig)s for all
six card pairs. We used rank correlations because we assume
only that card selection is a monotonic function o(SE(Ig). The
results of these analyses appear in the AA column in Table 5.
The SE(Ig)s for the similarly valenced cards are positively correlated, whereas the SE(Ig)s for the four dissimilarly valenced
card comparisons are negatively correlated. This pattern of correlations is the same as that observed experimentally. The
agreement in the sign of the correlation between model and data
was significant in a one-tailed binomial test (p < .025).
This analysis applies only to the AA or purely affirmative
rule. We now show how to extend this analysis to account for
data from the negations paradigm selection task.
Negations Paradigm Selection Task
In the negations paradigm selection task , the antecedent and consequent of a rule can contain negated constituents (not-p, not-q). There are four possible conditional rules: the original ifp, then q (AA), together with ifp, then
not q (affirmative antecedent, negative consequent [AN]); ifnotp, then q (negative antecedent, affirmative consequent [NA]);
and if not-p, then not-q (negative antecedent, negative consequent [NN]). Each subject performs a selection task for each of
these four rule types.
We have so far described the cards in the selection task in
terms of p, q, not-p, and not-q. In the negations paradigm, cards
are normally described in terms of whether they make the antecedent or consequent of the rule true or false. For example, con-
RATIONAL ANALYSIS OF THE SELECTION TASK.
t-- — 0 0 * ^ 1 0 — ^
m Q ^ — - Q ^ O —
— o - ^ r ^ o v ^ ^ o
O O O G J O O O O O O — O O O O — • -* O ^
_ _ , _ O C ) _ « _ « O — O — ' O O O (N <N —
( N f l o O T t O O v r v l O O O O ^
f S — < N — O O O O O O O O O O O O O O O O O O O O
— f N O f N O O — O O O O — ' O O O O — <N O O —' O < N < N
( S — . o — • O O O — . o — — O t N O ( N O t N — O — O O O — •
I I I I I I
I — O fN —' O — ' <N <N —' — < N < N — O < N
J — - O — • — ^ ' « O ( N O ( N — ' C N O — ^ O ( N — ' -
O O O ( N t ^ < J s o C O O l / ~ > C ^ O N r - - < N — Ol— O N O O O
O ( N — O O O O O O O O — O O O O O O O
z z « z z « z z « z z « z z
Z < Z < Z < Z < Z < Z < Z < Z < Z < Z
<= .2 ST c
"2 <a 55 ii
.Si, ? §• 5
if-S43 M i
MIKE OAKSFORD AND NICK CHATER
Mean SE(Ig)sfor Each Card for Each of the Four Rule Types in the
Negations Paradigm Selection Task
2.43(19.67)
2.31(21.00)
2.26(16.00)
2.12(18.5)
0.87(13.83)
0.55 (4.67)
1.08(15.83)
0.62(10.83)
0.43(6.33)
0.69(14.83)
0.42 (8.67)
0.95(12.33)
0.27 (4.00)
0.44(2.67)
0.23(1.23)
0.31 (6.00)
Note. p(N = 16) = .92 (p < .0001). Figures in parentheses indicate the mean number of each card selected
in the eight studies mentioned in the text. SE(Ig) = scaled expected information gain; TA = true antecedent;
TC = true consequent; FC = false consequent; FA = false antecedent; AA = affirmative antecedent and
affirmative consequent; AN = affirmative antecedent and negative consequent; NA = negative antecedent
and affirmative consequent; NN = negative antecedent and negative consequent.
sider the rule "if there is not an A on one side, then there is not
a 2 on the other side." For this rule the p card (A) and the q (2)
card are the false antecedent (FA) and false consequent (FC)
cases, respectively. The not-p (K) and not-q(7) cards are the true
antecedent (TA) and true consequent (TC) cases, respectively.
The TA, FA, TC, and FC logical cases permit a uniform classification of the cards in the negations paradigm.
Using the negations paradigm, Evans and Lynch reported an effect that they called matching bias. Subjects tend to
select the cards that are named in the rules, ignoring the negations. For example, consider the rule "if there is not an A on one
side, then there is not a 2 on the other side," and the four cards
showing an A, a K, a 2, and a 7. The matching response is to
select the A (FA) and the 2 (FC) cards. The confirmatory response is to select the K (TA) and 7 (TC) cards, and the falsificatory response is to select the K (TA) and 2 (FC) cards. More
recently it has become clear that matching occurs mainly for
the consequent cards ,
whereas antecedent card selections accord with the logical case
(subjects choose the TA card in preference to the FA card for all
four rule forms).
Card orderings. As in the standard selection task, we can
describe these data in terms of orderings over card selections.
The ordering in card selection frequencies for affirmative consequent rules (if p, then q; if not-p, then q) is TA > TC > FC
> FA. The ordering in card selection frequencies for negative
consequent rules (ifp, then not-q; if not-p, then not-q) is TA >
FC > TC > FA. Both these orders are consistent with matching.
A meta-analysis confirmed these orderings. We analyzed the
studies used in our meta-analysis of nonindependence of card
selections, less O & S4 and O & S5. We omitted these studies
because they included manipulations to counteract matching
(see below). We performed a one-way analysis of variance for
each rule type (AA, AN, NA, and NN) with card type as the
independent variable and number of cards selected as the dependent variable. For the affirmative consequent rules these
analyses were significant: AA, F(3, 15)= \ll.Sl,MSe = 2.75,p
< .0001; NA, F(3, 15) = 21.19, MSe = 6.78, p < .0001. For both
rules the order of mean number of cards selected reflected the
TA > TC > FC > FA order. Each pairwise comparison between
cards was significant at at least the .05 level in post hoc Neuman-Keuls tests except for the TA versus TC and FA versus FC
comparisons for the NA rule. Similar analyses were also significant for the negated consequent rules: AN, F(3, 15) =
100.64, MSC = 4.45, p < .0001; NN, F(3, 15) = 11.64, MSC =
13.69, p < .0005. For both rules the order of mean number of
cards selected reflected the TA > FC > TC > FA order. Each
pairwise comparison between cards was significant at at least
the .05 level in post hoc Neuman-Keuls tests except for the FA
versus FC comparison for the AN rule and the TC versus FC
comparison for the NN rule.
The mean frequencies of card selections for each rule type
appear in parentheses in Table 4. This table reveals that the orderings in card selections are weaker for the negative antecedent
rules, NA and NN. The NA rule also appears to cause comprehension problems. Subjects are significantly slower to comprehend this rule than each of the other three rule forms . This result is unexpected because
the standard finding has been that sentences containing negations are harder to comprehend . This would predict that a sentence with two negations
should be harder to comprehend than similar sentences containing single negations. However, subjects comprehend the NA
rule significantly more slowly than the NN and the AN rule.
Suppressing matching. Two experimental manipulations
suppress matching and reestablish the TA > TC > FC > FA
order for all four rule types in the negations paradigm. First,
although Manktelow and Evans initially found matching
even with thematic material, Reich and Ruth and Griggs
and Cox found that matching disappeared when they
used more realistic thematic material. Second, Oaksford and
Stenning found that matching also disappears when the
linguistic framing of the rules is more appropriate. They argued
that rules such as "if there is not an A on one side, then there is
a 2 on the other side" are ambiguous. The not-A card could be
the K, or the 2 or the 7 cards. Oaksford and Stenning found that
removing this ambiguity suppressed matching and reestablished the TA > TC > FC > FA order for all four rule types.
Nonindependence of card selections. Pollard also investigated associations between card selections for the negations
RATIONAL ANALYSIS OF THE SELECTION TASK
Spearman Rank Order Correlation Coefficients Between SE(Ig)sfor Each Card Pair for Each
of the Four Rule Types in the Negations Paradigm Selection Task
Comparison
0.11(0.59)
0.79(3.45)
-0.69 (-2.44)
-0.21 (-4.39)
-0.64 (-2.56)
-0.92 (-1.84)
-0.47 (-1.54)
-0.70 (-0.09)
-0.94 (-0.28)
-0.22 (-2.90)
0.59 (-2. 14)
-0.53(2.03)
-0.85 (-0.11)
0.67 (-0.59)
0.54(1.11)
0.97 (-6.49)
-0.88 (-0.36)
-0.93(0.13)
0.99(3.69)
0.92(2.95)
-0.99 (-3.23)
-0.92 (-7.08)
-0.99 (-4.72)
-0.94 (-2.73)
Note. Phi coefficient (r^) = .64 (p < .025; Fisher's exact test). Figures in parentheses indicate the combined
z scores for each pairwise comparison taken from Table 3. SE(Ie) = scaled expected information gain; AA
= affirmative antecedent and affirmative consequent; AN = affirmative antecedent and negative consequent;
NA = negative antecedent and affirmative consequent; NN = negative antecedent and negative consequent;
TA = true antecedent; TC = true consequent; FA = false antecedent; FC = false consequent.
paradigm. His meta-analysis looked at associations between
card cases (p, not-p, q, and not-q). He found positive associations
for similarly valenced cards and negative associations for dissimilarly valenced cards, not only for the standard affirmative
rule (AA) but also for the remaining three rules (AN: ifp, then
not-q; NA: if not-p, then q; and NN: if not-p, then not-q).
Our extended meta-analysis, shown in Table 3, shows these
associations in terms of the logical case. The overall combined
z score (final column in Table 3) for each pairwise comparison
treats each rule in each study as a separate unit of analysis (N =
32). The final column of Table 3 shows that the signs of the
combined z scores are positive for similarly valenced cards (TA
vs. TC and FA vs. FC) and negative for dissimilarly valenced
cards (TA vs. FA, TA vs. FC, TC vs. FA, and TC vs. FC). These
results follow the pattern for the standard selection task, discussed in the section Nonindependence of Card Selections.
Although this finding is clear for the similarly valenced rules,
AA and NN, the results for the dissimilarly valenced rules, AN
and NA, are more ambiguous. We highlight this division in Table 5, where we summarize the combined z scores for each rule
separately (see figures in parentheses).
The key to understanding a variety of effects in the negations
paradigm is the notion of a "contrast set" . Contrast sets provide the interpretations of negated constituents. For example, the interpretation of "Johnny didn't serve tea" (where the word in italics indicates the focus of a negation) is that he served a drink other
than tea. In terms of set theory, the superordinate category
"drinks" provides the universe of discourse. Contrast sets are
plausible subsets of the complement in a universe of discourse.
In our example, all other drinks less tea form the complement.
When Johnny didn't serve tea it is more likely he served soft
drinks rather than, for example, scotch on the rocks. Soft drinks
is therefore the contrast set, that is, a plausible subset of the
complement. Background knowledge may restrict the membership of the intended contrast set even further. So, in our example, coffee is perhaps the most likely single contrast set member.
This indicates that a negation rarely identifies the complement,
that is, the whole set consisting of the superordinate category
less the named constituent, as the intended contrast set. More
commonly the intention is to identify much more restricted
contrast sets. We now apply this behavior of contrast sets to the
negations paradigm.
We have good reason to believe that P(TA) or PCTC) are
greater when they are negated. This is because the class of things
referred to by a constituent is generally smaller than the size of
the contrast set defined by its negation. For example, there are
many things John could have drunk when he didn't drink tea.9
However, the intended contrast set is unlikely to be all drinks
other than tea. We made the reasonable assumption that the
probability of a contrast set does not exceed .5.'° We therefore
set the probabilities of unnegated constituents to vary between
0 and .2 as for the AA rule, and those of negated constituents to
vary between .2 and .5.
Card orderings. We explained the AA rule above. For the
other three rule types we sampled points at .025 intervals as in
the section Nonindependence of Card Selections. As for the A A
rule, we now outline our rationale for the region over which we
sampled points for each rule form. For the AN rule, P(T\) is
low but PCTC) is high. We therefore sampled points in the region that satisfied the inequalities, P(p) < .2 and .2 < P(q) < .5.
For the NA rule, P(TA) is high but PCTC) is low. This rule therefore violates the inequality that PCTC) > PCTA)P(MD) (see
Equation 8); that is, this rule corresponds to the region of the
parameter space in Figure 3 where E(Ig) is undefined. Informally, an NA rule is like the hypothesis that all black things are
ravens. This rule must be false because there are more black
9 It is possible that the token frequencies of the constituent are so large
that they might outweigh the token frequencies of all the other possible
drinks (i.e., if tea is the most common drink). But if this is true for tea,
it cannot be true for any other drink. Hence, in the domain of drinks,
negations will still most commonly identify highly probable contrast
10 Note that this means that the probability of an event and its most
plausible contrast set will rarely sum to 1, although the probability of
an event and its complement must sum to 1.
MIKE OAKSFORD AND NICK CRATER
things than ravens (i.e., the dependency model could not hold).
So, to interpret an NA rule as having a chance of being true
involves either revising /^TA) down or revising f\JC) up. Subjects appear to resolve the ambiguity by revising /fTA) down.
The NA rule then leads to low P(TA) and P(TC) values. However, it is reasonable to assume that P(TA) is still greater than
PCTC) because the antecedent is negated. We therefore sampled
points that satisfied the inequalities, P(p) < .2, P(q) < .2, f\q) <
P(p), and P(q) ^P(p)- .025. For the NN rule, both P(TA) and
P(TC) are high. We modeled this rule by restricting the values
of P(q) and f\p) to the range .2 to .5. We therefore sampled
points from the region that satisfied the inequalities, .2 < P(p)
< .5, .2 < P(q) < .5, F\q) > P(p), and P(q) >P(p) + .025.
Mean SE(Ig) values for points sampled across the four regions
appear in Table 4. The mean number of cards selected in the
eight studies used in our meta-analysis of the negations paradigm selection task appear in parentheses. As can be seen, there
was a good fit between data and model, Spearman's p( 16) = .92,
p < .0001. Finally the need to resolve the ambiguity between
revising P(TA) down or P(TC) up accounts for why subjects are
significantly slower to comprehend the NA rule than each of the
other three rule forms .
Suppressing matching. "Realistic" thematic content restores the TA > TC > FC > FA ordering in card selection frequencies for all rule forms . We argue that
this is because prior world knowledge restricts contrast sets to
the most plausible member(s). For example, if Johnny didn't
drink tea, then it is most likely that he drank coffee. In a context
where drinking tea is a possibility, drinking, for example, scotch
on the rocks is probably not. Relative to the class of drinks, tea
and coffee are both rare (subject to the caveats in Footnote 9).
Such examples show that familiar thematic material reduces
contrast set size, thereby reestablishing rarity. Unrealistic thematic content fails to restore the TA > TC > FC > FA ordering
because it cannot engage prior
knowledge to constrain contrast sets in this way.
The same reasoning explains the restoration of the TA > TC
> FC > FA ordering for all rule forms in Oaksford and Stenning
 . The intention in this task was for subjects to regard the
not-A contrast set to consist of only the ATcard. However, the K,
2, and 7 cards are all potential members of the not-A contrast
set. To restrict this contrast set the materials had to indicate
unambiguously that only other letters are potential contrast set
members. Oaksford and Stenning used the original vowels and even numbers material used by Wason . "Vowels"
and its complement set "consonants" only apply to letters. In
the context of the task, where AT is the only consonant, it should
therefore be clear that K is the only possible member of the not-
A contrast set. The antecedent is therefore unambiguously
about the K card, and so rarity again holds. This predicts the
standard TA > TC > FC > FA ordering for all four rule forms
as Oaksford and Stenning found.
Nonindependence of card selections. Even when negated
constituents are used, similarly valenced cards show positive associations and dissimilarly valenced cards show negative associations. We modeled this behavior in the same way as the AA
rule in the section Nonindependence of Card Selections. We
calculated Spearman rank order correlation coefficients for
each card pair for the same sets of points used to calculate mean
SE(Ig) values in Table 4. These coefficients appear in Table 5
together with the combined z scores taken from Table 3 (in parentheses). We assessed the fit between data and model in terms
of the direction (+/-) of association or correlation using the phi
coefficient , which
showed a significant fit between data and model (r^, = .64, p <
.025). The fit was perfect for the AA and NN rules. For the AN
and NA rules the fit was less good, although the model does
capture some of the interesting differences between the AA-NN
rules and the AN-NA rules. The poorest fit was for the NA rule
where subjects must revise P(TA) down and where they experience comprehension problems. Such problems may lead to a
residual matching tendency. The overall agreement between
data and model was highly significant, as assessed by a onetailed binomial test explicitly manipulated P(p) in a selection task. To understand the rationale behind Kirby's studies,
it is necessary to briefly outline his theoretical account of the
selection task. This account uses the vocabulary of subjective
expected utility and signal-detection theories. Kirby started
from the falsificationist assumption that a subject's goal is to
find p and not-q instances, which he called an inconsistent outcome. He then assumed that two factors determine performance. The first factor concerns the probability of an inconsistent outcome arising from a visible card face (Q. So like our
model, Kirby's took into account the probabilities of the "fictional" other sides of each card (fictional because the subjects
never turn the cards). The second factor concerns the utilities
associated with a card choice. There are four possibilities,
which Kirby classified using signal-detection theory: a hit, that
is, choosing a card with a hidden face that is inconsistent with
the rule; a miss, that is, not choosing such a card; a false alarm
(FA), that is, choosing a card with a hidden face that is consistent with the rule; and a correct rejection, (CR), that is, not
choosing such a card.
Kirby proposed that a subject should choose a card
when the posterior odds of an inconsistent outcome exceed a
simple function of the utilities (see Equation 9). In deriving predictions for his Experiments 1 to 3, Kirby assumed that the
utilities on the right-hand side of Equation 9 remain constant.
/•(inconsistent outcome present | Q
t/(CR) - t/(FA)
/"(inconsistent outcome absent | Q
t/(hjt) - £/(miss)
On this analysis the q and not-p cards have probability 0 of
yielding an inconsistent outcome. Therefore, as with other falsificationist accounts, Kirby predicted that subjects
should never turn these cards. The interest of his account therefore centers on the p and not-q cards.
Kirby noted that in Wason's original rule, if a
card has a vowel on one side, then it has an even number on the
other side, the posterior odds of finding an inconsistent outcome
RATIONAL ANALYSIS OF THE SELECTION TASK
SE(Ig)s for Each Card Using the P(p), P(q) values Used by Kirby 
in His Experiments 1-3, With P(M,) = .01
P(p)a.nAP(q)
Small P set
Large P set
1,000/1,001
Experiment 1
0.746 (.49)
1.372 (.73)
1.311 (.78)
0.911 (.55)
0.745 (.29)
0.858 (.48)
1.198 (.60)
0.858 (.41)
Small P set
Medium P set
Large P set
Experiment 2
0.745 (.40)
1.009 (.53)
1.248 (.61)
1.332 (.91)
1.337 (.80)
1.262 (.80)
0.740 (.17)
0.745 (.27)
0.737 (.38)
1.182 (.50)
0.909 (.57)
0.753 (.56)
Small P set
Medium P set
Large P set
Experiment 3
0.745 (.26)
1.009 (.39)
1.248 (.39)
1.332 (.88)
1.337 (.84)
1.262 (.78)
0.740 (.11)
0.745 (.28)
0.737 (.41)
p(N= 12) = .69(p<.025)
1.182 (.39)
0.909 (.61)
0.753 (.61)
Note. P(M,) = .01. Figures in parentheses show the proportion of cards that were selected in Kirby's
experiments. SE(Ig) = scaled expected information gain; P(Mi) = probability of the independence model.
with the not-q card are low, that is, 5/21 (this analysis assumes
that each letter is equally probable and that there are 5 vowels
and 21 consonants). Kirby suggested that this might be why subjects do not select the not-q card. Equation 9 predicts that these
odds will increase if P(p) is larger, and hence that subjects should
choose the not-q card more frequently. Equation 9 predicts no
changes for the p, not-q, and q cards, however.
In Kirby's Experiments 1 to 3, subjects checked
whether a computer had made a mistake in generating cards
with integers between 0 and 1,000 (or 0 and 100 in Experiments
2 and 3) on one side and either a "+" or "-" on the other side.
In Experiment 1, subjects were told that the computer had an
error rate of .01, and in Experiment 2, they were told it had an
error rate of. 1. In Experiment 1, the rules were if there is aOon
one side, there is a + on the other side (small P set condition),
and if there is a number between I and 1,000 on one side, there
is a + on the other side (large P set condition). If each number is
equally probable, then when 0 is the antecedent P(p) is 1/1,001
and when any number between 1 and 1,000 is the antecedent
P(p) is 1,000/1,001. In his Experiments 2 and 3, Kirby used
three values, so that P(p) =1/100, 50/100, or 90/100.
As he predicted, Kirby found that selections of the
not-q card increased as P(p) increased (see the figures in parentheses in Table 6). However, he also found unpredicted movements in the frequency of card selections for the other cards. As
f\p) increased, selections of the p card and q card decreased and
selections of the not-p card increased, although the finding for
the q card was not robust. Kirby considered a variety of possible
explanations for these effects that we now argue are direct consequences of our model of optimal data selection.
From the perspective of optimal data selection, the failure of
Kirby's model to predict the movements in card selections for the p, not-p, and q cards (indeed, the failure to predict
that subjects should choose the q and not-p cards at all) is due to
its exclusive focus on falsifying instances. In contrast, in our
optimal data selection framework, a card can still be informative even though it could not yield a falsification. This permits
us to model these data straightforwardly.
Our model predicts that the not-q card is informational to the
extent that F\p) is large. Our model therefore predicts Kirby's
 principle finding. It moreover predicts the other changes
in the frequency of card selections that he observed for the p,
not-p, and q cards. The independent variables Kirby manipulated in his Experiments 1 to 3 correspond closely to the parameters of our model. Kirby varied P(p) directly. We assume that
P(q) = P(p). This assumption is reasonable because Kirby's materials are binary, that is, the antecedent is either 0 or 1 to 100
(or 1,000) and the consequent is "+" or "-." Staudenmayer and
Bourne interpreted the effect of such material as leading
to a biconditional interpretation, which is consistent with the
assumption that P(q) = P(p). Kirby provided specific information about the error rate; that is, in Experiment 1 it was .01 and
in Experiment 2 it was. 1. We assume that subjects use the error
MIKE OAKSFORD AND NICK CHATER
rate as an estimate of the probability of the independence
model. However, because an error rate of. 1 is unreasonably
high for a computer, we assume that subjects take P(M,) to be
.01 for both Experiments 1 and 2. Using these parameter values
we calculated SE(Ig)s for all cards in each condition of Kirby's
Experiments 1 and 2. These appear in Table 6 together with the
proportions of cards selected in Kirby's data (in parentheses).
In his Experiment 3, Kirby provided no error rate information.
Nonetheless, it is reasonable to assume that subjects regard
computers as having a low error rate. Hence, we used the same
parameter values to model Experiment 3 as in Experiment 2.
As can be seen from Table 6, the fit between data and model
is very good. The correlation for Experiment 1 is .87 and for
Experiment 2 is .90. The fit in Experiment 3 is weaker (.69) but
nonetheless significant. This may be due to subjects assuming a
broader range off^Mj) values.
Therapy Experiments
As their names suggest, these experiments involved
therapeutic procedures to help subjects see where they were going "wrong" and to encourage them to adopt the falsificatory p,
not-q selection. The experimenter engaged subjects in a dialogue concerning their task performance, exposing them by degrees to inconsistencies "between their initial selections of cards
and their subsequent independent evaluations of specific cards
as falsifying or verifying the rule" . In Wason's study , subjects performed an
initial selection task and were then given three increasingly direct therapies: weak hypothetical contradiction, strong hypothetical contradiction, and concrete contradiction. Each therapy was followed by a further selection task to assess whether it
was successful. Subjects also performed a final selection task,
making five in all.
The therapies aimed to get agreement that a card with p and
not-q face falsified the rule. Three therapies laid an increasing
emphasis on the not-q card. The weak hypothetical contradiction therapy focused on the p card. The strong hypothetical contradiction therapy focused on the not-q card. Hypothetical contradictions involved asking subjects what they thought could be
on the hidden faces of the cards and getting them to agree that a
p and not-q instance falsified the rule. In the concrete contradiction therapy, the experimenter turned over the not-q card to
reveal a p on the hidden face.
These attempts at therapy were not wholly successful. By the
final selection task only 42% adopted the falsificatory response.
The steps by which these subjects moved to this response followed two main patterns , which later became
the focus of various "insight" models (see below). In the first,
subjects-begin with an initial p card only response, they then
mov£ to £,p, q, not-q response and finally to a p, not-q response
(i.e.^ p => a, q, not-q => p, not-q). Notice that therapy on the notq cakd unexpectedly causes some subjects to choose the q card.
In the\seobnd, subjects begin with an initial p and q response,
they then move to a p, q, and not-q response, and finally to a p,
not-q response . Subjects successfully completed the second sequence less often (just 23. 1 % of subjects who made the initial p,
q selection, rather than 62.5% of subjects who made an initial p
selection [Wason, 1 969]).
Various "insight" models attempted to explain these transition sequences . A common feature of these models is that
they postulate three levels of insight into the task, which subjects pass through sequentially and which correspond to the
stages in the transition sequences identified in the last paragraph. Thus, in a state of "no insight," the subjects turn the p
card and possibly also the q card; in a state of "partial insight,"
they turn the p, q, and not-q cards; and in a state of "complete
insight," they turn the p and not-q cards .
We explain these findings by the way subjects use the ordering
in SE(Ig) values (below) to determine which cards to select:
Let us consider the first transition sequence, which is the most
striking, because therapy concerning the not-q unexpectedly induces the selection of the q card. The subject initially chooses p
only and assumes that the SE(Ig) values of the other cards are
not high enough to warrant their being turned. The effect of
therapy is to persuade the subject that the experimenter considers that the not-q is worth turning. If, as we have assumed, subjects consider card choice to be a monotonic function
then they should also turn the q card, because SE[I^q)] >
SE[Ig(not-q)]. Hence, subjects subsequently choose p, q, and
not-q. Subjects require further therapy to persuade them to violate monotonicity concerning the informativeness ordering and
choose only p and not-q. We explain the second transition sequence in the same way, the only difference is that subjects' initial card selections are p, q. Thus, we can account for the main
transition sequences observed in the data.
Subjects are reluctant to make the falsificatory response, even
when strongly prompted to do so (only 42% of subjects finally
make the/?, not-q), because falsification requires them to violate
the informativeness ordering. Our model does not directly predict that the first transition sequence should lead to more p,
not-q responses. However, it does suggest a possible explanation.
Subjects' reluctance to move to the p, not-q response stems from
the tendency to want to turn the q card. It may, therefore, be
difficult to persuade subjects not to turn the q card when they
turned it initially. Hence, subjects who initially select p, q are
less likely to complete the transition sequence than subjects who
initially select p.
Reduced Array Selection Task (RAST)
In a RAST, subjects choose between the q and not-q options
only . The stimuli in the original RAST consisted of 30 colored shapes. The experimenter informs the subjects that there are 15 black shapes and 15 white shapes, each of
which is a triangle or a circle. The shapes are in two boxes, one
containing the white shapes and the other containing the black
shapes. On being presented with a test sentence, for example,
All the triangles are black, subjects have to assess the truth or
falsity of the sentence by asking to see the minimum number of
black or white shapes. In Johnson-Laird and Wason ,
although all subjects chose some confirmatory black shapes (no
subject chose more than 9), they all chose all 15 potentially falsificatory white shapes. Thus, where subjects in effect perform
multiple selection tasks, they tend to show falsificatory
Wason and Green reported a variant on this task. In
one condition, the materials consist of cards colored on one half
and depicting a shape on the other half. In this condition, the
rule is disjoint, for example, All the cards that have a triangle on
one half are red on the other Aa/f (the All the triangles are red
rule they described as unified). In this condition, Wason and
Green found that subjects predominantly select the q card.
They also observed that their "experiments show relatively
good performance in reasoning about conditional sentences using the RAST technique" . Even
in the disjoint rule condition there was a falsificatory response
rate of between 29% and 45%, compared with a falsificatory
response rate as low as 4% in the standard selection task.
The RAST makes explicit that the rule applies to a limited
domain of cards or shapes that the experimenter describes as
being in a box or a bag (or in Wason & Green , "under
the bar"). The experimenter also informs subjects that in this
limited domain, there are equal numbers of q and not-q instances. It follows that P(q) = .5, violating the rarity assumption. At this value, E[I£not-q)] is higher than E[I^(q)] (Figure
2), and hence our model predicts more not-q card selections
than q card selections.
Our model does not directly predict that a disjoint rule reduces the facilitatory effect of the RAST . It does suggest a possible explanation, however. The standard RAST rule specifies class inclusion, whereas the disjoint
rule specifies a relationship between two distinct items. The latter suggests a causal relationship that cannot be restricted to the
limited domain of cards "in the bag" or "in the box" . Hence, the default rarity assumption may again be
made in the disjoint condition.
Thematic Selection Task
Most recent work on the selection task has concentrated on
how thematic content affects reasoning . In the selection task, this work originated in the
attempt to facilitate falsificatory reasoning . For example, subjects may have to imagine that they are an immigration
official enforcing the rule that If a passenger's form says "EN-
TERING " on one side, then the other side must include cholera
 , or they may have to imagine that they are a tribal elder
enforcing the rule that If a man eats cassava root, then he must
have a tattoo on his face . Subjects are also
given a rationale for enforcing the rule (the prevention of disease
and that cassava root is a rare aphrodisiac that only married
men, who have their faces tattooed, are allowed to eat). These
thematic rules have typically facilitated the selection of the p
and not-q cards.
Researchers now generally accept that these versions of the
task address peoples' abilities at deontic reasoning, that is, reasoning concerning what ought to be the case . In the abstract tasks, subjects
"are asked to decide whether an indicative conditional is true or
false, while in ...
[the deontic tasks] . . . they are asked
whether a conditional obligation has or has not been violated.
A conditional obligation, of course, is not falsified when it is
violated" . Thus, a subject's
task in the deontic versions is very different to that confronted
in the abstract versions of the selection task. However, we argue
that the same probabilistic framework we used for the abstract
selection task also applies to these data.
We describe these findings following Gigerenzer and Hug's
 classification by rule type and perspective. We also discuss recent work by Kirby , who manipulated probabilities and utilities in a thematic task, in a separate section Utilities
and Probabilities of Fictional Outcomes.
Rule Type and Perspective
Data. There are two dimensions on which the pattern of
cards selected in the thematic selection task depends. The first
is rule type. Cheng and Holyoak used rules like If a passenger's form says "ENTERING" on one side, then the other
side must include cholera, which they described as "permissions." However, as Manktelow and Over observed, these rules are actually of the form of an obligation, that is, people who want to carry out the action described in the antecedent are obliged to satisfy the condition
stipulated in the consequent. Obligations are of the form if action (p), then must condition (q). A corresponding permission
would be If a passenger's form includes cholera on one side, then
they may enter the country. Here people who have satisfied the
condition described in the antecedent are permitted to perform
the action described in the consequent. Permissions are of the
form if condition (p), then may action (q). Notice that in going
from an obligation to a permission, action and condition switch
their clausal positions between antecedent and consequent of
the conditional sentence.
The second dimension on which the pattern of card selections
depends is the perspective a subject must adopt. Using an obligation rule, Cheng and Holyoak had subjects adopt the
role of enforcers of the rule, that is, subjects had to imagine they
MIKE OAKSFORD AND NICK CHATER
were immigration officials checking immigration forms. They
found that subjects were more likely to select the p and notq card combination under these conditions. Cosmides 
replicated this finding and
showed that from the enforcer's perspective, a permission (what
Cosmides called a "switched social contract") led subjects to
select the not-p and q cards. (Notice that both these responses
still correspond to selection of the action and not-condition pair.)
Using the obligation rule, Cosmides also asked subjects
to adopt the role of inquirer's into whether a deontic rule was in
force. She found similar results to those found in the abstract
selection task.
Manktelow and Over were the first to argue that social
role or perspective was an important factor in the deontic selection task. They induced response shifts between the p, not-q selections and the not-p, q selections by asking subjects to adopt
different perspectives. They used a permission rule and two perspectives: what we have been calling the enforcer's perspective
and what we shall refer to as the actor's perspective. For the
enforcer's perspective Manktelow and Over found the
same not-p and q card selections as Cosmides , but for the actor's perspective they
found that subjects predominantly chose the p and not-q cards.
We illustrate the reason for this change using the permission
form of the cholera rule. From the enforcer's perspective cheaters are people who try to enter the country without having been
inoculated against cholera, that is, relative to the permission
rule the not-p and q instances. However, for an actor, that is,
someone trying to enter the country, the enforcer cheats if having had the inoculations the actor is still not let into the country,
that is, again relative to the permission rule, the p and not-q
instances. Gigerenzer and Hug showed that the actor's
perspective on an obligation led subjects to select the not-p and
q cards, because an obligation reverses the clausal position of
action and condition. Gigerenzer and Hug also manipulated the same rules systematically along both dimensions (except the inquirer's perspective) for the first time.
The thematic selection task requires that we refocus
our existing probabilistic model away from rule testing and on
to rule use. In modeling rule testing, we used our basic probability model defined by the dependence and independence matrices (Table 1) to calculate expected information gain. In modeling rule use, we use these probability models to calculate expected utilities and argue that subjects use the rules to
maximize expected utility.
We assume that there is a small fixed cost for turning any
card. This cost is implicit in the task, because the instructions
say that subjects should only pick the cards that they "would
have to" or "must" turn. We further assume that subjects associate particular utilities with particular card combinations, dependent
on the perspective they adopt and on the particular rule.
The enforcer's goal is to discover instances of rule violation,
that is, where the actor performs the action without satisfying
the condition. We model this by assigning a positive utility to
instances of rule violation that is larger than the cost of turning
over a card (otherwise subjects would have no incentive to turn
cards at all). Subjects in the enforcer's perspective associate no
Utilities of Card Combinations for the Enforcer
and Actor Perspectives
Perspective
Not-condition
Not-condition
other cards with positive utility. In particular, this means that
whether someone performs the action when they satisfy the condition is not the enforcer's concern. The actor's goal is to discover instances of unfairness, where the enforcer disallows the
action even though the actor satisfies the condition. We model
this by assigning a positive utility to uncovering instances of
unfairness that is larger than the cost of turning over a card.
Subjects in the actor's perspective associate no other cards with
positive utility. In particular, this means that whether someone
performs the action when they do not satisfy the condition is
not the actor's concern. The inquirer's goal is to discover
whether the rule holds, just as in the abstract tasks. The inquirer
has no direct involvement in the situation and therefore has no
relevant utilities concerning it. We adopted the inquirer's perspective in modeling the abstract task, and the same analysis
applies to the inquirer's perspective in thematic tasks.
We summarize the utilities assigned to the enforcer's and the
actor's perspective in Table 7. In this table, we have adopted
the convention that p corresponds to the condition and q to the
action, that is, we assume a permission rule."
We assume that subjects do not know whether the rule is being obeyed. For simplicity, we assign equal prior probabilities to
the rule being obeyed and the rule being ignored, that is, P(Mi)
and P(MD) are equal. The interpretation of the parameter P(M,)
changes between the abstract and thematic tasks. In the abstract
task, it represented the probability that the independence model
is true of the world. In the thematic task, it represents the probability that an individual is disobeying the rule. With this modification, we model the task in exactly the same way as in the
abstract selection task, except that we introduce utilities with
respect to the model.
To model the card selections in the enforcer's and actor's perspective, we calculate expected utilities for each card as follows
(we use the abbreviations "act" for action and "con" for
condition):
EU(con) = P(act \ con)U(con, act) + P(act \ con)U(con, act)
EU(cbn) = P(act \ cbn)U(con, act) + P(aci \ con)U(con, 'act)
EU(act) = P(con \ act)U(con, act) + F\con \ act)U(con, act)
EU(act) = P(con \ aci)U(con, act) + J\con \ act)U(con, 'act).
1' We could have equally adopted an obligation rule and hence let p
be the action and Q the condition. However, this is only a convention,
and the expected utilities come out the same either way.
RATIONAL ANALYSIS OF THE SELECTION TASK
Mean Expected Utilities for Each Card Face (Action, Not-
Action, Condition, and Not-Condition) for the Enforcer's
Perspective and the Actor's Perspective
Not-action
Not-condition
Where the conditional probabilities P(x\y) are the expected values calculated with respect to the two models:
P(x\y) = P(x\ y, Mi)P(M,) + P(x \ y, MD)P(MD).
In Equations lOto 13, the expected utility of each card is calculated as the weighted sum of the probabilities of each possible
outcome given the visible face of the card. The weights are the
utilities, U(), of each outcome.
We derived expected utilities for each card by sampling
points in the parameter space denned by P(p) and P(q) at intervals of. 1 in the range. 1 to .9, with the utilities specified above
(-.1 fixed cost and 5 for the target). We sampled over a whole
range of values for P(p) and P(q) because for deontic rules it is
not reasonable to prejudge rarity. For example, in monitoring
passengers, whether most or only some of the passengers are entering will depend on factors such as the particular flight. If Manila is the flight destination, then most passengers will be entering. However, if the passengers are on the long haul flight from
London to Sydney, then only some passengers may be entering,
the rest will be in transit. We show the mean expected utilities
in Table 8 for the enforcer's perspective and for the actor's
perspective.
The enforcer seeks the case where the actor performs the action but does not satisfy the condition. In the model, selecting
the face that denotes the action being performed and the face
that denotes the condition not being satisfied maximizes expected utility. Table 8 shows that only the cards showing the
action and not-condition face have positive expected utilities.
Hence, subjects should turn only these cards. For an obligation,
ifp (action), then must q (condition), this corresponds to selecting the p and not-q cards. For a permission, ifp (condition),
then may q (action), this corresponds to selecting the not-p and
The actor seeks the case where, although the actor satisfies
the condition, the enforcer disallows the action. In the model,
selecting the face that denotes the condition being satisfied and
the face that denotes the action not being taken maximizes expected utility. Table 8 shows that only the cards showing the notaction and condition face have positive expected utilities. Hence,
subjects should turn only these cards. For an obligation, if p
(action), then must q (condition), this corresponds to selecting
not-p and q cards. For a permission, ifp (condition), then may q
(action), this corresponds to selecting the p and not-q cards.
In summary, our model makes the predictions for card selections in the deontic selection task shown in Table 9. These predictions agree perfectly with the results of the studies indicated.
We also predict that a permission rule with an inquirer's perspective will lead to the standard abstract results, because from
the inquirer's perspective our standard abstract model should
Utilities and Probabilities of Fictional Outcomes
Data. Kirby has recently demonstrated that the
utilities and probabilities of outcomes affect card selections in
the deontic selection task. Equation 9 again forms the basis of
his analysis. In his Experiment 4, Kirby used a drinking age
deontic rule as used by Griggs and Cox : if a person is
drinking beer, then the person must be over 21 years of age. Kirby
used the following cards, "drinking beer" (p), "drinking ginger
ale" (not-p), "22 years of age" (q), and "19 years of age" (notq). This rule is an obligation rule, and subjects must adopt the
enforcer's perspective that predicts the p and not-q response.
Kirby argued that the high frequency of not-q card selections
found by Griggs and Cox may be due to the high probability of finding a 19-year-old drinking beer. He therefore provided two additional not-q cards that varied this probability,
" 12 years of age" and "4 years of age"—the younger the person,
the less likely they are to be drinking beer.
In the same experiment, Kirby varied the utilities of
making correct and incorrect decisions. In a DON'T CHECK
condition, the instructions read, "However, keep in mind that
your employer does not want you to offend innocent customers,
and you could be fired if you check an innocent person." From
Equation 9 these instructions should increase the cost of a FA.
Kirby therefore predicted an overall decrease in the number of
cards selected. In a DON'T MISS condition, the instructions
read, "However, keep in mind that your employer is very concerned about illegal drinking, and you could be fired if you miss
a guilty person." From Equation 9 these instructions should increase the cost of a miss. Kirby therefore predicted an overall
increase in the number of cards selected. In a CHECK condition, the instructions read, "However, keep in mind that your
employer is very concerned about illegal drinking, and you
could receive a large bonus if you catch a guilty person." From
Equation 9 these instructions should increase the benefit of a
hit. Kirby therefore predicted an overall increase in the number
of cards selected. He compared these data to a BASELINE condition with no manipulation of these utilities.
In his Experiment 4, consistent with prediction, Kirby 
observed a trend for fewer selections for not-q cards with a lower
probability of an inconsistent outcome. Moreover, the DON'T
CHECK condition led to fewer card selections than the BASE-
LINE condition, and the DON'T MISS condition led to more
card selections than the BASELINE condition, as predicted.
Similar effects were not observed for the CHECK condition.
Kirby argued that this was because this condition involved a less
extreme benefit, and subjects weight costs more than benefits
 . Kirby's results for the BASE-
LINE, the DON'T CHECK, and the DON'T MISS conditions
appear in parentheses in Table 10.
Modeling Kirby's data is straightforward.
First, in the abstract task, we set the parameter P(M,) to the
MIKE OAKSFORD AND NICK CHATER
Patterns of Card Selections Observed in the Thematic Selection Task for Different Rule Types,
and Perspectives Indicating the Studies Reporting These Results
Perspective
Obligation
Permission
p, not-q 
 
 
standard abstract result:
p > q > not-q > not-p
 
standard abstract result:
p > q > not-q > not-p
(prediction)
error rate in Kirby's Experiments 1 to 3. For the thematic task,
this parameter reflects the probability that an individual is disobeying the rule. We therefore varied this parameter to model
the effect of the various ages of potential violators (not-q cards).
We set f\Mj) to .4 for the 4-year-olds and then incremented by
.1 for the 12-year-olds (P[Mt] = .5) and the 19-year-olds (P[M,]
= .6). These values seemed reasonable because even though 4year-olds in general are unlikely to be drinking beer, the probability of 4-year-olds in a bar drinking is far higher. Certainly the
subjects in Kirby's Experiment 4 felt it necessary to check the
4-year-olds, the proportion of these cards being turned never
dropping below .39.
To capture the effects of the different instructions we varied
the utilities specified in Table 7 for the enforcer's perspective.
The DON'T CHECK condition increases the cost of a false
alarm. We model this directly by increasing the costs for all cells
other than the action, not-condition cell, because any of the outcomes corresponding to these cells represents a false alarm. We
cannot increase costs too much, however, otherwise they will
outweigh the benefits for all possibilities and enforcers will carry
out no checks at all. We therefore increased the costs from —. 1
to -.5. We doubt whether subjects make the distinction between
a cost for a miss and a benefit for a hit. It seems more reasonable
to assume that the cognitive interpretation of costs for misses is
benefits (failure to incur a cost) for hits. Therefore, the DON'T
MISS condition is a more extreme version of the CHECK condition. We, therefore, model the DON'T MISS condition by increasing the utility of the action, not-condition cell in Table 7 for
the enforcer's perspective.
We illustrate the behavior of the model with P(p) = P(q) = .5.
(Any pair of values in the range. 1 to .9 displays the same behavior in response to variations in P(Mf) and the utilities.) Table 10
Expected Utilities for Each Card With P(p) = P(q) = .5 in Each
Condition of Kirby's Experiment 4
DON'T CHECK"
DON'T MISSC
1.38 (.80)
2.53 (.92)
Ginger ale
22 years old
1.40 (.86)
1.1 5 (.76)
0.90 (.71)
1.00 (.81)
0.75 (.47)
0.50 (.39)
2.00 (.97)
1.65 (.86)
1.30 (.80)
Note. p(N = 18) = .94 (p < .0001). Numbers in parentheses indicate the proportion of cards selected in
Kirby's experiment.
" Cost = -.1; benefit = 5. "Cost =-.5; benefit = 5. °Cost = -.1; benefit = 7.
RATIONAL ANALYSIS OF THE SELECTION TASK
shows the expected utilities for each card for the BASELINE,
the DON'T CHECK, and the DON'T MISS conditions. The fit
between data and model was good with a correlation of .94 (p <
.0001). As in the abstract task, our model captures effects that
Kirby's model cannot explain. From Equation 9, Kirby
must predict increases and decreases in card selections for all
cards. However, although there were overall increases and decreases according to Equation 9 in Kirby's data (see Table 10,
right-hand column), these effects were only in line with prediction for the p and not-q cards. There were no significant changes
in the proportion of cards selected for the not-p or q cards, and
where Kirby found differences they were counter to the predictions of Equation 9. In contrast, our model predicts no changes
for these cards in response to Kirby's manipulations, which is
consistent with Kirby's results.
Summary for Thematic Tasks
Our account of the thematic selection task is consistent with
some recent proposals. It supplements our standard probability
model with a maximum expected utility account of the role of
perspectives. Following Manktelow and Over and
Gigerenzer and Hug the notion of a perspective is the
main explanatory concept. Moreover, we have built on Manktelow and Over's qualitative explanation of the influence
of perspectives in terms of subjective utilities. Our emphasis on
the distinction between rule testing and rule use is also consistent with Jackson and Griggs' finding that the checking
(rule use) context is a more important factor in reasoning in
these tasks than their deontic nature. Our proposals are also in
the spirit of Gigerenzer and Hug's expectation that "the
two fields [i.e., deductive and probabilistic reasoning] will converge in the next few years" (p. 169). In this light, perhaps the
most important feature of our model is how well it accounts for
Kirby's recent data, where both probabilities and
utilities are explicitly varied in a deontic task.
Discussion
We begin the discussion by informally summarizing our
model and showing that it provides a rational analysis of the
selection task. We then compare our model to other accounts
of the selection task and to other accounts of reasoning more
generally. Finally, we relate our work to the wider program of
understanding human behavior as rational.
We view the standard abstract selection task as an inductive
reasoning task. Subjects must choose which card-turning experiments they expect to yield the most information about which
of two hypotheses are true. One hypothesis is that a dependency
of the form ifp, then q exists, the other is a foil hypothesis, that
p and q are independent. We denned expected information gain
as the expected decrease in information-theoretic uncertainty
between these two hypotheses in response to some data. We formalized expected information gain, E(Ig), using the theory of
optimal data selection from statistics. We then assumed that
card selection frequencies are a monotonic function of the expected information gain associated with each card.
The model has only three free parameters: the prior probability of the independence model, P(M,); the probability of p,
P(p); and the probability of q, P(q). We explain the majority of
the effects on the abstract selection task by assuming that p and
q are rare by default and that experimental manipulations influence these parameters by moving them away from their default values. When parameters P(p) and P(q) are low, the ordering in expected information gain corresponds to the standard
pattern of card selection frequencies (p > q > not-q > not-p).
With the standard results as a baseline, we explained the associations between card selections by making two minimal assumptions about the nature of the decision process that translates
expected information gains into card selections. These assumptions allowed us to compute scaled E(Ig)s that showed the same
pattern of associations found in the empirical data. We accounted for the negations paradigm selection task, using the
same assumptions and by allowing P(p) and P(q) to vary according to an account of contrast sets. Our account also captures all
of Kirby's recent data where he explicitly varies P(p).
Furthermore, our model accounts for unexpected selection
transitions in the therapy experiments and for the facilitation of
the logical response in the RAST.
Following Manktelow and Over ,
we assume that the strikingly different results observed in many
thematic selection tasks stem from the deontic nature of the
rules used. Such rules are not hypotheses to be tested but rules
that must be obeyed. We model the thematic task using decision
theory together with the same basic probability models used to
model the abstract task. Subjects have utilities concerning various outcomes that depend on their perspective toward the rule.
We assume that they choose cards to maximize expected utility.
The assumption that subjects follow this rational policy captures a broad range of data on the thematic selection task. In
particular, it captures the effects of perspective and rule type
that have been much studied recently; it also captures Kirby's
 data, where utilities and probabilities have been explicitly varied in a deontic selection task.
Relation to Rational Analysis
Our model provides a rational analysis of the selection task in
the sense of Anderson . According to Anderson , rational analysis involves six steps:
1. Specify precisely the goals of the cognitive system.
2. Develop a formal model of the environment to which the
system is adapted.
3. Make minimal assumptions about computational
limitations.
4. Derive the optimal behavior function given 1-3 above.
5. Examine the empirical evidence to see whether the predictions of the behavioral function are confirmed.
6. Repeat, iteratively refining the theory.
We assume that the goals (1) involve selecting relevant data,
that the structure of the environment (2) is given by the frequency of properties occurring in that environment, and that
MIKE OAKSFORD AND NICK CHATER
the costs (3) are incurred in looking at irrelevant data. We derived an optimal behavioral function using Bayesian optimal
data selection (4) and compared this with the empirical evidence (5). In summary, our model demonstrates the utility of
Anderson's approach by showing how it can organize
data on human reasoning that has previously seemed the most
recalcitrant to rational explanation.
Relations to Theories of Deductive Inference
We deal with the relation of our model to theories of deductive inference in two parts. First, we look at the relations between our model and theories of deductive inference taken to
account for the same data. Next we look at some recent probabilistic approaches in reasoning research to which our account
is more closely related.
Theories of deductive inference.
Evans has proposed
a taxonomy of deductive reasoning theories that divides the four
principle approaches into two classes: those that deal with the
issue of deductive competence and those that do not. On the
one hand, mental logic accounts and mental models are theories of deductive competence. On
the other hand, domain-specific approaches, such as pragmatic
reasoning schemas and social contract theory , and heuristic or "relevance" approaches , account for the content
effects and biases found in deductive reasoning tasks. We look
first at the relations between our model and accounts of deductive competence.
Accounts of deductive competence. According to the mental
logic approach , deductive
reasoning involves an abstract logical competence implemented in something like the syntactic rules of a standard logical system. This approach does not attempt to explain the selection task. Rips argued that the selection task is a
"loose" probabilistic task, rather than a "tight" deductive task.
We agree and note that our theoretical account shows how the
selection task can be modeled in probabilistic terms. Consequently, selection task data cannot be used as an argument
against a mental logic, and hence the case against a mental logic
is weaker than perhaps it once was.
The mental models framework proposes that people do not
reason by manipulating syntactic rules but by manipulating
representations of the semantic contents of an argument. Mental models has problems with accounting for the data on the
selection task. For example, Johnson-Laird and Byrne 
claimed that subjects who do not turn the not-q card do not
represent it in their mental model. This suggests that when an
experimental manipulation draws explicit attention toward this
card, subjects should select it. However, in the therapy experiments, where the experimenter focuses attention on the not-q
card and its falsifying properties, the majority of subjects still
do not select it.
Problems of predictive failure to one side, mental models has
most difficulty in accounting for the influence of probabilities
and utilities on reasoning performance. Johnson-Laird and
Byrne argued that such factors only enter into the construction of appropriate mental models, and hence they need
not incorporate these factors in their framework. Garnham
 has attempted a similar argument in defending mental
models theory from the criticism that it fails to account for everyday inference . In our view, theories of everyday inference will involve how factors such as
probabilities and utilities interact with reasoning processes . However, mental models theorists appear to believe that they already have a theory of everyday inference .
This is only true if the mechanisms that construct just the right
kinds of mental model are assumed as primitive. As Chater and
Oaksford observed, "this line of
reasoning has, in Russell's phrase, all the virtues of theft over
honest toil" (p. 81)—most interesting inferential processes are
presupposed and not explained .
In closing this section, we observe that, in contrast to Kirby's
 account, our model does not require an abstract logical
competence. Kirby's signal-detection theory analysis defines
hits as revealing a logically inconsistent outcome and hence presupposes an understanding of conditional logic. In our model,
we characterize the hypotheses probabilistically by the matrices
in Table 1. Thus, our account divorces selection task performance more completely from theories of deductive competence
than Kirby's account.
Accounts of biases and content effects.
Our principle criticism of approaches put forward to account for biases and
content effects is that they lack the generality of our model. Domain-specific approaches such as pragmatic reasoning schema
theory and social contract theory
 deal only with the data from the thematic
task. Both these accounts assume domain-specific rules for
checking for violators in deontic contexts. The main difference
is that on pragmatic reasoning schema theory these rules are
learned, whereas on social contract theory they are innate. The
emphasis on domain-specific information is compatible with
our account . Domain-specific knowledge
may influence the parameters in our model, and the utilities
subjects use, as noted in the Thematic Selection Task section.
The heuristic approach of Evans deals
only with the data from the abstract task. Evans proposed that various heuristic processes involved in language understanding may explain the biases observed in the abstract selection task. In particular, Evans has applied this approach to matching bias, which we discussed in the Negations
Paradigm Selection Task section. However, Oaksford and Stenning have shown that the particular heuristics Evans proposes are unnecessary to account for these data.
Relation to Probabilistic Approaches
There have been some loose probabilistic approaches to the selection task . Fischhoff and Beyth-
RATIONAL ANALYSIS OF THE SELECTION TASK
Marom and Rips both adopted a Bayesian approach, but as part of more general frameworks for looking at
hypothesis testing and loose reasoning tasks, respectively. In
consequence, neither of these Bayesian approaches went beyond
accounting for the standard p and q card selections, which is
perhaps why they have had little influence on selection task research. Similar comments apply to Klayman and Ha who generalized their "positive test strategy" from Wason's 2-4-6 task to the selection task. They based this strategy
on a demonstration that the likelihood of finding disconfirming
evidence was higher when using positive instances of a hypothesis than when using negative instances of a hypothesis (as in
falsification). In summary, these earlier accounts pointed the
way to our probabilistic treatment of the selection task but were
never themselves sufficiently developed to account for the range
of data reviewed here.
Only Kirby tried to explain both the abstract and the
thematic tasks from a probabilistic or decision theoretic perspective. The main failing of Kirby's otherwise excellent work
is that his theory (Equation 9) can explain so little of his important data. This is because his analysis requires that subjects concentrate solely on finding falsifying instances. In contrast, our
Bayesian analysis explains all of Kirby's data straightforwardly.
Interestingly, our model bears close relations to probabilistic
approaches to causal reasoning, an area that until now has been
treated as unrelated to the selection task. Anderson also
used Bayesian model comparison in his model of causal estimation that provides a rational explanation of biases in the analysis
of 2 X 2 contingency tables .
Cheng and Novick have also taken a probabilistic
approach to both causal inference and to causal attribution
 in their probabilistic contrast
model. Cheng and Novick did not propose a full
Bayesian treatment of these data. However, their emphasis on
probabilistic contrasts is similar to our emphasis on information gain in deciding between hypotheses. In both cases, subjects are assumed to concentrate on probabilistic differences.
We may, moreover, be able to derive the probabilistic contrast
model from our Bayesian framework. We can compare an independence model and a model (or family of models) in which
a contingency holds (or parameterized family of models, each
representing a different contingency reliability) not just for a
single data point (as in the current analysis) but for an entire set
of data. This could provide the basis for a normative analysis of
experiments on contingency judgments, causal reasoning, and
causal attribution. This opens up the exciting possibility of unified rational explanations of formerly disparate phenomena in
the reasoning field.
Finally, our model is consistent with a growing trend in accounting for putative biases in inferential behavior using rational probabilistic models. Anderson's work is the most comprehensive such approach, applying Bayesian methods to a variety of cognitive phenomena . Gigerenzer has also applied probability theory to
explaining biases in reasoning tasks, and Cheng and Novick's
 work is also consistent with this trend.
Rationality
There are two issues about rationality that require discussion.
The first concerns the actual parameter values that we have chosen in our analyses and whether we can provide them with a
rational justification. The second concerns the normative status
of our rational analysis.
Parameter values. We have explained the data on the selection task by assuming that p and q are rare by default and that
experimental manipulations influence the parameters P(p) and
P(q) by moving them away from their default values. An initial
and important point is that our model organizes a wide range of
data in a single theoretical framework. This argues strongly that
subjects behave as Bayesians with the rarity assumption. This
in itself is an important discovery, even if we could not rationally justify the rarity assumption. Testing the validity of this
assumption will require an environmental analysis of the type
Anderson proposed. However, we argue that there is evidence to support the view that most lexicalized properties refer
to objects and events that are rare in our environment. In consequence, subject's behavior in the selection task may be optimally adapted to that environment and hence rational.
First, note that no other parameter values are better justified.
For example, the principle of indifference that
P(q) = P(p) = .5 is only reasonable on the assumption of complete ignorance. However, subjects have extensive prior experience with other conditional rules. If these generally relate properties that respect rarity, then it is reasonable for subjects to
extrapolate from prior experience and assume that a novel task
rule also respects rarity. Other possibilities are equally questionable. For example, Kirby argued that the probability of
finding a vowel (p) on the back of an odd number (not-q) is low
because there are 5 vowels but 21 consonants. However, the level
of letter types may not be the relevant level at which to assess
these probabilities. It could equally be the level of letter tokens
in experience that is the determining factor.
The rarity assumption organizes data from more than the selection task. We mentioned above that in the 2-4-6 task, Klayman and Ha showed that positive tests were more likely
to yield falsifying evidence than negative tests. This result also
relies on a rarity assumption, or what Klayman and Ha called a
minority phenomenon assumption. That is, the properties that
figure in hypotheses about their causes are in the minority. For
example, AIDS has only an incidence of about 10"4 to 10"5 in
the population. A hypothesis of the form if you have contracted
HI V, then you will develop AIDS will therefore respect the rarity
assumption. This is because scientists are unlikely to put much
stock in this hypothesis if/"(HIV) > P(AIDS) (this is a further
application of our constraint that P[q] > P[p]). Furthermore,
Anderson's work on causal inference indicates that subjects make a rarity assumption in causal estimation from 2 X 2
contingency tables. In fitting parameters to Schustack and
Sternberg's data, Anderson derived expected prior probabilities of an effect of .27 and of a cause of .25.12 Given a causal
(if cause, then effect) relation, these results confirm the rarity
assumption and the reasonable constraint that although P(q) is
greater than P(p), it can only be marginally greater. In summary,
MIKE OAKSFORD AND NICK CHATER
the rarity assumption appears capable of organizing a great deal
of data on human reasoning.
Normative status. As we mentioned in the introduction,
Anderson drew the distinction between normative and
adaptive rationality . Normative rationality concerns reasoning according to the rules of a
formal logico-mathematical theory. Following such rules provides the standard account of rationality going back to Plato
 . Adaptive rationality concerns whether behavior is optimally adapted to the environment. We have shown
that in the selection task, subjects' behavior can be regarded as
rational in the second sense, that is, as optimally adapted to
an environment where the properties that enter into predictive
relations are rare. Although we have used a normative mathematical theory to derive this rational analysis, there is no requirement that people achieve this optimal adaptation by following the rules of the normative theory. Hence, although our
account argues for the adaptive rationality of reasoning on the
selection task, it need not address the question of normative
rationality.
However, although a rational analysis does not require that
people make Bayesian calculations, it does not preclude it either.
A range of views is possible. At one extreme, we can view the
calculations involved in deriving the rational analysis as specifying a set of mental operations carried out by the subject. This
view attributes people with sophisticated, though not necessarily explicit , probabilistic reasoning abilities. It
also corresponds to the view that people are not only adaptively
rational but are also normatively rational. At the opposite extreme, as we have mentioned, a rational analysis may just specify which behavior is optimal and remain neutral about the
mental operations underlying that behavior. The reason that
people conform to our analysis of the selection task might be
due to innate constraints or learning, rather than sophisticated
probabilistic calculation. Between these two extremes, that all
the calculations of the rational analysis are internally computed
and that none are, lies a smooth continuum of intermediate
positions, which assumes that some aspects of the analysis are
calculated internally and others are not.
The view taken toward rational analysis has behavioral and
computational significance. Insofar as people calculate optimal
behavior internally, a subject's knowledge of the specifics of the
task can influence those calculations. For example, in the
RAST, we assume that the way in which the materials violate
the rarity assumption influences a subject's behavior. If subjects
performed no calculation, but simply applied learned or innate
strategies, then it is unlikely that such parameter changes would
affect their performance. It is, of course, possible that subjects
choose between various strategies that do not involve calculation, depending on the specifics of the situation. Nonetheless, in
general, the more flexible a subject's behavior to relevant aspects of the task, the stronger the case for internal calculation,
and the less plausible noncomputational strategies. We will
need to conduct further empirical work to assess which aspects
of our rational analysis of the selection task people internally
calculate and which they have prestored.
Insofar as people make internal calculations of our rational
analysis, we must consider the computational feasibility of those
calculations. The calculations of our analysis of the selection
task are very simple. However, as we have argued elsewhere
 , plausible reasoning theories must "scale up" from laboratory reasoning tasks to everyday inferential processes. Simple
Bayesian calculations rapidly lead to the notorious combinatorial explosion . Recently
Pearl has proposed a novel and more tractable implementation of Bayesian inference using Bayesian networks. However, this method too does not scale well . These problems are not specific to probabilistic inference, but apply equally to logical reasoning (Oaksford & Chater,
The problems of computational tractability suggest that a
scaled-up rational analysis would have to pay considerably more
attention to computational limitations (Step 3 in Anderson's
 account of rational analysis) than is required for modeling laboratory tasks. In Simon's terms, this means that
people should be modeled as having bounded rationality .
Conclusions
We have shown how to develop a rational analysis of the selection task, which accords closely with empirical data, using a
Bayesian account of hypothesis testing. This account contrasts
sharply with the standard falsificationist model. The poor fit between this model and the empirical data has led to doubts about
whether humans are rational. We suggest that people are rational but that we must define rationality in terms of optimal
performance in real-world, uncertain, inductive tasks, rather
than purely in terms of deductive logic. Clarifying the detailed
relationship between normative theory and observed behavior
suggests a program of empirical investigation and theoretical
generalization to related tasks, the success of which remains to
be decided. Nonetheless, our model establishes that subjects'
behavior while performing the selection task need have no negative implications for human rationality.
12 Anderson reported expected prior probabilities of
an event (E), P(E), of .27; of an event given the presence of a cause (C),
P(E|C), of. 71; and of an event in the absence of a cause, P(E|not-C), of
. 12. By elementary probability theory, P(C) therefore equals .25.