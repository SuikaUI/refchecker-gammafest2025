This item was submitted to Loughborough's Research Repository by the author.
Items in Figshare are protected by copyright, with all rights reserved, unless otherwise indicated.
Deep-FS: a feature selection algorithm for Deep Boltzmann Machines
PLEASE CITE THE PUBLISHED VERSION
 
Elsevier BV
VoR (Version of Record)
PUBLISHER STATEMENT
This is an Open Access Article. It is published by Elsevier under the Creative Commons Attribution 4.0
International Licence (CC BY 4.0). Full details of this licence are available at:
 
REPOSITORY RECORD
Taherkhani, Aboozar, Georgina Cosma, and TM McGinnity. 2018. “Deep-fs: A Feature Selection Algorithm for
Deep Boltzmann Machines”. Loughborough University. 
Neurocomputing 322 22–37
Contents lists available at ScienceDirect
Neurocomputing
journal homepage: www.elsevier.com/locate/neucom
Deep-FS: A feature selection algorithm for Deep Boltzmann Machines
Aboozar Taherkhani a , ∗, Georgina Cosma a , T. M McGinnity a , b
a School of Science and Technology, Nottingham Trent University, Nottingham, UK
b Intelligent Systems Research Centre, Ulster University, Northern Ireland, Derry, UK
a r t i c l e
Article history:
Received 5 March 2018
Revised 12 August 2018
Accepted 17 September 2018
Available online 22 September 2018
Communicated by Jiayu Zhou
Deep Boltzmann Machine
Deep learning
Deep Neural Networks
Feature selection
Restricted Boltzmann Machine
Generative models
Missing features
a b s t r a c t
A Deep Boltzmann Machine is a model of a Deep Neural Network formed from multiple layers of neu-
rons with nonlinear activation functions. The structure of a Deep Boltzmann Machine enables it to learn
very complex relationships between features and facilitates advanced performance in learning of high-
level representation of features, compared to conventional Artiﬁcial Neural Networks. Feature selection
at the input level of Deep Neural Networks has not been well studied, despite its importance in reduc-
ing the input features processed by the deep learning model, which facilitates understanding of the data.
This paper proposes a novel algorithm, Deep Feature Selection (Deep-FS), which is capable of remov-
ing irrelevant features from large datasets in order to reduce the number of inputs which are modelled
during the learning process. The proposed Deep-FS algorithm utilizes a Deep Boltzmann Machine, and
uses knowledge which is acquired during training to remove features at the beginning of the learning
process. Reducing inputs is important because it prevents the network from learning the associations be-
tween the irrelevant features which negatively impact on the acquired knowledge of the network about
the overall distribution of the data. The Deep-FS method embeds feature selection in a Restricted Boltz-
mann Machine which is used for training a Deep Boltzmann Machine. The generative property of the
Restricted Boltzmann Machine is used to reconstruct eliminated features and calculate reconstructed er-
rors, in order to evaluate the impact of eliminating features. The performance of the proposed approach
was evaluated with experiments conducted using the MNIST, MIR-Flickr, GISETTE, MADELON and PAN-
CAN datasets. The results revealed that the proposed Deep-FS method enables improved feature selection
without loss of accuracy on the MIR-Flickr dataset, where Deep-FS reduced the number of input features
by removing 775 features without reduction in performance. With regards to the MNIST dataset, Deep-FS
reduced the number of input features by more than 45%; it reduced the network error from 0.97% to
0.90%, and also reduced processing and classiﬁcation time by more than 5.5%. Additionally, when com-
pared to classical feature selection methods, Deep-FS returned higher accuracy. The experimental results
on GISETTE, MADELON and PANCAN showed that Deep-FS reduced 81%, 57% and 77% of the number of in-
put features, respectively. Moreover, the proposed feature selection method reduced the classiﬁer training
time by 82%, 70% and 85% on GISETTE, MADELON and PANCAN datasets, respectively. Experiments with
various datasets, comprising a large number of features and samples, revealed that the proposed Deep-
FS algorithm overcomes the main limitations of classical feature selection algorithms. More speciﬁcally,
most classical methods require, as a prerequisite, a pre-speciﬁed number of features to retain, however in
Deep-FS this number is identiﬁed automatically. Deep-FS performs the feature selection task faster than
classical feature selection algorithms which makes it suitable for deep learning tasks. In addition, Deep-
FS is suitable for ﬁnding features in large and big datasets which are normally stored in data batches for
faster and more eﬃcient processing.
© 2018 The Authors. Published by Elsevier B.V.
This is an open access article under the CC BY license. ( )
∗Corresponding author.
addresses:
 
Taherkhani),
 (G. Cosma), (T.M. McGin-
1. Introduction
The successful performance of deep learning in various applica-
tions such as image recognition , speech recognition and
bioinformatics , has captured considerable attention in recent
literature. Deep learning (DL) methods provide promising results
on problems for which conventional machine learning methods
 
0925-2312/© 2018 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license. ( )
A. Taherkhani et al. / Neurocomputing 322 22–37
have not made major progress, despite many attempts . Con-
ventional machine learning methods have limited ability to pro-
cess raw data, and for this reason considerable effort is tradition-
ally placed on feature engineering. Feature engineering represents
data in a manner such that machine learning algorithms can iden-
tify patterns and classify the data. An important advantage of DL
methods over conventional approaches (e.g. Artiﬁcial Neural Net-
work, Support Vector Machine, Naïve Bayes), is that DL methods
integrate the feature extraction and learning process into a single
model, and thus feature engineering is dealt with as an integrated
rather than a separate task.
Feature selection, aims to eliminate redundant and irrelevant
features via different criteria. The most commonly used criteria
measure the relevance of each feature to the desired output, and
use this information to select the most important features .
Highly dependent features can be considered as redundant fea-
tures and some of these can be eliminated during a feature selec-
tion process. Eliminating irrelevant and redundant features, results
in a permanent reduction in the dimensionality of the data, and
this can increase the processing speed and accuracy of the utilized
machine learning methods.
Deep Neural Networks (DNNs), such as those proposed in
 use feature extraction rather than feature selection meth-
ods for extracting underlying features from big data. For example
Hinton et al. proposed a DNN method which reduces data di-
mensionality through a non-linear combination of all input fea-
tures in a number of layers of the neural network, and this ap-
proach inspired the development of new algorithms in the deep
learning ﬁeld . DNNs can learn very complex relationships
between variables through their high numbers of non-linear ele-
ments. However, if there exist a high number of irrelevant features
in the input feature space, then the relationship between these ir-
relevant features may also be modelled. Modelling the irrelevant
features acts as noise, and learning the associations between ir-
relevant features negatively impacts on the acquired knowledge of
the network about the overall distribution of the data, as well as
the computational time. Modelling the irrelevant features can also
lead to overﬁtting a model , because the method learns irrel-
evant details from the training data and it becomes more biased
to previously seen data . A technique called Dropout was
proposed to increase the generalisation ability of neural networks
which have a high number of neurons. However, a major limitation
of the Dropout method is that it could retain all the input features
and neurons that include redundant and irrelevant features.
The Deep Belief Networks (DBNs) proposed by Hinton and
Salakhutdinov , and the Deep Boltzmann Machines (DBMs) pro-
posed by Srivastava and Salakhutdinov et al. are two types
of DNNs which use densely connected Restricted Boltzmann Ma-
chines (RBMs). The high number of processing elements and con-
nections, which arise because of the full connections between the
visible and hidden units, increase the RBM’s computational cost
and training time. In addition, training several independent RBMs
increases the training time . When the scale of the net-
work is increased, the required training time is also increased non-
linearly. Reducing the number of input features can signiﬁcantly
reduce the size of the constructed weight matrix, and consequently
it can reduce the computational cost of running deep learning
methods, especially when a large network size is required for prac-
tical applications .
This paper proposes a novel algorithm, Deep-Feature Selection
(Deep-FS), for embedding feature selection capabilities into DBMs,
such that irrelevant features are removed from raw data to dis-
cover the underlying feature representations that are required for
feature classiﬁcation. DBMs primarily use an unsupervised learning
method to initialize the learning parameters of a DNN. Then the
initialized DNN is ﬁne-tuned by a backpropagation method. Deep-
FS combines the feature extraction property of DBM with a fea-
ture selection method which is based on the generative property
of RBMs. RBMs are generative models and they can reconstruct
missing input features. The proposed Deep-FS uses an RBM that is
trained during the learning procedure of DBM to improve the eﬃ-
ciency of the method in dealing with high volumes of data. Deep-
FS returns a reduced subset of features and improves the deep
learning method’s computational eﬃciency by reducing the size of
the constructed network. DBMs are known to have good perfor-
mance for feature extraction, and adding a feature selection ability
to DBMs can lead to a new generation of deep learning models
which have an improved ability to deal with highly dimensional
The remainder of the paper is structured as follows:
Section 2 discusses the background to the work; Section 3 pro-
vides details of the proposed method; Section 4 describes the
experimental results; and Section 5 provides a conclusion and
future work.
2. Background
This section provides a background on Deep Boltzmann Ma-
chines and feature selection methods.
2.1. Deep Boltzmann Machine
Deep Neural Networks (DNNs) are mainly based on stochas-
tic gradient descent and backpropagation training algorithms. Two
main techniques are used for training DNNs. The ﬁrst technique
is based on a ﬁltering strategy and the second one is based on
unsupervised pre-training. The ﬁrst ﬁltering technique is used by
Convolutional Neural Networks (CNNs) to locally ﬁlter inputs. Fil-
tering is performed by convolving the input by weight matrices.
In the second technique, information processing starts by using
an unsupervised learning method. In this stage, unlabelled data
can be used. Then, the DNN is ﬁne-tuned by a supervised method
using labelled data. Deep Belief Networks (DBN) and DBMs
 are examples which use this semi-supervised technique. Pre-
training using an unsupervised method improves the generalisa-
tion of the trained network especially when the dataset contains a
small amount of labelled data. This paper is focused on DBMs that
include unsupervised learning during their ﬁrst stage of training
procedure.
DBMs have been used in different applications such as image-
text recognition , facial expression recognition , 3D model
recognition , and audio-visual person identiﬁcation and
belong to a group of DNNs that uses a pre-training procedure. After
the pre-training procedure, DBM is ﬁne-tuned using labelled data
 . A DBM is composed of a set of visible units corresponding
to input data. Additionally, there are a network of symmetrically
coupled stochastic binary units called hidden units. The binary hid-
den units are arranged in different layers and there are top-down
and bottom up couplings between two adjacent layers. There are
no direct connections between units in the same layer. A DBM rep-
resents the input data in several layers with increasingly complex
representations. In DBM a learning procedure is executed to pre-
train a number of layers in conjunction with each other. DBM can
potentially be used to capture a complex internal representation
of input data. The interaction of different hidden layers during the
initial learning generates a deep network with a high ability to re-
construct ambiguous inputs through top-down feedback using dif-
ferent interacting layers. During the pre-training stage of a DBM, a
learning procedure similar to RBMs is used. The RBM is discussed
in Section 3 .
Recently, a multimodal data processing method was designed
based on DBMs . In the multimodal data processing method,
A. Taherkhani et al. / Neurocomputing 322 22–37
initially two networks of DBMs are trained separately on visual and
auditory data. Then the output of the two DBMs are combined in
a joint layer. The representation extracted from the joint layer is
considered as input to a Restricted Boltzmann Machine (RBM). The
RBM is trained on the joint layer representation. Then the entire
network is ﬁne-tuned by labelled data.
The fully interconnected network between the visible and hid-
den units increase the computational cost of a DBM, and ﬁnding
a method to reduce the number of input feature through feature
selection method can reduce the computational cost of a DBM.
2.2. Feature selection
The amount of data available to machine learning models is
increasing exponentially. The number of features in datasets is
also increasing, and many of the features are irrelevant or redun-
dant, and thus not needed to improve the performance of a ma-
chine learning model . Feature Selection (FS) can accelerate
the learning process of a machine learning method because it al-
lows the algorithm to learn using a smaller number of features.
Additionally, it can improve the classiﬁcation performance by pre-
venting overﬁtting . A feature selection method reduces the
number of input features without signiﬁcant decrease in the per-
formance of a machine learning method .
Feature selection methods for classiﬁcation can be divided into
three main categories: ﬁlter, wrapper and embedded methods.
These approaches are used to combine feature selection with a
classiﬁcation model. In ﬁlter methods, feature selection is per-
formed as a prepossessing stage which is independent of the clas-
siﬁcation model. Each ﬁlter method ranks the features based on a
criteria, and the highest ranked features are selected . In fea-
ture ranking methods, the relevance of each feature to a class la-
bel is evaluated individually, and a weight is calculated for each
feature. Then, features are ranked based on their weights and the
top features with the highest weights are selected . Maxi-
mum Relevance (MR) feature selection is a type of feature ranking
algorithm, where Mutual information and kernel-based indepen-
dence measures are usually employed to calculate the relevance
score . Feature ranking methods are simple and have low com-
putational cost. Kira and Rendell’s “Relief” algorithm , and the
feature selection method proposed by Hall are examples of
feature selection methods that work based on the dependency of
features to the class labels. Although the selected top features have
the highest relevance to the class labels, they might be correlated
with each other and have redundant information, and do not in-
clude all the useful information in the original feature sets.
The feature selection method proposed by Fleuret consid-
ers the dependency between input features using conditional Mu-
tual Information. The method considers the dependency of a new
feature to the already selected features and eliminates that fea-
tures that are similar to the previously selected features, because
the main idea is that if a feature is dependent on previously se-
lected features then it does not contain new information about the
class, and it can be eliminated.
Peng et al. proposed a feature selection method called
Minimum Redundancy Maximum Relevance (mRMR) to solve the
problem of feature redundancy. A subset of features which have
high relevance to class labels and which are non-redundant are
selected. The mRMR method has better performance compared to
the method that only works based on the relevant features. In the
mRMR ﬁltering feature selection method, the dimension of the se-
lected feature space is not set at the start of procedure.
In wrapper methods, the performance of a classiﬁer is used
to evaluate a subset of features. Different subsets of features are
searched using different searching algorithms to ﬁnd the optimal
feature subset that gives the highest classiﬁcation performance .
For an initial feature space dimensionality of N , a total of 2 N sub-
sets can be evaluated, and that becomes an NP-hard problem. Se-
quential search, and evolutionary algorithms such as the Genetic
Algorithm or Particle Swarm Optimization can be used to design
a computationally feasible method for wrapper feature selection
methods . Wrappers have higher computational cost compared
to ﬁlter feature selection methods.
With Embedded feature selection methods, the feature selec-
tion procedure is integrated into the training procedure of the
classiﬁer. It reduces the computational cost compared to wrap-
per methods where a high number of subsets should be retrained
by the classiﬁers . Guyon et al. used Support Vector Ma-
chine (SVM) as a classiﬁer to design an Embedded feature selection
method. Features are evaluated during iterations of learning and
the features that decrease the separation margin between classes
are removed.
Feature selection algorithms coupled with feature extraction
methods, can improve the performance of machine learning meth-
ods . Feature selection algorithms can reduce the inputs to
a classiﬁer which in turn reduces computational cost and increases
accuracy. However, classical feature selection algorithms are usu-
ally designed for small datasets and thus there is an emerging
need to implement feature selection algorithms which can opti-
mally search through large datasets with thousands of samples and
a high number of features. This paper focuses on taking the idea
of feature selection coupled with the feature extraction capabil-
ity of deep learning to improve the performance of deep learning
2.3. Deep learning and feature selection
This section discusses a number of feature selection methods
that are used with Deep Learning methods. Ruangkanokmas et al.
 have used a classical ﬁlter-based feature selection approach
for sentiment classiﬁcation. A ﬁlter-based feature selection tech-
nique, called the chi-squared method, was proposed to select fea-
tures and, thereafter the selected features were utilised to train a
DBN. Feature selection and training the DBN were performed in
two separate stages. Combining feature selection and DBN training,
has the potential to improve the eﬃciency of a classiﬁer.
Ibrahim et al. have also used DBN, classical feature se-
lection methods, and the unsupervised Active Learning method to
process gene expression data. DBN was used to model high dimen-
sional input data and to extract higher level representation of the
input data. Then statistical classical feature selection methods, such
as the t -test method, were used to select features from the higher
level extracted features. Ibrahim et al’s method has used three
cascaded independent modulus with separate computational costs.
Computational costs can signiﬁcantly increase when the number of
training data is high. DBN is designed to work with a high num-
ber of training data samples and it requires feature selection func-
tionality which is suitable for training large datasets, in order to
prevent unnecessary computational cost.
Nezhad et al. proposed a feature selection method
which is based on a ﬁve-layer stacked Auto-Encoder deep network.
Higher-level representative features were extracted from the hid-
den layer placed in the middle of the Auto-Encoder. Then, a classi-
cal feature selection method based on the feature ranking method
and random forest was used to select features. After that, a super-
vised learning method was trained on the selected features and
evaluated using the Mean Squared Error (MSE). The number of
hidden neurons in the Auto-Encoder hidden layers, is adjusted to
optimize the structure of the network based on the obtained re-
sults. This process is continued until reaching an acceptable result.
This method has used different modules that increase the compu-
tational cost of the method.
A. Taherkhani et al. / Neurocomputing 322 22–37
Li et al. proposed a deep feature selection (DFS) model us-
ing a deep structure of nonlinear neurons. They have used a multi-
layer perception (MLP) neural network to learn the nonlinearity of
input data. Additionally, they use a sparse one-to-one linear layer
before the MLP to select input features. The weight matrix of the
ﬁrst linear layer is a sparse matrix and the input features corre-
sponding to nonzero weights are selected. Their proposed feature
selection method is ﬂexible such that the one-to-one linear layer
corresponding to the feature selection can be added to other deep
learning architectures. Despite the ﬂexibility of the method, its ac-
curacy is not perfect and experimental results have shown that the
method did not outperform the random forest method.
Zhang and Wang proposed a feature selection method for
deep learning to classify scenes. They converted a feature selec-
tion method to a feature reconstruction problem. In a scene clas-
siﬁcation task they have selected the features that are more re-
constructive than discriminative. However, removing discriminative
features might reduce the classiﬁcation accuracy in a typical clas-
siﬁcation task.
Deep learning methods, such as DBM, are usually composed of
a high number of nonlinear processing elements that need a high
number of training samples. On the other hand, the number of re-
quired observations (training samples) should grow exponentially
with the number of input features to train a DL method.
Through feature selection, the number of input features and con-
sequently the required number of training samples for training a
DBM can be reduced. Therefore, feature selection is strongly re-
quired to help deep learning methods to be trained with less train-
ing data. In this paper, a feature selection method is proposed for
DBM to improve its processing ability and reduce the computa-
tional cost of feature selection for DBM.
3. Principles of the proposed deep feature selection method
In this section, the mathematical properties of RBM are ﬁrst
described. Then, the RBM is used to design the proposed Deep-
FS feature selection method. The principle of the proposed feature
selection method which works based on RBM is presented in the
second part of this section. Two versions of the proposed deep fea-
ture selection algorithm are presented. In the ﬁrst version of the
proposed feature selection algorithm, the RBM is not trained dur-
ing feature selection. However, in the second version, the RBM is
trained during the feature selection procedure.
3.1. Mathematical properties of Restricted Boltzmann Machines
A Boltzmann Machine (BM) is a parameterised probabilistic
model. The parameters of a BM are trained to approximately model
the important aspects of an unknown target distribution by using
available samples drawn from the target distribution. The available
samples that are used to train the parameters of the model are
called training samples . There are two types of units in a BM,
these are the visible and hidden units (or neurons). The two sets
of units are arranged in two layers. The ﬁrst layer is constructed of
visible units, and the hidden units (or neurons) are in the second
layer. Each neuron or unit in the ﬁrst layer corresponds to an input
feature. For instance, if the input is an image then each visible unit
corresponds to a pixel. In general, the visible units can accept dif-
ferent types of input features. The hidden units are used to model
complex dependencies between visible units.
Restricted Boltzmann Machines (RBMs) are a special case of the
general BM where there are no inter-connections between units in
a single layer, i.e. each unit is fully connected to all units in another
layer but does not have any connections with any units in its own
layer ( Fig. 1 ). RBMs have a close connection with statistical physics
 , and represent a type of energy-based model. During training,
Fig. 1. Structure of a Restricted Boltzmann Machine. It is composed of two layers
of neurons, namely the visible and hidden neurons. There are D visible neurons and
F hidden neurons in this RBM.
the parameters of an RBM are adjusted to generate a model repre-
senting a probability distribution that is close to the actual proba-
bility distribution from which the data are drawn. RBMs have been
successfully used for processing binary and real-value data [1,38–
Consider an RBM with D binary visible units. Let V be a vector
that contains the states of the D binary visible units, i.e. V ∈ {0, 1} D ,
and let h be a vector containing the states of the hidden units.
Assume there are F hidden units in the RBM. The F dimensional
hidden variables can be denoted by h ∈ {0, 1} F . The joint conﬁgu-
ration of V and h deﬁne the energy function (1) .
E ( V , h ) = −
W ij v i h j −
where W ij is a weight that connects the i th visible unit, v i , and
the j th hidden unit, h j . b i and a j are biases that are related to the
i th visible and the j th hidden units. The energy function is used to
assign a joint distribution over the visible and hidden variables as
shown in (2) .
P ( V , h ) = 1
Z exp ( −E ( V , h ) )
where Z is a normalizing term and it is called the partition func-
tion. Z is calculated by (3) .
exp ( −E ( V , h ) )
The sum is calculated over all possible pairs of ( V , h ). Let V be
a D dimensional vector and let h be an F dimensional binary vec-
tor. There are 2 D + F different pairs of ( V , h ) when the visible units
are binary. The conditional probabilities P ( h | V ) and P ( V | h ) can be
calculated by (4) and (5) .
P ( h | V ) =
P ( V | h ) =
p ( v i | h )
Conditional Probabilities (4) and (5) can be written as follows:
h j = 1 | V 
W ij v i + a j
p ( v i = 1 | h ) = g
W ij h j + b i
where g ( x ) is the logistic function, 1 / ( 1 + exp ( −x ) ) . The network
can be trained to increase its assigned probability to an input by
reducing the energy related to the input. The network parameters
are updated by using the gradient-based method to reach the ob-
jective. Eq. (8) shows the derivative of the log probability of visible
A. Taherkhani et al. / Neurocomputing 322 22–37
inputs on a set of observations, { V n } N
n =1 , with respect to a weight,
∂log ( P ( V n ) )
The ﬁrst term, E data [ v i h j ], is the expectation of v i h j with respect
to data distribution. E data [ v i h j ], shows the frequency in which both
visible unit, v i , and hidden unit, h i , have the binary values of one.
E model [ v i h j ] is the same expectation with respect to the distribution
deﬁned by the model. The one-step contrastive diversion approx-
imation is used to approximate E model [ v i h j ] . It is calculated
by running one iteration of the Gibbs sampler to reconstruct the
visible unit using (6) and (7) to obtain (9) .
∂log ( P ( V n ) )
where ˜ v i is the reconstructed i th visible unit obtained through
Gibbs sampling. Using the Gibbs sampler to approximate
E model [ v i h j ] is called the contrastive approximation method. A
similar procedure can be used to extract (10) and (11) for updating
∂log ( P ( V n ) )
∂log ( P ( V n ) )
≈E data [ v i ] −E model [ ˜ v i ]
Gaussian-Bernoulli RBMs are used for modelling real-
valued vectors. In this case, the visible vector is V ∈ R D , and the
hidden units are binary, h ∈ {0, 1} F . The RBM assigns the condi-
tional distribution shown in (12) and (13) .
h j = 1 | V 
p ( v i | h ) = N
W ij h j , σ 2
A Gaussian distribution with mean μ = b i + σi
j=1 W ij and
variance σ 2
i is used for modelling a visible unit, i.e. N( μ, σ 2
derivative of the log probability on a set of observations, { V n } N
is given by (14) , which is similar to (8) .
∂log ( P ( V n ) )
Similar to the binary visible unit, the ﬁrst expression on the
right side of (14) can be calculated from the training data and the
second expression can be approximated through a Gibbs sampler
 . The result is obtained using (15) .
∂log ( P ( V n ) )
The input features, v i , are usually normalized to have zero
mean, μ = 0 , and a variance of one, σi = 1 . This simpliﬁes (15) and
it becomes similar to (9) . The following sections refer to (9) for
both binary and real value input data.
3.2. Proposed RBM-based deep feature selection algorithm
An RBM is a generative probabilistic model and can generate
the probability of the value of a visible unit given the states of
hidden units. This property can be used to reconstruct missing
visible units. The generative property of RBMs has been adopted
to draw samples from the learned distribution to extract tex-
tures from images . The generative property of RBMs has also
been used to sample the missing parts of an input image in im-
age denoising tasks . The proposed Deep-FS algorithm adopts
the generative property of RBM to deﬁne a method for feature
selection.
Deep-FS aims to ﬁnd a set of features with useful information.
Therefore, features that do not hold useful information about the
input data are removed by the generative property of the RBM.
The ﬁnal selected features have a lower number of features and
they reduce the complexity of the network. Feature selection is
performed via three steps 1) Initial training: of RBM on training
data with all the features; 2) Feature elimination: Removing extra
features by the initially trained RBM; and 3) Main Training: Train-
ing the DBM with the initially trained RBM on the training data
which consists of the selected features. Each of these steps is de-
scribed in the remainder of this section.
Step 1 - Initial training: The ﬁrst step is performed via the
learning method described in Section 3.1 by using (9) . During the
training procedure, training data is input into the RBM. Then the
outputs of hidden neurons are calculated for all training data, and
the ﬁrst expression in (9) is calculated, i.e. E data [ v i h j ]. In the next
step the inputs are reconstructed by Gibbs sampling and the sec-
ond expression in (9) , E model [ ˜ v i h j ] , is calculated accordingly. Fi-
nally, the RBM weights are adjusted by calculating the difference
E data [ v i h j ] −E model [ ˜ v i h j ] as shown in (9) .
Step 2 - Feature elimination: In the second step, features are
eliminated through a proposed feature selection algorithm which
is describing in this step. The proposed algorithm can be tuned to
eliminate a single feature or a group of features in each evaluation.
The proposed algorithm starts with the set of all input features and
evaluates the effect of each group of features by using the trained
RBM. The learning aim in RBM is to minimize the network error or
maximize the log-likelihood as shown in (9) . The derivative of loss
or error function of RBM can be obtained by multiplying (9) by
the value of minus one . During the learning process, the ab-
solute value of the error is reduced. Consequently, the weight ad-
justments are stabilized. The learning procedure is stopped when
the error reaches zero or a predeﬁned number of learning epochs
is reached. The weight adjustment can become zero when the er-
ror reaches zero. In particular, consider the error related to one of
the input features. The error related to the i th visible unit is given
in (16) by using (9) .
∂log ( P ( V n ) )
The reconstruction error is deﬁned using (17) :
( ˜ v i −v i ) h j
e i = E model
( ˜ v i −v i ) 2
where e i is the reconstruction error related to input feature i . A
lower e i can cause a lower absolute weight adjustment.
The RBM learning rule is based on feature extraction and di-
mension reduction. In RBM’s feature extraction method, different
visible inputs are combined and hidden features are extracted.
During this learning procedure, the value of weight adjustment is
stabilized and consequently the absolute value of error shown in
(16) is reduced. e i described in (17) is used to deﬁne an elimina-
tion criterion for the proposed feature selection method.
The input features are investigated to ﬁnd whether a feature
v i can be reconstructed by using other input features, i.e. to ﬁnd
whether the other features contain enough information to recon-
struct the v i . The i th visible unit, v i , is eliminated and it is re-
constructed by the trained RBM. To eliminate v i , it is initialized
A. Taherkhani et al. / Neurocomputing 322 22–37
to zero. Therefore, the visible unit v i with the initial value of zero
does not contribute to the output of the hidden variables (see
(6) and (12) ). The hidden features are generated only by the other
visible inputs. Then the i th visible unit is reconstructed by the hid-
den units using (7) for binary or (13) for continuous or real value
data. The i th reconstructed feature is called ¯v i . Then (17) is used to
calculate the reconstruction error, ¯e i = E model [ ( ¯v i −v i ) 2 ]. Note that
¯v i and ˜ v i both are reconstructed visible units. However, ¯v i is gener-
ated when the initial value of v i is set to zero, and ˜ v i is generated
when v i is set to its original value.
If the reconstruction error after elimination of i th visible unit,
¯e i , is equal or less than the original reconstruction error of the vis-
ible unit, e i , it becomes evident that the visible unit does not add
any knowledge to the network or it reduces the general knowledge
of the network. Therefore, removing the i th input feature reduces
the complexity of the network and may also reduce the error of
the network. Additionally, reducing the error can lead to the reduc-
tion of the absolute value of the learning error shown in (16) . Con-
sequently, the absolute value of the weight adjustment shown in
(9) is reduced and the weights become stabilized. Thus, the feature
selection method can cooperatively work with the RBM weight ad-
justment method to reduce the ﬁnal error.
The proposed feature selection has been designed based on
training aim of RBM. Suppose that a RBM is trained on all the
input features. The weights are updated based on (9) . The weight
adjustment is dependent on E data [ v i h j ] −E model [ ˜ v i h j ] , a high value
for the expression leads to high weight adjustment and a low
value leads to low weight adjustment. If the weights are ad-
justed in such way that the RBM can regenerate the trained data,
i.e. v i = ˜ v i , then [ v i h j ] = [ ˜ v i h j ] leads to E data [ v i h j ] = E model [ ˜ v i h j ] .
The equality leads to zero adjustment for the weights, i.e.
E data [ v i h j ] −E model [ ˜ v i h j ] = 0 which implies that the network has
trained perfectly. So an absolute value close to zero is desirable
for the expression E data [ v i h j ] −E model [ ˜ v i h j ] , the zero value for the
expression means that the network is trained successfully and it
does not need more weight adjustments.
Based on the abovementioned desired aim for training an
RBM, reducing the weight adjustments by making E data [ v i h j ] −
E model [ ˜ v i h j ] close the zero, the proposed feature selection is de-
signed. The features are checked (individually or in groups) to de-
termine whether removing the features makes the RBM close to
the training aim. If the reconstructed visible unit is close to the
initial value of the visible unit i.e. v i = ˜ v i , the learning has become
close to its training aim and it does not need a high weight adjust-
ment (because E data [ v i h j ] −E model [ ˜ v i h j ] has a value close to zero).
Therefore, if by removing a visible unit a similar situation occurs,
i.e. the error of the reconstructed visible unit is reduced, which
implies that the removed feature not only does not add new in-
formation to the network (to reconstruct the visible unit) but it
also reduces the overall knowledge by increasing the reconstruc-
tion error. Therefore, it is better to remove the visible unit from
the selected feature set to make RBM become close to its training
aim. The proposed method, similar to the optimal brain surgeon
approach , works based on the comparison of two errors. In
the optimal brain surgeon approach the difference in the two er-
rors is created by pruning of learning parameters, however in the
proposed method the difference in errors is generated by eliminat-
ing the features. Then based on the derived reconstruction error
the features are selected. Additionally, the proposed feature selec-
tion is designed to work with DBM.
The procedure of the proposed feature selection is summarized
in the pseudocode described in Table 1 . After training the RBM, it
is used to calculate the reconstruction error e i for i th input fea-
tures by using (17) for all i . In the while loop in Table 1 , the pro-
posed method investigates a group of N e visible features. In each
iteration a group of N e features are eliminated by setting the cor-
Fig. 2. In each iteration, a number of input features, N e , are investigated, as a group
of features which can potentially be eliminated. The investigated group of features
are temporary removed by setting their values to zero, and then they are recon-
structed by the trained RBM. Setting N e to a high value reduces the number of
RBM reconstruction iterations required to investigate the entire input features. N e
is set at the start of learning and a ﬁxed N e is used in the all iterations.
responding visible units to zero. Investigation of a group of input
features when N e > 1 (see Fig. 2 ), reduces the number of required
iterations and consequently it reduces the processing time. The re-
construction error ¯e k is calculated for the N e eliminated features. ¯e k
is the reconstruction error of k th visible unit in the N e eliminated
features in the while loop in Table 1 . For the k th eliminated feature
if ¯e k < e k , where e k is the reconstruction error of same input fea-
ture before eliminating the features, then the k th eliminated fea-
ture is removed from input features permanently. Otherwise, the
k th eliminated feature is considered as selected features. The while
loop in Table 1 is continued until all the input features are inves-
In Fig. 2 the investigated group of features are selected by ad-
jacency. Reconstruction error ¯e i is utilized to evaluate the effect of
removing groups of features which have not been selected based
on adjacency; and groups of features which contain adjacent fea-
tures. ¯e i is the reconstruction error of the i th input feature when
N e input features are eliminated by setting them to zero. As shown
in Table 1 , the ¯e k < e k condition is used to decide whether to re-
move the k th feature, i.e. a visible unit with a lower reconstruction
error is removed permanently. Therefore, visible units with lower
¯e i in the current iteration are more likely to be removed in the
next feature selection iteration. Consequently, ¯e i can be used to
ﬁnd the next N e features that should be tested for removal. The
N e visible units that have the lowest ¯e i in current iteration of fea-
ture selection are selected for elimination test in the next iteration.
In this case, the N e selected visible units usually are not adjacent
An alternative version of the algorithm is presented in Table 2 ,
where the RBM is trained before feature selection on the all the
input features (similar to the ﬁrst version presented in Table 1 ).
Additionally, it is trained during the feature selection procedure
using the reduced number of features. In the alternative version,
the RBM is ﬁrst trained, then, e i is calculated using (17) for all vis-
ible features (see Table 2 ). After that, a set of N e features are se-
lected as candidate features and they are temporarily eliminated
from the input feature set by setting their values to zero. Then it
is decided if the k th eliminated feature should be removed per-
manently by using ¯e k . The elimination of N e features are repeated
to investigate all the input features. After removing of every N th
feature the RBM is trained with the reduced number of input fea-
tures. Then the new trained RBM is used to continue the feature
selection procedure (see Table 2 ).
Step 3 – Main training: After eliminating redundant features
by using one of the two algorithms provided in Tables 1 and 2 , the
training of the RBM continues using the selected features, i.e. RBM
with the remaining visible units is initialized by the previous cor-
responding weights and the learning is continued. The RBM and
the selected features are used for training the DBM similar to the
A. Taherkhani et al. / Neurocomputing 322 22–37
Pseudocode of the proposed feature selection method when the initially trained RBM is not trained during feature selection.
Train RBM on the training data.
Calculate the initial reconstruction error e i by the trained RBM using (17) for all i .
N v = ′ number of v isible features ′
While i < N v :
Select N e features for evaluation.
Set v k = 0 for k ∈ { N e selected features} (elimination of the N e features).
Calculate the reconstruction error ¯e k for each eliminated feature using (17) .
for k ∈ { N e eliminated features}:
if ¯e k < e k then :
Remove the k th visible unit.
N v = N v – 1
N s = N s – 1
Reset v k from 0 to its original value, and add it to selected features.
i = i + N s
Pseudocode of the alternative version of the proposed feature selection method when the RBM is trained during the feature
selection procedure.
Train RBM on the training data.
Calculate the initial reconstruction error e i by the trained RBM using (17) for all i .
N v = ′ number of v isible features ′
N r = 0 ; # number of removed features
While i < N v :
Select N e features for evaluation.
Set v k = 0 for k ∈ { N e selected features} (elimination of the N e features).
Calculate the reconstruction error ¯e k for each eliminated feature using (17) .
for k ∈ { N e eliminated features}:
if ¯e k < e k then :
Remove the k th visible unit.
N v = N v – 1
N s = N s −1
N r = N r + 1
Reset v k from 0 to its original value, and add it to selected features.
i = i + N s
If N r > N th :
Train RBM with current reduced number of features.
Calculate the initial reconstruction error e i by the trained RBM using (17) .
method used in . The learning method proposed in has an
unsupervised learning procedure where a DBM is initially trained.
Then the learning parameters of the trained DBM are used to ini-
tialize a corresponding multilayer neural network. Finally, a stan-
dard back propagation method is used to train the multilayer neu-
ral network.
In the proposed methods, the knowledge acquired during the
training process of the DBM (i.e. training of the RBM as a building
block of the DBM) is used to perform the feature selection, i.e. the
result of pre-training of DBM is used for feature selection. There-
fore, the computation cost of the feature selection task is reduced,
because feature selection is performed during the DBM’s learning
The proposed feature selection method uses the generative abil-
ity of RBM to reconstruct eliminated features and then calculates
reconstruction errors to determine whether to retain or remove
features. Other deep learning algorithms that have the generative
property can also be used by the proposed method. For instance,
the Deep Belief Network (DBN) is a deep learning method that
uses RBM during its training procedure; therefore, the proposed
feature selection method can be used with DBN. The Auto-Encoder
method is also a deep learning method that reconstructs its input
at the network output. Therefore, the proposed method can be ex-
tended to an Auto-Encoder. The proposed feature selection method
has the ability to work with data that are suitable for deep learning
3.3. Normalized error
The normalized error shown in (18) is used to evaluate the per-
formance of the proposed method.
where D is the number of visual input features, and ¯e i is the re-
construction error related to the i th visible unit calculated by (17) .
4. Experimental results
This section ﬁrst presents the experimental results of applying
the proposed Deep-FS method on ﬁve benchmark datasets, namely
A. Taherkhani et al. / Neurocomputing 322 22–37
Fig. 3. Deep-FS removes those peripheral pixels that are shown by dark dots on the
right column. The removed pixels do not have useful information about a digit.
MNIST , MIR-Flickr , GISETTE , MADELON , and PAN-
CAN . Datasets with a high number of features and train-
ing samples were selected in order to evaluate the accuracy and
speed of the proposed feature selection method on high dimen-
sional datasets. Thereafter, Deep-FS is compared with other feature
selection methods using the MNIST dataset. Finally, Deep-FS’s time
complexity is analysed using MNIST and randomly generated data.
4.1. Experimental results on MNIST
This section provides a brief description of the MNIST im-
age dataset, and provides an explanation and illustration of how
features are selected and removed using the proposed Deep-FS
method. Next, the effect of training an RBM during the feature se-
lection procedure is discussed via an empirical comparison with
the two proposed methods described in Tables 1 and 2 . After that
the effect of the application of the two selection methods for eval-
uating the features, i.e. selecting the N e adjacent features ( Fig. 1 ),
and selecting the N e features that have the lowest reconstruction
error, ¯e i , are investigated. Finally, the proposed feature selection
method, Deep-FS, is compared with the original DBM .
4.1.1. MNIST image dataset
In the ﬁrst set of the experiments, Deep-FS is applied on the
MNIST image dataset . MNIST contains 60,0 0 0 training samples
and 10,0 0 0 test image samples of handwritten digits. In the MNIST
dataset, each image is centred in a 28 × 28 pixel box. Each image in
the MNIST dataset is obtained from an original image which is in
a 20 × 20 pixel box through a transformation. The transformation is
fully performed in such a way that the centre of mass of the pixels
is preserved in the two images. This pre-processing is described by
LeCun et al. . The handwritten digits have different thickness,
angular alignment, height and relative position in the image frame.
4.1.2. Illustration of the selected and removed features on the MNIST
Fig. 3 illustrates some samples of reconstructed images after ap-
plying Deep-FS described in Table 2 on the MNIST dataset. The left
column of Fig. 3 shows samples of the original images and the
right-hand column shows the reconstructed images with the re-
moved pixels ﬁlled by black pixels. Fig. 3 shows that the method
has in practice removed pixels surrounding the digits. The pe-
ripheral pixels do not contain any useful information about the
Fig. 4. (a) Histogram of removed pixels. (b) Order of indexing the pixels. The his-
togram shows that a high number of pixels which are located in the upper and
lower sections (corresponding to the ﬁrst and the last columns in the histograms)
are removed. The removed pixels do not contain useful information about the digit.
digit, and therefore were removed. Some other pixels in the mid-
dle area of the images have been removed. The removed pixels can
be reconstructed by the information from neighbouring pixels, and
removing these pixels does not destroy the general appearance of
the digits.
In Fig. 4 (a), a histogram illustrating the removed pixels for digit
7 is shown. The pixels are indexed from 1 to 784 as shown in
Fig. 4 (b). The pixel located at the top right corner has the index
of 1, the pixel next to it on its right side has the index of 2 and
so forth. The ﬁrst and last columns in the histogram, shown in
Fig. 4 (b), have higher values than others and correspond to the
pixels that are at the top and bottom of the ﬁgure, respectively.
As shown in Fig. 3 , there is not much information in those pixels,
and therefore these were removed appropriately by the proposed
feature selection algorithm. Less pixels are removed from the area
in the middle part of each image of the digits, as shown by shorter
columns in the middle part of Fig. 4 (b).
In this paper the learning method proposed by Salakhutdinov
and Hinton is used as the baseline learning method for com-
parison purposes. In this baseline learning method, a DBM is ini-
tially trained, and thereafter the trained DBM is used to initial-
ize a multilayer neural network. Then, a standard back propagation
method is used to train the multilayer neural network.
The input vector has 28 × 28 = 784 units. The DBM has two hid-
den layers. The ﬁrst hidden layer has 500 hidden units and there
are 10 0 0 hidden neurons in the second hidden layer. A similar
structure is used is used in the proposed method to compare the
results. The difference between the Deep-FS and DBM is in
the ﬁrst layer. Deep-FS selects a set of input pixels to reduce the
number of input units. The maximum number of learning epochs
are the same for all the methods. For example, the DNNs are ﬁne-
tuned for 100 learning epochs using backpropagation methods.
4.1.3. The effect of training the RBM during feature selection
Table 3 shows the experimental results when N e adjacent fea-
tures are used during the feature selection s stage. In this exper-
iment, the results of training the ﬁrst RBM during the feature se-
lection stage(using the algorithm described in Table 1 ) are com-
pared with those results when the RBM is not trained during fea-
ture selection (i.e. using the algorithm of Table 2 ). In the ﬁrst
column, Deep-FS 1 , Deep-FS 5 , and Deep-FS 10 are proposed meth-
A. Taherkhani et al. / Neurocomputing 322 22–37
Experimental results on the MNIST data when adjacent features are used.
#Input features
Classiﬁcation error during
testing out of 10,0 0 0 b
Processing time in
seconds (s)
RBM is not trained during feature selection (See Table 1 )
Deep-FS 10
RBM is trained during feature selection (See Table 2 )
Deep-FS 10
a Number of eliminated features before a reconstruction.
b Classiﬁcation–error during Training Out of 60,0 0 0 is zero for all the methods.
Experimental results on the MNIST dataset when the features with the lowest reconstruction error, ¯e i , are used.
#Input features
Classiﬁcation error during
testing out of 10,0 0 0 b
Processing time in
seconds (s)
RBM is not trained during feature selection (Algorithm shown in Table 1 )
Deep-FS 10
RBM is trained during feature selection (Algorithm shown in Table 2 )
Deep-FS 10
a Number of eliminated features before a reconstruction.
b Classiﬁcation–error during Training Out of 60,0 0 0 is zero for all the methods.
ods when N e is set 1, 5, and 10, respectively. N e is the number
of pixels that are eliminated before feature reconstruction. In each
iteration of the feature selection phase, N e features are eliminated
and thereafter reconstructed together by RBM as explained in
Section 3.2 and Table 1 . Deep-FS can evaluate features (pixels) one
by one ( N e = 1) or it can evaluate a group of features (pixels) as
described in Section 3.2 ( N e > 1). In the later situation, instead of
searching by a single feature, N e = 1, the search process searches by
a group of features in groups of 5 and 10, i.e. N e = 5 and N e = 10,
respectively. The third column, #Input Features, is the number of
input pixels (features) which are selected during feature selection.
The fourth column, Classiﬁcation Error During Testing, shows
the classiﬁcation error on the testing data (i.e. test images). Each
test image is input into the network then the output of the ten
neurons, each of which is corresponding to a class, on the out-
put layer are investigated. The test input is assigned to a class
that corresponds to the output neuron that has the maximum
output value. The number of incorrect assignments are collected
over 10,0 0 0 testing images and the results are reported in col-
umn Classiﬁcation Error during Testing. The processing time col-
umn, is the total required time for the methods to perform training
on 60,0 0 0 training images and also to test the trained network on
the 10,0 0 0 testing images. Experiments were performed on an In-
tel E5-2640 v4 2.40 GHz processor with 64 GB RAM. Table 3 shows
training RBM during feature selection slightly reduces the errors
when N e > 1 adjacent features are used.
4.1.4. The effect of evaluating the features with the lowest ¯e i
Table 4 shows the results when the N e features that have
the lowest reconstruction error, ¯e i , are used during the fea-
ture selection process. The N e features with the lowest recon-
struction errors in the current iteration of feature selection are
evaluated in the next iteration of the feature selection proce-
dure, and these features are usually not adjacent. Comparing
Tables 3 and 4 reveals that using the features that have the low-
est reconstruction error, ¯e i , during the feature selection procedure
( Table 4 ) improves the accuracy of the proposed feature selection
method, compared to when adjacent features are used ( Table 3 ).
For instance, comparing Tables 3 and 4 , the classiﬁcation error for
Deep-FS 10 is reduced from 97 ( Table 3 when RBM is not trained)
to 90 ( Table 4 when RBM is not trained). The results show that
the approach which eliminates N e features with the lowest recon-
struction error has higher accuracy compared to the method that
eliminates N e features which are adjacent.
Additionally, the results in Table 4 show that using the feature
selection method, which does not train the RBM during the fea-
ture selection procedure (see Table 1 ), has higher accuracy com-
pared to when RBM is trained during feature selection in addi-
tion to the initial training of RBM (see Table 2 ) when N e > 1. For
instance, Table 4 shows that Deep-FS 10 misclassiﬁed fewer image
samples when RBM was not trained during the feature selection
procedure, and the number of misclassiﬁed images was reduced
from 94 to 90.
In conclusion, highest classiﬁcation accuracy is achieved when
a RBM is not trained during the feature selection procedure, and
hence when Deep-FS 10 uses the initially trained RBM before fea-
ture selection and then performs the feature selection procedure
by the initially trained RBM.
4.1.5. Comparing Deep-FS with the baseline DBM
The proposed Deep-FS method was compared against the DBM
 method using the MNIST dataset. The comparison considered
the effect of each approach on reducing the number of input fea-
tures and misclassiﬁed cases, and reduction in processing time.
The baseline DBM is the one which was originally introduced by
Salakhutdinov and Hinton . In this comparison, Deep-FS 10 is
used because the experiments in Sections 4.1.3 and 4.1.4 revealed
that removing features in groups of 10 is a better feature elimi-
nation strategy which provides higher classiﬁcation accuracy. For
Deep-FS 10 , RBM is not trained during the feature selection proce-
dure. Additionally, features are evaluated based on the lowest re-
construction error, ¯e i , as its results are reported in Table 4 .
A. Taherkhani et al. / Neurocomputing 322 22–37
Experimental results on the MNIST dataset when the features with the lowest reconstruction error,
¯e i , are used and the RBM is trained during feature selection.
#Input features
Classiﬁcation error during
testing out of 10,0 0 0 b
Processing time
in seconds (s)
Baseline DBM 
Deep-FS 10 a
a RBM is not trained during feature selection, and features with the lowest reconstruction error,
e i , are used (see Table 4 ).
b Classiﬁcation–error during Training Out of 60,0 0 0 is zero for all the methods.
The baseline DBM is trained on all 748 input pixels (see
ﬁrst row of Table 5 ). Table 5 shows that the proposed Deep-
FS 10 method has reduced the number of input features. The pro-
posed method selected 430 features out of 784 total input fea-
tures (see second row of Table 5 ). The results show that the pro-
posed method removes more than 45% of the input features. The
proposed method reduced the number of misclassiﬁed cases from
97 to 90 for the baseline DBM and the proposed method respec-
tively (see Table 5 ). The results show that the proposed method’s
capability in ﬁnding redundant features that do not add new in-
formation to the network and removing these redundant features
without reducing the classiﬁcation accuracy and it is much faster
than the baseline DBM. Selecting appropriate features based on
the lowest error difference helps the algorithm ﬁnd appropriate
features, and increases accuracy. The classiﬁcation error of all the
methods on the 60,0 0 0 training samples are zero.
The processing time of the experiment for the proposed method
is reduced by about 5% when N e is increased from 1 to 10 fea-
tures. The processing time is reduced from 55,134 s for N e = 1 to
52,442 s for N e = 10 (see Table 4 when RBM is not trained during
feature selection). Eliminating a number of input features, N e > 1,
in each investigation reduces the required number of reconstruc-
tion procedures which consequently reduces processing time. The
number of selected features is increased by about 4% when N e is
increased from 1 to 10 (see Table 4 when RBM is not trained dur-
ing feature selection). A reconstruction error related to each elimi-
nated feature is increased when the number of eliminated features,
N e , before the reconstruction is high. Eliminating a high number
of input features during feature selection increases the reconstruc-
tion errors, ¯e k . Consequently, a high number of features is kept in
the selected feature set when ¯e k < e k is used for removing the fea-
tures ( Table 1 ). Moreover, Table 5 shows that the processing time
of Deep-FS 10 is lower than the baseline DBM . The processing
time of the proposed Deep-FS 10 is 52,442 s while the processing
time of the baseline DBM is 55,505 s, i.e. it has about 5.5%
lower processing time. The low number of selected features, 430,
for Deep-FS 10 compared to the initial 784 features that should be
processed by the baseline DBM reduces the processing time of the
proposed Deep-FS 10 .
4.2. Experimental results on the MIR-Flickr dataset
In the second set of experiments, the performance of the pro-
posed Deep-FS 1 method, that was used for MNIST data in Section
4.1 , is tested using the MIR-Flickr dataset obtained from the
Flickr.com social photography site . The MIR-Flickr dataset
contains one million samples. Each input sample consists of an im-
age which may have user-deﬁned image tags. Additionally, some of
the input samples, image and user text tags, are labelled. Out of
the one million input samples, only 25,0 0 0 images with user as-
signed tags are annotated with labels and the remaining 975,0 0 0
samples are unlabelled. Labelling large data is a very demanding
task. The images and their corresponding user assigned tags are
annotated by 38 labels including object and scene categories such
Fig. 5. Samples from the MIR-Flickr dataset. The top words are the annotations and
the words behind each image are tags written by users. The ﬁrst image has no tags.
Most of the images belong to various classes with different annotations as shown
in the top row.
as tree, bird, people, clouds, indoor, sky, and sunset. Each sample
can belong to a number of classes.
The unsupervised learning ability of RBMs, which are the build-
ing blocks of a DBM, enables DBMs to be trained using a huge
number of unlabelled data, and RBMs and DBMs are known for
their suitability in training unlabelled data. After initial training,
a limited number of labelled data can be used for ﬁne-tuning
the model. Out of the 25,0 0 0 labelled samples in the MIR-Flickr
dataset, 10,0 0 0 samples are used for training and another 50 0 0 la-
belled samples are used as a validation set. The remaining 10,0 0 0
samples are used during the testing stage . Each sample in the
MIR-Flickr dataset has two sets of features, i.e. text and image fea-
tures. First, the text features are described then the image features
are introduced.
Many words which appear in the user deﬁned tags are not in-
formative and some of them are ﬁltered. To organize the text input,
a dictionary is generated. The dictionary contains the 20 0 0 most
frequent tags, which were extracted from a set of user tags found
in one million samples. Then each text input is reﬁned, and each
text input contains only the text in the dictionary. Therefore, the
text data of each sample is represented by the vocabularies of its
user tags that are in the dictionary (i.e. the tags are restricted to
the ones in the dictionary). Additionally, different f eature extrac-
tion methods are used to extract real value features for each image.
In the previous experiment on the MNIST dataset, (see Section 4.1 )
the inputs had binary values, however, here there are a number of
real value features for each image. In total, 3857 features were ex-
tracted for each image . The following features were extracted
for each image : 1. Concatenating Pyramid Histogram of Words
(PHOW) (20 0 0 features), 2. Gist (960 features), 3. Colour Layout
Descriptor (192 features), 4. Colour Structure Descriptor (256 fea-
tures), 5. Scalable Colour Descriptor (256 features), 6. Edge His-
togram Descriptor (150 dimensions), and 7. Homogenous Texture
Descriptor (150 features). The different features extract different
aspects of an image. Three samples from MIR-Flickr dataset are
shown in Fig. 5 . The top row of Fig. 5 shows the annotations or
labels and the bottom row shows the user tags.
A. Taherkhani et al. / Neurocomputing 322 22–37
Classiﬁcation results using features extracted from the ﬁrst hidden layer of
image pathway.
Image features
Baseline DBM 
0.476 ± 0.003
0.756 ± 0.005
Proposed Deep-FS
0.478 ± 0.003
0.756 ± 0.008
Classiﬁcation results using features extracted from joint hidden layer.
Image features
Based DBM 
0.622 ± 0.003
0.880 ± 0.005
Proposed Deep-FS
0.622 ± 0.003
0.879 ± 0.005
The DBM used in is employed as the baseline learning
method for comparison, for the MIR-Flickr image-text data in this
set of experiments. There are 3857 Gaussian visible units with real
number output values for image input, and there are two hidden
layers for image pathway each of them composed of 1024 binary
units. A Replicated Softmax model with 20 0 0 input units is
used for text inputs. The text pathway is completed by two hidden
layers each of which has 1024 binary units. A joint layer with 2048
binary hidden units is placed after the image and text pathways.
Each sample, image with corresponding user text tags, found
in the MIR-Flickr dataset can belong to a number of classes.
Mean Average Precision (MAP) and precision at top-50 predictions
(Prec@50) are two standard methods commonly adopted to evalu-
ate multi-label classiﬁcation tasks . MAP and Prec@50 are also
used in this research to evaluate the classiﬁcation performance of
the proposed Deep-FS and the baseline DBM on the MIR-Flickr
Deep-FS uses the ﬁrst 50 0 0 batches of data with whole features
to train the RBM. There are 128 samples in each batch. Then the
proposed feature selection method uses the trained RBM to select
features. The learning process is continued with the reminder of
the training data by using the selected features. Extracted features
from the ﬁrst hidden layer of the image pathway are classiﬁed by
the logistic regression method to show the effect of the feature se-
lection method on the classiﬁcation results. The results are shown
in Table 6 . Deep-FS returned a higher MAP than the baseline DBM
method, achieving values of 0.478 and 0.476 respectively. There is
no notable difference in Prec@50. Deep-FS selects 3082 out of 3857
image features. The proposed feature selection method removes
775 features to reduce the number of input features. The classi-
ﬁcation results on the hidden features extracted from joint hidden
layer are shown in Table 7 . Deep-FS removes features without af-
fecting the classiﬁcation performance of the testing data.
In Fig. 6 the errors of the Deep-FS method on the training and
evaluation sets are compared to those of the baseline method,
DBM , across various learning steps. In each learning step a
new batch of data is trained. Eq. (17) is used to calculate the er-
rors. Until step 50 0 0 the two methods use the all the features so
they reach the same error levels of 0.4228 and 0.2658 on training
and evaluation sets respectively. In the next steps of the learning
process, the errors of both methods are reduced, however, the er-
ror drops faster and reaches a ﬁnal lower value when using the
proposed method. For instance, the proposed method reaches the
level of 0.2590 on the training set which is lower than that of
the base method, i.e. 0.3075 ( Fig. 6 (a)) at the end of the learn-
ing steps. Similarly, the proposed method reaches the error level of
0.1745 compared to 0.2278 for the baseline method on evaluation
set ( Fig. 6 (b)). The errors for the proposed Deep-FS method and the
baseline DBM are 34% and 14% lower than the error at step 50 0 0,
i.e. 0.2658, respectively. The feature selection method removes the
redundant and irrelevant features and consequently prevents over-
Fig. 6. Comparison of the error of Deep-FS against the error of the baseline DBM
 on (a) training and (b) validation sets at different learning steps using the MIR-
Flickr dataset. In each learning step a batch of training data is trained. The errors
are reported after every 50 0 0 steps. The proposed method signiﬁcantly reduces the
ﬁtting the training data. The proposed method ﬁnds 775 irrelevant
and redundant features. The removed features construct about 20%
of the initial 3857 features.
4.3. Experimental results on the GISETTE dataset
The GISETTE dataset is a benchmark dataset generated for
binary classiﬁcation tasks. GISETTE is a handwritten digit recogni-
tion dataset, which was part of the Advances in Neural Informa-
tion Processing Systems feature selection challenge.
The GISETTE training data contains 60 0 0 samples and 50 0 0 fea-
tures. The GISETTE learning task is a binary classiﬁcation task to
discriminate between two confusable handwritten digits of 4 and
9. In GISETTE, each digit has a dimension of 28 × 28. The pixels
A. Taherkhani et al. / Neurocomputing 322 22–37
Performance comparison on the GISETTE dataset.
# Features
10-fold classiﬁcation
accuracy (%)
Classiﬁer training
time in seconds (s)
No feature
Proposed Deep-FS
Performance comparison on the MADELON dataset.
# Features
10-fold classiﬁcation
accuracy (%)
Classiﬁer training
time in seconds (s)
No feature
Proposed Deep-FS
are normalised on the dataset to put their values in the in-
terval. The feature set contains the normalized values of pixels in
addition to other features which have useful information for dis-
criminating between digits 4 and 9. In the experiment on GISETTE,
60 0 0 samples are used to train and test the proposed method. The
Decision Tree classiﬁer was adopted and k-fold cross validation
with k = 10 was applied to evaluate the performance of the pro-
posed feature selection method. The Decision Tree classiﬁer was
selected experimentally as it had achieved the highest classiﬁca-
tion accuracy compared to alternative conventional machine learn-
ing methods. Additionally, experiments showed that the Decision
Tree classiﬁer was trained in shorter time compared to most of
the other methods. The number of splits in the Decision Tree was
experimentally selected and set to 30. Table 8 shows that the pro-
posed Deep-FS reduces the number of input features from 50 0 0 to
951, i.e. reduction of 81% of input features. The accuracy of the De-
cision Tree classiﬁer on the selected features is 93.5%. Its accuracy
when using the entire set of features decreased to 92.9%. Using a
smaller subset comprising the selected features reduced the train-
ing time of the classiﬁer (see Table 8 ). The classiﬁer needed about
262 s to train using all 50 0 0 features, however, when the train-
ing was performed using the selected features (i.e. 951) features,
it only needed 47 s to train. The proposed method reduced 82% of
the classiﬁer’s training time (see Table 8 ).
4.4. Experimental results on the MADELON dataset
The MADELON dataset is an artiﬁcial dataset, which was
part of the Advances in Neural Information Processing Systems
 feature selection challenge. This is a two-class clas-
siﬁcation problem with continuous input variables. The challenge
is that the problem is multivariate and highly non-linear. In this
experiment, 20 0 0 training samples from the MADELON dataset are
used, and each sample has 500 features. The performance of the
proposed Deep-FS method on the MADELON dataset is reported
in Table 9 . Deep-FS reduces more than 57% of input features and
achieves a higher classiﬁcation accuracy, i.e. 80.3%. Additionally,
it reduced the computation time of training the classiﬁer, as re-
ported in Table 9 . It reduced 70% of the classiﬁer training time. In
summary, higher or very close accuracy is achieved using a much
smaller set of features, but in less time (i.e. 4.44 fewer seconds)
when Deep-FS is used.
4.5. Experimental results on the PANCAN dataset
PANCAN was obtained from TCGA Pan-Cancer Clinical Data
Resource (TCGA-CDR) . The data contains 801 data samples
from patients with 5 types of tumours: COAD, LUAD, PRAD, BRCA
and KIRC. Each patient sample contains 20,531 features. A total of
264 features had the same value for all samples in the dataset and
these were removed, resulting in a total number of 20,264 features.
Table 10 shows thatDeep-FS has reduced the number of features
from 20,264 to 4765, i.e. 76.49% reduction in the number of in-
put features, and increased the accuracy from 97.1% to 98.5%. 10-
fold cross validation was applied to achieve the results for each of
the two situations reported in Table 10 . Importantly, the results re-
vealed signiﬁcant reduction in the time needed by the classiﬁer to
train using the selected features. Training on the selected features
needed 59.19 s compared to 400.39 s when all the features are
used. Hence, training on the selected features was 341.20 s faster
(i.e. it reduced 85% of the classiﬁer’s training time).
4.6. Comparison of Deep-FS with other feature selection approaches
In the following experiments the proposed Deep-FS is com-
pared with other feature selection approaches. The comparison is
performed in the following two steps. Step 1: Select features using
the proposed and other existing feature selection algorithms; and
Step 2: Train DBM using the selected subset of features.
Step 1: Initially, the proposed Deep-FS method and three other
feature selection methods were separately applied to the MNIST
dataset. Each feature selection method returned a selected subset
of features, and then the selected features were used to train a
DBM (results are presented in Table 11 ). Please note that most of
the conventional existing feature selection algorithms are compu-
tationally very expensive and not suitable for large data. The Ge-
netic Algorithm (GA) for feature selection described in , the In-
ﬁnite Feature Selection (InfFS) , and the Laplacian Score (LS)
for feature selection methods were compared to Deep-FS. The
GA-based mRMR freature selection algorithm calculates the
joint mutual information matrix between pairs of features and
this makes the algorithm impractical for high dimensional datasets.
InfFS is a ﬁlter feature selection method that selects the most
important features based on their ranks. All other features are con-
sidered to evaluate the score of a feature. InfFS maps the feature
selection task to a graph and the feature selection is considered as
a subset of features that make a path in the graph. A cost matrix is
constructed to give pairwise associations between the features us-
ing variance and correlation of the features. The matrix is used to
evaluate relevance and redundancy of a feature with respect to all
the other features. Laplacian Score (LS) for feature selection is
another well-known method with the ability of ﬁnding features in
unlabelled data . The LS method uses a nearest neighbour
graph to evaluate local geometric structures and selects features
based on the constructed graph.
Step 2: The selected features identiﬁed by each feature selec-
tion method are used to train DBMs. The weights of each DBM are
initialized randomly, then the DBM is trained on the selected fea-
A. Taherkhani et al. / Neurocomputing 322 22–37
Performance comparison on the PANCAN dataset.
# Features
10-fold classiﬁcation
accuracy (%)
Classiﬁer training
time in seconds (s)
No feature
Proposed Deep-FS
Experimental results with the MNIST dataset. Applying different feature selection methods and
using the selected features to train a DBM. The number of selected features was set to 430. The
training accuracy reached 100% for all the methods.
Feature Selection method
Number of misclassiﬁed images
during testing
(out of 10,0 0 0)
Processing time
for FS in seconds
GA with DBM
InfFS with DBM
Laplacian with DBM
Proposed Deep-FS 10
Improvement in the number of misclassiﬁed images and processing time of Deep-FS 10 vs other methods.
Feature selection method
Improvement in the number
of misclassiﬁed images
Improvement in processing
time in seconds (s)
Deep-FS 10 vs GA 
+ 30,651 s (510.9 min)
Deep-FS 10 vs InfFS 
Deep-FS 10 vs Laplacian 
+ 14,136 s (235.6 min)
tures. In the proposed Deep-FS 10 , the weights which are trained
during feature selection are used for initializing the DBM. The re-
sults are reported in Table 11 .
Table 11 shows that the proposed Deep-FS 10 has achieved
higher accuracy than the alternative approaches. Additionally, the
proposed Deep-FS method can ﬁnd the number of the selected fea-
tures, i.e. 430, automatically. However, the other three feature se-
lection methods require that the user speciﬁes the number of se-
lected features at the start of the feature selection procedure. The
number of features (i.e. 430), obtained by the proposed method, is
used by the other feature selection methods. The datasets are large
and it is computationally very expensive to run the experiments
using various numbers of features to experimentally determine the
most suitable number of features to select. For this reason, there is
a need for feature selection algorithms, such as Deep-FS, which can
automatically identify the most relevant features in large data. The
simulation results in Table 11 show that the proposed method has
misclassiﬁed 90 images out of 10,0 0 0 (0.9% of the images) which
is a lower error rate than the alternative methods. The training
accuracy of the trained DBMs for all the methods is 100%. Table
11 also shows that the proposed feature selection method has the
shortest processing time compared to the other methods. The GA
 and Laplacian methods need a much longer computation
time to perform the feature selection task compared to Deep-FS.
For instance, GA took 30,784 s while the proposed method
only took 133 s for the same feature selection task. The classical
feature selection methods have high computational cost when ap-
plied to large datasets that have a high number of features and/or
training samples. The improvement in performance and time when
using the proposed Deep-FS 10 method instead of the other meth-
ods is provided in Table 12 .
Most Classical feature selection methods have not been de-
signed to work on datasets which contain a large number of fea-
tures. Furthermore, classical feature selection methods have been
designed to take as input a single matrix that contains all the
training samples, and this is another reason which makes them
unsuitable for large data. For instance, the unsupervised feature se-
lection for multi-cluster data (MCFS) method proposed by Cai et.al.
 was applied to the MNIST dataset, but computational prob-
lems were encountered. In particular, because MCFS constructs a
square matrix with size N × N , where N is the number of training
samples, and there exist N = 60,0 0 0 image samples in MNIST, the
square matrix was very big and MCFS could not converge when
applied to the MNIST data. Other methods such as GA method can
be applied to MNIST and other large datasets but have a high com-
putation cost and computation time. However, the proposed Deep-
FS overcomes the diﬃculties of working with datasets containing a
high number of features and samples by dividing the training sam-
ples in a number of batches similar to what is performed in deep
learning methods.
4.7. Time complexity analysis of the proposed method
In order to analyse the time complexity of the proposed
method, two experiments are performed. The time complexity of
the method is analysed in regard to the number of training sam-
ples and the number of input features.
In the ﬁrst experiment, the computation times of the proposed
Deep-FS method are obtained for different numbers of training
samples. The MNIST dataset is used in the ﬁrst experiment. The
number of training samples is increased from 50 0 0 to 50,0 0 0 in
steps of 50 0 0, and the running time of the proposed method for
each number of training samples is calculated. Fig. 7 shows the
computation time of the proposed Deep-FS against the number
of training samples. A line with the equation of T ( n ) = 0.0043 n +
0.8140 can ﬁt to the data points. The equation shows that the time
complexity of the proposed method in regard to the number of
training samples is O ( n ) in the big O (.) notation.
In the second experiment for analysing the time complexity
of the proposed method, the total number of input features is
changed and the computation time of the proposed Deep-FS is cal-
culated for the different number of input features. Uniformly dis-
tributed random datasets in interval with different numbers
of input features are generated to perform the second time com-
A. Taherkhani et al. / Neurocomputing 322 22–37
Fig. 7. Computation time of the proposed Deep-FS method when using various syn-
thetic datasets each containing a different number of features. A line ﬁts to the data
points which are shown by ‘O’.
Fig. 8. Running time of the proposed Deep-FS method when using various syn-
thetic datasets each containing a different number of features. A polynomial equa-
tion with the degree of 2 ﬁts to the data points.
plexity analysis. There are 10 0 0 training samples in the randomly
generated dataset. The number of features are increased from 100
to 1900 with the interval of 200 features. Therefore, 10 random
datasets, each with a different number of features. The running
time of the proposed Deep-FS is calculated for each of the datasets
and it is plotted in Fig. 8 with the sign ‘o’. Then, a polynomial
curve ﬁts to the data points. The equation of the ﬁtted curve is T(n)
= 10 −3 (0.0188 n 2 −3.2 n + 1171.8). The equation shows that the time
complexity of the algorithm is O ( n 2 ) using the big O (.) notation.
The simulation results show that the time complexity of the pro-
posed method in regard to the number of input features is higher
that its complexity related to the number of training samples, i.e.
O ( n 2 ) vs O ( n ).
5. Conclusion
This paper proposes a novel feature selection algorithm, Deep-
FS, which is embedded into the DBM classiﬁer. DBM is considered
as a non-linear feature extractor that can reduce the dimension-
ality of large or big data. The proposed Deep-FS feature selection
method works in conjunction with the feature extraction method
of DBM to reduce the number of input features and learning er-
rors. Deep-FS uses a RBM that is trained during the training stage
of a DBM to reduce computational cost. Deep-FS uses the genera-
tive property of RBMs which enables RBMs to reconstruct missing
input features. A group of features is temporary eliminated from
the input feature set to evaluate reconstruction error using a new
criterion based on RBM. RBM treats the eliminated feature(s) as
missing feature(s) and reconstructs these feature(s) by using the
information from other input features. Then, the reconstruction er-
ror is used as a criterion for feature selection. The proposed fea-
ture selection method has two versions. In the ﬁrst version of the
proposed method, a RBM is initially trained, then it is used for fea-
ture selection. In the second version of the proposed method, the
initially trained RBM is additionally trained on the reduced fea-
ture set during a feature selection procedure. Experiments revealed
that the ﬁrst version has a higher classiﬁcation accuracy than the
second version. Experiments also revealed that removing selected
groups of features instead of single adjacent features improves per-
formance and feature selection time.
Deep-FS was evaluated using the MNIST, MIR-Flickr, PANCAN,
GISETTE and MADELON benchmark datasets. The results demon-
strated that Deep-FS can reduce the number of inputs without af-
fecting classiﬁcation performance. Deep-FS reduced the number of
misclassiﬁed samples on the MNIST data from 97 to 90. The pro-
posed method automatically selected 430 features out of 784 fea-
tures and it reduced the total processing time by 3063 s. When ap-
plied to the MIR-Flickr dataset it altered MAP from 0.476 to 0.478.
The impact on classiﬁcation accuracy is minor, which is a desirable
result given that the number of inputs was reduced.
Moreover, Deep-FS has reduced the computation time. The pro-
posed algorithm was effective in reducing the number of input fea-
tures, i.e. it removed 15,499 features out of 20,264 features, and
reduced classiﬁer training time by 85% for the PANCAN dataset.
Experiment results also revealed that the proposed feature selec-
tion method reduced the number of input features, improved cross
validation accuracy, and reduced classiﬁer training time on the
GISETTE and MADELON datasets.
The proposed method was compared with three other feature
selection methods namely the: GA , Inﬁnite feature selection
(InfFS) , and Laplacian Score for feature selection us-
ing the MNIST dataset. The results showed that the proposed fea-
ture selection method reduced the number of misclassiﬁed images
compared to the other methods. Additionally, it reduced the pro-
cessing time of feature selection, for instance the proposed feature
selection method performed automatic feature selection in 133 s
while the GA method performed the same feature selection
task in 30,784 s.
Deep-FS can improve the processing ability of the deep learn-
ing method for multimodal data. Recently, Deep Neural Networks
have shown their ability to process multimodal data with a large
volume of data . One common property of the multimodal
data is their high dimensionality. Not all the input features might
have useful information and irrelevant input features can introduce
noise and degrade the performance. Reducing the number of in-
put features and removing the irrelevant features can improve the
ability of a deep learning model to process multimodal data. The
feature selection method reduces computational cost by reducing
the size of reconstructed matrix.
DBNs belong to a group of DNNs that uses an unsuper-
vised pre-training stage. During the ﬁrst learning phase of DBNs,
layer-wise unsupervised training is performed. Each layer learns a
non-linear transformation from its input to capture the main in-
formation of its input. Each adjacent pair of layers is considered
as an RBM . An RBM is used to govern the unsupervised train-
ing and to extract features. The proposed feature selection method,
A. Taherkhani et al. / Neurocomputing 322 22–37
which works based on RBM, can be applied to DNNs to improve its
processing abilities. DBNs have demonstrated good results in
different applications such as speech recognition , audio ,
image and text classiﬁcation . To apply the proposed method
to other deep learning methods, the proposed feature selection
method can be initially used to select features which will be
input into the deep learning classiﬁer (or other classiﬁer), and the
classiﬁer can be trained on the selected features.
Koller and Sahami’s Markov Blanket ﬁltering feature selection
method eliminates a feature if there is a Markov Blanket
for the feature. For a target feature, a Markov Blanket is a minimal
set of variables from a feature space on which all other variables
are conditionally independent of the target feature. However, it is
not straightforward to determine whether a set of features makes a
Markov Blanket for the target feature, especially when the number
of input features is high . The proposed Deep-FS method
deﬁnes a criterion for each feature and checks whether other fea-
tures can reconstruct the target feature. In particular, with the pro-
posed method, when the reconstruction error of a feature is re-
duced, the other features can contain a Markov Blanket for the tar-
get feature and that feature can be eliminated.
The proposed feature selection method will be very useful to
researchers working with large and big data. Currently there are
not many feature selection methods suitable for large data. The pa-
per demonstrates that the proposed Deep-FS can be applied to uni-
modal and multimodal data for various tasks. In particular, Deep-
FS has been applied to unimodal handwriting digit recognition
datasets (MNIST, and GISETTE), a multi-modal dataset comprising
images and text (MIR-Flickr), and a biomedical dataset (PANCAN).
Reducing the number of inputs and consequently the size of
constructed weight matrix can be useful to manage limited hard-
ware resources during hardware implementation of DNNs for com-
plex tasks . Deep-FS can be used to reduce the input size,
and the trained network for a speciﬁc task can be implemented
with less silicon area on hardware. Future work includes exploring
the capability of the proposed Deep-FS in reducing the complexity
of deep learning networks, through reducing the number of the in-
put features in real world applications when the inputs are gener-
ated by sensors. Reduction of input features leads to the reduction
of the number of sensors which can consequently reduce imple-
mentation costs. Deep-FS can offer a systematic way to ﬁnd an op-
timized number of sensors. For example, Deep-FS can be applied
to optimize the number and selected positions of sensors. Future
work also includes applying the algorithm to large-scale data ana-
lytics tasks, such as human activity recognition which require use
of deep learning algorithms.
Acknowledgements
The work was funded by The Leverhulme Trust Research Project
Grant RPG-2016-252 entitled “Novel Approaches for Constructing
Optimised Multimodal Data Spaces”.