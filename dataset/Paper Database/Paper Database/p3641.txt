Machine Learning, 47, 201–233, 2002
c⃝2002 Kluwer Academic Publishers. Manufactured in The Netherlands.
On the Learnability and Design of Output
Codes for Multiclass Problems
KOBY CRAMMER∗
 
YORAM SINGER†
 
School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel
Editor: Jyrki Kivinen
Output coding is a general framework for solving multiclass categorization problems. Previous research on output codes has focused on building multiclass machines given predeﬁned output codes. In this paper
we discuss for the ﬁrst time the problem of designing output codes for multiclass problems. For the design problem
of discrete codes, which have been used extensively in previous works, we present mostly negative results. We
then introduce the notion of continuous codes and cast the design problem of continuous codes as a constrained
optimization problem. We describe three optimization problems corresponding to three different norms of the code
matrix. Interestingly, for the l2 norm our formalism results in a quadratic program whose dual does not depend on
the length of the code. A special case of our formalism provides a multiclass scheme for building support vector
machines which can be solved efﬁciently. We give a time and space efﬁcient algorithm for solving the quadratic
program. We describe preliminary experiments with synthetic data show that our algorithm is often two orders of
magnitude faster than standard quadratic programming packages. We conclude with the generalization properties
of the algorithm.
multiclass categorization, output coding, SVM
Introduction
Many applied machine learning problems require assigning labels to instances where the
labels are drawn from a ﬁnite set of labels. This problem is often referred to as multiclass
categorization or classiﬁcation. Examples for machine learning applications that include a
multiclass categorization component include optical character recognition, text classiﬁcation, phoneme classiﬁcation for speech synthesis, medical analysis, and more. Some of the
well known binary classiﬁcation learning algorithms can be extended to handle multiclass
problems . A general approach is to reduce a multiclass problem to a set of multiple
binary classiﬁcation problems.
Dietterich and Bakiri described a general approach based on error-correcting
codes which they termed error-correcting output coding (ECOC), or in short output coding.
Output coding for multiclass problems is composed of two stages. In the training stage
we need to construct multiple (supposedly) independent binary classiﬁers each of which is
∗ 
† 
K. CRAMMER AND Y. SINGER
based on a different partition of the set of the labels into two disjoint sets. In the second
stage, the classiﬁcation part, the predictions of the binary classiﬁers are combined to extend
a prediction on the original label of a test instance. Experimental work has shown that output
coding can often greatly improve over standard reductions to binary problems . The
performance of output coding was also analyzed in statistics and learning theoretic contexts
 .
Most of the previous work on output coding has concentrated on the problem of solving
multiclass problems using predeﬁned output codes, independently of the speciﬁc application and the class of hypotheses used to construct the binary classiﬁers. Therefore, by
predeﬁning the output code we ignore the complexity of the induced binary problems. The
output codes used in experiments were typically conﬁned to a speciﬁc family of codes.
Several families of codes have been suggested and tested so far, such as, comparing each
class against the rest, comparing all pairs of classes , random codes , exhaustive codes , and linear error correcting codes . A
few heuristics attempting to modify the code so as to improve the multiclass prediction accuracy were suggested . However, they did not yield signiﬁcant
improvements and, furthermore, they lack any formal justiﬁcation.
Inthispaperweconcentrateontheproblemofdesigningagoodcodeforagivenmulticlass
problem. In Section 3 we study the problem of ﬁnding the ﬁrst column of a discrete code
matrix. Given a binary classiﬁer, we show that ﬁnding a good ﬁrst column can be done in
polynomial time. In contrast, when we restrict the hypotheses class from which we choose
the binary classiﬁers, the problem of ﬁnding a good ﬁrst column becomes difﬁcult. This
result underscores the difﬁculty of the code design problem. Furthermore, in Section 4
we discuss the general design problem and show that given a set of binary classiﬁers the
problem of ﬁnding a good code matrix is NP-complete.
Motivated by the intractability results we introduce in Section 5 the notion of continuous codes and cast the design problem of continuous codes as a constrained optimization
problem. As in discrete codes, each column of the code matrix divides the set of labels
into two subsets which are labeled positive (+) and negative (−). The sign of each entry in
the code matrix determines the subset association (+ or −) and the magnitude corresponds
to the conﬁdence in this association. Given this formalism, we seek an output code with
small empirical loss whose matrix norm is small. We describe three optimization problems
corresponding to three different norms of the code matrix: l1,l2 and l∞. For l1 and l∞we
show that the code design problem can be solved by linear programming (LP). Interestingly, for the l2 norm our formalism results in a quadratic program (QP) whose dual does
not depend on the length of the code. Similar to support vector machines, the dual program
can be expressed in terms of inner-products between input instances, hence we can employ
kernel-based binary classiﬁers. Our framework yields, as a special case, a direct and efﬁcient
method for constructing multiclass support vector machines.
LEARNABILITY AND DESIGN OF OUTPUT CODES
The number of variables in the dual quadratic problem is the product of the number of
samples by the number of classes. This value becomes very large even for small datasets. For
instance, an English letter recognition problem with 1,000 training examples would require
26,000 variables. In this case, the standard matrix representation of dual quadratic problem
would require more than 5 Giga bytes of memory. We therefore describe in Section 6.1 a
memoryefﬁcientalgorithmforsolvingthequadraticprogramforcodedesign.Ouralgorithm
is reminiscent of Platt’s sequential minimal optimization (SMO) . However,
unlike SMO, our algorithm optimizes on each round a reduced subset of the variables
that corresponds to a single example. Informally, our algorithm reduces the optimization
problem to a sequence of small problems, where the size of each reduced problem is equal
to the number of classes of the original multiclass problem. Each reduced problem can again
be solved using a standard QP technique. However, standard approaches would still require
large amount of memory when the number of classes is large and a straightforward solution
is also time consuming. We therefore further develop the algorithm and provide an analytic
solution for the reduced problems and an efﬁcient algorithm for calculating the solution.
The run time of the algorithm is polynomial and the memory requirements are linear in the
number of classes. We conclude with simulations results showing that our algorithm is at
least two orders of magnitude faster than a standard QP technique, even for small number of
classes. We conclude in Section 7 with a short description of the generalization properties
of the algorithm.
Discrete codes
Let S = {(x1, y1), . . . , (xm, ym)} be a set of m training examples where each instance xi
belongs to a domain X. We assume without loss of generality that each label yi is an integer
from the set Y = {1, . . . , k}. A multiclass classiﬁer is a function H : X →Y that maps an
instance x into an element y of Y. In this work we focus on a framework that uses output
codes to build multiclass classiﬁers from binary classiﬁers. A discrete output code M is a
matrix of size k ×l over {−1, +1} where each row of M correspond to a class y ∈Y. Each
column of M deﬁnes a partition of Y into two disjoint sets. Binary learning algorithms
are used to construct classiﬁers, one for each column t of M. That is, the set of examples
induced by column t of M is (x1, My1,t), . . . , (xm, Mym,t). This set is fed as training data
to a learning algorithm that ﬁnds a hypothesis ht : X →{−1, +1}. This reduction yields l
different binary classiﬁers h1, . . . , hl. We denote the vector of predictions of these classiﬁers
on an instance x as ¯h(x) = (h1(x), . . . , hl(x)). We denote the rth row of M by ¯Mr.
Given an example x we predict the label y for which the row ¯My is the “closest” to ¯h(x).
We will use a general notion for closeness and deﬁne it through an inner-product function
K : Rl × Rl →R. The higher the value of K(¯h(x), ¯Mr) is the more conﬁdent we are that r
is the correct label of x according to the classiﬁers ¯h. An example for a closeness function
is K(¯u, ¯v) = ¯u · ¯v. It is easy to verify that this choice of K is equivalent to picking the row
of M which attains the minimal Hamming distance to ¯h(x).
Givenaclassiﬁer H(x)andanexample(x, y),wesaythat H(x)misclassiﬁedtheexample
if H(x) ̸= y. Let [[π]] be 1 if the predicate π holds and 0 otherwise. Our goal is therefore
to ﬁnd a classiﬁer H(x) such that 1
i=1[[H(xi) ̸= yi]] is small. We would like to note
K. CRAMMER AND Y. SINGER
in passing that we mainly focus on the empirical loss minimization problem. We brieﬂy
describe bounds on the generalization error in Section 7.
When l is small there might be more then one row of M which attains the maximal value
according to the function K. To accommodate such cases we will relax our deﬁnition and
deﬁne a classiﬁer H(X) based on a code M to be the mapping H(x) : X →2Y given by
H(x) = {y | K(¯h(x), ¯My) = maxr K(¯h(x), ¯Mr)}. In this case we will pick one of the
labels in H(x) uniformly at random, and use the expected error of H(x),
1 −[[yi ∈H(xi)]]
[[yi ∈H(xi)]]
In the context of output codes, a multiclass mapping H(x) is thus determined by two
parameters: the coding matrix M and the set of binary classiﬁers ¯h(x). Assume that the
binary classiﬁers h1(x) . . . hl(x) are chosen from some hypothesis class H. The following
natural learning problems arise,
(a) Given a matrix M, ﬁnd a set ¯h which suffers small empirical loss.
(b) Given a set of binary classiﬁers ¯h, ﬁnd a matrix M which has small empirical loss.
(c) Find both a matrix M and a set ¯h which have small empirical loss.
Previous work has focused mostly on the ﬁrst problem. In this paper we mainly concentrate on the code design problem (problem b), that is, ﬁnding a good matrix M. A summary
of the notation is given in Appendix B.
Finding the ﬁrst column of an output code
Assume we are given a single binary classiﬁer h1(x) and we want to ﬁnd the ﬁrst (or the
single) column of the matrix M which minimizes the empirical loss ϵS(M, ¯h). For brevity,
let us denote by ¯u = (u1 . . . uk)T the ﬁrst column of M. We now describe an efﬁcient
algorithm that ﬁnds ¯u given h1(x). The algorithm’s running time is polynomial in the size
of the label set k = |Y| and the sample size m. First, note that in this case
yi ∈H(xi) ⇔h1(xi) = uyi.
Second, note that the sample can be divided into 2k equivalence classes according to
their labels and the classiﬁcation of h1(x). For r = 1, . . . , k and b ∈{−1, +1}, deﬁne ab
m |{i : yi = r, h1(xi) = b}| to be the fraction of the examples with labelr and classiﬁcation b
(according to h1(x)). For b ∈{−1, +1}, denote byab = k
r , and let wb = |{r : ur = b}|
be the number of elements in ¯u which are equal to b. (For brevity, we will often use + and
−to denote the value of b.) Let
[[yi ∈H(xi)]]
= 1 −ϵS(M, ¯h).
LEARNABILITY AND DESIGN OF OUTPUT CODES
We can assume without loss of generality that not all the elements in ¯u are the same
(otherwise, ξS(M, ¯h) = 1
k , which is equivalent to random guessing). Hence, the size of
w+ h1(x) = +1
w−h1(x) = −1.
Using Eqs. (2) and (4), we rewrite Eq. (3),
ξS(M, ¯h) = 1
i:yi∈H(xi)
i:h(xi)=uyi
w+ ur = +1
w+ ur = +1
w−ur = −1.
Using Eq. (5) we now can expand ξS(M, ¯h),
ξS(M, ¯h) =
2(1 + ur) a+
2(1 −ur) a−
For a particular choice of w+ (and w−= k −w+) ξS is maximized (and ϵS is minimized)
by setting ur = +1 at the w+ indices which attain the highest values for ( a+
set the rest w−of the indices to −1. This can be done efﬁciently in k log k time using
sorting. Therefore, the best choice of ¯u is found by enumerating all the possible values
for w+ ∈{1, . . . , k −1} and choosing the value of w+, w−which achieves the maximal
value for Eq. (6). Since it takes m operations to calculate a+
r , the total number of
operations needed to ﬁnd the optimal choice for the ﬁrst column is O(m + k2 log k). We
have proven the following theorem.
Theorem 1.
Let S = {(x1, y1), . . . , (xm, ym)} be a set of m training examples, where
each label is an integer from the set {1, . . . , k}. Let H be a binary hypothesis class. Given
K. CRAMMER AND Y. SINGER
an hypothesis h1(x) ∈H, the ﬁrst column of an output code which minimizes the empirical
loss deﬁned by Eq. (1) can be found in polynomial time.
To conclude this section we describe a reduction from SAT. This reduction demonstrates
the difﬁculty in using a limited learning algorithm for a restricted class of hypotheses from
which h1 can be chosen. Speciﬁcally, we show that if one can solve the problem of ﬁnding
simultaneously a binary classiﬁer from a restricted class and a single column of a discrete
code which minimizes the empirical error of Eq. (1), then the satisﬁability problem can
also be solved. Let
(x1 . . . xn) be a boolean formula over the variables xi ∈{−1, +1}
where we interpret x = −1 as False and x = +1 as True. We now give the reduction to
the induced learning problem. Deﬁne X = {xi, ¯xi}n
i=1 ∪{⊥} to be the instance space. Let
S = {(xi, i)}n
i=1 ∪{(¯xi, i +n)}n
i=1 ∪{(⊥, 2n+1)} be a sample of size m = 2n+1, where the
labels are taken from Y = {1, . . . , 2n+1}. Deﬁne the learning algorithm L
as follows. The
algorithm’s input is a binary labeled sample of the form {(xi, yi), (¯xi, yi+n), (⊥, y2n+1)}n
(x1, . . . , xn) = True and for all i yi ⊕yi+n = True, then the algorithm returns an
hypothesis which is consistent with the sample (the sample itself). Otherwise, the algorithm
returns the constant hypothesis, h(x) ≡1 or h1(x) ≡0, which agrees with the majority
of the sample by choosing h1(x) ≡arg maxb∈{−1,+1} |{i : yi = b}|. Note that the learning
algorithm is non-trivial in the sense that the hypothesis it returns has an empirical loss of
less than 1/2 on the binary labeled sample.
We now show that a multiclass learning algorithm that minimizes the empirical loss ϵS
over both the ﬁrst column ¯u and the hypothesis h1(x) which was returned by the algorithm
, can be used to check whether the formula
is satisﬁable. We need to consider two
cases. When
(x1, . . . , xn) = True and for all i
yi ⊕yi+n = True, then using the
deﬁnition from Eq. (3) we get ξS = 1
n + (n + 1)
m . If the above conditions do
not hold (h1(x) is constant), let v ≥n + 1 be the number of examples which the hypothesis
h1(x) classiﬁes correctly. Then, using Eq. (3) again we obtain ξS = 1
m . Thus, the
minimum of ϵS is achieved if and only if the formula
is satisﬁable. Therefore, a learning
algorithm for h1(x) and ¯u can also be used as an oracle for ﬁnding whether a boolean
is satisﬁable.
While the setting discussed in this section is somewhat superﬁcial, these results underscore the difﬁculty of the problem. We next show that the problem of ﬁnding a good output
code given a relatively large set of classiﬁers ¯h(x) is intractable. We would like to note
in passing that an efﬁcient algorithm for ﬁnding a single column might be useful in other
settings. For instance in building trees or directed acyclic graphs for multiclass problems
 . We leave this for future research.
Finding a general discrete output code
In this section we prove that given a set of l binary classiﬁers ¯h(x), ﬁnding a code
matrix which minimizes the empirical loss ϵS(M, ¯h) is NP-complete. Given a sample
S = {(x1, y1), . . . , (xm, ym)} and a set of classiﬁers ¯h, let us denote by ˜S the evaluation
of ¯h(·) on the sample S, that is ˜S = {(¯h1, y1), . . . , (¯hm, ym)}, where ¯hi
= ¯h(xi). We now
show that even when k = 2 and K(¯u, ¯v) = ¯u · ¯v the problem is NP-complete. (Clearly, the
LEARNABILITY AND DESIGN OF OUTPUT CODES
problem remains NPC for k > 2). Following the notation of previous sections, the output
code matrix is composed of two rows ¯M1 and ¯M2 and the predicted class for instance xi
is H(xi) = arg maxr=1,2{ ¯Mr · ¯hi}. For the simplicity of the presentation of the proof, we
assume that both the code M and the hypotheses’ values ¯hi are over the set {0, 1} (instead
of {−1, +1}). This assumption does not change the problem as it possible to show that the
same proof technique can be used with hypotheses whose outputs are in {±1}.
Theorem 2.
The following decision problem is NP-complete.
Input: A natural number q, a labeled sample ˜S = {(¯h1, y1), . . . , (¯hm, ym)}, where yi ∈
{1, 2}, and ¯hi ∈{0, 1}l.
Question: Does there exist a matrix M ∈{0, 1}2×l such that the classiﬁer H(x) based
on an output code M makes at most q mistakes on ˜S.
Our proof is based on a reduction technique introduced by H¨offgen and Simon
 . Since we can check in polynomial time whether the number of classiﬁcation errors
for a given a code matrix M exceeds the bound q, the problem is clearly in NP.
We show a reduction from Vertex Cover in order to prove that the problem is NP-hard.
Given an undirected graph G = (V, E) with |V | = n nodes, we code the structure of the
graph as follows. The sample ˜S will be composed of two subsets, ˜SE and ˜SV of size 2|E| and
|V | respectively. We set l = 2|V | + 1. Each edge {vi, v j} ∈E is encoded by two examples
(¯h, y) in ˜SE. We set the ﬁrst vector to hi = 1, h j = 1, hl = 1 and 0 elsewhere. We set
the second vector to hi+n = 1, h j+n = 1, hl = 1 and 0 elsewhere. We set the label y of
each example in ˜SE to 1. Each example (¯h, y) in ˜SV encodes a node vi ∈V where hi = 1,
hi+n = 1, hl = 1 and 0 elsewhere. We set the label y of each example in ˜SV to 2 (second
class). We now show that there exists a vertex cover U ⊆V with at most q nodes if and only
if there exists a coding matrix M ∈{0, 1}2×l that induces at most q classiﬁcation errors on
the sample ˜S.
(⇒): Let U ⊆V be a vertex cover such that |U| ≤q. We show that there exists a code
which has at most q mistakes on ˜S. Let u ∈{0, 1}|V | be the characteristic function of U, that
is, ui = 1 if vi ∈U and ui = 0 otherwise. Deﬁne the output code matrix to be ¯M1 = (u, u, 1)
and ¯M2 = (¬u, ¬u, 0). Here, ¬u denotes the component-wise logical not operator.
Since U is a cover, for each ¯h ∈˜SE we get
¯M1 · ¯h ≥2
¯M2 · ¯h ≤1
¯M2 · ¯h < ¯M1 · ¯h .
Therefore, for all the examples in ˜SE the predicted label equals the true label and we suffer
no errors on these examples. For each example ¯h ∈˜SV that corresponds to a node v ∈U
¯M1 · ¯h = 3 > 0 = ¯M2 · ¯h .
Therefore, these examples are misclassiﬁed (Recall that the label of each example
in ˜SV is 2). Analogously, for each example in ˜SV which corresponds to v ̸∈U we
K. CRAMMER AND Y. SINGER
¯M1 · ¯h = 1 < 2 = ¯M2 · ¯h ,
and these examples are correctly classiﬁed. We thus have shown that the total number of
mistakes according to M is |U| ≤q.
(⇐): Let M be a code which achieves at most q mistakes on ˜S. We construct a subset
U ⊆V as follows. We scan ˜S and add to U all vertices vi corresponding to misclassiﬁed
examples from ˜SV . Similarly, for each misclassiﬁed example from ˜SE corresponding to an
edge {vi, v j}, we pick either vi or v j at random and add it to U. Since we have at most q
misclassiﬁed examples in ˜S the size of U is at most q. We claim that the set U is a vertex
cover of the graph G. Assume by contradiction that there is an edge {vi, v j} for which neither
vi nor v j belong to the set U. Therefore, by construction, the examples corresponding to
the vertices vi and v j are classiﬁed correctly and we get,
M1,i + M1,i+n + M1,l < M2,i + M2,i+n + M2,l
M1, j + M1, j+n + M1,l < M2, j + M2, j+n + M2,l
Summing the above equations yields that,
M1,i + M1, j + M1,i+n + M1, j+n + 2M1,l
< M2,i + M2, j + M2,i+n + M2, j+n + 2M2,l.
In addition, the two examples corresponding to the edge {vi, v j} are classiﬁed correctly,
implying that
M1,i + M1, j + M1,l > M2,i + M2, j + M2,l
M1,i+n + M1, j+n + M1,l > M2,i+n + M2, j+n + M2,l
which again by summing the above equations yields,
M1,i + M1, j + M1,i+n + M1, j+n + 2M1,l
> M2,i + M2, j + M2,i+n + M2, j+n + 2M2,l.
Comparing Eqs. (7) and (8) we get a contradiction.
Continuous codes
The intractability results of previous sections motivate a relaxation of output codes. In this
section we describe a natural relaxation where both the classiﬁers’ output and the code
matrix are over the reals.
As before, the classiﬁer H(x) is constructed from a code matrix M and a set of binary
classiﬁers ¯h(x). The matrix M is of size k × l over R where each row of M corresponds to
LEARNABILITY AND DESIGN OF OUTPUT CODES
a class y ∈Y. Analogously, each binary classiﬁer ht(x) ∈H is a mapping ht(x) : X →R.
A column t of M deﬁnes a partition of Y into two disjoint sets. The sign of each element
of the tth column is interpreted as the set (+1 or −1) to which the class r belongs and the
magnitude |Mr,t| is interpreted as the conﬁdence in the associated partition. Similarly, we
interpret the sign of ht(x) as the prediction of the set (+1 or −1) to which the label of the
instance x belongs and the magnitude |ht(x)| as the conﬁdence of this prediction. Given
an instance x, the classiﬁer H(x) predicts the label y which maximizes the conﬁdence
function K(¯h(x), ¯Mr), H(x) = arg maxr∈Y{K(¯h(x), ¯Mr)}. Since the code is over the reals,
we can assume here without loss of generality that exactly one class attains the maximum
value according to the function K. We will concentrate on the problem of ﬁnding a good
continuous code given a set of binary classiﬁers ¯h.
The approach we will take is to cast the code design problem as constrained optimization
problem. Borrowing the idea of soft margin we replace the discrete
0–1 multiclass loss with the linear bound
K(¯h(xi), ¯Mr) + 1 −δyi,r
¯h(xi), ¯Myi
where δi, j equals 1 if i = j and 0 otherwise. This formulation is also motivated by the
generalization analysis of Allewein, Schapire, and Singer . The analysis they give is
based on the margin of examples where the margin is closely related to the deﬁnition of the
loss as given by Eq. (9).
Put another way, the correct label should have a conﬁdence value which is larger by at
least one than any of the conﬁdences for the rest of the labels. Otherwise, we suffer loss
which is linearly proportional to the difference between the conﬁdence of the correct label
and the maximum among the conﬁdences of the other labels. The bound on the empirical
loss is then,
ϵS(M, ¯h) = 1
[[H(xi) ̸= yi]]
K(¯h(xi), ¯Mr) + 1 −δyi,r
¯h(xi), ¯Myi
We say that a sample S is classiﬁed correctly using a set of binary classiﬁers ¯h if there exists
a matrix M such that the above loss is equal to zero,
K(¯h(xi), ¯Mr) + 1 −δyi,r
¯h(xi), ¯Myi
bi,r = 1 −δyi,r.
Thus, a matrix M that satisﬁes Eq. (10) would also satisfy the following constraints,
¯h(xi), ¯Myi
−K(¯h(xi), ¯Mr) ≥bi,r.
K. CRAMMER AND Y. SINGER
Motivated by Vapnik , Allwein, Schapire, and Singer and the results of
Section 7 we seek a matrix M with a small norm which satisﬁes Eq. (12). Throughout the paper we deﬁne the norm of a matrix M to be the norm of the concatenation
of the rows of M, ∥( ¯M1, . . . , ¯Mk)∥. Thus, when the entire sample S can be labeled correctly, the problem of ﬁnding a good matrix M can be stated as the following optimization
subject to:
¯h(xi), ¯Myi
−K(¯h(xi), ¯Mr) ≥bi,r
Here p is an integer. Note that m of the constraints for r = yi are automatically satisﬁed.
This is changed in the following derivation for the non-separable case. In the general case a
matrix M which classiﬁes all the examples correctly might not exist. We therefore introduce
slack variables ξi ≥0 and modify Eq. (10) to be,
K(¯h(xi), ¯Mr) + 1 −δyi,r
¯h(xi), ¯Myi
The corresponding optimization problem is,
subject to:
¯h(xi), ¯Myi
−K(¯h(xi), ¯Mr) ≥bi,r −ξi
for some constant β ≥0. This is an optimization problem with “soft” constraints. Analogously, we can deﬁne an optimization problem with “hard” constraints,
subject to:
¯h(xi), ¯Myi
−K(¯h(xi), ¯Mr) ≥bi,r −ξi
∥M∥p ≤A, for some A > 0
The relation between the “hard” and “soft” constraints and their formal properties is beyond the scope of this paper. For further discussion on the relation between the problems
see Vapnik .
Design of continuous codes using linear programming
We now further develop Eq. (15) for the cases p = 1, 2, ∞. We deal ﬁrst with the cases
p = 1 and p = ∞which result in linear programs. For the simplicity of presentation we
will assume that K(¯u, ¯v) = ¯u · ¯v .
For the case p = 1 the objective function of Eq. (15) becomes β 
i,r |Mi,r| + 
introduce a set of auxiliary variables αi,r = |Mi,r| to get a standard linear programming
LEARNABILITY AND DESIGN OF OUTPUT CODES
subject to:
ξi + ¯h(xi) · ¯Myi −¯h(xi) · ¯Mr ≥bi,r
αr,t ≥±Mr,t
To obtain its dual program (see also Appendix A) we deﬁne one variable for each constraint
of the primal problem. We use ηi,r for the ﬁrst set of constraints, and γ ±
t,r for the second set.
The dual program is,
subject to:
δyi,r −ηi,r
The case of p = ∞is similar. The objective function of Eq. (15) becomes β maxi,r
|Mi,r| + 
i ξi. We introduce a single new variable α = maxi,r |Mi,r| to obtain the primal
subject to:
ξi + ¯h(xi) · ¯Myi −¯h(xi) · ¯Mr ≥bi,r
Following the technique for p = 1, we get that the dual program is,
subject to:
δyi,r −ηi,r
Both programs (p = 1 and 0 = ∞) can be now solved using standard linear program
K. CRAMMER AND Y. SINGER
Design of continuous codes using quadric programming
We now discuss in detail Eq. (15) for the case p = 2. For convenience we use the square of
the norm of the matrix (instead the norm itself). Therefore, the primal program becomes,
subject to:
ξi + ¯h(xi) · ¯Myi −¯h(xi) · ¯Mr ≥bi,r
We solve the optimization problem by ﬁnding a saddle point of the Lagrangian:
L(M, ξ, η) = 1
¯h(xi) · ¯Mr −¯h(xi) · ¯Myi −ξi + bi,r
subject to:
The saddle point we are seeking is a minimum for the primal variables (M, ξ), and the
maximum for the dual ones (η). To ﬁnd the minimum over the primal variables we require,
ηi,r = 0 ⇒
Similarly, for ¯Mr we require:
ηi,r ¯h(xi) −
+β ¯Mr = 0
⇒¯Mr = β−1
δyi,r −ηi,r
Equation (19) implies that when the optimum of the objective function is achieved, each
row of the matrix M is a linear combination of ¯h(xi). We say that an example i is a support
pattern for class r if the coefﬁcient (δyi,r −ηi,r) of ¯h(xi) in Eq. (19) is not zero. There are
two settings for which an example i can be a support pattern for class r. The ﬁrst case is
when the label yi of an example is equal to r, then the ith example is a support pattern if
ηi,r < 1. The second case is when the label yi of the example is different from r, then the
ith pattern is a support pattern if ηi,r > 0.
Loosely speaking, since for all i,r, ηi,r ≥0 and 
r ηi,r = 1, the variable ηi,r can be
viewed as a distribution over the labels for each example. An example i affects the solution
for M (Eq. (19)) if and only if ¯ηi in not a point distribution concentrating on the correct
label yi. Thus, only the questionable patterns contribute to the learning process.
LEARNABILITY AND DESIGN OF OUTPUT CODES
We develop the Lagrangian using only the dual variables. Substituting Eq. (18) to Eq. (16)
we obtain,
ηi,r ¯h(xi) · ¯Mr −
ηi,r ¯h(xi) · ¯Myi −
ηi,rbi,r + 1
ηi,r ¯h(xi) · ¯Mr −
ηi,r ¯h(xi) · ¯Myi +
We substitute ¯Mr using Eq. (19) and get,
ηi,r ¯h(xi) · ¯Mr
ηi,r ¯h(xi) · β−1 
δy j,r −η j,r
¯h(xi) · ¯h(x j)
δy j,r −η j,r
ηi,r ¯h(xi) · ¯Myi
ηi,r ¯h(xi) · β−1 
δy j,yi −η j,yi
¯h(xi) · ¯h(x j)
δy j,yi −η j,yi
¯h(xi) · ¯h(x j)
δy j,yi −η j,yi
¯h(xi) · ¯h(x j)
δy j,yi −η j,yi
¯h(xi) · ¯h(x j)
δy j,r −η j,r
δyi,r −ηi,r
δy j,r −η j,r
¯h(xi) · ¯h(x j)
δyi,r −ηi,r
δy j,r −η j,r
K. CRAMMER AND Y. SINGER
Taking the difference S1 −S2 while using Eqs. (21) and (22) we get:
S1 −S2 = β−1 
¯h(xi) · ¯h(x j)
δy j,r −η j,r
¯h(xi) · ¯h(x j)
δy j,r −η j,r
¯h(xi) · ¯h(x j)
δyi,r −ηi,r
δy j,r −η j,r
Finally, plugging the values for S1, S2 and S3 from Eqs. (23) and (24) in Eq. (20) we get,
Q(η) = −β−1 
¯h(xi) · ¯h(x j)
δyi,r −ηi,r
δy j,r −η j,r
¯h(xi) · ¯h(x j)
δyi,r −ηi,r
δy j,r −η j,r
¯h(xi) · ¯h(x j)
δyi,r −ηi,r
δy j,r −η j,r
Let ¯1i be the vector with all components zero, except for the ith component which is
equal to one, and let ¯1 be the vector whose components are all one. Using this notation we
can rewrite the dual program in vector form as
[¯h(xi) · ¯h(x j)]
¯1yi −¯ηi
¯1y j −¯η j
subject to:
and ¯ηi · ¯1 = 1
It is easy to verify that Q(η) is strictly convex in η. Since the constraints are linear the
above problem has a single optimal solution and therefore QP methods can be used to
solve it. In Section 6 we describe a memory efﬁcient algorithm for solving this special QP
To simplify the equations we denote by ¯τi = ¯1yi −¯ηi the difference between the correct point distribution and the distribution obtained by the optimization problem, Eq. (19)
¯Mr = β−1 
¯h(xi)τi,r
Since we look for the value of the variables which maximize the objective function Q (and
not the optimum of Q itself), we can omit constants and write the dual problem given by
LEARNABILITY AND DESIGN OF OUTPUT CODES
Eq. (25) as,
[¯h(xi) · ¯h(x j)](¯τi · ¯τ j) −
subject to:
and ¯τi · ¯1 = 0
Finally, the classiﬁer H(x) can be written in terms of the variable τ as,
H(x) = arg max
r {¯h(x) · ¯Mr}
¯h(xi)τi,r
τi,r[¯h(x) · ¯h(xi)]
τi,r[¯h(x) · ¯h(xi)]
As in Support Vector Machines, the dual program and the classiﬁcation algorithm depend
only on inner products of the form ¯h(xi) · ¯h(x). Therefore, we can perform the calculations
in some high dimensional inner-product space Z using a transformation ¯φ : Rl →Z. We
thus replace the inner-product in Eq. (27) and in Eq. (28) with a general inner-product kernel
K that satisﬁes Mercer conditions . The general dual program is therefore,
K(¯h(xi), ¯h(x j)) (¯τi · ¯τ j) −
subject to:
and ¯τi · ¯1 = 0
and the classiﬁcation rule H(x) becomes,
H(x) = arg max
τi,r K(¯h(x), ¯h(xi))
ThegeneralframeworkfordesigningoutputcodesusingtheQPprogramdescribedabove,
also provides, as a special case, a new algorithm for building multiclass Support Vector
Machines. Assume that the instance space is the vector space Rn and deﬁne ¯h(¯x)
= ¯x (thus
l = n), then the primal program in Eq. (16) becomes
subject to:
ξi + ¯xi · ¯Myi −¯xi · ¯Mr ≥bi,r
Note that for k = 2 Eq. (31) reduces to the primal program of SVM, if we take ¯M1 = −¯M2
and C = β−1. We would also like to note that this special case is reminiscent of the multiclass
K. CRAMMER AND Y. SINGER
Summary of the sizes of the optimization problems for different norms.
m + kl + 1
0-Constraints
Constraints
0-Constraints
Constraints
m + kl + 1
approach for SVM’s suggested by Weston and Watkins . Their approach compared the
conﬁdence K(x, ¯My) to the conﬁdences of all other labels K(x, ¯Mr) and had m(k−1) slack
variables in the primal problem. In contrast, in our framework the conﬁdence K(x, ¯My) is
compared to maxr̸=y K(x, ¯Mr) and has only m slack variables in the primal program.
In Table 1 we summarize the properties of the programs discussed above. As shown in
the table, the advantage of using l2 in the objective function is that the number of variables
in the dual problem is only a function of k and m and does not depend on the number of
columnsl in M. The number of columns in M only affects the evaluation of the inner-product
The formalism given by Eq. (15) can also be used to construct the code matrix incrementally (column by column). We now outline the incremental (inductive) approach. However,
we would like to note that this method only applies when K(¯v, ¯u) = ¯v · ¯u. In the ﬁrst step
of the incremental algorithm, we are given a single binary classiﬁer h1(x) and we need to
construct the ﬁrst column of M. We rewrite Eq. (15) in a scalar form and obtain,
subject to:
h1(xi)Myi −h1(xi)Mr ≥bi,r −ξi.
Here, β ≥0 is a given constant and bi,r = 1 −δyi,r, as before. For the rest of the columns
we assume inductively that h1(x), . . . , hl(x) have been provided and the ﬁrst l columns of
the matrix M have been found. In addition, we are provided with a new binary classiﬁer
= h′(x) for the next column. We need to ﬁnd a new column of M (indexed l + 1).
We substitute the new classiﬁer and the matrix in Eq. (14) and get,
¯h(xi) · ¯Mr + h′(xi)M′
r + 1 −δyi,r
¯h(xi) · ¯Myi + h′(xi)M′
The constraints appearing in Eq. (15) now become
¯h(xi) · ¯Myi + h′(xi)M′
yi −¯h(xi) · ¯Mr −h′(xi)M′
r ≥1 −δyi,r −ξi
yi −h′(xi)M′
¯h(xi) · ¯Myi −¯h(xi) · ¯Mr
+ 1 −δyi,r −ξi.
LEARNABILITY AND DESIGN OF OUTPUT CODES
We now redeﬁne bi,r to be −[¯h(xi) · ¯Myi −¯h(xi) · ¯Mr] + 1 −δyi,r. It is straightforward to
verify that this deﬁnition of bi,r results in an equation of the same form of Eq. (32). We
can thus apply the same algorithms designed for the “batch” case. In the case of l1 and l∞,
this construction decomposes a single problem into l sub-problems with fewer variables
and constraints. However, for l2 the size of the program remains the same while we lose the
ability to use kernels. We therefore concentrate on the batch case for which we need to ﬁnd
the entire matrix at once.
An efﬁcient algorithm for the QP problem
The quadratic program presented in Eq. (29) can be solved using standard QP techniques.
As shown in Table 1 the dual program depends on mk variables and has km +m constraints
all together. Converting the dual program in Eq. (29) to a standard QP form requires storing
and manipulating a matrix with (mk)2 elements. Clearly, this would prohibit applications of
non-trivial size. We now introduce a memory efﬁcient algorithm for solving the quadratic
optimization problem given by Eq. (29).
First, note that the constraints in Eq. (29) can be divided into m disjoint subsets {¯τi ≤¯1yi,
¯τi · ¯1 = 0}m
i=1. The algorithm we describe works in rounds. On each round it picks a single
set {¯τi ≤¯1yi, ¯τi · ¯1 = 0}, and modiﬁes ¯τi so as to optimize the reduced optimization problem.
The algorithm is reminiscent of Platt’s SMO algorithm . Note, however, that
our algorithm optimizes one example on each round, and not two as in SMO.
Let us ﬁx an example index p and write the objective function only in terms of the
variables ¯τp. For brevity, let Ki, j = K(¯h(xi), ¯h(x j)). We isolate ¯τp in Q.
Ki, j(¯τi · ¯τ j) −
2β−1K p,p(¯τp · ¯τp) −β−1 
Ki,p(¯τp · ¯τi) −1
Ki, j(¯τi · ¯τ j)
−¯τp · ¯bp −
2β−1K p,p(¯τp · ¯τp) −¯τp ·
¯bp + β−1 
Ki, j(¯τi · ¯τ j) −
2 Ap(¯τp · ¯τp) −¯Bp · ¯τp + C p
Ap = β−1K p,p > 0
¯Bp = ¯bp + β−1 
K. CRAMMER AND Y. SINGER
Ki, j(¯τi · ¯τ j) −
For brevity, we will omit the index p and drop constants (that do not affect the solution).
The reduced optimization has k variables and k + 1 constraints,
Q(¯τ) = −1
2 A(¯τ · ¯τ) −¯B · ¯τ
subject to:
¯τ · ¯1 = 0
Although this program can be solved using a standard QP technique, it still requires large
amount of memory when k is large, and a straightforward solution is also time consuming.
Furthermore, this problem constitutes the core and inner-loop of the algorithm. We therefore
further develop the algorithm and describe a more efﬁcient method for solving Eq. (37).
We write Q(¯τ) in Eq. (37) using a completion to quadratic form,
Q(¯τ) = −1
2 A(¯τ · ¯τ) −¯B · ¯τ
Since A > 0 the program from Eq. (37) becomes,
Q(¯ν) = ∥¯ν∥2
subject to:
¯ν ≤¯D and ¯ν · ¯1 = ¯D · ¯1 −1
In Section 6.1 we discuss an analytic solution to Eq. (38) and in Section 6.2 we describe a
time efﬁcient algorithm for computing the analytic solution.
An analytic solution
While the algorithm we describe in this section is simple to implement and efﬁcient, its
derivation is quite complex. Before describing the analytic solution to Eq. (38), we would
like to give some intuition on our method. Let us ﬁx some vector ¯D and denote µ = ¯ν · ¯1.
First note that ¯ν = ¯D is not a feasible point since the constraint ¯ν · ¯1 = ¯D · ¯1 −1 is not
satisﬁed. Hence for any feasible point some of the constraints ¯ν ≤¯D are not tight. Second,
note that the differences between the bounds Dr and the variables νr sum to one. Let us
induce a uniform distribution over the components of ¯ν. Then, the variance of ¯ν is
σ 2 = E[ν2] −[Eν]2 = 1
k ∥¯ν∥2 −1
LEARNABILITY AND DESIGN OF OUTPUT CODES
An illustration of two feasible points for the reduced optimization problem with ¯D = (1.0, 0.2, 0.6,
0.8, 0.6). The x-axis is the index of the point, and the y-axis denotes the values ¯ν. The right plot has a smaller
variance hence it achieves a better value for Q.
Since the expectation µ is constrained to a given value, the optimal solution is the vector
achieving the smallest variance. That is, the components of of ¯ν should attain similar values,
as much as possible, under the inequality constraints ¯ν ≤¯D. In ﬁgure 1 we illustrate this
motivation. We picked ¯D = (1.0, 0.2, 0.6, 0.8, 0.6) and show plots for two different feasible
values for ¯ν. The x-axis is the index r of the point and the y-axis designates the values of
the components of ¯ν. The norm of ¯ν on the plot on the right hand side plot is smaller than
the norm of the plot on the left hand side. The right hand side plot is the optimal solution
for ¯ν. The sum of the lengths of the arrows in both plots is ¯D · ¯1 −¯ν · ¯1. Since both sets
of points are feasible, they satisfy the constraint ¯ν · ¯1 = ¯D · ¯1 −1. Thus, the sum of the
lengths of the “arrows” in both plots is one. We exploit this observation in the algorithm
we describe in the sequel.
We therefore seek a feasible vector ¯ν whose most of its components are equal to some
threshold θ. Given θ we deﬁne a vector ¯ν whose its rth component equal to the minimum
between θ and Dr, hence the inequality constraints are satisﬁed. We deﬁne
We denote by
F(θ) = ¯νθ · ¯1 =
Using F, the equality constraint from Eq. (38) becomes F(θ) = ¯D · ¯1 −1.
Let us assume without loss of generality that the components of the vector ¯ν are given in
a descending order, D1 ≥D2 ≥. . . Dk (this can be done in k log k time). Let Dk+1 = −∞
and D0 = ∞. To prove the main theorem of this section we need the following lemma.
K. CRAMMER AND Y. SINGER
F(θ) is piecewise linear with a slope r in each range (Dr+1, Dr) for r =
0, . . . , k.
Let us develop F(θ).
{[[θ > Dr]]Dr + [[θ ≤Dr]]θ}
[[θ > Dr]]Dr + θ
Note that if θ > Dr then θ > Du for all u ≥r. Also, the equality k
r′=1[[θ ≤Dr′]] = r holds
for each θ in the range Dr+1 < θ < Dr. Thus, for Dr+1 < θ < Dr (r = 0 · · · k), the function
F(θ) has the form,
This completes the proof.
Corollary 1.
There exists a unique θ0 ≤D1 such that F(θ0) = ¯D · ¯1 −1.
From Lemma 1 (Eq. (41)) we conclude that F(θ) is strictly increasing and continuous in the range θ ≤D1. Therefore, F(θ) has an inverse in that range, using the theorem that
everystrictlyincreasingandcontinuousfunctionhasaninverse.Since F(θ) = kθ forθ ≤Dk
then F(θ) →−∞as θ →−∞. Hence, the range of F for the interval (−∞, D1] is the interval (−∞, ¯D · ¯1] which clearly contains ¯D · ¯1−1. Thus θ0
= F−1( ¯D · ¯1−1) ∈[−∞, D1]
as needed. Uniqueness of θ0 follows the fact that the function F is a one-to-one mapping
onto (−∞, ¯D · ¯1].
We now can prove the main theorem of this section.
Theorem 3.
Let θ0 be the unique solution of F(θ) = ¯D · ¯1 −1. Then ¯νθ0 is the optimum
value of the optimization problem stated in Eq. (38).
The theorem tells us that the optimum value of Eq. (38) is of the form deﬁned by Eq. (40)
and that there is exactly one value of θ for which the equality constraint F(θ) = ¯νθ · ¯1 =
¯D · ¯1 −1 holds. A plot of F(θ) and the solution for θ from ﬁgure 1 are shown in ﬁgure 2.
Corollary 1 implies that a solution exists and is unique. Note also that from deﬁnition of θ0 we have that the vector ¯νθ0 is a feasible point of Eq. (38). We now prove that ¯νθ0
is the optimum of Eq. (38) by showing that ∥¯ν∥2 > ∥¯νθ0∥2 for all feasible points ¯ν ̸= ¯νθ.
LEARNABILITY AND DESIGN OF OUTPUT CODES
An illustration of the solution of the QP problem using the inverse of F(θ) for ¯D = (1.0, 0.2,
0.6, 0.8, 0.6). The optimal value is the solution for the equation F(θ) = 2.2 which is 0.5.
Assume, by contradiction, that there is a vector ¯ν such that ∥¯ν∥2 ≤∥¯νθ0∥2. Let ¯ϵ
¯ν −¯νθ ̸= ¯0, and deﬁne I
r = Dr} = {r : θ0 > Dr}. Since both ¯ν and ¯νθ0 satisfy the
equality constraint of Eq. (38), we have,
¯ν · ¯1 = ¯νθ0 · ¯1 ⇒(¯ν −¯νθ0) · ¯1 = 0
⇒¯ϵ · ¯1 =
Since ¯ν is a feasible point we have ¯ν = ¯νθ0 + ¯ϵ ≤¯D. Also, by the deﬁnition of the set I we
have that νθ
r = Dr for all r ∈I. Combining the two properties we get,
for all r ∈I
We start with the simpler case of ϵr = 0 for all r ∈I. In this case, ¯ν differs from ¯νθ0 only
on a subset of the coordinates r /∈I. However, for these coordinates the components of ¯νθ0
are equal to θ0, thus we obtain a zero variance from the constant vector whose components
K. CRAMMER AND Y. SINGER
are all θ0. Therefore, no other feasible vector can achieve a better variance. Formally, since
ϵr = 0 for all r ∈I, then the terms for r ∈I cancel each other,
∥¯ν∥2 −∥¯νθ0∥2 =
From the deﬁnition of ¯νθ0 in Eq. (40) we get that νθ0
r = θ0 for all r /∈I,
∥¯ν∥2 −∥¯νθ0∥2 =
(θ0 + ϵr)2 −
We use now the assumption that ϵr = 0 for all r ∈I and the equality k
r=1 ϵr = 0 (Eq. (42))
to obtain,
∥¯ν∥2 −∥¯νθ0∥2 = 2θ0
and we get a contradiction since ¯ϵ ̸= ¯0.
We now turn to prove the complementary case in which 
r∈I ϵr < 0. Since 
r∈I ϵr < 0,
then there exists u ∈I such that ϵu < 0. We use again Eq. (42) and conclude that there exists
also v /∈I such that ϵv > 0. Let us assume without loss of generality that ϵu + ϵv < 0
(The case ϵu + ϵv ≥0 follows analogously by switching the roles of u and v). Deﬁne ¯ν′ as
The vector ¯ν′ satisﬁes the constraints of Eq. (38) since ν′
u = νu + ϵv = Du + ϵu + ϵv < Du
v = νv −ϵv = θ0 + ϵv −ϵv = θ0 ≤Dv. Since ¯ν and ¯ν′ are equal except for their u
and v components we get,
∥¯ν′∥2 −∥¯ν∥2 = [(ν′
v)2] −[(νu)2 + (νv)2]
Substituting the values for ν′
v from the deﬁnition of ¯ν′ we obtain,
∥¯ν′∥2 −∥¯ν∥2 = [(νu + ϵv)2 + (νv −ϵv)2] −[(νu)2 + (νv)2]
v + 2νuϵv + ϵ2
v + 2(νu −νv)ϵv
LEARNABILITY AND DESIGN OF OUTPUT CODES
Using the deﬁnition of ¯ν and ¯νθ0 for νu = νθ0
u + ϵu = Du+ϵu and for νv = νθ0
v + ϵv = θ0+ϵv
we obtain,
∥¯ν′∥2 −∥¯ν∥2 = 2ϵ2
v + 2(Du + ϵu −θ0 −ϵv)ϵv
= 2(Du + ϵu −θ0)ϵv
= 2ϵuϵv + 2(Du −θ0)ϵv
The ﬁrst term of the bottom equation is negative since ϵu < 0 and ϵv > 0. Also u ∈I, hence
θ0 > Du and the second term is also negative. We thus get,
∥¯ν′∥2 −∥¯ν∥2 < 0.
which is a contradiction.
An efﬁcient algorithm for computing the analytic solution
The optimization problem of Eq. (38) can be solved using standard QP methods, and
interior point methods in particular . For these methods the computation
time is (k2). In this section we give an algorithm for solving that optimization problem
in O(k log k) time, by solving the equation F(θ) = ¯D · ¯1 −1.
As before, we assume that the components of the vector ¯ν are given in a descending order,
D1 ≥D2 ≥. . . Dk and we denote Dk+1 = −∞. The algorithm searches for the interval
[Dr+1, Dr) which contains θ0. We now use simple algebraic manipulations to derive the
search scheme for θ0. Since F(D1) = F(θ0) + 1,
θ0 ∈[Dr+1, Dr) ⇔1 > F(D1) −F(Dr)
F(D1) −F(Dr+1) ≥1.
For convenience, we deﬁne the potential function
(r) = 1 −[F(D1) −F(Dr)],
and obtain,
θ0 ∈[Dr+1, Dr) ⇔
(r + 1) ≤0
Also note that,
(r + 1) = {1 −[F(D1) −F(Dr)]} −{1 −[F(D1) −F(Dr+1)]}
= F(Dr) −F(Dr+1).
Recall that the function F(θ) is linear in each interval [Dr+1, Dr) with a slope r (Lemma 1),
F(Dr) −F(Dr+1) = r(Dr −Dr+1)
(r) −r(Dr −Dr+1).
K. CRAMMER AND Y. SINGER
To solve the equation F(θ) = ¯D· ¯1−1, we ﬁrst ﬁnd r such that
(r) > 0 and
(r +1) ≤0,
which implies that θ0 ∈[Dr+1, Dr). Using Eq. (44) and the equation F(D1) = F(θ0) + 1
F(Dr) −F(θ0) =
Using the linearity of F(θ) we obtain,
F(Dr) −F(θ0) = r(Dr −θ0) ⇒r(Dr −θ0) =
The complete algorithm is described in ﬁgure 3. Since it takes O(k log k) time to sort the
vector ¯D and another O(k) time for the loop search, the total run time is O(k log k).
We are ﬁnally ready to give the algorithm for solving learning problem described by
Eq. (29). Since the output code is constructed of the supporting patterns we term our
algorithm SPOC for Support Pattern Output Coding. The SPOC algorithm is described in
ﬁgure 4. We have also developed methods for choosing an example p to modify on each
round and a stopping criterion for the entire optimization algorithm. The complete details
of the algorithms along with the results of experiments we have conducted will appear
elsewhere.
We have performed preliminary experiments with synthetic data in order to check the
actual performance of our algorithm. We tested the special case corresponding to multiclass
SVM by setting ¯h(x) = x. The code matrices we test are of k = 4 rows (classes) and l = 2
columns. We varied the size of the training set size from m = 10 to m = 250. The examples
were generated using the uniform distribution over [−1, 1]×[−1, 1]. The domain [−1, 1]×
[−1, 1] was partitioned into four quarters of equal size: [−1, 0] × [−1, 0], [−1, 0] × ,
The algorithm for ﬁnding the optimal solution of the reduced quadratic program (Eq. (38)).
LEARNABILITY AND DESIGN OF OUTPUT CODES
A skeleton of the algorithm for ﬁnding a classiﬁer based on an output code by solving the quadratic
program deﬁned in Eq. (29).
Run time comparison of two algorithms for code design using quadratic programming: Matlab’s
standard QP package and the proposed algorithm (denoted SPOC). Note that we used a logarithmic scale for the
run-time (y) axis.
K. CRAMMER AND Y. SINGER
 × [−1, 0], and × . Each quarter was associated with a different label. For
each sample size we tested, we ran the algorithm three times, each run used a different
randomly generated training set. We compared the standard quadratic optimization routine
available from Matlab with our algorithm which was also implemented in Matlab. The
average running time results are shown in ﬁgure 5. Note that we used a log-scale for the y
(run-time) axis. The results show that the efﬁcient algorithm can be two orders of magnitude
faster than the standard QP package.
Generalization
In this section we analyze the generalization properties of the algorithm. For simplicity, we
give our analysis using the scalar product kernel K(x, y) = x · y, assuming ¯h(¯x) = ¯x. The
multiclass SVM version described in Eq. (13) now becomes,
subject to:
¯Myi · ¯xi −¯Mr · ¯xi ≥1 −δyi,r
Let M be a matrix of size k × l over R which satisﬁes the constraints of Eq. (47). Each
row of the matrix M corresponds to a class y ∈Y and the induced classiﬁer is given by
H(x) = arg max{ ¯Mr · ¯x}. Analogously, the matrix M induces a binary classiﬁer for each
pair of classes r and s as follows. We say that the label of an instance ¯x is not r iff
¯Mr · ¯x < ¯Ms · ¯x. Let ¯wr,s be the vector ¯Mr −¯Ms. Then, the resulting binary SVM classiﬁer
for distinguishing between classes r and s is
hr,s(¯x) = sign( ¯wr,s · ¯x),
where we interpret a positive prediction as a rejection of the label s.
We view the distance of a point ¯x to the hyperplane ¯wr,s · ¯x = 0 as the conﬁdence in the
classiﬁcation of hr,s(¯x). This distance is given by the equation
d(¯x, ¯wr,s) = | ¯wr,s · ¯x|
We now deﬁne the margin γr,s of the classiﬁer hr,s(¯x) to be the minimum conﬁdence
achieved on any of the instances labeled r or s. That is,
(¯x,y)∈S,y∈{r,s}{d(¯x, ¯wr,s)}
(¯x,y)∈S,y∈{r,s}
| ¯wr,s · ¯x|
Since the matrix M is a feasible solution for Eq. (47) then ¯Mr · ¯x −¯Ms · ¯x ≥1 holds for
any instance labeled r, and similarly ¯Ms · ¯x −¯Mr · ¯x ≥1 holds for any instance labeled s.
LEARNABILITY AND DESIGN OF OUTPUT CODES
Hence, we obtain that
∀i s.t. yi ∈{r, s}
| ¯wr,s · ¯x| ≥1,
which leads to the following lower bound on the margin,
We show in the sequel that minimization of the quantity D = 
γ 2r,s results in a small
generalization error. Using Eq. (49) we get that
∥¯Mr −¯Ms∥2
Since it is easier to minimize the above bound on D, we rewrite the problem as follows,
∥¯Mr −¯Ms∥2
subject to:
¯Myi · ¯xi −¯Mr · ¯xi ≥1 −δyi,r
We now show that this problem is equivalent to the optimization problem of Eq. (47), up to
additive and multiplicative constants.
To solve the primal problem of Eq. (47) we use the same technique described in Section
5.2.Formally,wesolvetheoptimizationproblembyﬁndingasaddlepointoftheLagrangian:
L(M, η) = 1
∥¯Mr −¯Ms∥2
ηi,r[¯xi · ¯Mr −¯xi · ¯Myi + bi,r]
subject to:
The saddle point we are seeking is a minimum for the primal variables (M), and the
maximum for the dual variables (η). To ﬁnd the minimum for the primal variables we
ηi,r ¯xi −
+ (k −1) ¯Mr −
ηi,q −ηi,r
Note that the k equations given in Eq. (53) are linearly dependent since by summing the
equation over all possible values for r we get,
K. CRAMMER AND Y. SINGER
Therefore, the above equations contain at least one degree of freedom. To prove that this is
the only degree of freedom we use the claim that for any two matrices A and B with the
same dimensions we have that,
rk(A −B) ≥rk(A) −rk(B).
To use the claim we deﬁne two matrices of size k × k. We set the matrix A to be a diagonal
matrix with values k on the diagonal, and the matrix B to be a matrix which all elements
are equal +1. Note that rk(A) = k and rk(B) = 1. Since we can write the above linear
equations (Eq. (53)) as a difference of A and B we can apply Eq. (54) and get that the rank
of the above system is at least k −1, and thus there is exactly one degree of freedom.
Let us represent the degree of freedom by setting k
r=1 ¯Mr = ¯c and obtain,
∥¯Mr∥2 + 2
Developing the objective function of Eq. (51) we get,
∥¯Mr −¯Ms∥2 = 1
( ¯Mr −¯Ms) · ( ¯Mr −¯Ms)
Finally, substituting Eq. (55) into Eq. (56) we get,
∥¯Mr −¯Ms∥2 = 1
2 (¯c · ¯c)
Note that the resulting equation is a monotonically increasing linear transformation of the
objective function of Eq. (47). By setting ¯c = 0 in Eq. (57) we maximize the value of the
LEARNABILITY AND DESIGN OF OUTPUT CODES
right hand side of the equation and obtain the following upper bound on the margin
To conclude, we have shown that solving the optimization problems given in Eqs. (47)
and (13) results in the minimization of the quantity D which, as we now show, is directly
related to the margin of the classiﬁer.
To relate the margin to the generalization error of the multiclass SVM we use the approach
proposed by Platt, Cristianini, and Shawe-Taylor for reducing multiclass problems
to multiple binary problems. Their method is also composed of two stages. In the training
stage the set of all ( k
2) binary SVM classiﬁers is constructed, where each classiﬁer is trained
to distinguish between a pair of distinct labels. In the classiﬁcation stage the algorithm
maintains a list of all possible labels for a given test instance (initialized to the list of all
labels). The classiﬁcation algorithm runs in rounds: on each round it picks two labels from
the list above and applies the appropriate binary classiﬁer to distinguish between the two
labels. The label that was rejected by the binary classiﬁer is removed from the list. After
k −1 such rounds exactly one label remains in the list. This label is the prediction of the
multiclass classiﬁer. Platt, Cristianini, and Shawe-Taylor presented the the above
classiﬁcation scheme by a rooted binary directed acyclic graph (DAG) which they named
DDAG for decision DAG. The following theorem gives a bound on the generalization error
for DDAGs.
Theorem 4 .
Suppose we are able to classify
a random m sample S of labeled examples using a SVM DDAG on k classes containing
2) decision nodes (and k leaves) with margin γr,s at node {r, s}, then we can bound
the generalization error with probability greater than 1 −δ to be less than
D′ log(4em)log(4m) + log 2(2m)K
where D′ = 
γ 2r,s ,and R istheradiusofaballcontainingthesupportofthedistribution.
Plugging the binary classiﬁers induced by a matrix M as given by Eq. (48) results a stepwise
method for calculating the maximum among { ¯Mr · ¯x}, and thus the prediction of the classiﬁer
H(x) = arg max{ ¯Mr · ¯x}. We now can use Theorem 4 from by combining it with the bound of Eq. (58).
Corollary 2.
Suppose we are able to classify a random m sample S of labeled examples
using a multiclass SVM with a matrix M, then we can bound the generalization error with
probability greater than 1 −δ to be less than
k∥M∥2 log(4em)log(4m) + log 2(2m)K
where R is the radius of a ball containing the support of the distribution.
K. CRAMMER AND Y. SINGER
We use a second theorem from to bound the
generalization error for each possible label individually. Following Platt, Cristianini, and
Shawe-Taylor we deﬁne the error in identifying class r as
ϵr(M) = Pr {(¯x, y) : [y = r ∧H(¯x) ̸= r] ∨[y ̸= r ∧H(¯x) = r]}
We use the same technique as above to to get a bound similar to Eq. (50). Based on Theorem 2
from we get the following corollary.
Corollary 3.
Suppose we are able to correctly distinguish some class r from the other
classes in a random sample S of m labeled examples using a multiclass SVM with a matrix
M, then with a probability greater than 1 −δ we have that,
ϵr(M) ≤130R2
(k∥¯Mr∥2 + ∥M∥2) log(4em)log(4m) + log 2(2m)k−1
where R is the radius of a ball containing the support of the distribution.
Conclusions and future research
In this paper we investigated the problem of designing output codes for solving multiclass
problems. We ﬁrst discussed discrete codes and showed that while the problem is intractable
in general we can ﬁnd the ﬁrst column of a code matrix in polynomial time. The question
whether the algorithm can be generalized to l ≥2 columns with running time of O(2l) or
less remains open. Another closely related question is whether we can ﬁnd efﬁciently the
next column given previous columns. Also left open for future research is further usage of
the algorithm for ﬁnding the ﬁrst column as a subroutine in constructing codes based on
trees or directed acyclic graphs , and as a tool for
incremental (column by column) construction of output codes.
Motivated by the intractability results for discrete codes we introduced the notion of
continuous output codes. We described three optimization problems for ﬁnding good continuous codes for a given a set of binary classiﬁers. We have discussed in detail an efﬁcient
algorithm for one of the three problems which is based on quadratic programming. As a
special case, our framework also provides a new efﬁcient algorithm for multiclass Support
Vector Machines. The importance of this efﬁcient algorithm might prove to be crucial in
large classiﬁcation problems with many classes such as Kanji character recognition. We
also devised efﬁcient implementation of the algorithm. Finally, an important question which
we have tackled barely in this paper is the problem of interleaving the code design problem
with the learning of binary classiﬁers. A viable direction in this domain is combining our
algorithm for continuous codes with the support vector machine algorithm.
LEARNABILITY AND DESIGN OF OUTPUT CODES
Appendix A:
Linear programming
Using the notation of Chvatal , given the linear program:
subject to:
ai jx j ≤bi (i ∈I)
ai jx j = bi (i ∈E)
x j ≥0 ( j ∈R) (0-Constraints)
( j ∈F) (Unconstrained variables),
its dual program is:
subject to:
ai j yi ≥c j ( j ∈R)
ai j yi = c j ( j ∈F)
yi ≥0 ( j ∈I) (0-Constraints)
( j ∈E) (Unconstrained variables)
Appendix B:
Description
Matrix code
Sample size
No. of classes (No. of rows in M)
No. of hypotheses (No. of columns in M)
Index of an example
Index of a class
Correct label (class)
Index of an hypothesis
Slack variables in optimization problem
K. CRAMMER AND Y. SINGER
Description
Dual variables in quadric problem
τi,r = δyi ,r −ηi,r
Coefﬁcient in reduced optimization problem
Coefﬁcient in reduced optimization problem
Coefﬁcient in reduced optimization problem
Acknowledgment
We would like to thank Rob Schapire for numerous helpful discussions, to Vladimir Vapnik
for his encouragement and support of this line of research, and to Nir Friedman and Ran
Bachrach for useful comments and suggestions. Thanks also to the anonymous referees for
their constructive comments.