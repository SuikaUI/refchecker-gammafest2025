Machine Learning, 47, 201â€“233, 2002
câƒ2002 Kluwer Academic Publishers. Manufactured in The Netherlands.
On the Learnability and Design of Output
Codes for Multiclass Problems
KOBY CRAMMERâˆ—
 
YORAM SINGERâ€ 
 
School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel
Editor: Jyrki Kivinen
Output coding is a general framework for solving multiclass categorization problems. Previous research on output codes has focused on building multiclass machines given predeï¬ned output codes. In this paper
we discuss for the ï¬rst time the problem of designing output codes for multiclass problems. For the design problem
of discrete codes, which have been used extensively in previous works, we present mostly negative results. We
then introduce the notion of continuous codes and cast the design problem of continuous codes as a constrained
optimization problem. We describe three optimization problems corresponding to three different norms of the code
matrix. Interestingly, for the l2 norm our formalism results in a quadratic program whose dual does not depend on
the length of the code. A special case of our formalism provides a multiclass scheme for building support vector
machines which can be solved efï¬ciently. We give a time and space efï¬cient algorithm for solving the quadratic
program. We describe preliminary experiments with synthetic data show that our algorithm is often two orders of
magnitude faster than standard quadratic programming packages. We conclude with the generalization properties
of the algorithm.
multiclass categorization, output coding, SVM
Introduction
Many applied machine learning problems require assigning labels to instances where the
labels are drawn from a ï¬nite set of labels. This problem is often referred to as multiclass
categorization or classiï¬cation. Examples for machine learning applications that include a
multiclass categorization component include optical character recognition, text classiï¬cation, phoneme classiï¬cation for speech synthesis, medical analysis, and more. Some of the
well known binary classiï¬cation learning algorithms can be extended to handle multiclass
problems . A general approach is to reduce a multiclass problem to a set of multiple
binary classiï¬cation problems.
Dietterich and Bakiri described a general approach based on error-correcting
codes which they termed error-correcting output coding (ECOC), or in short output coding.
Output coding for multiclass problems is composed of two stages. In the training stage
we need to construct multiple (supposedly) independent binary classiï¬ers each of which is
âˆ— 
â€  
K. CRAMMER AND Y. SINGER
based on a different partition of the set of the labels into two disjoint sets. In the second
stage, the classiï¬cation part, the predictions of the binary classiï¬ers are combined to extend
a prediction on the original label of a test instance. Experimental work has shown that output
coding can often greatly improve over standard reductions to binary problems . The
performance of output coding was also analyzed in statistics and learning theoretic contexts
 .
Most of the previous work on output coding has concentrated on the problem of solving
multiclass problems using predeï¬ned output codes, independently of the speciï¬c application and the class of hypotheses used to construct the binary classiï¬ers. Therefore, by
predeï¬ning the output code we ignore the complexity of the induced binary problems. The
output codes used in experiments were typically conï¬ned to a speciï¬c family of codes.
Several families of codes have been suggested and tested so far, such as, comparing each
class against the rest, comparing all pairs of classes , random codes , exhaustive codes , and linear error correcting codes . A
few heuristics attempting to modify the code so as to improve the multiclass prediction accuracy were suggested . However, they did not yield signiï¬cant
improvements and, furthermore, they lack any formal justiï¬cation.
Inthispaperweconcentrateontheproblemofdesigningagoodcodeforagivenmulticlass
problem. In Section 3 we study the problem of ï¬nding the ï¬rst column of a discrete code
matrix. Given a binary classiï¬er, we show that ï¬nding a good ï¬rst column can be done in
polynomial time. In contrast, when we restrict the hypotheses class from which we choose
the binary classiï¬ers, the problem of ï¬nding a good ï¬rst column becomes difï¬cult. This
result underscores the difï¬culty of the code design problem. Furthermore, in Section 4
we discuss the general design problem and show that given a set of binary classiï¬ers the
problem of ï¬nding a good code matrix is NP-complete.
Motivated by the intractability results we introduce in Section 5 the notion of continuous codes and cast the design problem of continuous codes as a constrained optimization
problem. As in discrete codes, each column of the code matrix divides the set of labels
into two subsets which are labeled positive (+) and negative (âˆ’). The sign of each entry in
the code matrix determines the subset association (+ or âˆ’) and the magnitude corresponds
to the conï¬dence in this association. Given this formalism, we seek an output code with
small empirical loss whose matrix norm is small. We describe three optimization problems
corresponding to three different norms of the code matrix: l1,l2 and lâˆ. For l1 and lâˆwe
show that the code design problem can be solved by linear programming (LP). Interestingly, for the l2 norm our formalism results in a quadratic program (QP) whose dual does
not depend on the length of the code. Similar to support vector machines, the dual program
can be expressed in terms of inner-products between input instances, hence we can employ
kernel-based binary classiï¬ers. Our framework yields, as a special case, a direct and efï¬cient
method for constructing multiclass support vector machines.
LEARNABILITY AND DESIGN OF OUTPUT CODES
The number of variables in the dual quadratic problem is the product of the number of
samples by the number of classes. This value becomes very large even for small datasets. For
instance, an English letter recognition problem with 1,000 training examples would require
26,000 variables. In this case, the standard matrix representation of dual quadratic problem
would require more than 5 Giga bytes of memory. We therefore describe in Section 6.1 a
memoryefï¬cientalgorithmforsolvingthequadraticprogramforcodedesign.Ouralgorithm
is reminiscent of Plattâ€™s sequential minimal optimization (SMO) . However,
unlike SMO, our algorithm optimizes on each round a reduced subset of the variables
that corresponds to a single example. Informally, our algorithm reduces the optimization
problem to a sequence of small problems, where the size of each reduced problem is equal
to the number of classes of the original multiclass problem. Each reduced problem can again
be solved using a standard QP technique. However, standard approaches would still require
large amount of memory when the number of classes is large and a straightforward solution
is also time consuming. We therefore further develop the algorithm and provide an analytic
solution for the reduced problems and an efï¬cient algorithm for calculating the solution.
The run time of the algorithm is polynomial and the memory requirements are linear in the
number of classes. We conclude with simulations results showing that our algorithm is at
least two orders of magnitude faster than a standard QP technique, even for small number of
classes. We conclude in Section 7 with a short description of the generalization properties
of the algorithm.
Discrete codes
Let S = {(x1, y1), . . . , (xm, ym)} be a set of m training examples where each instance xi
belongs to a domain X. We assume without loss of generality that each label yi is an integer
from the set Y = {1, . . . , k}. A multiclass classiï¬er is a function H : X â†’Y that maps an
instance x into an element y of Y. In this work we focus on a framework that uses output
codes to build multiclass classiï¬ers from binary classiï¬ers. A discrete output code M is a
matrix of size k Ã—l over {âˆ’1, +1} where each row of M correspond to a class y âˆˆY. Each
column of M deï¬nes a partition of Y into two disjoint sets. Binary learning algorithms
are used to construct classiï¬ers, one for each column t of M. That is, the set of examples
induced by column t of M is (x1, My1,t), . . . , (xm, Mym,t). This set is fed as training data
to a learning algorithm that ï¬nds a hypothesis ht : X â†’{âˆ’1, +1}. This reduction yields l
different binary classiï¬ers h1, . . . , hl. We denote the vector of predictions of these classiï¬ers
on an instance x as Â¯h(x) = (h1(x), . . . , hl(x)). We denote the rth row of M by Â¯Mr.
Given an example x we predict the label y for which the row Â¯My is the â€œclosestâ€ to Â¯h(x).
We will use a general notion for closeness and deï¬ne it through an inner-product function
K : Rl Ã— Rl â†’R. The higher the value of K(Â¯h(x), Â¯Mr) is the more conï¬dent we are that r
is the correct label of x according to the classiï¬ers Â¯h. An example for a closeness function
is K(Â¯u, Â¯v) = Â¯u Â· Â¯v. It is easy to verify that this choice of K is equivalent to picking the row
of M which attains the minimal Hamming distance to Â¯h(x).
Givenaclassiï¬er H(x)andanexample(x, y),wesaythat H(x)misclassiï¬edtheexample
if H(x) Ì¸= y. Let [[Ï€]] be 1 if the predicate Ï€ holds and 0 otherwise. Our goal is therefore
to ï¬nd a classiï¬er H(x) such that 1
i=1[[H(xi) Ì¸= yi]] is small. We would like to note
K. CRAMMER AND Y. SINGER
in passing that we mainly focus on the empirical loss minimization problem. We brieï¬‚y
describe bounds on the generalization error in Section 7.
When l is small there might be more then one row of M which attains the maximal value
according to the function K. To accommodate such cases we will relax our deï¬nition and
deï¬ne a classiï¬er H(X) based on a code M to be the mapping H(x) : X â†’2Y given by
H(x) = {y | K(Â¯h(x), Â¯My) = maxr K(Â¯h(x), Â¯Mr)}. In this case we will pick one of the
labels in H(x) uniformly at random, and use the expected error of H(x),
1 âˆ’[[yi âˆˆH(xi)]]
[[yi âˆˆH(xi)]]
In the context of output codes, a multiclass mapping H(x) is thus determined by two
parameters: the coding matrix M and the set of binary classiï¬ers Â¯h(x). Assume that the
binary classiï¬ers h1(x) . . . hl(x) are chosen from some hypothesis class H. The following
natural learning problems arise,
(a) Given a matrix M, ï¬nd a set Â¯h which suffers small empirical loss.
(b) Given a set of binary classiï¬ers Â¯h, ï¬nd a matrix M which has small empirical loss.
(c) Find both a matrix M and a set Â¯h which have small empirical loss.
Previous work has focused mostly on the ï¬rst problem. In this paper we mainly concentrate on the code design problem (problem b), that is, ï¬nding a good matrix M. A summary
of the notation is given in Appendix B.
Finding the ï¬rst column of an output code
Assume we are given a single binary classiï¬er h1(x) and we want to ï¬nd the ï¬rst (or the
single) column of the matrix M which minimizes the empirical loss ÏµS(M, Â¯h). For brevity,
let us denote by Â¯u = (u1 . . . uk)T the ï¬rst column of M. We now describe an efï¬cient
algorithm that ï¬nds Â¯u given h1(x). The algorithmâ€™s running time is polynomial in the size
of the label set k = |Y| and the sample size m. First, note that in this case
yi âˆˆH(xi) â‡”h1(xi) = uyi.
Second, note that the sample can be divided into 2k equivalence classes according to
their labels and the classiï¬cation of h1(x). For r = 1, . . . , k and b âˆˆ{âˆ’1, +1}, deï¬ne ab
m |{i : yi = r, h1(xi) = b}| to be the fraction of the examples with labelr and classiï¬cation b
(according to h1(x)). For b âˆˆ{âˆ’1, +1}, denote byab = k
r , and let wb = |{r : ur = b}|
be the number of elements in Â¯u which are equal to b. (For brevity, we will often use + and
âˆ’to denote the value of b.) Let
[[yi âˆˆH(xi)]]
= 1 âˆ’ÏµS(M, Â¯h).
LEARNABILITY AND DESIGN OF OUTPUT CODES
We can assume without loss of generality that not all the elements in Â¯u are the same
(otherwise, Î¾S(M, Â¯h) = 1
k , which is equivalent to random guessing). Hence, the size of
w+ h1(x) = +1
wâˆ’h1(x) = âˆ’1.
Using Eqs. (2) and (4), we rewrite Eq. (3),
Î¾S(M, Â¯h) = 1
i:yiâˆˆH(xi)
i:h(xi)=uyi
w+ ur = +1
w+ ur = +1
wâˆ’ur = âˆ’1.
Using Eq. (5) we now can expand Î¾S(M, Â¯h),
Î¾S(M, Â¯h) =
2(1 + ur) a+
2(1 âˆ’ur) aâˆ’
For a particular choice of w+ (and wâˆ’= k âˆ’w+) Î¾S is maximized (and ÏµS is minimized)
by setting ur = +1 at the w+ indices which attain the highest values for ( a+
set the rest wâˆ’of the indices to âˆ’1. This can be done efï¬ciently in k log k time using
sorting. Therefore, the best choice of Â¯u is found by enumerating all the possible values
for w+ âˆˆ{1, . . . , k âˆ’1} and choosing the value of w+, wâˆ’which achieves the maximal
value for Eq. (6). Since it takes m operations to calculate a+
r , the total number of
operations needed to ï¬nd the optimal choice for the ï¬rst column is O(m + k2 log k). We
have proven the following theorem.
Theorem 1.
Let S = {(x1, y1), . . . , (xm, ym)} be a set of m training examples, where
each label is an integer from the set {1, . . . , k}. Let H be a binary hypothesis class. Given
K. CRAMMER AND Y. SINGER
an hypothesis h1(x) âˆˆH, the ï¬rst column of an output code which minimizes the empirical
loss deï¬ned by Eq. (1) can be found in polynomial time.
To conclude this section we describe a reduction from SAT. This reduction demonstrates
the difï¬culty in using a limited learning algorithm for a restricted class of hypotheses from
which h1 can be chosen. Speciï¬cally, we show that if one can solve the problem of ï¬nding
simultaneously a binary classiï¬er from a restricted class and a single column of a discrete
code which minimizes the empirical error of Eq. (1), then the satisï¬ability problem can
also be solved. Let
(x1 . . . xn) be a boolean formula over the variables xi âˆˆ{âˆ’1, +1}
where we interpret x = âˆ’1 as False and x = +1 as True. We now give the reduction to
the induced learning problem. Deï¬ne X = {xi, Â¯xi}n
i=1 âˆª{âŠ¥} to be the instance space. Let
S = {(xi, i)}n
i=1 âˆª{(Â¯xi, i +n)}n
i=1 âˆª{(âŠ¥, 2n+1)} be a sample of size m = 2n+1, where the
labels are taken from Y = {1, . . . , 2n+1}. Deï¬ne the learning algorithm L
as follows. The
algorithmâ€™s input is a binary labeled sample of the form {(xi, yi), (Â¯xi, yi+n), (âŠ¥, y2n+1)}n
(x1, . . . , xn) = True and for all i yi âŠ•yi+n = True, then the algorithm returns an
hypothesis which is consistent with the sample (the sample itself). Otherwise, the algorithm
returns the constant hypothesis, h(x) â‰¡1 or h1(x) â‰¡0, which agrees with the majority
of the sample by choosing h1(x) â‰¡arg maxbâˆˆ{âˆ’1,+1} |{i : yi = b}|. Note that the learning
algorithm is non-trivial in the sense that the hypothesis it returns has an empirical loss of
less than 1/2 on the binary labeled sample.
We now show that a multiclass learning algorithm that minimizes the empirical loss ÏµS
over both the ï¬rst column Â¯u and the hypothesis h1(x) which was returned by the algorithm
, can be used to check whether the formula
is satisï¬able. We need to consider two
cases. When
(x1, . . . , xn) = True and for all i
yi âŠ•yi+n = True, then using the
deï¬nition from Eq. (3) we get Î¾S = 1
n + (n + 1)
m . If the above conditions do
not hold (h1(x) is constant), let v â‰¥n + 1 be the number of examples which the hypothesis
h1(x) classiï¬es correctly. Then, using Eq. (3) again we obtain Î¾S = 1
m . Thus, the
minimum of ÏµS is achieved if and only if the formula
is satisï¬able. Therefore, a learning
algorithm for h1(x) and Â¯u can also be used as an oracle for ï¬nding whether a boolean
is satisï¬able.
While the setting discussed in this section is somewhat superï¬cial, these results underscore the difï¬culty of the problem. We next show that the problem of ï¬nding a good output
code given a relatively large set of classiï¬ers Â¯h(x) is intractable. We would like to note
in passing that an efï¬cient algorithm for ï¬nding a single column might be useful in other
settings. For instance in building trees or directed acyclic graphs for multiclass problems
 . We leave this for future research.
Finding a general discrete output code
In this section we prove that given a set of l binary classiï¬ers Â¯h(x), ï¬nding a code
matrix which minimizes the empirical loss ÏµS(M, Â¯h) is NP-complete. Given a sample
S = {(x1, y1), . . . , (xm, ym)} and a set of classiï¬ers Â¯h, let us denote by ËœS the evaluation
of Â¯h(Â·) on the sample S, that is ËœS = {(Â¯h1, y1), . . . , (Â¯hm, ym)}, where Â¯hi
= Â¯h(xi). We now
show that even when k = 2 and K(Â¯u, Â¯v) = Â¯u Â· Â¯v the problem is NP-complete. (Clearly, the
LEARNABILITY AND DESIGN OF OUTPUT CODES
problem remains NPC for k > 2). Following the notation of previous sections, the output
code matrix is composed of two rows Â¯M1 and Â¯M2 and the predicted class for instance xi
is H(xi) = arg maxr=1,2{ Â¯Mr Â· Â¯hi}. For the simplicity of the presentation of the proof, we
assume that both the code M and the hypothesesâ€™ values Â¯hi are over the set {0, 1} (instead
of {âˆ’1, +1}). This assumption does not change the problem as it possible to show that the
same proof technique can be used with hypotheses whose outputs are in {Â±1}.
Theorem 2.
The following decision problem is NP-complete.
Input: A natural number q, a labeled sample ËœS = {(Â¯h1, y1), . . . , (Â¯hm, ym)}, where yi âˆˆ
{1, 2}, and Â¯hi âˆˆ{0, 1}l.
Question: Does there exist a matrix M âˆˆ{0, 1}2Ã—l such that the classiï¬er H(x) based
on an output code M makes at most q mistakes on ËœS.
Our proof is based on a reduction technique introduced by HÂ¨offgen and Simon
 . Since we can check in polynomial time whether the number of classiï¬cation errors
for a given a code matrix M exceeds the bound q, the problem is clearly in NP.
We show a reduction from Vertex Cover in order to prove that the problem is NP-hard.
Given an undirected graph G = (V, E) with |V | = n nodes, we code the structure of the
graph as follows. The sample ËœS will be composed of two subsets, ËœSE and ËœSV of size 2|E| and
|V | respectively. We set l = 2|V | + 1. Each edge {vi, v j} âˆˆE is encoded by two examples
(Â¯h, y) in ËœSE. We set the ï¬rst vector to hi = 1, h j = 1, hl = 1 and 0 elsewhere. We set
the second vector to hi+n = 1, h j+n = 1, hl = 1 and 0 elsewhere. We set the label y of
each example in ËœSE to 1. Each example (Â¯h, y) in ËœSV encodes a node vi âˆˆV where hi = 1,
hi+n = 1, hl = 1 and 0 elsewhere. We set the label y of each example in ËœSV to 2 (second
class). We now show that there exists a vertex cover U âŠ†V with at most q nodes if and only
if there exists a coding matrix M âˆˆ{0, 1}2Ã—l that induces at most q classiï¬cation errors on
the sample ËœS.
(â‡’): Let U âŠ†V be a vertex cover such that |U| â‰¤q. We show that there exists a code
which has at most q mistakes on ËœS. Let u âˆˆ{0, 1}|V | be the characteristic function of U, that
is, ui = 1 if vi âˆˆU and ui = 0 otherwise. Deï¬ne the output code matrix to be Â¯M1 = (u, u, 1)
and Â¯M2 = (Â¬u, Â¬u, 0). Here, Â¬u denotes the component-wise logical not operator.
Since U is a cover, for each Â¯h âˆˆËœSE we get
Â¯M1 Â· Â¯h â‰¥2
Â¯M2 Â· Â¯h â‰¤1
Â¯M2 Â· Â¯h < Â¯M1 Â· Â¯h .
Therefore, for all the examples in ËœSE the predicted label equals the true label and we suffer
no errors on these examples. For each example Â¯h âˆˆËœSV that corresponds to a node v âˆˆU
Â¯M1 Â· Â¯h = 3 > 0 = Â¯M2 Â· Â¯h .
Therefore, these examples are misclassiï¬ed (Recall that the label of each example
in ËœSV is 2). Analogously, for each example in ËœSV which corresponds to v Ì¸âˆˆU we
K. CRAMMER AND Y. SINGER
Â¯M1 Â· Â¯h = 1 < 2 = Â¯M2 Â· Â¯h ,
and these examples are correctly classiï¬ed. We thus have shown that the total number of
mistakes according to M is |U| â‰¤q.
(â‡): Let M be a code which achieves at most q mistakes on ËœS. We construct a subset
U âŠ†V as follows. We scan ËœS and add to U all vertices vi corresponding to misclassiï¬ed
examples from ËœSV . Similarly, for each misclassiï¬ed example from ËœSE corresponding to an
edge {vi, v j}, we pick either vi or v j at random and add it to U. Since we have at most q
misclassiï¬ed examples in ËœS the size of U is at most q. We claim that the set U is a vertex
cover of the graph G. Assume by contradiction that there is an edge {vi, v j} for which neither
vi nor v j belong to the set U. Therefore, by construction, the examples corresponding to
the vertices vi and v j are classiï¬ed correctly and we get,
M1,i + M1,i+n + M1,l < M2,i + M2,i+n + M2,l
M1, j + M1, j+n + M1,l < M2, j + M2, j+n + M2,l
Summing the above equations yields that,
M1,i + M1, j + M1,i+n + M1, j+n + 2M1,l
< M2,i + M2, j + M2,i+n + M2, j+n + 2M2,l.
In addition, the two examples corresponding to the edge {vi, v j} are classiï¬ed correctly,
implying that
M1,i + M1, j + M1,l > M2,i + M2, j + M2,l
M1,i+n + M1, j+n + M1,l > M2,i+n + M2, j+n + M2,l
which again by summing the above equations yields,
M1,i + M1, j + M1,i+n + M1, j+n + 2M1,l
> M2,i + M2, j + M2,i+n + M2, j+n + 2M2,l.
Comparing Eqs. (7) and (8) we get a contradiction.
Continuous codes
The intractability results of previous sections motivate a relaxation of output codes. In this
section we describe a natural relaxation where both the classiï¬ersâ€™ output and the code
matrix are over the reals.
As before, the classiï¬er H(x) is constructed from a code matrix M and a set of binary
classiï¬ers Â¯h(x). The matrix M is of size k Ã— l over R where each row of M corresponds to
LEARNABILITY AND DESIGN OF OUTPUT CODES
a class y âˆˆY. Analogously, each binary classiï¬er ht(x) âˆˆH is a mapping ht(x) : X â†’R.
A column t of M deï¬nes a partition of Y into two disjoint sets. The sign of each element
of the tth column is interpreted as the set (+1 or âˆ’1) to which the class r belongs and the
magnitude |Mr,t| is interpreted as the conï¬dence in the associated partition. Similarly, we
interpret the sign of ht(x) as the prediction of the set (+1 or âˆ’1) to which the label of the
instance x belongs and the magnitude |ht(x)| as the conï¬dence of this prediction. Given
an instance x, the classiï¬er H(x) predicts the label y which maximizes the conï¬dence
function K(Â¯h(x), Â¯Mr), H(x) = arg maxrâˆˆY{K(Â¯h(x), Â¯Mr)}. Since the code is over the reals,
we can assume here without loss of generality that exactly one class attains the maximum
value according to the function K. We will concentrate on the problem of ï¬nding a good
continuous code given a set of binary classiï¬ers Â¯h.
The approach we will take is to cast the code design problem as constrained optimization
problem. Borrowing the idea of soft margin we replace the discrete
0â€“1 multiclass loss with the linear bound
K(Â¯h(xi), Â¯Mr) + 1 âˆ’Î´yi,r
Â¯h(xi), Â¯Myi
where Î´i, j equals 1 if i = j and 0 otherwise. This formulation is also motivated by the
generalization analysis of Allewein, Schapire, and Singer . The analysis they give is
based on the margin of examples where the margin is closely related to the deï¬nition of the
loss as given by Eq. (9).
Put another way, the correct label should have a conï¬dence value which is larger by at
least one than any of the conï¬dences for the rest of the labels. Otherwise, we suffer loss
which is linearly proportional to the difference between the conï¬dence of the correct label
and the maximum among the conï¬dences of the other labels. The bound on the empirical
loss is then,
ÏµS(M, Â¯h) = 1
[[H(xi) Ì¸= yi]]
K(Â¯h(xi), Â¯Mr) + 1 âˆ’Î´yi,r
Â¯h(xi), Â¯Myi
We say that a sample S is classiï¬ed correctly using a set of binary classiï¬ers Â¯h if there exists
a matrix M such that the above loss is equal to zero,
K(Â¯h(xi), Â¯Mr) + 1 âˆ’Î´yi,r
Â¯h(xi), Â¯Myi
bi,r = 1 âˆ’Î´yi,r.
Thus, a matrix M that satisï¬es Eq. (10) would also satisfy the following constraints,
Â¯h(xi), Â¯Myi
âˆ’K(Â¯h(xi), Â¯Mr) â‰¥bi,r.
K. CRAMMER AND Y. SINGER
Motivated by Vapnik , Allwein, Schapire, and Singer and the results of
Section 7 we seek a matrix M with a small norm which satisï¬es Eq. (12). Throughout the paper we deï¬ne the norm of a matrix M to be the norm of the concatenation
of the rows of M, âˆ¥( Â¯M1, . . . , Â¯Mk)âˆ¥. Thus, when the entire sample S can be labeled correctly, the problem of ï¬nding a good matrix M can be stated as the following optimization
subject to:
Â¯h(xi), Â¯Myi
âˆ’K(Â¯h(xi), Â¯Mr) â‰¥bi,r
Here p is an integer. Note that m of the constraints for r = yi are automatically satisï¬ed.
This is changed in the following derivation for the non-separable case. In the general case a
matrix M which classiï¬es all the examples correctly might not exist. We therefore introduce
slack variables Î¾i â‰¥0 and modify Eq. (10) to be,
K(Â¯h(xi), Â¯Mr) + 1 âˆ’Î´yi,r
Â¯h(xi), Â¯Myi
The corresponding optimization problem is,
subject to:
Â¯h(xi), Â¯Myi
âˆ’K(Â¯h(xi), Â¯Mr) â‰¥bi,r âˆ’Î¾i
for some constant Î² â‰¥0. This is an optimization problem with â€œsoftâ€ constraints. Analogously, we can deï¬ne an optimization problem with â€œhardâ€ constraints,
subject to:
Â¯h(xi), Â¯Myi
âˆ’K(Â¯h(xi), Â¯Mr) â‰¥bi,r âˆ’Î¾i
âˆ¥Mâˆ¥p â‰¤A, for some A > 0
The relation between the â€œhardâ€ and â€œsoftâ€ constraints and their formal properties is beyond the scope of this paper. For further discussion on the relation between the problems
see Vapnik .
Design of continuous codes using linear programming
We now further develop Eq. (15) for the cases p = 1, 2, âˆ. We deal ï¬rst with the cases
p = 1 and p = âˆwhich result in linear programs. For the simplicity of presentation we
will assume that K(Â¯u, Â¯v) = Â¯u Â· Â¯v .
For the case p = 1 the objective function of Eq. (15) becomes Î² 
i,r |Mi,r| + 
introduce a set of auxiliary variables Î±i,r = |Mi,r| to get a standard linear programming
LEARNABILITY AND DESIGN OF OUTPUT CODES
subject to:
Î¾i + Â¯h(xi) Â· Â¯Myi âˆ’Â¯h(xi) Â· Â¯Mr â‰¥bi,r
Î±r,t â‰¥Â±Mr,t
To obtain its dual program (see also Appendix A) we deï¬ne one variable for each constraint
of the primal problem. We use Î·i,r for the ï¬rst set of constraints, and Î³ Â±
t,r for the second set.
The dual program is,
subject to:
Î´yi,r âˆ’Î·i,r
The case of p = âˆis similar. The objective function of Eq. (15) becomes Î² maxi,r
|Mi,r| + 
i Î¾i. We introduce a single new variable Î± = maxi,r |Mi,r| to obtain the primal
subject to:
Î¾i + Â¯h(xi) Â· Â¯Myi âˆ’Â¯h(xi) Â· Â¯Mr â‰¥bi,r
Following the technique for p = 1, we get that the dual program is,
subject to:
Î´yi,r âˆ’Î·i,r
Both programs (p = 1 and 0 = âˆ) can be now solved using standard linear program
K. CRAMMER AND Y. SINGER
Design of continuous codes using quadric programming
We now discuss in detail Eq. (15) for the case p = 2. For convenience we use the square of
the norm of the matrix (instead the norm itself). Therefore, the primal program becomes,
subject to:
Î¾i + Â¯h(xi) Â· Â¯Myi âˆ’Â¯h(xi) Â· Â¯Mr â‰¥bi,r
We solve the optimization problem by ï¬nding a saddle point of the Lagrangian:
L(M, Î¾, Î·) = 1
Â¯h(xi) Â· Â¯Mr âˆ’Â¯h(xi) Â· Â¯Myi âˆ’Î¾i + bi,r
subject to:
The saddle point we are seeking is a minimum for the primal variables (M, Î¾), and the
maximum for the dual ones (Î·). To ï¬nd the minimum over the primal variables we require,
Î·i,r = 0 â‡’
Similarly, for Â¯Mr we require:
Î·i,r Â¯h(xi) âˆ’
+Î² Â¯Mr = 0
â‡’Â¯Mr = Î²âˆ’1
Î´yi,r âˆ’Î·i,r
Equation (19) implies that when the optimum of the objective function is achieved, each
row of the matrix M is a linear combination of Â¯h(xi). We say that an example i is a support
pattern for class r if the coefï¬cient (Î´yi,r âˆ’Î·i,r) of Â¯h(xi) in Eq. (19) is not zero. There are
two settings for which an example i can be a support pattern for class r. The ï¬rst case is
when the label yi of an example is equal to r, then the ith example is a support pattern if
Î·i,r < 1. The second case is when the label yi of the example is different from r, then the
ith pattern is a support pattern if Î·i,r > 0.
Loosely speaking, since for all i,r, Î·i,r â‰¥0 and 
r Î·i,r = 1, the variable Î·i,r can be
viewed as a distribution over the labels for each example. An example i affects the solution
for M (Eq. (19)) if and only if Â¯Î·i in not a point distribution concentrating on the correct
label yi. Thus, only the questionable patterns contribute to the learning process.
LEARNABILITY AND DESIGN OF OUTPUT CODES
We develop the Lagrangian using only the dual variables. Substituting Eq. (18) to Eq. (16)
we obtain,
Î·i,r Â¯h(xi) Â· Â¯Mr âˆ’
Î·i,r Â¯h(xi) Â· Â¯Myi âˆ’
Î·i,rbi,r + 1
Î·i,r Â¯h(xi) Â· Â¯Mr âˆ’
Î·i,r Â¯h(xi) Â· Â¯Myi +
We substitute Â¯Mr using Eq. (19) and get,
Î·i,r Â¯h(xi) Â· Â¯Mr
Î·i,r Â¯h(xi) Â· Î²âˆ’1 
Î´y j,r âˆ’Î· j,r
Â¯h(xi) Â· Â¯h(x j)
Î´y j,r âˆ’Î· j,r
Î·i,r Â¯h(xi) Â· Â¯Myi
Î·i,r Â¯h(xi) Â· Î²âˆ’1 
Î´y j,yi âˆ’Î· j,yi
Â¯h(xi) Â· Â¯h(x j)
Î´y j,yi âˆ’Î· j,yi
Â¯h(xi) Â· Â¯h(x j)
Î´y j,yi âˆ’Î· j,yi
Â¯h(xi) Â· Â¯h(x j)
Î´y j,yi âˆ’Î· j,yi
Â¯h(xi) Â· Â¯h(x j)
Î´y j,r âˆ’Î· j,r
Î´yi,r âˆ’Î·i,r
Î´y j,r âˆ’Î· j,r
Â¯h(xi) Â· Â¯h(x j)
Î´yi,r âˆ’Î·i,r
Î´y j,r âˆ’Î· j,r
K. CRAMMER AND Y. SINGER
Taking the difference S1 âˆ’S2 while using Eqs. (21) and (22) we get:
S1 âˆ’S2 = Î²âˆ’1 
Â¯h(xi) Â· Â¯h(x j)
Î´y j,r âˆ’Î· j,r
Â¯h(xi) Â· Â¯h(x j)
Î´y j,r âˆ’Î· j,r
Â¯h(xi) Â· Â¯h(x j)
Î´yi,r âˆ’Î·i,r
Î´y j,r âˆ’Î· j,r
Finally, plugging the values for S1, S2 and S3 from Eqs. (23) and (24) in Eq. (20) we get,
Q(Î·) = âˆ’Î²âˆ’1 
Â¯h(xi) Â· Â¯h(x j)
Î´yi,r âˆ’Î·i,r
Î´y j,r âˆ’Î· j,r
Â¯h(xi) Â· Â¯h(x j)
Î´yi,r âˆ’Î·i,r
Î´y j,r âˆ’Î· j,r
Â¯h(xi) Â· Â¯h(x j)
Î´yi,r âˆ’Î·i,r
Î´y j,r âˆ’Î· j,r
Let Â¯1i be the vector with all components zero, except for the ith component which is
equal to one, and let Â¯1 be the vector whose components are all one. Using this notation we
can rewrite the dual program in vector form as
[Â¯h(xi) Â· Â¯h(x j)]
Â¯1yi âˆ’Â¯Î·i
Â¯1y j âˆ’Â¯Î· j
subject to:
and Â¯Î·i Â· Â¯1 = 1
It is easy to verify that Q(Î·) is strictly convex in Î·. Since the constraints are linear the
above problem has a single optimal solution and therefore QP methods can be used to
solve it. In Section 6 we describe a memory efï¬cient algorithm for solving this special QP
To simplify the equations we denote by Â¯Ï„i = Â¯1yi âˆ’Â¯Î·i the difference between the correct point distribution and the distribution obtained by the optimization problem, Eq. (19)
Â¯Mr = Î²âˆ’1 
Â¯h(xi)Ï„i,r
Since we look for the value of the variables which maximize the objective function Q (and
not the optimum of Q itself), we can omit constants and write the dual problem given by
LEARNABILITY AND DESIGN OF OUTPUT CODES
Eq. (25) as,
[Â¯h(xi) Â· Â¯h(x j)](Â¯Ï„i Â· Â¯Ï„ j) âˆ’
subject to:
and Â¯Ï„i Â· Â¯1 = 0
Finally, the classiï¬er H(x) can be written in terms of the variable Ï„ as,
H(x) = arg max
r {Â¯h(x) Â· Â¯Mr}
Â¯h(xi)Ï„i,r
Ï„i,r[Â¯h(x) Â· Â¯h(xi)]
Ï„i,r[Â¯h(x) Â· Â¯h(xi)]
As in Support Vector Machines, the dual program and the classiï¬cation algorithm depend
only on inner products of the form Â¯h(xi) Â· Â¯h(x). Therefore, we can perform the calculations
in some high dimensional inner-product space Z using a transformation Â¯Ï† : Rl â†’Z. We
thus replace the inner-product in Eq. (27) and in Eq. (28) with a general inner-product kernel
K that satisï¬es Mercer conditions . The general dual program is therefore,
K(Â¯h(xi), Â¯h(x j)) (Â¯Ï„i Â· Â¯Ï„ j) âˆ’
subject to:
and Â¯Ï„i Â· Â¯1 = 0
and the classiï¬cation rule H(x) becomes,
H(x) = arg max
Ï„i,r K(Â¯h(x), Â¯h(xi))
ThegeneralframeworkfordesigningoutputcodesusingtheQPprogramdescribedabove,
also provides, as a special case, a new algorithm for building multiclass Support Vector
Machines. Assume that the instance space is the vector space Rn and deï¬ne Â¯h(Â¯x)
= Â¯x (thus
l = n), then the primal program in Eq. (16) becomes
subject to:
Î¾i + Â¯xi Â· Â¯Myi âˆ’Â¯xi Â· Â¯Mr â‰¥bi,r
Note that for k = 2 Eq. (31) reduces to the primal program of SVM, if we take Â¯M1 = âˆ’Â¯M2
and C = Î²âˆ’1. We would also like to note that this special case is reminiscent of the multiclass
K. CRAMMER AND Y. SINGER
Summary of the sizes of the optimization problems for different norms.
m + kl + 1
0-Constraints
Constraints
0-Constraints
Constraints
m + kl + 1
approach for SVMâ€™s suggested by Weston and Watkins . Their approach compared the
conï¬dence K(x, Â¯My) to the conï¬dences of all other labels K(x, Â¯Mr) and had m(kâˆ’1) slack
variables in the primal problem. In contrast, in our framework the conï¬dence K(x, Â¯My) is
compared to maxrÌ¸=y K(x, Â¯Mr) and has only m slack variables in the primal program.
In Table 1 we summarize the properties of the programs discussed above. As shown in
the table, the advantage of using l2 in the objective function is that the number of variables
in the dual problem is only a function of k and m and does not depend on the number of
columnsl in M. The number of columns in M only affects the evaluation of the inner-product
The formalism given by Eq. (15) can also be used to construct the code matrix incrementally (column by column). We now outline the incremental (inductive) approach. However,
we would like to note that this method only applies when K(Â¯v, Â¯u) = Â¯v Â· Â¯u. In the ï¬rst step
of the incremental algorithm, we are given a single binary classiï¬er h1(x) and we need to
construct the ï¬rst column of M. We rewrite Eq. (15) in a scalar form and obtain,
subject to:
h1(xi)Myi âˆ’h1(xi)Mr â‰¥bi,r âˆ’Î¾i.
Here, Î² â‰¥0 is a given constant and bi,r = 1 âˆ’Î´yi,r, as before. For the rest of the columns
we assume inductively that h1(x), . . . , hl(x) have been provided and the ï¬rst l columns of
the matrix M have been found. In addition, we are provided with a new binary classiï¬er
= hâ€²(x) for the next column. We need to ï¬nd a new column of M (indexed l + 1).
We substitute the new classiï¬er and the matrix in Eq. (14) and get,
Â¯h(xi) Â· Â¯Mr + hâ€²(xi)Mâ€²
r + 1 âˆ’Î´yi,r
Â¯h(xi) Â· Â¯Myi + hâ€²(xi)Mâ€²
The constraints appearing in Eq. (15) now become
Â¯h(xi) Â· Â¯Myi + hâ€²(xi)Mâ€²
yi âˆ’Â¯h(xi) Â· Â¯Mr âˆ’hâ€²(xi)Mâ€²
r â‰¥1 âˆ’Î´yi,r âˆ’Î¾i
yi âˆ’hâ€²(xi)Mâ€²
Â¯h(xi) Â· Â¯Myi âˆ’Â¯h(xi) Â· Â¯Mr
+ 1 âˆ’Î´yi,r âˆ’Î¾i.
LEARNABILITY AND DESIGN OF OUTPUT CODES
We now redeï¬ne bi,r to be âˆ’[Â¯h(xi) Â· Â¯Myi âˆ’Â¯h(xi) Â· Â¯Mr] + 1 âˆ’Î´yi,r. It is straightforward to
verify that this deï¬nition of bi,r results in an equation of the same form of Eq. (32). We
can thus apply the same algorithms designed for the â€œbatchâ€ case. In the case of l1 and lâˆ,
this construction decomposes a single problem into l sub-problems with fewer variables
and constraints. However, for l2 the size of the program remains the same while we lose the
ability to use kernels. We therefore concentrate on the batch case for which we need to ï¬nd
the entire matrix at once.
An efï¬cient algorithm for the QP problem
The quadratic program presented in Eq. (29) can be solved using standard QP techniques.
As shown in Table 1 the dual program depends on mk variables and has km +m constraints
all together. Converting the dual program in Eq. (29) to a standard QP form requires storing
and manipulating a matrix with (mk)2 elements. Clearly, this would prohibit applications of
non-trivial size. We now introduce a memory efï¬cient algorithm for solving the quadratic
optimization problem given by Eq. (29).
First, note that the constraints in Eq. (29) can be divided into m disjoint subsets {Â¯Ï„i â‰¤Â¯1yi,
Â¯Ï„i Â· Â¯1 = 0}m
i=1. The algorithm we describe works in rounds. On each round it picks a single
set {Â¯Ï„i â‰¤Â¯1yi, Â¯Ï„i Â· Â¯1 = 0}, and modiï¬es Â¯Ï„i so as to optimize the reduced optimization problem.
The algorithm is reminiscent of Plattâ€™s SMO algorithm . Note, however, that
our algorithm optimizes one example on each round, and not two as in SMO.
Let us ï¬x an example index p and write the objective function only in terms of the
variables Â¯Ï„p. For brevity, let Ki, j = K(Â¯h(xi), Â¯h(x j)). We isolate Â¯Ï„p in Q.
Ki, j(Â¯Ï„i Â· Â¯Ï„ j) âˆ’
2Î²âˆ’1K p,p(Â¯Ï„p Â· Â¯Ï„p) âˆ’Î²âˆ’1 
Ki,p(Â¯Ï„p Â· Â¯Ï„i) âˆ’1
Ki, j(Â¯Ï„i Â· Â¯Ï„ j)
âˆ’Â¯Ï„p Â· Â¯bp âˆ’
2Î²âˆ’1K p,p(Â¯Ï„p Â· Â¯Ï„p) âˆ’Â¯Ï„p Â·
Â¯bp + Î²âˆ’1 
Ki, j(Â¯Ï„i Â· Â¯Ï„ j) âˆ’
2 Ap(Â¯Ï„p Â· Â¯Ï„p) âˆ’Â¯Bp Â· Â¯Ï„p + C p
Ap = Î²âˆ’1K p,p > 0
Â¯Bp = Â¯bp + Î²âˆ’1 
K. CRAMMER AND Y. SINGER
Ki, j(Â¯Ï„i Â· Â¯Ï„ j) âˆ’
For brevity, we will omit the index p and drop constants (that do not affect the solution).
The reduced optimization has k variables and k + 1 constraints,
Q(Â¯Ï„) = âˆ’1
2 A(Â¯Ï„ Â· Â¯Ï„) âˆ’Â¯B Â· Â¯Ï„
subject to:
Â¯Ï„ Â· Â¯1 = 0
Although this program can be solved using a standard QP technique, it still requires large
amount of memory when k is large, and a straightforward solution is also time consuming.
Furthermore, this problem constitutes the core and inner-loop of the algorithm. We therefore
further develop the algorithm and describe a more efï¬cient method for solving Eq. (37).
We write Q(Â¯Ï„) in Eq. (37) using a completion to quadratic form,
Q(Â¯Ï„) = âˆ’1
2 A(Â¯Ï„ Â· Â¯Ï„) âˆ’Â¯B Â· Â¯Ï„
Since A > 0 the program from Eq. (37) becomes,
Q(Â¯Î½) = âˆ¥Â¯Î½âˆ¥2
subject to:
Â¯Î½ â‰¤Â¯D and Â¯Î½ Â· Â¯1 = Â¯D Â· Â¯1 âˆ’1
In Section 6.1 we discuss an analytic solution to Eq. (38) and in Section 6.2 we describe a
time efï¬cient algorithm for computing the analytic solution.
An analytic solution
While the algorithm we describe in this section is simple to implement and efï¬cient, its
derivation is quite complex. Before describing the analytic solution to Eq. (38), we would
like to give some intuition on our method. Let us ï¬x some vector Â¯D and denote Âµ = Â¯Î½ Â· Â¯1.
First note that Â¯Î½ = Â¯D is not a feasible point since the constraint Â¯Î½ Â· Â¯1 = Â¯D Â· Â¯1 âˆ’1 is not
satisï¬ed. Hence for any feasible point some of the constraints Â¯Î½ â‰¤Â¯D are not tight. Second,
note that the differences between the bounds Dr and the variables Î½r sum to one. Let us
induce a uniform distribution over the components of Â¯Î½. Then, the variance of Â¯Î½ is
Ïƒ 2 = E[Î½2] âˆ’[EÎ½]2 = 1
k âˆ¥Â¯Î½âˆ¥2 âˆ’1
LEARNABILITY AND DESIGN OF OUTPUT CODES
An illustration of two feasible points for the reduced optimization problem with Â¯D = (1.0, 0.2, 0.6,
0.8, 0.6). The x-axis is the index of the point, and the y-axis denotes the values Â¯Î½. The right plot has a smaller
variance hence it achieves a better value for Q.
Since the expectation Âµ is constrained to a given value, the optimal solution is the vector
achieving the smallest variance. That is, the components of of Â¯Î½ should attain similar values,
as much as possible, under the inequality constraints Â¯Î½ â‰¤Â¯D. In ï¬gure 1 we illustrate this
motivation. We picked Â¯D = (1.0, 0.2, 0.6, 0.8, 0.6) and show plots for two different feasible
values for Â¯Î½. The x-axis is the index r of the point and the y-axis designates the values of
the components of Â¯Î½. The norm of Â¯Î½ on the plot on the right hand side plot is smaller than
the norm of the plot on the left hand side. The right hand side plot is the optimal solution
for Â¯Î½. The sum of the lengths of the arrows in both plots is Â¯D Â· Â¯1 âˆ’Â¯Î½ Â· Â¯1. Since both sets
of points are feasible, they satisfy the constraint Â¯Î½ Â· Â¯1 = Â¯D Â· Â¯1 âˆ’1. Thus, the sum of the
lengths of the â€œarrowsâ€ in both plots is one. We exploit this observation in the algorithm
we describe in the sequel.
We therefore seek a feasible vector Â¯Î½ whose most of its components are equal to some
threshold Î¸. Given Î¸ we deï¬ne a vector Â¯Î½ whose its rth component equal to the minimum
between Î¸ and Dr, hence the inequality constraints are satisï¬ed. We deï¬ne
We denote by
F(Î¸) = Â¯Î½Î¸ Â· Â¯1 =
Using F, the equality constraint from Eq. (38) becomes F(Î¸) = Â¯D Â· Â¯1 âˆ’1.
Let us assume without loss of generality that the components of the vector Â¯Î½ are given in
a descending order, D1 â‰¥D2 â‰¥. . . Dk (this can be done in k log k time). Let Dk+1 = âˆ’âˆ
and D0 = âˆ. To prove the main theorem of this section we need the following lemma.
K. CRAMMER AND Y. SINGER
F(Î¸) is piecewise linear with a slope r in each range (Dr+1, Dr) for r =
0, . . . , k.
Let us develop F(Î¸).
{[[Î¸ > Dr]]Dr + [[Î¸ â‰¤Dr]]Î¸}
[[Î¸ > Dr]]Dr + Î¸
Note that if Î¸ > Dr then Î¸ > Du for all u â‰¥r. Also, the equality k
râ€²=1[[Î¸ â‰¤Drâ€²]] = r holds
for each Î¸ in the range Dr+1 < Î¸ < Dr. Thus, for Dr+1 < Î¸ < Dr (r = 0 Â· Â· Â· k), the function
F(Î¸) has the form,
This completes the proof.
Corollary 1.
There exists a unique Î¸0 â‰¤D1 such that F(Î¸0) = Â¯D Â· Â¯1 âˆ’1.
From Lemma 1 (Eq. (41)) we conclude that F(Î¸) is strictly increasing and continuous in the range Î¸ â‰¤D1. Therefore, F(Î¸) has an inverse in that range, using the theorem that
everystrictlyincreasingandcontinuousfunctionhasaninverse.Since F(Î¸) = kÎ¸ forÎ¸ â‰¤Dk
then F(Î¸) â†’âˆ’âˆas Î¸ â†’âˆ’âˆ. Hence, the range of F for the interval (âˆ’âˆ, D1] is the interval (âˆ’âˆ, Â¯D Â· Â¯1] which clearly contains Â¯D Â· Â¯1âˆ’1. Thus Î¸0
= Fâˆ’1( Â¯D Â· Â¯1âˆ’1) âˆˆ[âˆ’âˆ, D1]
as needed. Uniqueness of Î¸0 follows the fact that the function F is a one-to-one mapping
onto (âˆ’âˆ, Â¯D Â· Â¯1].
We now can prove the main theorem of this section.
Theorem 3.
Let Î¸0 be the unique solution of F(Î¸) = Â¯D Â· Â¯1 âˆ’1. Then Â¯Î½Î¸0 is the optimum
value of the optimization problem stated in Eq. (38).
The theorem tells us that the optimum value of Eq. (38) is of the form deï¬ned by Eq. (40)
and that there is exactly one value of Î¸ for which the equality constraint F(Î¸) = Â¯Î½Î¸ Â· Â¯1 =
Â¯D Â· Â¯1 âˆ’1 holds. A plot of F(Î¸) and the solution for Î¸ from ï¬gure 1 are shown in ï¬gure 2.
Corollary 1 implies that a solution exists and is unique. Note also that from deï¬nition of Î¸0 we have that the vector Â¯Î½Î¸0 is a feasible point of Eq. (38). We now prove that Â¯Î½Î¸0
is the optimum of Eq. (38) by showing that âˆ¥Â¯Î½âˆ¥2 > âˆ¥Â¯Î½Î¸0âˆ¥2 for all feasible points Â¯Î½ Ì¸= Â¯Î½Î¸.
LEARNABILITY AND DESIGN OF OUTPUT CODES
An illustration of the solution of the QP problem using the inverse of F(Î¸) for Â¯D = (1.0, 0.2,
0.6, 0.8, 0.6). The optimal value is the solution for the equation F(Î¸) = 2.2 which is 0.5.
Assume, by contradiction, that there is a vector Â¯Î½ such that âˆ¥Â¯Î½âˆ¥2 â‰¤âˆ¥Â¯Î½Î¸0âˆ¥2. Let Â¯Ïµ
Â¯Î½ âˆ’Â¯Î½Î¸ Ì¸= Â¯0, and deï¬ne I
r = Dr} = {r : Î¸0 > Dr}. Since both Â¯Î½ and Â¯Î½Î¸0 satisfy the
equality constraint of Eq. (38), we have,
Â¯Î½ Â· Â¯1 = Â¯Î½Î¸0 Â· Â¯1 â‡’(Â¯Î½ âˆ’Â¯Î½Î¸0) Â· Â¯1 = 0
â‡’Â¯Ïµ Â· Â¯1 =
Since Â¯Î½ is a feasible point we have Â¯Î½ = Â¯Î½Î¸0 + Â¯Ïµ â‰¤Â¯D. Also, by the deï¬nition of the set I we
have that Î½Î¸
r = Dr for all r âˆˆI. Combining the two properties we get,
for all r âˆˆI
We start with the simpler case of Ïµr = 0 for all r âˆˆI. In this case, Â¯Î½ differs from Â¯Î½Î¸0 only
on a subset of the coordinates r /âˆˆI. However, for these coordinates the components of Â¯Î½Î¸0
are equal to Î¸0, thus we obtain a zero variance from the constant vector whose components
K. CRAMMER AND Y. SINGER
are all Î¸0. Therefore, no other feasible vector can achieve a better variance. Formally, since
Ïµr = 0 for all r âˆˆI, then the terms for r âˆˆI cancel each other,
âˆ¥Â¯Î½âˆ¥2 âˆ’âˆ¥Â¯Î½Î¸0âˆ¥2 =
From the deï¬nition of Â¯Î½Î¸0 in Eq. (40) we get that Î½Î¸0
r = Î¸0 for all r /âˆˆI,
âˆ¥Â¯Î½âˆ¥2 âˆ’âˆ¥Â¯Î½Î¸0âˆ¥2 =
(Î¸0 + Ïµr)2 âˆ’
We use now the assumption that Ïµr = 0 for all r âˆˆI and the equality k
r=1 Ïµr = 0 (Eq. (42))
to obtain,
âˆ¥Â¯Î½âˆ¥2 âˆ’âˆ¥Â¯Î½Î¸0âˆ¥2 = 2Î¸0
and we get a contradiction since Â¯Ïµ Ì¸= Â¯0.
We now turn to prove the complementary case in which 
râˆˆI Ïµr < 0. Since 
râˆˆI Ïµr < 0,
then there exists u âˆˆI such that Ïµu < 0. We use again Eq. (42) and conclude that there exists
also v /âˆˆI such that Ïµv > 0. Let us assume without loss of generality that Ïµu + Ïµv < 0
(The case Ïµu + Ïµv â‰¥0 follows analogously by switching the roles of u and v). Deï¬ne Â¯Î½â€² as
The vector Â¯Î½â€² satisï¬es the constraints of Eq. (38) since Î½â€²
u = Î½u + Ïµv = Du + Ïµu + Ïµv < Du
v = Î½v âˆ’Ïµv = Î¸0 + Ïµv âˆ’Ïµv = Î¸0 â‰¤Dv. Since Â¯Î½ and Â¯Î½â€² are equal except for their u
and v components we get,
âˆ¥Â¯Î½â€²âˆ¥2 âˆ’âˆ¥Â¯Î½âˆ¥2 = [(Î½â€²
v)2] âˆ’[(Î½u)2 + (Î½v)2]
Substituting the values for Î½â€²
v from the deï¬nition of Â¯Î½â€² we obtain,
âˆ¥Â¯Î½â€²âˆ¥2 âˆ’âˆ¥Â¯Î½âˆ¥2 = [(Î½u + Ïµv)2 + (Î½v âˆ’Ïµv)2] âˆ’[(Î½u)2 + (Î½v)2]
v + 2Î½uÏµv + Ïµ2
v + 2(Î½u âˆ’Î½v)Ïµv
LEARNABILITY AND DESIGN OF OUTPUT CODES
Using the deï¬nition of Â¯Î½ and Â¯Î½Î¸0 for Î½u = Î½Î¸0
u + Ïµu = Du+Ïµu and for Î½v = Î½Î¸0
v + Ïµv = Î¸0+Ïµv
we obtain,
âˆ¥Â¯Î½â€²âˆ¥2 âˆ’âˆ¥Â¯Î½âˆ¥2 = 2Ïµ2
v + 2(Du + Ïµu âˆ’Î¸0 âˆ’Ïµv)Ïµv
= 2(Du + Ïµu âˆ’Î¸0)Ïµv
= 2ÏµuÏµv + 2(Du âˆ’Î¸0)Ïµv
The ï¬rst term of the bottom equation is negative since Ïµu < 0 and Ïµv > 0. Also u âˆˆI, hence
Î¸0 > Du and the second term is also negative. We thus get,
âˆ¥Â¯Î½â€²âˆ¥2 âˆ’âˆ¥Â¯Î½âˆ¥2 < 0.
which is a contradiction.
An efï¬cient algorithm for computing the analytic solution
The optimization problem of Eq. (38) can be solved using standard QP methods, and
interior point methods in particular . For these methods the computation
time is (k2). In this section we give an algorithm for solving that optimization problem
in O(k log k) time, by solving the equation F(Î¸) = Â¯D Â· Â¯1 âˆ’1.
As before, we assume that the components of the vector Â¯Î½ are given in a descending order,
D1 â‰¥D2 â‰¥. . . Dk and we denote Dk+1 = âˆ’âˆ. The algorithm searches for the interval
[Dr+1, Dr) which contains Î¸0. We now use simple algebraic manipulations to derive the
search scheme for Î¸0. Since F(D1) = F(Î¸0) + 1,
Î¸0 âˆˆ[Dr+1, Dr) â‡”1 > F(D1) âˆ’F(Dr)
F(D1) âˆ’F(Dr+1) â‰¥1.
For convenience, we deï¬ne the potential function
(r) = 1 âˆ’[F(D1) âˆ’F(Dr)],
and obtain,
Î¸0 âˆˆ[Dr+1, Dr) â‡”
(r + 1) â‰¤0
Also note that,
(r + 1) = {1 âˆ’[F(D1) âˆ’F(Dr)]} âˆ’{1 âˆ’[F(D1) âˆ’F(Dr+1)]}
= F(Dr) âˆ’F(Dr+1).
Recall that the function F(Î¸) is linear in each interval [Dr+1, Dr) with a slope r (Lemma 1),
F(Dr) âˆ’F(Dr+1) = r(Dr âˆ’Dr+1)
(r) âˆ’r(Dr âˆ’Dr+1).
K. CRAMMER AND Y. SINGER
To solve the equation F(Î¸) = Â¯DÂ· Â¯1âˆ’1, we ï¬rst ï¬nd r such that
(r) > 0 and
(r +1) â‰¤0,
which implies that Î¸0 âˆˆ[Dr+1, Dr). Using Eq. (44) and the equation F(D1) = F(Î¸0) + 1
F(Dr) âˆ’F(Î¸0) =
Using the linearity of F(Î¸) we obtain,
F(Dr) âˆ’F(Î¸0) = r(Dr âˆ’Î¸0) â‡’r(Dr âˆ’Î¸0) =
The complete algorithm is described in ï¬gure 3. Since it takes O(k log k) time to sort the
vector Â¯D and another O(k) time for the loop search, the total run time is O(k log k).
We are ï¬nally ready to give the algorithm for solving learning problem described by
Eq. (29). Since the output code is constructed of the supporting patterns we term our
algorithm SPOC for Support Pattern Output Coding. The SPOC algorithm is described in
ï¬gure 4. We have also developed methods for choosing an example p to modify on each
round and a stopping criterion for the entire optimization algorithm. The complete details
of the algorithms along with the results of experiments we have conducted will appear
elsewhere.
We have performed preliminary experiments with synthetic data in order to check the
actual performance of our algorithm. We tested the special case corresponding to multiclass
SVM by setting Â¯h(x) = x. The code matrices we test are of k = 4 rows (classes) and l = 2
columns. We varied the size of the training set size from m = 10 to m = 250. The examples
were generated using the uniform distribution over [âˆ’1, 1]Ã—[âˆ’1, 1]. The domain [âˆ’1, 1]Ã—
[âˆ’1, 1] was partitioned into four quarters of equal size: [âˆ’1, 0] Ã— [âˆ’1, 0], [âˆ’1, 0] Ã— ,
The algorithm for ï¬nding the optimal solution of the reduced quadratic program (Eq. (38)).
LEARNABILITY AND DESIGN OF OUTPUT CODES
A skeleton of the algorithm for ï¬nding a classiï¬er based on an output code by solving the quadratic
program deï¬ned in Eq. (29).
Run time comparison of two algorithms for code design using quadratic programming: Matlabâ€™s
standard QP package and the proposed algorithm (denoted SPOC). Note that we used a logarithmic scale for the
run-time (y) axis.
K. CRAMMER AND Y. SINGER
 Ã— [âˆ’1, 0], and Ã— . Each quarter was associated with a different label. For
each sample size we tested, we ran the algorithm three times, each run used a different
randomly generated training set. We compared the standard quadratic optimization routine
available from Matlab with our algorithm which was also implemented in Matlab. The
average running time results are shown in ï¬gure 5. Note that we used a log-scale for the y
(run-time) axis. The results show that the efï¬cient algorithm can be two orders of magnitude
faster than the standard QP package.
Generalization
In this section we analyze the generalization properties of the algorithm. For simplicity, we
give our analysis using the scalar product kernel K(x, y) = x Â· y, assuming Â¯h(Â¯x) = Â¯x. The
multiclass SVM version described in Eq. (13) now becomes,
subject to:
Â¯Myi Â· Â¯xi âˆ’Â¯Mr Â· Â¯xi â‰¥1 âˆ’Î´yi,r
Let M be a matrix of size k Ã— l over R which satisï¬es the constraints of Eq. (47). Each
row of the matrix M corresponds to a class y âˆˆY and the induced classiï¬er is given by
H(x) = arg max{ Â¯Mr Â· Â¯x}. Analogously, the matrix M induces a binary classiï¬er for each
pair of classes r and s as follows. We say that the label of an instance Â¯x is not r iff
Â¯Mr Â· Â¯x < Â¯Ms Â· Â¯x. Let Â¯wr,s be the vector Â¯Mr âˆ’Â¯Ms. Then, the resulting binary SVM classiï¬er
for distinguishing between classes r and s is
hr,s(Â¯x) = sign( Â¯wr,s Â· Â¯x),
where we interpret a positive prediction as a rejection of the label s.
We view the distance of a point Â¯x to the hyperplane Â¯wr,s Â· Â¯x = 0 as the conï¬dence in the
classiï¬cation of hr,s(Â¯x). This distance is given by the equation
d(Â¯x, Â¯wr,s) = | Â¯wr,s Â· Â¯x|
We now deï¬ne the margin Î³r,s of the classiï¬er hr,s(Â¯x) to be the minimum conï¬dence
achieved on any of the instances labeled r or s. That is,
(Â¯x,y)âˆˆS,yâˆˆ{r,s}{d(Â¯x, Â¯wr,s)}
(Â¯x,y)âˆˆS,yâˆˆ{r,s}
| Â¯wr,s Â· Â¯x|
Since the matrix M is a feasible solution for Eq. (47) then Â¯Mr Â· Â¯x âˆ’Â¯Ms Â· Â¯x â‰¥1 holds for
any instance labeled r, and similarly Â¯Ms Â· Â¯x âˆ’Â¯Mr Â· Â¯x â‰¥1 holds for any instance labeled s.
LEARNABILITY AND DESIGN OF OUTPUT CODES
Hence, we obtain that
âˆ€i s.t. yi âˆˆ{r, s}
| Â¯wr,s Â· Â¯x| â‰¥1,
which leads to the following lower bound on the margin,
We show in the sequel that minimization of the quantity D = 
Î³ 2r,s results in a small
generalization error. Using Eq. (49) we get that
âˆ¥Â¯Mr âˆ’Â¯Msâˆ¥2
Since it is easier to minimize the above bound on D, we rewrite the problem as follows,
âˆ¥Â¯Mr âˆ’Â¯Msâˆ¥2
subject to:
Â¯Myi Â· Â¯xi âˆ’Â¯Mr Â· Â¯xi â‰¥1 âˆ’Î´yi,r
We now show that this problem is equivalent to the optimization problem of Eq. (47), up to
additive and multiplicative constants.
To solve the primal problem of Eq. (47) we use the same technique described in Section
5.2.Formally,wesolvetheoptimizationproblembyï¬ndingasaddlepointoftheLagrangian:
L(M, Î·) = 1
âˆ¥Â¯Mr âˆ’Â¯Msâˆ¥2
Î·i,r[Â¯xi Â· Â¯Mr âˆ’Â¯xi Â· Â¯Myi + bi,r]
subject to:
The saddle point we are seeking is a minimum for the primal variables (M), and the
maximum for the dual variables (Î·). To ï¬nd the minimum for the primal variables we
Î·i,r Â¯xi âˆ’
+ (k âˆ’1) Â¯Mr âˆ’
Î·i,q âˆ’Î·i,r
Note that the k equations given in Eq. (53) are linearly dependent since by summing the
equation over all possible values for r we get,
K. CRAMMER AND Y. SINGER
Therefore, the above equations contain at least one degree of freedom. To prove that this is
the only degree of freedom we use the claim that for any two matrices A and B with the
same dimensions we have that,
rk(A âˆ’B) â‰¥rk(A) âˆ’rk(B).
To use the claim we deï¬ne two matrices of size k Ã— k. We set the matrix A to be a diagonal
matrix with values k on the diagonal, and the matrix B to be a matrix which all elements
are equal +1. Note that rk(A) = k and rk(B) = 1. Since we can write the above linear
equations (Eq. (53)) as a difference of A and B we can apply Eq. (54) and get that the rank
of the above system is at least k âˆ’1, and thus there is exactly one degree of freedom.
Let us represent the degree of freedom by setting k
r=1 Â¯Mr = Â¯c and obtain,
âˆ¥Â¯Mrâˆ¥2 + 2
Developing the objective function of Eq. (51) we get,
âˆ¥Â¯Mr âˆ’Â¯Msâˆ¥2 = 1
( Â¯Mr âˆ’Â¯Ms) Â· ( Â¯Mr âˆ’Â¯Ms)
Finally, substituting Eq. (55) into Eq. (56) we get,
âˆ¥Â¯Mr âˆ’Â¯Msâˆ¥2 = 1
2 (Â¯c Â· Â¯c)
Note that the resulting equation is a monotonically increasing linear transformation of the
objective function of Eq. (47). By setting Â¯c = 0 in Eq. (57) we maximize the value of the
LEARNABILITY AND DESIGN OF OUTPUT CODES
right hand side of the equation and obtain the following upper bound on the margin
To conclude, we have shown that solving the optimization problems given in Eqs. (47)
and (13) results in the minimization of the quantity D which, as we now show, is directly
related to the margin of the classiï¬er.
To relate the margin to the generalization error of the multiclass SVM we use the approach
proposed by Platt, Cristianini, and Shawe-Taylor for reducing multiclass problems
to multiple binary problems. Their method is also composed of two stages. In the training
stage the set of all ( k
2) binary SVM classiï¬ers is constructed, where each classiï¬er is trained
to distinguish between a pair of distinct labels. In the classiï¬cation stage the algorithm
maintains a list of all possible labels for a given test instance (initialized to the list of all
labels). The classiï¬cation algorithm runs in rounds: on each round it picks two labels from
the list above and applies the appropriate binary classiï¬er to distinguish between the two
labels. The label that was rejected by the binary classiï¬er is removed from the list. After
k âˆ’1 such rounds exactly one label remains in the list. This label is the prediction of the
multiclass classiï¬er. Platt, Cristianini, and Shawe-Taylor presented the the above
classiï¬cation scheme by a rooted binary directed acyclic graph (DAG) which they named
DDAG for decision DAG. The following theorem gives a bound on the generalization error
for DDAGs.
Theorem 4 .
Suppose we are able to classify
a random m sample S of labeled examples using a SVM DDAG on k classes containing
2) decision nodes (and k leaves) with margin Î³r,s at node {r, s}, then we can bound
the generalization error with probability greater than 1 âˆ’Î´ to be less than
Dâ€² log(4em)log(4m) + log 2(2m)K
where Dâ€² = 
Î³ 2r,s ,and R istheradiusofaballcontainingthesupportofthedistribution.
Plugging the binary classiï¬ers induced by a matrix M as given by Eq. (48) results a stepwise
method for calculating the maximum among { Â¯Mr Â· Â¯x}, and thus the prediction of the classiï¬er
H(x) = arg max{ Â¯Mr Â· Â¯x}. We now can use Theorem 4 from by combining it with the bound of Eq. (58).
Corollary 2.
Suppose we are able to classify a random m sample S of labeled examples
using a multiclass SVM with a matrix M, then we can bound the generalization error with
probability greater than 1 âˆ’Î´ to be less than
kâˆ¥Mâˆ¥2 log(4em)log(4m) + log 2(2m)K
where R is the radius of a ball containing the support of the distribution.
K. CRAMMER AND Y. SINGER
We use a second theorem from to bound the
generalization error for each possible label individually. Following Platt, Cristianini, and
Shawe-Taylor we deï¬ne the error in identifying class r as
Ïµr(M) = Pr {(Â¯x, y) : [y = r âˆ§H(Â¯x) Ì¸= r] âˆ¨[y Ì¸= r âˆ§H(Â¯x) = r]}
We use the same technique as above to to get a bound similar to Eq. (50). Based on Theorem 2
from we get the following corollary.
Corollary 3.
Suppose we are able to correctly distinguish some class r from the other
classes in a random sample S of m labeled examples using a multiclass SVM with a matrix
M, then with a probability greater than 1 âˆ’Î´ we have that,
Ïµr(M) â‰¤130R2
(kâˆ¥Â¯Mrâˆ¥2 + âˆ¥Mâˆ¥2) log(4em)log(4m) + log 2(2m)kâˆ’1
where R is the radius of a ball containing the support of the distribution.
Conclusions and future research
In this paper we investigated the problem of designing output codes for solving multiclass
problems. We ï¬rst discussed discrete codes and showed that while the problem is intractable
in general we can ï¬nd the ï¬rst column of a code matrix in polynomial time. The question
whether the algorithm can be generalized to l â‰¥2 columns with running time of O(2l) or
less remains open. Another closely related question is whether we can ï¬nd efï¬ciently the
next column given previous columns. Also left open for future research is further usage of
the algorithm for ï¬nding the ï¬rst column as a subroutine in constructing codes based on
trees or directed acyclic graphs , and as a tool for
incremental (column by column) construction of output codes.
Motivated by the intractability results for discrete codes we introduced the notion of
continuous output codes. We described three optimization problems for ï¬nding good continuous codes for a given a set of binary classiï¬ers. We have discussed in detail an efï¬cient
algorithm for one of the three problems which is based on quadratic programming. As a
special case, our framework also provides a new efï¬cient algorithm for multiclass Support
Vector Machines. The importance of this efï¬cient algorithm might prove to be crucial in
large classiï¬cation problems with many classes such as Kanji character recognition. We
also devised efï¬cient implementation of the algorithm. Finally, an important question which
we have tackled barely in this paper is the problem of interleaving the code design problem
with the learning of binary classiï¬ers. A viable direction in this domain is combining our
algorithm for continuous codes with the support vector machine algorithm.
LEARNABILITY AND DESIGN OF OUTPUT CODES
Appendix A:
Linear programming
Using the notation of Chvatal , given the linear program:
subject to:
ai jx j â‰¤bi (i âˆˆI)
ai jx j = bi (i âˆˆE)
x j â‰¥0 ( j âˆˆR) (0-Constraints)
( j âˆˆF) (Unconstrained variables),
its dual program is:
subject to:
ai j yi â‰¥c j ( j âˆˆR)
ai j yi = c j ( j âˆˆF)
yi â‰¥0 ( j âˆˆI) (0-Constraints)
( j âˆˆE) (Unconstrained variables)
Appendix B:
Description
Matrix code
Sample size
No. of classes (No. of rows in M)
No. of hypotheses (No. of columns in M)
Index of an example
Index of a class
Correct label (class)
Index of an hypothesis
Slack variables in optimization problem
K. CRAMMER AND Y. SINGER
Description
Dual variables in quadric problem
Ï„i,r = Î´yi ,r âˆ’Î·i,r
Coefï¬cient in reduced optimization problem
Coefï¬cient in reduced optimization problem
Coefï¬cient in reduced optimization problem
Acknowledgment
We would like to thank Rob Schapire for numerous helpful discussions, to Vladimir Vapnik
for his encouragement and support of this line of research, and to Nir Friedman and Ran
Bachrach for useful comments and suggestions. Thanks also to the anonymous referees for
their constructive comments.