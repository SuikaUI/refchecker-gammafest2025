Learning More with Less: Conditional PGGAN-based
Data Augmentation for Brain Metastases Detection Using
Highly-Rough Annotation on MR Images
Changhee Han1,2,3,4
Kohei Murao1,2
 
1Research Center for Medical Big Data,
National Institute of Informatics
Tokyo, Japan
Tomoyuki Noguchi2
Yusuke Kawata2
Fumiya Uchiyama2
2Department of Radiology, National Center for
Global Health and Medicine
Tokyo, Japan
Leonardo Rundo3
3Department of Radiology,
University of Cambridge
Cambridge, UK
Hideki Nakayama4
Shin’ichi Satoh1,4
4Graduate School of Information Science and Technology,
The University of Tokyo
Tokyo, Japan
Accurate Computer-Assisted Diagnosis, associated with proper data
wrangling, can alleviate the risk of overlooking the diagnosis in a
clinical environment. Towards this, as a Data Augmentation (DA)
technique, Generative Adversarial Networks (GANs) can synthesize
additional training data to handle the small/fragmented medical
imaging datasets collected from various scanners; those images
are realistic but completely different from the original ones, filling
the data lack in the real image distribution. However, we cannot
easily use them to locate disease areas, considering expert physicians’ expensive annotation cost. Therefore, this paper proposes
Conditional Progressive Growing of GANs (CPGGANs), incorporating highly-rough bounding box conditions incrementally into
PGGANs to place brain metastases at desired positions/sizes on
256×256 Magnetic Resonance (MR) images, for Convolutional Neural Network-based tumor detection; this first GAN-based medical
DA using automatic bounding box annotation improves the training
robustness. The results show that CPGGAN-based DA can boost
10% sensitivity in diagnosis with clinically acceptable additional
False Positives. Surprisingly, further tumor realism, achieved with
additional normal brain MR images for CPGGAN training, does not
contribute to detection performance, while even three physicians
cannot accurately distinguish them from the real ones in Visual
Turing Test.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
CIKM ’19, November 3–7, 2019, Beijing, China
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6976-3/19/11...$15.00
 
CCS CONCEPTS
• Computing methodologies Object detection; • Applied
computing Health informatics.
Generative Adversarial Networks, Medical Image Augmentation,
Conditional PGGANs, Brain Tumor Detection, MRI
ACM Reference Format:
Changhee Han1,2,3,4, Kohei Murao1,2, Tomoyuki Noguchi2, Yusuke Kawata2,
Fumiya Uchiyama2, Leonardo Rundo3, Hideki Nakayama4, and Shin’ichi
Satoh1,4. 2019. Learning More with Less: Conditional PGGAN-based Data
Augmentation for Brain Metastases Detection Using Highly-Rough Annotation on MR Images. In The 28th ACM International Conference on Information
and Knowledge Management (CIKM’19), November 3–7, 2019, Beijing, China.
ACM, New York, NY, USA, 9 pages. 
INTRODUCTION
Accurate Computer-Assisted Diagnosis (CAD) with high sensitivity can alleviate the risk of overlooking the diagnosis in a clinical
environment. Specifically, Convolutional Neural Networks (CNNs)
have revolutionized medical imaging, such as diabetic eye disease
diagnosis , mainly thanks to large-scale annotated training data.
However, obtaining such annotated medical big data is demanding; thus, better diagnosis requires intensive Data Augmentation
(DA) techniques, such as geometric/intensity transformations of
original images . Yet, those augmented images intrinsically
have a similar distribution to the original ones, leading to limited
performance improvement; in this context, Generative Adversarial
Network (GAN) -based DA can boost the performance by filling
the real image distribution uncovered by the original dataset, since
it generates realistic but completely new samples showing good
generalization ability; GANs achieved outstanding performance
in computer vision, including 21% performance improvement in
eye-gaze estimation .
Also in medical imaging, where the primary problem lies in
small and fragmented imaging datasets from various scanners ,
 
Original Brain
Tumor Detection
(Bounding Boxes)
(Conditional
Novel Images w/ Tumors
at Desired Positions/Sizes
Synthetic Brain
Figure 1: CPGGAN-based DA for better tumor detection:
our CPGGANs generates a number of realistic/diverse brain
MR images with tumors at desired positions/sizes based on
bounding boxes, and the object detector uses them as additional training data.
GAN-based DA performs effectively: researchers improved classification by augmentation with noise-to-image GANs (e.g., random
noise samples to diverse pathological images) and segmentation
with image-to-image GANs (e.g., a benign image with a pathologyconditioning image to a malignant one) . Such applications
include 256 × 256 brain Magnetic Resonance (MR) image generation for tumor/non-tumor classification . Nevertheless, unlike
bounding box-based object detection, simple classification cannot
locate disease areas and rigorous segmentation requires physicians’
expensive annotation.
So, how can we achieve high sensitivity in diagnosis using GANs
with minimum annotation cost, based on highly-rough and inconsistent bounding boxes? As an advanced data wrangling approach,
we aim to generate GAN-based realistic and diverse 256 × 256 brain
MR images with brain metastases at desired positions/sizes for accurate CNN-based tumor detection; this is clinically valuable for
better diagnosis, prognosis, and treatment, since brain metastases
are the most common intra-cranial tumors, getting prevalent as
oncological therapies improve cancer patients’ survival . Conventional GANs cannot generate realistic 256×256 whole brain MR
images conditioned on tumor positions/sizes under limited training
data/highly-rough annotation ; since noise-to-image GANs cannot directly be conditioned on an image describing desired objects,
we have to use image-to-image GANs (e.g., input both the conditioning image/random noise samples or the conditioning image
alone with dropout noises on a generator )—it results in
unrealistic high-resolution MR images with odd artifacts due to
the limited training data/rough annotation, tumor variations, and
strong consistency in brain anatomy, unless we also input a benign
image sacrificing image diversity.
Such a high-resolution whole image generation approach, not
involving Regions of Interest (ROIs) alone, however, could facilitate
detection because it provides more image details and most CNN
architectures adopt around 256 × 256 input pixels. Therefore, as a
conditional noise-to-image GAN not relying on an input benign
image, we propose Conditional Progressive Growing of GANs (CPG-
GANs), incorporating highly-rough bounding box conditions incrementally into PGGANs to naturally place tumors of random
shape at desired positions/sizes on MR images. Moreover, we evaluate the generated images’ realism via Visual Turing Test by
three expert physicians, and visualize the data distribution via the t-
Distributed Stochastic Neighbor Embedding (t-SNE) algorithm .
Using the synthetic images, our novel CPGGAN-based DA boosts
10% sensitivity in diagnosis with clinically acceptable additional
False Positives (FPs). Surprisingly, we confirm that further realistic
tumor appearance, judged by the physicians, does not contribute
to detection performance.
Research Questions. We mainly address two questions:
• PGGAN Conditioning: How can we modify PGGANs to
naturally place objects of random shape, unlike rigorous
segmentation, at desired positions/sizes based on highlyrough bounding box masks?
• Medical Data Augmentation: How can we balance the
number of real and additional synthetic training data to
achieve the best detection performance?
Contributions. Our main contributions are as follows:
• Conditional Image Generation: As the first bounding
box-based 256×256 whole pathological image generation approach, CPGGANs can generate realistic/diverse images with
objects naturally at desired positions/sizes; the generated
images can play a vital role in clinical oncology applications,
such as DA, data anonymization, and physician training.
• Misdiagnosis Prevention: This study allows us to achieve
high sensitivity in automatic CAD using small/fragmented
medical imaging datasets with minimum annotation efforts
based on highly-rough/inconsistent bounding boxes.
• Brain Metastases Detection: This first bounding box-based
brain metastases detection method successfully detects tumors exploiting CPGGAN-based DA.
GENERATIVE ADVERSARIAL NETWORKS
In terms of realism and diversity, GANs have shown great
promise in image generation through a two-player minimax
game. However, the two-player objective function triggers difficult training, accompanying artifacts and mode collapse when
generating high-resolution images, such as 256 × 256 ones ;
to tackle this, multi-stage generative training methods have been
proposed: AttnGAN uses attention-driven multi-stage refinement
for fine-grained text-to-image generation ; PGGANs adopts
incremental training procedures from low to high resolution for
generating a realistic image . Moreover, GAN-based 128 × 128
conditional image synthesis using a bounding box can control generated images’ local properties . GANs can typically generate
more realistic images than other common deep generative models,
such as variational autoencoders suffering from the injected
noise and imperfect reconstruction due to a single objective function ; thus, as a DA technique, most researchers chose GANs
for facilitating classification , object detection , and
segmentation to tackle the lack of training data.
This GAN-based DA trend especially applies to medical imaging for handling various types of small/fragmented datasets from
multiple scanners: researchers used noise-to-image GANs for improving classification on brain tumor/non-tumor MR and liver
lesion Computed Tomography (CT) images ; others used imageto-image GANs focusing on ROI (i.e., small pathological areas) for
improving segmentation on 3D brain tumor MR and 3D lung
nodule CT images .
However, to the best of our knowledge, our work is the first
GAN-based medical DA method using automatic bounding box
annotation, despite 2D bounding boxes’ cheap annotation cost compared with rigorous 3D segmentation. Moreover, unlike the ROI
DA work generating only pedestrians without the background for
pedestrian detection , this is the first GAN-based whole image augmentation approach including the background, relying on
bounding boxes, in computer vision. Along with classic transformations of real images, a completely different approach—generating
novel whole 256×256 brain MR images with tumors at desired positions/sizes using CPGGANs—may become a clinical breakthrough
in terms of annotation cost.
MATERIALS AND METHODS
Brain Metastases Dataset
As a new dataset for the first bounding box-based brain metastases detection, this paper uses a dataset of contrast-enhanced
T1-weighted (T1c) brain axial MR images, collected by the authors
(National Center for Global Health and Medicine, Tokyo, Japan) and
currently not publicly available for ethical restrictions; for robust
clinical applications, it contains 180 brain metastatic cancer cases
from multiple MRI scanners—those images differ in contrast, magnetic field strength (i.e., 1.5 T, 3.0 T), and matrix size (i.e., 190 × 224,
216 × 256, 256 × 256, 460 × 460 pixels). In the clinical practice, T1c
MRI is well-established in brain metastases detection thanks to
its high-contrast in the enhancing region. We also use additional
brain MR images from 193 normal subjects only for CPGGAN training, not in tumor detection, to confirm the effect of combining the
normal and pathological images for training.
CPGGAN-based Image Generation
Data Preparation For tumor detection, our whole brain metastases
dataset (180 patients) is divided into: (i) a training set (126 patients);
(ii) a validation set (18 patients); (iii) a test set (36 patients); only the
training set is used for GAN training to be fair. Our experimental
dataset consists of:
• Training set (2, 813 images/5, 963 bounding boxes);
• Validation set (337 images/616 bounding boxes);
• Test set (947 images/3, 094 bounding boxes).
Our training set is relatively small/fragmented for CNN-based
applications, considering that the same patient’s tumor slices could
convey very similar information. To confirm the effect of realism
and diversity—provided by combining PGGANs and bounding box
conditioning—on tumor detection, we compare the following GANs:
(i) CPGGANs trained only with the brain metastases images; (ii)
CPGGANs trained also with additional 16, 962 brain images from
193 normal subjects; (iii) Image-to-image GAN trained only with
256 × 256 Real tumor
256 × 256 Highly-Rough Annotation
32 × 32 Real Tumor Bbox
256 × 256 Real Non-tumor
Figure 2: Example real 256 × 256 MR images with highlyrough annotation used for GAN training and resized 32 × 32
tumor bounding boxes.
Discriminator
Latent Space
Latent Space
Training progresses
Latent Space
Latent Space
Figure 3: Proposed CPGGAN architecture for synthetic 256×
256 MR image generation with tumors at desired positions/sizes based on bounding boxes.
the brain metastases images. After skull-stripping on all images
with various resolution, remaining brain parts are cropped and resized to 256 × 256 pixels (i.e., a power of 2 for better GAN training).
As Fig. 2 shows, we lazily annotate tumors with highly-rough and
inconsistent bounding boxes to minimize expert physicians’ labor.
CPGGANs is a novel conditional noise-to-image training method
for GANs, incorporating highly-rough bounding box conditions incrementally into PGGANs , unlike conditional image-to-image
GANs requiring rigorous segmentation masks . The original
PGGANs exploits a progressively growing generator and discriminator: starting from low-resolution, newly-added layers model finegrained details as training progresses. As Fig. 3 shows, we further
condition the generator and discriminator to generate realistic and
diverse 256 × 256 brain MR images with tumors of random shape at
desired positions/sizes using only bounding boxes without an input
benign image under limited training data/highly-rough annotation.
Our modifications to the original PGGANs are as follows:
• Conditioning image: prepare a 256 × 256 black image (i.e.,
pixel value: 0) with white bounding boxes (i.e., pixel value:
255) describing tumor positions/sizes for attention;
• Generator input: resize the conditioning image to the previous generator’s output resolution/channel size and concatenate them (noise samples generate the first 4 × 4 images);
• Discriminator input: concatenate the conditioning image
with a real or synthetic image.
CPGGAN Implementation Details We use the CPGGAN architecture with the Wasserstein loss using gradient penalty :
[D( ˜y)] −
[D(y)] + λ E
where the discriminator D belongs to the set of 1-Lipschitz functions, Pr is the data distribution by the true data sample y, and Pд
is the model distribution by the synthetic sample ˜y generated from
the conditioning image noise samples using uniform distribution
in [−1, 1]. The last term is gradient penalty for the random sample
Training lasts for 3, 000, 000 steps with a batch size of 4 and
2.0×10−4 learning rate for the Adam optimizer . We flip the discriminator’s real/synthetic labels once in three times for robustness.
During testing, as tumor attention images, we use the annotation of
training images with a random combination of horizontal/vertical
flipping, width/height shift up to 10%, and zooming up to 10%; these
CPGGAN-generated images are used as additional training images
for tumor detection.
Image-to-image GAN is a conventional conditional GAN that generates brain MR images with tumors, concatenating a 256×256 conditioning image with noise samples for a generator input and concatenating the conditioning image with a real/synthetic image for a
discriminator input, respectively. It uses a U-Net-like generator
with 4 convolutional/deconvolutional layers in encoders/decoders
respectively with skip connections, along with a discriminator with
3 decoders. We apply batch normalization to both convolution
with LeakyReLU and deconvolution with ReLU. It follows the same
implementation details as for the CPGGANs.
YOLOv3-based Brain Metastases Detection
You Only Look Once v3 (YOLOv3) is a fast and accurate
CNN-based object detector: unlike conventional classifier-based
detectors, it divides the image into regions and predicts bounding
boxes/probabilities for each region. We adopt YOLOv3 to detect
brain metastases on MR images since its high efficiency can play
a clinical role in real-time tumor alert; moreover, it shows very
comparable results with 608 × 608 network resolution against other
state-of-the-art detectors, such as Faster RCNN .
To confirm the effect of GAN-based DA, the following detection results are compared: (i) 2, 813 real images without DA, (ii),
(iii), (iv) with 4, 000/8, 000/12, 000 CPGGAN-based DA, (v), (vi), (vii)
with 4, 000/8, 000/12, 000 CPGGAN-based DA, trained with additional normal brain images, (viii), (ix), (x) with 4, 000/8, 000/12, 000
image-to-image GAN-based DA. Due to the risk of overlooking
the diagnosis via medical imaging, higher sensitivity matters more
than less FPs; thus, we aim to achieve higher sensitivity with a
clinically acceptable number of FPs, adding the additional synthetic
training images. Since our annotation is highly-rough, we calculate
sensitivity/FPs per slice with both Intersection over Union (IoU)
threshold 0.5 and 0.25. For better DA, GAN-generated images with
unclear tumor appearance are manually discarded.
YOLOv3 Implementation Details We use the YOLOv3 architecture with Darknet-53 as a backbone classifier and sum squared
error between the predictions/ground truth as a loss:
(xi −ˆxi)2 + (yi −ˆyi)2
ij (Ci −ˆCi)2 + λnoobj
j=0 1noobj
(Ci −ˆCi)2
c ∈classes
(pi(c) −ˆpi(c))2
where xi,yi are the centroid location of an anchor box, wi,hi are
the width/height of the anchor,Ci is the Objectness (i.e., confidence
score of whether an object exists), and pi(c) is the classification
loss. Let S2 and B be the size of a feature map and the number of
anchor boxes, respectively. 1obj
is 1 when an object exists in cell i
and otherwise 0.
During training, we use a batch size of 64 and 1.0 × 10−3 learning rate for the Adam optimizer. The network resolution is set
to 416 × 416 pixels during training and 608 × 608 pixels during
validation/testing respectively to detect small tumors better. We
recalculate the anchors at each DA setup. As classic DA, geometric/intensity transformations are also applied to both real/synthetic
images during training to achieve the best performance. For testing, we pick the model with the best sensitivity on validation with
detection threshold 0.1%/IoU threshold 0.5 between 96, 000-240, 000
steps to avoid severe FPs while achieving high sensitivity.
Clinical Validation via Visual Turing Test
To quantitatively evaluate how realistic the CPGGAN-based synthetic images are, we supply, in random order, to three expert
physicians a random selection of 50 real and 50 synthetic brain
metastases images. They take four tests in ascending order: (i), (ii)
test 1, 2: real vs CPGGAN-generated resized 32 × 32 tumor bounding boxes, trained without/with additional normal brain images;
(iii), (iv) test 3, 4: real vs CPGGAN-generated 256 × 256 MR images,
trained without/with additional normal brain images.
Then, the physicians are asked to constantly classify them as
real/synthetic, if needed, zooming/rotating them, without previous
training stages revealing which is real/synthetic. Such Visual Turing
Table 1: YOLOv3 brain metastases detection results with/without DA, using bounding boxes with detection threshold 0.1%.
Sensitivity
FPs per slice
Sensitivity
FPs per slice
2,813 real images
+ 4,000 CPGGAN-based DA
+ 8,000 CPGGAN-based DA
+ 12,000 CPGGAN-based DA
+ 4,000 CPGGAN-based DA (+ normal)
+ 8,000 CPGGAN-based DA (+ normal)
+ 12,000 CPGGAN-based DA (+ normal)
+ 4,000 Image-to-Image GAN-based DA
+ 8,000 Image-to-Image GAN-based DA
+ 12,000 Image-to-Image GAN-based DA
Test can probe the human ability to identify attributes/relationships
in images, also in evaluating GAN-generated images’ appearance .
This similarly applies to medical images in a clinical environment,
wherein physicians’ specialty is critical .
Visualization via t-SNE
To visually analyze the distribution of real/synthetic images, we
use t-SNE on a random selection of:
• 500 real tumor images;
• 500 CPGGAN-generated tumor images;
• 500 CPGGAN-generated tumor images, trained with additional normal brain images.
We normalize the input images to .
T-SNE is a machine learning algorithm for dimensionality reduction to represent high-dimensional data into a lower-dimensional
(2D/3D) space. It non-linearly adapts to input data using perplexity
to balance between the data’s local and global aspects.
t-SNE Implementation Details We use t-SNE with a perplexity
of 100 for 1, 000 iterations to get a 2D representation.
This section shows how CPGGANs and image-to-image GAN generate brain MR images. The results include instances of synthetic images and their influence on tumor detection, along with CPGGANgenerated images’ evaluation via Visual Turing Test and t-SNE.
MR Images Generated by CPGGANs
Fig. 4 illustrates example GAN-generated images. CPGGANs successfully captures the T1c-specific texture and tumor appearance at
desired positions/sizes. Since we use highly-rough bounding boxes,
the synthetic tumor shape largely varies within the boxes. When
trained with additional normal brain images, it clearly maintains
the realism of the original images with less odd artifacts, including
tumor bounding boxes, which the additional images do not include.
However, as expected, image-to-image GAN, without progressive
growing, generates clearly unrealistic images without an input
benign image due to the limited training data/rough annotation.
256 × 256 CPGGAN-generated Tumor w/o Normal
32 × 32 CPGGAN-generated Tumor Bbox w/o Normal
256 × 256 CPGGAN-generated Tumor w/ Normal
32 × 32 CPGGAN-generated Tumor Bbox w/ Normal
256 × 256 Image-to-Image GAN-generated Tumor w/o Normal
32 × 32 Image-to-Image GAN-generated Tumor Bbox w/o Normal
Figure 4: Example synthetic 256 × 256 MR images and resized 32 × 32 tumor bounding boxes yielded by (a), (b) CPG-
GANs trained without/with additional normal brain images;
(c) image-to-image GAN trained without normal images.
Brain Metastases Detection Results
Table 1 shows the tumor detection results with/without GAN-based
DA. As expected, the sensitivity remarkably increases with the
additional synthetic training data while FPs per slice also increase.
Ground Truth
4k GAN+Normal 8k GAN+Normal 12k GAN+Normal
Figure 5: Example detection results obtained by the seven DA setups on four different images, compared against the ground
truth: (a) ground truth; (b) without CPGGAN-based DA; (c), (d), (e) with 4k/8k/12k CPGGAN-based DA; (f), (g), (h) with 4k/8k/12k
CPGGAN-based DA, trained with additional normal brain images. Red V symbols indicate the brain metastases undetected
without CPGGAN-based DA, but detected with 4k CPGGAN-based DA.
Adding more synthetic images generally leads to a higher amount
of FPs, also detecting blood vessels that are small/hyper-intense on
T1c MR images, very similarly to the enhanced tumor regions (i.e.,
the contrast agent perfuses throughout the blood vessels). However, surprisingly, adding only 4, 000 CPGGAN-generated images
achieves the best sensitivity improvement by 0.10 with IoU threshold 0.5 and by 0.08 with IoU threshold 0.25, probably due to the
real/synthetic training image balance—the improved training robustness achieves sensitivity 0.91 with moderate IoU threshold 0.25
despite our highly-rough bounding box annotation.
Fig. 5 also visually indicates that it can alleviate the risk of overlooking the tumor diagnosis with clinically acceptable FPs; in the
clinical routine, the bounding boxes, highly-overlapping around
tumors, only require a physician’s single check by switching on/off
transparent alpha-blended annotation on MR images. It should be
noted that we cannot increase FPs to achieve such high sensitivity without CPGGAN-based DA. Moreover, our results reveal that
further realism—associated with the additional normal brain images during training—does not contribute to detection performance,
possibly as the training focuses less on tumor generation. Imageto-image GAN-based DA just moderately facilitates detection with
less additional FPs, probably because the synthetic images have
a distribution far from the real ones and thus their influence on
detection is limited during testing.
Visual Turing Test Results
Table 2 shows the confusion matrix for the Visual Turing Test. The
expert physicians easily recognize 256×256 synthetic images due to
the lack of training data. However, when CPGGANs is trained with
additional normal brain images, the experts classify a considerable
number of synthetic tumor bounding boxes as real; it implies that
the additional normal images remarkably facilitate the realism of
both healthy and pathological brain parts while they do not include
abnormality; thus, CPGGANs might perform as a tool to train
medical students and radiology trainees when enough medical
images are unavailable, such as abnormalities at rare positions/sizes.
Such GAN applications are clinically prospective , considering
the expert physicians’ positive comments about the tumor realism.
T-SNE Results
As presented in Fig. 6, synthetic tumor bounding boxes have a moderately similar distribution to real ones, but they also fill the real
image distribution uncovered by the original dataset, implying their
effective DA performance; especially, the CPGGAN-generated images trained without normal brain images distribute wider than the
center-concentrating images trained with the normal brain images.
Meanwhile, real/synthetic whole brain images clearly distribute differently, due to the real MR images’ strong anatomical consistency
(Fig. 7). Considering the achieved high DA performance, the tumor
(i.e., ROI) realism/diversity matter more than the whole image realism/diversity, since YOLOv3 look at an image patch instead of a
whole image, similarly to most other CNN-based object detectors.
Table 2: Visual Turing Test results by three physicians for classifying real vs CPGGAN-generated images: (a), (b) Test 1, 2:
resized 32 × 32 tumor bounding boxes, trained without/with additional normal brain images; (c), (d) Test 3, 4: 256 × 256 MR
images, trained without/with normal brain images. Accuracy denotes the physicians’ successful classification ratio between
the real/synthetic images.
Real Selected as Real
Real as Synt
Synt as Real
Synt as Synt
Physician1
Physician2
Physician3
Physician1
Physician2
Physician3
Physician1
Physician2
Physician3
Physician1
Physician2
Physician3
Figure 6: T-SNE results with 500 32 × 32 resized tumor bounding box images per each category: (a) Real tumor images; (b),
(c) CPGGAN-generated tumor images, trained without/with
additional normal brain images.
CONCLUSION
Without relying on an input benign image, our CPGGANs can
generate realistic and diverse 256 × 256 MR images with brain
metastases of random shape, unlike rigorous segmentation, naturally at desired positions/sizes, and achieve high sensitivity in
tumor detection—even with small/fragmented training data from
multiple MRI scanners and lazy annotation using highly-rough
bounding boxes; in the context of intelligent data wrangling, this
attributes to the CPGGANs’ good generalization ability to incrementally synthesize conditional whole images with the real image
Figure 7: T-SNE results with 500 256×256 images per each category: (a) Real tumor images; (b), (c) CPGGAN-generated tumor images, trained without/with additional normal brain
distribution unfilled by the original dataset, improving the training
robustness.
We confirm that the realism and diversity of the generated images, judged by three expert physicians via Visual Turing Test,
do not imply better detection performance; as the t-SNE results
show, the CPGGAN-generated images, trained with additional
non-tumor normal images, lack diversity probably because the
training less focuses on tumors. Moreover, we notice that adding
over-sufficient synthetic images leads to more FPs, but not always
higher sensitivity, possibly due to the training data imbalance
between real and synthetic images; as the t-SNE results reveal,
the CPGGAN-generated tumor bonding boxes have a moderately
similar—mutually complementary—distribution to the real ones;
thus, GAN-overwhelming training images may decrease the necessary influence of the real samples and harm training, rather than
providing robustness. Lastly, image-to-image GAN-based DA just
moderately facilitates detection with less additional FPs, probably due to the lack of realism. However, further investigations are
needed to maximize the effect of the CPGGAN-based medical image
augmentation.
For example, we could verify the effect of further realism in
return for less diversity by combining ℓ1 loss with the Wasserstein
loss using gradient penalty for GAN training. We can also combine
those CPGGAN-generated images, trained without/with additional
brain images, similarly to ensemble learning . Lastly, we plan to
define a new GAN loss function that directly optimizes the detection
results, instead of realism, similarly to the three-player GAN for
optimizing classification results .
Overall, minimizing expert physicians’ annotation efforts, our
novel CPGGAN-based DA approach sheds light on diagnostic and
prognostic medical applications, not limited to brain metastases
detection; future studies, especially on 3D bounding box detection
with highly-rough annotation, are required to extend our promising
results. Along with the DA, the CPGGANs has other potential clinical applications in oncology: (i) A data anonymization tool to share
patients’ data outside their institution for training while preserving
detection performance. Such a GAN-based application is reported
in Shin et al. ; (ii) A physician training tool to display random
synthetic medical images with abnormalities at both common and
rare positions/sizes, by training CPGGANs on highly unbalanced
medical datasets (i.e., limited pathological and abundant normal
samples, respectively). It can help train medical students and radiology trainees despite infrastructural and legal constraints .
ACKNOWLEDGMENTS
This research was supported by AMED Grant Number JP18lk1010028.