A review and comparison of strategies for multi-step ahead time series
forecasting based on the NN5 forecasting competition
Souhaib Ben Taieba, Gianluca Bontempia, Amir Atiyac, Antti Sorjamaab
aMachine Learning Group, D´epartement d’Informatique, Facult´e des Sciences, Universit´e Libre de Bruxelles,
bEnvironmental and Industrial Machine Learning Group, Adaptive Informatics Research Centre, Altoo University
School of Science, Finland
cFaculty of Engineering, Cairo University, Giza, Egypt
Multi-step ahead forecasting is still an open challenge in time series forecasting. Several approaches
that deal with this complex problem have been proposed in the literature but an extensive comparison on a large number of tasks is still missing. This paper aims to ﬁll this gap by reviewing existing
strategies for multi-step ahead forecasting and comparing them in theoretical and practical terms.
To attain such an objective, we performed a large scale comparison of these diﬀerent strategies using a large experimental benchmark (namely the 111 series from the NN5 forecasting competition).
In addition, we considered the eﬀects of deseasonalization, input variable selection, and forecast
combination on these strategies and on multi-step ahead forecasting at large. The following three
ﬁndings appear to be consistently supported by the experimental results: Multiple-Output strategies are the best performing approaches, deseasonalization leads to uniformly improved forecast
accuracy, and input selection is more eﬀective when performed in conjunction with deseasonalization.
Time series forecasting, Multi-step ahead forecasting, Long-term forecasting,
Strategies of forecasting,, Machine Learning, Lazy Learning, NN5 forecasting competition,
Friedman test.
 
11 february 2011
 
Introduction
Strategies for Multi-Step-Ahead Time Series Forecasting
Recursive strategy
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Direct strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
DirRec strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
MIMO strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
DIRMO strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Comparative Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Lazy Learning for Time Series Forecasting
Global vs local modeling for supervised learning . . . . . . . . . . . . . . . . . . . .
The Lazy Learning algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Single-Output Lazy Learning algorithm
. . . . . . . . . . . . . . . . . . . . . . . .
Multiple-Output Lazy Learning algorithm . . . . . . . . . . . . . . . . . . . . . . .
Model selection or model averaging . . . . . . . . . . . . . . . . . . . . . . . . . . .
Experimental Setup
Time Series Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Methodology
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Compared forecasting strategies . . . . . . . . . . . . . . . . . . . . . .
Forecasting performance evaluation . . . . . . . . . . . . . . . . . . . . . . .
Experimental phases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Pre-competition phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Competition phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Results and discussion
Pre-competition results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Competition results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conclusion
1. Introduction
Time series forecasting is a growing ﬁeld of interest playing an important role in nearly all ﬁelds
of science and engineering, such as economics, ﬁnance, meteorology and telecommunication . Unlike one-step ahead forecasting, multi-step ahead forecasting tasks are more
diﬃcult , since they have to deal with various additional complications, like
accumulation of errors, reduced accuracy, and increased uncertainty .
The forecasting domain has been inﬂuenced, for a long time, by linear statistical methods such
as ARIMA models. However, in the late 1970s and early 1980s, it became increasingly clear that
linear models are not adapted to many real applications . In the same
period, several useful nonlinear time series models were proposed such as the bilinear model , the threshold autoregressive model 
and the autoregressive conditional heteroscedastic (ARCH) model and for a review). Nowadays, Monte Carlo simulation
or bootstrapping methods are used to compute nonlinear forecasts. Since no assumptions are made
about the distribution of the error process, the latter approach is preferred . However, the study of nonlinear time series analysis and forecasting
is still in its infancy compared to the development of linear time series . These models, also called black-box or
data-driven models , are examples of nonparametric nonlinear models which use
only historical data to learn the stochastic dependency between the past and the future. For instance, Werbos found that Artiﬁcial Neural Networks (ANNs) outperforms the classical statistical
methods such as linear regression and Box-Jenkins approaches . A similar
study has been conducted by Lapedes and Farber who conclude that
ANNs can be successfully used for modeling and forecasting nonlinear time series. Later, others
models appeared such as decision trees, support vector machines and nearest neighbor regression . Moreover, the empirical accuracy of several machine
learning models has been explored in a number of forecasting competitions under diﬀerent data
conditions ) creating interesting scientiﬁc debates in the area of data mining and forecasting .
In the forecasting community, researchers have paid attention to several aspects of the forecasting procedure such as model selection , eﬀect of deseasonalization , forecasts combination and many other critical topics . However, approaches for generating multi-step ahead forecasts for machine learning
models did not receive as much attention, as pointed out by Kline: “One issue that has had limited
investigation is how to generate multiple-step-ahead forecasts” .
To the best of our knowledge, ﬁve alternatives (or strategies) have been proposed in the literature to tackle an H-step ahead forecasting task. The Recursive strategy iterates, H times, a one-step ahead forecasting model to obtain the H forecasts.
estimating the future series value, it is fed back as an input for the following forecast.
In contrast to the previous strategy which use a single model, the Direct strategy estimates a set of H forecasting models, each returning a forecast for the
i-th value (i ∈{1, . . . , H}).
A combination of the two previous strategies, called DirRec strategy has been proposed in . The idea behind this strategy is to combine aspects from both, the
Direct and the Recursive strategies. In other words, a diﬀerent model is used at each step but the
approximations from previous steps are introduced into the input set.
In order to preserve, between the predicted values, the stochastic dependency characterizing
the time series, the Multi-Input Multi-Output (MIMO) strategy has been introduced and analyzed
in . Unlike the previous strategies
where the models return a scalar value, the MIMO strategy returns a vector of future values in a
single step.
The last strategy, called DIRMO , aims to preserve the most appealing
aspects of both the DIRect and miMO strategies. This strategy aims to ﬁnd a trade-oﬀbetween the
property of preserving the stochastic dependency between the forecasted values and the ﬂexibility
of the modeling procedure.
In the literature, these ﬁve forecasting strategies have been presented separately, sometimes,
using diﬀerent terminologies. The ﬁrst contribution of this paper is to present a thorough uniﬁed
review as well as a theoretical comparative analysis of the existing strategies for multi-step ahead
forecasting.
Despite the fact that many studies have compared between the diﬀerent multi-step ahead
approaches, the collective outcome of these studies regarding forecasting performance has been
inconclusive. So the modeler is still left with little guidance as to which strategy to use. For example, research from provide experimental evidence in
favor of Recursive strategy against Direct strategy. However, results from support the fact that the Direct strategy is
better than the Recursive strategy. The work by shows that the
DirRec strategy gives better performance than Direct and Recursive strategies. The Direct and
Recursive strategies have been theoretically and empirically compared in . In
this study the authors obtained theoretical and experimental evidence in favor of Direct strategy.
Concerning the MIMO strategy, Kline and Cheng et al support
the idea that MIMO strategy provides worse forecasting performance than Recursive and Direct
strategies. However, in , the comparison between
MIMO, Recursive, and Direct was in favor of MIMO. Finally, show
that the DIRMO strategy gives better forecasting results than Direct and MIMO strategies when
the parameter controlling the degree of dependency between forecasts is correctly identiﬁed. These
previous comparisons have been performed with diﬀerent datasets in diﬀerent conﬁgurations using diﬀerent forecasting methods, such as Multiple Linear Regression, Artiﬁcial Neural Networks,
Hidden Markov Models and Nearest Neighbors.
All the contradictory ﬁndings of these studies make it all the more necessary to investigate
further to ﬁnd the truth concerning the relative performance of these strategies. The second contribution of this paper is an experimental comparison of the diﬀerent multi-step ahead forecasting
strategies on the 111 time series of the NN5 international forecasting competition benchmark.
These time series pose some of the realistic problems that one usually encounters in a typical
multi-step ahead forecasting task, for example the existence of several times series of possibly related dynamics, outliers, missing values, and multiple overlying seasonalities. This experimental
comparison is performed for a variety of diﬀerent conﬁgurations (regarding seasonality, input selection and combination), in order to have the comparison as encompassing as can be. In addition,
the methodology used for this experimental comparison is based on the guidelines and recommendations advocated in some of the methodological papers ) but rather to show for a given
learning algorithm, how the choice of the forecasting strategy can sensibly inﬂuence the performance
of the multi-step ahead forecasts. In this work, we adopted the Lazy Learning algorithm , a particular instance of local learning, which has been successfully applied to many real-world
forecasting tasks .
Last but not least, the paper proposes also a Lazy Learning entry to the NN5 forecasting competition (Crone, b). The goal is to assess how this model fares compared to the other computational
intelligence models that were proposed for the competition . This
will give us an idea about the potential of this approach.
The paper is organized as follows. The next section presents a review of the diﬀerent forecasting
strategies. Section 3 describes the Lazy Learning model and the associated algorithms for the
diﬀerent forecasting strategies. Section 4 gives a detailed presentation of the datasets and the
methodology applied for the experimental comparison. Section 5 presents the results and discusses
them. Finally, Section 6 gives a summary and concludes the work.
2. Strategies for Multi-Step-Ahead Time Series Forecasting
A multi-step ahead (also called long-term) time series forecasting task consists of predicting the
next H values [yN+1, . . . , yN+H] of a historical time series [y1, . . . , yN] composed of N observations,
where H > 1 denotes the forecasting horizon.
This section will ﬁrst give a presentation of the ﬁve forecasting strategies and next, a subsection
will be devoted to a comparative analysis of these strategies in terms of number and types of models
to learn as well as forecasting properties.
We will use a common notation where f and F denote the functional dependency between past
and future observations, d refers to the embedding dimension of the time
series, that is the number of past values used to predict future values and w represents the term
that includes modeling error, disturbances and/or noise.
2.1. Recursive strategy
The oldest and most intuitive forecasting strategy is the Recursive (also called Iterated or
Multi-Stage) strategy . In this strategy, a single model f is
trained to perform a one-step ahead forecast, i.e.
yt+1 = f(yt, . . . , yt−d+1) + w,
with t ∈{d, . . . , N −1}.
When forecasting H steps ahead, we ﬁrst forecast the ﬁrst step by applying the model. Subsequently, we use the value just forecasted as part of the input variables for forecasting the next
step (using the same one-step ahead model). We continue in this manner until we have forecasted
the entire horizon.
Let the trained one-step ahead model be ˆf. Then the forecasts are given by:
ˆf(yN, . . . , yN−d+1)
ˆf(ˆyN+h−1, . . . , ˆyN+1, yN, . . . , yN−d+h)
if h ∈{2, . . . , d}
ˆf(ˆyN+h−1, . . . , ˆyN+h−d)
if h ∈{d + 1, . . . , H}
Depending on the noise present in the time series and the forecasting horizon, the recursive
strategy may suﬀer from low performance in multi-step ahead forecasting tasks. Indeed, this is
especially true if the forecasting horizon h exceeds the embedding dimension d, as at some point
all the inputs are forecasted values instead of actual observations (Equation 2). The reason for the
potential inaccuracy is that the Recursive strategy is sensitive to the accumulation of errors with
the forecasting horizon. Errors present in intermediate forecasts will propagate forward as these
forecasts are used to determine subsequent forecasts.
In spite of these limitations, the Recursive strategy has been successfully used to forecast
many real-world time series by using diﬀerent machine learning models, like recurrent neural networks and nearest-neighbors .
2.2. Direct strategy
The Direct (also called Independent) strategy consists of forecasting each horizon independently from the others. In other terms, H models fh are learned (one
for each horizon) from the time series [y1, . . . , yN] where
yt+h = fh(yt, . . . , yt−d+1) + w,
with t ∈{d, . . . , N −H} and h ∈{1, . . . , H}.
The forecasts are obtained by using the H learned models ˆfh as follows:
ˆyN+h = ˆfh(yN, . . . , yN−d+1).
This implies that the Direct strategy does not use any approximated values to compute the
forecasts (Equation 4), being then immune to the accumulation of errors. However, the H models
are learned independently inducing a conditional independence of the H forecasts. This aﬀects the
forecasting accuracy as it prevents the strategy from considering complex dependencies between
the variables ˆyN+h . For example
consider a case where the best forecast is a linear or mildly nonlinear trend. The direct method
could yield a broken curve because of the “uncooperative” way the H forecasts are generated. Also,
this strategy demands a large computational time since there are as many models to learn as the
size of the horizon.
Diﬀerent machine learning models have been used to implement the Direct strategy for multistep ahead forecasting tasks, for instance neural networks , nearest neighbors and decision trees .
2.3. DirRec strategy
The DirRec strategy combines the architectures and the principles underlying the Direct and the Recursive strategies. DirRec computes the forecasts with
diﬀerent models for every horizon (like the Direct strategy) and, at each time step, it enlarges
the set of inputs by adding variables corresponding to the forecasts of the previous step (like the
Recursive strategy). However, note that unlike the two previous strategies, the embedding size d
is not the same for all the horizons. In other terms, the DirRec strategy learns H models fh from
the time series [y1, . . . , yN] where
yt+h = fh(yt+h−1, . . . , yt−d+1) + w,
with t ∈{d, . . . , N −H} and h ∈{1, . . . , H}.
To obtain the forecasts, the H learned models are used as follows:
ˆfh(yN, . . . , yN−d+1)
ˆfh(ˆyN+h−1, . . . , ˆyN+1, yN, . . . , yN−d+1)
if h ∈{2, . . . , H}
This strategy outperformed the Direct and the Recursive strategies on two real-world time
series: Santa Fe and Poland Electricity Load data sets .
research has been done regarding this strategy, so there is a need for further evaluation.
2.4. MIMO strategy
The three previous strategies (Recursive, Direct and DirRec) may be considered as Single-
Output strategies since they model the data as a (multiple-input) singleoutput function (see Equations 2, 4 and 6).
The introduction of the Multi-Input Multi-Output (MIMO) strategy ) has been motivated by the
need to avoid the modeling of single-output mapping, which neglects the existence of stochastic dependencies between future values and consequently aﬀects the forecast accuracy .
The MIMO strategy learns one multiple-output model F from the time series [y1, . . . , yN] where
[yt+H, . . . , yt+1] = F(yt, . . . , yt−d+1) + w,
with t ∈{d, . . . , N −H}, F : Rd →RH is a vector-valued function ,
and w ∈RH is a noise vector with a covariance that is not necessarily diagonal .
The forecasts are returned in one step by a multiple-output model ˆF where
[ˆyt+H, . . . , ˆyt+1] = ˆF(yN, . . . , yN−d+1).
The rationale of the MIMO strategy is to preserve, between the predicted values, the stochastic
dependency characterizing the time series.
This strategy avoids the conditional independence
assumption made by the Direct strategy as well as the accumulation of errors from which plagues
the Recursive strategy. So far, this strategy has been successfully applied to several real-world
multi-step ahead time series forecasting tasks .
However, the need to preserve the stochastic dependencies by using one model has a drawback
as it constrains all the horizons to be forecasted with the same model structure. This constraint
could reduce the ﬂexibility of the forecasting approach . This was the
motivation for the introduction of a new multiple-output strategy: DIRMO , presented next.
2.5. DIRMO strategy
The DIRMO strategy aims to preserve the most appealing aspects of
both the DIRect and miMO strategies. Taking a middle approach, DIRMO forecasts the horizon H
in blocks, where each block is forecasted in a MIMO fashion. Thus, the H-step-ahead forecasting
task is decomposed into n multiple-output forecasting tasks (n = H
s ), each with an output of size
s (s ∈{1, . . . , H}).
When the value of the parameter s is 1, the number of forecasting tasks n is equal to H
which correspond to the Direct strategy. When the value of the parameter s is H, the number of
forecasting tasks n is equal to 1 which correspond to the MIMO strategy. There are intermediate
conﬁgurations between these two extremes depending on the value of a parameter s.
The tuning of the parameter s allows us to improve the ﬂexibility of the MIMO strategy by
calibrating the dimensionality of the outputs (no dependency in the case s = 1 and maximal
dependency for s = H). This provides a beneﬁcial trade oﬀbetween the preserving a larger degree
of the stochastic dependency between future values and having a greater ﬂexibility of the predictor.
The DIRMO strategy, previously called MISMO strategy (renamed for
clarity reason), learns n models Fp from the time series [y1, . . . , yN] where
[yt+p∗s, . . . , yt+(p−1)∗s+1] = Fp(yt, . . . , yt−d+1) + w,
with t ∈{d, . . . , N −H}, p ∈{1, . . . , n} and Fp : Rd →Rs is a vector-valued function if s > 1.
The H forecasts are returned by the n learned models as follows:
[ˆyN+p∗s, . . . , ˆyN+(p−1)∗s+1] = ˆFp(yN, . . . , yN−d+1).
The DIRMO strategy has been successfully applied to two forecasting competitions: ESTSP’07 and NN3 .
2.6. Comparative Analysis
To summarize, there are ﬁve possible forecasting strategies that perform a multi-step ahead
forecasting task: Recursive, Direct, DirRec, MIMO and DIRMO strategies. Figure 1 shows the
diﬀerent forecasting strategies with links indicating their relationships.
Figure 1: The diﬀerent forecasting strategies with the links showing their relationship.
As we see, the DirRec strategy is a combination of the Direct and the Recursive strategy, while
the DIRMO strategy is a combination of the Direct and the MIMO strategy.
Contingent on the selected strategy, a diﬀerent number and type of models will be required.
Before presenting the general comparison of the multi-step ahead forecasting strategies, let us
highlight using an example the diﬀerences between the forecasting strategies.
Consider a multi-step ahead forecasting task for the time series [y1, . . . , yN] where the forecasting
horizon H is 4. Table 1 shows, for each strategy, the diﬀerent input sets and forecasting models
involved in the calculation of the four forecasts [ˆyN+1, . . . , ˆyN+4].
ˆf(yN,...,yN−d+1)
ˆf(ˆyN+1,yN,...,yN−d+2)
ˆf(ˆyN+2,ˆyN+1,...,yN−d+3)
ˆf(ˆyN+3,ˆyN+2,...,yN−d+4)
ˆf1(yN,...,yN−d+1)
ˆf2(yN,...,yN−d+1)
ˆf3(yN,...,yN−d+1)
ˆf4(yN,...,yN−d+1)
ˆf1(yN,...,yN−d+1)
ˆf2(ˆyN+1,yN,...,yN−d+1)
ˆf3(ˆyN+2,ˆyN+1,...,yN−d+1)
ˆf4(ˆyN+3,ˆyN+2,...,yN−d+1)
ˆF(yN,...,yN−d+1)
DIRMO (s = 2)
ˆF1(yN,...,yN−d+1)
ˆF2(yN,...,yN−d+1)
Table 1: The diﬀerent forecasting models used by each strategy to obtain the 4 forecasts needed.
Let TSO and TMO denote the amount of computational time needed to learn (with a given
learning algorithm) a Single-Output model and a Multiple-Output model, respectively.
given H-step ahead forecasting task, we can see in Table 2 for each strategy the number and type
of models to learn, the size of the output for each model as well as the computational time.
Number of Models
Types of models
Size of output
Computational time
H × (TSO + µ)
Table 2: For each forecasting strategy: the number and type of models (Single-Output or Multiple-
Output) to learn and the size of the output for each model.
Suppose TMO = TSO + δ, which is a reasonable assumption because learning a model with a
vector-valued output takes more time than learning a model with a single-valued output. This
allows us to rank the forecasting strategies according to their computation time for training given
in Table 2. Indeed, we have
< H × (TSO + µ)
where we suppose that the parameter s of DIRMO is not equal to 1 or H.
Note, in one hand, that the time needed to learn a SO model of the DirRec strategy equals
TSO + µ because the input size of each SO task increases at each step. On the other hand, if we
need to select the value of the parameter s by on some tuning, the DIRMO strategy will take more
time and hence will be the slowest one.
In the following, we conclude this section by summarizing the pros and cons of the ﬁve forecasting strategies as depicted on Table 3.
Computational time needed
Suitable for noise-free
time series (e.g.
Accumulation of errors
No accumulation of
Conditional
independence
assumption
Trade-oﬀbetween
Direct and Recursive
Input set grows linearly
No conditional
independence
assumption
Reduced ﬂexibility:
same model structure
for all the horizons
Trade-oﬀbetween total
dependence and total
independence of
One additional
parameter to estimate
Table 3: A Summary of the Pros and Cons of the Diﬀerent Multi-step Forecasting Strategies
3. Lazy Learning for Time Series Forecasting
Each of the forecasting strategies introduced in Section 2 demands the deﬁnition of a speciﬁc
forecasting model or learning algorithm to estimate either the scalar-valued function f (see Equations 1, 3 and 5) or the vector-valued function F (see Equations 7 and 9) which represent the
temporal stochastic dependencies. As the goal of the paper is not to compare forecasting models ) but rather multi-step ahead forecasting strategies, the choice of a
underlying forecasting model is required to setup the experiments. In this paper, we adopted the
Lazy Learning algorithm, which is a particular instance of local learning models, since it has been
shown to be particularly eﬀective in time series forecasting tasks .
Next section gives a general comparison of global models with local models. Section 3.2 presents
the Lazy Learning Algorithm in terms of learning properties. Section 3.3 and 3.4 describe two Lazy
Learning algorithms for two types of learning tasks, namely the Single-Output and Multiple-Output
Lazy Learning algorithms. Finally, the a discussion is presented on the model combination.
3.1. Global vs local modeling for supervised learning
Forecasting the future values of a time series using past observations can be reduced to a
supervised learning problem or, more precisely, to a regression problem. Indeed, the time series
can be seen as a dataset made of pairs where the ﬁrst component, called input, is a past temporal
pattern and the second, called output, is the corresponding future pattern. Being able to predict
the unknown output for a given input is equivalent to forecasting the future values given the last
observations of the time series.
Global modeling is the typical approach to the supervised learning problem. Global models
are parametric models that describe the relationship between the inputs and the output values
as an analytical function over the whole input domain.
Examples of global models are linear
models , nonlinear statistical regressions and
Neural Networks .
Another approach is the divide-and-conquer modeling which consists in relaxing the global
modeling assumptions by dividing a complex problem into simpler problems, whose solutions can
be combined to yield a solution to the original problem . The divide-and-conquer
has evolved in two diﬀerent paradigms: the modular architectures and the local modeling approach .
Modular techniques replace a global model with diﬀerent modules covering diﬀerent parts of the
input space. Examples based on this approach are Fuzzy Inference Systems , Radial Basis Functions , Local Model
Networks , Trees and Mixture of Experts . The modular approach is in the intermediate scale between the two extremes, the
global and the local approach. However, their identiﬁcation is still performed on the basis of the
whole dataset and requires the same procedures used for generic global models.
Local modeling techniques are at the extreme end of divide-and-conquer methods. They are
nonparametric models that combine excellent theoretical properties with a simple and ﬂexible
learning procedure. Indeed, they do not aim to return a complete description of the input/output
mapping but rather to approximate the function in a neighborhood of the point to be predicted (also
called the query point). There are diﬀerent examples of local models, for example nearest neighbor,
weighted average, and locally weighted regression . Each of these models use
data points near the point to be predicted for estimating the unknown output. Nearest neighbor
models simply ﬁnd the closest point and uses its output value. Weighted average models combines
the closest points by averaging them with weights inversely proportional to their distance to the
point to be predicted. Locally weighted regression models ﬁt a model to nearby points with a
weighted regression where the weights are function of distances to the query point.
The eﬀectiveness of local models is well-known in the time series and computational intelligence
community. For example, the method proposed by Sauer gave good performance and
ranked second best forecast for the Santa Fe A dataset from a forecasting competition organized
by Santa Fe institute. Moreover, the two top-ranked entries of the KULeuven competition used
local learning methods .
In this work, we will restrict to consider a particular instance of local modeling algorithms: the
Lazy Learning algorithm .
3.2. The Lazy Learning algorithm
It is possible to encounter diﬀerent degree of “laziness” in local learning algorithms. For instance, a k Nearest Neighbor (k-NN) algorithm, which learns the best value of k before the query is
requested, is hardly a lazy approach since, after the query is presented, it requires only a reduced
amount of learning procedure, only the computation of the neighbors and the average. On the
contrary, a local method, which depends on the query to select the number of neighbors or other
structural parameters presents a higher degree of “laziness”.
The Lazy Learning (LL) algorithm, extensively discussed in , is a query-based local modeling technique where the whole learning procedure
is deferred until a forecast is required. When the query is requested, the learning procedure may
start to select the best value of the number of neighbors (or other structural parameters) and next,
the dataset is searched for the nearest neighbors of the query point. The nearest neighbors are
then used for estimating a local model, which returns a forecast. The local model is then discarded
and the procedure is repeated from scratch for subsequent queries.
The LL algorithm has a number of attractive features , namely, the reduced number
of assumptions, the online learning capability and the capacity to model nonstationarity.
assumes no a priori knowledge on the process underlying the data, which is particularly relevant in
real datasets. These considerations motivate the adoption of the LL algorithm as a learning model
in a multi-step ahead forecasting context.
Local modeling techniques require the deﬁnition of a set of model parameters namely the number k of neighbors, the kernel function, the parametric family and the distance metric[REF]. In
the literature, diﬀerent methods have been proposed to select automatically the adequate conﬁguration . However, in this paper, we will limit the
search on only the selection of the number of neighbors (also called or equivalent to the bandwidth
selection). This is essentially the most critical parameter, as it controls the bias/variance trade-oﬀ.
Bandwidth selection is usually performed by rule-of-thumb techniques ,
plug-in methods or cross-validation strategies . Concerning the other parameters, we use the tricubic kernel as kernel function,
a constant model for the parametric family and the euclidean distance as metric.
Note that in order to apply local learning to a time series, we need to embed it into a dataset
made of pairs where the ﬁrst component is a temporal pattern of length d and the second component
is either the future value (in the case of Single-Output Modeling) or the consecutive temporal
pattern of length H (in the case of Multiple-Output Modeling). In the following sections, D will
refer to the embedded time series with M input/output pairs.
3.3. Single-Output Lazy Learning algorithm
In the case of Single-Output learning (i.e with scalar output), the Lazy Learning procedure
consists of a sequence of steps detailed in Algorithm 1. The algorithm assesses the generalization
performance of diﬀerent local models and compares them in order to select the best one in terms of
generalization capability. To perform that, the algorithm associate a Leave-One-Out (LOO) error
eLOO(k) to the estimation y(k)
obtained with k neighbors (lines 4 and 5).
The LOO error can provide a reliable estimate of the generalization capability. However the
disadvantage of such an approach is that it requires to repeat k times the training process, which
means a large computational eﬀort. Fortunately, in the case of linear models there exists a powerful
statistical procedure to compute the LOO cross-validation measure at a reduced computational
cost: the PRESS (Prediction Sum of Squares) statistic .
In case of constant model, the LOO error eLOO(k) for the estimation y(k)
of the query point
xq is calculated as follows :
eLOO(k) = 1
where ej(k) designates the error obtained by setting aside the j-th neighbor of xq (j ∈{1, . . . , k}).
If we deﬁne the output of the k closest neighbors of xq as {y , . . . , y[k]} then, ej(k) is deﬁned as
ej(k) = y[j] −
i=1(i̸=j) y[i]
(k −1)y[j] −Pk
i=1(i̸=j) y[i]
(k −1)y[j] + y[j] −y[j] −Pk
i=1(i̸=j) y[i]
= ky[j] −Pk
= ky[j] −y(k)
Note that if we use Equation 13 to calculate the LOO error (Equation 12), the training process
is repeated k times since the sum in Equation 13 is performed for each index j. However, by
using the PRESS statistic (Equation 17), we avoid this large computational eﬀort since the sum is
replaced by the previously computed y(k)
which was already calculated. This makes the PRESS
statistic an eﬃcient method to compute the LOO error.
After evaluating the performance of local models with diﬀerent number of neighbors k (lines 3
to 6), the best one which minimizes the LOO error (having index k∗) is selected (lines 7 and 8).
Finally, the prediction of the output of xq is returned (line 9).
Algorithm 1: Single-Output Lazy Learning
: D = {(xi, yi) ∈(Rd × R) with i ∈{1, . . . , M}}, dataset.
: xq ∈Rd, query point.
: Kmax, the maximum number of neighbors.
Output: ˆyq, the prediction of the (scalar) output of the query point xq.
1 Sort increasingly the set of vectors {xi} with respect to the distance to xq.
2 [j] designate the index of the jth closest neighbor of xq.
3 for k ∈{2, . . . , Kmax} do
Calculate eLOO(k) which is deﬁned in Equation 12.
7 k∗= arg mink∈{2,...,Kmax} eLOO(k).
8 ˆyq = y(k∗)
9 return ˆyq.
3.4. Multiple-Output Lazy Learning algorithm
The adoption of Multiple-Output strategies requires the design of multiple-output (or equivalently multi-response) modeling techniques where the output is no more a scalar quantity but a vector of values. Like in
the Single-Output case, we need criteria to assess and compare local models with diﬀerent number
of neighbors. In the following, we present two criteria: the ﬁrst one is an extension of the LOO
error for the Multiple-Output case (Algorithm 2) and the
second one is a criterion proper to the Multiple-Output modeling (Algorithm 3) . Note that, in the two algorithms, the output is a vector of
size l (e.g. l will equal H with the MIMO strategy or s in the DIRMO strategy).
Algorithm 2 is an extension of the Algorithm 1 for vectorial outputs. We still use the LOO
cross-validation measure as a criterion to estimate the generalization capability of the model but
Algorithm 2: Multiple-Output Lazy Learning (LOO criterion): MIMO-LOO
: D = {(xi, yi) ∈(Rd × Rl) with i ∈{1, . . . , M}}, dataset.
: xq ∈Rd, query point.
: Kmax, the maximum number of neighbors.
Output: ˆyq, the prediction of the (vectorial) output of the query point xq.
1 Sort increasingly the set of vectors {xi} with respect to the distance to xq.
2 [j] will designate the index of the jth closest neighbor of xq.
3 for k ∈{2, . . . , Kmax} do
ELOO(k) = 1
LOO(k) where el
LOO(k) is deﬁned in Equation 12.
7 k∗= arg mink∈{2,...,Kmax} ELOO(k).
8 ˆyq = y(k∗)
9 return ˆyq.
here, the LOO error is an aggregation of the errors obtained for each output (line 5). Note that
the same number of neighbors is selected for all the outputs (e.g. MIMO strategy) unlike what
could happen with diﬀerent Single-Output tasks (e.g. Direct strategy).
The second criterion uses the fact that the forecasting horizon H is supposed to be large (multistep ahead forecasting) and hence we have enough samples to estimate some descriptive statistics.
Then, instead of using the Leave-One-Out error, we can use as criterion a measure of stochastic
discrepancy between the forecasted values and the training time series. The lower the discrepancy
between the descriptors of the forecasts and the training time series, the better is the quality of
the forecasts .
Several measures of discrepancy can be deﬁned, both linear and non-linear. For example, the
autocorrelation can be used as linear statistics and the maximum likelihood as a non-linear one. In
this work, we will consider only one linear measure using both the autocorrelation and the partial
correlation.
The assessement of the quality of the estimation y(k)
of the query point xq is calculated as
E∆(k) = 1 −|cor[ρ(ts · y(k)
q ), ρ(ts)]|
} + 1 −|cor[π(ts · y(k)
q ), π(ts)]|
where the symbol “·” represents the concatenation, ts represent the training time series and cor is
the Pearson correlation. This discrepancy measure is composed of two parts where the ﬁrst part
uses the autocorrelation (noted ρ) and the second uses the partial autocorrelation (noted π).
For each part, we calculate the discrepancy (estimated with the correlation, noted cor) between,
on one hand, the autocorrelation (or partial autocorrelation) of the concatenation of the training
time series ts and the forecasted sequence y(k)
and, on the other hand, the autocorrelation (or
partial autocorrelation) of the training time series ts .
In Algorithm 3, after evaluating the performance of local models with diﬀerent number of
neighbors k (lines 3 to 6), the best one, which minimizes the discrepancy between the forecasting
sequence and the training time series (having index k∗), is selected (lines 7 and 8). In other words,
the goal is to select the best number of neighbors k∗which preserve the stochastic properties of
the time series in the forecasted sequence. Finally, the prediction of the output of xq is returned
Algorithm 3: Multiple-Output Lazy Learning (discreprancy criterion): MIMO-ACFLIN
: ts = [ts1, . . . , tsN], time series.
: D = {(xi, yi) ∈(Rd × Rl) with i ∈{1, . . . , M}}, dataset.
: xq ∈Rd, query point.
: Kmax, the maximum number of neighbors.
Output: ˆyq, the prediction of the (vectorial) output of the query point xq.
1 Sort increasingly the set of vectors {xi} with respect to the distance to xq.
2 [j] will designate the index of the jth closest neighbor of xq.
3 for k ∈{2, . . . , Kmax} do
Calculate E∆(k) which is deﬁned in Equation 18.
7 k∗= arg mink∈{2,...,Kmax} E∆(k).
8 ˆyq = y(k∗)
9 return ˆyq.
3.5. Model selection or model averaging
Considering the Algorithm 1, we can see that we generate, for the query xq, a set of predictions {y(2)
q , . . . , y(Kmax)
}, each obtained with diﬀerent number of neighbors. For each of these
predictions, a testing error {eLOO(2), eLOO(3), . . . , eLOO(Kmax)} has been calculated. Note that
the next considerations are also applicable to Algorithms 2 and 3.
The goal of model selection is to use all this information (set of predictions and testing errors)
to estimate the ﬁnal prediction ˆyq of the query point xq. There exist two main paradigms mainly
the winner-take-all and combination approaches.
In the Algorithm 1, we presented the winner-take-all approach (noted WINNER) which consists of comparing the set of models y(k)
and selecting the best one in terms
of testing error eLOO(k) (see line 7).
Selecting the best model according to the testing error is intuitively the approach which should
work the best. However, results in machine learning show that the performance of the ﬁnal model
can be improved by combining models having diﬀerent structures .
In order to apply the model averaging, lines 7 and 8 of the Algorithm 1 can be replaced by
ˆyq = p2 y(2)
+ · · · + pKmax y(Kmax)
where an average is calculated. The weights pk will take diﬀerent values depending on the combination approach adopted. If pk equals
Kmax, we are in the case of equally weighted combination
and ˆyq reduces to an arithmetic mean (noted COMB). Otherwise, if weights are assigned according
to testing errors, pk will equal
eLOO(k) and ˆyq reduces to a weighted mean (noted WCOMB).
4. Experimental Setup
4.1. Time Series Data
In the last decade, several time series forecasting competitions ) have been organized in order to compare
and evaluate the performance of computational intelligence methods.
Among them, the NN5
competition (Crone, b) is one of the most interesting one since it includes the challenges of a realworld multi-step ahead forecasting task, namely multiple time series, outliers, missing values as
well as multiple overlying seasonalities, etc. Figure 2 shows four time series from the NN5 dataset.
Each of the 111 time series of this competition represents roughly two years of daily cash money
withdrawal amounts (735 data points) at ATM machines at one of the various cities in the UK.
For each time series, the competition required to forecast the values of the next 56 days, using the
given historical data points, as accurately as possible. The performance of the forecasting methods
over one time series was assessed by the symmetric mean absolute percentage of error (SMAPE)
measure (Crone, b), deﬁned as
(ˆyh + yh)/2 × 100,
where yh is the target output and ˆyh is the prediction. Since this is a relative error measure, the
errors can be averaged over all time series to obtain a mean SMAPE deﬁned as
where SMAPEi denotes the SMAPE of the ith time series.
Time Series n° 9
Time Series n° 42
Time Series n° 67
Time Series n° 103
Figure 2: Four time series from NN5 time series forecasting competition.
4.2. Methodology
The aim of the experimental study is to compare the accuracy of the ﬁve forecasting strategies
in the context of the NN5 competition. Since the accuracy of a forecasting technique is known to
be dependent on several design choices (e.g. the deseasonalization or the input selection) and we
want to focus our analysis on the multi-step ahead forecasting strategies, we consider a number
of diﬀerent conﬁgurations in order to increase the statistical power of our comparison.
conﬁguration is composed of several preprocessing steps as sketched in Figure 3. Since some of
these steps (e.g. deseasonalization) can be performed in alternative ways (e.g. two alternatives
for the deseasonalization, two alternatives for input selection, three alternatives for the model
selection), we come up with 12 conﬁgurations. The details about each step are given in what
Gaps removal
Deseasonalization : Yes or No
Embedding Dimension Selection
Input Selection : Yes or No
Model Selection :
WINNER or COMB or WCOMB
For all NN5
time series
Figure 3: The Diﬀerent Preprocessing Steps.
Step 1: Gaps removal
The speciﬁcity of the NN5 series requires a preprocessing step called gaps removal where by
gap we mean two types of anomalies: (i) zero values that indicate that no money withdrawal
occurred and (ii) missing observations for which no value was recorded.
About 2.5% of the
data are corrupted by gaps. In our experiments we adopted the gap removal method proposed
in : if ym is the gap sample, this method replaces the gap with the median of the
set [ym+365, ym−365, ym+7 and ym−7] among which are available.
Step 2: Deseasonalization The adoption of deseasonalization may have a large impact on the
forecasting strategies because the NN5 time series possess a variety of periodic patterns. For that
reason we decided to consider tasks with and without deseasonalization in order to better account
for the role of the forecasting strategy. We adopt the deseasonalization methodology discussed in
 to remove the strong day of the week seasonality as well as the moderate
day of the month seasonality. Of course after we deseasonalize and apply the forecasting model we
restore back the seasonality.
Step 3: Embedding dimension selection
Every forecasting strategy requires the setting of the size d of the embedding dimension (see
Equations 1 to 9). Several approaches have been proposed in the literature to select this value . Since this aspect is not a central theme in our paper we just applied the
state-of-the-art approach reviewed in , which consists of selecting the
time-lagged realizations with signiﬁcant partial correlation function (PACF). This method allows
to select the value of the embedding dimension and then to identify the relevant variables within
the window of past observations. We set the maximum lag of the PACF to 200 to provide a suf-
ﬁciently comprehensive pool of features. However, note that the ﬁnal dimensionality of the input
vectors for all the time series is on average equal to 24.
Step 4: Input Selection
We considered the forecasting task with and without input variable selection step. A variable
selection procedure requires the setting of two elements: the relevance criterion, i.e. statistics which
estimates the quality of the selected variables, and the search procedure, which describes the policy
to explore the input space. We adopted the Delta test(DT) as relevance criterion. The DT has
been introduced in time series forecasting domain by Pi and Peterson in 
and later successfully applied to several forecasting task . This criterion is based on applying
some kind of a noise variance estimator, and then selecting the set of input variables that yield the
strongest and most deterministic dependence between inputs and outputs (Mateo and Lendasse,
Concerning the search procedure, we adopted a Forward-Backward Search (FBS) procedure
which is a combination of forward selection (sequentially adding input variables) and backward
search (sequentially removing some input variables). This choice was motivated by the ﬂexibility
of the FBS procedure which allows a deeper exploration of the input space. Note that the search
is initialized by the set of variables deﬁned in the previous step.
Step 5: Model Selection
Concerning the model selection procedure, three approaches (see Section 3.5) are taken into
consideration in our experiments:
WINNER : This approach selects the model that gives best performance for the test set (winnertake-all approach).
COMB : This approach combines all alternative models by simple averaging.
WCOMB : This approach combines models by weighted averaging where weights are inversely
proportional to the test errors.
4.2.1. The Compared forecasting strategies
Table 4 presents the eight forecasting strategies that we tested, showing also their respective
The Recursive forecasting strategy.
The Direct forecasting strategy.
The DirRec forecasting strategy.
4. MIMO-LOO
A variant of the MIMO forecasting
strategy with the LOO selection
5. MIMO-ACFLIN
A variant of the MIMO forecasting
strategy with the autocorrelation
selection criteria.
6. DIRMO-SEL
The DIRMO forecasting strategy
which select the best value of the
parameter s.
7. DIRMO-AVG
A variant of the DIRMO strategy
which calculates a simple average
of the forecasts obtained with
diﬀerent values of the parameter s.
8. DIRMO-WAVG
The DIRMO-AVG with a weighted
average where weights are inversely
proportional to testing errors.
Table 4: The ﬁve forecasting strategies with their respective variants which gives eight forecasting
strategies.
4.2.2. Forecasting performance evaluation
This section describes the assessment procedure (Figure 4) of the 8 forecasting strategies.
Test for significant
differences in average
forecasting ranks
Friedman test
Identification of groups
which significantly
differ from others
Post-hoc test
Measure of average
forecasting errors on
all NN5 time series
Figure 4: Diﬀerent steps of the forecasting performance evaluation.
The procedure for comparing between the eight forecasting strategies is shown in Figure 4. The
accuracy of each forecasting strategy is ﬁrst measured using the SMAPE* measure calculated over
the 111 time series and deﬁned in Equation 21. To test if there are signiﬁcant general diﬀerences
in performance between the diﬀerent strategies, we have to consider the problem of comparing
multiple models on multiple data sets. For such case Demˇsar in a detailed comparative study recommended using a two stage procedure: ﬁrst to apply
Friedman’s or Iman and Davenport’s tests to test if the compared models have the same mean
rank. If this test rejects the null-hypothesis, then post-hoc pairwise tests are to be performed to
compare the diﬀerent models. These tests adjust the critical values higher to ensure that there is
at most a 5% chance that one of the pairwise diﬀerences will be erroneously found signiﬁcant.
Friedman test
The Friedman test is a non-parametric procedure which tests the signiﬁcance of diﬀerences between multiple ranks. It ranks the algorithms for each dataset separately:
the rank of 1 will be given to the best performing algorithm, the rank of 2 to the second best and
so on. Note that average ranks are assigned in case of ties.
After ranking the algorithms for each dataset, the Friedman test compares the average ranks of
algorithms. Let ri
j be the rank of the j-th of k algorithms on the i-th of N data sets, the average
rank of the j-th algorithm is Rj = 1
The null-hypothesis states that all the algorithms are equivalent and so their ranks Rj should
be equal. Under the null-hypothesis, the Friedman statistic
j −k(k + 1)2
is distributed according to a chi-squared with k −1 degrees of freedom (χ2
k−1), when N and k are
large enough (as a rule of a thumb, N > 10 and k > 5) .
Iman and Danvenport , showing that Friedman’s statistic is undesirably conservative, derived another improved statistic, given by
N(k −1) −Q
which is distributed, under the null-hypothesis, according to the F-distribution with k −1 and
(k −1)(N −1) degrees of freedom.
Post-hoc test
When the null-hypothesis is rejected, i.e. there is a signiﬁcant diﬀerence between at least 2
strategies, a post-hoc test is performed to identify signiﬁcant pairwise diﬀerences among all the
algorithms. The test statistic for comparing the i-th and the j-th algorithm is
z = (Ri −Rj)
which is asymptotically normally distributed under the null hypothesis. After the corresponding
p-value is calculated, it is compared with a given level of signiﬁcance α.
However, in multiple comparisons, as there are a possibly large number of pairwise comparisons,
there is a relatively high chance that some pairwise test are incorrectly rejected. Several procedures
exist to adjust the value of α to compensate for this bias, for instance Nemenyi, Holm, Shaﬀer as well
as Bergmann and Hommel . Based on the suggestion of Garcia and Herrera we adopted Shaﬀer’s correction. The reason is that Garcia and Herrera showed that Shaﬀer’s procedure has the same complexity Holm’s procedure,
but with the advantage of using information about logically related hypothesis.
4.3. Experimental phases
In order to reproduce the same context of the NN5 forecasting competition the experimental
setting is made of two phases: the pre-competition and the competition phase.
4.3.1. Pre-competition phase
The pre-competition phase is devoted to the comparison of the diﬀerent forecasting strategies
using the available observations of 111 time series. The goal is to learn the diﬀerent parameters
and then estimate the forecasting performance and compare between the diﬀerent strategies.
To estimate the forecasting performance of each strategy, we used a learning scheme with
training-validation-testing sets. Each time series (containing 735 observations) is partitioned in
three mutually exclusive sets (A, B and C) as shown in Figure 5: training (Day 1 to Day 623: 623
values), validation (Day 624 to Day 679: 56 values) and testing (Day 680 to Day 735: 56 values).
The validation set (B in Figure 5) is used to build and tune the models.
Speciﬁcally, as
we use a Lazy Learning approach, we need to select, for each model, the range of number of
neighbors([2, . . . , Kmax]) to use in performing the forecasting task.
The test set (C in Figure 5) is used to measure the performances of each forecasting strategy.
To make the utmost use of the available data, we adopt a multiple time origin test as suggested by
Tashman in , where the time origin denotes the point from which the multi-step
ahead forecasts are generated.
The time origin and corresponding forecast intervals are given as:
1. Day 680 to Day 735 (56 data points)
2. Day 687 to Day 735 (49 data points)
3. Day 694 to Day 735 (42 data points)
In other words, we perform the forecast three times starting from the three diﬀerent starting
points, each time forecasting a number of steps ahead till the end of the interval. Note that we
used the same test period and evaluation criterion (i.e.
the SMAPE) as used by Andrawis et
al in . This allows us to compare our results with several other machine
learning models tested in this article.
4.3.2. Competition phase
In the competition phase we generate the ﬁnal forecasts, made up of 56 future observations,
which would have been submitted to the competition. This phase takes advantage of the lessons
learned and the design choices made in the pre-competition phase. Here, we combine the training
Time Series
Figure 5: Learning with three mutually exclusive sets for training (A), validation (B) and testing (C).
set with the test set (A+B in Figure 6) to retrain the models of the diﬀerent strategies and then
generate the ﬁnal forecast (which will be submitted to the competition). The training set (A+B
on Figure 6) is now made of 679 data points and the validation set (C on Figure 6) is composed
of the next 56 data points, as shown in Figure 6. In other words, the 735 values are then used to
build and tune the models, which will next return the forecasted values.
Time Series
Figure 6: Forecasting with two mutually exclusive sets for training (A+B) and validation (C).
5. Results and discussion
This section presents and discusses the prediction results of the forecasting strategies for the
pre-competition and competition phase. For each phase, we report the results obtained in the 12
diﬀerent conﬁgurations introduced in Section 4.2.
The forecasting performance of the strategies is measured using the criteria discussed in Section 4.2.2 and presented by means of two tables. The ﬁrst one provides the average SMAPE as well
as the ranking for each forecasting strategy. Since the null-hypothesis stating that all the algorithms
are equivalent has been rejected (using the Iman-Davenport statistic) for all the conﬁgurations,
we proceeded with the post-hoc test. The second table presents the results of this post-hoc test,
which partitions the set of strategies in several groups which are statistically signicantly diﬀerent
in terms of forecasting performance.
Note that the conﬁgurations which require the input selection do not contain the DIRMO
results since combining the selection of the inputs with the selection of the parameter s would have
needed an excessive amount of computation time.
5.1. Pre-competition results
The SMAPE and ranking results for the pre-competition phase are presented in Table 5 while
the results of the post-hoc test are summarized in Table 6.
Deseasonalization : No - Input Selection : No
Strategies
SMAPE* [Std err]
SMAPE* [Std err]
SMAPE* [Std err]
22.37[0.55](7)
22.19[0.54](7)
22.61[0.55](5)
21.41[0.59](6)
21.95[0.58](5)
22.91[0.62](7)
45.25[0.89](8)
40.73[0.83](8)
38.93[0.77](8)
21.17[0.60](2)
20.61[0.57](1)
20.61[0.58](1)
MIMO-ACFLIN
21.40[0.59](5)
20.69[0.58](2)
20.61[0.58](2)
21.21[0.56](3)
22.15[0.56](6)
22.68[0.56](6)
21.27[0.56](4)
20.90[0.56](3)
21.16[0.56](3)
DIRMO-WAVG
21.12[0.56](1)
20.96[0.56](4)
21.25[0.57](4)
Deseasonalization : No - Input Selection : Yes
Strategies
SMAPE* [Std err]
SMAPE* [Std err]
SMAPE* [Std err]
22.80[0.50](4)
22.73[0.52](4)
23.37[0.54](4)
21.17[0.55](2)
22.21[0.52](3)
23.20[0.56](3)
45.21[0.93](5)
40.57[0.83](5)
39.03[0.75](5)
21.06[0.57](1)
20.60[0.59](1)
20.64[0.59](1)
MIMO-ACFLIN
21.40[0.57](3)
20.65[0.59](2)
20.64[0.59](2)
Deseasonalization : Yes - Input Selection : No
Strategies
SMAPE* [Std err]
SMAPE* [Std err]
SMAPE* [Std err]
22.29[0.57](7)
20.41[0.57](7)
20.49[0.58](7)
21.61[0.61](6)
19.39[0.59](6)
19.31[0.59](5)
22.61[0.68](8)
20.67[0.64](8)
20.61[0.61](8)
19.81[0.59](4)
19.21[0.60](4)
19.25[0.60](3)
MIMO-ACFLIN
19.34[0.59](1)
19.19[0.59](3)
19.25[0.60](4)
19.49[0.57](2)
18.98[0.57](1)
19.04[0.58](1)
20.39[0.56](5)
19.36[0.58](5)
19.43[0.58](6)
DIRMO-WAVG
19.71[0.58](3)
19.02[0.57](2)
19.11[0.58](2)
Deseasonalization : Yes - Input Selection : Yes
Strategies
SMAPE* [Std err]
SMAPE* [Std err]
SMAPE* [Std err]
21.87[0.47](4)
20.07[0.49](4)
20.20[0.51](4)
20.52[0.53](3)
19.14[0.55](3)
19.20[0.55](3)
23.06[0.84](5)
21.03[0.68](5)
20.95[0.66](5)
20.02[0.53](2)
18.86[0.54](2)
18.88[0.54](1)
MIMO-ACFLIN
18.95[0.54](1)
18.81[0.54](1)
18.88[0.54](2)
Table 5: Pre-competition phase. Average forecasting errors (SMAPE*) with average ranking
for each strategy in the 12 diﬀerent conﬁgurations. The numbers in round bracket represent the
ranking within the column.
Deseasonalization : No - Input Selection : No
”DIRMO-WAVG” (3.07)
”MIMO-LOO” (3.64)
”DIRMO-SEL” (3.76)
”DIRMO-AVG” (3.88)
”REC” (3.98)
”MIMO-ACFLIN” (4.48)
”DIR” (5.19)
”DIRREC” (8.00)
”MIMO-LOO” (2.77)
”MIMO-ACFLIN” (3.17)
”DIRMO-AVG” (3.36)
”DIRMO-WAVG” (3.71)
”REC” (4.63)
”DIR” (4.96)
”DIRMO-SEL” (5.40)
”DIRREC” (8.00)
”MIMO-LOO” (2.71)
”MIMO-ACFLIN” (2.71)
”DIRMO-AVG” (3.42)
”DIRMO-WAVG” (3.65)
”REC” (4.73)
”DIRMO-SEL” (5.32)
”DIR” (5.47)
”DIRREC” (7.99)
Deseasonalization : No - Input Selection : Yes
”MIMO-LOO” (2.02)
”REC” (2.20)
”MIMO-ACFLIN” (2.58)
”DIR” (3.20)
”DIRREC” (5.00)
”MIMO-LOO” (1.79)
”MIMO-ACFLIN” (2.01)
”REC” (2.96)
”DIR” (3.27)
”DIRREC” (4.96)
”MIMO-LOO” (1.88)
”MIMO-ACFLIN” (1.88)
”REC” (3.01)
”DIR” (3.27)
”DIRREC” (4.96)
Deseasonalization : Yes - Input Selection : No
”DIRMO-SEL” (2.90)
”DIRMO-WAVG” (3.21)
”MIMO-ACFLIN” (3.21)
”MIMO-LOO” (3.83)
”DIRMO-AVG” (4.74)
”REC” (5.35)
”DIRREC” (6.33)
”DIR” (6.42)
”MIMO-LOO” (3.69)
”DIRMO-SEL” (3.72)
”DIRMO-WAVG” (3.72)
”REC” (3.87)
”MIMO-ACFLIN” (4.00)
”DIRMO-AVG” (4.61)
”DIRREC” (6.15)
”DIR” (6.23)
”DIRMO-SEL” (3.66)
”REC” (3.68)
”DIRMO-WAVG” (3.79)
”MIMO-LOO” (3.82)
”MIMO-ACFLIN” (3.82)
”DIRMO-AVG” (4.77)
”DIRREC” (6.04)
”DIR” (6.43)
Deseasonalization : Yes - Input Selection : Yes
”MIMO-ACFLIN” (1.76)
”MIMO-LOO” (2.59)
”REC” (2.95)
”DIRREC” (3.77)
”DIR” (3.95)
”MIMO-ACFLIN” (2.38)
”MIMO-LOO” (2.45)
”REC” (2.57)
”DIRREC” (3.77)
”DIR” (3.82)
”MIMO-LOO” (2.34)
”MIMO-ACFLIN” (2.34)
”REC” (2.61)
”DIRREC” (3.74)
”DIR” (3.97)
Table 6: Pre-competition phase. Group of strategies statistically signiﬁcantly diﬀerent (sorted
by increasing ranking) provided by Friedman and post-hoc tests for the 12 conﬁgurations.
The availability of the SMAPE* results, obtained according to the procedure used in , makes possible the comparison of our pre-competition results with those of several
others learning methods available in . For the sake of comparison, Table
7 reports the forecasting errors for some of the techniques considered in ,
notably Gaussian Process Regression (GPR), Neural Network (NN), Multiple Regression (MULT-
REGR), Simple Moving Average (MOV-AVG), Holt’s Exponential Smoothing and a combination
(Combined) of such techniques. The comparison shows that the best conﬁguration of Table 5d, that
MULT-REGR1
MULTI-REGR2
MULT-REGR3
Holt’s Exp Sm
Table 7: Forecasting errors for the diﬀerent forecasting models.
is the MIMO-ACFLIN strategy, is competitive with all these models with a SMAPE* amounting
to 18.81%.
5.2. Competition results
The SMAPE and ranking results for the competition phase are presented in Table 8 while the
results of the post-hoc test are summarized in Table 9.
Deseasonalization : No - Input Selection : No
Strategies
SMAPE* [Std err]
SMAPE* [Std err]
SMAPE* [Std err]
24.48[0.52](7)
23.06[0.50](6)
22.89[0.48](6)
24.22[0.62](6)
23.71[0.52](7)
23.54[0.54](7)
44.97[0.85](8)
40.37[0.80](8)
38.17[0.72](8)
21.92[0.56](1)
21.55[0.50](1)
21.64[0.49](1)
MIMO-ACFLIN
22.45[0.54](3)
21.57[0.50](2)
21.64[0.49](2)
22.60[0.54](5)
22.27[0.49](5)
22.50[0.50](5)
22.55[0.53](4)
21.73[0.49](4)
21.93[0.49](4)
DIRMO-WAVG
22.36[0.54](2)
21.70[0.49](3)
21.87[0.49](3)
Deseasonalization : No - Input Selection : Yes
Strategies
SMAPE* [Std err]
SMAPE* [Std err]
SMAPE* [Std err]
24.43[0.52](4)
23.14[0.48](4)
23.25[0.48](4)
22.73[0.69](3)
22.45[0.67](3)
22.57[0.65](3)
44.91[0.86](5)
39.88[0.77](5)
38.04[0.71](5)
22.18[0.53](1)
21.78[0.49](1)
21.84[0.49](1)
MIMO-ACFLIN
22.62[0.51](2)
21.83[0.50](2)
21.84[0.49](2)
Deseasonalization : Yes - Input Selection : No
Strategies
SMAPE* [Std err]
SMAPE* [Std err]
SMAPE* [Std err]
23.98[0.55](8)
21.65[0.46](8)
21.75[0.47](7)
23.12[0.59](6)
21.39[0.49](6)
21.86[0.49](8)
23.51[0.60](7)
21.58[0.52](7)
21.57[0.51](6)
21.11[0.54](4)
20.27[0.47](4)
20.34[0.47](3)
MIMO-ACFLIN
20.25[0.47](1)
20.18[0.46](2)
20.34[0.47](4)
20.85[0.51](2)
20.18[0.46](3)
20.23[0.45](1)
21.66[0.51](5)
20.38[0.46](5)
20.48[0.46](5)
DIRMO-WAVG
20.97[0.51](3)
20.15[0.46](1)
20.23[0.46](2)
Deseasonalization : Yes - Input Selection : Yes
Strategies
SMAPE* [Std err]
SMAPE* [Std err]
SMAPE* [Std err]
23.91[0.50](5)
21.54[0.47](5)
21.58[0.48](5)
22.57[0.64](3)
21.39[0.63](3)
21.45[0.64](3)
23.66[0.58](4)
21.48[0.52](4)
21.47[0.51](4)
20.74[0.51](2)
20.28[0.53](1)
20.28[0.52](1)
MIMO-ACFLIN
20.39[0.54](1)
20.28[0.53](2)
20.28[0.52](2)
Table 8: Competition phase. Average forecasting errors (SMAPE*) with average ranking for
each strategy in the 12 diﬀerent conﬁgurations.
The numbers in round bracket represent the
ranking within the column.
Deseasonalization : No - Input Selection : No
”MIMO-LOO” (2.90)
”DIRMO-WAVG” (3.21)
”MIMO-ACFLIN” (3.65)
”DIRMO-AVG” (3.78)
”DIRMO-SEL” (3.99)
”REC” (4.96)
”DIR” (5.58)
”DIRREC” (7.93)
”MIMO-LOO” (2.84)
”MIMO-ACFLIN” (3.08)
”DIRMO-WAVG” (3.52)
”DIRMO-AVG” (3.68)
”DIRMO-SEL” (4.46)
”DIR” (5.21)
”REC” (5.24)
”DIRREC” (7.97)
”MIMO-LOO” (2.79)
”MIMO-ACFLIN” (2.79)
”DIRMO-WAVG” (3.55)
”DIRMO-AVG” (3.84)
”DIRMO-SEL” (4.84)
”REC” (5.05)
”DIR” (5.20)
”DIRREC” (7.95)
Deseasonalization : No - Input Selection : Yes
”MIMO-LOO” (2.12)
”REC” (2.28)
”MIMO-ACFLIN” (2.39)
”DIR” (3.28)
”DIRREC” (4.94)
”MIMO-LOO” (2.09)
”MIMO-ACFLIN” (2.24)
”REC” (2.53)
”DIR” (3.22)
”DIRREC” (4.93)
”MIMO-LOO” (2.19)
”MIMO-ACFLIN” (2.19)
”REC” (2.42)
”DIR” (3.24)
”DIRREC” (4.95)
Deseasonalization : Yes - Input Selection : No
”MIMO-ACFLIN” (3.05)
”DIRMO-WAVG” (3.35)
”DIRMO-SEL” (3.45)
”MIMO-LOO” (3.69)
”DIRMO-AVG” (4.47)
”REC” (5.47)
”DIRREC” (6.15)
”DIR” (6.36)
”DIRMO-WAVG” (3.59)
”MIMO-ACFLIN” (3.67)
”MIMO-LOO” (3.77)
”DIRMO-SEL” (3.77)
”DIRMO-AVG” (4.34)
”REC” (5.04)
”DIRREC” (5.78)
”DIR” (6.05)
”DIRMO-SEL” (3.45)
”DIRMO-WAVG” (3.59)
”MIMO-LOO” (3.69)
”MIMO-ACFLIN” (3.69)
”DIRMO-AVG” (4.38)
”REC” (5.40)
”DIRREC” (5.67)
”DIR” (6.14)
Deseasonalization : Yes - Input Selection : Yes
”MIMO-ACFLIN” (1.87)
”MIMO-LOO” (2.14)
”REC” (3.05)
”DIRREC” (3.79)
”DIR” (4.14)
”MIMO-LOO” (2.27)
”MIMO-ACFLIN” (2.32)
”REC” (2.96)
”DIRREC” (3.50)
”DIR” (3.94)
”MIMO-LOO” (2.33)
”MIMO-ACFLIN” (2.33)
”REC” (2.93)
”DIRREC” (3.50)
”DIR” (3.91)
Table 9: Competition phase. Group of strategies statistically signiﬁcantly diﬀerent (sorted by
increasing ranking) provided by Friedman and post-hoc tests for the 12 conﬁgurations.
The pre-competition results presented in the previous section suggest us to use the MIMO-
ACFLIN strategy with the Comb model selection approach by removing the seasonality and applying the input selection procedure, since this conﬁguration obtains the smallest forecasting error (18.81%).
By using the MIMO-ACFLIN strategy and the corresponding conﬁguration in the competition
phase, we would generate forecasts with a SMAPE* equals to 20.28% which is quite good compared
to the best computational intelligence entries of the competition as shown in Table 10. Figure 7
shows the forecasts of the MIMO-ACFLIN strategy versus the actual values for four NN5 time
series to illustrate the forecasting capability of this strategy.
MIMO-ACFLIN
Puma-Villanueva
Table 10: Forecasting errors for diﬀerent computational intelligence forecasting models which participate to the NN5 forecasting competition.
Time Series n° 6
Time Series n° 22
Time Series n° 42
Time Series n° 87
Figure 7: The forecast versus the actual of the MIMO-ACFLIN strategy for four NN5 time series.
5.3. Discussion
From all presented results one can deduce the following observations below. These ﬁndings
refer mainly to the pre-competition results. But, one can easily see that they mostly also apply to
the competition phase results.
• The overall best method is MIMO-ACFLIN, used with input selection, deseasonalization and
equal weight combination (COMB).
• The Multiple-Output strategies (MIMO and DIRMO) are invariably the best strategies.
They beat the Single-Output strategies, such as DIR, REC, and DIRREC. Both MIMO
and DIRMO give comparable performancce. For DIRMO, the selection of the parameter s
is critical, since it has a great impact on the performance. Should there be an improved
selection approach, this strategy would have a big potential.
• Both versions of MIMO are comparable. Also the versions of DIRMO give close results, with
perhaps DIRMO-WAVG a little better than the other two versions.
• Among the Single-Output strategies, the REC strategy has almost always a smaller SMAPE
and a better ranking than the DIR strategy. DIRREC is the worse strategy overall, and gives
especially low accuracy when no deseasonalization is performed.
• Deseasonalization leads to consistently better results (in 38 out of 39 models). This result is
consistent with some other studies, such as . The possible reason for
this is that when no deseasonalization is performed, we are putting a higher burden on the
model to forecast the future seasonal pattern plus the trend and the other aspects, which
apparently is hard to simultaneously satisfy.
• Input selection is especially beneﬁcial when we perform a deseasonalization. Absent deseasonalization, the results are mixed (as to whether input selection improves the results or not).
The possible explanation is that when no deseasonalization is performed, the model needs
all the previous cycle to construct the future seasonal pattern. Performing an input selection
will deprive it from essential information.
• Concerning the model selection aspect, both combination approaches (COMB and WCOMB)
are superior to the winner-take-all (WINNER). Both COMB and WCOMB are comparable,
and the results do not diﬀer by much.
This is consistent with much of the ﬁndings in
forecast combination literature, e.g. 
• The relative performance and ranking of the diﬀerent strategies is persistent. Most ﬁndings
that are based on the pre-competition results are true for the competition phase results.
This is also true for the ﬁndings concerning the deseasonalization, input selection, and model
selection. This persistence is reassuring, as we can have some conﬁdence in relying on the
test or validation results for selecting the best strategies.
• The best strategy based on the pre-competition data, the MIMO-ACFLIN method, would
have topped all computational intelligence entries of the NN5 competition in the true competition hold-out data.
6. Conclusion
Forecasting a time series many steps into the future is a very hard problem because the larger the
forecast horizon, the higher is the uncertainty. In this paper we presented a comparative review
of existing strategies for multi-step ahead forecasting, together with an extensive comparison,
applied on the 111 time series of the NN5 forecasting competition. The comparison gave some
interesting lessons that could help researchers channel their experiments into the most promising
approaches.
The most consistent ﬁndings are that Multiple-Output approaches are invariably
better than Single-Output approaches. Also, deseasonalization had a very considerable positive
impact on the performance.
Finally, the results are clearly quite persistent.
So, selecting the
best strategy based on testing performance is a very potent approach. A possible direction for
future research could therefore be developing other new improved Multiple-Output strategies. Also,
possibly tailoring deseasonalization methods speciﬁcally for Multiple-Output strategies could also
be a promising research point.
Acknowledgments
We would like to thanks the authors of the paper for making their
methods available at