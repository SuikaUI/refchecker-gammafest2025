Forecaster: A Graph Transformer for Forecasting
Spatial and Time-Dependent Data
Yang Li and Jos´e M. F. Moura1
Spatial and time-dependent data is of interest in many
applications. This task is difﬁcult due to its complex spatial dependency, long-range temporal dependency, data non-stationarity, and
data heterogeneity. To address these challenges, we propose Forecaster, a graph Transformer architecture. Speciﬁcally, we start by
learning the structure of the graph that parsimoniously represents the
spatial dependency between the data at different locations. Based on
the topology of the graph, we sparsify the Transformer to account for
the strength of spatial dependency, long-range temporal dependency,
data non-stationarity, and data heterogeneity. We evaluate Forecaster
in the problem of forecasting taxi ride-hailing demand and show that
our proposed architecture signiﬁcantly outperforms the state-of-theart baselines.
Introduction
Spatial and time-dependent data describe the evolution of signals
(i.e., the values of attributes) at multiple spatial locations across time
 . It occurs in many domains, including economics , global
trade , environment studies , public health , or trafﬁc networks to name a few. For example, the gross domestic product
(GDP) of different countries in the past century, the daily temperature
measurements of different cities for the last decade, and the hourly
taxi ride-hailing demand at various urban locations in the recent year
are all spatial and time-dependent data. Forecasting such data allows
to proactively allocate resources and take actions to improve the ef-
ﬁciency of society and the quality of life.
However, forecasting spatial and time-dependent data is challenging — they exhibit complex spatial dependency, long-range temporal
dependency, heterogeneity, and non-stationarity. Take the spatial and
time-dependent data in a trafﬁc network as an example. The data at a
location (e.g., taxi ride-hailing demand) may correlate more with the
data at a geometrically remote location than a nearby location ,
exhibiting complex spatial dependency. Also, the data at a time instant may be similar to the data at a recent time instant, say an hour
ago, but may also highly correlate with the data a day ago or even
a week ago, showing strong long-range temporal dependency. Additionally, the spatial and time-dependent data may be inﬂuenced by
many other relevant factors (e.g., weather inﬂuences taxi demand).
These factors are relevant information, shall be taken into account.
In other words, in this paper, we propose to perform forecasting with
heterogeneous sources of data at different spatial and time scales and
including auxiliary information of a different nature or modality. Further, the data may be non-stationary due to unexpected incidents or
trafﬁc accidents . This non-stationarity makes the conventional
1 Carnegie Mellon University, USA
Email: {yangli1, moura}@andrew.cmu.edu
time series forecasting methods such as auto-regressive integrated
moving average (ARIMA) and vector autoregression (VAR), which
usually rely on stationarity, inappropriate for accurate forecasting
with spatial and time-dependent data .
Recently, deep learning models have been proposed for forecasting for spatial and time-dependent data .
To deal with spatial dependency, most of these models either use predeﬁned distance/similarity metrics or other prior knowledge like adjacency matrices of trafﬁc networks to determine dependency among
locations. Then, they often use a (standard or graph) convolutional
neural network (CNN) to better characterize the spatial dependency
between these locations. These ad-hoc methods may lead to errors in
some cases. For example, the locations that are considered as being
dependent (independent) may actually be independent (dependent)
in practice. As a result, these models may encode the data at a location by considering the data at independent locations and neglecting
the data at dependent locations, leading to inaccurate encoding. Regarding temporal dependency, most of these models use recurrent
neural networks (RNN), CNN, or their variants to capture the data
long-range temporal dependency and non-stationarity. But it is well
documented that these networks may fail to capture temporal dependency between distant time epochs .
To tackle these challenges, we propose Forecaster, a new deep
learning architecture for forecasting spatial and time-dependent data.
Our architecture consists of two parts. First, we use the theory of
Gaussian Markov random ﬁelds to learn the structure of the
graph that parsimoniously represents the spatial dependency between
the locations (we call such graph a dependency graph). Gaussian
Markov random ﬁelds model spatial and time-dependent data as a
multivariant Gaussian distribution over the spatial locations. We then
estimate the precision matrix of the distribution .2 The precision
matrix provides the graph structure with each node representing a
location and each edge representing the dependency between two locations. This contrasts prior work on forecasting — we learn from
the data its spatial dependency. Second, we integrate the dependency graph in the architecture of the Transformer for forecasting spatial and time-dependent data. The Transformer and its extensions have been shown to signiﬁcantly outperform RNN and CNN in NLP tasks, as they capture relations among
data at distant positions, signiﬁcantly improving the learning of longrange temporal dependency . In our Forecaster, in order to better
capture the spatial dependency, we associate each neuron in different layers with a spatial location. Then, we sparsify the Transformer
based on the dependency graph: if two locations are not connected in
2 The approach to estimate the precision matrix of a Gaussian Markov random ﬁeld (i.e., graphical lasso) can also be used with non-Gaussian distributions .
 
the graph, we prune the connection between their associated neurons.
In this way, the state encoding for each location is only impacted by
its own state encoding and encodings for other dependent locations.
Moreover, pruning the unnecessary connections in the Transformer
avoids overﬁtting.
To evaluate the effectiveness of our proposed architecture, we apply it to the task of forecasting taxi ride-hailing demand in New York
City . We pick 996 hot locations in New York City and forecast the hourly taxi ride-hailing demand around each location from
January 1st, 2009 to June 30th, 2016. Our architecture accounts for
crucial auxiliary information such as weather, day of the week, hour
of the day, and holidays. This improves signiﬁcantly the forecasting task. Evaluation results show that our architecture reduces the
root mean square error (RMSE) and mean absolute percentage error
(MAPE) of the Transformer by 8.8210% and 9.6192%, respectively,
and also show that our architecture signiﬁcantly outperforms other
state-of-the-art baselines.
In this paper, we present critical innovation:
• Forecaster combines the theory of Gaussian Markov random ﬁelds
with deep learning. It uses the former to ﬁnd the dependency graph
among locations, and this graph becomes the basis for the deep
learner forecast spatial and time-dependent data.
• Forecaster sparsiﬁes the architecture of the Transformer based on
the dependency graph, allowing the Transformer to capture better
the spatiotemporal dependency within the data.
• We apply Forecaster to forecasting taxi ride-hailing demand and
demonstrate the advantage of its proposed architecture over stateof-the-art baselines.
Methodology
In this section, we introduce the proposed architecture of Forecaster.
We start by formalizing the problem of forecasting spatial and timedependent data (Section 2.1). Then, we use Gaussian Markov random ﬁelds to determine the dependency graph among data at different locations (Section 2.2). Based on this dependency graph, we
design a sparse linear layer, which is a fundamental building block
of Forecaster (Section 2.3). Finally, we present the entire architecture
of Forecaster (Section 2.4).
Problem Statement
We deﬁne spatial and time-dependent data as a series of spatial signals, each collecting the data at all spatial locations at a certain time.
For example, hourly taxi demand at a thousand locations in 2019
is a spatial and time-dependent data, while the hourly taxi demand
at these locations between 8 a.m. and 9 a.m. of January 1st, 2019
is a spatial signal. The goal of our forecasting task is to predict the
future spatial signals given the historical spatial signals and historical/future auxiliary information (e.g., weather history and forecast).
We formalize forecasting as learning a function h (·) that maps T
historical spatial signals and T + T ′ historical/future auxiliary information to T ′ future spatial signals, as Equation (1):
RN, with xi
t the data at location i
at time t; N the number of locations; at the auxiliary information at
time t, at ∈RP , P the dimension of the auxiliary information;3 and
R is the set of the reals.
Gaussian Markov Random Field
We use Gaussian Markov random ﬁelds to ﬁnd the dependency graph
of the data over the different spatial locations. Gaussian Markov random ﬁelds model the spatial and time-dependent data {xt} as a multivariant Gaussian distribution over N locations, i.e., the probability
density function of the vector given by xt is
2 (xt −µ)T Q (xt −µ)
where µ and Q are the expected value (mean) and precision matrix
(inverse of the covariance matrix) of the distribution.
The precision matrix characterizes the conditional dependency between different locations — whether the data xi
t at the ith
and jth locations depend on each other or not given the data at all
the other locations x−ij
t | k ̸= i, j
). We can measure
the conditional dependency between locations i and j through their
conditional correlation coefﬁcient Corr
where Qij is the ith, jth entry of Q. In practice, we set a threshold
, and treat locations i and j as conditionally
dependent if the absolute value of Corr
is above the
threshold.
The non-zero entries deﬁne the structure of the dependency graph
between locations. Figure 1 shows an example of a dependency
graph. Locations 1 and 2 and locations 2 and 3 are conditionally dependent, while locations 1 and 3 are conditionally independent. This
principle example illustrates the advantage of Gaussian Markov random ﬁeld over ad-hoc pairwise similarity metrics — the former leads
to parsimonious (sparse) graph representations.
An example of a simple dependency graph.
We estimate the precision matrix by graphical lasso , an L1penalized maximum likelihood estimator:
tr (SQ) −log det (Q) + λ ∥Q∥1
Q = QT , Q ≻0
where S is the empirical covariance matrix computed from the data:
t=1 (xt −µ)T (xt −µ)
where M is the number of time samples used to compute S.
3 For simplicity, we assume in this work that different locations share the
same auxiliary information, i.e., at can impact xi
t, for any i. However, it
is easy to generalize our approach to the case where locations do not share
the same auxiliary information.
Building Block: Sparse Linear Layer
We use the dependency graph to sparsify the architecture of the
Transformer. This leads to the Transformer better capturing the spatial dependency within the data. There are multiple linear layers in
the Transformer. Our sparsiﬁcation on the Transformer replaces all
these linear layers by the sparse linear layers described in this section.
We use the dependency graph to build a sparse linear layer. Figure 2 shows an example (based on the dependency graph in Figure 1).
Suppose that initially the lth layer (of ﬁve neurons) is fully connected
to the l + 1th layer (of nine neurons). We assign neurons to the data
at different locations (marked as ”1”, ”2”, and ”3” for locations 1, 2,
and 3, respectively) and to the auxiliary information (marked as ”a”)
as illustrated next. How to assign neurons is a design choice for users.
In this example, assign one neuron to each location and two neurons
to the auxiliary information at the lth layer and assign two neurons
to each location and three neurons to the auxiliary information at the
l + 1th layer. After assigning neurons, we prune connections based
on the structure of the dependency graph. As locations 1 and 3 are
conditionally independent, we prune the connections between them.
We also prune the connections between the neurons associated with
locations and the auxiliary information to further simplify the architecture.4 This way, the encoding for the data at a location is only impacted by the encodings of itself and of its dependent locations, better
capturing the spatial dependency between locations. Moreover, pruning the unnecessary connections between conditionally independent
locations helps avoiding overﬁtting.
! + 1th layer
An example of a sparse linear layer based on the dependency
graph in Figure 1 (neurons marked as ”1”, ”2”, ”3”, and ”a” are for locations
1, 2, and 3, and auxiliary information, respectively).
Our sparse linear layer is similar to the state-of-the-art graph convolution approaches such as GCN and TAGCN — all of
them transform the data based on the adjacency matrix of the graph.
The major difference is our sparse linear layer learns the weights for
non-zero entries of the adjacency matrix (equivalent to the weights
of the sparse linear layer), considering that different locations may
have different strengths of dependency between each other.
Entire Architecture: Graph Transformer
Forecaster adopts an architecture similar to that of the Transformer
except for substituting all the linear layers in the Transformer with
our sparse linear layer designed based on the dependency graph. Figure 3 shows its architecture. Forecaster employs an encoder-decoder
architecture , which has been widely adopted in sequence generation tasks such as taxi demand forecasting and pose prediction . The encoder is used to encode the historical spatial signals
and historical auxiliary information; the decoder is used to predict
the future spatial signals based on the output of the encoder and the
4 However, our architecture still allows the encodings for the data at different
locations (i.e., the encoding for the spatial signal) to consider the auxiliary
information through the sparse multi-head attention layers in our architecture, which we will illustrate in the Section 2.4.
future auxiliary information. We omit what Forecaster shares with
the Transformer (e.g., positional encoding, multi-head attention) and
emphasize only on their differences in this section. Instead, we provide a brief introduction to multi-head attention in the appendix.
At each time step in the history, we concatenate the spatial signal
with its auxiliary information. This way, we obtain a sequence where
each element is a vector consisting of the spatial signal and the auxiliary information at a speciﬁc time step. The encoder takes this sequence as input. Then, a sparse embedding layer (consisting of a
sparse linear layer with ReLU activation) maps each element of this
sequence to the state space of the model and outputs a new sequence.
In Forecaster, except for the sparse linear layer at the end of the decoder, all the layers have the same output dimension. We term this dimension dmodel and the space with this dimension as the state space
of the model. After that, we add positional encoding to the new sequence, giving temporal order information to each element of the sequence. Next, we let the obtained sequence pass through N stacked
encoder layers to generate the encoding of the input sequence. Each
encoder layer consists of a sparse multi-head attention layer and a
sparse feedforward layer. These layers are the same multi-head attention layer and feedforward layer as in the Transformer, except that
sparse linear layers, which reﬂect the spatial dependency between locations, to replace linear layers within them. The sparse multi-head
attention layer enriches the encoding of each element with the information of other elements in the sequence, capturing the long-range
temporal dependency between elements. It takes each element as a
query, as a key, and also as a value. A query is compared with other
keys to obtain the similarities between an element and other elements, and then these similarities are used to weight the values to
Sparse Multi-Head
Sparse Embedding
Positional
Sparse Feed Forward
!"#$ ∥&"#$
' = −* + 1, … , 0
Sparse Embedding
Positional
!"0$#1 ∥&"0$
' = 1, … , *′
Sparse Masked
Multi-Head Attention
Sparse Multi-Head
Sparse Feed Forward
Sparse Linear
' = 1, … , *′
Architecture of Forecaster (a ∥b represents concatenating vector
a with vector b).
obtain the new encoding of the element. Note each query, key, and
value consists of two parts: the part for encoding the spatial signal
and the part for encoding the auxiliary information — both impact
the similarity between a query and a key. As a result, in the new encoding of each element, the part for encoding the spatial signal takes
into account the auxiliary information. The sparse feedforward layer
further reﬁnes the encoding of each element.
For each time step in the future, we concatenate its auxiliary information with the (predicted) spatial signal one step before. Then, we
input this sequence to the decoder. The decoder ﬁrst uses a sparse
embedding layer to map each element of the sequence to the state
space of the model, adds the positional encoding, and then passes it
through N stacked decoder layers to obtain the new encoding of each
element. Finally, the decoder uses a sparse linear layer to project
this encoding back and predict the next spatial signal. Similar to the
Transformer, the decoder layer contains two sparse multi-head attention layers and a sparse feedforward layer. The ﬁrst (masked) sparse
multi-head attention layer compares the elements in the sequence,
obtaining a new encoding for each element. Like the Transformer,
we put a mask here such that an element is compared with only earlier elements in the sequence. This is because, in the inference stage,
a prediction can be made based on only the earlier predictions and the
past history — information about later predictions are not available.
Hence, a mask needs to be placed here such that in the training stage
we also do the same thing as in the inference stage. The second sparse
multi-head attention layer compares each element of the sequence in
the decoder with the history sequence in the encoder so that we can
learn from the past history. If non-stationarity happens, the comparison will tell the element is different from the historical elements that
it is normally similar to, and therefore we should instead learn from
other more similar historical elements, handling this non-stationarity.
The following sparse feedforward layer further reﬁnes the encoding
of each element.
Evaluation
In this section, we apply Forecaster to the problem of forecasting
taxi ride-hailing demand in Manhattan, New York City. We demonstrate that Forecaster outperforms the state-of-the-art baselines (the
Transformer and DCRNN ) and a conventional time series
forecasting method (VAR ).
Evaluation Settings
Our evaluation uses the NYC Taxi dataset from 01/01/2009 to
06/30/2016 (7.5 years in total). This dataset records detailed information for each taxi trip in New York City, including its pickup and
dropoff locations. Based on this dataset, we select 996 locations with
hot taxi ride-hailing demand in Manhattan of New York City, shown
in Figure 4. Speciﬁcally, we compute the taxi ride-hailing demand at
each location by accumulating the taxi ride closest to that location.
Note that these selected locations are not uniformly distributed, as
different regions of Manhattan has distinct taxi demand.5 We compute the hourly taxi ride-hailing demand at these selected locations
5 We use the following algorithm to select the locations. Our roadmap has
5464 locations initially. Then, we compute the average hourly taxi demand
at each of these locations. After that, we use a threshold (= 10) and an it-
Longitude (degree)
Latitude (degree)
Selected Locations
Selected locations in Manhattan.
across time. As a result, our dataset contains 65.4 million data points
in total (996 locations × number of hours in 7.5 years). As far as
we know, it is the largest (in terms of data points) and longest (in
terms of time length) dataset in similar types of study. Our dataset
covers various types of scenarios and conditions (e.g., under extreme
weather condition). We split the dataset into three parts — training
set, validation set, and test set. Training set uses the data in the time
interval 01/01/2009 – 12/31/2011 and 07/01/2012 – 06/30/2015; validation set uses the data in 01/01/2012 – 06/30/2012; and the test set
uses the data in 07/01/2015 –06/30/2016.
Our evaluation uses hourly weather data from to construct
(part of) the auxiliary information. Each record in this weather data
contains seven entries — temperature, wind speed, precipitation, visibility, and the Booleans for rain, snow, and fog.
Details of the Forecasting Task
In our evaluation, we forecast taxi demand for the next three hours
based on the previous 674 hours and the corresponding auxiliary information (i.e., use a history of four weeks around; T = 674, T ′ = 3
in Equation (1)). Instead of directly inputing this history sequence
into the model, we ﬁrst ﬁlter it. This ﬁltering is based on the following observation: a future taxi demand correlates more with the
taxi demand at previous recent hours, the similar hours of the past
week, and the similar hours on the same weekday in the past several
weeks. In other words, we shrink the history sequence and only input
the elements relevant to forecasting. Speciﬁcally, our ﬁltered history
sequence contains the data for the following taxi demand (and the
corresponding auxiliary information):
• The recent past hours: xt−i, i = 0, ..., 5 ;
• Similar hours of the past week: xt+i−j×24, i = −1, ..., 5, j =
1, .., 6 ;
• Similar hours on the same weekday of the past several weeks:
xt+i−j×24×7, i = −1, ..., 5, j = 1, .., 4.
erative procedure to down select to the 996 hot locations. This algorithm
selects the locations from higher to lower demand. Every time when a location is added to the pool of selected locations, we compute the average
hourly taxi demand at each of the locations in the pool by remapping the
taxi rides to these locations. If every location in the pool has a demand no
less than the threshold, we will add the location; otherwise, remove it from
the pool. We reiterate this procedure over all the 5464 locations. This procedure guarantees that all the selected locations have an average hourly taxi
demand no less than the threshold.
Evaluation Metrics
Similar to prior work , we use root mean square error (RMSE) and mean absolute percentage error (MAPE) to
evaluate the quality of the forecasting results. Suppose that for
the jth forecasting job (j
1, · · · , S), the ground truth
| t = 1, · · · , T
′, i = 1, · · · , N
, and the prediction is
| t = 1, · · · , T
′, i = 1, · · · , N
, where N is the number of
locations, and T ′ is the length of the forecasted sequence. Then
RMSE and MAPE are:
Following practice in prior work , we set a threshold on xi(j)
when computing MAPE: if xi(j)
< 10, disregard the term associated
it. This practice prevents small xi(j)
dominating MAPE.
Models Details
We evaluate Forecaster and compare it against baseline models including VAR, DCRNN, and the Transformer.
Our model: Forecaster
Forecaster uses weather (7-dimensional vector), weekday (onehot encoding, 7-dimensional vector), hour (one-hot encoding, 24dimensional vector), and a Boolean for holidays (1-dimensional vector) as auxiliary information (39-dimensional vector). Concatenated
with a spatial signal (996-dimensional vector), each element of the
input sequence for Forecaster is a 1035-dimensional vector. Forecaster uses one encoder layer and one decoder layer (i.e., N = 1).
Except for the sparse linear layer at the end of the decoder, all the
layers of Forecaster use four neurons for encoding the data at each
location and 64 neurons for encoding the auxiliary information and
thus have 4048 neurons in total (i.e., dmodel = 4×996+64 = 4048).
The sparse linear layer at the end has 996 neurons. Forecaster uses
the following loss function:
loss (·) = η × RMSE2 + MAPE
where η is a constant balancing the impact of RMSE with MAPE,
η = 8 × 10−3.
Baseline model: Vector Autoregression
Vector autoregression (VAR) is a conventional multivariant time
series forecasting method. It predicts the future endogenous variables
(i.e., the spatial signal xt in our case) as a linear combination of the
past endogenous variables and the current exogenous variables (i.e.,
the auxiliary information at in our case):
ˆxt+1 = A1xt + · · · + Apxt−p+1 + Bat+1
where xt ∈RN, at+1 ∈RP , Ai ∈RN×N, i = 1, . . . , p, B ∈
RN×P . Matrices Ai and B are estimated during the training stage.
Our implementation is based on Statsmodels , a standard Python
package for statistics.
Baseline model: DCRNN
DCRNN is a deep learning model that models the dependency
relations between locations as a diffusion process guided by a predeﬁned distance metric. Then, it leverages graph CNN to capture
spatial dependency and RNN to capture the temporal dependency
within the data.
Baseline model: Transformer
The Transformer uses the same input and loss function as Forecaster. It also adopts a similar architecture except that all the layers
are fully-connected. For a comprehensive comparison, we evaluate
two versions of the Transformer:
• Transformer (same width): All the layers in this implementation
have the same width as Forecaster. The linear layer at the end of
decoder has a width of 996; other layers have a width of 4048 (i.e.,
dmodel = 4048).
• Transformer (best width): We vary the width of all the layers (except for the linear layer at the end of decoder which has a ﬁxed
width of 996) from 64 to 4096, and pick the best width in performance to implement.
Our evaluation of Forecaster starts by using Gaussian Markov random ﬁelds to determine the spatial dependency between the data at
different locations. Based on the method in Section 2.2, we can obtain a conditional correlation matrix where each entry of the matrix
represents the conditional correlation coefﬁcient between two locations. If the absolute value of an entry is less than a threshold, we will
treat the corresponding two locations as conditionally independent,
and round the value of the entry to zero. This threshold can be chosen
based only on the performance on the validation set. Figure 5 shows
the structure of the conditional correlation matrix under a threshold
of 0.1. We can see that the matrix is sparse, which means a location
generally depends on just a few other locations other than all the locations. We found that a location depends on only 2.5 other locations
on average. There are some locations which many other locations depend on. For example, there is a location in Lower Manhattan which
16 other locations depend on. This may be because there are many locations with signiﬁcant taxi demand in Lower Manhattan, with these
Location ID
Location ID
Structure of the conditional correlation matrix (under a threshold
of 0.1; each dot represents a non-zero entry).
RMSE and MAPE of Forecaster and baseline models.
Second next step
Third next step
5.3750 ± 0.0691
5.1627 ± 0.0644
5.4018 ± 0.0673
5.5532 ± 0.0758
Transformer (same width)
5.6802 ± 0.0206
5.4055 ± 0.0109
5.6632 ± 0.0173
5.9584 ± 0.0478
Transformer (best width)
5.6898 ± 0.0219
5.4066 ± 0.0302
5.6546 ± 0.0581
5.9926 ± 0.0472
Forecaster
5.1879 ± 0.0082
4.9629 ± 0.0102
5.2275 ± 0.0083
5.3651 ± 0.0065
24.9853 ± 0.1275
24.4747 ± 0.1342
25.0366 ± 0.1625
25.4424 ± 0.1238
Transformer (same width)
22.5787 ± 0.2153
21.8932 ± 0.2006
22.3830 ± 0.1943
23.4583 ± 0.2541
Transformer (best width)
22.2793 ± 0.1810
21.4545 ± 0.0448
22.1954 ± 0.1792
23.1868 ± 0.3334
Forecaster
20.1362 ± 0.0316
19.8889 ± 0.0269
20.0954 ± 0.0299
20.4232 ± 0.0604
locations sharing a strong dependency. Figure 6 shows the top 400
spatial dependencies. We see some long-range spatial dependency
between remote locations. For example, there is a strong dependency
between Grand Central Terminal and New York Penn Station, which
are important stations in Manhattan with a large trafﬁc of passengers.
Longitude (degree)
Latitude (degree)
Top 400 Connections
Grand Central Terminal
New York Penn Station
Top 400 dependency relations between locations.
After determining the spatial dependency between locations, we
use the graph Transformer architecture of Forecaster to predict the
taxi demand. Table 1 contrasts the performance of Forecaster to other
baseline models. Here we run all the evaluated deep learning models
six times (using different seeds) and report the mean and the standard deviation of the results. As VAR is not subject to the impact of
random initialization, we run it once. We can see for all the evaluated models, the RMSE and MAPE of predicting the next step are
lower than that of predicting later steps (e.g., the third next step).
This is because, for all the models, the prediction of later steps is
built upon the prediction of the next step, and thus the error of the
former includes the error of the latter. Comparing the performance
of these models, we can see the RMSE and MAPE of VAR is higher
than that of the deep learning models. This is because VAR does not
model well the non-linearity and non-stationarity within the data; it
also does not consider the spatial dependencies between locations
in the structure of its coefﬁcient matrices (matrices Ai and B in
Equation (8)). Among the deep learning models, DCRNN and the
Transformer perform similarly. The former captures the spatial dependency within the data but does not capture well the long-range
temporal dependency, while the latter focuses on exploiting the longrange temporal dependency but neglects the spatial dependency. As
for our method, Forecaster outperforms all the baseline methods at
every future step of forecasting. On average (over these future steps),
Forecaster achieves an RMSE of 5.1879 and a MAPE of 20.1362,
which is 8.8210% and 9.6192% better than Transformer (best width),
and 3.4809% and 19.4078% better than DCRNN. This demonstrates
the advantage of Forecaster in capturing both the spatial dependency
and the long-range temporal dependency.
Related Work
To our knowledge, this work is the ﬁrst (1) to integrate Gaussian
Markov Random ﬁelds with deep learning to forecast spatial and
time-dependent data, using the former to derive a dependency graph;
(2) to sparsify the architecture of the Transformer based on the dependency graph, signiﬁcantly improving the forecasting quality of
the result architecture. The most closely related work is a set of proposals on forecasting spatial and time-dependent data and the Transformer, which we brieﬂy review in this section.
Spatial and Time-Dependent Data Forecasting
Conventional methods for forecasting spatial and time-dependent
data such as ARIMA and Kalman ﬁltering-based methods 
usually impose strong stationary assumptions on the data, which
are often violated . Recently, deep learning-based methods have
been proposed to tackle the non-stationary and highly nonlinear nature of the data . Most of these works consist
of two parts: modules to capture spatial dependency and modules
to capture temporal dependency. Regarding spatial dependency, the
literature mostly uses prior knowledge such as physical closeness between regions to derive an adjacency matrix and/or pre-deﬁned distance/similarity metrics to decide whether two locations are dependent or not. Then, based on this information, they usually use a (standard or graph) CNN to characterize the spatial dependency between
dependent locations. However, these methods are not good predictors of dependency relations between the data at different locations.
Regarding temporal dependency, available works 
usually use RNNs and CNNs to extract the long-range temporal dependency. However, both RNN and CNN do not learn well the longrange temporal dependency, with the number of operations used to
relate signals at two distant time positions in a sequence growing at
least logarithmically with the distance between them .
We evaluate our architecture with the problem of forecasting taxi
ride-hailing demand around a large number of spatial locations. The
problem has two essential features: (1) These locations are not uniformly distributed like pixels in an image, making standard CNNbased methods not good for this problem; (2) it is desirable to perform multi-step forecasting, i.e., forecasting at several
time instants in the future, this implying that the work mainly designed for single-step forecasting is less applicable. DCRNN
 is the state-of-the-art baseline satisfying both features. Hence,
we compare our architecture with DCRNN and show that our work
outperforms DCRNN.
Transformer
The Transformer avoids recurrence and instead purely relies on
the self-attention mechanism to let the data at distant positions in a
sequence to relate to each other directly. This beneﬁts learning longrange temporal dependency. The Transformer and its extensions have
been shown to signiﬁcantly outperform RNN-based methods in NLP
and image generation tasks . It has also been
applied to graph and node classiﬁcation problems . However, it
is still unknown how to apply the architecture of Transformer to spatial and time-dependent data, especially to deal with spatial dependency between locations. Later work extends the architecture of
Transformer to video generation. Even though this also needs to address spatial dependency between pixels, the nature of the problem
is different from our task. In video generation, pixels exhibit spatial
dependency only over a short time interval, lasting for at most tens
of frames — two pixels may be dependent only for a few frames and
become independent in later frames. On the contrary, in spatial and
time-dependent data, locations exhibit long-term spatial dependency
lasting for months or even years. This fundamental difference of the
applications that we consider enables us to use Gaussian Markov random ﬁelds to determine the dependency graph as basis for sparsifying
the Transformer. Child et al. propose another sparse Transformer
architecture with a different goal of accelerating the multi-head attention operations in the Transformer. This architecture is very different
from our architecture.
Conclusion
Forecasting spatial and time-dependent data is challenging due to
complex spatial dependency, long-range temporal dependency, nonstationarity, and heterogeneity within the data. This paper proposes
Forecaster, a graph Transformer architecture to tackle these challenges. Forecaster uses Gaussian Markov random ﬁelds to determine
the dependency graph between the data at different locations. Then,
Forecaster sparsiﬁes the architecture of the Transformer based on
the structure of the graph and lets the sparsiﬁed Transformer (i.e.,
graph Transformer) capture the spatiotemporal dependency, nonstationarity, and heterogeneity in one shot. We apply Forecaster to
the problem of forecasting taxi-ride hailing demand at a large number
of spatial locations. Evaluation results demonstrate that Forecaster
signiﬁcantly outperforms state-of-the-art baselines (the Transformer
and DCRNN).
ACKNOWLEDGEMENTS
We thank the reviewers. This work is partially supported by NSF
CCF (award 1513936).