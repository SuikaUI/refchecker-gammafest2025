Revisiting Visual Question Answering Baselines
Allan Jabri, Armand Joulin, and Laurens van der Maaten
Facebook AI Research
{ajabri,ajoulin,lvdmaaten}@fb.com
Abstract. Visual question answering (VQA) is an interesting learning
setting for evaluating the abilities and shortcomings of current systems
for image understanding. Many of the recently proposed VQA systems include attention or memory mechanisms designed to support “reasoning”.
For multiple-choice VQA, nearly all of these systems train a multi-class
classiﬁer on image and question features to predict an answer. This paper questions the value of these common practices and develops a simple
alternative model based on binary classiﬁcation. Instead of treating answers as competing choices, our model receives the answer as input and
predicts whether or not an image-question-answer triplet is correct. We
evaluate our model on the Visual7W Telling and the VQA Real Multiple
Choice tasks, and ﬁnd that even simple versions of our model perform
competitively. Our best model achieves state-of-the-art performance on
the Visual7W Telling task and compares surprisingly well with the most
complex systems proposed for the VQA Real Multiple Choice task. We
explore variants of the model and study its transferability between both
datasets. We also present an error analysis of our model that suggests a
key problem of current VQA systems lies in the lack of visual grounding
of concepts that occur in the questions and answers. Overall, our results
suggest that the performance of current VQA systems is not signiﬁcantly
better than that of systems designed to exploit dataset biases.
Keywords: Visual question answering · dataset bias
Introduction
Recent advances in computer vision have brought us close to the point where
traditional object-recognition benchmarks such as Imagenet are considered to
be “solved” . These advances, however, also prompt the question how we can
move from object recognition to visual understanding; that is, how we can extend
today’s recognition systems that provide us with “words” describing an image or
an image region to systems that can produce a deeper semantic representation of
the image content. Because benchmarks have traditionally been a key driver for
progress in computer vision, several recent studies have proposed methodologies
to assess our ability to develop such representations. These proposals include
modeling relations between objects , visual Turing tests , and visual question
answering .
The task of Visual Question Answering (VQA) is to answer questions—posed
in natural language—about an image by providing an answer in the form of
Jabri, Joulin, and van der Maaten
What event is this?
taking place?
-Red and blue.
-A wedding.
-Day time.
-Graduation.
-Night time.
-A funeral.
-A picnic.
Fig. 1. Four images with associated questions and answers from the Visual7W dataset.
Correct answers are typeset in green.
short text. This answer can either be selected from multiple pre-speciﬁed choices
or be generated by the system. As can be seen from the examples in Figure 1,
VQA combines computer vision with natural language processing and reasoning.
VQA seems to be a natural playground to develop approaches able to perform basic “reasoning” about an image. Recently, many studies have explored
this direction by adding simple memory or attention-based components to VQA
systems. While in theory, these approaches have the potential to perform simple reasoning, it is not clear if they do actually reason, or if they do so in a
human-comprehensible way. For example, Das et al. recently reported that
“machine-generated attention maps are either negatively correlated with human
attention or have positive correlation worse than task-independent saliency”. In
this work, we also question the signiﬁcance of the performance obtained by current “reasoning”-based systems. In particular, this study sets out to answer a
simple question: are these systems better than baselines designed to solely capture the dataset bias of standard VQA datasets? We limit the scope of our study
to multiple-choice tasks, as this allows us to perform a more controlled study
that is not hampered by the tricky nuances of evaluating generated text .
We perform experimental evaluations on the Visual7W dataset and the
VQA dataset to evaluate the quality of our baseline models. We: (1) study
and model the bias in the Visual7W Telling and VQA Multiple Choice datasets,
(2) measure the eﬀect of using visual features from diﬀerent CNN architectures,
(3) explore the use of a LSTM as the system’s language model, and (4) study
transferability of our model between datasets.
Our best model outperforms the current state-of-the-art on the Visual7W
telling task with a performance of 67.1%, and competes surprisingly well with
the most complex systems proposed for the VQA dataset. Furthermore, our
models perform competitively even with missing information (that is, missing
images, missing questions, or both). Taken together, our results suggests that
the performance of current VQA systems is not signiﬁcantly better than that of
systems designed to exploit dataset biases.
Revisiting Visual Question Answering Baselines
Related work
The recent surge of studies on visual question answering has been fueled by the
release of several visual question-answering datasets, most prominently, the VQA
dataset , the DAQUAR dataset
 , the Visual Madlibs Q&A dataset ,
the Toronto COCO-QA dataset , and the Visual7W dataset . Most of these
datasets were developed by annotating subsets of the COCO dataset . Geman et al. proposed a visual Turing test in which the questions are automatically generated and require no natural language processing. Current approaches
to visual question answering can be subdivided into “generation” and “classiﬁcation” models:
Generation models. Malinowski et al. train a LSTM model to generate
the answer after receiving the image features (obtained from a convolutional network) and the question as input. Wu et al. extend a LSTM generation model
to use external knowledge that is obtained from DBpedia . Gao et al. 
study a similar model but decouple the LSTMs used for encoding and decoding.
Whilst generation models are appealing because they can generate arbitrary answers (also answers that were not observed during training), in practice, it is very
diﬃcult to jointly learn the encoding and decoding models from the questionanswering datasets of limited size. In addition, the evaluation of the quality of
the generated text is complicated in practice .
Classiﬁcation models. Zhou et al. study an architecture in which image
features are produced by a convolutional network, question features are produced
by averaging word embeddings over all words in the question, and a multi-class
logistic regressor is trained on the concatenated features; the top unique answers
are treated as outputs of the classiﬁcation model. Similar approaches are also
studied by Antol et al. and Ren et al. , though they use a LSTM to encode
the question text instead of an average over word embeddings. Zhu et al. 
present a similar method but extend the LSTM encoder to include an attention
mechanism for jointly encoding the question with information from the image.
Ma et al. replace the LSTM encoder by a one-dimensional convolutional
network that combines the word embeddings into a question embedding. Andreas et al. use a similar model but perform the image processing using a
compositional network whose structure is dynamically determined at run-time
based on a parse of the question. Fukui et al. propose the use of “bilinear
pooling” for combining multi-modal information. Lu et al. jointly learn a
hierarchical attention mechanism based on parses of the question and the image
which they call “question-image co-attention”.
Our study is similar to a recent study by Shih et al. , which also considers
models that treat the answer as an input variable and predicts whether or not
an image-question-answer triplet is correct. However, their study develops a substantially more complex pipeline involving image-region selection while achieving
worse performance.
Jabri, Joulin, and van der Maaten
Fig. 2. Overview of our system for visual question answering. See text for details.
System Overview
Figure 2 provides an overview of the architecture of our visual question answering system. The system takes an image-question-answer feature triplet as input.
Unless otherwise stated (that is, in the LSTM experiment of Section 4), both the
questions and the answers are represented by averaging word2vec embeddings
over all words in the question or answer, respectively. The images are represented using features computed by a pre-trained convolutional network. Unless
otherwise stated, we use the penultimate layer of Resnet-101 . The word2vec
embeddings are 300-dimensional and the image features are 2, 048-dimensional.
The three feature sets are concatenated and used to train a classiﬁcation model
that predicts whether or not the image-question-answer triplet is correct.
The classiﬁcation models we consider are logistic regressors and multilayer
perceptrons (MLP) trained on the concatenated features, and bilinear models that are trained on the answer features and a concatenation of the image
and question features. The MLP has 8, 192 hidden units unless otherwise speciﬁed. We use dropout after the ﬁrst layer. We denote the image, question,
and answer features by xi, xq, and xa, respectively. Denoting the sigmoid function σ(x) = 1/(1 + exp(−x)) and the concatenation operator xiq = xi ⊕xq, we
deﬁne the models as follows:
y = σ(Wxiqa + b)
iqWxa + b)
y = σ(W2 max(0, W1xiqa) + b).
The parameters of the classiﬁer are learned by minimizing the binary logistic
loss of predicting whether or not an image-question-answer triplet is correct using
stochastic gradient descent. During training we sampled two negative examples
from the multiple choices for each positive example, for a maximum of 300
epochs. The convolutional networks were pre-trained on the Imagenet dataset,
following , and were not further ﬁnetuned. We used pre-trained word2vec 
embeddings, which we did not ﬁnetune on VQA data either.
Revisiting Visual Question Answering Baselines
Table 1. Comparison of our models with the state-of-the-art for the Visual7W telling
task . Human accuracy on the task is 96.0%. Higher values are better.
LSTM (Q, I) 
LSTM-Att 
MCB + Att 
Bilinear (A, Q, I)
MLP (A, Q)
MLP (A, I)
MLP (A, Q, I)
Table 2. Comparison of our models with the state-of-the-art single models for the
VQA Real Multiple Choice task . Results are reported on the test2015-standard
split. Human accuracy on the task is 83.3%. * refers to results on test2015-dev.
Two-Layer LSTM 
Region selection 
Question-Image Co-Attention 
MCB + Att + GloVe + Genome *
Multi-modal Residual Network 
MLP (A, Q, I)
Experiments
We perform experiments on the following two datasets:
Visual7W Telling . The dataset includes 69, 817 training questions, 28, 020
validation questions, and 42, 031 test questions. Each question has four answer
choices. The negative choices are human-generated on a per-question basis. The
performance is measured by the percentage of correctly answered questions.
VQA Real Multiple Choice . The dataset includes 248, 349 questions for
training, 121, 512 for validation, and 244, 302 for testing. Each question has 18
answer choices. The negative choices are randomly sampled from a predeﬁned
set of answers. Performance is measured following the metric proposed by .
Comparison with State-of-the-Art
We ﬁrst compare the MLP variant of our model with the state-of-the-art. Table 1
shows the results of this comparison on Visual7W, using three variants of our
baseline with diﬀerent inputs: (1) answer and question (A+Q); (2) answer and
Jabri, Joulin, and van der Maaten
image (A+I); (3) and all three inputs (A+Q+I). The model achieves state-of-theart performance when it has access to all the information. Interestingly, as shown
by the results with the A+Q variant of our model, simply exploiting the most
frequent question-answer pairs obtains competitive performance. Surprisingly,
even a variant of our model that is trained on just the answers already achieves
a performance of 52.9%, simply by learning biases in the answer distribution.
In Table 2, we also compare our models with the published state-of-the-art
on the VQA dataset. Despite its simplicity, our baseline achieves comparable
performance with state-of-the-art models. We note that recent state-of-the-art
work used an ensemble of 7 models trained on additional data (the Visual Genome dataset ), performing 5% better than our model whilst being
substantially more complex.
Additional Experiments
In the following, we present the results of additional experiments to understand
why our model performs relatively well, and when it fails. All evaluations are
conducted on the Visual7W Telling dataset unless stated otherwise.
Table 3. Accuracy of models using either a softmax or a binary
loss. Results are presented for different models using answer, question and image. On VQA, we use
the test2015-dev split. Higher values are better.
Table 4. The ﬁve most similar answers in the Visual7W dataset for three answers
appearing in that dataset (in terms of cosine similarity between their feature vectors).
During the daytime.
On the bus stop bench.
On a tree branch.
During daytime.
Bus bench.
On the tree branch.
Outside, during the daytime.
In front of the bus stop.
The tree branch.
Inside, during the daytime.
The bus stop.
Tree branch.
In the daytime.
At the bus stop.
A tree branch.
In the Daytime.
The sign on the bus stop.
Tree branches.
Does it help to consider the answer as an input? In Table 4.2, we present
the results of experiments in which we compare the performance of our (binary)
baseline model with variants of the model that predict softmax probabilities over
a discrete set of the 5, 000 most common answers, as is commonly done in most
prior studies, for instance, .
The results in the table show a substantial advantage of representing answers
as inputs instead of outputs for the Visual7W Telling task and the VQA Real
Multiple Choice task. Taking the answer as an input allows the system to model
the similarity between diﬀerent answers. For example, the answers “two people”
Revisiting Visual Question Answering Baselines
Table 5. Accuracy on the Visual7W Telling task using visual features produced by
ﬁve diﬀerent convolutional networks. Higher values are better.
ResNet-101
and “two persons” are modeled by disjoint parameters in a softmax model,
whereas the binary model will assign similar scores to these answers because
they have similar bag-of-words word2vec representations.
To illustrate this, Table 4 shows examples of the similarities captured by the
BoW representation. For a given answer, the table shows the ﬁve most similar
answers in the dataset based on cosine similarity between the feature vectors.
The binary model can readily exploit these similarities, whereas a softmax model
has to learn them from the (relatively small) Visual7W training set.
Interestingly, the gap between the binary and softmax models is smaller on
the VQA datasets. This result may be explained by the way the incorrect-answer
choices were produced in both datasets: the choices are human-generated for each
question in the Visual7W dataset, whereas in the VQA dataset, the choices are
randomly chosen from a predeﬁned set that includes irrelevant correct answers.
What is the inﬂuence of convolutional network architectures? Nearly
all prior work on VQA uses features extracted using a convolutional network
that is pre-trained on Imagenet to represent the image in an image-question
pair. Table 5 shows to what extent the quality of these features inﬂuences the
VQA performance by comparing ﬁve diﬀerent convolutional network architectures: AlexNet , GoogLeNet , and residual networks with three diﬀerent
depths . While the performance on Imagenet is correlated with performance
in visual question answering, the results show this correlation is quite weak: a
reduction in the Imagenet top-5 error of 18% corresponds to an improvement of
only 3% in question-answering performance. This result suggests that the performance on VQA tasks is limited by either the fact that some of the visual
concepts in the questions do not appear in Imagenet, or by the fact that the
convolutional networks are only trained to recognize object presence and not to
predict higher-level information about the visual content of the images.
Accuracy on Visual7W Telling dataset
of a bag-of-words (BoW ) and a LSTM model. We
did not use image features to isolate the diﬀerence
between language models. Higher values are better.
Do recurrent networks improve over bag of words? Our baseline uses a
simple bag-of-words (BoW) model to represent the questions and answers. Recurrent networks (in particular, LSTMs ) are a popular alternative for BoW
Jabri, Joulin, and van der Maaten
Table 7. Accuracy on Visual7W of models (1) trained from scratch, (2) transfered
from the VQA dataset, and (3) ﬁnetuned after transferring. Higher values are better.
models. We perform an experiment in which we replace our BoW representations by a LSTM model. The LSTM was trained on the Visual7W Telling training set, using a concatenation of one-hot encodings and pre-trained word2vec
embeddings as input for each word in the question.
For the ﬁnal representation, we observed little diﬀerence between using the
average over time of the hidden states versus using only the last hidden state.
Here, we report the results using the last-state representation.
Table 6 presents the results of our experiment comparing BoW and LSTM
representations. To isolate the diﬀerence between the language models, we did
not use images features as input in this experiment. The results show that despite
their greater representation power, LSTMs actually do not outperform BoW
representations on the Visual7W Telling task, presumably, because the dataset is
quite small and the LSTM overﬁts easily. This may also explain why attentional
LSTM models perform poorly on the Visual7W dataset.
Can we transfer knowledge from VQA to Visual7W? An advantage of
the presented model is that it can readily be transfered between datasets: it
does not suﬀer from out-of-vocabulary problems nor does it require the set
of answers to be known in advance. Table 7 shows the results of a transferlearning experiment in which we train our model on the VQA dataset, and use
it to answer questions in the Visual7W dataset. We used three diﬀerent variants of our model, and experimented with three diﬀerent input sets. The table
presents three sets of results: (1) baseline results in which we trained on Visual7W from scratch, (2) transfer results in which we train on VQA but test
on Visual7W, and (3) results in which we train on VQA, ﬁnetune on Visual7W,
and then test on Visual7W.
The poor performance of the A+I transfer-learning experiment suggests that
there is a substantial diﬀerence in the answer distribution between both datasets,
especially since both use images from . Transferring the full model from VQA
to Visual7W works surprisingly well: we achieve 53.8% accuracy, which is less
than 2% worse than LSTM-Att , even though the model never learns from
Visual7W training data. If we ﬁnetune the transferred model on the Visual7W
dataset, it actually outperforms a model trained from scratch on that same
Revisiting Visual Question Answering Baselines
dataset, obtaining an accuracy of 68.5%. This additional boost likely stems
from the model adjusting to the biases in the Visual7W dataset.
Error Analysis
To better understand the shortcomings and limitations of our models, we performed an error analysis of the best model we obtained in Section 4 on six types
of questions, which are illustrated in Figure 3–5.
What is the color of
the tree leaves?
What is the color of
the train?
-Rectangle.
Fig. 3. Examples of good and bad predictions by our visual question answering model
on color and shape questions. Correct answers are typeset in green; incorrect predictions
by our model are typeset in red. See text for details.
Colors and Shapes. Approximately 5, 000 questions in the Visual7W test set
are about colors and approximately 200 questions are about shapes. While colors
and shapes are fairly simple visual features, our models only achieve around 57%
accuracy on these types of questions. For reference, our (A+Q) baseline already
achieves 52% in accuracy. This means that our models primarily learn the bias in
the dataset. For example, for shape, it predicts either “circle”, “round”, or “octagon” when the question is about a “sign”. For color questions, even though
the performances are similar, it appears that the image-based models are able
to capture additional information. For example, Figure 3 shows that the model
tends to predict the most salient color, but fails to capture color coming from
small objects, which constitute a substantial number of questions in the Visual7W dataset. This result highlights the limits of using global image features
in visual question answering.
Counting. There are approximately 5, 000 questions in the Visual7W test set
that involve counting the number of objects in the image (“how many ...?”). On
this type of questions, our model achieves an accuracy of 56%. This accuracy is
hardly better than that the 55% achieved by the (Q+A) baseline. Again, this
implies that our model does not really extract information from the image that
can be used for counting. In particular, our model has a strong preference for
answers such as: “none”, “one”, or “two”.
Jabri, Joulin, and van der Maaten
How many clouds are
in the sky?
How many giraﬀes sitting?
What is behind the
photographer?
What color leaves are
on the tree behind the
elephant on the left of
the photo?
-A dump truck.
-A plate of food.
Fig. 4. Examples of good and bad predictions by our visual question answering model
on counting and spatial reasoning. Correct answers are typeset in green; incorrect
predictions by our model are typeset in red. See text for details.
Spatial Reasoning. We refer to any question that refers to a relative position (“left”, “right”, “behind”, etc.) as questions about “spatial reasoning”.
There are approximately 1, 500 such questions in the Visual7W test set. On
questions requiring spatial reasoning, our models achieve an accuracy of approximately 55%, whereas a purely text-based model achieves an accuracy 50%. This
suggests that our models, indeed, extract some information from the images that
can be used to make inferences about spatial relations.
Actions. We refer to any question that asks what an entity is “doing” as an “action” question. There are approximately 1, 200 such questions in the Visual7W
test set. Our models achieve an accuracy of roughly 77% on action questions. By
contrast, the A+Q model achieves an accuracy of 63%, while the A+I model
achieves 75%. This result suggests that our model does learn to exploit image
features in recognizing actions, corroborating previous studies that show image
features transfer well to simple action-recognition tasks .
What is the man doing?
What is the man doing?
Why is his arm up?
-To serve the tennis ball.
-Playing tennis.
-About to hit the ball.
-Reaching for the ball.
-Concrete.
-Swinging his racket.
Fig. 5. Examples of good and bad predictions by our visual question answering model
on action and causality. Correct answers are typeset in green; incorrect predictions by
our model are typeset in red. See text for details.
Causality. “Why” questions test the model’s ability to capture a weak form of
causality. There are around 2, 600 of them. Our model has an accuracy of 68%
Revisiting Visual Question Answering Baselines
on such questions, but a simple text-based model already obtains 64%. This
means that most “why” questions can be answered by looking at the text. This
is unsurprising, as many of these questions refer to common sense that is encoded
in the text. For example, in Figure 5, one hardly needs the image to correctly
predict that the ground is “white” because of “snow” instead of “sand”.
Discussion and Future Work
This paper presented a simple alternative model for visual question answering
multiple choice, explored variants of this model, and experimented with transfer
between VQA datasets. Our study produced stronger baseline systems than those
presented in prior studies. In particular, our results demonstrate that featurizing
the answers and training a binary classiﬁer to predict correctness of an imagequestion-answer triplet leads to substantial performance improvements over the
current state-of-the-art on the Visual7W Telling task: our best model obtains an
accuracy of 67.1% when trained from scratch, and 68.5% when transferred from
VQA and ﬁnetuned on the Visual7W. On the VQA Real Multiple Choice task,
our model outperforms models that use LSTMs and attention mechanisms, and
is close to the state-of-the-art despite being very simple.
Our error analysis demonstrates that future work in visual question answering
should focus on grounding the visual entities that are present in the images, as
the “diﬃcult” questions in the Visual7W dataset cannot be answered without
such grounding. Whilst global image features certainly help in visual question
answering, they do not provide suﬃcient grounding of concepts of interest. More
precise grounding of visual entities, as well as reasoning about the relations
between these entities, is likely to be essential in making further progress.
Furthermore, in order to accurately evaluate future models, we need to understand the biases in VQA datasets. Many of the complex methods in prior
work perform worse than the simple model presented in this paper. We hypothesize that one of two things (or both) may explain these results: (1) it may be
that, currently, the best-performing models are those that can exploit biases in
VQA datasets the best, i.e., models that “cheat” the best; (2) it may be that
current, early VQA models are unsuitable for the diﬃcult task of visual question
answering, as a result of which all of them hit roughly the same ceiling in experiments and evaluations. In some of our experiments, we have seen that a model
that appears qualitatively better may perform worse quantitatively, because it
captures dataset biases less well. To address such issues, it may be necessary to
consider alternative evaluation criterions that are less sensitive to dataset bias.
Finally, the results of our transfer-learning experiments suggest that exploring the ability of VQA systems to generalize across datasets may be an interesting alternative way to evaluate such systems, the biases they learn, and the
underlying biases of datasets on which they are trained.
Jabri, Joulin, and van der Maaten