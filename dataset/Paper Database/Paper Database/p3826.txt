HAL Id: hal-03953027
 
Submitted on 23 Jan 2023
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Distributed under a Creative Commons Attribution - NonCommercial 4.0 International License
Gaussian Processes for Regression
Christopher K I Williams, Carl Edward Rasmussen
To cite this version:
Christopher K I Williams, Carl Edward Rasmussen. Gaussian Processes for Regression. Advances in
Neural Information Processing Systems, Nov 1995, Denver, United States. ￿hal-03953027￿
Gaussian Processes for Regression
Christopher K. I. Williams
Neural Computing Research Group
Aston University
Birmingham B4 7ET, UK
c.k.i.williamsaston.ac.uk
Carl Edward Rasmussen
Department of Computer .Science
University of Toronto
Toronto, ONT, M5S 1A4, Canada
carlcs.toronto.edu
The Bayesian analysis of neural networks is difficult because a sim­
ple prior over weights implies a complex prior distribution over
functions. In this paper we investigate the use of Gaussian process
priors over functions, which permit the predictive Bayesian anal­
ysis for fixed values of hyperparameters to be carried out exactly
using matrix operations. Two methods, using optimization and av­
eraging (via Hybrid Monte Carlo) over hyperparameters have been
tested on a number of challenging problems and have produced
excellent results.
INTRODUCTION
In the Bayesian approach to neural networks a prior distribution over the weights
induces a prior distribution over functions. This prior is combined with a noise
model, which specifies the probability of observing the targets t given function
values y, to yield a posterior over functions which can then be used for predictions.
For neural networks the prior over functions has a complex form which means
that implementations must either make approximations or use
Monte Carlo approaches to evaluating integrals .
As Neal has argued, there is no reason to believe that, for real-world prob­
lems, neural network models should be limited to nets containing only a "small"
number of hidden units. He has shown that it is sensible to consider a limit where
the number of hidden units in a net tends to infinity, and that good predictions can
be obtained from such models using the Bayesian machinery. He has also shown
that a large class of neural network models will converge to a Gaussian process prior
over functions in the limit of an infinite number of hidden units.
In this paper we use Gaussian processes specified parametrically for regression prob­
lems. The advantage of the Gaussian process formulation is that the combination of
the prior and noise models can be carried out exactly using matrix operations. We
also show how the hyperparameters which control the form of the Gaussian process
can be estimated from the data., using either a. maximum likelihood or Bayesian
approach, and that this leads to a form of "Automatic Relevance Determination"
 .
PREDICTION WITH GAUSSIAN PROCESSES
A stochastic process is a collection of random variables {Y ( z) I z E X} indexed by a.
set X. In our case X will be the input space with dimension d, the number of inputs.
The stochastic process is specified by giving the probability distribution for every
finite subset of variables Y(zC1)), ... , Y(z(k)) in a consistent manner. A Gaussian
process is a stochastic process which can be fully specified by its mean function
µ(z) = E[Y(z)] and its covariance function C(z,z') = E[(Y(z) - µ(z))(Y(z')­
µ(z'))]; any finite set of points will have a joint multivariate Gaussian distribution.
Below we consider Gaussian processes which have µ( z) = 0.
In section 2.1 we will show how to parameterise covariances using hyperparameters;
for now we consider the form of the covariance C as given. The training data
consists of n pairs of inputs and targets { ( z(i), t(i)), i = 1 .
. . n}. The input vector
for a test case is denoted z (with no superscript). The inputs are d-dimensional
x1, ... , xd and the targets are scalar.
The predictive distribution for a test case z is obtained from the n + 1 dimensional
joint Gaussian distribution for the outputs of the n training cases and the test
case, by conditioning on the observed targets in the training set. This procedure is
illustrated in Figure 1, for the case where there is one training point and one test
point. In general, the predictive distribution is Gaussian with mean and variance
C(z,z)-kT(z)K-1k(z),
where k(z)
= (C(z,z(1)), ... ,C(z,z(n)))T, [(is the covariance matrix for the
training cases Kij = C(z(i), :i;Ci)), and t = (t(l), ... , tCn))T.
The matrix inversion step in equations (1) and (2) implies that the algorithm has
O(n3) time complexity (if standard methods of matrix inversion are employed);
for a few hundred data points this is certainly feasible on workstation computers,
although for larger problems some iterative methods or approximations may be
PARAMETERIZING THE COVARIANCE FUNCTION
There are many choices of covariance functions which may be reasonable. Formally,
we are required to specify functions which will generate a non-negative definite
covariance matrix for any set of points (z(1), ... , z(k)). From a modelling point of
view we wish to specify covariances so that points with nearby inputs will give rise
to similar predictions. We find that the following covariance function works well:
vo exp{- I: w1(x
+ao + a1 L:x
j) + v1c5(i,j),
Figure 1: An illustration of prediction using a Gaussian process. There is one training
case (x(l), t(l)) and one test case for which we wish to predict y. The ellipse in the left­
hand plot is the one standard deviation contour plot of the joint distribution of y1 and
The dotted line represents an observation y1
In the right-hand plot we see
the distribution of the output for the test case, obtained by conditioning on the observed
target. The y axes have the same scale in both plots.
where 8 = log( v0, v1, w1, ... , wd, a0, a1) plays the role of hyperparameters1. We
define the hyperparameters to be the log of the variables in equation ( 4) since these
are positive scale-parameters.
The covariance function is made up of three parts; the first term, a linear regression
term (involving a0 and ai) and a noise term v16(i, j). The first term expresses the
idea that cases with nearby inputs will have highly correlated outputs; the wz pa­
rameters allow a different distance measure for each input dimension. For irrelevant
inputs, the corresponding w1 will become small, and the model will ignore that in­
put. This is closely related to the Automatic Relevance Determination (ARD) idea
of MacKay and Neal . The vo variable gives the overall
scale of the local correlations. This covariance function is valid for all input dimen­
sionalities as compared to splines, where the integrated squared mth derivative is
only a valid regularizer for 2m > d . ao and a1 are variables
controlling the scale the of bias and linear contributions to the covariance. The last
term accounts for the noise on the data; v1 is the variance of the noise.
Given a covariance function, the log likelihood of the training data is given by
l = -2 logdet I< - 2t K- t - 21og211".
In section 3 we will discuss how the hyperparameters in C can be adapted, in
response to the training data.
RELATIONSHIP TO PREVIOUS WORK
The Gaussian process view provides a unifying framework for many regression meth­
ods. ARMA models used in time series analysis and spline smoothing correspond to Gaussian process prediction with
1 We call 8 the hyperparameters as they correspond closely to hyperparameters in neural
networks; in effect the weights have been integrated out exactly.
a particular choice of covariance function2. Gaussian processes have also been used
in the geostatistics field , and are known there as "kriging", but
this literature has concentrated on the case where the input space is two or three
dimensional, rather than considering more general input spaces.
This work is similar to Regularization Networks , except that their derivation uses a smoothness functional
rather than the equivalent covariance function. Poggio et al suggested that the
hyperpara.meters be set by cross-validation. The main contributions of this paper
are to emphasize that a maximum likelihood solution for 8 is possible, to recognize
the connections to ARD and to use the Hybrid Monte Carlo method in the Bayesian
treatment (see section 3).
TRAINING A GAUSSIAN PROCESS
The partial derivative of the log likelihood of the training data l with respect to
all the hyperparameters can be computed using matrix operations, and takes time
O(n3). In this section we present two methods whjch can be used to adapt the
hyperparameters using these derivatives.
MAXIMUM LIKELIHOOD
In a maximum likelihood framework, we adjust the hyperparameters so as to max­
imize that likelihood of the training data. We initialize the hyperpara.meters to
random values (in a reasonable range) and then use an iterative method, for exam­
ple conjugate gradient, to search for optimal values of the hyperpa.rameters. Since
there are only a small number of hyperparameters ( d + 4) a relatively small number
of iterations a.re usually sufficient for convergence. However, we have found that
this approach is sometimes susceptible to local minima, so it is advisable to try a
number of random starting positions in hyperpara.meter space.
INTEGRATION VIA HYBRID MONTE CARLO
According to the Bayesian formalism, we should start with a prior distribution P( 8)
over the hyperpa.ra.meters which is modified using the training data D to produce
a posterior distribution P(8ID). To make predictions we then integrate over the
posterior; for example, the predicted mean y( z) for test input z is given by
y(#) = j Ye()P(OID)dO
where Y(}(z) is the predicted mean (as given by equation 1) for a particular value of
6. It is not feasible to do this integration analytically, but the Markov cha.in Monte
Carlo method of Hybrid Monte Carlo (HMC) seems promising
for this application. We assign broad Gaussia.ns priors to the hyperpa.ra.meters, and
use Hybrid Monte Carlo to give us samples from the posterior.
HMC works by creating a fictitious dynamical system in which the hyperpara.meters
a.re regarded as position variables, and augmenting these with momentum variables
p. The purpose of the dynamical system is to give the hyperpa.ra.meters "inertia,,
so that random-walk behaviour in 8-spa.ce can be avoided. The total energy, H, of
the system is the sum of the kinetic energy, J(, (a function of the momenta.) and the
potential energy, E. The potential energy is defined such that p(BID) ex: exp(-E).
We sample from the joint distribution for 8 and p given by p(8,p) ex: exp(-E -
2Technically splines require generalized covariance functions.
K); the marginal of this distribution for (J is the required posterior. A sample of
hyperparameters from the posterior can therefore be obtained by simply ignoring
the momenta.
Sampling from the joint distribution is achieved by two steps: (i) finding new points
in phase space with near-identical energies H by simulating the dynamical system
using a discretised approximation to Hamiltonian dynamics, and (ii) changing the
energy H by doing Gibbs sampling for the momentum variables.
Hamiltonian Dynamics
Hamilton's first order differential equations for H are approximated by a discrete
step (specifically using the leapfrog method). The derivatives of the likelihood
(equation 4) enter through the derivative of the potential energy. This proposed
state is then accepted or rejected using the Metropolis rule depending on the final
energy H* (which is not necessarily equal to the initial energy H because of the
discretization). The same step size€ is used for all hyperparameters, and should be
as large as possible while keeping the rejection rate low.
Gibbs Sampling for Momentum Variables
The momentum variables are updated using a modified version of Gibbs sampling,
thereby allowing the energy H to change. A "persistence" of 0.95 is used; the new
value of the momentum is a weighted sum of the previous value (with weight 0.95)
and the value obtained by Gibbs sampling (weight (1 - 0.952)112). With this form
of persistence, the momenta change approximately twenty times more slowly, thus
increasing the "inertia" of the hyperparameters, so as to further help in avoiding
random walks. Larger values of the persistence will further increase the inertia, but
reduce the rate of exploration of H.
Practical Details
The priors over hyperparameters are set to be Gaussian with a mean of -3 and a
standard deviation of 3. In all our simulations a step size€ = 0.05 produced a very
low rejection rate ( < 1 %). The hyperparameters corresponding to v1 and to the
w1 's were initialised to -2 and the rest to 0.
To apply the method we first rescale the inputs and outputs so that they have mean
of zero and a variance of one on the training set. The sampling procedure is run
for the desired amount of time, saving the values of the hyperparameters 200 times
during the last two-thirds of the run. The first third of the run is discarded; this
"burn-in" is intended to give the hyperparameters time to come close to their equi­
librium distribution. The predictive distribution is then a mixture of 200 Gaussians.
For a squared error loss, we use the mean of this distribution as a point estimate.
The width of the predictive distribution tells us the uncertainty of the prediction.
EXPERIMENTAL RESULTS
We report the results of prediction with Gaussian process on (i) a modified version
of MacKay's robot arm problem and (ii) five real-world data sets.
THE ROBOT ARM PROBLEM
We consider a version of MacKay's robot arm problem introduced by Neal .
The standard robot arm problem is concerned with the mappings
No. of inputs
sum squared test error
Gaussian process
Gaussian process
Table 1: Results on the robot arm task. The bottom three lines of data were obtained
from Neal . The MacKay result is the test error for the net with highest "evidence".
The data was generated by picking x1 uniformly from [-1.932, -0.453] and [0.453,
1.932) and picking x2 uniformly from [0.534, 3.142]. Neal added four further inputs,
two of which were copies of x1 and x2 corrupted by additive Gaussian noise of
standard deviation 0.02, and two further irrelevant Gaussian-noise inputs with zero
mean and unit variance. Independent zero-mean Gaussian noise of variance 0.0025
was then added to the outputs Y1 and Y2· We used the same datasets as Neal and
MacKay, with 200 examples in the training set and 200 in the test set.
The theory described in section 2 deals only with the prediction of a scalar quantity
Y, so predictors were constructed for the two outputs separately, although a joint
prediction is possible within the Gaussian process framework .
Two experiments were conducted, the first using only the two "true" inputs, and
the second one using all six inputs. In this section we report results using max­
imum likelihood training; similar results were obtained with HMC. The log( v )'s
and log(w)'s were all initialized to values chosen uniformly from [-3.0, 0.0), and
were adapted separately for the prediction of y1 and y2 (in these early experiments
the linear regression terms in the covariance function involving ao and a1 were not
present). The conjugate gradient search algorithm was allowed to run for 100 iter­
ations, by which time the likelihood was changing very slowly. Results are reported
for the run which gave the highest likelihood of the training data, although in fact
all runs performed very similarly. The results are shown in Table 1 and are encour­
aging, as they indicate that the Gaussian process approach is giving very similar
performance to two well-respected techniques. All of the methods obtain a level of
performance which is quite close to the theoretical minimum error level of 1.0 . ....Zt is
interesting to look at the values of the w's obtained after the optimization; for the
Y2 task the values were 0.243, 0.237, 0.0639, 7.0 x 10-4, 2.32x10-6,1.70 x 10-6,
and vo and v1 were 7.5278 and 0.0022 respectively. The w values show nicely that
the first two inputs are the most important, followed by the corrupted inputs and
then the irrelevant inputs. During training the irrelevant inputs are detected quite
quickly, but the w's for the corrupted inputs shrink more slowly, implying that the
input noise has relatively little effect on the likelihood.
FIVE REAL-WORLD PROBLEMS
Gaussian Processes as described above were compared to several other regression
algorithms on five real-world data sets in . The
data sets had between 80 and 256 training examples, and the input dimension
ranged from 6 to 16. The length of the HMC sampling for the Gaussian processes
was from 7 .5 minutes for the smallest training set size up to 1 hour for the largest
ones on a R4400 machine. The results rank the methods in the order (lowest error
first) a full-blown Bayesian treatment of neural networks using HMC, Gaussian
processes, ensembles of neural networks trained using cross validation and weight
decay, the Evidence framework for neural networks , and MARS.
We are currently working on assessing the statistical significance of this ordering.
DISCUSSION
We have presented the method of regression with Gaussian processes, and shown
that it performs well on a suite of real-world problems.
We have also conducted some experiments on the approximation of neural nets (with
a finite number of hidden units) by Gaussian processes, although space limitations
do not allow these to be described here. Some other directions currently under
investigation include (i) the use of Gaussian processes for classification problems by
softmaxing the outputs of k regression surfaces (for a k-class classification problem),
(ii) using non-stationary covariance functions, so that C(x, a:') ¥ C(lx - x'I) and
(iii) using a covariance function containing a sum of two or more terms of the form
given in line 1 of equation 3.
We hope to make our code for Gaussian process prediction publically available in the
near future. Check details.
Acknowledgements
We thank Radford Neal for many useful discussions, David MacKay for generously provid­
ing the robot arm data used in this paper, and Chris Bishop, Peter Dayan, Radford Neal
and Huaiyu Zhu for comments on earlier drafts. CW was partially supported by EPSRC
grant GR/ J75425.