Ranking Outliers Using Symmetric Neighborhood Relationship
Wen Jin1, Anthony K. H. Tung2, Jiawei Han3, and Wei Wang4
1 School of Computing Science,Simon Fraser University
 
2 Department of Computer Science, National University of Singapore
 
3 Department of Computer Science, Univ. of Illinois at Urbana-Champaign
 
4 Department of Computer Science, Fudan University
 
Abstract. Mining outliers in database is to ﬁnd exceptional objects that deviate from the rest of the data
set. Besides classical outlier analysis algorithms, recent studies have focused on mining local outliers, i.e.,
the outliers that have density distribution signiﬁcantly diﬀerent from their neighborhood. The estimation of
density distribution at the location of an object has so far been based on the density distribution of its k-nearest
neighbors . However, when outliers are in the location where the density distributions in the neighborhood
are signiﬁcantly diﬀerent, for example, in the case of objects from a sparse cluster close to a denser cluster,
this may result in wrong estimation. To avoid this problem, here we propose a simple but eﬀective measure on
local outliers based on a symmetric neighborhood relationship. The proposed measure considers both neighbors
and reverse neighbors of an object when estimating its density distribution. As a result, outliers so discovered
are more meaningful. To compute such local outliers eﬃciently, several mining algorithms are developed that
detects top-n outliers based on our deﬁnition. A comprehensive performance evaluation and analysis shows that
our methods are not only eﬃcient in the computation but also more eﬀective in ranking outliers.
Introduction
From a knowledge discovery standpoint, outliers are often more interesting than the common ones since they contain
useful information underlying the abnormal behavior. Basically, an outlier is deﬁned as an exceptional object that
deviates much from the rest of the dataset by some measure. Outlier detection has many important applications
in fraud detection, intrusion discovery, video surveillance, pharmaceutical test and weather prediction. Various data
mining algorithms for outlier detection were proposed. The outlierness of an
object typically appears to be more outstanding with respect to its local neighborhood. For example, a network
intrusion might cause a signiﬁcant spike in the number of network events within a low traﬃc period, but this spike
might be insigniﬁcant when a period of high network traﬃc is also included in the comparison. In view of this,
recent work on outlier detection has been focused on ﬁnding local outliers, which are essentially objects that have
signiﬁcantly lower density 5 than its local neighborhood . As an objective measure, the degree of outlierness of an
object p is deﬁned to be the ratio of its density and the average density of its neighboring objects .
To quantify what are p’s neighboring objects, users must specify a value k, and neighboring objects are deﬁned as
objects which are not further from p than p’s kth nearest objects 6. As an example, let us look at Figure 1 in which
k is given a value of 3. In this case, the three neighboring objects of p will have higher density than p and thus p will
have a high degree of outlierness according to the deﬁnition in . This is obviously correct based on our intuition.
Unfortunately, the same cannot hold in more complex situation. Let us look at the following example.
Example 1: We consider Figure 2 in which p is in fact part of a sparse cluster C2 which is near the dense cluster
C1. Compared to objects q and r, p obviously displays less outlierness. However, if we use the measure proposed in
 , p could be mistakenly regarded to having stronger outlierness in the following two cases:
Case I: The densities of the nearest neighboring objects for both p and q are the same, but q is slightly closer to
cluster C1 than p. In this case, p will have a stronger outlierness measure than q, which is obviously wrong.
5 The density of an object p is deﬁned as 1/kdist(p) where k is a user-supplied parameter and kdist(p) is the distance of the
kth nearest object to p.
6 Note that p ’s kth nearest neighbor might not be unique and thus p could have more than k neighboring objects.
local outlier
Fig. 1. A local outlier, p
Fig. 2. Comparing the outlierness of p, q, r
Fig. 3. Taking RNNs of p into account
Case II: Although the density of r is lower than p, the average density of its neighboring objects (consisting of 2
objects from C2 and an outlier) is less than those of p. Thus, when the proposed measure is computed, p could turn
out to have a stronger outlierness measure than r, which again is wrong.
Note that the two cases we described are not only applicable to p but also to the two objects above and below p.
In general, any member of C2 that is lying near the border between the two clusters could have been misclassiﬁed as
showing stronger outlierness than q and r.
From these examples, we can see that existing outlierness measure is not easily applicable to complex situation in
which the dataset contains multiple clusters with very diﬀerent density distribution. The reason for the above problem
lies in the inaccurate estimation for the density distribution of an object’s neighborhood. In Figure 2, although p
belongs to cluster C2, it is closer to cluster C1, and thus the estimation of p’s neighborhood density distribution is
derived from C1 instead of C2.
To get a better estimation of the neighborhood’s density distribution, we propose to take both the nearest neighbors (NNs) and reverse nearest neighbors (RNNs) into account. The RNNs of an object p are essentially objects
that have p as one of their k nearest neighbors. By considering the symmetric neighborhood relationship of both NN
and RNN, the space of an object inﬂuenced by other objects is well determined, the densities of its neighborhood
will be reasonably estimated, and thus the outliers found will be more meaningful. As a simple illustration in Figure
3 which depicts the same situation as Figure 2, we show that p has two RNNs: s and t. This distinguishes it from q
which has no RNNs, and r which has only an outlier as its RNNs. Later on in this paper, we will show how such an
observation can be incorporated to ensure that the outlierness measure for p will indicate that it is a weaker outlier
than both q and r. We now summarize our contributions in this paper:
(1) We propose the mining of outliers based on a symmetric neighborhood relationship. The proposed method
considers the inﬂuenced space considering both neighbors and reverse neighbors of an object when estimating its
neighborhood density distribution. To the best of our knowledge, previous work of outlier detection has not considered
the eﬀect of RNN. Such a symmetric relationship between NNs and RNNs will make the outlierness measurement
more robust and semantically correct comparing to the existing method.
(2) We assign each object of database the degree of being INFLuenced Outlierness(INFLO). The higher INFLO
is, the more likely that this object is an outlier. The lower INFLO is, the more likely that this object is a member
of a cluster. Speciﬁcally, INFLO ≈1 means the object locates in the core part of a cluster.
(3) We present several eﬃcient algorithms to mining top-n outliers based on INFLO. To reduce the expensive cost
incurred by a large number of KNN and RNN search, a two-way search method is developed by dynamically pruning
those objects with value INFLO ≈1 during the search process. Furthermore, we take advantage of the micro-cluster
 technique to compress dataset for eﬃcient symmetric queries, and use two-phase pruning method to prune out
those objects which will never be among the top-n outliers.
(4) Last but not the least, we give a comprehensive performance evaluation and analysis on synthetic and real
data sets. It shows that our method is not only eﬃcient and scalable in performance, but also eﬀective in ranking
meaningful outliers.
The rest of this paper is organized as follows. In section 2, we formally deﬁne a new outlier measurement using
symmetric neighborhood relationship and discuss some of its important properties. In section 3, we propose eﬃcient
methods for mining and ranking outliers in databases. In section 4, a comprehensive performance evaluation is made
and the results are analyzed. Related work is discussed in section 5 and section 6 concludes the paper.
Inﬂuential Measure of Outlierness by Symmetric Relationship
In this section, we will introduce our new measure and related properties. The following notations will be used in the
remaining of the paper. Let D be a database of size N, let p, q and o be some objects in D, and let k be a positive
integer. We use d(p, q) to denote the Euclidean distance between objects p and q.
Deﬁnition 1. (k-distance and nearest neighborhood of p) The k-distance of p, denoted as kdist(p), is the
distance d(p, o) between p and o in D, such that: (1) at least for k objects o′ ∈D it holds that d(p, o′) ≤d(p, o), and
(2) at most for (k −1) objects o′ ∈D it holds that d(p, o′) < d(p, o). The k-nearest neighborhood of p, NNk(p) is a
set of objects X in D with d(p, X) ≤kdist(p): NNk(p) = {X ∈D\{p}| d(p, X) ≤kdist(p))}.
Deﬁnition 2. (local density of p) The density of p, denoted as den(p), is the inverse of the k-distance of p, i.e.,
den(p) = 1/kdist(p).
Although the k-nearest neighbor of p may not be unique, kdist(p) is unique. Hence, the density of p is also unique.
The nearest neighbor relation is not symmetric. For a given p, the nearest neighbors of p may not have p as one of
their own nearest neighbors. As we discussed in Section 1, these neighbors should also be taken into account when
the outlierness of p is computed. Therefore, we introduce the concept of reverse nearest neighbors as follows.
Deﬁnition 3. (reverse nearest neighborhood of p) The reverse k-nearest neighborhood RNN is an inverse
relation which can be deﬁned as: RNNk(p) = {q|q ∈D, p ∈NNk(q)}.
Fig. 4. RNN and Inﬂuence Space
For any object p ∈D, NNk search always returns at least k results, while
the RNN can be empty, or have one or more elements. By combining NNk(p)
and RNNk(p) together in a novel way, we form a local neighborhood space
which will be used to estimate the density distribution around p. We call this
neighborhood space the k-inﬂuence space for p, denoted as ISk(p).
Example 2: Figure 4 gives a simple description of how to obtain RNN in
{p, q1, q2, q3, q4, q5} when k = 3. NNk(q1) = {p, q2, q4}, NNk(q2) =
{p, q1, q3}, NNk(q3) = {q1, q2, q5}, NNk(q4) = {p, q1, q2, q5}, NNk(q5)
= {q1, q2, q3}. During the search of k-nearest neighbors of p, q1, q2, q3, q4
and q5, RNNk(p) = {q1, q2, q4} is incrementally built. Similarly, RNNk(q1),
RNNk(q2), RNNk(q3), RNNk(q4) and RNNk(q5) are found. Note that
NNk(p) = {q1, q2, q4} = RNNk(p) (here IS3(p) = {q1, q2, q4}). If the value
of k changes, RNNk(p) may not be equal to NNk(p), or totally diﬀerent.
Unlike the nearest neighborhood, the inﬂuence space for an object p contains inﬂuential objects aﬀecting p, more precisely estimating density around p ’s neighborhood w.r.t. these objects.
Deﬁnition 4. (inﬂuenced outlierness of p) The inﬂuenced outlierness is deﬁned as: INFLOk(p) = denavg(ISk(p))
where denavg(ISk(p)) =
o∈ISk(p) den(o)
INFLO is the ratio of the average density of objects in ISk(p) to p’s local density. p’s INFLO will be very high
if its density is much lower than those of its inﬂuence space objects. In this sense, p will be an outlier. We can assert
p is a local outlier if INFLOk(p) > t where t ≫1. On the other hand, objects with density very close to those in
their inﬂuence space will have INFLO ≈1. Without loss of generality, we assume that for any local outlier object
q (INFLO(q) > t), we have |RNNk(q)| < j(a value < k), and any non-local outlier p cannot belong to RNNk(q).
Lemma 1. Given any object p, q ∈D, if maxp′∈ISk(p) kdist(p′) < minq′∈ISk(q) kdist(q′) then denavg(ISk(p)) >
denavg(ISk(q)).
Proof. denavg(ISk(p)) =
p′∈ISk(p) den(p′)
|ISk(p)|·1/ maxp′∈ISk(p) kdist(p′)
|ISk(p)|·1/ minq′∈ISk(q) kdist(q′)
|ISk(q)|·1/ minq′∈ISk(q) kdist(q′)
q′∈ISk(q) den(q′)
= denavg(ISk(q))
Lemma 2. For p ∈D, if
maxq′∈ISk(p)kdist(q′) > t, then p is a local outlier.
Proof. INFLOk(p) = denavg(ISk(p))
p′∈ISk(p) den(p′)
|ISk(p)|·den(p)
|ISk(p)|·1/ maxp′∈ISk(p) kdist(p′)
|ISk(p)|·den(p)
maxq′∈ISk(p)kdist(q′) > t.
Lemma 3. For p ∈D, if there exists r ∈RNNk(p) such that kdist(p) ≤kdist(r) ≤kdist(q) where q ∈NNk(RNNk(p)),
r ̸= q and denavg(ISk(q))
> t, then p is a local outlier.
Proof. Since kdist(p) ≤kdist(q), so q ∈NNk(p)∩RNNk(p), thus maxp′∈ISk(p) kdist(p′) = maxp′∈NNk(p)∪RNNk(p) kdist(p′)
= maxp′∈RNNk(p) kdist(p′) ≤kdist(r) = minq′∈NNk(q)∪RNNk(q) kdist(q′) = minq′∈ISk(q) kdist(q′). Based on Lemma
1, denavg(ISk(p)) > denavg(ISk(q)), so INFLOk(p) = denavg(ISk(p))
= denavg(ISk(p)) · kdist(p) > denavg(ISk(q)) ·
kdist(p) = denavg(ISk(q))
> t. So p is a local outlier.
Lemma 4. For p ∈D, the value of RNNk(p)∩NNk(p)
is proportional to the density value of p.
Proof. Because the size of any cluster should be larger than k (usually k = MinPts ), the higher the above ratio,
the more inﬂuence for the local neighborhood to the object, and the higher density for this object.
Mining Algorithms for Inﬂuence Outliers using Symmetric Relationship
Essentially, mining inﬂuenced outliers is based on the problem of ﬁnding the inﬂuence space of objects, which is in
KNN and RNN. In this section, we provide several techniques for ﬁnding inﬂuenced outliers, including the naive
index-based method, the two-way search method and the micro-cluster method.
A Naive Index-based method
Finding inﬂuence outliers requires the operations of KNN and RNN for each object in the database, so the search
cost is huge. If we maintain all the points in a spatial index like R-tree, the cost of range queries can be greatly
reduced by the state-of-the-art pruning technique . Suppose that we have computed the temporary kdist(p) by
checking a subset of the objects, the value that we have is clearly an upper bound for the actual kdist(p). If the
minimum distance between p and the MBR 7 of a node in the R-tree (called MinDist(p, MBR)) is greater than the
kdist(p) value that we currently have, none of the objects in the subtree rooted under the node will be among the
k-nearest neighbors of p. This optimization can prune entire sub-tree containing points irrelevant to the KNN search
for p. Along with the search of KNN, the RNN of each object can be dynamically maintained in R-tree . After
building the index of KNN and RNN, the outlier inﬂuence degree can be calculated and ranked. The following
algorithm is to mining top-n INFLO by building KNN and RNN index within R-tree.
Algorithm 1 Index-based method.
Input: k, D, n, the root of R-tree.
Output: Top-n INFLO of D.
1. FOR each object p ∈D DO
MBRList = root; kdist(p) = ∞; heap = 0;
WHILE (MBRList) != empty DO
Delete 1st MBR from MBRList;
IF (1stMBR is a leaf) THEN
FOR each object q in 1stMBR DO
IF (d(p, q) < kdist(p)) AND (heap.size < k) THEN
heap.insert(q);
kdist(p) = d(p, heap.top);
Append MBR’s children to MBRList;
Sort nodeList by MinDist;
7 Minimum bounding rectangle
FOR each MBR in MBRList DO
IF (kdist(p) ≤MinDist(p, MBR)) THEN
Remove Node from MBRList;
16. FOR each object q in heap DO
Add q into NNk(p), add p into RNNk(q);
18.FOR each object p ∈D DO
19. Ascending sort top-n INFLO from KNN and RNN;
Here MBRs are stored in ascending order based on MinDist(p, MBR), as lines 11-12. The algorithm searches
KNNp only in those MBRs with MinDist smaller than the temporary kdist(p), otherwise these MBRs are pruned
(lines 13-15). If any nearer object is located (lines 6-7), it will be inserted into the heap and the current kdist(p) will
be updated (lines 8-9). Whenever NNk(p) are found, they are stored as p’s nearest neighbors. Meanwhile, it need
store p as a reverse nearest neighbor (lines 16-17). Finally, INFLO is calculated based on KNN and RNN index.
A Two-way search method
Two major factors hamper the eﬃciency of the previous algorithm. First, for any object p, RNN space cannot be
determined unless all the other objects have ﬁnished nearest neighbor search. Second, large amount of extra storage
is required on R-tree, where each object at least stores k pointers of its KNN, and stores m pointers (m varies from
0 to o(k)) for its RNN. The total space cost will be prohibitive. Therefore, we need reduce the computation cost
for RNN and corresponding storage cost. By analyzing the characteristics of INFLO, it is clear that any object as
a member of a cluster must have INFLO ≈1 even without INFLO calculation. So we can prune oﬀthese cluster
objects, saving not only the computation cost but also the extra storage space.
Theorem 1. For p ∈D, if for each object q ∈NNk(p), it always exists p ∈NNk(q), then INFLOk(p) ≈1.
Proof. Because for each q ∈NNk(p), p ∈NNk(q), p and its nearest neighbors are close to each other. They are
actually in a mostly mutual-inﬂuenced neighborhood. Since k is potentially the number of objects forming a cluster,
under this circumstance, p resides in core part of a cluster.
To apply this theorem, we will ﬁrst search p’s k-nearest neighbor, then dynamically ﬁnd the NNk for each of
these nearest neighbors. If NNk(NNk(p)) still contains p, which shows p is in a closely inﬂuenced space and is a core
object of a cluster (INFLOk(p) ≈1), we can prune p immediately without searching corresponding RNN. Such a
early pruning technique will improve the performance signiﬁcantly. The two-way search algorithm is given as follows:
Algorithm 2 A Two-way search method.
Input: k, D, n, the root of R-tree, a threshold M.
Output: Top-n INFLO of D
1. FOR each p ∈D DO
count = |RNNk(p)|;
IF unvisited(p) THEN
S = getKNN(p); //search k-nearest neighbors
unvisited(p) = FALSE;
S = KNN(p); //get nearest neighbors directly
FOR each object q ∈S DO
IF unvisited(q) THEN
T = getKNN(q); unvisited(q) = FALSE;
IF p ∈T THEN
Add q into RNNk(p);
Add p into RNNk(q);
count + +;
15. IF count ≥|S| ∗M THEN //M is a threshold
Label p pruned mark;
17.FOR each object p ∈D′ DO //D′ is unpruned database
18. Ascending sort top-n INFLO from KNN and RNN;
The algorithm aims to search and prune objects that are likely to have low INFLO, thus avoid unnecessary
RNN search. The |RNNk(p)| is initialized to 0 for p. Search process is taken two directions, that is, from one object
to its nearest neighbors, then to the new nearest neighbors (lines 8-14). If for p’s nearest neighbors, their nearest
neighbors’ spaces contain p, or most of them contain p, p is a core object of a cluster and cannot be ranked as top-n
outliers, and can be pruned (lines 15-16). Finally, top-n INFLOs are calculated (lines 17-18).
A Micro-cluster-based method
In order to further reduce the cost of distance computation, we introduce micro-cluster to represent close objects 
so that the number of k-nearest neighbor search will be greatly reduced. The upper and lower bound of k-distance
for each micro-cluster can be estimated in inﬂuenced space. Under the guidance of the two-way search, those microclusters which actually are “core parts” of clusters can be pruned and top-n outliers are ranked in the remaining
Deﬁnition 5. (MicroCluster) The MicroCluster C for a d-dimensional dataset X is deﬁned as the (3·d+2)-tuple
(n, CF1(C), CF2(C), CF3(C), r), where CF1 and CF2 each corresponds to the linear sum and the sum of the
squares of the data values for each dimension respectively. The number of data points |C| is maintained in n, the
centroid of X1 . . . Xn is CF3(C) = CF 1(C)
. The radius of the MicroCluster is r = maxn
(Xj −CF3(C))2.
 introduced an eﬃcient clustering algorithm, BIRCH, with good linear scalability to the size of database, we
borrow its basic idea to partitioning the database into micro-clusters. The detailed procedure can be referenced in
 . The following theorem can be used to estimate the lower and upper bound of k-distance of any object.
Theorem 2. Let p ∈MC(n, c, r) and MC1(n1, c1, r1), . . . , MCl(nl, cl, rl) be a set of micro-clusters that could
potentially contain the k-nearest neighbors of p. Each object oi is treated as a micro-cluster MCi(1, oi, 0). Thus we
will now have l + n −1 micro-clusters.
1. Let {dMin(p, MC1),. . . , dMin(p, MCl+n−1)} be sorted in increasing order, then a lower bound on the k-distance
of p, denoted as min kdist(p) will be dMin(p, MCi) such that n1 + ... + ni ≥k, and n1 + ... + ni−1 < k
2. Let {dMax(p, MC1),. . . , dMax(p, MCl+n−1)} be sorted in increasing order, then an upper bound on the k-distance
of p, denoted as max kdist(p) will be dMax(p, MCi) such that n1 + ... + ni ≥k and n1 + ... + ni−1 < k.
The following is the micro-cluster based algorithm for mining top-n local outliers.
Algorithm 3 Micro-cluster method.
Input: A set of micro-clusters MC1, . . . , MCl, M.
Output: Top-n INFLO of D.
1. FOR each micro-cluster MCi DO
FOR each p ∈MCi Do
Get Max/Min of kdist(p); // based on theorem 2
IF Min kdist(p) < Minkdist(MCi) THEN
Min kdist(MCi) = Minkdist(p);
IF Max kdist(p) > Maxkdist(MCi) THEN
Max kdist(MCi) = Maxkdist(p);
8. FOR each micro-cluster MCi DO
count = |RNNk(MCi)|;
10. IF unvisited(MCi) THEN
S = getKNN(MCi); //search k-nearest micro-clusters
unvisited(MCi) = FALSE;
S = KNN(MCi); //get nearest micro-clusters directly
15. FOR each micro-cluster q ∈S DO
IF unvisited(q) THEN
T = getKNN(q); unvisited(q) = FALSE;
IF Min kdist(q) ≥Max kdist(MCi) THEN
Add q into RNNk(MCi);
Add MCi into RNNk(q);
count + +;
22. IF count ≥|S| ∗M THEN //M is a threshold
Label MCi pruned mark;
24.FOR each object p ∈unpruned micro-clusters MC′ DO
25. Ascending sort top-n INFLO from KNN and RNN;
After building micro-clusters, the process of ﬁnding outliers is similar to the two-way search method. We simply
treat each micro-cluster as a single object to search KNN. As the number of micro-clusters is much less than that
of database objects, the computational cost will be saved a lot. The |RNNk(MCi)| is initialized to 0 for each microcluster MCi, and the lower/upper bound of k-distance of each MCi is derived (lines 1-7) based on theorem 3.2.
Then irrelevant objects in micro-clusters which cannot become top-n outliers are pruned if most of the k-nearest
micro-clusters of a micro-cluster MC contain MC in their k-nearest micro-clusters as well, then MC will be located
in the core part of clusters (lines 20-22) and could be removed. If the lower bound of k-distance for any MC’s
neighboring micro-cluster q is bigger than the upper bound of that for MC, then q belongs to MC’s RNN (lines
18-21). By combining the two-way search and the micro-cluster technique, it achieves a signiﬁcant improvement in
performance.
Performance Evaluation
In this section, we will perform a comprehensive experimental evaluation on the eﬃciency and the eﬀectiveness of
our mining algorithm. We will compare our methods with the LOF method in and show that our methods not
only achieve a good performance but also identify more meaningful outliers than LOF. We perform tests on both
real life data and synthetic data. Our real life dataset is the statistics archive of 2000-2002 National Hockey League
(NHL), totally 22180 records with 12 dimensions8. Our synthetic datasets are generated based on multiple-gaussian
distribution, where the cardinality varies from 1,000 to 1,000,000 tuples and the dimensionality varies from 2 to 18.
The tests are run on 1.3GHZ AMD processor, with 512MB of main memory, under Windows 2000 advanced-server
operating system. All algorithms are implemented by Microsoft Visual C++ 6.0.
Experiments on Eﬀectiveness
To achieve a comprehensive understanding on the eﬀectiveness of the INFLO
measure, it is necessary to test on a series of datasets with diﬀerent sizes and dimensions. We generate our dataset
with complex density distribution by a mixture of Gaussian distribution. Most outliers detected by our methods are
meaningful with good explanations, and some of them cannot be found by LOF. For easily illustrating, we just pick
up a portion of 2-dimensional dataset containing a low density cluster A and a high density cluster B in Figure 5.
The top-6 outliers are listed by INFLO and LOF respectively in Table 1.
Table 1. Outliers Ranking
Rank Index LOF Index INFLO
Table 2. Outliers Ranking(INFLO)
Rank INFLO
Games Goals Shoot %
Table 3. Outliers Ranking(LOF)
Games Goals Shoot %
5.19 Nurminen
2.31 McDonald
Due to the limitation of space, we only show two instances. Table 1 lists the top 6 outliers based on the sample
dataset in Figure 5, by both LOF and INFLO measures. The most outstanding outliers can be recognized by
8 
0 100 200 300 400 500 600 700 800 900 1000
Runtime(sec)
Data size(x1000,k=10,d=5)
Two-way search
Index based
Microcluster
Fig. 7. Runtime vs datasize
0 100 200 300 400 500 600 700 800 900 1000
Runtime(sec)
Data size(x1000,d=5,two-way search)
Fig. 8. Eﬀects of k
Number of pruned objects
Data Size(x1000,k=10,d=5,two-way search)
Fig. 9. Pruning results (1)
either measure. In this sample, 50 percentage of the top 6 outliers are the same points by both measures. When n
is increased, INFLO will ﬁnd even more diﬀerent top outliers from LOF. By visual comparison, the top 6 outliers
found by INFLO is more meaningful. Even for the same objects appeared in top-n lists of both measure, their
position could be diﬀerent and INFLO-based results are obviously more reasonable. In addition, INFLO can
detect outliers which can be overlooked by LOF. For instance, the 50th object and the 4th object have inversely
ranking orders by diﬀerent measure. LOF only considers nearest neighborhood as a density estimation space, and
the NN of both the 1st and the 50th objects are in cluster A. Since the distance between the 50th object and A
is larger than that of the 1st object and A, so the 50th object with low density is ranked as a higher outlier than
the 1st with a high density. While INFLO measure considers both NN and RNN, some objects of B will inﬂuence
the 50th object, and thus make it being less outlierness than 1st object. It is clear that using INFLO as outlierness
measure preserves more semantics than using LOF. Another interesting phenomena in experiments is that INFLO
measure gives more rational indication for the outlier degree assignment. As an example, LOF value that are assigned
to those bordering objects of a cluster has only a tiny diﬀerence with those in the core of a cluster. By INFLO,
however, the bordering objects will have signiﬁcantly larger INFLO values than the core part of the same cluster
while the value diﬀerences are smaller than objects in diﬀerent cluster. Figure 6 presents such value diﬀerences curve
by LOF and INFLO, in which the diﬀerence is evaluated by cluster bordering objects and cluster mean center.
Fig. 5. A dataset
Difference value
Object index
Fig. 6. LOF and INFLO
In the following experiments, we run our proposed algorithms
with NHL 2000-2002 playoﬀdata (22180 tuples) to rank top-n
exceptional players in NHL. The results are compared with those
computed from LOF. We varied k from 10 to 50. Projection is
done on dataset by randomly selecting dimensions, and the outlierness of hockey players is evaluated. For example, we focus on
the statistics data in 3-dimensional subspace of Games played,
Goals and Shooting percentage. Due to the limitation of space,
we only list top-5 players in Table 2 and Table 3. Lots of interesting and useful information can be found in our examination.
For example, there are two players who are listed in both tables
as top-5 outliers. Nurminen is the strongest outlier. Although he
only took two games and got one point, his 100% shooting percentage dominated other two statistics numbers in comparison.
As it happens in the synthetic dataset, we can still ﬁnd some
surprising outliers which cannot be identiﬁed by LOF. For example, Rob Blake ranks 4th in our method but is only ranked as the 31th outlier using LOF. Our reasoning for such
surprising result is as follows. The variation of shooting percentage is usually small, since only a very few of players
can become excellent shooter. Comparing to those players who have similar statistics number in Games Played and
Goals dimensions, although Blake’s shooting percentage is rather low, Blake is still not too far away from other player
when viewed in term of distance. Thus based on LOF measure, Blake’s could not be ranked in the top players. But
the reason for him being a most exceptional player by INFLO is that there is no such type of player whose Shooting
Percentage is so low while having so many Goals. Actually, Blake is the only defence whose number of goals scored
is over 12. He must have shot too many times in the games without getting goals.
Number of unpruned objects(d=5)
Max radius of micro-cluster
Fig. 10. Pruning results (2)
50 100 150 200 250 300 350 400 450 500
Runtime(sec)
Data size(x1000, microcluster)
Dimension=18
Dimension=12
Dimension=6
Fig. 11. Eﬀects of dimensionality(1)
50 100 150 200 250 300 350 400 450 500
Runtime(sec)
Data size(x1000, two-way search)
Dimension=18
Dimension=12
Dimension=6
Fig. 12. Eﬀects of dimensionality(2)
Another interesting example is Jaromir Jagr, who scores in the 3rd position and ranks as the second outlier in
LOF, but the 24th in our measure. The reason is that even though Jagr has a strong goaling capability and a big
fame, there are over twenty players who have higher statistics than him in Shooting Percentage and Games. So
objectively, he is not ranked as the most exceptional player during 2000-2002 seasons. Note that we treat all the
hockey data equal in the analysis not like hockey fans who always weigh goals much higher than other factors.
Eﬃciency Issues of Experiments We evaluate the eﬃciency of the proposed mining algorithms by varying the
data size, dimension number, k and pruning parameter accordingly. Figure 7 shows the performance curves of diﬀerent
methods, along with the runtime (include CPU time and I/O time) corresponding to diﬀerent size of dataset with
5 dimensions. It shows that the run time of three methods are similar when the number of tuple is less than 100k.
When the data size increases to 200k or so, micro-cluster-based method is the best and the two-way search is better
than index-based method. When the size of the load is near to 1000k, swapping operation between R-tree and disk
will happen frequently. As such, the performance of index-based method starts to degrade. On the other hand, since
the two-way search method does early pruning in the search process, it reduces the total computation cost greatly
and saves much time. Micro-cluster method achieves best performance because it not only uses the similar pruning
technique as the two-way search, but also reduces the huge number of the nearest neighbor search. So it takes the
least time to ﬁnish mining outliers in each dataset and scales well to large databases. Unavoidably, this advantage in
performance is done by sacriﬁcing some precision in KNN approximation. However, if we adjust the micro-cluster
to a suitable size, good quality mining results can still be obtained. Figure 9 shows the pruning results under the
diﬀerent values of threshold M (see the two-way search part in section 3). It can be seen that when M increases,
more objects in the database remain unpruned, but the possibility of objects misses to be pruned will be reduced.
If M decreases, more objects will be removed, and the cost of future computation will be reduced. It is particularly
suitable for top-n case in which only a few objects can become the outlier candidates. Figure 10 shows the pruning
results under the diﬀerent radius of micro-cluster. We can see that when the radius increases, more objects will be
inside the micro-clusters, and the diﬀerence between lower and upper bound of micro-clusters’s k-distance will be
larger. As a result, more micro-clusters will not be pruned. Figure 8 presents diﬀerent performance results of the
two-way search method when k varies from 10 to 50. If k is less than 30, the scalability is good with the support
of R-tree. When k is over 30, the cost for the nearest search is rather expensive, more MBRs will be searched to
compute the distance between the objects and the query object. Thus the running time would increase drastically
with the increased number of distance computation.
We also studied the relationship between performance of our algorithm and the number of dimensions, and
Figures 11 and 12 show the runtime of our algorithm with diﬀerent dimensions and varying database with respect to
the microcluster-based method and the two-way search method respectively. From the experiment results, we know
that the algorithms on smaller dimensionality and data size always have shorter running time. Speciﬁcally, when
dimensionality is larger than 12, the running time will be increased drastically, thus seriously hindering the eﬃciency
of the algorithms.
Related Work
Knorr and Ng initialized the concept of distance-based outlier, which deﬁnes an object o being an outlier, if
at most p objects are within distance d of o. A cell-based outlier detection approach that partitions the dataset
into cells is also presented. The time complexity of this cell-based algorithm is O(N + ck) where k is dimension
number, N is dataset size, c is a number inversely proportional to d. For very large databases, this method achieves
better performance than depth-based method, but still exponential to the number of dimensions. Ramaswamy et al.
extended the notion of distance-based outliers by using the distance to the k-nearest neighbor to rank the outliers. An
eﬃcient algorithm to compute the top-n global outliers is given, but their notion of an outlier is still distance-based
Some clustering algorithms like CLARANS , DBSCAN , BIRCH , and CURE consider outliers,
but only to the point of ensuring that they do not interfere with the clustering process. Further, outliers are only
by-products of clustering algorithms, and these algorithms cannot rank the priority of outliers.
The concept of local outlier, which assigns each data a local outlier factor LOF of being an outlier depending
on their neighborhood, was introduced by Breunig et al. . This outlier factor can be used to rank the objects
regarding their outlierness. To compute LOF for all objects in a database, O(n*runtime of a KNN query) is needed.
The outlier factors can be computed eﬃciently if OPTICS is used to analyze the clustering structure. A top-n based
local outliers mining algorithm which uses distance bound of micro-cluster to estimate the density, was presented in
There are several recent studies on local outlier detection. In , , three enhancement schemes over LOF are
introduced, namely LOF’ and LOF” and GridLOF, and introduces a connectivity-based outlier factor (COF)
scheme that improves the eﬀectiveness of an existing local outlier factor LOF scheme when a pattern itself has
similar neighborhood density as an outlier. They extensively study the reason of missed outliers by LOF, and focus
on ﬁnding those outliers which are close to some non-outliers with similar densities. While our measure based on
the symmetric relationship is not only compatible with their improved measures, but also identiﬁes more meaningful
outliers. LOCI addresses the diﬃculty of choosing values for MinPts in the LOF technique by using statistical
values derived from the data itself.
Conclusion
In this paper, we discuss the problem with existing local outlier measure and proposed a new measure INFLO which is
based on a symmetric neighborhood relationship. We proposed various methods for computing INFLO including the
naive-index based method, the two-way pruning method and the micro-cluster based method. Extensive experiments
are conducted showing that our proposed methods are eﬃcient and eﬀective on both synthetic and real life datasets.