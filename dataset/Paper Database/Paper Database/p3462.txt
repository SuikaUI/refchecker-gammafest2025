Sparse Cooperative Q-learning
Jelle R. Kok
 
Nikos Vlassis
 
Informatics Institute, Faculty of Science, University of Amsterdam, The Netherlands
Learning in multiagent systems suﬀers from
the fact that both the state and the action
space scale exponentially with the number of
In this paper we are interested in
using Q-learning to learn the coordinated actions of a group of cooperative agents, using a sparse representation of the joint stateaction space of the agents. We ﬁrst examine
a compact representation in which the agents
need to explicitly coordinate their actions
only in a predeﬁned set of states. Next, we
use a coordination-graph approach in which
we represent the Q-values by value rules that
specify the coordination dependencies of the
agents at particular states. We show how Qlearning can be eﬃciently applied to learn a
coordinated policy for the agents in the above
framework.
We demonstrate the proposed
method on the predator-prey domain, and we
compare it with other related multiagent Qlearning methods.
1. Introduction
A multiagent system (MAS) consists of a group of
agents that can potentially interact with each other
 .
In this paper, we are
interested in fully cooperative multiagent systems in
which the agents have to learn to optimize a global
performance measure. One of the key problems in such
systems is the problem of coordination: how to ensure
that the individual decisions of the agents result in
jointly optimal decisions for the group.
Reinforcement learning (RL) techniques have been applied successfully in many
single-agent systems for learning the policy of an agent
Appearing in Proceedings of the 21 st International Conference on Machine Learning, Banﬀ, Canada, 2004. Copyright
2004 by the authors.
in uncertain environments.
In principle, it is possible to treat a multiagent system as a ‘big’ single
agent and learn the optimal joint policy using standard single-agent reinforcement learning techniques.
However, both the state and action space scale exponentially with the number of agents, rendering this
approach infeasible for most problems. Alternatively,
we can let each agent learn its policy independently
of the other agents, but then the transition model depends on the policy of the other learning agents, which
may result in oscillatory behavior.
On the other hand, in many problems the agents only
need to coordinate their actions in few states (e.g., two
cleaning robots that want to clean the same room),
while in the rest of the states the agents can act independently.
Even if these ‘coordinated’ states are
known in advance, it is not a priori clear how the
agents can learn to act cooperatively in these states.
In this paper we describe a multiagent Q-learning technique, called Sparse Cooperative Q-learning, that allows a group of agents to learn how to jointly solve a
task when the global coordination requirements of the
system (but not the particular action choices of the
agents) are known beforehand.
We ﬁrst examine a compact representation in which
the agents learn to take joint actions in a predeﬁned
set of states. In all other (uncoordinated) states, we
let the agents learn independently. Then we generalize
this approach by using a context-speciﬁc coordination
graph to specify the coordination dependencies of subsets of agents according to the
current context (dynamically). The proposed framework allows for a sparse representation of the joint
state-action space of the agents, resulting in large computational savings.
We demonstrate the proposed technique on the
‘predator-prey’ domain, a popular multiagent problem
in which a number of predator agents try to capture
a poor prey.
Our method achieves a good trade-oﬀ
between speed and solution quality.
2. MDPs and Q-learning
In this section, we review the Markov Decision Process (MDP) framework. An observable MDP is a tuple
⟨S, A, T, R⟩where S is a ﬁnite set of world states, A is
a set of actions, T : S × A × S → is the Markovian transition function that describes the probability p(s′|s, a) of ending up in state s′ when performing action a in state s, and R : S × A →IR is a
reward function that returns the reward R(s, a) obtained after taking action a in state s.
An agent’s
policy is deﬁned as a mapping π : S →A.
objective is to ﬁnd an optimal policy π∗that maximizes the expected discounted future reward U ∗(s) =
maxπ E [P∞
t=0 γtR(st)|π, s0 = s] for each state s. The
expectation operator E[·] averages over reward and
stochastic transitions and γ ∈[0, 1) is the discount factor. We can also represent this using Q-values which
store the expected discounted future reward for each
state s and possible action a:
Q∗(s, a) = R(s, a)+γ
p(s′|s, a) max
a′ Q∗(s′, a′). (1)
The optimal policy for a state s is the action
arg maxa Q∗(s, a) that maximizes the expected future
discounted reward.
Reinforcement learning (RL) 
can be applied to estimate Q∗(s, a). Q-learning is a
widely used learning method when the transition and
reward model are unavailable.
This method starts
with an initial estimate Q(s, a) for each state-action
pair. When an exploration action a is taken in state s,
reward R(s, a) is received and next state s′ is observed,
the corresponding Q-value is updated by
Q(s, a) := Q(s, a)+α[R(s, a)+γ max
a′ Q(s′, a′)−Q(s, a)]
where α ∈(0, 1) is an appropriate learning rate. Under conditions, Q-learning is known to converge to the
optimal Q∗(s, a) .
3. Multiagent Q-learning
The framework discussed in the previous section only
involves single agents. In this work, we are interested
in systems in which multiple agents, each with their
own set of actions, have to collaboratively solve a task.
A collaborative multiagent MDP extends the single agent MDP framework to include multiple agents whose joint action impacts the state transition and the received reward. Now, the transition
model T : S × A × S → represents the probability p(s′|s, a) the system will move from state s to
s′ after performing the joint action a ∈A = ×n
and Ri : S × A →IR is the reward function that returns the reward Ri(s, a) for agent i after the joint
action a is taken in state s. As global reward function
R(s, a) = Pn
i=1 Ri(s, a) we take the sum of all individual rewards received by the n agents. This framework
diﬀers from a stochastic game in that
each agent wants to maximize social welfare (sum of
all payoﬀs) instead of its own payoﬀ.
Within this framework diﬀerent choices can be made
which aﬀect the problem description and possible solution concepts, e.g., whether the agents are allowed to
communicate, whether they observe the selected joint
action, whether they perceive the individual rewards
of the other agents, etc. In our case we assume that the
agents are allowed to communicate and thus are able
to share individual actions and rewards. Before we discuss our approach, we ﬁrst describe two other learning
methods for environments with multiple agents.
3.1. MDP Learners
In principle, a collaborative multiagent MDP can be
regarded as one large single agent in which each joint
action is represented as a single action. The optimal
Q-values for the joint actions can then be learned using standard single-agent Q-learning. In order to apply
this MDP learners approach a central controller models the complete MDP and communicates to each agent
its individual action, or all agents model the complete
MDP separately and select the individual action that
corresponds to their own identity. In the latter case, no
communication is needed between the agents but they
all have to observe the joint action and all individual
rewards. Moreover, the problem of exploration can be
solved by using the same random number generator
(and the same seed) for all agents . Although this approach leads to the optimal solution, it
is infeasible for problems with many agents since the
joint action space, which is exponential in the number
of agents, becomes intractable.
3.2. Independent Learners
At the other extreme, we have the independent learners (IL) approach in which
the agents ignore the actions and rewards of the other
agents in the system, and learn their strategies independently. The standard convergence proof for Qlearning does not hold in this case, since the transition
model depends on the unknown policy of the other
learning agents. Despite the lack of guaranteed convergence, this method has been applied successfully in
multiple cases .
4. Context-Speciﬁc Q-learning
In many problems, agents only have to coordinate their
actions in a speciﬁc context .
For example, two cleaning robots only have to take
care that they do not obstruct each other when they
are cleaning the same room. When they work in two
diﬀerent rooms, they can work independently.
In this section, we describe a reinforcement learning
method which explicitly models these types of contextspeciﬁc coordination requirements. The main idea is
to learn joint action values only in those states where
the agents actually need to coordinate their actions.
We create a sparse representation of the joint stateaction space by specifying in which states the agents do
(and in which they do not) have to coordinate their actions. During learning the agents apply the IL method
in the uncoordinated states and the MDP learners approach in the coordinated states.
Since in practical
problems the agents typically need to coordinate their
actions only in few states, this framework allows for
a sparse representation of the complete action space,
resulting in large computational savings.
Because of the distinction in action types for diﬀerent
states, we also have to distinguish between diﬀerent
representations for the Q-values. Each agent i maintains a single-action value table Qi(s, ai) for the uncoordinated states, and one joint action value table
Q(s, a) for the coordinated states. In the coordinated
states the global Q-value Q(s, a) directly relates to the
shared joint Q-table. In the uncoordinated states, we
assume that the global Q-value is the sum of all individual Q-values:
Qi(s, ai).
When the agents observe a state transition, values
from the diﬀerent Q-tables are combined in order to
update the Q-values.
There are four diﬀerent situations that must be taken
into account. When moving between two coordinated
or between two uncoordinated states, we respectively
apply the MDP Learners and IL approach. In the case
that the agents move from a coordinated state s to an
uncoordinated state s′ we back up the individual Qvalues to the joint Q-value by
Q(s, a) := (1 −α)Q(s, a) +
Ri(s, a) + γ max
Conversely, when moving from an uncoordinated state
Q1(s′, a1)
Q2(s′, a2)
Q3(s′, a3)
Figure 1. Graphical representation of the Q-tables in the
case of three agents A1, A2, and A3. State s and s′′ are coordinated states, while state s′ is an uncoordinated state.
s′ to a coordinated state s′′ we back up the joint Qvalue to the diﬀerent individual Q-values by
Qi(s′, ai) := (1 −α)Qi(s′, ai) +
Ri(s′, ai) + γ 1
a′ Q(s′′, a′)
That is, in this case each agent is rewarded with the
same fraction of the expected future discounted reward
from the resulting coordinated state. This essentially
implies that each agent contributes equally to the coordination.
Fig. 1 shows a graphical representation of the transition between three states for a problem involving
three agents.
In state s the agents have to coordinate their actions and use the shared Q-table to determine the joint action. After taking the joint action
a and observing the transition to the uncoordinated
state s′, the joint action Q-value Q(s, a) is updated
using Eq. (4).
Similarly, in s′ each agent i chooses
its action independently and after moving to state s′′
updates its individual Q-value Qi using Eq. (5).
In terms of implementation, the shared Q-table can be
either stored centrally (and the agents should have access to this shared resource) or updated identically by
all individual agents. Note that in the latter case the
agents rely on (strong) common knowledge assumptions about the observed actions and rewards of the
other agents. Furthermore, all agents have to coordinate their actions in a coordinated state. In the remainder of this paper, we will discuss a coordinationgraph approach which is a generalization of the described algorithm in this section. In that framework
the coordination requirements are speciﬁed over subsets of agents and the global Q-value is distributed
among the diﬀerent agents.
Before we discuss this
generalized approach, we ﬁrst review the notion of a
context-speciﬁc coordination graph.
5. Context-Speciﬁc Coordination
A context-speciﬁc coordination graph (CG) represents
a dynamic (context-dependent) set of coordination requirements of a multiagent system the global payoﬀfunction is
distributed among the agents using a set of value rules.
These are propositional rules in the form ⟨ρ; c : v⟩,
where c (the context) is an element from the set of all
possible combinations of the state and action variables
c ∈C ⊆S ∪A, and ρ(c) = v ∈IR is a payoﬀthat is
added to the global payoﬀwhen c holds. By deﬁnition,
two agents are neighbors in the CG if and only if there
is a value rule ⟨ρ; c : v⟩that contains the actions of
these two agents in c. Clearly, the set of value rules
form a sparse representation of the global payoﬀfunction since not all state and action combinations have
to be deﬁned.
Fig. 2 shows an example of a context-speciﬁc CG,
where for simplicity all actions and state variables are
assumed binary1. In the left graph we show the initial CG together with the corresponding set of value
rules. Note that agents involved in the same rules are
neighbors in the graph. In the center we show how the
value rules, and therefore the CG, are updated after
the agents condition on the current context (the state
s = true). Based on this information about state s,
rule ρ5 is irrelevant and is removed. As a consequence,
the optimal joint action is independent of A4 and its
edge is deleted from the graph as shown in the center
of Fig. 2.
In order to compute the optimal joint action (with
maximum total payoﬀ) in a CG, a variable elimination
algorithm can be used which we brieﬂy illustrate in the
example of Fig. 2. After the agents have conditioned
1Action a1 corresponds to a1 = true and action a1 to
a1 = false.
⟨ρ1; a1 ∧a3 ∧s : 4⟩
⟨ρ2; a1 ∧a2 ∧s : 5⟩
⟨ρ3; a2 ∧s
⟨ρ4; a3 ∧a2 ∧s : 5⟩
⟨ρ5; a3 ∧a4 ∧s : 10⟩
⟨ρ1; a1 ∧a3 : 4⟩
⟨ρ2; a1 ∧a2 : 5⟩
⟨ρ4; a3 ∧a2 : 5⟩
⟨ρ2; a1 ∧a2 : 5⟩
⟨ρ7; a2 ∧a1 : 4⟩
Figure 2. Initial CG (left), after conditioning on the context s = true (center), and after elimination of A3 (right).
on the context (center ﬁgure), the agents are eliminated from the graph one by one. Let us assume that
we ﬁrst eliminate A3. This agent ﬁrst collects all rules
in which it is involved, these are ⟨a1∧a3 : 4⟩⟨a3∧a2 : 5⟩.
Next, for all possible actions of A1 and A2, agent A3
determines its conditional strategy, in this case equal
to ⟨a2 : 5⟩⟨a1 ∧a2 : 4⟩, and is then eliminated from the
graph. The algorithm continues with agent A2 which
computes its conditional strategy ⟨a1 : 11⟩⟨a1 : 5⟩, and
is then also eliminated. Finally, A1 is the last agent left
and ﬁxes its action to a1. Now a second pass in the reverse order is performed, where each agent distributes
its strategy to its neighbors, who then determine their
ﬁnal strategy. This results in the optimal joint action
{a1, a2, a3} with a global payoﬀof 11.
6. Sparse Cooperative Q-learning
The method discussed in section 4 deﬁned a state either as a coordinated state in which all agents coordinate their actions, or as an uncoordinated state in
which all agents act independently. However, in many
situations only some of the agents have to coordinate
their actions. In this section we describe Sparse Cooperative Q-learning which allows a group of agents to
learn how to coordinate based on a predeﬁned coordination structure that can diﬀer between states.
As in we begin by distributing
the global Q-value among the diﬀerent agents. Every
agent i is associated with a local value function Qi(s, a)
which only depends on a subset of all possible state and
action variables. The global Q-value equals the sum
of the local Q-values of all n agents:
Suppose that an exploration joint action a is taken
from state s, each agent receives reward Ri(s, a), and
next state s′ is observed. Based on the decomposition
(6) the global Q-learning update rule now reads
Qi(s, a) :=
Qi(s, a) + α
a′ Q(s′, a′) −
Using the variable elimination algorithm discussed in
section 5, the agents can compute the optimal joint
action a∗= arg maxa′ Q(s′, a′) in state s′, and from
this compute their contribution Qi(s′, a∗) to the total
payoﬀQ(s′, a∗), as we will show next.
This allows
the above update to be decomposed locally for each
Qi(s, a) := Qi(s, a)+α[Ri(s, a)+γQi(s′, a∗)−Qi(s, a)].
We still have to discuss how the local Q-functions are
represented.
In our notation, we use the value rule
representation of section 5 to specify the coordination
requirements between the agents for a speciﬁc state.
This is a much richer representation than the IL-MDP
variants since it allows us to represent all possible dependencies between the agents in a context-speciﬁc
manner. Every Qi(s, a) depends on those value rules
that are consistent with the given state-action pair
(s, a) and in which agent i is involved:
Qi(s, a) =
where nj is the number of agents (including agent i)
involved in rule ρi
Such a representation for Qi(s, a) can be regarded as a
linear expansion into a set of basis functions ρi
j, each of
them peaked on a speciﬁc state-action context which
may potentially involve many agents. The ‘weights’ of
these basis functions (the rules’ values) can then be
updated as follows:
ρj(s, a) := ρj(s, a) + α
[Ri(s, a)+
γQi(s′, a∗) −Qi(s, a)]
where we add the contribution of each agent involved
in the rule.
As an example, assume we have the following set of
Q2(s, a2, a3)
Q3(s, a2, a3)
Q1(s′, a1, a2)
Q2(s′, a1, a2)
Q3(s′, a3)
Figure 3. Example representation of the Q components of
three agents for a transition from state s to state s′.
value rules:
⟨ρ1 ; a1 ∧s
⟨ρ2 ; a1 ∧a2 ∧s′
⟨ρ3 ; a1 ∧a2 ∧s′
⟨ρ4 ; a1 ∧a2 ∧s
⟨ρ5 ; a2 ∧a3 ∧s
⟨ρ6 ; a3 ∧s′
Furthermore, assume that a = {a1, a2, a3} is the performed joint action in state s and a∗= {a1, a2, a3} is
the optimal joint action found with the variable elimination algorithm in state s′. After conditioning on the
context, the rules ρ1 and ρ5 apply in state s, whereas
the rules ρ3 and ρ6 apply in state s′. This is graphically depicted in Fig. 3.
Next, we use Eq. (10) to
update the value rules ρ1 and ρ5 in state s as follows:
ρ1(s, a) = v1 + α[R1(s, a) + γ v3
ρ5(s, a) = v5 + α[R2(s, a) + γ v3
R3(s, a) + γ v6
Note that in order to update ρ5 we have used
the (discounted) Q-values of Q2(s′, a∗) = v3/2 and
Q3(s′, a∗) = v6/1. Furthermore, the component Q2 in
state s′ is based on a coordinated action of agent A2
with agent A1 (rule ρ3), whereas in state s agent A2
has to coordinate with agent A3 (rule ρ5).
7. Experiments
In this section, we apply our method to a predatorprey problem in which the goal of the predators is to
capture a prey as fast as possible in a discrete gridlike world . We concentrate on
Figure 4. Possible capture position for two predators.
a coordination problem in which two predators in a
10×10 toroidal grid have to capture a single prey. Each
agent can move to one of its adjacent cells or remain on
its current position. The prey is captured when both
predators are located in an adjacent cell to the prey
and only one of the two agents moves to the location
of the prey. A possible capture situation is depicted
in Fig. 4. When the two predators move to the same
cell or a predator moves to the prey position without
a nearby predator, they are penalized and placed on
random positions on the ﬁeld. The policy of the prey is
ﬁxed: it stays on its current position with a probability
of 0.2, in all other cases it moves to one of its free
adjacent cells with uniform probability.
The complete state-action space for this problem consists of all combinations of the two predator positions
relative to the prey and the joint action of the two
predators (almost 250,000 states). However, in many
of these states the predators do not have to coordinate
their actions. Therefore, we ﬁrst initialize each predator with a set of individual value rules which do not
include the state and action of the other predator. An
example rule is deﬁned as
prey(−3, −3)
a1 = move none : 75⟩.
The payoﬀof all value rules are initialized with a value
of 75 which corresponds to the maximal reward at the
end of an episode.
This ensures that the predators
explore all possible action combinations suﬃciently.
Next, the speciﬁc coordination requirements between
the two predators are added. Since the predators only
have to coordinate their actions when they are close
to each other, we add extra value rules, depending on
the joint action, for the following situations:
• the (Manhattan) distance to the other predator is
smaller or equal than two cells
• both predators are within a distance of two cells
to the prey
The value rule for which the prey is captured in the
situation of Fig. 4 looks as
prey(0, −1)
pred(1, −1)
a1 = move none
a2 = move west : 75⟩
This results in the generation of 31,695 value rules for
the ﬁrst predator (31,200 for the 1,248 coordinated
states and 495 for the 99 uncoordinated states2). The
second predator holds only a set of 495 rules for the
uncoordinated states since its action is based on the
rules from the other predator in the coordinated states.
During learning we use Eq. (10) to update the payoﬀs of the rules.
Each predator i receives a reward
Ri = 37.5 when it helps to capture the prey and a
negative reward of −50.0 when it collides with another
predator. When an agent moves to the prey without
support the reward is −5.0. In all other cases the reward is −0.5. We use an ǫ-greedy exploration step of
0.2, a learning rate α of 0.3, and a discount factor γ
We compare our method to the two Q-learning methods mentioned in section 2. In case of the independent learners, each Q-value is derived from a state that
consists of both the position of the prey and the other
predator and one of the ﬁve possible actions. This corresponds to 48, 510 (= 99 · 98 · 5) diﬀerent state-action
pairs for each agent. For the MDP Learners we model
the system as a complete MDP with the joint action
represented as a single action. In this case, the number
of state action-pairs equals 242, 550 (= 99 · 98 · 52).
Fig. 5 shows the capture times for the learned policy
during the ﬁrst 500,000 episodes for the diﬀerent methods.
The results are generated by running the current learned policy after each interval of 500 episodes
ﬁve times on a ﬁxed set of 100 starting conﬁgurations.
During these 500 test episodes no exploration actions
were performed.
This was repeated for 10 diﬀerent
The 100 starting conﬁgurations were selected
randomly beforehand and were used during all 10 runs.
Both the independent learners and our proposed
method learn quickly in the beginning with respect
to the MDP learners since learning is based on fewer
state-action pairs. However, the independent learners
do not converge to a single policy but keep oscillating.
This is caused by the fact that they do not take the
action of the other agent into account.
predators are located next to the prey and one predator moves to the prey position, this predator is not able
to distinguish between the situation where the other
2Note that creating value rules based on the full state
information, and only decomposing the action space, would
result in 8,454 (= 99 · 98 −1, 248) uncoordinated states
capture time
MDP Learners
Independent Learners
Manual Policy
Sparse Cooperative Q−learning
Figure 5. Capture times for the learned policy for the four
diﬀerent methods during the ﬁrst 500,000 episodes. Results
are averaged over 10 runs.
predator remains on its current position or performs
one of its other actions (e.g., an exploration action).
In the ﬁrst case a positive reward is returned, while
in the second case a large negative reward is received.
However, in both situations the same Q-value is updated.
These coordination dependencies are explicitly taken
into account for the two other approaches.
MDP learners, they are modeled in every state which
results in a slowly decreasing learning curve; it takes
longer before all state-action pairs are explored. The
context-speciﬁc approach has a quicker decreasing
learning curve since only joint actions are considered
for these coordinated states. As we we see from Fig. 5,
both methods result in an almost identical policy.
Table 1 shows the average capture times for the different approaches for the last 10 test runs from Fig. 5
and a manual implementation in which both predators
ﬁrst minimize the distance to the prey and then wait
till both predators are located next to the prey. When
both predators are located next to the prey, social conventions based on the relative positioning are used to
decide which of the two predators moves to the prey
The context-speciﬁc learning approach converges to a
slightly higher capture time than that of the MDP
Learners. An explanation for this small diﬀerence is
the fact that not all necessary coordination requirements are added as value rules. In our construction of
value rules we assume that the agents do not have to
coordinate when they are located far away from each
Independent learners
Manual policy
Sparse Cooperative
MDP Learners
Table 1. Average capture time after learning (averaged
over 5,000 episodes) and the number of state-action pairs
for the diﬀerent methods.
other, but already coordinating in these states might
have a positive inﬂuence on the ﬁnal result. These constraints could be added as extra value rules, but then
the learning time would increase with the increased
state-action space. Clearly, a trade-oﬀexists between
the expressiveness of the model and the learning time.
8. Discussion and Conclusions
In this paper we discussed a Q-learning approach
for cooperative multiagent systems that is based on
context-speciﬁc coordination graphs, and in which
value rules specify the coordination requirements of
the system for a speciﬁc context. These rules can be
regarded as a sparse representation of the complete
state-action space, since they are deﬁned over a subset of all state and action variables. The value of each
rule contributes additively to the global Q-value and is
updated based on a Q-learning rule that adds the contribution of all involved agents in the rule. Eﬀectively,
each agent learns to coordinate only with its neighbors in a dynamically changing coordination graph.
Results in the predator-prey domain show that our
method improves the learning time of other multiagent Q-learning methods, and performs comparable to
the optimal policy.
Our approach is closely related to the coordinated
reinforcement learning approach of . In their approach the global Q-value is also
represented as the sum of local Q-functions, and each
local Q-function assumes a parametric function representation. The main diﬀerence with our work is that
they update the weights of each local Q-value (of each
agent) based on the diﬀerence between the global Qvalues (over all agents) of the current and (discounted)
next state (plus the immediate rewards). In our approach, the update of the Q-function of an agent is
based only on the rewards and Q-values of its neighboring agents in the graph. This can be advantageous
when subgroups of agents need to separately coordinate their actions.
From this perspective, our local
Q-learning updates seem closer in spirit to the local
Sarsa updates of .
Another related approach is the work of in which each agent updates its local Qvalue based on the Q-value of its neighboring nodes.
A weight function f(i, j) determines how much the Qvalue of an agent j contributes to the update of the
Q-value of agent i. Just as in our approach, this function deﬁnes a graph structure of agent dependencies.
However, these dependencies are ﬁxed throughout the
learning process (although they mention the possibility of a dynamically changing f). Moreover, in their
approach Q-learning involves back-propagating averages of individual Q-values, whereas in our case Qlearning involves back-propagating individual components of joint Q-values. We applied their distributed
value function approach on our predator-prey problem with a weighting function that averaged the value
evenly over the two agents. However, the policy did
not converge and oscillated around an average capture time of 33.25 cycles since the agents also aﬀect
each other in the uncoordinated states. For instance,
an agent ending up in a low-valued state after taking
an exploratory action inﬂuences the individual action
taken by the other agent negatively.
There are several directions for future work. In our
current implementation we have assumed that all
agents contribute equally to the rules in which they
are involved (see Eq. (9)). We would like to investigate
the consequence of this choice. Furthermore, we would
like to apply our approach to continuous domains with
more agent dependencies, and investigate methods to
learn the coordination requirements automatically.
Acknowledgments
We would like to thank the three reviewers for their
detailed and constructive comments. This research is
supported by PROGRESS, the embedded systems research program of the Dutch organization for Scientiﬁc Research NWO, the Dutch Ministry of Economic
Aﬀairs and the Technology Foundation STW, project