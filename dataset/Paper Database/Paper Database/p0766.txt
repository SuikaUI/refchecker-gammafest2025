Single-Shot Reﬁnement Neural Network for Object Detection
Shifeng Zhang1,2, Longyin Wen3, Xiao Bian3, Zhen Lei1,2, Stan Z. Li1,2
1 CBSR & NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China.
2 University of Chinese Academy of Sciences, Beijing, China.
3 GE Global Research, Niskayuna, NY.
{shifeng.zhang,zlei,szli}@nlpr.ia.ac.cn, {longyin.wen,xiao.bian}@ge.com
For object detection, the two-stage approach (e.g.,
Faster R-CNN) has been achieving the highest accuracy,
whereas the one-stage approach (e.g., SSD) has the advantage of high efﬁciency.
To inherit the merits of both
while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called ReﬁneDet,
that achieves better accuracy than two-stage methods and
maintains comparable efﬁciency of one-stage methods. Re-
ﬁneDet consists of two inter-connected modules, namely,
the anchor reﬁnement module and the object detection module.
Speciﬁcally, the former aims to (1) ﬁlter out negative anchors to reduce search space for the classiﬁer, and
(2) coarsely adjust the locations and sizes of anchors to
provide better initialization for the subsequent regressor.
The latter module takes the reﬁned anchors as the input
from the former to further improve the regression and predict multi-class label.
Meanwhile, we design a transfer
connection block to transfer the features in the anchor re-
ﬁnement module to predict locations, sizes and class labels of objects in the object detection module. The multitask loss function enables us to train the whole network
in an end-to-end way. Extensive experiments on PASCAL
VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that ReﬁneDet achieves state-of-the-art detection accuracy with high efﬁciency. Code is available at https:
//github.com/sfzhang15/RefineDet.
1. Introduction
Object detection has achieved signiﬁcant advances in recent years, with the framework of deep neural networks
(DNN). The current DNN detectors of state-of-the-art can
be divided into two categories: (1) the two-stage approach,
including , and (2) the one-stage approach,
including . In the two-stage approach, a sparse set
of candidate object boxes is ﬁrst generated, and then they
are further classiﬁed and regressed. The two-stage methods have been achieving top performances on several challenging benchmarks, including PASCAL VOC and MS
COCO .
The one-stage approach detects objects by regular and
dense sampling over locations, scales and aspect ratios. The
main advantage of this is its high computational efﬁciency.
However, its detection accuracy is usually behind that of
the two-stage approach, one of the main reasons being due
to the class imbalance problem .
Some recent methods in the one-stage approach aim to
address the class imbalance problem, to improve the detection accuracy. Kong et al. use the objectness prior constraint on convolutional feature maps to signiﬁcantly reduce
the search space of objects. Lin et al. address the class
imbalance issue by reshaping the standard cross entropy
loss to focus training on a sparse set of hard examples and
down-weights the loss assigned to well-classiﬁed examples.
Zhang et al. design a max-out labeling mechanism to
reduce false positives resulting from class imbalance.
In our opinion, the current state-of-the-art two-stage
methods, e.g., Faster R-CNN , R-FCN , and FPN
 , have three advantages over the one-stage methods as
follows: (1) using two-stage structure with sampling heuristics to handle class imbalance; (2) using two-step cascade to
regress the object box parameters; (3) using two-stage features to describe the objects1. In this work, we design a
novel object detection framework, called ReﬁneDet, to inherit the merits of the two approaches (i.e., one-stage and
two-stage approaches) and overcome their shortcomings. It
improves the architecture of the one-stage approach, by using two inter-connected modules (see Figure 1), namely, the
anchor 2 reﬁnement module (ARM) and the object detection
1In case of Faster R-CNN, the features (excluding shared features) in
the ﬁrst stage (i.e., RPN) are trained for the binary classiﬁcation (being an
object or not), while the features (excluding shared features) in the second stage(i.e., Fast R-CNN) are trained for the multi-class classiﬁcation
(background or object classes).
2We denote the reference bounding box as “anchor box”, which is also
called “anchor” for simplicity, as in . However, in , it is called
“default box”.
 
Anchor Refinement Module
Object Detection Module
Connection
Object Multi-Class
Classification and
Regression Loss
Anchor Binary
Classification and
Regression Loss
Connection
Connection
Connection
Block用版水印用版水印用版水印
Figure 1: Architecture of ReﬁneDet. For better visualization, we only display the layers used for detection. The celadon
parallelograms denote the reﬁned anchors associated with different feature layers. The stars represent the centers of the
reﬁned anchor boxes, which are not regularly paved on the image.
module (ODM). Speciﬁcally, the ARM is designed to (1)
identify and remove negative anchors to reduce search space
for the classiﬁer, and (2) coarsely adjust the locations and
sizes of anchors to provide better initialization for the subsequent regressor. The ODM takes the reﬁned anchors as the
input from the former to further improve the regression and
predict multi-class labels. As shown in Figure 1, these two
inter-connected modules imitate the two-stage structure and
thus inherit the three aforementioned advantages to produce
accurate detection results with high efﬁciency. In addition,
we design a transfer connection block (TCB) to transfer the
features3 in the ARM to predict locations, sizes, and class
labels of objects in the ODM. The multi-task loss function
enables us to train the whole network in an end-to-end way.
Extensive experiments on PASCAL VOC 2007, PAS-
CAL VOC 2012, and MS COCO benchmarks demonstrate
that ReﬁneDet outperforms the state-of-the-art methods.
Speciﬁcally, it achieves 85.8% and 86.8% mAPs on VOC
2007 and 2012, with VGG-16 network. Meanwhile, it outperforms the previously best published results from both
one-stage and two-stage approaches by achieving 41.8%
AP4 on MS COCO test-dev with ResNet-101. In ad-
3The features in the ARM focus on distinguishing positive anchors
from background. We design the TCB to transfer the features in the ARM
to handle the more challenging tasks in the ODM, i.e., predict accurate
object locations, sizes and multi-class labels.
4Based on the evaluation protocol in MS COCO , AP is the sindition, ReﬁneDet is time efﬁcient, i.e., it runs at 40.2 FPS
and 24.1 FPS on a NVIDIA Titan X GPU with the input
sizes 320 × 320 and 512 × 512 in inference.
The main contributions of this work are summarized as
(1) We introduce a novel one-stage framework
for object detection, composed of two inter-connected modules, i.e., the ARM and the ODM. This leads to performance
better than the two-stage approach while maintaining high
efﬁciency of the one-stage approach. (2) To ensure the effectiveness, we design the TCB to transfer the features in
the ARM to handle more challenging tasks, i.e., predict accurate object locations, sizes and class labels, in the ODM.
(3) ReﬁneDet achieves the latest state-of-the-art results on
generic object detection .
2. Related Work
Classical Object Detectors. Early object detection methods are based on the sliding-window paradigm, which apply the hand-crafted features and classiﬁers on dense image
grids to ﬁnd objects. As one of the most successful methods, Viola and Jones use Haar feature and AdaBoost
to train a series of cascaded classiﬁers for face detection,
gle most important metric, which is computed by averaging over all 10
intersection over union (IoU) thresholds (i.e., in the range [0.5:0.95] with
uniform step size 0.05) of 80 categories.
achieving satisfactory accuracy with high efﬁciency. DPM
 is another popular method using mixtures of multiscale deformable part models to represent highly variable
object classes, maintaining top results on PASCAL VOC 
for many years. However, with the arrival of deep convolutional network, the object detection task is quickly dominated by the CNN-based detectors, which can be roughly
divided into two categories, i.e., the two-stage approach and
one-stage approach.
Two-Stage Approach. The two-stage approach consists of
two parts, where the ﬁrst one (e.g., Selective Search ,
EdgeBoxes , DeepMask , RPN ) generates a
sparse set of candidate object proposals, and the second one
determines the accurate object regions and the corresponding class labels using convolutional networks. Notably, the
two-stage approach (e.g., R-CNN , SPPnet , Fast R-
CNN to Faster R-CNN ) achieves dominated performance on several challenging datasets . After that, numerous effective techniques are proposed to further improve the
performance, such as architecture diagram , training strategy , contextual reasoning 
and multiple layers exploiting .
One-Stage Approach. Considering the high efﬁciency, the
one-stage approach attracts much more attention recently.
Sermanet et al. present the OverFeat method for classiﬁcation, localization and detection based on deep ConvNets, which is trained end-to-end, from raw pixels to ultimate categories. Redmon et al. use a single feedforward convolutional network to directly predict object
classes and locations, called YOLO, which is extremely
After that, YOLOv2 is proposed to improve
YOLO in several aspects, i.e., add batch normalization on
all convolution layers, use high resolution classiﬁer, use
convolution layers with anchor boxes to predict bounding
boxes instead of the fully connected layers, etc. Liu et al.
 propose the SSD method, which spreads out anchors
of different scales to multiple layers within a ConvNet and
enforces each layer to focus on predicting objects of a certain scale. DSSD introduces additional context into
SSD via deconvolution to improve the accuracy. DSOD
 designs an efﬁcient framework and a set of principles to
learn object detectors from scratch, following the network
structure of SSD. To improve the accuracy, some one-stage
methods aim to address the extreme class imbalance problem by re-designing the loss function or classiﬁcation strategies. Although the one-stage detectors have
made good progress, their accuracy still trails that of twostage methods.
3. Network Architecture
Refer to the overall network architecture shown in Figure 1. Similar to SSD , ReﬁneDet is based on a feedforward convolutional network that produces a ﬁxed number of bounding boxes and the scores indicating the presence of different classes of objects in those boxes, followed
by the non-maximum suppression to produce the ﬁnal result. ReﬁneDet is formed by two inter-connected modules,
i.e., the ARM and the ODM. The ARM aims to remove negative anchors so as to reduce search space for the classiﬁer
and also coarsely adjust the locations and sizes of anchors
to provide better initialization for the subsequent regressor,
whereas ODM aims to regress accurate object locations and
predict multi-class labels based on the reﬁned anchors. The
ARM is constructed by removing the classiﬁcation layers
and adding some auxiliary structures of two base networks
(i.e., VGG-16 and ResNet-101 pretrained on ImageNet ) to meet our needs. The ODM is composed of
the outputs of TCBs followed by the prediction layers (i.e.,
the convolution layers with 3 × 3 kernel size), which generates the scores for object classes and shape offsets relative to
the reﬁned anchor box coordinates. The following explain
three core components in ReﬁneDet, i.e., (1) transfer connection block (TCB), converting the features from the ARM
to the ODM for detection; (2) two-step cascaded regression,
accurately regressing the locations and sizes of objects; (3)
negative anchor ﬁltering, early rejecting well-classiﬁed negative anchors and mitigate the imbalance issue.
Transfer Connection Block. To link between the ARM
and ODM, we introduce the TCBs to convert features of different layers from the ARM, into the form required by the
ODM, so that the ODM can share features from the ARM.
Notably, from the ARM, we only use the TCBs on the feature maps associated with anchors. Another function of the
TCBs is to integrate large-scale context by adding
the high-level features to the transferred features to improve
detection accuracy. To match the dimensions between them,
we use the deconvolution operation to enlarge the high-level
feature maps and sum them in the element-wise way. Then,
we add a convolution layer after the summation to ensure
the discriminability of features for detection. The architecture of the TCB is shown in Figure 2.
Two-Step Cascaded Regression. Current one-stage methods rely on one-step regression based on various
feature layers with different scales to predict the locations
and sizes of objects, which is rather inaccurate in some challenging scenarios, especially for the small objects. To that
end, we present a two-step cascaded regression strategy to
regress the locations and sizes of objects. That is, we use
the ARM to ﬁrst adjust the locations and sizes of anchors to
provide better initialization for the regression in the ODM.
Speciﬁcally, we associate n anchor boxes with each regularly divided cell on the feature map. The initial position of
each anchor box relative to its corresponding cell is ﬁxed.
At each feature map cell, we predict four offsets of the re-
ﬁned anchor boxes relative to the original tiled anchors and
3x3-s1, 256
3x3-s1, 256
3x3-s1, 256
Connection
4x4-s2, 256
Figure 2: The overview of the transfer connection block.
two conﬁdence scores indicating the presence of foreground
objects in those boxes. Thus, we can yield n reﬁned anchor
boxes at each feature map cell.
After obtaining the reﬁned anchor boxes, we pass them
to the corresponding feature maps in the ODM to further
generate object categories and accurate object locations and
sizes, as shown in Figure 1.
The corresponding feature
maps in the ARM and the ODM have the same dimension.
We calculate c class scores and the four accurate offsets of
objects relative to the reﬁned anchor boxes, yielding c + 4
outputs for each reﬁned anchor boxes to complete the detection task. This process is similar to the default boxes
used in SSD . However, in contrast to SSD directly uses the regularly tiled default boxes for detection,
ReﬁneDet uses two-step strategy, i.e., the ARM generates
the reﬁned anchor boxes, and the ODM takes the reﬁned
anchor boxes as input for further detection, leading to more
accurate detection results, especially for the small objects.
Negative Anchor Filtering. To early reject well-classiﬁed
negative anchors and mitigate the imbalance issue, we design a negative anchor ﬁltering mechanism. Speciﬁcally, in
training phase, for a reﬁned anchor box, if its negative con-
ﬁdence is larger than a preset threshold θ (i.e., set θ = 0.99
empirically), we will discard it in training the ODM. That is,
we only pass the reﬁned hard negative anchor boxes and re-
ﬁned positive anchor boxes to train the ODM. Meanwhile,
in the inference phase, if a reﬁned anchor box is assigned
with a negative conﬁdence larger than θ, it will be discarded
in the ODM for detection.
4. Training and Inference
Data Augmentation. We use several data augmentation
strategies presented in to construct a robust model to
adapt to variations of objects. That is, we randomly expand and crop the original training images with additional
random photometric distortion and ﬂipping to generate
the training samples. Please refer to for more details.
Backbone Network. We use VGG-16 and ResNet-101
 as the backbone networks in our ReﬁneDet, which are
pretrained on the ILSVRC CLS-LOC dataset . Notably,
ReﬁneDet can also work on other pretrained networks, such
as Inception V2 , Inception ResNet , and ResNeXt-
101 . Similar to DeepLab-LargeFOV , we convert
fc6 and fc7 of VGG-16 to convolution layers conv fc6 and
conv fc7 via subsampling parameters. Since conv4 3 and
conv5 3 have different feature scales compared to other layers, we use L2 normalization to scale the feature norms
in conv4 3 and conv5 3 to 10 and 8, then learn the scales
during back propagation. Meanwhile, to capture high-level
information and drive object detection at multiple scales,
we also add two extra convolution layers (i.e., conv6 1 and
conv6 2) to the end of the truncated VGG-16 and one extra
residual block (i.e., res6) to the end of the truncated ResNet-
101, respectively.
Anchors Design and Matching. To handle different scales
of objects, we select four feature layers with the total stride
sizes 8, 16, 32, and 64 pixels for both VGG-16 and ResNet-
1015, associated with several different scales of anchors for
prediction. Each feature layer is associated with one speciﬁc scale of anchors (i.e., the scale is 4 times of the total stride size of the corresponding layer) and three aspect
ratios (i.e., 0.5, 1.0, and 2.0).
We follow the design of
anchor scales over different layers in , which ensures
that different scales of anchors have the same tiling density on the image. Meanwhile, during the training phase, we determine the correspondence between the
anchors and ground truth boxes based on the jaccard overlap , and train the whole network end-to-end accordingly.
Speciﬁcally, we ﬁrst match each ground truth to the anchor
box with the best overlap score, and then match the anchor
boxes to any ground truth with overlap higher than 0.5.
Hard Negative Mining. After matching step, most of the
anchor boxes are negatives, even for the ODM, where some
easy negative anchors are rejected by the ARM. Similar
to SSD , we use hard negative mining to mitigate the
extreme foreground-background class imbalance, i.e., we
select some negative anchor boxes with top loss values to
make the ratio between the negatives and positives below
3 : 1, instead of using all negative anchors or randomly selecting the negative anchors in training.
Loss Function. The loss function for ReﬁneDet consists of
two parts, i.e., the loss in the ARM and the loss in the ODM.
5For the VGG-16 base network, the conv4 3, conv5 3, conv fc7, and
conv6 2 feature layers are used to predict the locations, sizes and con-
ﬁdences of objects.
While for the ResNet-101 base network, res3b3,
res4b22, res5c, and res6 are used for prediction.
For the ARM, we assign a binary class label (of being an
object or not) to each anchor and regress its location and
size simultaneously to get the reﬁned anchor. After that, we
pass the reﬁned anchors with the negative conﬁdence less
than the threshold to the ODM to further predict object categories and accurate object locations and sizes. With these
deﬁnitions, we deﬁne the loss function as:
L({pi}, {xi}, {ci}, {ti}) =
i Lb(pi, [l∗
i ≥1]Lr(xi, g∗
i Lm(ci, l∗
i ≥1]Lr(ti, g∗
where i is the index of anchor in a mini-batch, l∗
ground truth class label of anchor i, g∗
i is the ground truth
location and size of anchor i. pi and xi are the predicted
conﬁdence of the anchor i being an object and reﬁned coordinates of the anchor i in the ARM. ci and ti are the
predicted object class and coordinates of the bounding box
in the ODM. Narm and Nodm are the numbers of positive
anchors in the ARM and ODM, respectively. The binary
classiﬁcation loss Lb is the cross-entropy/log loss over two
classes (object vs. not object), and the multi-class classiﬁcation loss Lm is the softmax loss over multiple classes con-
ﬁdences. Similar to Fast R-CNN , we use the smooth
L1 loss as the regression loss Lr. The Iverson bracket indicator function [l∗
i ≥1] outputs 1 when the condition is true,
i ≥1 (the anchor is not the negative), and 0 otherwise. Hence [l∗
i ≥1]Lr indicates that the regression loss is
ignored for negative anchors. Notably, if Narm = 0, we set
Lb(pi, [l∗
i ≥1]) = 0 and Lr(xi, g∗
i ) = 0; and if Nodm = 0,
we set Lm(ci, l∗
i ) = 0 and Lr(ti, g∗
i ) = 0 accordingly.
Optimization. As mentioned above, the backbone network
(e.g., VGG-16 and ResNet-101) in our ReﬁneDet method is
pretrained on the ILSVRC CLS-LOC dataset . We use
the “xavier” method to randomly initialize the parameters in the two extra added convolution layers (i.e., conv6 1
and conv6 2) of VGG-16 based ReﬁneDet, and draw the parameters from a zero-mean Gaussian distribution with standard deviation 0.01 for the extra residual block (i.e., res6) of
ResNet-101 based ReﬁneDet. We set the default batch size
to 32 in training. Then, the whole network is ﬁne-tuned using SGD with 0.9 momentum and 0.0005 weight decay. We
set the initial learning rate to 10−3, and use slightly different learning rate decay policy for different dataset, which
will be described in details later.
Inference. At inference phase, the ARM ﬁrst ﬁlters out the
regularly tiled anchors with the negative conﬁdence scores
larger than the threshold θ, and then reﬁnes the locations
and sizes of remaining anchors. After that, the ODM takes
over these reﬁned anchors, and outputs top 400 high con-
ﬁdent detections per image.
Finally, we apply the nonmaximum suppression with jaccard overlap of 0.45 per
class and retain the top 200 high conﬁdent detections per
image to produce the ﬁnal detection results.
5. Experiments
Experiments are conducted on three datasets: PASCAL
VOC 2007, PASCAL VOC 2012 and MS COCO. The PAS-
CAL VOC and MS COCO datasets include 20 and 80 object classes, respectively. The classes in PASCAL VOC are
the subset of that in MS COCO. We implement ReﬁneDet
in Caffe . All the training and testing codes and the
trained models are available at 
sfzhang15/RefineDet.
5.1. PASCAL VOC 2007
All models are trained on the VOC 2007 and VOC 2012
trainval sets, and tested on the VOC 2007 test set. We
set the learning rate to 10−3 for the ﬁrst 80k iterations, and
decay it to 10−4 and 10−5 for training another 20k and 20k
iterations, respectively. We use the default batch size 32 in
training, and only use VGG-16 as the backbone network for
all the experiments on the PASCAL VOC dataset, including
VOC 2007 and VOC 2012.
We compare ReﬁneDet6 with the state-of-the-art detectors in Table 1. With low dimension input (i.e., 320 × 320),
ReﬁneDet produces 80.0% mAP without bells and whistles, which is the ﬁrst method achieving above 80% mAP
with such small input images, much better than several
modern objectors. By using larger input size 512 × 512,
ReﬁneDet achieves 81.8% mAP, surpassing all one-stage
methods, e.g., RON384 , SSD513 , DSSD513 ,
etc. Comparing to the two-stage methods, ReﬁneDet512
performs better than most of them except CoupleNet ,
which is based on ResNet-101 and uses larger input size
(i.e., ∼1000 × 600) than our ReﬁneDet512. As pointed
out in , the input size signiﬁcantly inﬂuences detection
accuracy. The reason is that high resolution inputs make
the detectors “seeing” small objects clearly to increase successful detections. To reduce the impact of input size for a
fair comparison, we use the multi-scale testing strategy to
evaluate ReﬁneDet, achieving 83.1% (ReﬁneDet320+) and
83.8% (ReﬁneDet512+) mAPs, which are much better than
the state-of-the-art methods.
Run Time Performance
We present the inference speed of ReﬁneDet and the stateof-the-art methods in the ﬁfth column of Table 1. The speed
is evaluated with batch size 1 on a machine with NVIDIA
Titan X, CUDA 8.0 and cuDNN v6. As shown in Table 1,
we ﬁnd that ReﬁneDet processes an image in 24.8ms (40.3
FPS) and 41.5ms (24.1 FPS) with input sizes 320 × 320
and 512 × 512, respectively. To the best of our knowledge,
6Due to the shortage of computational resources, we only train Re-
ﬁneDet with two kinds of input size, i.e., 320 × 320 and 512 × 512. We
believe the accuracy of ReﬁneDet can be further improved using larger
input images.
Table 1: Detection results on PASCAL VOC dataset. For VOC 2007, all methods are trained on VOC 2007 and VOC 2012
trainval sets and tested on VOC 2007 test set. For VOC 2012, all methods are trained on VOC 2007 and VOC 2012
trainval sets plus VOC 2007 test set, and tested on VOC 2012 test set. Bold fonts indicate the best mAP.
Input size
two-stage:
Fast R-CNN 
∼1000 × 600
Faster R-CNN 
∼1000 × 600
∼1000 × 600
HyperNet 
∼1000 × 600
Faster R-CNN 
ResNet-101
∼1000 × 600
∼1000 × 600
MR-CNN 
∼1000 × 600
ResNet-101
∼1000 × 600
CoupleNet 
ResNet-101
∼1000 × 600
one-stage:
GoogleNet 
RON384 
SSD321 
ResNet-101
SSD300∗ 
DSOD300 
DS/64-192-48-1
YOLOv2 
Darknet-19
DSSD321 
ResNet-101
SSD512∗ 
SSD513 
ResNet-101
DSSD513 
ResNet-101
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
ReﬁneDet is the ﬁrst real-time method to achieve detection
accuracy above 80% mAP on PASCAL VOC 2007. Comparing to SSD, RON, DSSD and DSOD, ReﬁneDet associates fewer anchor boxes on the feature maps (e.g., 24564
anchor boxes in SSD512∗ vs. 16320 anchor boxes in
ReﬁneDet512). However, ReﬁneDet still achieves top accuracy with high efﬁciency, mainly thanks to the design of two
inter-connected modules, (e.g., two-step regression), which
enables ReﬁneDet to adapt to different scales and aspect ratios of objects. Meanwhile, only YOLO and SSD300∗are
slightly faster than our ReﬁneDet320, but their accuracy are
16.6% and 2.5% worse than ours. In summary, ReﬁneDet
achieves the best trade-off between accuracy and speed.
Ablation Study
To demonstrate the effectiveness of different components
in ReﬁneDet, we construct four variants and evaluate them
on VOC 2007, shown in Table 3. Speciﬁcally, for a fair
comparison, we use the same parameter settings and input
size (320 × 320) in evaluation. All models are trained on
VOC 2007 and VOC 2012 trainval sets, and tested on
VOC 2007 test set.
Negative Anchor Filtering. To demonstrate the effectiveness of the negative anchor ﬁltering, we set the conﬁdence
threshold θ of the anchors to be negative to 1.0 in both training and testing. In this case, all reﬁned anchors will be
sent to the ODM for detection. Other parts of ReﬁneDet remain unchanged. Removing negative anchor ﬁltering leads
to 0.5% drop in mAP (i.e., 80.0% vs. 79.5%). The reason
is that most of these well-classiﬁed negative anchors will be
ﬁltered out during training, which solves the class imbalance issue to some extent.
Two-Step Cascaded Regression. To validate the effectiveness of the two-step cascaded regression, we redesign the
network structure by directly using the regularly paved anchors instead of the reﬁned ones from the ARM (see the
fourth column in Table 3). As shown in Table 3, we ﬁnd that
mAP is reduced from 79.5% to 77.3%. This sharp decline
(i.e., 2.2%) demonstrates that the two-step anchor cascaded
regression signiﬁcantly help promote the performance.
Transfer Connection Block. We construct a network by
cutting the TCBs in ReﬁneDet and redeﬁning the loss function in the ARM to directly detect multi-class of objects,
just like SSD, to demonstrate the effect of the TCB. The
detection accuracy of the model is presented in the ﬁfth column in Table 3. We compare the results in the fourth and
ﬁfth columns in Table 3 (77.3% vs. 76.2%) and ﬁnd that
the TCB improves the mAP by 1.1%. The main reason is
Table 2: Detection results on MS COCO test-dev set. Bold fonts indicate the best performance.
two-stage:
Fast R-CNN 
Faster R-CNN 
OHEM++ 
ResNet-101
CoupleNet 
ResNet-101
Faster R-CNN by G-RMI 
Inception-ResNet-v2 
Faster R-CNN+++ 
ResNet-101-C4
Faster R-CNN w FPN 
trainval35k
ResNet-101-FPN
Faster R-CNN w TDM 
Inception-ResNet-v2-TDM
Deformable R-FCN 
Aligned-Inception-ResNet
umd det 
ResNet-101
G-RMI 
trainval32k
Ensemble of Five Models
one-stage:
YOLOv2 
trainval35k
DarkNet-19 
SSD300∗ 
trainval35k
RON384++ 
SSD321 
trainval35k
ResNet-101
DSSD321 
trainval35k
ResNet-101
SSD512∗ 
trainval35k
SSD513 
trainval35k
ResNet-101
DSSD513 
trainval35k
ResNet-101
RetinaNet500 
trainval35k
ResNet-101
RetinaNet800 ∗
trainval35k
ResNet-101-FPN
ReﬁneDet320
trainval35k
ReﬁneDet512
trainval35k
ReﬁneDet320
trainval35k
ResNet-101
ReﬁneDet512
trainval35k
ResNet-101
ReﬁneDet320+
trainval35k
ReﬁneDet512+
trainval35k
ReﬁneDet320+
trainval35k
ResNet-101
ReﬁneDet512+
trainval35k
ResNet-101
∗This entry reports the single model accuracy of RetinaNet method, trained with scale jitter and for 1.5× longer than RetinaNet500.
Table 3: Effectiveness of various designs. All models are
trained on VOC 2007 and VOC 2012 trainval set and
tested on VOC 2007 test set.
ReﬁneDet320
negative anchor ﬁltering?
two-step cascaded regression?
transfer connection block?
that the model can inherit the discriminative features from
the ARM, and integrate large-scale context information to
improve the detection accuracy by using the TCB.
5.2. PASCAL VOC 2012
Following the protocol of VOC 2012, we submit the detection results of ReﬁneDet to the public testing server for
evaluation. We use VOC 2007 trainval set and test
set plus VOC 2012 trainval set (21, 503 images) for
training, and test on VOC 2012 test set (10, 991 images).
We use the default batch size 32 in training. Meanwhile, we
set the learning rate to 10−3 in the ﬁrst 160k iterations, and
decay it to 10−4 and 10−5 for another 40k and 40k iterations.
Table 1 shows the accuracy of the proposed ReﬁneDet algorithm, as well as the state-of-the-art methods. Among the
methods fed with input size 320 × 320, ReﬁneDet320 obtains the top 78.1% mAP, which is even better than most of
those two-stage methods using about 1000 × 600 input size
(e.g., 70.4% mAP of Faster R-CNN and 77.6% mAP
of R-FCN ). Using the input size 512 × 512, ReﬁneDet
improves mAP to 80.1%, which is surpassing all one-stage
methods and only slightly lower than CoupleNet (i.e.,
80.4%). CoupleNet uses ResNet-101 as base network with
1000 × 600 input size. To reduce the impact of input size
for a fair comparison, we also use multi-scale testing to
evaluate ReﬁneDet and obtain the state-of-the-art mAPs of
82.7% (ReﬁneDet320+) and 83.5% (ReﬁneDet512+).
Table 4: Detection results on PASCAL VOC dataset. All
models are pre-trained on MS COCO, and ﬁne-tuned on
PASCAL VOC. Bold fonts indicate the best mAP.
VOC 2007 test VOC 2012 test
two-stage:
Faster R-CNN 
OHEM++ 
ResNet-101
one-stage:
SSD300 
SSD512 
RON384++ 
DSOD300 
DS/64-192-48-1
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
5.3. MS COCO
In addition to PASCAL VOC, we also evaluate Re-
ﬁneDet on MS COCO . Unlike PASCAL VOC, the
detection methods using ResNet-101 always achieve better performance than those using VGG-16 on MS COCO.
Thus, we also report the results of ResNet-101 based Re-
ﬁneDet. Following the protocol in MS COCO, we use the
trainval35k set for training and evaluate the results
from test-dev evaluation server. We set the batch size to
32 in training7, and train the model with 10−3 learning rate
for the ﬁrst 280k iterations, then 10−4 and 10−5 for another
80k and 40k iterations, respectively.
Table 7 shows the results on MS COCO test-dev set.
ReﬁneDet320 with VGG-16 produces 29.4% AP that is better than all other methods based on VGG-16 (e.g., SSD512∗
 and OHEM++ ). The accuracy of ReﬁneDet can
be improved to 33.0% by using larger input size (i.e.,
512 × 512), which is much better than several modern object detectors, e.g., Faster R-CNN and SSD512∗ .
Meanwhile, using ResNet-101 can further improve the performance of ReﬁneDet, i.e., ReﬁneDet320 with ResNet-101
achieves 32.0% AP and ReﬁneDet512 achieves 36.4% AP,
exceeding most of the detection methods except Faster R-
CNN w TDM , Deformable R-FCN , RetinaNet800
 , umd det , and G-RMI . All these methods use a
much bigger input images for both training and testing (i.e.,
1000×600 or 800×800) than our ReﬁneDet (i.e., 320×320
and 512 × 512). Similar to PASCAL VOC, we also report
the multi-scale testing AP results of ReﬁneDet for fair comparison in Table 7, i.e., 35.2% (ReﬁneDet320+ with VGG-
16), 37.6% (ReﬁneDet512+ with VGG-16), 38.6% (Re-
7Due to the memory issue, we reduce the batch size to 20 (which is the
largest batch size we can use for training on a machine with 4 NVIDIA
M40 GPUs) to train the ResNet-101 based ReﬁneDet with the input size
512 × 512, and train the model with 10−3 learning rate for the ﬁrst 400k
iterations, then 10−4 and 10−5 for another 80k and 60k iterations.
ﬁneDet320+ with ResNet-101) and 41.8% (ReﬁneDet512+
with ResNet-101). The best performance of ReﬁneDet is
41.8%, which is the state-of-the-art, surpassing all published two-stage and one-stage approaches. Although the
second best detector G-RMI ensembles ﬁve Faster R-
CNN models, it still produces 0.2% lower AP than Re-
ﬁneDet using a single model. Comparing to the third and
fourth best detectors, i.e., umd det and RetinaNet800
 , ReﬁneDet produces 1.0% and 2.7% higher APs. In
addition, the main contribution: focal loss in RetinaNet800,
is complementary to our method. We believe that it can be
used in ReﬁneNet to further improve the performance.
5.4. From MS COCO to PASCAL VOC
We study how the MS COCO dataset help the detection accuracy on PASCAL VOC. Since the object classes
in PASCAL VOC are the subset of MS COCO, we directly
ﬁne-tune the detection models pretrained on MS COCO via
subsampling the parameters, which achieves 84.0% mAP
(ReﬁneDet320) and 85.2% mAP (ReﬁneDet512) on VOC
2007 test set, and 82.7% mAP (ReﬁneDet320) and 85.0%
mAP (ReﬁneDet512) on VOC 2012 test set, shown in Table 4. After using the multi-scale testing, the detection accuracy are promoted to 85.6%, 85.8%, 86.0% and 86.8%,
respectively. As shown in Table 4, using the training data in
MS COCO and PASCAL VOC, our ReﬁneDet obtains the
top mAP scores on both VOC 2007 and VOC 2012. Most
important, our single model ReﬁneNet512+ based on VGG-
16 ranks as the top 5 on the VOC 2012 Leaderboard (see
 ), which is the best accuracy among all one-stage methods. Other two-stage methods achieving better results are
based on much deeper networks (e.g., ResNet-101 and
ResNeXt-101 ) or using ensemble mechanism.
6. Conclusions
In this paper, we present a single-shot reﬁnement neural network based detector, which consists of two interconnected modules, i.e., the ARM and the ODM. The ARM
aims to ﬁlter out the negative anchors to reduce search space
for the classiﬁer and also coarsely adjust the locations and
sizes of anchors to provide better initialization for the subsequent regressor, while the ODM takes the reﬁned anchors as
the input from the former ARM to regress the accurate object locations and sizes and predict the corresponding multiclass labels. The whole network is trained in an end-to-end
fashion with the multi-task loss. We carry out several experiments on PASCAL VOC 2007, PASCAL VOC 2012, and
MS COCO datasets to demonstrate that ReﬁneDet achieves
the state-of-the-art detection accuracy with high efﬁciency.
In the future, we plan to employ ReﬁneDet to detect some
other speciﬁc kinds of objects, e.g., pedestrian, vehicle, and
face, and introduce the attention mechanism in ReﬁneDet to
further improve the performance.