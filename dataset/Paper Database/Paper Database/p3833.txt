A Survey on Deep Learning in Medical Image Analysis
Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi,
Mohsen Ghafoorian, Jeroen A.W.M. van der Laak, Bram van Ginneken, Clara I. S´anchez
Diagnostic Image Analysis Group
Radboud University Medical Center
Nijmegen, The Netherlands
Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for
analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis
and summarizes over 300 contributions to the ﬁeld, most of which appeared in the last year. We survey the use of
deep learning for image classiﬁcation, object detection, segmentation, registration, and other tasks. Concise overviews
are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal,
musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and
directions for future research.
Keywords: deep learning, convolutional neural networks, medical imaging, survey
1. Introduction
As soon as it was possible to scan and load medical images into a computer, researchers have built systems for automated analysis. Initially, from the 1970s
to the 1990s, medical image analysis was done with sequential application of low-level pixel processing (edge
and line detector ﬁlters, region growing) and mathematical modeling (ﬁtting lines, circles and ellipses) to
construct compound rule-based systems that solved particular tasks. There is an analogy with expert systems
with many if-then-else statements that were popular in
artiﬁcial intelligence in the same period.
These expert systems have been described as GOFAI (good oldfashioned artiﬁcial intelligence) and
were often brittle; similar to rule-based image processing systems.
At the end of the 1990s, supervised techniques, where
training data is used to develop a system, were becoming increasingly popular in medical image analysis. Examples include active shape models (for segmentation),
atlas methods (where the atlases that are ﬁt to new data
form the training data), and the concept of feature extraction and use of statistical classiﬁers (for computeraided detection and diagnosis). This pattern recognition or machine learning approach is still very popular
and forms the basis of many successful commercially
available medical image analysis systems.
have seen a shift from systems that are completely designed by humans to systems that are trained by computers using example data from which feature vectors
are extracted. Computer algorithms determine the optimal decision boundary in the high-dimensional feature
space. A crucial step in the design of such systems is
the extraction of discriminant features from the images.
This process is still done by human researchers and, as
such, one speaks of systems with handcrafted features.
A logical next step is to let computers learn the features that optimally represent the data for the problem at
hand. This concept lies at the basis of many deep learning algorithms: models (networks) composed of many
layers that transform input data (e.g. images) to outputs
(e.g. disease present/absent) while learning increasingly
higher level features. The most successful type of models for image analysis to date are convolutional neural networks (CNNs). CNNs contain many layers that
transform their input with convolution ﬁlters of a small
extent. Work on CNNs has been done since the late seventies and they were already applied
to medical image analysis in 1995 by Lo et al. .
They saw their ﬁrst successful real-world application in
LeNet for hand-written digit recog-
 
nition. Despite these initial successes, the use of CNNs
did not gather momentum until various new techniques
were developed for eﬃciently training deep networks,
and advances were made in core computing systems.
The watershed was the contribution of Krizhevsky et al.
 to the ImageNet challenge in December 2012.
The proposed CNN, called AlexNet, won that competition by a large margin. In subsequent years, further
progress has been made using related but deeper architectures . In computer vision,
deep convolutional networks have now become the technique of choice.
The medical image analysis community has taken notice of these pivotal developments. However, the transition from systems that use handcrafted features to systems that learn features from the data has been gradual.
Before the breakthrough of AlexNet, many different techniques to learn features were popular. Bengio et al. provide a thorough review of these
techniques. They include principal component analysis,
clustering of image patches, dictionary approaches, and
many more. Bengio et al. introduce CNNs that
are trained end-to-end only at the end of their review in
a section entitled Global training of deep models. In this
survey, we focus particularly on such deep models, and
do not include the more traditional feature learning approaches that have been applied to medical images. For
a broader review on the application of deep learning in
health informatics we refer to Ravi et al. , where
medical image analysis is brieﬂy touched upon.
Applications of deep learning to medical image analysis ﬁrst started to appear at workshops and conferences, and then in journals. The number of papers grew
rapidly in 2015 and 2016. This is illustrated in Figure
1. The topic is now dominant at major conferences and
a ﬁrst special issue appeared of IEEE Transaction on
Medical Imaging in May 2016 .
One dedicated review on application of deep learning
to medical image analysis was published by Shen et al.
 . Although they cover a substantial amount of
work, we feel that important areas of the ﬁeld were not
represented. To give an example, no work on retinal image analysis was covered. The motivation for our review
was to oﬀer a comprehensive overview of (almost) all
ﬁelds in medical imaging, both from an application and
a methodology-drive perspective.
This also includes
overview tables of all publications which readers can
use to quickly assess the ﬁeld. Last, we leveraged our
own experience with the application of deep learning
methods to medical image analysis to provide readers
with a dedicated discussion section covering the stateof-the-art, open challenges and overview of research directions and technologies that will become important in
the future.
This survey includes over 300 papers, most of them
recent, on a wide variety of applications of deep learning in medical image analysis.
To identify relevant
contributions PubMed was queried for papers containing (”convolutional” OR ”deep learning”) in title or abstract. ArXiv was searched for papers mentioning one
of a set of terms related to medical imaging.
Additionally, conference proceedings for MICCAI (including workshops), SPIE, ISBI and EMBC were searched
based on titles of papers. We checked references in all
selected papers and consulted colleagues. We excluded
papers that did not report results on medical image
data or only used standard feed-forward neural networks
with handcrafted features. When overlapping work had
been reported in multiple publications, only the publication(s) deemed most important were included. We expect the search terms used to cover most, if not all, of
the work incorporating deep learning methods. The last
update to the included papers was on February 1, 2017.
The appendix describes the search process in more detail.
Summarizing, with this survey we aim to:
• show that deep learning techniques have permeated
the entire ﬁeld of medical image analysis;
• identify the challenges for successful application
of deep learning to medical imaging tasks;
• highlight speciﬁc contributions which solve or circumvent these challenges.
The rest of this survey as structured as followed. In
Section 2 we introduce the main deep learning techniques that have been used for medical image analysis and that are referred to throughout the survey. Section 3 describes the contributions of deep learning to
canonical tasks in medical image analysis: classiﬁcation, detection, segmentation, registration, retrieval, image generation and enhancement. Section 4 discusses
obtained results and open challenges in diﬀerent application areas: neuro, ophthalmic, pulmonary, digital
pathology and cell imaging, breast, cardiac, abdominal,
musculoskeletal, and remaining miscellaneous applications. We end with a summary, a critical discussion and
an outlook for future research.
2. Overview of deep learning methods
The goal of this section is to provide a formal introduction and deﬁnition of the deep learning concepts,
Number of papers
RegistraƟon
SegmentaƟon (Object)
DetecƟon (Organ, region, landmark)
ClassiﬁcaƟon (Object)
ClassiﬁcaƟon (Exam)
DetecƟon (Object)
SegmentaƟon (Organ, substructure)
Color fundus photos
Mammography
Ultrasound
Microscopy
Number of papers
Number of papers
Figure 1: Breakdown of the papers included in this survey in year of publication, task addressed (Section 3), imaging modality, and application
area (Section 4). The number of papers for 2017 has been extrapolated from the papers published in January.
techniques and architectures that we found in the medical image analysis papers surveyed in this work.
2.1. Learning algorithms
Machine learning methods are generally divided into
supervised and unsupervised learning algorithms, although there are many nuances. In supervised learning,
a model is presented with a dataset D = {x, y}N
n=1 of input features x and label y pairs, where y typically represents an instance of a ﬁxed set of classes. In the case of
regression tasks y can also be a vector with continuous
values. Supervised training typically amounts to ﬁnding
model parameters Θ that best predict the data based on
a loss function L(y, ˆy). Here ˆy denotes the output of the
model obtained by feeding a data point x to the function
f(x; Θ) that represents the model.
Unsupervised learning algorithms process data without labels and are trained to ﬁnd patterns, such as latent subspaces. Examples of traditional unsupervised
learning algorithms are principal component analysis
and clustering methods. Unsupervised training can be
performed under many diﬀerent loss functions. One example is reconstruction loss L(x, ˆx) where the model has
to learn to reconstruct its input, often through a lowerdimensional or noisy representation.
2.2. Neural Networks
Neural networks are a type of learning algorithm
which forms the basis of most deep learning methods. A
neural network comprises of neurons or units with some
activation a and parameters Θ = {W, B}, where W is a
set of weights and B a set of biases. The activation represents a linear combination of the input x to the neuron
and the parameters, followed by an element-wise nonlinearity σ(·), referred to as a transfer function:
a = σ(wTx + b).
Typical transfer functions for traditional neural networks are the sigmoid and hyperbolic tangent function.
The multi-layered perceptrons (MLP), the most wellknown of the traditional neural networks, have several
layers of these transformations:
f(x; Θ) = σ(WTσ(WT . . . σ(WTx + b)) + b).
Here, W is a matrix comprising of columns wk, associated with activation k in the output. Layers in between
the input and output are often referred to as ’hidden’
layers. When a neural network contains multiple hidden
layers it is typically considered a ’deep’ neural network,
hence the term ’deep learning’.
At the ﬁnal layer of the network the activations are
mapped to a distribution over classes P(y|x; Θ) through
a softmax function:
P(y|x; Θ) = softmax(x; Θ) =
where wi indicates the weight vector leading to the output node associated with class i. A schematic representation of three-layer MLP is shown in Figure 2.
Maximum likelihood with stochastic gradient descent
is currently the most popular method to ﬁt parameters Θ
to a dataset D. In stochastic gradient descent a small
subset of the data, a mini-batch, is used for each gradient update instead of the full data set. Optimizing maximum likelihood in practice amounts to minimizing the
negative log-likelihood:
log P(yn|xn; Θ).
This results in the binary cross-entropy loss for twoclass problems and the categorical cross-entropy for
multi-class tasks. A downside of this approach is that
it typically does not optimize the quantity we are interested in directly, such as area under the receiveroperating characteristic (ROC) curve or common evaluation measures for segmentation, such as the Dice coef-
For a long time, deep neural networks (DNN) were
considered hard to train eﬃciently. They only gained
popularity in 2006 when it was
shown that training DNNs layer-by-layer in an unsupervised manner (pre-training), followed by supervised
ﬁne-tuning of the stacked network, could result in good
performance. Two popular architectures trained in such
a way are stacked auto-encoders (SAEs) and deep belief
networks (DBNs). However, these techniques are rather
complex and require a signiﬁcant amount of engineering to generate satisfactory results.
Currently, the most popular models are trained endto-end in a supervised fashion, greatly simplifying
the training process.
The most popular architectures
are convolutional neural networks (CNNs) and recurrent neural networks (RNNs).
CNNs are currently
most widely used in (medical) image analysis, although
RNNs are gaining popularity. The following sections
will give a brief overview of each of these methods,
starting with the most popular ones, and discussing their
diﬀerences and potential challenges when applied to
medical problems.
2.3. Convolutional Neural Networks (CNNs)
There are two key diﬀerences between MLPs and
CNNs. First, in CNNs weights in the network are shared
in such a way that it the network performs convolution
operations on images. This way, the model does not
need to learn separate detectors for the same object occurring at diﬀerent positions in an image, making the
network equivariant with respect to translations of the
input. It also drastically reduces the amount of parameters (i.e. the number of weights no longer depends on
the size of the input image) that need to be learned. An
example of a 1D CNN is shown in Figure 2.
At each layer, the input image is convolved with a
set of K kernels W = {W1, W2, . . . , WK} and added
biases B = {b1, . . . , bK}, each generating a new feature
map Xk. These features are subjected to an elementwise non-linear transform σ(·) and the same process is
repeated for every convolutional layer l:
k = σ Wl−1
∗Xl−1 + bl−1
The second key diﬀerence between CNNs and MLPs,
is the typical incorporation of pooling layers in CNNs,
where pixel values of neighborhoods are aggregated using a permutation invariant function, typically the max
or mean operation. This induces a certain amount of
translation invariance and again reduces the amount of
parameters in the network. At the end of the convolutional stream of the network, fully-connected layers
(i.e. regular neural network layers) are usually added,
where weights are no longer shared. Similar to MLPs,
a distribution over classes is generated by feeding the
activations in the ﬁnal layer through a softmax function
and the network is trained using maximum likelihood.
2.4. Deep CNN Architectures
Given the prevalence of CNNs in medical image analysis, we elaborate on the most common architectures
and architectural diﬀerences among the widely used
2.4.1. General classiﬁcation architectures
LeNet and AlexNet , introduced over a decade later, were in
essence very similar models. Both networks were relatively shallow, consisting of two and ﬁve convolutional
layers, respectively, and employed kernels with large receptive ﬁelds in layers close to the input and smaller
kernels closer to the output. AlexNet did incorporate
rectiﬁed linear units instead of the hyperbolic tangent as
activation function.
After 2012 the exploration of novel architectures took
oﬀ, and in the last three years there is a preference for
far deeper models. By stacking smaller kernels, instead
of using a single layer of kernels with a large receptive
ﬁeld, a similar function can be represented with less parameters. These deeper architectures generally have a
lower memory footprint during inference, which enable
their deployment on mobile computing devices such as
smartphones. Simonyan and Zisserman were the
ﬁrst to explore much deeper networks, and employed
small, ﬁxed size kernels in each layer. A 19-layer model
often referred to as VGG19 or OxfordNet won the ImageNet challenge of 2014.
On top of the deeper networks, more complex building blocks have been introduced that improve the eﬃciency of the training procedure and again reduce the
amount of parameters. Szegedy et al. introduced
a 22-layer network named GoogLeNet, also referred to
as Inception, which made use of so-called inception
blocks , a module that replaces the
mapping deﬁned in Eq. (5) with a set of convolutions of
diﬀerent sizes. Similar to the stacking of small kernels,
this allows a similar function to be represented with less
parameters. The ResNet architecture 
won the ImageNet challenge in 2015 and consisted of
so-called ResNet-blocks. Rather than learning a function, the residual block only learns the residual and is
thereby pre-conditioned towards learning mappings in
each layer that are close to the identity function. This
way, even deeper models can be trained eﬀectively.
Since 2014, the performance on the ImageNet benchmark has saturated and it is diﬃcult to assess whether
the small increases in performance can really be attributed to ’better’ and more sophisticated architectures.
The advantage of the lower memory footprint these
models provide is typically not as important for medical applications. Consequently, AlexNet or other simple models such as VGG are still popular for medical
data, though recent landmark studies all use a version
of GoogleNet called Inception v3 . Whether this is due
to a superior architecture or simply because the model
is a default choice in popular software packages is again
diﬃcult to assess.
2.4.2. Multi-stream architectures
The default CNN architecture can easily accommodate multiple sources of information or representations
of the input, in the form of channels presented to the
input layer. This idea can be taken further and channels can be merged at any point in the network. Under
the intuition that diﬀerent tasks require diﬀerent ways
of fusion, multi-stream architectures are being explored.
These models, also referred to as dual pathway architectures , have two main applications at the time of writing: (1) multi-scale image analysis and (2) 2.5D classiﬁcation; both relevant for medical
image processing tasks.
For the detection of abnormalities, context is often
an important cue. The most straightforward way to increase context is to feed larger patches to the network,
but this can signiﬁcantly increase the amount of parameters and memory requirements of a network. Consequently, architectures have been investigated where context is added in a down-scaled representation in addition to high resolution local information. To the best
of our knowledge, the multi-stream multi-scale architecture was ﬁrst explored by Farabet et al. , who
used it for segmentation in natural images. Several medical applications have also successfully used this concept .
As so much methodology is still developed on natural images, the challenge of applying deep learning
techniques to the medical domain often lies in adapting existing architectures to, for instance, diﬀerent input
formats such as three-dimensional data. In early applications of CNNs to such volumetric data, full 3D convolutions and the resulting large amount of parameters
were circumvented by dividing the Volume of Interest
(VOI) into slices which are fed as diﬀerent streams to a
network. Prasoon et al. were the ﬁrst to use this
approach for knee cartilage segmentation. Similarly, the
network can be fed with multiple angled patches from
the 3D-space in a multi-stream fashion, which has been
applied by various authors in the context of medical
imaging . These
approaches are also referred to as 2.5D classiﬁcation.
2.4.3. Segmentation Architectures
Segmentation is a common task in both natural and
medical image analysis and to tackle this, CNNs can
simply be used to classify each pixel in the image individually, by presenting it with patches extracted around
the particular pixel. A drawback of this naive ’slidingwindow’ approach is that input patches from neighboring pixels have huge overlap and the same convolutions
are computed many times. Fortunately, the convolution
and dot product are both linear operators and thus inner
products can be written as convolutions and vice versa.
By rewriting the fully connected layers as convolutions,
the CNN can take input images larger than it was trained
on and produce a likelihood map, rather than an output for a single pixel. The resulting ’fully convolutional
network’ (fCNN) can then be applied to an entire input
image or volume in an eﬃcient fashion.
However, because of pooling layers, this may result
in output with a far lower resolution than the input.
’Shift-and-stitch’ is one of several
methods proposed to prevent this decrease in resolution.
The fCNN is applied to shifted versions of the input image. By stitching the result together, one obtains a full
resolution version of the ﬁnal output, minus the pixels
lost due to the ’valid’ convolutions.
Ronneberger et al. took the idea of the fCNN
one step further and proposed the U-net architecture,
comprising a ’regular’ fCNN followed by an upsampling part where ’up’-convolutions are used to increase the image size, coined contractive and expansive
paths. Although this is not the ﬁrst paper to introduce
learned upsampling paths in convolutional neural networks ), the authors combined it
with so called skip-connections to directly connect opposing contracting and expanding convolutional layers.
A similar approach was used by C¸ ic¸ek et al. for
3D data. Milletari et al. proposed an extension
to the U-Net layout that incorporates ResNet-like residual blocks and a Dice loss layer, rather than the conventional cross-entropy, that directly minimizes this commonly used segmentation error measure.
2.5. Recurrent Neural Networks (RNNs)
Traditionally, RNNs were developed for discrete sequence analysis. They can be seen as a generalization
of MLPs because both the input and output can be of
varying length, making them suitable for tasks such as
machine translation where a sentence of the source and
target language are the input and output. In a classiﬁcation setting, the model learns a distribution over classes
P(y|x1, x2, . . . , xT; Θ) given a sequence x1, x2, . . . , xT,
rather than a single input vector x.
The plain RNN maintains a latent or hidden state h at
time t that is the output of a non-linear mapping from its
input xt and the previous state ht−1:
ht = σ(Wxt + Rht−1 + b),
where weight matrices W and R are shared over time.
For classiﬁcation, one or more fully connected layers
are typically added followed by a softmax to map the
sequence to a posterior over the classes.
P(y|x1, x2, . . . , xT; Θ) = softmax(hT; Wout, bout).
Since the gradient needs to be backpropagated from
the output through time, RNNs are inherently deep
(in time) and consequently suﬀer from the same
problems with training as regular deep neural networks
 . To this end, several specialized
memory units have been developed, the earliest and
most popular being the Long Short Term Memory
(LSTM) cell . The
Gated Recurrent Unit is a recent
simpliﬁcation of the LSTM and is also commonly used.
Although initially proposed for one-dimensional input, RNNs are increasingly applied to images. In natural images ’pixelRNNs’ are used as autoregressive models, generative models that can eventually produce new
images similar to samples in the training set. For medical applications, they have been used for segmentation
problems, with promising results 
in the MRBrainS challenge.
2.6. Unsupervised models
2.6.1. Auto-encoders (AEs) and Stacked Auto-encoders
AEs are simple networks that are trained to reconstruct the input x on the output layer x′ through one hidden layer h. They are governed by a weight matrix Wx,h
and bias bx,h from input to hidden state and Wh,x′ with
corresponding bias bh,x′ from the hidden layer to the reconstruction. A non-linear function is used to compute
the hidden activation:
h = σ(Wx,hx + bx,h).
Additionally, the dimension of the hidden layer |h| is
taken to be smaller than |x|. This way, the data is projected onto a lower dimensional subspace representing
a dominant latent structure in the input. Regularization
or sparsity constraints can be employed to enhance the
discovery process. If the hidden layer had the same size
as the input and no further non-linearities were added,
the model would simply learn the identity function.
The denoising auto-encoder is
another solution to prevent the model from learning a
trivial solution.
Here the model is trained to reconstruct the input from a noise corrupted version (typically
salt-and-pepper-noise). SAEs (or deep AEs) are formed
by placing auto-encoder layers on top of each other.
In medical applications surveyed in this work, autoencoder layer were often trained individually (‘greedily’) after which the full network was ﬁne-tuned using
supervised training to make a prediction.
2.6.2. Restricted Boltzmann Machines (RBMs) and
Deep Belief Networks (DBNs)
RBMs are a type of Markov Random Field (MRF), constituting an input layer or visible layer x = (x1, x2, . . . , xN) and a hidden layer h =
(h1, h2, . . . , hM) that carries the latent feature representation.
The connections between the nodes are bidirectional, so given an input vector x one can obtain
the latent feature representation h and also vice versa.
As such, the RBM is a generative model, and we can
sample from it and generate new data points. In analogy to physical systems, an energy function is deﬁned
for a particular state (x, h) of input and hidden units:
E(x, h) = hTWx −cTx −bTh,
with c and b bias terms. The probability of the ‘state’ of
the system is deﬁned by passing the energy to an exponential and normalizing:
p(x, h) = 1
Z exp{−E(x, h)}.
Computing the partition function Z is generally intractable. However, conditional inference in the form of
computing h conditioned on v or vice versa is tractable
and results in a simple formula:
1 + exp{−bj −W jx}.
Since the network is symmetric, a similar expression
holds for P(xi|h).
DBNs are
essentially SAEs where the AE layers are replaced by
RBMs. Training of the individual layers is, again, done
in an unsupervised manner.
Final ﬁne-tuning is performed by adding a linear classiﬁer to the top layer of
the DBN and performing a supervised optimization.
2.6.3. Variational Auto-Encoders and Generative Adverserial Networks
unsupervised
architectures
were introduced: the variational auto-encoder (VAE)
 and the generative adversarial network (GAN) . There
are no peer-reviewed papers applying these methods to
medical images yet, but applications in natural images
are promising. We will elaborate on their potential in
the discussion.
2.7. Hardware and Software
One of the main contributors to steep rise of deep
learning has been the widespread availability of GPU
and GPU-computing libraries (CUDA, OpenCL). GPUs
are highly parallel computing engines, which have an
order of magnitude more execution threads than central
processing units (CPUs). With current hardware, deep
learning on GPUs is typically 10 to 30 times faster than
Next to hardware, the other driving force behind the
popularity of deep learning methods is the wide availability of open source software packages.
These libraries provide eﬃcient GPU implementations of important operations in neural networks, such as convolutions; allowing the user to implement ideas at a high
level rather than worrying about low-level eﬃcient implementations. At the time of writing, the most popular
packages were (in alphabetical order):
• Caﬀe . Provides C++ and Python
interfaces, developed by graduate students at UC
• Tensorﬂow . Provides C++
and Python and interfaces, developed by Google
and is used by Google research.
• Theano . Provides a Python
interface, developed by MILA lab in Montreal.
• Torch . Provides a Lua interface and is used by, among others, Facebook AI
There are third-party packages written on top of one or
more of these frameworks, such as Lasagne (https://
github.com/Lasagne/Lasagne) or Keras (https:
//keras.io/). It goes beyond the scope of this paper
to discuss all these packages in detail.
3. Deep Learning Uses in Medical Imaging
3.1. Classiﬁcation
3.1.1. Image/exam classiﬁcation
Image or exam classiﬁcation was one of the ﬁrst areas in which deep learning made a major contribution
to medical image analysis. In exam classiﬁcation one
typically has one or multiple images (an exam) as input with a single diagnostic variable as output (e.g.,
disease present or not). In such a setting, every diagnostic exam is a sample and dataset sizes are typically
Concatenate
Up-convoluƟon
Down-sample
Input node
Weighted connecƟon
Weighted connecƟon
(similar colors indicate shared weights)
Hidden node
Output node
ProbabilisƟc node
Pooling connecƟon
Figure 2: Node graphs of 1D representations of architectures commonly used in medical imaging. a) Auto-encoder, b) restricted Boltzmann
machine, c) recurrent neural network, d) convolutional neural network, e) multi-stream convolutional neural network, f) U-net (with a single
downsampling stage).
small compared to those in computer vision (e.g., hundreds/thousands vs. millions of samples). The popularity of transfer learning for such applications is therefore
not surprising.
Transfer learning is essentially the use of pre-trained
networks (typically on natural images) to try to work
around the (perceived) requirement of large data sets
for deep network training. Two transfer learning strategies were identiﬁed: (1) using a pre-trained network as
a feature extractor and (2) ﬁne-tuning a pre-trained network on medical data. The former strategy has the extra
beneﬁt of not requiring one to train a deep network at
all, allowing the extracted features to be easily plugged
in to existing image analysis pipelines. Both strategies
are popular and have been widely applied. However,
few authors perform a thorough investigation in which
strategy gives the best result. The two papers that do,
Antony et al. and Kim et al. , oﬀer con-
ﬂicting results. In the case of Antony et al. , ﬁnetuning clearly outperformed feature extraction, achieving 57.6% accuracy in multi-class grade assessment of
knee osteoarthritis versus 53.4%. Kim et al. ,
however, showed that using CNN as a feature extractor
outperformed ﬁne-tuning in cytopathology image classiﬁcation accuracy (70.5% versus 69.1%). If any guidance can be given to which strategy might be most successful, we would refer the reader to two recent papers,
 
pre-trained version of Google’s Inception v3 architecture on medical data and achieved (near) human expert
performance .
As far as the authors are aware, such results have not yet
been achieved by simply using pre-trained networks as
feature extractors.
With respect to the type of deep networks that are
commonly used in exam classiﬁcation, a timeline similar to computer vision is apparent.
The medical
imaging community initially focused on unsupervised
pre-training and network architectures like SAEs and
RBMs. The ﬁrst papers applying these techniques for
exam classiﬁcation appeared in 2013 and focused on
neuroimaging.
Brosch and Tam , Plis et al.
 , Suk and Shen , and Suk et al. 
applied DBNs and SAEs to classify patients as having Alzheimer’s disease based on brain Magnetic Resonance Imaging (MRI). Recently, a clear shift towards
CNNs can be observed.
Out of the 47 papers published on exam classiﬁcation in 2015, 2016, and 2017,
36 are using CNNs, 5 are based on AEs and 6 on RBMs.
The application areas of these methods are very diverse,
ranging from brain MRI to retinal imaging and digital
pathology to lung computed tomography (CT).
In the more recent papers using CNNs authors also
often train their own network architectures from scratch
instead of using pre-trained networks. Menegola et al.
 performed some experiments comparing training
from scratch to ﬁne-tuning of pre-trained networks and
showed that ﬁne-tuning worked better given a small data
set of around a 1000 images of skin lesions. However,
these experiments are too small scale to be able to draw
any general conclusions from.
Three papers used an architecture leveraging the
unique attributes of medical data: two use 3D convolutions instead of 2D to classify patients as having
Alzheimer; Kawahara et al. applied a CNNlike architecture to a brain connectivity graph derived
from MRI diﬀusion-tensor imaging (DTI). In order to
do this, they developed several new layers which formed
the basis of their network, so-called edge-to-edge, edgeto-node, and node-to-graph layers. They used their network to predict brain development and showed that they
outperformed existing methods in assessing cognitive
and motor scores.
Summarizing, in exam classiﬁcation CNNs are the
current standard techniques.
Especially CNNs pretrained on natural images have shown surprisingly
strong results, challenging the accuracy of human experts in some tasks.
Last, authors have shown that
CNNs can be adapted to leverage intrinsic structure of
medical images.
3.1.2. Object or lesion classiﬁcation
Object classiﬁcation usually focuses on the classiﬁcation of a small (previously identiﬁed) part of the medical image into two or more classes (e.g. nodule classiﬁcation in chest CT). For many of these tasks both
local information on lesion appearance and global contextual information on lesion location are required for
accurate classiﬁcation.
This combination is typically
not possible in generic deep learning architectures. Several authors have used multi-stream architectures to resolve this in a multi-scale fashion (Section 2.4.2). Shen
et al. used three CNNs, each of which takes a
nodule patch at a diﬀerent scale as input. The resulting feature outputs of the three CNNs are then concatenated to form the ﬁnal feature vector. A somewhat similar approach was followed by Kawahara and Hamarneh
 who used a multi-stream CNN to classify skin
lesions, where each stream works on a diﬀerent resolution of the image. Gao et al. proposed to use
a combination of CNNs and RNNs for grading nuclear
cataracts in slit-lamp images, where CNN ﬁlters were
pre-trained. This combination allows the processing of
all contextual information regardless of image size. Incorporating 3D information is also often a necessity for
good performance in object classiﬁcation tasks in medical imaging. As images in computer vision tend to be
2D natural images, networks developed in those scenarios do not directly leverage 3D information. Authors
have used diﬀerent approaches to integrate 3D in an effective manner with custom architectures. Setio et al.
 used a multi-stream CNN to classify points of
interest in chest CT as a nodule or non-nodule. Up to
nine diﬀerently oriented patches extracted from the candidate were used in separate streams and merged in the
fully-connected layers to obtain the ﬁnal classiﬁcation
output. In contrast, Nie et al. exploited the 3D
nature of MRI by training a 3D CNN to assess survival
in patients suﬀering from high-grade gliomas.
Almost all recent papers prefer the use of end-to-end
trained CNNs. In some cases other architectures and
approaches are used, such as RBMs , SAEs and convolutional sparse auto-encoders
(CSAE) . The major diﬀerence
between CSAE and a classic CNN is the usage of unsupervised pre-training with sparse auto-encoders.
An interesting approach, especially in cases where
object annotation to generate training data is expensive,
is the integration of multiple instance learning (MIL)
and deep learning. Xu et al. investigated the use
of a MIL-framework with both supervised and unsu-
pervised feature learning approaches as well as handcrafted features. The results demonstrated that the performance of the MIL-framework was superior to handcrafted features, which in turn closely approaches the
performance of a fully supervised method. We expect
such approaches to be popular in the future as well, as
obtaining high-quality annotated medical data is challenging.
Overall, object classiﬁcation sees less use of pretrained networks compared to exam classiﬁcations,
mostly due to the need for incorporation of contextual
or three-dimensional information. Several authors have
found innovative solutions to add this information to
deep networks with good results, and as such we expect deep learning to become even more prominent for
this task in the near future.
3.2. Detection
3.2.1. Organ, region and landmark localization
Anatomical object localization (in space or time),
such as organs or landmarks, has been an important preprocessing step in segmentation tasks or in the clinical
workﬂow for therapy planning and intervention. Localization in medical imaging often requires parsing of
3D volumes. To solve 3D data parsing with deep learning algorithms, several approaches have been proposed
that treat the 3D space as a composition of 2D orthogonal planes. Yang et al. identiﬁed landmarks on
the distal femur surface by processing three independent sets of 2D MRI slices (one for each plane) with
regular CNNs. The 3D position of the landmark was
deﬁned as the intersection of the three 2D slices with
the highest classiﬁcation output. de Vos et al. 
went one step further and localized regions of interest
(ROIs) around anatomical regions (heart, aortic arch,
and descending aorta) by identifying a rectangular 3D
bounding box after 2D parsing the 3D CT volume. Pretrained CNN architectures, as well as RBM, have been
used for the same purpose , overcoming the lack of data
to learn better feature representations. All these studies cast the localization task as a classiﬁcation task and
as such generic deep learning architectures and learning
processes can be leveraged.
Other authors try to modify the network learning process to directly predict locations. For example, Payer
et al. proposed to directly regress landmark locations with CNNs. They used landmark maps, where
each landmark is represented by a Gaussian, as ground
truth input data and the network is directly trained to
predict this landmark map.
Another interesting approach was published by Ghesu et al. , in which
reinforcement learning is applied to the identiﬁcation
of landmarks. The authors showed promising results in
several tasks: 2D cardiac MRI and ultrasound (US) and
3D head/neck CT.
Due to its increased complexity, only a few methods
addressed the direct localization of landmarks and regions in the 3D image space. Zheng et al. reduced this complexity by decomposing 3D convolution
as three one-dimensional convolutions for carotid artery
bifurcation detection in CT data. Ghesu et al. 
proposed a sparse adaptive deep neural network powered by marginal space learning in order to deal with
data complexity in the detection of the aortic valve in
3D transesophageal echocardiogram.
CNNs have also been used for the localization of scan
planes or key frames in temporal data. Baumgartner
et al. trained CNNs on video frame data to detect up to 12 standardized scan planes in mid-pregnancy
fetal US. Furthermore, they used saliency maps to obtain a rough localization of the object of interest in the
scan plan (e.g. brain, spine). RNNs, particularly LSTM-
RNNs, have also been used to exploit the temporal information contained in medical videos, another type of
high dimensional data. Chen et al. , for example,
employed LSTM models to incorporate temporal information of consecutive sequence in US videos for fetal
standard plane detection. Kong et al. combined
an LSTM-RNN with a CNN to detect the end-diastole
and end-systole frames in cine-MRI of the heart.
Concluding, localization through 2D image classiﬁcation with CNNs seems to be the most popular strategy overall to identify organs, regions and landmarks,
with good results. However, several recent papers expand on this concept by modifying the learning process such that accurate localization is directly emphasized, with promising results. We expect such strategies to be explored further as they show that deep learning techniques can be adapted to a wide range of localization tasks (e.g. multiple landmarks). RNNs have
shown promise in localization in the temporal domain,
and multi-dimensional RNNs could play a role in spatial
localization as well.
3.2.2. Object or lesion detection
The detection of objects of interest or lesions in images is a key part of diagnosis and is one of the most
labor-intensive for clinicians. Typically, the tasks consist of the localization and identiﬁcation of small lesions
in the full image space. There has been a long research
tradition in computer-aided detection systems that are
designed to automatically detect lesions, improving the
detection accuracy or decreasing the reading time of human experts. Interestingly, the ﬁrst object detection system using CNNs was already proposed in 1995, using a
CNN with four layers to detect nodules in x-ray images
 .
Most of the published deep learning object detection
systems still uses CNNs to perform pixel (or voxel) classiﬁcation, after which some form of post processing is
applied to obtain object candidates. As the classiﬁcation task performed at each pixel is essentially object
classiﬁcation, CNN architecture and methodology are
very similar to those in section 3.1.2. The incorporation of contextual or 3D information is also handled using multi-stream CNNs and Roth et al. . Teramoto
et al. used a multi-stream CNN to integrate CT
and Positron Emission Tomography (PET) data. Dou
et al. used a 3D CNN to ﬁnd micro-bleeds in
brain MRI. Last, as the annotation burden to generate training data can be similarly signiﬁcant compared
to object classiﬁcation, weakly-supervised deep learning has been explored by Hwang and Kim , who
adopted such a strategy for the detection of nodules in
chest radiographs and lesions in mammography.
There are some aspects which are signiﬁcantly diﬀerent between object detection and object classiﬁcation.
One key point is that because every pixel is classiﬁed,
typically the class balance is skewed severely towards
the non-object class in a training setting. To add insult
to injury, usually the majority of the non-object samples are easy to discriminate, preventing the deep learning method to focus on the challenging samples. van
Grinsven et al. proposed a selective data sampling in which wrongly classiﬁed samples were fed back
to the network more often to focus on challenging areas
in retinal images. Last, as classifying each pixel in a
sliding window fashion results in orders of magnitude
of redundant calculation, fCNNs, as used in Wolterink
et al. , are important aspect of an object detection
pipeline as well.
Challenges in meaningful application of deep learning algorithms in object detection are thus mostly similar to those in object classiﬁcation.
Only few papers directly address issues speciﬁc to object detection
like class imbalance/hard-negative mining or eﬃcient
pixel/voxel-wise processing of images. We expect that
more emphasis will be given to those areas in the near
future, for example in the application of multi-stream
networks in a fully convolutional fashion.
3.3. Segmentation
3.3.1. Organ and substructure segmentation
The segmentation of organs and other substructures
in medical images allows quantitative analysis of clinical parameters related to volume and shape, as, for example, in cardiac or brain analysis. Furthermore, it is
often an important ﬁrst step in computer-aided detection
pipelines. The task of segmentation is typically deﬁned
as identifying the set of voxels which make up either
the contour or the interior of the object(s) of interest.
Segmentation is the most common subject of papers applying deep learning to medical imaging (Figure 1), and
as such has also seen the widest variety in methodology, including the development of unique CNN-based
segmentation architectures and the wider application of
The most well-known, in medical image analysis, of
these novel CNN architectures is U-net, published by
Ronneberger et al. (section 2.4.3). The two main
architectural novelties in U-net are the combination of
an equal amount of upsampling and downsampling layers. Although learned upsampling layers have been proposed before, U-net combines them with so-called skip
connections between opposing convolution and deconvolution layers. This which concatenate features from
the contracting and expanding paths. From a training
perspective this means that entire images/scans can be
processed by U-net in one forward pass, resulting in a
segmentation map directly. This allows U-net to take
into account the full context of the image, which can be
an advantage in contrast to patch-based CNNs. Furthermore, in an extended paper by C¸ ic¸ek et al. , it is
shown that a full 3D segmentation can be achieved by
feeding U-net with a few 2D annotated slices from the
same volume. Other authors have also built derivatives
of the U-net architecture; Milletari et al. , for
example, proposed a 3D-variant of U-net architecture,
called V-net, performing 3D image segmentation using
3D convolutional layers with an objective function directly based on the Dice coeﬃcient.
Drozdzal et al.
 investigated the use of short ResNet-like skip
connections in addition to the long skip-connections in
a regular U-net.
RNNs have recently become more popular for segmentation tasks. For example, Xie et al. used
a spatial clockwork RNN to segment the perimysium
in H&E-histopathology images.
This network takes
into account prior information from both the row and
column predecessors of the current patch.
To incorporate bidirectional information from both left/top and
right/bottom neighbors, the RNN is applied four times
in diﬀerent orientations and the end-result is concatenated and fed to a fully-connected layer. This produces
the ﬁnal output for a single patch. Stollenga et al. 
where the ﬁrst to use a 3D LSTM-RNN with convolutional layers in six directions. Andermatt et al. 
used a 3D RNN with gated recurrent units to segment
gray and white matter in a brain MRI data set. Chen
et al. combined bi-directional LSTM-RNNs
with 2D U-net-like-architectures to segment structures
in anisotropic 3D electron microscopy images. Last,
Poudel et al. combined a 2D U-net architecture
with a gated recurrent unit to perform 3D segmentation.
Although these speciﬁc segmentation architectures
oﬀered compelling advantages, many authors have also
obtained excellent segmentation results with patchtrained neural networks. One of the earliest papers covering medical image segmentation with deep learning
algorithms used such a strategy and was published by
Ciresan et al. . They applied pixel-wise segmentation of membranes in electron microscopy imagery in
a sliding window fashion. Most recent papers now use
fCNNs (subsection 2.4.3) in preference over slidingwindow-based classiﬁcation to reduce redundant computation.
fCNNs have also been extended to 3D and have
been applied to multiple targets at once: Korez et al.
 , used 3D fCNNs to generate vertebral body likelihood maps which drove deformable models for vertebral body segmentation in MR images, Zhou et al.
 segmented nineteen targets in the human torso,
and Moeskops et al. trained a single fCNN to
segment brain MRI, the pectoral muscle in breast MRI,
and the coronary arteries in cardiac CT angiography
One challenge with voxel classiﬁcation approaches
is that they sometimes lead to spurious responses. To
combat this, groups have tried to combine fCNNs with
graphical models like MRFs and Conditional Random Fields (CRFs)
 to reﬁne the segmentation output. In most of the
cases, graphical models are applied on top of the likelihood map produced by CNNs or fCNNs and act as label
regularizers.
Summarizing, segmentation in medical imaging has
seen a huge inﬂux of deep learning related methods.
Custom architectures have been created to directly target the segmentation task. These have obtained promising results, rivaling and often improving over results obtained with fCNNs.
3.3.2. Lesion segmentation
Segmentation of lesions combines the challenges of
object detection and organ and substructure segmentation in the application of deep learning algorithms.
Global and local context are typically needed to perform accurate segmentation, such that multi-stream networks with diﬀerent scales or non-uniformly sampled
patches are used as in for example Kamnitsas et al.
 and Ghafoorian et al. . In lesion segmentation we have also seen the application of U-net
and similar architectures to leverage both this global
and local context. The architecture used by Wang et al.
 , similar to the U-net, consists of the same downsampling and upsampling paths, but does not use skip
connections. Another U-net-like architecture was used
by Brosch et al. to segment white matter lesions
in brain MRI. However, they used 3D convolutions and
a single skip connection between the ﬁrst convolutional
and last deconvolutional layers.
One other challenge that lesion segmentation shares
with object detection is class imbalance, as most voxels/pixels in an image are from the non-diseased class.
Some papers combat this by adapting the loss function:
Brosch et al. deﬁned it to be a weighted combination of the sensitivity and the speciﬁcity, with a larger
weight for the speciﬁcity to make it less sensitive to the
data imbalance. Others balance the data set by performing data augmentation on positive samples .
Thus lesion segmentation sees a mixture of approaches used in object detection and organ segmentation. Developments in these two areas will most likely
naturally propagate to lesion segmentation as the existing challenges are also mostly similar.
3.4. Registration
Registration (i.e. spatial alignment) of medical images is a common image analysis task in which a coordinate transform is calculated from one medical image to
another. Often this is performed in an iterative framework where a speciﬁc type of (non-)parametric transformation is assumed and a pre-determined metric (e.g.
L2-norm) is optimized. Although segmentation and lesion detection are more popular topics for deep learning, researchers have found that deep networks can be
beneﬁcial in getting the best possible registration performance. Broadly speaking, two strategies are prevalent in current literature: (1) using deep-learning networks to estimate a similarity measure for two images
to drive an iterative optimization strategy, and (2) to directly predict transformation parameters using deep regression networks.
Wu et al. , Simonovsky et al. , and
Cheng et al. used the ﬁrst strategy to try to optimize registration algorithms. Cheng et al. used
two types of stacked auto-encoders to assess the local
similarity between CT and MRI images of the head.
Both auto-encoders take vectorized image patches of
CT and MRI and reconstruct them through four layers.
After the networks are pre-trained using unsupervised patch reconstruction they are ﬁne-tuned using
two prediction layers stacked on top of the third layer
of the SAE. These prediction layers determine whether
two patches are similar (class 1) or dissimilar (class 2).
Simonovsky et al. used a similar strategy, albeit with CNNs, to estimate a similarity cost between
two patches from diﬀering modalities. However, they
also presented a way to use the derivative of this metric to directly optimize the transformation parameters,
which are decoupled from the network itself. Last, Wu
et al. combined independent subspace analysis
and convolutional layers to extract features from input
patches in an unsupervised manner. The resultant feature vectors are used to drive the HAMMER registration
algorithm instead of handcrafted features.
Miao et al. and Yang et al. used deep
learning algorithms to directly predict the registration
transform parameters given input images. Miao et al.
 leveraged CNNs to perform 3D model to 2D xray registration to assess the pose and location of an
implanted object during surgery. In total the transformation has 6 parameters, two translational, 1 scaling
and 3 angular parameters. They parameterize the feature space in steps of 20 degrees for two angular parameters and train a separate CNN to predict the update
to the transformation parameters given an digitally reconstructed x-ray of the 3D model and the actual interoperative x-ray. The CNNs are trained with artiﬁcial
examples generated by manually adapting the transformation parameters for the input training data.
showed that their approach has signiﬁcantly higher registration success rates than using traditional - purely intensity based - registration methods. Yang et al. 
tackled the problem of prior/current registration in brain
MRI using the OASIS data set. They used the large
deformation diﬀeomorphic metric mapping (LDDMM)
registration methodology as a basis. This method takes
as input an initial momentum value for each pixel which
is then evolved over time to obtain the ﬁnal transformation. However, the calculation of the initial momentum map is often an expensive procure. The authors
circumvent this by training a U-net like architecture to
predict the x- and y-momentum map given the input images. They obtain visually similar results but with signiﬁcantly improved execution time: 1500x speed-up for
2D and 66x speed-up for 3D.
In contrast to classiﬁcation and segmentation, the research community seems not have yet settled on the best
way to integrate deep learning techniques in registration
methods. Not many papers have yet appeared on the
subject and existing ones each have a distinctly diﬀerent approach. Thus, giving recommendations on what
method is most promising seems inappropriate. However, we expect to see many more contributions of deep
learning to medical image registration in the near future.
3.5. Other tasks in medical imaging
3.5.1. Content-based image retrieval
Content-based image retrieval (CBIR) is a technique
for knowledge discovery in massive databases and offers the possibility to identify similar case histories, understand rare disorders, and, ultimately, improve patient
care. The major challenge in the development of CBIR
methods is extracting eﬀective feature representations
from the pixel-level information and associating them
with meaningful concepts. The ability of deep CNN
models to learn rich features at multiple levels of abstraction has elicited interest from the CBIR community.
All current approaches use (pre-trained) CNNs to extract feature descriptors from medical images. Anavi
et al. and Liu et al. applied their methods to databases of X-ray images. Both used a ﬁve-layer
CNN and extracted features from the fully-connected
layers. Anavi et al. used the last layer and a
pre-trained network. Their best results were obtained
by feeding these features to a one-vs-all support vector machine (SVM) classiﬁer to obtain the distance metric. They showed that incorporating gender information
resulted in better performance than just CNN features.
Liu et al. used the penultimate fully-connected
layer and a custom CNN trained to classify X-rays in
193 classes to obtain the descriptive feature vector. After descriptor binarization and data retrieval using Hamming separation values, the performance was inferior
to the state of the art, which the authors attributed to
small patch sizes of 96 pixels. The method proposed
by Shah et al. combines CNN feature descriptors with hashing-forests. 1000 features were extracted
for overlapping patches in prostate MRI volumes, after
which a large feature matrix was constructed over all
volumes. Hashing forests were then used to compress
this into descriptors for each volume.
Content-based image retrieval as a whole has thus
not seen many successful applications of deep learning
Figure 3: Collage of some medical imaging applications in which
deep learning has achieved state-of-the-art results. From top-left to
bottom-right: mammographic mass classiﬁcation ,
segmentation of lesions in the brain , leak
detection in airway tree segmentation , diabetic retinopathy classiﬁcation , prostate segmentation (top rank in PROMISE12 challenge), nodule classiﬁcation (top
ranking in LUNA16 challenge), breast cancer metastases detection in
lymph nodes (top ranking and human expert performance in CAME-
LYON16), human expert performance in skin lesion classiﬁcation , and state-of-the-art bone suppression in x-rays, image from Yang et al. .
methods yet, but given the results in other areas it seems
only a matter of time. An interesting avenue of research
could be the direct training of deep networks for the retrieval task itself.
3.5.2. Image Generation and Enhancement
A variety of image generation and enhancement
methods using deep architectures have been proposed,
ranging from removing obstructing elements in images, normalizing images, improving image quality,
data completion, and pattern discovery.
In image generation, 2D or 3D CNNs are used to
convert one input image into another. Typically these
architectures lack the pooling layers present in classiﬁcation networks. These systems are then trained with a
data set in which both the input and the desired output
are present, deﬁning the diﬀerences between the generated and desired output as the loss function. Examples
are regular and bone-suppressed X-ray in Yang et al.
 , 3T and 7T brain MRI in Bahrami et al. ,
PET from MRI in Li et al. , and CT from MRI in
Nie et al. . Li et al. even showed that one
can use these generated images in computer-aided diagnosis systems for Alzheimer’s disease when the original
data is missing or not acquired.
With multi-stream CNNs super-resolution images
can be generated from multiple low-resolution inputs
(section 2.4.2). In Oktay et al. , multi-stream networks reconstructed high-resolution cardiac MRI from
one or more low-resolution input MRI volumes. Not
only can this strategy be used to infer missing spatial information, but can also be leveraged in other domains;
for example, inferring advanced MRI diﬀusion parameters from limited data . Other image enhancement applications like intensity normalization and denoising have seen only limited application of
deep learning algorithms. Janowczyk et al. used
SAEs to normalize H&E-stained histopathology images
whereas Benou et al. used CNNs to perform denoising in DCE-MRI time-series.
Image generation has seen impressive results with
very creative applications of deep networks in signiﬁcantly diﬀering tasks. One can only expect the number
of tasks to increase further in the future.
3.5.3. Combining Image Data With Reports
The combination of text reports and medical image
data has led to two avenues of research: (1) leveraging reports to improve image classiﬁcation accuracy
 , and (2) generating text reports
from images ; the latter inspired by recent
caption generation papers from natural images . To the best of our knowledge,
the ﬁrst step towards leveraging reports was taken by
Schlegl et al. , who argued that large amounts of
annotated data may be diﬃcult to acquire and proposed
to add semantic descriptions from reports as labels. The
system was trained on sets of images along with their
textual descriptions and was taught to predict semantic
class labels during test time. They showed that semantic
information increases classiﬁcation accuracy for a variety of pathologies in Optical Coherence Tomography
(OCT) images.
Shin et al. and Wang et al. mined semantic interactions between radiology reports and images from a large data set extracted from a PACS system. They employed latent Dirichlet allocation (LDA),
a type of stochastic model that generates a distribution
over a vocabulary of topics based on words in a document. In a later work, Shin et al. proposed a sys-
Table 1: Overview of papers using deep learning techniques for brain image analysis. All works use MRI unless otherwise mentioned.
Application; remarks
Disorder classiﬁcation (AD, MCI, Schizophrenia)
Brosch and Tam 
AD/HC classiﬁcation; Deep belief networks with convolutional RBMs for manifold learning
Plis et al. 
Deep belief networks evaluated on brain network estimation, Schizophrenia and Huntington’s disease classiﬁcation
Suk and Shen 
AD/MCI classiﬁcation; Stacked auto encoders with supervised ﬁne tuning
Suk et al. 
AD/MCI/HC classiﬁcation; Deep Boltzmann Machines on MRI and PET modalities
Payan and Montana 
AD/MCI/HC classiﬁcation; 3D CNN pre-trained with sparse auto-encoders
Suk et al. 
AD/MCI/HC classiﬁcation; SAE for latent feature extraction on a large set of hand-crafted features from MRI and PET
Hosseini-Asl et al. 
AD/MCI/HC classiﬁcation; 3D CNN pre-trained with a 3D convolutional auto-encoder on fMRI data
Kim et al. 
Schizophrenia/NH classiﬁcation on fMRI; Neural network showing advantage of pre-training with SAEs, and L1 sparsiﬁcation
Ortiz et al. 
AD/MCI/HC classiﬁcation; An ensemble of Deep belief networks, with their votes fused using an SVM classiﬁer
Pinaya et al. 
Schizophrenia/NH classiﬁcation; DBN pre-training followed by supervised ﬁne-tuning
Sarraf and Toﬁghi 
AD/HC classiﬁcation; Adapted Lenet-5 architecture on fMRI data
Suk et al. 
MCI/HC classiﬁcation of fMRI data; Stacked auto-encoders for feature extraction, HMM as a generative model on top
Suk and Shen 
AD/MCI/HC classiﬁcation; CNN on sparse representations created by regression models
Shi et al. 
AD/MCI/HC classiﬁcation; Multi-modal stacked deep polynomial networks with an SVM classiﬁer on top using MRI and PET
Tissue/anatomy/lesion/tumor segmentation
Guo et al. 
Hippocampus segmentation; SAE for representation learning used for target/atlas patch similarity measurement
de Brebisson and Montana 
Anatomical segmentation; fusing multi-scale 2D patches with a 3D patch using a CNN
Choi and Jin 
Striatum segmentation; Two-stage (global/local) approximations with 3D CNNs
Stollenga et al. 
Tissue segmentation; PyraMiD-LSTM, best brain segmentation results on MRBrainS13 (and competitive results on EM-ISBI12)
Zhang et al. 
Tissue segmentation; multi-modal 2D CNN
Andermatt et al. 
Tissue segmentation; two convolutional gated recurrent units in diﬀerent directions for each dimension
Bao and Chung 
Anatomical segmentation; Multi-scale late fusion CNN with random walker as a novel label consistency method
Birenbaum and Greenspan 
Lesion segmentation; Multi-view (2.5D) CNN concatenating features from previous time step for a longitudinal analysis
Brosch et al. 
Lesion segmentation; Convolutional encoder-decoder network with shortcut connections and convolutional RBM pretraining
Chen et al. 
Tissue segmentation; 3D res-net combining features from diﬀerent layers
Ghafoorian et al. 
Lesion segmentation; CNN trained on non-uniformly sampled patch to integrate a larger context with a foviation eﬀect
Ghafoorian et al. 
Lesion segmentation; multi-scale CNN with late fusion that integrates anatomical location information into network
Havaei et al. 
Tumor segmentation; CNN handling missing modalities with abstraction layer that transforms feature maps to their statistics
Havaei et al. 
Tumor segmentation; two-path way CNN with diﬀerent receptive ﬁelds
Kamnitsas et al. 
Tumor segmentation; 3D multi-scale fully convolutional network with CRF for label consistency
Kleesiek et al. 
Brain extraction; 3D fully convolutional CNN on multi-modal input
Mansoor et al. 
Visual pathway segmentation; Learning appearance features from SAE for steering the shape model for segmentation
Milletari et al. 
Anatomical segmentation on MRI and US; Hough-voting to acquire mapping from CNN features to full patch segmentations
Moeskops et al. 
Tissue segmentation; CNN trained on multiple patch sizes
Nie et al. 
Infant tissue segmentation; FCN with a late fusion method on diﬀerent modalities
Pereira et al. 
Tumor segmentation; CNN on multiple modality input
Shakeri et al. 
Anatomical segmentation; FCN followed by Markov random ﬁelds
Zhao and Jia 
Tumor segmentation; Multi-scale CNN with a late fusion architecture
Lesion/tumor detection and classiﬁcation
Pan et al. 
Tumor grading; 2D tumor patch classiﬁcation using a CNN
Dou et al. 
Microbleed detection; 3D stacked Independent Subspace Analysis for candidate feature extraction, SVM classiﬁcation
Dou et al. 
Microbleed detection; 3D FCN for candidate segmentation followed by a 3D CNN as false positive reduction
Ghafoorian et al. 
Lacune detection; FCN for candidate segmentation then a multi-scale 3D CNN with anatomical features as false positive reduction
Survival/disease activity/development prediction
Kawahara et al. 
Neurodevelopment prediction; CNN with specially-designed edge-to-edge, edge-to-node and node-to-graph conv. layers for brain nets
Nie et al. 
Survival prediction; features from a Multi-modal 3D CNN is fused with hand-crafted features to train an SVM
Yoo et al. 
Disease activity prediction; Training a CNN on the Euclidean distance transform of the lesion masks as the input
van der Burgh et al. 
Survival prediction; DBN on MRI and fusing it with clinical characteristics and structural connectivity data
Image construction/enhancement
Li et al. 
Image construction; 3D CNN for constructing PET from MR images
Bahrami et al. 
Image construction; 3D CNN for constructing 7T-like images from 3T MRI
Benou et al. 
Denoising DCE-MRI; using an ensemble of denoising SAE (pretrained with RBMs)
Golkov et al. 
Image construction; Per-pixel neural network to predict complex diﬀusion parameters based on fewer measurements
Hoﬀmann et al. 
Image construction; Deep neural nets with SRelu nonlinearity for thermal image construction
Nie et al. 
Image construction; 3D fully convolutional network for constructing CT from MR images
Sevetlidis et al. 
Image construction; Encoder-decoder network for synthesizing one MR modality from another
Brosch et al. 
Manifold Learning; DBN with conv. RBM layers for modeling the variability in brain morphology and lesion distribution in MS
Cheng et al. 
Similarity measurement; neural network fusing the moving and reference image patches, pretrained with SAE
Huang et al. 
fMRI blind source separation; RBM for both internal and functional interaction-induced latent sources detection
Simonovsky et al. 
Similarity measurement; 3D CNN estimating similarity between reference and moving images stacked in the input
Wu et al. 
Correspondence detection in deformable registration; stacked convolutional ISA for unsupervised feature learning
Yang et al. 
Image registration; Conv. encoder-decoder net. predicting momentum in x and y directions, given the moving and ﬁxed image patches
tem to generate descriptions from chest X-rays. A CNN
was employed to generate a representation of an image
one label at a time, which was then used to train an
RNN to generate sequence of MeSH keywords. Kisilev
et al. used a completely diﬀerent approach and
predicted categorical BI-RADS descriptors for breast
lesions. In their work they focused on three descriptors used in mammography: shape, margin, and density,
Table 2: Overview of papers using deep learning techniques for retinal image analysis. All works use CNNs.
Color fundus images: segmentation of anatomical structures and quality assessment
Fu et al. 
Blood vessel segmentation; CNN combined with CRF to model long-range pixel interactions
Fu et al. 
Blood vessel segmentation; extending the approach by Fu et al. by reformulating CRF as RNN
Mahapatra et al. 
Image quality assessment; classiﬁcation output using CNN-based features combined with the output using saliency maps
Maninis et al. 
Segmentation of blood vessels and optic disk; VGG-19 network extended with specialized layers for each segmentation task
Wu et al. 
Blood vessel segmentation; patch-based CNN followed by mapping PCA solution of last layer feature maps to full segmentation
Zilly et al. 
Segmentation of the optic disk and the optic cup; simple CNN with ﬁlters sequentially learned using boosting
Color fundus images: detection of abnormalities and diseases
Chen et al. 
Glaucoma detection; end-to-end CNN, the input is a patch centered at the optic disk
Abr`amoﬀet al. 
Diabetic retinopathy detection; end-to-end CNN, outperforms traditional method, evaluated on a public dataset
Burlina et al. 
Age-related macular degeneration detection; uses overfeat pretrained network for feature extraction
van Grinsven et al. 
Hemorrhage detection; CNN dynamically trained using selective data sampling to perform hard negative mining
Gulshan et al. 
Diabetic retinopathy detection; Inception network, performance comparable to a panel of seven certiﬁed ophthalmologists
Prentasic and Loncaric 
Hard exudate detection; end-to-end CNN combined with the outputs of traditional classiﬁers for detection of landmarks
Worrall et al. 
Retinopathy of prematurity detection; ﬁne-tuned ImageNet trained GoogLeNet, feature map visualization to highlight disease
Work in other imaging modalities
Gao et al. 
Cataract classiﬁcation in slit lamp images; CNN followed by a set of recursive neural networks to extract higher order features
Schlegl et al. 
Fluid segmentation in OCT; weakly supervised CNN improved with semantic descriptors from clinical reports
Prentasic et al. 
Blood vessel segmentation in OCT angiography; simple CNN, segmentation of several capillary networks
where each have their own class label. The system was
fed with the image data and region proposals and predicts the correct label for each descriptor (e.g. for shape
either oval, round, or irregular).
Given the wealth of data that is available in PACS
systems in terms of images and corresponding diagnostic reports, it seems like an ideal avenue for future
deep learning research. One could expect that advances
in captioning natural images will in time be applied to
these data sets as well.
4. Anatomical application areas
This section presents an overview of deep learning
contributions to the various application areas in medical imaging. We highlight some key contributions and
discuss performance of systems on large data sets and
on public challenge data sets. All these challenges are
listed on http:\\www.grand-challenge.org.
4.1. Brain
DNNs have been extensively used for brain image
analysis in several diﬀerent application domains (Table 1). A large number of studies address classiﬁcation
of Alzheimer’s disease and segmentation of brain tissue and anatomical structures (e.g. the hippocampus).
Other important areas are detection and segmentation
of lesions (e.g. tumors, white matter lesions, lacunes,
micro-bleeds).
Apart from the methods that aim for a scan-level
classiﬁcation (e.g. Alzheimer diagnosis), most methods learn mappings from local patches to representations and subsequently from representations to labels.
However, the local patches might lack the contextual
information required for tasks where anatomical information is paramount (e.g. white matter lesion segmentation). To tackle this, Ghafoorian et al. used
non-uniformly sampled patches by gradually lowering
sampling rate in patch sides to span a larger context.
An alternative strategy used by many groups is multiscale analysis and a fusion of representations in a fullyconnected layer.
Even though brain images are 3D volumes in all surveyed studies, most methods work in 2D, analyzing the
3D volumes slice-by-slice. This is often motivated by
either the reduced computational requirements or the
thick slices relative to in-plane resolution in some data
sets. More recent publications had also employed 3D
DNNs have completely taken over many brain image
analysis challenges. In the 2014 and 2015 brain tumor
segmentation challenges (BRATS), the 2015 longitudinal multiple sclerosis lesion segmentation challenge,
the 2015 ischemic stroke lesion segmentation challenge
(ISLES), and the 2013 MR brain image segmentation
challenge (MRBrains), the top ranking teams to date
have all used CNNs. Almost all of the aforementioned
methods are concentrating on brain MR images. We expect that other brain imaging modalities such as CT and
US can also beneﬁt from deep learning based analysis.
Ophthalmic imaging has developed rapidly over the
past years, but only recently are deep learning algorithms being applied to eye image understanding. As
summarized in Table 2, most works employ simple
Table 3: Overview of papers using deep learning techniques for chest x-ray image analysis.
Application
Lo et al. 
Nodule detection
Classiﬁes candidates from small patches with two-layer CNN, each with 12 5 × 5 ﬁlters
Anavi et al. 
Image retrieval
Combines classical features with those from pre-trained CNN for image retrieval using SVM
Bar et al. 
Pathology detection
Features from a pre-trained CNN and low level features are used to detect various diseases
Anavi et al. 
Image retrieval
Continuation of Anavi et al. , adding age and gender as features
Bar et al. 
Pathology detection
Continuation of Bar et al. , more experiments and adding feature selection
Cicero et al. 
Pathology detection
GoogLeNet CNN detects ﬁve common abnormalities, trained and validated on a large data set
Hwang et al. 
Tuberculosis detection
Processes entire radiographs with a pre-trained ﬁne-tuned network with 6 convolution layers
Kim and Hwang 
Tuberculosis detection
MIL framework produces heat map of suspicious regions via deconvolution
Shin et al. 
Pathology detection
CNN detects 17 diseases, large data set (7k images), recurrent networks produce short captions
Rajkomar et al. 
Frontal/lateral classiﬁcation
Pre-trained CNN performs frontal/lateral classiﬁcation task
Yang et al. 
Bone suppression
Cascade of CNNs at increasing resolution learns bone images from gradients of radiographs
Wang et al. 
Nodule classiﬁcation
Combines classical features with CNN features from pre-trained ImageNet CNN
Table 4: Overview of papers using deep learning techniques for chest CT image analysis.
Application; remarks
Segmentation
Charbonnier et al. 
Airway segmentation where multi-view CNN classiﬁes candidate branches as true airways or leaks
Nodule detection and analysis
Ciompi et al. 
Used a standard feature extractor and a pre-trained CNN to classify detected lesions as benign peri-ﬁssural nodules
van Ginneken et al. 
Detects nodules with pre-trained CNN features from orthogonal patches around candidate, classiﬁed with SVM
Shen et al. 
Three CNNs at diﬀerent scales estimate nodule malignancy scores of radiologists (LIDC-IDRI data set)
Chen et al. 
Combines features from CNN, SDAE and classical features to characterize nodules from LIDC-IDRI data set
Ciompi et al. 
Multi-stream CNN to classify nodules into subtypes: solid, part-solid, non-solid, calciﬁed, spiculated, periﬁssural
Dou et al. 
Uses 3D CNN around nodule candidates; ranks #1 in LUNA16 nodule detection challenge
Li et al. 
Detects nodules with 2D CNN that processes small patches around a nodule
Setio et al. 
Detects nodules with end-to-end trained multi-stream CNN with 9 patches per candidate
Shen et al. 
3D CNN classiﬁes volume centered on nodule as benign/malignant, results are combined to patient level prediction
Sun et al. 
Same dataset as Shen et al. , compares CNN, DBN, SDAE and classical computer-aided diagnosis schemes
Teramoto et al. 
Combines features extracted from 2 orthogonal CT patches and a PET patch
Interstitial lung disease
Anthimopoulos et al. 
Classiﬁcation of 2D patches into interstitial lung texture classes using a standard CNN
Christodoulidis et al. 
2D interstitial pattern classiﬁcation with CNNs pre-trained with a variety of texture data sets
Gao et al. 
Propagates manually drawn segmentations using CNN and CRF for more accurate interstitial lung disease reference
Gao et al. 
AlexNet applied to large parts of 2D CT slices to detect presence of interstitial patterns
Gao et al. 
Uses regression to predict area covered in 2D slice with a particular interstitial pattern
Tarando et al. 
Combines existing computer-aided diagnosis system and CNN to classify lung texture patterns.
van Tulder and de Bruijne 
Classiﬁcation of lung texture and airways using an optimal set of ﬁlters derived from DBNs and RBMs
Other applications
Tajbakhsh et al. 
Multi-stream CNN to detect pulmonary embolism from candidates obtained from a tobogganing algorithm
Carneiro et al. 
Predicts 5-year mortality from thick slice CT scans and segmentation masks
de Vos et al. 
Identiﬁes the slice of interest and determine the distance between CT slices
CNNs for the analysis of color fundus imaging (CFI).
A wide variety of applications are addressed: segmentation of anatomical structures, segmentation and detection of retinal abnormalities, diagnosis of eye diseases,
and image quality assessment.
In 2015, Kaggle organized a diabetic retinopathy detection competition: Over 35,000 color fundus images
were provided to train algorithms to predict the severity of disease in 53,000 test images. The majority of
the 661 teams that entered the competition applied deep
learning and four teams achieved performance above
that of humans, all using end-to-end CNNs. Recently
Gulshan et al. performed a thorough analysis
of the performance of a Google Inception v3 network
for diabetic retinopathy detection, showing performance
comparable to a panel of seven certiﬁed ophthalmologists.
4.3. Chest
In thoracic image analysis of both radiography and
computed tomography, the detection, characterization,
and classiﬁcation of nodules is the most commonly addressed application. Many works add features derived
from deep networks to existing feature sets or compare
Table 5: Overview of papers using deep learning for digital pathology images. The staining and imaging modality abbreviations used in the table are
as follows: H&E: hematoxylin and eosin staining, TIL: Tumor-inﬁltrating lymphocytes, BCC: Basal cell carcinoma, IHC: immunohistochemistry,
RM: Romanowsky, EM: Electron microscopy, PC: Phase contrast, FL: Fluorescent, IFL: Immunoﬂuorescent, TPM: Two-photon microscopy, CM:
Confocal microscopy, Pap: Papanicolaou.
Staining\Modality
Nucleus detection, segmentation, and classiﬁcation
Cires¸an et al. 
Mitosis detection
CNN-based pixel classiﬁer
Cruz-Roa et al. 
Detection of basal cell carcinoma
Convolutional auto-encoder neural network
Malon and Cosatto 
Mitosis detection
Combines shapebased features with CNN
Wang et al. 
Mitosis detection
Cascaded ensemble of CNN and handcrafted features
Ferrari et al. 
Bacterial colony counting
Culture plate
CNN-based patch classiﬁer
Ronneberger et al. 
Cell segmentation
U-Net with deformation augmentation
Shkolyar et al. 
Mitosis detection
Live-imaging
CNN-based patch classiﬁer
Song et al. 
Segmentation of cytoplasm and nuclei
Multi-scale CNN and graph-partitioning-based method
Xie et al. 
Nucleus detection
CNN model that learns the voting oﬀset vectors and voting conﬁdence
Xie et al. 
Nucleus detection
H&E, Ki-67
CNN-based structured regression model for cell detection
Akram et al. 
Cell segmentation
FL, PC, H&E
fCNN for cell bounding box proposal and CNN for segmentation
Albarqouni et al. 
Mitosis detection
Incorporated ‘crowd sourcing’ layer into the CNN framework
Bauer et al. 
Nucleus classiﬁcation
CNN-based patch classiﬁer
Chen et al. 
Mitosis detection
Deep regression network (DRN)
Gao et al. 
Nucleus classiﬁcation
Classiﬁcation of Hep2-cells with CNN
Han et al. 
Nucleus classiﬁcation
Classiﬁcation of Hep2-cells with CNN
Janowczyk et al. 
Nucleus segmentation
Resolution adaptive deep hierarchical learning scheme
Kashif et al. 
Nucleus detection
Combination of CNN and hand-crafted features
Mao and Yin 
Mitosis detection
Hierarchical CNNs for patch sequence classiﬁcation
Mishra et al. 
Classiﬁcation of mitochondria
CNN-based patch classiﬁer
Phan et al. 
Nucleus classiﬁcation
Classiﬁcation of Hep2-cells using transfer learning (pre-trained CNN)
Romo-Bucheli et al. 
Tubule nuclei detection
CNN-based classiﬁcation of pre-selected candidate nuclei
Sirinukunwattana et al. 
Nucleus detection and classiﬁcation
CNN with spatially constrained regression
Song et al. 
Cell segmentation
Multi-scale CNN
Turkki et al. 
TIL detection
CNN-based classiﬁcation of superpixels
Veta et al. 
Nuclear area measurement
A CNN directly measures nucleus area without requiring segmentation
Wang et al. 
Subtype cell detection
Combination of two CNNs for joint cell detection and classiﬁcation
Xie et al. 
Nucleus detection and cell counting
FL and H&E
Microscopy cell counting with fully convolutional regression networks
Xing et al. 
Nucleus segmentation
CNN and selection-based sparse shape model
Xu et al. 
Nucleus detection
Stacked sparse auto-encoders (SSAE)
Xu and Huang 
Nucleus detection
General deep learning framework to detect cells in whole-slide images
Yang et al. 
Glial cell segmentation
fCNN with an iterative k-terminal cut algorithm
Yao et al. 
Nucleus classiﬁcation
Classiﬁes cellular tissue into tumor, lymphocyte, and stromal
Zhao et al. 
Classiﬁcation of leukocytes
CNN-based patch classiﬁer
Large organ segmentation
Ciresan et al. 
Segmentation of neuronal membranes
Ensemble of several CNNs with diﬀerent architectures
Kainz et al. 
Segmentation of colon glands
Used two CNNs to segment glands and their separating structures
Apou et al. 
Detection of lobular structures in breast
Combined the outputs of a CNN and a texture classiﬁcation system
BenTaieb and Hamarneh 
Segmentation of colon glands
fCNN with a loss accounting for smoothness and object interactions
BenTaieb et al. 
Segmentation of colon glands
A multi-loss fCNN to perform both segmentation and classiﬁcation
Chen et al. 
Neuronal membrane and fungus segmentation
Combination of bi-directional LSTM-RNNs and kU-Nets
Chen et al. 
Segmentation of colon glands
Deep contour-aware CNN
C¸ ic¸ek et al. 
Segmentation of xenopus kidney
Drozdzal et al. 
Segmentation of neuronal structures
fCNN with skip connections
Li et al. 
Segmentation of colon glands
Compares CNN with an SVM using hand-crafted features
Teikari et al. 
Volumetric vascular segmentation
Hybrid 2D-3D CNN architecture
Wang et al. 
Segmentation of messy and muscle regions
Conditional random ﬁeld jointly trained with an fCNN
Xie et al. 
Perimysium segmentation
2D spatial clockwork RNN
Xu et al. 
Segmentation of colon glands
Used three CNNs to predict gland and contour pixels
Xu et al. 
Segmenting epithelium & stroma
CNNs applied to over-segmented image regions (superpixels)
Detection and classiﬁcation of disease
Cruz-Roa et al. 
Detection of invasive ductal carcinoma
CNN-based patch classiﬁer
Xu et al. 
Patch-level classiﬁcation of colon cancer
Multiple instance learning framework with CNN features
Bychkov et al. 
Outcome prediction of colorectal cancer
Extracted CNN features from epithelial tissue for prediction
Chang et al. 
Multiple cancer tissue classiﬁcation
Transfer learning using multi-Scale convolutional sparse coding
G¨unhan Ertosun and Rubin 
Grading glioma
Ensemble of CNNs
K¨all´en et al. 
Predicting Gleason score
OverFeat pre-trained network as feature extractor
Kim et al. 
Thyroid cytopathology classiﬁcation
H&E, RM & Pap
Fine-tuning pre-trained AlexNet
Litjens et al. 
Detection of prostate and breast cancer
fCNN-based pixel classiﬁer
Quinn et al. 
Malaria, tuberculosis and parasites detection
Light microscopy
CNN-based patch classiﬁer
Rezaeilouyeh et al. 
Gleason grading and breast cancer detection
The system incorporates shearlet features inside a CNN
Schaumberg et al. 
SPOP mutation prediction of prostate cancer
Ensemble of ResNets
Wang et al. 
Metastases detection in lymph node
Ensemble of CNNs with hard negative mining
Other pathology applications
Janowczyk et al. 
Stain normalization
Used SAE for classifying tissue and subsequent histogram matching
Janowczyk and Madabhushi 
Deep learning tutorial
Covers diﬀerent detecting, segmentation, and classiﬁcation tasks
Sethi et al. 
Comparison of normalization algorithms
Presents eﬀectiveness of stain normalization for application of CNNs
CNNs with classical machine learning approaches using handcrafted features. In chest X-ray, several groups
detect multiple diseases with a single system. In CT
the detection of textural patterns indicative of interstitial lung diseases is also a popular research topic.
Chest radiography is the most common radiological
exam; several works use a large set of images with text
reports to train systems that combine CNNs for image
analysis and RNNs for text analysis. This is a branch of
research we expect to see more of in the near future.
In a recent challenge for nodule detection in CT,
LUNA16, CNN architectures were used by all top performing systems.
This is in contrast with a previous lung nodule detection challenge, ANODE09, where
handcrafted features were used to classify nodule candidates. The best systems in LUNA16 still rely on nodule
candidates computed by rule-based image processing,
but systems that use deep networks for candidate detection also performed very well (e.g. U-net). Estimating
the probability that an individual has lung cancer from
a CT scan is an important topic: It is the objective of
the Kaggle Data Science Bowl 2017, with $1 million in
prizes and more than one thousand participating teams.
4.4. Digital pathology and microscopy
The growing availability of large scale gigapixel
whole-slide images (WSI) of tissue specimen has made
digital pathology and microscopy a very popular application area for deep learning techniques. The developed
techniques applied to this domain focus on three broad
challenges: (1) Detecting, segmenting, or classifying
nuclei, (2) segmentation of large organs, and (3) detecting and classifying the disease of interest at the lesionor WSI-level. Table 5 presents an overview for each of
these categories.
Deep learning techniques have also been applied for
normalization of histopathology images. Color normalization is an important research area in histopathology
image analysis. In Janowczyk et al. , a method
for stain normalization of hematoxylin and eosin (H&E)
stained histopathology images was presented based on
deep sparse auto-encoders. Recently, the importance of
color normalization was demonstrated by Sethi et al.
 for CNN based tissue classiﬁcation in H&E
stained images.
The introduction of grand challenges in digital
pathology has fostered the development of computerized digital pathology techniques.
The challenges
that evaluated existing and new approaches for analysis of digital pathology images are: EM segmentation
challenge 2012 for the 2D segmentation of neuronal
processes, mitosis detection challenges in ICPR 2012
and AMIDA 2013, GLAS for gland segmentation and,
CAMELYON16 and TUPAC for processing breast cancer tissue samples.
In both ICPR 2012 and the AMIDA13 challenges on
mitosis detection the IDSIA team outperformed other
algorithms with a CNN based approach . The same team had the highest performing system in EM 2012 for 2D segmentation of neuronal processes. In their approach, the task
of segmenting membranes of neurons was performed
by mild smoothing and thresholding of the output of a
CNN, which computes pixel probabilities.
GLAS addressed the problem of gland instance segmentation in colorectal cancer tissue samples. Xu et al.
 achieved the highest rank using three CNN
models. The ﬁrst CNN classiﬁes pixels as gland versus non-gland.
From each feature map of the ﬁrst
CNN, edge information is extracted using the holistically nested edge technique, which uses side convolutions to produce an edge map. Finally, a third CNN
merges gland and edge maps to produce the ﬁnal segmentation.
CAMELYON16 was the ﬁrst challenge to provide
participants with WSIs.
Contrary to other medical
imaging applications, the availability of large amount
of annotated data in this challenge allowed for training very deep models such as 22-layer GoogLeNet
 , 16-layer VGG-Net , and 101-layer ResNet . The top-ﬁve performing systems used one of
these architectures. The best performing solution in the
Camelyon16 challenge was presented in Wang et al.
This method is based on an ensemble of
two GoogLeNet architectures, one trained with and
one without hard-negative mining to tackle the challenge. The latest submission of this team using the WSI
standardization algorithm by Ehteshami Bejnordi et al.
 achieved an AUC of 0.9935, for task 2, which
outperformed the AUC of a pathologist (AUC = 0.966)
who independently scored the complete test set.
The recently held TUPAC challenge addressed detection of mitosis in breast cancer tissue, and prediction
of tumor grading at the WSI level. The top performing system by Paeng et al. achieved the highest
performance in all tasks. The method has three main
components: (1) Finding high cell density regions, (2)
using a CNN to detect mitoses in the regions of interest,
(3) converting the results of mitosis detection to a feature vector for each WSI and using an SVM classiﬁer
to compute the tumor proliferation and molecular data
Table 6: Overview of papers using deep learning techniques for breast image analysis. MG = mammography; TS = tomosynthesis; US = ultrasound;
ADN = Adaptive Deconvolution Network.
Application; remarks
Sahiner et al. 
First application of a CNN to mammography
Jamieson et al. 
Four layer ADN, an early form of CNN for mass classiﬁcation
Fonseca et al. 
Pre-trained network extracted features classiﬁed with SVM for breast density estimation
Akselrod-Ballin et al. 
Use a modiﬁed region proposal CNN (R-CNN) for the localization and classiﬁcation of masses
Arevalo et al. 
Lesion classiﬁcation, combination with hand-crafted features gave the best performance
Dalmis et al. 
Breast and ﬁbroglandular tissue segmentation
Dubrovina et al. 
Tissue classiﬁcation using regular CNNs
Dhungel et al. 
Combination of diﬀerent CNNs combined with hand-crafted features
Fotin et al. 
Improved state-of-the art for mass detection in tomosynthesis
Hwang and Kim 
Weakly supervised CNN for localization of masses
Huynh et al. 
Pre-trained CNN on natural image patches applied to mass classiﬁcation
Kallenberg et al. 
Unsupervised CNN feature learning with SAE for breast density classiﬁcation
Kisilev et al. 
R-CNN combined with multi-class loss trained on semantic descriptions of potential masses
Kooi et al. 
Improved the state-of-the art for mass detection and show human performance on a patch level
Qiu et al. 
CNN for direct classiﬁcation of future risk of developing cancer based on negative mammograms
Samala et al. 
Microcalciﬁcation detection
Samala et al. 
Pre-trained CNN on mammographic masses transfered to tomosynthesis
Sun et al. 
Semi-supervised CNN for classiﬁcation of masses
Zhang et al. 
Classiﬁcation benign vs. malignant with shear wave elastography
Kooi et al. 
Pre-trained CNN on mass/normal patches to discriminate malignant masses from (benign) cysts
Wang et al. 
Detection of cardiovascular disease based on vessel calciﬁcation
Table 7: Overview of papers using deep learning techniques for cardiac image analysis.
Application; remarks
Emad et al. 
Left ventricle slice detection; simple CNN indicates if structure is present
Avendi et al. 
Left ventricle segmentation; AE used to initialize ﬁlters because training data set was small
Kong et al. 
Identiﬁcation of end-diastole and end-systole frames from cardiac sequences
Oktay et al. 
Super-resolution; U-net/ResNet hybrid, compares favorably with standard superresolution methods
Poudel et al. 
Left ventricle segmentation; RNN processes stack of slices, evaluated on several public datasets
Rupprecht et al. 
Cardiac structure segmentation; patch-based CNNs integrated in active contour framework
Tran 
Left and right ventricle segmentation; 2D fCNN architecture, evaluated on several public data sets
Yang et al. 
Left ventricle segmentation; CNN combined with multi-atlas segmentation
Zhang et al. 
Identifying presence of apex and base slices in cardiac exam for quality assessment
Ngo et al. 
Left ventricle segmentation; DBN is used to initialize a level set framework
Carneiro et al. 
Left ventricle segmentation; DBN embedded in system using landmarks and non-rigid registration
Carneiro and Nascimento 
Left ventricle tracking; extension of Carneiro et al. for tracking
Chen et al. 
Structure segmentation in 5 diﬀerent 2D views; uses transfer learning
Ghesu et al. 
3D aortic valve detection and segmentation; uses shallow and deeper sparse networks
Nascimento and Carneiro 
Left ventricle segmentation; DBN applied to patches steers multi-atlas segmentation process
Moradi et al. 
Automatic generation of text descriptions for Doppler US images of cardiac valves using doc2vec
G¨uls¨un et al. 
Coronary centerline extraction; CNN classiﬁes paths as correct or leakages
Lessmann et al. 
Coronary calcium detection in low dose ungated CT using multi-stream CNN (3 views)
Moradi et al. 
Labeling of 2D slices from cardiac CT exams; comparison with handcrafted features
de Vos et al. 
Detect bounding boxes by slice classiﬁcation and combining 3 orthogonal 2D CNNs
Wolterink et al. 
Coronary calcium detection in gated CTA; compares 3D CNN with multi-stream 2D CNNs
Zreik et al. 
Left ventricle segmentation; multi-stream CNN (3 views) voxel classiﬁcation
4.5. Breast
One of the earliest DNN applications from Sahiner
et al. was on breast imaging. Recently, interest
has returned which resulted in signiﬁcant advances over
the state of the art, achieving the performance of human
readers on ROIs . Since most breast
imaging techniques are two dimensional, methods successful in natural images can easily be transferred. With
one exception, the only task addressed is the detection
of breast cancer; this consisted of three subtasks: (1)
detection and classiﬁcation of mass-like lesions, (2) detection and classiﬁcation of micro-calciﬁcations, and (3)
breast cancer risk scoring of images. Mammography is
by far the most common modality and has consequently
enjoyed the most attention.
Work on tomosynthesis,
US, and shear wave elastography is still scarce, and we
have only one paper that analyzed breast MRI with deep
learning; these other modalities will likely receive more
attention in the next few years. Table 6 summarizes the
literature and main messages.
Since many countries have screening initiatives for
breast cancer, there should be massive amounts of data
available, especially for mammography, and therefore
enough opportunities for deep models to ﬂourish. Unfortunately, large public digital databases are unavailable and consequently older scanned screen-ﬁlm data
sets are still in use. Challenges such as the recently
launched DREAM challenge have not yet had the desired success.
As a result, many papers used small data sets resulting in mixed performance. Several projects have addressed this issue by exploring semi-supervised learning
 , weakly supervised learning , and transfer learning ). Another method combines deep
models with handcrafted features ,
which have been shown to be complementary still, even
for very big data sets . State of the
art techniques for mass-like lesion detection and classi-
ﬁcation tend to follow a two-stage pipeline with a candidate detector; this design reduces the image to a set
of potentially malignant lesions, which are fed to a deep
CNN . Alternatives use a region proposal network (R-CNN) that bypasses the cascaded approach .
When large data sets are available, good results can
be obtained.
At the SPIE Medical Imaging conference of 2016, a researcher from a leading company in
the mammography CAD ﬁeld told a packed conference
room how a few weeks of experiments with a standard
architecture (AlexNet) - trained on the company’s proprietary database - yielded a performance that was superior to what years of engineering handcrafted feature
systems had achieved .
4.6. Cardiac
Deep learning has been applied to many aspects of
cardiac image analysis; the literature is summarized in
Table 7. MRI is the most researched modality and left
ventricle segmentation the most common task, but the
number of applications is highly diverse: segmentation, tracking, slice classiﬁcation, image quality assessment, automated calcium scoring and coronary centerline tracking, and super-resolution.
Most papers used simple 2D CNNs and analyzed the
3D and often 4D data slice by slice; the exception is
Wolterink et al. where 3D CNNs were used.
DBNs are used in four papers, but these all originated
from the same author group. The DBNs are only used
for feature extraction and are integrated in compound
segmentation frameworks. Two papers are exceptional
because they combined CNNs with RNNs: Poudel et al.
 introduced a recurrent connection within the Unet architecture to segment the left ventricle slice by
slice and learn what information to remember from the
previous slices when segmenting the next one. Kong
et al. used an architecture with a standard 2D
CNN and an LSTM to perform temporal regression to
identify speciﬁc frames and a cardiac sequence. Many
papers use publicly available data.
The largest challenge in this ﬁeld was the 2015 Kaggle Data Science
Bowl where the goal was to automatically measure endsystolic and end-diastolic volumes in cardiac MRI. 192
teams competed for $200,000 in prize money and the
top ranking teams all used deep learning, in particular
fCNN or U-net segmentation schemes.
4.7. Abdomen
Most papers on the abdomen aimed to localize and
segment organs, mainly the liver, kidneys, bladder, and
pancreas (Table 8). Two papers address liver tumor segmentation. The main modality is MRI for prostate analysis and CT for all other organs. The colon is the only
area where various applications were addressed, but always in a straightforward manner: A CNN was used
as a feature extractor and these features were used for
classiﬁcation.
It is interesting to note that in two segmentation
challenges - SLIVER07 for liver and PROMISE12 for
prostate - more traditional image analysis methods were
dominant up until 2016. In PROMISE12, the current
second and third in rank among the automatic methods
used active appearance models.
The algorithm from
IMorphics was ranked ﬁrst for almost ﬁve years (now
ranked second). However, a 3D fCNN similar to Unet has recently taken the top position.
This paper has an interesting approach where a sumoperation was used instead of the concatenation operation used in U-net, making it a hybrid between a ResNet
and U-net architecture. Also in SLIVER07 - a 10-yearold liver segmentation challenge - CNNs have started
to appear in 2016 at the top of the leaderboard, replacing previously dominant methods focused on shape and
appearance modeling.
4.8. Musculoskeletal
Musculoskeletal images have also been analyzed by
deep learning algorithms for segmentation and identiﬁcation of bone, joint, and associated soft tissue abnormalities in diverse imaging modalities. The works are
summarized in Table 9.
Table 8: Overview of papers using deep learning for abdominal image analysis.
Hu et al. 
Segmentation
3D CNN with time-implicit level sets for segmentation of liver, spleen and kidneys
Segmentation tasks in liver imaging
Li et al. 
2D 17×17 patch-based classiﬁcation, Ben-Cohen et al. repeats this approach
Vivanti et al. 
2D CNN for liver tumor segmentation in follow-up CT taking baseline CT as input
Ben-Cohen et al. 
2D CNN similar to U-net, but without cross-connections; good results on SLIVER07
Christ et al. 
Liver & tumor
U-net, cascaded fCNN and dense 3D CRF
Dou et al. 
3D CNN with conditional random ﬁeld; good results on SLIVER07
Hoogi et al. 
2D CNN obtained probabilities are used to drive active contour model
Hu et al. 
3D CNN with surface evolution of a shape prior; good results on SLIVER07
Lu et al. 
3D CNN, competitive results on SLIVER07
Lu et al. 
Localization
Combines local patch and slice based CNN
Ravishankar et al. 
Localization
Combines CNN with classical features to detect regions around kidneys
Thong et al. 
Segmentation
2D CCN with 43×43 patches, tested on 20 scans
Pancreas segmentation in CT
Farag et al. 
Segmentation
Approach with elements similar to Roth et al. 
Roth et al. 
Segmentation
Orthogonal patches from superpixel regions are fed into CNNs in three diﬀerent ways
Cai et al. 
Segmentation
2 CNNs detect inside and boundary of organ, initializes conditional random ﬁeld
Roth et al. 
Segmentation
2 CNNs detect inside and boundary of pancreas, combined with random forests
Tajbakhsh et al. 
Polyp detection
Colonoscopy
CNN computes additional features, improving existing scheme
Liu et al. 
Colitis detection
Pre-trained ImageNet CNN generates features for linear SVM
Nappi et al. 
Polyp detection
Substantial reduction of false positives using pre-trained and ﬁne-tuned CNN
Tachibana et al. 
Electronic cleansing
Voxel classiﬁcation in dual energy CT, material other than soft tissue is removed
Zhang et al. 
Polyp detection
Colonoscopy
Pre-trained ImageNet CNN for feature extraction, two SVMs for cascaded classiﬁcation
Prostate segmentation in MRI
Liao et al. 
Application of stacked independent subspace analysis networks
Cheng et al. 
CNN produces energy map for 2D slice based active appearance segmentation
Guo et al. 
Stacked sparse auto-encoders extract features from patches, input to atlas matching and a deformable model
Milletari et al. 
3D U-net based CNN architecture with objective function that directly optimizes Dice coeﬃcient, ranks #5 in PROMISE12
Yu et al. 
3D fully convolutional network, hybrid between a ResNet and U-net architecture, ranks #1 on PROMISE12
Azizi et al. )
Lesion classiﬁcation
DBN learns features from temporal US to classify prostate lesions benign/malignant
Shah et al. 
Features from pre-trained CNN combined with features from hashing forest
Zhu et al. 
Lesion classiﬁcation
Learns features from multiple modalities, hierarchical random forest for classiﬁcation
Cha et al. 
Segmentation
CNN patch classiﬁcation used as initialization for level set
Table 9: Overview of papers using deep learning for musculoskeletal image analysis.
Application; remarks
Prasoon et al. 
Knee cartilage segmentation using multi-stream CNNs
Chen et al. 
Vertebrae localization; joint learning of vertebrae appearance and dependency on neighbors using CNN
Roth et al. 
Sclerotic metastases detection; random 2D views are analyzed by CNN and aggregated
Shen et al. 
Vertebrae localization and segmentation; CNN for segmenting vertebrae and for center detection
Suzani et al. 
Vertebrae localization, identiﬁcation and segmentation of vertebrae; CNN used for initial localization
Yang et al. 
Anatomical landmark detection; uses CNN for slice classiﬁcation for presence of landmark
Antony et al. 
Osteoarthritis grading; pre-trained ImageNet CNN ﬁne-tuned on knee X-rays
Cai et al. 
Vertebrae localization; RBM determines position, orientation and label of vertebrae
Golan et al. 
Hip dysplasia detection; CNN with adversarial component detects structures and performs measurements
Korez et al. 
Vertebral bodies segmentation; voxel probabilities obtained with a 3D CNN are input to deformable model
Jamaludin et al. 
Automatic spine scoring; VGG-19 CNN analyzes vertebral discs and ﬁnds lesion hotspots
Miao et al. 
Total Knee Arthroplasty kinematics by real-time 2D/3D registration using CNN
Roth et al. 
Posterior-element fractures detection; CNN for 2.5D patch-based analysis
ˇStern et al. 
Hand age estimation; 2D regression CNN analyzes 13 bones
Forsberg et al. 
Vertebrae detection and labeling; outputs of two CNNs are input to graphical model
Spampinato et al. 
Skeletal bone age assessment; comparison among several deep learning approaches for the task at hand
A surprising number of complete applications with
promising results are available; one that stands out is
Jamaludin et al. who trained their system with
12K discs and claimed near-human performances across
four diﬀerent radiological scoring tasks.
4.9. Other
This ﬁnal section lists papers that address multiple
applications (Table 10) and a variety of other applications (Table 11).
It is remarkable that one single architecture or approach based on deep learning can be applied without modiﬁcations to diﬀerent tasks; this illustrates the
versatility of deep learning and its general applicability. In some works, pre-trained architectures are used,
sometimes trained with images from a completely different domain.
Several authors analyze the eﬀect of
ﬁne-tuning a network by training it with a small data set
of images from the intended application domain. Combining features extracted by a CNN with ‘traditional’
features is also commonly seen.
From Table 11, the large number of papers that address obstetric applications stand out. Most papers address the groundwork, such as selecting an appropriate
frame from an US stream. More work on automated
measurements with deep learning in these US sequences
is likely to follow.
The second area where CNNs are rapidly improving the state of the art is dermoscopic image analysis. For a long time, diagnosing skin cancer from photographs was considered very diﬃcult and out of reach
for computers. Many studies focused only on images
obtained with specialized cameras, and recent systems
based on deep networks produced promising results. A
recent work by Esteva et al. demonstrated excellent results with training a recent standard architecture
(Google’s Inception v3) on a data set of both dermoscopic and standard photographic images. This data set
was two orders of magnitude larger than what was used
in literature before. In a thorough evaluation, the proposed system performed on par with 30 board certiﬁed
dermatologists.
5. Discussion
From the 308 papers reviewed in this survey, it is evident that deep learning has pervaded every aspect of
medical image analysis. This has happened extremely
quickly: the vast majority of contributions, 242 papers,
were published in 2016 or the ﬁrst month of 2017. A
large diversity of deep architectures are covered. The
earliest studies used pre-trained CNNs as feature extractors. The fact that these pre-trained networks could simply be downloaded and directly applied to any medical
image facilitated their use. Moreover, in this approach
already existing systems based on handcrafted features
could simply be extended. In the last two years, however, we have seen that end-to-end trained CNNs have
become the preferred approach for medical imaging interpretation (see Figure 1). Such CNNs are often integrated into existing image analysis pipelines and replace
traditional handcrafted machine learning methods. This
is the approach followed by the largest group of papers
in this survey and we can conﬁdently state that this is
the current standard practice.
Key aspects of successful deep learning methods
After reviewing so many papers one would expect
to be able to distill the perfect deep learning method
and architecture for each individual task and application area. Although convolutional neural networks (and
derivatives) are now clearly the top performers in most
medical image analysis competitions, one striking conclusion we can draw is that the exact architecture is not
the most important determinant in getting a good solution. We have seen, for example in challenges like
the Kaggle Diabetic Retinopathy Challenge, that many
researchers use the exact same architectures, the same
type of networks, but have widely varying results. A
key aspect that is often overlooked is that expert knowledge about the task to be solved can provide advantages that go beyond adding more layers to a CNN.
Groups and researchers that obtain good performance
when applying deep learning algorithms often diﬀerentiate themselves in aspects outside of the deep network, like novel data preprocessing or augmentation
techniques.
An example is that the best performing
method in the CAMELYON16-challenge improved signiﬁcantly (AUC from 0.92 to 0.99) by adding a stain
normalization pre-processing step to improve generalization without changing the CNN. Other papers focus
on data augmentation strategies to make networks more
robust, and they report that these strategies are essential
to obtain good performance. An example is the elastic deformations that were applied in the original U-Net
paper .
Augmentation and pre-processing are, of course, not
the only key contributors to good solutions. Several researchers have shown that designing architectures incorporating unique task-speciﬁc properties can obtain
better results than straightforward CNNs. Two examples which we encountered several times are multi-view
Table 10: Overview of papers using a single deep learning approach for diﬀerent tasks. DQN = Deep Q-Network
Shin et al. 
Heart, kidney, liver segmentation
SAE to learn temporal/spatial features on 2D + time DCE-MRI
Roth et al. 
2D slice classiﬁcation
Automatically classifying slices in 5 anatomical regions
Shin et al. 
2D key image labeling
Text and 2D image analysis on a diverse set of 780 thousand images
Cheng et al. 
Various detection tasks
Detection of breast lesions in US and pulmonary nodules in CT
Ghesu et al. 
Landmark detection
US, CT, MRI
Reinforcement learning with CNN features, cardiac MR/US, head&neck CT
Liu et al. 
Image retrieval
Combines CNN feature with Radon transform, evaluated on IRMA database
Merkow et al. 
Vascular network segmentation
Framework to ﬁnd various vascular networks
Moeskops et al. 
Various segmentation tasks
Single architecture to segment 6 brain tissues, pectoral muscle & coronaries
Roth et al. 
Various detection tasks
Multi-stream CNN to detect sclerotic lesions, lymph nodes and polyps
Shin et al. 
Abnormality detection
Compares architectures for detecting interstitial disease and lymph nodes
Tajbakhsh et al. 
Abnormality detection
Compares pre-trained with fully trained networks for three detection tasks
Wang et al. 
2D key image labeling
Text concept clustering, related to Shin et al. 
Yan et al. 
2D slice classiﬁcation
Automatically classifying CT slices in 12 anatomical regions
Zhou et al. 
Thorax-abdomen segmentation
21 structures are segmented with 3 orthogonal 2D fCNNs and majority voting
Table 11: Overview of papers using deep learning for various image analysis tasks.
Fetal imaging
Chen et al. 
Frame labeling
Locates abdominal plane from fetal ultrasound videos
Chen et al. 
Frame labeling
Same task as Chen et al. , now using RNNs
Baumgartner et al. 
Frame labeling
Labeling 12 standard frames in 1003 mid pregnancy fetal US videos
Gao et al. 
Frame labeling
4 class frame classiﬁcation using transfer learning with pre-trained networks
Kumar et al. 
Frame labeling
12 standard anatomical planes, CNN extracts features for support vector machine
Rajchl et al. 
Segmentation with non expert labels
Crowd-sourcing annotation eﬀorts to segment brain structures
Rajchl et al. 
Segmentation given bounding box
CNN and CRF for segmentation of structures
Ravishankar et al. 
Quantiﬁcation
Hybrid system using CNN and texture features to ﬁnd abdominal circumference
Yu et al. 
Left ventricle segmentation
Frame-by-frame segmentation by dynamically ﬁne-tuning CNN to the latest frame
Dermatology
Codella et al. 
Melanoma detection in dermoscopic images
Features from pre-trained CNN combined with other features
Demyanov et al. 
Pattern identiﬁcation in dermoscopic images
Comparison to simpler networks and simple machine learning
Kawahara et al. 
5 and 10-class classiﬁcation photographic images
Pre-trained CNN for feature extraction at two image resolutions
Kawahara and Hamarneh 
10-class classiﬁcation photographic images
Extending Kawahara et al. now training multi-resolution CNN end-to-end
Yu et al. 
Melanoma detection in dermoscopic images
Deep residual networks for lesion segmentation and classiﬁcation, winner ISIC16
Menegola et al. 
Classiﬁcation of dermoscopic images
Various pre-training and ﬁne-tuning strategies are compared
Esteva et al. 
Classiﬁcation of photographic and dermoscopic images
Inception CNN trained on 129k images; compares favorably to 29 dermatologists
Lymph nodes
Roth et al. 
Lymph node detection
Introduces multi-stream framework of 2D CNNs with orthogonal patches
Barbu et al. 
Lymph node detection
Compares eﬀect of diﬀerent loss functions
Nogues et al. 
Lymph node detection
2 fCNNs, for inside and for contour of lymph nodes, are combined in a CRF
Wang et al. 
Wound segmentation
photographs
Additional detection of infection risk and healing progress
Ypsilantis et al. 
Chemotherapy response prediction
CNN outperforms classical radiomics features in patients with esophageal cancer
Zheng et al. 
Carotid artery bifurcation detection
Two stage detection process, CNNs combined with Haar features
Alansary et al. 
Placenta segmentation
3D multi-stream CNN with extension for motion correction
Fritscher et al. 
Head&Neck tumor segmentation
3 orthogonal patches in 2D CNNs, combined with other features
Jaumard-Hakoun et al. 
Tongue contour extraction
Analysis of tongue motion during speech, combines auto-encoders with RBMs
Payer et al. 
Hand landmark detection
Various architectures are compared
Quinn et al. 
Disease detection
microscopy
Smartphone mounted on microscope detects malaria, tuberculosis & parasite eggs
Smistad and Løvstakken 
Vessel detection and segmentation
Femoral and carotid vessels analyzed with standard fCNN
Twinanda et al. 
Task recognition in laparoscopy
Fine-tuned AlexNet applied to video frames
Xu et al. 
Cervical dysplasia
cervigrams
Fine-tuned pre-trained network with added non-imaging features
Xue et al. 
Esophageal microvessel classiﬁcation
Microscopy
Simple CNN used for feature extraction
Zhang et al. 
Image reconstruction
Reconstructing from limited angle measurements, reducing reconstruction artefacts
Lekadir et al. 
Carotid plaque classiﬁcation
Simple CNN for characterization of carotid plaque composition in ultrasound
Ma et al. 
Thyroid nodule detection
CNN and standard features combines for 2D US analysis
and multi-scale networks. Other, often underestimated,
parts of network design are the network input size and
receptive ﬁeld (i.e. the area in input space that contributes to a single output unit). Input sizes should be
selected considering for example the required resolution
and context to solve a problem. One might increase the
size of the patch to obtain more context, but without
changing the receptive ﬁeld of the network this might
not be beneﬁcial. As a standard sanity check researchers
could perform the same task themselves via visual assessment of the network input. If they, or domain experts, cannot achieve good performance, the chance that
you need to modify your network input or architecture
The last aspect we want to touch on is model hyperparameter optimization (e.g. learning rate, dropout
rate), which can help squeeze out extra performance
from a network. We believe this is of secondary im-
portance with respect to performance to the previously
discussed topics and training data quality. Disappointingly, no clear recipe can be given to obtain the best set
of hyper-parameters as it is a highly empirical exercise.
Most researchers fall back to an intuition-based random
search , which often seems
to work well enough. Some basic tips have been covered before by Bengio . Researchers have also
looked at Bayesian methods for hyper-parameter optimization , but this has not been applied in medical image analysis as far as we are aware
Unique challenges in medical image analysis
It is clear that applying deep learning algorithms to
medical image analysis presents several unique challenges. The lack of large training data sets is often mentioned as an obstacle. However, this notion is only partially correct. The use of PACS systems in radiology
has been routine in most western hospitals for at least
a decade and these are ﬁlled with millions of images.
There are few other domains where this magnitude of
imaging data, acquired for speciﬁc purposes, are digitally available in well-structured archives. PACS-like
systems are not as broadly used for other specialties in
medicine, like ophthalmology and pathology, but this
is changing as imaging becomes more prevalent across
disciplines. We are also seeing that increasingly large
public data sets are made available: Esteva et al. 
used 18 public data sets and more than 105 training images; in the Kaggle diabetic retinopathy competition a
similar number of retinal images were released; and several chest x-ray studies used more than 104 images.
The main challenge is thus not the availability of image data itself, but the acquisition of relevant annotations/labeling for these images. Traditionally PACS systems store free-text reports by radiologists describing
their ﬁndings. Turning these reports into accurate annotations or structured labels in an automated manner
requires sophisticated text-mining methods, which is an
important ﬁeld of study in itself where deep learning is
also widely used nowadays. With the introduction of
structured reporting into several areas of medicine, extracting labels to data is expected to become easier in the
future. For example, there are already papers appearing
which directly leverage BI-RADS categorizations by radiologist to train deep networks or
semantic descriptions in analyzing optical coherence tomography images . We expect the
amount of research in optimally leveraging free-text and
structured reports for network training to increase in the
near future.
Given the complexity of leveraging free-text reports
from PACS or similar systems to train algorithms, generally researchers request domain experts (e.g. radiologist, pathologists) to make task-speciﬁc annotations for
the image data. Labeling a suﬃciently large dataset can
take a signiﬁcant amount of time, and this is problematic. For example, to train deep learning systems for
segmentation in radiology often 3D, slice-by-slice annotations need to be made and this is very time consuming. Thus, learning eﬃciently from limited data is
an important area of research in medical image analysis. A recent paper focused on training a deep learning
segmentation system for 3D segmentation using only
sparse 2D segmentations . Multipleinstance or active learning approaches might also offer beneﬁt in some cases, and have recently been pursued in the context of deep learning .
One can also consider leveraging non-expert labels via
crowd-sourcing .
Other potential solutions can be found within the medical ﬁeld itself; in histopathology one can sometimes use speciﬁc
immunohistochemical stains to highlight regions of interest, reducing the need for expert experience .
Even when data is annotated by domain expert, label
noise can be a signiﬁcant limiting factor in developing
algorithms, whereas in computer vision the noise in the
labeling of images is typically relatively low. To give
an example, a widely used dataset for evaluating image analysis algorithms to detect nodules in lung CT is
the LIDC-IDRI dataset . In this
dataset pulmonary nodules were annotated by four radiologists independently. Subsequently the readers reviewed each others annotations but no consensus was
forced. It turned out that the number of nodules they
did not unanimously agreed on to be a nodule, was three
times larger than the number they did fully agree on.
Training a deep learning system on such data requires
careful consideration of how to deal with noise and uncertainty in the reference standard.
One could think
of solutions like incorporating labeling uncertainty directly in the loss function, but this is still an open challenge.
In medical imaging often classiﬁcation or segmentation is presented as a binary task: normal versus abnormal, object versus background. However, this is often a gross simpliﬁcation as both classes can be highly
heterogeneous. For example, the normal category often consists of completely normal tissue but also several categories of benign ﬁndings, which can be rare,
and may occasionally include a wide variety of imaging artifacts. This often leads to systems that are ex-
tremely good at excluding the most common normal
subclasses, but fail miserably on several rare ones. A
straightforward solution would be to turn the deep learning system in a multi-class system by providing it with
detailed annotations of all possible subclasses. Obviously this again compounds the issue of limited availability of expert time for annotating and is therefore often simply not feasible. Some researchers have specifically looked into tackling this imbalance by incorporating intelligence in the training process itself, by applying selective sampling or
hard negative mining . However,
such strategies typically fail when there is substantial
noise in the reference standard. Additional methods for
dealing with within-class heterogeneity would be highly
Another data-related challenge is class imbalance. In
medical imaging, images for the abnormal class might
be challenging to ﬁnd, depending on the task at hand.
As an example, the implementation of breast cancer
screening programs has resulted in vast databases of
mammograms that have been established at many locations world-wide. However, the majority of these images are normal and do not contain any suspicious lesions. When a mammogram does contain a suspicious
lesion this is often not cancerous, and even most cancerous lesions will not lead to the death of a patient.
Designing deep learning systems that are adept at handling this class imbalance is another important area of
research. A typical strategy we encountered in current
literature is the application of speciﬁc data augmentation algorithms to just the underrepresented class, for
example scaling and rotation transforms to generate new
lesions. Pereira et al. performed a thorough evaluation of data augmentation strategies for brain lesion
segmentation to combat class imbalance.
In medical image analysis useful information is not
just contained within the images themselves. Physicians
often leverage a wealth of data on patient history, age,
demographics and others to arrive at better decisions.
Some authors have already investigated combining this
information into deep learning networks in a straightforward manner . However, as these
authors note, the improvements that were obtained were
not as large as expected. One of the challenges is to balance the number of imaging features in the deep learning network (typically thousands) with the number of
clinical features (typically only a handful) to prevent the
clinical features from being drowned out. Physicians
often also need to use anatomical information to come
to an accurate diagnosis. However, many deep learning
systems in medical imaging are still based on patch classiﬁcation, where the anatomical location of the patch is
often unknown to network. One solution would be to
feed the entire image to the deep network and use a different type of evaluation to drive learning, as was done
by, for example, Milletari et al. , who designed
a loss function based on the Dice coeﬃcient. This also
takes advantage of the fact that medical images are often acquired using a relatively static protocol, where the
anatomy is always roughly in the same position and at
the same scale. However, as mentioned above, if the
receptive ﬁeld of the network is small feeding in the entire image oﬀers no beneﬁt. Furthermore, feeding full
images to the network is not always feasible due to, for
example, memory constraints. In some cases this might
be solved in the near future due to advances in GPU
technology, but in others, for example digital pathology
with its gigapixel-sized images, other strategies have to
be invented.
Although most of the challenges mentioned above
have not been adequately tackled yet, several highproﬁle successes of deep learning in medical imaging
have been reported, such as the work by Esteva et al.
 and Gulshan et al. in the ﬁelds of dermatology and ophthalmology. Both papers show that it is
possible to outperform medical experts in certain tasks
using deep learning for image classiﬁcation. However,
we feel it is important to put these papers into context
relative to medical image analysis in general, as most
tasks can by no means be considered ’solved’. One aspect to consider is that both Esteva et al. and Gulshan et al. focus on small 2D color image classi-
ﬁcation, which is relatively similar to the tasks that have
been tackled in computer vision (e.g. ImageNet). This
allows them to take advantage of well-explored network
architectures like ResNet and VGG-Net which have
shown to have excellent results in these tasks. However,
there is no guarantee that these architectures are optimal
in for example regressions/detection tasks. It also allowed the authors to use networks that were pre-trained
on a very well-labeled dataset of millions of natural images, which helps combat the lack of similarly large,
labeled medical datasets. In contrast, in most medical
imaging tasks 3D gray-scale or multi-channel images
are used for which pre-trained networks or architectures
dont exist. In addition this data typically has very speciﬁc challenges, like anisotropic voxel sizes, small registration errors between varying channels (e.g. in multiparametric MRI) or varying intensity ranges. Although
many tasks in medical image analysis can be postulated
as a classiﬁcation problem, this might not always be the
optimal strategy as it typically requires some form of
post-processing with non-deep learning methods (e.g.
counting, segmentation or regression tasks). An interesting example is the paper by Sirinukunwattana et al.
 , which details a method directly predicting the
center locations of nuclei and shows that this outperforms classiﬁcation-based center localization. Nonetheless, the papers by Esteva et al. and Gulshan et al.
 do show what ideally is possible with deep learning methods that are well-engineered for speciﬁc medical image analysis tasks.
Looking at current trends in the machine learning
community with respect to deep learning, we identify a
key area which can be highly relevant for medical imaging and is receiving (renewed) interest: unsupervised
learning. The renaissance of neural networks started
around 2006 with the popularization of greedy layerwise pre-training of neural networks in an unsupervised
manner. This was quickly superseded by fully supervised methods which became the standard after the success of AlexNet during the ImageNet competition of
2012, and most papers in this survey follow a supervised
approach. However, interest in unsupervised training
strategies has remained and recently has regained traction.
Unsupervised methods are attractive as they allow
(initial) network training with the wealth of unlabeled
data available in the world.
Another reason to assume that unsupervised methods will still have a signiﬁcant role to play is the analogue to human learning, which seems to be much more data eﬃcient and
also happens to some extent in an unsupervised manner; we can learn to recognize objects and structures
without knowing the speciﬁc label. We only need very
limited supervision to categorize these recognized objects into classes. Two novel unsupervised strategies
which we expect to have an impact in medical imaging are variational auto-encoders (VAEs), introduced by
Kingma and Welling and generative adversarial networks (GANs), introduced by Goodfellow et al.
 . The former merges variational Bayesian graphical models with neural networks as encoders/decoders.
The latter uses two competing convolutional neural networks where one is generating artiﬁcial data samples
and the other is discriminating artiﬁcial from real samples. Both have stochastic components and are generative networks. Most importantly, they can be trained
end-to-end and learn representative features in a completely unsupervised manner. As we discussed in previous paragraphs, obtaining large amounts of unlabeled
medical data is generally much easier than labeled data
and unsupervised methods like VAEs and GANs could
optimally leverage this wealth of information.
Finally, deep learning methods have often been described as ‘black boxes’. Especially in medicine, where
accountability is important and can have serious legal
consequences, it is often not enough to have a good prediction system. This system also has to be able to articulate itself in a certain way. Several strategies have
been developed to understand what intermediate layers
of convolutional networks are responding to, for example deconvolution networks ,
guided back-propagation or
deep Taylor composition . Other
researchers have tied prediction to textual representations of the image (i.e. captioning) , which is another useful avenue to understand
what a network is perceiving. Last, some groups have
tried to combine Bayesian statistics with deep networks
to obtain true network uncertainty estimates Kendall
and Gal .
This would allow physicians to assess when the network is giving unreliable predictions.
Leveraging these techniques in the application of deep
learning methods to medical image analysis could accelerate acceptance of deep learning applications among
clinicians, and among patients. We also foresee deep
learning approaches will be used for related tasks in
medical imaging, mostly unexplored, such as image reconstruction . Deep learning will thus not
only have a great impact in medical image analysis, but
in medical imaging as a whole.
Acknowledgments
The authors would like to thank members of the Diagnostic Image Analysis Group for discussions and suggestions.
This research was funded by grants KUN
2012-5577, KUN 2014-7032, and KUN 2015-7970 of
the Dutch Cancer Society.
Appendix A: Literature selection
PubMed was searched for papers containing ”convolutional” OR ”deep learning” in any ﬁeld. We specifically did not include the term neural network here as
this would result in an enormous amount of ’false positive’ papers covering brain research. This search initially gave over 700 hits. ArXiv was searched for papers mentioning one of a set of terms related to medical
imaging. The exact search string was: ’abs:((medical
OR mri OR ”magnetic resonance” OR CT OR ”computed tomography” OR ultrasound OR pathology OR
xray OR x-ray OR radiograph OR mammography OR
fundus OR OCT) AND (”deep learning” OR convolutional OR cnn OR ”neural network”))’. Conference
proceedings for MICCAI (including workshops), SPIE,
ISBI and EMBC were searched based on titles of papers. Again we looked for mentions of ’deep learning’
or ’convolutional’ or ’neural network’. We went over all
these papers and excluded the ones that did not discuss
medical imaging (e.g. applications to genetics, chemistry), only used handcrafted features in combination
with neural networks, or only referenced deep learning as future work. When in doubt whether a paper
should be included we read the abstract and when the
exact methodology was still unclear we read the paper
itself. We checked references in all selected papers iteratively and consulted colleagues to identify any papers
which were missed by our initial search. When largely
overlapping work had been reported in multiple publications, only the publication deemed most important was
included. A typical example here was arXiv preprints
that were subsequently published or conference contributions which were expanded and published in journals.