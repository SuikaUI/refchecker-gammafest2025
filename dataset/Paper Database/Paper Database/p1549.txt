ShufﬂeNet: An Extremely Efﬁcient Convolutional Neural Network for Mobile
Xiangyu Zhang∗
Xinyu Zhou∗
Mengxiao Lin
Megvii Inc (Face++)
{zhangxiangyu,zxy,linmengxiao,sunjian}@megvii.com
We introduce an extremely computation-efﬁcient CNN
architecture named ShufﬂeNet, which is designed specially
for mobile devices with very limited computing power (e.g.,
10-150 MFLOPs). The new architecture utilizes two new
operations, pointwise group convolution and channel shuf-
ﬂe, to greatly reduce computation cost while maintaining
accuracy. Experiments on ImageNet classiﬁcation and MS
COCO object detection demonstrate the superior performance of ShufﬂeNet over other structures, e.g. lower top-1
error (absolute 7.8%) than recent MobileNet on ImageNet classiﬁcation task, under the computation budget of
40 MFLOPs. On an ARM-based mobile device, ShufﬂeNet
achieves ∼13× actual speedup over AlexNet while maintaining comparable accuracy.
1. Introduction
Building deeper and larger convolutional neural networks (CNNs) is a primary trend for solving major visual
recognition tasks . The most accurate CNNs usually have hundreds of layers and thousands
of channels , thus requiring computation at
billions of FLOPs. This report examines the opposite extreme: pursuing the best accuracy in very limited computational budgets at tens or hundreds of MFLOPs, focusing
on common mobile platforms such as drones, robots, and
smartphones. Note that many existing works focus on pruning, compressing, or low-bit representing a “basic” network architecture. Here we aim to explore
a highly efﬁcient basic architecture specially designed for
our desired computing ranges.
We notice that state-of-the-art basic architectures such as
Xception and ResNeXt become less efﬁcient in extremely small networks because of the costly dense 1 × 1
convolutions. We propose using pointwise group convolu-
* Equally contribution.
tions to reduce computation complexity of 1 × 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shufﬂe operation to help the information ﬂowing across feature channels.
Based on the two techniques, we build a highly efﬁcient architecture called ShufﬂeNet. Compared with popular structures like , for a given computation complexity
budget, our ShufﬂeNet allows more feature map channels,
which helps to encode more information and is especially
critical to the performance of very small networks.
We evaluate our models on the challenging ImageNet
classiﬁcation and MS COCO object detection 
tasks. A series of controlled experiments shows the effectiveness of our design principles and the better performance
over other structures. Compared with the state-of-the-art
architecture MobileNet , ShufﬂeNet achieves superior
performance by a signiﬁcant margin, e.g. absolute 7.8%
lower ImageNet top-1 error at level of 40 MFLOPs.
We also examine the speedup on real hardware, i.e. an
off-the-shelf ARM-based computing core. The ShufﬂeNet
model achieves ∼13× actual speedup (theoretical speedup
is 18×) over AlexNet while maintaining comparable
2. Related Work
Efﬁcient Model Designs
The last few years have seen
the success of deep neural networks in computer vision
tasks , in which model designs play an important role. The increasing needs of running high quality deep neural networks on embedded devices encourage the study on efﬁcient model designs .
For example, GoogLeNet increases the depth of networks
with much lower complexity compared to simply stacking convolution layers. SqueezeNet reduces parameters and computation signiﬁcantly while maintaining accuracy. ResNet utilizes the efﬁcient bottleneck structure to achieve impressive performance.
SENet introduces an architectural unit that boosts performance at
slight computation cost.
Concurrent with us, a very re-
 
Figure 1. Channel shufﬂe with two stacked group convolutions. GConv stands for group convolution. a) two stacked convolution layers
with the same number of groups. Each output channel only relates to the input channels within the group. No cross talk; b) input and
output channels are fully related when GConv2 takes data from different groups after GConv1; c) an equivalent implementation to b) using
channel shufﬂe.
cent work employs reinforcement learning and model
search to explore efﬁcient model designs. The proposed
mobile NASNet model achieves comparable performance
with our counterpart ShufﬂeNet model (26.0% @ 564
MFLOPs vs. 26.3% @ 524 MFLOPs for ImageNet classiﬁcation error). But do not report results on extremely
tiny models (e.g. complexity less than 150 MFLOPs), nor
evaluate the actual inference time on mobile devices.
Group Convolution
The concept of group convolution,
which was ﬁrst introduced in AlexNet for distributing the model over two GPUs, has been well demonstrated its effectiveness in ResNeXt . Depthwise separable convolution proposed in Xception generalizes the
ideas of separable convolutions in Inception series .
Recently, MobileNet utilizes the depthwise separable convolutions and gains state-of-the-art results among
lightweight models. Our work generalizes group convolution and depthwise separable convolution in a novel form.
Channel Shufﬂe Operation
To the best of our knowledge, the idea of channel shufﬂe operation is rarely mentioned in previous work on efﬁcient model design, although
CNN library cuda-convnet supports “random sparse
convolution” layer, which is equivalent to random channel
shufﬂe followed by a group convolutional layer. Such “random shufﬂe” operation has different purpose and been seldom exploited later. Very recently, another concurrent work
 also adopt this idea for a two-stage convolution. However, did not specially investigate the effectiveness of
channel shufﬂe itself and its usage in tiny model design.
Model Acceleration
This direction aims to accelerate inference while preserving accuracy of a pre-trained model.
Pruning network connections or channels reduces redundant connections in a pre-trained model while
maintaining performance. Quantization 
and factorization are proposed in literature to reduce redundancy in calculations to speed up inference. Without modifying the parameters, optimized convolution algorithms implemented by FFT and other
methods decrease time consumption in practice. Distilling transfers knowledge from large models into small
ones, which makes training small models easier.
3. Approach
3.1. Channel Shufﬂe for Group Convolutions
Modern convolutional neural networks usually consist of repeated building blocks with the
same structure.
Among them, state-of-the-art networks
such as Xception and ResNeXt introduce efﬁcient
depthwise separable convolutions or group convolutions
into the building blocks to strike an excellent trade-off
between representation capability and computational cost.
However, we notice that both designs do not fully take the
1 × 1 convolutions (also called pointwise convolutions in
 ) into account, which require considerable complexity. For example, in ResNeXt only 3 × 3 layers are
equipped with group convolutions. As a result, for each
residual unit in ResNeXt the pointwise convolutions occupy
93.4% multiplication-adds (cardinality = 32 as suggested in
 ). In tiny networks, expensive pointwise convolutions
result in limited number of channels to meet the complexity
constraint, which might signiﬁcantly damage the accuracy.
To address the issue, a straightforward solution is to ap-
Figure 2. ShufﬂeNet Units. a) bottleneck unit with depthwise convolution (DWConv) ; b) ShufﬂeNet unit with pointwise group
convolution (GConv) and channel shufﬂe; c) ShufﬂeNet unit with stride = 2.
ply channel sparse connections, for example group convolutions, also on 1 × 1 layers. By ensuring that each convolution operates only on the corresponding input channel
group, group convolution signiﬁcantly reduces computation
However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels.
Fig 1 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group
only relate to the inputs within the group. This property
blocks information ﬂow between channel groups and weakens representation.
If we allow group convolution to obtain input data from
different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Speciﬁcally, for the feature map generated from the previous group layer, we can
ﬁrst divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efﬁciently and elegantly implemented by a channel shufﬂe operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has
g × n channels; we ﬁrst reshape the output channel dimension into (g, n), transposing and then ﬂattening it back as
the input of next layer. Note that the operation still takes
effect even if the two convolutions have different numbers
of groups. Moreover, channel shufﬂe is also differentiable,
which means it can be embedded into network structures for
end-to-end training.
Channel shufﬂe operation makes it possible to build
more powerful structures with multiple group convolutional
layers. In the next subsection we will introduce an efﬁcient
network unit with channel shufﬂe and group convolution.
3.2. ShufﬂeNet Unit
Taking advantage of the channel shufﬂe operation, we
propose a novel ShufﬂeNet unit specially designed for small
networks. We start from the design principle of bottleneck
unit in Fig 2 (a). It is a residual block. In its residual
branch, for the 3 × 3 layer, we apply a computational economical 3 × 3 depthwise convolution on the bottleneck
feature map. Then, we replace the ﬁrst 1 × 1 layer with
pointwise group convolution followed by a channel shufﬂe
operation, to form a ShufﬂeNet unit, as shown in Fig 2 (b).
The purpose of the second pointwise group convolution is
to recover the channel dimension to match the shortcut path.
For simplicity, we do not apply an extra channel shufﬂe operation after the second pointwise layer as it results in comparable scores. The usage of batch normalization (BN) 
and nonlinearity is similar to , except that we do not
use ReLU after depthwise convolution as suggested by .
As for the case where ShufﬂeNet is applied with stride, we
simply make two modiﬁcations (see Fig 2 (c)): (i) add a
3 × 3 average pooling on the shortcut path; (ii) replace the
element-wise addition with channel concatenation, which
makes it easy to enlarge channel dimension with little extra
computation cost.
Thanks to pointwise group convolution with channel
shufﬂe, all components in ShufﬂeNet unit can be computed efﬁciently. Compared with ResNet (bottleneck
design) and ResNeXt , our structure has less complexity under the same settings. For example, given the input
size c × h × w and the bottleneck channels m, ResNet
unit requires hw(2cm + 9m2) FLOPs and ResNeXt has
hw(2cm + 9m2/g) FLOPs, while our ShufﬂeNet unit requires only hw(2cm/g + 9m) FLOPs, where g means the
Output size
Output channels (g groups)
GlobalPool
Complexity
Table 1. ShufﬂeNet architecture. The complexity is evaluated with FLOPs, i.e. the number of ﬂoating-point multiplication-adds. Note that
for Stage 2, we do not apply group convolution on the ﬁrst pointwise layer because the number of input channels is relatively small.
Complexity
Classiﬁcation error (%)
ShufﬂeNet 1×
ShufﬂeNet 0.5×
ShufﬂeNet 0.25×
Table 2. Classiﬁcation error vs. number of groups g (smaller number represents better performance)
number of groups for convolutions. In other words, given
a computational budget, ShufﬂeNet can use wider feature
maps. We ﬁnd this is critical for small networks, as tiny
networks usually have an insufﬁcient number of channels
to process the information.
In addition, in ShufﬂeNet depthwise convolution only
performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we ﬁnd it difﬁcult to efﬁciently implement on lowpower mobile devices, which may result from a worse computation/memory access ratio compared with other dense
operations. Such drawback is also referred in , which has
a runtime library based on TensorFlow . In ShufﬂeNet
units, we intentionally use depthwise convolution only on
bottleneck in order to prevent overhead as much as possible.
3.3. Network Architecture
Built on ShufﬂeNet units, we present the overall Shuf-
ﬂeNet architecture in Table 1. The proposed network is
mainly composed of a stack of ShufﬂeNet units grouped
into three stages. The ﬁrst building block in each stage is applied with stride = 2. Other hyper-parameters within a stage
stay the same, and for the next stage the output channels are
doubled. Similar to , we set the number of bottleneck
channels to 1/4 of the output channels for each ShufﬂeNet
unit. Our intent is to provide a reference design as simple
as possible, although we ﬁnd that further hyper-parameter
tunning might generate better results.
In ShufﬂeNet units, group number g controls the connection sparsity of pointwise convolutions. Table 1 explores
different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged
(∼140 MFLOPs). Obviously, larger group numbers result
in more output channels (thus more convolutional ﬁlters) for
a given complexity constraint, which helps to encode more
information, though it might also lead to degradation for an
individual convolutional ﬁlter due to limited corresponding
input channels. In Sec 4.1.1 we will study the impact of this
number subject to different computational constrains.
To customize the network to a desired complexity, we
can simply apply a scale factor s on the number of channels. For example, we denote the networks in Table 1 as
”ShufﬂeNet 1×”, then ”ShufﬂeNet s×” means scaling the
number of ﬁlters in ShufﬂeNet 1× by s times thus overall
complexity will be roughly s2 times of ShufﬂeNet 1×.
4. Experiments
We mainly evaluate our models on the ImageNet 2012
classiﬁcation dataset . We follow most of the training settings and hyper-parameters used in , with two
exceptions: (i) we set the weight decay to 4e-5 instead of
Cls err. (%, no shufﬂe)
Cls err. (%, shufﬂe)
ShufﬂeNet 1x (g = 3)
ShufﬂeNet 1x (g = 8)
ShufﬂeNet 0.5x (g = 3)
ShufﬂeNet 0.5x (g = 8)
ShufﬂeNet 0.25x (g = 3)
ShufﬂeNet 0.25x (g = 8)
Table 3. ShufﬂeNet with/without channel shufﬂe (smaller number represents better performance)
1e-4 and use linear-decay learning rate policy (decreased
from 0.5 to 0); (ii) we use slightly less aggressive scale augmentation for data preprocessing. Similar modiﬁcations are
also referenced in because such small networks usually suffer from underﬁtting rather than overﬁtting. It takes
1 or 2 days to train a model for 3×105 iterations on 4 GPUs,
whose batch size is set to 1024. To benchmark, we compare
single crop top-1 performance on ImageNet validation set,
i.e. cropping 224×224 center view from 256× input image
and evaluating classiﬁcation accuracy. We use exactly the
same settings for all models to ensure fair comparisons.
4.1. Ablation Study
The core idea of ShufﬂeNet lies in pointwise group convolution and channel shufﬂe operation. In this subsection
we evaluate them respectively.
Pointwise Group Convolutions
To evaluate the importance of pointwise group convolutions, we compare ShufﬂeNet models of the same complexity whose numbers of groups range from 1 to 8.
the group number equals 1, no pointwise group convolution is involved and then the ShufﬂeNet unit becomes an
”Xception-like” structure. For better understanding, we
also scale the width of the networks to 3 different complexities and compare their classiﬁcation performance respectively. Results are shown in Table 2.
From the results, we see that models with group convolutions (g > 1) consistently perform better than the counterparts without pointwise group convolutions (g = 1).
Smaller models tend to beneﬁt more from groups. For example, for ShufﬂeNet 1× the best entry (g = 8) is 1.2%
better than the counterpart, while for ShufﬂeNet 0.5× and
0.25× the gaps become 3.5% and 4.4% respectively. Note
that group convolution allows more feature map channels
for a given complexity constraint, so we hypothesize that
the performance gain comes from wider feature maps which
help to encode more information. In addition, a smaller
network involves thinner feature maps, meaning it beneﬁts
more from enlarged feature maps.
Table 2 also shows that for some models (e.g. Shuf-
ﬂeNet 0.5×) when group numbers become relatively large
g = 8), the classiﬁcation score saturates or even
drops. With an increase in group number (thus wider feature maps), input channels for each convolutional ﬁlter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as
ShufﬂeNet 0.25× larger group numbers tend to better results consistently, which suggests wider feature maps bring
more beneﬁts for smaller models.
Channel Shufﬂe vs. No Shufﬂe
The purpose of shufﬂe operation is to enable cross-group
information ﬂow for multiple group convolution layers. Table 3 compares the performance of ShufﬂeNet structures
(group number is set to 3 or 8 for instance) with/without
channel shufﬂe. The evaluations are performed under three
different scales of complexity. It is clear that channel shuf-
ﬂe consistently boosts classiﬁcation scores for different settings. Especially, when group number is relatively large
(e.g. g = 8), models with channel shufﬂe outperform the
counterparts by a signiﬁcant margin, which shows the importance of cross-group information interchange.
4.2. Comparison with Other Structure Units
Recent leading convolutional units in VGG ,
ResNet , GoogleNet , ResNeXt and Xception have pursued state-of-the-art results with large models (e.g.
≥1GFLOPs), but do not fully explore lowcomplexity conditions. In this section we survey a variety
of building blocks and make comparisons with ShufﬂeNet
under the same complexity constraint.
For fair comparison, we use the overall network architecture as shown in Table 1. We replace the ShufﬂeNet units
in Stage 2-4 with other structures, then adapt the number of
channels to ensure the complexity remains unchanged. The
structures we explored include:
• VGG-like.
Following the design principle of VGG
net , we use a two-layer 3×3 convolutions as the
basic building block. Different from , we add a
Batch Normalization layer after each of the convolutions to make end-to-end training easier.
Complexity (MFLOPs)
Xception-like
ShufﬂeNet (ours)
32.4 (1×, g = 8)
41.6 (0.5×, g = 4)
52.7 (0.25×, g = 8)
Table 4. Classiﬁcation error vs. various structures (%, smaller number represents better performance). We do not report VGG-like structure
on smaller networks because the accuracy is signiﬁcantly worse.
Complexity (MFLOPs)
Cls err. (%)
1.0 MobileNet-224
ShufﬂeNet 2× (g = 3)
ShufﬂeNet 2× (with SE , g = 3)
0.75 MobileNet-224
ShufﬂeNet 1.5× (g = 3)
0.5 MobileNet-224
ShufﬂeNet 1× (g = 8)
0.25 MobileNet-224
ShufﬂeNet 0.5× (g = 4)
ShufﬂeNet 0.5× (shallow, g = 3)
Table 5. ShufﬂeNet vs. MobileNet on ImageNet Classiﬁcation
• ResNet. We adopt the ”bottleneck” design in our experiment, which has been demonstrated more efﬁcient
in . Same as , the bottleneck ratio1 is also 1 : 4.
• Xception-like. The original structure proposed in 
involves fancy designs or hyper-parameters for different stages, which we ﬁnd difﬁcult for fair comparison
on small models. Instead, we remove the pointwise
group convolutions and channel shufﬂe operation from
ShufﬂeNet (also equivalent to ShufﬂeNet with g = 1).
The derived structure shares the same idea of “depthwise separable convolution” as in , which is called
an Xception-like structure here.
• ResNeXt. We use the settings of cardinality = 16 and
bottleneck ratio = 1 : 2 as suggested in . We also
explore other settings, e.g. bottleneck ratio = 1 : 4,
and get similar results.
We use exactly the same settings to train these models.
Results are shown in Table 4. Our ShufﬂeNet models outperform most others by a signiﬁcant margin under different
complexities. Interestingly, we ﬁnd an empirical relationship between feature map channels and classiﬁcation accuracy. For example, under the complexity of 38 MFLOPs,
output channels of Stage 4 (see Table 1) for VGG-like,
ResNet, ResNeXt, Xception-like, ShufﬂeNet models are 50,
192, 192, 288, 576 respectively, which is consistent with
1In the bottleneck-like units (like ResNet, ResNeXt or ShufﬂeNet) bottleneck ratio implies the ratio of bottleneck channels to output channels.
For example, bottleneck ratio = 1 : 4 means the output feature map is 4
times the width of the bottleneck feature map.
the increase of accuracy. Since the efﬁcient design of Shuf-
ﬂeNet, we can use more channels for a given computation
budget, thus usually resulting in better performance.
Note that the above comparisons do not include
GoogleNet or Inception series . We ﬁnd it nontrivial to generate such Inception structures to small networks because the original design of Inception module involves too many hyper-parameters. As a reference, the ﬁrst
GoogleNet version has 31.3% top-1 error at the cost of
1.5 GFLOPs (See Table 6). More sophisticated Inception
versions are more accurate, however, involve signiﬁcantly increased complexity. Recently, Kim et al. propose a lightweight network structure named PVANET 
which adopts Inception units. Our reimplemented PVANET
(with 224×224 input size) has 29.7% classiﬁcation error
with a computation complexity of 557 MFLOPs, while our
ShufﬂeNet 2x model (g = 3) gets 26.3% with 524 MFLOPs
(see Table 6).
4.3. Comparison with MobileNets and Other
Frameworks
Recently Howard et al. have proposed MobileNets 
which mainly focus on efﬁcient network architecture for
mobile devices. MobileNet takes the idea of depthwise separable convolution from and achieves state-of-the-art
results on small models.
Table 5 compares classiﬁcation scores under a variety of
complexity levels. It is clear that our ShufﬂeNet models are
superior to MobileNet for all the complexities. Though our
ShufﬂeNet network is specially designed for small models
(< 150 MFLOPs), we ﬁnd it is still better than MobileNet
Cls err. (%)
Complexity (MFLOPs)
VGG-16 
ShufﬂeNet 2× (g = 3)
GoogleNet *
ShufﬂeNet 1× (g = 8)
AlexNet 
SqueezeNet 
ShufﬂeNet 0.5× (g = 4)
Table 6. Complexity comparison. *Implemented by BVLC ( googlenet)
mAP [.5, .95] (300× image)
mAP [.5, .95] (600× image)
ShufﬂeNet 2× (g = 3)
ShufﬂeNet 1× (g = 3)
1.0 MobileNet-224 
1.0 MobileNet-224 (our impl.)
Table 7. Object detection results on MS COCO (larger numbers represents better performance). For MobileNets we compare two results:
1) COCO detection scores reported by ; 2) ﬁnetuning from our reimplemented MobileNets, whose training and ﬁnetuning settings are
exactly the same as that for ShufﬂeNets.
Cls err. (%)
720 × 1280
ShufﬂeNet 0.5× (g = 3)
ShufﬂeNet 1× (g = 3)
ShufﬂeNet 2× (g = 3)
AlexNet 
1.0 MobileNet-224 
Actual inference time on mobile device (smaller number represents better performance). The platform is based on a single
Qualcomm Snapdragon 820 processor. All results are evaluated with single thread.
for higher computation cost, e.g. 3.1% more accurate than
MobileNet 1× at the cost of 500 MFLOPs. For smaller
networks (∼40 MFLOPs) ShufﬂeNet surpasses MobileNet
by 7.8%. Note that our ShufﬂeNet architecture contains 50
layers while MobileNet only has 28 layers. For better understanding, we also try ShufﬂeNet on a 26-layer architecture by removing half of the blocks in Stage 2-4 (see ”Shuf-
ﬂeNet 0.5× shallow (g = 3)” in Table 5). Results show that
the shallower model is still signiﬁcantly better than the corresponding MobileNet, which implies that the effectiveness
of ShufﬂeNet mainly results from its efﬁcient structure, not
the depth.
Table 6 compares our ShufﬂeNet with a few popular
models. Results show that with similar accuracy ShufﬂeNet
is much more efﬁcient than others.
For example, Shuf-
ﬂeNet 0.5× is theoretically 18× faster than AlexNet 
with comparable classiﬁcation score. We will evaluate the
actual running time in Sec 4.5.
It is also worth noting that the simple architecture design makes it easy to equip ShuffeNets with the latest advances such as . For example, in the authors
propose Squeeze-and-Excitation (SE) blocks which achieve
state-of-the-art results on large ImageNet models. We ﬁnd
SE modules also take effect in combination with the backbone ShufﬂeNets, for instance, boosting the top-1 error of
ShufﬂeNet 2× to 24.7% (shown in Table 5). Interestingly,
though negligible increase of theoretical complexity, we
ﬁnd ShufﬂeNets with SE modules are usually 25 ∼40%
slower than the “raw” ShufﬂeNets on mobile devices, which
implies that actual speedup evaluation is critical on low-cost
architecture design. In Sec 4.5 we will make further discussion.
4.4. Generalization Ability
To evaluate the generalization ability for transfer learning, we test our ShufﬂeNet model on the task of MS COCO
object detection . We adopt Faster-RCNN as the
detection framework and use the publicly released Caffe
code for training with default settings. Similar to
 , the models are trained on the COCO train+val dataset
excluding 5000 minival images and we conduct testing on
the minival set. Table 7 shows the comparison of results
trained and evaluated on two input resolutions. Comparing
ShufﬂeNet 2× with MobileNet whose complexity are comparable (524 vs. 569 MFLOPs), our ShufﬂeNet 2× surpasses MobileNet by a signiﬁcant margin on both resolutions; our ShufﬂeNet 1× also achieves comparable results
with MobileNet on 600× resolution, but has ∼4× complexity reduction. We conjecture that this signiﬁcant gain
is partly due to ShufﬂeNet’s simple design of architecture
without bells and whistles.
4.5. Actual Speedup Evaluation
Finally, we evaluate the actual inference speed of Shuf-
ﬂeNet models on a mobile device with an ARM platform.
Though ShufﬂeNets with larger group numbers (e.g. g = 4
or g = 8) usually have better performance, we ﬁnd it less
efﬁcient in our current implementation. Empirically g = 3
usually has a proper trade-off between accuracy and actual
inference time. As shown in Table 8, three input resolutions
are exploited for the test. Due to memory access and other
overheads, we ﬁnd every 4× theoretical complexity reduction usually results in ∼2.6× actual speedup in our implementation. Nevertheless, compared with AlexNet 
our ShufﬂeNet 0.5× model still achieves ∼13× actual
speedup under comparable classiﬁcation accuracy (the theoretical speedup is 18×), which is much faster than previous AlexNet-level models or speedup approaches such as
 .