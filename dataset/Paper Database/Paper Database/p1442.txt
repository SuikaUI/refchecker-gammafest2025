Steering Self-Supervised Feature Learning Beyond Local Pixel Statistics
Simon Jenni1
Hailin Jin2
Paolo Favaro1
University of Bern1
Adobe Research2
{simon.jenni,paolo.favaro}@inf.unibe.ch
 
We introduce a novel principle for self-supervised feature learning based on the discrimination of speciﬁc transformations of an image.
We argue that the generalization capability of learned features depends on what image
neighborhood size is sufﬁcient to discriminate different image transformations: The larger the required neighborhood
size and the more global the image statistics that the feature can describe. An accurate description of global image
statistics allows to better represent the shape and conﬁguration of objects and their context, which ultimately generalizes better to new tasks such as object classiﬁcation and
detection. This suggests a criterion to choose and design
image transformations. Based on this criterion, we introduce a novel image transformation that we call limited context inpainting (LCI). This transformation inpaints an image
patch conditioned only on a small rectangular pixel boundary (the limited context). Because of the limited boundary
information, the inpainter can learn to match local pixel
statistics, but is unlikely to match the global statistics of the
image. We claim that the same principle can be used to justify the performance of transformations such as image rotations and warping. Indeed, we demonstrate experimentally
that learning to discriminate transformations such as LCI,
image warping and rotations, yields features with state of
the art generalization capabilities on several datasets such
as Pascal VOC, STL-10, CelebA, and ImageNet. Remarkably, our trained features achieve a performance on Places
on par with features trained through supervised learning
with ImageNet labels.
1. Introduction
The top-performance approaches to solve vision-based
tasks, such as object classiﬁcation, detection and segmentation, are currently based on supervised learning. Unfortunately, these methods achieve a high-performance only
through a large amount of labeled data, whose collection
is costly and error-prone. Learning through labels may also
encounter another fundamental limitation, depending on the
Figure 1: The importance of global image statistics. Top
row: Natural images. Bottom row: Images transformed
such that local statistics are preserved while global statistics
are signiﬁcantly altered.1An accurate image representation
should be able to distinguish these two categories. A linear
binary classiﬁer trained to distinguish original versus transformed images on top of conv5 features pre-trained on
ImageNet labels yields an accuracy of 78%. If instead we
use features pre-trained with our proposed self-supervised
learning task the classiﬁer achieves an accuracy of 85%.
Notice that this transformation was not used in the training
of our features and that the transformed images were built
independently of either feature.
training procedure and dataset: It might yield features that
describe mostly local statistics, and thus have limited generalization capabilities. An illustration of this issue is shown
in Fig. 1. On the bottom row we show images that have been
transformed such that local statistics of the corresponding
image on the top row are preserved, but global statistics are
not. We ﬁnd experimentally that features pre-trained with
ImageNet labels have difﬁculties in telling real images
apart from the transformed ones. This simple test shows that
the classiﬁcation task in ImageNet could be mostly solved
by focusing on local image statistics. Such problem might
not be noticed when evaluating these features on other tasks
and datasets that can be solved based on similar local statistics. However, more general classiﬁcation settings would
certainly expose such a limitation. also pointed out this
problem and showed that training supervised models to focus on the global statistics (which they refer to as shape) can
improve the generalization and the robustness of the learned
1The transformed images are obtained by partitioning an image into a
4 × 4 grid, by randomly permuting the tiles, and by training a network to
inpaint a band of pixels across the tiles through adversarial training .
 
Figure 2: Selected image transformations. Examples of
local patches from images that were (a) warped, (b) locally
inpainted, (c) rotated or (d) not transformed. The bottom
row shows the original images, the middle row shows the
corresponding transformed images and the top row shows a
detail of the transformed image. By only observing a local
patch (top row), is it possible in all of the above cases to tell
if and how an image has been transformed or is it instead
necessary to observe the whole image (middle row), i.e.,
the global pixel statistics?
image representation.
Thus, to address this fundamental shortcoming and to
limit the need for human annotation, we propose a novel
self-supervised learning (SSL) method. SSL methods learn
features without manual labeling and thus they have the
potential to better scale their training and leverage large
amounts of existing unlabeled data. The training task in our
method is to discriminate global image statistics. To this
end, we transform images in such a way that local statistics
are largely unchanged, while global statistics are clearly altered. By doing so, we make sure that the discrimination
of such transformations is not possible by working on just
local patches, but instead it requires using the whole image.
We illustrate this principle in Fig. 2. Incidentally, several
existing SSL tasks can be seen as learning from such transformations, e.g., spotting artifacts , context prediction
 , rotation prediction , and solving jigsaw puzzles
We cast our self-supervised learning approach as the task
of discriminating changes in the global image statistics by
classifying several image transformations (see Fig. 3). As
a novel image transformation we introduce limited context
inpainting (LCI). LCI selects a random patch from a natural
image, substitutes the center with noise (thus, it preserves a
small outer boundary of pixels), and trains a network to inpaint a realistic center through adversarial training. While
CNN Classifier
Figure 3: Learning global statistics. We propose to learn
image representations by training a convolutional neural
network to classify image transformations. The transformations are chosen such that local image statistics are preserved while global statistics are distinctly altered.
LCI can inpaint a realistic center of the patch so that it seamlessly blends with the preserved boundaries, it is unlikely to
provide a meaningful match with the rest of the original image. Hence, this mismatch can only be detected by learning
global statistics of the image. Our formulation is also highly
scalable and allows to easily incorporate more transformations as additional categories. In fact, we also include the
classiﬁcation of image warping and image rotations (see examples of such transformations in Fig. 2). An illustration of
the proposed training scheme is shown in Fig. 3.
Contributions.
Our proposed method has the following original contributions: 1) We introduce a novel selfsupervised learning principle based on image transformations that can be detected only through global observations;
2) We introduce a novel transformation according to this
principle and demonstrate experimentally its impact on feature learning; 3) We formulate the method so that it can
easily scale with additional transformations; 4) Our proposed method achieves state of the art performance in transfer learning on several data sets; in particular, for the ﬁrst
time, we show that our trained features when transferred to
Places achieve a performance on par with features trained
through supervised learning with ImageNet labels. Code is
available at 
2. Prior Work
Self-supervised Learning. Self-supervised learning is a
feature learning method that avoids the use of data labels
by introducing an artiﬁcial task. Examples of tasks deﬁned
on images are to ﬁnd: the spatial conﬁguration of parts
 , the color of a grayscale image ,
the image patch given its context , the image orientation , the artifacts introduced by a corruption process
 , the image instance up to data jittering ,
contrastive predictive coding or pseudo-labels obtained from a clustering process . Self-supervised
learning has also been applied to other data domains such as
video and audio .
Several self-supervised tasks can be seen as the prediction of some form of image transformation applied to an
image. Gidaris et al. for example predict the number
of 90◦rotations applied to an image. Jenni and Favaro 
predict the presence and position of artifacts introduced by
a corruption process. Doersch et al. predict transformations concerning image patches by predicting their relative
location. Noroozi and Favaro extend this idea to multiple patches by solving jigsaw puzzles. Recently Zhang et
al. proposed to predict the parameters of a relative projective transformation between two images using a Siamese
architecture. In our work, we show that by predicting a
combination of novel and previously explored image transformations we can form new and more challenging learning
tasks that learn better features.
Some works have explored the combination of different
self-supervised tasks via multi-task learning . Recently, Feng et al. showed that a combination of the rotation prediction task by Gidaris et al. with the instance
recognition task by Wu et al. achieve state-of-the-art
results in transfer experiments.
They do so by splitting
the penultimate feature vector into two parts: One to predict the transformation and a second transformation agnostic part, used to discriminate between different training images. Note that our work is orthogonal to these approaches
and thus it could be integrated in such multi-task formulations and would likely lead to further improvements.
Because in our LCI transformation we build an inpainting network through adversarial training, we brieﬂy discuss
works that exploit similar techniques.
Adversarial Feature Learning.
Generative Adversarial
Networks (GANs) have been used for the purpose of
representation learning in several works. Radford et al. 
ﬁrst showed that a convolutional discriminator can learn
reasonably good features. Donahue et al. learn features by training an encoder to produce the inverse mapping
of the generator. Pathak et al. use an adversarial loss
to train an autoencoder for inpainting. They use the trained
encoder as a feature extractor. Denton et al. also perform inpainting, but instead transfer the discriminator features. The work by Jenni and Favaro has some similarity to our LCI transformation. They generate image artifacts by erasing and locally repairing features of an autoencoder. Our limited context inpainting is different from these
methods in two important ways. First, we more strongly
limit the context of the inpainter and put the inpainted patch
back into a larger context to produce unrealistic global image statistics. Second, a separate patch discriminator allows
stable adversarial training independent of the feature learning component.
Recognizing Image Manipulations.
Many works have
considered the detection of image manipulations in the context of image forensics . For example, Wang
et al. predict subtle face image manipulations based
on local warping. Zhou et al. detect image tampering
generated using semantic masks. Transformations in these
cases are usually subtle and do not change the global image
statistics in a predictable way (images are manipulated to
appear realistic). The aim is therefore antithetical to ours.
3. Learning Features by Discriminating Global
Image Transformations
Our aim is to learn image representations without human
annotation by recognizing variations in global image statistics. We do so by distinguishing between natural images
and images that underwent several different image transformations. Our principle is to choose image transformations that: 1) Preserve local pixel statistics (e.g., texture),
but alter the global image statistics of an image and 2) Can
be recognized from a single transformed example in most
cases. In this paper we choose the following transformations: limited context inpainting, warping, rotations and the
identity. These transformations will be introduced in detail
in the next sections.
given a set of unlabelled training images {xi}i=1,...,N and a set of image transformations
{Tj}j=0,...,K, we train a classiﬁer C to predict the
transformation-label j given a transformed example Tj ◦xi.
In our case we set K = 5. We include the identity (notransformation) case by letting T0 ◦x .= x. We train the
network C by minimizing the following self-supervised objective
LSSL(T0, . . . , T5) .= min
where ℓcls is the standard cross-entropy loss for a multi-class
classiﬁcation problem.
3.1. Limited Context Inpainting
The ﬁrst transformation that we propose to use in eq. (1)
is based on the Limited Context Inpainting (LCI). The aim
Patch-Inpainter
Patch-Discriminator
Classifier
Figure 4: Training of the Limited Context Inpainting (LCI) network. A random patch is extracted from a training image
x and all but a thin border of pixels is replaced by random noise. The inpainter network F ﬁlls the patch with realistic textures
conditioned on the remaining border pixels. The resulting patch is replaced back into the original image, thus generating an
image with natural local statistics, but unnatural global statistics.
of LCI is to modify images only locally, i.e., at the scale
of image patches. We train an inpainter network F conditioned only on a thin border of pixels of the patch (see
Fig. 4). The inpainted patch should be realistic on its own
and blend in at the boundary with the surrounding image,
but should not meaningfully match the content of the whole
image (see an example in Fig. 2 (b)). The inpainter F is
trained using adversarial training against a patch discriminator D (which ensures that we match the local statistics)
as well as the transformation classiﬁer C. The patch to be
inpainted is randomly selected at a uniformly sampled location ∆∈Ω, where Ωis the image domain. Then, W∆⊂Ω
is a square region of pixels around ∆. We deﬁne ei as the
original patch of pixels at W∆and ri as the corresponding
inpainted patch
ei(p −∆) .= xi(p),
ri .= F(ei ⊙(1 −m) + z ⊙m)
with m a mask that is 1 in the center of the patch and 0 at
the boundary (2 to 4 pixels in our baseline), z ∼N(0, I) is
a zero-mean Gaussian noise and ⊙denotes the Hadamard
(pixel-to-pixel) product. The LCI transformation T5 is then
(T5 ◦xi)(p) .=
Finally, to train the inpainter F we minimize the cost
ℓGAN(ri, ei) + λborder |(ri −ei) ◦(1 −m)|2
−LSSL(T0, . . . , T5),
where λborder = 50 is a tuning parameter to regulate the importance of autoencoding the input boundary, and ℓGAN(·, ·)
is the hinge loss for adversarial training , which also
includes the maximization in the discriminator D.
Remark. In contrast to prior SSL methods , here
we do not take the features from the networks that we used
to learn the transformation (e.g., D or F). Instead, here we
take features from a separate classiﬁer C that has only a
partial role in the training of F. This separation has several
advantages: 1) A separate tuning of training parameters is
possible, 2) GAN tricks can be applied without affecting the
classiﬁer C, (3) GAN training can be stable even when the
classiﬁer wins (LSSL saturates w.r.t. F).
3.2. Random Warping
In addition to the LCI, which is a local image transformation, we consider random global warping as our
T4 transformation.
A warping is a smooth deformation
of the image coordinates deﬁned by n pixel coordinates
{(ui, vi)}i=1,...,n, which act as control points. We place
the control points on an uniform grid of the image domain
and then randomly offset each control point by sampling the
shifts from a rectangular range [−d, d]×[−d, d], where d is
typically 1/10-th of the image size. The dense ﬂow ﬁeld for
warping is then computed by interpolating between the offsets at the control points using a polyharmonic spline .
Warping affects the local image statistics only minimally:
In general, it is difﬁcult to distinguish a warped patch from
a patch undergoing a change in perspective. Therefore, the
classiﬁer needs to learn global image statistics to detect image warping.
3.3. Image Rotations
Finally, we consider as T1, T2, and T3 image rotations of
90◦, 180◦, and 270◦respectively. This choice is inspired by
Gidaris et al. who proposed RotNet, a network to predict image rotations by multiples of 90◦. This was shown to
be a simple yet effective SSL pretext task. These transformations are predictable because the photographer bias introduces a canonical reference orientation for many natural
images. They also require global statistics as local patches
Figure 5: Image statistics on CelebA. (a) The mean image
obtained from 8000 samples from CelebA. (b) Four local
patches extracted from the mean image. Because these patterns appear always with the same orientation in the dataset,
it is possible to distinguish rotated images by using only
these local statistics.
of rotated images often do not indicate the orientation of the
image, because similar patches can be found in the untransformed dataset.
Remark. There exist, however, several settings in which
the prediction of image rotations does not result in good
Many natural images for example do not have
a canonical image orientation.
Thus, in these cases the
prediction of image rotations is an ill-posed task. There
also exist entire data domains of interest, where the image
orientation is ambiguous, such as satellite and cell imaging datasets. Even when a clear upright image orientation
exists, this method alone can lead to non-optimal feature
learning. As an example, we show that the prediction of image rotations on CelebA , a dataset of face images, leads
to signiﬁcantly worse features than can be learned through
the prediction of other transformations (see Table 3). The
main reason behind this limitation is that local patches can
be found in the dataset always with the same orientation (see
Fig. 5). For instance, the classiﬁer can easily distinguish rotated faces by simply detecting one eye or the mouth.
3.4. Preventing Degenerate Learning
As was observed by Doersch et al. , networks trained
to solve a self-supervised task might do so by using very local statistics (e.g., localization by detecting the chromatic
aberration).
Such solutions are called shortcuts and are
a form of degenerate learning as they yield features with
poor generalization capabilities. When introducing artiﬁcial tasks, such as the discrimination of several image transformations, it is important to make sure that the trained network cannot exploit (local) artifacts introduced by the transformations to solve the task. For example, the classiﬁer
could learn to recognize processing artifacts of the inpainter
F in order to recognize LCI transformed images. Although
adversarial training should help to prevent this behavior, we
ﬁnd experimentally that it is not sufﬁcient on its own. To
further prevent such failure cases, we also train the network
F to autoencode image patches by modifying the loss Linp
in eq. (5) as Linp,AE = Linp + λAE 1
i=1 |F(ei) −ei|2,
where λAE = 50 is a tuning parameter to regulate the importance of autoencoding image patches. We create also
artiﬁcial untransformed images by substituting a random
patch with its autoencoded version. In each mini-batch to
the classiﬁer we replace half of the untransformed images
with these patch-autoencoded images. In this manner the
classiﬁer will not focus on the small artifacts (which could
even be not visible to the naked eye) as a way to discriminate the transformations. During training we also replace
half of the original images in a minibatch with these patchautoencoded images before applying the rotation.
4. On the Choice of Transformations
Our goal is to learn features by discriminating images
undergoing different transformations. We pointed out that
this approach should use transformations that can be distinguished only by observing large regions of pixels, and is
scalable, i.e., it can be further reﬁned by including more
transformations.
In this section, we would like to make
these two aspects clearer.
Determining suitable transformations. We ﬁnd that the
choice of what transformations to use depends on the data
distribution. An example of such dependency in the case of
RotNet on CelebA is shown in Fig. 5. Intuitively, an ideal
transformation is such that any transformed local patch
should be found in the original dataset, but any transformed
global patch should not be found in the dataset. This is also
the key idea behind the design of LCI.
Introducing additional transformations. As we will show
in the Experiments section, adding more transformations (as
speciﬁed above) can improve the performance. An important aspect is that the classiﬁer must be able to distinguish
the different transformations.
Otherwise, its task is ambiguous and can lead to degenerate learning. Put in simple
terms, a transformed global patch should be different from
any other global patch (including itself) transformed with a
different transformation. We verify that our chosen transformations satisfy this principle, as LCI and image warping
cannot produce rotated images and warping is a global deformation, while LCI is a local one.
5. Experiments
We perform an extensive experimental evaluation of
our formulation on several established unsupervised feature learning benchmarks. For a fair comparison with prior
work we implement the transformation classiﬁer C with a
standard AlexNet architecture . Following prior work,
we remove the local response normalization layers and add
batch normalization to all layers except for the ﬁ-
Table 1: Ablation experiments for different design choices
of Limited Context Inpainting (LCI) on STL-10 . We
pre-train an AlexNet to predict if an image has been transformed with LCI or not and transfer the frozen conv5 features for linear classiﬁcation.
(a) 32 × 32 patches
(b) 40 × 40 patches
(c) 56 × 56 patches
(d) Pre-trained and frozen F
(e) No adversarial loss w.r.t. C
(f) No patch autoencoding
Baseline (48 × 48 patches )
nal one. No other modiﬁcations to the original architecture were made (we preserve the two-stream architecture).
For experiments on lower resolution images we remove the
max-pooling layer after conv5 and use SAME padding
throughout the network. The standard data-augmentation
strategies (random cropping and horizontal ﬂipping) were
used. Self-supervised pre-training of the classiﬁer was performed using the AdamW optimizer with parameters
β1 = 0.5, β2 = 0.99 and a weight decay of 10−4. We decayed the learning rate from 3 · 10−4 to 3 · 10−7 over the
course of training using cosine annealing . The training
of the inpainter network F and patch-discriminator D was
done using the Adam optimizer with a ﬁxed learning
rate of 2 · 10−4 and β1 = 0.5. The size of the patch boundary is set to 2 pixels in experiments on STL-10 and CelebA.
On ImageNet we use a 4 pixel boundary. Details for the
network architectures and additional results are provided in
the supplementary material.
5.1. Ablation Experiments
Limited Context Inpainting. We perform ablation experiments on STL-10 to validate several design choices for
the joint inpainter and classiﬁer training.
We also illustrate the effect of the patch-size on the performance of the
learned features. We pre-train the transformation classiﬁer
for 200 epochs on 64 × 64 crops of the unlabelled training
set. The mini-batch size was set to 64. We then transfer the
frozen conv5 features by training a linear classiﬁer for 500
epochs on randomly cropped 96 × 96 images of the small
labelled training set. Only LCI was used as transformation
in these experiments. The results of the following ablations
are reported in Table 1:
(a)-(c) Varying Patch-Size: We vary the size of the inpainted patches. We observe that small patches lead
to a signiﬁcant drop in feature performance. Smaller
patches are easy to inpaint and the results often do not
alter the global image statistics;
Table 2: We report the test set accuracy of linear classiﬁers
trained on frozen features for models trained to predict different combinations of image transformations on STL-10.
Initialization
Warp + LCI
Rot + Warp
Rot + Warp + LCI
Table 3: We report the average precision of linear classi-
ﬁers trained to predict facial attributes on frozen features of
models trained to predict different combinations of image
transformations on CelebA.
Initialization
Warp + LCI
Rot + Warp
Rot + Warp + LCI
(d)-(f) Preventing Shortcuts: Following
show how adversarial training of F is necessary to
achieve a good performance by removing the feedback
of both D and C in (d) and only C in (e). We also
demonstrate the importance of adding autoencoded
patches to the non-transformed images in (f);
Combination of Image Transformations. We perform additional ablation experiments on STL-10 and CelebA 
where C is trained to predict different combinations of image transformations. These experiments illustrate how our
formulation can scale with the number of considered image transformations and how the effectiveness of transformations can depend on the data domain.
We pre-train the AlexNet to predict image transformations for 200 epochs on 64 × 64 crops on STL-10 and for
100 epochs on 96 × 96 crops on CelebA using the standard
data augmentations. For transfer we train linear classiﬁers
on top of the frozen convolutional features (without resizing of the feature-maps) to predict the 10 object categories
in the case of STL-10 and to predict the 40 face attributes in
the case of CelebA. Transfer learning is performed for 700
epochs on 64 × 64 crops in the case of STL-10 and for 100
Table 4: Transfer learning results for classiﬁcation, detection and segmentation on PASCAL compared to state-ofthe-art feature learning methods (* use a bigger AlexNet).
Classiﬁcation Detection Segmentation
Krizhevsky et al. 
Agrawal et al. 
Bojanowski et al. 
Donahue et al. 
Feng et al. 
Gidaris et al. 
Jayaraman & Grauman 
Jenni & Favaro 
Kr¨ahenb¨uhl et al. 
Larsson et al. 
Noroozi & Favaro 
Noroozi et al. 
Noroozi et al. 
Mahendran et al. 
Mundhenk et al. 
Owens et al. 
Pathak et al. 
Pathak et al. 
Wang & Gupta 
Zhan et al. 
Zhang et al. 
Zhang et al. 
Doersch et al. *
Caron et al. *
epochs on 96 × 96 crops in the case of CelebA. We report
results for STL-10 in Table 2 and for CelebA in Table 3.
We can observe that the discrimination of a larger number of image transformations generally leads to better feature performance on both datasets. When considering each
of the transformations in isolation we see that not all of
them generalize equally well to different data domains. Rotation prediction especially performs signiﬁcantly worse on
CelebA than on STL-10. The performance of LCI on the
other hand is good on both datasets.
5.2. Unsupervised Feature Learning Benchmarks
We compare our proposed model to state-of-the-art
methods on the established feature learning benchmarks.
We pre-train the transformation classiﬁer for 200 epochs on
the ImageNet training set. Images were randomly cropped
to 128 × 128 and the last max-pooling layer was removed
during pre-training to preserve the size of the feature map
before the fully-connected layers. We used a batch-size of
96 and trained on 4 GPUs.
Pascal VOC. We ﬁnetune our transformation classiﬁer features for multi-label classiﬁcation, object detection and semantic segmentation on the Pascal VOC dataset. We follow
the established experimental setup and use the framework
Table 5: Validation set accuracy on ImageNet with linear
classiﬁers trained on frozen convolutional layers.
† indicates multi-crop evaluation and * use a bigger AlexNet.
Model\Layer
ImageNet Labels
Donahue et al. 
Feng et al. 
Gidaris et al. 
Huang et al. 
Jenni & Favaro 
Noroozi & Favaro 
Noroozi et al. 
Noroozi et al. 
Tian et al. 
Wu et al. 
Zhang et al. 
Zhang et al. 
Zhang et al. 
Doersch et al. *
Caron et al. *
Zhuang et al. *†
Table 6: Validation set accuracy on Places with linear classiﬁers trained on frozen convolutional layers.
† indicates
multi-crop evaluation and * the use of a bigger AlexNet.
Model\Layer
Places Labels
ImageNet Labels
Donahue et al. 
Feng et al. 
Gidaris et al. 
Jenni & Favaro 
Noroozi & Favaro 
Noroozi et al. 
Noroozi et al. 
Owens et al. 
Pathak et al. 
Wu et al. 
Zhang et al. 
Zhang et al. 
Zhang et al. 
Doersch et al. *
Caron et al. *
Zhuang et al. *†
provided by Kr¨ahenb¨uhl et al. for multilabel classiﬁcation, the Fast-RCNN framework for detection and
the FCN framework for semantic segmentation. We
absorb the batch-normalization parameters into the param-
We report leave-one-out cross validation
(LOOCV) accuracy for k-nearest neighbor classiﬁers on the
Places validation set. We compare the performance of our
self-supervised transformation classiﬁer against features of
a supervised network for different values of k. Both networks were pre-trained on ImageNet.
eters of the associated layers in the AlexNet and apply the
data-dependent rescaling by Kr¨ahenb¨uhl et al. , as is
common practice. The results of these transfer learning experiments are reported in Table 4. We achieve state-of-theart performance in classiﬁcation and competitive results for
detection and segmentation.
Linear Classiﬁer Experiments on ImageNet and Places.
To measure the quality of our self-supervised learning task
we use the transformation classiﬁer as a ﬁxed feature extractor and train linear classiﬁers on top of each convolutional layer. These experiments are performed both on ImageNet (the dataset used for pre-training) and Places (to
measure how well the features generalize to new data). We
follow the same setup as the state-of-the-art methods and
report the accuracy achieved on a single crop. Results for
ImageNet are shown in Table 5 and for Places in Table 6.
Our learned features achieve state-of-the-art performance
for conv1, conv2 and conv4 on ImageNet. On Places
we achieve the best results on conv1, conv3 and conv4.
Our results on conv4 in particular are the best overall and
even slightly surpass the performance of an AlexNet trained
on ImageNet using supervision.
Nearest Neighbor Evaluation. Features learned in deep
CNNs through supervised learning tend to distribute so that
their Euclidean distance relates closely to the semantic visual similarity of the images they correspond to. We want
to see if also our SSL features enjoy the same property.
Thus, we compute the nearest neighbors of our SSL and
of SL features in conv5 features space on the validation
set of ImageNet. Results are shown in Fig. 7. We also show
a quantitative comparison of k-nearest neighbor classiﬁcation on the Places validation set in Figure 6. We report the
leave-one-out cross validation (LOOCV) accuracy for different values of k. This can be done efﬁciently by comput-
Figure 7: Comparison of nearest neighbor retrieval. The
left-most column shows the query image. Odd rows: Retrievals with our features.
Even rows: Retrievals with
features learned using ImageNet labels.
Nearest neighbors were computed on the validation set of ImageNet with
conv5 features using cosine similarity.
ing (k+1)-nearest neighbors using the complete dataset and
by excluding the closest neighbor for each query. The concatenation of features from ﬁve 128 × 128 crops (extracted
at the resolution the networks were trained on) is used for
nearest neighbors. The features are standardized and cosine
similarity is used for nearest neighbor computation.
6. Conclusions
We introduced the self-supervised feature learning task
of discriminating natural images from images transformed
through local inpainting (LCI), image warping and rotations, based on the principle that trained features generalize
better when their task requires detecting global natural image statistics. This principle is supported by substantial experimental evaluation: Trained features achieve SotA performance on several transfer learning benchmarks (Pascal
VOC, STL-10, CelebA, and ImageNet) and even slightly
outperform supervised training on Places.
Acknowledgements.
This work was supported by the
Swiss National Science Foundation (SNSF) grant number
200021 169622 and an Adobe award.