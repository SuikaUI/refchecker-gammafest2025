Deep Weakly-supervised Anomaly Detection
Guansong Pangâˆ—
Singapore Management University
Singapore, Singapore
 
Chunhua Shen
Zhejiang University
Hangzhou, China
 
Huidong Jin
Canberra, Australia
 
Anton van den Hengel
University of Adelaide
Adelaide, Australia
 
Recent semi-supervised anomaly detection methods that are trained
using small labeled anomaly examples and large unlabeled data
(mostly normal data) have shown largely improved performance
over unsupervised methods. However, these methods often focus on
fitting abnormalities illustrated by the given anomaly examples only
(i.e., seen anomalies), and consequently they fail to generalize to
those that are not, i.e., new types/classes of anomaly unseen during
training. To detect both seen and unseen anomalies, we introduce a
novel deep weakly-supervised approach, namely Pairwise Relation
prediction Network (PReNet), that learns pairwise relation features
and anomaly scores by predicting the relation of any two randomly
sampled training instances, in which the pairwise relation can
be anomaly-anomaly, anomaly-unlabeled, or unlabeled-unlabeled.
Since unlabeled instances are mostly normal, the relation prediction
enforces a joint learning of anomaly-anomaly, anomaly-normal,
and normal-normal pairwise discriminative patterns, respectively.
PReNet can then detect any seen/unseen abnormalities that fit the
learned pairwise abnormal patterns, or deviate from the normal
patterns. Further, this pairwise approach also seamlessly and significantly augments the training anomaly data. Empirical results on
12 real-world datasets show that PReNet significantly outperforms
nine competing methods in detecting seen and unseen anomalies.
We also theoretically and empirically justify the robustness of our
model w.r.t. anomaly contamination in the unlabeled data. The code
is available at 
CCS CONCEPTS
â€¢ Computing methodologies â†’Anomaly detection; Neural
Anomaly Detection, Deep Learning, Intrusion Detection
âˆ—Corresponding author: G. Pang
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from .
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
Â© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0103-0/23/08...$15.00
 
ACM Reference Format:
Guansong Pang, Chunhua Shen, Huidong Jin, and Anton van den Hengel.
2023. Deep Weakly-supervised Anomaly Detection. In Proceedings of the
29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™23), August 6â€“10, 2023, Long Beach, CA, USA. ACM, New York, NY,
USA, 13 pages. 
INTRODUCTION
Anomaly detection (AD) aims at identifying exceptional data instances that deviate significantly from the majority of data. It is of
critical practical importance due to its broad applications in defensing against cyber-crimes (e.g., network intrusions), user misbehavior (e.g., fraudulent user accounts/reviews), web advertising abuses,
and adverse drug reactions, to name a few . Numerous
AD methods have been introduced, most of which are unsupervised methods working on entirely unlabeled (mostly normal data)
data . The popularity of the unsupervised methods is mainly
because they avoid the significant cost of manually labeling largescale anomaly data, which is required to support fully-supervised
approaches. However, they operate without knowing what true
anomalies look like, and as a result, they identify many noisy or
uninteresting isolated data instances as anomalies, leading to high
detection errors.
Recent semi-supervised AD methods aim
to bridge the gap between supervised and unsupervised AD by
utilizing a limited number of anomaly examples to train anomalyinformed detection models. This line of research is motivated by
the fact that a small set of labeled anomaly examples (e.g., some
successfully detected anomalous events) can often be made available
with a small cost in real-world applications. These labeled anomalies
provide a strong indication of the anomalies of interest and can
substantially enhance the detection accuracy .
However, anomalies are unknown abnormal events, so the labeled
anomaly examples typically provides only an incomplete illustration
of anomalies. The current methods focus on fitting the abnormalities
illustrated by the small anomaly examples (i.e., seen anomalies);
they fail to generalize to those that are not, i.e., new types/classes of
anomaly unseen during training (unseen anomalies), such as zeroday attacks and novel defects/planet surfaces .
Also, their performance in detecting seen anomalies is restricted
due to the lack of large training anomaly data.
To tackle these issues, this work considers the problem of weaklysupervised AD, or alternatively open-set supervised AD, that aims
to detect both seen and unseen anomalies given an incomplete
 
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
Pang, et al.
illustration of anomaly classes seen during training. To this end,
we introduce a novel deep AD approach, namely Pairwise Relation
prediction Network (PReNet), that learns pairwise relation features and anomaly scores by predicting the relation of any two
training instances randomly sampled from the small anomaly data
and the unlabeled data, in which the pairwise relation labels can
be anomaly-anomaly, anomaly-unlabeled, or unlabeled-unlabeled.
During inference, a test instance is considered as an anomaly, if it
fits well to the first two types of pairs, or deviates from the last pair
type, when paired with a random training instance. In essence, our
approach unifies the relation prediction and anomaly scoring, and
learns to assign larger prediction scores (i.e., anomaly scores) to the
instance pairs that contain anomalies than the other instance pairs.
Our key insight is that since the unlabeled data is often mostly
normal, the pairwise class labels offer rich three-way pairwise
relation information that supports a joint learning of diverse discriminative patterns, including anomaly-anomaly, anomaly-normal,
and normal-normal pairwise feature patterns, avoiding the fitting
of the seen abnormalities only. Our approach can then detect any
seen/unseen abnormalities that fit the learned pairwise abnormal
patterns, or deviate from the normal patterns. Further, the pairwise
relation formulation seamlessly generates large-scale anomalyinformed surrogate class labels, i.e., the anomaly-anomaly and
anomaly-unlabeled labels vs unlabeled-unlabeled. This significantly
extends the training anomaly data, supporting effective training of
a generalized detection model with the limited labeled data.
In summary, this work makes four main contributions:
â€¢ Problem and Approach. We consider the under-explored
yet crucial problem â€“ weakly-supervised anomaly detection
â€“ and propose a novel pairwise relation learning approach
PReNet to address the problem. PReNet learns diverse discriminative pairwise relation features, offering more generalized detection models than existing methods.
â€¢ Detection Model. PReNet is instantiated to a novel detection model that learns pairwise anomaly scores by minimizing a three-way prediction loss using a relation neural
network. The model is trained with the support of significantly augmented pairwise anomaly data, enabling effective
training of a generalized detection model.
â€¢ Robustness. We theoretically and empirically show that
PReNet can effectively leverage the large unlabeled data
while being tolerant to anomaly contamination.
â€¢ Large Empirical Support. Our empirical results on 12 realworld datasets show that PReNet (i) significantly outperforms nine state-of-the-art (SOTA) competing methods in
detecting seen and unseen anomalies, and (ii) obtains substantially better sample efficiency, e.g., it requires 50%-87.5%
less labeled anomaly data to perform comparably well to, or
better than, the best competing models.
RELATED WORK
Toward Supervised Anomaly Detection. Previous semi-supervised
AD methods focus on leveraging labeled normal
instances to learn patterns of the normal class. Since a small amount
of anomaly data is often available in many real-world applications,
recent semi-supervised methods are dedicated to utilizing small labeled anomaly data to learn anomaly
detectors, e.g., label propagation , representation learning , classification models , or newly proposed loss
functions , and they show that these limited labeled anomalies can substantially improve the detection accuracy. Among
them, DevNet and Deep SAD (DSAD) are two most
relevant methods, which achieve impressive detection performance
by fitting a Gaussian prior-driven anomaly score distribution and
a one-class hypersphere, respectively. However, they are prone to
overfitting the given anomaly examples due to the lack of proper
regularization and large-scale, diversified training anomaly samples.
To address this issue, weakly-supervised AD and
open-set supervised AD tasks are recently introduced, aiming
to detect both seen and unseen anomalies. We follow this line and
introduce a novel pairwise relation learning approach.
This research line is also relevant to few-shot learning and positive and unlabeled data (PU) learning due to the availability of the limited labeled positive
instances (anomalies), but they are very different in that these two
areas assume that the few labeled instances share the same intrinsic
class structure as the other instances within the same class (i.e.,
the anomaly class), whereas the seen anomalies and the unseen
anomalies may have completely different class structures.
Deep Anomaly Detection. Traditional AD approaches are often
ineffective in high-dimensional or non-linear separable data due
to the curse of dimensionality and the deficiency in capturing the
non-linear relations . Deep AD has shown promising
results in handling those complex data, of which most methods
are based on pre-trained features , or features learned by
using autoencoder- or generative adversarial network-
 based objectives. One issue with these methods is
that these feature representations are not primarily optimized to
detect anomalies. Some very recent methods address
this issue by learning representations tailored for specific anomaly measures, e.g., cluster membership-based measure in ,
distance-based measure in and one-class classificationbased measure in . However, they still focus on optimizing the feature representations. By contrast, our
model unifies representation learning and anomaly scoring into
one pipeline to directly optimize anomaly scores, yielding more
optimized anomaly scores. Further, these methods overwhelmingly
focus on unsupervised/semi-supervised settings where detection
models are trained on unlabeled data or exclusively normal data,
which fail to utilize the valuable labeled anomaly data as available in many real-world applications. Some recent studies such as
DevNet directly optimize the anomaly scores via a loss
function called deviation loss . Our method instead uses a
novel formulation of pairwise relation prediction to achieve the
goal, which shows to be significantly better than the deviation loss.
Additionally, our relation prediction is formulated as a weaklysupervised three-way ordinal regression task which is different
from in both of the targeted problem and the approach taken
since uses self-trained ordinal regression for unsupervised AD.
Also, anomaly contamination estimation can help estimate
the proportion of anomalies in the unlabeled data, which may be
used as a prior for empowering weakly-supervised AD.
Deep Weakly-supervised Anomaly Detection
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
THE PROPOSED APPROACH
Overview of Our Approach PReNet
Problem Statement. Given a training dataset X = {x1, x2, Â· Â· Â· ,
xğ‘, xğ‘+1, Â· Â· Â· , xğ‘+ğ¾}, with xğ‘–âˆˆRğ·, where U = {x1, x2, Â· Â· Â· , xğ‘}
is a large unlabeled dataset and A = {xğ‘+1, xğ‘+2, Â· Â· Â· , xğ‘+ğ¾}
(ğ¾â‰ªğ‘) is a small set of labeled anomaly examples that often
do not illustrate every possible class of anomaly, our goal is to learn
a scoring function ğœ™: X â†’R that assigns anomaly scores to data
instances in a way that we have ğœ™(xğ‘–) > ğœ™(xğ‘—) if xğ‘–is an anomaly
(despite it is a seen or unseen anomaly) and xğ‘—is a normal instance.
Anomaly-informed Pairwise Relation Prediction. In our proposed approach PReNet, we formulate the problem as a pairwise
relation prediction-based anomaly score learning, in which we
learn to discriminate three types of random instance pairs, including anomaly-anomaly pairs, anomaly-unlabeled pairs, unlabeledunlabeled pairs. The formulation unifies the relation prediction
and anomaly scoring, and helps enforce the model to assign substantially larger prediction scores (i.e., anomaly scores) to the instance pairs that contain anomalies than the other instance pairs.
By doing so, the model learns diverse discriminative pairwise relation features embedded in the three-way pairwise interaction
data. This way helps alleviate the overfitting of the seen anomalies
as the model is regularized by simultaneously learning a variety
of pairwise normality/abnormality patterns, rather than the seen
abnormalities only. Further, the pairwise relation labels generate
significantly more labeled training data than the original data, offering sufficiently large surrogate labeled data to train a generalized
detection model.
Specifically, as shown in Fig. 1, our approach consists of two
main modules: anomaly-informed random instance pairing and pairwise relation-based anomaly score learning. The first module generates an instance pair dataset P =
 xğ‘–, xğ‘—,ğ‘¦{xğ‘–,xğ‘—}
 | xğ‘–, xğ‘—âˆˆ
X and ğ‘¦{xğ‘–,xğ‘—} âˆˆN
, where each pair {xğ‘–, xğ‘—} has one of the three
pairwise relations: ğ¶{a,a}, ğ¶{a,u} and ğ¶{u,u} (a âˆˆA and u âˆˆU)
and y âˆˆN|P| is an ordinal class feature with decreasing value
assignments to the respective ğ¶{a,a}, ğ¶{a,u} and ğ¶{u,u} pairs, i.e.,
ğ‘¦{a,a} > ğ‘¦{a,u} > ğ‘¦{u,u}. These pairwise labels are set to be ordinal
values to enable the anomaly score learning module ğœ™: P â†’R,
which can be treated as jointly learning a feature learner ğœ“and a
relation (anomaly score) learner ğœ‚. ğœ™is trained in an end-to-end
manner to learn the pairwise anomaly scores using P.
The Instantiated Model
The two modules of PReNet are specified as follows.
Anomaly-informed Random Instance Pairing. In this module,
PReNet generates large-scale instance pairs with surrogate class
labels to provide large labeled data for training subsequent pairwise
relation prediction models. Specifically, instance pairs are created
with instances randomly sampled from the small anomaly set A and
the large unlabeled dataset U. A pairwise class label is then assigned
to each instance pair, such that ğ‘¦{a,a} = ğ‘1, ğ‘¦{a,u} = ğ‘2, ğ‘¦{u,u} = ğ‘3
and ğ‘1 > ğ‘2 > ğ‘3 â‰¥0. By doing so, we efficiently synthesize A and
U to produce a large labeled dataset P =
 xğ‘–, xğ‘—,ğ‘¦{xğ‘–,xğ‘—}
 | xğ‘–, xğ‘—âˆˆ
X and ğ‘¦{xğ‘–,xğ‘—} âˆˆN
ğ’™ğ‘–= ğ’‚ğ‘–, ğ’™ğ‘—= ğ’‚ğ‘—, ğ‘¦ğ’™ğ‘–, ğ’™ğ‘—= ğ’„1
ğ’™ğ‘–= ğ’‚ğ‘–, ğ’™ğ‘—= ğ’–ğ‘–, ğ‘¦ğ’™ğ‘–, ğ’™ğ‘–= ğ’„2
ğ’™ğ‘–= ğ’–ğ‘–, ğ’™ğ‘—= ğ’–ğ‘—, ğ‘¦ğ’™ğ‘–, ğ’™ğ‘—= ğ’„3
where ğ’‚âˆˆğ’œ, ğ’–âˆˆğ’°, and
ğ’„1> ğ’„2 > ğ’„3
ğ’›ğ‘–= ğœ“(ğ’™ğ‘–; Î˜ğ‘Ÿ)
ğ’›ğ‘—= ğœ“(ğ’™ğ‘—; Î˜ğ‘Ÿ)
ğœ‚(ğ’›ğ‘–, ğ’›ğ‘—); Î˜ğ‘ 
ğ¿ğœ‚(ğ’›ğ‘–, ğ’›ğ‘—); Î˜ğ‘ , ğ‘¦ğ’™,ğ’™ğ‘—
Type equation here.
Anomaly-informed
Random Instance Pairing
Pairwise relation-based
Anomaly Score Learner ğœ™
Loss Function
Figure 1: Overview of PReNet. It takes anomaly-anomaly,
anomaly-unlabeled, and unlabeled-unlabeled instance pairs
as input and learns pairwise anomaly scores by discriminating these three types of linear pairwise interactions.
The resulting P contains critical information for discriminating
anomalies from normal instances. This is because ğ‘¦{a,a}, ğ‘¦{a,u} and
ğ‘¦{u,u} are approximately anomaly-anomaly, anomaly-normal and
normal-normal pairs, respectively, as U is typically dominated by
normal instances (per definition of anomaly ). A few ğ‘¦{a,u}
and ğ‘¦{u,u} pairs may be noisy pairs due to anomaly contamination
in U, but we show that PReNet is robust to these noisy pairs
Pairwise Relation-based Anomaly Score Learning. A pairwise
anomaly score learner ğœ™: P â†’R is then introduced to take P as
input to learn the anomaly scores of instance pairs. Let Z âˆˆRğ‘€
be an intermediate representation space, we define a two-stream
anomaly scoring network ğœ™ (Â·, Â·); Î˜ : P â†’R as a sequential
combination of a feature learner ğœ“(Â·; Î˜ğ‘Ÿ) : X â†’Z and an anomaly
scoring function ğœ‚ (Â·, Â·); Î˜ğ‘ 
 : (Z, Z) â†’R, where Î˜ = {Î˜ğ‘Ÿ, Î˜ğ‘ }.
Specifically, ğœ“(Â·; Î˜ğ‘Ÿ) is a neural feature learner with ğ»âˆˆN hidden
layers and the weight parameters Î˜ğ‘Ÿ.
z = ğœ“(x; Î˜ğ‘Ÿ),
where x âˆˆX and z âˆˆZ. We further specify ğœ‚ (Â·, Â·); Î˜ğ‘ 
anomaly score learner that uses a fully connected layer to learn
linear pairwise relation features and the anomaly scores, taking the
concatenation of the intermediate representations of each pair â€“ zğ‘–
and zğ‘—â€“ as input:
ğœ‚ (zğ‘–, zğ‘—); Î˜ğ‘ 
ğ‘€+ğ‘™ğ‘§ğ‘—ğ‘™+ ğ‘¤ğ‘œ
where z âˆˆZ and Î˜ğ‘ = {wğ‘œ} in which {ğ‘¤ğ‘œ
2, Â· Â· Â· ,ğ‘¤ğ‘œ
weight parameters and ğ‘¤ğ‘œ
2ğ‘€+1 is a bias term. As shown in Fig. 1,
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
Pang, et al.
PReNet uses a two-stream network with the shared weight parameters Î˜ğ‘Ÿto learn the representations zğ‘–and zğ‘—. Thus, ğœ™ (Â·, Â·); Î˜ can
be formally represented as
ğœ™ (xğ‘–, xğ‘—); Î˜ = ğœ‚
 ğœ“(xğ‘–; Î˜ğ‘Ÿ),ğœ“(xğ‘—; Î˜ğ‘Ÿ); Î˜ğ‘ 
which can be trained in an end-to-end fashion. Note that the pairwise relation learned in Eqn. (2) is a simple linear relation; learning
more complex relations can be done by adding more layers with
non-linear activation on top of the concatenated features, but it
does not show clear advantages in our setting (see Table 4).
PReNet then uses the pairwise ordinal class labels to optimize the
pairwise relation-based anomaly scores. Particularly, it minimizes
the difference between the prediction scores and the ordinal labels
ğ‘¦{a,a}, ğ‘¦{a,u} and ğ‘¦{u,u}. It is equivalent to learning to assign larger
prediction scores to the anomaly-related (i.e., anomaly-anomaly and
anomaly-unlabeled) instance pairs than the unlabeled-unlabeled
pairs. Our loss is defined as below to guide the optimization:
ğœ™ (xğ‘–, xğ‘—); Î˜,ğ‘¦{xğ‘–,xğ‘—}
ğ‘¦{xğ‘–,xğ‘—} âˆ’ğœ™ (xğ‘–, xğ‘—); Î˜
The three-class labels ğ‘¦{a,a} = 8, ğ‘¦{a,u} = 4 and ğ‘¦{u,u} = 0 are used
by default to enforce a large margin among the anomaly scores of
the three types of instance pairs. PReNet also works well with other
value assignments as long as there are reasonably large margins
among the ordinal labels (see Sec. 5.6). Lastly, PReNet is trained via:
(xğ‘–,xğ‘—,ğ‘¦{xğ‘–,xğ‘—})âˆˆB
ğ‘¦{xğ‘–,xğ‘—} âˆ’ğœ™ (xğ‘–, xğ‘—); Î˜
where B is a sample batch from P and ğ‘…(Î˜) is a regularization term
with hyperparameter ğœ†. For each batch, |B|
instance pairs are sampled from the ğ¶{u,u} class and |B|
instance pairs are respectively
sampled from the ğ¶{a,a} and ğ¶{a,u} classes. This is equivalent to
oversampling the two anomaly-related classes, ğ¶{a,a} and ğ¶{a,u},
to avoid bias toward the ğ¶{u,u} class due to the class imbalance.
Anomaly Detection Using PReNet
Training. Algorithm 1 presents the procedure of training PReNet.
Step 1 first extends the data X into a set of instance pairs with
ordinal class labels, P. After a uniform Glorot weight initialization
in Step 2, PReNet performs stochastic gradient descent (SGD) based
optimization to learn Î˜ in Steps 3-9 and obtains the optimized ğœ™in
Step 10. Particularly, stratified random sampling is used in Step 5 to
ensure the sample balance of the three classes in B, as discussed in
Sec. 3.2.2. Step 6 performs the forward propagation of the network
and computes the loss. Step 7 then uses the loss to perform gradient
descent steps.
Inference. During inference, given a test instance xğ‘˜, PReNet first
pairs it with data instances randomly sampled from A and U, and
then defines its anomaly score as
ğœ™  (ağ‘–, xğ‘˜) ; Î˜âˆ— +
ğœ™ (xğ‘˜, uğ‘—); Î˜âˆ—ï£¹ï£ºï£ºï£ºï£ºï£»
where Î˜âˆ—are the parameters of a trained ğœ™, and ağ‘–and uğ‘—are randomly sampled from the respective A and U. sxğ‘˜can be interpreted
Algorithm 1 Training PReNet
Input: X âˆˆRğ·with X = U âˆªA and âˆ…= U âˆ©A
Output: ğœ™: (X, X) â†’R - an anomaly score mapping
1: P â†Augment the training data with U and A
2: Randomly initialize Î˜
3: for ğ‘–= 1 to n_epochs do
for ğ‘—= 1 to n_batches do
B â†Randomly sample b data instance pairs from P
(xğ‘–,xğ‘—,ğ‘¦{xğ‘–,xğ‘—}) âˆˆB
ğ‘¦{xğ‘–,xğ‘—} âˆ’ğœ™ (xğ‘–, xğ‘—); Î˜
Perform a gradient descent step w.r.t. the parameters in Î˜
9: end for
10: return ğœ™
as an ensemble of the anomaly scores of a set of xğ‘˜-oriented pairs.
Due to the loss in Eqn. (4), sxğ‘˜is optimized to be greater than sxâ€²
given xğ‘˜is an anomaly and xâ€²
ğ‘˜is a normal instance. PReNet can
perform stably with a sufficiently large ğ¸due to the law of large
numbers (ğ¸= 30 is used by default; see Sec. 5.6 for other results).
THEORETICAL ANALYSIS
Pairwise Relation Feature Learning
The random instance pairing module seamlessly leverages the two
instance sets A and U to create large-scale proxy class labels to support pairwise relation feature learning. That is, the sample size of
the training data theoretically increases from ğ‘+ ğ¾in the original
data space to (ğ‘+ ğ¾)2 for the pairwise relation learning, including
ğ¾2 of ğ¶{a,a} pairs, 2ğ¾Ã— ğ‘of ğ¶{a,u} pairs and ğ‘2 of ğ¶{u,u} pairs
(note that as set notion is used, {a, u} = {u, a}). Such a large size
helps build up the generalizability and then the performance of
our detector. Note that PReNet uses a shared-weight two-stream
network inğœ“(Â·; Î˜ğ‘Ÿ), so the feature learning is still optimized on the
X data space rather than the higher-order pairwise P space. This
trick well supports the scale-up of the training sample size while
adding no extra model complexity. Further, the relation learning
in Eqn. (5) enforces PReNet to discriminate the representations of
anomaly-anomaly, anomaly-normal and normal-normal pairwise
interactions (as U contains mostly normal data). This results in a
joint learning of diverse patterns of abnormality, normality, and
their interactions, avoiding the exclusive fitting of the seen abnormalities that may consequently overfit the seen abnormalities and
fail to generalize to unseen abnormalities.
Robust Anomaly Score Learning
This section analyzes the robustness of PReNet to ğœ–-contamination
in the unlabeled data U, where ğœ–is the proportion of true anomalies in U. Per the definition of anomaly, ğœ–is typically small, e.g.,
< 2%. From the three-way modeling of anomaly-anomaly, anomalynormal and normal-normal interactions, we can obtain the expectation of the pairwise relation proportions in each batch B based on
uniformly random sampling. Let aT and nT indicate true anomaly
and true normal instances, respectively, we can then have the probability expectation of each type of the interactions in P in Table 1.
Deep Weakly-supervised Anomaly Detection
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
Table 1: Probability expectation of pairwise interactions
(1 âˆ’ğœ–) (1 âˆ’ğœ–)
Considering the 1
2 sampling probability of anomaly and
unlabeled pairs in B, there are 1
2ğœ–2 from true anomalyanomaly pairwise relations, and 1
4ğœ–âˆ’ğœ–2 from true anomalynormal pairwise relations, and 1
2ğœ–2 from normal-normal
pairwise relations. Their true expectation values when all the unlabeled cases are not true anomalies, are 1
2 respectively.
Thus, a small percentage of the pairwise relations,
=2ğœ–âˆ’ğœ–2, would be expected to be noisy pairs.
Considering the tolerance of the regression performance to about
5% outliers , PReNet can perform well when ğœ–is reasonably
small, â‰¤2.5%, which can often be satisfied for real-world anomaly
detection problems. On the other hand, from the regression analysis, we can derive the following theorem for the expectation for
different types of pairwise relation and their anomaly scores.
Theorem 4.1 (Robustness to Anomaly Contamination). Let
ğœ–â‰¥0 be the anomaly contamination rate in U, ğ‘¦{a,a} = ğ‘1, ğ‘¦{a,u} =
ğ‘2, and ğ‘¦{u,u} = ğ‘3 with ğ‘1 > ğ‘2 > ğ‘3 â‰¥0, then for a given test
instance xğ‘˜, we have E
ğ‘ xğ‘˜|xğ‘˜ğ‘–ğ‘ ğ‘ğ‘›ğ‘ğ‘›ğ‘œğ‘šğ‘ğ‘™ğ‘¦
, which is
guaranteed to be greater than E
ğ‘ xğ‘˜|xğ‘˜ğ‘–ğ‘ ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™
= ğ‘2+ğ‘3+ğœ–(ğ‘1+ğ‘2)
for ğœ–< ğ‘1âˆ’ğ‘3
ğ‘1+ğ‘2 (see Appendix A for the proof).
This theorem indicates that in PReNet a true anomaly is expected
to have a larger anomaly score than normal instances when the
contamination rate in U is not too large. That is, we have ğœ–< 2
our default setting: ğ‘1 = 8, ğ‘2 = 4 and ğ‘3 = 0 (see Sec. 3.2.2), which
is normally satisfied in real-world anomaly detection applications.
EXPERIMENTS
Multidimensional (or tabular) data is ubiquitous in real-world applications, so we focus on this type of publicly available datasets1. To
explicitly evaluate the performance of detecting seen/unseen anomalies, we have two groups of datasets from the literature ,
including 12 datasets used for the detection of seen anomalies and
another 28 datasets used for detecting unseen anomalies:
Seen Anomaly Detection Datasets As shown in Table 2, 12 realworld datasets are used for the detection of seen anomalies, which
are from diverse domains, e.g., cyber-attack detection, fraud detection, and disease risk assessment. Each dataset contains 0.2%-15.0%
anomalies of the same class. To replicate the real-world scenarios
where we have a few labeled seen anomalies and large unlabeled
data, we first have a stratified split of each dataset into two subsets,
with 80% data as training data and the other 20% data as a holdup
test set. Since the unlabeled data is often anomaly-contaminated,
we then combine some randomly selected anomalies with the normal training instances to form the unlabeled data U. We further
1See Appendix B.1 for more details about the used datasets.
randomly sample a limited number of anomalies from the anomaly
class to form the labeled anomaly set A.
Unseen Anomaly Detection Datasets The 28 datasets for detecting unseen anomalies are presented in Table 3. These datasets are
derived from four intrusion attack datasets dos, rec, fuz and bac in
Table 22, whose data instances are from the same data source and
spanned by the same feature space. To guarantee that the anomalies
in the test data are unseen during training, the anomaly class in one
of these four datasets is held up for evaluation, while the anomalies
in any combinations of the remaining three datasets are combined
to form the pool of seen anomalies. We have 28 possible permutations under this setting, resulting in 28 datasets with different
seen and/or unseen anomaly classes, as shown in Table 3. During
training, A contains the anomalies sampled from the pool of seen
anomalies, while the test data is composed of the held-up unseen
anomaly classes and the normal instances in the test set.
Competing Methods and Their Settings
PReNet is compared with six state-of-the-art methods from several related areas, including semi-supervised anomaly detectors,
DevNet and one-class classifier Deep SAD (DSAD) ,
highly class-imbalanced (few-shot) classifier FSNet and its
cost-sensitive variant cFSNet, and unsupervised anomaly detection
methods iForest and REPEN (REPEN represents unsupervised detectors that have a component to easily utilize any available
anomaly data to train their models). Similar to , we found
empirically that all deep methods using a multilayer perceptron
network architecture with one hidden layer perform better and
more stably than using two or more hidden layers. Thus, following
DevNet, one hidden layer with 20 neural units is used in all deep
methods. The ReLu activation function ğ‘”(ğ‘) = max(0,ğ‘) is used.
An â„“2-norm regularizer with the hyperparameter setting ğœ†= 0.01
is applied to avoid overfitting. The RMSprop optimizer with the
learning rate 0.001 is used. All deep detectors are trained using 50
epochs, with 20 batches per epoch. Similar to PReNet, oversampling
is also applied to the labeled anomaly set A to well train the deep
detection models of DevNet, REPEN, DSAD, FSNet and cFSNet.
iForest with recommended settings is used as a baseline here.
We also compare PReNet with XGBOD , PUMAD , and
FEAWAD . The comparison results are given in Appendix C
due to space limitation.
Performance Evaluation Metrics
Two popular metrics â€“ the Area Under Receiver Operating Characteristic Curve (AUC-ROC) and Area Under Precision-Recall Curve
(AUC-PR) â€“ are used. A larger AUC-ROC/AUC-PR reflects better
performance. AUC-ROC that summarizes the curve of true positives
against false positives often presents an overoptimistic view of the
performance, whereas AUC-PR is more practical as it summarizes
the precision and recall w.r.t. the anomaly class exclusively . The
reported results are averaged values over 10 independent runs. The
paired Wilcoxon signed-rank is used to examine the statistical
significance of PReNet against its competing methods.
2The other eight datasets cannot be used in evaluating unseen anomaly detection as
they are from different data sources and contain only one anomaly class.
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
Pang, et al.
Detection of Seen Anomalies
Effectiveness of PReNet using small anomaly examples. We
first evaluate PReNet on detecting seen anomalies in 12 real-world
datasets. A consistent anomaly contamination rate and the same
number of labeled anomalies are used across all datasets to gain
insights into the performance in different real-life applications.
Since anomalies are typically rare instances, the number of labeled
anomalies available per data is set to 60, i.e., |A| = 60, and the
anomaly contamination rate is set to 2% by default.
The results on the 12 datasets are shown in Table 2. In AUC-PR,
PReNet performs substantially better than, or comparably well to,
all competing methods across the 12 datasets. On average, PReNet
improves all five competing methods by a large margin, i.e., DevNet (3.7%), DSAD (19.6%), FSNet (54.0%), cFSNet (59.1%), REPEN
(114.2%), and iForest (331.85%), which are all statistically significant at the 95%/99% confidence level. Particularly, compared to
the top two contenders, PReNet significantly outperforms DevNet
on six datasets, with improvement ranging from 3%-6% on census,
bac, news20 and thyroid, and up to 10%-20% on campaign and w7a;
they perform comparably well on the rest of six datasets; PReNet
performs significantly better than DSAD on eight datasets, achieving 20%-320% improvement on six datasets, including donors, fuz,
w7a, campaign, news20 and thyroid, and they perform comparably
well on the rest of four datasets. In terms of AUC-ROC, PReNet
outperforms DevNet at the 90% confidence level, and performs significantly better than DSAD (2.9%), FSNet (11.7%), cFSNet (13.1%),
REPEN (10.6%) and iForest (40.7%) at the 95%/99% confidence level.
Using less/more labeled anomaly examples. We further examine PReNet by evaluating its performance w.r.t. different numbers
of labeled anomalies, ranging from 15 to 120, with the contamination rate fixed to 2%. The AUC-PR results are shown in Fig. 2.
iForest is omitted as it does not use labeled data. The results of all
methods generally increases with labeled data size. However, The
increased anomalies do not always help due to the heterogeneous
anomalous behaviors taken by different anomalies. PReNet is more
stable in the increasing trend. Consistent with the results in Table 2,
PReNet still significantly outperforms its state-of-the-art competing
methods with varying numbers of anomaly examples. Particularly,
PReNet demonstrates the most sample-efficient learning capability. Impressively, PReNet can be trained with 50%-75% less labeled
anomalies but achieves much better, or comparably good, AUC-PR
than the best contender DevNet on multiple datasets like dos, fuz,
w7a and campaign; and it is trained with 87.5% less labeled data
while obtains substantially better performance than the second-best
contender DSAD on donors, w7a, campaign, news20 and thyroid.
Similar observations apply to FSNet, cFSNet and REPEN.
Detection of Unseen Anomalies
Generalizing to unseen anomaly classes using small examples of seen anomaly classes. This section evaluates the detectors
that are trained with only seen anomaly classes to detect unseen
anomaly classes on the 28 datasets. Similarly to Sec. 5.4, the anomaly contamination rate of 2% and |A| = 60 are used here. The results
are presented in Table 3, in which iForest that is insensitive to the
change is used as baseline. The AUC-PR results show that PReNet
outperforms all the five competing methods by substantial margins on the 28 datasets. On average, PReNet improves DevNet by
more than 11%, DSAD by 17%, FSNet by 30%, cFSNet by 20% and
REPEN by 27%. It is impressive that, compared to the best competing
method DevNet, PReNet achieves 20%-130% AUC-PR improvement
on eight datasets, including 20%-40% improvement on â€˜rec â†’bacâ€™,
â€˜rec,fuz â†’dosâ€™, â€˜rec, bac, fuz â†’dosâ€™, â€˜bac, fuz â†’dosâ€™, â€˜fuz â†’dosâ€™,
â€˜rec, bac, dos â†’fuzâ€™, and over 100% improvement on â€˜rec, fuz â†’bacâ€™
and â€˜fuz â†’bacâ€™. The improvement over the other four contenders
is more substantial than that over DevNet. All these improvements
are statistically significant at the 99% confidence level. PReNet gains
similar superiority in AUC-ROC as well.
Detection of unseen anomaly classes with less/more seen
anomalies. We examine this question on 14 unknown anomaly
datasets where all methods work relatively well compared to the
other 14 datasets. The AUC-PR results are presented in Fig. 3. It
is interesting that most detectors gain improved performance in
detecting unseen anomalies when more seen anomalies are given.
This may be due to that more seen anomaly examples help better
train the detection models, enabling a better anomaly discriminability. The superiority of PReNet here is consistent with that in Table 3.
PReNet remains the most sample-efficient method, and can perform
substantially better than the best competing methods even when
the PReNet model is trained with 50%-87.5% less labeled data.
Further Analysis of PReNet
Robustness w.r.t. anomaly contamination. We investigate this
robustness by using different anomaly contamination rates, {0%,
2%, 5%, 10%}, with |A| = 60 fixed. The AUC-PR results for this
experiment are presented in Fig. 4. PReNet performs generally
stably on all datasets with the contamination rate below 10%, except
news20 that contains over one millions features and may therefore
require better relation learning designs to achieve good robustness
w.r.t. a large contamination rate.
Ablation study. In Table 4, PReNet is compared to its four ablated
variants to evaluate the importance of each module:
â€¢ Three-way relation modeling. To examine the importance
of learning diverse pairwise patterns via three-way relation
modeling, we compare PReNet with its Two-way Relation
Modeling (TRM) variant that learns two-way relations only,
i.e., to discriminate ğ‘¦{a,a}|{a,u} from ğ‘¦{u,u}. PReNet outperforms than TRM on nearly all datasets, resulting in large
average improvement (5.6%). This indicates that learning
more diverse patterns helps obtain better generalization.
â€¢ Relation features. Can PReNet perform similarly well by
learning features of individual instances rather than pairwise relation features? The answer is negative. Compared to
iPReNet in Table 4 that takes individual instances as input
and learns to discriminate seen anomalies from unlabeled
data, PReNet performs better by a large margin on 11 out of
12 datasets.
â€¢ Feature learning layer ğœ“. To evaluate the importance of
intermediate feature representations, PReNet is compared
with LDM that removes the hidden layers of PReNet (i.e., the
ğœ“function) and learns a Linear Direct Mapping (LDM) from
the original data space to anomaly scores. The results show
Deep Weakly-supervised Anomaly Detection
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
Table 2: Seen anomaly detection results. â€˜Sizeâ€™ is the data size. ğ·is the dimension. â€˜1Mâ€™ denotes news20 has 1,355,191 features.
Data Statistics
AUC-PR Results
AUC-ROC Results
Figure 2: AUC-PR results of seen anomaly detection w.r.t. the number of labeled anomalies
that eliminating the feature layer leads to over 10% loss in
the average AUC-PR performance.
â€¢ Deeper neural network. We also explore the possibility
of learning a deeper network in PReNet. A2H is a variant of
PReNet, which deepens PReNet with additional two hidden
(A2H) layers. Each added layer is regularized by an â„“2-norm
regularizer and a dropout layer of 0.5 dropout rate. Although
A2H performs well on some datasets like rec and thyroid, it
fails on most of the other datasets. As a result, PReNet can
still gain large average improvement over A2H. Thus, the default architecture used in PReNet is generally recommended.
â€¢ Non-linear relation learning. We compare PReNet with a
variant of Non-linear Pairwise Relation (NPR) learning that
adds a non-linear layer in-between the the concatenated
features and the anomaly scoring layer. Similar to A2H, NRP
can work better than PReNet on a few cases, but it is often
too complex and has an overfitting problem on most datasets.
Sensitivity test. We evaluate the sensitivity of PReNet w.r.t. three
key hyperparameters: ğ‘¦{xğ‘–,xğ‘—} and ğœ†in Eqn. (5), and ğ¸in Eqn. (6).
â€¢ Sensitivity w.r.t. pairwise class labels ğ‘¦{xğ‘–,xğ‘—}. This section examines the sensitivity of PReNet w.r.t. the synthetic
ordinal pairwise relation class labels. We fix the ordinal label
for ğ‘¦{u,u} to be zero, i.e., ğ‘3 = 0, and the same margin is
set between ğ‘¦{u,u} and ğ‘¦{a,u} pairs, and between ğ‘¦{a,u} and
ğ‘¦{a,a} pairs, i.e., (ğ‘2 âˆ’ğ‘3) = (ğ‘1 âˆ’ğ‘2) = ğ‘š. We test the sensitivity w.r.t. different values of the margin ğ‘š. The AUC-PR
results are shown in Figure 5. It is clear that PReNet is generally robust to different margin values. PReNet performs
well even when setting a rather small margin, e.g., ğ‘š= 0.25.
Larger margins are generally more desired, especially in
some challenging datasets such as thyroid and dos.
â€¢ Sensitivity w.r.t. ensemble size ğ¸. This section investigates
the sensitivity of PReNet w.r.t. the ensemble size ğ¸in Eqn. (6).
The AUC-PR results are shown in Figure 5. PReNet performs
very stably across all the 12 datasets. PReNet using small ğ¸
performs similarly well as that using a large ğ¸, indicating
that highly discriminative features are learned in PReNet.
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
Pang, et al.
Table 3: Unseen anomaly detection results. The models are trained with â€˜seenâ€™ anomaly classes to detect â€˜unseenâ€™ anomalies.
Anomaly Class
AUC-PR Results
AUC-ROC Results
rec, dos, fuz
rec, bac, fuz
rec, bac, dos
dos, bac, fuz
dos,fuz->bac
rec,dos->bac
rec,dos,fuz->bac
rec,fuz->bac
bac,fuz->rec
dos,bac->rec
dos,bac,fuz->rec
dos,fuz->rec
Figure 3: AUC-PR w.r.t. # of labeled anomalies. â€˜A -> Bâ€™ means the models trained with attacks â€˜Aâ€™ to detect unseen attacks â€˜Bâ€™.
Increasing ğ¸may offer better detection accuracy on some
datasets, but the improvement is often marginal.
â€¢ Sensitivity w.r.t. regularization parameter ğœ†. This part
investigates the sensitivity of PReNet w.r.t. a wide range of
ğœ†settings, ğœ†= {0.001, 0.005, 0.01, 0.05, 0.1}, in Eqn. (5). The
AUC-PR results are shown in Figure 5. PReNet is generally robust w.r.t. different ğœ†values on all the 12 datasets, especially
when ğœ†is chosen in the range [0.001, 0.01]. When increasing
Deep Weakly-supervised Anomaly Detection
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
reconnaissance
Contaminated Data
No Contamination
Figure 4: AUC-PR w.r.t. different contamination rates (%).
Table 4: AUC-PR results of ablation study.
ğœ†to larger values such as 0.05 or 0.1, the AUC-PR of PReNet
decreases on a few datasets, e.g., rec, bac, news20 and thyroid. This may be due to that given the limited number of
labeled anomaly data, enforcing strong model regularization
in PReNet can lead to underfitting on those datasets. Therefore, a small ğœ†, e.g., ğœ†= 0.01, is generally recommended.
CONCLUSIONS
This paper explores the generalized semi-supervised anomaly detection problem and introduces a novel deep approach and its instantiated model PReNet to address the problem. The approach
learns pairwise relation features and anomaly scores in a unified
framework by three-way pairwise relation modeling. In doing so,
it learns diverse abnormality and normality representations, alleviating the overfitting of the seen abnormalities. This is justified
by the substantial improvement of PReNet over its variants and
nine state-of-the-art competing methods that focus on learning
more homogeneous normal/abnormal representations for detecting
seen/unseen anomalies on 12 real-world datasets.
Particularly, our significantly improved precision-recall performance in unseen anomaly detection, i.e., 10%-30%, is encouraging
0.001 0.005
Figure 5: AUC-PR of PReNet w.r.t. three hyperparameters.
in that it is already very challenging to improve this metric for
seen anomalies, and the challenge is further largely increased for
the unseen anomalies. Our results also suggest that the labeled
anomaly data, regardless of its scale and coverage of the anomaly
classes, can be well leveraged to enable accurate anomaly detection
in the wild. In future work, we plan to extend our approach to
explore the supervision information from different domains of data
for anomaly detection.
ACKNOWLEDGMENTS
C. Shenâ€™s participation was supported by National Key R&D Program of China (No. 2022ZD0118700). We thank Hezhe Qiao for
helping obtain the results of XGBOD, PUMAD, and FEAWAD.
PROOF OF THEOREM
Theorem A.1 (Robustness to Anomaly Contamination). Let
ğœ–â‰¥0 be the anomaly contamination rate in U, ğ‘¦{a,a} = ğ‘1, ğ‘¦{a,u} =
ğ‘2, and ğ‘¦{u,u} = ğ‘3 with ğ‘1 > ğ‘2 > ğ‘3 â‰¥0, then for a given test
instance xğ‘˜, we have E
ğ‘ xğ‘˜|xğ‘˜ğ‘–ğ‘ ğ‘ğ‘›ğ‘ğ‘›ğ‘œğ‘šğ‘ğ‘™ğ‘¦
, which is
guaranteed to be greater than E
ğ‘ xğ‘˜|xğ‘˜ğ‘–ğ‘ ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™
= ğ‘2+ğ‘3+ğœ–(ğ‘1+ğ‘2)
for ğœ–< ğ‘1âˆ’ğ‘3
Proof. From the regression modeling, the expectation for the
pairwise anomaly score for a true anomaly ağ‘˜is
ğœ™ (ağ‘–, ağ‘˜) ; Î˜âˆ—
The corresponding expectation for a normal data instance nğ‘™is
ğœ™ (ağ‘–, nğ‘™) ; Î˜âˆ—
= ğ‘2 + ğœ–ğ‘1,
= ğ‘3 + ğœ–ğ‘2.
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
Pang, et al.
Thus, based on the anomaly scoring function in Eqn. (6):
ğœ™  (ağ‘–, xğ‘˜) ; Î˜âˆ— +
ğœ™ (xğ‘˜, uğ‘—); Î˜âˆ—ï£¹ï£ºï£ºï£ºï£ºï£»
ğ‘ xğ‘˜|xğ‘˜ğ‘–ğ‘ ğ‘ğ‘›ğ‘ğ‘›ğ‘œğ‘šğ‘ğ‘™ğ‘¦
ğœ™ (ağ‘–, ağ‘˜) ; Î˜âˆ—
 ; Î˜âˆ— o
ğ‘ xğ‘˜|xğ‘˜ğ‘–ğ‘ ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™
ğœ™ (ağ‘–, nğ‘™) ; Î˜âˆ—
 ; Î˜âˆ— o
= ğ‘2 + ğ‘3 + ğœ–(ğ‘1 + ğ‘2)
Since ğ‘1 > ğ‘2 > ğ‘3 â‰¥0, E
ğ‘ xğ‘˜|xğ‘˜ğ‘–ğ‘ ğ‘ğ‘›ğ‘ğ‘›ğ‘œğ‘šğ‘ğ‘™ğ‘¦
 is guaranteed
to be greater than E
ğ‘ xğ‘˜|xğ‘˜ğ‘–ğ‘ ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™
for ğœ–< ğ‘1âˆ’ğ‘3
DETAILED EXPERIMENT SETUP
Seen Anomaly Detection Datasets. The 12 seen anomaly detection datasets can be accessed via the links in Table 5. More specifically, the donors data is taken from KDD Cup 2014 for predicting
the excitement of donation projects, with exceptionally exciting
projects used as anomalies (6.0% of all data instances). The census
data is extracted from the US census bureau database, in which we
aim to detect the rare high-income persons (6.0%). fraud is for fraudulent credit card transaction detection, with fraudulent transactions
(0.2%) as anomalies. celeba contains more than 200K celebrity images, each with 40 attribute annotations. We use the bald attribute
as our detection target, in which the scarce bald celebrities (3.0%)
are treated as anomalies and the other 39 attributes form the feature
space. The dos, rec, fuz and bac datasets are derived from a popular
intrusion detection dataset called UNSW-NB15 with the respective DoS (15.0%), reconnaissance (13.1%), fuzzers (3.1%) and backdoor
(2.4%) attacks as anomalies against the â€˜normalâ€™ class. w7a is a web
page classification dataset, with the minority classes (3.0%) as anomalies. campaign is a dataset of bank marketing campaigns, with rare
positive campaigning records (11.3%) as anomalies. news20 is one of
the most popular text classification corpora, which is converted into
anomaly detection data via random downsampling of the minority
class (5.0%) based on . thyroid is for disease detection, in
which the anomalies are the hypothyroid patients (7.4%). Seven of
these datasets contain real anomalies,including donors, fraud, dos,
rec, fuz, bac and thyroid. The other five datasets contain semantically real anomalies, i.e., they are rare and very different from the
majority of data instances. So, they serve as a good testbed for the
evaluation of anomaly detection techniques.
To replicate the real-world scenarios where we have a few labeled
anomalies and large unlabeled data, we first have a stratified split
of the anomalies and normal instances into two subsets, with 80%
data as training data and the other 20% data as a holdup test set.
Since the unlabeled data is often anomaly-contaminated, we then
combine some randomly selected anomalies with the whole normal
training data instances to form the unlabeled dataset U. We further
randomly sample a limited number of anomalies from the anomaly
Table 5: Links for accessing the datasets
 
 
 
 
 
 
 
 
 
 
 
 
class to form the labeled anomaly set A. The resulting sample size
and dimensionality of the datasets are shown in Table 2.
Unseen Anomaly Detection Datasets. Table 3 presents the 28
datasets for the evaluation of detecting unseen anomalies. These
datasets are derived from the above four intrusion attack datasets
dos, rec, fuz and bac, with data instances spanned by the same
feature space. To guarantee that the evaluation data contains unseen
anomalies, the anomaly class in one of these four datasets is held
up for evaluation, while the anomalies in any combinations of the
remaining three datasets are combined to form the pool of seen
anomalies. The type of the holdup anomalies is always different
from that in the anomaly pool and can be safely treated as unseen
anomalies. We have 28 possible permutations under this setting,
resulting in 28 datasets with different seen and/or unseen anomalies.
For the training, A contains the anomalies sampled from the seen
anomalies pool, while the evaluation data is composed of the holdup
unseen anomaly class and the 20% holdup normal instances. Note
that the other eight datasets in Table 2 cannot be used in evaluating
unseen anomaly detection as they contain only one anomaly class
and they are from different data sources and feature spaces.
Implementation Details
Packages. PReNet is implemented using Tensorflow/Keras.
The main packages and their versions used in this work are provided
as follows:
â€¢ keras==2.3.1
â€¢ numpy==1.16.2
â€¢ pandas==0.23.4
â€¢ scikit-learn==0.20.0
â€¢ scipy==1.1.0
â€¢ tensorboard==1.14.0
â€¢ tensorflow==1.14.0
Hyperparameter Settings. Since our experiments focus on
unordered multidimensional data, multilayer perceptron networks
are used. Similar to , we empirically found that all deep
methods using an architecture with one hidden layer perform better and more stably than using two or more hidden layers. This may
Deep Weakly-supervised Anomaly Detection
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
be due to the limit of the available labeled data. Following ,
one hidden layer with 20 neural units is used in PReNet. The ReLu
activation function ğ‘”(ğ‘) = max(0,ğ‘) is used. An â„“2-norm regularizer with the hyperparameter setting ğœ†= 0.01 is applied to avoid
overfitting. The RMSprop optimizer with the learning rate 0.001
is used. The same network architecture is used in the competing
methods DevNet , REPEN , Deep SAD (DSAD) , FSNet
 and its variant cFSNet. iForest with the recommended settings
 , 100 isolation trees and 256 subsampling size, are used in our
experiments.
All deep detectors are trained using 50 epochs, with 20 batches
per epoch. The batch size is probed in {8, 16, 32, 64, 128, 256, 512}.
The best fits, 512 in PReNet, DevNet and DSAD, 256 in FSNet and
REPEN, are used by default. cFSNet uses the same settings as FS-
Net. Similar to PReNet, oversampling is also applied to the labeled
anomaly set A to well train the deep detection models of DevNet,
REPEN, DSAD, FSNet and cFSNet.
ADDITIONAL EMPIRICAL RESULTS
Comparison to Three More Methods
We also compare PReNet to three more methods XGBOD ,
PUMAD , and FEAWAD on both seen and unseen anomaly detection datasets, with the results shown in Tables 6 and 7
respectively. The official implementation of XGBOD and FEAWAD
was used to perform the experiments. The code of PUMAD is not
released; we use our own implementation based on a metric learning similar to REPEN. As for XGBOD, we used one-class SVM and
iForest to produce new features only; ğ‘˜NN and LOF were excluded
due to prohibitive computational cost on large datasets. All the
other settings in XGBOD remain unchanged.
Table 6: AUC-PR results for seen AD datasets. OOM denotes
an out-of-memory issue in a GeForce RTX 3090 24GB GPU.
The results show that these three methods, especially FEAWAD,
can work well on some datasets, but they still substantially underperform our method PReNet on most datasets.
Performance Ranking of All Methods
To have a holistic comparison of all 10 detectors, we calculate the
average (ordinal and percentile) rank of each method based on its
detection performance in both seen and unseen AD settings. The
results are shown in Table 8, where an average ordinal rank of
one (or a percentile of 1.00 ) is the perfect performance, indicating
the method is always the best performer compared to all other
methods across all datasets. That is, lower rank (or higher percentile)
indicates better performance.
The results show that PReNet is the best performer in both
settings, outperforming all nine competing methods for 94.2% and
90.2% of cases on 12 seen anomaly detection datasets and 28 unseen
anomaly detection datasets respectively. It is followed by DevNet
and DSAD in seen AD, and DevNet and FEAWAD in unseen AD.
When using a Conover post-hoc test at the 95% confidence level,
PReNet performs significant better than all methods except DevNet
and DSAD in seen AD; it significantly outperforms all other nine
methods in unseen AD.
Table 7: AUC-PR results for unseen AD datasets.
rec, dos, fuz
rec, bac, fuz
rec, bac, dos
dos, bac, fuz
Table 8: Average (ordinal and percentile) rank of methods
based on AUC-PR for seen and unseen AD across the 12 and
28 datasets, respectively. The methods are sequentially sorted
based on the ordinal rank of seen and unseen AD.
Percentile
Percentile
KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA.
Pang, et al.