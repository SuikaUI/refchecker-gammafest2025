Hypergraph Convolution and Hypergraph Attention
Song Bai 1 Feihu Zhang 1 Philip H.S. Torr 1
Recently, graph neural networks have attracted
great attention and achieved prominent performance in various research ﬁelds. Most of those
algorithms have assumed pairwise relationships
of objects of interest. However, in many real applications, the relationships between objects are
in higher-order, beyond a pairwise formulation.
To efﬁciently learn deep embeddings on the highorder graph-structured data, we introduce two endto-end trainable operators to the family of graph
neural networks, i.e., hypergraph convolution and
hypergraph attention. Whilst hypergraph convolution deﬁnes the basic formulation of performing
convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module.
With the two operators, a graph neural network is
readily extended to a more ﬂexible model and applied to diverse applications where non-pairwise
relationships are observed. Extensive experimental results with semi-supervised node classiﬁcation demonstrate the effectiveness of hypergraph
convolution and hypergraph attention.
1. Introduction
Convolution
(CNNs) have led to a wide spectrum of breakthrough in various research domains, such
as visual recognition , speech recognition , machine translation etc. Due to its innate nature, CNNs hold an
extremely strict assumption, that is, input data shall have a
regular and grid-like structure. Such a limitation hinders the
promotion and application of CNNs to many tasks where
data of irregular structures widely exists.
ubiquitous
structures,
there is a growing interest in Graph Neural Networks
1Department of Engineering Science, University of Oxford,
Oxford, United Kingdom. Correspondence to: Song Bai < >.
Preliminary work.
Figure 1. The difference between a simple graph (a) and a hypergraph (b). In a simple graph, each edge, denoted by a line, only
connects two vertices. In a hypergraph, each edge, denoted by a
colored ellipse, connects more than two vertices.
(GNNs) , a methodology for learning
deep models with graph data. GNNs have a wide application in social science , knowledge
graph , recommendation system , geometrical computation , etc. And most existing methods assume that the
relationships between objects of interest are in pairwise formulations. Speciﬁcally in a graph model, it means that each
edge only connects two vertices (see Fig. 1(a)).
However, in many real applications, the object relationships
are much more complex than pairwise. For instance in
recommendation systems, an item may be commented by
multiple users. By taking the items as vertices and the rating
of users as edges of a graph, each edge may connect more
than two vertices. In this case, the afﬁnity relations are
no longer dyadic (pairwise), but rather triadic, tetradic or
of a higher-order. This brings back the concept of hypergraph , a special
graph model which leverages hyperedges to connect multiple vertices simultaneously (see Fig. 1(b)). Unfortunately,
most existing variants of graph neural networks are not applicable to the
high-order structure encoded by hyperedges.
Although the machine learning community has witnessed
the prominence of graph neural networks in learning patterns on simple graphs, the investigation of deep learning
on hypergraphs is still in a very nascent stage. Considering
its importance, we propose hypergraph convolution and hypergraph attention in this work, as two strong supplemental
operators to graph neural networks. The advantages and
contributions of our work are as follows
 
Hypergraph Convolution and Hypergraph Attention
1) Hypergraph convolution deﬁnes a basic convolutional
operator in a hypergraph. It enables an efﬁcient information propagation between vertices by fully exploiting the
high-order relationship and local clustering structure therein.
We mathematically prove that graph convolution is a special case of hypergraph convolution when the non-pairwise
relationship degenerates to a pairwise one.
2) Apart from hypergraph convolution where the underlying
structure used for propagation is pre-deﬁned, hypergraph
attention further exerts an attention mechanism to learn a
dynamic connection of hyperedges. Then, the information
propagation and gathering is done in task-relevant parts
of the graph, thereby generating more discriminative node
embeddings.
3) Both hypergraph convolution and hypergraph attention
are end-to-end trainable, and can be inserted into most variants of graph neural networks as long as non-pairwise relationships are observed. Extensive experimental results on
benchmark datasets demonstrate the efﬁcacy of the proposed
methods for semi-supervised node classiﬁcation.
2. Related Work
Graphs are a classic kind of data structure, where its vertices
represent the objects and its edges linking two adjacent
vertices describe the relationship between the corresponding
Graph Neural Network (GNN) is a methodology for learning deep models or embeddings on graph-structured data,
which was ﬁrst proposed by . One key
aspect in GNN is to deﬁne the convolutional operator in the
graph domain. ﬁrstly deﬁne convolution
in the Fourier domain using the graph Laplacian matrix,
and generate non-spatially localized ﬁlters with potentially
intense computations. enable the spectral ﬁlters spatially localized using a parameterization with
smooth coefﬁcients. focus on the
efﬁciency issue and use a Chebyshev expansion of the graph
Laplacian to avoid an explicit use of the graph Fourier basis. further simplify the ﬁltering
by only using the ﬁrst-order neighbors and propose Graph
Convolutional Network (GCN), which has demonstrated
impressive performance in both efﬁciency and effectiveness
with semi-supervised classiﬁcation tasks.
Meanwhile, some spatial algorithms directly perform convolution on the graph. For instance, 
learn different parameters for nodes with different degrees,
then average the intermediate embeddings over the neighborhood structures. propose the PATCHY-
SAN architecture, which selects a ﬁxed-length sequence
of nodes as the receptive ﬁeld and generate local normalized neighborhood representations for each of the nodes in
the sequence. demonstrate that
diffusion-based representations can serve as an effective
basis for node classiﬁcation. further
explore a joint usage of diffusion and adjacency basis in
a dual graph convolutional network. 
deﬁnes a uniﬁed framework via a message passing function,
where each vertex sends messages based on its states and
updates the states based on the message of its immediate
neighbors. propose GraphSAGE,
which customizes three aggregating functions, i.e., elementwise mean, long short-term memory and pooling, to learn
embeddings in an inductive setting.
Moreover, some other works concentrate on gate mechanism , skip connection ,
jumping connection , attention mechanism , sampling strategy , hierarchical representation ,
generative models , adversarial attack , etc. As a thorough review is simply unfeasible due to the space limitation,
we refer interested readers to surveys for more representative
methods. For example, and present two systematical and comprehensive surveys
over a series of variants of graph neural networks. provide a review of geometric deep learning. generalize and extend various approaches and show how graph neural networks can support
relational reasoning and combinatorial generalization. particularly focus on the attention models for
graphs, and introduce three intuitive taxonomies. propose a uniﬁed framework called MoNet,
which summarizes Geodesic CNN ,
Anisotropic CNN , GCN and Diffusion CNN as its special cases.
As analyzed above, most existing variants of GNN assume
pairwise relationships between objects, while our work
operates on a high-order hypergraph where the between-object relationships
are beyond pairwise. Hypergraph learning methods differ
in the structure of the hypergraph, e.g., clique expansion
and star expansion , and the deﬁnition of
hypergraph Laplacians . Following , propose a hypergraph neural network using a Chebyshev expansion of the graph Laplacian. By analyzing the
incident structure of a hypergraph, our work directly deﬁnes
two differentiable operators, i.e., hypergraph convolution
and hypergraph attention, which is intuitive and ﬂexible in
learning more discriminative deep embeddings.
Hypergraph Convolution and Hypergraph Attention
3. Proposed Approach
In this section, we ﬁrst give the deﬁnition of hypergraph
in Sec. 3.1, then elaborate the proposed hypergraph convolution and hypergraph attention in Sec. 3.2 and Sec. 3.3,
respectively. At last, Sec. 3.4 provides a deeper analysis of
the properties of our methods.
3.1. Hypergraph Revisited
Most existing works operate on a simple graph G = (V, E), where
V = {v1, v2, ..., vN} denotes the vertex set and E ⊆V ×V
denotes the edge set. A graph adjacency matrix A ∈RN×N
is used to reﬂect the pairwise relationship between every
two vertices. The underlying assumption of such a simple
graph is that each edge only links two vertices. However, as
analyzed above, the relationships between objects are more
complex than pairwise in many real applications.
To describe such a complex relationship, a useful graph
model is hypergraph, where a hyperedge can connect more
than two vertices. Let G = (V, E) be a hypergraph with
N vertices and M hyperedges. Each hyperedge ϵ ∈E is
assigned a positive weight Wϵϵ, with all the weights stored
in a diagonal matrix W ∈RM×M. Apart from a simple
graph where an adjacency matrix is deﬁned, the hypergraph
G can be represented by an incidence matrix H ∈RN×M
in general. When the hyperedge ϵ ∈E is incident with a
vertex vi ∈V , in order words, vi is connected by ϵ, Hiϵ = 1,
otherwise 0. Then, the vertex degree is deﬁned as
and the hyperedge degree is deﬁned as
Note that D ∈RN×N and B ∈RM×M are both diagonal
In the following, we deﬁne the operator of convolution on
the hypergraph G.
3.2. Hypergraph Convolution
The primary obstacle to deﬁning a convolution operator in a
hypergraph is to measure the transition probability between
two vertices, with which the embeddings (or features) of
each vertex can be propagated in a graph neural network.
To achieve this, we hold two assumptions: 1) more propagations should be done between those vertices connected
by a common hyperedge, and 2) the hyperedges with larger
weights deserve more conﬁdence in such a propagation.
Then, one step of hypergraph convolution is deﬁned as
HiϵHjϵWϵϵx(l)
where x(l)
is the embedding of the i-th vertex in the
(l)-th layer. σ(·) is a non-linear activation function like
LeakyReLU and eLU . P ∈RF (l)×F (l+1) is the weight matrix between the
(l)-th and (l + 1)-th layer. Eq. (3) can be written in a matrix
X(l+1) = σ(HWHTX(l)P),
where X(l) ∈RN×F (l) and X(l+1) ∈RN×F (l+1) are the
input of the (l)-th and (l + 1)-th layer, respectively.
However, HWHT does not hold a constrained spectral radius, which means that the scale of X(l) will be possibly
changed. In optimizing a neural network, stacking multiple
hypergraph convolutional layers like Eq. (4) can then lead
to numerical instabilities and increase the risk of exploding/vanishing gradients. Therefore, a proper normalization
is necessary. Thus, we impose a symmetric normalization
and arrive at our ﬁnal formulation
X(l+1) = σ(D−1/2HWB−1HTD−1/2X(l)P).
Here, we recall that D and B are the degree matrices
of the vertex and hyperedge in a hypergraph, respectively.
It is easy to prove that the maximum eigenvalue of D−1/2HWB−1HTD−1/2 is no larger than 1,
which stems from a fact that I −
D−1/2HWB−1HTD−1/2 is a positive semi-deﬁnite matrix. I is an identity matrix of an appropriate size.
Alternatively, a row-normalization is also viable as
X(l+1) = σ(D−1HWB−1HTX(l)P),
which enjoys similar mathematical properties as Eq. (5),
except that the propagation is directional and asymmetric in
this case.
As X(l+1) is differentiable with respect to X(l) and P, we
can use hypergraph convolution in end-to-end training and
optimize it via gradient descent.
3.3. Hypergraph Attention
Hypergraph convolution has an innate attentional mechanism . As we can
ﬁnd from Eq. (5) and Eq. (6), the transition probability between vertices is non-binary, which means that for a given
vertex, the afferent and efferent information ﬂow is explicitly assigned a diverse magnitude of importance. However,
such an attentional mechanism is not learnable and trainable
after the incidence matrix H is deﬁned.
Hypergraph Convolution and Hypergraph Attention
Convolution
Transition Probability
Nonlinearity
Incidence Matrix
Figure 2. Schematic illustration of hypergraph convolution with 5 vertices and 2 hyperedges. With an optional attention mechanism,
hypergraph convolution upgrades to hypergraph attention.
One natural solution is to exert an attention learning module
on H. In this circumstance, instead of treating each vertex
as being connected by a certain hyperedge or not, the attention module presents a probabilistic model, which assigns
non-binary and real values to measure the degree of connectivity. We expect that the probabilistic model can learn more
category-discriminative embeddings and the relationship
between vertices can be more accurately described.
Nevertheless, hypergraph attention is only feasible when
the vertex set and the hyperedge set are from (or can be
projected to) the same homogeneous domain, since only in
this case, the similarities between vertices and hyperedges
are directly comparable. In practice, it depends on how the
hypergraph G is constructed. For example, apply hypergraph learning to image retrieval where
each vertex collects its k-nearest neighbors to form a hyperedge, as also the way of constructing hypergraphs in our
experiments. When the vertex set and the edge set are comparable, we deﬁne the procedure of hypergraph attention
inspired by . For a given vertex xi
and its associated hyperedge xj, the attentional score is
exp (σ(sim(xiP, xjP)))
k∈Ni exp (σ(sim(xiP, xkP))),
where σ(·) is a non-linear activation function and sim(·) is
a similarity function that computes the pairwise similarity
between two vertices. Ni is the neighborhood set of xi,
which can be pre-accessed on some benchmarks, such as
the Cora and Citeseer datasets .
With the incidence matrix H enriched by an attention module, one can also follow Eq. (5) and Eq. (6) to learn the
intermediate embedding of vertices layer-by-layer. Note
that hypergraph attention also propagates gradients to H in
addition to X(l) and P.
In some applications, the vertex set and the hyperedge set
are from two heterogeneous domains. For instance, assume that attributes are hyperedges to connect
objects like newspaper or text. Then, it is problematic to
directly learn an attention module over the incidence matrix
H. We leave this issue for future work.
3.4. Summary and Remarks
The pipeline of the proposed hypergraph convolution and
hypergraph attention is illustrated in Fig. 2. Both two operators can be inserted into most variants of graph neural
networks when non-pairwise relationships are observed, and
used for end-to-end training.
As the only difference between hypergraph convolution and
hypergraph attention is an optional attention module on the
incidence matrix H, below we take hypergraph convolution
as a representative to further analyze the properties of our
methods. Note that the analyses also hold for hypergraph
attention.
Relationship with Graph Convolution. We prove that
graph convolution is a special case
of hypergraph convolution mathematically.
Let A ∈RN×N be the adjacency matrix used in graph
convolution. When each edge only links two vertices in a
hypergraph, the vertex degree matrix B = 2I. Assuming
equal weights for all the hyperedges (i.e., W = I), we have
an interesting observation of hypergraph convolution. Based
on Eq. (5), the deﬁnition of hypergraph convolution then
X(l+1) =σ(1
2D−1/2HHTD−1/2X(l)P),
2D−1/2(A + D)D−1/2X(l)P
2(I + D−1/2AD−1/2)X(l)P
=σ( ˆAX(l)P),
where ˆA = 1/2 eA and eA = I+D−1/2AD−1/2. As we can
Hypergraph Convolution and Hypergraph Attention
see, Eq. (8) is exactly equivalent to the deﬁnition of graph
convolution ).
Note that eA has eigenvalues in the range . To avoid
scale changes, have suggested a
re-normalization trick, that is
ˆA = eD−1/2 eA eD−1/2.
In the speciﬁc case of hypergraph convolution, we are using
a simpliﬁed solution, that is dividing eA by 2.
With GCN as a bridge and springboard to the family of
graph neural networks, it then becomes feasible to build
connections with other frameworks, e.g., MoNet , and develop the higher-order counterparts of
those variants to deal with non-pairwise relationships.
Implementation in Practice. The implementation of hypergraph convolution appears sophisticated as 6 matrices
are multiplied for symmetric convolution (see Eq. (5)) and
5 matrices are multiplied for asymmetric convolution (see
Eq. (6)). However, it should be mentioned that D, W and
B are all diagonal matrices, which makes it possible to
efﬁciently implement it in common used deep learning platforms.
For asymmetric convolution, we have from Eq. (6) that
D−1HWB−1HT = (D−1H)W(HB−1)T,
where D−1H and HB−1 perform L1 normalization of the
incidence matrix H over rows and columns, respectively. In
space-saving applications where matrix-form variables are
allowed, normalization can be simply done using standard
built-in functions in public neural network packages.
In case of space-consuming applications, one can readily
implement a sparse version of hypergraph convolution as
well. Since H is usually a sparse matrix, Eq. (10) does
not necessarily decrease the sparsity too much. Hence, we
can conduct normalization only on non-zero indices of H,
resulting in a sparse transition matrix.
Symmetric hypergraph convolution deﬁned in Eq. (5) can
be implemented similarly, with a minor difference in normalization using the vertex degree matrix D.
Skip Connection. Hypergraph convolution can be integrated with skip connection as
X(l), Hk, Pk
where HConv(·) represents the hypergraph convolution operator deﬁned in Eq. (5) (or Eq. (6)). Some similar structures
 adopted in
Highway-GCN ) can be also applied.
It has been demonstrated that deep
graph models cannot improve the performance even with
Table 1. Overview of data statistics.
skip connections since the receptive ﬁeld grows exponentially with respect to the model depth. In the experiments,
we will verify the compatibility of the proposed operators
with skip connections in model training.
Multi-head. To stabilize the learning process and improve the representative power of networks, multi-head
(a.k.a. multi-branch) architecture is suggested in relevant
works, e.g., . hypergraph convolution can be also extended in that way, as
X(l), Hk, Pk
X(l+1) = Aggregate
where Aggregate(·) is a certain aggregation like concatenation or average pooling. Hk and Pk are the incidence
matrix and weight matrix corresponding to the k-th head,
respectively. Note that only in hypergraph attention, Hk is
different over different heads.
4. Experiments
In this section, we evaluate the proposed hypergraph convolution and hypergraph attention in the task of semisupervised node classiﬁcation.
4.1. Experimental Setup
Datasets. We employ three citation network datasets, including the Cora, Citeseer and Pubmed datasets , following previous representative works . Table 1 presents an
overview of the statistics of the three datasets.
The Cora dataset contains 2, 708 scientiﬁc publications divided into 7 categories. There are 5, 429 edges in total, with
each edge being a citation link from one article to another.
Each publication is described by a binary bag-of-word representation, where 0 (or 1) indicates the absence (or presence)
of the corresponding word from the dictionary. The dictionary consists of 1, 433 unique words.
Like the Cora dataset, the Citeseer dataset contains 3, 327
scientiﬁc publications, divided into 6 categories and linked
by 4, 732 edges. Each publication is described by a binary
bag-of-word representation of 3, 703 dimensions.
The Pubmed dataset is comprised of 19, 717 scientiﬁc pub-
Hypergraph Convolution and Hypergraph Attention
lications divided into 3 classes. The citation network has
44, 338 links. Each publication is described by a vectorial
representation using Term Frequency-Inverse Document
Frequency (TF-IDF), drawn from a dictionary with 500
As for the training-testing data split, we adopt the setting
used in . In each dataset, 20 articles per
category are used for model training, which means the size
of training set is 140 for Cora, 120 for Citeseer and 60 for
Pubmed, respectively. Another 500 articles are used for validation purposes and 1000 articles are used for performance
evaluation.
Hypergraph Construction. Most existing methods interpret the citation network as the adjacency matrix of a simple graph by a certain kind of normalization, e.g., . In this work, we construct a higher-order
graph to enable hypergraph convolution and hypergraph attention. The whole procedure is divided into three steps: 1)
all the articles constitute the vertex set of the hypergraph;
2) each article is taken as a centroid and forms a hyperedge
to connect those articles which have citation links to it (either citing it or being cited); 3) the hyperedges are equally
weighted for simplicity, but one can set non-equal weights
to encode a prior knowledge if existing in other applications.
Implementation Details. We implement the proposed hypergraph convolution and hypergraph attention using Pytorch. As for the parameter setting and network structure,
we closely follow without a carefully parameter tuning and model design.
In more detail, a two-layer graph model is constructed. The
ﬁrst layer consists of 8 branches of the same topology, and
each branch generates an 8-dimensional hidden representation. The second layer, used for classiﬁcation, is a singlebranch topology and generates C-dimensional feature (C is
the number of classes). Each layer is followed by a nonlinearity activation and here we use Exponential Linear Unit
(ELU) . L2 regularization is applied
to the parameters of network with λ = 0.0003 on the Cora
and Citeseer datasets and λ = 0.001 on the Pubmed dataset,
respectively.
Speciﬁcally in hypergraph attention, dropout with a rate of
0.6 is applied to both inputs of each layer and the attention
transition matrix. As for the computation of the attention
incidence matrix H in Eq. (7), we employ a linear transform
as the similarity function sim(·), followed by LeakyReLU
nonlinearity with the negative input slope
set to 0.2. On the Pubmed dataset, we do not use 8 output
attention heads for classiﬁcation to ensure the consistency
of network structures.
We train the model by minimizing the cross-entropy loss on
the training nodes using the Adam 
Table 2. The comparison with baseline methods in terms of classi-
ﬁcationa accuracy (%). “Hyper-Conv.” denotes hypergraph convolution and “Hyper-Atten.” denotes hypergraph attention.
Cora dataset
Citeseer dataset
Hyper-Conv. (ours)
GCN*+Hyper-Conv. (ours)
Hyper-Atten. (ours)
GAT*+Hyper-Atten. (ours)
optimizer with a learning rate of 0.005 on the Cora and Citeseer datasets and 0.01 on the Pubmed dataset, respectively.
An early stop strategy is adopted on the validation loss with
a patience of 100 epochs. For all the experiments, we report
the mean classiﬁcation accuracy of 100 trials on the testing dataset. The standard deviation, generally smaller than
0.5%, is not reported due to the space limitation.
4.2. Analysis
We ﬁrst analyze the properties of hypergraph convolution
and hypergraph attention with a series of ablation studies.
The comparison is primarily done with Graph Convolution
Network (GCN) and Graph Attention Network (GAT) , which are two
latest representatives of graph neural networks that have
close relationships with our methods.
For a fair comparison, we reproduce the performance of
GCN and GAT with exactly the same experimental setting
aforementioned. Thus, we denote them by GCN* and GAT*
in the following. Moreover, we employ the same normalization strategy as GCN, i.e., symmetric normalization in
Eq. (5) for hypergraph convolution, and the same strategy
as GAT, i.e., asymmetric normalization in Eq. (6) for hypergraph attention. They are denoted by Hyper-Conv. and
Hyper-Atten. for short, respectively.
We modify the model of GAT to implement GCN by removing the attention module and directly feeding the graph
adjacency matrix with the normalization trick proposed in
GCN. Two noteworthy comments are made here. First, although the architecture of GCN* differs from the original
one, the principle of performing graph convolution is the
same. Second, directly feeding the graph adjacency matrix
is not equivalent to the constant attention described in GAT
as normalization is used in our case. In GAT, the constant
attention weight is set to 1 without normalization.
Comparisons with Baselines. The comparison with baseline methods is given in Table 2.
We ﬁrst observe that hypergraph convolution and hypergraph attention, as non-pairwise models, consistently outperform its corresponding pairwise models, i.e., graph
Hypergraph Convolution and Hypergraph Attention
convolution network (GCN*) and graph attention network
(GAT*). For example on the Citeseer dataset, hypergraph
attention achieves a classiﬁcation accuracy of 70.88, an
improvement of 0.86 over GAT*. This demonstrates the
beneﬁt of considering higher-order models in graph neural
networks to exploit non-pairwise relationships and local
clustering structure parameterized by hyperedges.
Compared with hypergraph convolution, hypergraph attention adopts a data-driven learning module to dynamically
estimate the strength of each link associated with vertices
and hyperedges. Thus, the attention mechanism helps hypergraph convolution embed the non-pairwise relationships
between objects more accurately. As presented in Table 2,
the performance improvements brought by hypergraph attention are 0.42 and 0.53 over hypergraph convolution on
the Cora and Citeseer datasets, respectively.
Although non-pairwise models proposed in this work have
achieved improvements over pairwise models, one cannot
hastily deduce that non-pairwise models are more capable
in learning robust deep embeddings under all circumstances.
A rational claim is that they are suitable for different applications as real data may convey different structures. Some
graph-structured data can be only modeled in a simple graph,
some can be only modeled in a higher-order graph and
others are suitable for both. Nevertheless, as analyzed in
Sec. 3.4, our method presents a more ﬂexible operator in
graph neural networks, where graph convolution and graph
attention are special cases of non-pairwise models with guaranteed mathematical properties and performance.
One may be also interested in another question, i.e., does it
bring performance improvements if using hypergraph convolution (or attention) in conjunction with graph convolution
(or attention)? We further investigate this by averaging the
transition probability learned by non-pairwise models and
pairwise models with equal weights, and report the results
in Table 2. As it shows, a positive synergy is only observed
on the Cora dataset, where the best results of convolution
operator and attention operator are improved to 82.63 and
82.74, respectively. By contrast, our methods encounter a
slight performance decrease on the Citeseer dataset. From
another perspective, it also supports our above claim that
different data may ﬁt different structural graph models.
Analysis of Skip Connection. We study the inﬂuence of
skip connection by adding an identity
mapping in the ﬁrst hypergraph convolution layer. We report
in Table 3 two settings of the weight decay, i.e., λ=3e-4
(default setting) and λ=1e-3.
As it shows, both GCN* and GAT* report lower recognition
rates when integrated with skip connection compared with
those reported in Table 2. In comparison, the proposed
non-pairwise models, especially hypergraph convolution,
Table 3. The compatibility of skip connection in terms of classiﬁcation accuracy (%).
Hyper-Conv. (ours)
Hyper-Atten. (ours)
Table 4. The inﬂuence of the length of hidden representation on
the Cora dataset.
The length of hidden representation
Hyper-Conv.
seem to beneﬁt from skip connection. For instance, the bestperforming trial of hypergraph convolution yields 82.66 on
the Cora dataset, better than 82.19 achieved without skip
connection, and yields 70.83 on the Citeseer dataset, better
than 70.35 achieved without skip connection.
Such experimental results encourage us to further train a
much deeper model implemented with hypergraph convolution or hypergraph attention (say up to 10 layers). However,
we also witness a performance deterioration either with or
without skip connection. This reveals that a better training paradigm and architecture are still urgently required for
graph neural networks.
Analysis of Hidden Representation. Table 4 presents the
performance comparison between GCN* and hypergraph
convolution with an increasing length of the hidden representation. The number of heads is set to 1.
It is easy to ﬁnd that the performance keeps increasing with
an increase of the length of the hidden representation, then
peaks when the length is 16. Moreover, hypergraph convolution consistently beats GCN* with a variety of feature
dimensions. As the only difference between GCN* and
hypergraph convolution is the used graph structure, the performance gain purely comes from a more robust way of
establishing the relationships between objects. It ﬁrmly
demonstrates the ability of our methods in graph knowledge
embedding.
4.3. Comparison with State-of-the-art
We compare our method with the state-of-the-art algorithms,
which have followed the experimental setting in and reported classiﬁcation accuracies on the Cora,
Citeseer and Pubmed datasets. Note that the results are
directly quoted from the original papers, instead of being re-implemented in this work. Besides GCN and GAT,
the selected algorithms also include Manifold Regulariza-
Hypergraph Convolution and Hypergraph Attention
Table 5. Comparison with the state-of-the-art methods in terms of classiﬁcation accuracy (%). The best and second best results are marked
in red and blue, respectively.
Cora dataset
Citeseer dataset
Pubmed dataset
Multilayer Perceptron
Manifold Regularization 
Semi-supervised Embedding 
Label Propagation 
DeepWalk 
Iterative Classiﬁcation Algorithm 
Planetoid 
Chebyshev 
Graph Convolutional Network 
MoNet 
Variance Reduction 
Graph Attention Network 
tion , Semi-supervised Embedding , Label Propagation ,
DeepWalk , Iterative Classiﬁcation Algorithm , Planetoid ,
Chebyshev , MoNet , and Variance Reduction .
As presented in Table 5, our method achieves the second
best performance on the Cora and Citeseer dataset, which
is slightly inferior to GAT by 0.3
and 1.3, respectively. The performance gap is attributed
to multiple factors, such as the difference in deep learning
platforms and better parameter tuning. As shown in Sec. 4.2,
thorough experimental comparisons under the same setting
have demonstrated the beneﬁt of learning deep embeddings
using the proposed non-pairwise models. Nevertheless, we
emphasize again that pairwise and non-pairwise models
have different application scenarios, and existing variants
of graph neural networks can be easily extended to their
non-pairwise counterparts with the proposed two operators.
On the Pubmed dataset, hypergraph attention reports 78.4 in
classiﬁcation accuracy, better than 78.1 achieved by GAT*.
As described in Sec. 4.1, the original implementation of
GAT adopts 8 output attention heads while only 1 is used
in GAT* to ensure the consistency of model architectures.
Even though, hypergraph attention also achieves a comparable performance with the state-of-the-art methods.
5. Conclusion and Future Work
In this work, we have contributed two end-to-end trainable
operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. While most
variants of graph neural networks assume pairwise relationships of objects of interest, the proposed operators handle
non-pairwise relationships modeled in a high-order hypergraph. We theoretically demonstrate that some recent representative works, e.g., graph convolution network and graph attention network , are special cases of our methods. Hence, our
proposed hypergraph convolution and hypergraph attention
are more ﬂexible in dealing with arbitrary orders of relationships and diverse applications, where both pairwise and nonpairwise formulations are likely to be involved. Thorough
experimental results with semi-supervised node classiﬁcation demonstrate the efﬁcacy of the proposed methods.
There are still some challenging directions that can be further investigated. Some of them are inherited from the
limitation of graph neural networks, such as training substantially deeper models with more than a hundred of layers , handling dynamic structures , batch-wise model training, etc. Meanwhile, some issues are directly related to the
proposed methods in high-order learning. For example, although hyperedges are equally weighted in our experiments,
it is promising to exploit a proper weight mechanism when
extra knowledge of data distributions is accessible, and even,
adopt a learnable module in a neural network then optimize
the weight with gradient descent. The current implementation of hypergraph attention cannot be executed when the
vertex set and the hyperedge set are from two heterogeneous
domains. One possible solution is to learn a joint embedding
to project the vertex features and edge features in a shared
latent space, which requires further exploration.
Moreover, it is also interesting to plug hypergraph convolution and hypergraph attention into other variants of
graph neural network, e.g., MoNet ,
GraphSAGE and GCPN , and apply them to other domain-speciﬁc applications, e.g., 3D shape analysis , visual question answering , chemistry and NP-hard
problems .
Hypergraph Convolution and Hypergraph Attention