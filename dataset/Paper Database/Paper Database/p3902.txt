Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14–25,
October 25-29, 2014, Doha, Qatar. c⃝2014 Association for Computational Linguistics
Translation Modeling with Bidirectional Recurrent Neural Networks
Martin Sundermeyer1, Tamer Alkhouli1, Joern Wuebker1, and Hermann Ney1,2
1Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Aachen, Germany
2Spoken Language Processing Group
Univ. Paris-Sud, France and LIMSI/CNRS, Orsay, France
{surname}@cs.rwth-aachen.de
This work presents two different translation models using recurrent neural networks. The ﬁrst one is a word-based approach using word alignments.
we present phrase-based translation models that are more consistent with phrasebased decoding. Moreover, we introduce
bidirectional recurrent neural models to
the problem of machine translation, allowing us to use the full source sentence in our
models, which is also of theoretical interest. We demonstrate that our translation
models are capable of improving strong
baselines already including recurrent neural
IWSLT 2013 German→English, BOLT
Arabic→English and Chinese→English.
We obtain gains up to 1.6%
and 1.7% TER by rescoring 1000-best
Introduction
Neural network models have recently experienced
unprecedented attention in research on statistical
machine translation (SMT). Several groups have
reported strong improvements over state-of-the-art
baselines using feedforward neural network-based
language models , as well as translation models .
Different from the feedforward design, recurrent
neural networks (RNNs) have the advantage of being able to take into account an unbounded history of previous observations. In theory, this enables them to model long-distance dependencies
of arbitrary length. However, while previous work
on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented
on top of recurrent language models 
neural networks, a special type of recurrent neural
networks we make use of in this work, is given.
Section 4 describes our novel translation models.
Finally, experiments are presented in Section 5
and we conclude with Section 6.
Related Work
In this Section we contrast previous work to ours,
where we design RNNs to model bilingual dependencies, which are applied to rerank n-best lists
after decoding.
To the best of our knowledge, the earliest attempts to apply neural networks in machine translation (MT) are presented in , where they were used for
example-based MT.
Recently, Le et al. presented translation
models using an output layer with classes and
a shortlist for rescoring using feedforward networks. They compare between word-factored and
tuple-factored n-gram models, obtaining their best
results using the word-factored approach, which is
less amenable to data sparsity issues. Both of our
word-based and phrase-based models eventually
work on the word level. Kalchbrenner and Blunsom use recurrent neural networks with
full source sentence representations. The continuous representations are obtained by applying a sequence of convolutions, and the result is fed into
the hidden layer of a recurrent language model.
Rescoring results indicate no improvements over
the state of the art.
Auli et al. also include source sentence representations built either
using Latent Semantic Analysis or by concatenating word embeddings. This approach produced
no notable gain over systems using a recurrent
language model.
On the other hand, our proposed bidirectional models include the full source
sentence relying on recurrency, yielding improvements over competitive baselines already including a recurrent language model.
RNNs were also used with minimum translation
units , which are phrase pairs undergoing certain constraints. At the input layer,
each of the source and target phrases are modeled as a bag of words, while the output phrase
is predicted word-by-word assuming conditional
independence.
The approach seeks to alleviate
data sparsity problems that would arise if phrases
were to be uniquely distinguished. Our proposed
phrase-based models maintain word order within
phrases, but the phrases are processed in a wordpair manner, while the phrase boundaries remain
implicitly encoded in the way the words are presented to the network. Schwenk proposed
a feedforward network that predicts phrases of a
ﬁxed maximum length, such that all phrase words
are predicted at once.
The prediction is conditioned on the source phrase.
Since our phrasebased model predicts one word at a time, it does
not assume any phrase length.
Moreover, our
model’s predictions go beyond phrase boundaries
and cover unbounded history and future contexts.
Using neural networks during decoding requires tackling the costly output normalization
Vaswani et al. avoid this step by
training feedforward neural language models using noise contrastive estimation, while Devlin et
al. augment the training objective function
to produce approximately normalized scores directly. The latter work makes use of translation
and joint models, and pre-computes the ﬁrst hidden layer beforehand, resulting in large speedups.
They report major improvements over strong baselines. The speedups achieved by both works allowed to integrate feedforward neural networks
into the decoder.
LSTM Recurrent Neural Networks
Our work is based on recurrent neural networks.
In related ﬁelds like e. g. language modeling, this
type of neural network has been shown to perform
considerably better than standard feedforward architectures .
Most commonly, recurrent neural networks are
trained with stochastic gradient descent (SGD),
where the gradient of the training criterion is computed with the backpropagation through time algorithm . However, the combination of RNN networks with conventional backpropagation training leads to conceptual difﬁculties which are known as the vanishing (or exploding) gradient problem, described e. g. in . To remedy this problem, in it was suggested to
modify the architecture of a standard RNN in such
a way that vanishing and exploding gradients are
avoided during backpropagation. In particular, no
modiﬁcation of the training algorithm is necessary.
The resulting architecture is referred to as long
short-term memory (LSTM) neural network.
Bidirectional
(BRNNs) were ﬁrst proposed in and applied to speech recognition
tasks. They have been since applied to different
incredibly
(a) Original
ϵunaligned
incredibly
ϵunaligned
(b) One-to-one alignment
Figure 1: Example sentence from the German→English IWSLT data. The one-to-one alignment is
created by introducing ϵaligned and ϵunaligned tokens.
tasks like parsing and spoken
language understanding .
Bidirectional long short-term memory (BLSTM)
networks are BRNNs using LSTM hidden layers
 .
introduces BLSTMs to the problem of machine
translation, allowing powerful models that employ
unlimited history and future information to make
predictions.
While the proposed models do not make any assumptions about the type of RNN used, all of our
experiments make use of recurrent LSTM neural
networks, where we include later LSTM extensions proposed in . The cross-entropy error criterion is used
for training. Further details on LSTM neural networks can be found in .
Translation Modeling with RNNs
In the following we describe our word- and
phrase-based translation models in detail. We also
show how bidirectional RNNs can enable such
models to include full source information.
Resolving Alignment Ambiguities
Our word-based recurrent models are only de-
ﬁned for one-to-one-aligned source-target sentence pairs. In this work, we always evaluate the
model in the order of the target sentence. However, we experiment with several different ways
to resolve ambiguities due to unaligned or multiply aligned words.
To that end, we introduce
two additional tokens, ϵaligned and ϵunaligned. Undev
w/o ϵunaligned
source identity
target identity
Table 1: Comparison of including different sets
of ϵ tokens into the one-to-one alignment on the
IWSLT 2013 German→English task using the unidirectional RNN translation model.
aligned words are either removed or aligned to an
extra ϵunaligned token on the opposite side. If an
ϵunaligned is introduced on the target side, its position is determined by the aligned source word that
is closest to the unaligned source word in question,
preferring left to right. To resolve one-to-many
alignments, we use an IBM-1 translation table to
decide for one of the alignment connections to be
kept. The remaining words are also either deleted
or aligned to additionally introduced ϵaligned tokens on the opposite side. Fig. 1 shows an example sentence from the IWSLT data, where all ϵ
tokens are introduced.
In a short experiment, we evaluated 5 different setups with our unidirectional RNN translation
model (cf. next Section): without any ϵ tokens,
without ϵunaligned, source identity, target identity
and using all ϵ tokens. Source identity means we
introduce no ϵ tokens on source side, but all on
target side. Target identity is deﬁned analogously.
The results can be found in Tab. 1. We use the
setup with all ϵ tokens in all following experiments, which showed the best BLEU performance.
Word-based RNN Models
Given a pair of source sequence fI
1 = f1 . . . fI
and target sequence eI
1 = e1 . . . eI, where we assume a direct correspondence between fi and ei,
we deﬁne the posterior translation probability by
factorizing on the target words:
We denote the formulation (1) as the bidirectional
joint model (BJM). This model can be simpliﬁed
by several independence assumptions. First, we
drop the dependency on the future source information, receiving what we denote as the unidirectional joint model (JM) in (2). Here, d ∈N0 is
a delay parameter, which is set to d = 0 for all
experiments, except for the comparative results reported in Fig. 7. Finally, assuming conditional independence from the previous target sequence, we
receive the unidirectional translation model (TM)
in (3). Analogously, we can deﬁne a bidirectional
translation model (BTM) by keeping the dependency on the full source sentence fI
1 , but dropping
the previous target sequence ei−1
Fig. 2 shows the dependencies of the wordbased neural translation and joint models.
alignment points are traversed in target order and
at each time step one target word is predicted.
The pure translation model (TM) takes only source
words as input, while the joint model (JM) takes
the preceding target words as an additional input.
A delay of d > 0 is implemented by shifting the
target sequence by d time steps and ﬁlling the ﬁrst
d target positions and the last d source positions
with a dedicated ϵpadding symbol. The RNN architecture for the unidirectional word-based models
joint model
all models
bidirectional
ϵunaligned
incredibly
ϵunaligned
Figure 2: Dependencies modeled within the wordbased RNN models when predicting the target
word ’know’. Directly processed information is
depicted with solid rectangles, and information
available through recurrent connections is marked
with dashed rectangles.
is illustrated in Fig. 3, which corresponds to the
following set of equations:
yi = A1 ˆfi + A2ˆei−1
zi = ξ(yi; A3, yi−1
 c(ei)|ei−1
= ϕc(ei)(A4zi)
 ei|c(ei), ei−1
= ϕei(Ac(ei)zi)
 ei|c(ei), ei−1
 c(ei)|ei−1
Here, by ˆfi and ˆei−1 we denote the one-hot encoded vector representations of the source and
target words fi and ei−1. The outgoing activation values of the projection layer and the LSTM
layer are yi and zi, respectively. The matrices Aj
contain the weights of the neural network layers.
By ξ(· ; A3, yi−1
) we denote the LSTM formalism
that we plug in at the third layer. As the LSTM
layer is recurrent, we explicitly include the dependence on the previous layer activations yi−1
Finally, ϕ is the widely-used softmax function to
obtain normalized probabilities, and c denotes a
word class mapping from any target word to its
unique word class. For the bidirectional model,
the equations can be deﬁned analogously.
Due to the use of word classes, the output
layer consists of two parts. The class probability p
 c(ei)|ei−1
is computed ﬁrst, and then
 c(ei)|ei−1
 ei|c(ei), ei−1
class layer
output layer
LSTM layer
projection layer
input layer
Figure 3: Architecture of a recurrent unidirectional translation model. By including the dashed
parts, a joint model is obtained.
the word probability p
 ei|c(ei), ei−1
is obtained given the word class. This trick helps avoiding the otherwise computationally expensive normalization sum, which would be carried out over
all words in the target vocabulary.
In a classfactorized output layer where each word belongs
to a single class, the normalization is carried out
over all classes, whose number is typically much
less than the vocabulary size. The other normalization sum needed to produce the word probability is limited to the words belonging to the same
class .
Phrase-based RNN Models
One of the conceptual disadvantages of wordbased modeling as introduced in the previous section is that there is a mismatch between training and testing conditions: During neural network
training, the vocabulary has to be extended by additional ϵ tokens, and a one-to-one alignment is
used which does not reﬂect the situation in decoding.
In phrase-based machine translation, more
complex alignments in terms of multiple words
on both the source and the target sides are used,
which allow the decoder to make use of richer
short-distance dependencies and are crucial for the
performance of the resulting system.
From this perspective, it seems interesting to
standardize the alignments used in decoding, and
in training the neural network. However, it is dif-
ﬁcult to use the phrases themselves as the vocabulary of the RNN. Usually, the huge number of
potential phrases in comparison to the relatively
small amount of training data makes the learning of continuous phrase representations difﬁcult
zum Beispiel
, for example ,
zur Genüge
incredibly
Figure 4: Example phrase alignment for a sentence from the IWSLT training data.
due to data sparsity. This is conﬁrmed by results
presented in , which show that a
word-factored translation model outperforms the
phrase-factored version. Therefore, in this work
we continue relying on source and target word vocabularies for building our phrase representations.
However, we no longer use a direct correspondence between a source and a target word, as enforced in our word-based models.
Fig. 4 shows an example phrase alignment,
where a sequence of source words ˜fi is directly
mapped to a sequence of target words ˜ei for 1 ≤
i ≤˜I. By ˜I, we denote the number of phrases in
the alignment. We decompose the target sentence
posterior probability in the following way:
p(˜ei|˜ei−1
p(˜ei|˜ei−1
where the joint model in Eq. 5 would correspond
to a bidirectional RNN, and Eq. 6 only requires a
unidirectional RNN. By leaving out the conditioning on the target side, we obtain a phrase-based
translation model.
As there is no one-to-one correspondence between the words within a phrase, the basic idea of
our phrase-based approach is to let the neural network learn the dependencies itself, and present the
full source side of the phrase to the network before letting it predict target side words. Then the
probability for the target side of a phrase can be
computed, in case of Eq. 6, by:
p(˜ei|˜ei−1
 (˜ei)j|(˜ei)j−1
and analogously for the case of Eq. 5. Here, (˜ei)j
denotes the j-th word of the i-th aligned target
We feed the source side of a phrase into the neural network one word at a time. Only when the
output layer
LSTM layer
projection layer
input layer
incredibly
incredibly
Figure 5: A recurrent phrase-based joint translation model, unfolded over time. Source words are printed
in normal face, while target words are printed in bold face. Dashed lines indicate phrases from the
example sentence. For brevity, we omit the precise handling of sentence begin and end tokens.
presentation of the source side is ﬁnished we start
estimating probabilities for the target side. Therefore, we do not let the neural network learn a target
distribution until the very last source word is considered. In this way, we break up the conventional
RNN training scheme where an input sample is directly followed by its corresponding teacher signal. Similarly, the presentation of the source side
of the next phrase only starts after the prediction
of the current target side is completed.
To this end, we introduce a no-operation token,
denoted by ε, which is not part of the vocabulary
(which means it cannot be input to or predicted by
the RNN). When the ε token occurs as input, it indicates that no input needs to be processed by the
RNN. When the ε token occurs as a teacher signal
for the RNN, the output layer distribution is ignored, and does not even have to be computed. In
both cases, all the other layers are still processed
during forward and backward passes such that the
RNN state can be advanced even without additional input or output.
Fig. 5 depicts the evaluation of a phrase-based
joint model for the example alignment from Fig. 4.
For a source phrase ˜fi, we include (|˜ei|−1) many ε
symbols at the end of the phrase. Conversely, for
a target phrase ˜ei, we include (| ˜fi| −1) many ε
symbols at the beginning of the phrase.
E. g., in the ﬁgure, the second dashed rectangle from the left depicts the training of the English
phrase “, for example ,” and its German translation “zum Beispiel”. At the input layer, we feed in
the source words one at a time, while we present
ε tokens at the target side input layer and the output layer (with the exception of the very ﬁrst time
step, where we still have the last target word from
the previous phrase as input instead of ε). With
the last word of the source phrase “Beispiel” being
presented to the network, the full source phrase is
stored in the hidden layer, and the neural network
is then trained to predict the target phrase words
at the output layer. Subsequently, the source input
is ε, and the target input is the most recent target
side history word.
To obtain a phrase-aligned training sequence for
the phrase-based RNN models, we force-align the
training data with the application of leave-one-out
as described in .
Bidirectional RNN Architecture
While the unidirectional RNNs include an unbounded sentence history, they are still limited in
the number of future source words they include.
Bidirectional models provide a ﬂexible means to
also include an unbounded future context, which,
unlike the delayed unidirectional models, require
no tuning to determine the amount of delay.
Fig. 6 illustrates the bidirectional model architecture, which is an extension of the unidirectional
model of Fig. 3.
First, an additional recurrent
hidden layer is added in parallel to the existing
one. This layer will be referred to as the backward layer, since it processes information in backward time direction. This hidden layer receives
source word input only, while target words in the
case of a joint model are fed to the forward layer
as in the unidirectional case. Due to the backward
recurrency, the backward layer will make the information fI
i available when predicting the target
word ei, while the forward layer takes care of the
source history fi
1. Jointly, the forward and backward branches include the full source sentence fI
as indicated in Fig. 2. Fig. 6 shows the “deep”
variant of the bidirectional model, where the for-
 c(ei)|ei−1
 ei|c(ei), ei−1
class layer
output layer
2nd LSTM layer
1st LSTM layer
projection layer
input layer
Figure 6: Architecture of a recurrent bidirectional
translation model. By (+) and (−), we indicate
a processing in forward and backward time directions, respectively. The inclusion of the dashed
parts leads to a bidirectional joint model.
source projection matrix is used for the forward
and backward branches.
ward and backward layers converge into a hidden
layer. A shallow variant can be obtained if the
parallel layers converge into the output layer directly1.
Due to the full dependence on the source sequence, evaluating bidirectional networks requires
computing the forward pass of the forward and
backward layers for the full sequence, before being able to evaluate the next layers. In the backward pass of backpropagation, the forward and
backward recurrent layers are processed in decreasing and increasing time order, respectively.
Experiments
All translation experiments are performed with the
Jane toolkit . The largest part of our experiments is carried out on the IWSLT 2013 German→English
shared translation task.2 The baseline system is
trained on all available bilingual data, 4.3M sentence pairs in total, and uses a 4-gram LM with
modiﬁed Kneser-Ney smoothing , trained with
the SRILM toolkit . As additional
1In our implementation, the forward and backward layers
converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer.
2 
data sources for the LM we selected parts of the
Shufﬂed News and LDC English Gigaword corpora based on cross-entropy difference , resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT
system tuned with MERT
 . It contains a hierarchical reordering model and a 7gram word cluster language model . Here, we also compare against a feedforward joint model as described by Devlin et al.
 , with a source window of 11 words and a
target history of three words, which we denote as
BBN-JM. Instead of POS tags, we predict word
classes trained with mkcls. We use a shortlist
of size 16K and 1000 classes for the remaining
words. All neural networks are trained on the TED
portion of the data (138K segments) and are applied in a rescoring step on 1000-best lists.
To conﬁrm our results, we run additional
experiments
Arabic→English
Chinese→English tasks of the DARPA BOLT
project. In both cases, the neural network models
are added on top of our most competitive evaluation system. On Chinese→English, we use a
hierarchical phrase-based system trained on 3.7M
segments with 22 dense features, including an advanced orientation model . For
the neural network training, we selected a subset
of 9M running words.
The Arabic→English
system is a standard phrase-based decoder trained
on 6.6M segments, using 17 dense features. The
neural network training was performed using a
selection amounting to 15.5M running words.
For both tasks we apply the neural networks by
rescoring 1000-best lists and evaluate results on
two data sets from the ’discussion forum’ domain,
test1 and test2. The sizes of the data sets
for the Arabic→English system are: 1219 (dev),
1510 (test1), and 1137 (test2) segments, and
for the Chinese→English system are: 5074 (dev),
1844 (test1), and 1124 (test2) segments. All
results are measured in case-insensitive BLEU [%]
 and TER [%] on a single reference.
Our results on the IWSLT German→English task
are summarized in Tab. 2.
At this point, we
do not include a recurrent neural network lan-
BTM (deep)
BJM (deep)
PJM (10-best)
PJM (deep)
PBJM (deep)
German→English
models. T: translation, J: joint, B: bidirectional,
P: phrase-based.
guage model yet.
Here, the delay parameter d
from Equations 2 and 3 is set to zero. We observe that for all recurrent translation models, we
achieve substantial improvements over the baseline on the test data, ranging from 0.9 BLEU
up to 1.6 BLEU. These results are also consistent
with the improvements in terms of TER, where we
achieve reductions by 0.8 TER up to 1.8 TER.
These numbers can be directly compared to the
case of feedforward neural network-based translation modeling as proposed in 
which we include in the very last row of the table.
Nearly all of our recurrent models outperform the
feedforward approach, where the RNN model performing best on the dev data is better on test
by 0.3 BLEU and 1.0 TER.
Interestingly, for the recurrent word-based models, on the test data it can be seen that TMs perform better than JMs, even though TMs do not
take advantage of the target side history words.
However, exploiting this extra information does
not always need to result in a better model, as the
target side words are only derived from the given
source side, which is available to both TMs and
JMs. On the other hand, including future source
words in a bidirectional model clearly improves
the performance further. By adding another LSTM
Figure 7: BLEU scores on the IWSLT test set
with different delays for the unidirectional RNN-
TM and the bidirectional RNN-BTM.
layer that combines forward and backward time
directions (indicated as ‘deep’ in the table), we obtain our overall best model.
In Fig. 7 we compare the word-based bidirectional TM with a unidirectional TM that uses different time delays d = 0, . . . , 4. For a delay d =
2, the same performance is obtained as with the
bidirectional model, but this comes at the price of
tuning the delay parameter.
In comparison to the unidirectional word-based
models, phrase-based models perform similarly.
In the tables, we include those phrase-based variants which perform best on the dev data, where
phrase-based JMs always are at least as good or
better than the corresponding TMs in terms of
Therefore, we mainly report JM results
for the phrase-based networks.
A phrase-based
model can also be trained on multiple variants for
the phrase alignment.
For our experiments, we
tested 10-best alignments against the single best
alignment, which resulted in a small improvement
of 0.2 TER on both dev and test. We did not observe consistent gains by using an additional hidden layer or bidirectional models. To some extent, future information is already considered in
unidirectional phrase-based models by feeding the
complete source side before predicting the target
Tab. 3 shows different model combination results for the IWSLT task, where a recurrent language model is included in the baseline. Adding
a deep bidirectional TM or JM to the recurrent language model improves the RNN-LM baseline by 1.2 BLEU or 1.1 BLEU, respectively.
phrase-based model substantially improves over
baseline (w/ RNN-LM)
BTM (deep)
BJM (deep)
PBJM (deep)
4 RNN models
Table 3: Results for the IWSLT 2013 German→English task with different RNN models. All results
include a recurrent language model. T: translation, J: joint, B: bidirectional, P: phrase-based.
the RNN-LM baseline, but performs not as good
as its word-based counterparts. By adding four
different translation models, including models in
reverse word order and reverse translation direction, we are able to improve these numbers even
further. However, especially on the test data, the
gains from model combination saturate quickly.
Apart from the IWSLT track, we also analyze the performance of our translation models on
the BOLT Chinese→English and Arabic→English
translation tasks. Due to the large amount of training data, we concentrate on models of high performance in the IWSLT experiments. The results can
be found in Tab. 4 and 5. In both cases, we see
consistent improvements over the recurrent neural
network language model baseline, improving the
Arabic→English system by 0.6 BLEU and 0.5 TER
on test1. This can be compared to the rescoring
results for the same task reported by , where they achieved 0.3 BLEU, despite the
fact that they used multiple references for scoring,
whereas in our experiments we rely on a single
reference only. The models are also able to improve the Chinese→English system by 0.5 BLEU
and 0.5 TER on test2.
To investigate whether bidirectional models beneﬁt from future source information, we compare
the single-best output of a system reranked with a
unidirectional model to the output reranked with
a bidirectional model.
We choose the models
to be translation models in both cases, as they
predict target words independent of previous
predictions, given the source information (cf. Eqs.
(3, 4)). This makes it easier to detect the effect
of including future source information or the lack
thereof. The examples are taken from the IWSLT
BTM (deep)
BJM (deep)
+ BTM (deep)
+ BJM (deep)
Table 4: Results for the BOLT Arabic→English
task with different RNN models. The “+” sign in
the last two rows indicates that either of the corresponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, J: joint, B: bidirectional.
task, where we include the one-to-one source
information, reordered according to the target
source: nicht so wie ich
reference: not like me
Hypothesis 1:
1-to-1 source: so ich ϵ nicht wie
1-to-1 target: so I do n’t like
Hypothesis 2:
1-to-1 source: nicht so wie ich
1-to-1 target: not ϵ like me
In this example, the German phrase “so wie”
translates to “like” in English. The bidirectional
model prefers hypothesis 2, making use of the
future word “wie” when translating the German
word “so” to ϵ, because it has future insight that
this move will pay off later when translating
BTM (deep)
BJM (deep)
+ BTM (deep)
+ BJM (deep)
Table 5: Results for the BOLT Chinese→English
task with different RNN models. The “+” sign in
the last two rows indicates that either of the corresponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, B: bidirectional.
the rest of the sentence.
This information is
not available to the unidirectional model, which
prefers hypothesis 1 instead.
source: das taten wir dann auch und verschafften uns
so eine Zeit lang einen Wettbewerbs Vorteil .
reference: and we actually did that and it gave us a
competitive advantage for a while .
Hypothesis 1:
1-to-1 source:
das ϵ ϵ ϵ wir dann auch taten und
verschafften uns so eine Zeit lang einen Wettbewerbs
1-to-1 target: that ’s just what we ϵ ϵ did and gave us ϵ
a time , a competitive advantage .
Hypothesis 2:
1-to-1 source:
das ϵ ϵ ϵ wir dann auch taten und
verschafften uns so einen Wettbewerbs Vorteil ϵ eine
Zeit lang .
1-to-1 target: that ’s just what we ϵ ϵ did and gave us ϵ
a competitive advantage for a ϵ while .
Here, the German phrase “eine Zeit lang” translates to “for a while” in English.
Bidirectional
scoring favors hypothesis 2, while unidirectional
scoring favors hypothesis 1. It seems that the unidirectional model translates “Zeit” to “time” as the
object of the verb “give” in hypothesis 1, being
blind to the remaining part “lang” of the phrase
which changes the meaning.
The bidirectional
model, to its advantage, has the full source information, allowing it to make the correct prediction.
Conclusion
We developed word- and phrase-based RNN translation models. The former is simple and performs
well in practice, while the latter is more consistent
with the phrase-based paradigm. The approach inherently evades data sparsity problems as it works
on words in its lowest level of processing. Our
experiments show the models are able to achieve
notable improvements over baselines containing a
recurrent LM.
In addition, and for the ﬁrst time in statistical
machine translation, we proposed a bidirectional
neural architecture that allows modeling past and
future dependencies of any length.
Besides its
good performance in practice, the bidirectional architecture is of theoretical interest as it allows the
exact modeling of posterior probabilities.
Acknowledgments
This material is partially based upon work supported by the DARPA BOLT project under Contract No.
HR0011- 12-C-0015.
Any opinions,
ﬁndings and conclusions or recommendations expressed in this material are those of the authors and
do not necessarily reﬂect the views of DARPA.
The research leading to these results has also received funding from the European Union Seventh Framework Programme under grant agreements no 287658 and no 287755.
Experiments were performed with computing resources granted by JARA-HPC from RWTH
Aachen University under project ‘jara0085’. We
would like to thank Jan-Thorsten Peter for providing the BBN-JM system.