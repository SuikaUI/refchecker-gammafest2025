Lazy Gaussian Process Committee
for Real-Time Online Regression
Han Xiao and Claudia Eckert
Institute of Informatics, Technische Universit¨at M¨unchen
Garching, D-85748, Germany
{xiaoh, claudia.eckert}@in.tum.de
A signiﬁcant problem of Gaussian process (GP) is its unfavorable scaling with a large amount of data. To overcome this
issue, we present a novel GP approximation scheme for online regression. Our model is based on a combination of multiple GPs with random hyperparameters. The model is trained
by incrementally allocating new examples to a selected subset of GPs. The selection is carried out efﬁciently by optimizing a submodular function. Experiments on real-world data
sets showed that our method outperforms existing online GP
regression methods in both accuracy and efﬁciency. The applicability of the proposed method is demonstrated by the
mouse-trajectory prediction in an Internet banking scenario.
Introduction
Gaussian process (GP) is a promising Bayesian method
for non-linear regression and classiﬁcation . It has been demonstrated to be applicable to
a wide variety of real-world statistical learning problems. An
important advantage of GP over other non-Bayesian models
is the explicit probabilistic formulation of the model, allowing one to assess the uncertainty of predictions. In addition,
since GP has a simple parameterization and the hyperparameters can be adjusted by maximizing the marginal likelihood,
it is easy to implement and ﬂexible to use.
Nevertheless, GP is not always the method of choice especially for large date sets. GP has inherently dense representations in the sense that all training data is required for
the prediction. The training procedure requires computation,
storage and inversion of the full covariance matrix, which
can be time-consuming. Furthermore, the Bayesian posterior
update to incorporate data is also computationally cumbersome. These drawbacks prevent GP from applications with
large amounts of training data that require fast computation,
such as learning motor dynamics in real-time.
Much research in recent years has focused on reducing
the computational requirements of GP on large data sets.
Many of these methods are based on a small set of training inputs, which summarizes the bulk of information provided by all training data . All rights reserved.
2006; Qui˜nonero-Candela and Rasmussen 2005). Other
methods make structural assumptions about the covariance
matrix so that a GP can be decomposed into a number of
smaller GPs .
In this paper we present a novel approximation method
called lazy Gaussian process committee (LGPC) for learning from a continuous data stream. As its name suggests,
LGPC is based on a combination of multiple GPs, which
is closely related to several previous work . Unlike previous work, our model is updated in a “lazy” fashion
in the sense that new training examples are directly allocated
to a subset of GPs without adapting their hyperparameters.
The problem of selecting a near-optimal subset is formulated
as submodular optimization, allowing the training procedure
to be carried out efﬁciently. Experiments showed that LGPC
has comparable accuracy to the standard GP regression and
outperforms several GP online alternatives. The simplicity
and the efﬁciency of LGPC make it more appealing in realtime online applications.
We applied LGPC to mouse-trajectory prediction in an Internet banking scenario. The intention was to predict user’s
hand movements in real-time, so as to offer support to security applications, such as recognizing identity theft or an
abnormal funds transfer. Thus, the model’s learning and
prediction should be sufﬁciently fast. In the ﬁeld of psychology and cognitive science, there has been abundant evidence that a motor dynamic of the hand can reveal the
time course of mental process . Moreover, several studies have
showed that computer mouse-trajectory can afford valuable
information about the temporal dynamics of a variety of psychological process . For
instance, unintentional stress might manifest less smooth,
more complex and ﬂuctuating trajectories . An online learning technique is necessary as
it allows the adaption to changes in the trajectories. The effectiveness of the online mouse-trajectory learning conﬁrms
the applicability of our method.
Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence
Related Work
This section brieﬂy reviews Gaussian process and previous
attempts on reducing its computational complexity. The underlying problems that motivate this work are highlighted.
GP Regression
The problem of regression aims to ﬁnd a function estimation from the given data, which is usually formulated as follows: given a training set D := {(xn, yn)}N
n=1 of N pairs
of input vectors xn and noisy scalar outputs yn, the goal is
to learn a function f transforming an input into the output
given by yn = f(xn) + ϵn, where ϵn ∼N(0, σ2) and σ2
is the variance of the noise. A Gaussian process is a collection of random variables, any ﬁnite number of which have
consistent joint Gaussian distribution. Gaussian process regression (GPR) is a Bayesian approach which assumes a GP
prior over functions. As a result the observed outputs behave
according to
p(y | x1, . . . , xN) = N(0, K),
where y := [y1, . . . , yN]⊤is a vector of output values, and
K is an N × N covariance matrix; the entries are given by a
covariance function, i.e. Kij := k(xi, xj). In this work, we
consider a frequently used covariance function given by
k(xi, xj) := κ2 exp
2(xi −xj)⊤W(xi −xj)
where κ denotes the signal variance and W are the widths
of the Gaussian kernel. The last term represents an additive
Gaussian noise, i.e. δij := 1 if i = j, otherwise δij := 0.
In the setting of probabilistic regression, the goal is to ﬁnd
a predictive distribution of the output y∗at a test point x∗.
Under GPR, the predictive distribution of y∗conditional on
the training set D is also Gaussian
p(y∗| D, x∗) = N
∗K−1y, k∗−k⊤
[k(x∗, x1), . . . , k(x∗, xN)]⊤and k∗
k(x∗, x∗). One can observe that the training data is explicitly required at the test time in order to construct the predictive distribution, which makes GP a non-parametric method.
The hyperparameters are [κ2, σ2, {W}]⊤, where {W} denotes parameters in the width matrix W. The optimal hyperparameters for a particular data set can be derived by
maximizing the marginal likelihood function using a gradient based optimizer. For a more detailed background on
GP, readers are referred to the textbook .
It should be noted that in each iteration the computation
of the likelihood and the derivatives involves inversion of a
matrix of size N × N, which requires time O(N 3). Once
the inversion is done, inference on a new test point requires
O(N) for the predictive mean and O(N 2) for the predictive
variance. Thus, a simple implementation of GPR can handle
problems with at most a few thousands training examples,
which prevents it from real-time applications dealing with
large amounts of data.
GP Approximations
The sparse representation of data has been studied exhaustively . It has been shown that
the bulk of information provided by all training inputs can
be summarized by a small set of inputs, which is often
known as inducing inputs or support vectors. By assuming
additional dependency about the training data given the inducing inputs, various sparse GP approximations were derived, such as the subset of regressors , projected latent variables and sparse GP with pseudo-inputs . A unifying view of these sparse
GP methods was presented in .
It should be noted that the selection of inducing inputs
does leave an imprint on the ﬁnal solution. Loosely speaking, the selection can be carried out either in a passive (e.g.
random) or active fashion. An extensive range of proposals
were suggested to ﬁnd a near-optimal choice for inducing
inputs, such as posterior maximization , maximum information gain , matching pursuit 
and pseudo-input via continuous optimization . In particular, sparse online GP was developed by combining the idea of
a sparse representation with an online algorithm, allowing
the inducing inputs (basis vectors) to be constructed in a sequential fashion.
An alternative approach for speeding up GPR is Bayesian
committee machine (BCM) introduced by .
Loosely speaking, the original training data is partitioned
into parts, where each part is maintained by a GP. The computational cost is thus signiﬁcantly reduced due to much
smaller number of training examples within each GP. BCM
provides a principled approach to combining Bayesian estimators trained on different data sets for the prediction. Inspired by this idea, some other work have been focused on
combining multiple GPs for regression .
In the online setting, a straightforward application of the
approaches mentioned above is impeded by two obstacles.
First, it is inefﬁcient to optimize hyperparameters every time
a new training example is presented. Second, adding new
training examples to the model may cause non-smooth ﬂuctuations in the marginal likelihood function and its gradients, meaning that a smooth convergence is not guaranteed . Although several heuristics can be adapted to alleviate this problem, there
is no reliable way of learning hyperparameters. Moreover,
as training examples are presented sequentially rather than
in batch, selecting inducing inputs from a data stream in a
far-sighted fashion becomes extremely challenging. Furthermore, the inducing inputs selection and the hyperparameters
estimation are somewhat undermined by each other , which may adversely affect the
quality and the efﬁciency of online regression.
LGPC for Online Regression
The basic idea of our approach is straightforward. Instead of
training a single GP using all the training data, we partition
the data and allocate it to a committee consisted of Q independent GPs with different hyperparameters. That is, each
GP maintains a subset of the training data, which is denoted
as D1, . . . , DQ respectively, where Dq := {(xt, yt)}T
T is the maximum number of training examples of each GP.
Intuitively, each GP in the committee corresponds to an interpretation of the relationship between input and output.
For predicting the output y∗of a query point x∗, the outputs from all GPs are combined together. Assuming that any
partition Di is conditionally independent with all other partitions {Dj, . . .}, ∀j ̸= i given the outputs at test points,
which is reasonable when the number of test points is large,
we have p(y1, . . . , yQ | D) ≈QQ
q=1 p(yq | Dq), the predictive distribution can be approximated as
bp(y∗| D, x∗) ∝c
q=1 p(y∗| Dq, x∗)
[p(y∗)]Q−1
where c is a normalization constant. The denominator of
Eq. (2) serves as a prior distribution of y∗. For a single
point, it is considered as zero-mean Gaussian with unit variance. For a set of query points, the prior is considered as
N(0, Σ∗), where Σ∗is the sample covariance matrix.
This section describes LGPC in three parts, namely the
allocation of new training examples, the incremental update
and the predictions of query points. Note that all hyperparameters of LGPC are constants during online learning.
Allocation of New Training Examples
Denote all GPs in the committee as Q := {1, . . . , Q}. Given
a new training example (xN+1, yN+1), we select a subset
A ⊆Q and allocate the new example to their data collection,
respectively. Denote D⟨N⟩
as the training examples allocated
to the qth GP at time N, the update rule is formalized as
D⟨N+1 | A⟩
∪{(xN+1, yN+1)}
otherwise.
On the one hand, if A contains only one element, meaning
that only one GP is updated each time, then the information
provided by the new training example may not be well utilized. On the other hand, if we let A := Q, then all GPs
must update their corresponding Gram matrix to include the
new training example (no matter whether such inclusion will
contribute to the prediction of the committee or not), which
can degrade the efﬁciency and the quality of the prediction.
Thus, we need an active selection policy to choose at most S
GPs from the committee, such that their data inclusion can
yield the maximal improvement for prediction.
Clearly it can make sense to select which GPs are taken
into A by optimizing some criterion. The idea here is to
maximize the likelihood of LGPC on a small subset of training examples. To see this we introduce a reference set R,
in which both inputs XR and outputs yR are observed. The
reference set can be constructed, for instance, by subsampling all previous training data {D⟨N⟩
q=1 with the addition
of the current training example (xN+1, yN+1). Note that the
terms in the numerator and the denomination of Eq. (2) are
all Gaussian distributions over y∗. Thus, the predictive distribution for yR at time N can be approximated by a Gaussian
distribution with mean and covariance as follows
(yR) = C⟨N⟩
C(yR | D⟨N⟩
E(yR | D⟨N⟩
−(Q −1)Σ−1
C(yR | D⟨N⟩
, XR)−1−1
where ΣRR is the covariance matrix evaluated at XR.
The predictive mean and covariance of each GP, i.e.
E(yR | D⟨N⟩
, XR) and C(yR | D⟨N⟩
, XR), can be obtained
from Eq. (1). Note that as yR is known, the log probability
of yR under the current model can be evaluated by substituting Eqs. (3) and (4) into the following
2 log(2π) + 1
where |R| represents the number of references points in R.
When R is arbitrarily selected and sufﬁciently large, one can
consider L⟨N⟩
as a proxy for the likelihood of training examples for LGPC. Hence, we hereinafter call LR the log
pseudo-likelihood.
As a consequence, the problem of the optimal selection
A∗at time N + 1 can be formulated as
A∗:= arg max
A⊆Q L⟨N+1 | A⟩
R , subject to |A| ≤S,
which is unfortunately a combinatorial problem and cannot be solved efﬁciently. However, it is worth to highlight
that the increment of pseudo-likelihood L⟨N+1 | A⟩
satisﬁes the diminishing returns behavior. That is, adding a
new GP to the selection A increases the pseudo-likelihood
more, if we have selected few GPs; and less, if we have
already selected many GPs. This formalism can be formalized using the combinatorial concept of submodularity . Speciﬁcally, let
F(A) := L⟨N+1 | A⟩
R , the submodular characteristic
of F indicates that for all A ⊆B ⊆Q and q ∈Q \ B it
holds that F(A ∪{q}) −F(A) ≥F(B ∪{q}) −F(B).
Interest of optimizing a submodular function has grown
in the machine learning community recently . In practice, heuristics such as greedy selection are often used. The greedy algorithm starts with
the empty set, and iteratively adds the element q∗
arg maxq∈Q\A F(A ∪{q}), until S elements have been
selected. A fundamental result by stated that for submodular functions, the
greedy algorithm achieves at least a constant fraction (1 −
1/e) of the objective value obtained by the optimal solution.
Moreover, no polynomial time algorithm can provide a better approximation guarantee unless P = NP .
Algorithm 1: Greedy subset selection for LGPC.
Input : desired size of selection S (≥2)
Output: greedy selection A
1 Initialization A ←Ø, J ←{1, . . . , Q};
2 ∀j ∈J : ∆j ←F(A ∪{j}) −F(A);
3 j∗←arg maxj∈J ∆j;
4 A ←A ∪{j∗}, J ←J \ {j∗};
5 for s ←2 to S do
j∗←arg maxj∈J ∆j;
∆j∗←F(A ∪{j∗}) −F(A);
if ∀j ∈J \ {j∗} : ∆j∗> ∆j then
A ←A ∪{j∗}, J ←J \ {j∗};
until |A| = s;
Note that evaluating F(A ∪{q}) can be expensive as it is
required to re-compute Eqs. (3) and (4) for all q ∈Q in each
iteration. In fact, such computation is unnecessary due to
the submodularity of F . Deﬁne the marginal
beneﬁt of q as ∆q := F(A∪{q})−F(A), the submodularity
indicates that ∆q never increases over iterations. Based on
this observation, our greedy selection scheme is given in Algorithm 1. The algorithm starts with the set A being empty,
and J containing the indices of all GPs. In the ﬁrst iteration,
the GP with maximal ∆is selected from J and added to A.
This is achieved by evaluating the marginal beneﬁt for all
GPs in the committee. An ordered list of {∆j}j∈J is maintained. From the second iteration, only the top GP in this
ordered list will be evaluated. If the new marginal beneﬁt of
that GP stays on top, then we add it to A. Otherwise, the list
of marginal beneﬁts is re-sorted and, subsequently, the new
top GP is evaluated.
Theoretically, the worst-case complexity for Algorithm 1
is S2O(L), where O(L) denotes the complexity of one
pseudo-likelihood calculation. Empirically, we found on average only 3 calculations of pseudo-likelihood was required
at each iteration, which gave about 3SO(L).
Incremental Update of LGPC
Once a set of GPs is selected by Algorithm 1, the problem
turns to updating these GPs for the data inclusion in an incremental fashion. Although the update of inputs X and outputs
y can be done straightforwardly, the update of a covariance
matrix K and its inverse J := K−1 is more complicated.
Speciﬁcally, the effect of a new point xN+1 on K and J can
be expressed as
, J⟨N+1⟩:=
with u := [k(xN+1, x1), . . . , k(xN+1, xN)]⊤and v :=
k(xN+1, xN+1). Following the partitioned inversion matrix
equations we have
g := −µJ⟨N⟩u,
v −u⊤J⟨N⟩u
In practice, Cholesky factorization is often used so that
L⊤L := K, which leads to a more efﬁcient and accurate
solution for frequently occuring terms such as x⊤K−1x =
∥L−1x∥2 and log |K| = 2
 1⊤log(diag (L))
. The lower
triangular matrix L can be also updated incrementally such
where l can be solved by L⟨N⟩l = u, and subuently, η :=
Further notice that the model may deal with an endless
stream of data during online learning. Thus, we have to limit
the number of training examples maintained by each GP and
delete old training examples when necessary. Let m be the
index of training example being deleted. Construct P :=
I −(δm −δT +1)(δm −δT +1)⊤, as a (T + 1)-dimensional
permutation matrix, where δm is a (T +1)-dimensional zero
vector with one on the mth dimension. When a new point
comes in, it is ﬁrst appended to K⟨N+1⟩. Then, the deletion of the mth example can be performed efﬁciently using
⇑, where [·]⇑represents shrinking a (T + 1)dimensional matrix or vector to a T-dimensional one by
removing the last row and column of it. Thus, the nonincreasing update equation of J is given by
where s := [[k(xm, x1), . . . , k(xm, xN+1)]P]⇑and r :=
k(xm, xm).
So far we have not said which training example should
be deleted. One simple method is to choose it randomly or
to remove the oldest training example over time. Alternatively, one can remove the point that yields maximal mutual information with the new point. In this work, we follow
the score measure introduced by .
Speciﬁcally, for each point i in set Dq the score is given by
, where αt is the tth element of J⟨N+1⟩y. If
a deletion is needed for the qth GP, then the training example with minimal ξ will be removed from Dq. The scores
are computationally cheap as they can be calculated in linear time. Although there may exist more sophisticated selection schemes, they usually consume more computational
time and hence are not considered in this work.
Predictions of Query Points
Given a query point x∗, the predictive mean and variance
can be calculated by replacing R in Eqs. (3) and (4) with
the test set. Namely, we ﬁrst compute the predictive mean
and variance of each GP member in the traditional way, and
then substitute them into Eqs. (3) and (4) to get the committee’s prediction. One can observe that the way of combining predictions in Eq. (3) automatically assigns less weight
(through the inverse predictive variance) to those GPs that
are uncertain about their predictions. The time complexity is
O(QT) for predicting the mean and variance of a test point.
For the sake of efﬁciency and accuracy, it is reasonable to
only invoke nearby GPs in a neighborhood of x∗for the prediction. The key observation is that k(x∗, x) depends merely
on the constant σ if x∗is far away from x. As σ is randomly
initialized in LGPC, a poor prediction could be given by the
qth GP when x∗is far away from all points in the set Dq. The
search of neighboring GPs can be performed by evaluating
x∈Dq k(x∗, x) for all {Dq}Q
q=1 and, subsequently, selecting those having largest values.
Experimental Results
Two sets of experiments were carried out to validate our algorithm. First, we compared the accuracy and the efﬁciency
of LGPC with the standard GPR and state-of-the-art online
regression methods. Second, we investigated several factors
that affect the performance of LGPC in order to gain more
insights of it. An application to the mouse-trajectory prediction is demonstrated at the end.
The experiments were conducted on six large data sets
downloaded from the Internet. On each data set, we linearly
rescaled into the range of [−1, 1] and randomly held out half
of the data for training; the remaining was used as a test set.
Each experiment was repeated 10 times.
Five baseline methods were employed for comparison.
They were standard GP regression (GPR), sparse GP using
pseudo-inputs (SGPP) , local GP regression (LoGP) ,
Bayesian committee machine (BCM) and
sparse online GP regression (SOGP) . URLs of these MATLAB implementations are listed
in the references. Note that GPR and SGPP are ofﬂine algorithms, they are presented here to show the performance
when the whole training data is given beforehand. A Gaussian kernel function with white noise was used as the covariance function for all methods, which was obtained by
setting W in the covariance function as the identity matrix. The maximum number of inducing inputs in SGPP and
SOGP was 50. The threshold for creating a new local model
in LoGP was 0.5. The size of the committee was 20 for
BCM and LGPC, in which each GP maintained at most 100
training examples. For BCM and LGPC, the ﬁrst 20 training examples were sequentially assigned to each member
for initialization. The reference set of LGPC had a size of
3, which consisted of the current training example and two
examples randomly sampled from the aforetime data. The
number of selected GPs in LGPC for data inclusion was 5
(i.e. S := 5 in Algorithm 1). Moreover, three variations of
BCM were employed in the experiment. BCMo denotes that
each time only one GP is randomly selected for data inclusion; BCMs represents randomly selecting a subset with a
size of 5, which corresponds to a randomized counterpart of
LGPC; and BCMa represents selecting all GPs. For predicting test inputs, all GPs in the committee of BCM and LGPC
were invoked. The hyperparameters were randomly initialized for all methods. In particular, we drew a set of hyperparameters from Uniform(0, 10) for initializing BCM and
LGPC. The conjugate gradient method ) was employed to optimize the hyperparameters for GPR, SGPP, SOGP and LoGP. For SOGP, an EM
algorithm with 10 cycles was built for jointly optimizing the
posterior process and the hyperparameters.
Comparison of Predictive Accuracy
The comparison of predictive accuracy between different GP
methods is summarized in Table 1(a), where the root mean
square error was used as the evaluation metric. This causes
the trivial method of guessing the mean to have a SMSE
of approximately 1 and a MSLL of zero. Small value of
SMSE or negative value of MSLL indicates a better method.
It is evident from the results that, LGPC gave a considerably lower test error than LoGP, BCM and SGPP. In particular, LGPC showed a signiﬁcant improvement over all BCM
variants on the majority data sets, which indicates the effectiveness of our subset selection strategy. Empirically, we
found that good performance could have been achieved after
learning from ﬁrst few thousands examples. After that, the
accuracy of LGPC did not change signiﬁcantly. On delta,
cpuact and houses, the performance of LGPC was comparable to SOGP. In fact, the performance of LGPC can
be further improved by allowing each GP to maintain more
training examples, as it is shown in the third experiment.
Comparison of Computation Speed
The comparison of training and prediction speed is shown
in Fig. 1. Different online methods were trained (tested) on
houses data set with increasing training (testing) examples, i.e. 500, 1, 000, 2, 000, 4, 000 and 8, 000 data points,
respectively. As can be seen in Fig. 1, in both training and
prediction phrases, LGPC showed a substantial reduction of
time comparing to GPR and LoGP. In particular, LGPC took
less time for learning 8, 000 points (210s) than SGPP (240s)
and SOGP (340s). On the other hand, SGPP and SOGP were
found to be extremely efﬁcient in the prediction, as their
time cost only increased at a very low pace with respect to
the number of test points. This is due to the fact that the prediction of SGPP and SOGP involves only a small covariance
matrix based on 50 inducing inputs, whereas LGPC invokes
all GPs in the committee for prediction. Nonetheless, it is
possible to speed up LGPC by invoking only the nearest GP
for the prediction as aforementioned. In short, LGPC is a
more appropriate choice than SOGP in a real-time application which requires fast training.
One may notice that LGPC took more training time than
BCMs and its speed advantages over BCMa was not dramatic, which is slightly disappointing. This is attributed
to the fact that, although LGPC saves the computational
resources by limiting the number of GPs for data inclusion, it spends extra computational time on selecting a nearoptimal subset with Algorithm 1. The prediction complexity of LGPC, BCMo, BCMs and BCMa was virtually same.
Nonetheless, it should be noted that LGPC outperformed all
BCM variants signiﬁcantly on houses in terms of predictive accuracy as detailed in Table 1(a), which makes LGPC
overall more preferable than BCM.
Table 1: The root mean square error ± the standard deviation on different test sets. URLs of these data sets can be found in
the references. delta: 7, 129 × 6, bank: 8, 192 × 8, cpuact: 8, 192 × 12, elevator: 8, 752 × 17, houses: 20, 640 × 8,
sarcos: 44, 484 × 21. Results were averaged over ten runs. Small value for better methods.
(a) LGPC versus baseline methods.
0.041±0.001
0.084±0.011
0.072±0.010
0.053±0.005
0.157±0.004
0.032±0.002
0.065±0.009
0.188±0.069
0.219±0.041
0.107±0.009
0.232±0.028
0.089±0.001
0.044±0.002
0.113±0.012
0.115±0.011
0.066±0.004
0.164±0.008
0.070±0.003
0.043±0.001
0.108±0.012
0.114±0.016
0.069±0.005
0.180±0.008
0.073±0.003
0.045±0.001
0.122±0.017
0.119±0.009
0.083±0.003
0.203±0.007
0.077±0.004
0.040±0.001
0.047±0.001
0.074±0.007
0.038±0.001
0.143±0.001
0.023±0.001
0.039±0.001
0.041±0.001
0.030±0.001
0.031±0.001
0.115±0.002
0.016±0.002
0.045±0.013
0.061±0.045
0.079±0.057
0.065±0.035
0.161±0.046
0.095±0.059
(b) Predictive error of LGPC w.r.t. different size of the committee.
0.043±0.001
0.093±0.029
0.087±0.030
0.065±0.016
0.171±0.005
0.034±0.006
0.042±0.001
0.102±0.016
0.079±0.018
0.056±0.010
0.167±0.007
0.033±0.004
0.041±0.001
0.085±0.012
0.075±0.013
0.057±0.008
0.161±0.003
0.034±0.004
0.041±0.001
0.084±0.011
0.072±0.010
0.053±0.007
0.157±0.004
0.033±0.003
0.041±0.001
0.076±0.006
0.080±0.009
0.053±0.003
0.156±0.003
0.032±0.003
0.041±0.001
0.085±0.012
0.095±0.009
0.052±0.004
0.155±0.004
0.032±0.003
(c) Predictive error of LGPC w.r.t. different maximum number of examples.
0.042±0.001
0.114±0.012
0.098±0.016
0.072±0.014
0.172±0.007
0.040±0.004
0.041±0.001
0.084±0.011
0.072±0.010
0.053±0.007
0.157±0.004
0.032±0.002
0.040±0.001
0.068±0.011
0.057±0.005
0.040±0.003
0.152±0.003
0.028±0.003
0.040±0.001
0.054±0.004
0.050±0.007
0.039±0.002
0.146±0.002
0.022±0.002
# Training points (× 103)
Training time [s]
# Test points (× 103)
Prediction time [s]
Figure 1: Time cost in second (averaged over 10 runs) required for training and predicting, respectively. In each run,
a training set and a test set were randomly sampled from
houses data set and the time cost was measured respectively. The training and prediction time of 8, 000 data points
required for GPR was 1100s and 15s, respectively. Note that
prediction time of LGPC can be reduced to 0.02s if only the
nearest GP is invoked for predicting a test point.
Exploration of Model Parameters
In order for LGPC to be a practical tool in real-world applications, it is necessary to make decisions about the details of its speciﬁcation. Fortunately, there are not many free
parameters in LGPC, as all hyperparameters are randomly
initialized and are ﬁxed during the learning process1. Our
exploration focused on three parameters that mainly govern
the performance of LGPC. Namely, the committee size Q,
the maximum number of maintained training points T and
the size S of the selected subset for data inclusion.
The performance of LGPC with respect to different sizes
of the committee is summarized in Table 1(b), where S := 5
and T := 100. On delta, bank and cpuact, the predictive accuracy reached its peak when the size of the committee was around 20. After that, the performance dropped
slightly. On larger data sets such as houses and sarcos
the test error had a steady decline as Q increasing. This indicates that it sufﬁces to employ a small committee for learning a small amount of data. For a large data set increasing the
size of the committee provides more capability to account
for the complex pattern, which generally leads to higher predictive accuracy.
Table 1(c) summarizes the test results of LGPC with respect to different settings of T, where S := 5 and Q := 20.
As expected, one can improve the predictive accuracy sig-
1We did try few heuristics for initializing the hyperparameters, such as initializing 20 Gaussian kernels with different widths
(e.g. 2−9, . . . , 29, 210) and the same noise level; sampling from a
Gamma distribution; implementing a weak prior by optimizing on
a small validation set. However, such attempts did not yield better
predictive accuracy than the random initialization.
niﬁcantly by allowing each GP to maintain more training
examples. Moreover, when T := 200 it was observed that
the high accuracy was achieved much earlier than T := 50;
and the performance was often more robust afterwards.
Finally, to study the performance with respect to different
sizes of the selected subset, we ﬁxed T := 100, Q := 20
and trained LGPC with S := 2, 4, 6, 8 and 10, respectively.
It was found that on delta and sarcos the predictive accuracy was not sensitive to the size of selected subset. On
cpuact, elevator and houses, selecting more than
two GPs slightly degraded the performance. The optimal
size for bank was 6. In short, it seems that the optimal value
of S varies with data sets.
Mouse-Trajectory Prediction
We applied LGPC for learning mouse-trajectory of different
users in an Internet banking scenario. To collect the data,
we developed a website for simulating an environment, in
which participants were asked to transfer funds to a dummy
account. A complete procedure was composed of ﬁve interfaces, i.e. login, account overview, transaction details, TAN
authentication and conﬁrmation. Ten participants were involved and each with three trials; the input information was
same for all trials. A Javascript code was developed for
tracking mouse coordinates on every onmousemove event.
The trajectories of the ﬁrst two trials (ca. 2700 points/user)
were used for training models. The goal was to predict the
trajectory of the last trial (ca. 1000 points/user).
The predicted trajectories of three users using LGPC,
SOGP and ofﬂine GPR are visualized in Fig. 2. It was observed that users behaved differently even when they were
performing the same task. For instance, the ﬁrst user used
the tab key moves the cursor and entered the TAN code with
the numpad, resulting a short and simple mouse-trajectory.
On the other hand, the third user entered the TAN code using
a virtual keyboard on the web page, which made the trajectory sway horizontally. By learning from the ﬁrst two trials,
reasonable predictions of the third trial were obtained from
all methods. However, when the interface contained many
elements and the trajectory became more complicated, the
ofﬂine GPR gave noisy predictions as depicted in Fig. 2(a).
It can be seen that LGPC performed as good as the state-ofthe-art SOGP in learning users’ trajectories.
With a new training point arriving about every 10ms (less
than one minute of running time will result in thousands of
data points), LGPC is a more preferable method due to its
fast learning speed. Efﬁcient trajectory prediction would be
beneﬁcial in security applications, such as distinguishing between individuals and early warning of identity theft.
Conclusions
The efﬁciency problem of Gaussian process becomes a primary issue when it is applied to real-time online applications. This work has proposed a novel method for reducing the computational demand of GP regression. It consists
of multiple GPs where each maintains only a small set of
training examples. Each time a subset of GPs is selected
for including newly arrived training examples. The selection
(a) Mouse-trajectories on “transaction details”
(b) Mouse-trajectories on “TAN authentication”
Figure 2: The predictions of three users’ mouse-trajectories
on two interfaces. Each column represents a user. The gray
curve with □denotes a user’s trajectory in the third trial.
The model’s prediction is illustrated by the color curve with
⋆, whose head is blue and tail is red.
is performed by optimizing a submodular function. Unlike
previous work, our model removes the need for parameter-
ﬁtting and requires little tuning efforts. An improvement of
accuracy and efﬁciency over existing online GP methods
has been demonstrated in the experiment. In particular, as a
modiﬁed version of Bayesian committee machine, we have
showed that updating a chosen subset of GPs is more effective than updating the whole committee, which leads to
better predictive accuracy. As demonstrated in the task of
mouse-trajectory prediction, LGPC can be applied to a wide
range of real-time applications, such as learning motor dynamics and inferring temporal dynamics of mental phenomena.
An important question for future studies is to determine
the optimal size of the committee. One possible way is to
expand or shrink the committee size during the online learning. In addition, more effective strategies for initializing the
hyperparameters remain to be determined. Furthermore, it
should be interesting to infringe the independence between
GP members for a further improvement on the accuracy.
Acknowledgments
16BY1200D) of the German Federal Ministry of Education
and Research. We would like to thank Nan Li and Ping Luo
for their comments on an earlier draft, Yurong Tao for help
with experiments, and anonymous reviewers for helpful suggestions.