‚Äú¬© 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be
obtained for all other uses, in any current or future media, including reprinting/republishing this
material for advertising or promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of this work in other
Filter Pruning via Geometric Median
for Deep Convolutional Neural Networks Acceleration
Ping Liu1,2
Ziwei Wang3
Zhilan Hu4
Yi Yang1,5‚àó
1CAI, University of Technology Sydney
3Information Science Academy, CETC
5Baidu Research
{yang.he-1}@student.uts.edu.au
{pino.pingliu,wangziwei26,yee.i.yang}@gmail.com
Previous works utilized ‚Äúsmaller-norm-less-important‚Äù
criterion to prune Ô¨Ålters with smaller norm values in a convolutional neural network. In this paper, we analyze this
norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1)
the norm deviation of the Ô¨Ålters should be large; (2) the
minimum norm of the Ô¨Ålters should be small.
this problem, we propose a novel Ô¨Ålter pruning method,
namely Filter Pruning via Geometric Median (FPGM), to
compress the model regardless of those two requirements.
Unlike previous methods, FPGM compresses CNN models
by pruning Ô¨Ålters with redundancy, rather than those with
‚Äúrelatively less‚Äù importance. When applied to two image
classiÔ¨Åcation benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces
more than 52% FLOPs on ResNet-110 with even 2.69%
relative accuracy improvement.
Moreover, on ILSVRC-
2012, FPGM reduces more than 42% FLOPs on ResNet-
101 without top-5 accuracy drop, which has advanced the
state-of-the-art.
Code is publicly available on GitHub:
 
1. Introduction
The deeper and wider architectures of deep CNNs bring
about the superior performance of computer vision tasks [6,
However, they also cause the prohibitively expensive computational cost and make the model deployment on mobile devices hard if not impossible. Even the
latest architecture with high efÔ¨Åciencies, such as residual
connection or inception module , has millions
of parameters requiring billions of Ô¨Çoat point operations
(FLOPs) . Therefore, it is necessary to attain the deep
CNN models which have relatively low computational cost
‚àóCorrsponding Author. Part of this work was done when Yi Yang was
visiting Baidu Research during his Professional Experience Program.
(a) Criterion for filter pruning
Filters before pruning
of filters
Value of norm
Filters to be
(b) Requirements for norm-based criterion
Ideal distribution: ùïç
Requirement 1: œÉ (ùïç) >> 0
Requirement 2: ùíó1 ‚Üí0
Large norm
Medium norm
Small norm
Filter Space
An illustration of (a) the pruning criterion for normbased approach and the proposed method; (b) requirements for
norm-based Ô¨Ålter pruning criterion. In (a), the green boxes denote
the Ô¨Ålters of the network, where deeper color denotes larger norm
of the Ô¨Ålter. For the norm-based criterion, only the Ô¨Ålters with
the largest norm are kept based on the assumption that smallernorm Ô¨Ålters are less important. In contrast, the proposed method
prunes the Ô¨Ålters with redundant information in the network. In
this way, Ô¨Ålters with different norms indicated by different intensities of green may be retained. In (b), the blue curve represents
the ideal norm distribution of the network, and the v1 and v2 is the
minimal and maximum value of norm distribution, respectively.
To choose the appropriate threshold T (the red shadow), two requirements should be achieved, that is, the norm deviation should
be large, and the minimum of the norm should be arbitrarily small.
but high accuracy.
Recent developments on pruning can be divided into
two categories, i.e., weight pruning and Ô¨Ålter pruning . Weight pruning directly deletes weight values
in a Ô¨Ålter which may cause unstructured sparsities. This
irregular structure makes it difÔ¨Åcult to leverage the highefÔ¨Åciency Basic Linear Algebra Subprograms (BLAS) libraries . In contrast, Ô¨Ålter pruning directly discards the
whole selected Ô¨Ålters and leaves a model with regular structures. Therefore, Ô¨Ålter pruning is more preferred for accelerating the networks and decreasing the model size.
Current practice performs Ô¨Ålter pruning
by following the ‚Äúsmaller-norm-less-important‚Äù criterion,
which believes that Ô¨Ålters with smaller norms can be pruned
safely due to their less importance. As shown in the top
right of Figure 1(a), after calculating norms of Ô¨Ålters in a
model, a pre-speciÔ¨Åed threshold T is utilized to select Ô¨Ålters whose norms are smaller than it.
However, as illustrated in Figure 1(b), there are two prerequisites to utilize this ‚Äúsmaller-norm-less-important‚Äù criterion. First, the deviation of Ô¨Ålter norms should be signiÔ¨Åcant. This requirement makes the searching space for
threshold T wide enough so that separating those Ô¨Ålters
needed to be pruned would be an easy task. Second, the
norms of those Ô¨Ålters which can be pruned should be arbitrarily small, i.e., close to zero; in other words, the Ô¨Ålters
with smaller norms are expected to make absolutely small
contributions, rather than relatively less but positively large
contributions, to the network. An ideal norm distribution
when satisfactorily meeting those two requirements is illustrated as the blue curve in Figure 1. Unfortunately, based
on our analysis and experimental observations, this is not
always true.
To address the problems mentioned above, we propose
a novel Ô¨Ålter pruning approach, named Filter Pruning via
Geometric Median (FPGM). Different from the previous
methods which prune Ô¨Ålters with relatively less contribution, FPGM chooses the Ô¨Ålters with the most replaceable
contribution. SpeciÔ¨Åcally, we calculate the Geometric Median (GM) of the Ô¨Ålters within the same layer. According to the characteristics of GM, the Ô¨Ålter(s) F near it can
be represented by the remaining ones. Therefore, pruning
those Ô¨Ålters will not have substantial negative inÔ¨Çuences on
model performance. Note that FPGM does not utilize norm
based criterion to select Ô¨Ålters to prune, which means its
performance will not deteriorate even when failing to meet
requirements for norm-based criterion.
Contributions. We have three contributions:
(1) We analyze the norm-based criterion utilized in previous works, which prunes the relatively less important
Ô¨Ålters. We elaborate on its two underlying requirements
which lead to its limitations;
(2) We propose FPGM to prune the most replaceable Ô¨Ålters containing redundant information, which can
still achieve good performances when norm-based criterion
(3) The extensive experiment on two benchmarks
demonstrates the effectiveness and efÔ¨Åciency of FPGM.
2. Related Works
Most previous works on accelerating CNNs can be
roughly divided into four categories, namely, matrix decomposition , low-precision weights ,
knowledge distilling and pruning. Pruning-based
approaches aim to remove the unnecessary connections of
the neural network . Essentially, weight pruning always results in unstructured models, which makes it
hard to deploy the efÔ¨Åcient BLAS library, while Ô¨Ålter pruning not only reduces the storage usage on devices but also
decreases computation cost to accelerate the inference. We
could roughly divide the Ô¨Ålter pruning methods into two categories by whether the training data is utilized to determine
the pruned Ô¨Ålters, that is, data dependent and data independent Ô¨Ålter pruning. Data independent method is more efÔ¨Åcient than data dependent method as the utilizing of training
data is computation consuming.
Weight Pruning. Many recent works focus on pruning Ô¨Åne-grained weight of Ô¨Ålters.
For example, proposes an iterative method to discard
the small weights whose values are below the predeÔ¨Åned
threshold. formulates pruning as an optimization problem of Ô¨Ånding the weights that minimize the loss while satisfying a pruning cost condition.
Data Dependent Filter Pruning. Some Ô¨Ålter pruning
approaches 
need to utilize training data to determine the pruned Ô¨Ålters.
 adopts the statistics information from the next layer to
guide the Ô¨Ålter selections. aims to obtain a decomposition by minimizing the reconstruction error of training set
sample activation. proposes an inherently data-driven
method which use Principal Component Analysis (PCA)
to specify the proportion of the energy that should be preserved. applies subspace clustering to feature maps to
eliminate the redundancy in convolutional Ô¨Ålters.
Data Independent Filter Pruning. Concurrently with
our work, some data independent Ô¨Ålter pruning strategies have been explored. utilizes an ‚Ñì1norm criterion to prune unimportant Ô¨Ålters. proposes
to select Ô¨Ålters with a ‚Ñì2-norm criterion and prune those selected Ô¨Ålters in a soft manner. proposes to prune models by enforcing sparsity on the scaling parameter of batch
normalization layers. uses spectral clustering on Ô¨Ålters
to select unimportant ones.
Discussion.
To the best of our knowledge, only one
previous work reconsiders the smaller-norm-less-important
criterion . We would like to highlight our advantages
compared to this approach as below: (1) pays more
attention to enforcing sparsity on the scaling parameter in
the batch normalization operator, which is not friendly to
the structure without batch normalization. On the contrary,
our approach is not limited by this constraint. (2) After
pruning channels selected, need Ô¨Åne-tuning to reduce
performance degradation. However, our method combines
the pruning operation with normal training procedure. Thus
extra Ô¨Åne-tuning is not necessary. (3) Calculation of the
gradient of scaling factor is needed for ; therefore lots
of computation cost are inevitable, whereas our approach
could accelerate the neural network without calculating the
gradient of scaling factor.
3. Methodology
3.1. Preliminaries
We formally introduce symbols and notations in this subsection. We assume that a neural network has L layers. We
use Ni and Ni+1, to represent the number of input channels and the output channels for the ith convolution layer,
respectively. Fi,j represents the jth Ô¨Ålter of the ith layer,
then the dimension of Ô¨Ålter Fi,j is RNi√óK√óK, where K is
the kernel size of the network1. The ith layer of the network W(i) could be represented by {Fi,j, 1 ‚â§j ‚â§Ni+1}.
The tensor of connection of the deep CNN network could be
parameterized by {W(i) ‚ààRNi+1√óNi√óK√óK, 1 ‚â§i ‚â§L}.
3.2. Analysis of Norm-based Criterion
Figure 1 gives an illustration for the two requirements
for successful utilization of the norm-based criterion. However, these requirements may not always hold, and it might
lead to unexpected results. The details are illustrated in Figure 2, in which the blue dashed curve and the green solid
curve indicates the norm distribution in ideal and real cases,
respectively.
Number of filters
Number of filters
Problem 1: œÉ (ùïçùïç‚Ä≤) << œÉ (ùïçùïç)
(a) Small Norm Deviation
Problem 2: ùë£ùë£1
‚Ä≤‚Ä≤ ‚â´ùë£ùë£1 ‚Üí0
(b) Large Minimum Norm
Figure 2. Ideal and Reality of the norm-based criterion: (a) Small
Norm Deviation and (b) Large Minimum Norm. The blue dashed
curve indicates the ideal norm distribution, and the green solid
curve denotes the norm distribution might occur in real cases.
(1) Small Norm Deviation. The deviation of Ô¨Ålter norm
distributions might be too small, which means the norm values are concentrated to a small interval, as shown in Figure 2(a). A small norm deviation leads to a small search
space, which makes it difÔ¨Åcult to Ô¨Ånd an appropriate threshold to select Ô¨Ålters to prune.
(2) Large Minimum Norm. The Ô¨Ålters with the minimum norm may not be arbitrarily small, as shown in the
1Fully-connected layers equal to convolutional layers with k = 1
Figure 2(b), v‚Ä≤‚Ä≤
1 >> v1 ‚Üí0. Under this condition, those
Ô¨Ålters considered as the least important still contribute signiÔ¨Åcantly to the network, which means every Ô¨Ålter is highly
informative. Therefore, pruning those Ô¨Ålters with minimum
norm values will cast a negative effect on the network.
3.3. Norm Statistics in Real Scenarios
In Figure 3, statistical information collected from pretrained ResNet-110 on CIFAR-10 and pre-trained ResNet-
18 on ILSVRC-2012 demonstrates previous analysis. The
small green vertical lines show each observation in this
norm distribution, and the blue curves denote the Kernel Distribution Estimate (KDE) , which is a nonparametric way to estimate the probability density function
of a random variable. The norm distribution of Ô¨Årst layer
and last layer in both structures are drawn. In addition, to
clearly illustrate the relation between norm points, two different x-scale, i.e., linear x-scale and log x-scale, are presented.
(1) Small Norm Deviation in Network. For the Ô¨Årst convolutional layer of ResNet-110, as shown in Figure 3(b),
there is a large quantity of Ô¨Ålters whose norms are concentrated around the magnitude of 10‚àí6. For the last convolutional layer of ResNet-110, as shown in Figure 3(c), the
interval span of the value of norm is roughly 0.3, which is
much smaller than the interval span of the norm of the Ô¨Årst
layer (1.7). For the last convolutional layer of ResNet-18, as
shown in Figure 3(g), most Ô¨Ålter norms are between the interval [0.8, 1.0]. In all these cases, Ô¨Ålters are distributed too
densely, which makes it difÔ¨Åcult to select a proper threshold
to distinguish the important Ô¨Ålters from the others.
(2) Large Minimum Norm in Network. For the last convolutional layer of ResNet-18, as shown in Figure 3(g), the
minimum norm of these Ô¨Ålters is around 0.8, which is large
comparing to Ô¨Ålters in the Ô¨Årst convolutional layer (Figure 3(e)). For the last convolutional layer of ResNet-110,
as shown in Figure 3(c), only one Ô¨Ålter is arbitrarily small,
while the others are not. Under those circumstances, the Ô¨Ålters with minimum norms, although they are relatively less
important according to the norm-based criterion, still make
signiÔ¨Åcant contributions in the network.
3.4. Filter Pruning via Geometric Median
To get rid of the constraints in the norm-based criterion,
we propose a new Ô¨Ålter pruning method inspired from geometric median. The central idea of geometric median is
as follows: given a set of n points a(1), . . . , a(n) with each
a(i) ‚ààRd, Ô¨Ånd a point x‚àó‚ààRd that minimizes the sum of
Euclidean distances to them:
x‚àó‚ààarg min
f(x) where f(x)
‚à•x ‚àía(i)‚à•2
Norm of filters
first conv layer
(a) ResNet-110 (linear x-scale)
Norm of filters
first conv layer
(b) ResNet-110 (log x-scale)
Norm of filters
last conv layer
(c) ResNet-110 (linear x-scale)
Norm of filters
last conv layer
(d) ResNet-110 (log x-scale)
Norm of filters
first conv layer
(e) ResNet-18 (linear x-scale)
Norm of filters
first conv layer
(f) ResNet-18 (log x-scale)
Norm of filters
last conv layer
(g) ResNet-18 (linear x-scale)
Norm of filters
last conv layer
(h) ResNet-18 (log x-scale)
Norm distribution of Ô¨Ålters from different layers of ResNet-110 on CIFAR-10 and ResNet-18 on ILSVRC-2012. The small
green vertical lines and blue curves denote each norm and Kernel Distribution Estimate (KDE) of the norm distribution, respectively.
As the geometric median is a classic robust estimator of
centrality for data in Euclidean spaces , we use the geometric median FGM
to get the common information of all
the Ô¨Ålters within the single ith layer:
x‚ààRNi√óK√óK g(x),
j‚Ä≤ ‚àà[1,Ni+1]
‚à•x ‚àíFi,j‚Ä≤ ‚à•2.
In the ith layer, if some Ô¨Ålters have the same, or similar
values as the geometric median in that layer, which is:
j‚Ä≤ ‚àà[1,Ni+1]
‚à•Fi,j‚Ä≤ ‚àíF GM
then those Ô¨Ålters, Fi,j‚àó, can be represented by the other Ô¨Ålters in the same layer, and therefore, pruning them has little
negative impacts on the network performance.
As geometric median is a non-trivial problem in computational geometry, the previous fastest running times for
computing a (1 + œµ)-approximate geometric median were
eO(dn4/3 ¬∑ œµ‚àí8/3) by , O(nd log3(n/œµ)) by . In our
case, as the Ô¨Ånal result Fi,j‚àóare a list of know points, that
is, the candidate Ô¨Ålters in the layer, we could relax the above
We assume that
‚à•Fi,j‚àó‚àíF GM
so the Equation.4 is achieved. Then the above Equation.2
becomes to
Fi,j‚àó‚ààarg min
j‚àó‚àà[1,Ni+1]
j‚Ä≤ ‚àà[1,Ni+1]
‚à•x ‚àíFi,j‚Ä≤ ‚à•2
j‚àó‚àà[1,Ni+1]
Note that even if the Ô¨Ålter need to be pruned, Fi,j‚àó, is
not included in the calculation of the geometric median in
Equation.62, we could also achieve the same result. In this
setting, we want to Ô¨Ånd the Ô¨Ålter
j‚àó‚àà[1,Ni+1]
j‚Ä≤ ‚àà[1,Ni+1],j‚Ä≤ Ã∏=j‚àó
‚à•x ‚àíFi,j‚Ä≤ ‚à•2.
With the above Equation.6 and Equation.8, we could get
g‚Ä≤(x) = g(x) ‚àí
‚à•x ‚àíFi,j‚Ä≤ ‚à•2
= g(x) ‚àí‚à•x ‚àíFi,j‚àó‚à•2.
2To select multiple Ô¨Ålters, we choose several j that makes g(x) to the
smallest extent.
Algorithm 1 Algorithm Description of FPGM
Input: training data: X.
1: Given: pruning rate Pi
2: Initialize: model parameter W = {W(i), 0 ‚â§i ‚â§L}
3: for epoch = 1; epoch ‚â§epochmax; epoch + + do
Update the model parameter W based on X
for i = 1; i ‚â§L; i + + do
Find Ni+1Pi Ô¨Ålters that satisfy Equation 6
Zeroize selected Ô¨Ålters
9: end for
10: Obtain the compact model W‚àófrom W
Output: The compact model and its parameters W‚àó
then we could get
min g‚Ä≤(x) = min{g(x) ‚àí‚à•x ‚àíFi,j‚àó‚à•2}
= min g(x) ‚àímin ‚à•x ‚àíFi,j‚àó‚à•2
= g(Fi,j‚àó) ‚àímin ‚à•x ‚àíFi,j‚àó‚à•2.
For the second component of the right side for Equation.10, when x = Fi,j‚àó, we can get:
Fi,j‚àó‚Ä≤ = Fi,j‚àó
since ‚à•x ‚àíFi,j‚Ä≤ ‚à•2 = 0
Since the geometric median is a classic robust estimator
of centrality for data in Euclidean spaces , the selected
Ô¨Ålter(s), Fi,j‚àó, and left ones share the most common information. This indicates the information of the Ô¨Ålter(s) Fi,j‚àó
could be replaced by others. After Ô¨Åne-tuning, the network
could easily recover its original performance since the information of pruned Ô¨Ålters can be represented by the remaining ones. Therefore, the Ô¨Ålter(s) Fi,j‚àócould be pruned with
negligible effect on the Ô¨Ånal result of the neural network.
The FPGM is summarized in Algorithm 1.
3.5. Theoretical and Realistic Acceleration
Theoretical Acceleration
Suppose the shapes of input tensor I ‚ààNi √ó Hi √ó Wi and
output tensor O ‚ààNi+1 √ó Hi+1 √ó Wi+1. Set the Ô¨Ålter
pruning rate of the ith layer to Pi, then Ni+1 √ó Pi Ô¨Ålters
should be pruned. After Ô¨Ålter pruning, the dimension of
input and output feature map of the ith layer change to I‚Ä≤ ‚àà
[Ni √ó (1 ‚àíPi)] √ó Hi √ó Wi and O‚Ä≤ ‚àà[Ni+1 √ó (1 ‚àíPi)] √ó
Hi+1 √ó Wi+1, respectively.
If setting pruning rate for the (i + 1)th layer to Pi+1,
then only (1 ‚àíPi+1) √ó (1 ‚àíPi) of the original computation is needed. Finally, a compact model {W‚àó(i) ‚àà
RNi+1(1‚àíPi)√óNi(1‚àíPi‚àí1)√óK√óK} is obtained.
Realistic Acceleration
In the above analysis, only the FLOPs of convolution operations for computation complexity comparison is considered, which is common in previous works . This is
because other operations such as batch normalization (BN)
and pooling are insigniÔ¨Åcant comparing to convolution operations.
However, non-tensor layers (e.g., BN and pooling layers)
also need the inference time on GPU , and inÔ¨Çuence the
realistic acceleration. Besides, the wide gap between the
theoretical and realistic acceleration could also be restricted
by the IO delay, buffer switch, and efÔ¨Åciency of BLAS libraries. We compare the theoretical and practical acceleration in Table 5.
4. Experiments
We evaluate FPGM for single-branch network (VGGNet
 ), and multiple-branch network (ResNet) on two benchmarks:
CIFAR-10 and ILSVRC-2012 3.
CIFAR-10 dataset contains 60, 000 32 √ó 32 color images in 10 different classes, in which 50, 000 training images and 10, 000 testing images are included. ILSVRC-
2012 is a large-scale dataset containing 1.28 million
training images and 50k validation images of 1,000 classes.
4.1. Experimental Settings
Training setting. On CIFAR-10, the parameter setting
is the same as and the training schedule is the same
as . In the ILSVRC-2012 experiments, we use the default parameter settings which is same as . Data argumentation strategies for ILSVRC-2012 is the same as Py-
Torch ofÔ¨Åcial examples. We analyze the difference between starting from scratch and the pre-trained model. For
pruning the model from scratch, We use the normal training
schedule without additional Ô¨Åne-tuning process. For pruning the pre-trained model, we reduce the learning rate to
one-tenth of the original learning rate. To conduct a fair
comparison of pruning scratch and pre-trained models, we
use the same training epochs to train/Ô¨Åne-tune the network.
The previous work might use fewer epochs to Ô¨Ånetune
the pruned model, but it converges too early, and its accuracy can not improve even with more epochs, which can be
shown in section 4.2.
Pruning setting. In the Ô¨Ålter pruning step, we simply
prune all the weighted layers with the same pruning rate at
the same time, which is the same as . Therefore, only
one hyper-parameter Pi = P is needed to balance the acceleration and accuracy. The pruning operation is conducted at
3As stated in , ‚Äúcomparing with AlexNet or VGG , both VGG (on CIFAR-10) and Residual networks have fewer parameters in the fully connected layers‚Äù, which makes pruning Ô¨Ålters in
those networks challenging.
Fine-tune?
Baseline acc. (%)
Accelerated acc. (%)
FLOPs ‚Üì(%)
92.20 (¬±0.18)
90.83 (¬±0.31)
Ours (FPGM-only 30%)
92.20 (¬±0.18)
91.09 (¬±0.10)
Ours (FPGM-only 40%)
92.20 (¬±0.18)
90.44 (¬±0.20)
Ours (FPGM-mix 40%)
92.20 (¬±0.18)
91.99 (¬±0.15)
92.63 (¬±0.70)
92.08 (¬±0.08)
Ours (FPGM-only 30%)
92.63 (¬±0.70)
92.31 (¬±0.30)
Ours (FPGM-only 40%)
92.63 (¬±0.70)
91.93 (¬±0.03)
Ours (FPGM-mix 40%)
92.63 (¬±0.70)
92.82 (¬±0.03)
93.59 (¬±0.58)
92.26 (¬±0.31)
Ours (FPGM-only 40%)
93.59 (¬±0.58)
92.93 (¬±0.49)
Ours (FPGM-mix 40%)
93.59 (¬±0.58)
92.89 (¬±0.32)
Ours (FPGM-only 40%)
93.59 (¬±0.58)
93.49 (¬±0.13)
Ours (FPGM-mix 40%)
93.59 (¬±0.58)
93.26 (¬±0.03)
93.68 (¬±0.32)
93.38 (¬±0.30)
Ours (FPGM-only 40%)
93.68 (¬±0.32)
93.73 (¬±0.23)
Ours (FPGM-mix 40%)
93.68 (¬±0.32)
93.85 (¬±0.11)
Ours (FPGM-only 40%)
93.68 (¬±0.32)
93.74 (¬±0.10)
Table 1. Comparison of pruned ResNet on CIFAR-10. In ‚ÄúFine-tune?‚Äù column, ‚Äú‚Äù and ‚Äú‚Äù indicates whether to use the pre-trained model
as initialization or not, respectively. The ‚ÄúAcc. ‚Üì‚Äù is the accuracy drop between pruned model and the baseline model, the smaller, the
the end of every training epoch. Unlike previous work ,
sensitivity analysis is not essential in FPGM to achieve good
performances, which will be demonstrated in later sections.
Apart from FPGM only criterion, we also use a mixture of FPGM and previous norm-based method to
show that FPGM could serve as a supplement to previous methods. FPGM only criterion is denoted as ‚ÄúFPGMonly‚Äù, the criterion combining the FPGM and norm-based
criterion is indicated as ‚ÄúFPGM-mix‚Äù. ‚ÄúFPGM-only 40%‚Äù
means 40% Ô¨Ålters of the layer are selected with FPGM only,
while ‚ÄúFPGM-mix 40%‚Äù means 30% Ô¨Ålters of the layer
are selected with FPGM, and the remaining 10% Ô¨Ålters
are selected with norm-based criterion . We compare
FPGM with previous acceleration algorithms, e.g., MIL ,
PFEC , CP , ThiNet , SFP , NISP , Rethinking . Not surprisingly, our FPGM method achieves
the state-of-the-art result.
4.2. Single-Branch Network Pruning
VGGNet on CIFAR-10. As the training setup is not
publicly available for , we re-implement the pruning
procedure and achieve similar results to the original paper. The result of pruning pre-trained and scratch model
is shown in Table 3 and Table 4, respectively. Not surprisingly, FPGM achieves better performance than in both
4.3. Multiple-Branch Network Pruning
ResNet on CIFAR-10. For the CIFAR-10 dataset, we
test our FPGM on ResNet-20, 32, 56 and 110 with two different pruning rates: 30% and 40%.
As shown in Table 1, our FPGM achieves the stateof-the-art performance.
For example, MIL without
Ô¨Åne-tuning accelerates ResNet-32 by 31.2% speedup ratio
with 1.59% accuracy drop, but our FPGM without Ô¨Ånetuning achieves 53.2% speedup ratio with even 0.19% accuracy improvement. Comparing to SFP , when pruning
52.6% FLOPs of ResNet-56, our FPGM has only 0.66% accuracy drop, which is much less than SFP (1.33%). For
pruning the pre-trained ResNet-110, our method achieves
a much higher (52.3% v.s. 38.6%) acceleration ratio with
0.16% performance increase, while PFEC harms the
performance with lower acceleration ratio. These results
demonstrate that FPGM can produce a more compressed
model with comparable or even better performances.
ResNet on ILSVRC-2012.
For the ILSVRC-2012
Accelerated
Accelerated
Ours (FPGM-only 30%)
Ours (FPGM-mix 30%)
Ours (FPGM-only 30%)
Ours (FPGM-mix 30%)
Ours (FPGM-only 30%)
Ours (FPGM-mix 30%)
Ours (FPGM-only 30%)
Ours (FPGM-mix 30%)
Ours (FPGM-only 30%)
Ours (FPGM-mix 30%)
Ours (FPGM-only 40%)
ThiNet 
Ours (FPGM-only 30%)
Ours (FPGM-mix 30%)
Ours (FPGM-only 40%)
Rethinking 
Ours (FPGM-only 30%)
Table 2. Comparison of pruned ResNet on ILSVRC-2012. ‚ÄúFine-tune?‚Äù and ‚Äùacc. ‚Üì‚Äù have the same meaning with Table 1.
Model \ Acc (%) Baseline
160 epochs
Table 3. Pruning pre-trained VGGNet on CIFAR-10. ‚Äúw.o.‚Äù means
‚Äúwithout‚Äù and ‚ÄúFT‚Äù means ‚ÄúÔ¨Åne-tuning‚Äù the pruned model.
Pruned From Scratch FLOPs‚Üì(%)
PFEC Y 93.58 (¬±0.03)
93.31 (¬±0.02)
Y 93.58 (¬±0.03)
93.54 (¬±0.08)
N 93.58 (¬±0.03)
93.23 (¬±0.13)
Table 4. Pruning scratch VGGNet on CIFAR-10. ‚ÄúSA‚Äù means
‚Äúsensitivity analysis‚Äù. Without sensitivity analysis, FPGM can still
achieve comparable performances comparing to ; after introducing sensitivity analysis, FPGM can surpass .
dataset, we test our FPGM on ResNet-18, 34, 50 and 101
with pruning rates 30% and 40%. Same with , we do
not prune the projection shortcuts for simpliÔ¨Åcation.
Table 2 shows that FPGM outperforms previous methods on ILSVRC-2012 dataset, again. For ResNet-18, pure
Theoretical
ResNet-101
Table 5. Comparison on the theoretical and realistic acceleration.
Only the time consumption of the forward procedure is considered.
FPGM without Ô¨Åne-tuning achieves the same inference
speedup with , but its accuracy exceeds by 0.68%.
FPGM-only with Ô¨Åne-tuning could even gain 0.60% improvement over FPGM-only without Ô¨Åne-tuning, thus exceeds by 1.28%.
For ResNet-50, FPGM with Ô¨Ånetuning achieves more inference speedup than CP , but
our pruned model exceeds their model by 0.85% on the accuracy. Moreover, for pruning a pre-trained ResNet-101,
FPGM reduces more than 40% FLOPs of the model without
top-5 accuracy loss and only negligible (0.05%) top-1 accuracy loss. In contrast, the performance degradation is 2.10%
for Rethinking . Compared to the norm-based criterion,
Geometric Median (GM) explicitly utilizes the relationship
between Ô¨Ålters, which is the main cause of its superior per-
Accuracy (%)
Other Epochs
(a) Different pruning intervals
Pruned FLOPs (%)
Accuracy (%)
Pruned Model
(b) Different pruned FLOPs
Accuracy of ResNet-110 on CIFAR-10 regarding different hyper-parameters. Solid line and shadow denotes the mean
values and standard deviation of three experiments, respectively.
To compare the theoretical and realistic acceleration, we
measure the forward time of the pruned models on one
GTX1080 GPU with a batch size of 64. The results 4 are
shown in Table 5. As discussed in the above section, the
gap between the theoretical and realistic model may come
from the limitation of IO delay, buffer switch, and efÔ¨Åciency
of BLAS libraries.
4.4. Ablation Study
InÔ¨Çuence of Pruning Interval In our experiment setting, the interval of pruning equals to one, i.e., we conduct
our pruning operation at the end of every training epoch.
To explore the inÔ¨Çuence of pruning interval, we change the
pruning interval from one epoch to ten epochs.
the ResNet-110 under pruning rate 40% as the baseline, as
shown in Fig. 4(a). The accuracy Ô¨Çuctuation along with the
different pruning intervals is less than 0.3%, which means
the performance of pruning is not sensitive to this parameter. Note that Ô¨Åne-tuning this parameter could even achieve
better performance.
Varying Pruned FLOPs We change the ratio of Pruned
FLOPs for ResNet-110 to comprehensively understand
FPGM, as shown in Fig. 4(b). When the pruned FLOPs
is 18% and 40%, the performance of the pruned model even
exceeds the baseline model without pruning, which shows
FPGM may have a regularization effect on the neural network.
InÔ¨Çuence of Distance Type We use ‚Ñì1-norm and cosine
distance to replace the distance function in Equation 3. We
use the ResNet-110 under pruning rate 40% as the baseline,
the accuracy of the pruned model is 93.73 ¬± 0.23 %. The
accuracy based on ‚Ñì1-norm and cosine distance is 93.87 ¬±
0.22 % and 93.56 ¬± 0.13, respectively. Using ‚Ñì1-norm as
the distance of Ô¨Ålter would bring a slightly better result, but
cosine distance as distance would slightly harm the performance of the network.
4Optimization of the addition of ResNet shortcuts and convolutional
outputs would also affect the results.
Combining FPGM with Norm-based Criterion We
analyze the effect of combining FPGM and previous normbased criterion.
For ResNet-110 on CIFAR-10, FPGMmix is slightly better than FPGM-only.
For ResNet-18
on ILSVRC-2012, the performances of FPGM-only and
FPGM-mix are almost the same. It seems that the normbased criterion and FPGM together can boost the performance on CIFAR-10, but not on ILSVRC-2012. We believe
that this is because the two requirements for the norm-based
criterion are met on some layers of CIFAR-10 pre-trained
network, but not on that of ILSVRC-2012 pre-trained network, which is shown in Figure 3.
4.5. Feature Map Visualization
We visualize the feature maps of the Ô¨Årst layer of the
Ô¨Årst block of ResNet-50. The feature maps with red titles
(7,23,27,46,56,58) correspond to the selected Ô¨Ålter activation when setting the pruning rate to 10%. These selected
feature maps contain outlines of the bamboo and the panda‚Äôs
head and body, which can be replaced by remaining feature maps: (5,12,16,18,22, et al.) containing outlines of the
bamboo, and (0,4,33,34,47, et al.) containing the outline of
Input image (left) and visualization of feature maps
(right) of ResNet-50-conv1.
Feature maps with red bounding
boxes are the channels to be pruned.
5. Conclusion and Future Work
In this paper, we elaborate on the underlying requirements for norm-based Ô¨Ålter pruning criterion and point out
their limitations. To solve this, we propose a new Ô¨Ålter pruning strategy based on the geometric median, named FPGM,
to accelerate the deep CNNs. Unlike the previous normbased criterion, FPGM explicitly considers the mutual relations between Ô¨Ålters. Thanks to this, FPGM achieves the
state-of-the-art performance in several benchmarks. In the
future, we plan to work on how to combine FPGM with
other acceleration algorithms, e.g., matrix decomposition
and low-precision weights, to push the performance to a
higher stage.