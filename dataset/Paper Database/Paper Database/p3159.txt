ADAPTIVE DATA AUGMENTATION FOR IMAGE CLASSIFICATION
Alhussein Fawzi⋆, Horst Samulowitz†, Deepak Turaga†, Pascal Frossard⋆
⋆EPFL, Switzerland & †IBM Watson Research Center, USA
Data augmentation is the process of generating samples by
transforming training data, with the target of improving the
accuracy and robustness of classiﬁers. In this paper, we propose a new automatic and adaptive algorithm for choosing
the transformations of the samples used in data augmentation. Speciﬁcally, for each sample, our main idea is to seek a
small transformation that yields maximal classiﬁcation loss
on the transformed sample. We employ a trust-region optimization strategy, which consists of solving a sequence of
linear programs. Our data augmentation scheme is then integrated into a Stochastic Gradient Descent algorithm for training deep neural networks. We perform experiments on two
datasets, and show that that the proposed scheme outperforms
random data augmentation algorithms in terms of accuracy
and robustness, while yielding comparable or superior results
with respect to existing selective sampling approaches.
Index Terms— Data augmentation, transformation invariance, image robustness, trust-region optimization.
1. INTRODUCTION
In many classiﬁcation problems, the available data is insufﬁcient to train accurate and robust classiﬁers. To alleviate the
relative scarcity of the data compared to the number of free
parameters of a classiﬁer, one popular approach is data augmentation (DA). Data augmentation consists in transforming
the available samples into new samples using label-preserving
transformations. For example, it is well known that sufﬁciently small afﬁne transformations of the data preserve the
label of an image. In , the importance of data augmentation
is particularly outlined in order to train very large deep networks and improve the generalization error. Unfortunately,
data augmentation is an art, as it involves many manual
choices. Inappropriate choices of data augmentation schemes
are likely to result in augmented samples that are not informative enough, which leads to no effect or detrimental effect
on the accuracy and robustness of classiﬁers. The choice of
the data augmentation strategy is therefore quite important to
reach good accuracy and robustness properties, with a limited
number of additional training samples.
In this paper, we focus on the problem of optimally choosing sample transformations from a transformation group for
data augmentation. That is, we propose an automated and
principled way for ﬁnding transformation parameters that
lead to increased accuracy and robustness of classiﬁers. The
key idea is to transform samples by small transformations
that induce maximal loss to the current classiﬁer. This worstcase data augmentation scheme leads to informative samples;
i.e., when these are added to the training set, the classiﬁer
is signiﬁcantly improved in terms of classiﬁcation accuracy
and robustness. We then propose a simple modiﬁcation of
the Stochastic Gradient Descent (SGD) algorithm to incorporate the proposed DA scheme in the training of deep neural
networks classiﬁers. We show that, in cases where training
data is insufﬁcient, the proposed training algorithm yields
signiﬁcant improvements with respect to schemes using no
augmentation, or random data augmentation.
Data augmentation has played an active role in achieving
state-of-the-art results on many vision tasks. Very often, DA
is performed by randomly generating transformations from a
set of possible transformations (see e.g., ). In , the authors propose a greedy strategy that selects the best transformation from a set of candidate transformations. While leading to impressive results, this strategy involves a signiﬁcant
number of classiﬁer re-training steps, in addition to the necessity of hard-coding the parameters of candidate transformations, which can be computationally expensive when the
number of candidate transformations is large. Data augmentation has also been shown to improve the classiﬁers’ robustness to diverse sets of perturbations, such as additive adversarial noise or geometric transformations . In , the
authors propose a principled way for automatically computing elastic deformation ﬁelds for digits in a computationally
efﬁcient manner. The transformations considered in the paper are however speciﬁc to digits. The very recent work of
 introduces an elegant way for data augmentation by randomly generating diffeomorphisms. The approach we follow
in this paper is different, as we focus on optimizing the data
generation process, while keeping the transformation group
relatively small (e.g., afﬁne transformations). Despite working with a small transformation group, the proposed approach
gives results that are on par with the mentioned works, and
leads in addition to signiﬁcant improvements in terms of accuracy and robustness with respect to schemes that do not use
data augmentation, or use random DA.
The paper is structured as follows. In Sec. 2, we delve
into the main idea of the paper, namely the worst-case data
augmentation process. We also derive intuitions that motivate
this approach. In Sec. 3, we formalize the worst-case DA
framework, and propose approximate algorithms for transformation computation, as well as an adapted training algorithm.
Experimental results are provided in Sec. 4 showing the advantage of our adaptive data augmentation scheme, and we
conclude in Sec. 5.
2. WHY WORST-CASE DATA AUGMENTATION?
To motivate the proposed procedure for data augmentation,
we start with a simple example. Consider a simple classi-
ﬁcation task, where the goal is to classify between images
representing a vertical line and a horizontal line. At training time, we only have access to two centered training samples represented in Fig. 1 (a,b). However, at test time, due
to uncontrolled acquisition of images, the lines might not be
perfectly centered.
In particular, images can incur a horizontal or vertical translation. We deﬁne the linear classiﬁer
i∈Ivert xi −P
i∈Ihor xi,, where Ivert and Ihor denote
the indices of the pixels that are active in the vertical and horizontal training images (Fig. 1 (a,b)), respectively, and x is
the column-reshaped image. Note that f is a perfectly accurate classiﬁer on the training set, as f(x) > 0 for the vertical
image (Fig 1 (a)), and f(x) < 0 for the horizontal image
(Fig 1 (b)). It should be noted however that, without further
augmentation of the data, this classiﬁer is not robust to small
translations of the data. Fig. 1 (c) shows the effect of vertical and horizontal shifts of the vertical line image (Fig. 1
(a)) on the value of f. Note that the value of f is robust to a
large extent to vertical shifts, while being extremely unstable
to horizontal shifts. Therefore, by adding horizontal shifts of
Fig. 1 (a) to the training set, the classiﬁer will get more robust
to these transformations. On the other hand, adding vertical
shifts to the training set will essentially have no impact on the
decision. Conversely, adding vertical shifts of the horizontal line (Fig. 1 (b)) to the training set is certainly beneﬁcial
for boosting the robustness, but adding horizontal shifts will
not change the decision function. This example highlights the
importance of designing adaptive data augmentation strategies that are speciﬁc to the dataset and classiﬁer. Moreover,
it suggests a simple adaptive strategy for adding examples in
the training set by adding transformations that maximize the
current classiﬁer loss (e.g., horizontal translations of vertical
line images). In other words, this corresponds to searching
for sufﬁciently small worst-case transformations that lead to
images that are incorrectly captured by the (current) classi-
ﬁer; adding these informative training samples to the training
set will lead to a change of the decision function and maximize the robustness. In the following sections, we formalize
this idea in detail, and propose an efﬁcient algorithm for data
augmentation.
Fig. 1. (a): Class 1 image, (b): class -1 image, (c): score
of the linear classiﬁer when applying vertical and horizontal
translations to the vertical line image (in (a)).
3. DATA AUGMENTATION ALGORITHM
In this section, we introduce a formalism for the worst-case
data augmentation framework. Let T denote the set of possible transformations, which is assumed to have t degrees
of freedom.
For example, when T corresponds to twodimensional translations, we have t = 2. For a given image
I, and transformation θ ∈T , we let Iθ be the image I transformed by θ. For an image I having label y, the worst-case
DA can be described by
θ∈T ℓ(y, Φ(Iθ)) subject to |θ| ≤L,
where ℓis the classiﬁer’s loss function, Φ is a feature extraction mapping, and L is a user-speciﬁed limit that upper
bounds the entries of the vector |θ| ∈Rt.
Note that the
constraint in (P) has the role of keeping the transformation
sufﬁciently small, which is important to guarantee the labelpreserving property of the transformation. The problem (P)
is difﬁcult to solve due to the nonconvexity of the objective
function for typical classiﬁers, and approximations become
necessary. For example, in convolutional neural network architectures, Φ is a composition of several elementary operations (e.g., convolution, nonlinearity, pooling) and ℓis often
set to the softmax loss. We therefore propose a generic iterative trust-region optimization scheme, where a linear
approximation is done at each iteration.1 Speciﬁcally, by linearizing the objective function of (P) around the current iterate, we have for small enough ∆θ
ℓ(y, Φ(Iθ+∆θ)) ≈ℓ(y, Φ(Iθ)) + ∇θℓ(y, Φ(Iθ))T ∆θ.
The gradient ∇θℓ(y, Φ(Iθ)) can be explicitly computed using the chain rule ∇θℓ(y, Φ(Iθ)) = Jθ(Iθ)T ∇ℓ(y, Φ(·))|Iθ,
1The derivations and algorithms developed in this paper are not speciﬁc
to deep networks.
Algorithm 1 Worst-case data augmentation
Inputs: image I, classiﬁcation functions ℓand Φ, transformation space T , parameters ∆L, L.
Outputs: transformed image Iˆθ.
Initialize ˆθ0 ←0T , K ←⌊L/∆L⌋.
for all i ∈{1, . . . , K} do
Solve the linear program
∆θ ∇ℓ(y, Φ(·))|T
θi−1Jθ(Iˆθi−1)∆θ
subject to |∆θ| ≤∆L.
ˆθi ←ˆθi−1 + ∆θ.
Set the ﬁnal estimate ˆθ ←ˆθK.
where Jθ ∈Rd×t is the Jacobian matrix of the function θ 7→
Iθ (with d the number of pixels in I). Hence, the following
linear program is considered to estimate the transformation
parameter increment ∆θ:
∆θ ∇ℓ(y, Φ(·))|T
IθJθ(Iθ)∆θ
subject to |∆θ| ≤∆L.
The parameter ∆L deﬁnes the size of the trust region, where
the linear approximation holds. The approximate problem in
Eq. (1) is now a linear program (LP) of size t, and can be
solved in polynomial time using off-the-shelf linear solvers.
Given that t is taken to be small in practice (e.g., t = 6 for
afﬁne transformations), the LP in Eq. (1) can be solved very
efﬁciently.2 Our full iterative trust-region optimization procedure is given in Algorithm 1. Note that ⌊L/∆L⌋iterations of
Eq. (1) are solved, in order to guarantee that the ﬁnal estimate
ˆθ satisﬁes the constraint of problem (P), that is |ˆθ| ≤L.
Given the trust-region data augmentation scheme in Algorithm 1, we propose a modiﬁcation of the Stochastic Gradient
Descent training procedure that incorporates data augmentation. The proposed Stochastic Gradient Descent with Data
Augmentation (SGD-DA) is given in Algorithm 2. The algorithm follows the SGD procedure, but with an additional
feature that permits to transform training points using Algorithm 1. At each iteration of the training algorithm, the current image is transformed with probability p ∈(0, 1) following Algorithm 1; that is, the training point is transformed by
the worst-case transformation with respect to the current datapoint I, and current classiﬁer parameters W, which is then
added to the training set. This learning strategy has the beneﬁt
of automatically adapting to the current classiﬁer, in order to
generate points that are informative for the current estimated
classiﬁer. Note moreover that SGD-DA can be extended in a
straightforward way to work with mini-batches at each iteration, instead of individual samples.
2In the experiments, we use the simplex algorithm in the MOSEK optimization toolbox .
Algorithm 2 Stochastic Gradient Descent with Data Augmentation (SGD-DA)
Inputs: Training samples I , labels Y , probability p,
learning rate η, parameters ∆L, L, T , loss ℓ.
Outputs: Classiﬁer parameters W.
Initialize the classiﬁer parameters W randomly.
while termination criterion not met do
Select a training point at random I ∈I , and let y ∈Y
be its associated label.
With probability p:
1. Let θ be the transformation obtained from Alg. 1.
2. Update: W ←W −η∇W ℓ(y, ΦW (Iθ)).
Otherwise (i.e., with probability 1−p), use the traditional
update step: W ←W −η∇W ℓ(y, ΦW (I)).
4. EXPERIMENTAL RESULTS
4.1. Experimental setup
We consider the transformation set T in our DA scheme to be
the set of two-dimensional afﬁne transformations (t = 6). We
set the boundary condition parameter L = 0.25, the trust region parameter ∆L to 0.05 and the data transformation probability p = 0.3. In the following experiments, we evaluate
our DA scheme in Algorithm 2 to train deep convolutional
neural networks. Speciﬁcally, in a ﬁrst training step, we train
a neural network on the original dataset, using SGD. Then,
using Algorithm 2, we ﬁne-tune the network for a ﬁxed number of extra epochs. The proposed scheme is compared to
several DA algorithms. In particular, we compare it to a random DA scheme, which follows the same procedure as Algorithm 2 but applies a random transformation satisfying the
constraint of (P): |θ| ≤L. This comparison is particularly important, as random DA is commonly applied in practice. We
perform quantitative comparison between the different methods in terms of test error, and robustness to transformations.
4.2. Experimental results
We ﬁrst provide an evaluation of the proposed algorithm on
the MNIST handwritten digits dataset . The dataset is
composed of 60, 000 training images, and 10, 000 test images. To make the problem more challenging, we consider
that the number of available training data is limited; we sample randomly 500 digits from each class, which results in a
dataset composed of 5, 000 images in total. The test set is
however kept unchanged. We consider a Convolutional Neural Network (CNN) architecture containing three successive
layers of convolution, pooling and rectiﬁed linear units operations. Table 1 shows the test errors of the proposed method
(Algorithm 2), as well as competing methods. The classi-
ﬁer ﬁne-tuned with the proposed method outperforms the approach with no DA, as well as the random DA. This lends
Test error (%)
Without DA
Random (afﬁne)
InﬁMNIST 
AlignMNIST 
Proposed (afﬁne)
Table 1. Test error on the MNIST-500 dataset
Number of epochs
Test error (%)
Random (affine)
Without DA
Fig. 2. Evolution of the test error with the epochs for i) random, ii) proposed and iii) no DA. For random and proposed,
transformed samples are added starting from epoch 40.
credence to the intuitive idea that optimizing over the set of
transformations leads to better results, compared to a scheme
that chooses the parameters in a random fashion. To further
study the difference between the two approaches, Fig. 2 illustrates the evolution of the test error with respect to the number
of epochs used to train the neural network. While the random
scheme stabilizes quite quickly around an error rate of 1.5%,
the adaptive approach constantly improves its classiﬁcation
error as it seeks for the optimal transformation with respect
to the current classiﬁer. We further analyze the inﬂuence of
parameter p on the test error in Fig. 3. For this experiment, all
networks are ﬁne tuned for 20 epochs. It can be seen that, for
our method, larger p implies a lower classiﬁcation error. In
other words, adding more samples to the training set largely
improves the error rate. Compared to the random scheme, the
proposed method requires much less transformed samples in
order to achieve low classiﬁcation error.
The proposed algorithm is also compared to state-of-theart algorithms in in Table 1. As can be seen, our
results are on par with existing methods that either consider
digit-speciﬁc transformations or much larger transformation
groups (e.g., diffeomorphisms in ). Conversely, the proposed method exploits a relatively simple and generic transformation group (afﬁne), and maximizes the impact of transformed samples to improve classiﬁcation results.
We now conduct similar experiments on the more challenging Small-NORB dataset , which contains 3D objects
representing 5 categories. Similarly to the previous experiment, we train a CNN classiﬁer, and perform ﬁne-tuning using random and the proposed DA scheme. The results are
Probability p
Test error (%)
Random (affine)
Fig. 3. Error rates for different choices of p.
Test error (%)
Without DA
Random (afﬁne)
Proposed (afﬁne)
Table 2. Test error on the Small-NORB dataset
Small-NORB
Without DA
Random (afﬁne)
Proposed (afﬁne)
Table 3. Manitest invariance scores for the two datasets
shown in Table 2. Our adaptive DA scheme results in a signiﬁcant performance boost compared to the classiﬁer that is
trained without DA, as well as random DA, and recent reported results on this dataset in .
We ﬁnally assess the robustness to transformations of our
learned classiﬁer in Table 3 by reporting the Manitest invariance scores for similarity transformations. It can be seen
that, for both datasets, the scores signiﬁcantly increase after
data augmentation using the proposed approach. Hence, our
DA scheme not only results in higher accuracy, but also leads
to larger robustness to transformations, which can be crucial
in real-world applications where images are perturbed by unknown transformations.
5. CONCLUSION
We proposed a novel DA approach where small transformations are sought to maximize the classiﬁer’s loss.
problem is formalized as a constrained optimization problem,
and solved using a trust-region approach with an iterative linearization scheme. Experimental results on two datasets have
shown that this simple scheme yields results that are on-par
(or superior) to state-of-the-art methods. In future work, we
plan to build on this framework to provide DA strategies that
can handle very large datasets (potentially containing millions
of images) by providing transformations that are common to
a large number of samples in the training set.
6. REFERENCES
 A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet
classiﬁcation with deep convolutional neural networks,”
in Advances in Neural Information Processing Systems
(NIPS), 2012, pp. 1106–1114.
 D. Cires¸an, U. Meier, J. Masci, L. Gambardella,
and J. Schmidhuber,
“High-performance neural networks for visual object classiﬁcation,” arXiv preprint
 
 D. Ciresan, U. Meier, and J. Schmidhuber,
“Multicolumn deep neural networks for image classiﬁcation,”
in IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2012, pp. 3642–3649.
 M. Paulin, J. Revaud, Z. Harchaoui, F. Perronnin, and
C. Schmid, “Transformation pursuit for image classiﬁcation,” in IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2014, pp. 3646–3653.
 C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, “Intriguing properties of neural networks,” in International Conference on
Learning Representations (ICLR), 2014.
 S-M. Moosavi Dezfooli, A. Fawzi, and P. Frossard,
“DeepFool: a simple and accurate method to fool deep
neural networks,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
 A. Fawzi and P. Frossard,
“Manitest: Are classiﬁers
really invariant?,” in British Machine Vision Conference
(BMVC), 2015, pp. 106.1–106.13.
 G. Loosli, S. Canu, and L. Bottou, “Training invariant
support vector machines using selective sampling,” pp.
301–320. MIT Press, Cambridge, MA., 2007.
 S. Hauberg, O. Freifeld, A. Larsen, J. Fisher III, and
L. Hansen, “Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation,” arXiv preprint arXiv:1510.02795, 2015.
 Andrew R. Conn, Nicholas I. M. Gould, and Philippe L.
Toint, Trust-region Methods, Society for Industrial and
Applied Mathematics, Philadelphia, PA, USA, 2000.
 MOSEK ApS,
The MOSEK optimization toolbox for
MATLAB manual. Version 7.1 (Revision 28)., 2015.
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner,
“Gradient-based learning applied to document recognition,”
Proceedings of the IEEE, vol. 86, no. 11, pp.
2278–2324, 1998.
 Y. Amit and A. Trouv´e, “Pop: Patchwork of parts models for object recognition,”
International Journal of
Computer Vision, vol. 75, no. 2, pp. 267–282, 2007.
 Y. LeCun, F.J. Huang, and L. Bottou, “Learning methods for generic object recognition with invariance to
pose and lighting,” in IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2004, pp. 97–