JOURNAL OF COMPUTATIONAL BIOLOGY
Volume 10, Number 2, 2003
© Mary Ann Liebert, Inc.
Pp. 187–213
Protein Family Classi cation Using Sparse
Markov Transducers
ELEAZAR ESKIN,1 WILLIAM STAFFORD NOBLE,2;4 and YORAM SINGER3
We present a method for classifying proteins into families based on short subsequences of
amino acids using a new probabilistic model called sparse Markov transducers (SMT). We
classify a protein by estimating probability distributions over subsequences of amino acids
from the protein. Sparse Markov transducers, similar to probabilistic suf x trees, estimate
a probability distribution conditioned on an input sequence. SMTs generalize probabilistic
suf x trees by allowing for wild-cards in the conditioning sequences. Since substitutions of
amino acids are common in protein families, incorporating wild-cards into the model signi cantly improves classi cation performance. We present two models for building protein
family classi ers using SMTs. As protein databases become larger, data driven learning
algorithms for probabilistic models such as SMTs will require vast amounts of memory.
We therefore describe and use ef cient data structures to improve the memory usage of
SMTs. We evaluate SMTs by building protein family classi ers using the Pfam and SCOP
databases and compare our results to previously published results and state-of-the-art protein homology detection methods. SMTs outperform previous probabilistic suf x tree methods and under certain conditions perform comparably to state-of-the-art protein homology
Key words: protein family classi cation, probabilistic suf x trees, machine learning.
1. INTRODUCTION
s databases of proteins classi ed into families become increasingly available, and as the number
of sequenced proteins grows exponentially, techniques to automatically classify unknown proteins
into families become more important. Many approaches have been presented for protein classi cation.
Initially, the approaches examined pairwise similarity . Other
approaches to protein classi cation are based on creating pro les for protein families , those based on consensus patterns using motifs and HMMbased (hidden Markov model) approaches .
1Department of Computer Science, Columbia University, New York, NY 10027.
2Department of Genome Sciences, Health Sciences Center, University of Washington, Seattle, WA 98195.
3School of Computer Science and Engineering, Hebrew University, Jerusalem, Israel 91904.
4Formerly William Noble Grundy.
ESKIN ET AL.
Recently, probabilistic suf x trees (PST) have been applied to protein family classi cation. A PST
is a model that predicts the next symbol in a sequence based on the previous symbols (for a formal
description see, for instance, Willems et al. , Ron et al. , Helmbold and Shapire ).
These techniques have been shown to be effective in classifying proteins into their appropriate family
 . This approach is based on the presence of
common short sequences throughoutthe protein family. These common short sequences, or motifs , are well understood biologically and have been used effectively in protein classi cation . PSTs are generative models that induce probabilities over subsequences from a protein by
building a probability distribution for an element in the protein sequence using the neighboring elements
in the sequence. A PST estimates the conditional probability of each element using the suf x of the input
sequence, which is then used to measure how well an unknown sequence  ts into that family.
One drawback of probabilistic suf x trees is that they rely on exact matches to the conditional (input)
sequences. However, in protein sequences of the same family, substitutions of single amino acids in
a sequence are extremely common. For example, two subsequences taken from the 3-hydroxyacyl-CoA
dehydrogenase protein family, VAVIGSGT and VGVLGLGT , are clearly very similar. However, they have
at most only two consecutive matching symbols. If we allowed matching gaps or wild-cards (denoted by
?), we notice that they match very closely: V ? V ? G ? GT . We would therefore expect that probabilistic
suf x trees would perform better if they were able to condition the probabilities on sequences containing
wild-cards, i.e., they could ignore or skip some of the symbols in the input sequence.
In this paper we present sparse Markov transducers (SMTs), a generalization of probabilistic suf x
trees which induce conditional probabilities over a sequence that contains wild-cards as described above.
Sparse Markov transducers build on previous work on mixtures of probabilistic transducers presented by
Willems et al. , Singer and Pereira and Singer . Speci cally, they extend previous
probabilistic suf x tree models to allow the incorporation of wild-cards into the model. They also provide
a simple generalization from a prediction (generative) model to a transduction (discriminative) model
which probabilistically maps sequences over an input alphabet to corresponding sequences over an output
alphabet. This formalism allows the input alphabet to be different from the output alphabet. We make use
of this generalization in our experiments.
We present two methods of building protein family classi ers using sparse Markov transducers. The two
models are used to classify unknown proteins into their appropriate families by extracting all subsequences
from the protein using a sliding window. For each subsequence, we obtain a probability associated for
each family. A score is obtained for each protein and family by computing the normalized sum of logs of
the probabilities for each subsequence. In both of these methods, the key to effective protein classi cation
is the estimation of probability distributions conditional on short sequences (in this case, sequences of
amino acids). In both methods, SMTs are used to estimate the probability distributions. The  rst method
is a generative model, which builds a prediction model that approximates a probability distribution over
a single amino acid conditioned on the sequence of neighboring amino acids. The second method is a
discriminative method, which builds estimates of probability distributions over protein families conditional
on sequences of amino acids. That is, in the second method we employ Markov transducers as mappings
from amino acid sequences to protein families.
We perform experiments over the Pfam database of protein families and
build a model for each protein family in the database. We compare our method to the published results
from the PST method of Bejerano and Yona . We also perform experiments over the SCOP database
and compare our results to state-of-the-art protein homology methods such as BLAST, HMMER, and the
Fisher kernel method .
A challenge in using probabilistic modeling methods is that the models they generate tend to be space
inef cient. Apostolico and Bejerano present an ef cient implementation of their PST method. That
algorithm brings the memory ef ciency close to the theoretical limit so long as a single PST or SMT is
concerned. However, the algorithm is rather complex, and it is not clear how to generalize the algorithm to
the mixtures approach employed in this paper. We therefore present an ef cient implementation for sparse
Markov transducers incorporating ef cient data structures that use lazy evaluation to signi cantly reduce
the memory usage, allowing better models to  t into memory. We show how the ef cient data structures
allow us to compute better performing models, which would be impossible to compute otherwise.
PROTEIN FAMILY CLASSIFICATION
This paper builds upon that of Eskin et al. where sparse Markov transducers applied to protein
classi cation were originally presented. This paper includes a more complete set of experiments comparing
the method to traditional protein homology methods and a complete mathematical description of the
model and its learning algorithm. In addition, this paper also describes in depth an extension to compute
discriminative models more ef ciently.
The organization of the paper is as follows. We  rst present the formalism of sparse Markov transducers.
Then we describe how we use sparse Markov transducers to build models of protein families. We describe
in depth a new construction that involves a mixture of many different sparse Markov transducers and discuss
its ef cient implementation for protein sequences. Finally, we discuss our classi cation results over the
Pfam and SCOP databases. Some of the details of the sparse Markov transducers are technical in nature
and are deferred to the appendices. The SMT software is available at www.cs.columbia.edu/compbio/smt/.
2. SPARSE MARKOV TRANSDUCERS
Sparse Markov transducers induce a probability distribution of an output symbol conditioned on a
sequence of input symbols. In our application, we are speci cally concerned with modeling probability
distributions of individual amino acids (output symbol) conditioned on their surrounding amino acids
(input sequence). We are also interested in the setting where the underlying distribution is conditioned
on sequences that contain wild-cards, such as in the case of protein families. We refer to this case as
probabilistic estimation over sparse sequences.
To model the probability distribution, we employ Markov transducers. A Markov transducer is de ned
to be a probability distribution over output symbols conditioned on a  nite set of input symbols. A Markov
transducer of order L induces a conditional probability distribution of the form
P .YtjXtXt¡1Xt¡2Xt¡3 : : : Xt¡.L¡1//;
where Xk are random variables over an input alphabet 6in and Yk is a random variable over an output
alphabet 6out. In this form of probability distribution,the output symbol Yk is conditional on the L previous
input symbols. If Yt D XtC1, then the model is a prediction model.1 In our application, Equation (1)
de nes the probability distribution conditioned on the context or sequence of neighboring amino acids. As
we discuss below, in the context of protein family classi cation, the conditioning sequences are sequences
of amino acids while Yt is either a single amino acid or the name of the protein family to which the
sequence of amino acids belongs.
Markov models provide a systematic way to model probability distributions conditioned on complete
input sequences. However, many natural sequences, and in particular biological sequences, exhibit a phenomenon known as sparseness where only fractions of the input sequences carry statistically signi cant
information on the output sequences. In order to achieve good probabilistic estimates, we need to cope
with sparseness in an algorithmically ef cient manner. Speci cally, we employ a transduction model that
represents parts of the conditioning sequences as wild-cards. We denote a wild-card by the symbol Á,
which represents a placeholder for an arbitrary input symbol. Put another way, multiple input sequences
are mapped to the same conditioning events in all the places where wild-card symbols appear. Similarly,
for notational convenience, we use Án to represent n consecutive wild-cards (i.e., arbitrary sequences of
length n) and Á0 as a placeholder representing no wild-cards. Therefore, a sparse Markov transducer is a
conditional probability of the form
P.YtjÁn1Xt1Án2Xt2 : : : ÁnkXtk/;
1In order to model “look ahead,” we can map every element xt to xtC1t where 1t is a constant value that refers
to the number of input symbols that are “looked ahead.” Thus, in this case, the conditioning sequence of random
variables would be XtC1tXtC1t¡1XtC1t¡2: : : : The output, Yt, remains unchanged.
ESKIN ET AL.
where ti D t ¡ .Pi
jD1 nj/ ¡ .i ¡ 1/. For instance, if 6in D fA; C; T; Gg then both ACTGA and ATATA
will be mapped to the same conditioning event AÁ3A. In fact, there are exactly 43 different sequences of
length 5 that are mapped to the conditioning AÁ3A.
Note that a Markov transducer is a special case of a sparse Markov transducer where ni D 0 for all i. The
goal of our algorithm is to estimate a conditional probability of this form based on a set of input sequences
and their corresponding outputs. However, our task is complicated due to two factors. First, we do not
know which positions in the conditioning input sequence should be wild-cards. Second, the positions of
the wild-cards change depending on the context, or the speci c values of the conditioning sequence. This
means that the positions of the wild-cards depend on the actual amino acids in the conditional sequence.
We present two approaches for SMT-based protein classi cation. The  rst approach is a generative
model where for each protein from a family we estimate a distribution over amino acids conditioned on
neighboring amino acids. In the generative model, the output (Yt) is an amino acid and the input sequence
(XtXt¡1Xt¡2 : : : ) is the neighboring amino acids. Since protein families contain similar subsequences,
subsequences from proteins of a given family will  t better into the distribution estimated for the particular
family as opposed to distributions estimated using other families. In the second approach, we build a single
model for the entire database which maps a sequence of amino acids to the name of the protein family
from which the sequence originated. This model estimates the distribution over protein family names (Yt)
conditioned on a sequence of amino acids (XtXt¡1Xt¡2 : : : ). In this model, the input alphabet is the set
of amino acids, and the output alphabet is the set of protein family names.
In general, our approach is as follows. Building upon the work on suf x trees for learning and prediction
 , we de ne
a type of prediction suf x tree called a sparse prediction tree which is representationally equivalent to
sparse Markov transducers. These trees probabilistically map input strings to a probability distribution over
the output symbols. The topology of a tree encodes the positions of the wild-cards in the conditioning
sequence of the probability distribution. We estimate the probability distributions of these trees from the
set of examples. Since a priori we do not know the positions of the wild-cards, we do not know the best
tree topology. For this reason, we use a mixture (weighted sum) of trees and update the weights of each
tree based on its performance over the set of examples. We update the trees so that the better-performing
trees get larger weights while the worse-performing trees get smaller weights. Thus, the data is used to
choose the positions of the wild-cards in the conditioning sequences. We now formally describe sparse
prediction trees and discuss the algorithm for updating a mixture of such trees ef ciently. The technical
details of the algorithm that allow for the exact computation of the mixture weights for an exponential
number of trees are deferred until Section 5.
2.1. Sparse prediction trees
For the simplicity of the derivation of our algorithms, rather than using Markov transducers directly,
we employ a tree-based representation which is a speci c type of prediction suf x tree called a sparse
prediction tree. Sparse prediction trees generalize prediction suf x trees that were described by Ron et al.
 and used by Bejerano and Yona in the task of probabilistic modeling of biological sequences.
A sparse prediction tree is a rooted tree where each node is either a leaf node or contains one branch
labeled with Án for n ¸ 0 that forks into a branch for each element in 6in (each amino acid). Each leaf
node of the tree is associated with a probability distribution over the output alphabet, 6out (amino acids).
Figure 1 shows a sparse prediction tree. In this tree, each of the leaf nodes, u1; : : : ; u7, is associated with
a probability distribution. The path from the root node to a leaf node represents the conditioning sequence
in the probability distribution. We thus label each node using the path from the root of the tree to the node.
Since the path contains the wild-card symbol Á, there are multiple strings over 6in that are mapped to a
single node. Put another way, each edge labeled Án can be traversed by all the j6jn different sequences
of length n.
A tree associates a probability distribution over output symbols conditioned on the input sequence by
following an input sequence from the root node to a leaf node, skipping a symbol in the input sequence
for each Á along the path. The probability distribution conditioned on an input sequence is the probability
distribution associated with the leaf node that corresponds to the input sequence. As described below, the
tree is trained with a dataset of input sequences xt and their corresponding output symbols yt. The input
PROTEIN FAMILY CLASSIFICATION
An illustration of a sparse prediction tree. For space considerations, we do not draw branches for all 20
amino acids.
sequence at time step t, xt, is the de ned to be the entire string of input symbols observed from time step
1 through time step t. Using a notation analogous to Equation (1), let xi 2 6in be the input symbol that
was observed at time t; then xt D .xt; xt¡1; : : : ; x2; x1/. Note that this de nition implies that the input
sequences are strings of growing sizes. In practice, we set a limit on the maximum depth of the sparse
prediction trees which implies that each xt is a  nite string whose length is the same as the maximal
imposed depth of the learned tree.
For example, in Fig. 1 the sets of input strings that correspond to each of the two highlighted nodes
are u2 D Á1AÁ2C and u5 D Á1CÁ3C. In our setting, the two nodes would correspond to any amino acid
sequences ?A ? ? C and ?C ? ? ? C where the symbol ? denotes a wild-card. The node labeled u2 in the
 gure corresponds to many sequences including AACCC and DAACC. Similarly, the node labeled u5 in the
 gure corresponds to sequences ACAAAC and CCADCC. The string CCADCCCA is also mapped to u5
since the pre x of the sequence leads to u5. The probability corresponding to an observation from the input
sequence is the probability contained in the leaf node corresponding to the conditioning input sequence.
For instance, in this example P .AjAACCC/ is probability of observing the symbol A associated with
the leaf u2. These probabilities are estimated from counts of output symbols from training examples that
reach this node as described below. Note that this is slightly different from the traditional PST formulation
 where the suf x of a sequence is used to determine the corresponding leaf
node. However, these formulations are effectively equivalent because we can preprocess the data to map
from one to the other.
In summary, a sparse prediction tree, denoted T , can be used to induce conditional probability distributions over output symbols as follows. For an example pair containing an output symbol yt and an
input sequence xt, we can determine the conditional probability for the example, denoted PT .ytjxt/. As
described above, we  rst determine the node u which corresponds to the input sequence xt. Once that
node is determined, we use the probability distribution over output symbols associated with that node. The
prediction of the tree for the example is then PT .ytjxt/ D PT .ytju/.
We show in Appendix A that any sparse Markov transducer can be represented as a sparse prediction
tree of equivalent size. We use, however, the tree-based representation which can naturally be combined
with the mixture technique we employ.
2.2. Training a prediction tree
A prediction tree is trained from a set of training examples consisting of output symbols and the
corresponding sequences of input symbols. In our application, the training set is either a set of amino
acids and their corresponding contexts (neighboring sequence of amino acids) or a set of protein family
names and sequences of amino acids from that family. The input symbols are used to identify which leaf
node is associated with that training example. The output symbol is then used to update the count of the
appropriate predictor.
ESKIN ET AL.
Each predictor node keeps counts of each output symbol (amino acid) seen by that predictor. We smooth
each count by adding a constant value to the count of each output symbol. The predictor’s estimate of
the probability for a given output is the smoothed count for the output divided by the total count in the
predictor.
This method of smoothing is motivated by Bayesian statistics using the Dirichlet distribution which is the
conjugate family for the multinomial distribution. However, the discussion of Dirichlet priors is beyond the
scope of this paper. Further information on the Dirichlet family can be found in DeGroot . Dirichlet
priors have been shown to be effective in protein family modeling to the initial count values. If, for example, the  rst element of training data is the output A
and the input sequence ADCAAACDADCDA, we would  rst identify the leaf node that corresponds to the
sequence. In this case, the leaf node would be u7. We then update the predictor in u7 with the output
A by adding 1 to the count of A in u7. Similarly, if the next output is C and the input sequence is
DACDADDDCCA, we would update the predictor in u1 with the output C. If the next output is D and the
input sequence is CAAAACAD, we would update u1 with the output D.
After training on these three examples, we can use the tree to output a prediction for an input sequence by
using the probability distribution of the node corresponding to the input sequence. For example, assuming
the initial count is 0, the prediction of the the input sequence AACCAAA which correspond to the node u1
would give an output probability where the probability for C is :5 and the probability of D is :5.
3. MIXTURE OF SPARSE PREDICTION TREES
In the general case, we do not know a priori where to put the wild-cards in the conditioning sequence
of the probability distribution because we do not know on which input symbols the probability distribution
is conditional. Thus, we do not know which tree topology to use so as to obtain the best estimate by a
sparse prediction tree. Intuitively, we want to use the training data in order to learn which tree predicts
most accurately.
We use a Bayesian mixture approach for the problem. Instead of using a single tree as a predictor, we
use a mixture technique which employs a weighted sum of trees as our predictor. We then use a Bayesian
update procedure to update the weight of each tree based on its performance on each element of the dataset.
In this way, the weighted sum uses the data to make the best prediction.
We use a Bayesian mixture for two reasons. First, mixture models provide richer representations than
do individual models. Second, our proposed model is built online so that it can be improved on-the- y
with more data, without requiring the retraining of the model. This means that as more and more proteins
get classi ed into families, the models can be updated without retraining the model over the up-to-date
database. For a theoretical treatment and further discussion on mixture models of prediction tree see
 and the references therein.
The skeleton of the learning algorithm for the mixture of SMTs is as follows. We initialize the weights
of each SMT in the mixture to the prior probabilities of the trees (discussed below). Then, we update the
weight of each tree in an online fashion for each training example in the training set based on how well
the tree performed on predicting the most recent output. At the end of this process, we have a weighted
sum of (sparse Markov) trees in which the better performing trees have higher weights.
Speci cally, we assign a weight, wt
T , to each tree in the mixture after processing training example t.
The prediction of the mixture after training example t is the weighted sum of all the predictions of the
trees divided by the sum of all weights:
P t.YjXt/ D
T PT .YjXt/
where PT .YjXt/ is the prediction of tree T for input sequence Xt.
PROTEIN FAMILY CLASSIFICATION
Equation (3) employs a weighted sum of predictions. As discussed above, each weight wt
T re ects the
performance of a tree. Initially, prior to any observations, these weights are initialized in a way that re ects
the complexity of each tree in the mixture and are then updated after each round. We now discuss in detail
how these mixture weights are determined, starting with how the initial weights are determined.
4. PRIOR DISTRIBUTION OF SPARSE PREDICTION TREES
Our construction of the prior probability of a sparse Markov tree T , denoted w1
T , is based on the
complexity of the topology of the tree. Intuitively, the more complicated the topology of the tree, the
smaller its prior probability. We now discuss in detail our construction of a prior probability distribution
over SMTs. This construction is recursive and enables us to perform the weight update in an ef cient
manner. For simplicity of presentation, we describe our construction of a prior probability over SMTs as
a stochastic process that generates random trees. The prior probability of a speci c tree is the probability
of generating that tree according to the process that we now describe.
Given a probability distribution over the (nonnegative) integers, denoted PÁ, and starting at the root
node, we perform the following process. We pick an integer n at random according to the distribution PÁ.
If n D 0, we stop the generation process making the current node a leaf. Otherwise (n ¸ 1), we add to the
current node a branch labeled Án¡1 and generate child nodes below that branch, one for every symbol in
6in. For each of these new nodes, we repeat the process recursively. Clearly, this process induces a (prior)
probability of sparse prediction trees.
We refer to the probability distribution induced by the process described above as the generative probability distribution. Intuitively, the outcome of the generation process at each node determines how far
forward we look for the next input. If the outcome is 0, then we do not condition on any more inputs. If
the value is 1, we condition on the very next input. If the outcome is n > 2, then we skip (or mark as
wild-cards) the next n ¡ 1 inputs and condition on the nth next input.
We can associate a different prior distribution with each node of a sparse prediction tree. Let u be
an arbitrary node. We denote by P u
Á the generative probability distribution over the possible skip lengths
associated with u. Since P u
Á is a distribution, we have that
Á .i/ D 1:
For each node in a tree u, we denote the actual number of skips that were picked according to P u
uÁ. The value uÁ ¡ 1 is the number of Á’s associated with the (single) edge leaving the node u. Finally,
if a node u is a leaf, uÁ of that node is de ned to be 0.
To complete our description of the prior probability of a tree, we need to introduce a few more de nitions.
These de nitions will also be useful in the weight update procedure for sparse prediction trees. For a tree
T , we denote by LT the set of leaves of that tree. We also de ne NT to be the set of nodes of the tree
(including the leaf nodes). Let Tu denote the subtree rooted at u. Similarly, we de ne NTu and LTu to be
the set of nodes and leaf nodes, respectively, of the subtree Tu.
The prior probability of a tree can now easily be computed using the generative probability distribution
at each node and the actual Á value of each node. Summing up, the prior probability of tree T , denoted
T , is therefore
where uÁ is the Á value of the node u and P u
Á is the generative probability distribution associated with
the node u.
Assume for example that for all possible nodes P u
Á .n/ D 4¡n
for 0 · n · 3 and P u
Á .n/ D 0 otherwise.
Figure 2 illustrates the probability of the outcomes (Á values) at each node. The prior probability of the
entire tree shown in the  gure is . 2
5 D 0:0004096, which is the product of the probabilities of
each Á value at each node.
ESKIN ET AL.
A sparse prediction tree with its generative probabilities.
Finally, we would like to point out that a maximal depth limit Dmax on a sparse prediction tree can be
simply imposed by limiting the distributions P u
Á to a  nite set of values. Formally, for a node u at depth d,
we set P u
Á .n/ D 0 for all values n > Dmax ¡ d. Note that this implies that for any node u at the maximal
depth Dmax we have P u
Á .0/ D 1 and P u
Á .n/ D 0 for n > 0.
5. WEIGHT UPDATE ALGORITHM
As discussed above, we use a Bayesian approach to update the weights of the mixture after each training
example. The mixture weights are updated according to the evidence, which is simply the probability of an
output yt given the input sequence xt, PT .ytjxt/. The prediction is obtained by updating the tree according
to the previous example and then computing the prediction of the example. Intuitively, this gives a measure
of how well the tree performed on the given example. The unnormalized mixture weights are updated using
the following rule:
T PT .ytjxt/
T is de ned to be the prior weight of the tree. Thus, the weigh of a tree is the prior weight times
the evidence for each training example; that is,
PT .yijxi/:
After each training example we need to update the weights for every possible sparse prediction tree T .
Clearly, the number of prediction trees in the mixture is huge, even for relatively small values of Dmax
and Ámax. Therefore, a straightforward approach that directly updates every sparse prediction tree in the
mixture is not feasible. Instead, we present now an ef cient algorithm that tacitly updates the individual
tree weights by maintaining mixture weights of hierarchical subsets of the entire mixture. Our algorithm
stems from and builds upon the learning algorithms for prediction suf x trees described by Willems et al.
 and Helmhold and Shapire .
Efcient weight update
Instead of maintaining multiple sparse prediction trees of different sizes, the ef cient algorithm maintains
a single template tree that is the union of all the nodes composing the trees in the mixture. At each node
of the large template tree, we maintain two weights which we use to update the weights of all the trees
in the mixture. These weights are also used for calculating ef ciently the mixture’s prediction on the next
output. We denote the two weights we keep at each node u of the template tree by wt.u/ and wt.u/ where
t designates the length of the input sequence that has been observed so far.
PROTEIN FAMILY CLASSIFICATION
The  rst weight wt.u/ is the likelihood of the predictions induced by u on all subsequences reaching
the node u. Let us denote by xt 2 u the event that the input sequence xt has mapped to node u. Recall
that there are multiple nodes that xt is mapped to, including u’s ancestors and children. The algorithm
updates the weights of all of these nodes in one sweep from the root to all the leaves of the template tree.
The weight of each node u is initialized to one, and thus
w1.u/ D 1:
If xt 2 u, then we need to update the likelihood of u as follows:
wtC1.u/ D wt.u/P.ytju/:
If, however, xt 62 u, then we need not change the weight of u; therefore, we implicitly set wtC1.u/ D
wt.u/. Using the weights wt.u/, we can now rewrite the weight of a single tree T in the mixture as
PT .yijxi/ D
In order to calculate the predictions of the mixture as given by Equation (3), we must keep track of the
sum of all the tree weights at time t,
T . To do so ef ciently, we keep track of the sum of all weights
for the subtrees rooted at each node. This is the purpose of the second variable wt.u/. Let T u denote the
set of all possible subtrees rooted at u. Then wt.u/ is de ned to be the sum of all weights of trees in Tu.
We calculate wt.u/ using Equation (8) as follows:
We now can use the subtree weights to compute the sum of all tree weights in the mixture. Let ¸ denote
the empty string. According to our construction, the set of all prediction trees in the mixture is equal to the
set off all subtrees rooted at the root node of the template tree. Therefore, using the the de nition of Tu
and the fact that the root node is associated with the empty string, we can rewrite the sum of the weights
of all prediction trees in the mixture at time t as
A direct evaluation of the right hand side of Equation (10) is still computationally expensive. However,
we can exploit the recursive nature of the the weights to derive an ef cient update for wt.u/ from the
updated weights of all of its children nodes. The following claim gives the form of the update. Its proof
is deferred to Appendix B.
For t ¸ 1 and for all nodes u in a template sparse prediction tree, the following equality
wt.u/ D P u
Á .0/wt.u/ C
wt.uÁi¡1¾/:
Equation (11) is the center of the weight update scheme and deserves some further attention. First, note
that the equation indeed provides a recursive scheme for computing wt.u/ from the weight wt.u/ and the
weights of all of u’s children, which are of the form uÁi¡1¾. Though syntactically the recursion includes
an in nite sum, the number of the summands is bounded by the minimum between the length of the input
sequence so far, which is simply t, and the maximal depth of the prediction trees, Dmax. Thus, the update
ESKIN ET AL.
of the weights wt.u/ and wt.u/ can be done ef ciently in one bottom-up pass from the leaf nodes to the
root. To summarize, the weight update procedure is as follows:
Initialize: w1.u/ for all u in the template tree.
Update w: for all u such that xt 2 u: wtC1.u/ D wt.u/P.ytju/.
Update w: for all u such that xt 2 u wtC1.u/ D P u
Á .0/wt.u/ C P
¾ wt.uÁi¡1¾/:
Maintain: for all u such that xt 62 u wtC1.u/ D wt.u/; wtC1.u/ D wt.u/:
We note that the above update involves multiple paths. At each node along the path, the input sequence
can bifurcate going down to the child node associated with the next input symbol and to all the children
associated with wildcards of the form Ái.
In updating our weights, we take advantage of the fact that many of the sequences occur only once in
the data. Because of this, many of the subtrees will be traversed by only a single sequence. For the root
nodes of subtrees that were traversed only once, we can compute their subtree weight without having to
explicitly expand the node into its subtree. Our ability to make this computation stems from the fact that
each of the nodes along the path of the sequence will contain only that sequence and thus have the same
node weight. More precisely, let u be the root of a subtree that is reached only once. Then, for any node
v in the subtree below u, we have that wt.u/ D wt.v/ (for all time steps t). Furthermore, it is simple to
verify that the recursive priors imply that for all such nodes u we get wt.u/ D w.u/t.
The weight update scheme also serves for outputting predictions. In order to make predictions using the
mixtures after training, we can use node weights and subtree weights to compute our predictions ef ciently.
For any Oy 2 6out, the probability of prediction of Oy at time t is
P. Oyjxt/ D
T PT . Oyjxt/
If we set yt D Oy, then we have
P.ytjxt/ D
T PT .ytjxt/
Thus, the prediction of the SMT for an input sequence and output symbol is the ratio of the weight of
the root node if the input sequence and output symbol are used to update the tree to the original weight.
When using the mixture of sparse trees for making predictions, we need to compute the probability of
all protein families given the conditioning sequence. In other words, we want to compute the probability
of every output symbol given the input sequence. The simplest way to do this would be to enumerate over
all possible values for yt (we have j6outj alternatives) and then compute wtC1 6out using Equation (13).
However, this method requires a traversal of the tree j6outj times, which is time consuming if 6out is
large. Here we can take advantage of the fact that the input sequence is the same for all possible values for
yt and thus the traversed nodes are the same. We can make this computation more ef cient if we compute
the probabilities for each output symbol in a single transversal of the tree. Since the probability for an
output symbol is wtC1
with wtC1 updated for the output symbol, we compute a vector where each element
stores the wtC1 for the corresponding symbol in 6out. Now, as we traverse the tree, we update the whole
vector, but obtaining the predictions takes only a single sweep through the tree. We can further optimize
this scheme since we only need to keep track of the elements in the vector that have been observed in the
nodes. That is, the probability of all the unobserved symbols is the same and hence need to be computed
only once. These technical improvements signi cantly reduce the running time when dealing with large
alphabets as is the case of the protein families.
PROTEIN FAMILY CLASSIFICATION
6. IMPLEMENTATION ISSUES
In this section we discuss several implementation issues that enable us to cope with relatively large
protein datasets. We  rst describe the constraints that we impose on the structure of SMTs which enable
ef cient time and space implementation.
The components of the mixture are all possible trees with certain topologies, which can be enormous.
We use two parameters to restrict the possible tree topologies in the mixture: Dmax, the maximum depth
of the tree, and Ámax, the maximum number of wild-cards at every node, i.e., the number of consecutive
symbols allowed to be skipped. Recall that the depth of a node in the tree is de ned to be the length of
the input sequence that reaches the node. The maximum number of wild-cards de nes the highest power
of Á on a branch leaving a node. If Ámax D 0, no wild-cards are allowed, and the model reduces to a
mixture of prediction suf x trees (PST) where each tree is similar to the form used by Bejerano and Yona
 . Both Dmax and Ámax affect the number of trees in the mixture. Increasing either value increases
the running time of the SMTs and increases the number of total nodes. Even with small values of Dmax
and Ámax, the number of trees can be very large. For instance, there are ten different trees in the mixture
if Dmax D 2 and Ámax D 1 as shown in Fig. 3 where for illustrative purposes we use an alphabet of size
three, fA; C; Dg. With 20 amino acids, there are over a million sparse prediction trees in the mixture.
We can store the set of all trees in the mixture much more ef ciently using a template tree. This is a
single tree that stores the entire mixture. The template tree is similar to a sparse prediction tree except that
from each node it has a branch for every possible number of wild-cards at that point in the sequence. A
template tree for the trees in the mixture of Fig. 3 is shown in Fig. 4.
Even in the template tree, the maximum number of nodes in the model is also very large. In Fig. 4,
there are 16 nodes. However, not every node needs to be stored. We store only the nodes that are reached
during training. For example, if the training examples contain the input sequences, AA, AC, and CD, only
nine nodes need to be stored in the tree as shown in Fig. 5. This is implemented by starting the algorithm
with just a root node and adding elements to the tree as they are reached by examples.
The template tree stores all of the leaf nodes that occur in the trees of the mixture. Each node in the
template tree stores a weight of how well that node performs as well as a weight for the subtree rooted
An illustration of a mixture of sparse suf x trees for Dmax D 2 and Ámax D 1. In order to simplify the
 gure, we assume that the input alphabet consists of three symbols fA; C; Dg.
ESKIN ET AL.
The template tree for a mixture of sparse prediction trees shown in Fig. 3.
at that node. These weights are ef ciently updated during training as discussed in the previous section.
Using these weights, we can compute the exact prediction of the mixture as also discussed previously.
Data structures
Even if we only store the nodes reached by the input sequence in the data, the template tree can still
grow exponentially fast. With Ámax > 0, the tree branches at each node on every input. Intuitively, this
represents the fact that there is an exponential number of possible positions to place the wild-cards in the
input sequence. Table 1 shows the number of nodes in a tree with various values of Dmax and Ámax after
an empty tree was updated with a single example. Since performance of the SMT typically improves with
higher Ámax and Dmax, the memory usage becomes a bottleneck because it restricts these parameters to
values that will allow the tree to  t in memory and thus the full power of SMTs is not utilized.
Our solution is to use lazy evaluation to provide more ef cient data structures with respect to memory.
The intuitive idea is that instead of storing all of the nodes created by a training example, we store the tails
of the training example (sequence) and recompute part of the tree on demand when necessary. There is an
inherent computational cost to this data structure because in many cases the training examples need to be
recomputed on demand. Intuitively, we want the parts of the tree that are used often to be stored explicitly
as nodes, while the parts of the tree that are not used often are stored as sequences and are recomputed
when needed. The data structure is designed to perform exactly the same computation of the SMTs but
with signi cant savings in memory usage. This lazy evaluation approach is rather simple to implement and
enables an ef cient time and space computation of the prediction of the entire mixture. We would like to
note, though, that for a single prediction tree there are other approaches that are even more ef cient (see
Apostilico and Bejerano and the references therein). However, it is not obvious how to adopt these
approaches for our setting which involves multiple sparse prediction trees.
The template tree of Fig. 4 after processing input sequences AA, AC, and CD.
PROTEIN FAMILY CLASSIFICATION
Number of Nodes after a Single Training Example
without Ef cient Data Structuresa
aThe number of nodes generated per examples increases exponentially with Ámax.
The data structure de nes a new way to store the template tree. In this model, the children of nodes in
the template tree are either nodes or sequences. Figure 6 gives examples of the data structure. A parameter
to the data structure, Smax, de nes the maximum number of sequences that can be stored on the branch of
Let us look at an example where we are computing a SMT with Dmax D 7 and Ámax D 1 with the
following  ve input sequences (and the corresponding output symbols in parentheses): ACDACAC.A/,
DACADAC.C/, DACAAAC.D/, ACACDAC.A/, and ADCADAC.D/. Without the ef cient data structure,
the tree contains 241 nodes and takes 31 kilobytes to store. The ef cient template tree is shown in Fig. 6a.
The ef cient data structure contains only one node and ten sequences and takes about 1,000 bytes to store.
When Smax D 3, each node branch can store up to three sequences before it expands a sequence pointer
into a node. In the example shown in Fig. 6a, because there are already three sequences in the branch
labeled Á0A, any new sequence starting with A will force that branch to expand into a node. Thus, if we
add the input sequence ACDACAC.D/, we get the tree in Fig. 6b.
The classi cation performance of SMTs tends to improve with larger values of Dmax and Ámax, as we
will show in the results section. The ef cient data structures are therefore important since they allow us
to compute SMTs with higher values of these parameters.
Ef cient data structures for SMTs. The boxes represent input sequences with their corresponding output.
(a) The tree with Smax D 3 after input sequences ACDACAC.A/, DACADAC.C/, DACAAAC.D/, ACACDAC.A/,
and ADCADAC.D/. (b) The tree after input sequence ACDACAC.D/ is added. Note that a node has been expanded
because of the addition of the input.
ESKIN ET AL.
Short circuit evaluation
We can further optimize the performance by taking advantage of the data being sparse. Since the number
of possible sequences seen is signi cantly smaller than the number of all possible sequences, we can assume
that many of the observed sequences will occur only once. This means that many of the nodes in the tree
will be reached by only a single sequence. For these nodes, we can compute the subtree weights very
ef ciently by using the fact that the predictions of all the nodes that have not been reached even once are
all the same.
The subtree weight of a node that is only reached by a single sequence can be computed without
needing to expand the sequence. In this case, the subtree weight is equivalent to the node, assuming that
the predictors in all of the nodes are identical. An explanation of why this is the case was given in Section 5.
This optimization signi cantly improves performance. When updating a tree with a sequence, we can
stop expanding the tree when we reach a node that has not been reached by other sequences. At this point,
we can just store the subsequences as described above without having to expand the sequence. Since the
data is sparse, this situation occurs in the case of almost every biological sequence.
Coping with skewed distributions
A natural problem to any probability density estimation approach is skew in the distribution of the
classes constituting the dataset. For instance, one of the sets of experiments we performed was with the
SCOP database. In these experiments, a SMT is trained to recognize the difference between a single
family (positive examples) and the remaining families (negative examples). In many of these experiments,
the negative examples signi cantly outnumber the positive examples up to a factor of a hundred to one.
The mixture algorithm tends to concentrate on the component (sparse tree) which performs well on the
training set. In the case of a skewed class distribution, the tree containing only the root node will dominate
the mixture because in cases of extreme skew as above, the root node will be accurate 99% of the time. In
some cases, this will cause the tree containing just the root node to dominate the mixture. In these cases,
the prediction of the mixture would be the same regardless of the input sequence. While the empirical
distribution of the classes re ects the true distribution, for practical purposes we would like to be able to
accurately recognize and retrieve the protein family considered, possibly at the expense of a higher false
positive rate. To do so, we experimented with three methods for attempting to compensate for skew in
class distribution: leaving out data, replicating data, and weighting data.
The  rst two approaches are straightforward and involve changing the training data to avoid the problem
of skew. The  rst approach uses only a portion of the negative data. In this case, we keep the amount of
positive data  xed. We take a random sample of the negative data so that the numbers of positive and
negative examples are equal. In extreme cases, we are using only 1% of the negative data. The second
approach was to replicate the positive data in order to have the same amount of positive and negative data.
In this approach, we keep the amount of negative data  xed. We replicate the positive data until we have
the same amount of total data. In extreme cases, we may have close to hundred copies of the positive data
in the training set.
The third approach involves changing the way the predictions are calculated. In this approach, each
output symbol has a certain weight corresponding to its ratio in the data. In the case of extreme skew, the
positive example will have close to one hundred times the weight that the negative examples have. The
node predictors use this weight to update the counts during training. In this case, after training, the root
node will contain the same amount of probability mass for each output symbol. The weighted approach
is the most ef cient and does not dispose of examples and thus was used in our experiments. However,
in the cases of extreme skew, even weighting the examples did not completely compensate for the skew,
and performance decreases signi cantly as shown below. This is a known problem in decision theoretic
settings, and further investigation of it is beyond the scope of this paper.
7. METHODOLOGY
We use SMTs to perform two sets of experiments. The  rst set of experiments is using the Pfam database,
comparing our results to the results of Bejerano and Yona over the same data. The second set of
PROTEIN FAMILY CLASSIFICATION
Time-Space-Performance Tradeoffs for the SMT Family Model Trained on the
ABC Transporters Family Which Contained a Total of 330 Sequencesa
aTime is measured in seconds and space is measured in megabytes. The normal and ef cient columns refer to the
use of the ef cient sequence-based data structures. Because of memory limitations, without using the ef cient data
structures, many of the models with high values of the parameter values were impossible to compute and compares the results to state-of-the-art
methods in protein classi cation.
For the  rst set of experiments, our methodology draws from similar experiments conducted by Bejerano
and Yona , in which PSTs were applied to the problem of protein family classi cation in the Pfam
database. We employed two types of protein classi ers based on SMTs and evaluated them by comparing
them to the published results of Bejerano and Yona . The  rst approach builds an SMT model for
each protein family where wild-cards are incorporated in the model. We refer to these models as SMT
prediction models. The second model is a single SMT-based classi er trained over the entire database that
maps sequences to protein family names. We refer to the second model as the SMT classi er model.
In the second set of experiments, our methodologydraws from similar experiments conducted by Jaakkola
et al. , in which a support vector machine was trained over the SCOP database. We compare the
results of our experiments to the results of other state-of-the-art protein homology methods using the same
In our experiments, we did not perform any tuning of the parameters in order to prevent bias of our
results to the speci c data. However, in practice, a system for protein classi cation could improve its
performance by tuning parameters to the speci cs of the training data, which is a rather simple task when
using trained SMTs.
We also performed a set of experiments to test the ef ciency of our implementation method (see Table 2).
We performed experiments over one protein family to examine the time-space-performance tradeoffs with
various restrictions on the topology of the sparse prediction trees (Dmax, Ámax). We also examined the
time-space tradeoffs using the ef cient data structures.
8. Pfam EXPERIMENTS
The data examined comes from the Pfam database. We perform our experiments over two versions of
the database. To compare our results to the Bejerano and Yona method, we use the release version
1.0. The data consists of single-domain protein sequences classi ed into 175 protein families. We use the
SPROT33 database and label each protein into the family according to its Pfam labeling. Pfam identi es
a number of domains that are present in the sequences. Some proteins sequences have multiple domains.
ESKIN ET AL.
There are a total of 52,205 proteins of which 15,610 are classi ed into families. There are a total of
18,531,384 residues in the data.
The sequences for each family were split into training and test data with a ratio of 4:1. For example,
the 7 transmembrane receptor family contains a total of 530 domains spread over 515 protein sequences.
The training set contains 412 of the sequences and the test set contains 106 sequences. The 103 sequences
of the training set give 158,623 subsequences that are used to train the model.
Building SMT prediction models
A sliding window of size 11 was used over each sequence to obtain a set of subsequences of size 11,
x1; : : : ; x11. Using sparse Markov transducers, we built a model that predicted the middle symbol x6 using
the neighboring symbols. The conditional sequence interlaces the  ve next symbols with the  ve previous
symbols. Speci cally, in each training example for the sequence, the output symbol is x6 and the input
symbols are x5x7x4x8x3x9x2x10x1x11.
A model for each family is built by training over all of the training examples obtained using this method
from the protein sequences in the family. The parameters used for building the SMT prediction model are
Dmax D 7 and Ámax D 1. Note these parameters allow up to four wildcards in any sequence of length 7. For
evaluating how much the incorporation of wildcards improves performance, we also build SMT prediction
models with Dmax D 7 and Ámax D 0. We also performed experiments with Ámax D 2; however, the results
were not signi cantly different from when Ámax D 1.
Classi cation of a sequence using a SMT prediction model
We use the family models to compute the likelihood of an unknown sequence  tting into the protein
family. First, we convert the amino acids in the sequences into training examples by the method above. The
SMT then computes the probability for each training example. We then compute the length-normalized
sum of log probabilities for the sequence by dividing the sum by the number of residues in a sequence.
This is the likelihood for the sequence to  t into the protein family.
A sequence is classi ed into a family by computing the likelihood of the  t for the protein family. If
the likelihood is above a threshold, then the sequence is classi ed into the family.
Building the SMT classi er model
The second model we use to classify protein families estimates the probability over protein families
given a sequence of amino acids. This model is motivated by biological considerations. Since the protein
families are characterized by similar short sequences (motifs) we can map these sequences directly to the
protein family that they originated in. This type of model has been proposed for HMMs (Krogh et al.,
Each training example for the SMT Classi er model contains an input sequence which is an amino acid
sequence from a protein family and an output symbol which is the protein family name. For example, the
3-hydroxyacyl-CoA dehydrogenase family contains in one of the proteins a subsequence VAVIGSGT. The
training example for the SMT would be the sequence of amino acids (VAVIGSGT) as the input sequence
and the name of the protein as the output symbol (3-hydroxyacyl-CoA dehydrogenase).
The training set for the SMT classi er model is the collection of the training sets for each family in the
entire Pfam database. We use a sliding window of 10 amino acids, x1; : : : ; x10. In the training example,
the output symbol is the name of the protein family. The sequence of input symbols is the 10 amino acids
x1; : : : ; x10. Intuitively, this model maps a sequence in a protein family to the name of the family from
which the sequence originated. The parameters used for building the model are Dmax D 5 and Ámax D 1.
It took several minutes to train a mixture of sparse prediction trees, and the resulting model occupied
300 megabytes of memory. The reason for the shorter subsequences than those used for the generative
models is because the number of total examples when training the discriminative classi er is the total
number of subsequences in each training set for each family in the Pfam database. The number of total
subsequences in the discriminative training set is 4,770,636 as opposed to 349,634 examples in the largest
generative training set, which is for the family Protein kinase.
PROTEIN FAMILY CLASSIFICATION
We use the weighted output symbol approach for building our discriminative classi ers. This is because
some families have many more instances than other families. This causes the prediction to tend to be biased
toward the dominant families. We weigh each instance such that the total sum of the weighted counts at
the root node are the same for each family.
Classi cation of a sequence using an SMT classi er
A protein sequence is classi ed into a protein family using the complete Pfam model as follows. We
use a sliding window of 10 amino acids to compute the set of substrings of the sequence. Each position
of the sequence gives us a probability over the 175 families measuring how likely it is that the substring
originated from each family. To classify each sequence into a family, we compute for each family the lengthnormalized sum of the log likelihood of subsequences  tting into the family. This score corresponds to
assuming that all subsequences are independent and the prior probability over families is uniform. Our
second assumption is consistent with the way we weigh the output symbols to have the same weighted
count in the root node. If we were not reweighing our training examples, we would have to take into
account the relative frequencies of the output symbols when applying Bayes’ rule to make our prediction
over the entire sequence.
Pfam results
We compare the performance of the two models examined to the published results for PSTs . We train the models over the training set, and we evaluate performance of the model
in classifying proteins on the entire database. To compare with published results, we use the equivalence
score measure, which is the number of sequences missed when the threshold is set so that the number of
false negatives is equal to the number of false positives . We then compute the percentage
of sequences from the family recovered by the model and compare this to published results .
We evaluate the performance of each model on each protein family separately. We use the model to
attempt to distinguish between protein sequences belonging to a family and protein sequences belonging
to all other families. Table 3 gives the equivalence scores for the Pfam database version 1.0 and compares
them to previously published results. Figure 7 shows scatterplots of the SMT methods versus previously
published results.
We compute a two-tailed signed rank test to compare the classi ers. The two-tailed
signed rank test assigns a p-value to the null hypothesis that the means of the equivalence score distributions
of the two classi ers are not equal. As clearly shown in Table 3, both SMT models outperform the PST
models. The best performing model is the SMT Classi er, followed by the SMT Prediction model, followed
by the PST Prediction model. The signed rank test p-values for the signi cance between the classi ers are
all < 1%. One explanation as to why the SMT Classi er model performed better than the SMT Prediction
model is that it is a discriminative model instead of a purely generative model.
Results of Pfam Protein Classi cation Using SMTsa
prediction
equivalence
equivalence
equivalence
equivalence
ATP-synt_A
ATP-synt_C
(continued)
ESKIN ET AL.
(Continued)
prediction
equivalence
equivalence
equivalence
equivalence
ATP-synt_ab
COesterase
Cys-protease
DAG_PE-bind
DNA_methylase
E1-E2_ATPase
Kunitz_BPTI
Pribosyltran
RuBisCO_large
RuBisCO_small
STphosphatase
Y_phosphatase
alpha-amylase
(continued)
PROTEIN FAMILY CLASSIFICATION
(Continued)
prediction
equivalence
equivalence
equivalence
equivalence
beta-lactamase
cNMP_binding
copper-bind
cytochrome_b_C
cytochrome_b_N
cytochrome_c
 brinogen_C
hormone_rec
interferon
ketoacyl-synt
laminin_EGF
ldl_recept_a
ldl_recept_b
(continued)
ESKIN ET AL.
(Continued)
prediction
equivalence
equivalence
equivalence
equivalence
lectin_legA
lectin_legB
myosin_head
oxidored_fad
oxidored_molyb
oxidored_nitro
peroxidase
pro_isomerase
response_reg
tRNA-synt_1
tRNA-synt_2
thyroglobulin_1
(continued)
PROTEIN FAMILY CLASSIFICATION
(Continued)
prediction
equivalence
equivalence
equivalence
equivalence
zn-protease
zona_pellucida
aThe equivalence scores are shown for each model for the 170 families with more than 10 sequences in the database.
The parameters used to build the models were Dmax D 7, Ámax D 1 for the SMT prediction models and Dmax D 5,
Ámax D 1 for the SMT classi er model. To measure the improvement from incorporating wildcards, we also build
SMT prediction models with Ámax D 0. The two tailed signed rank test assigns a p-value to the null hypothesis that
the means of the equivalence score distributions of the two classi ers are not equal. The best performing model is
the SMT Classi er, followed by the SMT Prediction model followed by the PST Prediction model. The signed rank
test p-values for the signi cance between the classi ers are all < 1%.
Scatterplots of equivalence scores from three models of protein classi cation: (a) SMT Classi er versus
PST, (b) SMT Classi er versus SMT Generative, (c) SMT Generative versus PST.
ESKIN ET AL.
9. SCOP EXPERIMENTS
The second set of experiments were performed over the SCOP database. We used the experimental setup
described by Jaakkola et al. and used the data sets obtained from the website www.cse.ucsc.edu/
research/compbio/discriminative/. We give a brief overview of the data sets below, but more details are
available at the web site above.
Performance of SMT Compared to BLAST, HMMER, and
Fisher Kernel on the SCOP Databasea
Fisher-Kernel
aThe table is sorted with respect to the ration of positive to negative examples in the training set. As expected, for extremely
skewed ratios, the SMT performance degrades signi cantly. The training family is used to seed the HMM which constructs the
training set for the experiments. See Jaakkola et al. for a complete discussion.
PROTEIN FAMILY CLASSIFICATION
The data is from the SCOP version 1.37 PDB90 domain database. All SCOP families that contain at
least 5 PDB90 sequences and at least 10 PDB90 sequences in the other families in their superfamily
were used. This gave 33 test families from 16 superfamilies. Each experiment was created by training
on one family in a superfamily and testing the prediction on the remaining sequences in the superfamily.
The negative examples are most of the sequences outside of the superfamily. Some sequences outside the
superfamily are omitted because of complications described by Jaakkola et al. . This gives a total of
160 different training and test sets obtained from the SCOP database. We compare our performance over
these 160 tests to other methods on the same data.
Discriminative models over SCOP
In order to build training and test sets for the SMTs, we use an output symbol alphabet of two symbols,
6out D fpositive; negativeg. For each of the 160 experiments, we use a sliding window to extract
subsequences from the positive and negative training and testing sequences. We label each subsequence
positive or negative. We build a SMT for each experiment using the training sequences. We then use the
SMT to predict each sequence in the test set by computing the normalized sum of log likelihood of the
sequence being in the superfamily (a positive example). This gives a score for each sequence in the test
set. We repeat this process for each of the 160 experiments from the SCOP database.
SCOP results
We compare the results of the SMT method against several state-of-the-art protein-classifying methods.
We compare against BLAST , HMMER , and Fisher kernel based
methods . The best performing of these comparison methods is the Fisher kernel
based method which is a Support Vector Machine (SVM) approach that uses a generative hidden Markov
model as a kernel.
For each of the tests, we report ROC50 scores for SMTs and the comparision methods. The ROC50 score
 is used to evaluate the performance of each method. The ROC50 score is
the normalized area under the curve that plots true positives versus false positives, up to 50 false positives.
As expected, the performance of the SMT based method degrades with the increase in skew of the data
set. We compute the results after applying the three different strategies presented above to handle skew.
Table 4 shows the results for the  rst 40 of the experiments. The table was sorted with decreasing skew
to show the effect of skew. We can see that in data sets with low skew, the method performs comparably
or even better in some cases to the state-of-the-art methods. However, with higher skew, the performance
drops signi cantly. Note, however, that this skew is an artifact of the speci c set of experiments to evaluate
these protein homology methods. In practice, the SMT classi er model would be used, which has many
classes which prevents the possibility of skew in the data.
In general, SMTs perform worse than the Fisher kernel-based method, but in many cases they perform
better than BLAST or HMMER. Note, however, that in some of the “easy” data sets, where HMMER
performs perfectly, SMTs do not perform as well as HMMER. This is because HMMER is speci cally
optimized for these kinds of data sets.
10. EFFICIENCY EXPERIMENTS
We also examined the effect of different parameters on the performance of the model. We examined
one of the larger families, ABC transporters, containing 330 sequences. Table 2 shows performance of
the SMT family model for classifying elements into the ABC transporters family as well as the time and
space cost of training the model using the two data structures with various settings of the parameters. The
ef cient data structures allow models with larger parameter values to be computed.
11. DISCUSSION
We have presented two methods for protein classi cation using sparse Markov transducers (SMTs). The
sparse Markov transducers are a generalization of probabilistic suf x trees. The motivation for the sparse
ESKIN ET AL.
Markov transducers is the presence of common short sequences in protein families. Since substitutions
of amino acids are very common in proteins, the models perform more effectively if we model common
short subsequences that contain wild-cards. However, it is not clear where to place the wild-cards in the
subsequences. The optimal placement of the wild-cards within an amino acid sequence depends on the
context or neighboring amino acids. We use a mixture technique to learn from the data which placements
of wild-cards perform best. We present two models that incorporate SMTs to build a protein classi er.
Both of the models out-perform the baseline PST model that does not use wild-cards.
However, the inclusion of wild-cards requires a signi cant increase in the memory usage of the model.
These models can quickly exhaust the available memory. We present ef cient data structures that allow for
computation of models with wild-cards that otherwise would not be possible. As can be seen in Table 2,
without ef cient data structures, it would be impossible to compute the models for any but the smallest
parameter settings.
A problem with probability density estimators is the problem of skew in the data set. This is a problem
that is inherent to probability density estimators and is dif cult to fully address. Therefore, over these data
sets, it cannot be expected that SMTs perform better than SVM-based methods. However, in practice, SMTbased methods have some advantages over SVM-based methods. Although a single family is relatively
small compared to all known proteins, the problem of protein classi cation is a multiclass problem. In this
case, the output symbol alphabet is relatively large, and even the largest protein families comprise only a
small part of the overall data. Thus, in the actual application of protein family classi cation, the problem
of skew does not appear. In addition, SMTs have the advantage of being able to build a single model able
to discriminate between all protein families. An SVM can distinguish only between two classes so it must
build a separate model for each protein family. In the case when there are more than 2,000 protein families
this becomes dif cult.
The methods presented rely on very little biological intuition. Future work will involve incorporating
biological information into the model, such as Dirichlet mixture priors, which can incorporate information
about the amino acids .
A. Sparse Markov chains as sparse prediction trees
We now show that any sparse Markov transducer can be represented by a prediction tree. The paths
of the tree correspond to the conditioning events on the inputs of the sparse Markov transducer. Each
sparse Markov transducer of the form given by Equation (2) can be represented with a sparse prediction
tree as shown in Fig. 8. Notice that in this tree each of the branches marked Án attain the same value
of n at each level (depth) of the tree. Indeed, all trees that represent conditional probabilities of the form
of Equation (2) have this property. In fact, sparse prediction trees can represent a slightly larger class of
probability distributions, one that depends on the speci c context of the inputs as shown in Fig. 1.
A sparse prediction tree derived from a sparse Markov chain.
PROTEIN FAMILY CLASSIFICATION
More formally, a  xed order sparse Markov transducer de ned by a probability distribution of the form
P.YtjÁn1Xt¡n1Án2Xt¡n1¡n2¡1 : : : Ánj Xt¡.L¡1//
can be represented by a sparse prediction tree constructed as follows. We start with just the root node. We
add a branch with Án1 and then from this branch a node for every element in 6in. Then, from each of
these nodes, we add a branch with Án2 and then another node for every element in 6in. We repeat this
process, and in the last step we add a branch with Ánj and a node for every element in 6in. We make
these nodes leaf nodes. For each leaf node, u, we associate the probability distribution P.Yju/ determined
by the sparse Markov transducer. The probabilistic distribution induced by this tree is equivalent to the
probabilistic distribution of the originating sparse Markov transducer.
B. Proof of Claim 1
The claim states that for all t ¸ 1 and all nodes u of a template sparse prediction tree the following
wt.u/ D P u
Á .0/wt.u/ C
wt.uÁi¡1¾/:
First, recall the following de nitions. For a tree T , LT and NT , respectively, denote the set of leaves
and nodes of T . By Tu we denote the subtree rooted at u and similarly NTu and LTu were de ned to be
the set of nodes and leaf nodes of the the subtree Tu.
We now decompose the summation over all subtrees rooted at u with respect to the value of each branch
marked Ái. If the i is 0, there is a single tree with only one leaf node which consists of single node u. In
this case, the subtree weight is
wt.v/ D P u
Á .0/wt.u/:
Let us assume that the Á value of the node u is i > 0. In this case, a subtree Tu rooted at u is composed
of the node u and a set of subtrees branching off uÁi¡1¾, for each ¾ 2 6in. In accordance with our
de nition, these subtrees are denoted TuÁi¡1¾ . The set of leaf nodes of the subtree rooted at u will be the
union of the leaf nodes of these subtrees. Similarly, the set of nodes of Tu will be the union of the set of
nodes of these subtrees and the node u itself. Using this fact, we can represent the weight of each Tu as
Let k D j6inj. Then, using the above equation
wt.u/ D PÁ.0/wt.u/ C
D PÁ.0/wt.u/ C
where we changed the order of summation to get the second inequality. If we now apply the claim
recursively to each of the subtrees TuÁi¡1¾ , we get the desired equality,
wt.u/ D P u
Á .0/wt.u/ C
wt.uÁi¡1¾/:
ESKIN ET AL.
ACKNOWLEDGMENTS
Thanks to Gill Bejerano and Nir Friedman for helpful discussions. The authors thank Mark Diekhans
and David Haussler for providing detailed results from their previous experiments. WSN is supported
by an Award in Bioinformatics from the PhRMA Foundation and by National Science Foundation grants
DBI-0078523 and ISI-0093302. This research of YS was partially funded by the Israeli Science Foundation
grant 032.9627.