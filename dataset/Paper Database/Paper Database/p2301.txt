Eﬃcient Multi-Scale 3D CNN with fully connected CRF
for Accurate Brain Lesion Segmentation
Konstantinos Kamnitsasa, Christian Lediga, Virginia F.J. Newcombeb,c,
Joanna P. Simpsonb, Andrew D. Kaneb, David K. Menonb,c,
Daniel Rueckerta, Ben Glockera
aBiomedical Image Analysis Group, Imperial College London, UK
bUniversity Division of Anaesthesia, Department of Medicine, Cambridge University, UK
cWolfson Brain Imaging Centre, Cambridge University, UK
We propose a dual pathway, 11-layers deep, three-dimensional Convolutional
Neural Network for the challenging task of brain lesion segmentation. The
devised architecture is the result of an in-depth analysis of the limitations of
current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an eﬃcient
and eﬀective dense training scheme which joins the processing of adjacent
image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze
the development of deeper, thus more discriminative 3D CNNs. In order to
incorporate both local and larger contextual information, we employ a dual
pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network’s soft segmentation, we use a
3D fully connected Conditional Random Field which eﬀectively removes false
positives. Our pipeline is extensively evaluated on three challenging tasks of
lesion segmentation in multi-channel MRI patient data with traumatic brain
injuries, brain tumors, and ischemic stroke. We improve on the state-of-theart for all three applications, with top ranking performance on the public
benchmarks BRATS 2015 and ISLES 2015. Our method is computationally eﬃcient, which allows its adoption in a variety of research and clinical
settings. The source code of our implementation is made publicly available.
3D Convolutional Neural Network, Fully Connected CRF,
Segmentation, Brain Lesions, Deep Learning
January 10, 2017
 
1. Introduction
Segmentation and the subsequent quantitative assessment of lesions in
medical images provide valuable information for the analysis of neuropathologies and are important for planning of treatment strategies, monitoring of
disease progression and prediction of patient outcome. For a better understanding of the pathophysiology of diseases, quantitative imaging can reveal
clues about the disease characteristics and eﬀects on particular anatomical
structures. For example, the associations of diﬀerent lesion types, their spatial distribution and extent with acute and chronic sequelae after traumatic
brain injury (TBI) are still poorly understood ). However,
there is growing evidence that quantiﬁcation of lesion burden may add insight into the functional outcome of patients ; Moen et al.
 ). Additionally, exact locations of injuries relate to particular deﬁcits
depending on the brain structure that is aﬀected ;
Warner et al. ; Sharp et al. ). This is in line with estimates that
functional deﬁcits caused by stroke are associated with the extent of damage
to particular parts of the brain ). Lesion burden is commonly quantiﬁed by means of volume and number of lesions, biomarkers that
have been shown to be related to cognitive deﬁcits. For example, volume of
white matter lesions (WML) correlates with cognitive decline and increased
risk of dementia ). In clinical research on multiple sclerosis (MS), lesion count and volume are used to analyse disease progression and
eﬀectiveness of pharmaceutical treatment ; Kappos
et al. ). Finally, accurate delineation of the pathology is important in
the case of brain tumors, where estimation of the relative volume of a tumor’s sub-components is required for planning radiotherapy and treatment
follow-up ).
The quantitative analysis of lesions requires accurate lesion segmentation in multi-modal, three-dimensional images which is a challenging task
for a number of reasons. The heterogeneous appearance of lesions including
the large variability in location, size, shape and frequency make it diﬃcult
to devise eﬀective segmentation rules. It is thus highly non-trivial to delineate contusions, edema and haemorrhages in TBI ),
or sub-components of brain tumors such as proliferating cells and necrotic
core ). The arguably most accurate segmentation results
can be obtained through manual delineation by a human expert which is
tedious, expensive, time-consuming, impractical in larger studies, and intro-
duces inter-observer variability. Additionally, for deciding whether a particular region is part of a lesion multiple image sequences with varying contrasts
need to be considered, and the level of expert knowledge and experience are
important factors that impact segmentation accuracy. Hence, in clinical routine often only qualitative, visual inspection, or at best crude measures like
approximate lesion volume and number of lesions are used ;
Wen et al. ). In order to capture and better understand the complexity of brain pathologies it is important to conduct large studies with many
subjects to gain the statistical power for drawing conclusions across a whole
patient population. The development of accurate, automatic segmentation
algorithms has therefore become a major research focus in medical image
computing with the potential to oﬀer objective, reproducible, and scalable
approaches to quantitative assessment of brain lesions.
Figure 1 illustrates some of the challenges that arise when devising a computational approach for the task of automatic lesion segmentation. The ﬁgure
summarizes statistics and shows examples of brain lesions in the case of TBI,
but is representative of other pathologies such as brain tumors and ischemic
stroke. Lesions can occur at multiple sites, with varying shapes and sizes,
and their image intensity proﬁles largely overlap with non-aﬀected, healthy
parts of the brain or lesions which are not in the focus of interest. For example, stroke and MS lesions have a similar hyper-intense appearance in FLAIR
sequences as other WMLs ; Schmidt et al. ). It is
generally diﬃcult to derive statistical prior information about lesion shape
and appearance. On the other hand, in some applications there is an expectation on the spatial conﬁguration of segmentation labels, for example there
is a hierarchical layout of sub-components in brain tumors. Ideally, a computational approach is able to adjust itself to application speciﬁc characteristics
by learning from a set of a few example images.
1.1. Related Work
A multitude of automatic lesion segmentation methods have been proposed over the last decade, and several main categories of approaches can
be identiﬁed. One group of methods poses the lesion segmentation task as
an abnormality detection problem, for example by employing image registration. The early work of Prastawa et al. and more recent ones by
Schmidt et al. and Doyle et al. align the pathological scan to a
healthy atlas and lesions are detected based on deviations in tissue appearance between the patient and the atlas image. Lesions, however, may cause
Figure 1: Heterogeneous appearance of TBI lesions poses challenges in devising discriminative models. Lesion size varies signiﬁcantly with both large,
focal and small, diﬀused lesions (a,b). Alignment of manual lesion segmentations reveals the wide spatial distribution of lesions in (c,d) with some areas
being more likely than others. (e) shows the average of the normalized intensity histograms of diﬀerent MR channels over all the TBI cases in our
database, for healthy (green) and injured (red) tissue. One can observe a
large overlap between the distributions of healthy and non-healthy tissue.
large structural deformations that may lead to incorrect segmentation due
to incorrect registration. Gooya et al. ; Parisot et al. alleviate
this problem by jointly solving the segmentation and registration tasks. Liu
et al. showed that registration together with a low-rank decomposition gives as a by-product the abnormal structures in the sparse components,
although, this may not be precise enough for detection of small lesions. Abnormality detection has also been proposed within image synthesis works.
Representative approaches are those of Weiss et al. using dictionary
learning and Ye et al. using a patch-based approach. The idea is to
synthesize pseudo-healthy images that when compared to the patient scan
allow to highlight abnormal regions. In this context, Cardoso et al. 
present a generative model for image synthesis that yields a probabilistic seg-
mentation of abnormalities. Another unsupervised technique is proposed by
Erihov et al. , a saliency-based method that exploits brain asymmetry
in pathological cases. A common advantage of the above methods is that
they do not require a training dataset with corresponding manual annotations. In general, these approaches are more suitable for detecting lesions
rather than accurately segmenting them.
Some of the most successful, supervised segmentation methods for brain
lesions are based on voxel-wise classiﬁers, such as Random Forests. Representative work is that of Geremia et al. on MS lesions, employing
intensity features to capture the appearance of the region around each voxel.
Zikic et al. combine this with a generative Gaussian Mixture Model
(GMM) to obtain tissue-speciﬁc probabilistic priors ). This framework was adopted in multiple works, with representative
pipelines for brain tumors by Tustison et al. and TBI by Rao et al.
 . Both works incorporate morphological and contextual features to
better capture the heterogeneity of lesions. Rao et al. also incorporate brain structure segmentation results obtained from a multi-atlas label
propagation approach ) to provide strong tissue-class priors to the Random Forests. Tustison et al. additionally use a Markov
Random Field (MRF) to incorporate spatial regularization. MRFs are commonly used to encourage spatial continuity of the segmentation ; Mitra et al. ). Although those methods have been very
successful, it appears that their modeling capabilities still have signiﬁcant
limitations. This is conﬁrmed by the results of the most recent challenges 1,
and also by our own experience and experimentation with such approaches.
At the same time, deep learning techniques have emerged as a powerful alternative for supervised learning with great model capacity and the ability to
learn highly discriminative features for the task at hand. These features often
outperform hand-crafted and pre-deﬁned feature sets. In particular, Convolutional Neural Networks (CNNs) ; Krizhevsky et al.
 ) have been applied with promising results on a variety of biomedical
imaging problems. Ciresan et al. presented the ﬁrst GPU implementation of a two-dimensional CNN for the segmentation of neural membranes.
From the CNN based work that followed, related to our approach are the
methods of Zikic et al. ; Havaei et al. ; Pereira et al. ,
1links: www.isles-challenge.org
with the latter being the best performing automatic approach in the BRATS
2015 challenge ). These methods are based on 2D CNNs,
which have been used extensively in computer vision applications on natural
images. Here, the segmentation of a 3D brain scan is achieved by processing
each 2D slice independently, which is arguably a non-optimal use of the volumetric medical image data. Despite the simplicity in the architecture, the
promising results obtained by these methods indicate the potential of CNNs.
Fully 3D CNNs come with an increased number of parameters and signiﬁcant memory and computational requirements. Previous work discusses
problems and apparent limitations when employing a 3D CNN on medical
imaging data ; Li et al. ; Roth et al. ).
To incorporate 3D contextual information, multiple works used 2D CNNs
on three orthogonal 2D patches ; Roth et al. ;
Lyksborg et al. ). In their work for structural brain segmentation, Brebisson and Montana extracted large 2D patches from multiple scales
of the image and combined them with small single-scale 3D patches, in order
to avoid the memory requirements of fully 3D networks.
One of the reasons that discouraged the use of 3D CNNs is the slow inference due to the computationally expensive 3D convolutions. In contrast
to the 2D/3D hybrid variants ; Brebisson and Montana
 ), 3D CNNs can fully exploit dense-inference ; Sermanet et al. ), a technique that greatly decreases inference times and
which we will further discuss in section 2.1. By employing dense-inference
with 3D CNNs, Brosch et al. and Urban et al. reported computation times of a few seconds and approximately a minute respectively for
the processing of a single brain scan. Even though the size of their developed networks was limited, a factor that is directly related to a network’s
representational power, their results on MS and brain tumor segmentation
respectively were very promising.
Performance of CNNs is signiﬁcantly inﬂuenced by the strategy for extracting training samples.
A commonly adopted approach is training on
image patches that are equally sampled from each class. This, however, biases the classiﬁer towards rare classes and may result in over-segmentation.
To counter this, Cire¸san et al. proposes to train a second CNN on
samples with a class distribution close to the real one, but oversample pixels
that were incorrectly classiﬁed in the ﬁrst stage. A secondary training stage
was also suggested by Havaei et al. , who retrain the classiﬁcation layer
on patches extracted uniformly from the image. In practice, two stage train-
ing schemes can be prone to overﬁtting and sensitive to the state of the ﬁrst
classiﬁer. Alternatively, dense training ) has been used to
train a network on multiple or all voxels of a single image per optimisation
step ; Brosch et al. ; Ronneberger et al. ).
This can introduce severe class imbalance, similarly to uniform sampling.
Weighted cost functions have been proposed in the two latter works to alleviate this problem. Brosch et al. manually adjusted the sensitivity of
the network, but the method can become diﬃcult to calibrate for multi-class
problems. Ronneberger et al. ﬁrst balance the cost from each class,
which has an eﬀect similar to equal sampling, and further adjust it for the
speciﬁc task by estimating the diﬃculty of segmenting each pixel.
1.2. Contributions
We present a fully automatic approach for lesion segmentation in multimodal brain MRI based on an 11-layers deep, multi-scale, 3D CNN with the
following main contributions:
1. We propose an eﬃcient hybrid training scheme, utilizing dense training ) on sampled image segments, and analyze its
behaviour in adapting to class imbalance of the segmentation problem
2. We analyze in depth the development of deeper, thus more discriminative, yet computationally eﬃcient 3D CNNs. We exploit the utilization
of small kernels, a design approach previously found beneﬁcial in 2D
networks ) that impacts 3D CNNs
even more, and present adopted solutions that enable training deeper
3. We employ parallel convolutional pathways for multi-scale processing, a
solution to eﬃciently incorporate both local and contextual information
which greatly improves segmentation results.
4. We demonstrate the generalization capabilities of our system, which
without signiﬁcant modiﬁcations outperforms the state-of-the-art on a
variety of challenging segmentation tasks, with top ranking results in
two MICCAI challenges, ISLES and BRATS.
Furthermore, a detailed analysis of the network reveals valuable insights
into the powerful black box of deep learning with CNNs. For example, we
have found that our network is capable of learning very complex, high level
features that separate gray matter (GM), cerebrospinal ﬂuid (CSF) and other
anatomical structures to identify the image regions corresponding to lesions.
Additionally, we have extended the fully-connected Conditional Random
Field (CRF) model by Kr¨ahenb¨uhl and Koltun to 3D which we use
for ﬁnal post-processing of the CNN’s soft segmentation maps. This CRF
overcomes limitations of previous models as it can handle arbitrarily large
neighborhoods while preserving fast inference times.
To the best of our
knowledge, this is the ﬁrst use of a fully connected CRF on medical data.
To facilitate further research and encourage other researchers to build
upon our results, the source code of our lesion segmentation method including
the CNN and the 3D fully connected CRF is made publicly available on
 
Our proposed lesion segmentation method consists of two main components, a 3D CNN that produces highly accurate, soft segmentation maps,
and a fully connected 3D CRF that imposes regularization constraints on
the CNN output and produces the ﬁnal hard segmentation labels. The main
contributions of our work are within the CNN component which we describe
ﬁrst in the following.
2.1. 3D CNNs for Dense Segmentation – Setting the Baseline
CNNs produce estimates for the voxel-wise segmentation labels by classifying each voxel in an image independently taking the neighborhood, i.e.
local and contextual image information, into account. This is achieved by
sequential convolutions of the input with multiple ﬁlters at the cascaded layers of the network. Each layer l ∈[1, L] consists of Cl feature maps (FMs),
also referred to as channels. Every FM is a group of neurons that detects
a particular pattern, i.e. a feature, in the channels of the previous layer.
The pattern is deﬁned by the kernel weights associated with the FM. If the
neurons of the m-th FM in the l-th layer are arranged in a 3D grid, their
activations constitute the image ym
l = f(PCl−1
l ). This is the
result of convolving each of the previous layer’s channels with a 3-dimensional
kernel km,n
, adding a learned bias bm
and applying a non-linearity f. Each
kernel is a matrix of learned hidden weights Wm,n
. The images yn
to the ﬁrst layer, correspond to the channels of the original input image, for
instance a multi-sequence 3D MRI scan of the brain. The concatenation of
the kernels kl = (km,1
, ..., km,Cl−1
) can be viewed as a 4-dimensional kernel
convolving the concatenated channels yl−1 = (y1
l−1, ..., yCl−1
l−1 ), which then intuitively expresses that the neurons of higher layers combine the patterns
extracted in previous layers, which results in the detection of increasingly
more complex patterns. The activations of the neurons in the last layer L
correspond to particular segmentation class labels, hence this layer is also
referred to as the classiﬁcation layer. The neurons are thus grouped in CL
FMs, one for each of the segmentation classes.
Their activations are fed
into a position-wise softmax function that produces the predicted posterior
pc(x) = exp(yc
L(x))/ PCL
c=1 exp(yc
L(x)) for each class c, which form soft segmentation maps with (pseudo-)probabilities. yc
L(x) is the activation of the
c-th classiﬁcation FM at position x ∈N3. This baseline network is depicted
in Fig. 2.
Figure 2: Our baseline CNN consists of four layers with 53 kernels for feature
extraction, leading to a receptive ﬁeld of size 173. The classiﬁcation layer is
implemented as convolutional with 13 kernels, which enables eﬃcient denseinference. When the network segments an input it predicts multiple voxels
simultaneously, one for each shift of its receptive ﬁeld over the input. Number
of FMs and their size depicted as (Number × Size).
The neighborhood of voxels in the input that inﬂuence the activation of
a neuron is its receptive ﬁeld. Its size, ϕl, increases at each subsequent layer
l and is given by the 3-dimensional vector:
= ϕ{x,y,z}
+ (κ{x,y,z}
−1)τ {x,y,z}
where κl, τl ∈N3 are vectors expressing the size of the kernels and stride of
the receptive ﬁeld at layer l. τl is given by the product of the strides of kernels
in layers preceding l. In this work only unary strides are used, as larger
strides downsample the FMs ), which is unwanted
behaviour for accurate segmentation. Thus in our system τl = (1, 1, 1). The
receptive ﬁeld of a neuron in the classiﬁcation layer corresponds to the image
patch that inﬂuences the prediction for its central voxel. This is called the
CNN’s receptive ﬁeld, with ϕCNN = ϕL.
If input of size δin is provided, the dimensions of the FMs in layer l are
= ⌊(δ{x,y,z}
)/τ {x,y,z}
In the common patch-wise classiﬁcation setting, an input patch of size
δin = ϕCNN is provided and the network outputs a single prediction for its
central voxel. In this case the classiﬁcation layer consists of FMs with size
13. Networks that are implemented as fully-convolutionals are capable of
dense-inference, which is performed when input of size greater than ϕCNN
is provided ). In this case, the dimensions of FMs increase according to Eq. (2). This includes the classiﬁcation FMs which then
output multiple predictions simultaneously, one for each stride of the CNN’s
receptive ﬁeld on the input (Fig. 2). All predictions are equally trustworthy,
as long as the receptive ﬁeld is fully contained within the input and captures
only original content, i.e. no padding is used. This strategy signiﬁcantly reduces the computational costs and memory loads since the otherwise repeated
computations of convolutions on the same voxels in overlapping patches are
avoided. Optimal performance is achieved if the whole image is scanned in
one forward pass. If GPU memory constraints do not allow it, such as in the
case of large 3D networks where a large number of FMs need to be cached, the
volume is tiled in multiple image-segments, which are larger than individual
patches, but small enough to ﬁt into memory.
Before analyzing how we exploit the above dense-inference technique for
training, which is the ﬁrst main contribution of our work, we present the
commonly used setting in which CNNs are trained patch-by-patch. Random
patches of size ϕCNN are extracted from the training images. A batch is
formed out of B of these samples, which is then processed by the network for
one training iteration of Stochastic Gradient Descent (SGD). This step aims
to alter the network’s parameters Θ, such as weights and biases, in order
to maximize the log likelihood of the data or, equally, minimize the Cross
Entropy via the cost function:
J(Θ; Ii, ci) = −1
 P(Y = ci|Ii, Θ)
log(pci) ,
where the pair (Ii, ci), ∀i ∈[1, B] is the i-th patch in the batch and the true
label of its central voxel, while the scalar value pci is the predicted posterior for class ci. Regularization terms were omitted for simplicity. Multiple
sequential optimization steps over diﬀerent batches gradually lead to convergence.
2.2. Dense Training on Image Segments and Class Balance
Larger training batch sizes B are preferred as they approximate the overall
data more accurately and lead to better estimation of the true gradient by
SGD. However, the memory requirement and computation time increase with
the batch size. This limitation is especially relevant for 3D CNNs, where only
a few dozens of patches can be processed within reasonable time on modern
To overcome this problem, we devise a training strategy that exploits the
dense inference technique on image segments. Following from Eq. (2), if an
image segment of size greater than ϕCNN is given as input to our network,
the output is a posterior probability for multiple voxels V = Q
i={x,y,z} δ(i)
If the training batches are formed of B segments extracted from the training
images, the cost function (3) in the case of dense-training becomes:
JD(Θ; Is, cs) = −
log(pcvs(xv)) ,
where Is and cs are the s-th segment of the batch and the true labels of
its V predicted voxels respectively. cv
s is the true label of the v-th voxel, xv
the corresponding position in the classiﬁcation FMs and pcvs the output of the
softmax function. The eﬀective batch size is increased by a factor of V without a corresponding increase in computational and memory requirements, as
earlier discussed in Sec. 2.1. Notice that this is a hybrid scheme between
the commonly used training on individual patches and the dense training
scheme on a whole image ), with the latter being problematic to apply for training large 3D CNNs on volumes of high resolution
due to memory limitations.
An appealing consequence of this scheme is that the sampling of input
segments provides a ﬂexible and automatic way to balance the distribution
of training samples from diﬀerent segmentation classes which is an important issue that directly impacts the segmentation accuracy. Speciﬁcally, we
build the training batches by extracting segments from the training images
with 50% probability being centred on a foreground or background voxel,
Figure 3: Consider a network with a 2D receptive ﬁeld of 32 (for illustration)
densely-applied on the depicted lesion-centred image segments of size 72 or
92. Relatively more background (green) is captured by larger segments and
around smaller lesions.
alleviating class-imbalance. Note that the predicted voxels V in a segment
do not have to be of the same class, something that occurs when a segment
is sampled from a region near class boundaries (Fig. 3). Hence, the sampling
rate of the proposed hybrid method adjusts to the true distribution of the
segmentation task’s classes. Speciﬁcally, the smaller a labelled object, the
more background voxels will be captured within segments centred on the
foreground voxel. Implicitly, this yields a balance between sensitivity and
speciﬁcity in the case of binary segmentation tasks. In multi-class problems,
the rate at which diﬀerent classes are captured within a segment centred on
foreground reﬂects the real relative distribution of the foreground classes,
while adjusting their frequency relatively to the background.
2.3. Building Deeper Networks
Deeper networks have greater discriminative power due to the additional
non-linearities and better quality of local optima ).
However, convolutions with 3D kernels are computationally expensive in comparison to the 2D variants, which hampers the addition of more layers. Additionally, 3D architectures have a larger number of trainable parameters,
with each layer adding ClCl−1
i={x,y,z} κ(i)
weights to the model. Cl is the
number of FMs in layer l and κ{x,y,z}
the size of its kernel in the respective spatial dimension. Overall this makes the network increasingly prone to
over-ﬁtting.
In order to build a deeper 3D architecture, we adopt the sole use of small
33 kernels that are faster to convolve with and contain less weights. This
design approach was previously found beneﬁcial for classiﬁcation of natural
images ) but its eﬀect is even more drastic
on 3D networks. When compared to common kernel choices of 53 ; Urban et al. ; Prasoon et al. ) and in our baseline CNN,
the smaller 33 kernels reduce the element-wise multiplications by a factor of
approximately 53/33 ≈4.6 while reducing the number of trainable parameters by the same factor. Thus deeper network variants that are implicitly
regularised and more eﬃcient can be designed by simply replacing each layer
of common architectures with more layers that use smaller kernels (Fig. 4).
Figure 4: The replacement of the depicted layer with 55 kernels (left) with
two successive layers using 33 kernels (right) introduces an additional nonlinearity without altering the CNN’s receptive ﬁeld. Additionally, the number
of weights is reduced from 200k to 86.4k and the required convolutions are
cheaper (see text). Number of FMs and their size depicted as (Number ×
However, deeper networks are more diﬃcult to train. It has been shown
that the forward (neuron activations) and backwards (gradients) propagated
signal may explode or vanish if care is not given to retain its variance
 ). This occurs because at every successive layer
l, the variance of the signal is multiplied by nin
· var(Wl), where nin
i={x,y,z} κ(i)
is the number of weights through which a neuron of layer
l is connected to its input and var(Wl) is the variance of the layer’s weights.
To better preserve the signal in the initial training stage we adopt a scheme
recently derived for ReLu-based networks by He et al. and initialize
the kernel weights of our system by sampling from the normal distribution
A phenomenon of similar nature that hinders the network’s performance is
the “internal covariate shift” ). It occurs throughout
training, because the weight updates to deeper layers result in a continuously
changing distribution of signal at higher layers, which hinders the convergence
of their weights. Speciﬁcally, at training iteration t the weight updates may
cause deviation ϵl,t to the variance of the weights. At the next iteration the
signal will be ampliﬁed by nin
l · var(Wl,t+1) = nin
l · (var(Wl,t) + ϵl,t). Thus
before inﬂuencing the signal, any deviation ϵl,t is ampliﬁed by nin
exponential in the number of dimensions. For this reason the problem affects training of 3D CNNs more severely than conventional 2D systems. For
countering it, we adopt the recently proposed Batch Normalisation (BN)
technique to all hidden layers ), which allows normalization of the FM activations at every optimization step in order to better
preserve the signal.
2.4. Multi-Scale Processing via Parallel Convolutional Pathways
The segmentation of each voxel is performed by taking into account the
contextual information that is captured by the receptive ﬁeld of the CNN
when it is centred on the voxel. The spatial context is providing important
information for being able to discriminate voxels that otherwise appear very
similar when considering only local appearance. From Eq. (1) follows that an
increase of the CNN’s receptive ﬁeld requires bigger kernels or more convolutional layers, which increases computation and memory requirements. An
alternative would be the use of pooling ), which however leads to loss of the exact position of the segmented voxel and thus can
negatively impact accuracy.
In order to incorporate both local and larger contextual information into
our 3D CNN, we add a second pathway that operates on down-sampled
images. Thus, our dual pathway 3D CNN simultaneously processes the input
image at multiple scales (Fig. 5). Higher level features such as the location
within the brain are learned in the second pathway, while the detailed local
appearance of structures is captured in the ﬁrst. As the two pathways are
decoupled in this architecture, arbitrarily large context can be processed
by the second pathway by simply adjusting the down-sampling factor FD.
The size of the pathways can be independently adjusted according to the
computational capacity and the task at hand, which may require relatively
more or less ﬁlters focused on the down-sampled context.
To preserve the capability of dense inference, spatial correspondence of the
activations in the FMs of the last convolutional layers of the two pathways,
L1 and L2, should be ensured. In networks where only unary kernel strides
are used, such as the proposed architecture, this requires that for every FD
shifts of the receptive ﬁeld ϕL1 over the normal resolution input, only one
shift is performed by ϕL2 over the down-sampled input. Hence it is required
that the dimensions of the FMs in L2 are δ{x,y,z}
= ⌈δ{x,y,z}
/FD⌉. From
Eq. (2), the size of the input to the second pathway is δ{x,y,z}
= ϕ{x,y,z}
Figure 5: Multi-scale 3D CNN with two convolutional pathways. The kernels
of the two pathways are here of size 53 (for illustration only to reduce the
number of layers in the ﬁgure). The neurons of the last layers of the two
pathways thus have receptive ﬁelds of size 173 voxels. The inputs of the two
pathways are centered at the same image location, but the second segment
is extracted from a down-sampled version of the image by a factor of 3.
The second pathway processes context in an actual area of size 513 voxels.
DeepMedic, our proposed 11-layers architecture, results by replacing each
layer of the depicted pathways with two that use 33 kernels (see Sec. 2.3).
Number of FMs and their size depicted as (Number × Size).
−1 and similar is the relation between δin1 and δL1. These establish
the relation between the required dimensions of the input segments from the
two resolutions, which can then be extracted centered on the same image
location. The FMs of L2 are up-sampled to match the dimensions of L1’s
FMs and are then concatenated together. We add two more hidden layers for
combining the multi-scale features before the ﬁnal classiﬁcation, as shown in
Fig. 5. Integration of the multi-scale parallel pathways in architectures with
non-unary strides is discussed in Appendix A.
Combining multi-scale features has been found beneﬁcial in other recent
works ; Ronneberger et al. ), in which whole 2D images are processed in the network by applying a few number of convolutions
and then down-sampling the FMs for further processing at various scales.
Our decoupled pathways allow arbitrarily large context to be provided while
avoiding the need to load large parts of the 3D volume into memory. Additionally, our architecture extracts features completely independently from
the multiple resolutions. This way, the features learned by the ﬁrst pathway
retain ﬁnest details, as they are not involved in processing low resolution
2.5. 3D Fully Connected CRF for Structured Prediction
Because neighboring voxels share substantial spatial context, the soft segmentation maps produced by the CNN tend to be smooth, even though
neighborhood dependencies are not modeled directly. However, local minima in training and noise in the input images can still result in some spurious
outputs, with small isolated regions or holes in the predictions. We employ
a fully connected CRF ) as a post-processing
step to achieve more structured predictions. As we describe below, this CRF
is capable of modeling arbitrarily large voxel-neighborhoods but is also computationally eﬃcient, making it ideal for processing 3D multi-modal medical
For an input image I and the label conﬁguration (segmentation) z, the
Gibbs energy in a CRF model is given by
ψp(zi, zj) .
The unary potential is the negative log-likelihood ψu(zi) = −logP(zi|I),
where in our case P(zi|I) is the CNN’s output for voxel i. In a fully connected
CRF, the pairwise potential is of form ψp(zi, zj) = µ(zi, zj)k(fi, fj) between
any pair of voxels, regardless of their spatial distance. The Pott’s Model is
commonly used as the label compatibility function, giving µ(zi, zj) = [zi ̸=
zj]. The corresponding energy penalty is given by the function k, which is
deﬁned over an arbitrary feature space, with fi, fj being the feature vectors of
the pair of voxels. Kr¨ahenb¨uhl and Koltun observed that if the penalty
function is deﬁned as a linear combination of Gaussian kernels, k(fi, fj) =
m=1 w(m)k(m)(fi, fj), the model lends itself for very eﬃcient inference with
mean ﬁeld approximation, after expressing message passing as convolutions
with the Gaussian kernels in the space of the feature vectors fi, fj.
We extended the work of the original authors and implemented a 3D
version of the CRF for processing multi-modal scans. We make use of two
Gaussian kernels, which operate in the feature space deﬁned by the voxel
coordinates pi,d and the intensities of the c-th modality-channel Ii,c for voxel
i. The smoothness kernel, k(1)(fi, fj) = exp
|pi,d−pj,d|2
, is deﬁned
by a diagonal covariance matrix with elements the conﬁgurable parameters
σα,d, one for each axis.
These parameters express the size and shape of
neighborhoods that homogeneous labels are encouraged.
The appearance
kernel k(2)(fi, fj) = exp
|pi,d−pj,d|2
|Ii,c−Ij,c|2
similarly. The additional parameters σγ,c can be interpreted as how strongly
to enforce homogeneous appearance in the C input channels, when voxels
in an area spatially deﬁned by σβ,d are identically labelled.
Finally, the
conﬁgurable weights w(1), w(2) deﬁne the relative strength of the two factors.
3. Analysis of Network Architecture
In this section we present a series of experiments in order to analyze the
impact of each of the main contributions and to justify the choices made
in the design of the proposed 11-layers, multi-scale 3D CNN architecture,
referred to as the DeepMedic. Starting from the CNN baseline as discussed
in Sec. 2.1, we ﬁrst explore the beneﬁt of our proposed dense training scheme
(cf. Sec. 2.2), then investigate the use of deeper models (cf. Sec. 2.3) and
then evaluate the inﬂuence of the multi-scale dual pathway (cf. Sec. 2.4).
Finally, we compare our method with corresponding 2D variants to assess
the beneﬁt of processing 3D context.
3.1. Experimental Setting
The following experiments are conducted using the TBI dataset with 61
multi-channel MRIs which is described in more detail later in Sec. 4.1. Here,
the images are randomly split into a validation and training set, with 15 and
46 images each. The same sets are used in all analyses. To monitor the
progress of segmentation accuracy during training, we extract 10k random
patches at regular intervals, with equal numbers extracted from each of the
validation images. The patches are uniformly sampled from the brain region
in order to approximate the true distribution of lesions and healthy tissue.
Full segmentation of the validation datasets is performed every ﬁve epochs
and the mean Dice similarity coeﬃcient (DSC) is determined. Details on the
conﬁguration of the networks are provided in Appendix B.
3.2. Eﬀect of Dense Training on Image Segments
We compare our proposed dense training method with two other commonly used training schemes on the 5-layers baseline CNN (see Fig. 2). The
ﬁrst common scheme trains on 173 patches extracted uniformly from the
brain region, and the second scheme samples patches equally from the lesion
Figure 6: Comparison of the commonly used methods for training on patches
uniformly sampled from the brain region (Puni) and equally sampled from
lesion and background (Peq) against our proposed scheme (S-d) on cubic
segments of side length d, also equally sampled from lesion and background.
We varied d to observe its eﬀect. From left to right: percentage of training
samples extracted from the lesion class, mean accuracy, sensitivity, speciﬁcity
calculated on uniformly sampled validation patches and, ﬁnally, the mean
DSC of the segmentation of the validation datasets. The progress throughout
training is plotted. Because lesions are small, Puni achieves very high voxelwise accuracy by being very speciﬁc but not sensitive, with the opposite being
the case for Peq. Our method achieves an eﬀective balance between the two,
resulting in better segmentation as reﬂected by higher DSC.
and background class. We refer to these schemes as Puni and Peq. The results shown in Fig. 6 show a correlation of sensitivity and speciﬁcity with the
percentage of training samples that come from the lesion class. Peq performs
poorly because of over-segmentation (high sensitivity, low speciﬁcity). Puni
has better classiﬁcation on the background class (high speciﬁcity), which
leads to high mean voxel-wise accuracy since the majority corresponds to
background, but not particularly high DSC scores due to under-segmentation
(low sensitivity).
To evaluate our dense training scheme, we train multiple models with
varying sized image segments, equally sampled from lesions and background.
The tested sizes of the segments go from 193 upwards to 293. The models are
referred to as “S-d”, where d is the side length of the cubic segments. For
fair comparison, the batch sizes in all the experiments are adjusted to have
a similar memory footprint and lead to similar training times as compared
to training on Puni and Peq2. We observe a great performance increase for
model S-19 over Peq. We account this partly to the eﬃcient increase of the
eﬀective batch size (B · V in Eq. (4)), but also to the altered distribution of
training samples. As we increase the size of the training segments further,
we quickly reach a balance between the sensitivity of Peq and the speciﬁcity
of Puni, which results in improved segmentation as expressed by the DSC.
The segment size is a hyper-parameter in our model. We observe that
the increase in performance with increasing segment size quickly levels oﬀ,
and similar performance is obtained for a wide range of segment sizes, which
allows for easy conﬁguration. For the remaining experiments, all models were
trained on segments of size 253.
3.3. Eﬀect of Deeper Networks
Figure 7: Mean accuracy over validation samples and DSC for the segmentations of the validation images, as obtained from the “Shallow” baseline
and “Deep” variant with smaller kernels. Training of the plain deeper model
fails (cf. Sec. 3.3). This is overcome by adopting the initialization scheme of
 ), which further combined with Batch Normalization leads to
the enhanced (+) variants. Deep+ performs signiﬁcantly better than Shallow+
with similar computation time, thanks to the use of small kernels.
The 5-layers baseline CNN (Fig. 2), here referred to as the “Shallow”
model, is extended to 9-layers by replacing each convolutional layer that
uses 53 kernels with two layers that use 33 kernels (Fig. 4). This model is
2Dense training on a whole volume was inapplicable in these experimental settings due
to memory limitations but was previously shown to give similar results as training on
uniformly sampled patches ).
referred to as “Deep”. Training the latter, however, utterly fails with the
model making only predictions corresponding to the background class. This
problem is related to the challenge of preserving the signal as it propagates
through deep networks and its variance gets multiplied with the variance of
the weights, as previously discussed in Sec. 2.3. One of the causes is that the
weights of both models have been initialized with the commonly used scheme
of sampling from the normal distribution N(0, 0.01) ). In comparison, the initialization scheme by He et al. , derived
for preserving the signal in the initial stage of training, results in higher values
and overcomes this problem. Further preservation of the signal is obtained by
employing Batch Normalization. This results in an enhanced 9-layers model
which we refer to as “Deep+”, and using the same enhancements on the
Shallow model yields “Shallow+”. The signiﬁcant performance improvement
of Deep+ over Shallow+, as shown in Fig. 7, is the result of the greater
representational power of the deeper network. The two models need similar
computational times, which highlights the beneﬁts of utilizing small kernels in
the design of 3D CNNs. Although the deeper model requires more sequential
(layer by layer) computations on the GPU, those are faster due to the smaller
kernel size.
3.4. Eﬀect of the Multi-Scale Dual Pathway
Figure 8: Mean accuracy over validation samples and DSC for the segmentation of the validation images, as obtained by a single-scale model
(Deep+) and our dual pathway architecture (DeepMedic). We also trained
a single-scale model with larger capacity (BigDeep+), similar to the capacity of DeepMedic. DeepMedic yields best performance by capturing greater
context, while BigDeep+ seems to suﬀer from over-ﬁtting.
The ﬁnal version of the proposed network architecture, referred to as
“DeepMedic”, is built by extending the Deep+ model with a second convolutional pathway that is identical to the ﬁrst one. Two hidden layers are added
for combining the multi-scale features before the classiﬁcation layer, resulting
in a deep network of 11-layers (cf. Fig. 5). The input segments to the second
pathway are extracted from the images down-sampled by a factor of three.
Thus, the network is capable of capturing context in a 513 area of the original
image through the 173 receptive ﬁeld of the lower-resolution pathway, while
only doubling the computational and memory requirements over the single
pathway CNN. In comparison, the most recent 2D CNN systems proposed
for lesion segmentation ; Pereira et al. ) have a
receptive ﬁeld limited to 332 voxels.
Figure 8 shows the improvement DeepMedic achieves over the single pathway model Deep+. In Fig. 9 we show two representative visual examples
of this improvement when using the multi-scale CNN. Finally, we conﬁrm
that the performance increase can be accounted to the additional context
and not the additional capacity of DeepMedic. To this end, we build a big
single-scale model by doubling the FMs at each of the 9-layers of Deep+
and adding two hidden layers. This 11-layers deep and wide model, referred
to as “BigDeep+”, has the same number of parameters as DeepMedic. The
performance of the model is not improved, while showing signs of over-ﬁtting.
3.5. Processing 3D in comparison to 2D Context
Acquired brain MRI scans are often anisotropic.
Such is the case for
most sequences in our TBI dataset, which have been acquired with lower
axial resolution, except for the isotropic MPRAGE. We perform a series
of experiments to investigate the behaviour of 2D networks and assess the
beneﬁt of processing 3D context in this setting.
DeepMedic can be converted to 2D by setting the third dimension of each
kernel to one. This way only information from the surrounding context on
the axial plane inﬂuences the classiﬁcation of each voxel. If 2D segments are
given as input, the dimensionality of the feature maps decreases and so does
the memory required.
This allows developing 2D variants with increased
width, depth and size of training batch with similar requirements as the 3D
version, which are valid candidates for model selection in practical scenarios. We assess various conﬁgurations and present some representatives in
Table B.1b along with their performance. Best segmentation among investigated 2D variants is achieved by a 19-layers, multi-scale network, reaching
Figure 9: (Rows) Two cases from the severe TBI dataset, showing representative improvements when using the multi-scale CNN approach. (Columns)
From left to right: the MRI FLAIR sequence with the manually labeled lesions, predicted soft segmentation map obtained from a single-scale model
(Deep+) and the prediction of the multi-scale DeepMedic model. The incorporation of greater context enables DeepMedic to identify when it processes
an area within larger lesions (top). Spurious false positives are signiﬁcantly
reduced across the image on the bottom.
61.5% average DSC on the validation fold. The decline from the 66.6% DSC
achieved by the 3D version of DeepMedic indicates the importance of processing 3D context even in settings where most acquired sequences have low
resolution along a certain axis.
4. Evaluation on Clinical Data
The proposed system consisting of the DeepMedic CNN architecture, optionally coupled with a fully connected CRF, is evaluated on three lesion
segmentation tasks including challenging clinical data from patients with
traumatic brain injuries, brain tumors, and ischemic stroke. Quantitative
evaluation and comparisons with state-of-the-art are reported for each of the
4.1. Traumatic Brain Injuries
4.1.1. Material and Pre-Processing
Sixty-six patients with moderate-to-severe TBI who required admission
to the Neurosciences Critical Care Unit at Addenbrooke’s Hospital, Cambridge, UK, underwent imaging using a 3-Tesla Siemens Magnetom TIM Trio
within the ﬁrst week of injury. Ethical approval was obtained from the Local
Research Ethics Committee (LREC 97/290) and written assent via consultee agreement was obtained for all patients. The structural MRI sequences
that are used in this work are isotropic MPRAGE (1mm×1mm×1mm),
axial FLAIR, T2 and Proton Density (PD) (0.7mm×0.7mm×5mm), and
Gradient-Echo (GE) (0.86mm×0.86mm×5mm).
All visible lesions were
manually annotated on the FLAIR and GE sequences with separate labeling for each lesion type. In nine patients the presence of hyperintense white
matter lesions that were felt to be chronic in nature were also annotated.
Artifacts, for example, signal loss secondary to intraparenchymal pressure
probes, were also noted. For the purpose of this study we focus on binary
segmentation of all abnormalities within the brain tissue. Thus, we merged all
classes that correspond to intra-cerebral abnormalities into a single “lesion”
label. Extra-cerebral pathologies such as epidural and subdural hematoma
were treated as background. We excluded two datasets because of corrupted
FLAIR images, two cases because no lesions were found and one case because of a major scanning artifact corrupting the images. This results in a
total of 61 cases used for quantitative evaluation. Brain masks were obtained
using the ROBEX tool ). All images were resampled
to an isotropic 1mm3 resolution, with dimensions 193×229×193 and aﬃnely
registered ) to MNI space using the atlas by Grabner
et al. . No bias ﬁeld correction was used as preliminary results showed
that this can negatively aﬀect lesion appearance. Image intensities were normalized to have zero-mean and unit variance, as it has been reported that
this improves CNN results ).
4.1.2. Experimental Setting
Network conﬁguration and training: The network architecture corresponds to the one described in Sec. 3.4, i.e.
a dual-pathway, 11-layers
deep CNN. The training data is augmented by adding images reﬂected along
the sagittal axis. To make the network invariant to absolute intensities we
also shift the intensities of each MR channel c of every training segment by
ic = rcσc.
rc is sampled for every segment from N(0, 0.1) and σc is the
standard deviation of intensities under the brain mask in the corresponding
image. The network is regularized using dropout ) with a
rate of 2% on all convolutional layers, which is in addition to a 50% rate used
on the last two layers. The network is evaluated with 5-fold cross-validation
on the 61 subjects.
CRF conﬁguration: The parameters of the fully connected CRF are
determined in a conﬁguration experiment using random-search and 15 randomly selected subjects from the TBI database with predictions from a preliminary version of the corresponding model. The 15 subjects are reshuﬄed
into the 5-folds used for subsequent evaluation.
Random Forest baseline: We have done our best to set up a competitive baseline for comparison. We employ a context-sensitive Random Forest,
similar to the model presented by Zikic et al. for brain tumors except
that we apply the forest to the MR images without additional tissue speciﬁc
priors. We train a forest with 50 trees and maximum depth of 30. Larger
size did not improve results. Training data points are approximately equally
sampled from lesion and background classes, with the optimal balance empirically chosen. Two hundred randomized cross-channel box features are
evaluated at each split node with maximum oﬀsets and box sizes of 20mm.
The same folds of training and test sets are used as for our CNN approach.
4.1.3. Results
Table 1 summarizes the results on TBI. Our CNN signiﬁcantly outperforms the Random Forest baseline, while the relatively overall low DSC values
indicate the diﬃculty of the task. Due to randomness during training the local minima where a network converges are diﬀerent between training sessions
and some errors they produce diﬀer ). To clear the
unbiased errors of the network we form an ensemble of three similar networks,
aggregating their output by averaging. This ensemble yields better performance in all metrics but also allows us to investigate the behaviour of our
network focusing only on the biased errors. Fig. 10 shows the DSC obtained
Table 1: Performance of DeepMedic and an ensemble of three networks on
the TBI database. For comparison, we provide results for a Random Forest
baseline. Values correspond to the mean (and standard deviation). Numbers in bold indicate signiﬁcant improvement by the CRF post-processing,
according to a two-sided, paired t-test on the DSC metric (*p < 5 · 10−2,
**p < 10−4).
Sensitivity
51.1(20.0)
50.1(24.4)
60.1(15.8)
8.29(6.76)
64.17(15.98)
R. Forest+CRF
54.8(18.5)**
58.6(23.1)
56.9(17.4)
6.71(5.01)
59.45(15.52)
62.3(16.4)
65.3(18.8)
64.4(16.3)
4.24(2.64)
56.50(15.88)
DeepMedic+CRF
63.0(16.3)**
67.7(18.2)
63.2(16.7)
4.02(2.54)
55.68(15.93)
64.2(16.2)
67.7(18.3)
65.3(16.3)
3.88(2.33)
54.38(15.45)
Ensemble+CRF
64.5(16.3)*
69.8(17.8)
63.9(16.7)
3.72(2.29)
52.38(16.03)
by the ensemble on each subject in relation to the manually segmented and
predicted lesion volume. The network is capable of segmenting cases with
very small lesions, although, performance is less robust in these cases as even
small errors have large inﬂuence on the DSC metric. Investigation of the predicted lesion volume, which is an important biomarker for prognostication,
shows that the network is neither biased towards the lesion nor background
class, with promising results even on cases with very small lesions. Furthermore, we separately evaluate the inﬂuence of the post-processing with the
fully connected CRF. As shown in Table 1, the CRF yields improvements
over all classiﬁers. Eﬀects are more prominent when the performance of the
primary segmenter degrades, which shows the robustness of this regulariser.
Fig. 11 shows three representative cases.
4.2. Brain Tumor Segmentation
4.2.1. Material and Pre-Processing
For brain tumors, we evaluate our system on the data from the 2015
Brain Tumor Segmentation Challenge (BRATS) ). The
training set consists of 220 cases with high grade (HG) and 54 cases with
low grade (LG) glioma for which corresponding reference segmentations are
provided. The segmentations include the following tumor tissue classes: 1)
necrotic core, 2) edema, 3) non-enhancing and 4) enhancing core. The test
set consists of 110 cases of both HG and LG but the grade is not revealed.
Reference segmentations for the test set are hidden and evaluation is carried
out via an online system. For evaluation, the four predicted labels are merged
Figure 10: (Top) DSC achieved by our ensemble of three networks on each of
the 61 TBI datasets. (Bottom) Manually segmented (black) and predicted
lesion volumes (red).
Note here the logarithmic scale.
Continuous lines
represent mean values. The outlying subject 12 presents small TBI lesions,
which are successfully segmented, but also vascular ischemia. Because it is
the only case in the database with the latter pathology, the networks fail to
segment it as such lesion was not seen during training.
into diﬀerent sets of whole tumor (all four classes), the core (classes 1,3,4),
and the enhancing tumor (class 4)3. For each subject, four MRI sequences are
available, FLAIR, T1, T1-contrast and T2. The datasets are pre-processed by
the organizers and provided as skull-stripped, registered to a common space
and resampled to isotropic 1mm3 resolution.
Dimensions of each volume
are 240×240×155. We add minimal pre-processing of normalizing the braintissue intensities of each sequence to have zero-mean and unit variance.
4.2.2. Experimental Setting
Network conﬁguration and training: We modify the DeepMedic architecture to handle multi-class problems by extending the classiﬁcation layer
to ﬁve feature maps (four tumor classes plus background). The rest of the
conﬁguration remains unchanged. We enrich the dataset with sagittal re-
ﬂections. Opposite to the experiments on TBI, we do not employ the inten-
3For interpretation of the results note that, to the best of our knowledge, cases where
the “enhancing tumor” class is not present in the manual segmentation are considered as
zeros for the calculation of average performance by the evaluation platform, lowering the
upper bound for this class.
Figure 11: Three examples from the application of our system on the TBI
database. It is capable of precise segmentation of both small and large lesions.
Second row depicts one of the common mistakes observed. A contusion near
the edge of the brain is under-segmented, possibly mistaken for background.
Bottom row shows one of the worst cases, representative of the challenges
in segmenting TBI. Post-surgical sub-dural debris is mistakenly captured by
the brain mask. The network partly segments the abnormality, which is not
a celebral lesion of interest.
sity perturbation and dropout on convolutional layers, because the network
should not require as much regularisation with this large database.
network is trained on image segments extracted with equal probability centred on the whole tumor and healthy tissue. The distribution of the classes
captured by our training scheme is provided in Appendix C.
To examine our network’s behaviour, we ﬁrst evaluate it on the training
data of the challenge. For this, we run a 5-fold cross validation where each
fold contains both HG and LG images. We then retrain the network using
all training images, before applying it on the test data.
CRF conﬁguration: For the multi-class problem it is challenging to
ﬁnd a global set of parameters for the CRF which can consistently improve
the segmentation of all classes. So instead we merge the four predicted probability maps into a single “whole tumor” map for CRF post-processing. The
CRF then only reﬁnes the boundaries between tumor and background and
additionally removes isolated false positives. Similarly to the experiments on
TBI, the CRF is conﬁgured on a random subset of 44 HG and 18 LG training
images, which are then reshuﬄed into the subsequent 5-fold cross validation.
4.2.3. Results
Table 2: Average performance of our system on the training data of BRATS
2015 as computed on the online evaluation platform and comparison to other
submissions visible at the time of manuscript submission. Presenting only
teams that submitted more than half of the 274 cases. Numbers in bold indicate signiﬁcant improvement by the CRF, according to a two-sided, paired
t-test on the DSC metric (*p < 5 · 10−2, **p < 10−3).
Sensitivity
Ensemble+CRF
DeepMedic+CRF
Quantitative results from the application of the DeepMedic, the CRF and
an ensemble of three similar networks on the training data are presented in
Table 2. The latter two oﬀer an improvement, albeit fairly small since the
performance of DeepMedic is already rather high in this task. Also shown are
results from previous works, as reported on the online evaluation platform.
Various settings may vary among submissions, such as the pre-processing
pipeline or the number of folds used for cross-validation. Still it appears
that our system performs favourably compared to previous state-of-the-art,
including the semi-automatic system of Bakas et al. (bakas1) who
won the latest challenge and the method of Pereira et al. (peres1),
which is based on grade-speciﬁc 2D CNNs and requires visual inspection of
the tumor and identiﬁcation of the grade by the user prior to segmentation.
Examples of segmentations obtained with our method are shown in Fig. 12.
DeepMedic behaves very well in preserving the hierarchical structure of the
tumor, which we account to the large context processed by our multi-scale
Table 3 shows the results of our method on the BRATS test data. Results
of other submissions are not accessible. The decrease in performance is possibly due to the the inclusion of test images that vary signiﬁcantly from the
training data, such as cases acquired in clinical centers that did not provide
any of the training images, something that was conﬁrmed by the organisers.
Note that performance gains obtained with the CRF are larger in this case.
This indicates not only that its conﬁguration has not overﬁtted to the training database but also that the CRF is robust to factors of variation between
acquisition sites, which complements nicely the more sensitive CNN.
Table 3: Average performance of our system on the 110 test cases of BRATS
2015, as computed on the online evaluation platform. Numbers in bold indicate signiﬁcant improvement by the CRF, according to a two-sided, paired
t-test on the DSC metric (*p < 5 · 10−2, **p < 10−3). The decrease of the
mean DSC by the CRF and the ensemble for the “Core” class was not found
signiﬁcant.
Sensitivity
DeepMedic+CRF
Ensemble+CRF
4.3. Ischemic Stroke Lesion Segmentation
4.3.1. Material and Pre-Processing
We participated in the 2015 Ischemic Stroke Lesion Segmentation (ISLES)
challenge, where our system achieved the best results among all participants
on sub-acute ischemic stroke lesions ). In the training
phase of the challenge, 28 datasets have been made available, along with
Figure 12: Examples of DeepMedic’s segmentation from its evaluation on
the training datasets of BRATS 2015. cyan: necrotic core, green: oedema,
orange: non-enhancing core, red: enhancing core. (top and middle) Satisfying segmentation of the tumor, regardless motion artefacts in certain
sequences. (bottom) One of the worst cases of over-segmentation observed.
False segmentation of FLAIR hyper-intensities as oedema constitutes the
most common error of DeepMedic.
manual segmentations. Each dataset included T1, T1-contrast, FLAIR and
DWI sequences. All images were provided as skull-stripped and resampled
to isotropic 1mm3 voxel resolution. Each volume is of size 230×230×154.
In the testing stage, teams were provided with 36 datasets for evaluation.
The test data were acquired in two clinical centers, with one of them being
the same that provided all training images. Corresponding expert segmentations were hidden and results had to be submitted to an online evaluation
platform. Similar to BRATS, the only pre-processing that we applied is the
normalization of each image to the zero-mean and unit variance.
4.3.2. Experimental Setting
Network Conﬁguration and Training: The conﬁguration of the network employed is described in Kamnitsas et al. . The main diﬀerence
with the conﬁguration used for TBI and tumors as employed above is the
relatively smaller number of FMs in the low-resolution pathway. This choice
should not signiﬁcantly inﬂuence accuracy on the generally small SISS lesions
but it allowed us to lower the computational cost.
Similar to the other experiments, we evaluate our network with a 5-fold
cross validation on the training datasets. We use data augmentation with
sagittal reﬂections. For the testing phase of the challenge, we trained an ensemble of three networks on all training cases and aggregate their predictions
by averaging.
CRF conﬁguration: The parameters of the CRF were conﬁgured via a
random search on the whole training dataset.
4.3.3. Results
The performance of our system on the training data is shown in Table 4.
Signiﬁcant improvement is achieved by the structural regularisation oﬀered
by the CRF, although it could be partially accounted for by overﬁtting the
training data during the CRF’s conﬁguration. Examples for visual inspection
are shown in Fig. 13.
Table 4: Performance of our system on the training data of the ISLES-SISS
2015 competition. Values correspond to the mean (and standard deviation).
Numbers in bold indicate signiﬁcant improvement by the CRF, according to
a two-sided, paired t-test on the DSC metric (p < 10−2).
Sensitivity
6.99(9.91)
73.32(26.03)
DeepMedic+CRF
5.00(10.33 )
55.93(28.55)
For the testing phase of the challenge we formed an ensemble of three networks, coupled with the fully connected CRF. Our submission ranked ﬁrst,
indicating superior performance on this challenging task among 14 submissions. Table 5 shows our results, along with the other two top entries .
Sensitivity
kamnk1(ours)
7.87(12.63)
39.61(30.68)
8.13(15.15)
25.02(22.02)
14.61(20.17)
46.26(34.81)
et al. ; Halme et al. ). Among the other participating methods was the CNN of Havaei et al. with 3 layers of 2D convolutions.
That method perfomed less well on this challenging task ).
This points out the advantage oﬀered by 3D context, the large ﬁeld of view of
DeepMedic thanks to multi-scale processing and the representational power
of deeper networks. It is important to note the decrease of performance in
comparison to the training set. All methods performed worse on the data
coming from the second clinical center, including the method of Feng et al.
 that is not machine-learning based. This highlights a general diﬃculty
with current approaches when applied on multi-center data.
4.4. Implementation Details
Our CNN is implemented using the Theano library ).
Each training session requires approximately one day on an NVIDIA GTX
Titan X GPU using cuDNN v5.0. The eﬃcient architecture of DeepMedic
also allows models to be trained on GPUs with only 3GB of memory. Note
that although dimensions of the volumes in the processed databases do not
allow dense training on whole volumes for this size of network, dense inference
on a whole volume is still possible, as it requires only a forward-pass and
thus less memory. In this fashion segmentation of a volume takes less than
30 seconds but requires 12 GB of GPU memory.
Tiling the volume into
multiple segments of size 353 allows inference on 3 GB GPUs in less than
three minutes.
Our 3D fully connected CRF is implemented by extending the original
source code by Kr¨ahenb¨uhl and Koltun . A CPU implementation is
fast, capable of processing a ﬁve-channel brain scan in under three minutes.
Further speed-up could be achieved with a GPU implementation, but was
Figure 13: Examples of segmentations performed by our system on the training datasets of (SISS) ISLES 2015. (top and middle) The system is capable of
satisfying segmentation of both large and smaller lesions. (bottom) Common
mistakes are performed due to the challenge of diﬀerentiating stroke lesions
from White Matter lesions.
not found necessary in the scope of this work.
5. Discussion and Conclusion
We have presented DeepMedic, a 3D CNN architecture for automatic lesion segmentation that surpasses state-of-the-art on challenging data. The
proposed novel training scheme is not only computationally eﬃcient but also
oﬀers an adaptive way of partially alleviating the inherent class-imbalance
of segmentation problems. We analyzed the beneﬁts of using small convolu-
tional kernels in 3D CNNs, which allowed us to develop a deeper and thus
more discriminative network, without increasing the computational cost and
number of trainable parameters. We discussed the challenges of training deep
neural networks and the adopted solutions from the latest advances in deep
learning. Furthermore, we proposed an eﬃcient solution for processing large
image context by the use of parallel convolutional pathways for multi-scale
processing, alleviating one of the main computational limitations of previous
3D CNNs. Finally, we presented the ﬁrst application of a 3D fully connected
CRF on medical data, employed as a post-processing step to reﬁne the network’s output, a method that has also been shown promising for processing
2D natural images ). The design of the proposed system is well suited for processing medical volumes thanks to its generic 3D
nature. The capabilities of DeepMedic and the employed CRF for capturing 3D patterns exceed those of 2D networks and locally connected random
ﬁelds, models that have been commonly used in previous work. At the same
time, our system is very eﬃcient at inference time, which allows its adoption
in a variety of research and clinical settings.
The generic nature of our system allows its straightforward application
for diﬀerent lesion segmentation tasks without major adaptations. To the
best of our knowledge, our system achieved the highest reported accuracy
on a cohort of patients with severe TBI. As a comparison, we improved over
the reported performance of the pipeline in Rao et al. . Important to
note is that the latter work focused only on segmentation of contusions, while
our system has been shown capable of segmenting even small and diﬀused
pathologies. Additionally, our pipeline achieved state-of-the-art performance
on both public benchmarks of brain tumors and stroke lesions . We believe performance can be further improved
with task- and data-speciﬁc adjustments, for instance in the pre-processing,
but our results show the potential of this generically designed segmentation
When applying our pipeline to new tasks, a laborious process is the reconﬁguration of the CRF. The model improved our system’s performance
with statistical signiﬁcance in all investigated tasks, most profoundly when
the performance of the underlying classiﬁer degrades, proving its ﬂexibility
and robustness. Finding optimal parameters for each task, however, can be
challenging.
This became most obvious on the task of multi-class tumor
segmentation. Because the tumor’s substructures vary signiﬁcantly in appearance, ﬁnding a global set of parameters that yields improvements on all
classes proved diﬃcult. Instead, we applied the CRF in a binary fashion.
This CRF model can be conﬁgured with a separate set of parameters for
each class. However the larger parameter space would complicate its con-
ﬁguration further. Recent work from Zheng et al. showed that this
particular CRF can be casted as a neural network and its parameters can be
learned with regular gradient descent. Training it in an end-to-end fashion
on top of a neural network would alleviate the discussed problems. This will
be explored as part of future work.
The discriminative power of the learned features is indicated by the success of recent CNN-based systems in matching human performance in domains where it was previously considered too ambitious ;
Silver et al. ). Analysis of the automatically extracted information
could potentially provide novel insights and facilitate research on pathologies for which little prior knowledge is currently available. In an attempt to
illustrate this, we explore what patterns have been learned automatically for
the lesion segmentation tasks. We visualize the activations of DeepMedic’s
FMs when processing a subject from our TBI database. Many appearing
patterns are diﬃcult to interpret, especially in deeper layers. In Fig. 14 we
provide some examples that have an intuitive explanation. One of the most
interesting ﬁndings is that the network learns to identify the ventricles, CSF,
white and gray matter. This reveals that diﬀerentiation of tissue type is beneﬁcial for lesion segmentation. This is in line with ﬁndings in the literature,
where segmentation performance of traditional classiﬁers was signiﬁcantly
improved by incorporation of tissue priors ; Zikic
et al. ). It is intuitive that diﬀerent types of lesions aﬀect diﬀerent
parts of the brain depending on the underlying mechanisms of the pathology. A rigorous analysis of spatial cues extracted by the network may reveal
correlations that are not well deﬁned yet.
Similarly intriguing is the information extracted in the low-resolution
As they process greater context, these neurons gain additional
localization capabilities. The activations of certain FMs form ﬁelds in the
surrounding areas of the brain. These patterns are preserved in the deepest
hidden layers, which indicates they are beneﬁcial for the ﬁnal segmentation
(see two last rows of Fig. 14). We believe these cues provide a spatial bias
to the system, for instance that large TBI contusions tend to occur towards
the front and sides of the brain (see Fig. 1c). Furthermore, the interaction of
the multi-resolution features can be observed in FMs of the hidden layer that
follows the concatenation of the pathways. The network learns to weight the
Figure 14: (First row) GE scan and DeepMedic’s segmentation. (Second row)
FMs of earlier and (third row) deeper layers of the ﬁrst convolutional pathway. (Fourth row) Features learnt in the low-resolution pathway. (Last row)
FMs of the two last hidden layers, which combine multi-resolution features
towards the ﬁnal segmentation.
output of the two pathways, preserving low resolution in certain parts and
show ﬁne details in others (bottom row of Fig. 14, ﬁrst three FMs). Our
assumption is that the low-resolution pathway provides a rough localization
of large pathologies and brain areas that are challenging to segment, which
reserves the rest of the network’s capacity for learning detailed patterns associated with the detection of smaller lesions, ﬁne structures and ambiguous
The ﬁndings of the above exploration lead us to believe that great potential lies into fusing the discriminative power of the “deep black box” with
the knowledge acquired over years of targeted biomedical research. Clinical
knowledge is available for certain pathologies, such as spatial priors for white
matter lesions. Previously engineered models have been proven eﬀective in
tackling fundamental imaging problems, such as brain extraction, tissue segmentation and bias ﬁeld correction. We show that a network is capable of
automatically extracting some of this information. It would be interesting,
however, to investigate structured ways for incorporating such existing information as priors into the network’s feature space, which should simplify the
optimization problem while letting a specialist guide the network towards an
optimal solution.
Although neural networks seem promising for medical image analysis,
making the inference process more interpretable is required. This would allow understanding when the network fails, an important aspect in biomedical
applications. Although the output is bounded in the range and commonly referred to as probability for convenience, it is not a true probability in
a Bayesian sense. Research towards Bayesian networks aims to alleviate this
limitation. An example is the recent work of Gal and Ghahramani 
who show that model conﬁdence can be estimated via sampling the dropout
A general point should be made about the performance drop observed
when our system is applied on test datasets of BRATS and ISLES in comparison to its cross-validated performance on the training data.
cases, subsets of the test images were acquired in clinical centers diﬀerent
from the ones of training datasets. Diﬀerences in scanner type and acquisition protocols have signiﬁcant impact on the appearance of the images. The
issue of multi-center data heterogeneity is considered a major bottleneck for
enabling large-scale imaging studies. This is not speciﬁc to our approach, but
a general problem in medical image analysis. One possible way of making the
CNN invariant to the data heterogeneity is to learn a generative model for
the data acquisition process, and use this model in the data augmentation
step. This is a direction we explore as part of future work.
In order to facilitate further research in this area and to provide a baseline
for future evaluations, we make the source code of the entire system publicly
available.
Acknowledgements
This work is supported by the EPSRC First Grant scheme (grant ref no.
EP/N023668/1) and partially funded under the 7th Framework Programme
by the European Commission (TBIcare: CENTER-
TBI: This work was further supported by a
Medical Research Council (UK) Program Grant (Acute brain injury: heterogeneity of mechanisms, therapeutic targets and outcome eﬀects [G9439390
ID 65883]), the UK National Institute of Health Research Biomedical Research Centre at Cambridge and Technology Platform funding provided by
the UK Department of Health. KK is supported by the Imperial College
London PhD Scholarship Programme. VFJN is supported by a Health Foundation/Academy of Medical Sciences Clinician Scientist Fellowship. DKM is
supported by an NIHR Senior Investigator Award. We gratefully acknowledge the support of NVIDIA Corporation with the donation of two Titan X
GPUs for our research.
Appendix A. Additional Details on Multi-Scale Processing
The integration of multi-scale parallel pathways in architectures that use
solely unary kernel strides, such as the proposed, was described in Sec. 2.4.
The required up-sampling of the low-resolution features was performed with
simple repetition in our experiments. This was found suﬃcient, with the
following hidden layers learning to combine the multi-scale features. In the
case of architectures with strides greater than unary, the last convolutional
layers of the two pathways, L1 and L2, have receptive ﬁelds ϕL1 and ϕL2
with strides τL1 and τL2 respectively. To preserve spatial correspondence
of the multi-scale features and enable the network for dense inference, the
dimensions of the input segments should be chosen such that the FMs in L2
can be brought to the dimensions of the FMs in L1 after sequential resampling
by ↑τL2, ↑FD, ↓τL1 or equivalent combinations. Here ↑and ↓represent upand down-sampling by the given factor. Because they are more reliant on
these operations, utilization of more elaborate, learnt upsampling schemes
 ; Ronneberger et al. ; Noh et al. ) should be
beneﬁcial in such networks.
Appendix B. Additional Details on Network Conﬁgurations
3D Networks: The main description of our system is presented in Sec. 2.
All models discussed in this work outside Sec. 3.5 are fully 3D CNNs. Their
architectures are presented in Table B.1a.
They all use the PReLu nonlinearity ). They are trained using the RMSProp optimizer
 ) and Nesterov momentum ) with value m = 0.6. L1 = 10−6 and L2 = 10−4 regularisation is applied. We train the networks with dense-training on batches of 10 segments,
each of size 253. Exceptions are the experiments in Sec 3.2, where the batch
sizes were adjusted along with the segment sizes, to achieve similar memory
footprint and training time per batch. The weights of our shallow, 5-layers
networks are initialized by sampling from a normal distribution N(0, 0.01)
and their initial learning rate is set to a = 10−4. Deeper models (and the
“Shallow+” model in Sec 3.3) use the weight initialisation scheme of He et al.
 . The scheme increases the signal’s variance in our settings, which
leads to RMSProm decreasing the eﬀective learning rate. To counter this,
we accompany it with an increased initial learning rate a = 10−3. Throughout training, the learning rate of all models is halved whenever convergence
plateaus. Dropout with 50% rate is employed on the two last hidden layers
of 11-layers deep models.
2D Networks: Table B.1b presents representative examples of 2D con-
ﬁgurations that were employed for the experiments discussed in Sec. 3.5.
Width, depth and batch size were adjusted so that total required memory
was similar to the 3D version of DeepMedic. Wider or deeper variants than
the ones presented did not show greater performance. A possible reason is
that this number of ﬁlters is enough for the extraction of the limited 2D information and that the ﬁeld of view of the deep multi-scale variant is already
suﬃcient for the application. The presented 2D models were regularized with
L1 = 10−8 and L2 = 10−6 since they have less parameters than the 3D variants. All but Dm2dPatch were trained with momentum m = 0.6 and initial
learning rate a = 10−3, while the rest with m = 0.9 and a = 10−2 as this setting increased performance. The rest of the hyper parameters are the same
as for the 3D DeepMedic.
Table B.1: Network architectures investigated in Sec. 3 and ﬁnal validation
accuracy achieved in the corresponding experiments. (a) 3D and (b) 2D architectures. Columns from left to right: model’s name, number of parallel
identical pathways and number of feature maps at each of their convolutional
layers, number of feature maps at each hidden layer that follows the concatenation of the pathways, dimensions of input segment to the normal and low
resolution pathways, batch size and, ﬁnally, average DSC achieved on the
validation fold. Further conﬁguration details provided in Appendix B.
(a) 3D Network Architectures
#Pathways: FMs/Layer
FMs/Hidd. Seg.Norm.
Shallow(+)
1: 30,40,40,50
60.2(61.7)
1: 30,30,40,40,40,40,50,50
00.0(64.9)
1: 60,60,80,80,80,80,100,100
2: 30,30,40,40,40,40,50,50
(b) 2D Network Architectures
#Pathways: FMs/Layer
FMs/Hidd. Seg.Norm.
Dm2dPatch*
2: 30,30,40,40,40,40,50,50
2: 30,30,40,40,40,40,50,50
Wider2dSeg
2: 60,60,80,80,80,80,100,100
Deeper2dSeg
2: 16 layers, linearly 30 to 50
Large2dSeg
2: 12 layers, linearly 45 to 80
* Sampling was manually calibrated to achieve similar class balance as models that are trained on image
segments. Model underperformed otherwise.
Appendix C. Distribution of Tumor Classes Captured in Training
Table C.1: Real distribution of the classes in the training data of BRATS
2015, along with the distribution captured by our proposed training scheme,
when segments of size 253 are extracted centred on the tumor and healthy
tissue with equal probability. Relative distribution of the foreground classes
is closely preserved and the imbalance in comparison to the healthy tissue is
automatically alleviated.