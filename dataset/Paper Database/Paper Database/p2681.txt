Proceedings of SemEval-2016, pages 19–30,
San Diego, California, June 16-17, 2016. c⃝2016 Association for Computational Linguistics
SemEval-2016 Task 5: Aspect Based Sentiment Analysis
Maria Pontiki∗1, Dimitrios Galanis1, Haris Papageorgiou1, Ion Androutsopoulos1,2,
Suresh Manandhar3, Mohammad AL-Smadi4, Mahmoud Al-Ayyoub4, Yanyan Zhao5,
Bing Qin5, Orphée De Clercq6, Véronique Hoste6, Marianna Apidianaki7,
Xavier Tannier7, Natalia Loukachevitch8, Evgeny Kotelnikov9,
Nuria Bel10, Salud María Jiménez-Zafra11, Gülşen Eryiğit12
1Institute for Language and Speech Processing, Athena R.C., Athens, Greece,
2Dept. of Informatics, Athens University of Economics and Business, Greece,
3Dept. of Computer Science, University of York, UK,
4Computer Science Dept., Jordan University of Science and Technology Irbid, Jordan,
5Harbin Institute of Technology, Harbin, Heilongjiang, P.R. China,
6LT3, Ghent University, Ghent, Belgium,
7LIMSI, CNRS, Univ. Paris-Sud, Université Paris-Saclay, Orsay, France,
8Lomonosov Moscow State University, Moscow, Russian Federation,
9Vyatka State University, Kirov, Russian Federation,
10Universitat Pompeu Fabra, Barcelona, Spain,
11Dept. of Computer Science, Universidad de Jaén, Spain,
12Dept. of Computer Engineering, Istanbul Technical University, Turkey
This paper describes the SemEval 2016 shared
task on Aspect Based Sentiment Analysis
(ABSA), a continuation of the respective tasks
of 2014 and 2015. In its third year, the task
provided 19 training and 20 testing datasets
for 8 languages and 7 domains, as well as a
common evaluation procedure.
From these
datasets, 25 were for sentence-level and 14 for
text-level ABSA; the latter was introduced for
the first time as a subtask in SemEval. The task
attracted 245 submissions from 29 teams.
Introduction
Many consumers use the Web to share their experiences about products, services or travel destinations
 . Online opinionated texts
(e.g., reviews, tweets) are important for consumer
decision making and
constitute a source of valuable customer feedback
that can help companies to measure satisfaction and
improve their products or services. In this setting,
Aspect Based Sentiment Analysis (ABSA) - i.e.,
mining opinions from text about specific entities and
their aspects - can provide valuable insights to both consumers and businesses. An ABSA
∗*Corresponding author: .
Figure 1: Table summarizing the average sentiment for each
aspect of an entity.
method can analyze large amounts of unstructured
texts and extract (coarse- or fine-grained) information not included in the user ratings that are available
in some review sites (e.g., Fig. 1).
Sentiment Analysis (SA) touches every aspect
(e.g., entity recognition, coreference resolution,
negation handling) of Natural Language Processing
 and as Cambria et al. mention “it
requires a deep understanding of the explicit and implicit, regular and irregular, and syntactic and semantic language rules”. Within the last few years
several SA-related shared tasks have been organized
in the context of workshops and conferences focus-
ing on somewhat different research problems .
Such competitions provide training datasets and the
opportunity for direct comparison of different approaches on common test sets.
Currently,
most of the available SA-related
datasets, whether released in the context of shared
tasks or not ,
are monolingual and usually focus on English texts.
Multilingual datasets provide additional benefits enabling the development and testing of crosslingual methods . Following this direction, this year the SemEval ABSA task provided
datasets in a variety of languages.
ABSA was introduced as a shared task for the first
time in the context of SemEval in 2014; SemEval-
2014 Task 41 (SE-ABSA14) provided datasets of
English reviews annotated at the sentence level with
aspect terms (e.g., “mouse”, “pizza”) and their polarity for the laptop and restaurant domains, as well
as coarser aspect categories (e.g., “food”) and their
polarity only for restaurants .
SemEval-2015 Task 122 (SE-ABSA15) built upon
SE-ABSA14 and consolidated its subtasks into a
unified framework in which all the identified constituents of the expressed opinions (i.e., aspects,
opinion target expressions and sentiment polarities)
meet a set of guidelines and are linked to each other
within sentence-level tuples .
These tuples are important since they indicate the
part of text within which a specific opinion is expressed. However, a user might also be interested
in the overall rating of the text towards a particular aspect. Such ratings can be used to estimate
the mean sentiment per aspect from multiple reviews . Therefore, in addition
to sentence-level annotations, SE-ABSA163 accommodated also text-level ABSA annotations and provided the respective training and testing data. Fur-
1 
2 
3 
thermore, the SE-ABSA15 annotation framework
was extended to new domains and applied to languages other than English (Arabic, Chinese, Dutch,
French, Russian, Spanish, and Turkish).
The remainder of this paper is organized as follows: the task set-up is described in Section 2. Section 3 provides information about the datasets and
the annotation process, while Section 4 presents the
evaluation measures and the baselines. General information about participation in the task is provided
in Section 5. The evaluation scores of the participating systems are presented and discussed in Section
6. The paper concludes with an overall assessment
of the task.
Task Description
The SE-ABSA16 task consisted of the following
subtasks and slots. Participants were free to choose
the subtasks, slots, domains and languages they
wished to participate in.
Subtask 1 (SB1): Sentence-level ABSA. Given
an opinionated text about a target entity, identify all
the opinion tuples with the following types (tuple
slots) of information:
• Slot1: Aspect Category. Identification of the
entity E and attribute A pairs towards which
an opinion is expressed in a given sentence. E
and A should be chosen from predefined inventories4 of entity types (e.g., “restaurant”,
“food”) and attribute labels (e.g., “price”,
“quality”).
• Slot2: Opinion Target Expression (OTE).
Extraction of the linguistic expression used in
the given text to refer to the reviewed entity E
of each E#A pair. The OTE is defined by its
starting and ending offsets. When there is no
explicit mention of the entity, the slot takes the
value “null”. The identification of Slot2 values was required only in the restaurants, hotels,
museums and telecommunications domains.
• Slot3: Sentiment Polarity.
Each identified
E#A pair has to be assigned one of the following
polarity labels: “positive”, “negative”, “neutral” (mildly positive or mildly negative).
4The full inventories of the aspect category labels for each
domain are provided in Appendix A.
Table 1: Datasets provided for SE-ABSA16.
An example of opinion tuples with Slot1-3 values
from the restaurants domain is shown below: “Their
sake list was extensive, but we were looking for Purple Haze, which wasn’t listed but made for us upon
request!” →{cat: “drinks#style_options”, trg:
“sake list”, fr: “6”, to: “15”, pol: “positive”}, {cat:
“service#general”, trg: “null”, fr: “0”, to: “0”,
pol: “positive”}. The variable cat indicates the aspect category (Slot1), pol the polarity (Slot3), and
trg the ote (Slot2); fr, to are the starting/ending
offsets of ote.
Subtask 2 (SB2): Text-level ABSA. Given a customer review about a target entity, the goal was
to identify a set of {cat, pol} tuples that summarize the opinions expressed in the review. cat can
be assigned the same values as in SB1 (E#A tuple), while pol can be set to “positive”, “negative”,
“neutral”, or “conflict”.
For example, for the review text “The So called laptop Runs to Slow and
I hate it!
Do not buy it!
It is the worst laptop
ever ”, a system should return the following opinion tuples: {cat: “laptop#general”, pol: “negative”}, {cat: “laptop#operation_performance”,
pol: “negative”} .
Subtask 3 (SB3): Out-of-domain ABSA. In SB3
participants had the opportunity to test their systems
in domains for which no training data was made
available; the domains remained unknown until the
start of the evaluation period. Test data for SB3 were
provided only for the museums domain in French.
Data Collection and Annotation
A total of 39 datasets were provided in the context of
the SE-ABSA16 task; 19 for training and 20 for testing. The texts were from 7 domains and 8 languages;
English (en), Arabic (ar), Chinese (ch), Dutch (du),
French (fr), Russian (ru), Spanish (es) and Turkish (tu). The datasets for the domains of restaurants
(rest), laptops (lapt), mobile phones (phns), digital
cameras (came), hotels (hote) and museums (muse)
consist of customer reviews, whilst the telecommunication domain (telc) data consists of tweets. A total of 70790 manually annotated ABSA tuples were
provided for training and testing; 47654 sentencelevel annotations (SB1) in 8 languages for 7 domains, and 23136 text-level annotations (SB2) in 6
languages for 3 domains. Table 1 provides more information on the distribution of texts, sentences and
annotated tuples per dataset.
The rest, hote, and lapt datasets were annotated
at the sentence-level (SB1) following the respective
annotation schemas of SE-ABSA15 . Below are examples5 of annotated sentences
for the aspect category “service#general” in en
(1), du (2), fr (3), ru (4), es (5), and tu (6) for the
rest domain and in ar (7) for the hote domain:
1. Service was slow, but the people were friendly.
“Service”, pol:
“negative”}, {trg:
“people”, pol: “positive”}
2. Snelle bediening en vriendelijk personeel moet
ook gemeld worden!!
→{trg: “bediening”,
pol: “positive”}, {trg: “personeel”, pol: “positive”}
3. Le service est impeccable, personnel agréable.
→{trg: “service” , pol: “positive”}, {trg: “personnel”, pol: “positive”}
4. Про сервис ничего негативного не скажешь-
быстро подходят, все улябаются, подходят
спрашивают, всё ли нравится.
“сервис”, pol: “neutral” }
5. También la rapidez en el servicio. →{trg: “servicio”, pol: “positive” }
6. Servisi hızlı valesi var. →{trg: “Servisi”, pol:
“positive”}
7. ..{→ اﻟﺨﺪﻣﺔﺟﯿﺪةﺟﺪا وﺳﺮﯾﻌﺔtrg: ”, “اﻟﺨﺪﻣﺔpol:
“positive”}
The lapt annotation schema was extended to two
other domains of consumer electronics, came and
phns. Examples of annotated sentences in the lapt
(en), phns (du and ch) and came (ch) domains are
shown below:
1. It is extremely portable and easily connects to
WIFI at the library and elsewhere.
“laptop#portability”, pol: “positive”} , {cat:
“laptop#connectivity”, pol: “positive”}
“software#operation_performance”,
“positive”},
“phone#connectivity”,
pol: “positive”}
5The offsets of the opinion target expressions are omitted.
3. 当然屏幕这么好→{cat: “display#quality”,
pol: “positive”}
4. 更轻便的机身也便于携带。→{cat:
“camera# portability”, pol: “positive”}
In addition, the SE-ABSA15 framework was extended to two new domains for which annotation
guidelines were compiled: telc for tu and muse for
fr. Below are two examples:
1. #Internet kopuyor sürekli :( @turkcell →{cat:
“internet#coverage”, trg:
“Internet”, pol:
“positive”}
2. 5€ pour les étudiants, ça vaut le coup. →{cat:
“museum#prices”, “null”, “positive”}
The text-level (SB2) annotation task was based
on the sentence-level annotations; given a customer
review about a target entity (e.g., a restaurant) that
included sentence-level annotations of ABSA tuples, the goal was to identify a set of {cat, pol}
tuples that summarize the opinions expressed in it.
This was not a simple summation/aggregation of the
sentence-level annotations since an aspect may be
discussed with different sentiment in different parts
of the review. In such cases the dominant sentiment
had to be identified.
In case of conflicting opinions where the dominant sentiment was not clear, the
”conflict” label was assigned. In addition, each review was assigned an overall sentiment label about
the target entity (e.g., “restaurant#general”,
“laptop#general”), even if it was not included in
the sentence-level annotations.
Annotation Process
All datasets for each language were prepared by one
or more research groups as shown in Table 2. The
en, du, fr, ru and es datasets were annotated using
brat , a web-based annotation tool, which was configured appropriately for the
needs of the task. The tu datasets were annotated using a customized version of turksent , a sentiment annotation tool for social media.
For the ar and the ch data in-house tools6 were used.
6The ar annotation tool was developed by the technical
team of the Advanced Arabic Text Mining group at Jordan University of Science and Technology. The ch tool was developed
by the Research Center for Social Computing and Information
Retrieval at Harbin Institute of Technology.
Research team(s)
Institute for Language and Speech Processing, Athena R.C., Athens, Greece
Dept. of Informatics, Athens University of Economics and Business, Greece
Computer Science Dept., Jordan University of Science and Technology Irbid, Jordan
Harbin Institute of Technology, Harbin, Heilongjiang, P.R. China
LT3, Ghent University, Ghent, Belgium
LIMSI, CNRS, Univ. Paris-Sud, Université Paris-Saclay, Orsay, France
Lomonosov Moscow State University, Moscow, Russian Federation
Vyatka State University, Kirov, Russian Federation
Universitat Pompeu Fabra, Barcelona, Spain
SINAI, Universidad de Jaén, Spain
Dept. of Computer Engineering, Istanbul Technical University, Turkey
Turkcell Global Bilgi, Turkey
Table 2: Research teams that contributed to the creation of the datasets for each language.
Below are some further details about the annotation
process for each language.
English. The SE-ABSA15 
training and test datasets (with some minor corrections) were merged and provided for training (rest
and lapt domains). New data was collected and annotated from scratch for testing. In a first phase, the
rest test data was annotated by an experienced7 linguist (annotator A), and the lapt data by 5 undergraduate computer science students. The resulting
annotations for both domains were then inspected
and corrected (if needed) by a second expert linguist,
one of the task organizers (annotator B). Borderline
cases were resolved collaboratively by annotators A
Arabic. The hote dataset was annotated in repeated cycles. In a first phase, the data was annotated
by three native Arabic speakers, all with a computer
science background; then the output was validated
by a senior researcher, one of the task organizers. If
needed (e.g. when inconsistencies were found) they
were given back to the annotators.
Chinese. The datasets presented by Zhao et al.
 were re-annotated by three native Chinese
speakers according to the SE-ABSA16 annotation
schema and were provided for training and testing
(phns and came domains).
Dutch. The rest and phns datasets were initially annotated by a
trained linguist, native speaker of Dutch.
the output was verified by another Dutch linguist
and disagreements were resolved between them. Fi-
7Also annotator for SE-ABSA14 and 15.
nally, the task organizers inspected collaboratively
all the annotated data and corrections were made
when needed.
French. The train (rest) and test (rest, muse)
datasets were annotated from scratch by a linguist,
native speaker of French. When the annotator was
not confident, a decision was made collaboratively
with the organizers. In a second phase, the task organizers checked all the annotations for mistakes and
inconsistencies and corrected them, when necessary.
For more information on the French datasets consult
Apidianaki et al. .
Russian. The rest datasets of the SentiRuEval-
2015 task were automatically converted to the SE-ABSA16 annotation
schema; then a linguist, native speaker of Russian,
checked them and added missing information. Finally, the datasets were inspected by a second linguist annotator (also native speaker of Russian) for
mistakes and inconsistencies, which were resolved
along with one of the task organizers.
Spanish. Initially, 50 texts (134 sentences) from
the whole available data were annotated by 4 annotators. The inter-anotator agreement (IAA) in terms of
F-1 was 91% for the identification of OTE, 88% for
the aspect category detection (E#A pair), and 80%
for opinion tuples extraction (E#A, OTE, polarity).
Provided that the IAA was substantially high for all
slots, the rest of the data was divided into 4 parts and
each one was annotated by a different native Spanish
speakers (2 linguists and 2 software engineers). Subsequently, the resulting annotations were validated
and corrected (if needed) by the task organizers.
Turkish. The telc dataset was based on the data
used in , while the rest dataset
was created from scratch. Both datasets were annotated simultaneously by two linguists. Then, one of
the organizers validated/inspected the resulting annotations and corrected them when needed.
Datasets Format and Availability
Similarly to SE-ABSA14 and SE-ABSA15, the
datasets8 of SE-ABSA16 were provided in an XML
format and they are available under specific license
terms through META-SHARE9, a repository devoted to the sharing and dissemination of language
resources .
Evaluation Measures and Baselines
The evaluation ran in two phases. In the first phase
(Phase A), the participants were asked to return
separately the aspect categories (Slot1), the OTEs
(Slot2), and the {Slot1, Slot2} tuples for SB1. For
SB2 the respective text-level categories had to be
identified. In the second phase (Phase B), the gold
annotations for the test sets of Phase A were provided and participants had to return the respective
sentiment polarity values (Slot3). Similarly to SE-
ABSA15, F-1 scores were calculated for Slot1, Slot2
and {Slot1, Slot2} tuples, by comparing the annotations that a system returned to the gold annotations (using micro-averaging). For Slot1 evaluation,
duplicate occurrences of categories were ignored in
both SB1 and SB2. For Slot2, the calculation for
each sentence considered only distinct targets and
discarded “null” targets, since they do not correspond to explicit mentions. To evaluate sentiment
polarity classification (Slot3) in Phase B, we calculated the accuracy of each system, defined as the
number of correctly predicted polarity labels of the
(gold) aspect categories, divided by the total number of the gold aspect categories. Furthermore, we
implemented and provided baselines for all slots of
SB1 and SB2. In particular, the SE-ABSA15 baselines that were implemented for the English language
8The data are available at: 
gr:8080/repository/search/?q=semeval+2016
9META-SHARE ( was
implemented in the framework of the META-NET Network of
Excellence ( 
 , were adapted for the other languages by using appropriate stopword lists and tokenization functions. The baselines are briefly discussed below:
SB1-Slot1: For category (E#A) extraction, a Support Vector Machine (SVM) with a linear kernel is
In particular, n unigram features are extracted from the respective sentence of each tuple
that is encountered in the training data.
The category value (e.g., “service#general”) of the tuple is used as the correct label of the feature vector.
Similarly, for each test sentence s, a feature vector is built and the trained SVM is used
to predict the probabilities of assigning each possible category to s (e.g., {“service#general”, 0.2},
{“restaurant#general”, 0.4}.
Then, a threshold10 t is used to decide which of the categories will
be assigned11 to s. As features, we use the 1,000
most frequent unigrams of the training data excluding stopwords.
SB1-Slot2:
The baseline uses the training
reviews to
category c
“service#general”)
“service#general”
“waiter”}).
These are extracted from the (training) opinion
tuples whose category value is c . Then, given a test
sentence s and an assigned category c, the baseline
finds in s the first occurrence of each OTE of c’s
list. The OTE slot is filled with the first of the target
occurrences found in s. If no target occurrences are
found, the slot is assigned the value “null”.
SB1-Slot3: For polarity prediction we trained a
SVM classifier with a linear kernel. Again, as in
Slot1, n unigram features are extracted from the respective sentence of each tuple of the training data.
In addition, an integer-valued feature12 that indicates
the category of the tuple is used. The correct label
for the extracted training feature vector is the corresponding polarity value (e.g., “positive”). Then, for
each tuple {category, OTE} of a test sentence s, a
feature vector is built and classified using the trained
SB2-Slot1: The sentence-level tuples returned by
the SB1 baseline are copied to the text level and duplicates are removed.
10The threshold t was set to 0.2 for all datasets.
11We use the –b 1 option of LibSVM to obtain probabilities.
12Each E#A pair has been assigned a distinct integer value.
{Slot1,Slot2}
NLANG./U/73.031
NLANG./U/72.34
NLANG./U/52.607
XRCE/C/88.126
NileT./U/72.886
AUEB-./U/70.441
XRCE/C/48.891
IIT-T./U/86.729
BUTkn./U/72.396
UWB/U/67.089
NLANG./C/45.724
NileT./U/85.448
AUEB-./U/71.537
UWB/C/66.906
TGB/C/43.081*
IHS-R./U/83.935
BUTkn./C/71.494
GTI/U/66.553
bunji/U/41.113
ECNU/U/83.586
SYSU/U/70.869
Senti./C/66.545
UWB/C/41.108
AUEB-./U/83.236
XRCE/C/68.701
bunji/U/64.882
UWB/U/41.088
INSIG./U/82.072
UWB/U/68.203
NLANG./C/63.861
DMIS/U/39.796
UWB/C/81.839
INSIG./U/68.108
DMIS/C/63.495
DMIS/C/38.976
UWB/U/81.723
ESI/U/67.979
XRCE/C/61.98
basel./C/37.795
SeemGo/C/81.141
UWB/C/67.817
AUEB-./C/61.552
IHS-R./U/35.608
bunji/U/81.024
GTI/U/67.714
UWate./U/57.067
IHS-R./U/34.864
TGB/C/80.908*
AUEB-./C/67.35
KnowC./U/56.816*
UWate./U/34.536
ECNU/C/80.559
NLANG./C/65.563
TGB/C/55.054*
SeemGo/U/30.667
UWate./U/80.326
LeeHu./C/65.455
BUAP/U/50.253
BUAP/U/18.428
INSIG./C/80.21
TGB/C/63.919*
basel./C/44.071
DMIS/C/79.977
IIT-T./U/63.051
IHS-R./U/43.808
DMIS/U/79.627
DMIS/U/62.583
IIT-T./U/42.603
IHS-R./U/78.696
DMIS/C/61.754
SeemGo/U/34.332
Senti./U/78.114
IIT-T./C/61.227
LeeHu./C/78.114
bunji/U/60.145
basel./C/76.484
basel./C/59.928
bunji/C/76.251
UFAL/U/59.3
SeemGo/U/72.992
INSIG./C/58.303
AKTSKI/U/71.711
IHS-R./U/55.034
COMMI./C/70.547
IHS-R./U/53.149
SNLP/U/69.965
SeemGo/U/50.737
GTI/U/69.965
UWate./U/49.73
CENNL./C/63.912
CENNL./C/40.578
BUAP/U/60.885
BUAP/U/37.29
Table 3: English REST results for SB1.
SB2-Slot3: For each text-level aspect category c
the baseline traverses the predicted sentence-level
tuples of the same category returned by the respective SB1 baseline and counts the polarity labels (positive, negative, neutral). Finally, the polarity label
with the highest frequency is assigned to the textlevel category c. If there are no sentence-level tuples
for the same c, the polarity label is determined based
on all tuples regardless of c.
The baseline systems and evaluation scripts are
implemented in Java and are available for download from the SE-ABSA16 website13. The LibSVM
package14 is used for SVM
training and prediction. The scores of the baselines
13 
php?id=data-and-tools
14 
in the test datasets are presented in Section 6 along
with the system scores.
Participation
The task attracted in total 245 submissions from 29
teams. The majority of the submissions (216 runs)
were for SB1. The newly introduced SB2 attracted
29 submissions from 5 teams in 2 languages (en and
sp). Most of the submissions (168) were runs for
the rest domain. This was expected, mainly for two
reasons; first, the rest classification schema is less
fine-grained (complex) compared to the other domains (e.g., lapt). Secondly, this domain was supported for 6 languages enabling also multilingual or
language-agnostic approaches. The remaining submissions were distributed as follows: 54 in lapt, 12
in phns, 7 in came and 4 in hote.
{Slot1,Slot2}
GTI/U/70.588
GTI/C/68.515
TGB/C/41.219*
IIT-T./U/83.582
GTI/C/70.027
GTI/U/68.387
basel./C/36.379
TGB/C/82.09*
TGB/C/63.551*
IIT-T./U/64.338
UWB/C/81.343
UWB/C/61.968
TGB/C/55.764*
INSIG./C/79.571
INSIG./C/61.37
basel./C/51.914
basel./C/77.799
IIT-T./U/59.899
IIT-T./C/59.062
UFAL/U/58.81
basel./C/54.686
XRCE/C/61.207
IIT-T./U/66.667
XRCE/C/47.721
XRCE/C/78.826
IIT-T./U/57.875
XRCE/C/65.316
basel./C/33.017
UWB/C/75.262
IIT-T./C/57.033
basel./C/45.455
UWB/C/74.319
INSIG./C/53.592
INSIG./C/73.166
basel./C/52.609
IIT-T./U/72.222
UFAL/U/49.928
basel./C/67.4
UFAL/U/64.825
basel./C/49.308
basel./C/39.441
MayAnd/U/77.923
INSIG./C/62.802
Danii./U/33.472
Danii./U/22.591
INSIG./C/75.077
IIT-T./C/62.689
Danii./C/30.618
Danii./C/22.107
IIT-T./U/73.615
IIT-T./C/58.196
Danii./U/73.308
basel./C/55.882
Danii./C/72.538
Danii./C/39.601
basel./C/71
Danii./U/38.692
TGB/C/60.153*
IIT-T./U/56.986
TGB/C/45.167*
TGB/C/77.814*
INSIG./C/56
TGB/C/51.775*
basel./C/30.916
IIT-T./U/76.998
IIT-T./U/55.247
basel./C/50.64
INSIG./C/75.041
IIT-T./C/54.98
basel./C/69.331
UFAL/U/53.876
basel./C/42.816
UFAL/U/61.029
basel./C/41.86
basel./C/28.152
IIT-T./U/84.277
basel./C/58.896
INSIG./C/74.214
IIT-T./U/56.627
basel./C/72.327
IIT-T./C/55.728
INSIG./C/49.123
INSIG./C/52.114
basel./C/30.978
basel./C/18.806
INSIG./C/82.719
UFAL/U/47.302
IIT-T./U/81.72
basel./C/40.336
basel./C/76.421
Table 4: REST and HOTE results for SB1.
An interesting observation is that, unlike SE-
ABSA15, Slot1 (aspect category detection) attracted
significantly more submissions than Slot2 (OTE extraction); this may indicate a shift towards conceptlevel approaches. Regarding participation per language, the majority of the submissions (156/245)
were for en; see more information in Table 5. Most
teams (20) submitted results only for one language
(18 for en and 2 for ru). Of the remaining teams,
3 submitted results for 2 languages, 5 teams submitted results for 3-7 languages, while only one team
participated in all languages.
Evaluation Results
The evaluation results are presented in Tables 3
(SB1: rest-en), 4 (SB1: rest-es, fr, ru, du, tu
& hote-ar), 6 (SB1: lapt, came, phns), and 7
Each participating team was allowed to
submit up to two runs per slot and domain in each
phase; one constrained (C), where only the provided
training data could be used, and one unconstrained
(U), where other resources (e.g., publicly available
15No submissions were made for sb3-muse-fr & sb1-telctu.
Submissions
Table 5: Number of participating teams and submitted runs per
lexica) and additional data of any kind could be used
for training. In the latter case, the teams had to report the resources used. Delayed submissions (i.e.,
runs submitted after the deadline and the release of
the gold annotations) are marked with “*”.
As revealed by the results, in both SB1 and SB2
the majority of the systems surpassed the baseline
by a small or large margin and, as expected, the unconstrained systems achieved better results than the
constrained ones. In SB1, the teams with the highest scores for Slot1 and Slot2 achieved similar F-1
scores (see Table 3) in most cases (e.g., en/rest,
es/rest, du/rest, fr/rest), which shows that the
two slots have a similar level of difficulty. However, as expected, the {Slot1, Slot2} scores were significantly lower since the linking of the target expressions to the corresponding aspects is also required. The highest scores in SB1 for all slots (Slot1,
Slot2, {Slot1, Slot2}, Slot3) were achieved in the
en/rest; this is probably due to the high participation
and to the lower complexity of the rest annotation
schema compared to the other domains. If we compare the results for SB1 and SB2, we notice that the
SB2 scores for Slot1 are significantly higher (e.g.,
en/lapt, en/rest, es/rest) even though the respective annotations are for the same (or almost the same)
set of texts. This is due to the fact that it is easier to
identify whether a whole text discusses an aspect c
than finding all the sentences in the text discussing
c . On the other hand, for Slot3, the SB2 scores are
lower (e.g., en/rest, es/rest, ru/rest, en/lapt) than
the respective SB1 scores. This is mainly because an
aspect may be discussed at different points in a text
and often with different sentiment. In such cases a
system has to identify the dominant sentiment, which
NLANG./U/51.937
IIT-T./U/82.772
AUEB-./U/49.105
INSIG./U/78.402
SYSU/U/49.076
ECNU/U/78.152
BUTkn./U/48.396
IHS-R./U/77.903
UWB/C/47.891
NileT./U/77.403
BUTkn./C/47.527
AUEB-./U/76.904
UWB/U/47.258
LeeHu./C/75.905
NileT./U/47.196
Senti./U/74.282
NLANG./C/46.728
INSIG./C/74.282
INSIG./U/45.863
UWB/C/73.783
AUEB-./C/45.629
UWB/U/73.783
IIT-T./U/43.913
SeemGo/C/72.16
LeeHu./C/43.754
UWate./U/71.286
IIT-T./C/42.609
bunji/C/70.287
SeemGo/U/41.499
bunji/U/70.162
INSIG./C/41.458
ECNU/C/70.037
bunji/U/39.586
basel./C/70.037
IHS-R./U/39.024
COMMI./C/67.541
basel./C/37.481
GTI/U/67.291
UFAL/U/26.984
BUAP/U/62.797
CENNL./C/26.908
CENNL./C/59.925
BUAP/U/26.787
SeemGo/U/40.824
UWB/C/36.345
SeemGo/C/80.457
INSIG./C/25.581
INSIG./C/78.17
basel./C/18.434
UWB/C/77.755
SeemGo/U/17.757
basel./C/74.428
SeemGo/U/73.181
UWB/C/22.548
SeemGo/C/73.346
basel./C/17.03
INSIG./C/72.401
INSIG./C/16.286
UWB/C/72.023
SeemGo/U/10.43
basel./C/70.132
SeemGo/U/65.406
INSIG./C/45.551
INSIG./C/83.333
IIT-T./U/45.443
IIT-T./U/82.576
IIT-T./C/45.047
basel./C/80.808
basel./C/33.55
Table 6: LAPT, CAME, and PHNS results for SB1.
usually is not trivial.
Conclusions
In its third year, the SemEval ABSA task provided
19 training and 20 testing datasets, from 7 domains
and 8 languages, attracting 245 submissions from
29 teams. The use of the same annotation guidelines for domains addressed in different languages
gives the opportunity to experiment also with crosslingual or language-agnostic approaches. In addition, SE-ABSA16 included for the first time a text-
GTI/U/83.995
UWB/U/81.931
UWB/C/80.965
ECNU/U/81.436
UWB/U/80.163
UWB/C/80.941
bunji/U/79.777
ECNU/C/78.713
basel./C/78.711
basel./C/74.257
SYSU/U/68.841
bunji/U/70.545
SYSU/U/68.841
bunji/C/66.584
GTI/U/64.109
GTI/C/84.192
UWB/C/77.185
GTI/U/84.044
basel./C/74.548
basel./C/74.548
UWB/C/73.657
basel./C/84.792
basel./C/70.6
basel./C/84.792
basel./C/70.6
basel./C/70.323
basel./C/73.228
basel./C/72.642
basel./C/57.407
basel./C/42.757
basel./C/73.216
UWB/C/60.45
ECNU/U/75.046
UWB/U/59.721
UWB/U/75.046
bunji/U/54.723
UWB/C/74.495
basel./C/52.685
basel./C/73.028
SYSU/U/48.889
ECNU/C/67.523
SYSU/U/48.889
bunji/C/62.202
bunji/U/60
GTI/U/58.349
Table 7: Results for SB2.
level subtask.
Future work will address the creation of datasets in more languages and domains and
the enrichment of the annotation schemas with other
types of SA-related information like topics, events
and figures of speech (e.g., irony, metaphor).
Acknowledgments
The authors are grateful to all the annotators and
contributors for their valuable support to the task:
Konstantina Papanikolaou, Juli Bakagianni, Omar
Qwasmeh, Nesreen Alqasem, Areen Magableh, Saja
Alzoubi, Bashar Talafha, Zekui Li, Binbin Li,
Shengqiu Li, Aaron Gevaert, Els Lefever, Cécile
Richart, Pavel Blinov, Maria Shatalova, M. Teresa
Martín-Valdivia, Pilar Santolaria, Fatih Samet Çetin,
Ezgi Yıldırım, Can Özbey, Leonidas Valavanis,
Stavros Giorgis, Dionysios Xenos, Panos Theodorakakos, and Apostolos Rousas. The work described
in this paper is partially funded by the projects EOX
GR07/3712 and “Research Programs for Excellence
2014-2016 / CitySense-ATHENA R.I.C.”. The Arabic track was partially supported by the Jordan University of Science and Technology, Research Grant
The Dutch track has been
partly funded by the PARIS project (IWT-SBO-
Nr. 110067). The French track was partially supported by the French National Research Agency under project ANR-12-CORD-0015/TransRead. The
Russian track was partially supported by the Russian Foundation for Basic Research (RFBR) according to the research projects No. 14-07-00682a, 16-
07-00342a, and No. 16-37-00311mol_a. The Spanish track has been partially supported by a grant
from the Ministerio de Educación, Cultura y Deporte
(MECD - scholarship FPU014/00983) and REDES
project (TIN2015-65136-C2-1-R) from the Ministerio de Economía y Competitividad.
The Turkish track was partially supported by TUBITAK-
TEYDEB (The Scientific and Technological Research Council of Turkey – Technology and Innovation Funding Programs Directorate) project (grant
number: 3140671).