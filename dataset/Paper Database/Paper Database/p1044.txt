Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1103–1114
Copenhagen, Denmark, September 7–11, 2017. c⃝2017 Association for Computational Linguistics
Tensor Fusion Network for Multimodal Sentiment Analysis
Amir Zadeh†, Minghai Chen†
Language Technologies Institute
Carnegie Mellon University
{abagherz,minghail}@cs.cmu.edu
Soujanya Poria
Temasek Laboratories,
NTU, Singapore
 
Erik Cambria
School of Computer Science and
Engineering, NTU, Singapore
 
Louis-Philippe Morency
Language Technologies Institute
Carnegie Mellon University
 
Multimodal sentiment analysis is an increasingly popular research area, which
extends the conventional language-based
deﬁnition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper,
we pose the problem of multimodal sentiment analysis as modeling intra-modality
and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion
Network, which learns both such dynamics end-to-end. The proposed approach is
tailored for the volatile nature of spoken
language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-ofthe-art approaches for both multimodal and
unimodal sentiment analysis.
Introduction
Multimodal sentiment analysis is
an increasingly popular area of affective computing research that focuses on
generalizing text-based sentiment analysis to opinionated videos, where three communicative modalities are present: language (spoken words), visual
(gestures), and acoustic (voice).
This generalization is particularly vital to part
of the NLP community dealing with opinion mining and sentiment analysis 
since there is a growing trend of sharing opinions
in videos instead of text, specially in social media
(Facebook, YouTube, etc.). The central challenge
in multimodal sentiment analysis is to model the
inter-modality dynamics: the interactions between
† means equal contribution
Figure 1: Unimodal, bimodal and trimodal interaction in multimodal sentiment analysis.
language, visual and acoustic behaviors that change
the perception of the expressed sentiment.
Figure 1 illustrates these complex inter-modality
dynamics. The utterance “This movie is sick” can
be ambiguous (either positive or negative) by itself,
but if the speaker is also smiling at the same time,
then it will be perceived as positive. On the other
hand, the same utterance with a frown would be perceived negatively. A person speaking loudly “This
movie is sick” would still be ambiguous. These
examples are illustrating bimodal interactions. Examples of trimodal interactions are shown in Figure 1 when loud voice increases the sentiment to
strongly positive. The complexity of inter-modality
dynamics is shown in the second trimodal example where the utterance “This movie is fair” is still
weakly positive, given the strong inﬂuence of the
word “fair”.
A second challenge in multimodal sentiment
analysis is efﬁciently exploring intra-modality dynamics of a speciﬁc modality (unimodal interaction).
Intra-modality dynamics are particularly
challenging for the language analysis since multimodal sentiment analysis is performed on spoken language. A spoken opinion such as “I think
it was alright ...Hmmm ...let me think ...yeah
...no ...ok yeah” almost never happens in written text. This volatile nature of spoken opinions,
where proper language structure is often ignored,
complicates sentiment analysis. Visual and acoustic modalities also contain their own intra-modality
dynamics which are expressed through both space
Previous works in multimodal sentiment analysis
does not account for both intra-modality and intermodality dynamics directly, instead they either perform early fusion (a.k.a., feature-level fusion) or
late fusion (a.k.a., decision-level fusion). Early fusion consists in simply concatenating multimodal
features mostly at input level . This
fusion approach does not allow the intra-modality
dynamics to be efﬁciently modeled. This is due to
the fact that inter-modality dynamics can be more
complex at input level and can dominate the learning process or result in overﬁtting. Late fusion,
instead, consists in training unimodal classiﬁers independently and performing decision voting . This prevents the
model from learning inter-modality dynamics in
an efﬁcient way by assuming that simple weighted
averaging is a proper fusion approach.
In this paper, we introduce a new model, termed
Tensor Fusion Network (TFN), which learns both
the intra-modality and inter-modality dynamics
end-to-end. Inter-modality dynamics are modeled
with a new multimodal fusion approach, named
Tensor Fusion, which explicitly aggregates unimodal, bimodal and trimodal interactions. Intramodality dynamics are modeled through three
Modality Embedding Subnetworks, for language,
visual and acoustic modalities, respectively.
In our extensive set of experiments, we show (a)
that TFN outperforms previous state-of-the-art approaches for multimodal sentiment analysis, (b) the
characteristics and capabilities of our Tensor Fusion approach for multimodal sentiment analysis,
and (c) that each of our three Modality Embedding Subnetworks (language, visual and acoustic)
are also outperforming unimodal state-of-the-art
unimodal sentiment analysis approaches.
Related Work
Sentiment Analysis is a well-studied research area
in NLP . Various approaches
have been proposed to model sentiment from language, including methods that focus on opinionated
words , n-grams and
language models , sentiment compositionality and dependency-based analysis , and distributional
representations for sentiment .
Multimodal Sentiment Analysis is an emerging research area that integrates verbal and
nonverbal behaviors into the detection of user
sentiment.
There exist several multimodal
annotations,
newly-introduced
dataset , as well as other
datasets including ICT-MMMO , YouTube , and
MOUD , however CMU-
MOSI is the only English dataset with utterancelevel sentiment labels. The newest multimodal sentiment analysis approaches have used deep neural
networks, including convolutional neural networks
(CNNs) with multiple-kernel learning , SAL-CNN which learns
generalizable features across speakers, and support
vector machines (SVMs) with a multimodal dictionary .
Audio-Visual Emotion Recognition is closely
tied to multimodal sentiment analysis . Both audio and visual features have been
shown to be useful in the recognition of emotions . Using facial expressions and audio cues jointly has been the focus of
many recent studies .
Multimodal Machine Learning has been a growing trend in machine learning research that is
closely tied to the studies in this paper. Creative
and novel applications of using multiple modalities have been among successful recent research
directions in machine learning .
CMU-MOSI Dataset
Multimodal Opinion Sentiment Intensity (CMU-
MOSI) dataset is an annotated dataset of video
Negative Weakly
Neutral Weakly
Positive Highly
Percentage of Sentiment Degrees
Utterance Size
Highly Positive
Weakly Positive
Weakly Negative
Highly Negative
Number of Opinion Segments
Figure 2: Distribution of sentiment across different opinions (left) and opinion sizes (right) in CMU-MOSI.
opinions from YouTube movie reviews . Annotation of sentiment has closely
followed the annotation scheme of the Stanford
Sentiment Treebank , where
sentiment is annotated on a seven-step Likert scale
from very negative to very positive.
whereas the Stanford Sentiment Treebank is segmented by sentence, the CMU-MOSI dataset is
segmented by opinion utterances to accommodate
spoken language where sentence boundaries are not
as clear as text. There are 2199 opinion utterances
for 93 distinct speakers in CMU-MOSI. There are
an average 23.2 opinion segments in each video.
Each video has an average length of 4.2 seconds.
There are a total of 26,295 words in the opinion
utterances. These utterance are annotated by ﬁve
Mechanical Turk annotators for sentiment. The
ﬁnal agreement between the annotators is high in
terms of Krippendorf’s alpha α = 0.77. Figure 2
shows the distribution of sentiment across different
opinions and different opinion sizes. CMU-MOSI
dataset facilitates three prediction tasks, each of
which we address in our experiments: 1) Binary
Sentiment Classiﬁcation 2) Five-Class Sentiment
Classiﬁcation (similar to Stanford Sentiment Treebank ﬁne-grained classiﬁcation with seven scale
being mapped to ﬁve) and 3) Sentiment Regression in range [−3, 3]. For sentiment regression, we
report Mean-Absolute Error (lower is better) and
correlation (higher is better) between the model
predictions and regression ground truth.
Tensor Fusion Network
Our proposed TFN consists of three major components: 1) Modality Embedding Subnetworks take as
input unimodal features, and output a rich modality
embedding. 2) Tensor Fusion Layer explicitly models the unimodal, bimodal and trimodal interactions
using a 3-fold Cartesian product from modality embeddings. 3) Sentiment Inference Subnetwork is a
network conditioned on the output of the Tensor
Fusion Layer and performs sentiment inference.
Depending on the task from Section 3 the network
output changes to accommodate binary classiﬁcation, 5-class classiﬁcation or regression. Input to
the TFN is an opinion utterance which includes
three modalities of language, visual and acoustic.
The following three subsections describe the TFN
subnetworks and their inputs in detail.
Modality Embedding Subnetworks
Spoken Language Embedding Subnetwork:
Spoken text is different than written text (reviews,
tweets) in compositionality and grammar. We revisit the spoken opinion: “I think it was alright
...Hmmm ...let me think ...yeah ...no ...ok
yeah”. This form of opinion rarely happens in
written language but variants of it are very common in spoken language. The ﬁrst part conveys the
actual message and the rest is speaker thinking out
loud eventually agreeing with the ﬁrst part. The
key factor in dealing with this volatile nature of
spoken language is to build models that are capable
of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts
of speech.
Our proposed approach to deal with challenges
of spoken language is to learn a rich representation of spoken words at each word interval and
use it as input to a fully connected deep network
(Figure 3). This rich representation for ith word
contains information from beginning of utterance
through time, as well as ith word. This way as the
model is discovering the meaning of the utterance
through time, if it encounters unusable information
in word i + 1 and arbitrary number of words after,
the representation up until i is not diluted or lost.
Also, if the model encounters usable information
again, it can recover by embedding those in the long
short-term memory (LSTM). The time-dependent
128 ReLU 128 ReLU
Figure 3: Spoken Language Embedding Subnetwork (Ul)
encodings are usable by the rest of the pipeline by
simply focusing on relevant parts using the nonlinear afﬁne transformation of time-dependent embeddings which can act as a dimension reducing
attention mechanism. To formally deﬁne our proposed Spoken Language Embedding Subnetwork
(Ul), let l = {l1, l2, l3, . . . , lTl; lt ∈R300}, where
Tl is the number of words in an utterance, be the
set of spoken words represented as a sequence of
300-dimensional GloVe word vectors .
A LSTM network with a forget gate is used
to learn time-dependent language representations
hl = {h1, h2, h3, . . . , hTl; ht ∈R128} for words
according to the following LSTM formulation.
ct = f ⊙ct−1 + i ⊙m
ht = o ⊗tanh(ct)
hl = [h1; h2; h3; . . . ; hTl]
hl is a matrix of language representations formed
from concatenation of h1, h2, h3, . . . hTl. hl is then
used as input to a fully-connected network that
generates language embedding zl:
zl = Ul(l; Wl) ∈R128
where Wl is the set of all weights in the Ul network (including Wld, Wle,Wlfc, and blfc), σ is the
sigmoid function.
Visual Embedding Subnetwork: Since opinion videos consist mostly of speakers talking to
the audience through close-up camera, face is the
most important source of visual information. The
speaker’s face is detected for each frame (sampled
at 30Hz) and indicators of the seven basic emotions
(anger, contempt, disgust, fear, joy, sadness, and
surprise) and two advanced emotions (frustration
and confusion) are extracted using
FACET facial expression analysis framework1. A
set of 20 Facial Action Units ,
indicating detailed muscle movements on the face,
are also extracted using FACET. Estimates of head
position, head rotation, and 68 facial landmark locations also extracted per frame using OpenFace .
Let the visual features ˆvj = [v1
j , . . . , vp
for frame j of utterance video contain the set of p
visual features, with Tv the number of total video
frames in utterance. We perform mean pooling
over the frames to obtain the expected visual features v = [E[v1], E[v2], E[v3], . . . , E[vl]].
then used as input to the Visual Embedding Subnetwork Uv. Since information extracted using
FACET from videos is rich, using a deep neural
network would be sufﬁcient to produce meaningful
embeddings of visual modality. We use a deep neural network with three hidden layers of 32 ReLU
units and weights Wv. Empirically we observed
that making the model deeper or increasing the
number of neurons in each layer does not lead to
better visual performance. The subnetwork output
provides the visual embedding zv:
zv = Uv(v; Wv) ∈R32
Acoustic Embedding Subnetwork: For each
opinion utterance audio, a set of acoustic features are extracted using COVAREP acoustic analysis framework , including
12 MFCCs, pitch tracking and Voiced/UnVoiced
segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH)
method ), glottal
source parameters ), peak slope parameters , maxima dispersion quotients (MDQ) , and estimations of the Rd shape
parameter of the Liljencrants-Fant (LF) glottal
model . These extracted features capture different characteristics of
human voice and have been shown to be related to
emotions .
1 
Language(zl)
Acoustic(za)
Visual(zv)
Early Fusion
Language(zl)
Acoustic(za)
Visual(zv)
Tensor Fusion
zl ⊗zv ⊗za
Figure 4: Left: Commonly used early fusion (multimodal concatenation). Right: Our proposed tensor
fusion with three types of subtensors: unimodal, bimodal and trimodal.
For each opinion segment with Ta audio frames
(sampled at 100Hz; i.e., 10ms), we extract the set
of q acoustic features ˆaj = [a1
j, . . . , aq
audio frame j in utterance. We perform mean
pooling per utterance on these extracted acoustic features to obtain the expected acoustic features a = [E[a1], E[a2], E[a3], . . . , E[q]]. Here, a
is the input to the Audio Embedding Subnetwork
Ua. Since COVAREP also extracts rich features
from audio, using a deep neural network is sufﬁcient to model the acoustic modality. Similar to
Uv, Ua is a network with 3 layers of 32 ReLU units
with weights Wa.
Here, we also empirically observed that making the model deeper or increasing the number
of neurons in each layer does not lead to better
performance. The subnetwork produces the audio
embedding za:
za = Ua(a; Wa) ∈R32
Tensor Fusion Layer
While previous works in multimodal research has
used feature concatenation as an approach for multimodal fusion, we aim to build a fusion layer in TFN
that disentangles unimodal, bimodal and trimodal
dynamics by modeling each of them explicitly. We
call this layer Tensor Fusion, which is deﬁned as
the following vector ﬁeld using three-fold Cartesian product:
(zl, zv, za) | zl ∈
The extra constant dimension with value 1 generates the unimodal and bimodal dynamics. Each
neural coordinate (zl, zv, za) can be seen as a 3-D
point in the 3-fold Cartesian space deﬁned by the
language, visual, and acoustic embeddings dimensions [zl1]T , [zv1]T , and [za1]T .
This deﬁnition is mathematically equivalent to a
differentiable outer product between zl, the visual
representation zv, and the acoustic representation
Here ⊗indicates the outer product between vectors
and zm ∈R129×33×33 is the 3D cube of all possible combination of unimodal embeddings with
seven semantically distinct subregions in Figure 4.
The ﬁrst three subregions zl, zv, and za are unimodal embeddings from Modality Embedding Subnetworks forming unimodal interactions in Tensor
Fusion. Three subregions zl ⊗zv, zl ⊗za, and
zv ⊗za capture bimodal interactions in Tensor
Fusion. Finally, zl ⊗zv ⊗za captures trimodal
interactions.
Early fusion commonly used in multimodal research dealing with language, vision and audio,
can be seen as a special case of Tensor Fusion with
only unimodal interactions. Since Tensor Fusion
is mathematically formed by an outer product, it
has no learnable parameters and we empirically
observed that although the output tensor is high
dimensional, chances of overﬁtting are low.
We argue that this is due to the fact that the output neurons of Tensor Fusion are easy to interpret
and semantically very meaningful (i.e., the manifold that they lie on is not complex but just high
dimensional). Thus, it is easy for the subsequent
layers of the network to decode the meaningful
information.
Sentiment Inference Subnetwork
After Tensor Fusion layer, each opinion utterance
can be represented as a multimodal tensor zm. We
use a fully connected deep neural network called
Sentiment Inference Subnetwork Us with weights
Ws conditioned on zm. The architecture of the network consists of two layers of 128 ReLU activation
units connected to decision layer. The likelihood
function of the Sentiment Inference Subnetwork
is deﬁned as follows, where φ is the sentiment
prediction:
p(φ | zm; Ws) = arg max
Us(zm; Ws)
In our experiments, we use three variations of the
Us network. The ﬁrst network is trained for binary
sentiment classiﬁcation, with a single sigmoid output neuron using binary cross-entropy loss. The
second network is designed for ﬁve-class sentiment
classiﬁcation, and uses a softmax probability function using categorical cross-entropy loss. The third
network uses a single sigmoid output, using meansquarred error loss to perform sentiment regression.
Experiments
In this paper, we devise three sets of experiments
each addressing a different research question:
Experiment 1: We compare our TFN with previous state-of-the-art approaches in multimodal sentiment analysis.
Experiment 2: We study the importance of the
TFN subtensors and the impact of each individual
modality (see Figure 4). We also compare with the
commonly-used early fusion approach.
Experiment 3: We compare the performance
of our three modality-speciﬁc networks (language,
visual and acoustic) with state-of-the-art unimodal
approaches.
Section 5.4 describes our experimental methodology which is kept constant across all experiments.
Section 6 will discuss our results in more details
with a qualitative analysis.
Multimodal
Regression
Acc(%) MAE
↓0.23 ↑0.17
Comparison with state-of-the-art approaches for multimodal sentiment analysis. TFN
outperforms both neural and non-neural approaches
as shown by ∆SOTA.
E1: Multimodal Sentiment Analysis
In this section, we compare the performance of
TFN model with previously proposed multimodal
sentiment analysis models. We compare to the
following baselines:
C-MKL Convolutional
MKL-based model is a multimodal sentiment classiﬁcation model which uses a CNN to extract textual features and uses multiple kernel learning for
sentiment analysis. It is current SOTA (state of the
art) on CMU-MOSI.
SAL-CNN Select-Additive
Learning is a multimodal sentiment analysis model
that attempts to prevent identity-dependent information from being learned in a deep neural network.
We retrain the model for 5-fold cross-validation using the code provided by the authors on github.
 is a SVM
model trained on multimodal features using early
fusion. The model used in 
and also similarly use
SVM on multimodal concatenated features. We
also present the results of Random Forest RF-MD
to compare to another non-neural approach.
The results ﬁrst experiment are reported in Table 1. TFN outperforms previously proposed neural and non-neural approaches. This difference is
speciﬁcally visible in the case of 5-class classiﬁcation.
E2: Tensor Fusion Evaluation
Table 4 shows the results of our ablation study. The
ﬁrst three rows are showing the performance of
each modality, when no intermodality dynamics are
modeled. From this ﬁrst experiment, we observe
that the language modality is the most predictive.
Regression
TFNlanguage
TFNacoustic
TFNbimodal
TFNtrimodal
TFNnotrimodal
Table 2: Comparison of TFN with its subtensor
variants. All the unimodal, bimodal and trimodal
subtensors are important. TFN also outperforms
early fusion.
As a second set of ablation experiments, we test
our TFN approach when only the bimodal subtensors are used (TFNbimodal) or when only the trimodal subtensor is used (TFNbimodal). We observe
that bimodal subtensors are more informative when
used without other subtensors. The most interesting comparison is between our full TFN model
and a variant (TFNnotrimodal) where the trimodal
subtensor is removed (but all the unimodal and bimodal subtensors are present). We observe a big
improvement for the full TFN model, conﬁrming
the importance of the trimodal dynamics and the
need for all components of the full tensor.
We also perform a comparison with the early fusion approach (TFNearly) by simply concatenating
all three modality embeddings < zl, za, zv > and
passing it directly as input to Us. This approach
was depicted on the left side of Figure 4. When
looking at Table 4 results, we see that our TFN
approach outperforms the early fusion approach2.
E3: Modality Embedding Subnetworks
Evaluation
In this experiment, we compare the performance
of our Modality Embedding Networks with stateof-the-art approaches for language-based, visualbased and acoustic-based sentiment analysis.
Language Sentiment Analysis
We selected the following state-of-the-art approaches to include variety in their techniques,
2We also performed other comparisons with variants of the
early fusion model TFNearly where we increased the number
of parameters and neurons to replicate the numbers from our
TFN model. In all cases, the performances were similar to
TFNearly (and lower than our TFN model). Because of space
constraints, we could not include them in this paper.
Regression
Acc(%) MAE
(0.99) (0.59)
TFNlanguage
↓0.01 ↑0.03
Table 3: Language Sentiment Analysis. Comparison of with state-of-the-art approaches for language
sentiment analysis. ∆SOTA
language shows improvement.
based on dependency parsing (RNTN), distributional representation of text (DAN), and convolutional approaches (DynamicCNN). When possible,
we retrain them on the CMU-MOSI dataset (performances of the original pre-trained models are
shown in parenthesis in Table 3) and compare them
to our language only TFNlanguage.
RNTN The Recursive Neural Tensor Network is among the most well-known
sentiment analysis methods proposed for both binary and multi-class sentiment analysis that uses
dependency structure.
DAN The Deep Average Network approach is a simple but efﬁcient sentiment
analysis model that uses information only from
distributional representation of the words and not
from the compositionality of the sentences.
DynamicCNN DynamicCNN is among the state-of-the-art models
in text-based sentiment analysis which uses a convolutional architecture adopted for the semantic
modeling of sentences.
CMK-L, SAL-CNN-L and SVM-MD-L are
multimodal models from section using only language modality 5.1.
Results in Table 3 show that our model using
only language modality outperforms state-of-theart approaches for the CMU-MOSI dataset. While
previous models are well-studied and suitable models for sentiment analysis in written language, they
underperform in modeling the sentiment in spoken
language. We suspect that this underperformance is
due to: RNTN and similar approaches rely heavily
on dependency structure, which may not be present
Regression
Acc(%) MAE
↓0.11 ↑0.14
Table 4: Visual Sentiment Analysis. Comparison
with state-of-the-art approaches for visual sentiment analysis and emotion recognition. ∆SOTA
shows the improvement.
in spoken language; DAN and similar sentence embeddings approaches can easily be diluted by words
that may not relate directly to sentiment or meaning; D-CNN and similar convolutional approaches
rely on spatial proximity of related words, which
may not always be present in spoken language.
Visual Sentiment Analysis
We compare the performance of our models using
visual information (TFNvisual) with the following
well-known approaches in visual sentiment analysis and emotion recognition (retrained for sentiment analysis):
3DCNN a network using 3D CNN is trained using the face of the speaker.
Face of the speaker is extracted in every 6 frames
and resized to 64 × 64 and used as the input to the
proposed network.
CNN-LSTM is a
recurrent model that at each timestamp performs
convolutions over facial region and uses output to
an LSTM. Face processing is similar to 3DCNN.
LSTM-FA similar to both baselines above, information extracted by FACET is used every 6 frames
as input to an LSTM with a memory dimension of
100 neurons.
SAL-CNN-V, SVM-MD-V, CMKL-V, RF-V
use only visual modality in multimodal baselines
from Section 5.1.
The results in Table 5 show that Uv is able to
outperform state-of-the-art approaches on visual
sentiment analysis.
Acoustic Sentiment Analysis
We compare the performance of our models using
visual information (TFNacoustic) with the following
well-known approaches in audio sentiment analysis
Regression
Acc(%) MAE
TFNacoustic
↑0.02 ↑0.02
Table 5: Acoustic Sentiment Analysis. Comparison with state-of-the-art approaches for audio sentiment analysis and emotion recognition. ∆SOTA
shows improvement.
and emotion recognition (retrained for sentiment
analysis):
 uses an
LSTM on high-level audio features. We use the
same features extracted for Ua averaged over time
slices of every 200 intervals.
Adieu-Net is an endto-end approach for emotion recognition in audio
using directly PCM features.
SER-LSTM is a model that
uses recurrent neural networks on top of convolution operations on spectrogram of audio.
SAL-CNN-A, SVM-MD-A, CMKL-A, RF-A
use only acoustic modality in multimodal baselines
from Section 5.1.
Methodology
All the models in this paper are tested using ﬁve-fold cross-validation proposed by CMU-
MOSI . All of our experiments
are performed independent of speaker identity, as
no speaker is shared between train and test sets
for generalizability of the model to unseen speakers in real-world. The best hyperparameters are
chosen using grid search based on model performance on a validation set (using last 4 videos in
train fold). The TFN model is trained using the
Adam optimizer with the
learning rate 5e4. Uv and Ua, Us subnetworks are
regularized using dropout on all hidden layers with
p = 0.15 and L2 norm coefﬁcient 0.01. The train,
test and validation folds are exactly the same for
all baselines.
Qualitative Analysis
We analyze the impact of our proposed TFN multimodal fusion approach by comparing it with the
Spoken words +
acoustic and visual behaviors
“You can’t even tell funny jokes”
frowning expression
“I gave it a B”
+ smile expression +
excited voice
“But I must say those are some pretty
big shoes to ﬁll so I thought maybe
it has a chance” + headshake
“The only actor who can really sell
their lines is Erin Eckart” +
low-energy voice
Table 6: Examples from the CMU-MOSI dataset. The ground truth sentiment labels are between strongly
negative (-3) and strongly positive (+3). For each example, we show the prediction output of the three
unimodal models ( TFNacoustic, TFNvisual and TFNlanguage), the early fusion model TFNearly and our
proposed TFN approach. TFNearly seems to be mostly replicating language modality while our TFN
approach successfully integrate intermodality dynamics to predict the sentiment level.
early fusion approach TFNearly and the three unimodal models.
Table 6 shows examples taken
from the CMU-MOSI dataset. Each example is
described with the spoken words as well as the
acoustic and visual behaviors. The sentiment predictions and the ground truth labels range between
strongly negative (-3) and strongly positive (+3).
As a ﬁrst general observation, we observe that
the early fusion model TFNearly shows a strong
preference for the language modality and seems to
be neglecting the intermodality dynamics. We can
see this trend by comparing it with the language
unimodal model TFNlanguage. In comparison, our
TFN approach seems to capture more complex interaction through bimodal and trimodal dynamics
and thus performs better. Speciﬁcally, in the ﬁrst
example, the utterance is weakly negative where
the speaker is referring to lack of funny jokes in
the movie. This example contains a bimodal interaction where the visual modality shows a negative
expression (frowning) which is correctly captured
by our TFN approach.
In the second example, the spoken words are
ambiguous since the model has no clue what a B is
except a token, but the acoustic and visual modalities are bringing complementary evidences. Our
TFN approach correctly identify this trimodal interaction and predicts a positive sentiment. The third
example is interesting since it shows an interaction where language predicts a positive sentiment
but the strong negative visual behaviors bring the
ﬁnal prediction of our TFN approach almost to a
neutral sentiment. The fourth example shows how
the acoustic modality is also inﬂuencing our TFN
predictions.
Conclusion
We introduced a new end-to-end fusion method
for sentiment analysis which explicitly represents
unimodal, bimodal, and trimodal interactions between behaviors. Our experiments on the publiclyavailable CMU-MOSI dataset produced state-ofthe-art performance when compared against both
multimodal approaches.
Furthermore, our approach brings state-of-the-art results for languageonly, visual-only and acoustic-only multimodal sentiment analysis on CMU-MOSI.
Acknowledgments
This project was partially supported by Oculus research grant. We would like to thank the reviewers
for their valuable feedback.