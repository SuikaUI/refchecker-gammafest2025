Synthetic Data for Text Localisation in Natural Images
Ankush Gupta
Andrea Vedaldi
Andrew Zisserman
Dept. of Engineering Science, University of Oxford
{ankush,vedaldi,az}@robots.ox.ac.uk
In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic
images of text in clutter. This engine overlays synthetic text
to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the
synthetic images to train a Fully-Convolutional Regression
Network (FCRN) which efﬁciently performs text detection
and bounding-box regression at all locations and multiple
scales in an image. We discuss the relation of FCRN to the
recently-introduced YOLO detector, as well as other end-toend object detection systems based on deep learning. The
resulting detection network signiﬁcantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013
benchmark. Furthermore, it can process 15 images per second on a GPU.
1. Introduction
Text spotting, namely the ability to read text in natural scenes, is a highly-desirable feature in anthropocentric
applications of computer vision. State-of-the-art systems
such as achieved their high text spotting performance
by combining two simple but powerful insights. The ﬁrst
is that complex recognition pipelines that recognise text by
explicitly combining recognition and detection of individual characters can be replaced by very powerful classiﬁers
that directly map an image patch to words . The
second is that these powerful classiﬁers can be learned by
generating the required training data synthetically .
While successfully addressed the problem of recognising text given an image patch containing a word, the process of obtaining these patches remains suboptimal. The
pipeline combines general purpose features such as HoG
 , EdgeBoxes and Aggregate Channel Features 
and brings in text speciﬁc (CNN) features only in the later
stages, where patches are ﬁnally recognised as speciﬁc
words. This state of affair is highly undesirable for two
Synthetic Text in the Wild
real image
detected text
Figure 1. We propose a Fully-Convolutional Regression Network
(FCRN) for high-performance text recognition in natural scenes
(bottom) which detects text up to 45× faster than the current stateof-the-art text detectors and with better accuracy. FCRN is trained
without any manual annotation using a new dataset of synthetic
text in the wild. The latter is obtained by automatically adding text
to natural scenes in a manner compatible with the scene geometry
reasons. First, the performance of the detection pipeline
becomes the new bottleneck of text spotting: in recognition accuracy for correctly cropped words is 98% whereas
the end-to-end text spotting F-score is only 69% mainly due
to incorrect and missed word region proposals. Second, the
pipeline is slow and inelegant.
In this paper we propose improvements similar to to
the complementary problem of text detection. We make two
key contributions. First, we propose a new method for generating synthetic images of text that naturally blends text
in existing natural scenes, using off-the-shelf deep learning
and segmentation techniques to align text to the geometry
of a background image and respect scene boundaries. We
use this method to automatically generate a new synthetic
dataset of text in cluttered conditions (ﬁgure 1 (top) and
section 2). This dataset, called SynthText in the Wild (ﬁgure 2), is suitable for training high-performance scene text
detectors. The key difference with existing synthetic text
datasets such as the one of is that these only contains
Figure 2. Sample images from our synthetically generated scenetext dataset. Ground-truth word-level axis-aligned bounding boxes
are shown.
Train Test Train
ICDAR {11,13,15}
Table 1. Size of publicly available text localisation datasets —
ICDAR , the Street View Text (SVT) dataset .
Word numbers for the entry “ICDAR{11,13,15}” are from the IC-
DAR15 Robust Reading Competition’s Focused Scene Text Localisation dataset.
word-level image regions and are unsuitable for training detectors.
The second contribution is a text detection deep architecture which is both accurate and efﬁcient (ﬁgure 1
(bottom) and section 3). We call this a fully-convolutional
regression network. Similar to models such as the Fully-
Convolutional Networks (FCN) for image segmentation, it
performs prediction densely, at every image location. However, differently from FCN, the prediction is not just a class
label (text/not text), but the parameters of a bounding box
enclosing the word centred at that location. The latter idea
is borrowed from the You Look Only Once (YOLO) technique of Redmon et al. , but with convolutional regressors with a signiﬁcant boost to performance.
The new data and detector achieve state-of-the-art text
detection performance on standard benchmark datasets
(section 4) while being an order of magnitude faster than
traditional text detectors at test time (up to 15 images per
second on a GPU). We also demonstrate the importance of
verisimilitude in the dataset by showing that if the detector is trained on images with words inserted synthetically
that do not take account of the scene layout, then the detection performance is substantially inferior. Finally, due to
the more accurate detection step, end-to-end word recognition is also improved once the new detector is swapped in
for existing ones in state-of-the-art pipelines. Our ﬁndings
are summarised in section 5.
1.1. Related Work
Object Detection with CNNs. Our text detection network
draws primarily on Long et al.’s Fully-Convolutional network and Redmon et al.’s YOLO image-grid based
bounding-box regression network . YOLO is part of
a broad line of work on using CNN features for object category detection dating back to Girshick et al.’s Region-CNN
(R-CNN) framework combination of region proposals and CNN features. The R-CNN framework has three
broad stages — (1) generating object proposals, (2) extracting CNN feature maps for each proposal, and (3) ﬁltering
the proposals through class speciﬁc SVMs. Jaderberg et
al.’s text spotting method also uses a similar pipeline for
detection . Extracting feature maps for each region independently was identiﬁed as the bottleneck by Girshick et
al. in Fast R-CNN . They obtain 100× speed-up over
R-CNN by computing the CNN features once and pooling
them locally for each proposal; they also streamline the last
two stages of R-CNN into a single multi-task learning problem. This work exposed the region-proposal stage as the
new bottleneck. Lenc et al. drop the region proposal
stage altogether and use a constant set of regions learnt
through K-means clustering on the PASCAL VOC data.
Ren et al. also start from a ﬁxed set of proposal, but
reﬁned them prior to detection by using a Region Proposal
Network which shares weights with the later detection network and streamlines the multi-stage R-CNN framework.
Synthetic Data.
Synthetic datasets provide detailed
ground-truth annotations, and are cheap and scalable alternatives to annotating images manually. They have been
widely used to learn large CNN models — Wang et al. 
and Jaderberg et al. use synthetic text images to train
word-image recognition networks; Dosovitskiy et al. 
use ﬂoating chair renderings to train dense optical ﬂow regression networks. Detailed synthetic data has also been
used to learn generative models — Dosovitskiy et al. 
train inverted CNN models to render images of chairs, while
Yildirim et al. use deep CNN features trained on synthetic face renderings to regress pose parameters from face
Augmenting Single Images.
There is a large body of
work on inserting objects photo-realistically, and inferring
3D structure from single images — Karsch et al. develop an impressive semi-automatic method to render objects with correct lighting and perspective; they infer the
actual size of objects based on the technique of Criminisi
et al. . Hoiem et al. categorise image regions into
ground-plane, vertical plane or sky from a single image and
use it to generate “pop-ups” by decomposing the image into
planes . Similarly, we too decompose a single image
into local planar regions, but use instead the dense depth
prediction of Liu et al. .
gPb-UCM Segmentation
Text Regions
Sample Scene-Text Images
Figure 3. (Top, left to right): (1) RGB input image with no text instance. (2) Predicted dense depth map (darker regions are closer).
(3) Colour and texture gPb-UCM segments. (4) Filtered regions: regions suitable for text are coloured randomly; those unsuitable retain
their original image pixels. (Bottom): Four synthetic scene-text images with axis-aligned bounding-box annotations at the word level.
2. Synthetic Text in the Wild
Supervised training of large models such as deep CNNs,
which contain millions of parameters, requires a very signiﬁcant amount of labelled training data , which is expensive to obtain manually. Furthermore, as summarised in
Table 1, publicly available text spotting or detection datasets
are quite small. Such datasets are not only insufﬁcient to
train large CNN models, but also inadequate to represent the
space of possible text variations in natural scenes — fonts,
colours, sizes, positions. Hence, in this section we develop
a synthetic text-scene image generation engine for building
a large annotated dataset for text localisation.
Our synthetic engine (1) produces realistic scene-text
images so that the trained models can generalise to real
(non-synthetic) images, (2) is fully automated and, is (3)
fast, which enables the generation of large quantities of
data without supervision. The text generation pipeline can
be summarised as follows (see also Figure 3). After acquiring suitable text and image samples (section 2.1), the
image is segmented into contiguous regions based on local
colour and texture cues , and a dense pixel-wise depth
map is obtained using the CNN of (section 2.2). Then,
for each contiguous region a local surface normal is estimated. Next, a colour for text and, optionally, for its outline
is chosen based on the region’s colour (section 2.3). Finally, a text sample is rendered using a randomly selected
font and transformed according to the local surface orientation; the text is blended into the scene using Poisson image
editing . Our engine takes about half a second to generate a new scene-text image.
This method is used to generate 800,000 scene-text images, each with multiple instances of words rendered in different styles as seen in Figure 2. The dataset is available at:
 
2.1. Text and Image Sources
The synthetic text generation process starts by sampling
some text and a background image. The text is extracted
from the Newsgroup20 dataset in three ways — words,
lines (up to 3 lines) and paragraphs (up to 7 lines). Words
are deﬁned as tokens separated by whitespace characters,
lines are delimited by the newline character. This is a rich
dataset, with a natural distribution of English text interspersed with symbols, punctuation marks, nouns and numbers.
To favour variety, 8,000 background images are extracted from Google Image Search through queries related
to different objects/scenes and indoor/outdoor and natural/artiﬁcial locales. To guarantee that all text occurrences
are fully annotated, these images must not contain text of
their own (a limitation of the Street View Text is that
annotations are not exhaustive). Hence, keywords which
would recall a large amount of text in the images (e.g.
“street-sign”, “menu” etc.) are avoided; images containing
text are discarded through manual inspection.
2.2. Segmentation and Geometry Estimation
In real images, text tends to be contained in well deﬁned
regions (e.g. a sign). We approximate this constraint by requiring text to be contained in regions characterised by a
uniform colour and texture. This also prevents text from
crossing strong image discontinuities, which is unlikely to
Figure 4. Local colour/texture sensitive placement. (Left) Example image from the Synthetic text dataset. Notice that the text is restricted within the boundaries of the step in the street. (Right) For
comparison, the placement of text in this image does not respect
the local region cues.
occur in practice. Regions are obtained by thresholding the
gPb-UCM contour hierarchies at 0.11 using the efﬁcient
graph-cut implementation of . Figure 4 shows an example of text respecting local region cues.
In natural images, text tends to be painted on top of
surfaces (e.g. a sign or a cup).
In order to approximate
a similar effect in our synthetic data, the text is perspectively transformed according to local surface normals. The
normals are estimated automatically by ﬁrst predicting a
dense depth map using the CNN of for the regions
segmented above, and then ﬁtting a planar facet to it using
RANSAC .
Text is aligned to the estimated region orientations as follows: ﬁrst, the image region contour is warped to a frontalparallel view using the estimated plane normal; then, a rectangle is ﬁtted to the fronto-parallel region; ﬁnally, the text is
aligned to the larger side (“width”) of this rectangle. When
placing multiple instances of text in the same region, text
masks are checked for collision against each other to avoid
placing them on top of each other. Not all segmentation
regions are suitable for text placement — regions should
not be too small, have an extreme aspect ratio, or have surface normal orthogonal to the viewing direction; all such
regions are ﬁltered in this stage. Further, regions with too
much texture are also ﬁltered, where the degree of texture
is measured by the strength of third derivatives in the RGB
Discussion.
An alternative to using a CNN to estimate
depth, which is an error prone process, is to use a dataset
of RGBD images. We prefer to estimate an imperfect depth
map instead because: (1) it allows essentially any scene
type background image to be used, instead of only the
ones for which RGBD data are available, and (2) because
publicly available RGBD datasets such as NYUDv2 ,
B3DO , Sintel , and Make3D have several
limitations in our context: small size (1,500 images in
NYUDv21, 400 frames in Make3D, and a small number
of videos in B3DO and Sintel), low-resolution and motion blur, restriction to indoor images (in NYUDv2 and
B3DO), and limited variability in the images for videobased datasets (B3DO and Sintel).
2.3. Text Rendering and Image Composition
Once the location and orientation of text has been decided, text is assigned a colour. The colour palette for text
is learned from cropped word images in the IIIT5K word
dataset . Pixels in each cropped word images are partitioned into two sets using K-means, resulting in a colour
pair, with one colour approximating the foreground (text)
colour and the other the background. When rendering new
text, the colour pair whose background colour matches the
target image region the best (using L2-norm in the Lab
colour space) is selected, and the corresponding foreground
colour is used to render the text.
About 20% of the text instances are randomly chosen to
have a border. The border colour is chosen to be either the
same as foreground colour with its value channel increased
or decreased, or is chosen to be the mean of the foreground
and background colours.
To maintain the illumination gradient in the synthetic
text image, we blend the text on to the base image using
Poisson image editing , with the guidance ﬁeld deﬁned
as in their equation (12). We solve this efﬁciently using the
implementation provided by Raskar1
3. A Fast Text Detection Network
In this section we introduce our CNN architecture for
text detection in natural scenes. While existing text detection pipelines combine several ad-hoc steps and are slow,
we propose a detector which is highly accurate, fast, and
trainable end-to-end.
Let x denote an image. The most common approach for
CNN-based detection is to propose a number of image regions R that may contain the target object (text in our case),
crop the image, and use a CNN c = φ(cropR(x)) ∈{0, 1}
to score them as correct or not. This approach, which has
been popularised by R-CNN , works well but is slow as
it entails evaluating the CNN thousands of times per image.
An alternative and much faster strategy for object detection is to construct a ﬁxed ﬁeld of predictors (c, p) =
φuv(x), each of which specialises in predicting the presence
c ∈R and pose p = (x−u, y−v, w, h) of an object around
a speciﬁc image location (u, v). Here the pose parameters
(x, y) and (w, h) denote respectively the location and size
of a bounding box tightly enclosing the object. Each predictor φuv is tasked with predicting objects which occurs in
some ball (x, y) ∈Bρ(u, v) of the predictor location.
While this construction may sound abstract, it is actually
a common one, implemented for example by Implicit Shape
Models (ISM) and Hough voting . There a predictor φuv looks at a local image patch, centred at (u, v), and
 
Sine Transform.
tries to predict whether there is an object around (u, v), and
where the object is located relative to it.
In this paper we propose an extreme variant of Hough
voting, inspired by Fully-Convolutional Network (FCN) of
Long et al. and the You Look Only Once (YOLO) technique of Redmon et al. . In ISM and Hough voting,
individual predictions are aggregated across the image, in a
voting scheme. YOLO is similar, but avoids voting and uses
individual predictions directly; since this idea can accelerate detection, we adopt it here.
The other key conceptual difference between YOLO and
Hough voting is that in Hough voting predictors φuv(x) are
local and translation invariant, whereas in YOLO they are
not: First, in YOLO each predictor is allowed to pool evidence from the whole image, not just an image patch centred at (u, v). Second, in YOLO predictors at different locations (u, v) ̸= (u′, v′) are different functions φuv ̸= φu′v′
learned independently.
While YOLO’s approach allows the method to pick up
contextual information useful in detection of PASCAL or
ImageNet objects, we found this unsuitable for smaller and
more variable text occurrences. Instead, we propose here a
method which is in between YOLO and Hough voting. As
in YOLO, each detector φuv(x) still predicts directly object
occurrences, without undergoing an expensive voting accumulation process; however, as in Hough voting, detectors
φuv(x) are local and translation invariant, sharing parameters. We implement this ﬁeld of translation-invariant and local predictors as the output of the last layer of a deep CNN,
obtaining a fully-convolutional regression network (FCRN).
3.1. Architecture
This section describes the structure of the FCRN. First,
we describe the ﬁrst several layers of the architecture, which
compute text-speciﬁc image features. Then, we describe the
dense regression network built on top of these features and
ﬁnally its application at multiple scales.
Single-scale features.
Our architecture is inspired by
VGG-16 , using several layers of small dense ﬁlters;
however, we found that a much smaller model works just
as well and more efﬁciently for text. The architecture comprises nine convolutional layers, each followed by the Recti-
ﬁed Linear Unit non-linearity, and, occasionally, by a maxpooling layer. All linear ﬁlters have a stride of 1 sample,
and preserve the resolution of feature maps through zero
Max-pooling is performed over 2×2 windows
with a stride of 2 samples, therefore halving the feature
maps resolution.2
Class and bounding box prediction. The single-scale features terminate with a dense feature ﬁeld. Given that there
2The sequence of layers is as follows: 64 5×5 convolutional ﬁlters +
ReLU (CR-64-5×5), max pooling (MP), CR-128-5×5, MP, CR128-3×3,
CR-128-3×3-conv, MP, CR-256-3×3, CR-256-3×3, MP, CR-512-3×3,
CR-512-3×3, CR-512-5×5.
are four downsampling max-pooling layers, the stride of
these features is ∆= 16 pixels, each containing 512 feature
channels φf
uv(x) (we express uv in pixels for convenience).
Given the features φf
uv(x), we can now discuss the construction of the dense text predictors φuv(x) = φr
These predictors are implemented as a further seven 5 × 5
linear ﬁlters (C-7-5×5) φr
uv, each regressing one of seven
numbers: the object presence conﬁdence c, and up to six
object pose parameters p = (x−u, y−v, w, h, cos θ, sin θ)
where x, y, w, h have been discussed before and θ is the
bounding box rotation.
Hence, for an input image of size H×W, we obtain a
∆predictions, one each for an image cell of
size ∆×∆pixels. Each predictor is responsible for detecting a word if the word centre falls within the corresponding cell.3 YOLO is similar but operates at about half this
resolution; a denser predictor sampling is important to reduce collisions (multiple words falling in the same cell) and
therefore to increase recall (since at most one word can be
detected per cell). In practice, for a 224×224 image, we
obtain 14×14 cells/predictors
Multi-scale detection. Limited receptive ﬁeld of our convolutional ﬁlters prohibits detection of large text instances.
Hence, we get the detections at multiple down-scaled versions of the input image and merge them through nonmaximal suppression. In more detail, the input image is
scaled down by factors {1, 1/2, 1/4, 1/8} (scaling up is an
overkill as the baseline features are already computed very
densely). Then, the resulting detections are combined by
suppressing those with a lower score than the score of an
overlapping detection.
Training loss. We use a squared loss term for each of the
∆×7 outputs of the CNN as in YOLO . If a cell
does not contain a ground-truth word, the loss ignores all
parameters but c (text/no-text).
Comparison with YOLO. Our fully-convolutional regression network (FCRN) has 30× less parameters than the
YOLO network (which has ∼90% of the parameters in the
last two fully-connected layers). Due to its global nature,
standard YOLO must be retrained for each image size, including multiple scales, further increasing the model size
(while our model requires 44MB, YOLO would require
2GB). This makes YOLO not only harder to train, but also
less efﬁcient (2× slower that FCRN).
4. Evaluation
First, in section 4.1 we describe the text datasets on
which we evaluate our model. Next, we evaluate our model
on the text localisation task in section 4.2. In section 4.3,
to investigate which components of the synthetic data generation pipeline are important, we perform detailed ablation
3For regression, it was found beneﬁcial to normalise the pose parameters as follows: ¯p = ((x −u)/∆, (y −v)/∆, w/W, h/H, cos θ, sin θ).
PASCAL Eval
Huang 
Jaderberg 
77.2 87.5 69.2 70.6 76.2 86.7 68.0 69.3 53.6 62.8 46.8 55.4
76.8 88.2 68.0 76.8 88.5 67.8 24.7 27.7 22.3
(trained on SynthText)
77.3 89.2 68.4 72.3 76.7 88.9 67.5 71.4 53.6 58.9 49.1 56.1
75.5 87.5 66.4 75.5 87.9 66.3 24.7 27.8 22.3
Neumann 
68.7 73.1 64.7
Neumann 
72.3 79.3 66.4
Zhang 
FCRN single-scale
60.6 78.8 49.2 49.2 61.0 77.7 48.9 48.9 45.6 50.9 41.2 41.2
64.5 81.9 53.2 64.3 81.3 53.1 31.4 34.5 28.9
FCRN multi-scale
70.0 78.4 63.2 64.6 69.5 78.1 62.6 67.0 46.2 47.0 45.4 53.0
73.0 77.9 68.9 73.4 80.3 67.7 34.5 29.9 40.7
FCRN + multi-ﬁlt
78.7 95.3 67.0 67.5 78.0 94.8 66.3 66.7 56.3 61.5 51.9 54.1
78.0 94.5 66.4 78.0 94.8 66.3 25.5 26.8 24.3
FCRNall + multi-ﬁlt
84.7 94.3 76.9 79.6 84.2 93.8 76.4 79.6 62.4 65.1 59.9 75.0
82.3 91.5 74.8 83.0 92.0 75.5 26.7 26.2 27.4
Table 2. Comparison with previous methods on text localisation. Precision (P) and Recall (R) at maximum F-measure (F) and the maximum
recall (RM) are reported.
experiments. In section 4.4, we use the results from our
localisation model for end-to-end text spotting. We show
substantial improvements over the state-of-the-art in both
text localisation and end-to-end text spotting. Finally, in
section 4.5 we discuss the speed-up gained by using our
models for text localisation.
4.1. Datasets
We evaluate our text detection networks on standard
benchmarks: ICDAR 2011, 2013 datasets and the
Street View Text dataset . These datasets are reviewed
next and their statistics are given in Table 1.
SynthText in the Wild. This is a dataset of 800,000 training images generated using our synthetic engine from section 2. Each image has about ten word instances annotated
with character and word-level bounding-boxes.
ICDAR Datasets. The ICDAR datasets (IC011, IC013) are
obtained from the Robust Reading Challenges held in 2011
and 2013 respectively. They contain real world images of
text on sign boards, books, posters and other objects with
world-level axis-aligned bounding box annotations.
datasets largely contain the same images, but shufﬂe the test
and training splits. We do not evaluate on the more recent
ICDAR 2015 dataset as it is almost identical to the 2013
Street View Text. This dataset, abbreviated SVT, consists
of images harvested from Google Street View annotated
with word-level axis-aligned bounding boxes. SVT is more
challenging than the ICDAR data as it contains smaller and
lower resolution text. Furthermore, not all instances of text
are annotated. In practice, this means that precision is heavily underestimated in evaluation. Lexicons consisting of 50
distractor words along with the ground-truth words are provided for each image; we refer to testing on SVT with these
lexicons as SVT-50.
4.2. Text Localisation Experiments
We evaluate our detection networks to — (1) compare
the performance when applied to single-scale and multiple
down-scaled versions of the image and, (2) improve upon
the state-of-the-art results in text detection when used as
high-quality proposals.
Training. FCRN is trained on 800,000 images from our
SynthText in the Wild dataset. Each image is resized to a size
of 512×512 pixels. We optimise using SGD with momentum and batch-normalisation after every convolutional
layer (except the last one). We use mini-batches of 16 images each, set the momentum to 0.9, and use a weight-decay
of 5−4. The learning rate is set to 10−4 initially and is reduced to 10−5 when the training loss plateaus.
As only a small number (1-2%) of grid-cells contain text,
we weigh down the non-text probability error terms initially
by multiplying with 0.01; this weight is gradually increased
to 1 as the training progresses. Due to class imbalance, all
the probability scores collapse to zero if such a weighting
scheme is not used.
Inference. We get the class probabilities and bounding-box
predictions from our FCRN model. The predictions are ﬁltered by thresholding the class probabilities (at a threshold
t). Finally, multiple detections from nearby cells are suppressed using non-maximal suppression, whereby amongst
two overlapping detections the one with the lower probability is suppressed. In the following we ﬁrst give results for a
conservative threshold of t = 0.3, for higher precision, and
then relax this to t = 0.0 (i.e., all proposals accepted) for
higher recall.
Evaluation protocol. We report text detection performance
using two protocols commonly used in the literature —
(1) DetEval popularly used in ICDAR competitions
for evaluating localisation methods, and (2) PASCAL VOC
style intersection-over-union overlap method (≥0.5 IoU for
a positive detection).
Single & multi-scale detection. The “FCRN single-scale”
FCRN multi
scale 69.5
multi-filt
(SynthText)
Figure 5. Precision-Recall curves for various text detection methods on IC13.
The methods are: (1) multi-scale application of
FCRN (“FCRN-multi”); (2) The original curve of Jaderberg et
al. ; (3) Jaderberg et al. retrained on the SynthText in the
Wild dataset; and, (4) “FCRNall + multi-ﬁlt” methods. Maximum
F-score (F), Average Precision (AP) and maximum Recall (Rmax)
are also given. The gray curve at the bottom is of multi-scale detections from our FCRN network (max. recall = 85.9%), which is fed
into the multi-ﬁltering post-processing to get the reﬁned “FCRNall
+ multi-ﬁlt” detections.
entry in Table 2 shows the performance of our FCRN model
on the test datasets. The precision at maximum F-measure
of single-scale FCRN is comparable to the methods of Neuman et al. , while the recall is signiﬁcantly worse by
The “FCRN multi-scale” entry in Table 2 shows performance on multi-scale application of our network. This
method improves maximum recall by more than 12% over
the single-scale method and outperforms the methods of
Neumann et al.
Post-processing proposals. Current end-to-end text spotting (detection and recognition) methods boost
performance by combining detection with text recognition.
To further improve FCRN detections, we use the multiscale detections from FCRN as proposals and reﬁne them
by using the post-processing stages of Jaderberg et al. .
There are three stages: ﬁrst ﬁltering using a binary text/notext random-forest classiﬁer; second, regressing an improved bounding-box using a CNN; and third recognition
based NMS where the word images are recognised using
a large ﬁxed lexicon based CNN, and the detections are
merged through non-maximal suppression based on word
identities. Details are given in . We use code provided
by the authors for fair comparison.
We test this in two modes — (1) low-recall: where only
high-scoring (probability > 0.3) multi-scale FCRN detections are used (the threshold previously used in the singleand multi-scale inference). This typically yields less than
30 proposals. And, (2) high-recall: where all the multiscale FCRN detections (typically about a thousand in number) are used. Performance of these methods on text detecrecall
Colour/Texture
Perspective
distortion
Figure 6. Precision-Recall curves text localisation on the SVT
dataset using the model “FCRNall+multi-ﬁlt” when trained on increasingly sophisticated training sets (section 4.3).
tion are shown by the entries named “FCRN + multi-ﬁlt”
and “FCRNall + multi-ﬁlt” respectively in Table 2. Note
that the low-recall method achieves better than the state-ofthe-art performance on text detection, whereas high-recall
method signiﬁcantly improves the state-of-the-art with an
improvement of 6% in the F-measure for all the datasets.
Figure 5 shows the Precision-Recall curves for text detection on the IC13 dataset. Note the high recall (85.9%) of
the multi-scale detections output from FCRN before reﬁnement using the multi-ﬁltering post-processing. Also, note
the drastic increase in maximum recall (+10.3%) and in
Average Precision (+11.1%) for “FCRNall + multi-ﬁlt” as
compared to Jaderberg et al.
Further, to establish that the improvement in text detection is due to the new detection model, and not merely due
to the large size of our synthetic dataset, we trained Jaderberg et al.’s method on our SynthText in the Wild dataset
– in particular, the ACF component of their region proposal
stage.4 Figure 5 and Table 2 show that, even with 10× more
(synthetic) training data, Jaderberg et al.’s model improves
only marginally (+0.8% in AP, +2.1% in maximum recall).
A common failure mode is text in unusual fonts which
are not present in the training set.
The detector is also
confused by symbols or patterns of constant stroke width
which look like text, for example road-signs, stick ﬁgures
etc. Since the detector does not scale the image up, extremely small sized text instances are not detected. Finally,
words get broken into multiple instances or merged into one
instance due to large or small spacing between the characters.
4.3. Synthetic Dataset Evaluation
We investigate the contribution that the various stages
of the synthetic text-scene data generation pipeline bring to
4Their other region proposal method, EdgeBoxes, was not re-trained;
as it is learnt from low-level edge features from the Berkeley Segmentation
Dataset, which is not text speciﬁc.
IC11 IC11* IC13 SVT SVT-50
Wang & Wu 
Alsharif 
Neumann 
Jaderberg 
Jaderberg 
FCRN + multi-ﬁlt
(77.8) 54.7
FCRNall + multi-ﬁlt
(81.8) 55.7
Table 3. Comparison with previous methods on end-to-end text
spotting. Maximum F-measure% is reported. IC11* is evaluated
according to the protocol described in . Numbers in parenthesis are obtained if words containing non-alphanumeric characters
are not ignored – SVT does not have any of these.
localisation accuracy: We generate three synthetic training
datasets with increasing levels of sophistication, where the
text is (1) is placed at random positions within the image,
(2) restricted to the local colour and texture boundaries, and
(3) distorted perspectively to match the local scene depth
(while also respecting the local colour and texture boundaries as in (2) above). All other aspects of the datasets were
kept the same — e.g. the text lexicon, background images,
colour distribution.
Figure 6 shows the results on localisation on the SVT
dataset of our method “FCRNall+multi-ﬁlt”.
to random placement, restricting text to the local colour
and texture regions signiﬁcantly increases the maximum recall (+6.8%), AP (+3.85%), and the maximum F-measure
(+2.1%). Marginal improvements are seen with the addition of perspective distortion: +0.75% in AP, +0.55% in
maximum F-measure, and no change in the maximum recall. This is likely due to the fact that most text instances in
the SVT datasets are in a fronto-parallel orientation. Similar trends are observed with the ICDAR 2013 dataset, but
with more contained differences probably due to the fact
that ICDAR’s text instances are much simpler than SVT’s
and beneﬁt less from the more advanced datasets.
4.4. End-to-End Text Spotting
Text spotting is limited by the detection stage, as stateof-the-art cropped word image recognition accuracy is over
98% . We utilise our improvements in text localisation
to obtain state-of-the-art results in text spotting.
Evaluation protocol. Unless otherwise stated, we follow
the standard evaluation protocol by Wang et al. , where
all words that are either less than three characters long or
contain non-alphanumeric characters are ignored. An overlap (IoU) of at least 0.5 is required for a positive detection.
Table 3 shows the results on end-to-end text spotting task
using the “FCRN + multi-ﬁlt” and “FCRNall + multi-ﬁlt”
methods. For recognition we use the output of the interme-
Total Time
BB-regression
& recognition
FCRN+multi-ﬁlt
FCRNall+multi-ﬁlt
Jaderberg et al.
Table 4. Comparison of end-to-end text-spotting time (in seconds).
diary recognition stage of the pipeline based on the lexiconencoding CNN of Jaderberg et al. . We improve upon
previously reported results (F-measure): +8% on the IC-
DAR datasets, and +3% on the SVT dataset. Given the high
recall of our method (as noted before in Figure 5), the fact
that many text instances are unlabelled in SVT cause precision to drop; hence, we see smaller gains in SVT and do
worse on SVT-50.
4.5. Timings
At test time FCRN can process 20 images per second
(of size 512×512px) at single scale and about 15 images
per second when run on multiple scales (1,1/2,1/4,1/8) on
a GPU. When used as high-quality proposals in the text localisation pipeline of Jaderberg et al. , it replaces the
region proposal stage which typically takes about 3 seconds per image. Hence, we gain a speed-up of about 45
times in the region proposal stage. Further, the “FCRN +
multi-ﬁlt” method, which uses only the high-scoring detections from multi-scale FCRN and achieves state-of-the-art
results in detection and end-to-end text spotting, cuts down
the number of proposals in the later stages of the pipeline by
a factor of 10: the region proposal stage of Jaderberg et al.
proposes about 2000 boxes which are quickly ﬁltered using
a random-forest classiﬁer to a manageable set of about 200
proposals, whereas the high-scoring detections from multiscale FCRN are typically less than 30. Table 4 compares
the time taken for end-to-end text-spotting; our method is
between 3× to 23× faster than Jaderberg et al.’s, depending on the variant.
5. Conclusion
We have developed a new CNN architecture for generating text proposals in images. It would not have been
possible to train this architecture on the available annotated
datasets, as they contain far too few samples, but we have
shown that training images of sufﬁcient verisimilitude can
be generated synthetically, and that the CNN trained only on
these images exceeds the state-of-the-art performance for
both detection and end-to-end text spotting on real images.
Acknowledgements. We thank Max Jaderberg for generously providing code and helpful advice. We are grateful
for comments from Jiri Matas. Financial support was provided by the UK EPSRC CDT in Autonomous Intelligent
Machines and Systems Grant EP/L015987/2, EPSRC Programme Grant Seebibyte EP/M013774/1, and the Clarendon Fund scholarship.