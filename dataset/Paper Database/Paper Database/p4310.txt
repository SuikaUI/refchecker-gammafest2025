Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–6173
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Synthetic QA Corpora Generation with Roundtrip Consistency
Chris Alberti
Daniel Andor
Emily Pitler
Jacob Devlin
Michael Collins
Google Research
{chrisalberti, andor, epitler, jacobdevlin, mjcollins}@google.com
We introduce a novel method of generating
synthetic question answering corpora by combining models of question generation and answer extraction, and by ﬁltering the results to
ensure roundtrip consistency. By pretraining
on the resulting corpora we obtain signiﬁcant
improvements on SQuAD2 and NQ , establishing a new state-of-the-art on the latter.
Our synthetic data generation models, for both
question generation and answer extraction, can
be fully reproduced by ﬁnetuning a publicly
available BERT model 
on the extractive subsets of SQuAD2 and NQ.
We also describe a more powerful variant that
does full sequence-to-sequence pretraining for
question generation, obtaining exact match
and F1 at less than 0.1% and 0.4% from human performance on SQuAD2.
Introduction
Signiﬁcant advances in Question Answering (QA)
have recently been achieved by pretraining deep
transformer language models on large amounts of
unlabeled text data, and ﬁnetuning the pretrained
models on hand labeled QA datasets, e.g. with
BERT .
Language modeling is however just one example of how an auxiliary prediction task can be constructed from widely available natural text, namely
by masking some words from each passage and
training the model to predict them. It seems plausible that other auxiliary tasks might exist that are
better suited for QA, but can still be constructed
from widely available natural text. It also seems
intuitive that such auxiliary tasks will be more
helpful the closer they are to the particular QA task
we are attempting to solve.
Based on this intuition we construct auxiliary tasks for QA, generating millions of syn-
... in 1903, boston participated in the
ﬁrst modern world series, going up
against the pittsburgh pirates ...
when did the red sox ﬁrst go to the
world series
Table 1: Example of how synthetic question-answer
pairs are generated. The model’s predicted answer (A′)
matches the original answer the question was generated
from, so the example is kept.
thetic question-answer-context triples from unlabeled passages of text, pretraining a model on
these examples, and ﬁnally ﬁnetuning on a particular labeled dataset. Our auxiliary tasks are illustrated in Table 1.
For a given passage C, we sample an extractive short answer A (Step (1) in Table 1). In Step
(2), we generate a question Q conditioned on A
and C, then (Step (3)) predict the extractive answer A′ conditioned on Q and C. If A and A′
match we ﬁnally emit (C, Q, A) as a new synthetic training example (Step (4)).
We train a
separate model on labeled QA data for each of
the ﬁrst three steps, and then apply the models
in sequence on a large number of unlabeled text
passages. We show that pretraining on synthetic
data generated through this procedure provides us
with signiﬁcant improvements on two challenging
datasets, SQuAD2 and NQ
 , achieving a new state
of the art on the latter.
Related Work
Question generation is a well-studied task in its
own right . Yang et al. 
and Dhingra et al. both use generated
question-answer pairs to improve a QA system,
showing large improvements in low-resource settings with few gold labeled examples. Validating
and improving the accuracy of these generated QA
pairs, however, is relatively unexplored.
In machine translation, modeling consistency
with dual learning or backtranslation across both
translation directions improves the quality of
translation models. Back-translation, which adds
synthetically generated parallel data as training
examples, was an inspiration for this work, and
has led to state-of-the-art results in both the supervised and the unsupervised
settings .
Lewis and Fan model the joint distribution of questions and answers given a context and
use this model directly, whereas our work uses
generative models to generate synthetic data to be
used for pretraining.
Combining these two approaches could be an area of fruitful future work.
Given a dataset of contexts, questions, and answers: {(c(i), q(i), a(i)) : i = 1, . . . , N}, we train
three models: (1) answer extraction: p(a|c; θA),
(2) question generation:
p(q|c, a; θQ), and (3)
question answering: p(a|c, q; θA′).
We use BERT ∗to model
each of these distributions. Inputs to each of these
models are ﬁxed length sequences of wordpieces,
listing the tokenized question (if one was available) followed by the context c. The answer extraction model is detailed in §3.1 and two variants of question generation models in §3.2 and
§3.3. The question answering model follows Alberti et al. .
Question (Un)Conditional Extractive QA
We deﬁne a question-unconditional extractive answer model p(a|c; θA) and a question-conditional
extractive answer model p(a|q, c; θA′) as follows:
p(a|c; θA) =
efJ(a,c;θA)
a′′ efJ(a′′,c;θA)
p(a|c, q; θA′) =
efI(a,c,q;θA′)
a′′ efI(a′′,c,q;θA′)
experiments
masks out whole words at training time,
similar to
 
google-research/bert for both the original and
whole word masked versions of BERT.
where a, a′′ are deﬁned to be token spans over c.
For p(a|c; θA), a and a′′ are constrained to be of
length up to LA, set to 32 word piece tokens. The
key difference between the two expressions is that
fI scores the start and the end of each span independently, while fJ scores them jointly.
Speciﬁcally we deﬁne fJ : Rh →R and fI :
Rh →R to be transformations of the ﬁnal token
representations computed by a BERT model:
fJ(a, c; θA) =
MLPJ(CONCAT(BERT(c)[s], BERT(c)[e]))
fI(a, q, c; θA′)) =
AFFI(BERT(q, c)[s]) + AFFI(BERT(q, c)[e]).
Here h is the hidden representation dimension,
(s, e) = a is the answer span, BERT(t)[i] is the
BERT representation of the i’th token in token sequence t. MLPJ is a multi-layer perceptron with
a single hidden layer, and AFFI is an afﬁne transformation.
We found it was critical to model span start and
end points jointly in p(a|c; θA) because, when the
question is not given, there are usually multiple
acceptable answers for a given context, so that the
start point of an answer span cannot be determined
separately from the end point.
Question Generation: Fine-tuning Only
Text generation allows for a variety of choices in
model architecture and training data. In this section we opt for a simple adaptation of the public
BERT model for text generation. This adaptation
does not require any additional pretraining and no
extra parameters need to be trained from scratch at
ﬁnetuning time. This question generation system
can be reproduced by simply ﬁnetuning a publicly
available pretrained BERT model on the extractive
subsets of datasets like SQuAD2 and NQ.
Fine-tuning
We deﬁne the p(q|c, a; θQ) model
as a left-to-right language model
p(q|a, c; θQ) =
p(qi|q1, . . . , qi−1, a, c; θQ)
efQ(q1,...,qi,a,c;θQ)
i efQ(q1,...,q′
i,a,c;θQ) ,
where q = (q1, . . . , qLQ) is the sequence of question tokens and LQ is a predetermined maximum question length, but, unlike the more usual
encoder-decoder approach, we compute fQ using
the single encoder stack from the BERT model:
fQ(q1, . . . , qi, a, c; θQ) =
BERT(q1, . . . , qi−1, a, c)[i −1] · W ⊺
where WBERT is the word piece embedding matrix in BERT. All parameters of BERT including
WBERT are ﬁnetuned. In the context of question
generation, the input answer is encoded by introducing a new token type id for the tokens in the
extractive answer span, e.g. the question tokens
being generated have type 0 and the context tokens
have type 1, except for the ones in the answer span
that have type 2. We always pad or truncate the
question being input to BERT to a constant length
LQ to avoid giving the model information about
the length of the question we want it to generate.
This model can be trained efﬁciently by using
an attention mask that forces to zero all the attention weights from c to q and from qi to qi+1 . . . qLQ
for all i.
Question Generation
At inference time we
generate questions through iterative greedy decoding, by computing argmaxqi fQ(q1, . . . , qi, a, c)
for i = 1, . . . , LQ. Question-answer pairs are kept
only if they satisfy roundtrip consistency.
Question Generation: Full Pretraining
The prior section addressed a restricted setting
in which a BERT model was ﬁne-tuned, without
any further changes. In this section, we describe
an alternative approach for question generation
that fully pretrains and ﬁne-tunes a sequence-tosequence generation model.
Pretraining
Section 3.2 used only an encoder
for question generation. In this section, we use a
full sequence-to-sequence Transformer (both encoder and decoder). The encoder is trained identically (BERT pretraining, Wikipedia data), while
the decoder is trained to output the next sentence.
Fine-tuning
Fine-tuning is done identically as
in Section 3.2, where the input is (C, A) and
the output is Q from tuples from a supervised
question-answering dataset (e.g., SQuAD).
Question Generation
To get examples of synthetic (C, Q, A) triples, we sample from the decoder with both beam search and Monte Carlo
search. As before, we use roundtrip consistency
to keep only the high precision triples.
Why Does Roundtrip Consistency Work?
A key question for future work is to develop a
more formal understanding of why the roundtrip
method improves accuracy on question answering tasks and
Sennrich et al. ; a similar theory may apply to these methods). In the supplementary material we sketch a possible approach, inspired by the
method of Balcan and Blum for learning
with labeled and unlabeled data. This section is
intentionally rather speculative but is intended to
develop intuition about the methods, and to propose possible directions for future work on developing a formal grounding.
In brief, the approach discussed in the supplementary material suggests optimizing the loglikelihood of the labeled training examples, under
a constraint that some measure of roundtrip consistency β(θA′) on unlabeled data is greater than
some value γ. The value for γ can be estimated
using performance on development data. The auxiliary function β(θA′) is chosen such that: (1) the
constraint β(θA′) ≥γ eliminates a substantial part
of the parameter space, and hence reduces sample
complexity; (2) the constraint β(θA′) ≥γ nevertheless includes ‘good’ parameter values that ﬁt
the training data well. The ﬁnal step in the argument is to make the case that the algorithms
described in the current paper may effectively
be optimizing a criterion of this kind.
Speciﬁcally, the auxiliary function β(θA′) is deﬁned as
the log-likelihood of noisy (c, q, a) triples generated from unlabeled data using the C →A and
C, A →Q models; constraining the parameters
θA′ to achieve a relatively high value on β(θA′) is
achieved by pre-training the model on these examples. Future work should consider this connection
in more detail.
Experiments
Experimental Setup
We considered two datasets in this work: SQuAD2
 and the Natural Questions
(NQ) .
a dataset of QA examples of questions with answers formulated and answered by human annotators about Wikipedia passages. NQ is a dataset
of Google queries with answers from Wikipedia
pages provided by human annotators. We used the
full text from the training set of NQ (1B words) as
Fine-tuning Only
BERT-Large (Original)
+ 3M synth SQuAD2
+ 4M synth NQ
Full Pretraining
BERT (Whole Word Masking)†
+ 50M synth SQuAD2
+ ensemble
Table 2: Our results on SQuAD2. For our ﬁne-tuning
only setting, we compare a BERT baseline (BERT single model - Google AI Language on the SQuAD2
leaderboard) to similar models pretrained on our synthetic SQuAD2-style corpus and on a corpus containing both SQuAD2- and NQ-style data. For the full pretraining setting, we report our best single model and
ensemble results.
a source of unlabeled data.
In our ﬁne-tuning only experiments (Section
3.2) we trained two triples of models (θA, θQ, θA′)
on the extractive subsets of SQuAD2 and NQ.
We extracted 8M unlabeled windows of 512 tokens from the NQ training set.
For each unlabeled window we generated one example from the
SQuAD2-trained models and one example from
the NQ-trained models. For A we picked an answer uniformly from the top 10 extractive answers
according to p(a|c; θA). For A′ we picked the best
extractive answer according to p(a|c, q; θA′). Filtering for roundtrip consistency gave us 2.4M and
3.2M synthetic positive instances from SQuAD2and NQ-trained models respectively.
added synthetic unanswerable instances by taking
the question generated from a window and associating it with a non-overlapping window from the
same Wikipedia page. We then sampled negatives
to obtain a total of 3M and 4M synthetic training
instances for SQuAD2 and NQ respectively. We
trained models analogous to Alberti et al. 
initializing from the public BERT model, with a
batch size of 128 examples for one epoch on each
of the two sets of synthetic examples and on the
union of the two, with a learning rate of 2 · 10−5
and no learning rate decay. We then ﬁne-tuned the
the resulting models on SQuAD2 and NQ.
In our full pretraining experiments (Section 3.3)
we only trained (θA, θQ, θA′) on SQuAD2. How-
† 
Best exact match
on SQuAD2.0 dev set
Number of synthetic examples (M)
NQ+SQuAD Synth
NQ+SQuAD Synth no-RT
SQuAD Synth
SQuAD Synth no-RT
Figure 1: Learning curves for pretraining using synthetic question-answering data (ﬁne-tuning only setting). “no-RT” refers to omitting the roundtrip consistency check. Best exact match is reported after ﬁnetuning on SQuAD2. Performance improves with the
amount of synthetic data. For a ﬁxed amount of synthetic data, having a more diverse source (NQ+SQuAD
vs. just SQuAD) yields higher accuracies. Roundtrip
ﬁltering gives further improvements.
ever, we pretrained our question generation model
on all of the BERT pretraining data, generating the
next sentence left-to-right. We created a synthetic,
roundtrip ﬁltered corpus with 50M examples. We
then ﬁne-tuned the model on SQuAD2 as previously described. We experimented with both the
single model setting and an ensemble of 6 models.
The ﬁnal results are shown in Tables 2 and 3. We
found that pretraining on SQuAD2 and NQ synthetic data increases the performance of the ﬁnetuned model by a signiﬁcant margin. On the NQ
short answer task, the relative reduction in headroom is 50% to the single human performance and
10% to human ensemble performance. We additionally found that pretraining on the union of synthetic SQuAD2 and NQ data is very beneﬁcial on
the SQuAD2 task, but does not improve NQ results.
The full pretraining approach with ensembling
obtains the highest EM and F1 listed in Table 2.
This result is only 0.1 −0.4% from human performance and is the third best model on the SQuAD2
leaderboard as of this writing (5/31/19).
Roundtrip Filtering
Roundtrip ﬁltering appears to be consistently beneﬁcial. As shown in
Figure 1, models pretrained on roundtrip consistent data outperform their counterparts pretrained
without ﬁltering. From manual inspection, of 46
(C, Q, A) triples that were roundtrip consistent
Long Answer Dev
Long Answer Test
Short Answer Dev
Short Answer Test
+ 4M synth NQ
Single Human
Super-annotator
Table 3: Our results on NQ, compared to the previous best system and to the performance of a human annotator
and of an ensemble of human annotators. BERTjoint is the model described in Alberti et al. .
what was the population of chicago in 1857?
over 90,000
what was the weight of the brigg’s hotel?
22,000 tons
where is the death of the virgin located?
what person replaced the painting?
carlo saraceni
when did rick and morty get released?
what executive suggested that rick be a grandfather?
nick weidenfeld
Table 4: Comparison of question-answer pairs generated by NQ and SQuAD2 models for the same passage of text.
39% were correct, while of 44 triples that were
discarded only 16% were correct.
Data Source
Generated question-answer pairs
are illustrative of the differences in the style of
questions between SQuAD2 and NQ. We show a
few examples in Table 4, where the same passage
is used to create a SQuAD2-style and an NQ-style
question-answer pair. The SQuAD2 models seem
better at creating questions that directly query a
speciﬁc property of an entity expressed in the text.
The NQ models seem instead to attempt to create questions around popular themes, like famous
works of art or TV shows, and then extract the
answer by combining information from the entire
Conclusion
We presented a novel method to generate synthetic QA instances and demonstrated improvements from this data on SQuAD2 and on NQ. We
additionally proposed a possible direction for formal grounding of this method, which we hope to
develop more thoroughly in future work.