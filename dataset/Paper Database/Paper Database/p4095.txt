Published as a conference paper at ICLR 2020
FREELB: ENHANCED ADVERSARIAL TRAINING FOR
NATURAL LANGUAGE UNDERSTANDING
Chen Zhu1, Yu Cheng2, Zhe Gan2, Siqi Sun2, Tom Goldstein1, Jingjing Liu2
1University of Maryland, College Park
2Microsoft Dynamics 365 AI Research
{chenzhu,tomg}@cs.umd.edu, {yu.cheng,zhe.gan,siqi.sun,jingjl}@microsoft.com
Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of
language models. In this work, we propose a novel adversarial training algorithm,
FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial
risk inside different regions around input samples. To validate the effectiveness
of the proposed approach, we apply it to Transformer-based models for natural
language understanding and commonsense reasoning tasks. Experiments on the
GLUE benchmark show that when applied only to the ﬁnetuning stage, it is able
to improve the overall test scores of BERT-base model from 78.3 to 79.4, and
RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach
achieves state-of-the-art single-model test accuracies of 85.44% and 67.75% on
ARC-Easy and ARC-Challenge. Extensive experiments further demonstrate that
FreeLB can be generalized and boost the performance of RoBERTa-large and AL-
BERT on other tasks as well. 1
INTRODUCTION
Adversarial training is a method for creating robust neural networks. During adversarial training,
mini-batches of training samples are contaminated with adversarial perturbations (alterations that
are small and yet cause misclassiﬁcation), and then used to update network parameters until the
resulting model learns to resist such attacks. Adversarial training was originally proposed as a means
to enhance the security of machine learning systems , especially for safetycritical systems like self-driving cars and copyright detection . Surprisingly, people observe the opposite result
for language models , showing that adversarial training can
improve both generalization and robustness.
We will show that adversarial training signiﬁcantly improves performance of state-of-the-art models for many language understanding tasks. In particular, we propose a novel adversarial training
algorithm, called FreeLB (Free Large-Batch), which adds adversarial perturbations to word embeddings and minimizes the resultant adversarial loss around input samples. The method leverages
recently proposed “free” training strategies to enrich the
training data with diversiﬁed adversarial samples under different norm constraints at no extra cost
than PGD-based (Projected Gradient Descent) adversarial training , which enables us to perform such diversiﬁed adversarial training on large-scale state-of-the-art models. We
observe improved invariance in the embedding space for models trained with FreeLB, which is positively correlated with generalization.
1Code is available at 
Published as a conference paper at ICLR 2020
We perform comprehensive experiments to evaluate the performance of a variety of adversarial training algorithms on state-of-the-art language understanding models and tasks. In the comparisons
with standard PGD , FreeAT and YOPO , FreeLB stands out to be the best for the datasets and models we evaluated. With FreeLB,
we achieve state-of-the-art results on several important language understanding benchmarks. On
the GLUE benchmark, FreeLB pushes the performance of the BERT-base model from 78.3 to 79.4.
The overall score of the RoBERTa-large models on the GLUE benchmark is also lifted from 88.5 to
88.8, achieving best results on most of its sub-tasks. Experiments also show that FreeLB can boost
the performance of RoBERTa-large on question answering tasks, such as the ARC and CommonsenseQA benchmarks. We also provide a comprehensive ablation study and analysis to demonstrate
the effectiveness of our training process.
RELATED WORK
ADVERSARIAL TRAINING
To improve the robustness of neural networks against adversarial examples, many defense strategies
and models have been proposed, in which PGD-based adversarial training is
widely considered to be the most effective, since it largely avoids the the obfuscated gradient problem . It formulates a class of adversarial training algorithms into solving a minimax problem on the cross-entropy loss, which can be achieved reliably
through multiple projected gradient ascent steps followed by a SGD (Stochastic Gradient Descent)
Despite being veriﬁed by Athalye et al. to avoid obfuscated gradients, Qin et al. shows
that PGD-based adversarial training still leads to highly convolved and non-linear loss surfaces when
K is small, which could be readily broken under stronger adversaries. Thus, to be effective, the cost
of PGD-based adversarial training is much higher than conventional training. To mitigate this cost,
Shafahi et al. proposed a “free” adversarial training algorithm that simultaneously updates
both model parameters and adversarial perturbations on a single backward pass. Using a similar
formulation, Zhang et al. effectively reduce the total number of full forward and backward
propagations for obtaining adversarial examples by restricting most of its adversarial updates in the
ﬁrst layer.
ADVERSARIAL EXAMPLES IN NATURAL LANGUAGES
Adversarial examples have been explored primarily in the image domain, and received many attention in text domain recently. Previous works on text adversaries have focused on heuristics for creating adversarial examples in the black-box setting, or on speciﬁc tasks. Jia & Liang propose
to add distracting sentences to the input document in order to induce mis-classiﬁcation. Zhao et al.
 generate text adversaries by projecting the input data to a latent space using GANs, and
searching for adversaries close to the original instance. Belinkov & Bisk manipulate every
word in a sentence with synthetic or natural noise in machine translation systems. Iyyer et al. 
propose a neural paraphrase model based on back-translated data to produce paraphrases that have
different sentence structures. Different from previous work, ours is not to produce actual adversarial
examples, but only take the beneﬁt of adversarial training for natural language understanding.
We are not the ﬁrst to observe that robust language models may perform better on clean test data.
Miyato et al. extend adversarial and virtual adversarial training to the
text domain to improve the performance on semi-supervised classiﬁcation tasks. Ebrahimi et al.
 propose a character/word replacement for crafting attacks, and show employing adversarial
examples in training renders the models more robust. Ribeiro et al. show that adversarial
attacks can be used as a valuable tool for debugging NLP models. Cheng et al. also ﬁnd
that crafting adversarial examples can help neural machine translation signiﬁcantly. Notably, these
studies have focused on simple models or text generation tasks. Our work explores how to efﬁciently use the gradients obtained in adversarial training to boost the performance of state-of-the-art
transformer-based models.
Published as a conference paper at ICLR 2020
ADVERSARIAL TRAINING FOR LANGUAGE UNDERSTANDING
Pre-trained large-scale language models, such as BERT , RoBERTa , ALBERT and T5 , have proven to be highly effective
for downstream tasks. We aim to further improve the generalization of these pre-trained language
models on the downstream language understanding tasks by enhancing their robustness in the embedding space during ﬁnetuning on these tasks. We achieve this goal by creating “virtual” adversarial examples in the embedding space, and then perform parameter updates on these adversarial
embeddings. Creating actual adversarial examples for language is difﬁcult; even with state-of-theart language models as guidance ), it remains unclear how to construct
label-preserving adversarial examples via word/character replacement without human evaluations,
because the meaning of each word/character depends on the context . Since
we are only interested in the effects of adversarial training, rather than producing actual adversarial
examples, we add norm-bounded adversarial perturbations to the embeddings of the input sentences
using a gradient-based method. Note that our embedding-based adversary is strictly stronger than a
more conventional text-based adversary, as our adversary can make manipulations on word embeddings that are not possible in the text domain.
For models that incorporate various input representations, including word or subword embeddings,
segment embeddings and position embeddings, our adversaries only modify the concatenated word
or sub-word embeddings, leaving other components of the sentence representation unchanged. 2 Denote the sequence of one-hot representations of the input subwords as Z = [z1, z2, ..., zn], the embedding matrix as V , and the language model (encoder) as a function y = fθ(X), where X = V Z
is the subword embeddings, y is the output of the model (e.g., class probabilities for classiﬁcation
models), and θ denotes all the learnable parameters including the embedding matrix V . We add adversarial perturbations δ to the embeddings such that the prediction becomes y′ = fθ(X + δ). To
preserve the semantics, we constrain the norm of δ to be small, and assume the model’s prediction
should not change after the perturbation. This formulation is analogous to Miyato et al. , with
the difference that we do not require X to be normalized.
PGD FOR ADVERSARIAL TRAINING
Standard adversarial training seeks to ﬁnd optimal parameters θ∗to minimize the maximum risk for
any δ within a norm ball as:
θ E(Z,y)∼D
∥δ∥≤ǫ L(fθ(X + δ), y)
where D is the data distribution, y is the label, and L is some loss function. We use the Frobenius
norm to constrain δ. For neural networks, the outer “min” is non-convex, and the inner “max” is
non-concave. Nonetheless, Madry et al. demonstrated that this saddle-point problem can be
solved reliably with SGD for the outer minimization and PGD and ), for the
inner maximization. In particular, for the constraint ∥δ∥F ≤ǫ, with an additional assumption that
the loss function is locally linear, PGD takes the following step (with step size α) in each iteration:
δt+1 = Π∥δ∥F ≤ǫ (δt + αg(δt)/∥g(δt)∥F ) ,
where g(δt) = ∇δL(fθ(X + δt), y) is the gradient of the loss with respect to δ, and Π∥δ∥F ≤ǫ
performs a projection onto the ǫ-ball. To achieve high-level robustness, multi-step adversarial examples are needed during training, which is computationally expensive. The K-step PGD (K-PGD)
requires K forward-backward passes through the network, while the standard SGD update requires
only one. As a result, the adversary generation step in adversarial training increases run-time by an
order of magnitudea catastrophic amount when training large state-of-the-art language models.
LARGE-BATCH ADVERSARIAL TRAINING FOR FREE
In the inner ascent steps of PGD, the gradients of the parameters can be obtained with almost no
overhead when computing the gradients of the inputs. From this observation, FreeAT (Shafahi et al.,
2“Subword embeddings” refers to the embeddings of sub-word encodings such as the popular Byte Pair
Encoding (BPE) .
Published as a conference paper at ICLR 2020
Algorithm 1 “Free” Large-Batch Adversarial Training (FreeLB-K)
Require: Training samples X = {(Z, y)}, perturbation bound ǫ, learning rate τ, ascent steps K,
ascent step size α
1: Initialize θ
2: for epoch = 1 . . . Nep do
for minibatch B ⊂X do
√Nδ U(−ǫ, ǫ)
for t = 1 . . . K do
Accumulate gradient of parameters θ
gt ←gt−1 + 1
K E(Z,y)∈B[∇θ L(fθ(X + δt−1), y)]
Update the perturbation δ via gradient ascend
gadv ←∇δ L(fθ(X + δt−1), y)
δt ←Π∥δ∥F ≤ǫ(δt−1 + α · gadv/∥gadv∥F)
15: end for
2019) and YOPO have been proposed to accelerate adversarial training. They
achieve comparable robustness and generalization as standard PGD-trained models using only the
same or a slightly larger number of forward-backward passes as natural training (i.e., SGD on clean
samples). FreeAT takes one descent step on the parameters together with each of the K ascent steps
on the perturbation. As a result, FreeAT may suffer from the “stale gradient” problem , where in every step t, δt does not necessarily maximize the model with parameter θt since
its update is based on ∇δL(fθt−1(X + δt−1), y), and vice versa, θt does not necessarily minimize
the adversarial risk with adversary δt since its update is based on ∇θL(fθt−1(X + δt−1), y). Such
a problem may be more signiﬁcant when the step size is large.
Different from FreeAT, YOPO accumulates the gradient of the parameters from each of the ascent
steps, and updates the parameters only once after the K inner ascent steps. YOPO also advocates
that after each back-propagation, one should take the gradient of the ﬁrst hidden layer as a constant
and perform several additional updates on the adversary using the product of this constant and the
Jacobian of the ﬁrst layer of the network to obtain strong adversaries. However, when the ﬁrst
hidden layer is a linear layer as in their implementation, such an operation is equivalent to taking
a larger step size on the adversary. The analysis backing the extra update steps also assumes a
twice continuously differentiable loss, which does not hold for ReLU-based neural networks they
experimented with, and thus the reasons for the success of such an algorithm remains obscure. We
give empirical comparisons between YOPO and our approach in Sec. 4.3.
To obtain better solutions for the inner max and avoid fundamental limitations on the function class,
we propose FreeLB, which performs multiple PGD iterations to craft adversarial examples, and
simultaneously accumulates the “free” parameter gradients ∇θL in each iteration. After that, it
updates the model parameter θ all at once with the accumulated gradients. The overall procedure
is shown in Algorithm 1, in which X + δt is an approximation to the local maximum within the
intersection of two balls It = BX+δ0(αt) ∩BX(ǫ). By taking a descent step along the averaged
gradients at X + δ0, ..., X + δK−1, we approximately optimize the following objective:
θ E(Z,y)∼D
δt∈It L(fθ(X + δt), y)
which is equivalent to replacing the original batch X with a K-times larger virtual batch, consisting
of samples whose embeddings are X + δ0, ..., X + δK−1. Compared with PGD-based adversarial
training (Eq. 1), which minimizes the maximum risk at a single estimated point in the vicinity of each
training sample, FreeLB minimizes the maximum risk at each ascent step at almost no overhead.
Intuitively, FreeLB could be a learning method with lower generalization error than PGD.
Sokolic et al. have proved that the generalization error of a learning method invariant to a
set of T transformations may be up to
T smaller than a non-invariant learning method. Accord-
Published as a conference paper at ICLR 2020
ing to their theory, FreeLB could have a more signiﬁcant improvement over natural training, since
FreeLB enforces the invariance to K adversaries from a set of up to K different norm constraints,3
while PGD only enforces invariance to a single norm constraint ǫ.
Empirically, FreeLB does lead to higher robustness and invariance than PGD in the embedding
space, in the sense that the maximum increase of loss in the vicinity of X for models trained with
FreeLB is smaller than that with PGD. See Sec. 4.3 for details. In theory, such improved robustness
can lead to better generalization , which is consistent with our experiments.
Qin et al. also demonstrated that PGD-based method leads to highly convolved and nonlinear loss surfaces in the vicinity of input samples when K is small, indicating a lack of robustness.
WHEN ADVERSARIAL TRAINING MEETS DROPOUT
Usually, adversarial training is not used together with dropout . However, for
some language models like RoBERTa , dropout is used during the ﬁnetuning stage.
In practice, when dropout is turned on, each ascent step of Algorithm 1 is optimizing δ for a different
network. Speciﬁcally, denote the dropout mask as m with each entry mi ∼Bernoulli(p). Similar to
our analysis for FreeAT, the ascent step from δt−1 to δt is based on ∇δL(fθ(mt−1)(X + δt−1), y),
so δt is sub-optimal for L(fθ(mt)(X + δ), y). Here θ(m) is the effective parameters under dropout
The more plausible solution is to use the same m in each step. When applying dropout to any network, the objective for θ is to minimize the expectation of loss under different networks determined
by the dropout masks, which is achieved by minimizing the Monte Carlo estimation of the expected
loss. In our case, the objective becomes:
θ E(Z,y)∼D,m∼M
δt∈It L(fθ(m)(X + δt), y)
where the 1-sample Monte Carlo estimation should be 1
t=0 maxδt∈It L(fθ(m0)(X + δt), y)
and can be minimized by using FreeLB with dropout mask m0 in each ascent step. This is similar
to applying Variational Dropout to RNNs as used in Gal & Ghahramani .
EXPERIMENTS
In this section, we provide comprehensive analysis on FreeLB through extensive experiments on
three Natural Language Understanding benchmarks: GLUE , ARC and CommonsenseQA . We also compare the robustness and generalization of FreeLB with other adversarial training algorithms to demonstrate its strength. Additional
experimental details are provided in the Appendix.
GLUE Benchmark.
The GLUE benchmark is a collection of 9 natural language understanding tasks, namely Corpus of Linguistic Acceptability ), Stanford
Sentiment Treebank ), Microsoft Research Paraphrase Corpus ), Semantic Textual Similarity Benchmark ),
Quora Question Pairs ), Multi-Genre NLI ),
Question NLI ), Recognizing Textual Entailment ; Bar Haim et al. ; Giampiccolo et al. ; Bentivogli et al. ) and Winograd
NLI ).
8 of the tasks are formulated as classiﬁcation problems
and only STS-B is formulated as regression, but FreeLB applies to all of them.
For BERTbase, we use the HuggingFace implementation4, and follow the single-task ﬁnetuning procedure
as in Devlin et al. . For RoBERTa, we use the fairseq implementation5. Same as Liu et al.
3The cardinality of the set is approximately min{K, ⌈ǫ−E[∥δ0∥]
4 
5 
Published as a conference paper at ICLR 2020
Reported 90.2
85.61 (1.7) 96.56 (.3) 90.69 (.5) 67.57 (1.3) 92.20 (.2)
90.53 (.2) 94.87 (.2) 92.49 (.07) 87.41 (.9)
96.44 (.1) 90.93 (.2) 69.67 (1.2) 92.43 (7.)
90.02 (.2) 94.66 (.2) 92.48 (.08) 86.69 (15.) 96.10 (.2) 90.69 (.4) 68.80 (1.3) 92.40 (.3)
90.61 (.1) 94.98 (.2) 92.60 (.03) 88.13 (1.2) 96.79 (.2) 91.42 (.7) 71.12 (.9)
92.67 (.08)
Table 1: Results (median and variance) on the dev sets of GLUE based on the RoBERTa-large model, from 5
runs with the same hyperparameter but different random seeds. ReImp is our reimplementation of RoBERTalarge. The training process can be very unstable even with the vanilla version. Here, both PGD on STS-B and
FreeAT on RTE demonstrates such instability, with one unconverged instance out of ﬁve.
Score CoLA SST-2
MNLI-m/mm QNLI RTE WNLI AX
BERT-base1
93.5 88.9/84.8 87.1/85.8 71.2/89.2
FreeLB-BERT 79.4
93.6 88.1/83.5 87.7/86.7 72.7/89.6
96.5 92.7/90.3 91.1/90.7 73.7/89.9
XLNet-Large3 88.4
96.8 93.0/90.7 91.6/91.1 74.2/90.3
96.7 92.3/89.8 92.2/91.9 74.3/90.2
FreeLB-RoB
96.8 93.1/90.8 92.4/92.2 74.8/90.3
97.8 86.3/80.8 92.7/92.6 59.5/80.4
Table 2: Results on GLUE from the evaluation server, as of Sep 25, 2019. Metrics are the same as the
leaderboard. Number under each task’s name is the size of the training set. FreeLB-BERT is the single-model
results of BERT-base ﬁnetuned with FreeLB, and FreeLB-RoB is the ensemble of 7 RoBERTa-Large models
for each task. References: 1: ; 2: ; 3: ; 4: , we also use single-task ﬁnetuning for all dev set results, and start with MNLI-ﬁnetuned
models on RTE, MRPC and STS-B for the test submissions.
ARC Benchmark. The ARC dataset is a collection of multi-choice science
questions from grade-school level exams. It is further divided into ARC-Challenge set with 2,590
question answer (QA) pairs and ARC-Easy set with 5,197 QA pairs. Questions in ARC-Challenge
are more difﬁcult and cannot be handled by simply using a retrieval and co-occurence based algorithm . A typical question is:
Which property of a mineral can be determined just by looking at it?
(A) luster [correct] (B) mass (C) weight (D) hardness.
CommonsenseQA Benchmark. The CommonsenseQA dataset consists of
12,102 natural language questions that require human commonsense reasoning ability to answer. A
typical question is :
Where can I stand on a river to see water falling without getting wet?
(A) waterfall, (B) bridge [correct], (C) valley, (D) stream, (E) bottom.
Each question has ﬁve candidate answers from ConceptNet . To make the question more difﬁcult to solve, most answers have the same relation in ConceptNet to the key concept
in the question. As shown in the above example, most answers can be connected to “river” by “At-
Location” relation in ConceptNet. For a fair comparison with the reported results in papers and
leaderboard6, we use the ofﬁcial random split 1.11.
EXPERIMENTAL RESULTS
GLUE We summarize results on the dev sets of GLUE in Table 1, comparing the proposed FreeLB
against other adversatial training algorithms and FreeAT 
RoBERTa (ReImp)
FreeLB-RoBERTa
AristoRoBERTaV7 (MTL)
XLNet + RoBERTa (MTL+Ens)
FreeLB-RoBERTa (MTL)
Table 3: Results on ARC and CommonsenseQA (CQA). ARC-Merge is the combination of ARC-Easy and
ARC-Challenge, “MTL” stands for multi-task learning and “Ens” stands for ensemble. Results of XLNet +
RoBERTa (MTL+Ens) and AristoRoBERTaV7 (MTL) are from the ARC leaderboards. Test (E) denotes the
test set results with ensembles. For CQA, we report the highest dev and test accuracies among all models. The
models with 78.81/72.19 dev/test accuracy (as in the table) have 71.84/78.64 test/dev accuracies respectively.
YOPO are provided in Sec. 4.3. We have also submitted our results to the evaluation server, results provided in Table 2. FreeLB lifts the performance of the BERT-base model
from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8 on overall scores.
ARC For ARC, a corpus of 14 million related science documents (from ARC Corpus, Wikipedia
and other sources) is provided. For each QA pair, we ﬁrst use a retrieval model to select top 10
related documents. Then, given these retrieved documents7, we use RoBERTa-large model to encode
⟨s⟩Retrieved Documents ⟨/s⟩Question + Answer ⟨/s⟩, where ⟨s⟩and ⟨/s⟩are special tokens for
RoBERTa model8. We then apply a fully-connected layer to the representation of the [CLS] token
to compute the ﬁnal logit, and use standard cross-entropy loss for model training.
Results are summarized in Table 3. Following Sun et al. , we ﬁrst ﬁnetune the RoBERTa
model on the RACE dataset . The ﬁnetuned RoBERTa model achieves 85.70% and
85.24% accuracy on the development and test set of RACE, respectively. Based on this, we further
ﬁnetune the model on both ARC-Easy and ARC-Challenge datasets with the same hyper-parameter
searching strategy (for 5 epochs), which achieves 84.13%/64.44%test accuracy on ARC-Easy/ARC-
Challenge. And by adding FreeLB ﬁnetuning, we can reach 84.81%/65.36%, a signiﬁcant boost on
ARC benchmark, demonstrating the effectiveness of FreeLB.
To further improve the results, we apply a multi-task learning (MTL) strategy using additional
datasets. We ﬁrst ﬁnetune the model on RACE , and then ﬁnetune on a joint
dataset of ARC-Easy, ARC-Challenge, OpenbookQA and Regents Living
Environment9. Based on this, we further ﬁnetune our model on ARC-Easy and ARC-Challenge
with FreeLB. After ﬁnetuning, our single model achieves 67.75% test accuracy on ARC-Challenge
and 85.44% on ARC-Easy, both outperforming the best submission on the ofﬁcial leaderboard10.
CommonsenseQA Similar to the training strategy in Liu et al. , we construct ﬁve inputs
for each question by concatenating the question and each answer separately, then encode each input
with the representation of the [CLS] token. A ﬁnal score is calculated by applying the representation
of [CLS] to a fully-connected layer. Following the fairseq repository11, the input is formatted as:
”⟨s⟩Q: Where can I stand on a river to see water falling without getting wet? ⟨/s⟩A: waterfall ⟨/s⟩”,
where ’Q:’ and ’A:’ are the preﬁx for question and answer, respectively.
Results are summarized in Table 3. We obtained a dev-set accuracy of 77.56% with the RoBERTalarge model. When using FreeLB ﬁnetuning, we achieved 78.81%, a 1.25% absolute gain. Compared with the results reported from fairseq repository, which obtains 78.43% accuracy on the devset, FreeLB still achieves better performance. Our submission to the CommonsenseQA leaderboard
achieves 72.2% single-model test set accuracy, and the result of a 20-model ensemble is 73.1%,
which achieves No.1 among all the submissions without making use of ConceptNet.
7We thank AristoRoBERTa team for providing retrieved documents and additional Regents Living Environments dataset.
8Equivalent to [CLS] and [SEP] token in BERT.
9 
10 and easy/
submissions/public
11 qa
Published as a conference paper at ICLR 2020
85.61 (1.67)
87.14 (1.29)
88.13 (1.21)
87.05 (1.36)
87.05 (0.20)
67.57 (1.30)
69.31 (1.16)
71.12 (0.90)
70.40 (0.91)
69.91 (1.16)
90.69 (0.54)
90.93 (0.66)
91.42 (0.72)
90.44 (0.62)
90.69 (0.37)
Table 4: The median and standard deviation of the scores on the dev sets of RTE, CoLA and MRPC from the
GLUE benchmark, computed from 5 runs with the same hyper-parameters except for the random seeds. We
use FreeLB-m to denote FreeLB with m ascent steps, and FreeLB-3∗to denote the version without reusing the
dropout mask.
Table 5: Median of the maximum increase in loss in the vicinity of the dev set samples for RoBERTa-Large
model ﬁnetuned with different methods. Vanilla models are naturally trained RoBERTa’s. M-Inc: Max Inc, M-
Inc (R): Max Inc (R). Nat Loss (N-Loss) is the loss value on clean samples. Notice we require all clean samples
here to be correctly classiﬁed by all models, which results in 227, 850 and 355 samples for RTE, CoLA and
MRPC, respectively. We also give the variance in the Appendix.
ABLATION STUDY AND ANALYSIS
In this sub-section, we ﬁrst show the importance of reusing dropout mask, then conduct a thorough
ablation study on FreeLB over the GLUE benchmark to analyze the robustness and generalization
strength of different approaches. We observe that it is unnecessary to perform shallow-layer updates
on the adversary as YOPO for our case, and FreeLB results in improved robustness and generalization compared with PGD.
Importance of Reusing Mask Table 4 (columns 2 to 4) compares the results of FreeLB with and
without reusing the same dropout mask in each ascent step, as proposed in Sec. 3.3. With reusing,
FreeLB can achieve a larger improvement over the naturally trained models. Thus, we enable mask
reusing for all experiments involving RoBERTa.
Comparing the Robustness Table 5 provides the comparisons of the maximum increment of loss
in the vicinity of each sample, deﬁned as:
∆Lmax(X, ǫ) = max
∥δ∥≤ǫ L(fθ(X + δ), y) −L(fθ(X), y),
which reﬂects the robustness and invariance of the model in the embedding space. In practice, we
use PGD steps as in Eq. 2 to ﬁnd the value of ∆Lmax(X, ǫ). We found that when using a step size of
5·10−3 and ǫ = 0.01∥X∥F, the PGD iterations converge to almost the same value, starting from 100
different random initializations of δ for the RoBERTa models, trained with or without FreeLB. This
indicates that PGD reliably ﬁnds ∆Lmax for these models. Therefore, we compute ∆Lmax(X, ǫ)
for each X via a 2000-step PGD.
Samples with small margins exist even for models with perfect accuracy, which could give a false
sense of vulnerability of the model. To rule out the outlier effect and make ∆Lmax(X, ǫ) comparable across different samples, we only consider samples that all the evaluated models can correctly
classify, and search for an ǫ for each sample such that the reference model can correctly classify all
samples within the ǫ ball.12 However, such choice of per-sample ǫ favors the reference model by
design. To make fair comparisons, Table 5 provides the median of ∆Lmax(X, ǫ) with per-sample ǫ
from models trained by FreeLB (Max Inc) and PGD (Mac Inc (R)), respectively.
Across all three datasets and different reference models, FreeLB has the smallest median increment
even when starting from a larger natural loss than vanilla models. This demonstrates that FreeLB
is more robust and invariant in most cases. Such results are also consistent with the models’ dev
set performance (the performances for Vanilla/PGD/FreeLB models on RTE, CoLA and MRPC are
86.69/87.41/89.21, 69.91/70.84/71.40, 91.67/91.17/91.17, respectively).
12For each sample, we start from a value slightly larger than the norm constraint during training for ǫ, and
then decrease ǫ linearly until the model trained with the reference model can correctly classify after a 2000-step
PGD attack. The reference model is either trained with FreeLB or PGD.
Published as a conference paper at ICLR 2020
MNLI QNLI QQP
RTE SST-2 MRPC CoLA
(Acc) (Acc) (Acc) (Acc) (Acc) (Acc) (Mcc) (Pearson)
BERT-large
XLNet-large
RoBERTa-large
RoBERTa-FreeLB
ALBERT-xxlarge-v2
ALBERT-FreeLB
Table 6: Results (median) on the dev sets of GLUE from 5 runs with the same hyperparameter but different
random seeds. RoBERTa-FreeLB and ALBERT-FreeLB are RoBERTa-large and ALBERT-xxlarge-v2 models
ﬁne-tuned with FreeLB on GLUE. All other results are copied from .
Comparing with YOPO The original implementation of YOPO chooses the
ﬁrst convolutional layer of the ResNets as f0 for updating the adversary in the “s-loop”. As a result,
each step of the “s-loop” should be using exactly the same value to update the adversary,and YOPOm-n degenerates into FreeLB with a n-times large step size. To avoid that, we choose the layers up to
the output of the ﬁrst Transformer block as f0 when implementing YOPO. To make the total amount
of update on the adversary equal, we take the hyper-parameters for FreeLB-m and only change the
step size α into α/n for YOPO-m-n. Table 4 shows that FreeLB performs consistently better than
YOPO on all three datasets. Accidentally, we also give the results comparing with YOPO-m-n
without changing the step size α for YOPO in Table 9. The gap between two approaches seem to
shrink, which may be caused by using a larger total step size for the YOPO adversaries. We leave
exhaustive hyperparameter search for both models as our future work.
Improving ALBERT
To further explore its ability to improve more sophisticated language models, we apply FreeLB to
the ﬁne-tuning stage of ALBERT-xxlarge-v2 model on the dev set of GLUE. The
implementation is based on HuggingFace’s transformers library. The results are shown in Table 6.
Our model are able to surpass ALBERT on all datasets.
CONCLUSION
In this work, we have developed an adversarial training approach, FreeLB, to improve natural language understanding. The proposed approach adds perturbations to continuous word embeddings
using a gradient method, and minimizes the resultant adversarial risk in an efﬁcient way. FreeLB
is able to boost Transformer-based model (BERT, RoBERTa and ALBERT) on several datasets and
achieve new state of the art on GLUE and ARC benchmarks. Empirical study demonstrates that
our method results in both higher robustness in the embedding space than natural training and better
generalization ability. Such observation is also consistent with recent ﬁndings in Computer Vision.
However, adversarial training still takes signiﬁcant overhead compared with vanilla SGD. How to
accelerate this process while improving generalization is an interesting future direction.
Acknowledgements:
This work was done when Chen Zhu interned at Microsoft Dynamics 365
AI Research. Goldstein and Zhu were supported in part by the DARPA GARD, DARPA QED for
RML, and AFOSR MURI programs.