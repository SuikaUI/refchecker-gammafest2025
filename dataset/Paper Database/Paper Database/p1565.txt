A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts
Yizhe Zhu1,
Mohamed Elhoseiny2,
Bingchen Liu1,
and Ahmed Elgammal1
 ,
 ,
{bingchen.liu, xipeng.cs}@rutgers.edu,
 
1Rutgers University, Department of Computer Science, 2 Facebook AI Research
Most existing zero-shot learning methods consider the
problem as a visual semantic embedding one. Given the
demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage
GANs to imagine unseen categories from text descriptions
and hence recognize novel classes with no examples being
seen. Speciﬁcally, we propose a simple yet effective generative model that takes as input noisy text descriptions about
an unseen class (e.g.Wikipedia articles) and generates synthesized visual features for this class. With added pseudo
data, zero-shot learning is naturally converted to a traditional classiﬁcation problem. Additionally, to preserve the
inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision. Unlike previous methods using complex engineered
regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that
our method consistently outperforms the state of the art on
the largest available benchmarks on Text-based Zero-shot
1. Introduction
In the conventional object classiﬁcation tasks, samples
of all classes are available for training a model. However,
objects in the real world have a long-tailed distribution. In
spite that images of common concepts can be readily found,
there remains a tremendous number of concepts with insuf-
ﬁcient and sparse visual data, thus making the conventional
object classiﬁcation methods infeasible. Targeting on tackling such an unseen object recognition problem, zero-shot
learning has been widely researched recently.
The underlying secret ensuring the success of zero-shot
learning is to ﬁnd an intermediate semantic representation
(e.g. attributes or textual features) to transfer the knowledge
learned from seen classes to unseen ones . The majority of state-of-the-art approaches 
consider zero-shot learning as a visual-semantic embedding
Figure 1: Illustration of our proposed approach. We leverages GANs to visually imagine the objects given noisy
Wikipedia articles. With hallucinated features, a supervised
classiﬁer is trained to predict image’s label.
problem. The paradigm can be generalized as training mapping functions that project visual features and/or semantic
features to a common embedding space. The class label
of an unseen instance is predicted by ranking the similarity scores between semantic features of all unseen classes
and the visual feature of the instance in embedding space.
Such a strategy conducts a one-to-one projection from semantic space to visual space.
However, textual descriptions for categories and objects are inherently mapped to
a variety of points in the image space. For example, “a
blue bird with white head” can be the description of all
birds with a blue body and a white head. This motivates
us to study how adversarial training learns a one-to-many
mapping with adding stochasticity. In this paper, we propose a generative adversarial approach for zero-shot learning that outperforms the state of the art by 6.5% and 5.3%
on Caltech UCSD Birds-2011(CUB) and North America Birds(NAB) datasets respectively.
In this paper, we adopt a novel strategy that casts zeroshot learning as an imagination problem as shown in Fig. 1.
We focus on investigating how to hallucinate competent
data instances that provide the intra-class diversity while
keeping inter-class discrimination for unseen novel classes.
Once this pseudo data is generated, a supervised classiﬁer
is directly trained to predict the labels of unseen images.
Recent years witness the success of generative adversarial networks (GANs) to generate high compelling im-
 
(a) Ground truth features
(b) Synthesized features by our approach
(c) Synthesized features by ACGAN
Figure 2: t-SNE visualization of features from randomly selected unseen classes. The color indicates different class labels.
Groundtruth features are marked as circles and synthesized ones as triangles. Our proposed method provides the intra-class
diversity while preserving inter-class discrimination.
ages. Our approach leverages GANs as a powerful computational model to imagine how unseen objects look like
purely based on textual descriptions. Speciﬁcally, we ﬁrst
extract the semantic representation for each class from the
Wikipedia articles.
The proposed conditional generative
model then takes as input the semantic representations of
classes and hallucinates the pseudo visual features for corresponding classes. Unlike previous methods , our
approach does not need any prior assumption of feature distribution and can imagine an arbitrary amount of plausible
features indeﬁnitely. The idea is conceptually simple and
intuitive, yet the proper design is critical.
Unlike attributes consisting of the discriminative properties shared among categories, Wikipedia articles are rather
noisy as most words are irrelevant to visually recognizing
the objects. Realizing that noise suppression is critical in
this scenario, previous methods usually involve complex designs of regularizers, such as L2,1 norm
in and autoencoder in . In this work, we simply pass textual features through additional fully connected
(FC) layer before feeding it into the generator. We argue
that this simple modiﬁcation achieves the comparable performance of noise suppression and increases the ZSL performance of our method by ∼3%(40.85%vs.43.74%) on
CUB dataset.
Besides, the sparsity of training data(∼60 samples per
class in CUB) makes GANs alone hardly simulate well
the conditional distribution of the high dimensional feature
(∼3500D). As shown in Fig. 2.c, the generated features
disperse enormously and destroy the cluster structure in real
features, thus hardly preserving enough discriminative information across classes to perform unseen image classi-
ﬁcation. To remedy this limitation, we proposed a visual
pivot regularizer to provide an explicit guide for the generator to synthesize features in a proper range, thus preserving enough inter-class discrimination. Empirically, it
aligns well the generated features as shown in Fig. 2.b, and
boosts the ZSL performance of our method from 22.83% to
43.74% on CUB.
Succinctly, our contributions are three-fold:
1) We propose a generative adversarial approach for ZSL
(GAZSL) that convert ZSL to a conventional classiﬁcation
problem, by synthesizing the missing features for unseen
classes purely based on the noisy Wikipedia articles.
2) We present two technical contributions: additional FC
layer to suppress noise, and visual pivot regularizer to provide a complementary cue for GAN to simulate the visual
distribution with greater inter-class discrimination.
3) We apply the proposed GAZSL to multiple tasks,
such as zero-shot recognition, generalized zero-shot learning, and zero-shot retrieval, and it consistently outperforms
state-of-the-art methods on several benchmarks.
2. Related Work
Zero-Shot Learning Strategy As one of the pioneering
works, Lampert et al. proposed a Direct Attribute
Prediction (DAP) model that assumed independence of attributes and estimated the posterior of the test class by
combining the attribute prediction probabilities. Without
the independence assumption, Akata et al. proposed
an Attribute Label Embedding(ALE) approach that considers attributes as the semantic embedding of classes and
thus tackles ZSL as a visual semantic embedding problem.
Consequently, the majority of state-of-the-art methods converges to embedding-based methods. The core of such approaches is to (a) learn a mapping function from the visual
feature space to the semantic space , or conversely , (b) or jointly learn the embedding function between the visual and semantic space through a latent
space .
Apart from the aforementioned methods, a new strategy
converted the zero-shot recognition to a conventional supervised classiﬁcation problem by generating pseudo samples
for unseen classes . Guo et al. assumed a
Gaussian distribution prior for visual features of each class
and estimated the distribution of unseen class as a linear
combination of those of seen classes. Long et al. retained one-to-one mapping strategy and synthesized visual
data via mapping attributes of classes or instances to the
visual space. The number of synthesized data is rigidly restrained by the size of the dataset. Guo et al. drew
pseudo images directly from seen classes that inevitably introduces noise and bias. In contrast, our approach does not
need any prior assumption of data distribution and can generate an arbitrary amount of pseudo data.
Semantic representations Zero-shot learning tasks require
leveraging side information as semantic representations of
classes. Human speciﬁed attributes are popularly utilized
as the semantic representation . Despite the merit of attributes that provide a lessnoisy and discriminative description of classes, the signiﬁcant drawback is that attributes require being manually de-
ﬁned and collected, and ﬁeld experts are often needed for
such annotation, especially in ﬁne-grained datasets .
Many researchers seek handier yet effective semantic
representations based on class taxonomies or text
descriptions .
Compared with class
taxonomies, text descriptions(e.g., Wikipedia articles) are
more expressive and distinguishable. However, Wikipedia
articles are rather noisy with superﬂuous information irrelevant to visual images. In this scenario, TF-IDF features 
are commonly used for textual representation 
due to its superior performance. Elhoseiny et al. proposed an approach to that combines domain transfer and regression to predict visual classiﬁers from a TF-IDF textual
representation. Qiao et al. suppressed the noise in the
text descriptions by encouraging group sparsity on the connections to the textual terms. More recently, Elhoseiny et
al. proposed a learning framework that is able to connect text terms to its relevant parts of objects and suppress
connections to non-visual text terms without any part-text
annotations. Our method also leverages TF-IDF features
while comparably suppressing the non-visual information
without complicated regularizations.
3. Background
In this section, we brieﬂy describe several previous
works that our method is built upon.
3.1. Generative Adversarial Models
Generative
adversarial
networks(GANs)
shown promising performance on generating realistic images . They consist of a generator G
and a discriminator D that contest against each other: the
discriminator tries to distinguish the synthetic data from the
real data while the generator tries to fool the discriminator. Much work has been proposed to
improve GANs by stabilizing training behavior and eliminating mode collapse, via using alternative objective losses.
WGAN leveraged the Wasserstein distance between two
distributions as the objectives, and demonstrated its capability of extinguishing mode collapse. They apply weight clipping on the discriminator to satisfy the Lipschitz constraint.
The following work used additional gradient penalty to
replace weight clipping to get rid of the pathological behavior in .
To involve more side information to guide training procedure, conditional GANs were proposed to condition the
generator and discriminator on some extra information,
such as class labels , texts or even images . Auxiliary Classiﬁer GAN further stabilized
training by adding an extra category recognition branch to
the discriminator. The proposed approach employed AC-
GAN as the basic structure while adopting the Weseertain
distance with gradient penelty for objectives.
3.2. Visual Part Detector/Encoder Network
Instead of CNN-representation of the whole image, Visual Part Detector/Encoder network (VPDE-net) leverages the features of several semantic parts of objects for object recognition. The visual part detector has demonstrated
a superior performance on visual semantic part detection.
The visual part encoding proceeds by feed-forwarding the
images through the conventional network (VGG as backbone) and extracting the visual features from parts detected
from Visual Part Detector via ROI pooling.
The encoding features of each visual part are concatenated as the visual representation of images. Our approach employs the
VPDE-net as our feature extractor of images.
4. Methodology
We start by introducing some notations and the problem deﬁnition. The semantic representations of seen classes
and unseen classes zs
i are deﬁned in the semantic space Z. Assume N s labeled instances of seen classes
i=1 are given as training data, where
i ∈X denotes the visual feature, ys
i is the corresponding class label. Given the visual feature xu
i of a new instance and a set of semantic representation of unseen classes
i=1, the goal of zero-shot learning is to predict the
class label yu. Note that the seen class set S and the unseen class set U are disjointed, S ∩U = ∅. We denote the
generator as G: RZ × RT →RX, the discriminator as D
: RX →{0, 1} × Lcls, where Lcls is the set of class labels. θ and w are the parameters of the generator and the
discriminator, respectively.
The core of our approach is the design of a generative
model to hallucinate the qualiﬁed visual features for unseen
classes that further facilitate the zero-shot learning. Fig. 3
shows an overview of our generative adversarial model. Our
approach adopts the basic framework of GANs for data gen-
Feature Discriminator
Fully Connected
1*512 feature
concatenated 7
feature vectors
VGG Backbone
Feature Extractor on images
� ~ �(0, 1)
All adults are
brownish on
upperparts
with yellow
underparts…
Feature Generator from texts
Fully Connected
Leaky ReLU
Fully Connected
Word Embed
Fully Connected
Text feature
Figure 3: Model overview. Our approach ﬁrst extracts deep visual features from VPDE-net (the orange part). The extracted
visual features are used as the real samples for GAN. In the generator (the red part), noisy texts are embedded to semantic
feature vectors of classes, then passed though a FC layer to suppress noise and reduce the dimensionality. The generator takes
as input the compressed textual feature concatenated with random vector z and produces the synthesized visual feature. The
discriminator(the cyan part) is designed to distinguish the real or fake features and categorizes features to correct classes.
eration. Fed with the semantic representation from the raw
textual description of a class, the generator of our approach
simulates the conditional distribution of visual features for
the corresponding class. We employ VPDE-net to extract
features of images, which serve as real samples. The discriminator is designed to distinguish real or fake features
drawn from the dataset and the generator, and identify the
object categories as well. See Sec 4.1. Additionally, to increase the distinction of synthetic feature cross the classes,
visual pivot regularization is designed as an explicit cue for
the generator to simulate the conditional distribution of features. See Sec 4.1.2. Once the pseudo features are available
for each unseen class, we naturally convert zero-shot learning to a supervised classiﬁcation problem. See Sec 4.2.
4.1. Data Generation
Model Achitecture
Our model mainly consists of three components: Generator
G to produce synthetic features; Feature extractor E to provide the real image features; Discriminator D to distinguish
fake features from real ones.
Generator G: we ﬁrst embed the noisy text description using text encoder φ. The text embedding φ(Tc) of
class c is ﬁrst passed through a fully connected (FC) layer
to reduce the dimensionality. We will show that this additional FC layer has a critical contribution to noise suppression. The compressed text embedding is concatenated to
a random vector z ∈RZ sampled from Gaussian distribution N(0, 1). The following inference proceeds by feeding
it forward through two FC layers associated with two activators - Leaky Rectiﬁed Linear Unit (Leaky ReLU) and
Tanh, respectively. The plausible image feature ˜x is generated via ˜xc ←Gθ(Tc, z). Feature generation corresponds
to the feed-forward inference in the generator G conditioned
on the text description of class c. The loss of generator is
deﬁned as:
LG = −Ez∼pz[Dw(Gθ(T, z))] + Lcls(Gθ(T, z)),
where the ﬁrst term is Wasserstein loss and the second
term is the additional classiﬁcation loss corresponding to
class labels.
Discriminator D: D takes as input the real image features from E or synthesized features from G, and forward
them through a FC layer with ReLU. Following this, two
branches of the network are designed: (i) one FC layer for a
binary classiﬁer to distinguish if the input features are real
or fake. (ii) another FC for n-ways classiﬁer to categorize
the input samples to correct classes. The loss for the discriminator is deﬁned as:
LD =Ez∼pz[Dw(Gθ(T, z))] −Ex∼pdata[Dw(x)] + λLGP
2(Lcls(Gθ(T, z)) + Lcls(x)),
where the ﬁrst two terms approximate Wasserstein distance
of the distribution of real features and fake features, the
third term is the gradient penalty to enforce the Lipschitz
constraint: LGP = λ(|| ▽ˆx Dw(ˆx)||2 −1)2 with ˆx being
the linear interpolation of the real feature x and the fake feature ˜x. We refer readers to for more details. The last
two terms are classiﬁcation losses of real and synthesized
features corresponding to category labels.
Feature Extractor E: Following the small part proposal method proposed in , we adopt fast-RCNN framework with VGG16 architecture as the backbone
to detect seven semantic parts of birds. We feed forward
the input image through VGG16 convolutional layers. The
region proposals by are passed through ROI pooling
layer and fed into an n-way classiﬁer (n is the number of semantic parts plus background) and a bounding box regressor. The proposed region of part p with the highest con-
ﬁdence is considered as detected semantic part p. If the
highest conﬁdence is below a threshold, the part is treated
as missing. The detected part regions are then fed to Visual
Encoder subnetwork, where they pass through ROI pooling layer and are encoded to 512D feature vectors by the
following FC layer. We concatenate the feature vectors of
each visual part as the visual representation of images.
Visual Pivot Regularization
Although the basic architecture provides a way to generating samples with the similar distribution of real visual features, it is still hard to achieve superior simulation. The potential reason is the sparsity of training samples (∼60 images per class in CUB) which makes it hard to learn the distribution of high dimensional of visual feature (∼3500D).
We observe the visual features of seen classes have a higher
intra-class similarity and relatively lower inter-class similarity. See Fig. 2.a. The distribution of visual features clearly
preserves the cluster-structure in X space with less overlap. Motivated by this observation, we want the generated
features of each class to be distributed around if not inside
the corresponding cluster. To achieve it, we designed a visual pivot regularization (VP) to encourage the generator to
generate features of each class that statistically match real
features of that class.
The visual pivot of each class is deﬁned as the centroid
of the cluster of visual features in X space. It can be either the mean calculated by averaging real visual features
or the mode computed via the Mean-shift technique . In
practice, we ﬁnd there is no difference in the performance
of our approach. For simplicity, we adopt the former way,
and the visual pivot corresponds to the ﬁrst order moment
of visual features. To be more speciﬁc, we regularize the
mean of generated features of each class to be the mean of
real feature distribution. The regularizer is formulated as:
||E˜xc∼pcg[˜xc] −Exc∼pc
data[xc]||2,
where C is the number of seen classes, xc is the visual
feature of class c, and ˜xc is the generated feature of class
data are conditional distributions of synthetic
and real features respectively.
Since we have no access
to the real distribution, in practice, we instead use the empirical expectation Exc∼pcg[ˆxc] =
c, where Nc
is the number of samples of class c in the dataset. Similarly, the expectation of synthesized features is approxi-
Algorithm 1 Training procedure of our approach. We use
default values of nd = 5, α = 0.001, β1 = 0.5, β2 = 0.9
1: Input: the maximal loops Nstep, the batch size m, the
iteration number of discriminator in a loop nd, the balancing parameter λp, the visual pivots {¯xc}C
hyperparameters α, β1, β2.
2: for iter = 1, ..., Nstep do
for t = 1, ..., nd do
Sample a minibatch of images x, matching texts
T, random noise z
˜x ←Gθ(T, z)
Compute the discriminator loss LD using Eq. 2
w ←Adam(▽wLD, w, α, β1, β2)
Initialize each set in {Setc}C
Sample a minibatch of class labels c, matching texts
Tc, random noise z
˜x ←Gθ(Tc, z)
Compute the generator loss LG using Eq. 1
Add ˜x to the corresponding sets of {Setc}C
for c = 1,..., C do
reg = ||mean(Setc) −¯xc||2
θ ←Adam(▽θ[LG +λp 1
reg], θ, α, β1, β2)
18: end for
mated by averaging the synthesized visual features for class
c, Exc∼pcg[˜xc] =
i=1 Gθ(Tc, zi), where N s
number of synthesized features for class c. A technique related to our VP regularizer is feature matching proposed in
 , which aims to match statistics in the discriminator’s
intermediate activations w.r.t. the data distribution. Note
that zero-shot learning is a recognition problem that favors
features preserving large intra-class distinction. Compared
with feature matching, matching the statistics of the data
distribution can explicitly make the generator produce more
distinctive features across classes.
Training Procedure
To train our model, we view visual-semantic feature pairs as
joint observation. Visual features are either extracted from a
feature extractor or synthesized by a generator. We train the
discriminator to judge features as real or fake and predict
the class labels of images, as well as optimize the generator to fool the discriminator. Algorithm 1 summarizes the
training procedure. In each iteration, the discriminator is
optimized for nd steps (lines 3 −7), and the generator is
optimized for 1 step (lines 9 −17). To compute VP loss,
we create Nc empty sets and add each synthetic feature ˜x
to the corresponding set w.r.t the class label (line 13). The
loss of each class is Euclidean distance between the mean
of synthesized features and visual pivots (line 15).
4.2. Zero-Shot Recognition
With the well-trained generative model, the visual features of unseen classes can be easily synthesized by the generator with the corresponding semantic representation.
xu = Gθ(Tu, z)
It is worth mentioning that we can generate an arbitrary number of visual features since z can be sampled
indeﬁnitely. With synthesized data of unseen classes, the
zero-shot recognition becomes a conventional classiﬁcation
problem. In practice, any supervised classiﬁcation methods can be employed. In this paper, we simply use nearest
neighbor prediction to demonstrate the ability of our generator.
5. Experiments
5.1. Experiment Setting
We evaluated our method with state-of-theart approaches on two benchmark datasets:
UCSD Birds-2011 (CUB) and North America Birds
(NAB) .
Both are datasets of birds for ﬁne-grained
classiﬁcation.
The CUB dataset contains 200 categories
of bird species with a total of 11,788 images, and NAB
is a larger dataset of birds with 1011 classes and 48,562
images. Elhoseiny et al. extended both datasets by
adding the Wikipedia article of each class, and they also
reorganized NAB to 404 classes by merging subtle division
of classes, such as “American Kestrel (Female, immature)”
and “American Kestrel (Adult male)”. In , two different
split settings were proposed for both datasets, named Super-
Category-Shared and Super-Category-Exclusive splittings,
in term of how close the seen classes are related to the unseen ones. For brevity, we denote them as SCS-split and
SCE-split. In the scenario of SCS-split, for each unseen
class, there exists one or more seen classes that belong to the
same parent category. For instance, both “Cooper’s Hawk”
in the training set and “Harris’s Hawk” in the testing set are
under the parent category “Hawks”. Note that the conventional ZSL setting is SCS-split used in . On
the contrary, in SCS-split, the parent categories of unseen
classes are exclusive to those of the seen classes. Intuitively,
SCE-split is much harder than SCS-split as the relevance
between seen and unseen classes is minimized. We follow
both split settings to evaluate the capability of our approach.
Textual Representation: We use the raw Wikipedia articles collected by for both benchmark datasets. Text
articles are ﬁrst tokenized into words, the stop words are
removed, and porter stemmor is applied to reduce in-
ﬂected words to their word stem. Then, Term Frequency-
Inverse Document Frequency(TF-IDF) feature vector 
is extracted. The dimensionalities of TF-IDF features for
CUB and NAB are 7551 and 13217.
Visual Features: We extract the visual features from the
activations of the part-based FC layer of VPDE-net. All
input images are resized to 224 × 224 and fed into the
VPDE-net. There are seven semantic parts in CUB dataset:
“head”, “back”, “belly”, “breast”, “leg”, “wing”, “tail”.
NAB dataset contains the same semantic parts except for
“leg”. For each part, a 512-dimensional feature vector is
extracted. Concatenating those part-based features in order, we obtain the visual representation of the image. The
dimensionalities of visual features for CUB and NAB are
3583 and 3072 respectively.
The implementation details of our model and the parameter settings can be found in the supplementary material.
5.2. Zero-Shot Recognition
Competing Methods: The performance of our method is
compared to seven state-of-the-art algorithms: ZSLPP ,
MCZSL , ZSLNS , ESZSL , SJE , WAC ,
SynC . The source code of ZSLPP, ESZSL, and ZSLNS
are available online, and we get the code of WAC from
its author. For MCZSL and SJE, since their source codes
are not available, we directly copy the highest scores for
non-attribute settings reported in .
We conduct the experiments on both SCS and SCE splits
on two benchmark datasets to show the performance of
our approach.
Note that some of the compared methods are attribute-based methods but applicable in our setting by replacing the attribute vectors with textual features.
Among these methods MCZSL and ZSLPP leverage the semantic parts of birds for visual representations of images.
MCZSL directly uses part annotations as strong supervision to extract CNN representation of each semantic part
in the test phase. Unlike MCZSL, our approach and ZSLPP
are merely based on the detected semantic parts during both
training and testing. The performance of the ﬁnal zero-shot
classiﬁcation is expected to degrade due to less accurate detection of semantic parts compared to manual annotation
in MCZSL. Table 1 shows the performance comparisons
on CUB and NAB datasets. Generally, our method consistently outperforms the state-of-the-art methods. On the conventional split setting (SCS), our approach outperforms the
runner-up (ZSLPP) by a considerable gap: 6.5% and 5.3%
on CUB dataset and NAB dataset, respectively. Note that
ZSL on SCE-split remains rather challenging. The fact that
there is less relevant information between the training and
testing set makes it hard to transfer knowledge from seen
classes to unseen classes. Although our method just improves the performance by less than 1%, we will show the
great improvement on the general merit of ZSL in Sec 5.3
WAC-Linear 
WAC-Kernel 
ESZSL 
ZSLNS 
SynCfast 
SynCOV O 
ZSLPP 
Top-1 accuracy (%) on CUB and NAB datasets
with two split settings.
Ablation Study
We now do the ablation study of the effect of the visual pivot
regularization(VP) and the GAN. We trained two variants
of our model by only keeping the VP or GAN, denoted as
VP-only and GAN-only, respectively. Speciﬁcally, in the
case of only using VP, our model discards the discriminator
and thus is reduced to a visual semantic embedding method
with VP loss. The generator essentially becomes a mapping
function that projects the semantic feature of classes to the
visual feature space. It is worth mentioning that compared
to previous linear embedding methods that
usually have one or two projection matrices, our generator
has a deeper architecture with three projection matrices if
we can roughly treat FC layers as project matrices. Additionally, our generator also differs in adding nonlinearity by
using Leaky-ReLU and Tanh as the activation function as
shown in Fig. 3.
Table 2 shows the performance of each setting. Without
the visual pivot regularization, the performance drops drastically by 20.91% (22.83%vs.43.74%) on CUB and 11.36%
(24.22%vs.35.58%) on NAB, highlighting the importance
of the designed VP regularizer to provide a proper explicit
supervision to GAN. Interestingly, we observe that only using VP regularizer as the objective of the generator achieves
the accuracy of 28.52% on CUB and 25.75% on NAB,
which are even higher than that of GAN-only model. This
observation naturally introduces another perspective of our
approach. We can regard our approach as a visual semantic embedding one with the GAN as a constraint. We argue
that generative adversarial model and the visual pivot regularization are critically complementary to each other. GAN
makes it possible to get rid of the one-to-one mapping existing in previous embedding methods by generating diverse
embeddings on descriptions. On the other hand, VP regularization attempts to restrict such imaginary samples of the
generator within the proper range, thus keeping the discrimination of generated features across the classes. The importance of both components is veriﬁed by the superior performance of our complete approach compared to two variants.
ESZSL: 0.185
ZSLNS: 0.147
WACkernal : 0.225
WAClinear : 0.239
SynCFast : 0.131
SynCOvO : 0.017
ZSLPP: 0.304
Ours: 0.354
(a) CUB with SCS splitting
ESZSL: 0.045
ZSLNS: 0.044
WACkernal : 0.054
WAClinear : 0.049
SynCFast : 0.040
SynCOvO : 0.010
ZSLPP: 0.061
Ours: 0.087
(b) CUB with SCE splitting
ESZSL: 0.092
ZSLNS: 0.093
WACkernal : 0.007
WAClinear : 0.235
SynCFast : 0.027
SynCOvO : 0.001
ZSLPP: 0.126
Ours: 0.204
(c) NAB with SCS splitting
ESZSL: 0.029
ZSLNS: 0.023
WACkernal : 0.023
SynCFast : 0.008
ZSLPP: 0.035
Ours: 0.058
(d) NAB with SCE splitting
Figure 4: Seen-Unseen accuracy Curve on two benchmarks
datasets with two split settings
Table 2: Effects of different components on zero-shot classiﬁcation accuracy (%) on CUB and NAB datasets with SCS
split setting.
We also analyzed the effectiveness of our additional FC
layer on textual features for noise suppression. As shown in
Table 2, in general, our method with FC layer outperforms
one without FC layer by 2% to 3% in most cases. The
superiority can also be observed in two variants. In practice, the high dimensional TF-IDF feature is compacted to a
1000D feature vector. Unlike the traditional dimensionality
reduction technique (e.g., PCA), FC layer contains trainable
weights and is optimized in an end-to-end fashion.
5.3. Generalized Zero-Shot Learning
The conventional zero-shot recognition considers that
queries come from only unseen classes. However, as the
seen classes are often the most common objects, it is unrealistic to assume that we will never encounter them during
the test phase . Chao et al. presented a more general
metric for ZSL that involves classifying images of both seen
classes S and unseen classes U into T = S ∪U. The accuracies are denoted as AS→T and AU→T respectively. They
introduced a balancing parameter λ to draw Seen-Unseen
accuracy Curve(SUC) and use Area Under SUC to measure the general capability of methods for ZSL. In our case,
we use the trained GAN to synthesize the visual features
of both training classes and testing classes. The visual features of each class are averaged to obtain the visual pivots.
The nearest neighbor strategy to visual pivots is adopted to
predict the class label of images.
In Fig. 4, we plot SUC and report the AUSUC scores
of our method and the competitors. Our method compares
favorably to competitors in all cases except on NAB with
SCS-split, where very high AS→T and low AU→T indicate
that WAClinear is overﬁtting the dataset. It’s worth noting
that although our method only slightly outperforms competitors in the zero-shot recognition task with SCE splitting,
the AUSUC scores of our method are 42.6% and 56.7%
higher than those of the runner-up on CUB and NAB respectively, indicating a superior and balanced performance
on AS→T and AU→T .
5.4. Zero-Shot Retrieval
Zero-shot image retrieval is deﬁned as retrieving images
giving the semantic representation of an unseen class as the
query. We use mean average precision (mAP) to evaluate
the performance. In Table 3, we report the performance of
different settings: retrieving 25%, 50%, 100% of the number of images for each class from the whole dataset. The
precision is deﬁned as the ratio of the number of correct
retrieved images to that of all retrieved images. We adopt
the same strategy as in GZSL to obtain the visual pivots of
unseen classes. Given the visual pivot, we retrieve images
based on the nearest neighbor strategy in X space.
ESZSL 
ZSLNS 
ZSLPP 
Table 3: Zero-Shot Retrieval using mean Average Precision
(mAP) (%) on CUB and NAB with SCS splitting.
Overall, our method outperforms competitors by 4% ∼
7%. We discover that although VP-only performs better
than GAN-only on the recognition task, its performance is
inferior to GAN-only on retrieval tasks. Without the constraints of WGAN, VP-only suffers from heavy mode collapse, and synthetic features easily collapse into the visual
pivot of each class with less diversity.
We also provide qualitative results of our method as
shown in Fig. 5. Each row is one class, and the class name
and the precision are shown on the left. The ﬁrst column
is Top-1 within-class nearest neighbor. The following ﬁve
Great Grey
Flycatcher:
Chickadee:
Scrub-Jay:
Figure 5: Qualitative results of zero-shot retrieval. The ﬁrst
three rows are classes from CUB and the rest from NAB.
Correct and incorrect retrieved instances are shown in green
and red respectively.
columns are Top-5 overall nearest neighbors without considering the instances in the ﬁrst column.
5.5. t-SNE Demonstration
Fig. 2 demonstrates the t-SNE visualization of the
real features, the synthesized features for unseen classes under different settings. Despite some overlaps, the real features roughly distribute in separate clusters w.r.t class labels. The features generated by our method keep the same
structure. Ideally, we expect the distribution of generated
features matches that of the real ones in X space. Empirically, despite that some deviating clusters of synthesized
features due to the bias between testing and training data,
many clusters are well aligned to the real ones. Without the
VP regularizer, GAN encourages the high diversity of features with no constraints. The features disperse greatly and
lack the discrimination across classes.
6. Conclusion
In this paper, we proposed a novel generative adversarial
approach for ZSL, which leverages GANs to dispersively
imagine the visual features given the noisy textual descriptions from Wikipedia. We introduced the visual pivot regularizer to explicitly guide the imagery samples of GANs
to the proper direction. We also showed that adding a FC
layer for textual feature results in comparable noise suppression. Experiments showed that our approach consistently performs favorably against the state-of-the-art methods on multiple zero-shot tasks, with an outstanding capability of visual feature generation.
Acknowledgment.
This work was supported NSF-
USA award #1409683.