GAG: Global Attributed Graph Neural Network for Streaming
Session-based Recommendation
Ruihong Qiu
The University of Queensland
Brisbane, Australia
 
Hongzhi Yinâˆ—
The University of Queensland
Brisbane, Australia
 
The University of Queensland
Brisbane, Australia
 
The University of Queensland
Brisbane, Australia
 
Streaming session-based recommendation (SSR) is a challenging
task that requires the recommender system to do the session-based
recommendation (SR) in the streaming scenario. In the real-world
applications of e-commerce and social media, a sequence of useritem interactions generated within a certain period are grouped as
a session, and these sessions consecutively arrive in the form of
streams. Most of the recent SR research has focused on the static setting where the training data is first acquired and then used to train
a session-based recommender model. They need several epochs of
training over the whole dataset, which is infeasible in the streaming setting. Besides, they can hardly well capture long-term user
interests because of the neglect or the simple usage of the user
information. Although some streaming recommendation strategies
have been proposed recently, they are designed for streams of individual interactions rather than streams of sessions. In this paper, we
propose a Global Attributed Graph (GAG) neural network model
with a Wasserstein reservoir for the SSR problem. On one hand,
when a new session arrives, a session graph with a global attribute
is constructed based on the current session and its associate user.
Thus, the GAG can take both the global attribute and the current
session into consideration to learn more comprehensive representations of the session and the user, yielding a better performance in
the recommendation. On the other hand, for the adaptation to the
streaming session scenario, a Wasserstein reservoir is proposed to
help preserve a representative sketch of the historical data. Extensive experiments on two real-world datasets have been conducted
to verify the superiority of the GAG model compared with the
state-of-the-art methods.
âˆ—Corresponding author and contributing equally with the first author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
SIGIR â€™20, July 25â€“30, 2020, Virtual Event, China
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-8016-4/20/07...$15.00
 
CCS CONCEPTS
â€¢ Information systems â†’Recommender systems.
streaming recommendation, session-based recommendation, graph
neural networks
ACM Reference Format:
Ruihong Qiu, Hongzhi Yin, Zi Huang, and Tong Chen. 2020. GAG: Global
Attributed Graph Neural Network for Streaming Session-based Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference
on Research and Development in Information Retrieval (SIGIR â€™20), July
25â€“30, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages.
 
INTRODUCTION
In many modern online systems, such as e-commerce and social
media platforms, there usually exist a large number of interactions
between users and items, such as clicking goods and playing songs.
As illustrated in Fig. 1, a sequence of interactions occurring in a
certain period can be considered as a session. Session-based recommendation (SR) has been widely studied recently by the academia
and the industry , which aims to recommend
items to users based on sessions. However, most of them focus on
the static setting, which is not suitable in real-life situation.
In practice, sessions are dynamically produced as a stream, which
leads to urgent requirements of streaming session-based recommendation (SSR). As presented in Fig. 1, a general procedure for SSR is
to train the recommendation model with the historical sessions to
preserve the usersâ€™ long-term interests and then conduct the online
update with the streaming sessions to adapt to their recent preferences. Most of the current research for the SR focuses on the static
scenario, where the recommendation models are trained in a batch
way. As usersâ€™ preferences are changing over time, it is infeasible to
apply a static model for new coming sessions. To precisely capture
the user preference, the model needs to be online updated with
the latest sessions. Due to the memory space limit, it is unpractical
to stack up the training data by absorbing every new session. In
the meanwhile, the training time will be another concern in the
streaming scenario, where the recommendation model is expected
to be updated promptly. However, a typical SR model needs to train
for a long time to converge, which cannot be guaranteed within a
short period.
 
ğ‘£6 ğ‘£1 ğ‘£2 ğ‘£1
ğ‘£5 ğ‘£23 ğ‘£17 ğ‘£6 ğ‘£5
ğ‘£4 ğ‘£5 ğ‘£4 ğ‘£4
ğ‘£58 ğ‘£2 ğ‘£9 ğ‘£32 ğ‘£47 ğ‘£32
Session Stream
Historical Sessions
Offline Training
Online Update
Figure 1: The general framework for SSR consists of two
phases: the offline training and the online update. During
the offline training, the recommender system is trained in
a static style with the whole historical session data. When
newly generated sessions arrive, the model is expected to
conduct an efficient online update with streaming techniques to preserve the usersâ€™ long-term interests and adapt
to the newest preferences.
Recently, a few methods utilize the reservoir technique for streaming tasks . In these cases, the interaction data is stored
with the same probability in the reservoir and then sampled for
the online training of the model. However, if streaming sessions
are processed in such a manner, the SSR models will suffer from
the loss of the session information because the data samples are
stored and drawn as discrete interactions. Moreover, the reservoir
for traditional streaming tasks is designed to capture the matrix
factorization information rather than the sessionâ€™s sequence pattern. Besides, online learning can barely adapt to the session-based
recommendation task for newly arrived data as well. For online
learning, when a new session comes, the model will update accordingly to capture the recent transition pattern in the latest session.
However, these models will easily overfit the new data and fail
to maintain usersâ€™ long-term preferences learned from historical
data. Therefore, it is important for the SSR model to effectively
exploit the userâ€™s information and thus obtain a comprehensive
representation for both long- and short-term preferences.
More recently, Guo et al. applied the reservoir technique to
the SSR task with a weighted sampling scheme by evaluating how
informative each session is. This method cannot be generalized
to other models mainly because it needs to generate an informativeness score for every item in a session with pre-computed item
feature vectors, which are commonly unavailable in other models.
Moreover, this model directly combines the session-based method
with a matrix factorization module for recommendation, which can
hardly learn the complicated correlations between users and items
in the SSR problem.
To address the issues discussed above, we propose a Global
Attributed Graph (GAG) neural network model with a Wasserstein
reservoir as a solution to SSR. To make the most usage of the user
embeddings and maintain long-term preference information for
SSR, we firstly convert a userâ€™s session sequence into a session
graph having the user embedding as a global attribute associated
with the embeddings of interacted items. Based on the global attributed session graph, the GAG model performs graph convolution to
learn an updated global attribute, which is passed to the ranking
module to output a recommendation list. In the GAG model, the
global attribute is applied to effectively assist the joint representation learning of both the entire session and the items within the
session. To develop a general reservoir for the SSR problem, we propose the Wasserstein reservoir, which stores and samples session
data according to the Wasserstein distance between the generated
recommendation lists and the userâ€™s real interactions. During the
sampling procedure, the Wasserstein reservoir samples the sessions
whose recommendation results have a higher Wasserstein distance.
Intuitively, the model makes worse predictions in these sessions
with a higher Wasserstein distance, which is more informative to
refine the model during the online update.
The main contributions of this paper are summarized as follows:
â€¢ We propose the GAG model to effectively memorize and
incorporate usersâ€™ long-term preferences into the embedding vectors for SSR by treating the user embedding as a
global attribute for the session graphs to allow for more
expressiveness when learning representations.
â€¢ A Wasserstein reservoir is designed to actively select the
most informative training cases for updating the model in
streaming settings. Moreover, our Wasserstein reservoir is
an effective yet generic online learning approach that can
be easily applied to other steaming session data.
â€¢ Extensive experimental results on two real-world datasets
demonstrate that the proposed GAG model and the Wasserstein reservoir achieve the state-of-the-art performance.
RELATED WORK
Session-based Recommendation
Sequential recommendation is mainly based on the Markov
chain model , which learns the dependency of items
in a sequence data. Using probabilistic decision-tree models, Zimdars et al. proposed to encode the state of the transition pattern
of items. Shani et al. made use of a Markov Decision Process
(MDP) to compute item transition probabilities.
Deep learning models are popular with the boom of recurrent
neural networks . Hidasi et al. proposed the
GRU4REC, which applies the GRU to treat the data as time
series. Some recent approaches use the attention mechanism to
avoid the time order. NARM stacks GRU as the encoder to
extract information and then a self-attention layer to sum up as
the session embedding. To further alleviate the bias by time series,
STAMP replaces the recurrent encoder with the attention layer.
Recently, GNN has been widely used in recommendation . Some methods utilize GNN to encode the session information
to prevent the misguidance of the session order .
SSRM considers a specific userâ€™s historical sessions and applies
the attention mechanism to combine them.
Streaming Recommendation
Online learning focuses on updating the old model with the new
data to capture the most recent interest of the user .
For instance, He et al. proposed an element-wise alternative
to the least squares technique to address the missing data. Jugovac
et al. applied a replay-based evaluation protocol to update
the model with the new arrival events and articles in the news
recommendation. Although the models above capture the userâ€™s
recent interest by updating the model with new interactions, they
fail to remember historical interactions.
Random sampling is a technique to address the history-ignoring
problem by introducing a reservoir to store the userâ€™s long-term
interactions . For example, Diaz-Aviles et al. 
applied sampling strategies based on active learning principles
on the matrix factorization method to update the model. More recently, Guo et al. used the same reservoir technique to process
the streaming sessions.
Graph Neural Networks
Originally, GNN is applied basically on directed graphs in a simple
situation . In recent years, many GNN methods work
very similar to the message passing network to perform an
aggregation over the neighborhood of nodes to compute the node
embeddings. In , the global attribute is introduced into the
GNN layer to maintain a graph level feature in physical systems.
Task Definition
In the SSR problem, there is an item set V = {ğ‘£1, ğ‘£2, ğ‘£3, . . . , ğ‘£ğ‘š},
where all items are unique and ğ‘šdenotes the number of items.
Usually, an embedding layer is applied to represent all items, xi =
Embedğ‘£(ğ‘£ğ‘–),ğ‘–â‰¤ğ‘š, where Embedğ‘£is a mapping function that transforms an item into a continuous and dense representation xi âˆˆRğ‘‘.
There is also a user set U = {ğ‘¢1,ğ‘¢2,ğ‘¢3, . . . ,ğ‘¢ğ‘›}, where all users are
unique andğ‘›denotes the number of users. Similarly, another embedding layer performs mapping to all user ID, uj = Embedğ‘¢(ğ‘¢ğ‘—), ğ‘—â‰¤ğ‘›,
where Embedğ‘¢is another mapping function from a user to pj âˆˆRğ‘‘.
A session sequence at a time step ğ‘¡from a user ğ‘¢is defined as a list
ğ‘†ğ‘¢,ğ‘¡= [ğ‘£1, ğ‘£2, . . . , ğ‘£ğ‘™], ğ‘£âˆ—âˆˆV. ğ‘™is the length of the session ğ‘†, which
may contain duplicated items, ğ‘£ğ‘= ğ‘£ğ‘, ğ‘,ğ‘< ğ‘™. In the setting of the
SSR, at time step ğ‘¡, the recommender system needs to recommend
an item ğ‘£ğ‘¡+1 based on {ğ‘†ğ‘¢,0,ğ‘†ğ‘¢,1, . . . ,ğ‘†ğ‘¢,ğ‘¡}, which are all sessions of
a user from the history to the current. The item ğ‘£ğ‘¡+1 should match
the userâ€™s preference the most. In the meantime, sessions arrive at a
high speed, which means that the computation resource is limited
to calculation. As a result, an algorithm should have an efficient
way to process the history sessions as well as the current session.
Usually, we only recommend the top-ğ¾ranked items to users.
In this paper, we propose a novel Global Attributed Graph (GAG)
neural network model to address the SSR problem mainly by transforming a userâ€™s information into the global attribute and incorporating it in the session graph. The architecture of the GAG model is
demonstrated in Fig. 2. There are two key components: GAG model
for generating recommendation and Wasserstein reservoir for the
streaming data learning.
Global Attributed Session Graph
As shown in Fig. 2, at the first stage, the session sequence is converted into a session graph with a global attribute for the purpose
to process each session via GNN. Similar to and , because
of the natural order of the session sequence, we convert it into
a weighted directed graph. In addition, we incorporate the userâ€™s
general information as the global attribute ğ‘¢into the session graph,
ğºğ‘ = (u,ğ‘‰ğ‘ , ğ¸ğ‘ ), ğºğ‘ âˆˆG, where G is the set of all session graphs.
In the session graph ğºğ‘ , the node set ğ‘‰ğ‘ represents the nodes in
the session graph, which are items ğ‘£ğ‘ ,ğ‘›from ğ‘†. For every node
Item set ğ’±
Session graph ğºğ‘ 
Recommendation Procedure
Training Data
Wasserstein
Figure 2: The pipeline of the GAG model for the SSR problem. In the upper half, for a specific session ğ‘†from a user
ğ‘¢, the GAG model first converts it into a global attributed
session graph ğºğ‘ . The GAG layer takes ğºğ‘ as input and computes the graph convolution based on node features, edge
weights and the global attribute. The output of the ğ‘˜-layer
GAG model is the updated global attribute uk. To make a personalized recommendation, uk is applied to compare with
the whole item set to generate the recommendation list. In
the bottom half, it shows the procedure of the Wasserstein
reservoir dealing with streaming sessions. New streaming
sessions and the current reservoir are united together and
sampled the online training data according to their respective Wasserstein distance. The online training data is fed to
help with the update of the current GAG model. After the
online training of the model, the reservoir is updated with
the session stream and itself. The updated reservoir is for
the following new arrival sessions.
v, the input feature is the initial embedding vector x. The edge
set ğ¸ğ‘ represents all directed edges (ğ‘¤ğ‘ ,(ğ‘›âˆ’1)ğ‘›, ğ‘£ğ‘ ,ğ‘›âˆ’1, ğ‘£ğ‘ ,ğ‘›), where
ğ‘¤ğ‘ ,(ğ‘›âˆ’1)ğ‘›is the weight of the edge and ğ‘£ğ‘ ,ğ‘›is the click of the item
after ğ‘£ğ‘ ,ğ‘›âˆ’1 in ğ‘†. The weight of the edge is defined as the frequency
of the occurrence of the edge within the session.
Global Attributed Graph Neural Network
With the construction of the global attributed session graph ğºğ‘ , we
propose the GAG model to perform graph convolution on ğºğ‘ with
the node features, edge features and the global attribute. When the
GAG model is fed with the session graph as the input, the computation proceeds from the edge, the node to the global attribute.
First, the per-edge update is calculated among all edges to compute the output features from sender nodes vsk to receiver nodes
vrk with additional features of the edge itself ek and the global
attribute u. In our design of GAG, the edge feature, i.e., the weight
of the edge, will not be updated because the edge feature is not in
the dense vector form. This setting means that the output of the
edge update function ekâ€², will only be used to update other node
features and global features. The output new session graph ğºâ€²ğ‘ has
the same edge set ğ¸ğ‘ as the input session graph ğºğ‘ . Because the
session graph is built in the directed situation, we compute the
propagation in both directions to represent the different meanings
for a node as a sender and a receiver in an edge. Therefore, the ğœ™ğ‘’
Session graph ğºğ‘ 
Figure 3: The usage of the global attribute in GAG. (a) In
the input stage, the global attribute is concatenated with the
node feature for every node, which gives out a node feature
concatenated with the global attribute. (b) In the global attribute update procedure, the attention weight ğ›¼ğ‘–is calculated based on the concatenation of the features of the last
node vl, the individual node vi and the global attribute u itself.
function is designed as:
ek,inâ€² = ğœ™ğ‘’
 ek, vrk, vsk, u
= ğ‘¤ğ‘˜Â· MLP(vsk ||u),
ek,outâ€² = ğœ™ğ‘’
 ek, vrk, vsk, u
= ğ‘¤ğ‘˜Â· MLP(vsk ||u),
where ğ‘¤ğ‘˜is the scalar form of ek, MLP stands for the multi-layer
perceptron to encode the features provided by a concatenation
of the sender and receiver node and || means the concatenation
between two vectors along the unit dimension. MLPs in both equations are not shared because they perform different operations to
the node features. In Fig. 3 (a), the procedure of exploiting the global
attribute in the node update function is demonstrated in detail.
After updating the new per-edge features, per-node features are
updated based on the per-edge features when the node is the sender
and the receiver. The new per-node feature consists of the normalized summation of the in-coming and out-going neighborhoods.
The aggregation procedures are as:
ğ‘—âˆˆ{vsj=vri }
ğ‘ğ‘–ğ‘›(ğ‘–)ğ‘ğ‘œğ‘¢ğ‘¡(ğ‘—)
ğ‘—âˆˆ{vrj=vsi }
ğ‘ğ‘œğ‘¢ğ‘¡(ğ‘–)ğ‘ğ‘–ğ‘›(ğ‘—)
where ğ‘ğ‘–ğ‘›(âˆ—) and ğ‘ğ‘œğ‘¢ğ‘¡(âˆ—) represent the in-coming and the outgoing degree of a node.
The final result of the neighborhood aggregation is a linear
transformation of the in-coming and the out-going feature:
viâ€² = MLP(vi,inâ€²||vi,outâ€²).
The updated node feature viâ€² actually includes the information
from the node feature of itself and the neighborhood, the edge
weight and the global attribute.
At the last step of the GAG layer forward computations, the
global attribute is updated based on all the features of nodes, edges
and the global attribute itself in the graph. It is worth noting that
the purpose of the session-based recommendation is to generate a
representation of a session to recommend items. Therefore, the final
global attribute is exactly the representation we desire to represent
the whole graph. Similar to the previous methods to separate the
representation of the preference in long-term and short-term parts
inside a session , a self-attention on the last input item ğ‘£ğ‘™,
of the session is applied to aggregate all item features of the session
to be the session-level feature. The computation of updating u to
usg is defined as:
uâ€² = ğœ™ğ‘¢ ğ‘‰â€², u
= Self-Att(ğ‘£â€²
ğ‘–, u) + u,
where ğ‘£ğ‘–âˆˆğ‘‰â€²,ğ‘–= 1, 2, 3, . . . ,ğ‘™represent all items in the session
after being updated to the new features. In the setting of the session graph, items are converted into nodes and the Self-Att can be
divided into two steps:
ğ›¼ğ‘–= MLP(vlâ€²||viâ€²||u),
where an MLP is utilized to learn the weights that aggregate the
node features, the last node feature and the global attribute. In Fig. 3
(b), the detail of the computation of attention weights is presented.
Besides, because the userâ€™s profile is applicable in the SSR setting,
the incorporation of the user embedding can provide the extra user
information. Therefore, the final formula for how to compute the
output of the global attribute with user information is defined as:
uâ€² = usg + u.
The residual addition can help to alleviate the burden of directly
learning the updated global attribute.
Recommendation
The last stage of the GAG to perform the recommendation is the
generation of candidate items based on the representation of the
input session and the userâ€™s profile. We compute a score for every
item and form a score vector Ë†z âˆˆRğ‘›, where n is the size of the item
set. Specifically, the score vector Ë†z is calculated as:
Ë†z = uâ€²âŠ¤X,
where X is embeddings of all items in the item set.
The probabilistic form of the prediction Ë†y is defined as:
Ë†y = Softmax(Ë†z).
Wasserstein Reservoir for Streaming Model
In this section, we extend our offline model to the streaming setting.
Our purpose is to update our model with the new arrival session
data while keeping the knowledge learned from the historical sessions. Traditionally, online learning methods update the model only
with the new data, which will always lead to forgetting the past .
To prevent the model from losing the awareness of historical data,
we leverage the reservoir to maintain a long-term memory of the
historical data . The reservoir technique is widely used
in the streaming database management systems.
The purpose of applying a reservoir is to maintain a representative sketch of all the historical data. Therefore, we conduct a
random sampling to select the data stored in the reservoir.
Let ğ¶denote the reservoir, which contains |ğ¶| sessions. Let ğ‘¡be
the time order of the arrival session instance. When ğ‘¡> |ğ¶|, the
reservoir will store this ğ‘¡-th session with the probability:
ğ‘store = |ğ¶|
and replaces a uniformly random session that is already in ğ¶. This
method of generating the reservoir is actually sampling randomly
from the current dataset, and it can successfully maintain the
modelâ€™s long-term memory .
Although the reservoir can be updated in the way introduced
above, the probability for a new arrival session to be included tends
to be smaller over time, and the reservoir will have a chance of
overlooking the recent data. However, the recent data is crucial for
predicting the userâ€™s varying preference. Besides, new users and
new items are exposed to the system continually. In consequence,
when the new session data ğ¶ğ‘›ğ‘’ğ‘¤arrives, we update the pre-trained
model with ğ¶ğ‘›ğ‘’ğ‘¤and the reservoir ğ¶.
The reservoir sampling procedure above enables the model to
continually update according to the new and old data. However, it
can narrowly achieve a good performance in reality. The reason
is that most training sessions in ğ¶are already learned well by ğ‘€,
which results in ğ¶ğ‘Ÿğ‘ğ‘›ğ‘‘mainly containing helpless training samples.
Actually, if the current model makes a worse prediction on a session, it is more worthwhile to update the model with this session
because it either contains the latest preference of a user or there is
some item transition patterns that the current model cannot learn
well. Such a session is called an informative session to the model
and this session is more significant to the model update. In our
work, the informativeness of a session is defined as the distribution
distance ğ‘‘between its predicted recommendation result Ë†y and the
real interaction y. Ë†y is a distribution given by the model ğ‘€and y is
a one-hot vector. Intuitively, the greater of this distance, ğ‘€predicts
the worse over the session. There are different distance metrics of
two distributions:
â€¢ The Wasserstein distance (EMD distance) :
ğ‘‘ğ‘Š(Pğ‘Ÿ, Pğ‘”) =
ğ›¾âˆˆÎ (Pğ‘Ÿ,Pğ‘”)
E(ğ‘¥,ğ‘¦)âˆ¼ğ›¾[âˆ¥ğ‘¥âˆ’ğ‘¦âˆ¥]
â€¢ The Kullback-Leibler (KL) divergence :
ğ‘‘ğ¾ğ¿(Pğ‘Ÿâˆ¥Pğ‘”) =
ğ‘ƒğ‘Ÿ(ğ‘¥) log ğ‘ƒğ‘Ÿ(ğ‘¥)
â€¢ The Total Variation (TV) distance:
ğ‘‘ğ‘‡ğ‘‰(Pğ‘Ÿ, Pğ‘”) = sup
Pğ‘Ÿ(ğ´) âˆ’Pğ‘”(ğ´)
Within the recommendation task, the real distribution ğ‘ƒğ‘Ÿis always a one-hot vector y. Under this situation, the KL divergence
between y and Ë†y, ğ‘‘ğ¾ğ¿(yâˆ¥Ë†y) is:
ğ‘‘ğ¾ğ¿(yâˆ¥Ë†y) = âˆ’log ğ‘ƒğ‘”(ğ‘£ğ‘–),
where ğ‘ƒğ‘”(ğ‘£ğ‘–) is the predicted probability over the ground truth item
ğ‘£ğ‘–given by the model ğ‘€. Therefore, the KL divergence fails to take
the whole distribution into consideration.
As for the TV distance, ğ‘‘ğ‘‡ğ‘‰(y, Ë†y) is:
ğ‘‘ğ‘‡ğ‘‰(y, Ë†y) = max
ğ‘—â‰ ğ‘–(1 âˆ’ğ‘ƒğ‘”(ğ‘£ğ‘–), ğ‘ƒğ‘”(ğ‘£ğ‘—)),
Algorithm 1 Online Update with Wasserstein Reservoir
Input: the current time step ğ‘¡, the current model ğ‘€, the current
reservoir ğ¶and new sessions ğ¶ğ‘›ğ‘’ğ‘¤;
Output: the updated model ğ‘€â€² and the updated reservoir ğ¶â€²;
1: initialize a blank update dataset ğ‘†;
2: if the last epoch finished then
for each session ğ‘ ğ‘–in ğ¶ğ‘›ğ‘’ğ‘¤do
if a new item or a new user appears then
append ğ‘ ğ‘–to ğ‘†;
for each session ğ‘ ğ‘–in ğ¶âˆªğ¶ğ‘›ğ‘’ğ‘¤âˆ’ğ‘†do
compute the Wasserstein distance ğ‘‘ğ‘–of ğ‘ ğ‘–;
compute the sample probability ğ‘(ğ‘ ğ‘–);
sample the the rest of ğ‘†according to Eq. (18);
13: end if
14: update the current model ğ‘€with ğ‘†to ğ‘€â€²;
15: for each session ğ‘ ğ‘–in ğ¶ğ‘›ğ‘’ğ‘¤do
update the reservoir with ğ‘ ğ‘–according to Eq (12) to ğ¶â€²;
18: end for
where ğ‘ƒğ‘”(ğ‘£ğ‘—) is the predicted probability over the item ğ‘£ğ‘—given by
the model ğ‘€other than the ground truth item ğ‘£ğ‘–. The TV distance
either captures the difference over the real interacted item or other
unrelated items.
Obviously, both KL divergence and TV distance have drawbacks
of focusing on a certain elementary event while neglecting the
whole distribution. The only metric that can preserve the difference
at each prediction score is the Wasserstein distance.
Therefore, we propose a Wasserstein reservoir construction strategy that samples the session whose output probability distribution
over the recommendation item has a higher Wasserstein distance
to the userâ€™s real interaction with a higher probability. Intuitively,
when the output probability of a session has higher Wasserstein
distance, the output will contain more information compared with
those with lower Wasserstein distance. Therefore, we sample sessions according to the Wasserstein distance of their output probabilities. For session ğ‘ ğ‘–with corresponding Wasserstein distance ğ‘‘ğ‘–
for its output, the sampling probability is calculated as follows:
ğ‘sample(ğ‘ ğ‘–) =
ğ‘ ğ‘—âˆˆğ¶âˆªğ¶ğ‘›ğ‘’ğ‘¤âˆ’ğ‘†ğ‘‘ğ‘—
where ğ‘†is the ongoing updated dataset. The detailed construction
of the Wasserstein reservoir is shown in Algorithm 1. During the
sampling procedure, because there are always new items and new
users in the streaming data, their corresponding embedding vectors
are not trained before they show up. To prevent the model from
neglecting the new sessions, these sessions are directly included.
Since the recommendation task is considered as a classification
problem over the whole item set, we can apply a multi-class crossentropy loss between the predicted recommendation distribution Ë†y
and the real interaction y:
yğ‘–log (Ë†yğ‘–) ,
where ğ‘™is the number of training sessions in a mini-batch.
EXPERIMENT SETUP
LastFM 1 is a real-world music recommendation dataset, which is
released by Celma Herrada . In this work, we mainly focus on
music artist recommendation. As shown by Guo et al. , we also
consider doing recommendations on artists and choose the 10,000
most popular ones. Based on the time order, we group transactions
in 8 hours from the same user as a session. Following , sessions
that contain more than 20 transactions or less than 2 will be filtered
out. In total, there are 298,919 sessions after the pre-processing.
Gowalla 2 is a point-of-interest real-world dataset collected from
a social network for usersâ€™ check-in. The same as Guo et al. , the
30,000 most popular places are used for experiments and check-ins
within 1 day are defined as a session. Again, sessions that contain
more than 20 transactions or less than 2 will be filtered out. Finally,
we have 198,680 sessions during experiments.
Following Chang et al. and Guo et al. , to simulate the streaming situation of the data arriving situation, the dataset is split into
two proportions (60% and 40%) by the chronological order of all
data. The first part is defined as the training set (Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›) while the
second part is the candidate set (Dğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’). Specifically, Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›is
used for training the GAG model as offline data. As for Dğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’,
it is designed to simulate the online streaming session data. Especially for Dğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’, it is further divided into five same-sized test
set by time order, Dğ‘¡ğ‘’ğ‘ ğ‘¡,1, . . . , Dğ‘¡ğ‘’ğ‘ ğ‘¡,5. Test sets are provided in the
time order to the model during test time. And after testing on the
current test set, the model will be updated according to it and the
updated model accounts for the test for the next test set. Such an
online update is designed for the streaming occasion.
To evaluate the performance of our model, according to the
nature of the userâ€™s picking of the first few recommended items,
the top-20 recommendation is applied here and we mainly compare
different models based on the Recall@ğ¾and MRR@ğ¾.
In our experiments, we will mainly compare our GAG model to the
following representative baseline methods:
â€¢ POP always chooses the most popular items of all users to
recommend to other users. It is a simple yet strong baseline.
â€¢ S-POP recommends the most popular items that appear in
the current session instead of the whole item set.
â€¢ BPR-MF is a method that mainly makes use of a pairwise
ranking loss . Also, the Matrix Factorization is modified
to suit the session-based recommendation as in .
1 
2 
â€¢ GRU4REC utilizes GRU layers to learn the session embedding in the anonymous setting .
â€¢ NARM adds an attention layer to item level across the session to encode the session information within an anonymous
setting .
â€¢ FGNN makes use of GNN to learn an embedding of an anonymous session to make recommendation . This method
does not consider the user information.
â€¢ SSRM is a state-of-the-art method for the SSR problem,
which applies a reservoir to sample the history sessions
to help the current session embedding learning .
Training Detail
In the implementation of the model, we set all MLPs with 1 layer
and the embedding size is 200 for the fairness of comparison. We
use Adam with a learning rate of 0.003 and set the batch size
as 100 to train the GAG model. The size of the reservoir is set to
|ğ·|/100 and the window size is set to |ğ¶ğ‘›ğ‘’ğ‘¤|/2 on each ğ¶âˆªğ¶ğ‘›ğ‘’ğ‘¤.
EXPERIMENT RESULTS
In this section, we will describe our experiments on two real-world
datasets and demonstrate the efficacy of our proposed model GAG.
Specifically, four research questions will be addressed:
â€¢ RQ1 How does our proposed GAG model perform compared
with current state-of-the-art methods? (Section 5.1)
â€¢ RQ2 How does the global attribute help to solve the SSR
problem? (Section 5.2)
â€¢ RQ3 How is the performance of the Wasserstein reservoir?
(Section 5.3)
â€¢ RQ4 How is the parameter sensitivity of the GAG model?
(Section 5.4)
Comparisons with Baseline Methods
To evaluate the overall performance of the GAG, we compare the
GAG model with the baseline methods mentioned in Section 4.3
by the Recall@20 and MRR@20 scores on Gowalla and LastFM
datasets. The overall results are demonstrated in Fig. 4. We also
use the top-5 and top-10 recommendation results for a more indepth comparison. For fairness, we have the GAG-50 model with
the embedding size as 50 in accordance with the baseline methods.
For the state-of-the-art performance, we have the GAG model with
the embedding size as 200.
General Comparison. The overall performance is shown
in Fig. 4. Clearly, the proposed GAG-50 model outperforms all
the baseline methods in all situations. With the embedding size
increasing to 200, the GAG model achieves the state-of-the-art
results. Both of them show the superiority of the GAG model.
The performance is much worse for conventional methods, such
as POP and S-POP, both of which recommend the most popular
items to users. POP recommends the most popular ones from the
whole item set while S-POP chooses the most popular item in the
current session. POP fixes the recommendation list, which fails to
detect the different patterns of usersâ€™ behaviors in different sessions.
However, S-POP is still a strong session-based baseline method
because it can capture the itemâ€™s re-occurrence patterns of the
(a) Recall@20 on Gowalla.
(b) MRR@20 on Gowalla.
(c) Recall@20 on LastFM.
(d) MRR@20 on LastFM.
Figure 4: Results of streaming session-based recommendation performance.
session. Besides, for the shallow method BPR-MF, which performs
a matrix factorization of the whole user-item interaction matrix, it
has higher performance compared with POP because it can perform
the personalized recommendation. However, BPR-MF still fails to
outperform S-POP in the SSR problem because S-POP can further
extract the session-specific information.
For deep learning models, GRU4REC is a method that utilizes
GRU to process the session as a sequence and output a session
embedding to make a recommendation. It outperforms traditional
methods in most situations, which is proof of the superiority of
the deep learning-based approaches. Besides, methods utilizing
the attention mechanism, e.g., NARM and SSRM, obtain a great
improvement compared with GRU4REC, which shows the capability
of the attention mechanism to learn the inter-dependency of items.
Especially, SSRM is specifically for the SSR problem and it is the
strongest baseline in the experiment.
Recently, graph neural networks have been demonstrated to have
a strong ability to model structured data. For example, FGNN is a
state-of-the-art method for SR. In this experiment, we can see that
FGNN achieves a comparable performance with SSRM. Compared
with GAG-50, FGNN has a worse performance because it is only
designed for SR and cannot model the user information.
In-depth Comparison. We further evaluate GAG model
by the top-5 and top-10 recommendation results on the Gowalla
dataset. Specifically, we use the Recall@ğ¾and the MRR@ğ¾(ğ¾=
5, 10) scores to demonstrate the result in Fig. 5.
According to the results, GAG and GAG-50 still have superiority
in the higher standard recommendation. Compared with GNNbased methods, SSRM has a greater drop in both top-5 and top-10
performance, implying that the graph structure and GNN are more
suitable for the session representation and the generalization ability.
(a) Recall@5 index.
(b) MRR@5 index.
(c) Recall@10 index.
(d) MRR@10 index.
Figure 5: In-depth results of streaming session-based recommendation performance on Gowalla.
In contrast, the attention mechanism fails to distinguish the item
transition pattern in sessions.
Effect of Global Attribute
In our GAG model, we utilize the global attribute in both the node
embedding update and the global attribute update itself. In this
experiment, we conduct the ablation study and make different
substitutions of the global attribute to evaluate its efficacy. We use
the Recall@20 and MRR@20 on Gowalla and LastFM datasets to
evaluate the performance.
Ablation Study. In this experiment, we compare the GAG
model with the following variants:
â€¢ FGNN: FGNN uses the GNN layer that does not take the user
information in both the node update layer and the readout
function (readout function in a normal GNN model represents the graph level output function). It serves as the basic
baseline method.
â€¢ GAG-FGNN: We substitute the node update function with
FGNNâ€™s node update layer and maintain the global attribute
update function in GAG to evaluate the integration of the
userâ€™s information in the global attribute update.
â€¢ GAG-NoGA: In this variant, we keep the global attribute in
the node update procedure while removing it in the global
attribute update function.
The results are presented in Fig. 6. Each module using the global
attribute has a contribution to the recommendation performance.
In general, FGNN is the worst because it neglects the global attribute. GAG-FGNN and GAG-NoGA both make improvements
by introducing the global attribute. Specifically, GAG-FGNN uses
the global attribute in the global attribute update function while
GAG-NoGA incorporates the global attribute in the node update
function. Comparing these two variants, the results prove that the
(a) Recall@20 on Gowalla.
(b) MRR@20 on Gowalla.
(c) Recall@20 on LastFM.
(d) MRR@20 on LastFM.
Figure 6: Results of the ablation study of the global attribute.
GAG-Static
GAG-RanUni
GAG-FixNew
GAG-WassUni
(a) Recall@20 on Gowalla.
GAG-Static
GAG-RanUni
GAG-FixNew
GAG-WassUni
(b) MRR@20 on Gowalla.
GAG-Static
GAG-RanUni
GAG-FixNew
GAG-WassUni
(c) Recall@20 on LastFM.
GAG-Static
GAG-RanUni
GAG-FixNew
GAG-WassUni
(d) MRR@20 on LastFM.
Figure 7: Results of the different reservoir sampling strategies.
global attribute applied to the node update procedure has a greater
impact on the recommendation performance than in its self-update.
Effect of Wasserstein Reservoir
In this section, we conduct experiments to prove the efficacy of the
Wasserstein reservoir. The reservoir consists of two major designs:
(1) The sampling procedure is based on the Wasserstein distance between the sessionâ€™s predictive distribution and the real interaction;
(2) Sessions containing new items or new users will be added to the
training sample directly. We substitute the Wasserstein reservoir
with other reservoirs to evaluate how the design of the reservoir
affects the performance of the GAG model in the SSR problem.
Ablation Study. In this experiment, an ablation study is
conducted to prove the efficacy of both designs in our Wasserstein
reservoir. The variants are listed out as follows:
â€¢ GAG-Static: For this method, we simply eliminate the online
training of the model.
â€¢ GAG-RanUni: This variant only performs random sampling on the union set of the current reservoir and new
arrival sessions. It is the most common design of a reservoir.
â€¢ GAG-FixNew: This variant directly adds a new session containing new items or new users to the training data. For the
rest, it still performs a random sampling.
â€¢ GAG-WassUni: This method samples the training data from
the union set of the current reservoir and the new sessions
according to their Wasserstein distance.
According to the result in Fig. 7, our proposed Wasserstein reservoir achieves the best performance in all situations. For the static
recommendation version, GAG-Static, its performance decreases
along with the time because there is a shift of the usersâ€™ preference
and the streaming sessions contain new items and new users. In
most cases, the random sampling version variant, GAG-RanUni,
performs worse than other methods that use a specialized reservoir
sampling strategy. The conclusion can be drawn from these two
figures that incorporating the sessions containing new items and
new users helps with the online update of the model. Comparing
GAG-FixNew with GAG-RanUni, the performance of GAG-FixNew
is better on Gowalla while it has a decrease in the long-term prediction of Dğ‘¡ğ‘’ğ‘ ğ‘¡,4 and Dğ‘¡ğ‘’ğ‘ ğ‘¡,5 on LastFM. The model is distracted
because the new items and new users in these two parts of the
dataset are not representative. Comparing the complete GAG model
with GAG-WassUni, it can be seen that the incorporation of new
users and items can help with online training. Furthermore, the efficacy of Wasserstein distance is demonstrated. With the Wasserstein
distance-based sampling, GAG-WassUni outperforms GAG-RanUni
in most cases. Similarly, GAG also has higher scores than GAG-
FixNew, which randomly samples the training data.
Reservoir Efficiency. There are two important parameters
in the design of the Wasserstein reservoir: the reservoir size and the
window size. On one hand, the reservoir size indicates the volume
of the reservoir, which determines the storage requirement of the
online update for the recommender system. On the other hand, the
window size restricts how many data instances will be sampled for
the online training, which represents the work load of the online
update for the recommender system.
The default reservoir size is set to |ğ·|/100. For comparison, we
change the reservoir size to {|ğ·|/5, |ğ·|/20, |ğ·|/400} to evaluate the
effect of the reservoir size. Results of different reservoir sizes are
presented in Fig. 8. When the reservoir size is set to |ğ·|/100, our
GAG model achieves the best performance. When the reservoir size
increases, the probability of the new sessions stored in the reservoir
decreases, which makes the model concentrate more on the historical data. However, for the streaming data, the recent ones are more
(a) Recall@20 on Gowalla.
(b) MRR@20 on Gowalla.
(c) Recall@20 on LastFM.
(d) MRR@20 on LastFM.
Figure 8: Results of different reservoir size.
(a) Recall@20 on Gowalla.
(b) MRR@20 on Gowalla.
(c) Recall@20 on LastFM.
(d) MRR@20 on LastFM.
Figure 9: Results of different embedding sizes.
representative of the usersâ€™ recent preference. When the reservoir
size decreases, the streaming performance drops on a smaller scale,
which indicates that new sessions are more important for the recommendation performance. For the state-of-the-art method SSRM,
it achieves its highest performance with the reservoir size set to
|ğ·|/20, which is 5 times larger than our GAG model. Apparently,
our design has a higher efficiency in reservoir storage.
To evaluate the effect of the window size, we substitute the
default window size, |ğ¶|/2, with {|ğ¶|, |ğ¶|/4, |ğ¶|/8, |ğ¶|/16, |ğ¶|/32}
to evaluate the effect of the window size. In Fig. 10, we demonstrate
the results. It is clear that when the window size is larger, the model
(a) Recall@20 on Gowalla.
(b) MRR@20 on Gowalla.
(c) Recall@20 on LastFM.
(d) MRR@20 on LastFM.
Figure 10: Results of different window size.
can achieve a better recommendation performance because it can
utilize more data to update itself.
Parameter Sensitivity
In this section, we conduct experiments to evaluate the parameter
sensitivity of our GAG model.
Embedding Size. The previous methods achieve the best
results when the embedding size is set to 50 or 100. Therefore, we
test the following variants of our GAG model with the embedding
size of : GAG-50, GAG-100, GAG and GAG-400.
In Fig. 9, results of the sensitivity of the embedding size are
presented. It is clear that when the embedding size is set to 200, the
GAG model has the highest performance in all situations. Size 100 is
a relatively strong variant when compared with 50 and 400. When
the embedding size is set as 50 and 400, they are unrepresentative
and over-parameterized in their respective methods, which causes
difficulty in training a strong model.
Number of Layers. The number of GAG layers controls the
depth of the model. We test our model with different numbers of
layers of : GAG, GAG-2 and GAG-3.
In Fig. 11, the result of different layers is presented. Generally,
GNN models always suffer from an increase in the depth of the
model because of the gradient explosion. In our experiment, the
performance of the GAG model decreases as the model goes deeper,
which is consistent with the common observation. Furthermore,
the connectivity of sessions is smaller than the traditional graph
data, which also limits the power of deeper GNN models.
CONCLUSION
In this paper, we proposed a GAG model with a Wasserstein reservoir to perform SSR. We addressed the problem of how to preserve
usersâ€™ long-term interests by introducing the global attribute and
the GAG layer. We designed an effective and generic Wasserstein
(a) Recall@20 on Gowalla.
(b) MRR@20 on Gowalla.
(c) Recall@20 on LastFM.
(d) MRR@20 on LastFM.
Figure 11: Results of different numbers of layers.
reservoir, which samples sessions according to the Wasserstein
distance between their recommendation results and the real interactions. In the future, it is significant to investigate how to incorporate
the cross-session information for the SSR problem.
ACKNOWLEDGMENTS
The work has been supported by Australian Research Council
(Grant No. DP190101985, DP170103954 and FT200100825).