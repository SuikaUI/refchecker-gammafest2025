Multi-task Self-Supervised Visual Learning
Carl Doersch†
Andrew Zisserman†,∗
∗VGG, Department of Engineering Science, University of Oxford
We investigate methods for combining multiple selfsupervised tasks—i.e., supervised tasks where data can be
collected without manual labeling—in order to train a single visual representation. First, we provide an apples-toapples comparison of four different self-supervised tasks
using the very deep ResNet-101 architecture. We then combine tasks to jointly train a network. We also explore lasso
regularization to encourage the network to factorize the
information in its representation, and methods for “harmonizing” network inputs in order to learn a more uni-
ﬁed representation. We evaluate all methods on ImageNet
classiﬁcation, PASCAL VOC detection, and NYU depth
prediction. Our results show that deeper networks work
better, and that combining tasks—even via a na¨ıve multihead architecture—always improves performance. Our best
joint network nearly matches the PASCAL performance of a
model pre-trained on ImageNet classiﬁcation, and matches
the ImageNet network on NYU depth prediction.
1. Introduction
Vision is one of the most promising domains for unsupervised learning. Unlabeled images and video are available in practically unlimited quantities, and the most prominent present image models—neural networks—are data
starved, easily memorizing even random labels for large image collections . Yet unsupervised algorithms are still
not very effective for training neural networks: they fail
to adequately capture the visual semantics needed to solve
real-world tasks like object detection or geometry estimation the way strongly-supervised methods do. For most vision problems, the current state-of-the-art approach begins
by training a neural network on ImageNet or a similarly
large dataset which has been hand-annotated.
How might we better train neural networks without manual labeling?
Neural networks are generally trained via
backpropagation on some objective function. Without labels, however, what objective function can measure how
good the network is? Self-supervised learning answers this
question by proposing various tasks for networks to solve,
where performance is easy to measure, i.e., performance
can be captured with an objective function like those seen
in supervised learning. Ideally, these tasks will be difﬁcult to solve without understanding some form of image
semantics, yet any labels necessary to formulate the objective function can be obtained automatically. In the last few
years, a considerable number of such tasks have been proposed , such as asking a neural network to
colorize grayscale images, ﬁll in image holes, solve jigsaw
puzzles made from image patches, or predict movement in
videos. Neural networks pre-trained with these tasks can
be re-trained to perform well on standard vision tasks (e.g.
image classiﬁcation, object detection, geometry estimation)
with less manually-labeled data than networks which are
initialized randomly. However, they still perform worse in
this setting than networks pre-trained on ImageNet.
This paper advances self-supervision ﬁrst by implementing four self-supervision tasks and comparing their performance using three evaluation measures. The self-supervised
tasks are: relative position , colorization , the “exemplar” task , and motion segmentation (described
in section 2). The evaluation measures (section 5) assess a
diverse set of applications that are standard for this area, including ImageNet image classiﬁcation, object category detection on PASCAL VOC 2007, and depth prediction on
Second, we evaluate if performance can be boosted by
combining these tasks to simultaneously train a single trunk
network. Combining the tasks fairly in a multi-task learning objective is challenging since the tasks learn at different
rates, and we discuss how we handle this problem in section 4. We ﬁnd that multiple tasks work better than one, and
explore which combinations give the largest boost.
Third, we identify two reasons why a na¨ıve combination
of self-supervision tasks might conﬂict, impeding performance: input channels can conﬂict, and learning tasks can
conﬂict. The ﬁrst sort of conﬂict might occur when jointly
training colorization and exemplar learning: colorization receives grayscale images as input, while exemplar learning
receives all color channels. This puts an unnecessary burden
on low-level feature detectors that must operate across domains. The second sort of conﬂict might happen when one
task learns semantic categorization (i.e. generalizing across
instances of a class) and another learns instance matching
(which should not generalize within a class). We resolve the
ﬁrst conﬂict via “input harmonization”, i.e. modifying network inputs so different tasks get more similar inputs. For
the second conﬂict, we extend our mutli-task learning architecture with a lasso-regularized combination of features
from different layers, which encourages the network to separate features that are useful for different tasks. These architectures are described in section 3.
We use a common deep network across all experiments,
a ResNet-101-v2, so that we can compare various diverse
self-supervision tasks apples-to-apples. This comparison is
the ﬁrst of its kind. Previous work applied self-supervision
tasks over a variety of CNN architectures (usually relatively
shallow), and often evaluated the representations on different tasks; and even where the evaluation tasks are the same,
there are often differences in the ﬁne-tuning algorithms.
Consequently, it has not been possible to compare the performance of different self-supervision tasks across papers.
Carrying out multiple fair comparisons, together with the
implementation of the self-supervised tasks, joint training,
evaluations, and optimization of a large network for several
large datasets has been a signiﬁcant engineering challenge.
We describe how we carried out the large scale training efﬁciently in a distributed manner in section 4. This is another
contribution of the paper.
As shown in the experiments of section 6, by combining
multiple self-supervision tasks we are able to close further
the gap between self-supervised and fully supervised pretraining over all three evaluation measures.
1.1. Related Work
Self-supervision tasks for deep learning generally involve taking a complex signal, hiding part of it from the
network, and then asking the network to ﬁll in the missing
information. The tasks can broadly be divided into those
that use auxiliary information or those that only use raw
Tasks that use auxiliary information such as multi-modal
information beyond pixels include: predicting sound given
videos , predicting camera motion given two images of
the same scene , or predicting what robotic motion caused a change in a scene . However,
non-visual information can be difﬁcult to obtain: estimating
motion requires IMU measurements, running robots is still
expensive, and sound is complex and difﬁcult to evaluate
quantitatively.
Thus, many works use raw pixels. In videos, time can
be a source of supervision.
One can simply predict future , although such predictions may be difﬁcult to
evaluate. One way to simplify the problem is to ask a network to temporally order a set of frames sampled from a
video . Another is to note that objects generally appear
across many frames: thus, we can train features to remain
invariant as a video progresses . Finally,
motion cues can separate foreground objects from background. Neural networks can be asked to re-produce these
motion-based boundaries without seeing motion .
Self-supervised learning can also work with a single image. One can hide a part of the image and ask the network
to make predictions about the hidden part. The network
can be tasked with generating pixels, either by ﬁlling in
holes , or recovering color after images have been
converted to grayscale . Again, evaluating the quality of generated pixels is difﬁcult. To simplify the task, one
can extract multiple patches at random from an image, and
then ask the network to position the patches relative to each
other . Finally, one can form a surrogate “class” by
taking a single image and altering it many times via translations, rotations, and color shifts , to create a synthetic
categorization problem.
Our work is also related to multi-task learning. Several
recent works have trained deep visual representations using multiple tasks , including one work 
which combines no less than 7 tasks. Usually the goal is
to create a single representation that works well for every
task, and perhaps share knowledge between tasks. Surprisingly, however, previous work has shown little transfer between diverse tasks. Kokkinos , for example, found a
slight dip in performance with 7 tasks versus 2. Note that
our work is not primarily concerned with the performance
on the self-supervised tasks we combine: we evaluate on
a separate set of semantic “evaluation tasks.” Some previous self-supervised learning literature has suggested performance gains from combining self-supervised tasks ,
although these works used relatively similar tasks within
relatively restricted domains where extra information was
provided besides pixels.
In this work, we ﬁnd that pretraining on multiple diverse self-supervised tasks using only
pixels yields strong performance.
2. Self-Supervised Tasks
Too many self-supervised tasks have been proposed in
recent years for us to evaluate every possible combination. Hence, we chose representative self-supervised tasks
to reimplement and investigate in combination. We aimed
for tasks that were conceptually simple, yet also as diverse
as possible. Intuitively, a diverse set of tasks should lead
to a diverse set of features, which will therefore be more
likely to span the space of features needed for general semantic image understanding. In this section, we will brieﬂy
describe the four tasks we investigated. Where possible, we
followed the procedures established in previous works, although in many cases modiﬁcations were necessary for our
multi-task setup. Further implementation details are provided in the arXiv extended version of this paper.
Relative Position :
This task begins by sampling two
patches at random from a single image and feeding them
both to the network without context. The network’s goal is
to predict where one patch was relative to the other in the
original image. The trunk is used to produce a representation separately for both patches, which are then fed into a
head which combines the representations and makes a prediction. The patch locations are sampled from a grid, and
pairs are always taken from adjacent grid points (including diagonals). Thus, there are only eight possible relative
positions for a pair, meaning the network output is a simple eight-way softmax classiﬁcation. Importantly, networks
can learn to detect chromatic aberration to solve the task,
a low-level image property that isn’t relevant to semantic
tasks. Hence, employs “color dropping”, i.e., randomly
dropping 2 of the 3 color channels and replacing them with
noise. We reproduce color dropping, though our harmonization experiments explore other approaches to dealing with
chromatic aberration that clash less with other tasks.
Colorization :
Given a grayscale image (the L channel of the Lab color space), the network must predict the
color at every pixel (speciﬁcally, the ab components of Lab).
The color is predicted at a lower resolution than the image
(a stride of 8 in our case, a stride of 4 was used in ),
and furthermore, the colors are vector quantized into 313
different categories. Thus, there is a 313-way softmax classiﬁcation for every 8-by-8 pixel region of the image. Our
implementation closely follows .
Exemplar :
The original implementation of this task
created pseudo-classes, where each class was generated by
taking a patch from a single image and augmenting it via
translation, rotation, scaling, and color shifts . The network was trained to discriminate between pseudo-classes.
Unfortunately, this approach isn’t scalable to large datasets,
since the number of categories (and therefore, the number
of parameters in the ﬁnal fully-connected layer) scales linearly in the number of images. However, the approach can
be extended to allow an inﬁnite number of classes by using a triplet loss, similar to , instead of a classiﬁcation loss per class. Speciﬁcally, we randomly sample two
patches x1 and x2 from the same pseudo-class, and a third
patch x3 from a different pseudo-class (i.e. from a different image). The network is trained with a loss of the form
max(D(f(x1), f(x2)) −D(f(x1), f(x3)) + M, 0), where
D is the cosine distance, f(x) is network features for x (including a small head) for patch x, and M is a margin which
we set to 0.5.
Motion Segmentation :
Given a single frame of
video, this task asks the network to classify which pixels
will move in subsequent frames. The “ground truth” mask
of moving pixels is extracted using standard dense tracking
algorithms. We follow Pathak et al. , except that we
replace their tracking algorithm with Improved Dense Trajectories . Keypoints are tracked over 10 frames, and
any pixel not labeled as camera motion by that algorithm is
treated as foreground. The label image is downsampled by
a factor of 8. The resulting segmentations look qualitatively
similar to those given in Pathak et al. . The network is
trained via a per-pixel cross-entropy with the label image.
The three image-based tasks are all trained on
ImageNet, as is common in prior work. The motion segmentation task uses the SoundNet dataset . It is an open
problem whether performance can be improved by different choices of dataset, or indeed by training on much larger
3. Architectures
In this section we describe three architectures: ﬁrst, the
(na¨ıve) multi-task network that has a common trunk and a
head for each task (ﬁgure 1a); second, the lasso extension
of this architecture (ﬁgure 1b) that enables the training to
determine the combination of layers to use for each selfsupervised task; and third, a method for harmonizing input
channels across self-supervision tasks.
3.1. Common Trunk
Our architecture begins with Resnet-101 v2 , as implemented in TensorFlow-Slim . We keep the entire architecture up to the end of block 3, and use the same block3
representation solve all tasks and evaluations (see ﬁgure 1a).
Thus, our “trunk” has an output with 1024 channels, and
consists of 88 convolution layers with roughly 30 million
parameters. Block 4 contains an additional 13 conv layers
and 20 million parameters, but we don’t use it to save computation.
Each task has a separate loss, and has extra layers in
a “head,” which may have a complicated structure.
instance, the relative position and exemplar tasks have a
siamese architecture.
We implement this by passing all
patches through the trunk as a single batch, and then rearranging the elements in the batch to make pairs (or
triplets) of representations to be processed by the head. At
each training iteration, only one of the heads is active. However, gradients are averaged across many iterations where
different heads are active, meaning that the overall loss is a
sum of the losses of different tasks.
3.2. Separating features via Lasso
Different tasks require different features; this applies for
both the self-supervised training tasks and the evaluation
tasks. For example, information about ﬁne-grained breeds
of dogs is useful for, e.g., ImageNet classiﬁcation, and also
colorization. However, ﬁne-grained information is less useful for tasks like PASCAL object detection, or for relative
positioning of patches.
Furthermore, some tasks require
only image patches (such as relative positioning) whilst others can make use of entire images (such as colorization),
and consequently features may be learnt at different scales.
Figure 1. The structure of our multi-task network. It is based on ResNet-101, with block 3 having 23 residual units. a) Naive shared-trunk
approach, where each “head” is attached to the output of block 3. b) the lasso architecture, where each “head” receives a linear combination
of unit outputs within block3, weighted by the matrix α, which is trained to be sparse.
This suggests that, while training on self-supervised tasks,
it might be advantageous to separate out groups of features
that are useful for some tasks but not others. This would
help us with evaluation tasks: we expect that any given
evaluation task will be more similar to some self-supervised
tasks than to others. Thus, if the features are factorized into
different tasks, then the network can select from the discovered feature groups while training on the evaluation tasks.
Inspired by recent works that extract information across
network layers for the sake of transfer learning ,
we propose a mechanism which allows a network to choose
which layers are fed into each task. The simplest approach
might be to use a task-speciﬁc skip layer which selects a single layer in ResNet-101 (out of a set of equal-sized candidate layers) and feeds it directly into the task’s head. However, a hard selection operation isn’t differentiable, meaning
that the network couldn’t learn which layer to feed into a
task. Furthermore, some tasks might need information from
multiple layers. Hence, we relax the hard selection process,
and instead pass a linear combination of skip layers to each
head. Concretely, each task has a set of coefﬁcients, one
for each of the 23 candidate layers in block 3. The representation that’s fed into each task head is a sum of the layer
activations weighted by these task-speciﬁc coefﬁcients. We
impose a lasso (L1) penalty to encourage the combination to
be sparse, which therefore encourages the network to concentrate all of the information required by a single task into
a small number of layers. Thus, when ﬁne-tuning on a new
task, these task-speciﬁc layers can be quickly selected or
rejected as a group, using the same lasso penalty.
Mathematically, we create a matrix α with N rows and
M columns, where N is the number of self-supervised
tasks, and M is the number of residual units in block 3.
The representation passed to the head for task n is then:
αn,m ∗Unitm
where Unitm is the output of residual unit m. We enforce that M
n,m = 1 for all tasks n, to control the
output variance (note that the entries in α can be negative,
so a simple sum is insufﬁcient). To ensure sparsity, we add
an L1 penalty on the entries of α to the objective function.
We create a similar α matrix for the set of evaluation tasks.
3.3. Harmonizing network inputs
Each self-supervised task pre-processes its data differently, so the low-level image statistics are often very different across tasks. This puts a heavy burden on the trunk
network, since its features must generalize across these statistical differences, which may impede learning. Furthermore, it gives the network an opportunity to cheat: the network might recognize which task it must solve, and only
represent information which is relevant to that task, instead
of truly multi-task features. This problem is especially bad
for relative position, which pre-processes its input data by
discarding 2 of the 3 color channels, selected at random,
and replacing them with noise. Chromatic aberration is also
hard to detect in grayscale images. Hence, to “harmonize,”
we replace relative position’s preprocessing with the same
preprocessing used for colorization: images are converted
to Lab, and the a and b channels are discarded (we replicate
the L channel 3 times so that the network can be evaluated
on color images).
4. Training the Network
Training a network with nearly 100 hidden layers requires considerable compute power, so we distribute it
across several machines. As shown in ﬁgure 2, each machine trains the network on a single task. Parameters for
Figure 2. Distributed training setup. Several GPU machines are
allocated for each task, and gradients from each task are synchronized and aggregated with separate RMSProp optimizers.
the ResNet-101 trunk are shared across all replicas. There
are also several task-speciﬁc layers, or heads, which are
shared only between machines that are working on the same
task. Each worker repeatedly computes losses which are
then backpropagated to produce gradients.
Given many workers operating independently, gradients
are usually aggregated in one of two ways. The ﬁrst option is asynchronous training, where a centralized parameter server receives gradients from workers, applies the updates immediately, and sends back the up-to-date parameters . We found this approach to be unstable, since
gradients may be stale if some machines run slowly. The
other approach is synchronous training, where the parameter server accumulates gradients from all workers, applies
the accumulated update while all workers wait, and then
sends back identical parameters to all workers , preventing stale gradients. “Backup workers” help prevent slow
workers from slowing down training. However, in a multitask setup, some tasks are faster than others. Thus, slow
tasks will not only slow down the computation, but their
gradients are more likely to be thrown out.
Hence, we used a hybrid approach: we accumulate gradients from all workers that are working on a single task,
and then have the parameter servers apply the aggregated
gradients from a single task when ready, without synchronizing with other tasks. Our experiments found that this
approach resulted in faster learning than either purely synchronous or purely asynchronous training, and in particular,
was more stable than asynchronous training.
We also used the RMSProp optimizer, which has been
shown to improve convergence in many vision tasks versus
stochastic gradient descent. RMSProp re-scales the gradients for each parameter such that multiplying the loss by
a constant factor does not change how quickly the network
learns. This is a useful property in multi-task learning, since
different loss functions may be scaled differently. Hence,
we used a separate RMSProp optimizer for each task. That
is, for each task, we keep separate moving averages of the
squared gradients, which are used to scale the task’s accumulated updates before applying them to the parameters.
For all experiments, we train on 64 GPUs in parallel, and
save checkpoints every roughly 2.4K GPU (NVIDIA K40)
hours. These checkpoints are then used as initialization for
our evaluation tasks.
5. Evaluation
Here we describe the three evaluation tasks that we transfer our representation to: image classiﬁcation, object category detection, and pixel-wise depth prediction.
ImageNet with Frozen Weights:
We add a single linear
classiﬁcation layer (a softmax) to the network at the end of
block 3, and train on the full ImageNet training set. We
keep all pre-trained weights frozen during training, so we
can evaluate raw features. We evaluate on the ImageNet
validation set. The training set is augmented in translation
and color, following , but during evaluation, we don’t
use multi-crop or mirroring augmentation. This evaluation
is similar to evaluations used elsewhere (particularly Zhang
et al. ). Performing well requires good representation
of ﬁne-grained object attributes (to distinguish, for example, breeds of dogs). We report top-5 recall in all charts
(except Table 1, which reports top-1 to be consistent with
previous works). For most experiments we use only the
output of the ﬁnal “unit” of block 3, and use max pooling
to obtain a 3x3x1024 feature vector, which is ﬂattened and
used as the input to the one-layer classiﬁer. For the lasso
experiments, however, we use a weighted combination of
the (frozen) features from all block 3 layers, and we learn
the weight for each layer, following the structure described
in section 3.2.
Detection:
RCNN , which trains a single network base with
multiple heads for object proposals, box classiﬁcation, and
box localization.
Performing well requires the network
to accurately represent object categories and locations,
with penalties for missing parts which might be hard to
recognize (e.g., a cat’s body is harder to recognize than its
head). We ﬁne-tune all network weights. For our ImageNet
pre-trained ResNet-101 model, we transfer all layers up
through block 3 from the pre-trained model into the trunk,
and transfer block 4 into the proposal categorization head,
as is standard. We do the same with our self-supervised
network, except that we initialize the proposal categorization head randomly. Following Doersch et al. , we use
multi-scale data augmentation for all methods, including
baselines. All other settings were left at their defaults. We
train on the VOC 2007 trainval set, and evaluate Mean
Average Precision on the VOC 2007 test set. For the lasso
experiments, we feed our lasso combination of block 3
layers into the heads, rather than the ﬁnal output of block 3.
NYU V2 Depth Prediction:
Depth prediction measures
how well a network represents geometry, and how well that
information can be localized to pixel accuracy. We use a
modiﬁed version of the architecture proposed in Laina et
al. . We use the “up projection” operator deﬁned in that
work, as well as the reverse Huber loss. We replaced the
ResNet-50 architecture with our ResNet-101 architecture,
and feed the block 3 outputs directly into the up-projection
layers (block 4 was not used in our setup). This means we
need only 3 levels of up projection, rather than 4. Our up
projection ﬁlter sizes were 512, 256, and 128. As with our
PASCAL experiments, we initialize all layers up to block 3
using the weights from our self-supervised pre-training, and
ﬁne-tune all weights. We selected one measure—percent of
pixels where relative error is below 1.25—as a representative measure (others available in our extended arXiv report).
Relative error is deﬁned as max
, where dgt is
groundtruth depth and dp is predicted depth. For the lasso
experiments, we feed our lasso combination of block3 layers into the up projection layers, rather than the ﬁnal output
of block 3.
6. Results: Comparisons and Combinations
ImageNet Baseline:
As an “upper bound” on performance, we train a full ResNet-101 model on ImageNet,
which serves as a point of comparison for all our evaluations. Note that just under half of the parameters of this
network are in block 4, which are not pre-trained in our
self-supervised experiments (they are transferred from the
ImageNet network only for the Pascal evaluations). We use
the standard learning rate schedule of Szegedy et al. 
for ImageNet training (multiply the learning rate by 0.94
every 2 epochs), but we don’t use such a schedule for our
self-supervised tasks.
6.1. Comparing individual self-supervision tasks
Table 1 shows the performance of individual tasks for
the three evaluation measures.
Compared to previouslypublished results, our performance is signiﬁcantly higher
in all cases, most likely due to the additional depth of
ResNet (cf. AlexNet) and additional training time. Note,
our ImageNet-trained baseline for Faster-RCNN is also
above the previously published result using ResNet (69.9
in cf. 74.2 for ours), mostly due to the addition of multiscale augmentation for the training images following .
Of the self-supervised pre-training methods, relative position and colorization are the top performers, with relative
position winning on PASCAL and NYU, and colorization
winning on ImageNet-frozen. Remarkably, relative position performs on-par with ImageNet pre-training on depth
Pre-training
ImageNet top1
INet Labels
Table 1. Comparison of our implementation with previous results
on our evaluation tasks: ImageNet with frozen features (left), and
PASCAL VOC 2007 mAP with ﬁne-tuning (middle), and NYU
depth (right, not used in previous works). Unlike elsewhere in this
paper, ImageNet performance is reported here in terms of top 1
accuracy (versus recall at 5 elsewhere). Our ImageNet pre-training
performance on ImageNet is lower than the performance He et
al. (78.25) reported for ResNet-101 since we remove block 4.
prediction, and the gap is just 7.5% mAP on PASCAL. The
only task where the gap remains large is the ImageNet evaluation itself, which is not surprising since the ImageNet pretraining and evaluation use the same labels. Motion segmentation and exemplar training are somewhat worse than
the others, with exemplar worst on Pascal and NYU, and
motion segmentation worst on ImageNet.
Figure 3 shows how the performance changes as pretraining time increases (time is on the x-axis). After 16.8K
GPU hours, performance is plateauing but has not completely saturated, suggesting that results can be improved
slightly given more time. Interestingly, on the ImageNetfrozen evaluation, where colorization is winning, the gap
relative to relative position is growing. Also, while most
algorithms slowly improve performance with training time,
exemplar training doesn’t ﬁt this pattern: its performance
falls steadily on ImageNet, and undulates on PASCAL and
NYU. Even stranger, performance for exemplar is seemingly anti-correlated between Pascal and NYU from checkpoint to checkpoint. A possible explanation is that exemplar
training encourages features that aren’t invariant beyond the
training transformations (e.g. they aren’t invariant to object
deformation or out-of-plane rotation), but are instead sensitive to the details of textures and low-level shapes. If these
irrelevant details become prominent in the representation,
they may serve as distractors for the evaluation classiﬁers.
Note that the random baseline performance is low relative to a shallower network, especially the ImageNet-frozen
evaluation (a linear classiﬁer on random AlexNet’s conv5
features has top-5 recall of 27.1%, cf. 10.5% for ResNet).
All our pre-trained nets far outperform the random baseline.
The fact that representations learnt by the various selfsupervised methods have different strengths and weaknesses suggests that the features differ. Therefore, combining methods may yield further improvements. On the other
hand, the lower-performing tasks might drag-down the performance of the best ones. Resolving this uncertainty is a
key motivator for the next section.
            
  
 
   
  
  
 

 
Comparison of performance for different selfsupervised methods over time.
X-axis is compute time on the
self-supervised task (∼2.4K GPU hours per tick). “Random Init”
shows performance with no pre-training.
Implementation Details:
Unfortunately, intermittent network congestion can slow down experiments, so we don’t
measure wall time directly. Instead, we estimate compute
time for a given task by multiplying the per-task training
step count by a constant factor, which is ﬁxed across all experiments, representing the average step time when network
congestion is minimal. We add training cost across all tasks
used in an experiment, and snapshot when the total cost
crosses a threshold. For relative position, 1 epoch through
the ImageNet train set takes roughly 350 GPU hours; for
Pre-training
RP+Col+Ex+MS
INet Labels
Table 2. Comparison of various combinations of self-supervised
tasks. Checkpoints were taken after 16.8K GPU hours, equivalent to checkpoint 7 in Figure 3. Abbreviation key: RP: Relative
Position; Col: Colorization; Ex: Exemplar Nets; MS: Motion Segmentation. Metrics: ImageNet: Recall@5; PASCAL: mAP; NYU:
% Pixels below 1.25.
colorization it takes roughly 90 hours; for exemplar nets
roughly 60 hours.
For motion segmentation, one epoch
through our video dataset takes roughly 400 GPU hours.
multi-task
combination
selfsupervision tasks
Table 2 shows results for combining self-supervised
pre-training tasks.
Beginning with one of our strongest
performers—relative position—we see that adding any of
our other tasks helps performance (except on NYU, where
differences are likely not signiﬁcant). Adding either colorization or exemplar leads to more than 6 points gain on
ImageNet. Furthermore, it seems that the boosts are complementary: adding both colorization and exemplar gives a
further 2% boost. Our best-performing method was a combination of all four self-supervised tasks. We don’t ﬁnd
that multitask learning slows down training time either: the
combined method has the highest performance even if we
stop training at 2.4K GPU hours.
To further probe how well our representation localizes
objects, we evaluated the PASCAL detector at a more stringent overlap criterion: 75% IoU . Our model gets 43.91% mAP in this
setting, versus the standard ImageNet model’s performance
of 44.27%, a gap of less than half a percent. Thus, the selfsupervised approach may be especially useful when accurate localization is important.
6.3. Mediated combination of self-supervision tasks
Harmonization:
We train two versions of a network on
relative position and colorization: one using harmonization
to make the relative position inputs look more like colorization, and one without it (equivalent to RP+Col in section 6.2
above). As a baseline, we make the same modiﬁcation to
a network trained only on relative position alone: i.e., we
convert its inputs to grayscale. In this baseline, we don’t
expect any performance boost over the original relative position task, because there are no other tasks to harmonize
with. Results are shown in Table 3. However, on the Im-
Pre-training
RP+Col / H
Table 3. Comparison of methods with and without harmonization,
where relative position training is converted to grayscale to mimic
the inputs to the colorization network. H denotes an experiment
done with harmonization.
Weights learned via the lasso technique. Each row
shows one task: self-supervised tasks on top, evaluation tasks on
bottom. Each square shows |α| for one ResNet “Unit” (shallowest layers at the left). Whiter colors indicate higher |α|, with a
nonlinear scale to make smaller nonzero values easily visible.
Net structure
Eval Only Lasso
Pre-train Only Lasso
Pre-train & Eval Lasso
Table 4. Comparison of performance with and without the lasso
technique for factorizing representations, for a network trained on
all four self-supervised tasks for 16.8K GPU-hours. “No Lasso”
is equivalent to table 2’s RP+Col+Ex+MS. “Eval Only” uses the
same pre-trained network, with lasso used only on the evaluation
task, while “Pre-train Only” uses it only during pre-training. The
ﬁnal row uses lasso always.
ageNet evaluation there is an improvement when we pretrain using only relative position (due to the change from
adding noise to the other two channels to using grayscale
input (three equal channels)), and this improvement follows
through to the the combined relative position and colorization tasks. The other two evaluation tasks do not show any
improvement with harmonization. This suggests that our
networks are actually quite good at dealing with stark differences between pre-training data domains when the features
are ﬁne-tuned at test time.
Lasso training:
As a ﬁrst sanity check, Figure 4 plots the
α matrix learned using all four self-supervised tasks. Different tasks do indeed select different layers. Somewhat
surprisingly, however, there are strong correlations between
the selected layers: most tasks want a combination of lowlevel information and high-level, semantic information. The
depth evaluation network selects relatively high-level information, but evaluating on ImageNet-frozen and PASCAL
makes the network select information from several levels,
often not the ones that the pre-training tasks use. This suggests that, although there are useful features in the learned
representation, the ﬁnal output space for the representation
is still losing some information that’s useful for evaluation
tasks, suggesting a possible area for future work.
The ﬁnal performance of this network is shown in Table 4. There are four cases: no lasso, lasso only on the
evaluation tasks, lasso only at pre-training time, and lasso
in both self-supervised training and evaluation. Unsurprisingly, using lasso only for pre-training performs poorly
since not all information reaches the ﬁnal layer. Surprisingly, however, using the lasso both for self-supervised
training and evaluation is not very effective, contrary to
previous results advocating that features should be selected
from multiple layers for task transfer . Perhaps
the multi-task nature of our pre-training forces more information to propagate through the entire network, so explicitly extracting information from lower layers is unnecessary.
7. Summary and extensions
In this work, our main ﬁndings are: (i) Deeper networks improve self-supervision over shallow networks; (ii)
Combining self-supervision tasks always improves performance over the tasks alone; (iii) The gap between ImageNet pre-trained and self-supervision pre-trained with four
tasks is nearly closed for the VOC detection evaluation, and
completely closed for NYU depth, (iv) Harmonization and
lasso weightings only have minimal effects; and, ﬁnally, (v)
Combining self-supervised tasks leads to faster training (see
graphs in the extended arxiv version of this paper).
There are many opportunities for further improvements:
we can add augmentation (as in the exemplar task) to all
tasks; we could add more self-supervision tasks (indeed
new ones have appeared during the preparation of this paper, e.g. ); and we can experiment with methods for
dynamically weighting the importance of tasks in the optimization. It would also be interesting to repeat these experiments with a deep network such as VGG-16 where consecutive layers are less correlated, or with even deeper networks
(ResNet-152, DenseNet and beyond) to tease out the
match between self-supervision tasks and network depth.
For the lasso, it might be worth investigating block level
weightings using a group sparsity regularizer.
For the future, given the performance improvements
demonstrated in this paper, there is a possibility that selfsupervision will eventually augment or replace fully supervised pre-training.
Acknowledgements:
This research was funded by Deep-
Thanks to Relja Arandjelovi´c, Jo˜ao Carreira, Viorica
P˘atr˘aucean and Karen Simonyan for helpful discussions.