Marginal Likelihood from the Gibbs Output
Siddhartha Chib
Journal of the American Statistical Association, Vol. 90, No. 432. , pp. 1313-1321.
Stable URL:
 
Journal of the American Statistical Association is currently published by American Statistical Association.
Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at
 JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained
prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in
the JSTOR archive only for your personal, non-commercial use.
Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
 
Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed
page of such transmission.
The JSTOR Archive is a trusted digital repository providing for long-term preservation and access to leading academic
journals and scholarly literature from around the world. The Archive is supported by libraries, scholarly societies, publishers,
and foundations. It is an initiative of JSTOR, a not-for-profit organization with a mission to help the scholarly community take
advantage of advances in technology. For more information regarding JSTOR, please contact .
 
Wed Feb 6 14:39:37 2008
Marginal Likelihood From the Gibbs Output
Siddhartha CHIB
In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for
computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution.
Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this
calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the
prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate
of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form
expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of
resulting estimate is derived. The ideas are applied to probit regression and finite mixture models.
KEY WORDS: Bayes factor; Estimation of normalizing constant; Finite mixture models; Linear regression; Markov chain Monte
Carlo; Markov mixture model; Multivariate density estimation; Numerical standard error; Probit regression;
Reduced conditional density.
1. INTRODUCTION
 showed that the marginal likelihood (equivalently,
The advent of Markov chain Monte Carlo (MCMC) meth-
the marginal density of y ) under model Mk, that is,
ods to
simulate posterior distributions has virtually revolutionized
the practice of Bayesian statistics. For the most part, these
methods have been used for estimation and out-of-sample
can be estimated as
prediction, because both of those problems are easily solved
given a sample of draws from the posterior distribution. On
the other hand, the problem of calculating the marginal like-
lihood, which is the normalizing constant of the posterior
density and an input to the computation of Bayes factors
which is the harmonic mean of the likelihood values. Al-
 , has proved extremely challenging. This is
m ( y1Mk), it is not stable, because the inverse likelihood
because the marginal likelihood is obtained by integrating
does not have finite variance. But consider the quantity
the likelihood function with respect to the prior density,
proposed by Gelfand and Dey :
whereas the MCMC method produces draws from the pos-
One way to deal with this problem is to compute Bayes
factors without attempting to calculate the marginal like-
lihood by introducing a model indicator into the list of
unknown parameters. Work along these lines has been re-
where p(0) is a density with tails thinner than the product of
ported by Carlin and Polson , Carlin and Chib , the prior and the likelihood. This can be shown to have the
and many others. To use these methods, however, it is nec-
property that ~
D m ( y Mk) as G becomes large without
essary to specify all of the competing models at the out-
the instability of mNR.Nonetheless, this approach requires
set, which may not be always possible, and to carefully
a tuning function, which can be quite difficult to determine
specify certain tuning constants to ensure that the simula-
in high-dimensional problems, and subsequent monitoring
tion algorithm mixes suitably in model space. In this artito ensure that the numbers are stable. In fact, we have
cle, therefore, we concern ourselves with methods that di-
found that the somewhat obvious choices of p(.)-a
norrectly address the calculation of the marginal likelihood.
ma1 density or t density with mean and covariance equal
Suppose that f ( yO k ,M k ) is the density function of the
to the posterior mean and covariance--do not necessarily
data y = (yl,. . . ,y,) under model Mk ( k = 1 , 2 , . . . ,K ) satisfy the thinness requirement. Other attempts to mod-
given the model-specific parameter vector Ok. Let the prior
ify the harmonic mean estimator, though requiring samples
density of Ok (assumed to be proper) be given by ~ ( 0 kMk), from both the prior and posterior distributions, have been
discussed by Newton and Raftery .
and let {QP)}I { O f ) ,' ' ' 7 ':6)} be
draws from the
The purpose of this article is to demonstrate that a sim-
T ( O k ~ ,M k )
using a MCMC
ple approach to
the marginal likelihood and the
say the Gibbs
Newton and Raftery
Bayes factor is available that is free of the problems just de-
scribed. This approach is developed in the setting where the
Siddhartha Chib is Professor of Econometrics, John M. Olin School
of Business, Washington University, St. Louis, MO 63130. This article
has benefited from valuable comments of two anonymous referees, the
@ 1995 Aiiierican Statistical Association
associate editor, and the editor. In addition, discussions with Jim Albert,
Journal of the American Statistical Association
Ed Greenberg, and Radford Neal are gratefully acknowledged.
December 1995, Vol. 90, No. 432, Theory and Methods
Gibbs sampling algorithm, with or without data augmenta-
tion, has been used to provide a sample of draws from the
posterior distribution. To compute the marginal density by
our approach, it is necessary that all integrating constants
of the full conditional distributions in the Gibbs sampler be
known. This requirement is usually satisfied in models fit
with conjugate priors and covers almost all applications of
the Gibbs sampler that have appeared in the literature.
The rest of the article is organized as follows. Section 2
presents the approach, and Section 3 illustrates the deriva-
tion of the numerical standard error of the estimate. Section
4 presents applications of the approach, first for variable
selection in probit regression and then for model compar-
isons in finite mixture models. The final section contains
brief concluding remarks.
2. THE APPROACH
Suppress the model index k and consider the situation
wherein f (y 0 ) is the sampling density (likelihood function)
for the given model and ~ ( 0 )is the prior density. To allow
for the possibility that posterior simulation requires data
augmentation, let z denote latent data and suppose that for
a given set of vector blocks 0 = (01, 02,. . . ,OB), the Gibbs
sampling algorithm is applied to the set of (B + 1) complete conditional densities,
The objective is to compute the marginal density m(y Mk)
from the output { o ( ~ ) ,
z(g))globtained from (4).
The approach developed here consists of two related
ideas. First, m(y),by virtue of being the normalizing con-
stant of the posterior density, can be written as
where the numerator is just the product of the sampling
density and the prior, with all integrating constants in-
cluded, and the denominator is the posterior density of
0. It is worthwhile to refer to this simple identity, which
holds for any 0, as the basic marginal likelihood identity
(BMI). Second, for a given 0 (say O*), the posterior or-
dinate ~(O*ly)can be estimated by exploiting the infor-
mation in the collection of complete conditional densities
( ~ ( 0 ,y, 0, (s # r),z)):,
. The technique for doing so is
described later, but for the present, if the posterior density
estimate at O* is denoted by +(O* y ) , then the proposed
estimate of the marginal density, on the computationally
convenient logarithm scale, is
lnm(y) = In f (ylO*) + lnr(O*)- ln+(O* y ) .
It is important to observe the simplicity and benefits of
this expression: all it requires is the evaluation of the log-
likelihood function and the prior and an estimate of pos-
terior ordinate. The estimate does not suffer from any
instability problem, because it is a density value that is
averaged rather than its inverse. In addition, the entire es-
timation (simulation) error arises from the estimation of
the posterior ordinate, and this simulation error can be de-
Journal of the American Statistical Association, December 1995
rived, as shown in Section 3. It is now time to examine the
method for calculating the posterior density estimate from
the Gibbs output.
2.1 Estimation of ~(O*ly).
Consider now the estimation of the multivariate density
T(O* y) and the selection of the point O*. As was pointed
out, the BMI expression holds for any 0,and thus the choice
of the point is not critical, but efficiency considerations dic-
tate that for a given number of posterior draws, the density
is likely to be more accurately estimated at a high density
point, where more samples are available, than at a point in
the tails. It should be noted that a modal value such as the
posterior mode, or the maximum likelihood estimate, can
be computed from the Gibbs output, at least approximately,
if it is easy to evaluate the log-likelihood function for each
draw in the simulation. Alternatively, one can make use of
the posterior mean provided that there is no concern that it
is a low density point.
We now explain how the posterior density ordinate can
be estimated from the Gibbs output, starting with a canoni-
cal situation consisting of two blocks of parameters before
turning to the general case. We show that the proposed
multivariate density estimation method is easy to imple-
ment, requires only the available complete conditional den-
sities, and produces a simulation consistent estimate of the
posterior ordinate.
2.1.1 Two Vector Blocks. Suppose that Gibbs sampling
is applied to the complete conditional densities
which is the setting of Tanner and Wong . Let the
output from the Gibbs algorithm be given by { o ( ~ ) ,
and suppose that O* is the selected point. If the posterior
density is written as
then it follows that an appropriate Monte Carlo estimate of
~ ( 0 y )at O* is
because z(g) is a draw from the distribution zly. Gelfand
and Smith referred to this technique as Rao-
Blackwellization and argued that it improves on the multi-
variate kernel method . Also, under regularity
conditions, the estimate is simulation consistent; that is,
+(O* ly) -+ T(O* y ) as G becomes large, almost surely, as a
consequence of the ergodic theorem . Sub-
stituting the estimate of the posterior ordinate into (6) gives
Chib: Marginal Likelihood from the Gibbs Output
the following estimate of the marginal likelihood:
This simple expression can be used for a large class of mod-
els, including the probit regression model discussed later.
Observe that the calculation amounts to evaluating the like-
lihood, the prior, and the "complete data" posterior density
at the point 8".
2.1.2 Three Vector Blocks. An even larger class of mod-
els can be covered by slightly generalizing the Tanner and
Wong structure. Suppose that the Gibbs sampler is defined
through the complete conditional densities
Models such as linear regression, linear regression with in-
dependent Student-t errors, Zellner's seemingly unrelated
regression, and censored regression either fall in this cat-
egory or are a special case of this structure if z is absent.
Once again, the objective is to estimate ~ ( 8 *
ly),which now
is expressed as
is the reduced conditional density ordinate. It should be
clear that the normalizing constants of ~ ( e l l y ,02,Z) and
r(B2ly, el,z) must be included in the integration for the
decomposition in (8) to be valid. The first ordinate, ~ ( 0 ;ly),
can be estimated in an obvious way, by taking the ergodic
average of the full conditional density with the posterior
draws of (02,z), leading to the estimate
eF),~(g)).
A similar technique, with an important twist, can be invoked
to obtain the reduced conditional ordinate in (9). Recognize
that the draws of z from the Gibbs sampler are from the dis-
tribution [ziy] and not from [zly, 871. Therefore, the com-
plete conditional density of 82 cannot be averaged directly.
A simple solution is available to deal with this complica-
tion: Continue sampling for an additional G iterations with
the complete conditional densities
where in each of these densities, el is set equal to 8;. From
MCMC theory, it can be verified that the draws {z(j))from
this run follow the density p(zly,By), as required. Conse-
quently, +(8;j y, 8;) = G-I C 7r(6J;ly, 07, z(j)) is a simu-
lation consistent estimate of (9). Although this procedure
leads to an increase in the number of iterations, it is impor-
tant to stress that it does not require new programming and
thus is straightforward to implement. Note that the reduced
conditional run is not necessary if z is absent from the sam-
pling. In this case the reduced conditional density of e2is
identical to its complete conditional density, and the density
estimate reduces to one used by Zellner and Min in
a different context.
Substituting the two density estimates into (6) yields the
2.1.3 General Case. Although the technique described
thus far will apply to many problems of importance, con-
sider the situation with an arbitrary number of blocks. Even
in this case, the posterior density ordinate can be estimated
rather easily.
Begin by writing the posterior density at the selected
where the first term is the marginal ordinate, which can
be estimated from the draws of the initial Gibbs run,
and the typical term is the reduced conditional ordinate
"(8: y , 07, e;, . . . ,e:-,). The latter is given by
where T is being used to denote density and distribution
function interchangeably. To estimate this term, continue
the sampling with the complete conditional densities of
{e,, Or+,, . . . ,eB,z), where in each of these full condi-
tional densities, 0, is set equal to 8:, (s < r - 1).If the
draws from the reduced complete conditional Gibbs run are
denoted by {@, Q:$,,
. . . ,B!),
z(j)},then an estimate of
whereas an estimate of the joint density is n:=,
8: (s < r)). The log of the marginal likelihood is
lnm(y) = In f (ye*)+ Inr(B*)- xln+(8:ly, e:(s < r ) ) .
As an illustration of this procedure, suppose that B = 3,
a situation that arises in longitudinal random effects models
and many other models. Then ~ ( 8 %
y, 8;) is estimated
as G-l C~ ( 8 %
y, 8T, OF),~ ( j ) ) ,where the draws {OF),z(j))
are obtained by continuing the Gibbs sampler with
Finally, additional G iterations with the densities
produce draws { z ( j ) )that follow the distribution [ z y, 81, Qa].
These draws yield an estimate ~ ( 8 ;
This tech-
y, 8;,8%).
nique is illustrated in Section 4.2 for mixture models.
2.2 Bayes Factor Estimate
To compute the Bayes factor for any two models k and
is, m ( y1Mk)/m(yMl)-the
calculation described
earlier is repeated for all models, and the following estimate
An estimate of the posterior odds of any two models is
given by multiplying the estimated Bayes factor by the prior
2.3 Remarks
In some situations there are two sets of latent vectors
( z ,+) such that the density f ( y e ,+) = J f ( y , zl0, +) dz
is available in closed form but the likelihood f(yl8)
= J f ( y , $18) d+ is not. This occurs, for example, in dis-
crete response data models with random effects. To analyze
this situation, one can use the BMI expression
Both the numerator and denominator can be evaluated at
the point (O*,+*), and the posterior mean of (8,+) and
n(8, +ly) can be estimated using the method in Section 2.1
by treating + as an additional block.
The BMI can also be used to assess the convergence
of the Gibbs sampler, by computing and monitoring its
stability for different iterations. Such an idea, combined
with a different approach for computing the posterior den-
sity, appears in the Gibbs stopper proposed by Ritter and
Tanner . Raftery mentioned using the ker-
nel estimate of the posterior density in connection with the
BMI, but the resulting estimate can inherit the inaccuracy
of the kernel method, especially in high dimensions. Finally, another identity similar to the BMI is available in
the prediction context. Suppose that yf denotes an out-of-
sample observation. Then the Bayesian prediction density,
~ Y ,~ ) T ( ~ I Y )
do, can be expressed as
Journal of the American Statistical Association, December 1995
 . This identity follows in a straightfor-
ward manner from the definition of the posterior density
n ( 8 y, yf ) and cross-multiplying. Besag alluded to
a different proof.
3. NUMERICAL STANDARD ERROR
As mentioned in the preceding section, the proposed den-
sity estimation procedure is likely to produce an accurate
estimate of n ( 8 y ) at the point 8*. In fact, it is possible
to calculate the accuracy achieved by a computation that
uses the Gibbs output. This calculation yields the numerical
standard error of the marginal density estimate (or, equiva-
lently, that of the posterior density estimate). The numerical
standard error gives the variation that can be expected in
the estimate if the simulation were to be done afresh, but
the point at which the ordinate is evaluated is kept fixed.
To concentrate on the main ideas, consider the case in
Section 2.1.2 and define the vector stochastic process
where in the first component the latent vector (e2,Z )
while in the second component the latent vector z follows
the distribution [. y, 81]. In general, h is a B x 1vector with
the rth component given by "(8: Iy, 87, e;,.. . ,
> r ) ,z),the integrand of (10).
It should be noted that due to the procedure used to es-
timate the reduced conditional ordinate, the second com-
ponent of h is approximately independent of the first. But
for expositional simplifications, it is worthwhile to proceed
with the vector formulation. Then in this notation,
and our objective is to find the variance of two functions
of h, namely
= hl x h2 and $2 = ln(hl)+ 1n(h2)
-. In ?(BT y ) + In ?(Ba y, 8;). The variance of these two
functions is found by the delta method as soon as the vari-
ance of h is determined. Because h inherits the ergodic-
ity of the Gibbs output, it follows by the ergodic theorem
 that
almost surely, where p =
( ~ ( O y l y ) , y , Q T ) ) ' ,
lirn G { E ( ~- p)(h- p)') = 27rS(O),
and S(0) is the spectral density matrix at frequency zero.
An estimate of f2 = 27rS(O)can be obtained by the approach
of Newey and West or Geweke . If
Chib: Marginal Likelihood from the Gibbs Output
Table 1. Nodal lnvolvement Data
where q is some constant, essentially the value at which the
autocorrelation function tapers off. In the applications to
follow q is conservatively set equal to 10, although there
was negligible to vanishing serial correlation in the h ( g )
process. The variance of $ J ~ ,for example, is found by the
delta method to be
where the derivative vector consists of elements h;'
h;'. The square root of this variance is the numerical stan-
dard error of the marginal likelihood in the log scale.
4. EXAMPLES
In this section the approach developed earlier is applied to
two important classes of models. In particular, the methods
are discussed in the context of variable selection in binary
probit regression models and in the context of two broad
classes of finite mixture models, the iid mixture model and
the Markov mixture model.
By way of notation, for a d-dimensional normal random vector with mean p and covariance matrix X ,
the density at the point t is denoted by $(tip,X )
- ( ~ T ) ~ / ~ c - ' / ~
exp(-(t - j.~)'X-'(t- p)/2) and the
inverse gamma density at the point s is denoted by
pIG(sa, b) E (ba/r(a))
exp(-bls). Finally, for
( l / ~ ) ( ~ + ' )
a m vector q on the unit simplex, the Dirichlet
D(a1, a2,. . .,a,) density is denoted by p ~ ( q l a 1 , ... ,a,)
-r(C,aj)qP1-l... q$-l/ njr ( a j ) .
4.1 Binary Probit Regression
Consider the data in Table 1 on the presence of prostatic
nodal involvement collected on 53 patients with cancer of
the prostate. The data ; see also Collett 1991) include a binary response
variable y that takes the value 1 if cancer had spread to the
surrounding lymph nodes and value zero otherwise. The
objective is to explain the binary response with five vari-
ables: age of the patient in years at diagnosis ( z l ) ;level of
serum acid phosphate (x2);the result of an X-ray exami-
nation, coded 0 if negative and 1 if positive ( Q ) ; the size
of the tumor, coded 0 if small and 1 if large ( 5 4 ) ; and the
pathological grade of the tumor, coded 0 if less serious and
1 if more serious (x5).
The probability of positive response can be explained
through a probit link function or, as by Collett , by
a logit link. If interactions and powers of explanatory vari-
ables are excluded, then there are 32 possible models that
can be fit. Collett's finding from the classical deviance
statistic (-2 times the maximized log-likelihood) is that
the logistic model containing log(z2),xs, and x4 provides
a suitable fit for the data among these 32 models. These
data are reanalyzed to demonstrate the computation of the
marginal likelihood using nine of these models (defined
later and selected entirely for illustrative purposes).
Under model Ic, suppose that
where @(.) is the cumulative distribution function of the
standard normal density, xik are the covariates included in
model Ic, and 0, is the corresponding regression parame-
ter vector. The likelihood function under Mk, assuming a
Journal of the American Statistical Association, December 1995
Summary of Results for Nodal Involvement Data
the sampler for G = 5,000 cycles after deleting the first
500, and the estimate P* =
is obtained.
c~ ( ~ ) / 5 , 0 0 0
Then the logarithm of the marginal likelihood of model
(maximized
Terms fitted in model
(marginal)
random sample, is then
For this situation, the marginal likelihood can be computed
rather simply by the Laplace method , but given the small sample size, it is difficult to know
the accuracy of the Laplace approximation. Harmonic mean
type estimators, on the other hand, are rather more difficult
to obtain with this likelihood, because its tails generally
decline quite sharply.
A procedure that works extremely well in conjunction with the technique developed above is the data
augmentation-Gibbs sampling method of Albert and Chib
 . Suppose that the prior information about PI, is
weak, but not improper, and is represented by a multivari-
ate normal prior with the mean of each parameter equal to
.75 (because each covariate is expected to have a positive
impact on the probability of response), and a standard de-
viation of 5. Under the assumption that the parameters are
independent, the prior of PI,takes the form
PI, - N(aI,, A,').
Suppressing the model index k , the Gibbs draws for each
model are obtained as follows. Define a normally dis-
tributed latent variable, zi, such that
where I(A) is an indicator function of the event A. This
in fact is equivalent to the probit model, because Pr(zi
> 0) = Q(xiP).Then, following Albert and Chib ,
the Gibbs sampler is defined through the complete condi-
tional densities
.(Ply, z) = ~(PIB,, B)
where, it should be noted, the mean vector of the third den-
sity (i.e., p, ) is produced as a by-product of the sampling
algorithm.
The results are summarized in Table 2, where for each
of nine models, the maximized likelihood is reported along
with the degrees of freedom, the log of the marginal like-
lihood, and its numerical standard error. From this table it
can be seen that the marginal likelihood is very precisely
estimated in all the fitted models. Of course, these results
are obtained with G = 5,000 draws, and further improve-
ments in accuracy can be achieved by increasing G. For
comparison, the BMI expression was evaluated at a point
that was one posterior standard deviation from P*.As ex-
pected, this led to an increase in the numerical standard
error of the estimate and, for example, was .26 in Mg,with
G = 5,000.The Laplace method was also used to determine
the marginal likelihood, and the results were in agreement
up to the second decimal place. We also examined if a
multivariate kernel estimate of the posterior ordinate (with
a Gaussian product kernel) could be used in the BMI ex-
pression. This procedure did not produce equally accurate
results. Also note that xl (the age variable) does not im-
prove on the model with just a constant (the Bayes factor for
the second model vs. the first is .009), whereas the model
with the variable x3 (X-ray) has a Bayes factor of approxi-
mately 25 versus the model with just a constant. The Bayes
factor for M8 versus Mg is 5.33, supporting the conclusion
of Collett , who argued that M8 is the best model,
and also demonstrating the value of the marginal likelihood
in providing information about the comparative value of a
fitted model.
4.2 Marginal Likelihood in Mixture Models
To further illustrate the usefulness of our approach, con-
Table 3. Velocity (km/second) for Galaxies in the Corona Borealis Region
where B, = (A + XfX)-'(Aa +Xfz),B= (A + XfX)-l,
z = (zl,2 2 , . . . , zn)', X is the matrix of all the covariates,
and 4(.Ip, l)I[a, b] is the normal density truncated to the in-
terval [a,b].The output {P(~),
is obtained by running
Chib: Marginal Likelihood from the Gibbs Output
Table 4. Summary of Results for Galaxy Data
distributions
Model fitted
log(margina1)
Two components: a: = a', V j
Three components: a: = a', V j
Three components: a: unrestricted
sider the calculation of the marginal likelihood in two broad
applications that involve mixture models. The first is concerned with determining the number of components in a
Gaussian finite mixture model applied to astronomical data
on the velocity of galaxies. The second is concerned with a
mixture model that applies to time series data. This model,
which is also referred to as a Markov switching model or a
hidden Markov model, is illustrated with data on the growth
rates of U.S. gross national product for the postwar period.
4.2.1 Determining the Number of Components in a
Consider the data set in Table 3 on velocities
of 82 galaxies from 6 well-separated conic sections of the
Corona Borealis region, originally presented by Postman,
Huchra, and Geller . The objective is to find the
best-fitting Gaussian finite mixture model. This data set
has been analyzed by Roeder who developed a nonparametric density approach to determine the number of
modes. Subsequently, Carlin and Chib reanalyzed
the data by parametric Bayesian methods and estimated
Gaussian mixture models with two to five components using the Gibbs sampler. Their results indicate symptoms of
overfitting when models with four or five components are
estimated. The Gibbs output from these models displays
nonvanishing serial correlation for extremely high lags, indicating difficulties with convergence and nonidentifiability
of parameters. For this reason, models
with two and three components are fit.
For the model with d components, suppose that the
jth component is given by 4(yilpj,a;),where yi i s ith
data value (velocity/1,000) and ( p j ,a;) is the componentspecific mean and variance. If each component is sampled
with probability qj (Cq, = I ) , then the density function of
the data y = ( y l ,.. . ,ys2) given the parameters 8 is
where8 = ( q , p , u 2 )withq = ( q l , q ~ , . . . , q d ) , p
= ( ~ 1 , p 2 ,
. . . ,pd), and u2= (a?,. . .,a;).It is useful to refer to this
model as the "iid mixture model" because, as is well known,
by introducing iid latent variables zi E { 1 , 2 , . . . ,d ) such
and defining f (yilzi = j, 8) = 4(yilpj,a;) leads to the
mixture model in (16).
Assume that all components of 8 are mutually independent, and define the prior information through the
where po = 20,A-' = 100,vo = 6 ,
= 40, and a, = 1. As
can be observed, these priors reflect weak prior information
about the parameters. Under these prior distributions, the
objective is to compute the marginal likelihood for models
with two and three components. In addition, models obtained by restricting the variance a; to be constant across
components are also of interest.
The Gibbs implementation for this model is straightforward . Let
z = (zl,. .. ,z,), then Gibbs sampling is defined through
the conditional densities of p, u2,q , and z. Let Tj = {i :
zi = j ) be the set of observation indices for the observations classified into the jth population and let n, represent
the number of observations so assigned. Now pick out the
observations that correspond to the jth population and place
them in the vector y, and define an n, vector i, comprising
of units. Then
and Pr(zi = jly, 0) oc qj x 4(yilp,,a2),i 5 n, where f i j
= ( A + a j 2 n j ) - ' ( ~ p ~
+ a j 2 i i y j ) ,~j
= ( A + a-'n.)-l
and 6, = ( y j - ijpj)'(yj- ijp,).
The posterior density ordinate can be computed from the
decomposition
where 8*is taken to be the (approximate) maximum likelihood estimate computed by evaluating (16) for each simulated draw. Now apply (11) as follows:
The draws from the full Gibbs run are used to estimate
T ( P * Y )= J Ilg=l 4 ( , Ib,, Bj)n(z,a21y)dz dm2.
Next, the draws from the reduced Gibbs run with
the densities ~ ( a ; l y ,z , p*),n(qly,z ) and {Pr(zily,
p*,a2,q ) ) are used to estimate n(a2*Ip*,y )
- J dlP I G ( ~ ~ I ~ { v o + ~ ) ,
~ { 6 0 + 6 j ) ) n ( z y ,
Finally, the draws from the subsequent reduced
Gibbs run with the densities n(qly,z ) and {Pr(zil y ,
p*,u2*,q ) ) are used to estimate ~ ( q * l y ,
= J ~ ~ ( q l a l
+nl, . .. ,ad + nd)p(zly,p*,a2*)dz.
Journal of the American Statistical Association, December 1995
Table 5. Summary of Results for U.S. GNP Growth Rates Data
Log marginal likelihood
-229.496 (.028)
An estimate of the marginal likelihood is given by substi-
tuting these quantities into (12).
Our results, which are based on G = 5,000 draws, are
summarized in Table 4. (Almost identical results were ob-
tained when the BMI expression was evaluated at the poste-
rior mean instead of the approximate maximum likelihood
value.) First, the two-component model is clearly domi-
nated by both three-component models. Second, the three-
component model with a2unrestricted appears to be better
than the three-component model with a2restricted to be the
same across components. This result would not be obvi-
ous from just looking at posterior distributions of the fitted
models, because all the parameters in both three-component
models are tightly estimated. Third, all the numerical stan-
dard errors are small, indicating that the marginal likelihood
has been accurately estimated.
4.2.2 Markov Mixture Model. As a final illustration of
the value of our approach, consider data on the quarterly
growth rates of U.S. gross national product (GNP) for the
postwar period 1951.2 to 1992.4. Many different time series
models have been fit to this data, and our objective is to
demonstrate how the marginal likelihood can be calculated
in one particular case, of substantial practical importance,
for which this calculation has hitherto not been attempted.
The model of interest is the Markov mixture model,
also sometimes referred to as the Markov switching model
 . Let yt denote
the growth rate of GNP (multiplied by loo), and suppose
where p = (pl, p2) and zt is an unobserved state variable
that follows a two-state Markov chain,
where P = {pij) is the one-step transition probability ma-
trix of the chain (i.e., pij = Pr(zt = jlztPl = i), and rl is
the probability distribution at t = 1. This model is a gen-
eralization of the iid mixture model of the last subsection.
Furthermore, it is a model that is particularly appropriate
for modeling correlation in growth rates that are observed
in practice.
Let 8 = (p,a2,91,qz), where qi is the ith row of P;
then the likelihood function for the Markov mixture model
is given in terms of the one-step ahead prediction densities,
where K-l is the observed data up to time t - 1 and p(zt
= 1lK-l, 8) is a time-varying conditional probability. The
joint density of all the data is then
A little reflection shows that, given z, this model has the
same structure as the iid mixture model, and thus the
marginal likelihood calculation proceeds in virtually the
same way. The complete conditional densities of (p,a2)
are identical to those in the iid mixture model, and, if one
assumes that the prior density on qi is Dirichlet(cril, cri2),
where nij denotes the number of one-step transitions from
i to j in the sequence z . A
decomposition similar to (18) is again available while each
of the ordinates can be estimated by the reduced conditional
Gibbs sampling procedure described earlier.
The Gibbs implementation of this model, and the cal-
culation of the marginal likelihood, require the simula-
tion of the latent variables z from p(zl y, 8). As described
by Chib , the latent variables are simulated through
the following recursive steps, which are initiated with
p(zo = ilYo, 8). These recursions require one pass from
t = 1to n and then a second pass from t = n to t = 1.
Step 1: Repeat for t = 1,2,.. . , n.
Prediction step: Calculate
Update step: Calculate
Step 2: Simulate z, from p(zn =j lYn,8),the mass func-
tion produced by the last update step.
Step 3: Repeat for t = n - 1,.. . ,2,1.
Given the draw zt+l = 1, calculate
p(zt = jlYn,zt+l = 48) pjl x ~ ( z t= jlK,8),
Simulate zt from p(zt = jlYn, zt+l = 1,8).
Note that the prediction step gives the time-varying prob-
ability mass function required to calculate the likelihood
function in (20) at the point 8*.
Our results for this model and data are summarized
in Table 5.
These results are based on G = 6,000
draws and rely on the prior distributions pl - N(O,2), p2
Chib: Marginal Likelihood from the Gibbs Output
- N(.75,2),a2 N 2G(4,4),q 1
Dirichlet(4, I), and q z
- Dirichlet(l,4). These priors are relatively vague and are
designed to model the potential persistence in low and high
growth rates. Thus the marginal likelihood is seen to be
equal to -229.496 on the log scale and is accurately esti-
mated with a numerical standard error of .028. In com-
parison, the marginal likelihood is also calculated for a
first-order autoregressive model yt = po+ ,!Il ytPl + E ~ ,E~
N N(0, a'), by treating this as a linear regression model
after conditioning on the first observation. Under the prior
N2(0, diag(l0,lO)) and a2 - 26(3,3), the log
marginal likelihood is estimated to be -231.94. Thus the
data support the Markov mixture model to the first-order
autoregressive model.
5. CONCLUDING REMARKS
In summary, this article has developed and illustrated a
new approach to calculating the marginal likelihood that
relies on the output of the Gibbs sampling algorithm. The
approach is fully automatic and stable, requiring no inputs
beyond the draws from the simulation. Thus draws from the
prior, or additional maximizations, or importance sampling
functions, or any other tuning function, are not required.
It was shown that the numerical standard error of the es-
timate can be derived from the posterior sample and the
calculations are exhibited in problems dealing with probit
regression and finite-mixture models. In all the examples,
the marginal likelihood is estimated easily and very accu-
rately. As a result, this approach should encourage the rou-
tine calculation of Bayes factors in models estimated by the
Gibbs sampler.
[Received May 1994. Revised February 1995.1