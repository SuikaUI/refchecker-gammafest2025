“© 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for
all other uses, in any current or future media, including reprinting/republishing this material for
advertising or promotional purposes, creating new collective works, for resale or redistribution to
servers or lists, or reuse of any copyrighted component of this work in other works.”
Learning Filter Pruning Criteria
for Deep Convolutional Neural Networks Acceleration
Yuhang Ding2
Linchao Zhu1
Hanwang Zhang3
1ReLER, University of Technology Sydney
2Baidu Research
3Nanyang Technological University
 ,
{dyh.ustc.uts,pino.pingliu,zhulinchao7}@gmail.com
 ,
 
Filter pruning has been widely applied to neural network compression and acceleration. Existing methods usually utilize pre-deﬁned pruning criteria, such as ℓp-norm, to
prune unimportant ﬁlters. There are two major limitations
to these methods. First, prevailing methods fail to consider
the variety of ﬁlter distribution across layers. To extract
features of the coarse level to the ﬁne level, the ﬁlters of
different layers have various distributions. Therefore, it is
not suitable to utilize the same pruning criteria to different
functional layers. Second, prevailing layer-by-layer pruning methods process each layer independently and sequentially, failing to consider that all the layers in the network
collaboratively make the ﬁnal prediction.
In this paper, we propose Learning Filter Pruning Criteria (LFPC) to solve the above problems. Speciﬁcally, we
develop a differentiable pruning criteria sampler. This sampler is learnable and optimized by the validation loss of the
pruned network obtained from the sampled criteria. In this
way, we could adaptively select the appropriate pruning criteria for different functional layers. Besides, when evaluating the sampled criteria, LFPC comprehensively considers
the contribution of all the layers at the same time. Experiments validate our approach on three image classiﬁcation
benchmarks. Notably, on ILSVRC-2012, our LFPC reduces
more than 60% FLOPs on ResNet-50 with only 0.83% top-5
accuracy loss.
1. Introduction
Convolutional neural networks have achieved signiﬁcant
advancement in various computer vision research applications . However, most of these manually designed architectures, e.g., VGG , ResNet , usually
come with the enormous model size and heavy computation cost. It is hard to deploy these models in scenarios de-
∗Corresponding Author.
Network Layer
Layer after pruning
Forward process
Pruning process
Previous pruning method
Our pruning method
(a) Previous ﬁlter pruning methods manually select a
criterion and apply it to all layers; (b) our pruning method learns
appropriate criteria for different layers based on the ﬁlter distribution. In the blue dashed box, the solid boxes of different colors denote different pruning criteria. The yellow boxes without shadow
correspond to unpruned layers of the network, while the ones with
shadow are the layers pruned by a selected pruning criterion.
manding a real-time response. Recently, studies on model
compression and acceleration are emerging. Due to its efﬁcacy, the pruning strategy attracts attention in previous studies .
Recent developments on pruning can be divided into two
categories, i.e., weight pruning , and ﬁlter pruning .
Filter pruning is preferred compared to weight pruning because ﬁlter pruning could make the pruned model more
structural and achieve practical acceleration . The existing ﬁlter pruning methods follow a three-stage pipeline. (1)
Training: training a large model on the target dataset. (2)
Pruning: based on a particular criterion, unimportant ﬁlters from the pre-trained model are pruned. (3) Fine-tuning
(retraining): the pruned model is retrained to recover the
original performance. During the three stages, select an appropriate pruning criterion is the key ingredient.
However, the previous works have a few drawbacks and
might not be the best choice in real scenarios. First, previ-
ous works manually specify a pruning criterion and utilize
the same pruning criterion for different layers. As shown
in , different layers have different ﬁlter distributions and
various functions. The lower layers tend to extract coarse
level features, such as lines, dots, and curves, while the
higher layers tend to extract ﬁne level features, such as common objects and shapes. In this situation, ﬁxing one pruning
criterion for all the functional layers may not be suitable.
Second, prevailing methods prune the network in a greedy
layer-by-layer manner, i.e., the pruning process at different
layers is independent of each other. Considering that during training and inference, the ﬁlters of all the layers work
collaboratively to make a ﬁnal prediction, it is natural to
suggest to conduct pruning in a collaborative, not an independent, manner. In other words, it is preferred that the ﬁlter
importance of all layers could be evaluated concurrently.
We propose Learning Filter Pruning Criteria (LFPC) to
solve the mentioned problems.
The core component of
LFPC is a Differentiable Criteria Sampler (DCS), which
aims to sample different criteria for different layers. This
sampler, since it is differentiable, can be updated efﬁciently
to ﬁnd the appropriate criteria. First, DCS initializes a learnable criteria probability for all layers. For every layer, DCS
conducts criteria forward to get the criteria feature map
based on the ﬁlters and criteria probability. The process of
criteria forward is shown in Sec. 3.2.3. After criteria forward for all the layers, we get the criteria loss and utilize
it as a supervision signal. The criteria loss can be backpropagated to update the criteria probability distribution to
ﬁt the ﬁlter distribution of the network better.
from previous layer-by-layer pruning works, our LFPC can
consider all the layers and all the pruning criteria simultaneously through the criteria loss. After ﬁnishing training the
DCS, the optimized criteria servers as the pruning criteria
for the network, as shown in Fig. 1. After pruning, we ﬁnetune the pruned model once to get an efﬁcient and accurate
Contributions. Contributions are summarized as follows:
(1) We propose an effective learning framework, Learning
Filter Pruning Criteria (LFPC). This framework can learn to
select the most appropriate pruning criteria for each functional layer. Besides, the proposed Differentiable Criteria
Sampler (DCS) can be trained end-to-end and consider all
the layers concurrently during pruning. To the best of our
knowledge, this is the ﬁrst work in this research direction.
(2) The experiment on three benchmarks demonstrates the
effectiveness of our LFPC. Notably, it accelerates ResNet-
110 by two times, with even 0.31% relative accuracy improvement on CIFAR-10. Additionally, we reduce more
than 60% FLOPs on ResNet-50 with only 0.83% top-5 accuracy loss.
2. Related Work
Previous work on pruning can be categorized into weight
pruning and ﬁlter pruning. Weight pruning focuses on pruning ﬁne-grained weight of ﬁlters,
so that leading to unstructured sparsity in models. In contrast, ﬁlter pruning could achieve the structured sparsity, so the pruned model could take full advantage of highefﬁciency Basic Linear Algebra Subprograms (BLAS) libraries to achieve better acceleration.
Considering how to evaluate the ﬁlter importance, we
can roughly divide the ﬁlter pruning methods into two categories, i.e., weight-based criteria, and activation-based criteria. Furthermore, the pruning algorithms could also be
roughly grouped by the frequency of pruning, i.e., greedy
pruning, and one-shot pruning. We illustrate the categorization in Tab. 1.
Algorithms
PFEC , SFP , FPGM 
RSA , PRE 
SLIM , PFA , NISP ,
CCP , GAL 
CP , SLIM , ThiNet ,
PRE , DCP , LPF ,
AOFP , GATE 
Different categories of ﬁlter pruning algorithms. “W”
and “A” denote the weight-based and activation-based criteria. “O”
and “G” indicate the one-shot and greedy pruning.
Weight-based Criteria. Some methods utilize the weights of the ﬁlters to determine the
importance of the ﬁlters. prunes the ﬁlters with small
ℓ1-norm. utilizes ℓ2-norm criterion to select ﬁlters and
prune those selected ﬁlters softly. introduces sparsity
on the scaling parameters of batch normalization (BN) layers to prune the network. claims that the ﬁlters near the
geometric median should be pruned. All the works utilize
the same pruning criteria for different layers and do not take
into account that different layers have various functions and
different ﬁlter distributions.
Activation-based Criteria. Some works utilize the training data and ﬁlter activations to determine the pruned ﬁlters. adopts the Principal Component Analysis (PCA)
method to specify which part of the network should be preserved. proposes to use the information from the next
layer to guide the ﬁlter selection. minimizes the reconstruction error of training set sample activations and applies
Singular Value Decomposition (SVD) to obtain a decomposition of ﬁlters. explores the linear relationship in
different feature maps to eliminate the redundancy in convolutional ﬁlters.
Greedy and One-shot Pruning. Greedy pruning ,
or oracle pruning, means the pruning and retraining should
be operated for multiple times. Although greedy pruning
is beneﬁcial for accuracy, it is time-consuming and requires
a large number of computation resources. In contrast, oneshot pruning prunes the network once and retrained
once to recover the accuracy. It is more efﬁcient than the
greedy pruning, but it requires careful pruning criteria selection. We focus on one-shot pruning in this paper.
Other Pruning and Searching Methods. Some works utilize reinforcement learning or meta-learning 
for pruning. In contrast, we focus on learning the proper
pruning criteria for different layers via the differential sampler.
 proposes centripetal SGD to make several ﬁlters
to converge into a single point.
 is a global pruning method, but the importance of pruned neurons is not
propagated. The idea of our learning criteria shares some
similarities with the Neural Architecture Search (NAS)
works and Autoaugment , the difference is that
our search space is the pruning criteria instead of network
architectures or augmentation policies.
3. Methodology
3.1. Preliminaries
We assume that a neural network has L layers, and we
represent the weight for lth convolutional layers as W(l) ∈
O , where K is the kernel size , C(l)
is the number of input and output channels, respectively. In
this way, W(l)
∈RK×K×C(l)
represents the ith ﬁlter of lth
convolutional layer. We denote the I and O as the input and
output feature maps, and I ∈C(l)
O , where H()
∗is the height and
width of the feature map, respectively. The convolutional
operation of the ith layer can be written as:
∗I for 1 ≤i ≤C(l)
Assume the ﬁlter set F consists all the ﬁlters in the network: F =
i , i ∈[1, C(l)
O ], l ∈[1, L]
. We divide
F into two disjoint subsets: the kept ﬁlter set K and removed ﬁlter set R, and we have:
Now our target becomes clear. Filter pruning aims to
minimize the loss function value under sparsity constraints
on ﬁlters. Given a dataset D = {(xn, yn)}N
n=1 where xn
denotes the nth input and yn is the corresponding output,
the constrained optimization problem can be formulated as:
K L(K; D) = min
L(K; (xn, yn))
Filter without score
Filter with scores
The pruned filter
Normal Forward
Normal Forward
Criteria Forward
Criteria Forward
Criteria Backward
Criteria Space
Criteria Distribution
Pruning with Criteria
Criteria forward and backward in the network. Grey
boxes are the normal ﬁlters. The probability distribution of criteria for three layers are initialized, as shown in the big orange
shadow. After pruning with four criteria, we obtain four “pruned
versions” for every layer, which are denoted as boxes in purple,
green, orange, and blue color. These ﬁlters are utilized to conduct
criteria forward. Then we get the criteria loss on the validation
set to update the “criteria distribution”.
where L(·) is a standard loss function (e.g., cross-entropy
loss), C(·) is the computation cost of the network built from
the ﬁlter set, and r is the ratio of the computation cost of between pruned network and the original unpruned network.
3.2. Learning Filter Pruning Criteria
In this section, we illustrate our proposed LFPC, which
can automatically and adaptively choose an appropriate criterion for each layer based on their respective ﬁlter distribution. The overall learning process is shown in Fig. 2.
Pruning Criteria
For simplicity, we introduce the pruning criteria based on
lth layer. The ﬁlters in lth layer are denoted as a ﬁlter set
i , i ∈[1, C(l)
. In lth layer, a pruning criterion, denoted as Crit(l)(·), is utilized to get the importance
scores for the ﬁlters. Then we have
score(l) = Crit(l)(F (l))
where score(l)
is the importance score vector of the ﬁlters in lth layer.
For example, ℓ1-norm
criteria could be formulated as Crit(l)(F(l))
Crit(l)(W(l)
i ) = ∥W(l)
i ∥1 for i ∈[1, C(l)
Then ﬁlter pruning is conducted based on score(l):
keepid(l) = Topk(score(l), n(l))
K(l) = Prune(F (l), keepid(l)),
Input feature
Aligned Weight Sum
Candidate Pruned Layer
Output Feature Map
Scored Filters
Scored Filters
Probability
Criteria Distribution
Scores via Criteria
Get Scores
Figure 3. Criteria forward within a layer. Boxes of different colors indicate the different pruning criteria. First, we evaluate the importance
of the ﬁlter based on different criteria. Second, we prune the ﬁlter with small importance scores and get four versions of pruned layers with
various probabilities. After that, the output feature map is the aligned weighted sum of four feature maps of the pruned layers.
where n(l) is the number of ﬁlters to be kept, and Topk(·)
returns the indexes of the k most important ﬁlter based
on their importance scores.
The indexes are denoted as
keepid(l). Given keepid(l), Prune(·) keeps the critical ﬁlters with the indexes speciﬁed in keepid(l), and prunes the
other ﬁlters. The ﬁlter set after pruning is denoted as K(l).
Criteria Space Complexity
If we want to keep n(l) ﬁlters in lth layer, which has totally C(l)
ﬁlters, then the number of selection could be
n(l)! (C(l)
O −n(l))!, where
denotes the combination . For those frequently used CNN architectures,
the number of selections might be surprisingly big. For example, pruning 10 ﬁlters from a 64-ﬁlter-layer has
151, 473, 214, 816 selections. This number would increase
dramatically if the more layers are considered. Therefore, it
is impossible to learn the pruning criteria from scratch. Fortunately, with the help of the proposed criteria of previous
works , we could reduce the criteria space complexity from
to S, which is the number of criteria that we
Differentiable Criteria Sampler
Assuming there are S candidate criteria in the criteria space,
we could use α(l) ∈RS to indicate the distribution of the
possible criteria for lth layer. The probability of choosing
the ith criterion can be formulated as:
where 1 ≤i ≤S
However, since Eq. 6 needs to sample from a discrete
probability distribution, we cannot back-propagate gradients through pi to α(l)
i . To allow back-propagation, inspired
from , we apply Gumbel-Softmax to reformulate
Eq. 6 as Eq. 7:
Algorithm 1: Algorithm Description of LFPC
Input : training data X, validation data Y ;
the pre-trained model with parameters
W = {W(l), 1 ≤l ≤L};
S candidate criteria and criteria parameters α ;
expected ratio of computational cost r ;
Output: The compact model and its parameters W∗
1 for epoch ←1 to epochmax do
for l ←1 to L do
Sample criteria based on Eq. 7 ;
Calculate criteria feature with Eq. 9 and r;
Update α based on Y using Eq. 11 ;
8 Get ﬁnal criteria set T and conduct pruning ;
9 Re-training pruned model with X and obtain W∗.
exp ((log (pi) + oi) /τ)
j=1 exp ((log (pj) + oj) /τ)
s.t. oi = −log(−log(u)) & u ∼U(0, 1)
where U(0, 1) is the uniform distribution between 0 and
1, u is a sample from the distributio U(0, 1), and τ is the
softmax temperature. We denote ˆp = [ˆp1, . . . , ˆpj, . . . ] as
the Gumbel-softmax distribution. Change the parameter τ
would lead to different ˆp. When τ →∞, ˆp becomes a uniform distribution. When τ →0, samples from ˆp become
one-shot, and they are identical to the samples from the categorical distribution .
Criteria Forward. The illustration of criteria forward
for lth layer is shown in Fig 3. For simplicity, we rewrite
the Eq. 4 and Eq. 5 as K(l) = g(F(l), Crit(l), n(l)). For
lth layer, it has S sampled “pruned version” which can be
formulated as:
= g(F (l), Crit(l)
s , n(l)) for s ∈[1, S]
where Crit(l)
denotes the process of utilizing sth pruning
criterion to get the importance scores for the ﬁlters in lth
layer, and K(l)
is the kept ﬁlter set under sth criterion. To
comprehensively consider the contribution of every criterion during training, the output feature map is deﬁned as
the Aligned Weighted Sum (AWS) of the feature maps from
different K(l)
s , which can be formulated as:
Align(ˆps × ˆOs),
Align( ˆOs,i) = ˆO
s,keepid(l)
i ∈[1, n(l)].
where OAW S is the criteria feature map of the layer, ˆps is
the probability for sth criteria, × denotes the scalar multiplication, ˆOs is the output feature map of K(l)
s , and ˆO
the aligned feature. For the second formulation, keepid(l)
is the ith element of the keepid (Eq. 5) under sth criteria in
lth layer. To explain the Align(·) function, we take the third
ﬁgure of Fig. 3 for example. The ﬁrst channel of purple
network could only be added with the ﬁrst channel of orange network, not the green and blue one. This operation
can avoid the interference of the information from different
channels. Further we have:
 ˆOs,1, ˆOs,2, ..., ˆOs,n(l)
ˆOs,i = K(l)
s ∈[1, S], i ∈[1, n(l)],
where ˆOs,i is the ith output feature of K(l)
s , and ∗is the convolution operation. After criteria forward for all the layers,
we could get the criteria loss, as shown in Fig. 2.
Training Objectives. For a L-layer network, the criteria parameter α =
α(1), α(2), ..., α(L)
. We aim to ﬁnd a
proper α to give us guidance about which criterion is suitable for different layers. Speciﬁcally, α is found by minimizing the validation loss Lval after trained the criteria network θα by minimizing the training loss Ltrain:
α Lval (θ∗
α = arg min
θα Ltrain(θα, α),
α is the optimized criteria network under the optimized criteria set α. The training loss is the cross-entropy
classiﬁcation loss of the networks. To further consider the
computation cost of the pruned network, the penalty for the
computation cost is also included in the validation loss:
Lval = Lcrit + λcompLcomp,
where Lcrit is the standard classiﬁcation loss of the criteria
network, namely the criteria loss, and Lcomp is the computation loss of the pruned network. λcomp is a balance of
these two losses, whose details can be found in the supplementary material. In this way, we could get the optimized
criteria parameters α for the network under different computation constraints.
Criteria Backward. We backward Lval in Eq. 12 to α
to update these parameters collaboratively at the same time.
The illustration of this process is shown in Fig. 2.
After DCS Training. By choosing the criterion with the
maximum probability, we get the ﬁnal criteria set T for all
the layers. Then we conduct a conventional pruning operation based on the optimized criteria T to get the pruned
network. The pruned network is then retrained to get the
ﬁnal accurate pruned model. The whole process is shown
in the Alg. 1.
4. Experiments
4.1. Experimental Setting
Datasets. In this section, we validate the effectiveness
of our acceleration method on three benchmark datasets,
CIFAR-10, CIFAR-100 , and ILSVRC-2012 . The
CIFAR-10 dataset contains 50, 000 training images and
10, 000 testing images, in total 60, 000 32×32 color images
in 10 different classes. CIFAR-100 has 100 classes, and
the number of images is the same as CIFAR-10. ILSVRC-
2012 contains 1.28 million training images and 50k
validation images of 1, 000 classes.
Architecture Setting. As ResNet has the shortcut structure, existing works claim that ResNet has less
redundancy than VGGNet and accelerating ResNet is
more difﬁcult than accelerating VGGNet. Therefore, we
follow to focus on pruning the challenging ResNet.
Normal Training Setting.
For ResNet on CIFAR-
10 and CIFAR-100, we utilize the same training schedule
as . In the CIFAR experiments, we run each setting
three times and report the “mean ± std”. In the ILSVRC-
2012 experiments, we use the default parameter settings,
which are the same as , and the same data argumentation strategies as the ofﬁcial PyTorch examples.
DCS training Setting. The weight-based criteria are selected as our candidate criteria for their efﬁciency. Specifically, ℓ1-norm , ℓ2-norm and geometric median based criteria.
The criteria could be formulated as Crit(l)(W(l)
i ) = ∥W(l)
i ∥p and Crit(l)(W(l)
for i ∈[1, C(l)
O ]. Note that our
framework is able to extend to more criteria.
We set desired FLOPs according to compared pruning
algorithms and set λcomp of Eq. 12 as 2. We randomly split
half of the training set as the validation set for Eq. 11. We
optimize the criteria parameters via Adam, and we use the
constant learning rate of 0.001 and a weight decay of 0.001.
On CIFAR, we train the DCS for 600 epochs with a batch
size of 256. On ILSVRC-2012, we train the DCS for 35
epochs with a batch size of 256. The τ in Eq. 7 is linearly
decayed from 5 to 0.1. During training DCS, we ﬁx the
pre-trained weights to reduce overﬁtting.
Pruning Setting. After training DCS, we prune the network with the optimized criteria and ﬁne-tune the network
with the full training set. We analyze the difference be-
Init pretrain
Baseline acc. (%)
Pruned acc. (%)
FLOPs ↓(%)
92.63 (±0.70)
92.08 (±0.08)
92.63 (±0.70)
92.31 (±0.30)
92.63 (±0.70)
92.12 (±0.32)
93.59 (±0.58)
93.56 (±0.29)
93.59 (±0.58)
92.26 (±0.31)
93.59 (±0.58)
92.89 (±0.32)
93.59 (±0.58)
93.34 (±0.08)
93.59 (±0.58)
93.72 (±0.29)
93.59 (±0.58)
93.26 (±0.03)
93.59 (±0.58)
93.24 (±0.17)
93.68 (±0.32)
93.38 (±0.30)
Rethink 
93.77 (±0.23)
93.70 (±0.16)
93.68 (±0.32)
93.73 (±0.23)
93.68 (±0.32)
93.79 (±0.38)
93.68 (±0.32)
93.74 (±0.10)
93.68 (±0.32)
93.07 (±0.15)
Table 2. Comparison of the pruned ResNet on CIFAR-10. In “Init pretrain” column, “” and “” indicate whether to use the pre-trained
model as initialization or not, respectively. The “Acc. ↓” is the accuracy drop between pruned model and the baseline model, the smaller,
the better. A negative value in “Acc. ↓” indicates an improved model accuracy.
tween pruning a scratch model and the pre-trained model.
For pruning the scratch model, we utilize the regular training schedule without additional ﬁne-tuning. For pruning the
pre-trained model, we reduce the learning rate to one-tenth
of the original learning rate. To conduct a fair comparison,
we use the same baseline model as for pruning. During
retraining, we use the cosine scheduler for a stable
result. The pruning rate of every layer is sampled in the
same way as DCS1, so we could search the ratio automatically and adaptively .
We compare our method with existing state-of-the-art acceleration algorithms, e.g., MIL , PFEC , CP ,
ThiNet , SFP , NISP , FPGM , LFC ,
ELR , GAL , IMP , DDS . Experiments
show that our LFPC achieves a comparable performance
with those works. Our experiments are based on the Py-
Torch framework. No signiﬁcant performance differ-
1See supplementary material for details.
ence has been observed with the PaddlePaddle framework.
4.2. ResNet on CIFAR-10
For the CIFAR-10 dataset, we test our LFPC on ResNet
with depth 32, 56, and 110. As shown in Tab. 2, the experiment results validate the effectiveness of our method.
For example, MIL accelerates the random initialized
ResNet-32 by 31.2% speedup ratio with 1.59% accuracy
drop, but our LFPC achieves 52.6% speedup ratio with only
0.51% accuracy drop. When we achieve similar accuracy
with FPGM on ResNet-32, our acceleration ratio is
much larger than FPGM .
Comparing to SFP ,
when we prune similar FLOPs of the random initialized
ResNet-56, our LFPC has 1.07% accuracy improvement
over SFP . For pruning the pre-trained ResNet-56, our
method achieves a higher acceleration ratio than CP 
with a 0.65% accuracy increase over CP . Comparing
to PFEC , our method accelerates the random initialized
ResNet-110 by 60.3% speedup ratio with even 0.11% accu-
ThiNet 
Table 3. Comparison of the pruned ResNet on ImageNet. “Init Pretrain” and ”acc. ↓” have the same meaning with Table 2.
racy improvement, while PFEC achieves 21.7% less
acceleration ratio with 0.61% accuracy drop.
The reason for our superior result is that our proposed
method adaptively selects suitable criteria for each functional layer based on their respective ﬁlter distribution. On
the contrary, none of previous works did this.
We notice that pruning from a scratch model sometimes
achieves a slightly better performance than pruning a pretrained model, which is consistent with . Note that we
achieve a higher acceleration ratio than on ResNet-110
with similar accuracy. We conjecture that the optimized
criteria might change the random initialization to “biased”
random initialization, which is beneﬁcial to the ﬁnal performance. This result is consistent with the conclusion of 
that a proper initialization is critical for the network.
Criteria Visualization. The learned pruning criteria for
ResNet-56 on CIFAR-10 is shown in Figure 4. The blue, orange and green denote pruning this layer with ℓ1-norm, ℓ2norm and geometric median, respectively. The pruned network achieve 93.54(±0.14)% accuracy with pruning 53.0%
FLOPs. In this ﬁgure, we ﬁnd that the GM-based criterion
is adopted more at higher layers, while the ℓp-norm-based
criteria are preferred at lower layers. An explanation is that
ﬁlters of higher layers tend to extract semantic information, and their activations are semantically related to each
other . Therefore, our LFPC chooses the relation-based
criteria instead of magnitude-based criteria when pruning
higher layers. 2
4.3. ResNet on CIFAR-100
The results of pruning ResNet-56 on CIFAR-100 is
shown in Tab. 4.
We only list a few methods as other
2GM is a relation-based criterion, while ℓp-norm is a magnitude-based
criterion. See supplementary material for different ﬁlter distribution.
Number of filters
Layer index
Before pruning
Pruned with L1 norm
Pruned with L2 norm
Pruned with GM
Figure 4. Visualization of the learned criteria and kept ﬁlters for
ResNet-56 on CIFAR-10. The grey strip indicates the layers before pruning. The blue, orange and green color denote ℓ1-norm,
ℓ2-norm and geometric median criteria, respectively. For example, the bottom green strip means that for all the 64 ﬁlters in 55th
layer, GM criterion is automatically selected to prune half of those
ﬁlters, base on the ﬁlter distribution on that layer.
methods have no experiment results on CIFAR-100. When
achieving a similar ratio of acceleration, our LFPC could
obtain much higher accuracies than the candidate algorithms and . This result again validates the effectiveness of our method.
Table 4. Comparison of the pruned ResNet-56 on CIFAR-100.
Number of filters
Layer index
Before pruning
Pruned with L1 norm
Pruned with L2 norm
Pruned with adversarial L1 norm
Pruned with adversarial L2 norm
Figure 5. Visualization of the conventional and adversarial criteria
for ResNet-56 on CIFAR-10. The grey strip indicates the layers
before pruning. Different blue and green colors represent different
pruning criteria.
4.4. ResNet on ILSVRC-2012
For the ILSVRC-2012 dataset, we test our method on
ResNet-50. Same as , we do not prune the projection
shortcuts. Tab. 3 shows that our LPFC outperforms existing methods on ILSVRC-2012. For the random initialized
ResNet-50, when our LFPC prunes 7.3% more FLOPs than
FPGM , the accuracy is even higher than FPGM .
For pruning the pre-trained ResNet-50, we achieve 92.04%
top-5 accuracy when we prune 60.8% FLOPs. While the
previous methods (CP , LFC , ELR ) have
lower top-5 accuracy when pruning less FLOPs (50%).
ThiNet also has a lower accuracy than our LFPC when
its acceleration ratio is lower than ours. The superior performance comes from that our method considers the different
ﬁlter distribution of different layers.
4.5. More Explorations
Adversarial Criteria.
To further validate the effeteness of our LFPC, we add the adversarial criteria, which
is the adversarial version of the current pruning criteria, to
our system. For example, conventional norm-based criteria keep the ﬁlters with large ℓp-norm.
In contrast, adversarial norm-based criteria keep the ﬁlters with small
ℓp-norm, which could be formulate as Crit(l)(W(l)
∥p for i ∈[1, C(l)
The learned criteria for ResNet-56 on CIFAR-10 are
shown in Fig. 5. In this experiment, we utilize four criteria,
including ℓ1-norm, ℓ2-norm, adversarial ℓ1-norm, adversarial ℓ2-norm. As shown in Tab. 5, for all the 55 criteria for
ResNet-56, the adversarial criteria only account for a small
proportion (16.4%). This means that our LFPC successfully
selects conventional criteria and circumvents the adversarial
criteria, which would be another evidence of the effectiveness of our LFPC.
Criteria During Training The learned criteria during
Adversarial
criteria (%)
Conventional
criteria(%)
93.09 (±0.09)
93.45 (±0.13)
Table 5. Analysis of adversarial criteria. “w Adv” and “w/o Adv”
denote containing the adversarial criteria or not, respectively.
Training Epochs
Layer index
Figure 6. The learned criteria during training the criteria sampler.
The L1, L2, and GM denote conventional ℓ1-norm, ℓ2-norm, and
geometric median criteria, respectively.
training DCS is shown in Fig. 6. A small strip of a speciﬁc
color means the layer of the network utilizes a corresponding pruning criterion at the current epoch. We ﬁnd that the
sampler gradually converges to a regular pattern of criteria,
which provides stable guidance for the next pruning step.
Retraining Scheduler. We compare the cosine scheduler and step scheduler during retraining. When
pruning 47.6% FLOPs of the ResNet-56, cosine scheduler
can achieve 93.56(±0.15)% accuracy, while step scheduler
can obtain 93.54(±0.16)% accuracy. It shows that LFPC
can achieve a slightly stable result with a cosine scheduler.
5. Conclusion and Future Work
In this paper, we propose a new learning ﬁlter pruning criteria (LFPC) framework for deep CNNs acceleration.
Different from the existing methods, LFPC explicitly considers the difference between layers and adaptively selects
a set of suitable criteria for different layers. To learn the
criteria effectively, we utilize Gumbel-softmax to make the
criteria sampler process differentiable. LFPC achieves comparable performance with state-of-the-art methods in several benchmarks. In the future, we could consider utilizing
more kinds of criteria into LFPC and combine it with other
acceleration algorithms, e.g., matrix decomposition , to
improve the performance further. Moreover, it is meaningful to adopt the proposed method to recent compact ConvNets such as MobileNets.