Deep Learning for Hyperspectral Image
Classiﬁcation: An Overview
Shutao Li, Fellow, IEEE, Weiwei Song, Student Member, IEEE, Leyuan Fang, Senior Member, IEEE, Yushi
Chen, Member, IEEE, Pedram Ghamisi, Senior Member, IEEE, and J´on Atli Benediktsson, Fellow, IEEE
Abstract—Hyperspectral image (HSI) classiﬁcation has become
a hot topic in the ﬁeld of remote sensing. In general, the complex
characteristics of hyperspectral data make the accurate classiﬁcation of such data challenging for traditional machine learning
methods. In addition, hyperspectral imaging often deals with an
inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep
learning has been recognized as a powerful feature-extraction
tool to effectively address nonlinear problems and widely used
in a number of image processing tasks. Motivated by those
successful applications, deep learning has also been introduced to
classify HSIs and demonstrated good performance. This survey
paper presents a systematic review of deep learning-based HSI
classiﬁcation literatures and compares several strategies for this
topic. Speciﬁcally, we ﬁrst summarize the main challenges of HSI
classiﬁcation which cannot be effectively overcome by traditional
machine learning methods, and also introduce the advantages
of deep learning to handle these problems. Then, we build a
framework which divides the corresponding works into spectralfeature networks, spatial-feature networks, and spectral-spatialfeature networks to systematically review the recent achievements
in deep learning-based HSI classiﬁcation. In addition, considering
the fact that available training samples in the remote sensing ﬁeld
are usually very limited and training deep networks require a
large number of samples, we include some strategies to improve
classiﬁcation performance, which can provide some guidelines for
future studies on this topic. Finally, several representative deep
learning-based classiﬁcation methods are conducted on real HSIs
in our experiments.
Index Terms—Hyperspctral image, deep learning, classiﬁcation, feature extraction.
I. INTRODUCTION
YPERSPECTRAL imaging is an important technique
in remote sensing, which collects the electromagnetic
spectrum from the visible to the near-infrared wavelength
This work was supported by the National Natural Science Fund of China
under Grant 61890962, 61520106001 and 61771192, the Science and Technology Plan Project Fund of Hunan Province under Grant CX2018B171,
2017RS3024 and 2018TP1013, the Science and Technology Talents Program
of Hunan Association for Science and Technology under Grant 2017TJ-Q09,
and the National Key RD Program of China under Grant 2018YFB1305200.
S. Li, W. Song, and L. Fang are with the College of Electrical and
Information Engineering, Hunan University, Changsha, 410082, China, and
also with the Key Laboratory of Visual Perception and Artiﬁcial Intelligence
of Hunan Province, Changsha, 410082, China (e-mail: shutao ;
weiwei ; ).
Y. Chen is with the Department of Information Engineering, School of
Electronics and Information Engineering, Harbin Institute of Technology,
Harbin 150001, China (e-mail: ).
P. Ghamisi is with the Helmholtz-Zentrum Dresden-Rossendorf (HZDR),
Helmholtz Institute Freiberg for Resource Technology (HIF), Exploration, D-
09599 Freiberg, Germany (e-mail: ).
J. A. Benediktsson is with the Faculty of Electrical and Computer Engineering, University of Iceland, 101 Reykjavk, Iceland (e-mail: ).
ranges. Hyperspectral imaging sensors often provide hundreds
of narrow spectral bands from the same area on the surface
of the earth. In hyperspectral images (HSIs), each pixel
can be regarded as a high-dimensional vector whose entries
correspond to the spectral reﬂectance in a speciﬁc wavelength.
With the advantage of distinguishing subtle spectral difference,
HSIs have been widely applied in many ﬁelds – .
Based on the recent studies published in , HSI classiﬁcation (i.e., assigning each pixel to one certain class based on its
spectral characteristics) is the most vibrant ﬁeld of research in
the hyperspectral community and has drawn broad attentions
in the remote sensing ﬁeld. In HSI classiﬁcation tasks, there
exist two main challenges: 1) the large spatial variability
of spectral signatures and 2) the limited available training
samples versus the high dimensionality of hyperspectral data.
The ﬁrst challenge is often brought by many factors such
as changes in illumination, environmental, atmospheric, and
temporal conditions. The second challenge will result in illposed problems for some methods and reduce the generalization ability of classiﬁers.
In the early stage of the study on HSI classiﬁcation, most
methods have focused on exploring the role of the spectral
signatures of HSIs for the purpose of classiﬁcation. Thus,
numerous pixel-wise classiﬁcation methods (e.g., neural networks , support vector machines (SVM) , multinomial
logistic regression , , and dynamic or random subspace , ) have been proposed to classify HSIs. In
addition, some other classiﬁcation approaches have focused
on designing an effective feature-extraction or dimensionreduction technique, such as principle component analysis
(PCA) , , independent component analysis (ICA) ,
and linear discriminant analysis (LDA) . However, the
classiﬁcation maps obtained by these pixel-wise classiﬁers are
unsatisfactory since the spatial contexts are not considered.
Recently, spatial features have been reported to be very useful
in improving the representation of hyperspectral data and
increasing the classiﬁcation accuracies , . More and
more spectral-spatial features-based classiﬁcation frameworks
have been developed, which incorporates the spatial contextual
information into pixel-wise classiﬁers. For example, in ,
 , extended morphological proﬁles (EMPs) were used to
exploit the spatial information via multiple morphological
operations. Multiple kernel learning (e.g., composite kernel
 and morphological kernel , ) was designed to
explore the spectral-spatial information of HSIs. In ,
edge-preserving ﬁltering was considered as a postprocessing
technique to optimize the probabilistic results of an SVM. In
 
 – , the spatial information within a neighboring region
was incorporated into a sparse representation model. These
sparse representation methods are based on the observation
that hyperspectral pixels can usually be represented by a linear
combination of a few common pixels from the same class.
Furthermore, spatial consistency was explored by segmenting
HSIs into multiple superpixels based on the similarity of either
intensity or texture – . Although most of spectralspatial-based HSI classiﬁcation methods have obtained good
performance, they heavily depend on hand-crafted or shallowbased descriptors. However, most hand-crafted features are
usually designed for a speciﬁc task and depend on expert
knowledge in the parameter setup phase, which limits the
applicability of those approaches in difﬁcult scenarios. The
ability of representation of hand-crafted features may not be
enough to discriminate subtle variation between the different
classes or a large variation between the same classes. Extracting more discriminative features is considered as a crucial
procedure for HSI classiﬁcation.
Recently, deep learning has become a growing trend in big
data analysis and great breakthrough has been made with
the approach in many computer vision tasks, e.g., image
classiﬁcation , , object detection , and natural
language processing . Motivated by those successful applications, deep learning has been introduced to classify HSIs
and achieved good performance. Fig. 1 demonstrates the
statistics for published papers related to deep learning-based
HSI classiﬁcation in the past ﬁve years according to the web
of science1. From this ﬁgure, we can conclude that the topic
will be further explored with deep learning and more and
more research works will be published in the next several
years. Compared with traditional hand-crafted methods, deep
learning techniques can extract informative features from the
original data via a series of hierarchical layers. Speciﬁcally,
earlier layers extract some simple features like texture and
edge information. Furthermore, the deeper layers are able to
represent more complicated features. The learning process is
totally automatic, which makes deep learning more suitable for
coping with the varieties of situations. In general, deep learning is recognized as an effective feature extraction approach
during HSI classiﬁcation while different networks focus on
extracting different feature types.
In this paper, we focus on deep learning-based classiﬁcation
methodologies for HSIs and aim at providing a relatively
general and comprehensive overview on the existing methods.
The motivation of our work is based on two aspects. On
one hand, this work is expected to clarify the mechanism
behind existing deep learning-based classiﬁcation methods. We
systematically review a large number of relevant papers in the
literature within the framework, where the papers are divided
into spectral-feature networks, spatial-feature networks, and
spectral-spatial-feature networks with respect to the types of
features extracted by the adopted deep networks. On the
other hand, we intend to include some strategies to handle
the problem of limited available samples (i.e., the second
challenge mentioned above), which is very important for
1 
The statistics for published papers related to deep learning in HSI
classiﬁcation according to the web of science.
Reconstruction
Fig. 2. A single hidden-layer auto-encoder. The model learns a hidden feature
“h” by minimizing the error between the input “x” and the reconstructed
output “y” .
designing a deep-learning method for HSI classiﬁcation.
The remainder of this paper is organized as follows: Section
II introduces several typical deep models. In Section III,
we categorize the previous works into three aspects with
respect to the types of features extracted by deep networks.
Some strategies to cope with the problem of limited available
samples are introduced in Section IV. In Section V, several
representative deep learning-based classiﬁcation methods are
compared on real HSIs. Finally, conclusions and suggestions
are provided in Section VI.
II. DEEP MODELS
In this section, we brieﬂy introduce several deep network
models that have been widely used in the HSI classiﬁcation
ﬁeld. These deep networks include stacked auto-encoders
(SAEs), deep belief networks (DBNs), convolutional neural
networks (CNNs), recurrent neural networks (RNNs), and
generative adversarial networks (GANs).
Auto-encoder (AE) is known as the main building block of
the stacked AE (SAE) . Fig. 2 shows an AE composing of
one visible layer of d inputs, one hidden layer of L units, and
one reconstruction layer of d units. The training procedure has
two steps. First, x ∈IRd is mapped to h ∈IRL in the hidden
Input Pixel
Hyperspectral
regression
Stacked Auto-Encoders
Class labels
Fig. 3. A general illustration of an SAE connected with a subsequent logistic
regression classiﬁer .
layer (i.e., “encoder”). Second, h is mapped to y ∈IRd (i.e.,
“decoder”). These two steps can also be deﬁned as follows:
h = f(whx + bh)
y = f(wyx + by)
where wh and wy denote the input-to-hidden and the hiddento-output weights, respectively. bh and by represent the bias
of the hidden and output units, and f(·) represents an activation function. The reconstruction error is estimated using the
Euclidean distance between x and y to approximate the input
data x by minimizing ∥x −y∥2
SAE can be constructed by stacking multiple layers of
AEs that attaches the output of one layer to the input of the
following layers. Fig. 3 illustrates a simple representation of an
SAE connected with a subsequent logistic regression classiﬁer.
The SAE can be used as a spectral classiﬁer where each pixel
vector can be considered as input.
Restricted Boltzmann machine (RBM) is a layer-wise training model and considered as the main building block of a
DBN . Fig. 4 illustrates an RBM, consisting of a twolayer network with “visible” units v = {0, 1}d and “hidden”
units h = {0, 1}L. Given these units, the energy of a joint
conﬁguration of the units can be deﬁned by:
E(v, h; θ) = −
= −bTv −aTh −vTwh
where θ = {bi, aj, wij}. Here, wij is the weight between
the visible unit i and hidden unit j. bi and aj represent the
bias of the visible and hidden units, respectively. The weight,
wij, is learned using the divergence . In RBM, the hidden
units are conditionally independent given the visible states,
and consequently, can obtain an unbiased sample from the
posterior distribution when given a data vector.
In order to increase the feature representation capability of
a single RBM, several RBMs can be stacked one after another
which establishes a DBN, which can be learned to extract a
deep hierarchical representation of the training data. Fig. 5
illustrates a DBN composing of several RBM layers. In order
Graphical illustration of a restricted Boltzmann machine. The top
layer represents the hidden units and the bottom layer represents the visible
units .
Input pixel
Hyperspectral
regression
Class labels
Deep Belief Network
Fig. 5. A spectral classiﬁer based on DBN. The classiﬁcation scheme shown
here has four layers: one input layer, two RBMs, and one logistic regression
layer .
to use DBN as a classiﬁer, one can add a logistic regression
layer at the end of the network to form a spectral classiﬁer.
CNNs are inspired by the structure of the visual system. In
contrast to fully connected networks, CNNs make use of local
connections to extract the contextual 2-D spatial features of
images. In addition, network parameters can be signiﬁcantly
reduced via the weight-share mechanism. A representative
structure of CNNs is shown in Fig. 6, which mainly consists
of a stack of alternating convolution layers and pooling layers
with a number of fully connected layers. In the convolutional
layers, the image patches with spatial context information are
convolved with a set of kernels. Then, the pooling layers
reduce the size of feature maps created by the convolutional
layers to obtain more general and abstract features. Finally,
these feature maps are further transformed into feature vectors
via several fully connected layers. Here, each component is
described as follows.
1) Convolutional Layers: In general, convolutional layers
are the most important parts of CNNs. At each convolutional
layer, the input cube is convoluted with multiple learnable ﬁlters, resulting in generating multiple feature maps. Speciﬁcally,
let X be the input cube and its size is m×n×d, where m×n
refers to the spatial size of X, d is the number of channels,
and xi is the i-th feature map of X. Supposing there are k
ﬁlters at this convolutional layer and the j-th ﬁlter can be
characterized by the weight wj and bias bj. The j-th output
of the convolutional layer can be represented as follows:
f(xi ∗wj + bj), j = 1, 2, ..., k
Fig. 6. The architecture of a conventional CNN, which consists of two convolutional-pooling layers and two fully connected layers.
where ∗refers to the convolution operator and f(·) is an activation function which is utilized to improve the nonlinearity
of the network. Recently, ReLU has been the mostly-used
activation function. ReLU mainly has two advantages: a fast
convergence and robustness for gradient vanishing . The
formulation of ReLU is denoted as:
σ(x) = max(0, x).
2) Pooling Layers: Due to the existence of redundant
information in images, the pooling layers are periodically
inserted after several convolutional layers in the CNNs. With
the pooling operation, the spatial size of the feature maps
is progressively reduced, at the same time, the amount of
parameters and computation of the network are also decreased.
Through the pooling operations, the sizes of feature maps tend
to shrink, and the representation of extracted features becomes
more abstract. Speciﬁcally, for a p × p window-size neighbor
denoted as S, the average pooling operation can be denoted
where F is the number of elements in S and xij is the
activation value corresponding to the position (i, j).
3) Fully Connected Layers: After the pooling layers, the
feature maps of previous layer are ﬂattened and fed to fully
connected layers. In a traditional neural network, the fully
connected layers are used to extract more deep and abstract
features by reshaping feature maps into an n-dimension vector
(e.g., n=4096 in AlexNet ). In general, the fully connected
layer can be deﬁned as:
f(WX′ + b)
where X′, Y′, W, and b refer to the input, output, weight,
and bias of a fully connected layer, respectively.
RNNs were originally introduced in , . In contrast to
a feedforward neural network, an RNN can recognize patterns
in sequences of data and dynamic temporal characteristics by
using a recurrent hidden state whose activation at each step
depends on that of the previous steps.
Let x = (x1, x2, · · · , xT ) be sequential data, where xi is
the data at the i-th time step. The recurrent hidden state h<t>
of the RNN at temporal sequence t can be updated by
f1(h<t−1>, x<t>)
where f1 is a nonlinear function (e.g., a logistic sigmoid
function or hyperbolic tangent function). Conventionally, the
update rule of the recurrent hidden state in (6) has been
obtained by:
h<t> = f1(wx<t> + uh<t−1> + bh)
where w and u represent the coefﬁcient matrices for the input
at the current step and for the activation of recurrent hidden
units at the previous step, respectively, and bh shows the
corresponding bias vector. Then, h<t> will be used to predict
y<t> at time step t as follows:
y<t> = f2(ph<t> + by)
where f2 is a nonlinear function, p is the coefﬁcient matrix for
the activation of recurrent hidden units at the current step, and
by is the corresponding bias vector. As can be noticed, one
can use different nonlinearities, f1 and f2, to estimate h<t>
The performance of the conventional RNN can be downgraded when it deals with long-term sequential data due to
the vanishing gradient or exploding gradient. To address this
issue, long short-term memory (LSTM) , and gated
recurrent unit (GRU) were introduced.
As discussed above, RNNs are able to recognize patterns in
sequences of data. In this context, RNNs are well-suited for
the analysis of HSI since each pixel vector in hyperspectral
data can be considered as a set of orderly and continuing
spectra sequences in the spectral space. In , the concept
of RNN was adopted for pixel-wise spectral classiﬁcation by
developing a new activation function and a modiﬁed gated
recurrent unit, which can effectively analyze hyperspectral
pixels as sequential data and then assign a classiﬁcation label
via network reasoning.
In general, there are two typical classes of models in the
machine learning community: generative approaches and discriminate approaches. Generative approaches try to learn the
distribution parameters from data, and then they can generate
new samples according to the learned models. Discriminate
approaches attempt to model the dependence of labels (i.e.,
Fig. 7. A general illustration of a GAN .
y) on training data (i.e., x), which can be used for predicting
GANs are the relatively new kind of models, which contains
a generative model G and a discriminative model D .
Models G and D are trained in an adversarial manner.
Model G tries to generate fake inputs as real as possible,
whereas model D tries to distinguish between real and fake
inputs. Through the adversarial manner and competition of two
models, the training process of the discriminator will proceed
both continuously and effectively. The architecture of a GAN
is shown in Fig. 7.
The generator G accepts random noise z as input and
produces fake data G(z). The discriminator D estimates the
probability that x is a true sample. The discriminator D is
trained to maximize log(D(x)), which is the probability of
assigning the right labels to the training samples; the generator
G is trained to minimize log(1−D(G(z)). So, the aim of the
GAN is to solve the following minimax problem
V (D, G) = Ex∼p(x) [log(D(x))]
+Ez∼p(z) [log(1 −D(G(z)))]
where E is the expectation operator. Due to the advantages of
CNN, the deep convolutional generative adversarial network
architecture, which uses deep convolution networks in G and
D, was proposed in . Although GAN is a promising
technique, the original discriminator D only estimates whether
the input sample is either real or fake. Therefore, it is not
suitable for multi-class classiﬁcation. In , Odena at al.
proposed an auxiliary classiﬁer GAN, which can be used for
classiﬁcation, in which D was modiﬁed to a softmax classiﬁer
that can output multi-class labels probabilities .
A proposed framework of GAN-based HSI classiﬁcation is
shown in Fig. 8. From Fig. 8, we can see that the generator
G also accepts HSI class labels c in addition to the noise
z and the output of G can be deﬁned by xfake = G(z).
The training samples with corresponding class labels and the
fake data generated by G are regarded as the input of the
discriminator D. The objective function contains two parts:
the log-likelihood of the right source of input data Ls and the
log-likelihood of the right class labels Lc:
Ls = E [log P(s = real|xreal)]
+E [log P(s = fake|xfake)]
Lc = E [log P(c = real|creal)]
+E [log P(c = fake|cfake)] .
Therefore, D is optimized to maximize the Ls + Lc while G
is optimized to maximize the Lc −Ls.
III. DEEP NETWORKS-BASED HSI CLASSIFICATION
Recently, deep learning has become one of the most successful techniques and achieved impressive performance in the
computer vision ﬁeld . Motivated by those great breakthroughs, deep learning has also been introduced to classify
HSIs in the remote sensing ﬁeld – . Compared with
traditional hand-crafted features-based methods, deep learning
can automatically learn high-level features from complex
hyperspectral data. With these discriminative features, deep
learning-based methods can effectively deal with the ﬁrst
problem mentioned above in Section I (i.e., the large spatial
variability of spectral signature). Based on the fact, a large
number of deep networks have been developed to extract
features of HSIs and achieved good classiﬁcation performance.
However, the types of features extracted from deep networks
may be different, e.g., spectral, spatial, and spectral-spatial
features can be extracted by different deep networks. In this
section, we systematically review the deep learning-based
methods for HSI classiﬁcation in a framework. In such framework, the deep networks used for HSI classiﬁcation are divided
into spectral-feature networks, spatial-feature networks, and
spectral-spatial-feature networks. These different networks are
expected to extract corresponding features for the subsequent
classiﬁcation. The following subsections will introduce the
three kinds of networks in details.
A. Spectral-Feature Networks
Spectral information is the most important characteristic of
HSIs and plays the vital role for the classiﬁcation tasks .
However, the hyperspecrtral remote sensors usually provide
hundreds of spectral bands which also contain redundant
information. Therefore, the direct exploration of original spectral vectors not only results in high computational cost but
also decreases the classiﬁcation performance. Although some
traditional spectral feature extraction methods (e.g., PCA ,
 , ICA , and LDA ) can extract effective spectral
features, the simple linear processing existed in these linear
models is hard to handle the complex spectral property of
HSIs. In this section, we introduce the deep learning-based
framework, named spectral-feature networks, to extract deep
spectral features.
In early research attempts, the hyperspectral pixel vector
was intuitively fed into fully connected networks due to the
requirement of the vector-based input in the network layers.
Speciﬁcally, the original spectral vector was directly used to
train the SAE or DBN in an unsupervised way , .
Subsequently, some improved methods were also developed
for HSI classiﬁcation. For example, Liu et al. proposed an
effective classiﬁcation framework based on deep learning and
active learning, where DBN was used to extract deep spectral
features and an active learning algorithm was employed to
Fig. 8. Illustration of the GAN-based HSI classiﬁcation approach developed in .
Fig. 9. Paradigms of spectral-spatial-feature networks, which can be further divided into three categories based on the stage of feature fusion, i.e., preprocessingbased networks, integrated networks, and postprocessing based networks.
iteratively select good-quality labeled samples as training
samples. At the same time, Zhong et al. developed an
improved DBN model, named diversiﬁed DBN, to regularize
the pretraining and ﬁne-tuning procedures of DBN, which
signiﬁcantly improved the performance of DBN in terms of
classiﬁcation accuracies. In addition, 1-D CNN – ,
1-D GAN , , and RNN , , were also
used to extract spectral features for HSI classiﬁcation. In
 , Li et al. used pixel-pair features extracted by CNN to
explore correlation between hyperpsectral pixels, where the
convolution operation was mainly executed in the spectral
domain. Furthermore, in , , the training of a deep
network with the dictionary learning was reformulated. In
such works, the multiple levels of dictionaries were learned
in a robust fashion. This novel perspective obtained a better
classiﬁcation result than general deep networks.
B. Spatial-Feature Networks
Previous researches on HSI classiﬁcation have proven that
classiﬁcation accuracies can be further improved by incorporating spatial features into classiﬁers , . In this
section, we will discuss the spatial-feature networks that
exploit deep networks to extract spatial features of HSIs. For
accurate HSI classiﬁcation, the learned spatial features are
subsequently fused with spectral features extracted by other
feature extraction techniques.
In , , – , PCA was ﬁrst performed on the
whole hyperspectral data to reduce the dimensionality of the
original space, and then, the spatial information contained
in the neighborhood region of the input hyperspectral pixel
was exploited by a 2-D CNN. The above methods combined
the PCA and CNN, which not only extracted the discriminative spatial features but also reduced the computational
cost. More advanced, Liang et al. introduced a sparse
representation technique to encode the deep spatial features
extracted by a CNN into low-dimensional sparse features,
which improved the ability of feature representation and the
ﬁnal classiﬁcation accuracies. Chen et al. adopted the offthe-shell CNNs (e.g., AlexNet and GoogLeNet ) to
extracts deep spatial feature . In addition, Zhao et al.
 proposed a spectral-spatial feature-based classiﬁcation
(SSFC) framework for HSI classiﬁcation. In that framework,
the spectral and spatial features were extracted by the balanced
local discriminant embedding (BLDE) and a CNN, respectively. Then, the spectral and spatial features were fused to
train a multiple-feature-based classiﬁer. Instead of extracting
spatial-spectral information by incorporating the pixels within
a small spatial neighborhood into a classiﬁer, a novel HSI
classiﬁcation framework, named the deep multiscale spatialspectral feature extraction algorithm, was proposed in .
In more detail, the well pretrained FCN-8 was ﬁrst
used to explore deep multiscale spatial structural information.
Then, the weighted fusion mechanism was adopted to fuse the
original spectral features and deep multiscale spatial features.
Finally, the fused features were fed into a classiﬁer to perform
the classiﬁcation operation.
C. Spectral-Spatial-Feature Networks
Instead of designing a deep network to extract either
spectral features or spatial features, this kind of networks
(i.e., spectral-spatial-feature networks) can extract joint deep
spectral-spatial features for HSI classiﬁcation. The joint deep
spectral-spatial features are mainly obtained by the following
three ways: 1) mapping the low-level spectral-spatial features to high-level spectral-spatial features via deep networks;
2) directly extracting deep features from original data or
several principal components of the original data; 3) fusing two separate deep features (i.e., deep spectral features
and deep spatial features). Based on this observation, the
spectral-spatial-feature networks can be further divided into
three categories: preprocessing-based networks, integrated networks, and postprocessing-based networks. Fig. 9 depicts
the paradigm of three kinds of networks, which is actually
divided in terms of the processing stage, where the spectral
information and the spatial information are fused. Here, the
three networks are discussed in the following parts.
1) Preprocessing-based networks:
In this category, the
spectral-spatial features are fused before feeding them into the
subsequent deep network. In general, the whole classiﬁcation
process can be divided into three phases: 1) low-level spectralspatial feature fusion; 2) high-level spectral-spatial feature
extraction using deep networks; 3) joint deep spectral-spatial
feature-based classiﬁcation with simple classiﬁers (e.g., SVM,
ELM, or multinomial logistic regression ). As mentioned
above, the fully connected networks (e.g., DBN, SAE, and
their variants) only deal with 1-D inputs. To extract joint
spectral-spatial features, a common idea is to ﬂatten the spatial
neighboring region into a 1-D vector, and then, the obtained
spatial vector and the original spectral vector are stacked and
fed into fully connected networks. For those works, readers can
refer to , , , . In – , a new spectral vector was ﬁrst computed by averaging all spectral pixels within
a spatial neighboring. Finally, the averaged spectral vector
which actually includes the spatial contextual information was
processed by the following deep network. Furthermore, instead
of directly exploiting the spatial information within a neighboring window, some different ﬁltering methods (e.g., Gobar
ﬁltering , , attribute ﬁltering , extinction ﬁltering
 , and rolling guidance ﬁltering ) were introduced to
process the original hyperspectral data aiming to extract more
effective spatial features. These ﬁlter-based works combine the
deep learning techniques with other spatial-feature extraction
methods and deliver more accurate classiﬁcation results.
2) Integrated networks: Instead of separately acquiring
spectral and spatial features and then processing them together,
the joint deep spectral-spatial features were directly extracted
by the original data via 2-D CNN , – . Actually, the
hyperspectral data can be typically represented in the format
of a 3-D cube. Thus, 3-D convolution in spectral and spatial
dimensions can naturally offer a more effective method for
simultaneously extracting the spectral-spatial features within
such images. Based on this fact, 3-D CNN was used to
effectively extract deep spectral-spatial-combined features for
accurate HSI classiﬁcation without relying on any preprocessing or postprocessing techniques – , – . Apart
from CNN, some powerful deep models were used to classify
HSIs. Speciﬁcally, the fully convolutional network (FCN) ,
a very successful network in the semantic segmentation ﬁeld,
was utilized to reconstruct hyperspectral data, which can learn
the deep features of HSIs in a supervised way or an unsupervised way . Besides, references , , – 
introduced residual learning to build very deep and wide
networks with the purpose of extracting more discriminative
features for HSI classiﬁcation. Fig. 10 shows a residual block,
where the output is the sum of input and convoluted value
of input. In , , a 3-D GAN was utilized as a
spectral-spatial classiﬁer. In the GAN-based HSI classiﬁcation
framework, a CNN was ﬁrst designed to discriminate the
inputs, which is called the discriminative model. Then, another
CNN was used to generate the so-called fake inputs as the
generative model. Paoletti et al. reﬁned the CapsNets as
the spectral-spatial capsules to extract spectral-spatial features
of HSIs . Furthermore, researchers were also focused
on building hybrid deep networks to achieve the accurate
classiﬁcation of HSIs, which can make full use of different
deep models. For instance, Kemker et al. developed
an unsupervised feature extraction framework to learn generalizable features from unlabeled hyperspectral pixels via a
three-layer stacked convolutional autoencoder. Wu et al. 
proposed a novel deep convolutional recurrent neural networks
(CRNN), taking advantages of CNN and RNN models, to
classify HSIs by using pseudo labels. In such a network, the
convolutional layers were used to extract middle-level locally
invariant features from the input hyperspectral sequence, and
the recurrent layers can extract contextual information from
the middle-level feature sequences generated by the previous
convolutional layers. In addition, a spectral-spatial cascaded
RNN model was proposed to extract spectral-spatial features
for HSI classiﬁcation. .
Fig. 10. Illustration of a residual block.
3) Postprocessing-based networks: In this category, the
whole classiﬁcation procedure includes the following steps: 1)
deep spectral features and deep spatial features are obtained
via two deep networks; 2) the two kinds of features are fused
in a fully connected layer to generate the joint deep spectralspatial features; 3) joint deep spectral-spatial feature-based
HSI classiﬁcation with the subsequent classiﬁer (e.g., SVM,
ELM, or multinomial logistic regression ). It is worth
mentioning that the two deep networks used to separately
extract deep spectral and spatial features may share the same
weights or may be totaly different. For instance, in ,
 , a deep CNN with two-branch architecture was proposed
to extract the joint spectral-spatial features from HSIs. In
such works, a 1-D CNN branch and a 2-D CNN branch
were used to extract spectral features and spatial features,
respectively. After that, the learned spectral features and spatial
features were concatenated and fed to fully connected layers
to extract joint spectral-spatial features for classiﬁcation. In
 , Hao et al. developed a novel two-stream architecture
for HSI classiﬁcation, where one stream employed a stacked
denoising autoencoder (SDAE) to encode the spectral values of
each input pixel, and the other stream exploited a deep CNN to
learn spatial features by processing the corresponding image
patch. Finally, the prediction probabilities from two streams
were fused by adaptive class-speciﬁc weights, which can be
obtained by a fully connected layer. In addition, references
 , used two similar network architectures (e.g.,
two principal component analysis-based networks , two
stacked sparse AE ) to extract the spectral features and
spatial features, respectively. Subsequently, these two kinds of
features were fused in a fully connected layer and an SVM
was further trained for classiﬁcation. Furthermore, Santara et
al. adopted multiple CNNs to simultaneously process
multiple spectral sets, where multiple CNNs shared the same
network parameters. In fact, each CNN aimed to extract the
corresponding spatial feature of several neighboring spectral
bands, and the spectral-spatial features can be obtained by
fusing these individual spatial features in a fully connected
IV. STRATEGIES FOR LIMITED AVAILABLE SAMPLES
As a matter of fact, training a deep network requires a large
number of training samples to learn the network parameters.
However, in the remote sensing ﬁeld, there is usually only
a small amount of labeled data available since the collection
of such labeled data is either expensive or time-demanding.
This issue, also known as the imbalance between lots of
weights and limited availability of training samples, may result
in poor classiﬁcation performance. Recently, some effective
methods have been proposed to cope with the problem to some
extent. In this section, we include some strategies to improve
deep learning-based HSI classiﬁcation under the condition of
limited available samples.
A. Data Augmentation
Data augmentation is considered as an intuitional way to
effectively solve the above problem. It tries to create new
training samples from given known samples. By investigating
the literature, we pointed out two main strategies to generate
the additional virtual samples: 1) transformation-based sample
generation; 2) mixture-based sample generation. In the following part, we will discuss the two methods in details.
1) Transformation-based sample generation: Due to the
complex situation of lighting in HSIs, objects of the same class
in different locations may be affected by different radiations.
Based on this fact, the virtual samples can be generated
by transforming the current known samples. This method of
augmenting data is widely used in , , , , .
Speciﬁcally, let xi be a known training sample, the new virtual
sample y can be obtained by
y = f(xi) + γn
where f is a transforming function (e.g., performing the rotating, ﬂipping, or mirroring operation), γ controls the weight
of the random Gaussian noise n, which may be produced via
the interaction of neighboring pixels or imaging error. Finally,
the newly generated virtual sample y has the same class with
xi and can be used to train a deep network.
2) Mixture-based sample generation: In general, the objects of the same class usually show similar spectral characteristics in a certain range. This phenomenon makes it possible
to generate a virtual sample from two given samples of the
same class. In , , the virtual sample y is the linear
combination of two training samples xi and xj, which can be
represented by the following formulation
y = αijxi + (1 −αij)xj
where αij represents the afﬁnity between two training samples
(i.e., xi and xj) coming from the same class. αij is usually
deﬁned as follows:
αij = exp(−∥xi −xj∥2 /2σ2).
Although there are many other techniques to generate the
virtual samples, the above two methods are simple yet effective
B. Transfer Learning
Transfer learning is regarded as a set of techniques
that introduces the useful information learned from the source
data to the target data, which can signiﬁcantly decrease the
demand on training samples. In recent years, transfer learning
has been successfully applied in many ﬁelds, especially in
the remote sensing ﬁeld , where the available
Fig. 11. The general framework of transfer learning based deep network training.
Fig. 12. Illustration of unsupervised/semisupervised feature learning associated with deep learning. The top part represents the unsupervised deep feature
learning, and the bottom part illustrates the semisupervised way for feature
training samples are not sufﬁcient. In , , ,
 , , transfer learning was employed to set the initial
values of network parameters copied from other trained deep
networks, which provides better classiﬁcation performance
compared with the random initialization-based methods. Fig.
11 shows the general framework of transfer learning for deep
networks. Considering the fact that the low and middle layers
usually capture generic features of input images which has
high generalization for other images, it is feasible to directly
transfer the network parameters of low and middle layers to a
new network which has the same architecture as the previous
network. Remarkably, the parameters of top layers are still
initialized in a random way in order to deal with a speciﬁc task.
Once the network parameters are transferred, the subsequent
classiﬁcation can be further divided into unsupervised and
supervised methods. The former directly uses the deep features
extracted from the transferred network to train the classiﬁer.
For the latter method, the network is further ﬁne-tuned by
using a small amount of training samples of the target data.
In summary, by making full use of existing data sets, transfer
learning can effectively solve the problem of degradation of
network performance when the available training samples are
C. Unsupervised/Semi-supervised Feature Learning
Though supervised feature learning has gained great breakthrough in the HSI classiﬁcation ﬁeld, there is still an urgent
need to learn HSI features in an unsupervised or semisupervised way. The main purpose of unsupervised/semisupervised
feature learning is to extract useful features from an amount
of unlabeled data. Recently, more and more research works
 , , , , , , focus on designing
a robust and effective unsupervised/semisupervised feature
learning framework based on deep learning to classify HSIs.
Fig. 12 illustrates unsupervised/semisupervised deep feature
learning. From this ﬁgure, the top ﬂowchart only uses unlabeled data to extract the informative features of HSIs, which is
actually regarded as an unsupervised feature learning way. The
deep network is elaborately designed as the encoder-decoder
paradigm to learn networks without using label information.
Furthermore, the classiﬁcation performance can be improved
via transferring the trained network and ﬁne-tuning on the
labeled data set, which is shown at the bottom of the ﬂowchart
in Fig. 12.
For example, research works , , , , ,
 , , utilized a fully connected network to
classify HSIs, where the training of the network was divided
into pre-training in an unsupervised manner and ﬁne-tuning
with labeled data. In the pre-training stage, the sample from
unlabeled data set was ﬁrst mapped into an intermediate
feature. Then, the intermediate feature was subsequently reconstructed via a decoding operation. At last, the parameters
of each layer can be learned by minimizing the error between
original sample and reconstructed sample. However, the above
greedy layer-wise training fashion may not be efﬁcient when
the network is deep. Move advanced, an unsupervised endto-end training framework was proposed in , where the
convolutional and deconvolutional networks were regarded as
encoder and decoder, respectively. In addition, the GAN 
was also adopted to construct semisupervised feature learning
framework for HSI classiﬁcation , . In such works, the
generator created fake hyperspectral samples that were similar
to the real data to train the GAN. Besides, Wu et al. used
abundant unlabeled data with their pseudo labels obtained by
a non-parametric Bayesian clustering algorithm to pre-train
a CRNN for HSI classiﬁcation . The semisupervised
learning mechanism can extract more representative spectralspatial features and provide satisfactory classiﬁcation result.
Fig. 13. The Houston data set. (a) The three-band false color composite. (b)
Ground reference data and color code.
D. Network Optimization
The main purpose of network optimization is to further
improve the network performance through adopting more
efﬁcient modules or functions. For instance, the current mainstream deep networks use ReLU as nonlinear activation function followed by the batch normalization operation
 , which can effectively alleviate the overﬁtting during
the training phase. In addition, research works , ,
 , , introduce residual learning to build
a very deep network with the purpose of extracting more
discriminative features for HSI classiﬁcation. By optimizing
several convolutional layers as the identity mapping, residual
learning can ease the training process. Fig. 10 shows the
mechanism of residual learning. Let X be the input of the
ﬁrst layer and F(X) be the original underlying function to
be learned by two stacked convolutional layers. The residual
learning actually attempts to optimize the two convolutional
layers as identity mapping, which is achieved by using skip
connections (the red line in Fig. 10). By introducing residual
function G(X) = F(X) −X, the objective function from
the original F(X) = X equivalently converts to G(X) = 0.
As described above, F(X) can be written by the following
formulation:
F(X) = G(X) + X.
As shown in Fig. 10, G(X) can be computed by two convolutions with X:
G(X) = f(f(X ∗W1 + b1) ∗W2 + b2)
where W1 and W2 are convolutional kernels, b1 and b2
are trainable bias parameters, and f refers to the ReLU 
function. By stacking multiple residual blocks, the extracted
features become more and more discriminative and the accuracy is gained from considerably increased depth. More details
about theoretical and experimental proofs of the DRN can refer
Besides, other network optimization strategies are also exploited to boost the representation ability of features extracted
by deep networks. For example, Zhong et al. improved
the performance of DBN via regularizing the pre-training and
The University of Pavia data set. (a) The three-band false color
composite. (b) Ground reference data and color code.
Fig. 15. The Salinas data set. (a) The three-band false color composite. (b)
Ground reference data and color code.
ﬁne-tuning procedure, which delivers a better classiﬁcation accuracy than usual DBN models. In , the label consistency
constraint was enforced into the training procedure of SAE.
Moreover, the correlation between samples was considered in
networks investigated in , .
In general, network optimization is still a challenging problem. In the future works, the unique characteristics of HSI
should be considered as much as possible when designing an
improved network for HSI classiﬁcation.
V. EXPERIMENTS
In this section, we mainly conduct a comprehensive set of
experiments from four aspects. Firstly, a series of experiments
are designed to demonstrate the advantages of deep learning on
HSI classiﬁcation over traditional methods. Secondly, the classiﬁcation performance of several recent state-of-the-art deep
learning approaches is systematacially compared. Thirdly, we
visualize the learned deep features and network weights to
further explore the “black box”. Finally, the effectiveness
of strategies included in Section IV is further analyzed. To
complete our experiments, three benchmark HSIs are used,
i.e., the Houston, University of Pavia, and Salinas images. The
three images are introduced in the following subsection.
(e) 3D-CNN
(f) CNN-PPF
(g) Gabor-CNN
(i) 3D-GAN
Fig. 16. Classiﬁcation maps for the Houston data set obtained by (a) SVM
 , (b) EMP , (c) JSR , (d) EPF , (e) 3D-CNN , (f) CNN-PPF
 , (g) Gabor-CNN , (h) S-CNN , (i) 3D-GAN , and (j) DFFN
NUMBER OF TRAINING AND TEST SAMPLES USED FOR THE HOUSTON
Class name
Healthy grass
Stressed grass
Synthetic grass
Residential
Commercial
Parking Lot 1
Parking Lot 2
Tennis Court
Running Track
NUMBER OF TRAINING AND TEST SAMPLES USED FOR THE UNIVERSITY
OF PAVIA DATA SET
Class name
Painted metal sheets
Self-Blocking Bricks
NUMBER OF TRAINING AND TEST SAMPLES USED FOR THE SALINAS
Class name
Brocoli green weeds 1
Brocoli green weeds 2
Fallow rough plow
Fallow smooth
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
A. Experimental Data Sets
The Houston data was distributed for the 2013 IEEE
Geoscience and Remote Sensing Society (GRSS) data fusion
contest. This scene was captured in 2012 by an airborne
sensor over the area of University of Houston campus and
the neighboring urban area. The size of the data is 349×1905
pixels with a spatial resolution of 2.5 m. This HSI consists of
144 spectral bands with wavelength ranging from 0.38 to 1.05
µm and includes 15 classes. Fig. 13 shows the false color composite of the Houston data set and the corresponding ground
reference data, respectively. Table I shows the information on
the number of training and test samples for the different classes
of interests.
The University of Pavia data set, which captures an urban
area surrounding the University of Pavia, Italy, was collected
by the ROSIS-03 sensor in Northern Italy in 2001. This scene
is of size 610 × 340 × 115 with a spatial resolution of 1.3 m
per pixel and spectral coverage ranging from 0.43 to 0.86 µm.
This image includes 9 classes of interest and has 103 spectral
bands after removing 12 very noisy bands. Fig. 14 shows the
false color composite of the University of Pavia image and
the corresponding ground reference map, respectively. Table II
shows information on the number of training and test samples
for the different classes of interests.
The Salinas data set was captured by the AVIRIS sensor
over Salinas Valley, California. This image comprises of
512 × 217 pixels with a spatial resolution of 3.7 m and 204
bands after removing 20 water absorption bands. The available
ground reference map covers 16 classes of interest. The
false color composite of Salinas image and the corresponding
ground reference data are shown in Fig. 15. Table III shows
information on the number of training and test samples for the
different classes of interests.
B. Compared Methods
In this review, we have investigated several recent stateof-the-art deep learning-based approaches, including 3D-CNN
 , Gabor-CNN , CNN with pixel-pair features (CNN-
PPF) , siamese CNN (S-CNN) , 3D-GAN , and
the deep feature fusion network (DFFN) , for HSI classi-
ﬁcation. Speciﬁcally, the 3D-CNN exploits 3-D convolutional
ﬁlters to directly extract spectral-spatial features from the
original hyperspectral cube. However, the network architecture
adopted in 3D-CNN is relatively simple and the correlation
between different layers is neglected. The DFFN adopts the
DRN , which can be considered a more powerful network, to extract more discriminative features. In addition, the
features from different-level layers are further fused to explore
the correlation between layers. The main drawback of DFFN
is that the optimal feature fusion mechanism depends on a
hand-crafted setting with abundant experiments. In the Gabor-
CNN, the Gabor ﬁltering is ﬁrst utilized as a preprocessing
technique to extract spatial features of HSIs. Then, the ﬁltered
features are fed into a simple CNN-based classiﬁer. Instead
of considering a pixel-wised semantic information, CNN-
PPF and S-CNN focus on exploring the correlation between
samples. In more detail, CNN-PPF extracts the pixel-pair
features via CNN. However, the convolutional operation is
mainly conducted in the spectral domain, and thus, the spatial
information is not considered for CNN-PPF. By contrast, S-
CNN adopts a two-branch CNN to simultaneously extract
spectral-spatial features of HSIs. But, in such a method, the
computational cost may be huge due to the high dimension
vector in the Euclidean space. In addition, 3D-GAN utilizes
adversarial training to improve the generalization capability of
the discriminative CNN, which is very useful when the number
of training samples is limited.
Apart from the above deep learning-based methods, some
classical classiﬁcation techniques, including the SVM ,
EMP , joint spare representation (JSR) , and edgepreserving ﬁltering (EPF) are also considered for comparison. As the spectral feature-based method, the SVM was
implemented in the library for support vector machines (LIB-
SVM) library , where the Gaussian kernel with ﬁvefold
cross validation was adopted for this classiﬁer. The other
three methods are all spectral-spatial feature-based methods.
Speciﬁcally, for the EMP, the morphological proﬁles were
constructed with the ﬁrst three principal components. Furthermore, a circular structural element, a step size increment
of two, and four openings and closings were performed for
each principal component. For the JSR, the spatial information
within a ﬁxed-size local region was utilized by the joint sparse
regularization. The EPF method was implemented by using the
code which is available on Dr. Xudong Kang’s homepage2.
Among these investigated methods, the SVM and CNN-PPF
have only utilized spectral features during the classiﬁcation.
In contrast, the rest of methods, including the EMP, JSR,
EPF, 3D-CNN, Gabor-CNN, S-CNN, 3D-GAN, and DFFN,
are belong to classiﬁcation methods based on spectral-spatial
C. Classiﬁcation Results
The ﬁrst experiment was performed on the Houston data set.
In this experiment, the training samples were given according
to the 2013 GRSS data fusion contest. The amount of training
and test samples per class is shown in Table I. Fig. 16 shows
the classiﬁcation maps obtained by different methods. From
this ﬁgure, we can see that the classiﬁcation maps obtained
by the SVM and JSR methods are not very satisfactory since
some noisy estimations are still visible. By contrast, other
methods perform much better in removing “noisy pixels” and
deliver a smoother appearance in their classiﬁcation results. By
comparing two ﬁltering-based methods, i.e., EPF and Gabor-
CNN, we can see that the classiﬁcation map of EPF seems
to be over-smoothing, but the Gabor-CNN preserves more
details in edges. Apart from visual comparison, Table IV
gives quantitative results of various methods on the image,
where three metrics, i.e., overall accuracy (OA), average
accuracy (AA), and Kappa coefﬁcient, are adopted to evaluate
the classiﬁcation performance. All the classiﬁcation accuracy
values reported in Table IV are the average results over
ten experiments. As can be seen, except for 3D-GAN, the
deep learning-based methods obtain satisfactory classiﬁcation
2 
THE CLASSIFICATION ACCURACIES (IN PERCENTAGES) OBTAINED BY SVM , EMP , EPF , JSR , 3D-CNN , GABOR-CNN ,
CNN-PPF , S-CNN , 3D-GAN , AND DFFN ON THE HOUSTON IMAGE. THE BEST ACCURACIES ARE MARKED IN BOLD.
(e) 3D-CNN
(f) CNN-PPF
(g) Gabor-CNN
(i) 3D-GAN
Fig. 17. Classiﬁcation maps for the University of Pavia data set obtained by (a) SVM , (b) EMP , (c) JSR , (d) EPF , (e) 3D-CNN , (f)
CNN-PPF , (g) Gabor-CNN , (h) S-CNN , (i) 3D-GAN , and (j) DFFN .
(e) 3D-CNN
(f) CNN-PPF
(g) Gabor-CNN
(i) 3D-GAN
Fig. 18. Classiﬁcation maps for the Salinas data set obtained by (a) SVM , (b) EMP , (c) JSR , (d) EPF , (e) 3D-CNN , (f) CNN-PPF
 , (g) Gabor-CNN , (h) S-CNN , (i) 3D-GAN , and (j) DFFN .
THE CLASSIFICATION ACCURACIES (IN PERCENTAGES) OBTAINED BY SVM , EMP , EPF , JSR , 3D-CNN , GABOR-CNN ,
CNN-PPF , S-CNN , 3D-GAN , AND DFFN ON THE UNIVERSITY OF PAVIA IMAGE. THE BEST ACCURACIES ARE MARKED IN BOLD.
THE CLASSIFICATION ACCURACIES (IN PERCENTAGES) OBTAINED BY SVM , EMP , EPF , JSR , 3D-CNN , GABOR-CNN ,
CNN-PPF , S-CNN , 3D-GAN , AND DFFN ON THE SALINAS IMAGE. THE BEST ACCURACIES ARE MARKED IN BOLD.
accuracies (e.g., all OAs are above 80%), which is overall
higher than traditional methods, including the SVM, EMP, JSR
and EPF. Among these deep learning-based methods, CNN-
PPF which only considers spectral information of HSIs obtains
the inferior classiﬁcation results compared with other spectralspatial features methods, e.g., 3D-CNN, Gabor-CNN, S-CNN,
and DFFN. Moreover, the DFFN which combines the residual
learning with feature fusion in the a deep CNN framework
delivers the best classiﬁcation accuracies in terms of OA, AA,
and Kappa.
The second and third experiments were conducted on the
University of Pavia and Salinas images. As mentioned above,
200 samples per class were randomly selected as the training
samples and the rest of samples as the test samples (see Tables
II and III). The selected training samples account for about 4%
and 6% of the whole labeled reference data for the University
of Pavia and Salinas images, respectively, which provide a
challenging test set. Figs. 17 and 18 show the classiﬁcation
maps obtained by different methods. Tables V and VI present
the quantitative results (averaged over ten experiments) of
different methods.
From the above experimental results, the deep learningbased methods show great advantages over other traditional
methods in terms of visual classiﬁcation maps and quantitative
results. For instance, considering that in comparison of several
SVM-based classiﬁers, including SVM, EMP, EPF, and S-
CNN, S-CNN performs the best on three hyperspectral data
sets, which can be used to verify the effectiveness of deep
features compared with hand-crafted features. In addition, for
two ﬁlter-based methods, i.e., EPF and Gobar-CNN, the OA
of Gabor-CNN is about 3.5% higher than that of EPF on
the Houston image, which demonstrates that combining the
ﬁlter technique with deep leaning can give good classiﬁcation
results. Although CNN-PPF only utilizes spectral information,
it still delivers a better performance than what was obtained
by the other four traditional-based methods on the Houston
and University of Pavia images.
D. Deep Feature Visualization
In general, deep learning can be regarded as a black box in
most applications, where the inside information of network
is often unclear. Actually, exploring the inside features is
very useful for analyzing the network performance and further
designing the deep architecture. In this section, we used
Salinas data set as an example to visualize the deep features.
The weights of different convolutional kernels in the ﬁrst
convolutional layer are shown in Fig. 19. The Fig. 19 (a)
is random initial weights, and Fig. 19 (b) demonstrates the
learned weights. From this ﬁgure, we can see that the distribution of the weights for each ﬁlter become more regular and
present evident textural features after training. For example,
the intensities of the learned weights in the ﬁrst kernel are low
on the left side and high on the right side, i.e., the weights
reveal a trend of gradual decrease, which is different from the
structure of randomly initialized weights signiﬁcantly.
In addition, Fig. 20 shows the features after the convolutional layer, ReLU layer, and pooling layer. Fig. 21 shows the
extracted features after three convolutional layers of Fallow
and Fallow roughg plow samples. From Figs. 20 and 21,
one can see that the earlier convolution layers can extract
some simple features like texture and edge information, and
those lower level features can be composed to become more
complicated high-level features though the deeper convolution
layers. The learning process is totally automatic, which makes
CNNs more suitable for coping with varieties of situations.
E. Effectiveness Analysis of Strategies for Limited Samples
In this section, we will validate the effectiveness of some
strategies proposed in Section IV for the problem of limited
training samples. Speciﬁcally, we build a simple CNN and
adopt three strategies, i.e., data augmentation, transfer learning
and residual learning, to improve the classiﬁcation accuracy.
These methods, i.e., original CNN, CNN with data augmentation, CNN with transfer learning, and CNN with residual
Fig. 19. Weights of the ﬁrst convolutional layer. The size of each convolutional kernel is 4 × 4. Each kernel of 32 kernels in the ﬁrst convolutional layer is
shown in the forms of a tiny image. (a) Randomly initialized weights of the ﬁrst convolution layer on the Salinas data set. (b) Learned weights of the ﬁrst
convolution layer on the Salinas data set.
Extracted features of the Salinas data set. Each row of the images represents one class. There are four columns in the ﬁgure. The ﬁrst column
represents the input images. The second column shows the four feature maps after the ﬁrst convolution. The third column shows the four feature maps after
the ﬁrst ReLU operation. The last column shows the four features of the ﬁrst pooling operation.
learning, are denoted as CNN-Original, CNN-DA, CNN-
TL, and CNN-RL, respectively. The following experiments
were conducted in the Salinas data set, where the training
set consists of 5, 10, 15, 20, 25, and 30 labeled samples
per class, respectively, and the remaining available sample
were regarded as test data sets. All the used CNNs have
the same architectures which consist of seven convolutional
layers followed by the batch normalization operation, one
global pooling layer, and two fully connected layers. The
detailed structural information is presented in Table VII. For
the CNN-DA, new samples were generated according to the
equation (14). For the CNN-TL, the CNN was pretrained on
the Indian Pines data set because the Indian Pines and Salinas
images were collected by the same sensor, i.e., the Airborne
Visible/Infrared Imaging Spectrometer sensor. For CNN-RL,
the residual learning technique was incorporated into the CNN
to optimize the network.
Fig. 22 shows the classiﬁcation accuracies obtained by
different methods in terms of OA values. From this ﬁgure,
one can see that OA values of CNN-DA, CNN-TL, and
CNN-RL are higher than OA values of CNN-Original for all
training situations, which demonstrates that these strategies,
indeed, improve the network performance to some extent when
few training samples are available. The improvement almost
approaches 1% by comparing CNN-TL with CNN-Original
when the training set consists of 5 samples per class. In
addition, CNN-RL performs better than other methods for
most situations, which demonstrates that the residual learning
Extracted features after three convolutional layers on Fallow and
Fallow rough plow samples. The ﬁrst column represents the 27 × 27 input
images; the second column shows 32 feature maps after the ﬁrst convolution
layer, for which the size of each feature map is 24 × 24; the third column
shows 64 feature maps after the second convolution layer, for which the size
of each feature map is 8 × 8; and the number of feature maps after the third
convolution layer shown in the last column is 128, and the size of each feature
map is 1 × 1.
is a very useful network optimization method for HSI classi-
VI. CONCLUSION
Recently, deep learning-based HSI classiﬁcation has drawn
a signiﬁcant attention in the remote sensing ﬁeld and obtained
THE CONFIGURATION OF CNN MODEL
Feature maps
Convolution
Convolution
Convolution
Fully connected
Fully connected
The OA values obtained by different methods versus number of
good performance. In contrast to traditional hand-crafted
feature-based classiﬁcation methods, deep learning can automatically learn complex features of HSIs with a large number
of hierarchical layers. In this literature survey, we brieﬂy
introduced several deep models that are often used to classify
HSIs, including SAE, DBN, CNN, RNN, and GAN. Then, we
focused on deep learning-based classiﬁcation methodologies
for HSIs and provided a general and comprehensive overview
on the existing methods in a uniﬁed framework. Speciﬁcally,
these deep networks used in the HSI classiﬁcation were divided into spectral-feature networks, spatial-feature networks,
and spectral-spatial-feature networks, where each category
extracts the corresponding feature. Through this framework,
we can easily see that deep networks make full use of different feature types for classiﬁcation. We have also compared
and analyzed the performances of various HSI classiﬁcation
methods, including four traditional machine learning-based
methods and six deep learning-based methods. The classiﬁcation accuracies obtained by different methods demonstrate that
deep learning-based methods overall outperform the non-deeplearning-based methods and the DFFN which combines the
residual learning and feature fusion achieves best classiﬁcation
performance. Furthermore, deep features and network weights
were visualized, which is useful for analyzing the network
performance and further designing the deep architecture. In
addition, considering the fact that available training samples
in remote sensing are usually very limited and training deep
network requires a large number of samples, we also included
some strategies to improve classiﬁcation performance. We
have also conducted experiments to verify and compared the
effectiveness of these strategies. The ﬁnal results show that
the residual learning obtains the highest improvement among
all approaches. This experimental result may provide some
guidelines for the future study on this topic.