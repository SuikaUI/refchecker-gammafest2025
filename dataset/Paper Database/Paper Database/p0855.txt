Boosting Few-Shot Visual Learning with Self-Supervision
Spyros Gidaris1, Andrei Bursuc1, Nikos Komodakis2, Patrick Pérez1, Matthieu Cord1,3
2LIGM, Ecole des Pont ParisTech
3Sorbonne Université
Few-shot learning and self-supervised learning address
different facets of the same problem: how to train a model
with little or no labeled data. Few-shot learning aims for
optimization methods and models that can learn efﬁciently
to recognize patterns in the low data regime. Self-supervised
learning focuses instead on unlabeled data and looks into
it for the supervisory signal to feed high capacity deep neural networks. In this work we exploit the complementarity of these two domains and propose an approach for improving few-shot learning through self-supervision. We use
self-supervision as an auxiliary task in a few-shot learning pipeline, enabling feature extractors to learn richer and
more transferable visual representations while still using few
annotated samples. Through self-supervision, our approach
can be naturally extended towards using diverse unlabeled
data from other datasets in the few-shot setting. We report
consistent improvements across an array of architectures,
datasets and self-supervision techniques.
1. Introduction
Deep learning based methods have achieved impressive
results on various image understanding tasks, such as image
classiﬁcation , object detection , or semantic segmentation . However, in order to successfully
learn these tasks, such models need to access large volumes
of manually labeled training data. If not, the trained models
might suffer from poor generalization performance on the
test data. In image classiﬁcation for instance, learning to recognize reliably a set of categories with convolutional neural
networks (convnets) requires hundreds or thousands of training examples per class. In contrast, humans are perfectly
capable of learning new visual concepts from only one or
few examples, generalizing without difﬁculty to new data.
The aim of few-shot learning is to endow
artiﬁcial perception systems with a similar ability, especially
with the help of modern deep learning.
Hence, the goal of few-shot visual learning is to devise
recognition models that are capable of efﬁciently learning
to recognize a set of classes despite the fact that there are
available very few training examples for them (e.g., only 1
or 5 examples per class). In order to avoid overﬁtting due to
data scarcity, few-shot learning algorithms rely on transfer
learning techniques and have two learning stages. During a
ﬁrst stage, the model is usually trained using a different set
of classes, called base classes, which is associated to a large
set of annotated training examples. The goal of this stage is
to let the few-shot model acquire transferable visual analysis
abilities, typically in the form of learned representations, that
are mobilized in the second stage. In this subsequent step,
the model indeed learns to recognize novel classes, unseen
during the ﬁrst learning stage, using only a few training
examples per class.
Few-shot learning relates with self-supervised representation learning . The latter is a form of
unsupervised learning that trains a model on an annotationfree pretext task deﬁned using only the visual information
present in images. The purpose of this self-supervised task
is to make the model learn image representations that would
be useful when transferred to other image understanding
tasks. For instance, in the seminal work of Doersch et al. ,
a network, by being trained on the self-supervised task of
predicting the relative location of image patches, manages to
learn image representations that are successfully transferred
to the vision tasks of object recognition, object detection,
and semantic segmentation. Therefore, as in few-shot learning, self-supervised methods also have two learning stages,
the ﬁrst that learns image representations with a pretext
self-supervised task, and the second that adapts those representations to the actual task of interest. Also, both learning
approaches try to limit the dependence of deep learning
methods on the availability of large amounts of labeled data.
Inspired by the connection between few-shot learning
and self-supervised learning, we propose to combine the
two methods to improve the transfer learning abilities of
few-shot models. More speciﬁcally, we propose to add
a self-supervised loss to the training loss that a few-shot
model minimizes during its ﬁrst learning stage (see Figure 1). Hence, we artiﬁcially augment the training task(s)
that a few-shot model solves during its ﬁrst learning stage.
We argue that this task augmentation forces the model to
learn a more diversiﬁed set of image features, and this in
 
Object classiﬁer
Self-supervised loss
Annotated images
Supervised loss Lfew for
few-shot classiﬁcation
Self-supervised
rotation classiﬁer
Figure 1: Combining self-supervised image rotation prediction and supervised base class recognition in ﬁrst learning stage of a fewshot system. We train the feature extractor Fθ(·) with both annotated (top branch) and non-annotated (bottom branch) data in a multi-task
setting. We use the annotated data to train the object classiﬁer C(·) with the few-shot classiﬁcation loss Lfew. For the self-supervised task,
we sample images from the annotated set (and optionally from a different set of non-annotated images). Here, we generate four rotations for
each input image, process them with Fθ(·) and train the rotation classiﬁer Rφ with the self-supervised loss Lself. The pipeline for relative
patch location self-supervision is analogue to this one.
turn improves its ability to adapt to novel classes with few
training data. Moreover, since self-supervision does not require data annotations, one can include extra unlabeled data
to the ﬁrst learning stage. By extending the size and variety
of training data in this manner, one might expect to learn
richer image features and to get further performance gain
in few-shot learning. At the extreme, using only unlabeled
data in the ﬁrst learning stage, thus removing the use of
base classes altogether, is also appealing. We will show that
both these semi-supervised and unsupervised regimes can
be indeed put at work for few-shot recognition thanks to
self-supervised tasks.
In summary, the contributions of our work are: (1) We
propose to weave self-supervision into the training objective
of few-shot learning algorithms. The goal is to boost the
ability of the latter to adapt to novel classes with few training
data. (2) We study the impact of the added self-supervised
loss by performing exhaustive quantitative experiments on
MiniImagenet, CIFAR-FS, and tiered-MiniImagenet fewshot datasets. In all of them self-supervision improves the
few-shot learning performance leading to state-of-the-art results. (3) Finally, we extend the proposed few-shot recognition framework to semi-supervised and unsupervised setups,
getting further performance gain in the former, and showing
with the latter that our framework can be used for evaluating and comparing self-supervised representation learning
approaches on few-shot object recognition.
2. Related work
Few-shot learning.
There is a broad array of few-shot
learning approaches, including, among many: gradient
descent-based approaches , which learn how
to rapidly adapt a model to a given few-shot recognition task
via a small number of gradient descent iterations; metric
learning based approaches that learn a distance metric between a query, i.e., test image, and a set of support images,
i.e., training images, of a few-shot task ;
methods learning to map a test example to a class label by
accessing memory modules that store training examples for
that task ; approaches that learn how to
generate the weights of a classiﬁer or of a
multi-layer neural network for the new classes
given the few available training data for each of them; methods that “hallucinate” additional examples of a class from a
reduced amount of data .
In our work we consider two approaches from the metric
learning category, namely Prototypical Networks and
Cosine Classiﬁers for their simplicity and ﬂexibility.
Nevertheless, the proposed auxiliary self-supervision is compatible with several other few-shot classiﬁcation solutions.
Self-supervised learning.
This is a recent paradigm
which is mid-way between unsupervised and supervised
learning, and aims to mitigate the challenging need for large
amounts of annotated data. Self-supervised learning deﬁnes
an annotation-free pretext task, in order to provide a surrogate supervision signal for feature learning. Predicting
the colors of images , the relative position of image
patches , the random rotation that has been applied to
an image , or the missing central part of an image ,
are some of the many methods for selfsupervised feature learning. The intuition is that, by solving
such tasks, the trained model extracts semantic features that
can be useful for other downstream tasks. In our case, we
consider a multi-task setting where we train the backbone
convnet using joint supervision from the supervised end-task
and an auxiliary self-supervised pretext task. Unlike most
multi-task settings aiming at good results on all tasks simultaneously , we are interested in improving performance
on the main task only by leveraging supervision from the
surrogate task, as also shown in . We expect that, in a
few-shot setting where squeezing out generalizable features
from the available data is highly important, the use of selfsupervision as an auxiliary task will bring improvements.
Also, related to our work, Chen et al. recently added
rotation prediction self-supervision to generative adversarial
networks leading to signiﬁcant quality improvements
of the synthesized images.
3. Methodology
As already explained, few-shot learning algorithms have
two learning stages and two corresponding sets of classes.
Here, we deﬁne as Db = {(x, y)} ⊂I × Yb the training set
of base classes used during the ﬁrst learning stage, where
x ∈I is an image with label y in label set Yb of size Nb.
Also, we deﬁne as Dn = {(x, y)} ⊂I × Yn the training set
of Nn novel classes used during the second learning stage,
where each class has K samples (K = 1 or 5 in benchmarks).
One talks about Nn-way K-shot learning. Importantly, the
label sets Yn and Yb are disjoint.
In the remainder of this section, we ﬁrst describe in §3.1
the two standard few-shot learning methods that we consider
and introduce in §3.2 the proposed method to boost their
performance with self-supervision.
3.1. Explored few-shot learning methods
The main component of all few-shot algorithms is a feature extractor Fθ(·), which is a convnet with parameters
θ. Given an image x, the feature extractor will output a
d-dimensional feature Fθ(x). In this work we consider two
few-shot algorithms, Prototypical Networks (PN) and
Cosine Classiﬁers (CC) , described below. They
are fairly similar, with their main difference lying in the
ﬁrst learning stage: only CC learns actual base classiﬁers
along with the feature extractor, while PN simply relies on
class-level averages.
Prototypical Networks (PN) .
During the ﬁrst stage
of this approach, the feature extractor Fθ(·) is learned on
sampled few-shot classiﬁcation sub-problems that are analogue to the targeted few-shot classiﬁcation problem. In each
training episode of this learning stage, a subset Y∗⊂Yb
of N∗base classes are sampled (they are called “support
classes”) and, for each of them, K training examples are
randomly picked from within Db. This yields a training set
D∗. Given current feature extractor Fθ, the average feature
for each class j ∈Y∗, its “prototype”, is computed as
Fθ(x), with Xj
∗= { x | (x, y) ∈D∗, y = j }
and used to build a simple similarity-based classiﬁer. Then,
given a new image xq from a support class but different
from samples in D∗, the classiﬁer outputs for each class j
the normalized classiﬁcation score
Cj(Fθ(xq); D∗) = softmaxj
 Fθ(xq), pi
where sim(·, ·) is a similarity function, which may be cosine
similarity or negative squared Euclidean distance. So, in
practice, the image xq will be classiﬁed to its closest prototype. Note that the classiﬁer is conditioned on D∗in order to
compute the class prototypes. The ﬁrst learning stage ﬁnally
amounts to iteratively minimizing the following loss w.r.t. θ:
Lfew(θ; Db) =
−log Cyq(Fθ(xq); D∗)
where (xq, yq) is a training sample from a support class
deﬁned in D∗but different from images in D∗.
In the second learning stage, the feature extractor Fθ is
frozen and the classiﬁer of novel classes is simply deﬁned as
C(·; Dn), with prototypes deﬁned as in (1) with D∗= Dn.
Cosine Classiﬁers (CC) .
In CC few-shot learning, the ﬁrst stage trains the feature extractor Fθ together
with a cosine-similarity based classiﬁer on the (standard)
supervised task of classifying the base classes. Denoting
Wb = [w1, ..., wNb] the matrix of the d-dimensional classiﬁcation weight vectors, the normalized score for an input
image x reads
Cj(Fθ(x); Wb) = softmaxj
 Fθ(x), wi
where cos(·, ·) is the cosine operation between two vectors,
and the scalar γ is the inverse temperature parameter of the
softmax operator.1
The ﬁrst learning stage aims at minimizing w.r.t. θ and
Wb the negative log-likelihood loss:
Lfew(θ, Wb; Db) =
−log Cy(Fθ(x); Wb)
One of the reasons for using the cosine-similarity based
classiﬁer instead of the standard dot-product based one, is
that the former learns feature extractors that reduce intraclass variations and thus can generalize better on novel
classes. By analogy with PN, the weight vectors wj’s can
be interpreted as learned prototypes for the base classes, to
which input image features are compared for classiﬁcation.
As with PN, the second stage boils down to computing
one representative feature wj for each new class by simple
averaging of associated K samples in Dn, and to deﬁne the
ﬁnal classiﬁer C(.; [w1 · · · wNn]) the same way as in (4).
1Speciﬁcally, γ controls the peakiness of the probability distribution
generated by the softmax operator .
3.2. Boosting few-shot learning via self-supervision
A major challenge in few-shot learning is encountered
during the ﬁrst stage of learning. How to make the feature
extractor learn image features that can be readily exploited
for novel classes with few training data during the second
stage? With this goal in mind, we propose to leverage the
recent progress in self-supervised feature learning to further
improve current few-shot learning approaches.
Through solving a non-trivial pretext task that can be
trivially supervised, such as recovering the colors of images
from their intensities, a network is encouraged to learn rich
and generic image features that are transferable to other
downstream tasks such as image classiﬁcation. In the ﬁrst
stage of few-shot learning, we propose to extend the training of the feature extractor Fθ(.) by including such a selfsupervised task besides the main task of recognizing base
We consider two ways for incorporating self-supervision
into few-shot learning algorithms: (1) by using an auxiliary
loss function based on a self-supervised task, and (2) by
exploiting unlabeled data in a semi-supervised way during
training. In the following we will describe the two techniques.
Auxiliary loss based on self-supervision
We incorporate self-supervision to a few-shot learning algorithm by adding an auxiliary self-supervised loss during
its ﬁrst learning stage. More formally, let Lself(θ, φ; Xb)
be the self-supervised loss applied to the set Xb = { x |
(x, y) ∈Db } of training examples in Db deprived of their
class labels. The loss Lself(θ, φ; Xb) is a function of the
parameters θ of the feature extractor and of the parameters φ
of a network only dedicated to the self-supervised task. The
ﬁrst training stage of few-shot learning now reads
θ,[Wb],φ Lfew(θ, [Wb]; Db) + αLself(θ, φ; Xb) ,
where Lfew stands either for the PN few-shot loss (3) or for
the CC one (5), with additional argument Wb in the latter
case (hence bracket notation). The positive hyper-parameter
α controls the importance of the self-supervised term2. An
illustration of the approach is provided in Figure 1.
For the self-supervised loss, we consider two tasks in the
present work: predicting the rotation incurred by an image,
 , which is simple and readily incorporated into a fewshot learning algorithm; predicting the relative location of
two patches from the same image , a seminal task in selfsupervised learning. In a recent study, both methods have
been shown to achieve state-of-the-art results .
2In our experiments, we use α = 1.0.
Image rotations.
In this task, the convnet must recognize among four possible 2D rotations in R
{0◦, 90◦, 180◦, 270◦} the one applied to an image (see Figure 1). Speciﬁcally, given an image x, we ﬁrst create its
four rotated copies { xr | r ∈R }, where xr is the image x
rotated by r degrees. Based on the features Fθ(xr) extracted
from such a rotated image, the new network Rφ attempts to
predict the rotation class r. Accordingly, the self-supervised
loss of this task is deﬁned as:
Lself(θ, φ; X) =
where X is the original training set of non-rotated images
φ(·) is the predicted normalized score for rotation r.
Intuitively, in order to do well for this task the model should
reduce the bias towards up-right oriented images, typical for
ImageNet-like datasets, and learn more diverse features to
disentangle classes in the low-data regime.
Relative patch location.
Here, we create random pairs of
patches from an image and then predict, among eight possible positions, the location of the second patch w.r.t. to the
ﬁrst, e.g., “on the left and above” or “on the right and below”. Speciﬁcally, given an image x, we ﬁrst divide it into 9
regions over a 3 × 3 grid and sample a patch within each region. Let’s denote ¯x0 the central image patch, and ¯x1 · · · ¯x8
its eight neighbors lexicographically ordered. We compute
the representation of each patch3 and then generate patch
feature pairs
 Fθ(¯x0), Fθ(¯xp)
by concatenation. We train
a fully-connected network Pφ(·, ·) to predict the position of
¯xp from each pair.
The self-supervised loss of this task is deﬁned as:
Lself(θ, φ; X) =
 Fθ(¯x0), Fθ(¯xp)
where X is a set of images and P p
φ is the predicted normalized score for the relative location p. Intuitively, a good
model on this task should somewhat recognize objects and
parts, even in presence of occlusions and background clutter. Note that, in order to prevent the model from learning
low-level image statistics such as chromatic aberration ,
the patches are preprocessed with aggressive color augmentation (i.e., converting to grayscale with probability 0.66 and
normalizing the pixels of each patch individually to have
zero mean and unit standard deviation).
Semi-supervised few-shot learning
The self-supervised term Lself in the training loss (6) does
not depend on class labels. We can easily extend it to learn
3If the architecture of Fθ(·) is fully convolutional, we can apply it to
both big images and smaller patches.
as well from additional unlabeled data. Indeed, if a set Xu
of unlabeled images is available besides Db, we can make
the self-supervised task beneﬁt from them by redeﬁning the
ﬁrst learning stage as:
θ,[Wb],φ Lfew(θ, [Wb]; Db) + α · Lself(θ, φ; Xb ∪Xu) . (9)
By training the feature extractor Fθ to also minimize the selfsupervised loss on these extra unlabeled images, we open up
its visual scope with the hope that this will further improve
its ability to accommodate novel classes with scarce data.
This can be seen as a semi-supervised training approach
for few-shot algorithms. An interesting aspect of this semisupervised training approach is that it does not require the
extra unlabeled data to be from the same (base) classes as
those in labeled dataset Db. Thus, it is much more ﬂexible
w.r.t. the source of the unlabeled data than standard semisupervised approaches.
4. Experimental Results
In this section we evaluate self-supervision as auxiliary
loss function in §4.2 and in §4.3 as a way of exploiting
unlabeled data in semi-supervised training. Finally, in §4.4
we use the few-shot object recognition task for evaluating
self-supervised methods.
We perform experiments on three few-shot
datasets, MiniImageNet , tiered-MiniImageNet and
CIFAR-FS . MiniImageNet consists of 100 classes randomly picked from the ImageNet dataset (i.e., 64 base
classes, 16 validation classes, and 20 novel test classes);
each class has 600 images with size 84 × 84 pixels. tiered-
MiniImageNet consists of 608 classes randomly picked
from ImageNet (i.e., 351 base classes, 97 validation
classes, and 160 novel test classes); in total there are
779, 165 images again with size 84 × 84. Finally, CIFAR-
FS is a few-shot dataset created by dividing the 100 classes
of CIFAR-100 into 64 base classes, 16 validation classes,
and 20 novel test classes. The images in this dataset have
size 32 × 32 pixels.
Evaluation metrics.
Few-shot classiﬁcation algorithms
are evaluated based on the classiﬁcation performance in their
second learning stage (when the learned classiﬁer is applied
to test images from the novel classes). More speciﬁcally,
a large number of Nn-way K-shot tasks are sampled from
the available set of novel classes. Each task is created by
randomly selecting Nn novel classes from the available test
(validation) classes and then within the selected images randomly selecting K training images and M test images per
class (making sure that train and test images do not overlap).
The classiﬁcation performance is measured on the Nn × M
test images and is averaged over all the sampled few-shot
tasks. Except otherwise stated, for all experiments we used
M = 15, Nn = 5, and K = 1 or K = 5 (1-shot and 5-shot
settings respectively).
4.1. Implementation details
Network architectures.
We conduct experiments with
3 different feature extractor architectures Fθ, Conv-4-64,
Conv-4-512, and WRN-28-10. Conv-4-64 consists of 4
convolutional blocks each implemented with a 3 × 3 convolutional layer followed by BatchNorm + ReLU + 2 × 2
max-pooling units . All blocks of Conv-4-64 have 64 feature channels. The ﬁnal feature map has size 5×5×64 and is
ﬂattened into a ﬁnal 1600-dimensional feature vector. Conv-
4-512 is derived from Conv-4-64 by gradually increasing its
width across layers leading to 96, 128, 256, and 512 feature
channels for its 4 convolutional blocks respectively. Therefore, its output feature vector has 5 × 5 × 512 = 12, 800
dimensions after ﬂattening. Finally, WRN-28-10 is a 28layer Wide Residual Network with width factor 10. Its
output feature map has size 10×10×640 which after global
average pooling creates a 640-dimensional feature vector.
The network Rφ(·) speciﬁc to the rotation prediction
task gets as input the output feature maps of Fθ and is implemented as a convnet. Given two patches, the network
Pφ(·, ·) speciﬁc to the relative patch location task gets the
concatenation of their feature vectors extracted with Fθ as
input, and forwards it to two fully connected layers.
For further architecture details see Appendix B.1.
Training optimization routine for ﬁst learning stage.
The training loss is optimized with mini-batch stochastic
gradient descent (SGD). For the labeled data we apply
both recognition Lfew and self-supervised Lself losses. For
the semi-supervised training, at each step we sample minibatches that consist of labeled data, for which we use both
losses, and unlabeled data, in which case we apply only Lself.
Learning rates, number of epochs, and batches sizes, were
cross-validated on the validation sets. For further implementation details see Appendices B.2 and B.3.
Implementation of relative patch location task.
the aggressive color augmentation of the patches in the patch
localization task, and the fact that the patches are around 9
times smaller than the original images, the data distribution
that the feature extractor “sees” from them is very different
from that of the images. To overcome this problem we apply
an extra auxiliary classiﬁcation loss to the features extracted
from the patches. Speciﬁcally, during the ﬁrst learning stage
of CC we merge the features Fθ(¯xp) of the 9 patches of
an image (e.g., with concatenation or averaging) and then
apply the cosine classiﬁer (4) to the resulting feature .
61.80 ± 0.30%
78.02 ± 0.24%
63.45 ± 0.31%
79.79 ± 0.24%
Conv-4-512
65.26 ± 0.31%
81.14 ± 0.23%
65.87 ± 0.30%
81.92 ± 0.23%
71.83 ± 0.31%
84.63 ± 0.23%
73.62 ± 0.31%
86.05 ± 0.22%
62.82 ± 0.32%
79.59 ± 0.24%
64.69 ± 0.32%
80.82 ± 0.24%
Conv-4-512
66.48 ± 0.32%
80.28 ± 0.23%
67.94 ± 0.31%
82.20 ± 0.23%
68.35 ± 0.34%
81.79 ± 0.23%
68.60 ± 0.34%
81.25 ± 0.24%
Table 2: Rotation prediction as auxiliary loss on CIFAR-FS.
Average 5-way classiﬁcation accuracies for the novel classes on the
test set of CIFAR-FS with 95% conﬁdence intervals (using 5000
test episodes).
applied to the original images features). Note that this patch
based auxiliary classiﬁcation loss has the same weight as
the original classiﬁcation loss Lfew. Also, during the second
learning stage we do not use the patch based classiﬁer.
4.2. Self-supervision as auxiliary loss function
Rotation prediction as auxiliary loss function.
study the impact of adding rotation prediction as selfsupervision to the few-shot learning algorithms of Cosine
Classiﬁers (CC) and Prototypical Networks (PN). We perform this study using the MiniImageNet and CIFAR-FS
datasets and report results in Tables 1 and 2 respectively. For
the CC case, we use as strong baselines, CC models without
self-supervision but trained to recognize all the 4 rotated
53.72 ± 0.42%
70.96 ± 0.33%
54.30 ± 0.42%
71.58 ± 0.33%
Conv-4-512
55.58 ± 0.42%
73.52 ± 0.33%
56.87 ± 0.42%
74.84 ± 0.33%
58.43 ± 0.46%
75.45 ± 0.34%
60.71 ± 0.46%
77.64 ± 0.34%
Table 3: Relative patch location as auxiliary loss on MiniImageNet. Average 5-way classiﬁcation accuracies for the novel
classes on the test set of MiniImageNet with 95% conﬁdence intervals .
versions of an image. The reason for using this baseline
is that during the ﬁrst learning stage, the model “sees” the
same type of data, i.e., rotated images, as the model with
rotation prediction self-supervision. Note that despite the
rotation augmentations of the ﬁrst learning stage, during the
second stage the model uses as training examples for the
novel classes only the up-right versions of the images. Still
however, using rotation augmentations improves the classi-
ﬁcation performance of the baseline models when adapted
to the novel classes. Therefore, for fair comparison, we also
apply rotation augmentations to the CC models with rotation
prediction self-supervision. For the PN case, we do not use
rotation augmentation since in our experiments this lead to
performance degradation.
The results in Tables 1 and 2 demonstrate that (1) indeed, adding rotation prediction self-supervision improves
the few-shot classiﬁcation performance, and (2) the performance improvement is more signiﬁcant for high capacity
architectures, e.g., WRN-28-10.
Relative patch location prediction as auxiliary loss function.
As explained in §3.2.1, we consider a second selfsupervised task, namely relative patch location prediction.
For simplicity, we restrict its assessment to the CC few-shot
algorithm, which in our experiments proved to perform better than PN and to be simpler to train. Also, for this study we
consider only the MiniImageNet dataset and not CIFAR-FS
since the latter contains thumbnail images of size 32 × 32
from which it does not make sense to extract patches: their
size would have to be less than 8 × 8 pixels, which is too
small for any of the evaluated architectures. We report results
on MiniImageNet in Table 3. As a strong baseline we used
CC models without self-supervision but with the auxiliary
patch based classiﬁcation loss described in §4.1.
Based on the results of Table 3 we observe that: (1) relative patch location also manages to improve the few-shot
classiﬁcation performance and, as in the rotation prediction
case, the improvement is more signiﬁcant for high capacity
network architectures. (2) Also, comparing to the rotation
prediction case, the relative patch location offers smaller
48.70 ± 1.84% 63.10 ± 0.92%
Prototypical Nets 
49.42 ± 0.78% 68.20 ± 0.66%
56.20 ± 0.86% 72.81 ± 0.62%
RelationNet 
50.40 ± 0.80% 65.30 ± 0.70%
48.70 ± 0.60% 65.50 ± 0.60%
Conv-4-512
51.20 ± 0.60% 68.20 ± 0.60%
TADAM 
58.50 ± 0.30% 76.70 ± 0.30%
Munkhdalai et al. ResNet-12
57.10 ± 0.70% 70.04 ± 0.63%
SNAIL 
55.71 ± 0.99% 68.88 ± 0.92%
Qiao et al. ∗
WRN-28-10 59.60 ± 0.41% 73.74 ± 0.19%
WRN-28-10 61.76 ± 0.08% 77.59 ± 0.12%
54.83 ± 0.43% 71.86 ± 0.33%
Conv-4-512
56.27 ± 0.43% 74.30 ± 0.34%
WRN-28-10 62.93 ± 0.45% 79.87 ± 0.33%
CC+rot+unlabeled
WRN-28-10 63.77 ± 0.45% 80.70 ± 0.33%
Table 4: Comparison with prior work on MiniImageNet. Average 5-way classiﬁcation accuracies for the novel classes on the test
set of MiniImageNet with 95% conﬁdence intervals . ∗: using also the validation classes for training. For
the description of the CC+rot+unlabeled model see §4.3.
performance improvement.
Comparison with prior work.
In Tables 4, 5, and 6,
we compare our approach with prior few-shot methods on
the MiniImageNet, CIFAR-FS, and tiered-MiniImageNet
datasets respectively. For our approach we used CC and
rotation prediction self-supervision, which before gave the
best results. In all cases we achieve state-of-the-art results
surpassing prior methods with a signiﬁcant margin. For instance, in the 1-shot and 5-shot settings of MiniImageNet
we outperform the previous leading method LEO by
around 1.3 and 2.3 percentage points respectively.
More detailed results are provided in Appendix A.
4.3. Semi-supervised few-shot learning
Next, we evaluate the proposed semi-supervised training
approach. In these experiments we use CC models with
rotation prediction self-supervision. We perform two types
of semi-supervised experiments: (1) training with unlabeled
data from the same base classes, and (2) training with unlabeled data that are not from the base classes.
Training with unlabeled data from the same base classes.
From the base classes of MiniImageNet, we use only a
small percentage of the training images (e.g., 5% of images per class) as annotated training data, while the rest
of the images (e.g., 95%) are used as the unlabeled data
in the semi-supervised training. We provide results in the
ﬁrst two sections of Table 7. The proposed semi-supervised
55.50 ± 0.70%
72.00 ± 0.60%
Conv-4-512
57.90 ± 0.80%
76.70 ± 0.60%
62.82 ± 0.32%
79.59 ± 0.24%
Conv-4-512
66.48 ± 0.32%
80.28 ± 0.23%
MAML †
58.90 ± 1.90%
71.50 ± 1.00%
MAML †
Conv-4-512
53.80 ± 1.80%
67.60 ± 1.00%
RelationNet †
55.00 ± 1.00%
69.30 ± 0.80%
Conv-4-512
60.00 ± 0.70%
76.10 ± 0.60%
Conv-4-512
64.00 ± 0.80%
78.90 ± 0.60%
63.45 ± 0.31%
79.79 ± 0.24%
Conv-4-512
65.87 ± 0.30%
81.92 ± 0.23%
WRN-28-10 73.62 ± 0.31%
86.05 ± 0.22%
Table 5: Comparison with prior work in CIFAR-FS. Average
5-way classiﬁcation accuracies for the novel classes on the test set
of CIFAR-FS with 95% conﬁdence (using 5000 test episodes). †:
results from . ‡: our implementation.
MAML †
51.67 ± 1.81% 70.30 ± 0.08%
Prototypical Nets Conv-4-64
53.31 ± 0.89% 72.69 ± 0.74 %
RelationNet †
54.48 ± 0.93% 71.32 ± 0.78%
Liu et al. 
57.41 ± 0.94% 71.55 ± 0.74
WRN-28-10 66.33 ± 0.05% 81.44 ± 0.09 %
WRN-28-10 70.04 ± 0.51% 84.14 ± 0.37%
WRN-28-10 70.53 ± 0.51% 84.98 ± 0.36%
Rotation prediction as auxiliary loss on tiered-
MiniImageNet. Average 5-way classiﬁcation accuracies for the
novel classes on the test set of tiered-MiniImageNet with 95%
conﬁdence . †: results
from .
training approach is compared with a CC model without selfsupervision and with a CC model with self-supervision but
no recourse to the unlabeled data. The results demonstrate
that indeed, our method manages to improve few-shot classi-
ﬁcation performance by exploiting unlabeled images. Compared to Ren et al. , that also propose a semi-supervised
method, our method with Conv-4-64 and 20% annotations
achieves better results than their method with Conv-4-64
and 40% annotations (i.e., our 51.21% and 68.89% Mini-
ImageNet accuracies vs. their 50.41% and 64.39% for the
1-shot and 5-shot settings respectively).
Training with unlabeled data not from the base classes.
This is a more realistic setting, since it is hard to constrain
the unlabeled images to be from the same classes as the
base classes. For this experiment, we used as unlabeled
data the training images of the tiered-MiniImageNet base
classes minus the classes that are common with the base, validation, or test classes of MiniImageNet. In total, 408, 726
Table 7: Semi-supervised training with rotation prediction as
self-supervision on MiniImageNet. Average 5-way classiﬁcation accuracies on the test set of MiniImageNet. µ is the percentage of the base class training images of MiniImageNet that
are used as annotated data during training. Rot indicates adding
self-supervision, M indicates using as unlabeled data the (rest of)
MiniImageNet training dataset, and T using as unlabeled data the
tiered-MiniImageNet training dataset.
CACTUs 
Table 8: Evaluating self-supervised representation learning
methods on few-shot recognition. Average 5-way classiﬁcation
accuracies on the test set of MiniImageNet. Rot refers to the rotation prediction task, Loc to the relative patch location task, and CC
to the supervised method of Cosine Classiﬁers.
unlabeled images are used from tiered-MiniImageNet. We
report results in the last row of Table 7. Indeed, even in
this difﬁcult setting, our semi-supervised approach is still
able to exploit unlabeled data and improve the classiﬁcation
performance. Furthermore, we did an extra experiment in
which we trained a WRN-28-10 based model using 100%
of MiniImageNet training images and unlabeled data from
tiered-MiniImageNet. This model achieved 63.77% and
80.70% accuracies for the 1-shot and 5-shot settings respectively on MiniImageNet (see entry CC+rot+unlabeled
of Table 4), which improves over the already very strong
CC+rot model (see Table 4).
4.4. Few-shot object recognition to assess selfsupervised representations
Given that our framework allows the easy combination
of any type of self-supervised learning approach with the
adopted few-shot learning algorithms, we also propose to
use it as an alternative way for comparing/evaluating the
effectiveness of different self-supervised approaches. To this
end, the only required change to our framework is to use
uniquely the self-learning loss (i.e., no labeled data is now
used) in the ﬁrst learning stage (for implementation details
see Appendix B.2). The performance of the few-shot model
resulting from the second learning stage can then be used for
evaluating the self-supervised method under consideration.
Comparing competing self-supervised techniques is not
straightforward since it must be done by setting up another,
somewhat contrived task that exploits the learned representations . Instead, given the very similar goals of fewshot and self-supervised learning, we argue that the proposed
comparison method could be more meaningful for assessing
different self-supervised features. Furthermore, it is quite
simple and fast to perform when compared to some alternatives such as ﬁne-tuning the learned representations on the
PASCAL detection task , with the beneﬁt of obtaining
more robust statistics aggregated over evaluations of thousands of episodes with multiple different conﬁgurations of
classes and training/testing samples.
To illustrate our point, we provide in Table 8 quantitative results of this type of evaluation on the MiniImageNet dataset, for the self-supervision methods of rotation
prediction and relative patch location prediction. For selfsupervised training we used the training images of the base
classes of MiniImageNet and for the few-shot classiﬁcation
step we used the test classes of MiniImageNet. We observe that the explored self-supervised approaches achieve
relatively competitive classiﬁcation performance when compared to the supervised method of CC and obtain results
that are on par or better than other, more complex, unsupervised systems. We leave as future work a more detailed and
thorough comparison of self-learned representations in this
evaluation setting.
5. Conclusions
Inspired by the close connection between few-shot and
self-supervised learning, we propose to add an auxiliary loss
based on self-supervision during the training of few-shot
recognition models. The goal is to boost the ability of the
latter to recognize novel classes using only few training data.
Our detailed experiments on MiniImagenet, CIFAR-FS, and
tiered-MiniImagenet few-shot datasets reveal that indeed
adding self-supervision leads to signiﬁcant improvements
on the few-shot classiﬁcation performance, which makes
the employed few-shot models achieve state-of-the-art results. Furthermore, the annotation-free nature of the selfsupervised loss allows us to exploit diverse unlabeled data in
a semi-supervised manner, which further improves the classiﬁcation performance. Finally, we show that the proposed
framework can also be used for evaluating self-supervised or
unsupervised methods based on few-shot object recognition.