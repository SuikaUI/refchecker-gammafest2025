Filippone, M., Camastra, F., Masulli, F., and Rovetta, S. A survey
of kernel and spectral methods for clustering. Pattern Recognition, 41 (1).
pp. 176-190. ISSN 0031-3203
 
Deposited on: 21 March 2012
Enlighten – Research publications by members of the University of Glasgow
 
A survey of kernel and spectral methods for
clustering
Maurizio Filippone a Francesco Camastra b Francesco Masulli a
Stefano Rovetta a
aDepartment of Computer and Information Science, University of Genova, and
CNISM, Via Dodecaneso 35, I-16146 Genova, Italy
bDepartment of Applied Science, University of Naples Parthenope, Via A. De
Gasperi 5 I-80133 Napoli, Italy
Clustering algorithms are a useful tool to explore data structures and have been
employed in many disciplines. The focus of this paper is the partitioning clustering problem with a special interest in two recent approaches: kernel and spectral
methods. The aim of this paper is to present a survey of kernel and spectral clustering methods, two approaches able to produce nonlinear separating hypersurfaces
between clusters. The presented kernel clustering methods are the kernel version of
many classical clustering algorithms, e.g., K-means, SOM and Neural Gas. Spectral
clustering arise from concepts in spectral graph theory and the clustering problem
is conﬁgured as a graph cut problem where an appropriate objective function has to
be optimized. An explicit proof of the fact that these two paradigms have the same
objective is reported since it has been proven that these two seemingly diﬀerent approaches have the same mathematical foundation. Besides, fuzzy kernel clustering
methods are presented as extensions of kernel K-means clustering algorithm.
Key words: partitional clustering, Mercer kernels, kernel clustering, kernel fuzzy
clustering, spectral clustering
Email addresses: (Maurizio Filippone),
 (Francesco Camastra),
 (Francesco Masulli), criteria where
groups (or clusters) are set of similar patterns. Crucial aspects in clustering
are pattern representation and the similarity measure. Each pattern is usually
represented by a set of features of the system under study. It is very important to notice that a good choice of representation of patterns can lead to
improvements in clustering performance. Whether it is possible to choose an
appropriate set of features depends on the system under study. Once a representation is ﬁxed it is possible to choose an appropriate similarity measure
among patterns. The most popular dissimilarity measure for metric representations is the distance, for instance the Euclidean one .
Clustering techniques can be roughly divided into two categories:
• hierarchical;
• partitioning.
Hierarchical clustering techniques are able to ﬁnd structures which
can be further divided in substructures and so on recursively. The result is a
hierarchical structure of groups known as dendrogram.
Partitioning clustering methods try to obtain a single partition of data without
any other sub-partition like hierarchical algorithms do and are often based
on the optimization of an appropriate objective function. The result is the
creation of separations hypersurfaces among clusters. For instance we can
consider two nonlinear clusters as in ﬁgure 1. Standard partitioning methods
(e.g., K-Means, Fuzzy c-Means, SOM and Neural Gas) using two centroids
are not able to separate in the desired way the two rings. The use of many
centroids could solve this problem providing a complex description of a simple
data set. For this reason several modiﬁcations and new approaches have been
introduced to cope with this problem.
Among the large amount of modiﬁcations we can mention the Fuzzy c-Varieties ,
but the main drawback is that some a priori information on the shape of
clusters must be included. Recently, some clustering methods that produce
nonlinear separating hypersurfaces among clusters have been proposed. These
algorithms can be divided in two big families: kernel and spectral clustering
Regarding kernel clustering methods, several clustering methods have been
Figure 1. A data set composed of two rings of points.
modiﬁed incorporating kernels (e.g., K-Means, Fuzzy c-Means, SOM and Neural Gas). The use of kernels allows to map implicitly data into an high dimensional space called feature space; computing a linear partitioning in this feature
space results in a nonlinear partitioning in the input space.
Spectral clustering methods arise from concepts in spectral graph theory. The
basic idea is to construct a weighted graph from the initial data set where each
node represents a pattern and each weighted edge simply takes into account
the similarity between two patterns. In this framework the clustering problem
can be seen as a graph cut problem, which can be tackled by means of the
spectral graph theory. The core of this theory is the eigenvalue decomposition
of the Laplacian matrix of the weighted graph obtained from data. In fact,
there is a close relationship between the second smallest eigenvalue of the
Laplacian and the graph cut .
The aim of this paper is to present a survey of kernel and spectral clustering
methods. Moreover, an explicit proof of the fact that these two approaches
have the same mathematical foundation is reported. In particular it has been
shown by Dhillon et al. that Kernel K-Means and spectral clustering with the
ratio association as the objective function are perfectly equivalent .
The core of both approaches lies in their ability to construct an adjacency
structure between data avoiding to deal with a preﬁxed shape of clusters.
These approaches have a slight similarity with hierarchical methods in the use
of an adjacency structure with the main diﬀerence in the philosophy of the
grouping procedure.
A comparison of some spectral clustering methods has been recently proposed
in , while there are some theoretical results on the capabilities and convergence properties of spectral methods for clustering . Recently
kernel methods have been applied to Fuzzy c-Varieties also with the aim
of ﬁnding varieties in feature space and there are some interesting clustering
methods using kernels such as and .
Since the choice of the kernel and of the similarity measure is crucial in these
methods, many techniques have been proposed in order to learn automatically
the shape of kernels from data as in .
Regarding the applications, most of these algorithms (e.g., ) have
been applied to standard benchmarks such as Ionosphere , Breast Cancer and Iris 1 . Kernel Fuzzy c-Means proposed in has been
applied in image segmentation problems while in it has been applied
in handwritten digits recognition. There are applications of kernel clustering
methods in face recognition using kernel SOM , in speech recognition 
and in prediction of crop yield from climate and plantation data . Spectral
methods have been applied in clustering of artiﬁcial data , in image
segmentation , in bioinformatics , and in co-clustering problems
of words and documents and genes and conditions . A semi-supervised
spectral approach to bioinformatics and handwritten character recognition
have been proposed in . The protein sequence clustering problem has been
faced using spectral techniques in and kernel methods in .
In the next section we brieﬂy introduce the concepts of linear partitioning
methods by recalling some basic crisp and fuzzy algorithms. Then the paper is
organized as follows: section 3 shows the kernelized version of the algorithms
presented in section 2, in section 4 we discuss spectral clustering, while in
section 5 we report the equivalence between spectral and kernel clustering
methods. In the last section conclusions are drawn.
Partitioning Methods
In this section we brieﬂy recall some basic facts about partitioning clustering
methods and we will report the clustering methods for which a kernel version
has been proposed. Let X = {x1, . . . , xn} be a data set composed by n patterns
for which every xi ∈Rd. The codebook (or set of centroids) V is deﬁned as the
set V = {v1, . . . , vc}, typically with c ≪n. Each element vi ∈Rd is called
codevector (or centroid or prototype) 2 .
The Voronoi region Ri of the codevector vi is the set of vectors in Rd for which
1 These data sets can be found at: ftp://ftp.ics.uci.edu/pub/machine-learningdatabases/
2 Among many terms to denote such objects, we will use codevectors as in vector
quantization theory.
vi is the nearest vector:
i = arg min
It is possible to prove that each Voronoi region is convex and the boundaries of the regions are linear segments.
The deﬁnition of the Voronoi set πi of the codevector vi is straightforward. It
is the subset of X for which the codevector vi is the nearest vector:
i = arg min
that is, the set of vectors belonging to Ri. A partition on Rd induced by all
Voronoi regions is called Voronoi tessellation or Dirichlet tessellation.
Figure 2. An example of Voronoi tessellation where each black point is a codevector.
Batch K-Means
A simple algorithm able to construct a Voronoi tessellation of the input space
was proposed in 1957 by Lloyd and it is known as batch K-Means. Starting
from the ﬁnite data set X this algorithm moves iteratively the k codevectors to
the arithmetic mean of their Voronoi sets {πi}i=1,...,k. Theoretically speaking, a
necessary condition for a codebook V to minimize the Empirical Quantization
is that each codevector vi fulﬁlls the centroid condition . In the case of a
ﬁnite data set X and with Euclidean distance, the centroid condition reduces
Batch K-Means is formed by the following steps:
(1) choose the number k of clusters;
(2) initialize the codebook V with vectors randomly picked from X;
(3) compute the Voronoi set πi associated to the codevector vi;
(4) move each codevector to the mean of its Voronoi set using Eq. 4;
(5) return to step 3 if any codevector has changed otherwise return the codebook.
At the end of the algorithm a codebook is found and a Voronoi tessellation
of the input space is provided. It is guaranteed that after each iteration the
quantization error does not increase. Batch K-Means can be viewed as an
Expectation-Maximization algorithm, ensuring the convergence after a ﬁnite number of steps.
This approach presents many disadvantages . Local minima of E(X) make
the method dependent on initialization, and the average is sensitive to outliers.
Moreover, the number of clusters to ﬁnd must be provided, and this can be
done only using some a priori information or additional validity criterion.
Finally, K-Means can deal only with clusters with spherically symmetrical
point distribution, since Euclidean distances of patterns from centroids are
computed leading to a spherical invariance. Diﬀerent distances lead to diﬀerent
invariance properties as in the case of Mahalanobis distance which produces
invariance on ellipsoids .
The term batch means that at each step the algorithm takes into account
the whole data set to update the codevectors. When the cardinality n of
the data set X is very high (e.g., several hundreds of thousands) the batch
procedure is computationally expensive. For this reason an on-line update has
been introduced leading to the on-line K-Means algorithm . At each
step, this method simply randomly picks an input pattern and updates its
nearest codevector, ensuring that the scheduling of the updating coeﬃcient is
adequate to allow convergence and consistency.
Self Organizing Maps - SOM
A Self Organizing Map (SOM ) also known as Self Organizing Feature
Map (SOFM ) represents data by means of codevectors organized on a grid
with ﬁxed topology. Codevectors move to adapt to the input distribution,
but adaptation is propagated along the grid also to neighboring codevectors,
according to a given propagation or neighborhood function. This eﬀectively
constrains the evolution of codevectors. Grid topologies may diﬀer, but in
this paper we consider a two-dimensional, square-mesh topology . The
distance on the grid is used to determine how strongly a codevector is adapted
when the unit aij is the winner. The metric used on a rectangular grid is the
Manhattan distance, for which the distance between two elements r = (r1, r2)
and s = (s1, s2) is:
drs = |r1 −s1| + |r2 −s2| .
The SOM algorithm is the following:
(1) Initialize the codebook V randomly picking from X
(2) Initialize the set C of connections to form the rectangular grid of dimension n1 × n2
(3) Initialize t = 0
(4) Randomly pick an input x from X
(5) Determine the winner
s(x) = arg min
vj∈V ∥x −vj∥
(6) Adapt each codevector:
∆vj = ǫ(t)h(drs)(x −vj)
where h is a decreasing function of d as for instance:
h(drs) = exp
(7) Increment t
(8) if t < tmax go to step 4
σ(t) and ǫ(t) are decreasing functions of t, for example :
where σi, σf and ǫi, ǫf are the initial and ﬁnal values for the functions σ(t) and
A ﬁnal note on the use of SOM for clustering. The method was originally
devised as a tool for embedding multidimensional data into typically two dimensional spaces, for data visualization. Since then, it has also been frequently
used as a clustering method, which was originally not considered appropriate
because of the constraints imposed by the topology. However, the topology
itself serves an important purpose, namely, that of limiting the ﬂexibility of
the mapping in the ﬁrst training cycles, and gradually increasing it (while
decreasing the magnitude of updates to ensure convergence) as more cycles
were performed. The strategy is similar to that of other algorithms, including
these described in the following, in the capacity control of the method which
has the eﬀect of avoiding local minima. This accounts for the fast convergence
often reported in experimental works.
Neural Gas
Another technique that tries to minimize the distortion error is the neural gas
algorithm , based on a soft adaptation rule. This technique resembles the
SOM in the sense that not only the winner codevector is adapted. It is diﬀerent
in that codevectors are not constrained to be on a grid, and the adaptation of
the codevectors near the winner is controlled by a criterion based on distance
ranks. Each time a pattern x is presented, all the codevectors vj are ranked
according to their distance to x (the closest obtains the lowest rank). Denoting
with ρj the rank of the distance between x and the codevector vj, the update
∆vj = ǫ(t)hλ(ρj)(x −vj)
with ǫ(t) ∈ gradually lowered as t increases and hλ(ρj) a function decreasing with ρj with a characteristic decay λ; usually hλ(ρj) = exp (−ρj/λ).
The Neural Gas algorithm is the following:
(1) Initialize the codebook V randomly picking from X
(2) Initialize the time parameter t = 0
(3) Randomly pick an input x from X
(4) Order all elements vj of V according to their distance to x, obtaining the
(5) Adapt the codevectors according to Eq. 10
(6) Increase the time parameter t = t + 1
(7) if t < tmax go to step 3.
Fuzzy clustering methods
Bezdek introduced the concept of hard and fuzzy partition in order to
extend the notion of membership of pattern to clusters. The motivation of
this extension is related to the fact that a pattern often cannot be thought of
as belonging to a single cluster only. In many cases, a description in which the
membership of a pattern is shared among clusters is necessary.
Deﬁnition 2.1 Let Acn denote the vector space of c × n real matrices over
R. Considering X, Acn and c ∈N such that 2 ≤c < n, the Fuzzy c-partition
space for X is the set:
uih ∈ ∀i, h;
uih = 1 ∀h ; 0 <
uih < n ∀i
The matrix U is the so called membership matrix since each element uih is
the fuzzy membership of the h-th pattern to the i-th cluster. The deﬁnition of
Mfc simply tells that the sum of the memberships of a pattern to all clusters
is one (probabilistic constraint) and that a cluster cannot be empty or contain
all patterns. This deﬁnition generalizes the notion of hard c-partitions in .
The mathematical tool used in all these methods for working out the solution
procedure is the Lagrange multipliers technique. In particular a minimization of the intraclusters distance functional with a probabilistic constraint
on the memberships of a pattern to all clusters has to be achieved. Since all
the functionals involved in these methods depend on both memberships and
codevectors, the optimization is iterative and follows the so called Picard iterations method where each iteration is composed of two steps. In the ﬁrst
step a subset of variables (memberships) is kept ﬁxed and the optimization is
performed with respect to the remaining variables (codevectors) while in the
second one the role of the ﬁxed and moving variables is swapped. The optimization algorithm stops when variables change less than a ﬁxed threshold.
Fuzzy c-Means
The Fuzzy c-Means algorithm identiﬁes clusters as fuzzy sets. It minimizes
the functional:
J(U, V ) =
(uih)m ∥xh −vi∥2
with respect to the membership matrix U and the codebook V with the probabilistic constraints:
∀i = 1, . . . , n .
The parameter m controls the fuzziness of the memberships and usually it is
set to two; for high values of m the algorithm tends to set all the memberships
equals while for m tending to one we obtain the K-Means algorithm where
the memberships are crisp. The minimization of Eq. 12 is done introducing a
Lagrangian function for each pattern for which the constraint is in Eq. 13.
(uih)m ∥xh −vi∥2 + αh
Then the derivatives of the sum of the Lagrangian are computed with respect
to the uih and vi and are set to zero. This yields the iteration scheme of these
equations:
h=1 (uih)m xh
h=1 (uih)m
At each iteration it is possible to evaluate the amount of change of the memberships and codevectors and the algorithm can be stopped when these quantities
reach a predeﬁned threshold. At the end a soft partitioning of the input space
is obtained.
Possibilistic clustering methods
As a further modiﬁcation of the K-Means algorithm, the possibilistic approach relaxes the probabilistic constraint on the membership of a
pattern to all clusters. In this way a pattern can have a low membership to
all clusters in the case of outliers, whereas for instance, in the situation of
overlapped clusters, it can have high membership to more than one cluster. In
this framework the membership represents a degree of typicality not depending on the membership values of the same pattern to other clusters. Again the
optimization procedure is the Picard iteration method, since the functional
depends both on memberships and codevectors.
Possibilistic c-Means
There are two formulations of the Possibilistic c-Means, that we will call PCM-
I and PCM-II . The ﬁrst one aims to minimize the following functional
with respect to the membership matrix U and the codebook V = {v1, . . . , vc}:
J(U, V ) =
(uih)m ∥xh −vi∥2 +
(1 −uih)m ,
while the second one addresses the functional:
J(U, V ) =
uih∥xh −vi∥2 +
(uih ln(uih) −uih) .
The minimization of Eq. 17 and Eq. 18 with respect to the uih leads respectively to the following equations:
−∥xh −vi∥2
The constraint on the memberships uih ∈ is automatically satisﬁed given
the form assumed by Eq. 19 and Eq. 20. The updates of the centroids for
PCM-I and PCM-II are respectively:
h=1 (uih)m xh
h=1 (uih)m
The parameter ηi regulates the trade-oﬀbetween the two terms in Eq. 17 and
Eq. 18 and it is related to the width of the clusters. The authors suggest to
estimate ηi for PCM-I using this formula:
h=1 (uih)m ∥xh −vi∥2
h=1 (uih)m
which is a weighted mean of the intracluster distance of the i-th cluster and
the constant γ is typically set at one. The parameter ηi can be estimated with
scale estimation techniques as developed in the robust clustering literature
for M-estimators . The value of ηi can be updated at each step of the
algorithm or can be ﬁxed for all iterations. The former approach can lead to
instabilities since the derivation of the equations has been obtained considering
ηi ﬁxed. In the latter case a good estimation of ηi can be done only starting
from an approximate solution. For this reason often the Possibilistic c-Means
is run as a reﬁning step of a Fuzzy c-Means.
Kernel Clustering Methods
In machine learning, the use of the kernel functions has been introduced
by Aizerman et al. in 1964. In 1995 Cortes and Vapnik introduced Support
Vector Machines (SVMs) which perform better than other classiﬁcation
algorithms in several problems. The success of SVM has brought to extend the
use of kernels to other learning algorithms (e.g., Kernel PCA ). The choice
of the kernel is crucial to incorporate a priori knowledge on the application,
for which it is possible to design ad hoc kernels.
Mercer kernels
We recall the deﬁnition of Mercer kernels , considering, for the sake of
simplicity, vectors in Rd instead of Cd.
Deﬁnition 3.1 Let X = {x1, . . . , xn} be a nonempty set where xi ∈Rd. A
function K : X ×X →R is called a positive deﬁnite kernel (or Mercer kernel)
if and only if K is symmetric (i.e. K(xi, xj) = K(xj, xi)) and the following
equation holds:
cicjK(xi, xj) ≥0
where cr ∈R ∀r = 1, . . . , n
Each Mercer kernel can be expressed as follows:
K(xi, xj) = Φ(xi) · Φ(xj) ,
where Φ : X →F performs a mapping from the input space X to a high
dimensional feature space F. One of the most relevant aspects in applications
is that it is possible to compute Euclidean distances in F without knowing
explicitly Φ. This can be done using the so called distance kernel trick :
∥Φ(xi) −Φ(xj)∥2 = (Φ(xi) −Φ(xj)) · (Φ(xi) −Φ(xj))
= Φ(xi) · Φ(xi) + Φ(xj) · Φ(xj) −2Φ(xi) · Φ(xj)
= K(xi, xi) + K(xj, xj) −2K(xi, xj)
in which the computation of distances of vectors in feature space is just a
function of the input vectors. In fact, every algorithm in which input vectors
appear only in dot products with other input vectors can be kernelized .
In order to simplify the notation we introduce the so called Gram matrix K
where each element kij is the scalar product Φ(xi) · Φ(xi). Thus, Eq. 26 can
be rewritten as:
∥Φ(xi) −Φ(xj)∥2 = kii + kjj −2kij .
Examples of Mercer kernels are the following :
K(l)(xi, xj) = xi · xj
• polynomial of degree p:
K(p)(xi, xj) = (1 + xi · xj)p
• Gaussian:
K(g)(xi, xj) = exp
−∥xi −xj∥2
It is important to stress that the use of the linear kernel in Eq. 26 simply leads
to the computation of the Euclidean norm in the input space. Indeed:
∥xi −xj∥2 = xi · xi + xj · xj −2xi · xj
= K(l)(xi, xi) + K(l)(xj, xj) −2K(l)(xi, xj)
= ∥Φ(xi) −Φ(xj)∥2 ,
shows that choosing the kernel K(l) implies Φ = I (where I is the identity
function). Following this consideration we can think that kernels can oﬀer a
more general way to represent the elements of a set X and possibly, for some
of these representations, the clusters can be easily identiﬁed.
In literature there are some applications of kernels in clustering. These methods can be broadly divided in three categories, which are based respectively
• kernelization of the metric ;
• clustering in feature space ;
• description via support vectors .
Methods based on kernelization of the metric look for centroids in input space
and the distances between patterns and centroids is computed by means of
∥Φ(xh) −Φ(vi)∥2 = K(xh, xh) + K(vi, vi) −2K(xh, vi) .
Clustering in feature space is made by mapping each pattern using the function
Φ and then computing centroids in feature space. Calling vΦ
i the centroids in
feature space, we will see in the next sections that it is possible to compute
the distances
2 by means of the kernel trick.
The description via support vectors makes use of One Class SVM to ﬁnd a
minimum enclosing sphere in feature space able to enclose almost all data in
feature space excluding outliers. The computed hypersphere corresponds to
nonlinear surfaces in input space enclosing groups of patterns. The Support
Vector Clustering algorithm allows to assign labels to patterns in input space
enclosed by the same surface. In the next subsections we will outline these
three approaches.
Kernel K-Means
Given the data set X, we map our data in some feature space F, by means of a
nonlinear map Φ and we consider k centers in feature space (vΦ
i ∈F with i =
1, . . . , k) . We call the set V Φ = (vΦ
1 , . . . , vΦ
k ) Feature Space Codebook
since in our representation the centers in the feature space play the same role
of the codevectors in the input space. In analogy with the codevectors in the
input space, we deﬁne for each center vΦ
i its Voronoi Region and Voronoi Set
in feature space. The Voronoi Region in feature space (RΦ
i ) of the center vΦ
is the set of all vectors in F for which vΦ
i is the closest vector
i = arg min
The Voronoi Set in Feature Space πΦ
i of the center vΦ
i is the set of all vectors
x in X such that vΦ
i is the closest vector to their images Φ(x) in the feature
i = arg min
The set of the Voronoi Regions in feature space deﬁne a Voronoi Tessellation
of the Feature Space. The Kernel K-Means algorithm has the following steps:
(1) Project the data set X into a feature space F, by means of a nonlinear
mapping Φ.
(2) Initialize the codebook V Φ = (vΦ
1 , . . . , vΦ
k ) with vΦ
(3) Compute for each center vΦ
i the set πΦ
(4) Update the codevectors vΦ
(5) Go to step 3 until any vΦ
(6) Return the feature space codebook.
This algorithm minimizes the quantization error in feature space.
Since we do not know explicitly Φ it is not possible to compute directly Eq. 35.
Nevertheless, it is always possible to compute distances between patterns and
codevectors by using the kernel trick, allowing to obtain the Voronoi sets in
feature space πΦ
i . Indeed, writing each centroid in feature space as a combination of data vectors in feature space we have:
γjhΦ(xh) ,
where γjh is one if xh ∈πΦ
j and zero otherwise. Now the quantity:
can be expanded by using the scalar product and the kernel trick in Eq. 26:
γjrγjskrs .
This allows to compute the closest feature space codevector for each pattern
and to update the coeﬃcients γjh. It is possible to repeat these two operations
until any γjh changes to obtain a Voronoi tessellation of the feature space.
An on-line version of the kernel K-Means algorithm can be found in . A further version of K-Means in feature space has been proposed by Girolami .
In his formulation the number of clusters is denoted by c and a fuzzy membership matrix U is introduced. Each element uih denotes the fuzzy membership
of the point xh to the Voronoi set πΦ
i . This algorithm tries to minimize the
following functional with respect to U:
JΦ(U, V Φ) =
The minimization technique used by Girolami is Deterministic Annealing 
which is a stochastic method for optimization. A parameter controls the fuzziness of the membership during the optimization and can be thought proportional to the temperature of a physical system. This parameter is gradually
lowered during the annealing and at the end of the procedure the memberships
have become crisp; therefore a tessellation of the feature space is found. This
linear partitioning in F, back to the input space, forms a nonlinear partitioning of the input space.
Kernel SOM
The kernel version of the SOM algorithm is based on the distance
kernel trick. The method tries to adapt the grid of codevectors vΦ
j in feature
space. In kernel SOM we start writing each codevector as a combination of
points in feature space:
γjhΦ(xh) ,
where the coeﬃcients γih are initialized once the grid is created. Computing
the winner by writing Eq. 6 in feature space leads to:
s(Φ(xi)) = arg min
j ∈V ∥Φ(xi) −vΦ
that can be written, using the kernel trick:
s(Φ(xi)) = arg min
To update the codevectors we rewrite Eq. 7:
j + ǫ(t)h(drs)
Using Eq. 40:
γjhΦ(xh) + ǫ(t)h(drs)
Thus the rule for the update of γjh is:
(1 −ǫ(t)h(drs))γjh
(1 −ǫ(t)h(drs))γjh + ǫ(t)h(drs) otherwise.
Kernel Neural Gas
The Neural Gas algorithm provides a soft update rule for the codevectors in
input space. The kernel version of neural gas applies the soft rule for the
update to the codevectors in feature space. Rewriting Eq. 10 in feature space
for the update of the codevectors we have:
j = ǫhλ(ρj)
Here ρj is the rank of the distance ∥Φ(x) −vΦ
j ∥. Again it is possible to write
j as a linear combination of Φ(xi) as in Eq. 40, allowing to compute such
distances by means of the kernel trick. As in the kernel SOM technique, the
updating rule for the centroids becomes an updating rule for the coeﬃcients
of such combination.
One Class SVM
This approach provides a support vector description in feature space .
The idea is to use kernels to project data into a feature space and then to ﬁnd
the sphere enclosing almost all data, namely not including outliers. Formally
a radius R and the center v of the smallest enclosing sphere in feature space
are deﬁned. The constraint is thus:
∥Φ(xj) −v∥2 ≤R2 + ξj
where the non negative slack variables ξj have been added. The Lagrangian
for this problem is deﬁned :
(R2 + ξj −∥Φ(xj) −v∥2)βj −
where βj ≥0 and µj ≥0 are Lagrange multipliers, C is a constant and C
is a penalty term. Computing the partial derivative of L with respect to R, v
and ξj and setting them to zero leads to the following equations:
βj = C −µj.
The Karush-Kuhn-Tucker (KKT) complementary conditions result in:
(R2 + ξj −∥Φ(xj) −v∥2)βj = 0.
Following simple considerations regarding all these conditions it is possible to
• when ξj > 0, the image of xj lies outside the hypersphere. These points are
called bounded support vectors;
• when ξj = 0 and 0 < βj < C, the image of xj lies on the surface of the
hypersphere. These points are called support vectors.
Moreover, it is possible to write the Wolfe dual form , whose optimization
leads to this quadratic programming problem with respect to the βj:
The distance from the image of a point xj and the center v of the enclosing
sphere can be computed as follows:
dj = ∥Φ(xj) −v∥2 = kjj −2
In Fig. 3 it is possible to see the ability of this algorithm to ﬁnd the smallest
enclosing sphere without outliers.
Support Vector Clustering
Once boundaries in input space are found, a labeling procedure is necessary in
order to complete clustering. In the cluster assignment procedure follows
a simple geometric idea. Any path connecting a pair of points belonging to
diﬀerent clusters must exit from the enclosing sphere in feature space. Denoting with Y the image in feature space of one of such paths and with y the
elements of Y , it will result that R(y) > R for some y. Thus it is possible to
Figure 3. One class SVM applied to two data sets with outliers. The gray line shows
the projection in input space of the smallest enclosing sphere in feature space. In
(a) a linear kernel and in (b) a Gaussian kernel have been used.
deﬁne an adjacency structure in this form:
if R(y) < R
otherwise.
Clusters are simply the connected components of the graph with the adjacency
matrix just deﬁned. In the implementation in the check is made sampling
the line segment Y in 20 equidistant points. There are some modiﬁcations on
this labeling algorithm (e.g., ) that improve performances. An improved
version of SVC algorithm with application in handwritten digits recognition
can be found in .
Camastra and Verri algorithm
A technique combining K-Means and One Class SVM can be found in .
The algorithm uses a K-Means-like strategy, i.e., moves repeatedly all centers
i in the feature space, computing One Class SVM on their Voronoi sets πΦ
until no center changes anymore. Moreover, in order to introduce robustness
against outliers, the authors have proposed to compute One Class SVM on
i (ρ) of each center vΦ
i . The set πΦ
i (ρ) is deﬁned as
i (ρ) = {xj ∈πΦ
i and ∥Φ(xj) −vΦ
i (ρ) is the Voronoi set in the feature space of the center vΦ
i without outliers,
that is the images of data points whose distance from the center is larger than
ρ. The parameter ρ can be set up using model selection techniques (e.g.,
cross-validation). In summary, the algorithm has the following steps:
(1) Project the data Set X into a feature space F, by means of a nonlinear
mapping Φ.
(2) Initialize the codebook V Φ = (vΦ
1 , . . . , vΦ
k ) with vΦ
(3) Compute πΦ
i (ρ) for each center vΦ
(4) Apply One Class SVM to each πΦ
i (ρ) and assign the center obtained to
(5) Go to step 2 until any vΦ
(6) Return the feature space codebook.
Kernel fuzzy clustering methods
Here we show some kernelized versions of Fuzzy c-Means algorithms, showing
in particular Fuzzy and Possibilistic c-Means. In the ﬁrst subsection we show
the method of the kernelization of the metric while in the second one the
Fuzzy c-Means in feature space is shown. The third subsection is devoted to
the kernelized version of the Possibilistic c-Means.
Kernel Fuzzy c-Means with kernelization of the metric
The basic idea is to minimize the functional :
JΦ(U, V ) =
(uih)m ∥Φ(xh) −Φ(vi)∥2 ,
with the probabilistic constraint over the memberships (Eq. 13). The procedure for the optimization of JΦ(U, V ) is again the Picard iteration technique.
Minimization of the functional in Eq. 55 has been proposed only in the case
of a Gaussian kernel K(g). The reason is that the derivative of JΦ(U, V ) with
respect to the vi using a Gaussian kernel is particularly simple since it allows
to use the kernel trick:
∂K(xh, vi)
= (xh −vi)
K(xh, vi) .
We obtain for the memberships:
1 −K(xh, vi)
1 −K(xh, vj)
and for the codevectors:
h=1 (uih)m K(xh, vi)xh
h=1 (uih)m K(xh, vi)
Kernel Fuzzy c-Means in feature space
Here we derive the Fuzzy c-Means in feature space, which is a clustering
method which allows to ﬁnd a soft linear partitioning of the feature space.
This partitioning, back to the input space, results in a soft nonlinear partitioning of data. The functional to optimize with the probabilistic
constraint in Eq. 13 is:
JΦ(U, V Φ) =
It is possible to rewrite the norm in Eq. 59 explicitly by using:
h=1 (uih)m Φ(xh)
h=1 (uih)m
(uih)m Φ(xh) ,
which is the kernel version of Eq. 16. For simplicity of notation we used:
Now it is possible to write the kernel version of Eq. 15:
(uir)m khr + a2
(uir)m (uis)m krs
(ujr)m khr + a2
(ujr)m (ujs)m krs
Eq. 62 gives the rule for the update of the membership uih.
Possibilistic c-Means with the kernelization of the metric
The formulation of the Possibilistic c-Means PCM-I with the kernelization of
the metric used in involves the minimization of the following functional:
JΦ(U, V ) =
(uih)m ∥Φ(xh) −Φ(vi)∥2 +
Minimization leads to:
∥Φ(xh) −Φ(vi)∥2
that can be rewritten, considering a Gaussian kernel, as:
ih = 1 + 2
1 −K(xh, vi)
The update of the codevectors follows:
h=1 (uih)m K(xh, vi)xh
h=1 (uih)m K(xh, vi)
The computation of the ηi is straightforward.
Spectral Clustering
Spectral clustering methods have a strong connection with graph theory . A comparison of some spectral clustering methods has been recently proposed in . Let X = {x1, . . . , xn} be the set of patterns to cluster.
Starting from X, we can build a complete, weighted undirected graph G(V, A)
having a set of nodes V = {v1, . . . , vn} corresponding to the n patterns and
edges deﬁned through the n × n adjacency (also aﬃnity) matrix A. The adjacency matrix for a weighted graph is given by the matrix whose element aij
represents the weight of the edge connecting nodes i and j. Being an undirected graph, the property aij = aji holds. Adjacency between two patterns
can be deﬁned as follows:
h(xi, xj) if i ̸= j
otherwise.
The function h measures the similarity between patterns and typically a Gaussian function is used:
h(xi, xj) = exp
−d(xi, xj)
where d measures the dissimilarity between patterns and σ controls the rapidity of decay of h. This particular choice has the property that A has only
some terms signiﬁcantly diﬀerent from 0, i.e., it is sparse.
The degree matrix D is the diagonal matrix whose elements are the degrees
of the nodes of G.
In this framework the clustering problem can be seen as a graph cut problem where one wants to separate a set of nodes S ⊂V from the complementary set ¯S = V \ S. The graph cut problem can be formulated in several
ways depending on the choice of the function to optimize. One of the most
popular functions to optimize is the cut :
cut(S, ¯S) =
vi∈S,vj∈¯S
It is easy to verify that the minimization of this objective function favors partitions containing isolated nodes. To achieve a better balance in the cardinality
of S and ¯S it is suggested to optimize the normalized cut :
Ncut(S, ¯S) = cut(S, ¯S)
assoc(S, V ) +
assoc( ¯S, V )
where the association assoc(S, V ) is also known as the volume of S:
assoc(S, V ) =
aij ≡vol(S) =
There are other deﬁnitions of functions to optimize (e.g., the conductance ,
the normalized association , ratio cut ).
The complexity in optimizing these objective functions is very high (e.g., the
optimization of the normalized cut is a NP-hard problem ) and for
this reason it has been proposed to relax it by using spectral concepts of
graph analysis. This relaxation can be formulated by introducing the Laplacian
matrix :
L = D −A ,
which can be seen as a linear operator on G. In addition to this deﬁnition of
Laplacian there are alternative deﬁnitions:
• Normalized Laplacian LN = D−1
• Generalized Laplacian LG = D−1L
• Relaxed Laplacian Lρ = L −ρD
Each deﬁnition is justiﬁed by special properties desirable in a given context.
The spectral decomposition of the Laplacian matrix can give useful information about the properties of the graph. In particular it can be seen that
the second smallest eigenvalue of L is related to the graph cut and the
corresponding eigenvector can cluster together similar patterns .
Spectral approach to clustering has a strong connection with Laplacian Eigenmaps . The dimensionality reduction problem aims to ﬁnd a proper low
dimensional representation of a data set in a high dimensional space. In ,
each node in the graph, which represents a pattern, is connected just with
nodes corresponding to neighboring patterns and the spectral decomposition
of the Laplacian of the obtained graph permits to ﬁnd a low dimensional representation of X. The authors point out the close connection with spectral
clustering and Local Linear Embedding providing theoretical and experimental validations.
Shi and Malik algorithm
The algorithm proposed by Shi and Malik. applies the concepts of spectral
clustering to image segmentation problems. In this framework each node is
a pixel and the deﬁnition of adjacency between them is suitable for image
segmentation purposes. In particular, if xi is the position of the i-th pixel
and fi a feature vector which takes into account several of its attributes (e.g.,
intensity, color and texture information), they deﬁne the adjacency as:
−∥fi −fj∥2
if ∥xi −xj∥< R
otherwise.
Here R has an inﬂuence on how many neighboring pixels can be connected
with a pixel, controlling the sparsity of the adjacency and Laplacian matrices.
They provide a proof that the minimization of Ncut(S, ¯S) can be done solving
the eigenvalue problem for the normalized Laplacian LN. In summary, the
algorithm is composed of these steps:
(1) Construct the graph G starting from the data set X calculating the adjacency between patterns using Eq. 74
(2) Compute the degree matrix D
(3) Construct the matrix LN = D−1
(4) Compute the eigenvector e2 associated to the second smallest eigenvalue
(5) Use D−1
2e2 to segment G
In the ideal case of two non connected subgraphs, D−1
2e2 assumes just two
values; this allows to cluster together the components of D−1
2e2 with the same
value. In a real case the splitting point must be chosen to cluster the components of D−1
2e2 and the authors suggest to use the median value, zero or
the value for which the clustering gives the minimum Ncut. The successive
partitioning can be made recursively on the obtained sub-graphs or it is possible to use more than one eigenvector. An interesting approach for clustering
simultaneously the data set in more than two clusters can be found in .
Ng, Jordan and Weiss algorithm
The algorithm that has been proposed by Ng et al. uses the adjacency
matrix A as Laplacian. This deﬁnition allows to consider the eigenvector associated with the largest eigenvalues as the “good” one for clustering. This
has a computational advantage since the principal eigenvectors can be computed for sparse matrices eﬃciently using the power iteration technique. The
idea is the same as in other spectral clustering methods, i.e., one ﬁnds a new
representation of patterns on the ﬁrst k eigenvectors of the Laplacian of the
The algorithm is composed of these steps:
(1) Compute the aﬃnity matrix A ∈Rn×n:
(2) Construct the matrix D
(3) Compute a normalized version of A, deﬁning this Laplacian:
(4) Find the k eigenvectors {e1, . . . , ek} of L associated to the largest eigenvalues {λ1, . . . , λk}.
(5) Form the matrix Z by stacking the k eigenvectors in columns.
(6) Compute the matrix Y by normalizing each of the Z’s rows to have unit
In this way all the original points are mapped into a unit hypersphere.
(7) In this new representation of the original n points apply a clustering
algorithm that attempts to minimize distortion such as K-means.
As a criterion to choose σ they suggest to use the value that guarantees the
minimum distortion when the clustering stage is performed on Y . They tested
this algorithm on artiﬁcial data sets showing the capability of the algorithm
to separate nonlinear structures.Here we show the steps of the algorithm when
applied to the data set in Fig. 1. Once the singular value decomposition of L
is computed, we can see the matrices Z and Y in Fig. 4 (here obtained with
σ = 0.4). Once Y is computed, it is easy to cluster the two groups of points
Figure 4. (a) The matrix Z obtained with the ﬁrst two eigenvectors of the matrix
L. (b) The matrix Y obtained by normalizing the rows of Z clustered by K-means
algorithm with two centroids.
obtaining the result shown in Fig. 5.
Figure 5. The result of the Ng and Jordan algorithm on the ring data set.
Other Methods
An interesting view of spectral clustering is provided by Meil˘a et al. who
describe it in the framework of Markov random walks leading to a diﬀerent interpretation of the graph cut problem. It is known, from the theory of
Markov random walks, that if we construct the stochastic matrix P = D−1A,
each element pij represents the probability of moving from node i to node j. In
their work they provide an explicit connection between the spectral decomposition of L and P showing that both have the same solution with eigenvalues
of P equal to 1 −λi where λi are the eigenvalues of L. Moreover they propose a method to learn a function of the features able to produce a correct
segmentation starting from a segmented image.
An interesting study on spectral clustering has been conducted by Kannan
et al. . The authors exploit the objective function with respect to some
artiﬁcial data sets showing that there is no objective function able to properly
cluster every data set. In other words there always exists some data set for
which the optimization of a particular objective function has some drawback.
For this reason they propose a bi-criteria objective function. These two objectives are respectively based on the conductance and the ratio between the
auto-association of a subset of nodes S and its volume. Again the relaxation
of this problem is achieved by the decomposition of the Laplacian of the graph
associated to the data set.
A Uniﬁed View of Spectral and Kernel Clustering Methods
Recently a possible connection between unsupervised kernel algorithms and
spectral methods has been studied to ﬁnd whether these two seemingly diﬀerent approaches can be described under a more general framework. The hint
for this unifying theory lies the adjacency structure constructed by both these
approaches. In the spectral approach there is an adjacency between patterns
which is the analogous of the kernel functions in kernel methods.
A direct connection between Kernel PCA and spectral methods has been
shown . More recently a unifying view of kernel K-means and spectral
clustering methods has been pointed out. In this section we show explicitly the equivalence between them highlighting that these two approaches
have the same foundation and in particular that both can be viewed as a
matrix trace maximization problem.
Kernel clustering methods objective
To show the direct equivalence between kernel and spectral clustering methods
we introduce the weighted version of the kernel K-means . We introduce
a weight matrix W having weights wk on the diagonal. Recalling that we
denote with πi the i-th cluster we have that the functional to minimize is the
following:
JΦ(W, V Φ) =
xk∈πi wkΦ(xk)
xk∈πi wkΦ(xk)
where we have introduced:
Now let’s deﬁne the matrix Z having:
otherwise.
Since the columns of Z are mutually orthogonal it is easy to verify that:
= (ZTZ)ii ,
and that only the diagonal elements are not null.
Now we denote with F the matrix whose columns are the Φ(xk). It is easy to
verify that the matrix FW yields a matrix whose columns are the wkΦ(xk).
Moreover the expression FWZZT gives a matrix having n columns which are
the nearest centroids in feature space of the Φ(xk).
Thus, substituting Eq. 79 in Eq. 78 we obtain the following matrix expression
for JΦ(W, V Φ):
JΦ(W, V Φ) =
F·k −(FWZZT)·k
Here the dot has to be considered as a selection of the k-th column of the
matrices. Introducing the matrix Y = W 1/2Z, which is orthonormal (Y TY =
I), the objective function can be rewritten as:
JΦ(W, V Φ) =
F·k −(FW 1/2Y Y TW −1/2)·k
FW 1/2 −FW 1/2Y Y T
where the norm ∥∥F is the Frobenius norm . Using the fact that ∥A∥F =
tr(AAT) and the properties of the trace, it is possible to see that the minimization of the last equation is equivalent to the maximization of the following :
JΦ(W, V Φ) = tr(Y TW 1/2F TFW 1/2Y )
Spectral clustering methods objective
Recalling that the deﬁnition of association between two sets of edges S and T
of a weighted graph is the following:
assoc(S, T) =
it is possible to deﬁne many objective functions to optimize in order to perform
clustering. Here, for the sake of simplicity, we consider just the ratio association
problem, where one has to maximize:
J(S1, . . . , Sc) =
assoc(Si, Si)
where |Si| is the size of the i-th partition. Now we introduce the indicator
vector zi whose k-th value is zero if xk ̸∈πi and one otherwise. Rewriting the
last equation in a matrix form we obtain the following:
J(S1, . . . , Sc) =
Normalizing the zi letting:
we obtain:
J(S1, . . . , Sc) =
i Ayi = tr(Y TAY )
A uniﬁed view of the two approaches
Comparing Eq. 90 and Eq. 85 it is possible to see the perfect equivalence
between kernel K-means and the spectral approach to clustering when one
wants to maximize the ratio association. To this end, indeed, it is enough to
set the weights in the weighted kernel K-means equal to one obtaining the
classical kernel K-means. It is possible to obtain more general results when
one wants to optimize other objective functions in the spectral approach, such
as the ratio cut , the normalized cut and the Kernighan-Lin objective.
For instance, in the case of the minimization of the normalized cut which is
one of the most used objective functions, the functional to minimize is:
J(S1, . . . , Sc) = tr(Y TD−1/2AD−1/2Y )
Thus the correspondence with the objective in the kernel K-means imposes
to choose Y = D1/2Z, W = D and K = D−1AD−1. It is worth noting that
for an arbitrary A it is not guaranteed that D−1AD−1 is deﬁnite positive. In
this case the kernel K-means will not necessarily converge. To cope with this
problem in the authors propose to enforce positive deﬁniteness by means
of a diagonal shift :
K = σD−1 + D−1AD−1
where σ is a positive coeﬃcient large enough to guarantee the positive deﬁniteness of K. Since the mathematical foundation of these methods is the same,
it is possible to choose which algorithm to use for clustering choosing, for instance, the approach with the less computational complexity for the particular
application.
Conclusions
Clustering is a classical problem in pattern recognition. Recently spectral and
kernel methods for clustering have provided new ideas and interpretations
to the solution of this problem. In this paper spectral and kernel methods
for clustering have been reviewed paying attention to fuzzy kernel methods
for clustering and to the connection between kernel and spectral approaches.
Unlike classical partitioning clustering algorithms they are able to produce
nonlinear separating hypersurfaces among data since they construct an adjacency structure from data. These methods have been successfully tested on
several benchmarks, but we can ﬁnd few applications to real world problem
due to the high computational cost. Therefore an extensive validation on real
world applications remains a big challenge for kernel and spectral clustering
Acknowledgments
Work partially supported by the Italian Ministry of Education, University, and
Research and by the University of Genova.