The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)
Is BERT Really Robust? A Strong Baseline for
Natural Language Attack on Text Classiﬁcation and Entailment
Di Jin,1∗Zhijing Jin,2∗Joey Tianyi Zhou,3 Peter Szolovits1
1Computer Science & Artiﬁcial Intelligence Laboratory, MIT
2University of Hong Kong
3A*STAR, Singapore
{jindi15, psz}@mit.edu, , 
Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the
original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness
of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TEXTFOOLER, a
simple but strong baseline to generate adversarial text. By
applying it to two fundamental natural language tasks, text
classiﬁcation and textual entailment, we successfully attacked
three target models, including the powerful pre-trained BERT,
and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework:
(1) effective—it outperforms previous attacks by success rate
and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classiﬁed
by humans, and (3) efﬁcient—it generates adversarial text
with computational complexity linear to the text length.1
Introduction
In the last decade, Machine Learning (ML) models have
achieved remarkable success in various tasks such as classi-
ﬁcation, regression and decision making. However, recently
they have been found vulnerable to adversarial examples that
are legitimate inputs altered by small and often imperceptible perturbations .
These carefully curated examples are correctly classiﬁed by
a human observer but can fool a target model, raising serious concerns regarding the security and integrity of existing
ML algorithms. On the other hand, it is showed that robustness and generalization of ML models can be improved by
crafting high-quality adversaries and including them in the
training data .
While existing works on adversarial examples have obtained success in the image and speech domains , it is still challenging to
∗Equal Contribution. Order determined by swapping that in the
previous paper at 
Copyright c⃝2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
1The code, pre-trained target models, and test examples are
available at 
deal with text data due to its discrete nature. Formally, besides the ability to fool the target models, outputs of a natural
language attacking system should also meet three key utilitypreserving properties: (1) human prediction consistency—
prediction by humans should remain unchanged, (2) semantic similarity—the crafted example should bear the same
meaning as the source, as judged by humans, and (3) language ﬂuency—generated examples should look natural and
grammatical. Previous works barely conform to all three
requirements. For example, methods such as word misspelling , single-word erasure , and phrase insertion
and removal result in unnatural sentences. Moreover, there is almost no work that attacks the
newly-risen BERT model on text classiﬁcation.
In this work, we present TEXTFOOLER, a simple but
strong baseline for natural language attack in the blackbox setting, a common case where no model architecture
or parameters are accessible. We design a more comprehensive paradigm to create both semantically and syntactically
similar adversarial examples that meet the aforementioned
three desiderata. Speciﬁcally, we ﬁrst identify the important words for the target model and then prioritize to replace
them with the most semantically similar and grammatically
correct words until the prediction is altered. We successfully
applied this framework to attack three state-of-the-art models in ﬁve text classiﬁcation tasks and two textual entailment
tasks, respectively. On the adversarial examples, we can reduce the accuracy of almost all target models in all tasks to
below 10% with only less than 20% of the original words
perturbed. In addition, we validate that the generated examples are (1) correctly classiﬁed by human evaluators, (2) semantically similar to the original text, and (3) grammatically
acceptable by human judges.
Our main contributions are summarized as follows:
• We propose a simple but strong baseline, TEXTFOOLER,
to quickly generate high-proﬁle utility-preserving adversarial examples that force the target models to make
wrong predictions under the black-box setting.
• We evaluate TEXTFOOLER on three state-of-the-art deep
learning models over ﬁve popular text classiﬁcation tasks
and two textual entailment tasks, and it achieved the state-
of-the-art attack success rate and perturbation rate.
• We propose a comprehensive four-way automatic and
three-way human evaluation of language adversarial attacks to evaluate the effectiveness, efﬁciency, and utilitypreserving properties of our system.
• We open-source the code, pre-trained target models, and
test samples for the convenience of future benchmarking.
Problem Formulation
Given a corpus of N sentences X = {X1, X2, . . . , XN},
and a corresponding set of N labels Y = {Y1, Y2, . . . , YN},
we have a pre-trained model F : X →Y, which maps the
input text space X to the label space Y.
For a sentence X ∈X, a valid adversarial example Xadv
should conform to the following requirements:
F(Xadv) ̸= F(X), and Sim(Xadv, X) ≥ϵ,
where Sim : X × X →(0, 1) is a similarity function
and ϵ is the minimum similarity between the original and
adversarial examples. In the natural language domain, Sim
is often a semantic and syntactic similarity function.
Threat Model
Under the black-box setting, the attacker is not aware of the
model architecture, parameters, or training data. It can only
query the target model with supplied inputs, getting as results the predictions and corresponding conﬁdence scores.
The proposed approach for adversarial text generation is
shown in Algorithm 1, and consists of the two main steps:
Step 1: Word Importance Ranking (line 1-6)
sentence of n words X = {w1, w2, . . . , wn}, we observe
that only some key words act as inﬂuential signals for the
prediction model F, echoing with the discovery of that BERT attends to the statistical cues of
some words. Therefore, we create a selection mechanism to
choose the words that most signiﬁcantly inﬂuence the ﬁnal
prediction results. Using this selection process, we minimize
the alterations, and thus maintain the semantic similarity as
much as possible.
Note that the selection of important words is trivial in a
white-box scenario, as it can be easily solved by inspecting
the gradients of the model F, while most other words are
irrelevant. However, under the more common black-box set
up in our paper, the model gradients are unavailable. Therefore, we create a selection mechanism as follows. We use
the score Iwi to measure the inﬂuence of a word wi ∈X
towards the classiﬁcation result F(X) = Y . We denote the
sentence after deleting the word wi as X\wi = X \ {wi} =
{w1, . . . , wi−1, wi+1, . . . wn}, and use FY (·) to represent
the prediction score for the Y label.
The importance score Iwi is therefore calculated as the
prediction change before and after deleting the word wi,
which is formally deﬁned as follows,
Algorithm 1 Adversarial Attack by TEXTFOOLER
Input: Sentence example X = {w1, w2, ..., wn}, the corresponding ground truth label Y , target model F, sentence
similarity function Sim, sentence similarity threshold ϵ,
word embeddings Emb over the vocabulary V ocab.
Output: Adversarial example Xadv
1: Initialization: Xadv ←X
2: for each word wi in X do
Compute the importance score Iwi via Eq.2
4: end for
6: Create a set W of all words wi ∈X sorted by the descending order of their importance score Iwi.
7: Filter out the stop words in W.
8: for each word wj in W do
Initiate the set of candidates CANDIDATES by extracting the top N synonyms using CosSim(Embwj,
Embword) for each word in V ocab.
CANDIDATES ←POSFilter(CANDIDATES)
FINCANDIDATES ←{ }
for ck in CANDIDATES do
X′ ←Replace wj with ck in Xadv
if Sim(X′, Xadv) > ϵ then
Add ck to the set FINCANDIDATES
Pk ←FYk(X′)
if there exists ck whose prediction result Yk ̸= Y
In FINCANDIDATES, only keep the candidates ck
whose prediction result Yk ̸= Y
c∈FINCANDIDATES
Xadv ←Replace wj with c∗in Xadv
return Xadv
else if PYk(Xadv) >
ck∈FINCANDIDATES Pk then
ck∈FINCANDIDATES
Xadv ←Replace wj with c∗in Xadv
29: end for
30: return None
FY (X) −FY (X\wi),
if F(X) = F(X\wi) = Y
(FY (X) −FY (X\wi)) + (F ¯Y (X\wi) −F ¯Y (X)),
if F(X) = Y, F(X\wi) = ¯Y , and Y ̸= ¯Y .
After ranking the words by their importance score, we further ﬁlter out stop words derived from NLTK2 and spaCy3
libraries such as “the”, “when”, and “none”. This simple step
of ﬁltering is important to avoid grammar destruction.
2 
3 
Step 2: Word Transformer (line 7-30)
For a given word
wi ∈X with a high importance score obtained in Step 1, we
need to design a word replacement mechanism. A suitable
replacement word needs to fulﬁll the following criteria: it
should (1) have similar semantic meaning with the original
one, (2) ﬁt within the surrounding context, and (3) force the
target model to make wrong predictions. In order to select
replacement words that meet such criteria, we propose the
following workﬂow.
Extraction:
CANDIDATES for all possible replacements of the selected
word wi. CANDIDATES is initiated with N closest synonyms
according to the cosine similarity between wi and every
other word in the vocabulary.
To represent the words, we use word embeddings from
 . These word vectors are specially curated for ﬁnding synonyms, as they achieve the state-of-theart performance on SimLex-999, a dataset designed to measure how well different models judge semantic similarity between words .
Using this set of embedding vectors, we identify top N
synonyms whose cosine similarity with w are greater than δ.
Note that enlarging N or lowering δ would both generate
more diverse synonym candidates; however, the semantic
similarity between the adversary and the original sentence
would decrease. In our experiments, empirically setting N
to be 50 and δ to be 0.7 strikes a balance between diversity
and semantic similarity control.
POS Checking: In the set CANDIDATES of the word wi,
we only keep the ones with the same part-of-speech (POS)4
as wi. This step is to assure that the grammar of the text is
mostly maintained (line 10 in Algorithm 1).
Semantic Similarity Checking: For each remaining word
c ∈CANDIDATES, we substitute it for wi in the sentence X, and obtain the adversarial example Xadv
{w1, . . . , wi−1, c, wi+1, . . . , wn}. We use the target model
F to compute the corresponding prediction scores F(Xadv).
We also calculate the sentence semantic similarity between
the source X and adversarial counterpart Xadv. Speciﬁcally,
we use Universal Sentence Encoder (USE) 
to encode the two sentences into high dimensional vectors
and use their cosine similarity score as an approximation of
semantic similarity. The words resulting in similarity scores
above a preset threshold ϵ are placed into the ﬁnal candidate
pool FINCANDIDATES (line 11-19 in Algorithm 1).
Finalization of Adversarial Examples: In the ﬁnal candidate pool FINCANDIDATES, if there exists any candidate
that can already alter the prediction of the target model, then
we select the word with the highest semantic similarity score
among these winning candidates. But if not, then we select
the word with the least conﬁdence score of label y as the best
replacement word for wi, and repeat Step 2 to transform the
next selected word (line 20-30 in Algorithm 1).
Overall, the algorithm ﬁrst uses Step 1 to rank the words
by their importance scores, and then repeats Step 2 to ﬁnd
replacements for each word in the sentence X until the pre-
off-the-shelf
 
diction of the target model is altered.
Experiments
We study the effectiveness of our adversarial attack on two
important NLP tasks, text classiﬁcation and textual entailment. The dataset statistics are summarized in Table 1. Following the practice by Alzantot et al. , we evaluate
our algorithm on a set of 1,000 examples randomly selected
from the test set.
Classiﬁcation
Entailment
Table 1: Overview of the datasets.
Text Classiﬁcation
To study the robustness of our model,
we use text classiﬁcation datasets with various properties,
including news topic classiﬁcation, fake news detection, and
sentence- and document-level sentiment analysis, with average text length ranging from tens to hundreds of words.
• AG’s News (AG): Sentence-level classiﬁcation with regard to four news topics: World, Sports, Business, and
Science/Technology. Following the practice of Zhang,
Zhao, and LeCun , we concatenate the title and description ﬁelds for each news article.
• Fake News Detection (Fake): Document-level classiﬁcation on whether a news article is fake or not. The dataset
comes from the Kaggle Fake News Challenge.5
• MR: Sentence-level sentiment classiﬁcation on positive
and negative movie reviews . We use
90% of the data as the training set and 10% as the test set,
following the practice in .
• IMDB: Document-level sentiment classiﬁcation on positive and negative movie reviews.6
• Yelp Polarity (Yelp): Document-level sentiment classiﬁcation on positive and negative reviews . Reviews with a rating of 1 and 2 are labeled
negative and 4 and 5 positive.
Textual Entailment
• SNLI: A dataset of 570K sentence pairs derived from image captions. The task is to judge the relationship between
two sentences: whether the second sentence can be derived from entailment, contradiction, or neutral relationship with the ﬁrst sentence .
5 
6 
• MultiNLI: A multi-genre entailment dataset with a coverage of transcribed speech, popular ﬁction, and government reports .
Compared to SNLI, it contains more linguistic complexity
with various written and spoken English text.
Attacking Target Models
For each dataset, we train three state-of-the-art models on
the training set, and achieved test set accuracy scores similar to the original implementation, as shown in Table 2. We
then generate adversarial examples which are semantically
similar to the test set to attack the trained models and make
them generate different results.
Table 2: Original accuracy of target models on standard test
On the sentence classiﬁcation task, we target at three models: word-based convolutional neural network (WordCNN)
 , word-based long-short term memory (WordL-
STM) , and the state-ofthe-art Bidirectional Encoder Representations from Transformers (BERT) .
For the WordCNN model, we used three window sizes
of 3, 4, and 5, and 100 ﬁlters for each window size with
dropout of 0.3. For the WordLSTM, we used a 1-layer bidirectional LSTM with 150 hidden units and a dropout of 0.3.
For both models, we used the 200 dimensional Glove word
embeddings pre-trained on 6B tokens from Wikipedia and
Gigawords . We
used the 12-layer BERT model with 768 hidden units and
12 heads, with 110M parameters, which is called the baseuncased version.7
We also implemented three target models on the textual
entailment task: standard InferSent8 ,
ESIM9 , and ﬁne-tuned BERT.
Setup of Automatic Evaluation
We ﬁrst report the accuracy of the target models on the original test samples before attack as the original accuracy. Then
we measure the accuracy of the target models against the
adversarial samples crafted from the test samples, denoted
as after-attack accuracy. By comparing these two accuracy
scores, we can evaluate how successful the attack is, — the
7 
8 
9 
larger gap between the original and after-attack accuracy
signals the more successful our attack is. Apart from these
accuracies, we also report the perturbed word percentage as
the ratio of the number of perturbed words to the text length.
Furthermore, we apply USE10 to measure the semantic similarity between the original and adversarial texts. These two
metrics, the perturbed words percentage and the semantic
similarity score, together evaluate how semantically similar the original and adversarial texts are. We ﬁnally report
the number of queries the attack system made to the target
model and fetches the output probability scores. This metric
can reveal the efﬁciency of the attack model.
Setup of Human Evaluation
We conduct human evaluation on three criteria: semantic
similarity, grammaticality, and classiﬁcation accuracy. We
randomly select 100 test sentences of each task to generate adversarial examples, one targeting WordLSTM on MR
dataset and another targeting BERT on SNLI. We ﬁrst shuf-
ﬂed a mix of original and adversarial texts and asked human
judges to rate the grammaticality of them on a Likert scale
of 1 −5, similar to the practice of . Next, we evaluate the classiﬁcation consistency by
asking humans to classify each example in the shufﬂed mix
of the original and adversarial sentences and then calculate
the consistency rate of both classiﬁcation results. Lastly, we
evaluated the semantic similarity of the original and adversarial sentences by asking humans to judge whether the generated adversarial sentence is similar, ambiguous, or dissimilar to the source sentence. Each task is completed by two
independent human judges who are native English speakers.
The volunteers have university-level education backgrounds
and passed a test batch before they started annotation.
Automatic Evaluation
The main results of black-box attacks in terms of automatic
evaluation on ﬁve text classiﬁcation and two textual entailment tasks are summarized in Table 3 and 4, respectively.
Overall, as can be seen from our results, TEXTFOOLER
achieves a high success rate when attacking with a limited
number of modiﬁcations on both tasks. No matter how long
the text sequence is, and no matter how accurate the target model is, TEXTFOOLER can always reduce the accuracy from the state-of-the-art values to below 15% (except
on the Fake dataset) with less than 20% word perturbation
ratio (except the AG dataset under the BERT target model).
For instance, it only perturbs 5.1% of the words on average when reducing the accuracy from 89.8% to only 0.3%
on the IMDB dataset against the WordLSTM model. Notably, our attack system makes the WordCNN model on the
IMDB dataset totally wrong (reaching the accuracy of 0%)
with only 3.5% word perturbation rate. In the IMDB dataset
which has an average length of 215 words, the system only
perturbed 10 words or fewer per sample to conduct successful attacks. This means that our attack system can success-
10 universal-sentence-encoder
Original Accuracy
After-Attack Accuracy
% Perturbed Words
Semantic Similarity
Query Number
Average Text Length
Table 3: Automatic evaluation results of the attack system on text classiﬁcation datasets, including the original model prediction accuracy before being attacked (“Original Accuracy”), the model accuracy after the adversarial attack (“After-Attack
Accuracy”), the percentage of perturbed words with respect to the original sentence length (“% Perturbed Words”), and the
semantic similarity between original and adversarial samples (“Semantic Similarity”).
MultiNLI (m/mm)
MultiNLI (m/mm)
MultiNLI (m/mm)
Original Accuracy
After-Attack Accuracy
% Perturbed Words
Semantic Similarity
Query Number
Average Text Length
Table 4: Automatic evaluation results of the attack system on textual entailment datasets. “m” means matched, and “mm” means
mismatched, which are the two variants of the MultiNLI development set.
fully mislead the classiﬁers into assigning wrong predictions
via subtle manipulation.
Even for BERT, which has achieved seemingly “robust”
performance compared with the non-pretrained models such
as WordLSTM and WordCNN, our attack model can still reduce its prediction accuracy by about 5–7 times on the classiﬁcation task (e.g., from 95.6% to 6.8% for Yelp dataset)
and about 9-22 times on the NLI task (e.g., from 89.4% to
4.0% for SNLI dataset), which is unprecedented. Our curated adversarial examples can contribute to the study of the
interpretability of the BERT model .
Another two observations can be drawn from Table 3 and
4. (1) Models with higher original accuracy is, in general,
more difﬁcult to be attacked. For instance, the after-attack
accuracy and perturbed word ratio are both higher for the
BERT model compared with WordCNN on all datasets. (2)
The after-attack accuracy of the Fake dataset is much higher
than all other classiﬁcation datasets for all three target models. We found in experiments that it is easy for the attack
system to convert a real news to a fake one, whereas the reverse process is much harder, which is in line with intuition.
Comparing the semantic similarity scores and the perturbed word ratios in both Table 3 and 4, we ﬁnd that the two
results have a high positive correlation. Empirically, when
the text length is longer than 10 words, the semantic similarity measurement becomes more stable. Since the average
text lengths of text classiﬁcation datasets are all above 20
words and those of textual entailment datasets are around
or below 10 words, we need to treat the semantic similarity
scores of these two tasks individually. Therefore, we performed a linear regression analysis between the word perturbation ratio and semantic similarity for each task and
Success Rate
% Perturbed Words
 
 
 
 
Table 5: Comparison of our attack system against other published systems. The target model for IMDB and Yelp is
LSTM and SNLI is InferSent.
obtained r-squared values of 0.94 and 0.97 for text classiﬁcation and textual entailment tasks, respectively. Such
high values of r-squared reveal that our proposed semantic
similarity has high correlation (negative) with the perturbed
words ratio, which can both be good automatic measurements to evaluate the degree of alterations of original text.
We include the average text length of each dataset in the
last row of Table 3 and 4 so that it can be conveniently compared against the query number. The query number is almost
linear to the text lengthm, with a ratio in (2, 8). Longer text
correlates with a smaller ratio, which validates the efﬁciency
of TEXTFOOLER.
Benchmark Comparison
We compared TEXTFOOLER
with the previous state-of-the-art adversarial attack systems
against the same target model and dataset. Our baselines
include that generates misspelled words
by character- and word-level perturbation, that iterates through every word in the sentence and
ﬁnd its perturbation, and that uses
Movie Review (Positive (POS) ↔Negative (NEG))
Original (Label: NEG)
The characters, cast in impossibly contrived situations, are totally estranged from reality.
Attack (Label: POS)
The characters, cast in impossibly engineered circumstances, are fully estranged from reality.
Original (Label: POS)
It cuts to the knot of what it actually means to face your scares, and to ride the overwhelming
metaphorical wave that life wherever it takes you.
Attack (Label: NEG)
It cuts to the core of what it actually means to face your fears, and to ride the big metaphorical
wave that life wherever it takes you.
SNLI (Entailment (ENT), Neutral (NEU), Contradiction (CON))
Two small boys in blue soccer uniforms use a wooden set of steps to wash their hands.
Original (Label: CON)
The boys are in band uniforms.
The boys are in band garment.
A child with wet hair is holding a butterﬂy decorated beach ball.
Original (Label: NEU)
The child is at the beach.
The youngster is at the shore.
Table 6: Examples of original and adversarial sentences from MR (WordLSTM) and SNLI (BERT) datasets.
Source Text
(WordLSTM)
Adversarial
Table 7: Grammaticality of original and adversarial examples for MR (WordLSTM) and SNLI (BERT) on 1−5 scale.
word replacement by greedy heuristics. From the results in
Table 5, we can see that our system beats the previous stateof-the-art models by both the attack success rate (calculated
by dividing the number of wrong predictions by the total
number of adversarial examples) and perturbed word ratio.
Human Evaluation
We sampled 100 adversarial examples on the MR dataset
with the WordLSTM and 100 examples on SNLI with
BERT. We veriﬁed the quality of our examples via three experiments. First, we ask human judges to give a grammaticality score of a shufﬂed mix of original and adversarial text.
As shown in Table 7, the grammaticality of the adversarial
text are close to the original text on both datasets. By sensibly substituting synonyms, TEXTFOOLER generates smooth
outputs such as “the big metaphorical wave” in Table 6.
We then asked the human raters to assign classiﬁcation labels to a shufﬂed set of original and adversarial samples. The
overall agreement between the labels of the original sentence
and the adversarial sentence is relatively high, with 92% on
MR and 85% on SNLI. Though our adversarial examples
are not perfect in every case, this shows that majorities of
adversarial sentences have the same attribute as the original
sentences from humans’ perspective. Table 6 shows typical
examples of sentences with almost the same meanings that
result in contradictory classiﬁcations by the target model.
Lastly, we asked the judges to decide whether each adversarial sample retains the meaning of the original sentence.
They need to decide whether the synthesized adversarial example is similar, ambiguous, or dissimilar to the provided
original sentence. We regard similar as 1, ambiguous as 0.5,
and dissimilar as 0, and obtained sentence similarity scores
of 0.91 on MR and 0.86 on SNLI, which shows the perceived
difference between original and adversarial text is small.
Discussion
Ablation Study
Word Importance Ranking
To validate the effectiveness
of Step 1 in Algorithm 1, i.e., the word importance ranking,
we remove this step and instead randomly select the words in
text to perturb. We keep the perturbed word ratio and Step 2
the same. We use BERT as the target model and test on three
datasets: MR, AG, and SNLI. The results are summarized in
Table 8. After removing Step 1 and instead randomly selecting the words to perturb, the after-attack accuracy increases
by more than 45% on all three datasets, which reveals that
the attack becomes ineffective without the word importance
ranking step. The word importance ranking process is crucial to the algorithm in that it can accurately and efﬁciently
locate the words which cast the most signiﬁcant effect on
the predictions of the target model. This strategy can also
reduce the number of perturbed words so as to maintain the
semantic similarity as much as possible.
% Perturbed Words
Original Accuracy
After-Attack Accuracy
After-Attack Accuracy (Random)
Table 8: Comparison of the after-attack accuracies before
and after removing the word importance ranking of Algorithm 1. For control, Step 2 and the perturbed words ratio
are kept the same. BERT model is used as the target model.
Semantic Similarity Constraint
In Step 2 of Algorithm
1, for every possible word replacement, we check the semantic similarity between the newly generated sample and
the original text, and adopt this replacement only when
the similarity is above a preset threshold ϵ. We found that
this strategy can effectively ﬁlter out irrelevant synonyms
to the selected word. As we can see from the examples in
Table 9, the synonyms extracted by word embeddings are
noisy, so directly injecting them into the text as adversarial
samples would probably shift the semantic meaning significantly. By applying the sentence-level semantic similarity
constraint, we can obtain more related synonyms as good
replacements.11
like a south of the border melrose place
Adversarial
like a south of the border melrose spot
like a south of the border melrose mise
their computer animated faces are very expressive
Adversarial
their computer animated face are very affective
their computer animated faces are very diction
Table 9: Qualitative comparison of adversarial attacks with
and without the semantic similarity constraint (“-Sim.”). We
highlight the original word, TextFooler’s replacement, and
the replacement without semantic constraint.
After-Attack Accu.
% Perturbed Words
Query Number
Semantic Similarity
Table 10: Comparison of automatic evaluation metrics with
and without the semantic similarity constraint (numbers in
the left and right of the symbol “/” represent results with
and without the constraint, respectively). The target model
is BERT-Base.
Transferability
We examined transferability of adversarial text, that is,
whether adversarial samples curated based on one model can
also fool another. For this, we collected the adversarial examples from IMDB and SNLI test sets that are wrongly predicted by one target model and then measured the prediction
accuracy of them against the other two target models. As we
can see from the results in the Table 11, there is a moderate
degree of transferability between models, and the transferability is higher in the textual entailment task than in the text
classiﬁcation task. Moreover, the adversarial samples generated based on the model with higher prediction accuracy, i.e.
the BERT model here, show higher transferability.
Adversarial Training
Our work casts insights on how to better improve the original models through these adversarial examples. We con-
11Please check our arXiv for
a comparison of the automatic evaluation before and after removing
the semantic similarity constraint.
Table 11: Transferability of adversarial examples on IMDB
and SNLI dataset. Row i and column j is the accuracy of
adversaries generated for model i evaluated on model j.
ducted a preliminary experiment on adversarial training, by
feeding the models both the original data and the adversarial examples (adversarial examples share the same labels as
the original counterparts), to see whether the original models can gain more robustness. We collected the adversarial
examples curated from the MR and SNLI training sets that
fooled BERT and added them to the original training set. We
then used the expanded data to train BERT from scratch and
attacked this adversarially-trained model. As is seen in the
attack results in Table 12, both the after-attack accuracy and
perturbed words ratio after adversarial re-training get higher,
indicating the greater difﬁculty to attack. This reveals one of
the potency of our attack system,— we can enhance the robustness of a model to future attacks by training it with the
generated adversarial examples.
+ Adv. Training
Table 12: Comparison of the after-attack accuracy (“Af.
Acc.”) and percentage of perturbed words (“Pert.”) of original training (“Original”) and adversarial training (“+ Adv.
Train”) of BERT model on MR and SNLI dataset.
Related Work
Adversarial attack has been extensively studied in computer
vision . Most works make gradient-based perturbation on continuous input spaces .
Adversarial attack on discrete data such as text is more
challenging. Inspired by the approaches in computer vision,
early work in language adversarial attack focus on variations of gradient-based methods. For example, Zhao, Dua,
and Singh transform input data into a latent representation by generative adversarial networks (GANs), and
then retrieved adversaries close to the original instance in
the latent space.
Other works observed the intractability of GAN-based
models on text and the shift in semantics in the latent representations, so heuristic methods such as scrambling, misspelling, or removing words were proposed .
Ribeiro, Singh, and Guestrin automatically craft the
semantically equivalent adversarial rules from the machine
generated paraphrases using back-translation techniques.
Conclusion
Overall, we study adversarial attacks against state-of-the-art
text classiﬁcation and textual entailment models under the
black-box setting. Extensive experiments demonstrate that
the effectiveness of our proposed system, TEXTFOOLER, at
generating targeted adversarial texts. Human studies validated that the generated adversarial texts are legible, grammatical, and similar in meaning to the original texts.
Acknowledgements
We thank Professor Zheng Zhang for insightful discussions,
and we appreciate Heather Berlin, Yilun Du, Laura Koemmpel and other helpers for high quality human evaluation.