Auto-Embedding Generative Adversarial Networks
for High Resolution Image Synthesis
Yong Guo∗, Qi Chen∗, Jian Chen∗, Qingyao Wu, Qinfeng Shi, and Mingkui Tan†
Abstract—Generating images via generative adversarial network (GAN) has attracted much attention recently. However,
most of the existing GAN-based methods can only produce low
resolution images of limited quality. Directly generating high
resolution images using GANs is nontrivial, and often produces
problematic images with incomplete objects. To address this issue,
we develop a novel GAN called Auto-Embedding Generative
Adversarial Network (AEGAN), which simultaneously encodes
the global structure features and captures the ﬁne-grained details.
In our network, we use an autoencoder to learn the intrinsic highlevel structure of real images and design a novel denoiser network
to provide photo-realistic details for the generated images. In
the experiments, we are able to produce 512 × 512 images of
promising quality directly from the input noise. The resultant
images exhibit better perceptual photo-realism, i.e., with sharper
structure and richer details, than other baselines on several
datasets, including Oxford-102 Flowers, Caltech-UCSD Birds
(CUB), High-Quality Large-scale CelebFaces Attributes (CelebA-
HQ), Large-scale Scene Understanding (LSUN) and ImageNet.
Index Terms—Generative models, adversarial learning, lowdimensional embedding, autoencoder.
I. INTRODUCTION
UILDING a generative model that produces photorealistic images of high resolution has been a challenging
problem in the ﬁeld of computer vision. Compared to low
resolution images, the high resolution images of promising
quality often provide richer information and beneﬁt the training of deep neural networks (DNNs) in many real-world
applications, such as texture synthesis , , , ,
super-resolution , , and attribute editing ,
 , . However, how to produce high-quality data when
increasing the image resolution still remains an open question.
Recently, generative adversarial networks (GANs) have
achieved great success and become the workhorse of many
challenging tasks, including image generation , , 
video prediction , and image translation , ,
 , . Typically, GANs learn to generate data by playing
a two-player game: a generator attempts to produce samples
from a simple prior distribution (e.g., Gaussian distribution),
while a discriminator acts as a judge to distinguish the
generated data from the real one.
When generating images directly from the prior distribution,
the quality of the images generated by most of the existing
models can be quite limited, especially when synthesizing
very high resolution images. To be speciﬁc, deep generative
models often produce meaningless images that may contain
∗Authors contributed equally.
† Corresponding author.
Samples generated by DCGAN (top) and AEGAN (bottom) of
different resolutions on the Oxford-102 Flowers dataset.
multiple distorted regions with blurred structure . To
illustrate this issue, in Fig. 1, we compare generated samples
of different resolutions produced by the well-known model
DCGAN and by our proposed method. From Fig. 1 top,
the low resolution images of size 64 × 64 and 128 × 128
preserve clear and complete object structure, including stamen,
petals, etc. However, for the higher resolutions of 256 × 256
or 512 × 512, DCGAN fails to produce meaningful samples
and yields very poor results compared to low resolutions.
Regarding this issue, it is necessary and important to explore a
new method to improve the performance of generative models
when increasing the image resolution.
The difﬁculties of generating high resolution images are
generally attributed to two reasons. The ﬁrst is that it is hard
to directly learn a mapping between the prior distribution (i.e.,
input noise) and the distribution of high-dimensional real data,
due to the large distribution gap. Since high-dimensional data
often lie on some low-dimensional manifold , , ,
we use low-dimensional embedding to uncover the image’s
structural information, which acts as a bridge to connect
the prior distribution with the distribution of real data. This
embedding is extracted by an autoencoder that is able to
reconstruct the real images while preserving clear photostructure. More critically, matching a low-dimensional embedding representation can signiﬁcantly ease the training of GANs
compared to learning high resolution images directly.
The second reason regarding this issue is that there is no
additional knowledge, such as label or semantic information
obtained from real data, to help the model training. Note that
the prior distribution is usually very simple, e.g., standard
Gaussian distributions or uniform distributions , and is
independent of the real data, which may lose the original
semantic information. In practice, GANs can easily lose the
 
primary characteristics of data that are used by humans in
image recognition, resulting in meaningless images (see results
in Section IV). On the contrary, the extracted embedding,
which contains the spatial structural information, is able to
act as a kind of additional knowledge to improve the training.
By translating the input noise into an embedding and
decoding it into a corresponding synthetic image, however,
the model may introduce many noisy artifacts. To alleviate
this issue, we further develop an adversarial denoiser network
to enhance the photo-realism of the generated images. This
denoiser model takes synthetic images as input and forces
them to be perceptually indistinguishable from the real images
in terms of texture details. In practice, we feed the generated
images into the denoiser model to remove these artifacts and
provide photo-realistic details.
Based on the above intuitions, we propose the Auto-
Embedding Generative Adversarial Networks (AEGANs) for
high resolution image synthesis. Unlike the considered baselines, AEGAN is able to consistently generate perceptually
promising images at different resolutions (see Fig. 1 bottom).
Moreover, the proposed method signiﬁcantly outperforms the
alternative approaches and produces photo-realistic 512 × 512
images, which simultaneously capture the high-level photostructure and preserve the low-level details.
In this paper, we make the following contributions:
• We devise a novel Auto-Embedding Generative Adversarial Net (AEGAN) that generates high resolution
images by learning a latent embedding extracted from
an autoencoder. AEGAN exploits the high-level photostructure and acts as a bridge to connect the distributions
of the input noise and real data. As a result, the proposed
method can produce much more meaningful images with
clear structure and rich details.
• We develop a new denoiser network that uses an encoderdecoder network as the generator to remove artifacts and
provide photo-realistic details in the generated images.
• The proposed method is able to produce high resolution
images with promising quality. For example, when the
desired resolution is 512 × 512, the generated images
are of much higher quality than those obtained using the
considered methods.
II. RELATED WORK
A. Generative Adversarial Networks
Recently, Generative Adversarial Networks (GANs) 
have shown promising performance for generating natural
images, such as DCGAN , WGAN , WGAN-GP ,
LAPGAN , etc. More recently, Chen et al. propose a
modiﬁcation of generators in GANs, called Self-Modulation
framework that improves the performance of the generated images by modulating the architectural features of
GAN generators. Moreover, GANs have also been applied to
range of other interesting applications, such as text to image
synthesis , , , , super-resolution , ,
image inpainting , , and so on.
B. High-Resolution Image Generation
Generating high resolution images has gained much attention in the last few years in light of the advances in deep
learning. To achieve this, one can compare the distribution
divergences of the real and the generated data in a lowdimensional data space, such as StackGAN , StackGANv2 , AGE and AttnGAN .
In StackGAN, the low resolution image obtained from
Stage-I and the conditional text are used to produce high
resolution image through Stage-II GAN, just as is done in super resolution. However, for unconditional image generation,
the conditional text is not necessary and can be discarded.
Moreover, the Stage-II GAN is trained by minimizing the
JS distance between real and fake data. Unlike StackGANs,
in addition to minimizing the JS divergence, we seek to
increase the ﬁdelity of images while preserving the image
content by introducing a pixel-wise loss.
StackGAN-v2 is
an extension of StackGAN that uses a cascade mechanism to
generate the image in gradually growing resolution. Based on
StackGAN-v2, AttnGAN introduces the attention mechanism
to improve the visual quality of the generated images. AGE
uses an autoencoder to project both the real data and the
generated data into a latent space. To compare the distribution
divergence, AGE aligns a simple prior distribution in the
latent space and the data distribution in the image space.
More recently, Zhang et al. design a SAGAN model that
incorporates the self-attention mechanism into the generator
and discriminator. Based SAGAN model, Brock et al. propose
a BigGAN model that improves the performance using a
suite of tricks for training.
C. Autoencoder GAN
Autoencoders have been widely used in GANs and a
plethora of attempts have been made to improve the training.
For example, the adversarial autoencoder (AAE) leverages an
autoencoder to match the distribution of latent embedding
with the prior. Boundary-equilibrium GANs and energybased GANs use an autoencoder as the discriminator
to stabilize the training. Warde-Farley et al. combine
an autoencoder loss with the GAN loss by matching highlevel feature similarity. CycleGANs and DualGANs 
construct a bidirectional loss with autoencoder and GAN for
data translation between two domains. The methods described
in , , which are based on autoencoders, combine a
Variational Autoencoder (VAE) with GAN using variational
inference to solve the intractability of the marginal likelihood
in GAN. Plug and play generative networks (PPGN) combines an autoencoder loss with a GAN loss and a classiﬁcation
Very recently, Karras et al. have proposed a new
progressively-growing training method that gradually adds one
more block per stage in both the generator and discriminator
networks. In this way, it can signiﬁcantly improve the synthetic
image quality. However, this training method takes a long
time to converge (i.e., more than two days), while DCGANs
and our proposed AEGANs only require several hours for
training. Unlike other methods, in this paper, we use an
autoencoder to transform high-dimensional data to a lowdimensional embedding in the latent space. The extracted
embedding contains rich structural information of real images
and aids the training to produce images of promising quality.
III. THE PROPOSED METHOD
Considering that high-dimensional data often lie on some
low-dimensional
lowdimensional embedding extracted from large training data
to improve the performance of GANs.
To this end, we
present a novel generative method called Auto-Embedding
Generative Adversarial Network (AEGAN). Corresponding to
the overall structure in Fig. 2, the detailed algorithm is shown
in Algorithm 1.
We propose to train GANs by matching the distribution of
the low-dimensional embeddings on a latent manifold (see
Step 1) instead of the distributions of pixel-level images,
which is very different from the existing GANs methods.
Speciﬁcally, we use the embeddings extracted from an autoencoder to bridge the distribution gap between the input noise
and real data. The embeddings often contain rich structural
information and help to generalize well with high resolution images. After using the autoencoder to extract the lowdimensional embedding, we learn a GAN in the embedding
space to effectively exploit the structural information. Moreover, we devise a denoiser network to remove artifacts/noises
and reﬁne the photo-realistic details (see Step 2). During
training, we can use a ﬁne-tuning strategy (which, however,
is optional) to further improve the performance. As shown
in Fig. 2, the whole scheme mainly includes three important
components, namely an autoencoder, a generative module, and
a denoiser network. In this section, we will depict each part
of the method in the following subsections.
A. Learning Embedding by Autoencoder
In this paper, we seek to bridge the distribution gap between
the input noise and the real images using a latent embedding
extracted from an autoencoder. The autoencoder contains an
encoder H which maps the high resolution images into a
low-dimensional embedding and a decoder F which translates
the latent embedding back to high resolution images. Here,
the encoder is a fully convolutional network that extracts
the high-level features of the data, while the decoder is
a fully deconvolutional model that effectively recovers the
high resolution images. Given a collection of n real samples
i=1, we minimize the following reconstruction loss:
LAE(θF , θH) = 1
∥F (H(xi)) −xi∥1,
where θH and θF denote the parameters of the encoder H
and the decoder F, respectively. Here, we use L1 loss instead
of the L2 loss to better capture the high-level and salient
features . This feature becomes the embedding which is
able to uncover the primary characteristics of real data, e.g. the
structural information or image style. In this way, the learned
latent embedding has the potential to produce meaningful
Algorithm 1 Training algorithm for AEGAN
Initialize: Real data {xi}n
i=1; prior distribution p(z), z∈R100.
Step 1: Train autoencoder to learn low-dimensional embeddings
Update the encoder H and decoder F by minimizing the reconstruction loss:
LAE(θF , θH) = 1
∥F (H(xi)) −xi∥1
Step 2: Train GANs to produce high resolution images
for number of training iterations do
• Train GE and DE by optimizing the objective:
LE(θGE, θDE)
• Train φ and DR by optimizing the objective:
Lφ(θφ, θDR, θGE, θF )
Step 3: End-to-end ﬁne-tuning
// optional
Update P={θGE,θF , θφ, θH}, D={θDE, θDR} by optimizing:
Q LAE(θF )+LE(θGE, θDE)+Lφ(θφ, θDR, θGE, θF )
There are two advantages of extracting the embedding from
data using an autoencoder. Firstly, the autoencoder extracts
the high-level features which effectively preserve the primary
data characteristics, e.g., structural information , , to
reconstruct the original images. It is helpful to train a generator
on the extracted features to produce meaningful samples (see
results in Section IV). In practice, we set the embedding to
32 × 32 with 64 channels to represent the 512 × 512 RGB
images. In this sense, the dimensionality has been largely
reduced by over 10 fold in terms of the number of pixels.
Secondly, the generator only needs to learn a mapping from
the input noise z ∈R100 to the extracted low-dimensional
embedding rather than the high-dimensional images, which
greatly facilitates the training of deep generative models.
B. Adversarial Embedding Generator
With the extracted embedding which contains image structural information, we seek to exploit it to improve the training
of GANs. We deﬁne a generative model to match the meaningful embedding extracted from real data. The objective function
of the generative model is as follows:
LE(θGE, θDE) = 1
log DE(H(xi))
log(1 −DE(GE(zj))),
where H(y) denotes the extracted embedding from real images and z denotes the input noise sampled from a prior
distribution. We optimize GE and DE in an alternating manner
by solving the minimax problem:
LE(θGE, θDE).
During training, we ﬁx the parameters of autoencoder and
train the GAN model to match the distribution of embeddings
in the latent space. Note that we share the decoder during
both extracting the high-level feature in the autoencoder and
recovering the synthetic high resolution images from the
generated fake embeddings. Ideally, with a well-trained model,
high-dimensional data
low-dimensional
Learning Embedding
by an Autoencoder
Adversarial Denoiser Network
Convolution
Deconvolution
generated samples
of high resolution
decoded data
Adversarial Denoiser
Low-dimensional
Data Generator
discriminator
Fig. 2. Overall architecture of the proposed method. An autoencoder is trained to encode the low-dimensional embedding of high-dimensional images. The
embedding will be used to guide the learning of the generative model from the input noise. A denoiser network is devised to enrich the details and denoise
the artifacts in the generated images. Blue squares denote the synthetic embedding and the red circles denote the extracted embedding of real data. The bold
lines in the ﬁgure indicate the main data stream to produce high resolution images.
we can map a noise vector to obtain a latent embedding with
photo-structure information, which can be then decoded into
a meaningful high resolution image.
C. Adversarial Denoiser Network
We observe that the decoded images often encounter visual
noisy artifacts after going through the pipeline of the GAN and
the decoder (see Figs. 2 and 10(b)). An inevitable problem is
how to remove these artifacts and provide photo-realism in
the generated images. We thus develop a denoiser network to
address this issue.
Actually, reﬁning images using adversarial training has been
developed in , which uses a pixel-level method, i.e., a
chain of convolutional layers without striding or pooling to
reﬁne the blurring regions. However, when producing high
resolution images, the synthetic images often contain severe
visual artifacts which are hard to be reﬁned using such a
pixel-level method without scaling. To address this issue, we
develop a new denoiser model φ that contains multiple layers
of convolution and deconvolution operators. The structure is
shown in Fig. 2 (bottom).
The denoiser network φ follows the encoder-decoder design.
It is worth mentioning that the stride operation of convolution
can effectively extract the primary features of the data and
discard pixel-level noises . To improve the ﬁdelity of the
synthetic images, we introduce an adversarial loss to provide
photo-realistic details in the generated images. We implement
the DR as a convolutional network which outputs the probability of samples being positive. In addition to denoising
artifacts from the generated images, we also have to preserve
the content of generated images. To this end, a pixel-wise
loss is taken into account. As a result, we can simultaneously
preserve the image content and increase the perceptual ﬁdelity
of the images. The training objective becomes
Lφ(θφ, θDR, θGE, θF )= 1
log DR(H(xi))
log(1−DR(φ(ˆxj)))+λ ∥φ(ˆxj)−ˆxj∥1 ,
where ˆx = F(GE(z)) indicates the synthetic images before
feeding them into the denoiser network. During training, we ﬁx
GE and θF and only update the adversarial denoiser network.
Similar to the embedding generative model, we also train the
denoiser model by solving the minimax problem:
Lφ(θφ, θDR, θGE, θF ).
With the denoiser network, the noisy artifacts can be effectively removed from the generated images. As a result, the
proposed method is able to produce promising images with
high perceptual ﬁdelity.
D. Training and Inference Method
The proposed method consists of three components, namely
the autoencoder, the embedding generative model and the
denoiser network. To effectively train the whole model, we
adopt a step-wise strategy to train all the components and then
conduct end-to-end ﬁne-tuning to obtain a new uniﬁed model.
The training procedure is shown in Algorithm 1.
We divide the training of the proposed AEGAN into three
steps. In the ﬁrst step, we train the autoencoder by minimizing
the reconstruction loss to extract the low-dimensional embedding. The learned embedding is able to capture the image
structural information and effectively recover the high resolution images. In the second step, by ﬁxing the autoencoder
model, we train an embedding generative model and a denoiser
network in an alternating manner. In the proposed model,
the autoencoder acts as a bridge that connects all the three
components.
However, when one of the components fails, there is no way
to correct it. To ensure the ﬁnal performance and obtain a new
uniﬁed model, in the last step, we conduct end-to-end training
to ﬁne-tune the whole model by optimizing the following joint
LAE(θF , θH)+LE(θGE, θDE)+Lφ(θφ, θDR, θGE, θF ),
where P = {θGE, θF , θφ, θH} and Q = {θDE, θDR}. In each
iteration of the ﬁnal step, we ﬁrst update the discriminators
DE and DR using the gradients propagated from LE and Lφ,
respectively. We then backpropagate the signals of each loss
through the model and the gradient for each component will
accumulate. The ﬁne-tuning step ensures the coherence of the
whole model and is able to achieve slightly better performance.
In practice, we observe that failure of the components rarely
occurs. Thus, the ﬁne-tuning step is optional. Compared to
directly optimizing the objective in Eqn. (6), each separately
trained model can be viewed as a good initialization and can be
used to accelerate the training to obtain a new uniﬁed model.
For inference, we sample a vector z ∈R100 from a
prior distribution to produce a latent embedding. We then
feed the latent embedding into the decoder F to produce a
corresponding high resolution image. Finally, we take the generated images into the denoiser model. Through the pipeline
of matching the random noise to a high resolution image,
AEGAN is able to generate 512 × 512 RGB images with
promising quality in terms of both quantitative and qualitative
E. Model Architecture
We build the embedding generator GE and the decoder
F with a stack of up-sampling blocks (i.e., a strided deconvolutional layer followed by a residual blocks ). The
number of blocks is determined by the upscaling factor of
the feature maps. In the experiment, we add 3 and 4 upsampling blocks in GE and F, respectively. For the encoder
H, we build the model with a stack of down-sampling blocks
to encode RGB images to the low-dimensional embeddings.
Each down-sampling block contains a strided deconvolutional
layer followed by a residual blocks . Similar to the design
of the decoder F, we also add 4 down-sampling blocks in H.
For the discriminators DR and DE, we build the model with
a stack of strided convolution layers to down-sample the input
images or embeddings. We add a Batch Normalization 
and a ReLU layer behind each convolutional layer. The
denoiser network φ follows the encoder-decoder design. The
input generated images are fed into several down-sampling
modules (i.e., a strided convolutional layer followed by a Batch
Normalization and a LeakyReLU layer) until it has a
size of 512. A series of up-sampling modules (i.e., strided
deconvolutional layer followed by a Batch Normalization and
a ReLU layer) are then used to generate 512×512 RGB highresolution images. The strides of all the strided convolutional
and deconvolution layer are set to 2 and 1/2, respectively.
F. Implementation Details
We follow the experimental settings used in to train the
proposed AEGAN in PyTorch. There is no preprocessing or
data augmentation of the training samples other than resizing
all the training images to 512 × 512 RGB images. In all
the experiments, we set the embedding to 32 × 32 with 64
channels to represent the 512×512 RGB images. The weights
are initialized from a normal distribution with zero-mean and
standard deviation of 0.02. The hyperparameter λ in Eqn. (4)
is set to λ = 100. We use ReLU activation in the upsamplers,
such as the embedding generator in GANs and the decoder in
the autoencoder, while using LeakyReLU in the downsamplers
including discriminators in GANs and the encoder in the
autoencoder. The slope of the leak in LeakyReLU is set to 0.2.
In the training, we use Adam with β1 = 0.9 to update
the model parameters. In the ﬁrst training step, we use a ﬁxed
learning rate of 10−5 to train the autoencoder. For the second
step, we follow the same settings used in DCGAN and
set the learning rate to 0.0002. At the ﬁnal step, we adjust
the learning rate to 10−7 for end-to-end ﬁne-tuning. During
training, we set the minibatch size to 16. For the three steps
in Algorithm 1, we train the corresponding models with 100
epochs, 200 epochs and 100 epochs, respectively.
IV. EXPERIMENTS
In the experiments, we focus on generating high resolution images of 512 × 512. To evaluate the performance of
the proposed method, several generative models are adopted
for comparison, including DCGAN , WGAN-GP ,
AGE , StackGAN , StackGAN-v2 and Progressive GAN .
For the comparison methods, we use the ofﬁcial source
codes and the original settings speciﬁed in their papers. Specifically, we train DCGAN, WGAN-GP, AGE and StackGAN-v2
for 200, 3906, 150 and 600 epochs, respectively, following the
default settings in the original papers. We train StackGAN for
220 epochs, i.e., 120 for Stage-I and 100 for Stage-II. For
Progressive GAN, we start with 4 × 4 resolution and train the
model at each resolution using 1200k images, i.e., 600k for
fade-in and 600k for stabilizing.
For convenience, we organize the experiments as follows.
Firstly, we introduce some details about the benchmark
datasets and the evaluation metrics in Section IV-A. Then, we
compare the performance of our method with several baselines
on ﬁve benchmark datasets and show both quantitative and
qualitative comparisons in Section IV-B and IV-C.
COMPARISONS OF VARIOUS GENERATIVE MODELS IN TERMS OF FID, INCEPTION SCORE AND MULTI-SCALE STRUCTURAL SIMILARITY (MS-SSIM) ON
OXFORD-102, CUB, CELEBA-HQ, LSUN AND IMAGENET DATASETS. WE USE 10, 000 SAMPLES FOR TESTING. HIGHER INCEPTION SCORE AND LOWER
FID INDICATE BETTER IMAGE QUALITY. LOWER MS-SSIM SCORES INDICATE HIGHER DIVERSITY.
Oxford-102
Conference
Promontory
DCGAN 
WGAN-GP 
StackGAN 
StackGAN-v2 
Progressive GAN 
AEGAN (ours)
Inception Score
DCGAN 
WGAN-GP 
StackGAN 
StackGAN-v2 
Progressive GAN 
AEGAN (ours)
DCGAN 
WGAN-GP 
StackGAN 
StackGAN-v2 
Progressive GAN 
AEGAN (ours)
Fig. 3. Visual comparison of 512 × 512 images generated using different methods on the Oxford-102 (top) and CUB (bottom) datasets.
A. Datasets and Evaluation Metrics
We evaluate the proposed method using a wide variety
of benchmark datasets, including Oxford-102 Flowers ,
Caltech-UCSD Birds (CUB) , High-Quality Large-scale
CelebFaces Attributes (CelebA-HQ) , , Large-scale
Scene Understanding (LSUN) and ImageNet .
Oxford-102 Flowers contains 8189 images of ﬂowers from
102 ﬁne-grained classes and CUB contains 200 bird species
with 11, 788 high resolution images in total. The images in
Oxford-102 Flowers have resolutions of over 500×500 while
CUB contains images with resolution ranging from 300 to 500.
The CelebA-HQ dataset,which is generated from the original
CelebA dataset , contains 30K celebrity face images at
1024 × 1024 resolution. In our experiments, we resize the
training samples to 512 × 512 resolution. LSUN contains
approximately one million images at the resolutions ranging
from 300 to 500. ImageNet contains 1,000 classes and 1.28
million images with the resolutions ranging from 200 to 700
(the average image resolution is 469×387). Due to the setting
of unconditional image generation, we train all the methods
on each category separately. For each dataset, we resize all
the images to 512 × 512 resolution during training.
For quantitative evaluation, we use Frechet Inception Distance (FID) , Inception Score and MS-SSIM , 
to evaluate the generated samples. FID is a widely used metric
for implicit generative models, as it correlates well with the
visual quality of generated samples. Inception Score can be
used to measure both image quality and image diversity over
a large number of samples. In general, larger Inception Score
value indicates the better performance. MS-SSIM measures
Visual comparison of 512 × 512 face examples produced by different generative models on CelebA-HQ dataset. The samples generated by each
method are chosen randomly.
Fig. 5. Visual comparison of high resolution images of face generated by various methods on the CelebA-HQ dataset. We compare the detailed structure and
texture including the eyes, nose, and mouth. The top row shows the results of different baselines; the bottom row shows the results of the proposed AEGAN.
the diversity of the generated samples and the resulting values
range from 0.0 to 1.0. Higher MS-SSIM values correspond to
perceptually more similar images. In the experiments, we use
10, 000 images to calculate these scores. For Inception Score,
10 splits are used to compute the standard deviations.
B. Quantitative Comparisons
In this section, we compare the performance of different
methods on 5 benchmark datasets in terms of FID ,
Inception Score and MS-SSIM , . The results are
shown in Table I.
On the Oxford-102 and CUB datasets, the proposed AE-
GAN model obtains slightly worse results than Progressive
GAN in terms of MS-SSIM but achieves the best FID
and Inception Score. On the CelebA-HQ dataset, AEGAN
still obtains comparable performance with the strong baseline
Progressive GAN. These results demonstrate that the proposed
method is able to produce images of promising quality while
maintaining a large diversity.
For the more challenging datasets LSUN and ImageNet,
we train AEGAN and the considered baseline methods on
each category separately. On LSUN, we compare the results
in 3 categories, including Bedroom, Classroom and Conference. However, on ImageNet (containing 1000 categories in
total), training 1000 models is infeasible and impractical. For
convenience, we choose two of them, i.e., Promontory and
Volcano, to evaluate the performance of our proposed method.
From Table I, AEGAN outperforms the considered baseline
methods in most categories in terms of FID, Inception score
and MS-SSIM. These results demonstrate the superiority of
the proposed method in high resolution image synthesis.
Fig. 6. Visual comparison of high resolution examples generated using different methods on the ImageNet dataset. For convenience, we choose two categories,
i.e., Promontory and Volcano, to show the results.
Fig. 7. Visual comparison of 512 × 512 images produced by different generative models on the LSUN datasets. We randomly choose samples generated
using each method for careful observation.
C. Qualitative Comparisons
In this section, we compare the visual quality of the images
generated by the proposed AEGAN and several baseline methods on 5 benchmark datasets, including Oxford-102, CUB,
CelebA-HQ, LSUN and ImageNet.
Figs. 3 and 4 show that, when producing high resolution
images of 512 × 512, DCGAN tends to produce colorful
but meaningless images that contain many isolated regions.
WGAN-GP and AGE are able to capture the rough
structure and produce meaningful images. However, the generated samples appear very blurred and lack photo-realistic
details. Note that StackGAN is originally devised to generate images by taking the input text as a condition. In this
experiment, since there is no additional information acting
as the condition, the transition of StackGAN from the ﬁrst
stage to the second one will be incoherent. As a result,
the model fails to effectively capture the style and texture
information. Compared to Progressive GAN , AEGAN is
able to produce promising images with sharper object structure
and richer details than the other methods.
On CelebA-HQ, we also compare some detailed regions of
the generated images in Fig. 5. Speciﬁcally, we compare the
detailed structure and texture in the generated face images,
such as eyes, nose, and mouth. The top row shows the images
generated by the ﬁve considered baselines and their detailed
regions, and the bottom row shows a set of samples generated
by AEGAN. Compared to the considered baselines, AEGAN is
able to produce promising images with sharper facial structure
and ﬁner details.
When generating more complex images on LSUN and
ImageNet, AEGAN is able to produce meaningful images of
better perceptual quality compared to the baseline methods.
Figs. 6 and 7 show that none of the baseline methods except
Progressive GAN, is able to produce visually acceptable scene
images. The images generated by Progressive GAN also lack
some structural and textural information. However, most of
DEMONSTRATION OF THE EFFECT OF DENOISER NETWORK ON OXFORD-102 DATASET. WE COMPARE AEGAN WITH 6 BASELINE METHODS WITH AND
WITHOUT THE DENOISER MODULE IN TERMS OF FID, INCEPTION SCORE AND MS-SSIM.
Inception Score
with denoiser
with denoiser
with denoiser
DCGAN 
3.27 ± 0.08
3.45 ± 0.05
WGAN-GP 
3.52 ± 0.07
3.74 ± 0.09
2.33 ± 0.06
3.56 ± 0.09
StackGAN 
3.38 ± 0.07
3.86 ± 0.10
StackGAN-v2 
3.92 ± 0.08
3.94 ± 0.10
Progressive GAN 
3.83 ± 0.10
3.87 ± 0.09
3.98 ± 0.07
Fig. 8. Results of latent space interpolation by AEGANs on the CelebA-HQ dataset. For each row, we conduct linear interpolation between two data points
sampled from the prior distribution p(z). The leftmost and rightmost columns are the generated images of AEGAN, and the columns 2 to 9 show the
interpolated images.
(a) Failure case.
(b) AEGAN results.
Fig. 9. Demonstration of the possible failure case without end-to-end ﬁnetuning on the Oxford-102 dataset.
the images generated by AEGAN capture the salient features
of the speciﬁc category and obtain objects with complete and
sharp structure, e.g. the structure of beds and the shape of
desks. This is attributed to the extracted high-level embedding.
The images generated by AEGAN are more photo-realistic
than the images generated by the considered baselines in all
categories. These results demonstrate the effectiveness of the
proposed AEGAN in producing high resolution images.
V. FURTHER EXPERIMENTS
In this section, we conduct further analyses and discussions
of our proposed method. In Section V-A, we perform an ablation study of the end-to-end ﬁne-tuning step. In section V-B,
we discuss the effect of our proposed denoiser networks. In
Section V-C, we investigate the effect of embedding dimension
on the performance of AEGAN. In Section V-D, we conduct an
experiment of latent space interpolation. Finally, we compare
the training time of our AEGAN to baselines in Section V-E.
A. Ablation Study on End-to-end Fine-tuning
In this section, we investigate the effect of the ﬁnal ﬁnetuning step (the third step in Algorithm 1) in generating high
resolution images by ablation study. The experimental results
are shown in Table III. From Table III, the model with ﬁnetuning achieves slightly better performance than the model
without ﬁne-tuning.
We note that there is a risk of model failure in the ﬁrst
two steps. Fortunately, the end-to-end ﬁne-tuning is able to
effectively address this issue, thereby producing a new uniﬁed
model with better performance. To verify this, we show some
examples of a failure case in Fig. 9. However, the failure
rarely occurs in practice. The end-to-end ﬁne-tuning helps to
guarantee the performance of our method.
B. Effect of Denoiser
In this section, we investigate the effect of the denoiser module on the performance of high resolution images synthesis.
To this end, we add the denoiser module to all the considered
baselines and compare the results obtained using these methods with or without the denoiser module. The experimental
results are shown in Table III. From Table III, the denoiser
module is able to improve the performance of different methods in terms of both FID and Inception Score, demonstrating
the effectiveness of the proposed denoiser network. Moreover,
AEGAN consistently outperforms the baseline methods with
or without the denoiser network.
C. Effect of Embedding Dimension
In this section, we investigate the effect of embedding
dimension on the performance of AEGAN. Actually, the
dimension of the extracted embedding indicates the representation ability of latent space. Although the representation ability
will become more powerful with the increase of dimension, the
computational complexity will increase accordingly. Therefore, a smaller dimension is preferable when the representation
PERFORMANCE COMPARISON OF AEGAN WITH OR WITHOUT THE
END-TO-END FINE-TUNING ON OXFORD-102 DATASET.
AEGAN without ﬁne-tuning
AEGAN with ﬁne-tuning
Inception Score
3.91 ± 0.08
3.98 ± 0.07
(a) 8 × 8 × 64
(b) 32 × 32 × 64
Fig. 10. Decoded images from different embedding dimensions.
ability of the embedding is sufﬁcient. Here, we test smaller
embedding dimensions of 8 × 8 with 64 channels in the
autoencoder. The results are shown in Fig. 10.
Fig. 10 shows that, when we map the input noise to such
a small embedding, the ﬁnal decoded images contain much
more visual artifacts than the standard setting in previous
experiments, i.e., 32 × 32 with 64 channels. These results
demonstrate that to synthesize a high resolution image, we
must use a sufﬁciently large embedding space to represent the
information of style and structure.
D. Results of Latent Space Interpolation
In this section, we conduct an experiment in which we
investigate the landscape of the latent space. Following the
settings in , we conduct linear interpolations between
two data points in the latent space and feed them into the
generative models. The generated samples are shown in Fig. 8.
In Fig. 8, there are no sharp transitions and the generated
images change smoothly. These results demonstrate that the
proposed AEGAN generalizes well to unseen data rather than
simply memorizing the training samples.
E. Comparison of Training Time
In this section, we compare the training time of the considered methods. We train all the methods except Progressive
GAN on single TITAN X Pascal GPU. For Progressive GAN,
we train the model on two GPUs with a batch size of 32 for
each of GPU (i.e., 64 in total). The training times and the
corresponding FID scores are shown in Fig. 11.
From Fig. 11, the proposed AEGAN achieves the best FID
score of 65.41 and only requires 17 hours for training, which is
much more efﬁcient than WGAN-GP and Progressive GAN.
These results demonstrate the effectiveness of the proposed
method in terms of both image generation performance and
training efﬁciency.
GPU hours (h)
StackGAN-v2
Progressive GAN
Fig. 11. Performance vs training time of different GAN methods on Oxford-
102 dataset.
VI. CONCLUSION
In this paper, we have proposed a novel scheme for high
resolution image synthesis. In contrast to traditional GAN
methods, we use low-dimensional embedding to bridge the
distribution gap between input noise and real data. Furthermore, we also devise a denoiser network that removes the
noisy artifacts and provides low-level photo-realistic details in
the generated images. The proposed method produces images
with sharp structures and rich photo-realistic details, and
signiﬁcantly outperforms the considered generative models.