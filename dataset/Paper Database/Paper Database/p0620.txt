Simulation-based optimal Bayesian experimental design for
nonlinear systems
Xun Huan, Youssef M. Marzouk
Massachusetts Institute of Technology, Cambridge, MA 02139, USA
The optimal selection of experimental conditions is essential to maximizing the
value of data for inference and prediction, particularly in situations where experiments
are time-consuming and expensive to conduct. We propose a general mathematical
framework and an algorithmic approach for optimal experimental design with nonlinear simulation-based models; in particular, we focus on ﬁnding sets of experiments
that provide the most information about targeted sets of parameters.
Our framework employs a Bayesian statistical setting, which provides a foundation for inference from noisy, indirect, and incomplete data, and a natural mechanism
for incorporating heterogeneous sources of information. An objective function is constructed from information theoretic measures, reﬂecting expected information gain
from proposed combinations of experiments. Polynomial chaos approximations and a
two-stage Monte Carlo sampling method are used to evaluate the expected information gain. Stochastic approximation algorithms are then used to make optimization
feasible in computationally intensive and high-dimensional settings. These algorithms
are demonstrated on model problems and on nonlinear parameter inference problems
arising in detailed combustion kinetics.
Uncertainty quantiﬁcation, Bayesian inference, Optimal experimental design,
Nonlinear experimental design, Stochastic approximation, Shannon information,
Chemical kinetics
1. Introduction
Experimental data play an essential role in developing and reﬁning models of
physical systems. For example, data may be used to update knowledge of parameters
in a model or to discriminate among competing models. Whether obtained through
ﬁeld observations or laboratory experiments, however, data may be diﬃcult and expensive to acquire. Even controlled experiments can be time-consuming or delicate to
perform. In this context, maximizing the value of experimental data—designing experiments to be “optimal” by some appropriate measure—can dramatically accelerate
the modeling process. Experimental design thus encompasses questions of where and
Email addresses: (Xun Huan), (Youssef M. Marzouk)
 
November 27, 2024
when to measure, which variables to interrogate, and what experimental conditions
to employ.
These questions have received much attention in the statistics community and
in many science and engineering applications. When observables depend linearly on
parameters of interest, common solution criteria for the optimal experimental design problem are written as functionals of the information matrix . These criteria
include the well-known ‘alphabetic optimality’ conditions, e.g., A-optimality to minimize the average variance of parameter estimates, or G-optimality to minimize the
maximum variance of model predictions. Bayesian analogues of alphabetic optimality, reﬂecting prior and posterior uncertainty in the model parameters, can be derived
from a decision-theoretic point of view . For instance, Bayesian D-optimality can
be obtained from a utility function containing Shannon information while Bayesian
A-optimality may be derived from a squared error loss. In the case of linear-Gaussian
models, the criteria of Bayesian alphabetic optimality reduce to mathematical forms
that parallel their non-Bayesian counterparts .
For nonlinear models, however, exact evaluation of optimal design criteria is much
more challenging. More tractable design criteria can be obtained by imposing additional assumptions, eﬀectively changing the form of the objective; these assumptions
include linearizations of the forward model, Gaussian approximations of the posterior distribution, and additional assumptions on the marginal distribution of the
data . In the Bayesian setting, such assumptions lead to design criteria that may
be understood as approximations of an expected utility. Most of these involve prior
expectations of the Fisher information matrix . Cruder “locally optimal” approximations require selecting a “best guess” value of the unknown model parameters and
maximizing some functional of the Fisher information evaluated at this point .
None of these approximations, though, is suitable when the parameter distribution
is broad or when it departs signiﬁcantly from normality . A more general design
framework, free of these limiting assumptions, is preferred .
More rigorous information theoretic criteria have been proposed throughout the
literature. The seminal paper of Lindley suggests using expected gain in Shannon
information, from prior to posterior, as a measure of the information provided by an
experiment; the same objective can be justiﬁed from a decision theoretic perspective . Sebastiani and Wynn propose selecting experiments for which the
marginal distribution of the data has maximum Shannon entropy; this may be understood as a special case of Lindley’s criterion. Maximum entropy sampling (MES) has
seen use in applications ranging from astronomy to geophysics , and is well
suited to nonlinear models. Reverting to Lindley’s criterion, Ryan introduces a
Monte Carlo estimator of expected information gain to design experiments for a model
of material fatigue. Terejanu et al. use a kernel estimator of mutual information
(equivalent to expected information gain) to identify parameters in chemical kinetic
model. The latter two studies evaluate their criteria on every element of a ﬁnite set
of possible designs (on the order of ten designs in these examples), and thus sidestep
the challenge of optimizing the design criterion over general design spaces. And both
report signiﬁcant limitations due to computation expense; concludes that “full
blown search” over the design space is infeasible, and that two order-of-magnitude
gains in computational eﬃciency would be required even to discriminate among the
enumerated designs.
The application of optimization methods to experimental design has thus favored
simpler design objectives.
The chemical engineering community, for example, has
tended to use linearized and locally optimal design criteria or other objectives 
for which deterministic optimization strategies are suitable. But in the broader context of decision theoretic design formulations, sampling is required. proposes a
curve ﬁtting scheme wherein the expected utility was ﬁt with a regression model,
using Monte Carlo samples over the design space. This scheme relies on problemspeciﬁc intuition about the character of the expected utility surface. Clyde et al. 
explore the joint design, parameter, and data space with a Markov chain Monte Carlo
(MCMC) sampler; this strategy combines integration with optimization, such that the
marginal distribution of sampled designs is proportional to the expected utility. This
idea is extended with simulated annealing in to achieve more eﬃcient maximization of the expected utility. use expected utilities as design criteria but do not
pursue information theoretic design metrics. Indeed, direct optimization of information theoretic metrics has seen much less development. Building on the enumeration
approaches of and the one-dimensional design space considered in , 
iteratively ﬁnds MES designs in multi-dimensional spaces by greedily choosing one
component of the design vector at a time. Hamada et al. also ﬁnd “near-optimal”
designs for linear and nonlinear regression problems by maximizing expected information gain via genetic algorithms. But the coupling of rigorous information theoretic
design criteria, complex physics-based models, and eﬃcient optimization strategies
remains an open challenge.
This paper addresses exactly these issues. Our interest is in physically realistic
and hence computationally intensive models.
We advance the state of the art by
introducing ﬂexible approximation and optimization strategies that yield optimal experimental designs for nonlinear systems, using a full information theoretic formalism,
eﬃciently and with few limiting assumptions.
In particular, we employ a Bayesian statistical approach and focus on the case
of parameter inference. Expected Shannon information gain is taken as our design
criterion; this objective naturally incorporates prior information about the model
parameters and accommodates very general probabilistic relationships among the
experimental observables, model parameters, and design conditions. The need for
such generality is illustrated in the numerical examples (Sections 5 and 6). To make
evaluations of expected information gain computationally tractable, we introduce a
generalized polynomial chaos surrogate that captures smooth dependence of
the observables jointly on parameters and design conditions. The surrogate carries no
a priori restrictions on the degree of nonlinearity and uses dimension-adaptive sparse
quadrature to identify and exploit anisotropic parameter and design dependencies
for eﬃciency in high dimensions. We link the surrogate with stochastic approximation
algorithms and use the resulting scheme to maximize the design objective.
formulation allows us to plan single experiments without discretizing the design space,
and to rigorously identify optimal “batch” designs of multiple experiments over the
product space of design conditions.
Figure 1 shows the key components of our approach, embedded in a ﬂowchart
describing a design–experimentation–model improvement cycle. The upper boxes focus on experimental design: the design criterion is formulated in Sections 2.1 and
2.2; estimation of the objective function is described in Section 2.3; and stochastic
optimization approaches are described in Section 2.4. The construction of polynomial chaos surrogates for computationally intensive models is presented in Section 3.
Section 4 brieﬂy reviews computational approaches for Bayesian parameter inference,
which come into play after the selected experiments have been performed and data
have been collected. All of these tools are demonstrated on two example problems: a
simple nonlinear model in Section 5 and a shock tube autoignition experiment with
detailed chemical kinetics in Section 6.
2. Experimental design formulation
2.1. Experimental goals
Optimal experimental design relies on the construction of a design criterion, or
objective function, that reﬂects how valuable or relevant an experiment is expected
to be. A fundamental consideration in specifying this objective is the application of
interest—i.e., what does the user intend to do with the results of the experiments?
For example, if one would like to estimate a particular physical constant, then a good
objective function might reﬂect the uncertainty in the inferred values, or the error in
a point estimate. On the other hand, if one’s ultimate goal is to make accurate model
predictions, then a more appropriate objective function should directly consider the
distribution of the model outputs conditioned on data. If one would like to ﬁnd the
“best” model among a set of candidate models, then the objective function should
reﬂect how well the data are expected to support each model, favoring experiments
that maximize the ability of the data to discriminate. These considerations motivate
the intuitive notion that an objective function should be based on speciﬁc experimental
In this paper, we shall assume that the experimental goal is to infer a ﬁnite number
of model parameters of interest. Parameter inference is of course an integral part
of calibrating models from experimental data . The expected utility framework
developed below can be generalized to other experimental goals, and we will mention
this where appropriate. Note that one could also augment the objective function by
adding a penalty that reﬂects experimental eﬀort or cost. More broadly, one can
always add resource constraints to the experimental design optimization problem.
In the interest of simplicity, and since costs and constraints are inevitably problemspeciﬁc, we do not pursue such additions here.
2.2. Design criterion and expected utility
We will formulate our experimental design criterion in a Bayesian setting. Bayesian
statistics oﬀers a foundation for inference from noisy, indirect, and incomplete data;
a mechanism for incorporating physical constraints and heterogeneous sources of information; and a complete assessment of uncertainty in parameters, models, and predictions. The Bayesian approach also provides natural links to decision theory , a
framework we will exploit below. (For discussions contrasting Bayesian and frequentist approaches to statistics, see and .)
The Bayesian paradigm treats unknown parameters as random variables. Let
(Ω, F, P) be a probability space, where Ωis a sample space, F is a σ-ﬁeld, and P
is a probability measure on (Ω, F). Let the vector of real-valued random variables
θ : Ω→Rnθ denote the uncertain parameters of interest, i.e., the parameters to be
conditioned on experimental data. θ is associated with a measure µ on Rnθ, such that
µ(A) = P (θ−1 (A)) for A ∈Rnθ. We then deﬁne p(θ) = dµ/dθ to be the density of
θ with respect to Lebesgue measure. For the present purposes, we will assume that
such a density always exists. Similarly we treat the data y as an Rny-valued random
variable endowed with an appropriate density. d ∈Rnd denotes the design variables
or experimental conditions. Hence nθ is the number of uncertain parameters, ny is the
number of observations, and nd is the number of design variables. If one performs an
experiment at conditions d and observes a realization of the data y, then the change
in one’s state of knowledge about the model parameters is given by Bayes’ rule:
p(θ|y, d) = p(y|θ, d)p(θ|d)
Here p(θ|d) is the prior density, p(y|θ, d) is the likelihood function, p(θ|y, d) is
the posterior density, and p(y|d) is the evidence. It is reasonable to assume that
prior knowledge on θ does not vary with the experimental design, leading to the
simpliﬁcation p(θ|d) = p(θ).
Taking a decision theoretic approach, Lindley suggests that an objective for
experimental design should have the following general form:
u(d, y, θ) p(θ, y|d) dθ dy
u(d, y, θ) p(θ|y, d) p(y|d) dθ dy,
where u(d, y, θ) is a utility function, U(d) is the expected utility, Θ is the support
of p(θ), and Y is the support of p(y|d). The utility function u should be chosen to
reﬂect the usefulness of an experiment at conditions d, given a particular value of the
parameters θ and a particular outcome y. Since we do not know the precise value of
θ and we cannot know the outcome of the experiment before it is performed, we take
the expectation of u over the joint distribution of θ and y.
Our choice of utility function is rooted in information theory. In particular, following , we put u(d, y, θ) equal to the relative entropy, or Kullback-Leibler (KL)
divergence, from the posterior to the prior. For generic distributions A and B, the
KL divergence from A to B is
DKL(A||B) =
where pA and pB are probability densities, Θ is the support of pB(θ), and 0 ln 0 ≡0.
This quantity is non-negative, non-symmetric, and reﬂects the diﬀerence in information carried by the two distributions (in units of nats) . Specializing to the
inference problem at hand, the KL divergence from the posterior to the prior is
u(d, y, θ) ≡DKL (pθ(·|y, d)||pθ(·)) =
p( ˜θ|y, d) ln
p( ˜θ|y, d)
d ˜θ = u(d, y).
Note that this choice of utility function involves an “internal” integration over the
parameter space ( ˜θ is a dummy variable representing the parameters), therefore it is
not a function of the parameters θ. Thus we have
u(d, y)p(θ|y, d) dθ p(y|d) dy
u(d, y) p(y|d) dy
p( ˜θ|y, d) ln
p( ˜θ|y, d)
d ˜θ p(y|d) dy.
To simplify notation, ˜θ in Equation (5) is replaced by θ, yielding
p(θ|y, d) ln
p(θ|y, d)
dθ p(y|d) dy
Ey|d [DKL (p(θ|y, d)||p(θ))] .
The expected utility U is therefore the expected information gain in θ. The intuition
behind this expression is that a large KL divergence from posterior to prior implies
that the data y decrease entropy in θ by a large amount, and hence those data are
more informative for parameter inference. As we have only a distribution for the
data y|d that may be observed, we are interested in maximizing information gain on
average. We also note that U is equivalent to the mutual information between the
parameters θ and the data y.1 When applied to a linear-Gaussian design problem,
U reduces to the Bayesian D-optimality condition.2
Finally, the expected utility must be maximized over the design space D to ﬁnd
the optimal experimental design
d∗= arg max
1Using the deﬁnition of conditional probability, we have
p(θ|y, d) ln
p(θ|y, d)
dθ p(y|d) dy
p(θ, y|d) ln
 p(θ, y|d)
p(θ)p(y|d)
I(θ; y|d),
which is the mutual information between parameters and data, given the design.
2D-optimality maximizes the determinant of the information matrix in a linear design problem .
Bayesian D-optimality, in a linear-Gaussian problem, maximizes the determinant of the sum of the
information matrix and the prior covariance .
What if resources allow multiple (say N > 1) experiments to be carried out,
but time and setup constraints require them to be planned (and perhaps performed)
simultaneously? If d∗is the optimal design parameter for a single experiment, the
best choice is not necessarily to repeat the experiment at d∗N times; this does
not generally yield the optimal expected information gain from all the experiments.
(Appendix A shows that the expected utility of two experiments is not, in general,
equal to the sum of the expected utilities of the individual experiments. Section 5
provides a numerical example of this situation.) Instead, all of the experiments should
be incorporated into the likelihood function, where now d ∈RNnd, y ∈RNny, and the
data from the diﬀerent experiments are conditionally independent given θ and the
augmented d. The new optimal design d∗∈RNnd then carries the N sets of conditions
for all the experiments, maximizing the expected total information gain when these
experiments are simultaneously performed. It is interesting to note that a simpler
objective function often used in experimental design—the predictive variance of the
data y—would always suggest repeating all N experiments at the single-experiment
design optimum.
If the N experiments need not be carried out simultaneously, then sequential
experimental design may be performed. In general, a sequential design uses the results
of one set of experiments (i.e., the y that are actually observed) to help plan the next
set of experiments. In one possible approach—in fact, a greedy approach—an optimal
experiment is initially computed and carried out, and its data are used to perform
inference. The resulting posterior p(θ, y|d1) is then used as the prior in the design of
the next experiment d2, and the process is repeated. This approach is not necessarily
optimal over a horizon of many experiments, however. A more rigorous treatment
would involve formulating the sequential design problem as a dynamic programming
problem, but this is beyond the scope of the present paper. Intuitively, a sequential
experimental design should be at least as good as a ﬁxed design, due to the extra
information gained in the intermediate stages.
2.3. Numerical evaluation of the expected utility
Typically, the expected utility in Equation (6) has no closed form and must be
approximated numerically. One approach is to rewrite U(d) as
p(θ|y, d) ln
p(θ|y, d)
dθ p(y|d) dy
p(y|θ, d)
p(y|θ, d) p(θ) dθ dy
{ln [p(y|θ, d)] −ln [p(y|d)]} p(y|θ, d) p(θ) dθ dy,
where the second equality is due to the application of Bayes’ theorem to the quantities
both inside and outside the logarithm. In the special case where the Shannon entropy
of p(y, θ|d) is independent of the design variables d, the ﬁrst term in Equation (8)
becomes constant for all designs and can be dropped from the objective function.
Maximizing the remaining term—which is the entropy of p(y|d)—is then equivalent
to the maximum entropy sampling approach of Sebastiani and Wynn .
we retain the more general formulation of Equation (8) in order to accommodate,
for example, likelihood functions containing a measurement error whose magnitude
depends on y or d.
Monte Carlo sampling can then be used to estimate the integral in Equation (8)
p(y(i)|θ(i), d)
where θ(i) are drawn from the prior p(θ); y(i) are drawn from the conditional distribution p(y|θ = θ(i), d) (i.e., the likelihood); and nout is the number of samples in
this “outer” Monte Carlo estimate. The evidence evaluated at y(i), p(y(i)|d), typically does not have an analytical form, but it can be approximated using yet another
importance sampling estimate:
p(y(i)|d) =
p(y(i)|θ, d)p(θ) dθ ≈1
p(y(i)|θ(i,j), d),
where θ(·,j) are drawn from the prior p(θ) and nin is the number of samples in this
“inner” Monte Carlo sum. The combination of Equations (9) and (10) yields a biased estimator ˆU(d) of U(d) . The variance of this estimator is proportional to
A(d)/nout + B(d)/noutnin, where A and B are terms that depend only on the distributions at hand. The bias is proportional to C(d)/nin . Hence nin controls the
bias while nout controls the variance.
Evaluating and sampling from the likelihood for each new sample of θ constitutes
the most signiﬁcant computational cost above (see Section 3). In order to mitigate
the cost of the nested Monte Carlo estimator, we draw a fresh batch of prior samples
{θ(k)}nout
k=1 for every d, and use this set for both the outer Monte Carlo sum (i.e.,
θ(i) = θ(k)) and all the inner Monte Carlo estimates at that d (i.e., θ(·,j) = θ(k),
and consequently nout = nin). This treatment reduces the computational cost for a
ﬁxed d from O (noutnin) to O (nout). In practice, sample reuse also avoids producing
near-zero evidence estimates (and hence inﬁnite values for the expected utility) at
small sample sizes. The reuse of samples contributes to the bias of the estimator, but
this eﬀect is very small . See Appendix B for a numerical study of the bias.
2.4. Stochastic optimization
Now that the expected utility U(d) can be estimated at any value of the design
variables, we turn to the optimization problem (7). Maximizing U via a grid search
over D is clearly impractical, since the number of grid points grows exponentially
with dimension. Since only a Monte Carlo estimate ˆU(d) of the objective function
is available, another naïve approach would be to use a large sample size (nout, nin)
at each d and then apply a deterministic optimization algorithm, but this is still
too expensive. (And even with large sample sizes, ˆU(d) is eﬀectively non-smooth.)
Instead, we would like to use only a few Monte Carlo samples to evaluate the objective
at any given d, and thus we need algorithms suited to noisy objective functions. Two
such algorithms are simultaneous perturbation stochastic approximation (SPSA) and
Nelder-Mead nonlinear simplex (NMNS).
SPSA, proposed by Spall , is a stochastic approximation method that has
received considerable attention .
The method is similar to a steepest-descent
method using ﬁnite diﬀerence estimates of the gradient, except that SPSA only uses
two random perturbations to estimate the gradient regardless of the problem’s dimension:
dk −akgk(dk)
ˆU(dk + ck∆k) −ˆU(dk −ck∆k)
where k is the iteration number,
(A + k + 1)α,
(k + 1)γ ,
and a, A, α, c, and γ are algorithm parameters with recommended values available,
e.g., in . ∆k is a random vector whose entries are i.i.d. draws from a symmetric
distribution with ﬁnite inverse moments ; here, we choose ∆k,i ∼Bernoulli (0.5).
Common random numbers are also used to evaluate each pair of estimates ˆU(dk +
ck∆k) and ˆU(dk −ck∆k) at a given dk, in order to reduce variance in estimating the
gradient .
An intuitive justiﬁcation for SPSA is that error in the gradient “averages out” over
a large number of iterations . Convergence proofs with varying conditions and
assumptions can be found in . Randomness introduced through the noisy
objective ˆU and the ﬁnite-diﬀerence-like perturbations allows for a global convergence
property . Constraints in SPSA are handled by projection: if the current position
does not remain feasible under all possible random perturbations, then it is projected
to the nearest point that does satisfy this condition.
The NMNS algorithm has been well studied and is widely used for deterministic optimization. The details of the algorithm are thus omitted from this discussion
but can be found, e.g., in . This algorithm has a natural advantage in
dealing with noisy objective functions because it requires only a relative ordering of
function values, rather than the magnitudes of diﬀerences (as in estimating gradients). Minor modiﬁcations to the algorithm parameters can improve optimization
performance for noisy functions . Constraints in NMNS are handled simply by
projecting from the infeasible point to the nearest feasible point.
There are advantages and disadvantages to both algorithms. SPSA is a gradientbased approach, taking advantage of any regularity in the underlying objective function while requiring only two function evaluations per step to estimate the gradient
instead of 2nd evaluations, as with a full ﬁnite-diﬀerence scheme. However, a very
high noise level can lead to slow convergence and cause the algorithm to stagnate in
local optima. NMNS is relatively less sensitive to the noise level, but the simplex can
be unfavorably distorted due to the projection treatment of constraints, leading to
slow or false convergence.
Using either of the algorithms described in this section, we can approximately
solve the stochastic optimization problem posed in Equation (7) and obtain the best
experimental design.
In a sense, this completes the experimental design phase of
Figure 1. But a remaining diﬃculty is one of computational cost. Even with an
eﬀective Monte Carlo estimator of the expected utility, and with eﬃcient algorithms
for stochastic optimization, the complex physical model embedded in Equation (9)
still must be evaluated repeatedly, over many values of the model parameters and
design variables. Methods for making this task more tractable are discussed in the
next section.
3. Polynomial chaos surrogate
Expensive physical models can render the evaluation and maximization of expected information gain impractical. Models enter the formulation through the likelihood function p(y|θ, d). For example, a simple likelihood function might allow for
an additive discrepancy between experimental observations and model predictions:
y = G (θ, d) + ǫ.
Here, ǫ is a random variable with density pǫ; we leave the form of this density nonspeciﬁc for now. The “forward model” of the experiment is G : Rnθ × Rnd −→Rny; it
maps both the design variables and the parameters into the data space. Drawing a
realization from p(y|θ, d) thus requires evaluating G at a particular (θ, d). Evaluating
the density p(y|θ, d) = pǫ (y −G(θ, d)) again requires evaluating G.
To make these calculations tractable, one would like to replace G with a cheaper
“surrogate” model that is accurate over the entire prior support Θ and the entire
design space D. Many options exist, ranging from projection-based model reduction to spectral methods based on polynomial chaos (PC) expansions . The latter approaches do not reduce the internal state of a
deterministic model; rather, they explicitly exploit regularity in the dependence of
model outputs on uncertain input parameters. Polynomial chaos has seen extensive
use in a range of engineering applications (e.g., ) including parameter
estimation and inverse problems (e.g., ), and this is the approach we shall
Let ξi : Ω→R be i.i.d. random variables deﬁned on a probability space (Ω, F, P),
where Ωis the sample space, F is the σ-ﬁeld generated by all the ξi, and P is the
probability measure. Then any random variable θ : Ω→R, measurable with respect
to (Ω, F) and possessing ﬁnite variance, θ ∈L2(Ω, P), can be represented as follows:
θiΨi(ξ1(ω), ξ2(ω), . . .),
where ω ∈Ωis an element of the sample space; i = (i1, i2, . . .) , ij ∈N0, is an inﬁnitedimensional multi-index; |i| = i1 + i2 + . . . is the l1 norm; θi ∈R are the expansion
coeﬃcients; and
Ψi(ξ1, ξ2, . . .) =
are multivariate polynomial basis functions . Here ψij is an orthogonal polynomial
of order ij in the variable ξj, where orthogonality is with respect to the distribution
Eξ [ψmψn] =
ψm(ξ)ψn(ξ)p(ξ) dξ = δm,nEξ
and Ξ is the support of p(ξ). The expansion (15) is convergent in the mean-square
sense . For computational purposes, the inﬁnite sum and inﬁnite dimension must
be truncated to some ﬁnite stochastic dimension ns and polynomial order. A common
choice is the “total-order” truncation |i| ≤p:
θiΨi(ξ1, ξ2, . . . , ξns)
Ψi(ξ1, ξ2, . . . , ξns)
The total number of terms in this expansion is
= (ns + p)!
The choice of p is inﬂuenced by the degree of nonlinearity in the relationship between
θ and ξj, and the choice of ns reﬂects the degrees of freedom needed to capture the
stochasticity of the system. These choices might also be constrained by the availability
of computational resources, as nPC grows quickly when these numerical parameters
are increased.
3.1. Joint expansion for design variables
In the Bayesian setting, the model parameters θ are random variables, for which
PC expansions are easily applied. But the model outputs also depend on the design
conditions, and constructing a separate PC expansion at each value of d required
during optimization would be impractical. Instead, we can construct a single PC expansion for each component of G, depending jointly on θ and d. (Similar suggestions
have recently appeared in the context of robust design .) To proceed, we increase
the stochastic dimension by the number of design dimensions, putting ns = nθ + nd,
where we have assigned one stochastic dimension to each component of θ and one
to each component of d for simplicity. Further, we assume an aﬃne transformation
between each component of d and the corresponding {ξi}ns
i=nθ+1; any value of d can
thus be uniquely associated with a vector of these ξi. Since the design parameters
will usually be supported on a bounded domain (e.g., inside some hyper-rectangle)
the corresponding ξi are given uniform distributions. (The corresponding univariate
ψi are thus Legendre polynomials.) These distributions eﬀectively deﬁne a uniform
weight function over the design space D that governs where the L2-convergent PC
expansions should be accurate.
3.2. Pseudospectral projection
Constructing the PC expansion involves computing the coeﬃcients θi; this generally can proceed via two alternative approaches, intrusive and non-intrusive. The
intrusive approach results in a new system of equations that is larger than the original
deterministic system, but it needs be solved only once. The diﬃculty of this latter
step depends strongly on the character of the original equations, however, and may
be prohibitive for arbitrary nonlinear systems. The non-intrusive approach computes
the expansion coeﬃcients by directly projecting the quantity of interest (e.g., the
model output) onto the basis functions {Ψi}. One advantage of this method is that
the deterministic solver can be reused and treated as a black box. The deterministic
problem needs to be solved many times, but typically at carefully chosen parameter
values. The non-intrusive approach also oﬀers ﬂexibility in choosing arbitrary functionals of the state trajectory as observables; these functionals may depend smoothly
on ξ even when the state itself has a less regular dependence. (The combustion model
in Section 6 provides an example of such a situation.)
Taking advantage of orthogonality, the PC coeﬃcients are simply:
Gc,i = Eξ [GcΨi]
Ξ Gc(θ(ξ), d(ξ))Ψi(ξ)p(ξ) dξ
i (ξ)p(ξ) dξ
, c = 1 . . . ny,
where Gc,i is the PC coeﬃcient with multi-index i for the cth observable.3 Analytical
expressions are available for the denominators Eξ [Ψ2
i ], but the numerators must be
evaluated via numerical quadrature, because of the forward model G. The resulting
approach is termed pseudospectral projection, or non-intrusive spectral projection
(NISP). When the evaluations of the integrand (and hence the forward model) are
expensive and ns is large, an eﬃcient method for high-dimensional integration is
essential.
3.3. Dimension-adaptive sparse quadrature
A host of useful methods are available for numerical integration . In the present context, we seek a method that can evaluate the numerator of Equation (21) eﬃciently in high dimensions, i.e., with a minimal number of
integrand evaluations, taking advantage of regularity and anisotropy in the dependence of G on θ and d. We thus employ the dimension-adaptive sparse quadrature
(DASQ) algorithm of Gerstner and Griebel , an eﬃcient extension of Smolyak
3Here we are equating the dimension of the forward model output with the number of observables
ny. If the data contain repeated observations of the same quantity, for instance, in the case of multiple
experiments, then the same PC approximation can be used for all model-based predictions of that
sparse quadrature that adaptively tensorizes quadrature rules in each coordinate direction. It has a weak dependence on dimension, making it an excellent candidate for
problems of moderate size (e.g., ns < 100). Its formulation is brieﬂy described below.
be the lth level (with nl quadrature points) of some univariate quadrature rule, where
wi are the weights, ξi are the abscissae, and f(ξ) is the integrand. The level is usually
deﬁned to take advantage of any nestedness in the quadrature rule and to reduce the
overall computational cost. We have chosen to use Clenshaw-Curtis (CC) quadrature
for ξ’s with compact support, with the following level deﬁnition
nl = 2l−1 + 1, l ≥2,
The CC rule is especially appealing because it is accurate,4 nested, and easy to
construct.5
The diﬀerence formulas, deﬁned by
∆kf = (Q(1)
are the diﬀerences between 1D quadrature rules at two consecutive levels. The subtraction is carried out by subtracting the weights at the quadrature points of the
lower level. Then, for k ∈Nd
1 (where each entry of the multi-index k represents the
level in that dimension, with a total of d dimensions), the multivariate quadrature
rule is deﬁned to be
(∆k1 ⊗· · · ⊗∆kd)f,
where K is some set determined by the adaptation algorithm, to be described below.
For example, K : |k|1 ≤L + d −1 where L is some user-deﬁned level, corresponds to
the Smolyak sparse quadrature, while K : |k|∞≤N corresponds to a tensor-product
quadrature.
The original DASQ algorithm can be found in . The idea is to divide all the
multi-indices k into two sets: an old set and an active set. A member of the active
set is able to propose a new candidates by increasing the level in any dimension by 1.
4Although Gauss-Legendre quadrature (which is not nested) has a higher degree of polynomial
exactness, notes that
“the Clenshaw-Curtis and Gauss formulas have essentially the same accuracy unless f
is analytic in a sizable neighborhood of the interval of the integration—in which case
both methods converge so fast that the diﬀerence hardly matters.”
5The abscissae are simply xi = cos
, and the weights can be computed very eﬃciently via
FFT , requiring only O (n log n) time and introducing very little roundoﬀerror.
However, the candidate can only be accepted if all its backward neighbors are in the
old set; this so-called admissibility condition ensures the validity of the telescoping
expansion of the general sparse quadrature formulas via the diﬀerences ∆k. Finally,
each multi-index has an error indicator, which is proportional to its corresponding
summand value in Equation (25). Intuitively, if this term contributes little to the
overall integral estimate, then integration error due to this term should be small.
New candidates are proposed from the multi-index corresponding to the highest error
estimate. The process iterates until the sum of error indicators for the active set
members falls below some user-speciﬁed tolerance. More details, including proposed
data structures for this algorithm, can be found in . One drawback of DASQ is
that parallelization can only be implemented within the evaluation of each k, which
is not as eﬃcient as the parallelization in non-adaptive methods. The original DASQ
algorithm also does not address how integrand evaluations at nested quadrature points
can easily be identiﬁed and reused as adaptation proceeds. Huan proposes an
algorithm to solve this problem, taking advantage of the speciﬁc quadrature structure.
The ultimate goal of quadrature is to compute the polynomial chaos coeﬃcients
of the model outputs in Equation (21). There are a total of nPC (see Equation (20))
coeﬃcients for each output variable, and a total of ny model outputs, yielding a total
of ncoef = nPCny integrals. To simplify notation, let the PC coeﬃcients Gc,i, c =
1 . . . ny, |i| ≤p, be re-indexed by Gr, r = 1 . . . ncoef. It would be very ineﬃcient to
compute each integral from scratch, since the corresponding quadrature points will
surely overlap and any evaluations of G(θ(ξ), d(ξ)) ought to be reused. To realize
these computational savings, the original DASQ algorithm is altered to integrate
for all the coeﬃcients Gr simultaneously. We guide all the integrations via a single
adaptation route, which uses a “total eﬀect” local error indicator ¯hk that reﬂects all
the local error indicators hr,k from the integrals. The total eﬀect indicator at a given
k may be deﬁned as the max or 2-norm of the local error indicators {hr,k}ncoef
new algorithm is presented as Algorithm 1.
Lastly, compensated summation (the Kahan algorithm ) is used throughout
our implementation, as it signiﬁcantly reduces numerical error when summing long
sequences of ﬁnite-precision ﬂoating point numbers as required above.
4. Bayesian parameter inference
Once data are collected by performing an optimal experiment, they can be used
in the manner speciﬁed by the original experimental goal. In the present case, the
goal is to infer the model parameters θ by exploring or characterizing the posterior
distribution in Equation (1). Ideally the data will lead to a narrow posterior such
that, with high probability, the parameters can only take on small range of values.
The posterior can be evaluated pointwise up to a constant factor, but computing it on a grid is immediately impractical as the number of dimensions increases.
A more economical method is to generate independent samples from the posterior,
but given the arbitrary form of this distribution (particularly for nonlinear G), direct Monte Carlo sampling is seldom possible. Instead, one must resort to Markov
chain Monte Carlo (MCMC) sampling. Using only pointwise evaluations of the unnormalized posterior density, MCMC constructs a Markov chain whose stationary
and limiting distribution is the posterior. Samples generated in this way are correlated, such that the eﬀective sample size is smaller than the number of MCMC
steps. Nonetheless, a well-tuned MCMC algorithm can be reasonably eﬃcient. The
resulting samples can then be used in various ways—to evaluate marginal posterior
densities, for instance, or to approximate posterior expectations
Eθ|y,d [f(θ)] =
f(θ)p(θ|y, d) dθ
with the nM-sample average
where θ(t) are samples extracted from the chain (perhaps after burn-in or thinning).
For example, the minimum mean square error (MMSE) estimator is simply the mean
of the posterior, while the corresponding Bayes risk is the posterior variance, both of
which can be estimated using MCMC.
A very simple and powerful MCMC method is the Metropolis-Hastings (MH) algorithm, ﬁrst proposed by Metropolis et al. , and later generalized by Hastings ;
details of the algorithm can be found in . Two useful improvements
to MH are the concepts of delayed rejection (DR) and adaptive Metropolis
(AM) ; combining these lead to the DRAM algorithm of Haario et al. . While
countless other MCMC algorithms exist or are under active development, some involving derivatives (e.g., Langevin) or even Hessians of the posterior density, DRAM
oﬀers a good balance of simplicity and eﬃciency in the present context.
Even with eﬃcient proposals, MCMC typically requires a tremendous number of
samples (tens of thousands or even millions) to compute posterior estimates with
acceptable accuracy.
Since each MCMC step requires evaluation of the posterior
density, which in turn requires evaluation of the likelihood and thus the forward
model G, surrogate models for the dependence of G on θ can oﬀer tremendous
computational savings. Polynomial chaos surrogates, as described in Section 3, can
be quite helpful in this context .
5. Application: nonlinear model
We ﬁrst illustrate the optimal design of experiments using a simple algebraic
model, nonlinear in both the parameters and the design variables. Since the model
is inexpensive to evaluate, we use it to illustrate features of the core formulation—
estimating expected information gain, designing single and multiple experiments, and
the role of prior information—leaving demonstrations of stochastic optimization and
polynomial chaos surrogates to the next section.
5.1. Design of a single experiment
Consider a simple nonlinear model with a scalar observable y, one uncertain parameter θ, and one design variable d:
G(θ, d) + ǫ
θ3d2 + θ exp(−|0.2 −d|) + ǫ,
where G(θ, d) denotes the model output (without noise) and ǫ ∼N (0, 10−4) is an
additive Gaussian measurement error. Let the prior be θ ∼U(0, 1) and the design
space be d ∈ .
Suppose our experimental goal is to infer the uncertain parameter θ based on a
single measurement y. The expected utility U(d) in Equation (6) and its estimate
ˆU(d) in Equation (9) are appropriate choices, and our ultimate goal is to maximize
U(d). Figure 2(a) shows estimates of the expected utility, using nout = nin = 105,
plotted along a 101-node uniform grid spanning the entire design space. Local maxima
appear at d = 0.2 and d = 1.0, a pattern which can be understood by examining
Equation (28). A d value away from 0.2 or 1.0 (such as d = 0) would lead to an
observation y that is dominated by the noise ǫ, which is not useful for inferring the
uncertain parameter θ. But if d is chosen close to 0.2 or 1.0, such that the noise is
insigniﬁcant compared to the ﬁrst or second term of the equation, then y would be
very informative for θ.
5.2. Design of two experiments
Consider the “batch” or ﬁxed design of two experiments (where the results of
one experiment cannot be used to design the other, as described in Section 2.2).
Moreover, assume that both experiments are described by the same model; this is
not a requirement, but an assumption adopted here for simplicity. Then the overall
algebraic model is simply extended to
G(θ, d) + ǫ
1 + θ exp(−|0.2 −d1|)
2 + θ exp(−|0.2 −d2|)
where the subscripts ·1 and ·2 denote variables associated with experiments 1 and 2,
respectively. Note that there is still a single common parameter θ. The errors ǫ1 and
ǫ2 are i.i.d. with ǫ1, ǫ2 ∼N (0, 10−4).
Again using a U(0, 1) prior on θ, the expected utility is plotted in Figure 3(a).
First, note the symmetry in the contours along the d1 = d2 line, which is expected
since the two experiments have identical structure. Second, the optimal pair of experiments is not just a repeat of the optimal single-experiment design: d∗̸= (1, 1), and
instead we have d∗= (0.2, 1.0) or (1.0, 0.2). Some insight can be obtained by examining Figure 4, which plots the single-experiment model output G(θ, d) as a function
of the uncertain parameter θ at the two locally optimal designs: d = 0.2 and d = 1.
Intuitively, a high slope of G should be more informative for the inference of θ, as the
output is then more sensitive to variations in the input. The plot shows that neither
design has a greater slope over the entire range of the prior θ ∼U(0, 1). Instead, the
slope is greater for θ ∈[0, θe] with design d = 0.2, and greater for θ ∈[θe, 1.0] with
design d = 1.0, where θe =
Let us then examine the cases of “restricted” priors θ ∼U(0, θe) and θ ∼U(θe, 1).
Expected utilities for a single experiment, under either of these priors, are shown in
Figures 2(b) and 2(c). The optimal design for U(0, θe) is at 0.2 and for U(θe, 1) it is
at 1.0, supporting intuition from the analysis of slopes. Next, the expected utilities
of two experiments, under the restricted priors, are shown in Figures 3(b) and 3(c).
Since in both cases, a single design point can give G maximum slope over the entire
restricted prior range of θ, it is not surprising that the optimal pair of experiments
involves repeating the respective single-experiment optima. In contrast, the lack of
a “clear winner” over the full prior U(0, 1) intuitively explains why a combination
of diﬀerent design conditions may yield more informative data overall. Note that
we have only focused on the two local optima d = 0.2 and d = 1 from the original
θ ∼U(0, 1) analysis, but it is possible that new local or globally optimal design points
could emerge as the prior is changed.
6. Application: combustion kinetics
Experimental diagnostics play an essential role in the development and reﬁnement
of chemical kinetic models for combustion . Available diagnostics are often
indirect, imprecise, and incomplete, leaving signiﬁcant uncertainty in relevant rate
parameters and thermophysical properties . Uncertainties are particularly acute when developing kinetic models for new combustion regimes or for fuels
derived from new feedstocks, such as biofuels. Questions of experimental design—e.g.,
which species to interrogate and under what conditions—are thus of great practical
importance in this context.
6.1. Model description
We demonstrate our optimal experimental design framework on shock tube ignition experiments, which are a canonical source of kinetic data. In a shock tube
experiment, the mixture behind the reﬂected shock experiences a sharp rise in tempeature and pressure; if conditions are suitable, this mixture then ignites after some
time, known as the ignition delay time. Ignition delays and other observables extracted from the experiment carry indirect information about the elementary chemical kinetic processes occurring in the mixture. These experiments are well described
by the dynamics of a spatially homogeneous, adiabatic, constant-pressure chemical
We model the evolution of the mixture using ordinary diﬀerential equations (ODEs)
expressing conservation of energy and of individual chemical species. Governing equations are detailed in Appendix C. We consider an initial mixture of hydrogen and
oxygen. (Note that H2-O2 kinetics are a key subset of the reaction mechanisms associated with the combustion of complex hydrocarbons.) Our baseline kinetic model
is a 19-reaction mechanism proposed in , reproduced in Table 1. Detailed chemical kinetics lead to a stiﬀset of nonlinear ODEs, with state variables consisting
of temperature and species mass or molar fractions.
The initial condition of the
system is speciﬁed by the initial temperature T0 and the fuel-oxidizer equivalence
ratio φ. Species production rates depend on the mixture conditions and on a set of
kinetic parameters: pre-exponential factors Am, temperature exponents bm, and activation energies Ea,m, where m is the reaction number in Table 1. These parameters
are important in determining combustion characteristics and are of great interest in
practice. Thermodynamic parameters and reaction rates in the governing equations
are evaluated with the help of Cantera 1.7.0 , an open-source chemical kinetics software package. ODEs are solved implicitly, using the variable-order backwards
diﬀerentiation formulas implemented in CVODE .
6.2. Experimental goals
In this study, the experimental goal is to infer selected kinetic parameters (Am, bm,
and Ea,m) associated with the elementary reactions in Table 1. For demonstration, we
let the kinetic parameters of interest be A1 and Ea,3. Reaction 1 is a chain-branching
reaction, leading to a net increase in the number of radical species in the system.
Reaction 3 is a chain-propagating reaction, exchanging one radical for another, but
nonetheless relevant to the overall dynamics.6 We infer ln(A1/A0
1) rather than A1
directly, where A0
1 is the nominal value of A1 in ; this transformation ensures
positivity and lets us easily impose a log-uniform prior on A1, which is appropriate
since the pre-exponential is a multiplicative factor. The design variables are the initial
temperature T0 and equivalence ratio φ.
We once again use the expected utility deﬁned in Equation (6) (and its estimator
in Equation (9)) as the objective to be maximized for optimal design. Unlike the
algebraic model in Section 5, however, this combustion problem oﬀers many possible
choices of observable. Some observables are more informative than others; we explore
this choice in the next section.
6.3. Observables and likelihood function
Typical trajectories of the state variables are shown in Figure 5. The temperature
rises suddenly upon ignition; reactant species are rapidly consumed and product
species are produced as the mixture comes to equilibrium. The most complete and
detailed set of system observables are the state variables as a function of time. One
could simply discretize the time domain to produce a ﬁnite-dimensional data vector
y. Too few discretization points might fail to capture the state behaviour, however.
And because the kinetic parameters aﬀect ignition delay, the state at any given time
may have a nearly discontinuous dependence on the parameters. (This is due to the
sharpness of the ignition front; at a ﬁxed time, the state is most probably either
pre-ignition or post-ignition.) Such a dependence makes construction of a polynomial
chaos surrogate far more challenging . It is desirable to transform the state into
6As the methodology explored here is quite general, we have the freedom to select any parameters
appearing in the mechanism. The selection reﬂects the particular goals of the experimentalist or
investigator. We also note that the “evaluated” combustion kinetic data in can help select
parameters to target for inference and help deﬁne their prior ranges.
alternate observables that somehow “compress” the information and depend relatively
smoothly on the kinetic parameters, while retaining features that are relevant to the
experimental goals. We would also like to select observables that are easy to obtain
experimentally.
Taking the above factors into consideration, we will use the observables in Table 2.
The observables are the peak value of the heat release rate, the peak concentrations
of various intermediate chemical species (O, H, HO2, H2O2), and the times at which
these peak values occur. Examples of τign, τH, dh
τ, and XH,τ are shown in Figure 6.
The time of peak heat release coincides with the time at which temperature rises
most rapidly. We thus take it as our deﬁnition of ignition delay, τign. We use the
logarithm of all the characteristic time variables in our actual implementation, as
the times are positive and vary over several orders of magnitude as a function of the
kinetic parameters and design variables.
The likelihood is deﬁned using the ODE model predictions and independent additive Gaussian measurement errors: y = G(θ, d) + ǫ, with components ǫc ∼N (0, σ2
For the concentration observables, the standard deviation of the measurement error
is taken to be 10% of the value of the corresponding signal:
c = 0.1Gc(θ, d).
For the characteristic-time observables, we add a small constant α = 10−5 s to the
standard deviation, reﬂecting the minimum resolution of the timing technology:
c = 0.1Gc(θ, d) + α.
Note that the noise magnitude depends implicitly on both the kinetic parameters
and the design variables. Both terms contributing to the expected information gain
in Equation (8) are therefore inﬂuential, and one would expect a maximum entropy
sampling approach to yield diﬀerent results than the present experimental design
methodology.
6.4. Polynomial chaos construction
Each solve of the ODE system deﬁning G(θ, d) is expensive, and thus we employ
a polynomial chaos surrogate. In practice, since non-intrusive construction of the surrogate requires many forward model evaluations, the surrogate is only worth forming
if the total number of model evaluations required for optimization of the expected
utility exceeds the number required for surrogate construction. A detailed analysis of
this tradeoﬀand the potential computational gains can be found in Section 6.7.
Uniform priors are assigned to the model parameters θ ≡[ln(A1/A0
1), Ea,3] and
uniform input distributions are assumed for the design variables [T0, φ] (see Section 3.1), with the supports given in Table 3. The polynomial chaos expansions thus
use Legendre polynomials, with ξ1, ξ2, ξ3, ξ4 ∼U(−1, 1). Our goal now is to construct
PC expansions for the model outputs G(θ, d) = (ln τign, ln τO, ln τH, ln τHO2, ln τH2O2,
τ , XO,τ, XH,τ, XHO2,τ, XH2O2,τ), using the projection given in Equation (21). For
each desired PC coeﬃcient, the numerator in that equation is evaluated using the
modiﬁed DASQ algorithm described in Section 3.3. The expansions are truncated
at a total order of p = 12, and DASQ is stopped once a total of nquad = 106 function evaluations have been exceeded.
The degree of this expansion is admittedly
(and deliberately) chosen rather high. The performance of lower-order expansions is
examined below and in Section 6.7.
Indeed, the accuracy of the PC surrogate can be analyzed more rigorously by
evaluating its relative L2 error over a range of p and nquad values:
Gc(θ(ξ), d(ξ)) −G
Ξ |Gc(θ(ξ), d(ξ))|2 p(ξ) dξ
c = 1 . . . ny.
For the cth observable, Gc is the output of the original ODE model and G
corresponding PC surrogate. θ and d are aﬃne functions of ξ (the PC expansions
of the model inputs). Accurately evaluating the L2 error is expensive, certainly more
expensive than computing G
in the ﬁrst place. But additional integration error
must be minimized, and the integrals in Equation (32) are thus evaluated using a level-
15 isotropic Clenshaw-Curtis sparse quadrature rule, containing 3,502,081 distinct
abscissae.
Figure 7 shows contours of log10 of the L2 error over a range of p and nquad, for the
PC expansion of the peak enthalpy release rate. The nquad values are approximate,
as DASQ is terminated at the end of the iteration that exceeds nquad. When nquad
is too small, the error is dominated by aliasing (integration) error and increases with
When a suﬃciently large nquad is used such that truncation error dominates,
exponential convergence with respect to p can be observed, as expected for smooth
functions. Ideally, nquad and p should be selected at the “knees” of these contour
plots, since little accuracy can be gained when nquad is increased any further, but
these locations can be diﬃcult to pinpoint a priori.
6.5. Design of a single experiment
Figures 8(a) and 8(b) show contours of the expected utility estimates, ˆU(d), in the
two-dimensional design space, constructed using the full ODE model (with estimator
parameters nin = nout = 103) and the PC surrogate (with estimator parameters
nin = nout = 104), respectively. Contours from the PC surrogate are very similar to
those from the full model, though the former have less variability due to the larger
number of Monte Carlo samples used to compute ˆU. Most importantly, both plots
yield the same optimal experimental design at around (T ∗
0 , φ∗) = (975, 0.5).
To test how well the expected information gain anticipates the performance of an
experiment, the inference problem is solved at three diﬀerent design points A, B, and
C, listed in Table 4 and illustrated in Figure 8(b). Since the expected utility is highest
at design A, then the posterior is expected to reﬂect the largest information gain at
that experimental condition. We use the full ODE model to generate artiﬁcial data
at each of the three design conditions, then perform inference. Contours of posterior
density are shown in Figure 9, using the full ODE model and the PC surrogate. The
posteriors of the full model and PC surrogate match very well; hence the PC surrogate
is suitable not only for experimental design, but also for inference.
As expected
from the expected utility plots, the posterior distribution of the kinetic parameters
is tightest at design A; this was the most informative of the three experimental
conditions. Posterior modes of the ODE model and PC results are not precisely the
same, however, due to the modeling error associated with the PC surrogate. Also, the
posterior modes obtained with the full ODE model do not exactly match the values
used to generate the artiﬁcial data, due to the noise in the likelihood model.
What if a diﬀerent set of observables are used? Two cases are explored: ﬁrst,
using only the characteristic time observables (i.e., the ﬁrst ﬁve rows of Table 2); and
second, using only the peak value observables (i.e., the last ﬁve rows of Table 2). The
corresponding expected utility plots are given in Figure 10 (using the PC surrogate
only). Several remarks can be made. First, the characteristic time observables are
more informative than the peak value observables, as demonstrated by the higher
expected utility values in Figure 10(b) than Figure 10(c).
Second, the choice of
observables can greatly inﬂuence the optimal value of the design parameters. Third,
even though the observables from the two cases form a partition of the full observable
set, their expected utility values do not simply sum to that of the full-observable
case. (This is a special case of the analysis in Appendix A.) The lesson is that the
selection of appropriate observables is a very important part of the design procedure,
especially if one is forced to select only a few modes of observation. This selection
could be made into an argument of the objective function, augmenting d and leading
to a mixed-integer optimization problem.
6.6. Design of two experiments: stochastic optimization
Now we perform a study analogous to that in Section 5.2, designing two ignition
experiments (of the same structure) simultaneously. The experimental goal of inferring A1 and Ea,3 is unchanged, and for computational eﬃciency we use only the
p = 12, nquad = 106 PC surrogate. The design space is now four-dimensional, with
d = [T0,1, φ1, T0,2, φ2]. Stochastic optimization is used to ﬁnd the optimal experimental design, as a grid search is entirely impractical.
Coupling stochastic optimization schemes (Section 2.4) with the estimator ˆU(d)
of expected information gain introduces a few new numerical tradeoﬀs. The number
of samples in the outer loop of the estimator controls the variance of ˆU, which dictates
the noise level of the objective function. Lower noise in the objective function might
imply fewer optimization iterations overall, while a noisier objective may require many
more iterations of either SPSA or NMNS to make progress towards the optimum. On
the other hand, noise should not be reduced too much for SPSA, since the usefulness
of its gradient approximation relies on the existence of a non-negligible noise level.
In general, the task of balancing nout against the number of optimization iterations,
in order to minimize the number of model evaluations, is not trivial. We therefore
test diﬀerent values of nout to understand their impact on the SPSA and NMNS
optimization schemes. We ﬁx nin at 104 in order to maintain a low bias contribution
from the inner loop.
Since the results of stochastic optimization are themselves random, we use an
ensemble of 100 independent optimization runs at any given parameter setting to
analyze performance. Each optimization run is capped at nnoisyObj = 104 evaluations
of the noisy objective—i.e., of the summand in Equation (9)—where each noisy objective evaluation itself involves nin evaluations of the model G or the surrogate Gp.
We can thus compare performance at a ﬁxed computational cost.
Runs are performed for both SPSA and NMNS, with nout ranging from 1 to 100.
The ﬁnal design conditions and convergence histories (the latter plotted for 10 runs
only) are shown in Figures 11 and 12, respectively. In Figure 11, each connected
blue cross and red circle represent a pair of ﬁnal experimental designs, where the red
circle is arbitrarily chosen to represent the lower T0 design. The optimization results
indicate that both experiments should be performed near T0 = 975 K, although the
best φ is less precisely determined and less inﬂuential. This pattern is similar to that
of a single-experiment optimal design. Overall, a tighter clustering of the ﬁnal design
points is observed as nout is increased. SPSA groups the majority of the ﬁnal design
points more tightly, but it also yields more outliers than NMNS; in other words, it
results in more of a “hit-or-miss” situation. Figure 11 indicates that, for the NMNS
cases, a lower nout lets the algorithm reach the convergence “plateau” more quickly.
This result is aﬀected both by the shrinkage rate of the simplex and by the fact that
a higher nout simply requires more model evaluations even to complete one iteration.
The choice of nout then should take into consideration how many model evaluations
are available. Even then, the best choice may depend on the shape of the expected
utility surface and the variance of its estimator (which is not stationary in d).
Our observations so far are based on an assessment of the design locations and
convergence history. A more quantitative analysis should focus on the expected utility value of the ﬁnal designs, which is what really matters in the end. Figure 13
shows histograms of expected utility for the 100 ﬁnal design points resulting from
each optimization case. To compare the design points, we want to make the error
incurred in estimating U(d∗) relatively negligible, and thus we employ a high-quality
estimator with nout = nin = 104. This is not the small-sample estimator used inside
the optimization algorithms; it is a more expensive estimator used afterwards, only
for diagnostic purposes. The histograms indicate that increasing nout is actually not
very eﬀective for SPSA, as the persistence of outliers creates a spread in the ﬁnal values, supporting our suspicion that too small a noise level may be bad for SPSA. On
the other hand, increasing nout is eﬀective for NMNS. The bimodal structure of the
histograms is due to the two groups: good designs on the center “ridge” and outliers,
with few designs having expected utility values in between. Overall, NMNS performs
better than SPSA in this study, both in terms of the asymptotic distribution of design
parameters and how quickly the convergence plateau or “knee” is reached.
To validate the results, the parameter inference problem is solved with data from
three two-experiment design points (labeled D, E, and F) and with data from a fourexperiment factorial design. All of the experimental conditions are listed in Table 5.
Design D, a pair of experiments lying on the ridge of “good designs,” is expected to
have the tightest posterior among the two-experiment designs. The posteriors are
shown in Figure 14, using the PC surrogate only. Indeed, design D has the tightest
posterior, and is much better than the four-experiment factorial design even though it
uses fewer experiments! The factorial design blindly picks all the corners in the design
space, which are in general not good design points. (The number of experiments
in a factorial design would also increase exponentially with the number of design
parameters, becoming impractical very quickly.) Model-based optimal experimental
design is far more robust and capable than this traditional method.
6.7. Is using a PC surrogate worthwhile?
In producing the previous results, we used a PC surrogate with p = 12 and nquad =
106, though analysis for one observable in Figure 7 suggests that this polynomial
truncation and adapted quadrature rule are perhaps of higher quality than necessary
for our problem. To quantitatively determine whether a polynomial surrogate oﬀers
eﬃciency gains in this study (or any other), we must (i) check if a lower quality PC
surrogate may be used while still achieving results of similar quality, and (ii) analyze
the computational cost.
To check if a lower-quality PC surrogate would suﬃce, the same two-experiment
optimal experimental design problem is solved but now with p = 6. We ﬁnd that
nquad = 104 is roughly the smallest nquad that still yields reasonable results. The
ﬁnal optimization positions (obtained via NMNS only), the convergence history, and
histogram of ﬁnal expected utilities are shown in Figure 15. In fact, the histogram
appears to be even tighter than that obtained with the p = 12 surrogate.
To analyze the computational cost, we assume that optimization is terminated after 5000 noisy objective function evaluations, as the practical convergence plateau is
reached by that point. If each noisy objective function requires 104 inner Monte Carlo
samples and each PC evaluation is negligible compared to full model, then using the
full ODE model requires 5000 × 104 = 5 × 107 model evaluations, whereas the construction of the PC surrogate requires 104 full ODE model evaluations. The surrogate
thus provides a speedup of roughly 3.5 orders of magnitude, saving 49,990,000 full
model evaluations (roughly 4 months of computational time for the present problem,
if run serially).
Even though a low-quality surrogate may be suﬃcient for optimal experimental
design, it may not be suﬃciently accurate for parameter inference. For example, the
two-experiment and four-experiment posteriors obtained using the p = 6, nquad = 104
surrogate are shown in Figure 16. These posterior density contours show a substantial
loss of accuracy compared to the corresponding plots in Figure 14. Because inference
does not involve averaging over the data space and broadly exploring the design space,
and because it generally favors a more restricted range of the model parameters
θ, it may be more sensitive to local errors than the optimal experimental design
formulation. There are two possible solutions to this issue:
1. Build and use a high-order polynomial chaos surrogate at the outset of analysis,
and use it for both optimal experimental design and inference. Because inference
typically employs MCMC and thus requires many thousands or even millions
of model evaluations, the combined computational savings will make such a
surrogate almost certainly worth constructing.
2. A more eﬃcient approach is to use a low-order polynomial chaos expansion to
perform the optimal experimental design. Upon reaching the optimal design
conditions, construct a new PC expansion for inference at that design only. The
new expansion does not need to capture dependence on the design variables,
and thus it involves a smaller dimension and fewer interactions. This less expensive local PC expansion can more easily be made suﬃciently accurate for
the inference problem.
7. Conclusions
This paper presents a systematic framework and a set of computational tools for
the optimal design of experiments. The framework can incorporate nonlinear and
computationally intensive models of the experiment, linking them to rigorous information theoretic design criteria and requiring essentially no simplifying assumptions.
A ﬂowchart of the overall framework is given in Figure 1, showing the steps of optimal
design and their role in a larger design–experimentation–model improvement cycle.
We focus on the experimental goal of parameter inference, in a Bayesian statistical setting, where a good design criterion is shown to maximize expected information
gain from prior to posterior. A two-stage Monte Carlo estimator of expected information gain is coupled to algorithms for stochastic optimization.
The estimation
of expected information gain, which would otherwise be prohibitive with computationally intensive models, is substantially accelerated through the use of ﬂexible and
accurate polynomial chaos surrogate representations.
We demonstrate our method ﬁrst on a simple nonlinear algebraic model, then
on shock tube ignition experiments described by a stiﬀsystem of nonlinear ODEs.
The latter system is challenging to approximate, as certain model observables depend
sharply on the combustion kinetic parameters and design conditions, and ignition
delays vary over several orders of magnitude. In both these examples, we illustrate the
design of single and multiple experiments. We analyze the impact of prior information
on the optimal designs, and examine the selection of observables according to their
information value. We also investigate numerical tradeoﬀs between variance in the
estimator of expected utility and performance of the stochastic optimization schemes.
Overall we ﬁnd that inference at optimal design conditions is indeed very informative about the targeted parameters, and that model-based optimal experiments
are far more informative than those obtained with simple heuristics.
The use of
surrogates oﬀers signiﬁcant computational savings over stochastic optimization with
the full model, more than three orders of magnitude in the examples tested here.
Moreover, we ﬁnd that the polynomial surrogate used in optimal experimental design
need not be extremely accurate in order to reveal the correct design points; surrogate
requirements for optimal design are less stringent than for parameter inference.
Several promising avenues exist for future work.
More eﬃcient means of constructing polynomial chaos expansions, adaptively and in conjunction with stochastic
optimization, may oﬀer considerable computational gains. Uncertainty in the design
parameters themselves can also be incorporated into the framework, as in real-world
experiments where the design conditions cannot be precisely imposed; this additional
uncertainty could be treated with a hierarchical Bayesian approach. Structural inadequacy of the model G is another important issue; how successful is an “optimal” design
(or indeed an inference process) based on a forward model that cannot fully resolve
the physical processes at hand? Our current experience on design with lower-order
polynomial surrogates provides a glimpse into issues of structural uncertainty, but
a much more thorough exploration is needed. Finally, the Bayesian optimal design
methodology has a natural extension to sequential experimental design, where one set
of experiments can be performed and analyzed before designing the next set. A rigorous approach to sequential design, incorporating ideas from dynamic programming
and perhaps sequential Monte Carlo, may be quite eﬀective.
Acknowledgments
The authors would like to acknowledge support from the KAUST Global Research
Partnership and from the US Department of Energy, Oﬃce of Science, Oﬃce of
Advanced Scientiﬁc Computing Research (ASCR) under Grant No. DE-SC0003908.