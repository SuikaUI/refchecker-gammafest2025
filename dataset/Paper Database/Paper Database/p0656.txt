Published as a conference paper at ICLR 2020
ON THE RELATIONSHIP BETWEEN SELF-ATTENTION
AND CONVOLUTIONAL LAYERS
Jean-Baptiste Cordonnier, Andreas Loukas & Martin Jaggi
´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)
{first.last}@epfl.ch
Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. showed that attention can completely replace convolution and
achieve state-of-the-art performance on vision tasks. This raises the question: do
learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Speciﬁcally, we prove that a multi-head self-attention
layer with sufﬁcient number of heads is at least as expressive as any convolutional
layer. Our numerical experiments then show that self-attention layers attend to
pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code
is publicly available1.
INTRODUCTION
Recent advances in Natural Language Processing (NLP) are largely attributed to the rise of the transformer . Pre-trained to solve an unsupervised task on large corpora of text,
transformer-based architectures, such as GPT-2 , BERT 
and Transformer-XL , seem to possess the capacity to learn the underlying structure
of text and, as a consequence, to learn representations that generalize across tasks. The key difference between transformers and previous methods, such as recurrent neural networks and convolutional neural networks (CNN), is that the former can simultaneously attend to every word of their input sequence. This is made possible thanks to the attention
mechanism—originally introduced in Neural Machine Translation to better handle long-range dependencies . With self-attention in particular, the similarity of two words in
a sequence is captured by an attention score measuring the distance of their representations. The
representation of each word is then updated based on those words whose attention score is highest.
Inspired by its capacity to learn meaningful inter-dependencies between words, researchers have
recently considered utilizing self-attention in vision tasks. Self-attention was ﬁrst added to CNN
by either using channel-based attention or non-local relationships across the image
 . More recently, Bello et al. augmented CNNs by replacing some convolutional layers with self-attention layers, leading to improvements on image classiﬁcation and object
detection tasks. Interestingly, Ramachandran et al. noticed that, even though state-of-the
art results are reached when attention and convolutional features are combined, under same computation and model size constraints, self-attention-only architectures also reach competitive image
classiﬁcation accuracy.
These ﬁndings raise the question, do self-attention layers process images in a similar manner to
convolutional layers? From a theoretical perspective, one could argue that transfomers have the
capacity to simulate any function—including a CNN. Indeed, P´erez et al. showed that a multilayer attention-based architecture with additive positional encodings is Turing complete under some
strong theoretical assumptions, such as unbounded precision arithmetic. Unfortunately, universality
results do not reveal how a machine solves a task, only that it has the capacity to do so. Thus, the
question of how self-attention layers actually process images remains open.
1Code: github.com/epfml/attention-cnn. Website: epfml.github.io/attention-cnn.
 
Published as a conference paper at ICLR 2020
Contributions.
In this work, we put forth theoretical and empirical evidence that self-attention
layers can (and do) learn to behave similar to convolutional layers:
I. From a theoretical perspective, we provide a constructive proof showing that self-attention
layers can express any convolutional layers.
Speciﬁcally, we show that a single multi-head self-attention layer using relative positional encoding
can be re-parametrized to express any convolutional layer.
II. Our experiments show that the ﬁrst few layers of attention-only architectures do learn to attend on grid-like pattern around each query pixel, similar to
our theoretical construction.
Strikingly, this behavior is conﬁrmed both for our quadratic encoding, but also for relative encoding
that is learned. Our results seem to suggest that localized convolution is the right inductive bias
for the ﬁrst few layers of an image classifying network. We provide an interactive website2 to
explore how self-attention exploits localized position-based attention in lower layers and contentbased attention in deeper layers. For reproducibility purposes, our code is publicly available.
BACKGROUND ON ATTENTION MECHANISMS FOR VISION
We here recall the mathematical formulation of self-attention layers and emphasize the role of positional encodings.
THE MULTI-HEAD SELF-ATTENTION LAYER
Let X ∈RT ×Din be an input matrix consisting of T tokens in of Din dimensions each. While in
NLP each token corresponds to a word in a sentence, the same formalism can be applied to any
sequence of T discrete objects, e.g. pixels. A self-attention layer maps any query token t ∈[T]
from Din to Dout dimensions as follows:
Self-Attention(X)t,: := softmax (At,:) XWval,
where we refer to the elements of the T × T matrix
A := XWqryW ⊤
as attention scores and the softmax output3 as attention probabilities. The layer is parametrized
by a query matrix Wqry ∈RDin×Dk, a key matrix Wkey ∈RDin×Dk and a value matrix Wval ∈
RDin×Dout.For simplicity, we exclude any residual connections, batch normalization and constant
A key property of the self-attention model described above is that it is equivariant to reordering, that
is, it gives the same output independently of how the T input tokens are shufﬂed. This is problematic
for cases we expect the order of things to matter. To alleviate the limitation, a positional encoding
is learned for each token in the sequence (or pixel in an image), and added to the representation of
the token itself before applying self-attention
A := (X + P )WqryW ⊤
key(X + P )⊤,
where P ∈RT ×Din contains the embedding vectors for each position. More generally, P may be
substituted by any function that returns a vector representation of the position.
It has been found beneﬁcial in practice to replicate this self-attention mechanism into multiple heads,
each being able to focus on different parts of the input by using different query, key and value
matrices. In multi-head self-attention, the output of the Nh heads of output dimension Dh are
concatenated and projected to dimension Dout as follows:
MHSA(X) := concat
Self-Attentionh(X)
Wout + bout
and two new parameters are introduced: the projection matrix Wout ∈RNhDh×Dout and a bias term
bout ∈RDout.
2epfml.github.io/attention-cnn
3softmax (At,:)k = exp(At,k)/ P
p exp(At,p)
Published as a conference paper at ICLR 2020
ATTENTION FOR IMAGES
Convolutional layers are the de facto choice for building neural networks that operate on images.
We recall that, given an image tensor X ∈RW ×H×Din of width W, height H and Din channels, the
output of a convolutional layer for pixel (i, j) is given by
Conv(X)i,j,: :=
(δ1,δ2)∈∆∆K
Xi+δ1,j+δ2,:Wδ1,δ2,:,: + b,
where W is the K × K × Din × Dout weight tensor 4, b ∈RDout is the bias vector and the set
contains all possible shifts appearing when convolving the image with a K × K kernel.
In the following, we review how self-attention can be adapted from 1D sequences to images.
With images, rather than tokens, we have query and key pixels q, k ∈[W] × [H]. Accordingly, the
input is a tensor X of dimension W × H × Din and each attention score associates a query and a key
To keep the formulas consistent with the 1D case, we abuse notation and slice tensors by using a 2D
index vector: if p = (i, j), we write Xp,: and Ap,: to mean Xi,j,: and Ai,j,:,:, respectively. With this
notation in place, the multi-head self attention layer output at pixel q can be expressed as follows:
Self-Attention(X)q,: =
softmax (Aq,:)k Xk,: Wval
and accordingly for the multi-head case.
POSITIONAL ENCODING FOR IMAGES
There are two types of positional encoding that has been used in transformer-based architectures:
the absolute and relative encoding (see also Table 3 in the Appendix).
With absolute encodings, a (ﬁxed or learned) vector Pp,: is assigned to each pixel p. The computation of the attention scores we saw in eq. (2) can then be decomposed as follows:
q,k = (Xq,: + Pq,:)WqryW ⊤
key(Xk,: + Pk,:)⊤
= Xq,:WqryW ⊤
k,: + Xq,:WqryW ⊤
k,: + Pq,:WqryW ⊤
keyXk,: + Pq,:WqryW ⊤
where q and k correspond to the query and key pixels, respectively.
The relative positional encoding was introduced by Dai et al. . The main idea is to only
consider the position difference between the query pixel (pixel we compute the representation of)
and the key pixel (pixel we attend) instead of the absolute position of the key pixel:
qryWkey Xk,: + X⊤
Wkey rδ + u⊤Wkey Xk,: + v⊤c
In this manner, the attention scores only depend on the shift δ := k −q. Above, the learnable
vectors u and v are unique for each head, whereas for every shift δ the relative positional encoding
rδ ∈RDp is shared by all layers and heads. Moreover, now the key weights are split into two types:
Wkey pertain to the input and c
Wkey to the relative position of pixels.
SELF-ATTENTION AS A CONVOLUTIONAL LAYER
This section derives sufﬁcient conditions such that a multi-head self-attention layer can simulate a
convolutional layer. Our main result is the following:
Theorem 1. A multi-head self-attention layer with Nh heads of dimension Dh, output dimension Dout and a relative positional encoding of dimension Dp ≥3 can express any convolutional
layer of kernel size √Nh × √Nh and min(Dh, Dout) output channels.
4To simplify notation, we index the ﬁrst two dimensions of the tensor from −⌊K/2⌋to ⌊K/2⌋.
Published as a conference paper at ICLR 2020
The theorem is proven constructively by selecting the parameters of the multi-head self-attention
layer so that the latter acts like a convolutional layer. In the proposed construction, the attention
scores of each self-attention head should attend to a different relative shift within the set ∆∆K =
{−⌊K/2⌋, . . . , ⌊K/2⌋}2 of all pixel shifts in a K × K kernel. The exact condition can be found in
the statement of Lemma 1.
Then, Lemma 2 shows that the aforementioned condition is satisﬁed for the relative positional encoding that we refer to as the quadratic encoding:
v(h) := −α(h) (1, −2∆(h)
1 , −2∆(h)
rδ := (∥δ∥2, δ1, δ2)
Wqry =Wkey := 0
The learned parameters ∆(h) = (∆(h)
2 ) and α(h) determine the center and width of attention
of each head, respectively. On the other hand, δ = (δ1, δ2) is ﬁxed and expresses the relative shift
between query and key pixels.
It is important to stress that the above encoding is not the only one for which the conditions of
Lemma 1 are satisﬁed. In fact, in our experiments, the relative encoding learned by the neural
network also matched the conditions of the lemma (despite being different from the quadratic encoding). Nevertheless, the encoding deﬁned above is very efﬁcient in terms of size, as only Dp = 3
dimensions sufﬁce to encode the relative position of pixels, while also reaching similar or better
empirical performance (than the learned one).
The theorem covers the general convolution operator as deﬁned in eq. (17). However, machine
learning practitioners using differential programming frameworks might question if the theorem holds for all hyper-parameters of 2D convolutional layers:
• Padding: a multi-head self-attention layer uses by default the "SAME" padding while a
convolutional layer would decrease the image size by K −1 pixels. The correct way to
alleviate these boundary effects is to pad the input image with ⌊K/2⌋zeros on each side.
In this case, the cropped output of a MHSA and a convolutional layer are the same.
• Stride: a strided convolution can be seen as a convolution followed by a ﬁxed pooling
operation—with computational optimizations. Theorem 1 is deﬁned for stride 1, but a
ﬁxed pooling layer could be appended to the Self-Attention layer to simulate any stride.
• Dilation: a multi-head self-attention layer can express any dilated convolution as each head
can attend a value at any pixel shift and form a (dilated) grid pattern.
Remark for the 1D case.
Convolutional layers acting on sequences are commonly used in the literature for text , as well as audio and time series . Theorem 1 can be straightforwardly extended to show that multi-head self-attention
with Nh heads can also simulate a 1D convolutional layer with a kernel of size K = Nh with
min(Dh, Dout) output channels using a positional encoding of dimension Dp ≥2. Since we have
not tested empirically if the preceding construction matches the behavior of 1D self-attention in
practice, we cannot claim that it actually learns to convolve an input sequence—only that it has the
capacity to do so.
PROOF OF MAIN THEOREM
The proof follows directly from Lemmas 1 and 2 stated below:
Lemma 1. Consider a multi-head self-attention layer consisting of Nh = K2 heads, Dh ≥Dout
and let f : [Nh] →∆∆K be a bijective mapping of heads onto shifts. Further, suppose that for
every head the following holds:
softmax(A(h)
if f(h) = q −k
otherwise.
Then, for any convolutional layer with a K × K kernel and Dout output channels, there exists
val }h∈[Nh] such that MHSA(X) = Conv(X) for every X ∈RW ×H×Din.
Published as a conference paper at ICLR 2020
Attention maps for pixel
Filter matrices
Multi-Head Self-Attention Layer
at position
the query pixel
a key pixel
at position
concatenate
Figure 1: Illustration of a Multi-Head Self-Attention layer applied to a tensor image X. Each head h
attends pixel values around shift ∆(h) and learn a ﬁlter matrix W (h)
val . We show attention maps
computed for a query pixel at position q.
Proof. Our ﬁrst step will be to rework the expression of the Multi-Head Self-Attention operator from
equation (1) and equation (4) such that the effect of the multiple heads becomes more transparent:
MHSA(X) = bout +
softmax(A(h))X W (h)
val Wout[(h −1)Dh + 1 : hDh + 1]
Note that each head’s value matrix W (h)
∈RDin×Dh and each block of the projection matrix Wout
of dimension Dh × Dout are learned. Assuming that Dh ≥Dout, we can replace each pair of
matrices by a learned matrix W (h) for each head. We consider one output pixel of the multi-head
self-attention:
MHSA(X)q,: =
softmax(A(h)
q,: )kXk,:
W (h) + bout
Due to the conditions of the Lemma, for the h-th attention head the attention probability is one when
k = q −f(h) and zero otherwise. The layer’s output at pixel q is thus equal to
MHSA(X)q =
Xq−f(h),:W (h) + bout
For K = √Nh, the above can be seen to be equivalent to a convolutional layer expressed in eq. 17:
there is a one to one mapping (implied by map f) between the matrices W (h) for h = [Nh] and the
matrices Wk1,k2,:,: for all (k1, k2) ∈[K]2.
Remark about Dh and Dout.
It is frequent in transformer-based architectures to set
= Dout/Nh, hence Dh < Dout. In that case, W (h) can be seen to be of rank Dout −Dh,
which does not sufﬁce to express every convolutional layer with Dout channels. Nevertheless, it can
be seen that any Dh out of Dout outputs of MHSA(X) can express the output of any convolutional
layer with Dh output channels. To cover both cases, in the statement of the main theorem we assert
that the output channels of the convolutional layer should be min(Dh, Dout). In practice, we advise
to concatenate heads of dimension Dh = Dout instead of splitting the Dout dimensions among heads
to have exact re-parametrization and no “unused” channels.
Lemma 2. There exists a relative encoding scheme {rδ ∈RDp}δ∈Z2 with Dp ≥3 and parameters Wqry, Wkey, c
Wkey, u with Dp ≤Dk such that, for every ∆∈∆∆K there exists some vector v
(conditioned on ∆) yielding softmax(Aq,:)k = 1 if k −q = ∆and zero, otherwise.
Proof. We show by construction the existence of a Dp = 3 dimensional relative encoding scheme
yielding the required attention probabilities.
Published as a conference paper at ICLR 2020
As the attention probabilities are independent of the input tensor X, we set Wkey = Wqry = 0 which
leaves only the last term of eq. (8). Setting c
Wkey ∈RDk×Dp to the identity matrix (with appropriate
row padding), yields Aq,k = v⊤rδ where
δ := k −q. Above, we have assumed that Dp ≤Dk
such that no information from rδ is lost.
Now, suppose that we could write:
Aq,k = −α(∥δ −∆∥2 + c)
for some constant c. In the above expression, the maximum attention score over Aq,: is −αc and it
is reached for Aq,k with δ = ∆. On the other hand, the α coefﬁcient can be used to scale arbitrarily
the difference between Aq,∆and the other attention scores.
In this way, for δ = ∆, we have
α→∞softmax(Aq,:)k = lim
e−α(∥δ−∆∥2+c)
k′ e−α(∥(k−q′)−∆∥2+c)
k′ e−α∥(k−q′)−∆∥2 =
1 + limα→∞
k′̸=k e−α∥(k−q′)−∆∥2 = 1
and for δ ̸= ∆, the equation becomes limα→∞softmax(Aq,:)k = 0, exactly as needed to satisfy
the lemma statement.
What remains is to prove that there exist v and {rδ}δ∈Z2 for which eq. (14) holds. Expanding the
RHS of the equation, we have −α(∥δ −∆∥2 + c) = −α(∥δ∥2 + ∥∆∥2 −2⟨δ, ∆⟩+ c) . Now if we
set v = −α (1, −2∆1, −2∆2) and rδ = (∥δ∥2, δ1, δ2), then
Aq,k = v⊤rδ = −α(∥δ∥2 −2∆1δ1 −2∆2δ2) = −α(∥δ∥2 −2⟨δ, ∆⟩) = −α(∥δ−∆∥2 −∥∆∥2),
which matches eq. (14) with c = −∥∆∥2 and the proof is concluded.
Remark on the magnitude of α.
The exact representation of one pixel requires α (or the matrices
Wqry and Wkey) to be arbitrary large, despite the fact that the attention probabilities of all other
pixels converge exponentially to 0 as α grows. Nevertheless, practical implementations always rely
on ﬁnite precision arithmetic for which a constant α sufﬁces to satisfy our construction. For instance,
since the smallest positive float32 scalar is approximately 10−45, setting α = 46 would sufﬁce
to obtain hard attention.
EXPERIMENTS
The aim of this section is to validate the applicability of our theoretical results—which state that
self-attention can perform convolution—and to examine whether self-attention layers in practice
do actually learn to operate like convolutional layers when trained on standard image classiﬁcation
tasks. In particular, we study the relationship between self-attention and convolution with quadratic
and learned relative positional encodings. We ﬁnd that, for both cases, the attention probabilities
learned tend to respect the conditions of Lemma 1, supporting our hypothesis.
IMPLEMENTATION DETAILS
We study a fully attentional model consisting of six multi-head self-attention layers. As it has already
been shown by Bello et al. that combining attention features with convolutional features
improves performance on Cifar-100 and ImageNet, we do not focus on attaining state-of-the-art
performance. Nevertheless, to validate that our model learns a meaningful classiﬁer, we compare
it to the standard ResNet18 on the CIFAR-10 dataset (Krizhevsky et al.). In all
experiments, we use a 2 × 2 invertible down-sampling on the input to reduce
the size of the image. As the size of the attention coefﬁcient tensors (stored during forward) scales
quadratically with the size of the input image, full attention cannot be applied to bigger images.
The ﬁxed size representation of the input image is computed as the average pooling of the last layer
representations and given to a linear classiﬁer.
Published as a conference paper at ICLR 2020
Test accuracy
SA quadratic emb.
SA learned emb.
SA learned emb. + content-based att.
Figure 2: Test accuracy on CIFAR-10.
# of params
# of FLOPS
SA quadratic emb.
SA learned emb.
SA learned emb. + content
Table 1: Test accuracy on CIFAR-10 and model
sizes. SA stands for Self-Attention.
Figure 3: Centers of attention of each attention head (different colors) at layer 4 during the training
with quadratic relative positional encoding. The central black square is the query pixel, whereas
solid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.
We used the PyTorch library and based our implementation on PyTorch Transformers5. We release our code on Github6 and hyper-parameters are listed in Table 2 (Appendix).
Remark on accuracy.
To verify that our self-attention models perform reasonably well, we display in Figure 6 the evolution of the test accuracy on CIFAR-10 over the 300 epochs of training
for our self-attention models against a small ResNet (Table 1). The ResNet is faster to converge,
but we cannot ascertain whether this corresponds to an inherent property of the architecture or an
artifact of the adopted optimization procedures. Our implementation could be optimized to exploit
the locality of Gaussian attention probabilities and reduce signiﬁcantly the number of FLOPS. We
observed that learned embeddings with content-based attention were harder to train probably due to
their increased number of parameters. We believe that the performance gap can be bridged to match
the ResNet performance, but this is not the focus of this work.
QUADRATIC ENCODING
As a ﬁrst step, we aim to verify that, with the relative position encoding introduced in equation (9),
attention layers learn to behave like convolutional layers. We train nine attention heads at each layer
to be on par with the 3 × 3 kernels used predominantly by the ResNet architecture. The center of
attention of each head h is initialized to ∆(h) ∼N(0, 2I2).
Figure 3 shows how the initial positions of the heads (different colors) at layer 4 changed during
training. We can see that after optimization, the heads attend on speciﬁc pixel of the image forming a
grid around the query pixel. Our intuition that Self-Attention applied to images learns convolutional
ﬁlters around the queried pixel is conﬁrmed.
Figure 4 displays all attention head at each layer of the model at the end of the training. It can be
seen that in the ﬁrst few layers the heads tend to focus on local patterns (layers 1 and 2), while deeper
layers (layers 3-6) also attend to larger patterns by positioning the center of attention further from
the queried pixel position. We also include in the Appendix a plot of the attention positions for a
higher number of heads (Nh = 16). Figure 14 displays both local patterns similar to CNN and long
range dependencies. Interestingly, attention heads do not overlap and seem to take an arrangement
maximizing the coverage of the input space.
5github.com/huggingface/pytorch-transformers
6github.com/epfml/attention-cnn
Published as a conference paper at ICLR 2020
Figure 4: Centers of attention of each attention head (different colors) for the 6 self-attention layers
using quadratic positional encoding. The central black square is the query pixel, whereas solid and
dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.
LEARNED RELATIVE POSITIONAL ENCODING
We move on to study the positional encoding used in practice by fully-attentional models on images.
We implemented the 2D relative positional encoding scheme used by : we learn a ⌊Dp/2⌋position encoding vector for each row and each column pixel
shift. Hence, the relative positional encoding of a key pixel at position k with a query pixel at position q is the concatenation of the row shift embedding δ1 and the column shift embedding δ2 (where
δ = k −q). We chose Dp = Dout = 400 in the experiment. We differ from their (unpublished)
implementation in the following points: (i) we do not use convolution stem and ResNet bottlenecks
for downsampling, but only a 2 × 2 invertible downsampling layer at input,
(ii) we use Dh = Dout instead of Dh = Dout/Nh backed by our theory that the effective number of
learned ﬁlters is min(Dh, Dout).
At ﬁrst, we discard the input data and compute the attention scores solely as the last term of eq. (8).
The attention probabilities of each head at each layer are displayed on Figure 5. The ﬁgure conﬁrms
our hypothesis for the ﬁrst two layers and partially for the third: even when left to learn the positional
encoding scheme from randomly initialized vectors, certain self-attention heads (depicted on the left)
learn to attend to individual pixels, closely matching the condition of Lemma 1 and thus Theorem
1. At the same time, other heads pay attention to horizontally-symmetric but non-localized patterns,
as well as to long-range pixel inter-dependencies.
We move on to a more realistic setting where the attention scores are computed using both positional
and content-based attention ) which corresponds to
a full-blown standalone self-attention model.
The attention probabilities of each head at each layer are displayed in Figure 6. We average the
attention probabilities over a batch of 100 test images to outline the focus of each head and remove
the dependency on the input image. Our hypothesis is conﬁrmed for some heads of layer 2 and 3:
even when left to learn the encoding from the data, certain self-attention heads only exploit positionbased attention to attend to distinct pixels at a ﬁxed shift from the query pixel reproducing the
receptive ﬁeld of a convolutional kernel. Other heads use more content-based attention (see Figures 8
to 10 in Appendix for non-averaged probabilities) leveraging the advantage of Self-Attention over
CNN which does not contradict our theory. In practice, it was shown by Bello et al. that
combining CNN and self-attention features outperforms each taken separately. Our experiments
shows that such combination is learned when optimizing an unconstrained fully-attentional model.
The similarity between convolution and multi-head self-attention is striking when the query pixel is
slid over the image: the localized attention patterns visible in Figure 6 follow the query pixel. This
characteristic behavior materializes when comparing Figure 6 with the attention probabilities at a
different query pixel (see Figure 7 in Appendix). Attention patterns in layers 2 and 3 are not only
localized but stand at a constant shift from the query pixel, similarly to convolving the receptive
ﬁeld of a convolutional kernel over an image. This phenomenon is made evident on our interactive
website7. This tool is designed to explore different components of attention for diverse images with
or without content-based attention. We believe that it is a useful instrument to further understand
how MHSA learns to process images.
7epfml.github.io/attention-cnn
Published as a conference paper at ICLR 2020
Figure 5: Attention probabilities of each head (column) at each layer (row) using learned relative
positional encoding without content-based attention. The central black square is the query pixel. We
reordered the heads for visualization and zoomed on the 7x7 pixels around the query pixel.
Figure 6: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using
learned relative positional encoding and content-content based attention. Attention maps are averaged over 100 test images to display head behavior and remove the dependence on the input content.
The black square is the query pixel. More examples are presented in Appendix A.
Published as a conference paper at ICLR 2020
RELATED WORK
In this section, we review the known differences and similarities between CNNs and transformers.
The use of CNN networks for text—at word level or character level —is more seldom than transformers (or RNN). Transformers and convolutional models have
been extensively compared empirically on tasks of Natural Language Processing and Neural Machine Translation. It was observed that transformers have a competitive advantage over convolutional model applied to text . It is only recently that Bello et al. ;
Ramachandran et al. used transformers on images and showed that they achieve similar accuracy as ResNets. However, their comparison only covers performance and number of parameters
and FLOPS but not expressive power.
Beyond performance and computational-cost comparisons of transformers and CNN, the study of
expressiveness of these architectures has focused on their ability to capture long-term dependencies
 . Another interesting line of research has demonstrated that transformers are Turingcomplete , which is an important theoretical result but is
not informative for practitioners. To the best of our knowledge, we are the ﬁrst to show that the class
of functions expressed by a layer of self-attention encloses all convolutional ﬁlters.
The closest work in bridging the gap between attention and convolution is due to Andreoli
They cast attention and convolution into a uniﬁed framework leveraging tensor outerproduct. In this framework, the receptive ﬁeld of a convolution is represented by a “basis” tensor
A ∈RK×K×H×W ×H×W . For instance, the receptive ﬁeld of a classical K × K convolutional
kernel would be encoded by A∆,q,k = 1{k −q = ∆} for ∆∈∆∆K. The author distinguishes
this index-based convolution with content-based convolution where A is computed from the value
of the input, e.g., using a key/query dot-product attention. Our work moves further and presents
sufﬁcient conditions for relative positional encoding injected into the input content (as done in practice) to allow content-based convolution to express any index-based convolution. We further show
experimentally that such behavior is learned in practice.
CONCLUSION
We showed that self-attention layers applied to images can express any convolutional layer (given
sufﬁciently many heads) and that fully-attentional models learn to combine local behavior (similar
to convolution) and global attention based on input content. More generally, fully-attentional models seem to learn a generalization of CNNs where the kernel pattern is learned at the same time as
the ﬁlters—similar to deformable convolutions . Interesting directions for future work include translating existing insights from the rich CNNs literature back to
transformers on various data modalities, including images, text and time series.
ACKNOWLEDGMENTS
Jean-Baptiste Cordonnier is thankful to the Swiss Data Science Center (SDSC) for funding this
work. Andreas Loukas was supported by the Swiss National Science Foundation (project Deep
Learning for Graph Structured Data, grant number PZ00P2 179981).
Published as a conference paper at ICLR 2020