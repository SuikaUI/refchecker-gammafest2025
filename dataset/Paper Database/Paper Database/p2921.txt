DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation
Gwanghyun Kim1
Taesung Kwon1
Jong Chul Ye2,1
Dept. of Bio and Brain Engineering1, Kim Jaechul Graduate School of AI2
Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea
{gwang.kim, star.kwon, jong.ye}@kaist.ac.kr
Figure 1. DiffusionCLIP enables faithful text-driven manipulation of real images by (a) preserving important details when the stateof-the-art GAN inversion-based methods fail. Other novel applications include (b) image translation between two unseen domains, (c)
stroke-conditioned image synthesis to an unseen domain, and (d) multi-attribute transfer.
Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zeroshot image manipulation guided by text prompts. However,
their applications to diverse real images are still difficult
due to the limited GAN inversion capability. Specifically,
these approaches often have difficulties in reconstructing
images with novel poses, views, and highly variable contents
compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems
This research was supported by Field-oriented Technology Development Project for Customs Administration through the National Research
Foundation of Korea(NRF) funded by the Ministry of Science & ICT and
Korea Customs Service (NRF-2021M3I1A1097938), and supported by
the Institute of Information & communications Technology Planning &
Evaluation (IITP) grant funded by the Korea government (MSIT) ).
and enable faithful manipulation of real images, we propose
a novel method, dubbed DiffusionCLIP, that performs textdriven image manipulation using diffusion models. Based on
full inversion capability and high-quality image generation
power of recent diffusion models, our method performs zeroshot image manipulation successfully even between unseen
domains and takes another step towards general application by manipulating images from a widely varying ImageNet
dataset. Furthermore, we propose a novel noise combination
method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed
robust and superior manipulation performance of our methods compared to the existing baselines. Code is available at
 
1. Introduction
Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP)
 
 has become popular thanks to their ability for zero-shot
image manipulation guided by text prompts . Nevertheless, its real-world application on diverse types of images
is still tricky due to the limited GAN inversion performance.
Specifically, successful manipulation of images should
convert the image attribute to that of the target without unintended changes of the input content. Unfortunately, the
current state-of-the-art (SOTA) encoder-based GAN inversion approaches often fail to reconstruct images
with novel poses, views, and details. For example, in the
left panel of Fig. 1(a), e4e and ReStyle with pSp encoder fail to reconstruct unexpected hand on the cheek,
inducing the unintended change. This is because they have
rarely seen such faces with hands during the training phase.
This issue becomes even worse in the case of images from a
dataset with high variance such as church images in LSUN-
Church and ImageNet dataset. As shown in the
right panel of Fig. 1(a) for the conversion to a department
store, existing GAN inversion methods produce artificial
architectures that can be perceived as different buildings.
Recently, diffusion models such as denoising diffusion
probabilistic models (DDPM) and score-based generative models have achieved great successes in image
generation tasks . The latest works 
have demonstrated even higher quality of image synthesis
performance compared to variational autoencoders (VAEs)
 , flows , auto-regressive models ,
and generative adversarial networks (GANs) .
Furthermore, a recent denoising diffusion implicit models
(DDIM) further accelerates sampling procedure and
enables nearly perfect inversion .
Inspired by this, here we propose a novel DiffusionCLIP
- a CLIP-guided robust image manipulation method by diffusion models. Here, an input image is first converted to
the latent noises through a forward diffusion. In the case
of DDIM, the latent noises can be then inverted nearly perfectly to the original image using a reverse diffusion if the
score function for the reverse diffusion is retained the same
as that of the forward diffusion. Therefore, the key idea
of DiffusionCLIP is to fine-tune the score function in the
reverse diffusion process using a CLIP loss that controls the
attributes of the generated image based on the text prompts.
Accordingly, DiffusionCLIP can successfully perform
image manipulation both in the trained and unseen domain
(Fig. 1(a)). We can even translate the image from an unseen
domain into another unseen domain (Fig. 1(b)), or generate images in an unseen domain from the strokes (Fig. 1(c)).
Moreover, by simply combining the noise predicted from several fine-tuned models, multiple attributes can be changed simultaneously through only one sampling process (Fig. 1(d)).
Furthermore, DiffsuionCLIP takes another step towards general application by manipulating images from a widely varying ImageNet dataset (Fig. 6), which has been rarely
explored with GAN-inversion due to its inferior reconstruction. 
Additionally, we propose a systematic approach to find
the optimal sampling conditions that lead to high quality
and speedy image manipulation. Qualitative comparison
and human evaluation results demonstrate that our method
can provide robust and accurate image manipulation, outperforming SOTA baselines.
2. Related Works
2.1. Diffusion Models
Diffusion probabilistic models are a type of latent
variable models that consist of a forward diffusion process
and a reverse diffusion process. The forward process is a
Markov chain where noise is gradually added to the data
when sequentially sampling the latent variables xt for t =
1, · · · , T. Each step in the forward process is a Gaussian
transition q(xt | xt−1) := N(√1 −βtxt−1, βtI), where
t=0 are fixed or learned variance schedule. The resulting
latent variable xt can be expressed as:
w a rd_d dpm} \xb _t = \sqrt {\alpha _t}\xb _0 + \sqrt {1 - \alpha _t}\bm {\wb }, \ \ \ \bm {\wb } \sim \mathcal {N} (\mathbf {0,I}),
where αt := Qt
s=1 (1 −βs). The reverse process q(xt−1 |
xt) is parametrized by another Gaussian transition pθ(xt−1 |
xt) := N(xt−1; µθ(xt, t), σθ(xt, t)I). µθ(xt, t) can be
decomposed into the linear combination of xt and a noise
approximation model ϵθ(xt, t), which can be learned by
solving the optimization problem as follows:
in _{\theta }\mathbb {E } _{\xb _0 \s
im q(\xb _0), \bm {\wb } \sim \mathcal {N}(\mathbf {0,I}), t} ||\bm {\wb }-\epsilonb _\theta (\xb _t, t) ||^2_2.
After training ϵθ(x, t), the data is sampled using following
reverse diffusion process:
_ {t-1} = \frac {1}{\sqrt {1-\beta _t}}\left (\xb _t - \frac {\beta _t}{\sqrt {1-\alpha _t}} \epsilonb _\theta (\xb _t, t)\right ) + \sigma _t\zb , (3)
where z ∼N(0, I). It was found that the sampling process
of DDPM corresponds to that of the score-based generative
models with the following relationship:
c o re_dd pm_ relation} \epsilonb _\theta (\xb _t, t) = -\sqrt {1-\alpha _t}\nabla _{\xb _t}\log p_{\theta }(\xb _t).
Meanwhile, proposed an alternative non-Markovian
noising process that has the same forward marginals as
DDPM but has a distinct sampling process as follows:
\xb _{t-1} = \s qr t
{ \ alph a _
{ t-1}}\ fb _\
t heta (\xb _{t}, t) + \sqrt {1 - \alpha _{t-1} - \sigma ^2_t}{\epsilonb }_{\theta }(\xb _{t}, t) + \sigma ^2_t\zb , \label {eq:ddim_original}
where, z ∼N(0, I) and fθ(xt, t) is a the prediction of x0
at t given xt and ϵθ(xt, t):
\labe l {e q: f } \ vf _\the ta
xb _{t}, t):= \frac {\xb _{t} - \sqrt {1-\alpha _t}\epsilonb _{\theta }(\xb _t,t)}{\sqrt {\alpha _{t}}}.
This sampling allows using different samplers by changing
the variance of the noise σt. Especially, by setting this noise
to 0, which is a DDIM sampling process , the sampling
process becomes deterministic, enabling full inversion of the
latent variables into the original images with significantly
fewer steps . In fact, DDIM can be considered as
an Euler method to solve an ordinary differential equation
(ODE) by rewriting Eq. 5 as follows:
\sqr t {\ f
xb _{t -1} - \sqrt {\frac {1}{\alpha _{t}}}\xb _{t} = \left (\sqrt {\frac {1}{\alpha _{t-1}}-1} - \sqrt {\frac {1}{\alpha _{t}}-1}\right ) \epsilonb _\theta (\xb _t, t).
For mathematical details, see Supplementary Section A.
2.2. CLIP Guidance for Image Manipulation
CLIP was proposed to efficiently learn visual concepts with natural language supervision. In CLIP, a text
encoder and an image encoder are pretrained to identify
which texts are matched with which images in the dataset.
Accordingly, we use a pretrained CLIP model for our textdriven image manipulation.
To effectively extract knowledge from CLIP, two different
losses have been proposed: a global target loss , and local
directional loss . The global CLIP loss tries to minimize
the cosine distance in the CLIP space between the generated
image and a given target text as follows:
\mathcal {L }_{\t e xt {global} }(\xb _\text {gen}, {y_\text {tar}}) = D_{\text {CLIP}}(\xb _\text {gen}, {y_\text {tar}}),
where ytar is a text description of a target, xgen denotes the
generated image, and DCLIP returns a cosine distance in the
CLIP space between their encoded vectors. On the other
hand, the local directional loss is designed to alleviate
the issues of global CLIP loss such as low diversity and
susceptibility to adversarial attacks. The local directional
CLIP loss induces the direction between the embeddings of
the reference and generated images to be aligned with the
direction between the embeddings of a pair of reference and
target texts in the CLIP space as follows:
Ldirection (xgen, ytar; xref, yref) := 1 −⟨∆I, ∆T⟩
∆T = ET (ytar) −ET (yref), ∆I = EI(xgen) −EI(xref).
Here, EI and ET are CLIP’s image and text encoders, respectively, and yref, xref are the source domain text and image,
respectively. The manipulated images guided by the directional CLIP loss are known robust to mode-collapse issues
because by aligning the direction between the image representations with the direction between the reference text and
the target text, distinct images should be generated. Also, it
is more robust to adversarial attacks because the perturbation
will be different depending on images . More related
works are illustrated in Supplementary Section A.
3. DiffusionCLIP
The overall flow of the proposed DiffusionCLIP for image
manipulation is shown in Fig. 2. Here, the input image x0 is
first converted to the latent xt0(θ) using a pretrained diffusion model ϵθ. Then, guided by the CLIP loss, the diffusion
model at the reverse path is fine-tuned to generate samples
driven by the target text ytar. The deterministic forwardreverse processes are based on DDIM . For translation
between unseen domains, the latent generation is also done
by forward DDPM process as will be explained later.
Figure 2. Overview of DiffusionCLIP. The input image is first
converted to the latent via diffusion models. Then, guided by
directional CLIP loss, the diffusion model is fine-tuned, and the
updated sample is generated during reverse diffusion.
3.1. DiffusionCLIP Fine-tuning
In terms of fine-tuning, one could modify the latent or
the diffusion model itself. We found that direct model finetuning is more effective, as analyzed in Supplementary Section D. Specifically, to fine-tune the reverse diffusion model
ϵθ, we use the following objective composed of the directional CLIP loss Ldirection and the identity loss LID:
q:main_o bject ive } {\
a thcal {L}_{\ text {direction}}\left (\hat \xb _0(\hat {\theta }),y_{\text {tar}}; \xb _{0},y_{\text {ref}}\right )+{\mathcal {L}_{\text {id}}( \hat \xb _0(\hat {\theta }), \xb _0) }}, \vspace {-0.5em}
where x0 is the original image,
ˆx0(ˆθ) is the generated
image from the latent xt0 with the optimized parameter ˆθ,
yref is the reference text, ytar is the target text given for image
manipulation.
Here, the CLIP loss is the key component to supervise
the optimization. Of two types of CLIP losses as discussed
above, we employ directional CLIP loss as a guidance thanks
to the appealing properties as mentioned in Section 2.2. For
the text prompt, directional CLIP loss requires a reference
text yref and a target text ytar while training. For example, in
the case of changing the expression of a given face image
into an angry expression, we can use ‘face’ as a reference
text and ‘angry face’ as a target text. In this paper, we often
use concise words to refer to each text prompt (e.g. ‘tanned
face’ to ‘tanned’).
The identity loss Lid is employed to prevent the unwanted
changes and preserve the identity of the object. We generally
use ℓ1 loss as identity loss, and in case of human face image
manipulation, face identity loss in is added:
{ { \mathc al { L}_{\t e xt {id}} ( \hat \xb _0(\hat { \theta }), \xb _0 ) = \lambda _{\text {L1}} \|\xb _0 - \hat {\xb }_0( \hat {\theta }) \| + \lambda _\text {face}\mathcal {L}_{\text {face}}(\hat \xb _0(\hat {\theta }), \xb _0 )},}
where Lface is the face identity loss , and λL1 ≥0 and
λface ≥0 are weight parameters for each loss. The necessity
of identity losses depends on the types of the control. For
some controls, the preservation of pixel similarity and the
human identity are significant (e.g. expression, hair color)
while others prefer the severe shape and color changes (e.g.
artworks, change of species).
Figure 3. Gradient flows during fine-tuning the diffusion model
with the shared architecture across t.
Existing diffusion models adopt the shared
U-Net architecture for all t, by inserting the information
of t using sinusoidal position embedding as used in the
Transformer . With this architecture, the gradient flow
during DiffusionCLIP fine-tuning can be represented as Fig.
3, which is a similar process of training recursive neural
network .
Once the diffusion model is fine-tuned, any image from
the pretrained domain can be manipulated into the image
corresponding to the target text ytar as illustrated in Fig. 4(a).
For details of the fine-tuning procedure and the model architecture, see Supplementary Section B and C.
3.2. Forward Diffusion and Generative Process
As the DDPM sampling process in Eq. 3 is stochastic,
the samples generated from the same latent will be different
every time. Even if the sampling process is deterministic,
the forward process of DDPM, where the random Gaussian
noise is added as in Eq. 1, is also stochastic, hence the reconstruction of the original image is not guaranteed. To fully
leverage the image synthesis performance of diffusion models with the purpose of image manipulation, we require the
deterministic process both in the forward and reverse direction with pretrained diffusion models for successful image
manipulation. On the other hand, for the image translation
between unseen domains, stochastic sampling by DDPM is
often helpful, which will be discussed in more detail later.
For the full inversion, we adopt deterministic reverse
DDIM process as generative process and ODE approximation of its reversal as a forward diffusion process.
Specifically, the deterministic forward DDIM process to
obtain latent is represented as:
\l a bel {eq:for wa r
_ d dim} \vx _ {t+1} = \sqrt {\alpha _{t+1}}\vf _\theta (\vx _{t}, t) + \sqrt {1 - \alpha _{t+1}}\bm {\epsilon }_{{\theta }}(\vx _{t}, t)
and the deterministic reverse DDIM process to generate
sample from the obtained latent becomes:
\l a bel {eq:rev er s
_ d dim} \xb _ {t-1} = \sqrt {\alpha _{t-1}}\fb _\theta (\xb _{t}, t) + \sqrt {1 - \alpha _{t-1}}\bm {\epsilon }_{\theta }(\xb _{t}, t)
where fθ is defined in Eq. 24. For the derivations of ODE
approximation, see Supplementary Sec A.
Another important contribution of DiffusionCLIP is a
fast sampling strategy. Specifically, instead of performing
forward diffusion until the last time step T, we found that
we can accelerate the forward diffusion by performing up
to t0 < T, which we call ‘return step’. We can further accelerate training by using fewer discretization steps between
[1, t0], denoted as Sfor and Sgen for forward diffusion and
generative process, respectively . Through qualitative
and quantitative analyses, we found the optimal groups of
hyperparameters for t0, Sfor and Sgen. For example, when T
is set to 1000 as a common choice , the choices
of t0 ∈ and (Sfor, Sgen) = (40, 6) satisfy our goal.
Although Sgen = 6 may give imperfect reconstruction, we
found that the identity of the object that is required for training is preserved sufficiently. We will show the results of
quantitative and qualitative analyses on Sfor, Sgen and t0 later
through experiments and Supplementary Section F.
Lastly, if several latents have been precomputed (grey
square region in Fig. 2), we can further reduce the time
for fine-tuning by recycling the latent to synthesize other
attributes. With these settings, the fine-tuning is finished in
1∼7 minutes on NVIDIA Quardro RTX 6000.
3.3. Image Translation between Unseen Domains
The fine-tuned models through DiffusionCLIP can be
leveraged to perform the additional novel image manipulation tasks as shown in Fig. 4.
First, we can perform image translation from an unseen
domain to another unseen domain, and stroke-conditioned
image synthesis in an unseen domain as described in Fig. 4(b)
and (c), respectively. A key idea to address this difficult
problem is to bridge between two domains by inserting the
diffusion models trained on the dataset that is relatively easy
to collect. Specifically, in , it was found that with
pretrained diffusion models, images trained from the unseen
domain can be translated into the images in the trained domain. By combining this method with DiffsuionCLIP, we
can now translate the images in zero-shot settings for both
source and target domains. Specifically, the images in the
source unseen domain x0 are first perturbed through the forward DDPM process in Eq. 1 until enough time step t0 when
Figure 4. Novel applications of DiffusionCLIP. (a) Manipulation
of images in pretrained domain to CLIP-guided domain. (b) Image
translation between unseen domains. (c) Stroke-conditioned image
generation in an unseen domain. (d) Multi-attribute transfer. ϵθ and
ϵˆθ indicate the original pretrained and fine-tuned diffusion models,
respectively.
the domain-related component are blurred but the identity or
semantics of object is preserved. This is usually set to 500.
Next, the images in the pretrained domain x′
0 are sampled
with the original pretrained model ϵθ using reverse DDIM
process in Eq. 13. Then, x′
0 is manipulated into the image
x0 in the CLIP-guided unseen domain as we do in Fig. 4(a)
with the fine-tuned model ϵˆθ.
3.4. Noise Combination
Multi-attribute transfer.
We discover that when the
noises predicted from multiple fine-tuned models {ϵˆθi}M
are combined during the sampling, multiple attributes can
be changed through only one sampling process as described
in Fig. 4(d). Therefore, we can flexibly mix several single
attribute fine-tuned models with different combinations without having to fine-tune new models with target texts that
define multiple attributes. In detail, we first invert the image with the original pretrained diffusion model and use the
multiple diffusion models by the following sampling rule:
lit } {{\xb }}_{t -1
{\ alpha _{t-1}} \textstyle \sum \nolimits _{i=1}^M \gamma _i(t) \vf _{\hat {\theta }_i}(\boldsymbol {\xb }_t, t) \\ &+ \sqrt {1 - \alpha _{t-1}} \textstyle \sum \nolimits _{i=1}^M \gamma _i(t) \boldsymbol {\epsilon }_{\hat {\theta }_i}(\xb _t, t), \end {split}
where {γi(t)}T
t=1 is the sequence of weights of each finetuned model ϵˆθi satisfying PM
i=1 γi(t) = 1 , which can be
used for controlling the degree of each attribute. From Eq.
4, we can interpret this sampling process as increasing the
joint probability of conditional distributions as following:
\sm all \textstyl e \ sum \no li
mit s _{i=1}^M \gamma _i(t) \boldsymbol {\epsilon }_{\hat {\theta }_i}(\xb _t, t) \propto -\nabla _{\xb _t}\log \prod \nolimits _{i=1}^Mp_{\hat {\theta }_i}(\xb _t | y_{\text {tar},i})^{\gamma _i(t)}, \normalsize
where ytar, i is the target text for each fine-tuned model ϵˆθi.
In the existing works , users require the combination of tricky task-specific loss designs or dataset preparation
with large manual effort for the task, while ours enable the
task in a natural way without such effort.
Continuous transition.
We can also apply the above noise
combination method for controlling the degree of change
during single attribute manipulation. By mixing the noise
from the original pretrained model ϵθ and the fine-tuned
model ϵˆθ with respect to a degree of change γ ∈ , we
can perform interpolation between the original image and
the manipulated image smoothly.
For more details and pseudo-codes of the aforementioned
applications, see Supplementary Section B.
4. Experiments
For all manipulation results by DiffusionCLIP, we use
2562 size of images. We used the models pretrained on
CelebA-HQ , AFHQ-Dog , LSUN-Bedroom and
LSUN-Church datasets for manipulating images of human faces, dogs, bedrooms, and churches, respectively. We
use images from the testset of these datasets for the test. To
fine-tune diffusion models, we use Adam optmizer with an
initial learning rate of 4e-6 which is increased linearly by 1.2
per 50 iterations. We set λL1 and λID to 0.3 and 0.3 if used.
As mentioned in Section 3.2, we set t0 in when
the total timestep T is 1000. We set (Sfor, Sgen) = (40, 6)
for training; and to (200, 40) for the test time. Also, we
precomputed the latents of 50 real images of size 2562 in
each training set of pretrained dataset. For more detailed
hyperparameter settings, see Supplementary Section F.
Table 1. Quantitative comparison for face image reconstruction.
Optimization
ReStyle w pSp
ReStyle w e4e
HFGI w e4e
Diffusion (t0 = 300)
Diffusion (t0 = 400)
Diffusion (t0 = 500)
Diffusion (t0 = 600)
Table 2. Human evaluation results of real image manipulation on
CelebA-HQ . The reported values mean the preference rate of
results from DiffusionCLIP against each method.
StyleGAN-NADA
(+ Restyle w pSp)
Out-of-domain
All domains
Out-of-domain
All domains
Figure 5. Comparison with the state-of-the-art text-driven manipulation methods: TediGAN , StyleCLIP and StyleGAN-NADA .
StyleCLIP-LO and StyleCLIP-GD refer to the latent optimization (LO) and global direction (GD) methods of StyleCLIP.
Figure 6. Manipulation results of real dog face, bedroom and general images using DiffusionCLIP.
4.1. Comparison and Evaluation
Reconstruction.
To demonstrate the nearly perfect reconstruction performance of our method, we perform the quantitative comparison with SOTA GAN inversion methods,
pSp , e4e , ReStyle and HFGI . As in Tab. 1,
our method shows higher reconstruction quality than all baselines in terms of all metrics: MAE, SSIM and LPIPS .
Qualitative comparison.
For the qualitative comparison
of manipulation performance with other methods, we use the
state-of-the-art text manipulation methods, TediGAN ,
StyleCLIP and StyleGAN-NADA where images
Table 3. Quantitative evaluation results. Our goal is to achieve
the better score in terms of Directional CLIP similarity (Sdir),
segmentation-consistency (SC), and face identity similarity (ID).
LSUN-Church
StyleGAN-NADA
DiffusionCLIP (Ours)
Figure 7. Results of image translation between unseen domains.
Figure 8. Results of multi-attribute transfer.
Figure 9. Results of continuous transition.
for the target control is not required similar to our method.
StyleGAN2 pretrained on FFHQ-1024 and LSUN-
Church-256 is used for StyleCLIP and StyleGAN-
NADA. StyleGAN pretrained on FFHQ-256 is
used for TediGAN. For GAN inversion, e4e encoder 
is used for StyleCLIP latent optimization (LO) and global
Figure 10. Reconstruction results varying the number of forward
diffusion steps Sfor and generative steps Sgen.
Figure 11. Manipulation results depending on t0 values.
direction (GD), Restyle encoder with pSp is used
for StyleGAN-NADA, and IDInvert is used for Tedi-
GAN, as in their original papers. Face alignment algorithm
is used for StyleCLIP and StyleGAN-NADA as their official
implementations. Our method uses DDPM pretrained on
CelebA-HQ-256 and LSUN-Church-256 .
As shown in Fig. 5, SOTA GAN inversion methods fail
to manipulate face images with novel poses and details producing distorted results. Furthermore, in the case of church
images, the manipulation results can be recognized as the
results from different buildings. These results imply significant practical limitations. On the contrary, our reconstruction
results are almost perfect even with fine details and background, which enables faithful manipulation. In addition to
the manipulation in the pretrained domain, DiffusonCLIP
can perform the manipulation into the unseen domain successfully, while StyleCLIP and TediGAN fail.
User study.
We conduct user study to evaluate real face
image manipulation performance on CelebA-HQ with
our method, StyleCLIP-GD and StyleGAN-NADA .
We get 6000 votes from 50 people using a survey platform.
We use the first 20 images in CelebA-HQ testset as general
cases and use another 20 images with novel views, hand
pose, and fine details as hard cases. For a fair comparison,
we use 4 in-domain attributes (angry, makeup, beard, tanned)
and 2 out-of-domain attributes (zombie, sketch), which are
used in the studies of baselines. Here, we use official pretrained checkpoints and implementation for each approach.
As shown in Tab. 2, for both general cases and hard cases, all
of the results from DiffusionCLIP are preferred compared
to baselines (> 50%). Of note, in hard cases, the preference
rates for ours were all increased, demonstrating robust manipulation performance. It is remarkable that the high preference rates (≈90%) against StyleCLIP in out-of-domain
manipulation results suggest that our method significantly
outperforms StyleCLIP in out-of-domain manipulation.
Quantitative evaluation.
We also compare the manipulation performance using the following quality metrics: Directional CLIP similarity (Sdir), segmentation-consistency (SC),
and face identity similarity (ID). To compute each metric,
we use a pretrained CLIP , segmentation 
and face recognition models , respectively. Then, during the translation between three attributes in CelebA-HQ
(makeup, tanned, gray hair) and LSUN-Church (golden,
red brick, sunset) , our goal is to achieve the better score
in terms of Sdir, SC, and ID. As shown in Tab. 3, our method
outperforms baselines in all metrics, demonstrating high
attribute-correspondence (Sdir) as well as well-preservation
of identities without unintended changes (SC, ID).
For more experimental details and results of the comparison, see Supplementary Section D and E.
4.2. More Manipulation Results on Other Datasets
Fig. 6 presents more examples of image manipulations
on dog face, bedroom and general images using the diffusion models pretrained on AFHQ-Dog-256 , LSUN-
Bedroom-256 and ImageNet-512 datasets, respectively. The results demonstrate that the reconstruction is
nearly flawless and high-resolution images can be flexibly
manipulated beyond the boundary of the trained domains.
Especially, due to the diversity of the images in ImageNet,
GAN-based inversion and its manipulation in the latent space
of ImageNet show limited performance . Diffusion-
CLIP enables the zero-shot text-driven manipulation of general images, moving a step forward to the general text-driven
manipulation. For more results, see Supplementary Section
4.3. Image Translation between Unseen Domains
With the fine-tuned diffusion models using Diffusion-
CLIP, we can even translate the images in one unseen domain to another unseen domain. Here, we are not required
to collect the images in the source and target domains or
introduce external models. In Fig. 7, we perform the image
translation results from the portrait artworks and animation
images to other unseen domains, Pixar, paintings by Gogh
and Neanderthal men. We also show the successful image
generation in the unseen domains from the stroke which is
the rough image painting with several color blocks. These
applications will be useful when enough images for both
source and target domains are difficult to collect.
4.4. Noise Combination
As shown in Fig. 8 we can change multiple attributes in
one sampling. As discussed before, to perform the multiattribute transfer, complex loss designs, as well as specific
data collection with large manual efforts, aren’t required.
Finally, Fig. 9 shows that we can control the degree of change
of single target attributes according to γ by mixing noises
from the original model and the fine-tuned model.
4.5. Dependency on Hyperparameters
In Fig. 10, we show the results of the reconstruction performance depending on Sfor, Sgen when t0 = 500. Even
with Sfor = 6, we can see that the reconstruction preserves
the identity well. When Sfor = 40, the result of Sgen = 6
lose some high frequency details, but it’s not the degree of
ruining the training. When Sfor = 200 and Sgen = 40, the
reconstruction results are so excellent that we cannot differentiate the reconstruction with the result when the original
images. Therefore, we just use (Sfor, Sgen) = (40, 6) for the
training and (Sfor, Sgen) = (200, 40) for the inference.
We also show the results of manipulation by changing t0
while fixing other parameters in Fig. 11. In case of skin color
changes, 300 is enough. However, in case of the changes
with severe shape changes such as the Pixar requires stepping back more as t0 = 500 or t0 = 700. Accordingly,
we set different t0 depending on the attributes. The additional analyses on hyperparameters and ablation studies are
provided in Supplementary Section F.
5. Discussion and Conclusion
In this paper, we proposed DiffusionCLIP, a method of
text-guided image manipulation method using the pretrained
diffusion models and CLIP loss. Thanks to the near-perfect
inversion property, DiffusionCLIP has shown excellent performance for both in-domain and out-of-domain manipulation by fine-tuning diffusion models. We also presented
several novel applications of using fine-tuned models by
combining various sampling strategies.
There are limitations and societal risks on DiffusionCLIP.
Therefore, we advise users to make use of our method carefully for proper purposes. Further details on limitations and
negative social impacts are given in Supplementary Section