LETTER-BASED SPEECH RECOGNITION
WITH GATED CONVNETS
A PREPRINT
Vitaliy Liptchinsky
Facebook AI Research
 
Gabriel Synnaeve
Facebook AI Research
 
Ronan Collobert
Facebook AI Research
 
February 19, 2019
In the recent literature, “end-to-end” speech systems often refer to letter-based acoustic models trained
in a sequence-to-sequence manner, either via a recurrent model or via a structured output learning
approach ). In contrast to traditional phone (or senone)-based
approaches, these “end-to-end” approaches alleviate the need of word pronunciation modeling, and
do not require a “forced alignment” step at training time. Phone-based approaches remain however
state of the art on classical benchmarks. In this paper, we propose a letter-based speech recognition
system, leveraging a ConvNet acoustic model. Key ingredients of the ConvNet are Gated Linear
Units and high dropout. The ConvNet is trained to map audio sequences to their corresponding letter
transcriptions, either via a classical CTC approach, or via a recent variant called ASG . Coupled with a simple decoder at inference time, our system matches the best existing
letter-based systems on WSJ (in word error rate), and shows near state of the art performance on
LibriSpeech .
Introduction
State of the art speech recognition systems leverage pronunciation models as well as speaker adaptation techniques
involving speaker-speciﬁc features. These systems rely on lexicon dictionaries, which decompose words into one or
more sequences of phones. Phones themselves are decomposed into smaller sub-word units, called senones. Senones are
carefully selected through a procedure involving a phonetic-context-based decision tree built from another GMM/HMM
system. In the recent literature, “end-to-end” speech systems attempt to break away from these hardcoded a-priori, the
underlying assumption being that with enough data pronunciations should be implicitly inferred by the model, and
speaker robustness should be also achieved. A number of works have thus naturally proposed ways how to learn to
map audio sequences directly to their corresponding letter sequences. Recurrent models, structured-output learning or
combination of both are the main contenders.
In this paper, we show that simple convolutional neural networks (CNNs) coupled with structured-output learning
can outperform existing letter-based solutions. Our CNNs employ Gated Linear Units (GLU). Gated ConvNets have
been shown to reduce the vanishing gradient problem, as they provide a linear path for the gradients while retaining
non-linear capabilities, leading to state of the art performance both in natural language modeling and machine translation
tasks . We train our system with a structured-output learning approach, either
with CTC or ASG . Coupled with a custom-made simple beam-search
decoder, we exhibit word error rate (WER) performance matching the best existing letter-based systems, both for the
WSJ and LibriSpeech datasets . While phone-based systems still lead on WSJ (81h of labeled
data), our system is competitive with the existing state of the art systems on LibriSpeech (960h).
The rest of the paper is structured as follows: the next section goes over the history of the work in the automatic speech
recognition area. We then detail the convolutional networks used for acoustic modeling, along with the structured-output
learning and decoding approaches. The last section shows experimental results on WSJ and LibriSpeech.
 
A PREPRINT - FEBRUARY 19, 2019
GLU-ConvNet
Figure 1: Overview of our acoustic model, which computes log-mel ﬁlterbanks (MFSC) that are fed to a Gated
ConvNet. The ConvNet outputs one score for each letter in the dictionary, and for each input feature frame. At inference
time, these scores are fed to a decoder (see Section 3.3) to form the most likely sequence of words. At training time, the
scores are fed to the CTC or ASG criterions (see Figure 2) which promote sequences of letters leading to the target
transcription sequence (here “c a t”).
Background
The historic pipeline for speech recognition requires ﬁrst training an HMM/GMM model to force align the units on
which the ﬁnal acoustic model operates (most often context-dependent phone or senone states) Woodland and Young
 . The performance improvements brought by deep neural networks (DNNs) and convolutional neural networks (CNNs) for acoustic modeling only
extend this training pipeline. Current state of the art models on LibriSpeech also employ this approach , with an additional step of speaker adaptation .
Departing from this historic pipeline, Senior et al. proposed GMM-free training, but the approach still requires to
generate a forced alignment. Recently, maximum mutual information (MMI) estimation was used to
train neural network acoustic models . The MMI criterion maximizes the mutual
information between the acoustic sequence and word sequences or the Minimum Bayes Risk (MBR) criterion , and belongs to segmental discriminative training criterions, although compatible with generative
Even though connectionist approaches long coexisted with HMM-based approaches,
they had a recent resurgence. A modern work that directly cut ties with the HMM/GMM pipeline used a recurrent neural
network (RNN) for phoneme transcription with the connectionist temporal classiﬁcation (CTC)
sequence loss . This approach was then extended to character-based systems and improved with attention mechanisms . But the best such systems are
often still behind state of the art phone-based (or senone-based) systems. Competitive end-to-end approaches leverage
acoustic models (often ConvNet-based) topped with RNN layers as in ), trained with a sequence criterion and MMI ). A survey of segmental models can be found in . On conversational speech (that is not
the topic of this paper), the state of the art is still held by complex ConvNets+RNNs acoustic models (which are also
trained or reﬁned with a sequence criterion), coupled with domain-adapted language models .
Architecture
Our acoustic model (see an overview in Figure 1) is a Convolutional Neural Network (ConvNet) ,
with Gated Linear Units (GLUs) and dropout applied to activations of each layer except the
output one. The model is fed with log-mel ﬁlterbank features, and is trained with either the Connectionist Temporal
Classiﬁcation (CTC) criterion , or with the ASG criterion: a variant of CTC that does not have
blank labels but employs a simple duration model through letter transition scores . At inference,
the acoustic model is coupled with a decoder which performs a beam search, constrained with a count-based language
model. We detail each of these components in the following.
Gated ConvNets for Acoustic Modeling
The acoustic model architecture is a 1D Gated Convolutional Neural Network (Gated ConvNet), trained to map a
sequence of audio features to its corresponding letter transcription. Given a dictionary of letters L, the ConvNet (which
acts as a sliding-approach over the input sequence) outputs one score for each letter in the dictionary, for each input
frame. In the transcription, words are separated by a special letter, denoted #.
A PREPRINT - FEBRUARY 19, 2019
1D ConvNets were introduced early in the speech community, and are also referred as Time-Delay Neural Networks
(TDNNs) . Gated ConvNets stack 1D convolutions with Gated Linear Units. More
formally, given an input sequence X ∈RT ×di with T frames of d-dimensional vectors, the ith layer of our network
performs the following computation:
hi(X) = (X ∗Wi + bi) ⊗σ(X ∗Vi + ci) ,
where ∗is the convolution operator, Wi, Vi ∈Rdi+1×di×ki and bi, ci ∈Rdi+1 are the learned parameters (with
convolution kernel size ki), σ(·) is the sigmoid function and ⊗is the element-wise product between matrices.
Gated ConvNets have been shown to reduce the vanishing gradient problem, as they provide a linear path for the
gradients while retaining non-linear capabilities, leading to state of the art performance both for natural language
modeling and machine translation tasks .
Feature Normalization and Zero-Padding
Each input feature sequence is normalized to have zero mean and unit variance. Given an input sequence X ∈RT ×d, a
convolution with kernel size k will output T −k + 1 frames, due to border effects. To compensate those border effects,
we pad the log-mel ﬁlterbanks X0 with zeroed frames. To take into account the whole network, the padding size is
i(ki −1), divided into two equal parts at the beginning and the end of the sequence.
Acoustic Model Training
Figure 2: (a) The CTC graph which represents all the acceptable sequences of letters for the transcription “cat” over
6 frames. (b) The same graph used by ASG, where blank labels have been discarded. (c) The fully connected graph
describing all possible sequences of letter; this graph is used for normalization purposes in ASG. Un-normalized
transitions scores are possible on edges of these graphs. At each time step, nodes are assigned a conditional unnormalized score, output by the Gated ConvNet acoustic model.
We considered two structured-output learning approaches to train our acoustic models: the Connectionist Temporal
Classiﬁcation (CTC), and a variant called AutoSeG (ASG).
The CTC Criterion
CTC efﬁciently enumerates all possible sequences of sub-word units (e.g. letters) which can lead
to the correct transcription, and promotes the score of these sequences. CTC also allows a special “blank” state to be
optionally inserted between each sub-word unit. The rationale behind the blank state is two-fold: (i) modeling “garbage”
frames which might occur between each letter and (ii) identifying the separation between two identical consecutive
sub-word units in a transcription. Figure 2a shows the CTC graph describing all the possible sequences of letters
leading to the word “cat”, over 6 frames. We denote Gctc(θ, T) the CTC acceptance graph over T frames for a given
transcription θ, and π = π1, . . . , πT ∈Gctc(θ, T) a path in this graph representing a (valid) sequence of letters for this
transcription. CTC assumes that the network outputs probability scores, normalized at the frame level. At each time
step t, each node of the graph is assigned with its corresponding log-probability letter i (that we denote f t
i (X)) output
by the acoustic model (given an acoustic sequence X). CTC minimizes the Forward score over the graph Gctc(θ, T):
CTC(θ, T) = −
π∈Gctc(θ,T )
where the “logadd” operation (also called “log-sum-exp”) is deﬁned as logadd(a, b) = log(exp(a) + exp(b)). This
overall score can be efﬁciently computed with the Forward algorithm.
A PREPRINT - FEBRUARY 19, 2019
The ASG Criterion
Blank labels introduce code complexity when decoding letters into words. Indeed, with blank labels “ø”, a word gets
many entries in the sub-word unit transcription dictionary (e.g. the word “cat” can be represented as “c a t”, “c ø a t”, “c
ø a t”, “c ø a ø t”, etc... – instead of only “c a t”). We replace the blank label by special letters modeling repetitions
of preceding letters. For example “caterpillar” can be written as “caterpil1ar”, where “1” is a label to represent one
repetition of the previous letter.
The AutoSeG (ASG) criterion removes the blank labels from the CTC acceptance graph
Gctc(θ, T) (shown in Figure 2a) leading to a simpler graph that we denote Gasg(θ, T) (shown in Figure 2b). In contrast
to CTC which assumes per-frame normalization for the acoustic model scores, ASG implements a sequence-level
normalization to prevent the model from diverging (the corresponding graph enumerating all possible sequences of
letters is denoted Gasg(θ, T), as shown in Figure 2c). ASG also uses unnormalized transition scores gi,j(·) on each
edge of the graph, when moving from label i to label j, that are trained jointly with the acoustic model. This leads to
the following criterion::
ASG(θ, T) = −
π∈Gasg(θ,T )
πt(X) + gπt−1,πt(X))
π∈Gfull(θ,T )
πt(X) + gπt−1,πt(X)) .
The left-hand part in Equation (3) promotes the score of letter sequences leading to the right transcription (as in
Equation (2) for CTC), and the right-hand part demotes the score of all sequences of letters. As for CTC, these two
parts can be efﬁciently computed with the Forward algorithm.
When removing transitions in Equation (3), the sequence-level normalization becomes equivalent to the frame-level
normalization found in CTC, and the ASG criterion is mathematically equivalent to CTC with no blank labels. However,
in practice, we observed that acoustic models trained with a transition-free ASG criterion had a hard time to converge.
Other Training Considerations
We apply dropout at the output to all layers of the acoustic model. Dropout retains each output with a probability p, by
applying a multiplication with a Bernoulli random variable taking value 1 with probability p and 0 otherwise .
Following the original implementation of Gated ConvNets , we found that using both weight
normalization and gradient clipping were speeding up training
convergence. The clipping we implemented performs:
∇C = max(||∇C||, ϵ) ∇C
where C is either the CTC or ASG criterion, and ϵ is some hyper-parameter which controls the maximum amplitude of
the gradients.
Beam-Search Decoder
We wrote our own one-pass decoder, which performs a simple beam-search with beam thresholding, histogram pruning
and language model smearing . We kept the decoder as simple as possible (under 1000 lines of C
code). We did not implement any sort of model adaptation before decoding, nor any word graph rescoring. Our decoder
relies on KenLM for the language modeling part. It also accepts unnormalized acoustic scores
(transitions and emissions from the acoustic model) as input. The decoder attempts to maximize the following:
π∈Glex(θ,T )
(fπt(x) + gπt−1,πt(x))
+ α log Plm(θ) + γ|{i|πi = #}| ,
where Glex(θ, T) is a graph constrained by lexicon, Plm(θ) is the probability of the language model given a transcription
θ, α, β, and γ are three hyper-parameters which control the weight of the language model, and the silence (#) insertion
penalty, respectively.
A PREPRINT - FEBRUARY 19, 2019
Table 1: Architecture details. “#conv.” is the number of convolutional layers. Dropout amplitude, “#hu” (number
of output hidden units) and “kw” (convolution kernel width) are provided for the ﬁrst and last layer (all are linearly
increased with layer depth). The size of the ﬁnal layer is also provided.
Architecture
ﬁrst/last layer
ﬁrst/last layer
ﬁrst/last layer
full connect
Low Dropout
LibriSpeech
Low Dropout
High Dropout
The beam of the decoder tracks paths with highest scores according to Equation (5), by bookkeeping pairs of (language
model, lexicon) states, as it goes through time. The language model state corresponds to the (n −1)-gram history of
the n-gram language model, while the lexicon state is the sub-word unit position in the current word hypothesis. To
maintain diversity in the beam, paths with identical (language model, lexicon) states are merged. Note that traditional
decoders combine the scores of the merged paths with a max(·) operation (as in a Viterbi beam-search) – which would
correspond to a max(·) operation in Equation (5) instead of logadd(·). We consider instead the logadd(·) operation
 ), as it takes into account the contribution of all the paths leading to the same
transcription, in the same way we do during training (see Equation (3)). In Section 4.1, we show that this leads to better
accuracy in practice.
Experiments
We benchmarked our system on WSJ (about 81h of labeled audio data) and LibriSpeech (about
960h). We kept the original 16 kHz sampling rate. For WSJ, we considered the classical setup SI284 for training,
DEV93 for validation, and EVAL92 for evaluation. For LibriSpeech, we considered the two available setups CLEAN and
OTHER. All the hyper-parameters of our system were tuned on validation sets. Test sets were used only for the ﬁnal
evaluations.
The letter vocabulary L contains 30 graphemes: the standard English alphabet plus the apostrophe, silence (#), and two
special “repetition” graphemes which encode the duplication (once or twice) of the previous letter (see Section 3.2.2).
Decoding is achieved with our own decoder (see Section 3.3). We used standard language models for both datasets, i.e.
a 4-gram model (with about 165K words in the dictionary) trained on the provided data for WSJ, and a 4-gram model1
(about 200K words) for LibriSpeech. In the following, we either report letter-error-rates (LERs) or word-error-rates
Training was performed with stochastic gradient descent on WSJ, and mini-batches of 4 utterances on LibriSpeech.
Clipping parameter (see Equation (4)) was set to ϵ = 0.2. We used a momentum of 0.9. Input features, log-mel
ﬁlterbanks, were computed with 40 coefﬁcients, a 25 ms sliding window and 10 ms stride.
We implemented everything using TORCH72. The CTC and ASG criterions, as well as the decoder were implemented in
C (and then interfaced into TORCH).
Architecture
We tuned our acoustic model architectures by grid search, validating on the dev sets. We consider here two architectures,
with low and high amount of dropout (see the parameter p in Section 3.2.3). Table 1 reports the details of our
architectures. The amount of dropout, number of hidden units, as well as the convolution kernel width are increased
linearly with the depth of the neural network. Note that as we use Gated Linear Units (see Section 3.1), each layer is
duplicated as stated in Equation (1). Convolutions are followed by a fully connected layer, before the ﬁnal layer which
outputs 30 scores (one for each letter in the dictionary). Concerning WSJ, the LOW DROPOUT (p = 0.2) architecture
has about 17M trainable parameters. For LibriSpeech, architectures have about 130M and 208M of parameters for the
LOW DROPOUT (p = 0.2) and HIGH DROPOUT (p = 0.2 →0.6) architectures, respectively.
1 
2 
A PREPRINT - FEBRUARY 19, 2019
Figure 3: LibriSpeech Letter Error Rate (LER) and Word Error Rate (WER) for the ﬁrst training epochs of our LOW
DROPOUT architecture. (a) is on dev-clean, (b) on dev-other.
Table 2: Comparison in LER and WER of variants of our model on (a) WSJ and (b) LibriSpeech. LER is computed
with no decoding. Operator max and logadd refer to the aggregation of beam hypotheses (see Section 3.3).
LOW DROP. (ASG, max)
LOW DROP. (ASG, logadd)
(b) LibriSpeech
LOW DROP. (ASG, logadd)
HIGH DROP. (CTC)
HIGH DROP. (ASG, max)
HIGH DROP. (ASG, logadd)
Figure 3 shows the LER and WER on the LibriSpeech development sets, for the ﬁrst 40 training epochs of our LOW
DROPOUT architecture. LER and WER appear surprisingly well correlated, both on the “clean” and “other” version of
the dataset.
In Table 2b, we report WERs on the LibriSpeech development sets, both for our LOW DROPOUT and HIGH DROPOUT
architectures. Increasing dropout regularize the acoustic model in a way which impacts signiﬁcantly generalization, the
effect being stronger on noisy speech.
Table 2a and Table 2b also report the WER for the decoder ran with the max(·) operation (instead of logadd(·) for other
results) used to aggregate paths in the beam with identical (language model, lexicon) states. It appears advantageous (as
there is no code complexity increase in the decoder – one only needs to replace max(·) by logadd(·) in the code) to use
the logadd(·) aggregation.
Figure 4: Comparison of alignments produced by the models with CTC (top) and ASG (bottom) criterions on audio
spectrogram over time (each time frame on X axis corresponds to a 40ms window with 10 ms stride).
A PREPRINT - FEBRUARY 19, 2019
Table 3: Comparison of different near state of the art ASR systems on LibriSpeech. We report the type of acoustic
model used for various systems, as well as the type of sub-word units. HMM stands for Hidden Markov Model, CNN
for ConvNet; when not speciﬁed, CNNs are 1D. pNorm is a particular non-linearity . We also report
extra information (besides word transcriptions) which might be used by each system, including speaker adaptation, or
any other domain-speciﬁc data.
Acoustic Model
Sub-word Spkr Adapt. Extra Resources
Panayotov et al. 
HMM+DNN+pNorm
phone lexicon
Peddinti et al. 
phone lexicon
Povey et al. 
phone lexicon,
phone LM, data augm.
Ko et al. 
HMM+CNN+pNorm
phone lexicon, data augm.
Amodei et al. 
2D-CNN+RNN
11.9Kh train set,
Common Crawl LM
Zhou et al. 
CNN+GRU+policy learning
data augmentation
Zeyer et al. 
RNN+attention
this paper
Figure 4 depicts alignments of the models with CTC and ASG criterions when forced aligned to a given target. Our
analysis shows that the model with CTC criterion exhibits 500 ms delay compared to the model with ASG criterion.
Similar observation was also previously noted in Sak et al. .
Comparison with other systems
In Table 4, we compare our system with existing phone-based and letter-based approaches on WSJ and LibriSpeech.
Phone-based acoustic state of the art models are reported as reference. These systems output in general senones;
senones are carefully selected through a procedure involving a phonetic-context-based decision tree built from another
GMM/HMM system. Phone-based systems also require an additional word lexicon which translates words into a
sequence of phones. Most state of the art systems also perform speaker adaptation; iVectors compute a speaker
embedding capturing both speaker and environment information , while fMMLR is a two-pass
decoder technique which computes a speaker transform in the ﬁrst pass . Even though
Table 3 associates speaker adaptation exclusively with phone-based systems, speaker adaptation can be also applied to
letter-based systems.
State of the art performance for letter-based models on LibriSpeech is held by DEEP SPEECH 2 
and on noisy and clean subsets respectively. On WSJ state of the art performance is held by DEEP
SPEECH 2. DEEP SPEECH 2 uses an acoustic model composed of a ConvNet and a Recurrent Neural Network (RNN).
DEEP SPEECH 2 relies on a lot of extra speech data at training, combined with a very large 5-gram language model at
inference time to make the letter-based approach competitive. Our system outperforms DEEP SPEECH 2 on clean data,
even though our system has been trained with an order of magnitude less data. Acoustic model in is
also based on RNNs and in addition employs attention mechanism. With LSTM language model their system shows
lower WER than our, but with a simple 4-gram language model our system has slightly lower WER.
On WSJ the state of the art is a phone-based approach which leverages an acoustic model
combining CNNs, bidirectional LSTMs, and deep fully connected neural networks. The system also performs speaker
adaptation at inference. We also compare with existing letter-based approaches on WSJ, which are abundant in the
literature. They rely on recurrent neural networks, often bi-directional, and in certain cases combined with ConvNet
architectures. Our system matches the best reported letter-based WSJ performance. The Gated ConvNet appears to be
very strong at modeling complete words as it achieves 6.7% WER on LibriSpeech clean data even with no decoder, i.e.
on the raw output of the neural network.
Concerning LibriSpeech, we summarize existing state of the art systems in Table 3. We highlighted the acoustic model
architectures, as well as the type of underlying sub-word units.
Conclusion
We have introduced a simple end-to-end automatic speech recognition system, which combines a ConvNet acoustic
model with Gated Linear Units, and a simple beam-search decoder. The acoustic model is trained to map audio
A PREPRINT - FEBRUARY 19, 2019
Table 4: Comparison in WER of our model with other systems on WSJ and LibriSpeech. Systems with ⋆or † use
additional data or data augmentation at training, respectively.
WSJ eval92
LibriSpeech test-clean
LibriSpeech test-other
 
 
 
 
 †
 
 
 
 
 
 †
 
 †
 ⋆
 
 (LSTM LM)
this paper (CTC)
this paper (ASG)
sequences to sequences of characters using a structured-output learning approach based on a variant of CTC. Our
system outperforms existing letter-based approaches (which do not use extra data at training time or powerful LSTM
language models), both on WSJ and LibriSpeech. Overall phone-based approaches are still holding the state of the art,
but our system’s performance is competitive on LibriSpeech, suggesting pronunciations is implicitly well modeled with
enough training data. Further work should include leveraging speaker identity, training from the raw waveform, data
augmentation, training with more data, and better language models.