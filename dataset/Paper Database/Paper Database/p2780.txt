HAL Id: hal-01061506
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
On the asymptotics of random forests
Erwan Scornet
To cite this version:
Erwan Scornet. On the asymptotics of random forests. 2014. ￿hal-01061506￿
On the asymptotics of random forests
Erwan Scornet
Sorbonne Universit´es, UPMC Univ Paris 06, F-75005, Paris, France
 
The last decade has witnessed a growing interest in random forest models
which are recognized to exhibit good practical performance, especially in
high-dimensional settings. On the theoretical side, however, their predictive power remains largely unexplained, thereby creating a gap between
theory and practice. The aim of this paper is twofold. Firstly, we provide
theoretical guarantees to link ﬁnite forests used in practice (with a ﬁnite
number M of trees) to their asymptotic counterparts (with M = ∞).
Using empirical process theory, we prove a uniform central limit theorem
for a large class of random forest estimates, which holds in particular for
Breiman’s original forests. Secondly, we show that inﬁnite forest consistency implies ﬁnite forest consistency and thus, we state the consistency
of several inﬁnite forests. In particular, we prove that q quantile forests—
close in spirit to Breiman’s forests but easier to study—are able to combine
inconsistent trees to obtain a ﬁnal consistent prediction, thus highlighting
the beneﬁts of random forests compared to single trees.
Index Terms — Random forests, randomization, consistency, central limit
theorem, empirical process, number of trees, q-quantile.
2010 Mathematics Subject Classiﬁcation: 62G05, 62G20.
Introduction
Random forests are a class of algorithms used to solve classiﬁcation and regression problems. As ensemble methods, they grow several trees as base estimates
and aggregate them to make a prediction. In order to obtain many diﬀerent trees
based on a single training set, random forests procedures introduce randomness
in the tree construction. For instance, trees can be built by randomizing the set
of features , the data set , or both at the same time .
Among all random forest algorithms, the most popular one is that of Breiman
 , which relies on CART procedure to grow the individual trees. As highlighted by many applied studies , Breiman’s random forests often outperform state-of-the-art methods. They are recognized for their ability to handle high-dimensional data sets,
thus being useful in ﬁelds such as genomics and pattern recognition
 , just to name a few. On the computational side, Breiman’s
 forests are easy to run and robust to changes in the parameters they
depend on . As a proof of their
success, many extensions have been developed in ranking problems , quantile estimation , and survival analysis
 . Interesting new developments in the context of massive
data sets have been achieved. For instance, Geurts et al. modiﬁed the
procedure to reduce calculation time, while other authors extended the procedure to online settings .
While Breiman’s forests are extensively used in practice, some of their
mathematical properties remain under active investigation. In fact, most theoretical studies focus on simpliﬁed versions of the algorithm, where the forest
construction is independent of the training set. Consistency of such simpliﬁed
models has been proved . However, these results do not extend to Breiman’s original
forests whose construction critically depends on the whole training set. Recent
attempts to bridge the gap between theoretical forest models and Breiman’s
 forests have been made by Wager and Scornet et al. who
establish consistency of the original algorithm under suitable assumptions.
Apart from the dependence of the forest construction on the data set, there is
another fundamental diﬀerence between existing forest models and ones implemented. Indeed, in practice, a forest can only be grown with a ﬁnite number M
of trees although most theoretical works assume, by convenience, that M = ∞.
Since the predictor with M = ∞does not depend on the speciﬁc tree realizations that form the forest, it is therefore more amenable to analysis. However,
surprisingly, no study aims at clarifying the link between ﬁnite forests (ﬁnite
M) and inﬁnite forests (M = ∞) even if some authors proved results on ﬁnite forest predictions at a ﬁxed
In the present paper, our goal is to study the connection between inﬁnite forest
models and ﬁnite forests used in practice in the context of regression. We start
by proving a uniform central limit theorem for various random forests estimates,
including Breiman’s ones. In Section 3, we also point out that the L2
risk of inﬁnite forests is lower than that of ﬁnite forests, which supports the
interest of theoretical studies for inﬁnite forests. Besides, this result shows that
inﬁnite forest consistency implies ﬁnite forest consistency. Finally, in Section 4,
we prove the consistency of several inﬁnite random forests. In particular, taking
one step toward the understanding of Breiman’s forests, we prove that q
quantile forests, a variety of forests whose construction depends on the positions
Xi’s of the data, are consistent. As for Breiman’s forests, each leaf of each tree
in q quantile forests contains a small number of points that does not grow to
inﬁnity with the sample size. Thus, q quantile forests average inconsistent trees
estimate to build a consistent prediction.
We start by giving some notation in Section 2. All proofs are postponed to
Section 5.
Throughout the paper, we assume to be given a training sample Dn = (X1, Y1),
. . . , (Xn, Yn) of d× R-valued independent random variables distributed as
the prototype pair (X, Y ), where E[Y 2] < ∞. We aim at predicting the response
Y , associated with the random variable X, by estimating the regression function
m(x) = E [Y |X = x]. In this context, we use random forests to build an estimate
mn : d →R of m, based on the data set Dn.
A random forest is a collection of M randomized regression trees . For the j-th tree in
the family, the predicted value at point x is denoted by mn(x, Θj, Dn), where
Θ1, . . . , ΘM are independent random variables, distributed as a generic random
variable Θ, independent of the sample Dn. This random variable can be used
to sample the training set or to select the candidate directions or positions for
splitting. The trees are combined to form the ﬁnite forest estimate
mM,n(x, Θ1, . . . , ΘM) = 1
mn(x, Θm).
By the law of large numbers, for any ﬁxed x, conditionally on Dn, the ﬁnite
forest estimate tends to the inﬁnite forest estimate
m∞,n(x) = EΘ [mn(x, Θ)] .
The risk of m∞,n is deﬁned by
R(m∞,n) = E[m∞,n(X) −m(X)]2,
while the risk of mM,n equals
R(mM,n) = E[mM,n(X, Θ1, . . . , ΘM) −m(X)]2.
It is stressed that both risks R(m∞,n) and R(mM,n) are deterministic since the
expectation in (2) is over X, Dn, and the expectation in (3) is over X, Dn and
Θ1, . . . , ΘM.
Throughout the paper, we say that m∞,n (resp.
mM,n) is L2
consistent if R(m∞,n) (resp. R(mM,n)) tends to zero as n →∞.
As mentioned earlier, there is a large variety of forests, depending on how trees
are grown and how the randomness Θ inﬂuences the tree construction.
instance, tree construction can be independent of Dn , depend only
on the Xi’s or depend on the whole training set . Throughout the paper, we
use Breiman’s forests and uniform forests to exemplify our results. In Breiman’s
original procedure, splits depend on the whole sample and are performed to
minimize variance within the two resulting cells.
The algorithm stops when
each cell contains less than a small pre-speciﬁed number of points (typically, 5
in regression and 1 in classiﬁcation). On the other hand, uniform forests are a
simpler procedure since, at each node, a coordinate is uniformly selected among
{1, . . . , d} and a split position is uniformly chosen in the range of the cell, along
the pre-chosen coordinate. The algorithm stops when a full binary tree of level
k is built, that is if each cell has been cut exactly k times, where k ∈N is a
parameter of the algorithm.
In the rest of the paper, we will repeatedly use the random forest connection
function Kn, deﬁned as
 d × d
↔z is the event where x and z belong to the same cell in the tree
Tn(Θ) designed with Θ and Dn. Moreover, notation PΘ denotes the probability
with respect to Θ, conditionally on Dn. The same notational convention holds
for the expectation EΘ and the variance VΘ. Thus, if we ﬁx the training set
Dn, we see that the connection Kn(x, z) is just the proportion of trees in which
x and z are connected.
We say that a forest is discrete (resp.
continuous) if, keeping Dn ﬁxed, its
connection function Kn(•, •) is piecewise constant (resp. continuous). In fact,
most existing forest models fall in one of these two categories. For example, if,
at each cell, the number of possible splits is ﬁnite, then the forest is discrete.
This is the case of Breiman’s forests, where splits can only be performed at the
middle of two consecutive data points along any coordinate. However, if splits
are drawn according to some density along each coordinate, the resulting forest
is continuous. For instance, uniform forests are continuous.
Finite and inﬁnite random forests
Contrary to ﬁnite forests which depend upon the particular Θj’s used to design
trees, inﬁnite forests do not and are therefore more amenable to mathematical
analysis. Besides, ﬁnite forests predictions can be diﬃcult to interpret since they
depend on the random parameters Θj’s. In addition, the Θj’s are independent
of the data set and thus unrelated to the particular prediction problem.
In this section, we study the link between ﬁnite forests and inﬁnite forests. More
speciﬁcally, assuming that the data set Dn is ﬁxed, we examine the asymptotic
behavior of the ﬁnite forest estimate mM,n(•, Θ1, . . . , ΘM) as M tends to inﬁnity. This setting is consistent with practical problems, where the Dn is ﬁxed,
and one can grow as many trees as possible.
Clearly, by the law of large numbers, we know that conditionally on Dn, for all
x ∈ d, almost surely,
mM,n(x, Θ1, . . . , ΘM)
M→∞m∞,n(x).
The following theorem extend the pointwise convergence in (4) to the convergence of the whole functional estimate mM,n(•, Θ1, . . . , ΘM), towards the functional estimate m∞,n(•).
Theorem 3.1. Consider a continuous or discrete random forest. Then, conditionally on Dn, almost surely, for all x ∈ d, we have
mM,n(x, Θ1, . . . , ΘM)
M→∞m∞,n(x).
Remark 1. Since the set d is not countable, we cannot reverse the “almost
sure” and “for all x ∈ d” statements in (4). Thus, Theorem 3.1 is not a
consequence of (4).
Theorem 3.1 is a ﬁrst step to prove that inﬁnite forest estimates can be uniformly approximated by ﬁnite forest estimates. To pursue the analysis, a natural
question is to determine the rate of convergence in Theorem 3.1. The pointwise
rate of convergence is provided by the central limit theorem which says that,
conditionally on Dn, for all x ∈ d,
 mM,n(x, Θ1, . . . , ΘM) −m∞,n(x)
 0, ˜σ2(x)
˜σ2(x) = VΘ
and Nn(x, Θ) is the number of data points falling into the cell of the tree Tn(Θ)
which contains x.
Equation (5) is not suﬃcient to determine the asymptotic distribution of the
functional estimate mM,n(•, Θ1, . . . , ΘM). To make it explicit, we need to introduce the empirical process GM deﬁned
where δΘm is the Dirac function at Θm. We also let F2 = {gx : θ 7→mn(x, θ); x ∈
 d} be the collection of all possible tree estimates in the forest. In order to
prove that a uniform central limit theorem holds for random forest estimates,
we need to show that there exists a Gaussian process G such that
|g(θ)|dGM(θ) −
|g(θ)|dG(θ)
where the ﬁrst part on the left side can be written as
|g(θ)|dGM(θ) =
|g(Θm)| −EΘ
For more clarity, instead of (6), we will write
mn(•, Θm) −EΘ [mn(•, Θ)]
To establish identity (7), we ﬁrst deﬁne, for all ε > 0, the random forest grid
step δ(ε) by
δ(ε) = sup
x1,x2∈ d
∥x1−x2∥∞≤η
1 −Kn(x1, x2)
where Kn is the connection function of the forest. The function δ can be seen
as the modulus of continuity of Kn in the sense that it is the distance such that
Kn(x1, x2) does not vary of much that ε2/8 if ∥x1 −x2∥∞≤δ(ε). We will also
need the following assumption.
(H1) One of the following properties is satisﬁed:
• The random forest is discrete,
• There exist C, A > 0, α < 2 such that, for all ε > 0,
δ(ε) ≥C exp(−A/εα).
Observe that (H1) is mild since most forests are discrete and the only continuous
forest we have in mind, the uniform forest, satisﬁes (H1), as stated in Lemma
Lemma 1. Let k ∈N. Then, for all ε > 0, the grid step δ(ε) of uniform forests
of level k satisﬁes
where Ak,d = (8de(k + 2)!)1/3.
The following theorem states that a uniform central limit theorem is valid over
the class of random forest estimates, providing that (H1) is satisﬁed.
Theorem 3.2. Consider a random forest which satisﬁes (H1). Then,
M (mM,n(•) −m∞,n(•))
where G is a Gaussian process with mean zero and a covariate function
CovΘ(Ggx, Ggz) = CovΘ
According to the discussion above, Theorem 3.2 holds for uniform forests (by
Lemma 1) and Breiman’s forests (since they are discrete). Moreover, according
to this Theorem, the ﬁnite forest estimates tend uniformly to the inﬁnite forest
estimates, with the standard rate of convergence
M. This result contributes
to bridge the gap between ﬁnite forests used in practice and inﬁnite theoretical
The proximity between two estimates can also be measured in terms of their L2
risk. In this respect, Theorem 3.3 states that the risk of inﬁnite forests is lower
than the one of ﬁnite forests and provides a bound on the diﬀerence between
these two risks. We ﬁrst need an assumption on the regression model.
(H2) One has
Y = m(X) + ε,
where ε is a centered Gaussian noise with ﬁnite variance σ2, independent of X,
x∈ d |m(x)| < ∞.
Theorem 3.3. Assume that (H2) is satisﬁed. Then, for all M, n ∈N⋆,
R(mM,n) = R(m∞,n) + 1
VΘ [mn(X, Θ)]
In particular,
0 ≤R(mM,n) −R(m∞,n) ≤8
∞+ σ2(1 + 4 log n)
Theorem 3.3 reveals that the prediction accuracy of inﬁnite forests is better than
that of ﬁnite forests. In practice however, there is no simple way to implement
inﬁnite forests and, in fact, ﬁnite forests are nothing but Monte Carlo approximations of inﬁnite forests. But, since the diﬀerence of risks between both types
of forests is bounded (by Theorem 3.3), the prediction accuracy of ﬁnite forests
is almost as good as that of inﬁnite forests providing the number of trees is large
enough. More precisely, under (H2), for all ε > 0, if
+ 32σ2 log n
then R(mM,n) −R(m∞,n) ≤ε.
Anoter interesting consequence of Theorem 3.3 is that, assuming that (H2)
holds and that M/ log n →∞as n →∞, ﬁnite random forests are consistent as
soon as inﬁnite random forests are. This alows to extend all previous consistency
results regarding inﬁnite forests 
to ﬁnite forests.
It must be stressed that the “log n” term comes from the
Gaussian noise, since, if ε1, . . . , εn are independent and distributed as a Gaussian
noise ε ∼N(0, σ2), we have,
≤σ2(1 + 4 log n),
 . Therefore, the required number
of trees depends on the noise in the regression model. For instance, if Y is
bounded, then the condition turns into M →∞.
Consistency of some random forest models
Section 3 was devoted to the connection between ﬁnite and inﬁnite forests. In
particular, we proved in Theorem 3.3 that the consistency of inﬁnite forests
implies that of ﬁnite forests, as soon as (H2) is satisﬁed and M/ log n →∞.
Thus, it is natural to focus on the consistency of inﬁnite forest estimates, which
can be written as
ni (X) = EΘ
are the random forest weights.
Proving consistency of inﬁnite random forests is in general a diﬃcult task,
mainly because forest construction can depend on both the Xi’s and the Yi’s.
This feature makes the resulting estimate highly data-dependent, and therefore diﬃcult to analyze (this is particularly the case for Breiman’s forests). To
simplify the analysis, we investigate hereafter inﬁnite random forest estimates
whose weights depends only on X, X1, . . . , Xn which is called the X-property.
The good news is that when inﬁnite forest estimates have the X-property, they
fall in the general class of local averaging estimates, whose consistency can be
addressed using Stone’s theorem.
Therefore, using Stone’s theorem as a starting point, we ﬁrst prove the consistency of random forests whose construction is independent of Dn, which is the
simplest case of random forests satisfying the X-property. For such forests, the
construction is based on the random parameter Θ only. As for now, we say that
a forest is totally non adaptive of level k (k ∈N, with k possibly depending
on n) if each tree of the forest is built independently of the training set and if
each cell is cut exactly k times. The resulting cell containing X, designed with
randomness Θ, is denoted by An(X, Θ).
Theorem 4.1. Assume that X is distributed on d and consider a totally
non adaptive forest of level k. In addition, assume that for all ρ, ε > 0, there
exists N > 0 such that, with probability 1 −ρ, for all n > N,
diam(An(X, Θ)) ≤ε.
Then, providing k →∞and 2k/n →0, the inﬁnite random forest is L2 consistent, that is
R(m∞,n) →0
Theorem 4.1 is a generalization of some consistency results in Biau et al. 
for the case of totally non adaptive random forest. Together with Theorem 3.3,
we see that if (H2) is satisﬁed and M/ log n →∞as n →∞, then the ﬁnite
random forest is L2 consistent.
According to Theorem 4.1, a totally non adaptive forest of level k is consistent
if the cell diameters tend to zero as n →∞and if the level k is properly tuned.
This is in particular true for uniform random forests, as shown in the following
corollary.
Corollary 1. Assume that X is distributed on d and consider a uniform
forest of level k.
Then, providing that k →∞and 2k/n →0, the uniform
random forest is L2 consistent.
For totally non adaptive forests, the main diﬃculty that consists in using the
data set to build the forest and to predict at the same time, vanishes. However,
because of their simpliﬁed construction, these forests are far from accurately
modelling Breiman’s forest. To take one step further into the understanding of
Breiman’s forest behavior, we study the q (q ∈[1/2, 1)) quantile random
forest, which satisﬁes the X-property. Indeed, their construction depends on
the Xi’s which is a good trade oﬀbetween the complexity of Breiman’s forests
and the simplicity of totally non adaptive forests. As an example of q quantile
trees, the median tree (q = 1/2) has already been studied by Devroye et al.
 , such as the k-spacing tree whose construction is
based on quantiles.
In the spirit of Breiman’s algorithm, before growing each tree, data are subsampled, that is an points (an < n) are selected without replacement. Then, each
split is performed on an empirical qn-quantile (where qn ∈[1 −q, q] can be prespeciﬁed by the user or randomly chosen) along a coordinate, chosen uniformly
at random among the d coordinates. Recall that the q′-quantile (q′ ∈[1−q, q]) of
X1, . . . , Xn is deﬁned as the only X(ℓ) satisfying Fn(X(ℓ−1)) ≤qn < Fn(X(ℓ)),
where the X(i)’s are ordered increasingly. Note that data points on which splits
are performed are not sent down to the resulting cells. Finally, the algorithm
stops when each cell contains exactly one point. The full procedure is described
in Algorithm 2.
Algorithm 1: q quantile forest predicted value at x.
Input: Fix an ∈{1, . . . , n}, and x ∈ d.
Data: A training set Dn.
1 for j = 1, . . . , M do
Select an points, without replacement, uniformly in Dn.
Set P = { p} the partition associated with the root of the tree.
while there exists A ∈P which contains strictly more than two points do
Select uniformly one dimension j within {1, . . . , p}.
Let N be the number of data points in A and select qn ∈[1 −q, q]∩
(1/N, 1 −1/N).
Cut the cell A at the position given by the qn empirical quantile (see
deﬁnition below) along the j-th coordinate.
Call AL and AR the two resulting cell.
Set P ←(P\{A}) ∪AL ∪AR.
for each A ∈P which contains exactly two points do
Select uniformly one dimension j within {1, . . . , p}.
Cut along the j-th direction, in the middle of the two points.
Call AL and AR the two resulting cell.
Set P ←(P\{A}) ∪AL ∪AR.
Compute the predicted value mn(x, Θj) at x equal to the single Yi falling
in the cell of x, with respect to the partition P.
19 Compute the random forest estimate mM,n(x; Θ1, . . . , ΘM, Dn) at the query
point x according to equality (1).
Since the construction of q quantile forests depends on the Xi’s and is based
on subsampling, it is a more realistic modeling of Breiman’s forests than totally
non adaptive forests. It also provides a good understanding on why random
forests are still consistent even when there is exactly one data point in each leaf.
Theorem 4.2 states that with a proper subsampling rate of the training set, the
q quantile random forests are consistent.
(H3) One has
Y = m(X) + ε,
where ε is a centred Gaussian noise with ﬁnite variance σ2 variable, independent
of X. Moreover, X has a density bounded from below and from above and m is
continuous.
Theorem 4.2.
Assume that (H3) is satisﬁed. Then, providing an →∞et
an/n →∞, the inﬁnite q quantile random forest is L2 consistent.
Some remarks are in order. At ﬁrst, observe that each tree in the q quantile
forest is inconsistent , because each leaf
contains exactly one data point, a number which does not grow to inﬁnity as
n →∞. Thus, Theorem 4.2 shows that q quantile forest combines inconsistent
trees to form a consistent estimate.
Secondly, many random forests can be seen as quantile forests if they satisfy the
X-property and if splits do not separate a small fraction of data points from the
rest of the sample. The last assumption is true, for example, if X has a density
on d bounded from below and from above, and if some splitting rule forces
splits to be performed far away from the cell edges. This assumption is explicitly
made in the analysis of Meinshausen and Wager to ensure that
cell diameters tend to zero as n →∞, which is a necessary condition to prove
the consistency of partitioning estimates .
We note ﬁnally that Theorem 4.2 does not cover the bootstrap case since in that
case, an = n data points are selected with replacement. However, the condition
on the subsampling rate can be replaced by the following one: for all x,
→0 as n →∞.
Condition (9) can be interpreted by saying that a point x should not be connected too often to the same data point in the forest, thus meaning that trees
have to be various enough to ensure the forest consistency. This idea of diversity among trees has already been suggested by Breiman . In bootstrap case, a single data point is selected in about 64% of trees.
term maxi PΘ
is roughly upper bounded by 0.64 which is not suf-
ﬁcient to prove (9).
It does not mean that random forests based on bootstrap are inconsistent but that a more detailed analysis is required. A possible,
but probably diﬃcult, route is an in-depth analysis of the connection function
Kn(x, Xi) = PΘ
Proof of Theorem 3.1
We assume that Dn is ﬁxed and prove Theorem 3.1 for d = 2. The general case
can be treated similarly. Throughout the proof, we write, for all θ, x, z ∈ 2,
Let us ﬁrst consider a discrete random forest. By deﬁnition of such random
forests, there exists p ∈N⋆and a partition {Ai : 1 ≤i ≤p} of 2 such that
the connection function Kn is constant over the sets Ai × Aj’s (1 ≤i, j ≤p).
For all 1 ≤i ≤p, denote by ai, the center of the cell Ai. Take x, z ∈R2. There
exist i, j such that x ∈Ai, z ∈Aj. Thus, for all θ,
Nn(x, θ) −
Nn(x, θ) −
Nn(ai, θ) +
Nn(ai, θ) −
Nn(ai, θ)1x
Nn(ai, θ)1aj
Thus, the set
θ 7→fx,z(θ) : x, z ∈ 2
is ﬁnite. Therefore, by the strong law of large numbers, almost surely, for all
Noticing that W M
m=1 fx,Xi(Θm), we obtain that, almost surely, for
all x ∈ 2,
ni (x) →W ∞
Since Dn is ﬁxed and random forest estimates are linear in the weights, the
proof of the discrete case is complete.
Let us now consider a continuous random forest. We deﬁne, for all x, z ∈ 2,
n (x, z) = 1
Nn(x, Θm),
n (x, z) = EΘ
According to the strong law of large numbers, almost surely, for all x, z ∈
 2 ∩Q2,
n (x, z) = W ∞
Set x, z ∈ 2 where x = (x(1), x(2)) and z = (z(1), z(2)). Assume, without
loss of generality, that x(1) < z(1) and x(2) < z(2). Let
Ax = {u ∈ 2, u(1) ≤x(1) and u(2) ≤x(2)},
and Az = {u ∈ 2, u(1) ≥z(1) and u(2) ≥z(2)}.
Choose x1 ∈Ax ∩Q2 (resp. z2 ∈Az ∩Q2) and take x2 ∈ 2 ∩Q2 (resp.
z1 ∈ 2 ∩Q2) such that x1, x, x2 (resp. z2, z, z1) are aligned in this order
(see Figure 1).
Figure 1: Respective positions of x, x1, x2 and z, z1, z2
n (x, z) −W ∞
n (x, z) −W M
n (x1, z2)
n (x1, z2) −W ∞
n (x1, z2)
n (x1, z2) −W ∞
n (x, z)| .
Set ε > 0. Because of the continuity of Kn, we can choose x1, x2 close enough
to x and z2, z1 close enough to z such that,
|Kn(x2, x1) −1| ≤ε,
|Kn(z1, z2) −1| ≤ε,
|1 −Kn(x1, x)| ≤ε,
|1 −Kn(z2, z)| ≤ε.
Let us consider the second term in equation (10). Since x1, z2 belong to 2 ∩
Q2, almost surely, there exists M1 > 0 such that, if M > M1,
n (x1, z2) −W ∞
n (x1, z2)
Considering the ﬁrst term in (10), we have
n (x, z) −W ∞
n (x1, z2)
Nn(x, Θm) −
Observe that, given the positions of x, x1, z, z2, the only case where
Nn(x, Θm) −
occurs when x1
Nn(x, Θm) −
Nn(x, Θm) −
Again, given the relative positions of x, x1, x2, z, z2, z1, we obtain
Collecting the previous inequalities, we have
n (x, z) −W ∞
n (x1, z2)
Since x2, z1, x1, z2 ∈ 2 ∩Q2, we deduce that there exists M2 such that, for
all M > M2,
n (x, z) −W ∞
n (x1, z2)
≤2 −K∞(x2, x1) −K∞(z1, z2) + 2ε.
Considering the third term in (10), using the same arguments as above, we see
n (x1, z2) −W ∞
n (x, z)| ≤EΘ
Nn(x1, Θ) −
Nn(x1, Θ) −
Nn(x, Θ)1x1
≤2 −Kn(x1, x2) −Kn(z2, z1).
Using inequalities (11) and (12) in (10), we ﬁnally conclude that, for all M >
max(M1, M2),
n (x, z) −W ∞
≤4 −2K∞(x2, x1) −2K∞(z1, z2) + 3ε
This completes the proof of Theorem 3.1.
Proof of Lemma 1 and Theorem 3.2
Proof of Lemma 1. Set k ∈N and ε > 0. We start by considering the case
where d = 1. Take x, z ∈ and let w = −log (|x −z|). The probability that
x and z are not connected in the uniform forest after k cuts is given by
1 −Kk(x, z) ≤1 −Kk(0, |z −x|)
(according to Technical Lemma 1, see the end of the section)
(according to Technical Lemma 2, see the end of the section)
≤(k + 2)!e
for all w > 1. Now, consider the multivariate case, and let x, z ∈ d. Set,
for all 1 ≤j ≤d, wj = −log (|xj −zj|).
By union bound, recalling that
1 −Kk(x, z) = PΘ(x
↮z), we have
1 −Kk(x, z) ≤
(1 −Kk(xj, zj))
≤d(k + 2)!e
Thus, if, for all 1 ≤j ≤d,
|xj −zj| ≤exp
−(Ak,d)1/3
1 −Kk(x, z) ≤ε2
where Ak,d = (8de(k + 2)!)1/3. Consequently,
−(Ak,d)1/3
Proof of Theorem 3.2. We start the proof by proving that the class
θ 7→fx,z(θ) : x, z ∈R2
is PΘ-Donsker, that is, there exists a Gaussian process G such that
E|f| (dGM −dG)
At ﬁrst, let us consider a ﬁnite random forest.
As noticed in the proof of
Theorem 3.1, the set H is ﬁnite. Consequently, by the central limit theorem,
the set H is PΘ-Donsker.
Now, consider a random forest which satisﬁes the second statement in Assumption 1. Set ε > 0. Consider a regular grid of d with a step δ and let Gδ be
the set of nodes of this grid. We start by ﬁnding a condition on δ such that the
˜Gδ = {[fx1,z1, fx2,z2] : x1, x2, z1, z2 ∈Gδ}
is a covering of ε-bracket of the set H, that is, for all f ∈H, there exists
x1, z1, x2, z2 ∈Gδ such that
fx1,z1 ≤f ≤fx2,z2 and E1/2 [fx2,z2(Θ) −fx1,z1(Θ)]2 ≤ε.
To this aim, set x, z ∈ d and choose x1, x2, z1, z2 ∈Gδ (see Figure 2). Note
that, for all θ,
Nn(x1, θ) ≤
Nn(x, θ) ≤
Nn(x2, θ),
that is, fx1,z2 ≤fx,z ≤fx2,z1. To prove the second statement in (13), observe
fx2,z2(Θ) −fx1,z1(Θ)
Nn(x1, Θ) −
Nn(x1, Θ) −
1 −Kn(x1, x2) + 1 −Kn(z1, z2).
Figure 2: Respective positions of x, x1, x2 and z, z1, z2 with d = 2.
Thus, we have to choose the grid step δ such that
x1,x2∈ d
∥x1−x2∥∞≤δ
1 −Kn(x1, x2)
By Assumption 1 and the deﬁnition of the random forest grid step, there exist
constants C, A > 0 and 0 < α < 2 such that, for all ε > 0, if
δ ≥C exp(−A/εα),
then (14) is satisﬁed. Hence, if δ satisﬁes (15), then ˜Gδ is a covering of ε-bracket
of H. In that case, the number N[ ](ε, F, L2(P)) of ε-bracket needed to cover
H satisﬁes
N[ ](ε, F, L2(P)) ≤Card( ˜Gδ) ≤Card(Gδ)4 ≤
Consequently,
log N[ ](ε, F, L2(P)) ≤
εα −2d log C
where the last term is integrable near zero since α < 2. Thus, according to
Theorem 2.5.6 in van der Vaart and Wellner , the class H is
PΘ-Donsker.
To conclude the proof, consider a random forest satisfying (H1). From above,
we see that the class H is PΘ-Donsker. Recall that F2 = {gx : θ 7→mn(x, θ) :
x ∈ d}, where
mn(x, Θ) =
Yifx,Xi(Θ).
Since the training set Dn is ﬁxed, we have
E|gx| (dGM −dG)
E|fx,Xi| (dGM −dG)
x,z∈ d
E|fx,z| (dGM −dG)
which tends to zero as M tends to inﬁnity, since the class H is PΘ-Donsker.
Finally, note that Breiman’s random forests are discrete, thus satisfying (H1).
Uniform forests are continuous and satisfy (H1) according to Lemma 1.
Proof of Theorem 3.3
Observe that,
mM,n(X, Θ1, . . . , Θm) −m(X)
mM,n(X, Θ1, . . . , Θm) −EΘ [mn(X, Θ)]
EΘ [mn(X, Θ)] −m(X)
EΘ [mn(X, Θ)] −m(X)
mM,n(X, Θ1, . . . , Θm) −EΘ [mn(X, Θ)]
Taking the expectation on both sides, we obtain
R(mM,n, m) = R(m∞,n, m) + E
mM,n(X, Θ1, . . . , Θm) −EΘ [mn(X, Θ)]
by noticing that
mM,n(X, Θ1, . . . , Θm) −EΘ [mn(X, Θ)]
EΘ [mn(X, Θ)] −m(X)
EΘ [mn(X, Θ)] −m(X)
× EΘ1,...,ΘM
mM,n(X, Θ1, . . . , Θm) −EΘ
according to the deﬁnition of mM,n. Fixing X and Dn, note that random variables mn(X, Θ1), . . . , mn(X, Θ1) are independent and identically distributed.
Thus, we have
E [mM,n(X, Θ1, . . . , Θm) −EΘ [mn(X, Θ)]]2
= EX,DnEΘ1,...,ΘM
mn(X, Θm) −EΘ [mn(X, Θ)]
VΘ [mn (X, Θ)]
which conludes the ﬁrst part of the proof. Now, note that,
R(mM,n) −R(m∞,n) = 1
VΘ [mn(X, Θ)]
Wni(X, Θ)(m(Xi) + εi)
Wni(X, Θ)εi
1≤i≤n εi −min
The term inside the brackets is the maximum of n χ2-squared distributed random variables. Thus, for all n ∈N⋆,
≤1 + 4 log n,
 . Therefore,
R(mM,n) −R(m∞,n) ≤8
∞+ σ2(1 + 4 log n)
Proof of Theorem 4.1 and Proposition 1
The proof of Theorem 4.1 is based on Stone’s theorem which is recalled here.
Stone’s theorem . Assume that the following conditions are satisﬁed
for every distribution of X:
(i) There is a constant c such that for every non negative measurable function
f satisfying Ef(X) < ∞and any n,
Wni(X)f(Xi)
≤c E (f(X)) .
(ii) There is a D > 1 such that, for all n,
Wni(X) < D
(iii) For all a > 0,
Wni(X)1∥X−Xi∥>a
(iv) The sum of weights satisﬁes
in probability.
1≤i≤n Wni(X)
Then the corresponding regression function estimate mn is universally L2 consistent, that is,
n→∞E [m∞,n(X) −m(X)]2 = 0,
for all distributions of (X, Y ) with EY 2 < ∞.
Proof of Theorem 4.1. We check the assumptions of Stone’s theorem. For every
non negative measurable function f satisfying Ef(X) < ∞and for any n, almost
Wni(X, Θ)f(Xi)
≤EX (f(X)) ,
Wni(X, Θ) = 1Xi∈An(X,Θ)
are the weights of the random tree Tn(Θ) . Taking expectation with respect to Θ from both sides, we have
ni (X)f(Xi)
≤EX (f(X)) ,
which proves the ﬁrst condition of Stone’s theorem.
According to the deﬁnition of random forest weights W ∞
ni , since Pn
i=1 Wni(X, Θ)
≤1 almost surely, we have
ni (X) = EΘ
To check condition (iii), note that, for all a > 0,
ni (X)1∥X−Xi∥∞>a
↔Xi1∥X−Xi∥∞>a
↔Xi1∥X−Xi∥∞>a
× 1diam(An(X,Θ))≥a/2
because 1∥X−Xi∥∞>a1diam(An(X,Θ))<a/2 = 0. Thus,
ni (X)1∥X−Xi∥∞>a
1diam(An(X,Θ))≥a/2
↔Xi1∥X−Xi∥∞>a
diam(An(X, Θ)) ≥a/2
which tends to zero, as n →∞, by assumption.
To prove assumption (iv), we follow the arguments developed by Biau et al.
 . For completeness, these arguments are recalled here. Let us consider the
partition associated with the random tree Tn(Θ). By deﬁnition, this partition
has 2k cells, denoted by A1, . . . , A2k. For 1 ≤i ≤2k, let Ni be the number of
points among X, X1, . . . , Xn falling into Ai. Finally, set S = {X, X1, . . . , Xn}.
Since these points are independent and identically distributed, ﬁxing the set S
(but not the order of the points) and Θ, the probability that X falls in the i-th
cell is Ni/(n + 1). Thus, for every ﬁxed t > 0,
Nn(X, Θ) < t
Nn(X, Θ) < t
Thus, by assumption, Nn(X, Θ) →∞in probability, as n →∞. Consequently,
observe that
ni (X) = EΘ
1Nn(X,Θ)̸=0
= PΘ [Nn(X, Θ) ̸= 0]
At last, to prove (v), note that,
1Xi∈An(X,Θ)
since Nn(X, Θ) →∞in probability, as n →∞.
Proof of Proposition 1. We check conditions of Theorem 4.1. Let us denote by
Vnj(X, Θ) the length of the j-th side of the cell containing X and Knj(X, Θ)
the number of times the cell containing X is cut along the j-coordinate. Note
that, if U1, . . . , Un are independent uniform on ,
E [Vnj(X, Θ)] ≤E
max(Ui, 1 −Ui)|Knj(X, Θ)
max(U1, 1 −U1)
iKnj(X,Θ)
Knj(X,Θ)#
Since Knj(X, Θ) is distributed as a binomial B(kn, 1/d), Knj(X, Θ) →+∞in
probability, as n tends to inﬁnity. Thus E [Vnj(X, Θ)] →0 as n →∞.
Proof of Theorem 4.2
To prove Theorem 4.2, we need the following lemma which states that the cell
diameter of a quantile tree tends to zero.
Lemma 2. Assume that X has a density f over d, with respect to the
Lebesgue measure and that there exist two constants c, C > 0 such that, for all
x ∈ d,
c ≤f(x) ≤C.
Thus, for all q ∈[1/2, 1), the q quantile tree deﬁned in Algorithm 2 satisﬁes,
for all γ,
diam(An(X, Θ)) > γ
Proof of Lemma 2. Set q ∈[1/2, 1). At ﬁrst, consider a theoretical q quantile
tree where cuts are made similarly as in the q quantile tree but by selecting
qn ∈[1 −q, q] and by performing the cut at the qn theoretical quantile (instead
of empirical one). The tree is then stopped at level k, where k ∈N is a parameter
to be chosen later. Then, consider a cell A = Qd
j=1[ai, bi] of the theoretical q
quantile tree. Assume that this cell is cut along the ﬁrst coordinate and let z be
the split position. Thus, by deﬁnition of the theoretical q quantile tree, there
exists q′ ∈[1 −q, q] such that
f(x)dx1 . . . dxd = q′
f(x)dx1 . . . dxd,
g(x1)dx1 = q′
where g(x1) =
ad f(x)dx2 . . . dxd. Letting µd−1(A) = Qd
j=2(bj −aj), by
assumption, we have, for all x1 ∈[a1, b1],
cµd−1(A) ≤g(x1) ≤Cµd−1(A).
Hence, using (16), we obtain
(1 −q′)(z −a1)cµd−1(A) ≤q′
g(x1)dx1 ≤q′(b1 −z)Cµd−1(A),
which leads to
q′C + (1 −q′)c ∈]0, 1[.
Similarly,
q′(b1 −z)cµd−1(A) ≤(1 −q′)
g(x1)dx1 ≤(1 −q′)(z −a1)Cµd−1(A),
which yields
q′c + (1 −q′)C ∈]0, 1[.
Combining (17) and (18), we deduce that
, 1 −z −a1
q′C + (1 −q′)c,
q′c + (1 −q′)C
(1 −q)(c + C).
Consequently, letting
(1 −q)(c + C) ∈(0, 1),
the ﬁrst dimension of A is reduced at most by a factor 1 −α > 0 and at least
by a factor α < 1.
Denote by Vik(X, Θ) the length of the i-th side of the cell containing X at level
k in the theoretical q quantile tree, and let Kik(X, Θ) the number of times this
cell has been cut along the i-th coordinate. Hence, for all i ∈{1, . . . , d},
E [Vik(X, Θ)] ≤E
αKik(X,Θ)
which proves that the cell diameter of the theoretical q quantile tree tends to
zero, as the level k tends to inﬁnity.
Now, consider the empirical q quantile tree as deﬁned in Algorithm 2 but
stopped at level k. Thus, for n large enough, at each step of the algorithm, qn
is selected in [1 −q, q]. Set ε, η > 0 and let Gk(X, Θ) be the event where all k
cuts used to build the cell Ak(X, Θ) are distant of less than η from cuts used to
build the cell A⋆
k(X, Θ) of the theoretical q quantile tree. Thus,
diam(Ak(X, Θ))
diam(Ak(X, Θ)1Gk(X,Θ))
diam(Ak(X, Θ))1Gk(X,Θ)c)
diam(Ak(X, Θ)1Gk(X,Θ))
Gk(X, Θ)c
From equation (19), there exists k0 ∈N⋆such that
Since, on the event Gk(X, Θ), the k consecutive cuts used to build A⋆
are distant of less than η from the k cuts used to design Ak(X, Θ), we have
diam(Ak0(X, Θ))
< k0η + ε.
With respect to the second term in equation (20), consider a cell A of the empirical q quantile tree. Without loss of generality, we assume that the next split
is performed along the ﬁrst coordinate. Let F A (resp. F A
n ) be the one dimensional conditional distribution function (resp. empirical distribution function)
of X given that X ∈A. Denote by zA
n the position of the empirical split performed in A. Since X is uniformly distributed over d, (F A)′ ≥c/µ(A).
Thus, since F A is an increasing function, if sup
|F A(x) −F A
n (x)| ≤cη/µ(A)
where ZA = {z, F A(z) ∈[1−q, q]}. Recall that A1(X, Θ), . . . , Ak(X, Θ) are the
consecutive cells containing X designed with Θ. Observe that, conditionally on
the position of the split, data on the left side of the split are still independent
and identically distributed according to Proposition 2.1 in Biau et al. .
Thus, we have
Gk(X, Θ)c
z∈ZAℓ(X,Θ) |zAℓ(X,Θ)
−zAℓ(X,Θ)| > η
z∈ZAℓ(X,Θ) |zAℓ(X,Θ)
−zAℓ(X,Θ)| > η
N(Aℓ(X, Θ))
x |F(x) −Fn(x)| ≥
N(Aℓ(X, Θ)),
Consequently,
Gk(X, Θ)c
−2N(Aℓ(X, Θ))c2η2
µ(Aℓ(X, Θ))2
 
−2c2η2 (1 −q)kn −
N(Aℓ(X, Θ)) ≥(1 −q)kn −
Thus, for all k and for all n > log k/(2c2η2(1 −q)k), we obtain
Gk(X, Θ)c
Gathering (20), (21) and (22), we conclude that, for all n > log k0/(2c2η2(1 −
diam(Ak(X, Θ))
≤(k0 + 1)η + ε.
Since the diameter is a non increasing function of the level of the tree, the cell
diameter of the fully developed tree (which contain exactly one point in each
leaf) is lower than that of the tree stopped at k0. Letting A(X, Θ) the cell of
the (fully developed) empirical q quantile tree (deﬁned in Algorithm 2), we
diam(An(X, Θ))
diam(Ak(X, Θ))
≤(k0 + 1)η + ε,
which concludes the proof, since ε and η can be made arbitrarily small.
Proof of Theorem 4.2. We check the conditions of Stone’s theorem.
Condition (i) is satisﬁed since the regression function is uniformly continuous and
Var[Y |X] = σ2 .
Condition (ii) is always satisﬁed for random trees. Condition (iii) is veriﬁed
PX,Θ [diam(An(X, Θ)) > γ]
according to Lemma 2.
Since each cell contains exactly one data point,
1Xi∈An(X,Θ)
1Xi∈An(X,Θ)
Thus, conditions (iv) of Stone theorem is satisﬁed.
To check (v), observe that in the subsampling step, there are exactly
choices to pick a ﬁxed observation Xi. Since x and Xi belong to the same cell
only if Xi is selected in the subsampling step, we see that
1≤i≤n Wni(X)
which tends to zero by assumption.
Proofs of Technical Lemmas 2 and 1
Technical Lemma 1. Take k ∈N and consider a uniform random forest where
each tree is stopped at level k. For all x, z ∈ d, its connection function
Kk(0, |x −z|) ≤Kk(x, z),
where |x −z| = (|x1 −z1|, . . . , |xd −zd|).
Proof. Take x, z ∈ . Without loss of generality, one can assume that x < z
and let µ = z −x. Consider the following two conﬁgurations.
Figure 3: Scheme of conﬁguration 1 (at the top) and 2 (at the bottom).
For any k ∈N⋆, we let dk = (d1, . . . , dk) (resp.
1, . . . , d′
consecutive cuts in conﬁguration 1 (resp. in conﬁguration 2). We denote by Ak
k) the set where dk (resp. d′
k) belong.
We show that for all k ∈N⋆, there exists a coupling between Ak and A′
satisfying the following property: any k-tuple dk is associated with a k-tuple
k such that
1. if dk separates [x, z] then d′
k separates [0, z −x],
2. if dk does not separate [x, z] and d′
k does not separate [0, z −x], then the
length of the cell built with dk is higher than the one built with d′
We call Hk this property. We now proceed by induction. For k = 1, we use the
function g to map A1 into A′
1 such that:
Thus, for any d1 ∈A1, if d1 separates [x, z], then d′
1 = g1(d1) separates [0, z−x].
Besides, the length of the cell containing [x, z] designed with the cut d1 is higher
than that of the cell containing [0, z−x] designed with the cut d′
1. Consequently,
H1 is true.
Now, take k > 1 and assume that Hk is true. Consequently, if dk separates
[x, z] then gk(dk) separates [0, z −x]. In that case, dk+1 separates [x, z] and
gk+1(dk+1) separates [0, z −x]. Thus, in the rest of the proof, we assume that
dk does not separate [x, z] and gk(dk) does not separate [0, z −x]. Let [ak, bk]
be the cell containing [x, z] built with cuts dk. Since the problem is invariant by
translation, we assume, without loss of generality, that [ak, bk] = [0, δk], where
δk = bk −ak and [x, z] = [xk, xk + µ] (see Figure 4).
Figure 4: Conﬁguration 1a (at the top) and 1b (at the bottom).
In addition, according to Hk, the length of the cell built with dk is higher than
the one built with d′
k. Thus, one can ﬁnd λ ∈(0, 1) such that d′
k = λδk. This
is summarized in Figure 5.
Figure 5: Conﬁguration 1b (at the top) and 2b (at the bottom).
Thus, one can map [0, δk] into [0, λδk] with gk+1 deﬁned as
if u > xk + µ
λ(x + µ −u)
if u ≤xk + µ
Note that, for all dk+1, the length of the cell containing [xk, xk+µ] designed with
the cut dk+1 (conﬁguration 1b) is bigger than the length of the cell containing
[0, µ] designed with the cut d′
k+1 = gk+1(dk+1) (conﬁguration 2b). Besides, if
dk+1 ∈[xk, xk + µ] then gk+1(dk+1) ∈[0, µ]. Consequently, the set of functions
g1, . . . , gk+1 induce a mapping of Ak+1 into A′
k+1 such that Hk+1 holds. Thus,
Technical Lemma 1 holds for d = 1.
To address the case where d > 1, note that
Kk(x, z) =
k1! . . . kd!
Kkm(xm, zm)
k1! . . . kd!
Kkm(0, |zm −xm|)
≥Kk(0, |z −x|),
which concludes the proof.
Technical Lemma 2. Take k ∈N and consider a uniform random forest where
each tree is stopped at level k. For all x ∈ , its connection function Kk(0, x)
Kk(0, x) = 1 −x
with the notational convention that the last sum is zero if k = 0.
Proof of Technical Lemma 2. The result is clear for k = 0. Thus, set k ∈N⋆
and consider a uniform random forest where each tree is stopped at level k.
Since the result is clear for x = 0, take x ∈]0, 1] and let I = [0, x]. Thus
Kk(0, x) = P
p(dzk|zk−1)p(dzk−1|zk−2) . . . p(dz2|z1)p(dz1),
where z1, . . . , zk are the positions of the k cuts (see Figure 6).
Figure 6: Positions of cuts z1, . . . , zk and x with d = 1
We prove by induction that, for every integer p,
p(dzk|zk−1) . . . p(dzk−p|zk−p−1)
[ln(zk−p−1/x)]j
Denote by Hp this property. Since, given zk−1, zk is uniformly distributed over
[0, zk−1], we have
p(dzk|zk−1) = 1 −
Thus H0 is true. Now, ﬁx p > 0 and assume that Hp is true. Let u = zk−p−1/x.
Thus, integrating both sides of Hp, we deduce,
zk−p−1 /∈I
p(dzk|zk−1) . . . p(dzk−p|zk−p−1)p(dzk−p−1|zk−p−2)
zk−p−1 /∈I
[ln(zk−p−1/x)]j
p(dzk−p−1|zk−p−2)
[ln(zk−p−1/x)]j
Z zk−p−2/x
Using integration by parts on the last term, we conclude that Hp+1 is true.
Thus, for all p > 0, Hp is veriﬁed. Finally, using Hk−1 and the fact that z0 = 1,
we conclude the proof.