Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1549–1559
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Towards Scalable and Reliable Capsule Networks
for Challenging NLP Applications
Wei Zhao†, Haiyun Peng‡, Steffen Eger†, Erik Cambria‡ and Min YangΦ
† Computer Science Department, Technische Universit¨at Darmstadt, Germany
‡ School of Computer Science and Engineering, Nanyang Technological University, Singapore
Φ Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China
www.aiphes.tu-darmstadt.de
Obstacles hindering the development of capsule networks for challenging NLP applications include poor scalability to large output spaces and less reliable routing processes.
In this paper, we introduce (i) an agreement
score to evaluate the performance of routing
processes at instance level; (ii) an adaptive
optimizer to enhance the reliability of routing; (iii) capsule compression and partial routing to improve the scalability of capsule networks. We validate our approach on two NLP
tasks, namely: multi-label text classiﬁcation
and question answering. Experimental results
show that our approach considerably improves
over strong competitors on both tasks. In addition, we gain the best results in low-resource
settings with few training instances.1
Introduction
In recent years,
deep neural networks have
achieved outstanding success in natural language
processing (NLP), computer vision and speech
recognition. However, these deep models are datahungry and generalize poorly from small datasets,
very much unlike humans .
This is an important issue in NLP since sentences with different surface forms can convey the
same meaning (paraphrases) and not all of them
can be enumerated in the training set. For example, Peter did not accept the offer and Peter turned
down the offer are semantically equivalent, but use
different surface realizations.
In image classiﬁcation, progress on the generalization ability of deep networks has been made
by capsule networks . They are capable of generalizing to
the same object in different 3D images with various viewpoints.
1Our code is publicly available at 
Jerry completed his
Jerry managed to finish
his project.
Jerry succeeded in
finishing his project.
Extrapolate
Extrapolated sentences
Unseen sentences
Observed sentences
Extrapolate operation
Extrapolation regime
Jerry is sleeping.
Figure 1: The extrapolation regime for an observed
sentence can be found during training. Then, the unseen sentences in this regime may be generalized successfully.
Such generalization capability can be learned
from examples with few viewpoints by extrapolation .
This suggests that
capsule networks can similarly abstract away from
different surface realizations in NLP applications.
Figure 1 illustrates this idea of how observed
sentences in the training set are generalized to unseen sentences by extrapolation. In contrast, traditional neural networks require massive amounts
of training samples for generalization.
especially true in the case of convolutional neural networks (CNNs), where pooling operations
wrongly discard positional information and do not
consider hierarchical relationships between local
features .
Figure 2: Outputs attend to a) active neurons found by
pooling operations b) all neurons c) relevant capsules
found in routing processes.
Capsule networks, instead, have the potential for learning hierarchical relationships between consecutive layers by using routing processes without parameters, which are clusteringlike methods and additionally
improve the generalization capability. We contrast
such routing processes with pooling and fully connected layers in Figure 2.
tasks , a few important obstacles still hinder the
development of capsule networks for mature NLP
applications.
For example, selecting the number of iterations
is crucial for routing processes, because they iteratively route low-level capsules to high-level capsules in order to learn hierarchical relationships
between layers. However, existing routing algorithms use the same number of iterations for all
examples, which is not reliable to judge the convergence of routing. As shown in Figure 3, a routing process with ﬁve iterations on all examples
converges to a lower training loss at system level,
but on instance level for one example, convergence
has still not obtained.
Additionally, training capsule networks is more
difﬁcult than traditional neural networks like CNN
and long short-term memory (LSTM) due to the
large number of capsules and potentially large
output spaces, which requires extensive computational resources in the routing process.
In this work, we address these issues via the following contributions:
• We formulate routing processes as a proxy
problem minimizing a total negative agreement
score in order to evaluate how routing processes
perform at instance level, which will be discussed more in depth later.
• We introduce an adaptive optimizer to selfadjust the number of iterations for each example
in order to improve instance-level convergence
and enhance the reliability of routing processes.
• We present capsule compression and partial
routing to achieve better scalability of capsule
networks on datasets with large output spaces.
• Our framework outperforms strong baselines on
multi-label text classiﬁcation and question answering. We also demonstrate its superior generalization capability in low-resource settings.
Training Step
Training Loss
5 iteration
3 iteration
1 iteration
Number of Iterations
Negative Agreement Score
Figure 3: left) System-level routing evaluation on all
examples; right) Instance-level routing evaluation on
one example.
NLP-Capsule Framework
We have motivated the need for better capsule networks being capable of scaling to large output
spaces and higher reliability for routing processes
at instance level.
We now build a uniﬁed capsule framework, which we call NLP-Capsule. It
is shown in Figure 4 and described below.
Convolutional Layer
We use a convolutional operation to extract features from documents by taking a sliding window
over document embeddings.
∈Rl×v be a matrix of stacked vdimensional word embeddings for an input document with l tokens. Furthermore, let W a ∈Rl×k
be a convolutional ﬁlter with a width k. We apply this ﬁlter to a local region X⊺
i:i+k−1 ∈Rk×l to
generate one feature:
mi = f(W a ◦X⊺
where ◦denotes element-wise multiplication, and
f is a nonlinear activation function (i.e., ReLU).
For ease of exposition, we omit all bias terms.
Then, we can collect all mi into one feature map
(m1, . . . , m(v−k+1)/2) after sliding the ﬁlter over
the current document. To increase the diversity of
features extraction, we concatenate multiple feature maps extracted by three ﬁlters with different
window sizes (2,4,8) and pass them to the primary
capsule layer.
Primary Capsule Layer
In this layer, we use a group-convolution operation to transform feature maps into primary capsules. As opposed to using a scalar for each element in the feature maps, capsules use a group of
neurons to represent each element in the current
layer, which has the potential for preserving more
information.
= d-dimension
Conv Layer
Compression
PrimCap Layer
Representation Layer
Aggregation Layer
Figure 4: An illustration of NLP-Capsule framework.
Using 1×1 ﬁlters W b = {w1, ..., wd} ∈Rd, in
total d groups are used to transform each scalar mi
in feature maps to one capsule pi, a d- dimensional
vector, denoted as:
pi = g(pi1 ⊕pi2 ⊕· · · ⊕pid) ∈Rd
where pij = mi · wj ∈R and ⊕is the concatenation operator. Furthermore, g is a non-linear function (i.e., squashing function). The length ||pi|| of
each capsule pi indicates the probability of it being useful for the task at hand. Hence, a capsule’s
length has to be constrained into the unit interval
 by the squashing function g:
1 + ||x||2
Capsule Compression
One major issue in this
layer is that the number of primary capsules becomes large in proportion to the size of the input documents, which requires extensive computational resources in routing processes (see Section 2.3). To mitigate this issue, we condense the
large number of primary capsules into a smaller
amount. In this way, we can merge similar capsules and remove outliers. Each condensed capsule ui is calculated by using a weighted sum over
all primary capsules, denoted as:
where the parameter bj is learned by supervision.
Aggregation Layer
Pooling is the simplest aggregation function routing condensed capsules into the subsequent layer,
but it loses almost all information during aggregation.
Alternatively, routing processes are introduced to iteratively route condensed capsules
into the next layer for learning hierarchical relationships between two consecutive layers. We
now describe this iterative routing algorithm. Let
{u1, . . . , ˆum} and {v1, . . . , vn} be a set of condensed capsules in layer ℓand a set of high-level
capsules in layer ℓ+1, respectively. The basic idea
of routing is two-fold.
First, we transform the condensed capsules into
a collection of candidates
ˆuj|1, . . . , ˆuj|m
the j-th high-level capsule in layer ℓ+ 1. Following Sabour et al. , each element ˆuj|i is
calculated by:
ˆuj|i = W cui ∈Rd
where W c is a linear transformation matrix.
Then, we represent a high-level capsule vj by a
weighted sum over those candidates, denoted as:
where cij is a coupling coefﬁcient iteratively updated by a clustering-like method.
Our Routing
As discussed earlier, routing algorithms like dynamic routing 
and EM routing , which use
the same number of iterations for all samples, perform well according to training loss at system
level, but on instance level for individual examples, convergence has still not been reached. This
increases the risk of unreliability for routing processes (see Figure 3).
To evaluate the performance of routing processes at instance level, we formulate them as a
proxy problem minimizing the negative agreement
score (NAS) function:
c,v f(u) = −
cij⟨vj, uj|i⟩
∀i, j : cij > 0,
The basic intuition behind this is to assign higher weights cij to one agreeable pair
⟨vj, uj|i⟩if the capsule vj and uj|i are close to
each other such that the total agreement score
i,j cij⟨vj, uj|i⟩is maximized.
However, the
choice of NAS functions remains an open problem.
Hinton et al. hypothesize that the
agreeable pairs in NAS functions are from Gaussian distributions. Instead, we study NAS functions by introducing Kernel Density Estimation
(KDE) since this yields a non-parametric density
estimator requiring no assumptions that the agreeable pairs are drawn from parametric distributions.
Here, we formulate the NAS function in a KDE
c,v f(u) = −
cijk(d(vj, uj|i))
where d is a distance metric with ℓ2 norm, and k is
a Epanechnikov kernel function with:
The solution we used for KDE is taking Mean
Shift to minimize the
NAS function f(u):
cijk′(d(vj, uj|i))∂d(vj, uj|i)
First, vτ+1
can be updated while cτ+1
j , ˆuj|i))uj|i
i,j k′(d(vτ
j , uj|i))
Then, cτ+1
can be updated using standard gradient
ij + α · k(d(vτ
j , uj|i))
where α is the hyper-parameter to control step
To address the issue of convergence not being
reached at instance level, we present an adaptive
optimizer to self-adjust the number of iterations
for individual examples according to their negative agreement scores (see Algorithm 1).
Following Zhao et al. , we replace standard
softmax with leaky-softmax, which decreases the
strength of noisy capsules.
Algorithm 1 Our Adaptive KDE Routing
1: procedure ROUTING(uj|i, ℓ)
2: Initialize ∀i, j : cij = 1/nℓ+1
3: while true do
foreach capsule i, j in layer ℓ, ℓ+ 1 do
cij ←leaky-softmax(cij)
foreach capsule j in layer ℓ+ 1 do
i cijk′(d(vj,uj|i))ˆuj|i
i=1 k′(d(vi,uj|i))
foreach capsule i, j in layer ℓ, ℓ+ 1 do
cij ←cij + α · k(d(vj, uj|i))
foreach capsule j in layer ℓ+ 1 do
NAS = log(P
i,j cijk(d(vj, uj|i)))
if |NAS −Last NAS| < ϵ then
Last NAS ←NAS
17: return vj, aj
Representation Layer
This is the top-level layer containing ﬁnal capsules calculated by iteratively minimizing the NAS
function (See Eq. 1), where the number of ﬁnal
capsules corresponds to the entire output space.
Therefore, as long as the size of an output space
goes to a large scale (thousands of labels), the
computation of this function would become extremely expensive, which yields the bottleneck of
scalability of capsule networks.
Partial Routing
As opposed to the entire output space on data sets, the sub-output space corresponding to individual examples is rather small,
i.e., only few labels are assigned to one document
in text classiﬁcation, for example. As a consequence, it is redundant to route low-level capsules
to the entire output space for each example in the
training stage, which motivated us to present a
partial routing algorithm with constrained output
spaces, such that our NAS function is described
cij⟨vj, uj|i⟩
cik⟨vk, uk|i⟩)
where D+ and D−denote the sets of real (positive) and randomly selected (negative) outputs
for each example, respectively.
Both sets are
far smaller than the entire output space.
the hyper-parameter to control aggregation scores
from negative outputs.
Experiments
The major focus of this work is to investigate the
scalability of our approach on datasets with a large
output space, and generalizability in low-resource
settings with few training examples. Therefore,
we validate our capsule-based approach on two
speciﬁc NLP tasks: (i) multi-label text classiﬁcation with a large label scale; (ii) question answering with a data imbalance issue.
Multi-label Text Classiﬁcation
Multi-label text classiﬁcation task refers to assigning multiple relevant labels to each input document, while the entire label set might be extremely
large. We use our approach to encode an input
document and generate the ﬁnal capsules corresponding to the number of labels in the representation layer. The length of ﬁnal capsule for each
label indicates the probability whether the document has this label.
#Train/Test/Labels
23.1K/781.2K/103
15.4K/3.8K/3.9K
Table 1: Characteristics of the datasets. Each label of
RCV1 has about 729.67 training examples, while each
label of EUR-Lex has merely about 15.59 examples.
Experimental Setup
We conduct our experiments on two datasets selected from the extreme
classiﬁcation repository:2
a regular label scale
dataset (RCV1), with 103 labels , and a large label scale dataset (EUR-Lex),
with 3,956 labels ,
described in Table 1.
The intuition behind our
datasets selection is that EUR-Lex, with 3,956 labels and 15.59 examples per label, ﬁts well with
our goal of investigating the scalability and generalizability of our approach. We contrast EUR-Lex
with RCV1, a dataset with a regular label scale,
and leave the study of datasets with extremely
large labels, e.g., Wikipedia-500K with 501,069
labels, to future work.
We compare our approach to the following baselines: non-deep learning approaches
2 
using TF-IDF features of documents as inputs:
FastXML , and PD-
Sparse , deep learning approaches using raw text of documents as inputs:
FastText , Bow-CNN , CNN-Kim , XML-
CNN ), and a capsule-based approach Cap-Zhao . For evaluation, we use standard rank-based measures such as Precision@k, and Normalized
Discounted Cumulative Gain (NDCG@k).
Implementation Details
The word embeddings
are initialized as 300-dimensional GloVe vectors .
In the convolutional layer, we use a convolution operation with
three different window sizes (2,4,8) to extract features from input documents. Each feature is transformed into a primary capsule with 16 dimensions
by a group-convolution operation. All capsules in
the primary capsule layer are condensed into 256
capsules for RCV1 and 128 capsules for EUR-Lex
by a capsule compression operation.
To avoid routing low-level capsules to the entire
label space in the inference stage, we use a CNN
baseline trained on the same dataset
with our approach, to generate 200 candidate labels and take these labels as a constrained output
space for each example.
Experimental Results
In Table 2, we can see a
noticeable margin brought by our capsule-based
approach over the strong baselines on EUR-Lex,
and competitive results on RCV1. These results
appear to indicate that our approach has superior
generalization ability on datasets with fewer training examples, i.e., RCV1 has 729.67 examples per
label while EUR-Lex has 15.59 examples.
In contrast to the strongest baseline XML-CNN
with 22.52M parameters and 0.08 seconds per
batch, our approach has 14.06M parameters, and
takes 0.25 seconds in an acceleration setting with
capsule compression and partial routing, and 1.7
seconds without acceleration. This demonstrates
that our approach provides competitive computational speed with fewer parameters compared to
the competitors.
Discussion on Generalization
To further study
the generalization capability of our approach, we
vary the percentage of training examples from
100% to 50% on the entire training set, leading
to the number of training examples per label de-
Table 2: Comparisons of our NLP-Cap approach and baselines on two text classication benchmarks, where ’-’
denotes methods that failed to scale due to memory issues.
Figure 5: Performance on EUR-Lex by varying the percentage of training examples (X-axis).
100% examples
NLP-Capsule
50% examples
60% examples
70% examples
80% examples
90% examples
100% examples
100% examples
NLP-Capsule
50% examples
60% examples
70% examples
80% examples
90% examples
100% examples
Table 3: Experimental results on different fractions of
training examples from 50% to 100% on EUR-Lex.
creasing from 15.59 to 7.77. Figure 5 shows that
our approach outperforms the strongest baseline
XML-CNN with different fractions of the training
This ﬁnding agrees with our speculation on generalization: the distance between our approach
and XML-CNN increases as fewer training data
samples are available. In Table 3, we also ﬁnd
that our approach with 70% of training examples
achieves about 5% improvement over XML-CNN
with 100% of examples on 4 out of 6 metrics.
Routing Comparison
We compare our routing
with and on EUR-Lex dataset and observe that it
performs best on all metrics (Table 4). We speculate that the improvement comes from enhanced
reliability of routing processes at instance level.
Question Answering
Question-Answering (QA) selection task refers to
selecting the best answer from candidates to each
question. For a question-answer pair (q, a), we use
our capsule-based approach to generate two ﬁnal
capsules vq and va corresponding to the respective question and answer. The relevance score of
question-answer pair can be deﬁned as their cosine
similarity:
s(q, a) = cos(vq, va) =
||vq|| · ||va||
Experiment Setup
In Table 5, we conduct our
experiments on the TREC QA dataset collected
from TREC QA track 8-13 data . The intuition behind this dataset selection
is that the cost of hiring human annotators to collect positive answers for individual questions can
be prohibitive since positive answers can be conveyed in multiple different surface forms. Such issue arises particularly in TREC QA with only 12%
NLP-Capsule + Sabour‘s Routing
NLP-Capsule + Zhang‘s Routing
NLP-Capsule + Our Routing
NLP-Capsule + Sabour‘s Routing
NLP-Capsule + Zhang‘s Routing
NLP-Capsule + Our Routing
Table 4: Performance on EUR-Lex dataset with different routing process.
#Questions
Train/Dev/Test
1229/82/100
53417/1148/1517
Table 5: Characteristic of TREC QA dataset. %Positive denotes the percentage of positive answers.
positive answers. Therefore, we use this dataset to
investigate the generalizability of our approach.
We compare our approach to the following baselines: CNN + LR 
using unigrams and bigrams, CNN using additional bilinear similarity features, CNTN using
neural tensor network, LSTM using single and multi-layer, MV-LSTM , NTN-LSTM and HD-LSTM using holographic dual LSTM and Capsule-
Zhao using capsule networks.
For evaluation, we use standard measures such as Mean Average Precision
(MAP) and Mean Reciprocal Rank (MRR).
Implementation Details
The word embeddings
used for question answering pairs are initialized
as 300-dimensional GloVe vectors.
In the convolutional layer, we use a convolution operation
with three different window sizes (3,4,5). All 16dimensional capsules in the primary capsule layer
are condensed into 256 capsules by the capsule
compression operation.
Experimental Results and Discussions
In Table 6, the best performance on MAP metric is
achieved by our approach, which veriﬁes the effectiveness of our model. We also observe that
our approach exceeds traditional neural models
like CNN, LSTM and NTN-LSTM by a noticeable
This ﬁnding also agrees with the observation
CNN + LR (unigram)
CNN + LR (bigram)
LSTM (1 layer)
Capsule-Zhao
NLP-Capsule
Table 6: Experimental results on TREC QA dataset.
we found in multi-label classiﬁcation: our approach has superior generalization capability in
low-resource setting with few training examples.
In contrast to the strongest baseline HD-LSTM
with 34.51M and 0.03 seconds for one batch, our
approach has 17.84M parameters and takes 0.06
seconds in an acceleration setting, and 0.12 seconds without acceleration.
Related Work
Multi-label Text Classiﬁcation
Multi-label text classiﬁcation aims at assigning a
document to a subset of labels whose label set
might be extremely large. With increasing numbers of labels, issues of data sparsity and scalability arise. Several methods have been proposed for
the large multi-label classiﬁcation case.
Tree-based models induce a tree structure that recursively partitions the feature space with nonleaf nodes.
Then, the restricted label spaces at
leaf nodes are used for classiﬁcation. Such a solution entails higher robustness because of a dynamic hyper-plane design and its computational
efﬁciency. FastXML 
is one such tree-based model, which learns a hierarchy of training instances and optimizes an
NDCG-based objective function for nodes in the
tree structure.
Label embedding models 
address the data sparsity issue with two steps:
compression and decompression. The compression step learns a low-dimensional label embedding that is projected from original and highdimensional label space.
When data instances
are classiﬁed to these label embeddings, they will
be projected back to the high-dimensional label
space, which is the decompression step.
Recent works came up with different compression
or decompression techniques, e.g., SLEEC .
Deep learning models: FastText uses averaged word embeddings to classify documents, which is computationally efﬁcient but ignores word order. Various CNNs inspired by Kim explored MTC with dynamic pooling, such as Bow-CNN and XML-CNN .
Linear classiﬁers: PD-Sparse 
introduces a Fully-Corrective Block-Coordinate
Frank-Wolfe algorithm to address data sparsity.
Question and Answering
State-of-the-art approaches to QA fall into two
categories: IR-based and knowledge-based QA.
IR-based QA ﬁrstly preprocesses the question
and employ information retrieval techniques to
retrieve a list of relevant passages to questions.
reading comprehension techniques are
adopted to extract answers within the span of retrieved text. For answer extraction, early methods
manually designed patterns to get them (Pasca). A
recent popular trend is neural answer extraction.
Various neural network models are employed to
represent questions .
Since the attention
mechanism naturally explores relevancy, it has
been widely used in QA models to relate the question to candidate answers . Moreover, some
researchers leveraged external large-scale knowledge bases to assist answer selection .
Recent developments focused on modeling the interaction between question and answer pairs: Tensor layers 
and holographic composition 
have pushed the state-of-the-art.
Capsule Networks
Capsule networks were initially proposed by Hinton to improve representations learned by neural networks against vanilla
Subsequently, Sabour et al. replaced the scalar-output feature detectors of CNNs
with vector-output capsules and max-pooling with
routing-by-agreement.
Hinton et al. then proposed a new iterative routing procedure between capsule layers
based on the EM algorithm, which achieves better accuracy on the smallNORB dataset. Zhang
et al. applied capsule networks to relation
extraction in a multi-instance multi-label learning
framework. Xiao et al. explored capsule
networks for multi-task learning.
Xia et al. studied the zero-shot intent
detection problem with capsule networks, which
aims to detect emerging user intents in an unsupervised manner. Zhao et al. investigated
capsule networks with dynamic routing for text
classiﬁcation, and transferred knowledge from the
single-label to multi-label cases. Cho et al. 
studied capsule networks with determinantal point
processes for extractive multi-document summarization.
Our work is different from our predecessors in
the following aspects: (i) we evaluate the performance of routing processes at instance level, and
introduce an adaptive optimizer to enhance the reliability of routing processes; (ii) we present capsule compression and partial routing to achieve
better scalability of capsule networks on datasets
with a large output space.
Conclusion
Making computers perform more like humans is
a major issue in NLP and machine learning. This
not only includes making them perform on similar
levels , but also requests them
to be robust to adversarial examples and generalize from few data points . In this work, we have addressed the
latter issue.
In particular, we extended existing capsule networks into a new framework with advantages concerning scalability, reliability and generalizability.
Our experimental results have demonstrated its effectiveness on two NLP tasks: multi-label text
classiﬁcation and question answering.
Through our modiﬁcations and enhancements,
we hope to have made capsule networks more suitable to large-scale problems and, hence, more mature for real-world applications. In the future, we
plan to apply capsule networks to even more challenging NLP problems such as language modeling
and text generation.
Acknowledgments
We thank the anonymous reviewers for their comments, which greatly improved the ﬁnal version of
the paper. This work has been supported by the
German Research Foundation as part of the Research Training Group Adaptive Preparation of In-
formation from Heterogeneous Sources (AIPHES)
at the Technische Universit¨at Darmstadt under
grant No. GRK 1994/1.