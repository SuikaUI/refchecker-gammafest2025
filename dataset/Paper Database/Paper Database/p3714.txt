Topology Attack and Defense for Graph Neural Networks:
An Optimization Perspective
Kaidi Xu1∗, Hongge Chen2∗, Sijia Liu3 , Pin-Yu Chen3 , Tsui-Wei Weng2 ,
Mingyi Hong4 and Xue Lin1
1Electrical & Computer Engineering, Northeastern University, Boston, USA
2Electrical Engineering & Computer Science, Massachusetts Institute of Technology, Cambridge, USA
3MIT-IBM Watson AI Lab, IBM Research
4Electrical & Computer Engineering, University of Minnesota, Minneapolis, USA
 , , , ,
 , , 
Graph neural networks (GNNs) which apply the
deep neural networks to graph data have achieved
signiﬁcant performance for the task of semisupervised node classiﬁcation.
However, only
few work has addressed the adversarial robustness of GNNs.
In this paper, we ﬁrst present
a novel gradient-based attack method that facilitates the difﬁculty of tackling discrete graph data.
When comparing to current adversarial attacks on
GNNs, the results show that by only perturbing a
small number of edge perturbations, including addition and deletion, our optimization-based attack
can lead to a noticeable decrease in classiﬁcation
performance. Moreover, leveraging our gradientbased attack, we propose the ﬁrst optimizationbased adversarial training for GNNs. Our method
yields higher robustness against both different gradient based and greedy attack methods without sacriﬁcing classiﬁcation accuracy on original graph.
Introduction
Graph structured data plays a crucial role in many AI applications. It is an important and versatile representation to
model a wide variety of datasets from many domains, such
as molecules, social networks, or interlinked documents with
citations.
Graph neural networks (GNNs) on graph structured data have shown outstanding results in various applications [Kipf and Welling, 2016; Veliˇckovi´c et al., 2017;
Xu et al., 2019a]. However, despite the great success on inferring from graph data, the inherent challenge of lacking adversarial robustness in deep learning models still carries over
to security-related domains such as blockchain or communication networks.
In this paper, we aim to evaluate the robustness of GNNs
from a perspective of ﬁrst-order optimization adversarial attacks. It is worth mentioning that ﬁrst-order methods have
achieved great success for generating adversarial attacks on
∗Equal contribution
audios or images [Carlini and Wagner, 2018; Xu et al., 2019b;
Chen et al., 2018b; Xu et al., 2019c; Chen et al., 2018a].
However, some recent works [Dai et al., 2018; Bojcheski and
G¨unnemann, 2018] suggested that conventional (ﬁrst-order)
continuous optimization methods do not directly apply to attacks using edge manipulations (we call topology attack) due
to the discrete nature of graphs. We close this gap by studying
the problem of generating topology attacks via convex relaxation so that gradient-based adversarial attacks become plausible for GNNs. Benchmarking on node classiﬁcation tasks
using GNNs, our gradient-based topology attacks outperform
current state-of-the-art attacks subject to the same topology
perturbation budget. This demonstrates the effectiveness of
our attack generation method through the lens of convex relaxation and ﬁrst-order optimization. Moreover, by leveraging our proposed gradient-based attack, we propose the ﬁrst
optimization-based adversarial training technique for GNNs,
yielding signiﬁcantly improved robustness against gradientbased and greedy topology attacks.
Our new attack generation and adversarial training methods for GNNs are built upon the theoretical foundation of
spectral graph theory, ﬁrst-order optimization, and robust
(mini-max) optimization. We summarize our main contributions as follows:
• We propose a general ﬁrst-order attack generation
framework under two attacking scenarios: a) attacking
a pre-deﬁned GNN and b) attacking a re-trainable GNN.
This yields two new topology attacks: projected gradient descent (PGD) topology attack and min-max topology attack. Experimental results show that the proposed
attacks outperform current state-of-the-art attacks.
• With the aid of our ﬁrst-order attack generation methods,
we propose an adversarial training method for GNNs
to improve their robustness. The effectiveness of our
method is shown by the considerable improvement of robustness on GNNs against both optimization-based and
greedy-search-based topology attacks.
Related Works
Some recent attentions have been paid to the robustness of
graph neural network. Both [Z¨ugner et al., 2018] and [Dai et
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
al., 2018] studied adversarial attacks on neural networks for
graph data. [Dai et al., 2018] studied test-time non-targeted
adversarial attacks on both graph classiﬁcation and node classiﬁcation. Their work restricted the attacks to perform modi-
ﬁcations on discrete structures, that is, an attacker is only allowed to add or delete edges from a graph to construct a new
graph. White-box, practical black-box and restricted blackbox graph adversarial attack scenarios were studied. Authors
in [Z¨ugner et al., 2018] considered both test-time (evasion)
and training-time (data poisoning) attacks on node classiﬁcation task. In contrast to [Dai et al., 2018], besides adding
or removing edges in the graph, attackers in [Z¨ugner et al.,
2018] may modify node attributes. They designed adversarial attacks based on a static surrogate model and evaluated
their impact by training a classiﬁer on the data modiﬁed by
the attack. The resulting attack algorithm is for targeted attacks on single nodes. It was shown that small perturbations
on the graph structure and node features are able to achieve
misclassiﬁcation of a target node. A data poisoning attack
on unsupervised node representation learning, or node embeddings, has been proposed in [Bojcheski and G¨unnemann,
2018]. This attack is based on perturbation theory to maximize the loss obtained from DeepWalk [Perozzi et al., 2014].
In [Z¨ugner and G¨unnemann, 2019], training-time attacks on
GNNs were also investigated for node classiﬁcation by perturbing the graph structure. The authors solved a min-max
problem in training-time attacks using meta-gradients and
treated the graph topology as a hyper-parameter to optimize.
Problem Statement
We begin by providing preliminaries on GNNs. We then formalize the attack threat model of GNNs in terms of edge perturbations, which we refer as ‘topology attack’.
Preliminaries on GNNs
It has been recently shown in [Kipf and Welling, 2016;
Veliˇckovi´c et al., 2017; Xu et al., 2019a] that GNN is powerful in transductive learning, e.g., node classiﬁcation under
graph data. That is, given a single network topology with
node features and a known subset of node labels, GNNs are
efﬁcient to infer the classes of unlabeled nodes.
deﬁning GNN, we ﬁrst introduce the following graph notations. Let G = (V, E) denote an undirected and unweighted
graph, where V is the vertex (or node) set with cardinality
|V| = N, and E ∈(V × V) denotes the edge set with cardinality |E| = M. Let A represent a binary adjancency matrix.
By deﬁnition, we have Aij = 0 if (i, j) /∈E. In a GNN, we
assume that each node i is associated with a feature vector
xi ∈RM0 and a scalar label yi. The goal of GNN is to predict the class of an unlabeled node under the graph topology
A and the training data {(xi, yi)}Ntrain
i=1 . Here GNN uses input
features of all nodes but only Ntrain < N nodes with labeled
classes in the training phase.
Formally, the kth layer of a GNN model obeys the propagation rule of the generic form
{W(k−1)h(k−1)
˜Aij, ∀j ∈N(i)}
, ∀i ∈[N] (1)
where h(k)
∈RMk denotes the feature vector of node i at
layer k, h(0)
= xi ∈RM0 is the input feature vector of node
i, g(k) is a possible composite mapping (activation) function,
W(k−1) ∈RMk×Mk−1 is the trainable weight matrix at layer
(k −1), ˜Aij is the (i, j)th entry of ˜A that denotes a linear
mapping of A but with the same sparsity pattern, and N(i)
denotes node i’s neighbors together with itself, i.e., N(i) =
{j|(i, j) ∈E, or j = i}.
A special form of GNN is graph convolutional networks
(GCN) [Kipf and Welling, 2016]. This is a recent approach
of learning on graph structures using convolution operations
which is promising as an embedding methodology. In GCNs,
the propagation rule (1) becomes [Kipf and Welling, 2016]
W(k−1)h(k−1)
where σ(·) is the ReLU function. Let ˜Ai,: denote the ith row
of ˜A and H(k) =
1 )⊤; . . . ; (h(k)
, we then have the
standard form of GCN,
˜AH(k−1)(W(k−1))⊤
Here ˜A is given by a normalized adjacency matrix ˜A =
ˆD−1/2 ˆA ˆD−1/2, where ˆA = A + I, and ˆDij = 0 if i ̸= j
and ˆDii = 1⊤ˆA:,i.
Topology Attack in Terms of Edge
Perturbation
We introduce a Boolean symmetric matrix S ∈{0, 1}N×N to
encode whether or not an edge in G is modiﬁed. That is, the
edge connecting nodes i and j is modiﬁed (added or removed)
if and only if Sij = Sji = 1. Otherwise, Sij = 0 if i = j or
the edge (i, j) is not perturbed. Given the adjacency matrix
A, its supplement is given by ¯A = 11T −I −A, where I
is an identity matrix, and (11T −I) corresponds to the fullyconnected graph. With the aid of edge perturbation matrix S
and ¯A, a perturbed graph topology A′ against A is given by
A′ = A + C ◦S, C = ¯A −A,
where ◦denotes the element-wise product. In (4), the positive entry of C denotes the edge that can be added to the
graph A, and the negative entry of C denotes the edge that
can be removed from A. We then formalize the concept of
topology attack to GNNs: Finding minimum edge perturbations encoded by S in (4) to mislead GNNs. A more detailed
attack formulation will be studied in the next section.
Topology Attack Generation: A First-Order
Optimization Perspective
In this section, we ﬁrst deﬁne attack loss (beyond the conventional cross-entropy loss) under different attacking scenarios.
We then develop two efﬁcient attack generation methods by
leveraging ﬁrst-order optimization. We call the resulting attacks projected gradient descent (PGD) topology attack and
min-max topology attack, respectively.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
Attack Loss & Attack Generation
Let Z(S, W; A, {xi}) denote the prediction probability of a
GNN speciﬁed by A′ in (4) and W under input features {xi}.
Then Zi,c denotes the probability of assigning node i to class
c. It has been shown in existing works [Goodfellow et al.,
2015; Kurakin et al., 2017] that the negative cross-entropy
(CE) loss between the true labels (yi) and the predicted labels
({Zi,c}) can be used as an attack loss at node i, denoted by
fi(S, W; A, {xi}, yi). We can also propose a CW-type loss
similar to Carlili-Wagner (CW) attacks for attacking image
classiﬁers [Carlini and Wagner, 2017],
fi(S, W; A, {xi}, yi) = max
Zi,yi −max
c̸=yi Zi,c, −κ
where κ ≥0 is a conﬁdence level of making wrong decisions.
To design topology attack, we seek S in (4) to minimize
the per-node attack loss (CE-type or CW-type) given a ﬁnite
budge of edge perturbations. We consider two threat models:
a) attacking a pre-deﬁned GNN with known W; b) attacking
an interactive GNN with re-trainable W. In the case a) of
ﬁxed W, the attack generation problem can be cast as
i∈V fi(s; W, A, {xi}, yi)
subject to
1⊤s ≤ϵ, s ∈{0, 1}n,
where we replace the symmetric matrix variable S with its
vector form that consists of n := N(N −1)/2 unique perturbation variables in S. We recall that fi could be either a
CE-type or a CW-type per-node attack loss. In the case b) of
re-trainable W, the attack generation problem has the following min-max form
1⊤s≤ϵ,s∈{0,1}n maximize
fi(s, W; A, {xi}, yi),
where the inner maximization aims to constrain the attack
loss by retraining W so that attacking GNN is more difﬁcult.
Motivated by targeted adversarial attacks against image
classiﬁers [Carlini and Wagner, 2017], we can deﬁne targeted
topology attacks that are restricted to perturb edges of targeted nodes. In this case, we require to linearly constrain S
in (4) as Si,· = 0 if i is not a target node. As a result, both
attack formulations (6) and (7) have extra linear constraints
with respect to s, which can be readily handled by the optimization solver introduced later. Without loss of generality,
we focus on untargeted topology attacks in this paper.
PGD Topology Attack
Problem (6) is a combinatorial optimization problem due to
the presence of Boolean variables. For ease of optimization,
we relax s ∈{0, 1}n to its convex hull s ∈ n and solve
the resulting continuous optimization problem,
i∈V fi(s; W, A, {xi}, yi)
subject to
where S = {s | 1T s ≤ϵ, s ∈ n}. Suppose that the solution of problem (8) is achievable, the remaining question is
how to recover a binary solution from it. Since the variable s
in (8) can be interpreted as a probabilistic vector, a randomization sampling [Liu et al., 2016] is suited for generating a
near-optimal binary topology perturbation; see details in Algorithm 1.
Algorithm 1 Random sampling from probabilistic to binary
topology perturbation
1: Input: probabilistic vector s, K is # of random trials
2: for k = 1, 2, . . . , K do
draw binary vector u(k) following
with probability si
with probability 1 −si
4: end for
5: choose a vector s∗from {u(k)} which yields the smallest
attack loss f(u(k)) under 1T s ≤ϵ.
We solve the continuous optimization problem (8) by projected gradient descent (PGD),
s(t−1) −ηtˆgt
where t denotes the iteration index of PGD, ηt > 0 is the
learning rate at iteration t, ˆgt = ∇f(s(t−1)) denotes the gradient of the attack loss f evaluated at s(t−1), and ΠS(a) :=
arg mins∈S ∥s −a∥2
2 is the projection operator at a over the
constraint set S. In Proposition 1, we show that the projection
operation yields the closed-form solution.
Proposition 1 Given S = {s | 1T s ≤ϵ, s ∈ n}, the
projection operation at the point a with respect to S is
P [a −µ1]
If µ > 0 and
1T P [a −µ1] = ϵ,
If 1T P [a] ≤ϵ,
where P (x) = x if x ∈ , 0 if x < 0, and 1 if x > 1.
Proof: We express the projection problem as
2 + I (s)
subject to
where I (s) = 0 if s ∈ n, and ∞otherwise.
The Lagrangian function of problem (12) is given by
2 + I (s) + µ(1⊤s −ϵ)
2(si −ai)2 + I (si) + µsi
where µ ≥0 is the dual variable. The minimizer to the above
Lagrangian function (with respect to the variable s) is
s = P (a −µ1),
where P is taken elementwise.
Besides the stationary
condition (14), other KKT conditions for solving problem
µ(1⊤s −ϵ) = 0,
If µ > 0, then the solution to problem (12) is given by (14),
where the dual variable µ is determined by (14) and (15)
1T P [a −µ1] = ϵ, and µ > 0.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
If µ = 0, then the solution to problem (12) is given by (14)
s = P (a), and 1⊤s ≤ϵ,
The proof is complete.
In the projection operation (11), one might need to solve
the scalar equation 1T P [a −µ1] = ϵ with respect to
the dual variable µ.
This can be accomplished by applying the bisection method [Boyd and Vandenberghe, 2004;
Liu et al., 2015] over µ ∈[min(a −1), max(a)].
is because 1T P [a −max(a)1] ≤ϵ and 1T P [a −
min(a −1)1] ≥ϵ, where max and min return the largest
and smallest entry of a vector.
We remark that the bisection method converges in the logarithmic rate given by
log2 [(max(a) −min(a −1))/ξ] for the solution of ξ-error
tolerance. We summarize the PGD topology attack in Algorithm 2.
Algorithm 2 PGD topology attack on GNN
1: Input: s(0), ϵ > 0, learning rate ηt, and iterations T
2: for t = 1, 2, . . . , T do
gradient descent: a(t) = s(t−1) −ηt∇f(s(t−1))
call projection operation in (11)
5: end for
6: call Algorithm 1 to return s∗, and the resulting A′ in (4).
Min-max Topology Attack
We next solve the problem of min-max attack generation in
(7). By convex relaxation on the Boolean variables, we obtain
the following continuous optimization problem
fi(s, W; A, {xi}, yi),
where S has been deﬁned in (8). We solve problem (20)
by ﬁrst-order alternating optimization [Lu et al., 2019a;
2019b], where the inner maximization is solved by gradient ascent, and the outer minimization is handled by PGD
same as (10). We summarize the min-max topology attack
in Algorithm 3. We remark that one can perform multiple
maximization steps within each iteration of alternating optimization. This strikes a balance between the computation
efﬁciency and the convergence accuracy [Chen et al., 2017;
Qian et al., 2018].
Robust Training for GNNs
With the aid of ﬁrst-order attack generation methods, we now
introduce our adversarial training for GNNs via robust optimization. Similar formulation is also used in [Madry et al.,
2017]. In adversarial training, we solve a min-max problem
for robust optimization:
where f(x, W) denotes the attack loss speciﬁed in (20). Following the idea of adversarial training for image classiﬁers
in [Madry et al., 2017], we restrict the loss function f as the
CE-type loss. This formulation tries to minimize the training
loss at the presence of topology perturbations.
Algorithm 3 Min-max topology attack to solve (20)
1: Input: given W(0), s(0), learning rates βt and ηt, and
iteration numbers T
2: for t = 1, 2, . . . , T do
inner maximization over W: given s(t−1), obtain
Wt = Wt−1 + βt∇Wf(st−1, Wt−1)
outer minimization over s:
given W(t), running
PGD (10), where ˆgt = ∇sf(st−1, Wt)
5: end for
6: call Algorithm 1 to return s∗, and the resulting A′ in (4).
We note that problems (21) and (7) share a very similar
min-max form, however, they are not equivalent since the loss
f is neither convex with respect to s nor concave with respect
to W, namely, lacking saddle point property [Boyd and Vandenberghe, 2004]. However, there exists connection between
(7) and (21); see Proposition 2.
Proposition 2 Given a general attack loss function f, problem (21) is equivalent to
which further yields (22) ≤(7).
Proof: By introducing epigraph variable p [Boyd and Vandenberghe, 2004], problem (21) can be rewritten as
subject to
−f(s, W) ≤p, ∀s ∈S.
By changing variable q := −p, problem (23) is equivalent to
subject to
f(s, W) ≥q, ∀s ∈S.
By eliminating the epigraph variable q, problem (24) becomes (22).
By max-min inequality [Boyd and Vandenberghe, 2004, Sec. 5.4], we ﬁnally obtain that
f(s, W) ≤minimize
The proof is now complete.
We summarize the robust training algorithm in Algorithm 4
for solving problem (22). Similar to Algorithm 3, one usually
performs multiple inner minimization steps (with respect to s)
within each iteration t to have a solution towards minimizer
during alternating optimization. This improves the stability
of convergence in practice [Qian et al., 2018; Madry et al.,
Experiments
In this section, we present our experimental results for both
topology attack and defense methods on a graph convolutional networks (GCN) [Kipf and Welling, 2016]. We demonstrate the misclassiﬁcation rate and the convergence of the
proposed 4 attack methods: negative cross-entropy loss via
PGD attack (CE-PGD), CW loss via PGD attack (CW-PGD),
negative cross-entropy loss via min-max attack (CE-minmax), CW loss via min-max attack (CW-min-max). We then
show the improved robustness of GCN by leveraging our proposed robust training against topology attacks.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
Algorithm 4 Robust training for solving problem (22)
1: Input: given W(0), s(0), learning rates βt and ηt, and
iteration numbers T
2: for t = 1, 2, . . . , T do
inner minimization over s: given W(t−1), running
PGD (10), where ˆgt = ∇sf(st−1, Wt−1)
outer maximization over W: given s(t), obtain
Wt = Wt−1 + βt∇Wf(st, Wt−1)
5: end for
6: return WT .
Experimental Setup
We evaluate our methods on two well-known datasets: Cora
and Citeseer [Sen et al., 2008]. Both datasets contain unweighted edges which can be generated as symmetric adjacency matrix A and sparse bag-of-words feature vectors
which can be treated the input of GCN. To train the model,
all node feature vectors are fed into GCN but with only
140 and 120 labeled nodes for Cora and Citeseer, respectively. The number of test labeled nodes is 1000 for both
datasets. At each experiment, we repeat 5 times based on
different splits of training/testing nodes and report mean ±
standard deviation of misclassiﬁcation rate (namely, 1 −prediction accuracy) on testing nodes. Source code avaliable at
 
Attack Performance
We compare our four attack methods (CE-PGD, CW-PGD,
CE-min-max, CW-min-max) with DICE (‘delete edges internally, connect externally’) [Waniek et al., 2018], Meta-
Self attack [Z¨ugner and G¨unnemann, 2019] and greedy attack, a variant of Meta-Self attack without weight re-training
for GCN. The greedy attack is considered as a fair comparison with our CE-PGD and CW-PGD attacks, which are generated on a ﬁxed GCN without weight re-training. In minmax attacks (CE-min-max and CW-min-max), we show misclassiﬁcation rates against both natural and retrained models
from Algorithm 3, and compare them with the state-of-theart Meta-Self attack. For a fair comparison, we use the same
performance evaluation criterion in Meta-Self, testing nodes’
predicted labels (not their ground-truth label) by an independent pre-trained model that can be used during the attack. In
the attack problems (6) and (7), unless speciﬁed otherwise the
maximum number of perturbed edges is set to be 5% of the
total number of existing edges in the original graph. In Algorithm 1, we set the iteration number of random sampling as
K = 20 and choose the perturbed topology with the highest
misclassiﬁcation rate which also satisﬁes the edge perturbation constraint.
In Table 1, we present the misclassiﬁcation rate of different
attack methods against both natural and retrained model from
(20). Here we recall that the retrained model arises due to
the scenario of attacking an interactive GCN with re-trainable
weights (Algorithm 3). For comparison, we also show the
misclassiﬁcation rate of a natural model with the true topology (denoted by ‘clean’). As we can see, to attack the natural
model, our proposed attacks achieve better misclassiﬁcation
rate than the existing methods. We also observe that compared to min-max attacks (CE-min-max and CW-min-max),
CE-PGD and CW-PGD yield better attacking performance
since it is easier to attack a pre-deﬁned GCN. To attack the
model that allows retraining, we set 20 steps of inner maximization per iteration of Algorithm 3. The results show that
our proposed min-max attack achieves very competitive performance compared to Meta-Self attack. Note that evaluating
the attack performance on the retrained model obtained from
(20) is not quite fair since the retrained weights could be suboptimal and induce degradation in classiﬁcation.
18.2 ± 0.1
28.9 ± 0.3
18.9 ± 0.2
29.8 ± 0.4
25.2 ± 0.2
34.6 ± 0.3
22.7 ± 0.3
31.2 ± 0.5
28.0 ± 0.1
36.0 ± 0.2
27.8 ± 0.4
37.1 ± 0.5
CE-min-max
26.4 ± 0.1
34.1 ± 0.3
CW-min-max
26.0 ± 0.3
34.7 ± 0.6
29.6 ± 0.4
39.7 ± 0.3
CE-min-max
30.8 ± 0.2
37.5 ± 0.3
CW-min-max
30.5 ± 0.5
39.6 ± 0.4
Table 1: Misclassiﬁcation rates (%) under 5% perturbed edges
In Fig. 1, we present the CE-loss and the CW-loss of the
proposed topology attacks against the number of iterations in
Algorithm 2. Here we choose T = 200 and ηt = 200/
we can see, the method of PGD converges gracefully against
iterations. This veriﬁes the effectiveness of the ﬁrst-order optimization based attack generation method.
CW-type attack loss
CW-type attack loss
Figure 1: CE-PGD and CW-PGD attack losses on Cora and Citeseer
Defense Performance
In what follows, we invoke Algorithm 4 to generate robust
GCN via adversarial training. We set T = 1000, βt = 0.01
and ηt = 200/
t. We run 20 steps for inner minimization.
Inspired by [Madry et al., 2017], we increase the hidden units
from 16 to 32 in order to create more capacity for this more
complicated classiﬁer. Initially, we set the maximum number
of edges we can modify as 5% of total existing edges.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
In Figure 2, we present convergence of our robust training.
As we can see, the loss drops reasonably and the 1, 000 iterations are necessary for robust training rather than normal
training process which only need 200 iterations. We also observe that our robust training algorithm does not harm the test
accuracy when ϵ = 5%, but successfully improves the robustness as the attack success rate drops from 28.0% to 22.0% in
Cora dataset as shown in Table 2,
After showing the effectiveness of our algorithm, we explore deeper in adversarial training on GCN. We aim to show
how large ϵ we can use in robust training. So we set ϵ from
5% to 20% and apply CE-PGD attack following the same ϵ
setting. The results are presented in Table 3. Note that when
ϵ = 0, the ﬁrst row shows misclassiﬁcation rates of test nodes
on natural graph as the baseline for lowest misclassiﬁcation
rate we can obtain; the ﬁrst column shows the CE-PGD attack misclassiﬁcation rates of natural model as the baseline
for highest misclassiﬁcation rate we can obtain. We can conclude that when a robust model trained under an ϵ constraint,
the model will gain robustness under this ϵ distinctly. Considering its importance to keep the original graph test performance, we suggest generating robust model under ϵ = 0.1.
Moreover, please refer to Figure 3 that a) our robust trained
model can provide universal defense to CE-PGD, CW-PGD
and Greedy attacks; b) when increasing ϵ, the difference between both test accuracy and CE-PGD attack accuracy increases substantially, which also implies the robust model under larger ϵ is harder to obtain.
A/natural model
18.2 ± 0.1
28.9 ± 0.1
A/robust model
18.1 ± 0.3
28.7 ± 0.4
A′/natural model
28.0 ± 0.1
36.0 ± 0.2
A′/robust model
22.0 ± 0.2
32.2 ± 0.4
Table 2: Misclassiﬁcation rates (%) of robust training (smaller is
better for defense task) with at most 5% of edge perturbations. A
means the natural graph, A′ means the generated adversarial graph
under ϵ = 5%. X/M means the misclassiﬁcation rate of using
model M on graph X.
ϵ in robust training (in %)
Table 3: Misclassiﬁcation rates (%) of CE-PGD attack against robust training model versus (smaller is better) different ϵ (%) on Cora
dataset. Here ϵ = 0 in training means natural model and ϵ = 0 in
attack means unperturbed topology.
Conclusion
In this paper, we ﬁrst introduce an edge perturbation based
topology attack framework that overcomes the difﬁculty of
CE loss (robust training)
Figure 2: Robust training loss on Cora and Citeseer datasets.
Perturbed edges ratio: ε
test acc (robust model)
CE attack acc (robust model)
CW attack acc (robust model)
Greedy attack acc (robust model)
CE attack acc (natural model)
Figure 3: Test accuracy of robust model (no attack), CE-PGD attack against robust model, CW-PGD attack against robust model,
Greedy attack against robust model and CE-PGD attack against natural model for different ϵ used in robust training and test on Cora
attacking discrete graph structure data from a ﬁrst-order optimization perspective. Our extensive experiments show that
with only a fraction of edges changed, we are able to compromise state-of-the-art graph neural networks model noticeably.
Additionally, we propose an adversarial training framework
to improve the robustness of GNN models based on our attack methods. Experiments on different datasets show that
our method is able to improve the GNN model’s robustness
against both gradient based and greedy search based attack
methods without classiﬁcation performance drop on original
graph. We believe that this paper provides potential means for
theoretical study and improvement of the robustness of deep
learning models on graph data.
Acknowledgments
This work is supported by Air Force Research Laboratory
FA8750-18-2-0058 and the MIT-IBM Watson AI Lab.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)