Published as a conference paper at ICLR 2018
FASTGCN: FAST LEARNING WITH GRAPH CONVOLU-
TIONAL NETWORKS VIA IMPORTANCE SAMPLING
Jie Chen∗, Tengfei Ma∗, Cao Xiao
IBM Research
 , , 
The graph convolutional networks (GCN) recently proposed by Kipf and Welling
are an effective graph model for semi-supervised learning. This model, however,
was originally designed to be learned with the presence of both training and test
data. Moreover, the recursive neighborhood expansion across layers poses time
and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as
integral transforms of embedding functions under probability measures. Such an
interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose
in this work—FastGCN. Enhanced with importance sampling, FastGCN not only
is efﬁcient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN
and related models. In particular, training is orders of magnitude more efﬁcient
while predictions remain comparably accurate.
INTRODUCTION
Graphs are universal representations of pairwise relationship. Many real world data come naturally
in the form of graphs; e.g., social networks, gene expression networks, and knowledge graphs.
To improve the performance of graph-based learning tasks, such as node classiﬁcation and link
prediction, recently much effort is made to extend well-established network architectures, including
recurrent neural networks (RNN) and convolutional neural networks (CNN), to graph data; see, e.g.,
Bruna et al. ; Duvenaud et al. ; Li et al. ; Jain et al. ; Henaff et al. ;
Niepert et al. ; Kipf & Welling .
Whereas learning feature representations for graphs is an important subject among this effort, here,
we focus on the feature representations for graph vertices. In this vein, the closest work that applies
a convolution architecture is the graph convolutional network (GCN) .
Borrowing the concept of a convolution ﬁlter for image pixels or a linear array of signals, GCN uses
the connectivity structure of the graph as the ﬁlter to perform neighborhood mixing. The architecture
may be elegantly summarized by the following expression:
H(l+1) = σ( ˆAH(l)W (l)),
where ˆA is some normalization of the graph adjacency matrix, H(l) contains the embedding (rowwise) of the graph vertices in the lth layer, W (l) is a parameter matrix, and σ is nonlinearity.
As with many graph algorithms, the adjacency matrix encodes the pairwise relationship for both
training and test data. The learning of the model as well as the embedding is performed for both
data simultaneously, at least as the authors proposed. For many applications, however, test data
may not be readily available, because the graph may be constantly expanding with new vertices
(e.g. new members of a social network, new products to a recommender system, and new drugs
for functionality tests). Such scenarios require an inductive scheme that learns a model from only a
training set of vertices and that generalizes well to any augmentation of the graph.
∗These two authors contribute equally.
 
Published as a conference paper at ICLR 2018
A more severe challenge for GCN is that the recursive expansion of neighborhoods across layers
incurs expensive computations in batched training. Particularly for dense graphs and powerlaw
graphs, the expansion of the neighborhood for a single vertex quickly ﬁlls up a large portion of the
graph. Then, a usual mini-batch training will involve a large amount of data for every batch, even
with a small batch size. Hence, scalability is a pressing issue to resolve for GCN to be applicable to
large, dense graphs.
To address both challenges, we propose to view graph convolutions from a different angle and
interpret them as integral transforms of embedding functions under probability measures. Such a
view provides a principled mechanism for inductive learning, starting from the formulation of the
loss to the stochastic version of the gradient. Speciﬁcally, we interpret that graph vertices are iid
samples of some probability distribution and write the loss and each convolution layer as integrals
with respect to vertex embedding functions. Then, the integrals are evaluated through Monte Carlo
approximation that deﬁnes the sample loss and the sample gradient. One may further alter the
sampling distribution (as in importance sampling) to reduce the approximation variance.
The proposed approach, coined FastGCN, not only rids the reliance on the test data but also yields
a controllable cost for per-batch computation. At the time of writing, we notice a newly published
work GraphSAGE that proposes also the use of sampling to reduce the
computational footprint of GCN. Our sampling scheme is more economic, resulting in a substantial
saving in the gradient computation, as will be analyzed in more detail in Section 3.3. Experimental
results in Section 4 indicate that the per-batch computation of FastGCN is more than an order of
magnitude faster than that of GraphSAGE, while classiﬁcation accuracies are highly comparable.
RELATED WORK
Over the past few years, several graph-based convolution network models emerged for addressing applications of graph-structured data, such as the representation of molecules . An important stream of work is built on spectral graph theory . They deﬁne parameterized ﬁlters in the spectral domain, inspired by graph Fourier transform. These approaches learn a feature representation for the whole
graph and may be used for graph classiﬁcation.
Another line of work learns embeddings for graph vertices, for which Goyal & Ferrara is a
recent survey that covers comprehensively several categories of methods. A major category consists
of factorization based algorithms that yield the embedding through matrix factorizations; see, e.g.,
Roweis & Saul ; Belkin & Niyogi ; Ahmed et al. ; Cao et al. ; Ou et al.
 . These methods learn the representations of training and test data jointly. Another category
is random walk based methods that compute node
representations through exploration of neighborhoods. LINE is also such a technique that is motivated by the preservation of the ﬁrst and second-order proximities. Meanwhile,
there appear a few deep neural network architectures, which better capture the nonlinearity within
graphs, such as SDNE . As motivated earlier, GCN is
the model on which our work is based.
The most relevant work to our approach is GraphSAGE , which learns node
representations through aggregation of neighborhood information. One of the proposed aggregators
employs the GCN architecture. The authors also acknowledge the memory bottleneck of GCN and
hence propose an ad hoc sampling scheme to restrict the neighborhood size. Our sampling approach
is based on a different and more principled formulation. The major distinction is that we sample
vertices rather than neighbors. The resulting computational savings are analyzed in Section 3.3.
TRAINING AND INFERENCE THROUGH SAMPLING
One striking difference between GCN and many standard neural network architectures is the lack of
independence in the sample loss. Training algorithms such as SGD and its batch generalization are
designed based on the additive nature of the loss function with respect to independent data samples.
For graphs, on the other hand, each vertex is convolved with all its neighbors and hence deﬁning a
sample gradient that is efﬁcient to compute is beyond straightforward.
Published as a conference paper at ICLR 2018
Concretely, consider the standard SGD scenario where the loss is the expectation of some function
g with respect to a data distribution D:
L = Ex∼D[g(W; x)].
Here, W denotes the model parameter to be optimized. Of course, the data distribution is generally
unknown and one instead minimizes the empirical loss through accessing n iid samples x1, . . . , xn:
xi ∼D, ∀i.
In each step of SGD, the gradient is approximated by ∇g(W; xi), an (assumed) unbiased sample
of ∇L. One may interpret that each gradient step makes progress toward the sample loss g(W; xi).
The sample loss and the sample gradient involve only one single sample xi.
For graphs, one may no longer leverage the independence and compute the sample gradient
∇g(W; xi) by discarding the information of i’s neighboring vertices and their neighbors, recursively. We therefore seek an alternative formulation. In order to cast the learning problem under
the same sampling framework, let us assume that there is a (possibly inﬁnite) graph G′ with the
vertex set V ′ associated with a probability space (V ′, F, P), such that for the given graph G, it is an
induced subgraph of G′ and its vertices are iid samples of V ′ according to the probability measure
P. For the probability space, V ′ serves as the sample space and F may be any event space (e.g., the
power set F = 2V ′). The probability measure P deﬁnes a sampling distribution.
To resolve the problem of lack of independence caused by convolution, we interpret that each layer
of the network deﬁnes an embedding function of the vertices (random variable) that are tied to the
same probability measure but are independent. See Figure 1. Speciﬁcally, recall the architecture of
˜H(l+1) = ˆAH(l)W (l),
H(l+1) = σ( ˜H(l+1)),
l = 0, . . . , M −1,
g(H(M)(i, :)).
For the functional generalization, we write
˜h(l+1)(v) =
ˆA(v, u)h(l)(u)W (l) dP(u),
h(l+1)(v) = σ(˜h(l+1)(v)),
l = 0, . . . , M −1,
L = Ev∼P [g(h(M)(v))] =
g(h(M)(v)) dP(v).
Here, u and v are independent random variables, both of which have the same probability measure
P. The function h(l) is interpreted as the embedding function from the lth layer. The embedding
functions from two consecutive layers are related through convolution, expressed as an integral
transform, where the kernel ˆA(v, u) corresponds to the (v, u) element of the matrix ˆA. The loss is
the expectation of g(h(M)) for the ﬁnal embedding h(M). Note that the integrals are not the usual
Riemann–Stieltjes integrals, because the variables u and v are graph vertices but not real numbers;
however, this distinction is only a matter of formalism.
Writing GCN in the functional form allows for evaluating the integrals in the Monte Carlo manner,
which leads to a batched training algorithm and also to a natural separation of training and test data,
as in inductive learning. For each layer l, we use tl iid samples u(l)
1 , . . . , u(l)
tl ∼P to approximately
evaluate the integral transform (2); that is,
tl+1 (v) := 1
ˆA(v, u(l)
tl+1 (v) := σ(˜h(l+1)
tl+1 (v)),
l = 0, . . . , M −1,
with the convention h(0)
t0 ≡h(0). Then, the loss L in (3) admits an estimator
Lt0,t1,...,tM := 1
The follow result establishes that the estimator is consistent. The proof is a recursive application of
the law of large numbers and the continuous mapping theorem; it is given in the appendix.
Published as a conference paper at ICLR 2018
Graph convolution view
Integral transform view
Figure 1: Two views of GCN. On the left (graph convolution view), each circle represents a graph
vertex. On two consecutive rows, a circle i is connected (in gray line) with circle j if the two corresponding vertices in the graph are connected. A convolution layer uses the graph connectivity
structure to mix the vertex features/embeddings. On the right (integral transform view), the embedding function in the next layer is an integral transform (illustrated by the orange fanout shape) of the
one in the previous layer. For the proposed method, all integrals (including the loss function) are
evaluated by using Monte Carlo sampling. Correspondingly in the graph view, vertices are subsampled in a bootstrapping manner in each layer to approximate the convolution. The sampled portions
are collectively denoted by the solid blue circles and the orange lines.
Theorem 1. If g and σ are continuous, then
t0,t1,...,tM→∞Lt0,t1,...,tM = L
with probability one.
In practical use, we are given a graph whose vertices are already assumed to be samples. Hence, we
will need bootstrapping to obtain a consistent estimate. In particular, for the network architecture (1),
the output H(M) is split into batches as usual. We will still use u(M)
, . . . , u(M)
to denote a batch of
vertices, which come from the given graph. For each batch, we sample (with replacement) uniformly
each layer and obtain samples u(l)
i , i = 1, . . . , tl, l = 0, . . . , M −1. Such a procedure is equivalent
to uniformly sampling the rows of H(l) for each l. Then, we obtain the batch loss
Lbatch = 1
g(H(M)(u(M)
where, recursively,
H(l+1)(v, :) = σ
ˆA(v, u(l)
j )H(l)(u(l)
j , :)W (l)
l = 0, . . . , M −1.
Here, the n inside the activation function σ is the number of vertices in the given graph and is used to
account for the normalization difference between the matrix form (1) and the integral form (2). The
corresponding batch gradient may be straightforwardly obtained through applying the chain rule on
each H(l). See Algorithm 1.
VARIANCE REDUCTION
As for any estimator, one is interested in improving its variance.
Whereas computing the full
variance is highly challenging because of nonlinearity in all the layers, it is possible to consider
each single layer and aim at improving the variance of the embedding function before nonlinearity.
Speciﬁcally, consider for the lth layer, the function ˜h(l+1)
tl+1 (v) as an approximation to the convolution
R ˆA(v, u)h(l)
tl (u)W (l) dP(u). When taking tl+1 samples v = u(l+1)
, . . . , u(l+1)
tl+1 , the sample
average of ˜h(l+1)
tl+1 (v) admits a variance that captures the deviation from the eventual loss contributed
by this layer. Hence, we seek an improvement of this variance. Now that we consider each layer
separately, we will do the following change of notation to keep the expressions less cumbersome:
Published as a conference paper at ICLR 2018
Algorithm 1 FastGCN batched training (one epoch)
1: for each batch do
For each layer l, sample uniformly tl vertices u(l)
1 , . . . , u(l)
for each layer l do
▷Compute batch gradient ∇Lbatch
If v is sampled in the next layer,
∇˜H(l+1)(v, :) ←n
ˆA(v, u(l)
j , :)W (l)o
W ←W −η∇Lbatch
7: end for
Num. samples
Layer l + 1; random variable v
tl+1 (v) →y(v)
Layer l; random variable u
tl (u)W (l) →x(u)
Under the joint distribution of v and u, the aforementioned sample average is
ˆA(vi, uj)x(uj)
First, we have the following result.
Proposition 2. The variance of G admits
Var{G} = R + 1
ˆA(v, u)2x(u)2 dP(u) dP(v),
e(v)2 dP(v) −1
e(v) dP(v)
ˆA(v, u)x(u) dP(u).
The variance (6) consists of two parts. The ﬁrst part R leaves little room for improvement, because
the sampling in the v space is not done in this layer. The second part (the double integral), on
the other hand, depends on how the uj’s in this layer are sampled. The current result (6) is the
consequence of sampling uj’s by using the probability measure P. One may perform importance
sampling, altering the sampling distribution to reduce variance. Speciﬁcally, let Q(u) be the new
probability measure, where the uj’s are drawn from. We hence deﬁne the new sample average
approximation
yQ(v) := 1
ˆA(v, uj)x(uj)
u1, . . . , ut ∼Q,
and the quantity of interest
yQ(vi) = 1
ˆA(vi, uj)x(uj)
Clearly, the expectation of GQ is the same as that of G, regardless of the new measure Q. The
following result gives the optimal Q.
Theorem 3. If
b(u)|x(u)| dP(u)
b(u)|x(u)| dP(u)
ˆA(v, u)2 dP(v)
Published as a conference paper at ICLR 2018
then the variance of GQ admits
Var{GQ} = R + 1
b(u)|x(u)| dP(u)
where R is deﬁned in Proposition 2. The variance is minimum among all choices of Q.
A drawback of deﬁning the sampling distribution Q in this manner is that it involves |x(u)|, which
constantly changes during training. It corresponds to the product of the embedding matrix H(l)
and the parameter matrix W (l). The parameter matrix is updated in every iteration; and the matrix
product is expensive to compute. Hence, the cost of computing the optimal measure Q is quite high.
As a compromise, we consider a different choice of Q, which involves only b(u). The following
proposition gives the precise deﬁnition. The resulting variance may or may not be smaller than (6).
In practice, however, we ﬁnd that it is almost always helpful.
Proposition 4. If
b(u)2 dP(u)
b(u)2 dP(u)
where b(u) is deﬁned in (7), then the variance of GQ admits
Var{GQ} = R + 1
b(u)2 dP(u)
x(u)2 dP(u),
where R is deﬁned in Proposition 2.
With this choice of the probability measure Q, the ratio dQ(u)/dP(u) is proportional to b(u)2,
which is simply the integral of ˆA(v, u)2 with respect to v. In practical use, for the network architecture (1), we deﬁne a probability mass function for all the vertices in the given graph:
q(u) = ∥ˆA(:, u)∥2/
∥ˆA(:, u′)∥2,
and sample t vertices u1, . . . , ut according to this distribution. From the expression of q, we see that
it has no dependency on l; that is, the sampling distribution is the same for all layers. To summarize,
the batch loss Lbatch in (4) now is recursively expanded as
H(l+1)(v, :) = σ
ˆA(v, u(l)
j )H(l)(u(l)
j , :)W (l)
l = 0, . . . , M −1.
The major difference between (5) and (10) is that the former obtains samples uniformly whereas the
latter according to q. Accordingly, the scaling inside the summation changes. The corresponding
batch gradient may be straightforwardly obtained through applying the chain rule on each H(l). See
Algorithm 2.
Algorithm 2 FastGCN batched training (one epoch), improved version
1: For each vertex u, compute sampling probability q(u) ∝∥ˆA(:, u)∥2
2: for each batch do
For each layer l, sample tl vertices u(l)
1 , . . . , u(l)
tl according to distribution q
for each layer l do
▷Compute batch gradient ∇Lbatch
If v is sampled in the next layer,
∇˜H(l+1)(v, :) ←1
ˆA(v, u(l)
j , :)W (l)o
W ←W −η∇Lbatch
8: end for
Published as a conference paper at ICLR 2018
The sampling approach described in the preceding subsection clearly separates out test data from
training. Such an approach is inductive, as opposed to transductive that is common for many graph
algorithms. The essence is to cast the set of graph vertices as iid samples of a probability distribution,
so that the learning algorithm may use the gradient of a consistent estimator of the loss to perform
parameter update. Then, for inference, the embedding of a new vertex may be either computed
by using the full GCN architecture (1), or approximated through sampling as is done in parameter
learning. Generally, using the full architecture is more straightforward and easier to implement.
COMPARISON WITH GRAPHSAGE
GraphSAGE is a newly proposed architecture for generating vertex embeddings through aggregating neighborhood information. It shares the same memory bottleneck with
GCN, caused by recursive neighborhood expansion. To reduce the computational footprint, the authors propose restricting the immediate neighborhood size for each layer. Using our notation for
the sample size, if one samples tl neighbors for each vertex in the lth layer, then the size of the
expanded neighborhood is, in the worst case, the product of the tl’s. On the other hand, FastGCN
samples vertices rather than neighbors in each layer. Then, the total number of involved vertices is
at most the sum of the tl’s, rather than the product. See experimental results in Section 4 for the
order-of-magnitude saving in actual computation time.
EXPERIMENTS
We follow the experiment setup in Kipf & Welling and Hamilton et al. to demonstrate the effective use of FastGCN, comparing with the original GCN model as well as Graph-
SAGE, on the following benchmark tasks: (1) classifying research topics using the Cora citation
data set ; (2) categorizing academic papers with the Pubmed database; and
(3) predicting the community structure of a social network modeled with Reddit posts. These data
sets are downloaded from the accompany websites of the aforementioned references. The graphs
have increasingly more nodes and higher node degrees, representative of the large and dense setting under which our method is motivated. Statistics are summarized in Table 1. We adjusted the
training/validation/test split of Cora and Pubmed to align with the supervised learning scenario.
Speciﬁcally, all labels of the training examples are used for training, as opposed to only a small
portion in the semi-supervised setting . Such a split is coherent with that
of the other data set, Reddit, used in the work of GraphSAGE. Additional experiments using the
original split of Cora and Pubmed are reported in the appendix.
Table 1: Dataset Statistics
Training/Validation/Test
1, 208/500/1, 000
18, 217/500/1, 000
11, 606, 919
152, 410/23, 699/55, 334
Implementation details are as following. All networks (including those under comparison) contain
two layers as usual. The codes of GraphSAGE and GCN are downloaded from the accompany
websites and the latter is adapted for FastGCN. Inference with FastGCN is done with the full GCN
network, as mentioned in Section 3.2. Further details are contained in the appendix.
We ﬁrst consider the use of sampling in FastGCN. The left part of Table 2 (columns under “Sampling”) lists the time and classiﬁcation accuracy as the number of samples increases. For illustration
purpose, we equalize the sample size on both layers. Clearly, with more samples, the per-epoch
training time increases, but the accuracy (as measured by using micro F1 scores) also improves
generally.
An interesting observation is that given input features H(0), the product ˆAH(0) in the bottom layer
does not change, which means that the chained expansion of the gradient with respect to W (0) in
Published as a conference paper at ICLR 2018
Table 2: Beneﬁt of precomputing ˆAH(0) for
the input layer. Data set: Pubmed. Training time is in seconds, per-epoch (batch size
1024). Accuracy is measured by using micro
Precompute
Importance
Importance
Sample size
Importance
Figure 2: Prediction accuracy: uniform versus importance sampling. The three data sets from top to bottom
are ordered the same as Table 1.
the last step is a constant throughout training. Hence, one may precompute the product rather than
sampling this layer to gain efﬁciency. The compared results are listed on the right part of Table 2
(columns under “Precompute”). One sees that the training time substantially decreases while the
accuracy is comparable. Hence, all the experiments that follow use precomputation.
Next, we compare the sampling approaches for FastGCN: uniform and importance sampling. Figure 2 summarizes the prediction accuracy under both approaches. It shows that importance sampling
consistently yields higher accuracy than does uniform sampling. Since the altered sampling distribution (see Proposition 4 and Algorithm 2) is a compromise alternative of the optimal distribution
that is impractical to use, this result suggests that the variance of the used sampling indeed is smaller
than that of uniform sampling; i.e., the term (9) stays closer to (8) than does (6). A possible reason
is that b(u) correlates with |x(u)|. Hence, later experiments will apply importance sampling.
We now demonstrate that the proposed method is signiﬁcantly faster than the original GCN as well
as GraphSAGE, while maintaining comparable prediction performance. See Figure 3. The bar
heights indicate the per-batch training time, in the log scale. One sees that GraphSAGE is a substantial improvement of GCN for large and dense graphs (e.g., Reddit), although for smaller ones
(Cora and Pubmed), GCN trains faster. FastGCN is the fastest, with at least an order of magnitude
improvement compared with the runner up (except for Cora), and approximately two orders of magnitude speed up compared with the slowest. Here, the training time of FastGCN is with respect to
the sample size that achieves the best prediction accuracy. As seen from the table on the right, this
accuracy is highly comparable with the best of the other two methods.
Time (seconds)
Micro F1 Score
GraphSAGE-GCN
GraphSAGE-mean
GCN (batched)
GCN (original)
Figure 3: Per-batch training time in seconds (left) and prediction accuracy (right). For timing,
GraphSAGE refers to GraphSAGE-GCN in Hamilton et al. . The timings of using other aggregators, such as GraphSAGE-mean, are similar. GCN refers to using batched learning, as opposed
to the original version that is nonbatched; for more details of the implementation, see the appendix.
The nonbatched version of GCN runs out of memory on the large graph Reddit. The sample sizes
for FastGCN are 400, 100, and 400, respectively for the three data sets.
Published as a conference paper at ICLR 2018
In the discussion period, the authors of GraphSAGE offered an improved implementation of their
codes and alerted that GraphSAGE was better suited for massive graphs. The reason is that for small
graphs, the sample size (recalling that it is the product across layers) is comparable to the graph size
and hence improvement is marginal; moreover, sampling overhead might then adversely affect the
timing. For fair comparison, the authors of GraphSAGE kept the sampling strategy but improved the
implementation of their original codes by eliminating redundant calculations of the sampled nodes.
Now the per-batch training time of GraphSAGE compares more favorably on the smallest graph
Cora; see Table 3. Note that this implementation does not affect large graphs (e.g., Reddit) and our
observation of orders of magnitude faster training remains valid.
Table 3: Further comparison of per-batch training time (in seconds) with new implementation of
GraphSAGE for small graphs. The new implementation is in PyTorch whereas the rest are in TensorFlow.
GraphSAGE-GCN (old impl)
GraphSAGE-GCN (new impl)
GCN (batched)
CONCLUSIONS
We have presented FastGCN, a fast improvement of the GCN model recently proposed by Kipf &
Welling for learning graph embeddings. It generalizes transductive training to an inductive
manner and also addresses the memory bottleneck issue of GCN caused by recursive expansion of
neighborhoods. The crucial ingredient is a sampling scheme in the reformulation of the loss and the
gradient, well justiﬁed through an alternative view of graph convoluntions in the form of integral
transforms of embedding functions. We have compared the proposed method with additionally
GraphSAGE , a newly published work that also proposes using sampling
to restrict the neighborhood size, although the two sampling schemes substantially differ in both
algorithm and cost. Experimental results indicate that our approach is orders of magnitude faster
than GCN and GraphSAGE, while maintaining highly comparable prediction performance with the
The simplicity of the GCN architecture allows for a natural interpretation of graph convolutions in
terms of integral transforms. Such a view, yet, generalizes to many graph models whose formulations
are based on ﬁrst-order neighborhoods, examples of which include MoNet that applies to (meshed)
manifolds , as well as many message-passing neural networks ; Gilmer et al. ). The proposed work elucidates the basic Monte Carlo ingredients
for consistently estimating the integrals. When generalizing to other networks aforementioned, an
additional effort is to investigate whether and how variance reduction may improve the estimator, a
possibly rewarding avenue of future research.