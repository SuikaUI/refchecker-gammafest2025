Pedestrian Detection with Spatially Pooled
Features and Structured Ensemble Learning
Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel
Abstract—Many typical applications of object detection operate within a prescribed false-positive range. In this situation the
performance of a detector should be assessed on the basis of the area under the ROC curve over that range, rather than over
the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve
(pAUC). We propose a novel ensemble learning method which achieves a maximal detection rate at a user-deﬁned range of false
positive rates by directly optimizing the partial AUC using structured learning. In order to achieve a high object detection performance,
we propose a new approach to extract low-level visual features based on spatial pooling. Incorporating spatial pooling improves the
translational invariance and thus the robustness of the detection process. Experimental results on both synthetic and real-world data
sets demonstrate the effectiveness of our approach, and we show that it is possible to train state-of-the-art pedestrian detectors using
the proposed structured ensemble learning method with spatially pooled features. The result is the current best reported performance
on the Caltech-USA pedestrian detection dataset.
Index Terms—Pedestrian detection, boosting, ensemble learning, spatial pooling, structured learning.
INTRODUCTION
Pedestrian detection has gained a great deal of attention
in the research community over the past decade. It is
one of several fundamental topics in computer vision.
The task of pedestrian detection is to identify visible
pedestrians in a given image using knowledge gained
through analysis of a set of labelled pedestrian and
non-pedestrian exemplars. Signiﬁcant progress has been
made in the last decade in this area due to its practical
use in many computer vision applications such as video
surveillance, robotics and human computer interaction.
The problem is made difﬁcult by the inevitable variation in target appearance, lighting and pose, and by
occlusion. In a recent literature survey on pedestrian
detection, the authors evaluated several pedestrian detectors and concluded that combining multiple features
can signiﬁcantly boost the performance of pedestrian detection . Hand-crafted low-level visual features have
been applied to several computer vision applications and
shown promising results , , , . Inspired by the
recent success of spatial pooling on object recognition
and pedestrian detection problems , , , , we
propose to perform the spatial pooling operation to
create the new feature type for the task of pedestrian
detection.
Once the detector has been trained, the most commonly adopted evaluation method by which to compare the detection performance of different algorithms
is the Receiver Operating Characteristic (ROC) curve.
The curve illustrates the varying performance of a binary classiﬁer system as its discrimination threshold
The authors are with School of Computer Science, The University of Adelaide,
SA 5005, Australia. C. Shen and A. van den Hengel are also with Australian
Research Council Centre of Excellence for Robotic Vision.
Corresponding author: C. Shen (e-mail: ).
is altered. In the face and human detection literature,
researchers are often interested in the low false positive
area of the ROC curve since this region characterizes the
performance needed for most real-world vision applications (see Fig. 1). An algorithm that achieves a high
detection rate with many false positives would be less
preferable than the algorithm that achieves a moderate
detection rate with very few false positives. For human
detection, researchers often report the partial area under
the ROC curve (pAUC), typically over the range 0.01 and
1.0 false positives per image . As the name implies,
pAUC is calculated as the area under the ROC curve
between two speciﬁed false positive rates (FPRs). It
summarizes the practical performance of a detector and
often is the primary performance measure of interest.
Although pAUC is the metric of interest that has been
adopted to evaluate detection performance, many classiﬁers do not directly optimize this evaluation criterion,
and as a result, often under-perform. In this paper, we
present a principled approach for learning an ensemble
classiﬁer which directly optimizes the partial area under
the ROC curve, where the range over which the area
is calculated may be selected according to the desired
application. Built upon the structured learning framework, we thus propose here a novel form of ensemble
classiﬁer which directly optimizes the partial AUC score,
which we call pAUCEnsT. As with all other boosting
algorithms, our approach learns a predictor by building
an ensemble of weak classiﬁcation rules. It also relies on
a sample re-weighting mechanism to pass the information between each iteration. However, unlike traditional
boosting, at each iteration, the proposed approach places
a greater emphasis on samples which have the incorrect
 
Our detector is designed to perform
well in this practical range [0.01-1 FPPI]
Not practical
Not practical
(a) < 0.001 False Positive Per Image (FPPI)
(b) 0.01 – 0.1 FPPI
(c) 1 FPPI
(d) 3 FPPI
Fig. 1: An illustration of pedestrian detection performance at four different points on the ROC curve. (a) Setting the classiﬁcation threshold
value to be very high such that the detector achieves < 0.001 False Positive Per Image (FPPI) is not practical as the detector fails to detect most
pedestrians in the image. (b) The detector that achieves a false positive rate between 0.01 −1 FPPI is more practical in real-world applications
as the detector detects most pedestrians with very few false detections on average. In this example, the detector fails to detect one pedestrian
in the image. (c) This example illustrates the pedestrian detector that detects all pedestrians in the image with one false detection on average.
(d) By setting the classiﬁcation threshold value to be very small, we achieve a very high detection rate (the detector detect all pedestrians in
the image) at the expense of increased false positives. Existing pedestrian detectors focus their performance in all scenarios, i.e., (a) −(d). As a
result, the detection performance in the practical range, i.e., between scenario (b) and (c), can be sub-optimal. In contrast, our proposed detector
has been designed to perform well only in the region, in which the performance is needed for real-world vision applications.
ordering1 to achieve the optimal partial AUC score. The
result is the ensemble learning method which yields
the scoring function consistent with the correct relative
ordering of positive and negative samples and optimizes
the partial AUC score in a false positive rate range [α, β]
where 0 ≤α < β ≤1.
Main contributions
The main contributions of our work can be summarized
as follows.
• We propose a novel approach to extract low-level
visual features based on spatial pooling for the
problem of pedestrian detection. Spatial pooling
has been successfully applied in sparse coding for
generic image classiﬁcation problems. We show that
spatial pooling applied to commonly-used features
such as covariance features and LBP descriptors
 improves accuracy of pedestrian detection.
• We propose a new structured ensemble learning
approach which explicitly optimizes the partial area
under the ROC curve (pAUC) between any two
given false positive rates. The method is of particular interest in the wide variety of applications
where performance is most important over a particular range within the ROC curve. The proposed
ensemble learning is termed pAUCEnsT (pAUC EN-
Semble learning with a Tight bound). The approach
1. The positive sample has an incorrect ordering if it is ranked below
the negative sample. In other words, we want all positive samples to
be ranked above all negative samples.
shares similarities with conventional boosting methods, but differs signiﬁcantly in that the proposed
method optimizes a multivariate performance measure using structured learning. Our design is simple
and a conventional boosting-based visual detector
can be transformed into a pAUCEnsT-based visual
detector with few modiﬁcations to the existing code.
Our approach is efﬁcient since it exploits both the
efﬁcient weak classiﬁer training and the efﬁcient
cutting plane solver for optimizing the partial AUC
score in the structured SVM setting. To our knowledge, our approach is the ﬁrst principled ensemble
method that directly optimizes the partial AUC in
an arbitrary false positive range [α, β].
• We build the best known pedestrian detector by combining the these two new techniques. Experimental results on several data sets, especially on challenging human detection data sets, demonstrate
the effectiveness of the proposed approach. The
new approach outperforms all previously reported
pedestrian detection results and achieves state-ofthe-art performance on INRIA, ETH, TUD-Brussels
and Caltech-USA pedestrian detection benchmarks.
Early versions of our work introduced a pAUCbased node classiﬁer for cascade classiﬁcation, which
optimizes the detection rate in the FPR range around
[0.49, 0.51]. and the low-level visual features based on
spatial pooling . Here we train a single strong classiﬁer with a new structured learning formulation which
has a tighter convex upper bound on the partial AUC
risk compared to . A region proposals generation,
known as binarized normed gradients (BING) , is
applied to speed up the evaluation time of detector.
We have also introduced new image features and a few
careful design when learning the detector. This leads to a
further improvement in accuracy and evaluation time as
compared to , . Our new detection framework
outperforms all reported pedestrian detectors (at the
time of submission), including several complex detectors such as LatSVM (a part-based approach which
models unknown parts as latent variables), ConvNet 
(deep hierarchical models) and DBN-Mut (discriminative deep model with mutual visibility relationship).
Related work
A few pedestrian detectors have been proposed over the
past decade along with newly created pedestrian detection benchmarks such as INRIA, ETH, TUD-Brussels,
Caltech and Daimler Pedestrian data sets. We refer readers to for an excellent review on pedestrian detection
frameworks and benchmark data sets. In this section,
we brieﬂy discuss some relevant work on object detection and review several recent state-of-the-art pedestrian
detectors that are not covered in .
Recent work in the ﬁeld of object recognition has
considered spatial pooling as one of crucial key components for computer vision system to achieve stateof-the-art performance on challenging benchmarks, e.g.,
Pascal VOC, Scene-15, Caltech, ImageNet , , ,
 . Spatial pooling is a method to extract visual representation based on encoded local features. In summary,
visual features are extracted from a patch representing
a small sub-window of an image. Feature values in
each sub-window are spatially pooled and concatenate
to form a ﬁnal feature vector for classiﬁcation. Invariant
representation is generally obtained by pooling feature
vectors over spatially local neighbourhoods.
The use of spatial pooling has long been part of
recognition architectures such as convolutional networks
 , , , . Spatial pooling (max pooling) is
considered as one of critical key ingredients behind deep
convolutional neural networks (CNN) which achieves
the best performance in recent large scale visual recognition tasks. In CNN, max pooling has been used to reduce
the computational complexity for upper layers and provide a form of translation invariance. Spatial pooling is
general and can be applied to various coding methods,
such as sparse coding, orthogonal matching pursuit and
soft threshold . Yang et al. propose to compute an image representation based on sparse codes of SIFT features
with multi-scale spatial max pooling . They conclude
that the new representation signiﬁcantly outperforms
the linear spatial pyramid matching kernel. Max pooling achieves the best performance in their experiments
compared with square root of mean squared statistics
pooling and the mean of absolute values pooling due
to its robustness to local spatial variations. To further
improve the performance of spatial pooling, Boureau et
al. transform the pooling process to be more selective
by applying pooling in both image space and descriptor
space . The authors show that this simple technique
can signiﬁcantly boost the recognition performance even
with relatively small dictionaries.
Various ensemble classiﬁers have been proposed in
the literature. Of these, AdaBoost is one the most well
known as it has achieved tremendous success in computer vision and machine learning applications. In object
detection, the cost of missing a true target is often
higher than the cost of a false positive. Classiﬁers that
are optimal under the symmetric cost, and thus treat
false positives and negatives equally, cannot exploit this
information , . Several cost sensitive learning
algorithms, where the classiﬁer weights a positive class
more heavily than a negative class, have thus been
Viola and Jones introduced the asymmetry property
in Asymetric AdaBoost (AsymBoost) . However, the
authors reported that this asymmetry is immediately
absorbed by the ﬁrst weak classiﬁer. Heuristics are then
used to avoid this problem. In addition, one needs to
carefully cross-validate this asymmetric parameter in
order to achieve the desired result. Masnadi-Shirazi and
Vasconcelos proposed a cost-sensitive boosting algorithm based on the statistical interpretation of boosting.
Their approach is to optimize the cost-sensitive loss by
means of gradient descent. Most work along this line
addresses the pAUC evaluation criterion indirectly. In
addition, one needs to carefully cross-validate the asymmetric parameter in order to maximize the detection rate
in a particular false positive range.
Several algorithms that directly optimize the pAUC
score have been proposed in bioinformatics , .
Dodd and Pepe propose a regression modeling framework based on the pAUC score . Komori and Eguchi
optimize the pAUC using boosting-based algorithms
 . Their algorithm is heuristic in nature. Narasimhan
and Agarwal develop structural SVM based methods
which directly optimize the pAUC score , . They
demonstrate that their approaches signiﬁcantly outperform several existing algorithms, including pAUCBoost
 and asymmetric SVM . Building on Narasimhan
and Agarwal’s work, we propose the principled fullycorrective ensemble method which directly optimizes the
pAUC evaluation criterion. The approach is ﬂexible and
can be applied to an arbitrary false positive range [α, β].
To our knowledge, our approach is the ﬁrst principled
ensemble learning method that directly optimizes the
partial AUC in a false positive range not bounded by
zero. It is important to emphasize here the difference
between our approach and that of . In the
authors train a linear structural SVM while our approach
learns the ensemble of classiﬁers.
A few recently proposed pedestrian detectors are
as follows. Sermanet et al. train a pedestrian detector
using a convolutional network model . Instead of
using hand designed features, they propose to use unsupervised sparse auto encoders to automatically learn
features in a hierarchy. The features generated from a
multi-scale convolutional network capture both global
and local details such as shapes, silhouette and facial components. Experimental results show that their
detector achieves competitive results on major benchmark data sets. Benenson et al. investigate different lowlevel aspects of pedestrian detection . The authors
show that by properly tuning low-level features, such
as feature selection, pre-processing the raw image and
classiﬁer training, it is possible to reach state-of-theart results on major benchmarks. From their paper, one
key observation that signiﬁcantly improves the detection
performance is to apply image normalization to the test
image before extracting features. Park et al. propose new
motion features for detecting pedestrians in a video
sequence . The authors use optical ﬂow to align large
objects in a sequence of image frames and use temporal
difference features to capture the information that remains. By factoring out camera motion and combining
their proposed motion features with channel features
 , the new detector achieves a ﬁve-fold reduction in
false positives over previous best results on the Caltech
pedestrian benchmark.
Another related work that applies structured SVM
learning to object detection is the work of Desai et
al. . The authors train the model that captures the
spatial arrangements of various object classes in the
image by considering which spatial layouts of objects
to suppress and which spatial layouts of objects to
favor. Although both our approach and cast the
problem as a structured prediction and apply the cutting
plane optimization, the underlying assumptions and
resulting models are quite different. The formulation of
 focuses on incorporating geometric conﬁgurations
between multiple object classes instead of optimizing the
detection rate within a prescribed false positive range.
Our approach learns a function that optimizes the partial
AUC risk between two false positive rates. In addition, it
is not trivial to extend the formulation of to boosting
Vectors are denoted by lower-case bold letters, e.g., x,
matrices are denoted by upper-case bold letters, e.g., X
and sets are denoted by calligraphic upper-case letters,
e.g., X. All vectors are assumed to be column vectors. The
(i, j) entry of X is xij. Let {x+
i=1 be a set of pedestrian
training examples, {x−
j=1 be a set of non-pedestrian
training examples and x ∈Rd be a d dimensional feature
vector. The tuple of all training samples is written as
S = (S+, S−) where S+ = (x+
1 , · · · , x+
m) ∈Xm and
1 , · · · , x−
n ) ∈Xn. We denote by H a set of
all possible outputs of weak learners. Assuming that we
have τ possible weak learners, the output of weak learners for positive and negative data can be represented as
H = (H+, H−) where H+ ∈Rτ×m and H−∈Rτ×n,
respectively. Here h+
ti is the label predicted by the weak
learner ℏt(·) on the positive training data x+
i . Each column h:l of the matrix H represents the output of all weak
learners when applied to the training instance xl. Each
row ht: of the matrix H represents the output predicted
by the weak learner ℏt(·) on all the training data. In
this paper, we are interested in the partial AUC (area
under the ROC curve) within a speciﬁc false positive
range [α, β]. Given n negative training samples, we let
jα = ⌈nα⌉and jβ = ⌊nβ⌋. Let Zβ =
denote the set
of all subsets of negative training instances of size jβ. We
deﬁne ζ = {x−
j=1 ∈Zβ as a given subset of negative
instances, where k = [k1, . . . , kjβ] is a vector indicating
which elements of S−are included. The goal is to learn
a set of binary weak learners and a scoring function,
f : Rd →R, that acheive good performance in terms of
the pAUC between some speciﬁed false positive rates α
and β where 0 ≤α < β ≤1. Here f(x) = Pτ
t=1 wtℏt(x)
where w ∈Rτ is the linear coefﬁcient vector, {ℏt(·)}τ
denote a set of binary weak learners and τ is the number
of weak learners.
OUR APPROACH
Despite several important work on object detection, the
most practical and successful pedestrian detector is still
the sliding-window based method of Viola and Jones .
Their method consists of two main components: feature
extraction and the AdaBoost classiﬁer. For pedestrian
detection, the most commonly used features are HOG
 and HOG+LBP . Doll´ar et al. propose Aggregated
Channel Features (ACF) which combine gradient histogram (a variant of HOG), gradients and LUV . ACF
uses the same channel features as ChnFtrs , which is
shown to outperform HOG , .
To train the classiﬁer, the procedure known as bootstrapping is often applied, which harvests hard negative
examples and re-trains the classiﬁer. Bootstrapping can
be repeated several times. It is shown in that at
least two bootstrapping iterations are required for the
classiﬁer to achieve good performance. In this paper, we
design the new pedestrian detection framework based
on the new spatially pooled features, a novel form of ensemble classiﬁer which directly optimizes the partial area
under the ROC curve and an efﬁcient region proposals
generation. We ﬁrst propose the new feature type based
on a modiﬁed low-level descriptor and spatial pooling.
In the next section, we discuss how the performance
measure can be further improved using the proposed
structured learning framework. Finally, we discuss our
modiﬁcations to in order to achieve state-of-theart detection results on Caltech pedestrian detection
benchmark data sets.
Spatially pooled features
Spatial pooling has been proven to be invariant to
various image transformations and demonstrate better
robustness to noise , , . Several empirical results have indicated that a pooling operation can greatly
improve the recognition performance. Pooling combines
several visual descriptors obtained at nearby locations
into some statistics that better summarize the features
over some region of interest (pooling region). The new
feature representation preserves visual information over
a local neighbourhood while discarding irrelevant details
and noises. Combining max-pooling with unsupervised
feature learning methods have led to state-of-the art
image recognition performance on several object recognition tasks. Although these feature learning methods
have shown promising results over hand-crafted features, computing these features from learned dictionaries
is still a time-consuming process for many real-time
applications. In this section, we further improve the
performance of low-level features by adopting the pooling operator commonly applied in unsupervised feature
learning. This simple operation can enhance the feature
robustness to noise and image transformation. In the
following section, we investigate two visual descriptors
which have shown to complement HOG in pedestrian
detection, namely covariance descriptors and LBP. It is
important to point out here that our approach is not
limited to these two features, but can be applied to any
low-level visual features.
Background A covariance matrix is positive semideﬁnite. It provides a measure of the relationship between two or more sets of variates. The diagonal entries of covariance matrices represent the variance of
each feature and the non-diagonal entries represent the
correlation between features. The variance measures the
deviation of low-level features from the mean and provides information related to the distribution of lowlevel features. The correlation provides the relationship
between multiple low-level features within the region.
In this paper, we follow the feature representation as
proposed in . However, we introduce an additional
edge orientation which considers the sign of intensity
derivatives. Low-level features used in this paper are:
[x, y, |Ix|, |Iy|, |Ixx|, |Iyy|, M, O1, O2]
where x and y represent the pixel location, and Ix and
Ixx are ﬁrst and second intensity derivatives along the
x-axis. The last three terms are the gradient magnitude
I2x + I2y), edge orientation as in (O1
arctan(|Iy|/|Ix|)) and an additional edge orientation O2
atan2(Iy, Ix)
if atan2(Iy, Ix) > 0,
atan2(Iy, Ix) + π
otherwise.
The orientation O2 is mapped over the interval [0, π].
Although some O1 features might be redundant after
introducing O2, these features would not deteriorate
the performance as they will not be selected by the
weak learner. Our preliminary experiments show that
using O1 alone yields slightly worse performance than
combining O1 and O2. With the deﬁned mapping, the
input image is mapped to a 9-dimensional feature image.
The covariance descriptor of a region is a 9 × 9 matrix,
and due to symmetry, only the upper triangular part is
stored, which has only 45 different values.
Local Binary Pattern (LBP) is a texture descriptor that
represents the binary code of each image patch into a
feature histogram . The standard version of LBP is
formed by thresholding the 3×3-neighbourhood of each
pixel with the centre pixel’s value. All binary results
are combined to form an 8-bit binary value (28 different
labels). The histogram of these 256 different labels can
be used as texture descriptor. The LBP descriptor has
shown to achieve good performance in many texture
classiﬁcation . In this work, we adopt an extension of
LBP, known as the uniform LBP, which can better ﬁlter
out noises . The uniform LBP is deﬁned as the binary
pattern that contains at most two bitwise transitions
from 0 to 1 or vice versa.
Spatially pooled covariance In this section, we improve the spatial invariance and robustness of the original covariance descriptor by applying the operator
known as spatial pooling. There exist two common
pooling strategies in the literature: average pooling and
max-pooling. We use max-pooling as it has been shown
to outperform average pooling in image classiﬁcation
 , . We divide the image window into small patches
(refer to Fig. 2). For each patch, covariance features
are calculated over pixels within the patch. For better
invariance to translation and deformation, we perform
spatial pooling over a pre-deﬁned spatial region (pooling
region) and use the obtained results to represent covariance features in the pooling region. The pooling operator
thus summarizes multiple covariance matrices within
each pooling region into a single matrix which represents
covariance information. We refer to the feature extracted
from each pooling region as spatially pooled covariance
(sp-Cov) feature. Note that extracting covariance features
in each patch can be computed efﬁciently using the integral image trick . Our sp-Cov differs from covariance
features in in the following aspects:
1. We apply spatial pooling to a set of covariance
descriptors in the pooling region. To achieve this, we
ignore the geometry of covariance matrix and stack the
upper triangular part of the covariance matrix into a
vector such that pooling is carried out on the vector
space. For simplicity, we carry out pooling over a square
image region of ﬁxed resolution. Considering pooling
over a set of arbitrary rectangular regions as in 
is likely to further improve the performance of our
2. Instead of normalizing the covariance descriptor of
each patch based on the whole detection window ,
we calculate the correlation coefﬁcient within each patch.
The correlation coefﬁcient returns the value in the range
[−1, 1]. As each patch is now independent, the feature
extraction can be done in parallel on the GPU.
Implementation We extract sp-Cov using multi-scale
Pooling region
sp-Cov feature
Fig. 2: Architecture of our pooled features. In this example, sp-Cov are
extracted from each ﬁxed sized pooling region.
patches with the following sizes: 8 × 8, 16 × 16 and
32 × 32 pixels. Each scale will generate a different set
of visual descriptors. Multi-scale patches have also been
used in . In this paper, the use of multi-scale patches
is important as it expands the richness of our feature
representations and enables us to capture human body
parts at different scales. In our experiments, we set the
patch spacing stride (step-size) to be 1 pixel. The pooling
region is set to be 4 × 4-pixel and the pooling spacing
stride is set to 4 pixels in our experiments.
Spatially pooled LBP Similar to sp-Cov, we divide the
image window into small patches and extract LBP over
pixels within the patch. The histogram, which represents
the frequency of each pattern occurring, is computed
over the patch. For better invariance to translation, we
perform spatial pooling over a pooling region and use
the obtained results to represent the LBP histogram
in the pooling region. We refer to the new feature as
spatially pooled LBP (sp-LBP) feature.
Implementation For the LBP operator, we use the
3 × 3-neighbourhood of each pixel and extract the local
histogram using a patch size of 4 × 4, 8 × 8 and 16 × 16
pixels. For sp-LBP, the patch spacing stride, the pooling
region and the pooling spacing stride are set to 1 pixel,
8 × 8-pixel and 4 pixels, respectively.
Discussion Although we make use of spatial pooling,
our approach differs signiﬁcantly from the unsupervised
feature learning pipeline, which has been successfully
applied to image classiﬁcation problem , . Instead
of pooling encoded features over a pre-trained dictionary, we compute sp-Cov and sp-LBP by performing
pooling directly on covariance and LBP features extracted from local patches. In other words, our proposed
approach removes the dictionary learning and feature
encoding from the conventional unsupervised feature
learning , . The advantage of our approach over
conventional feature learning is that our features have
much less dimensions than the size of visual words
often used in generic image classiﬁcation . Using too
few visual words can signiﬁcantly degrade the recognition performance as reported in and using too
many visual words would lead to very high-dimensional
features and thus make the classiﬁer training become
computationally infeasible.
Optimizing partial AUC
Structured learning approach Before we propose our
approach, we brieﬂy review the concept of SVM tight
[α, β] , in which our ensemble learning approach is
built upon. The area under the empirical ROC curve
(AUC) can be deﬁned as,
i ) > f(x−
The objective is to learn a scoring function f, f : Rd →R,
that maximizes the AUC, or equivalently, minimizes the
empirical risk,
RAUC(f) = 1 −AUC.
For the partial AUC (pAUC) in the false positive range
[α, β], the empirical pAUC risk can be written as :
RpAUC(f) = 1
j=jα+11(f(x+
i ) < f(x−
(j)f|ζ)). (3)
Here c is a constant, c = mn(β −α), x+
denotes the
i-th positive training instance, x−
(j)f|ζ denotes the j-th
negative training instance sorted by the scoring function,
f, in the set ζ = {x−
j=1 ∈Zβ, ζ denote the chosen
subset of negative instances and Zβ =
denote the
set of all subsets of negative training instances of size
jβ. In other words, we sort all negative instances based
on their scoring values to obtain {x−
j=1 in which
(1)f ) ≥f(x−
(2)f ) ≥. . . ≥f(x−
(jβ)f ) ≥. . . ≥f(x−
and ζ = {x−
(1)f|ζ, · · · , x−
(jβ)f|ζ}. Although the number of
elements in ζ is jβ (there are jβ negative samples in the
set), the empirical pAUC risk deﬁned in (3) is computed
from jβ −jα negative samples.
Clearly (3) is minimal when all positive samples,
i=1, are ranked above {x−
j=jα+1, which represent negative samples in our prescribed false positive
range [α, β] (in this case, the log-average miss rate would
be zero). The structural SVM framework can be adopted
to optimize the pAUC risk by considering a classiﬁcation
problem of all m × jβ pairs of positive and negative
samples. We deﬁne a new label matrix π ∈Πm,jβ =
{0, 1}m×jβ (on the entire positive instances {x+
a given subset of negative instances ζ = {x−
where k = [k1, . . . , kjβ] is a vector indicating which
elements of S−are included) whose value for the pair
(i, j) is deﬁned as:
i is ranked above x−
otherwise.
The true pair-wise label is deﬁned as π∗where π∗
for all pairs (i, j). The pAUC loss is calculated from
the number of pairs of examples that are ranked in the
wrong order, i.e., negative examples are ranked before
positive examples. Hence the pAUC loss between the
prediction π and the true pair-wise label π∗can be
written as:
∆(α,β)(π, π∗) = 1
πi,(j)π −π∗
 πi,(j)π −0
j=jα+1πi,(j)π,
where (j)π denotes the index of the negative instance
in S−ranked in the j-th position by any ﬁxed ordering
consistent with the matrix π. We deﬁne a joint feature
map, φζ : (Xm × Xn) × Πm,jβ →Rd, which takes a set
of training instances (m positive samples and n negative
samples) and an ordering matrix of dimension m × jβ
and produce a vector output in Rd as:
φζ(S, π) = 1
j=1(1 −πij)(x+
This feature map ensures that the variable w (w ∈Rd)
that optimizes w⊤φζ(S, π) will also produce the optimal
pAUC score for w⊤x. We can summarize the above
problem as the following convex optimization problem
s.t. w⊤(φζ(S, π∗) −φζ(S, π)) ≥∆(α,β)(π, π∗) −ξ,
∀ζ ∈Zβ, ∀π ∈Πm,jβ and ξ ≥0. Note that π∗denotes
the correct relative ordering and π denotes any arbitrary
orderings and ν controls the amount of regularization.
Partial AUC based ensemble classiﬁer In order to
design an ensemble-like algorithm for the pAUC, we
ﬁrst introduce a projection function, ℏ(·), which projects
an instance vector x to {−1, +1}. This projection function is also known as the weak learner in boosting. In
contrast to the previously described structured learning,
we learn the scoring function, which optimizes the area
under the curve between two false positive rates of
the form: f(x) = Pτ
t=1 wtℏt(x) where w ∈Rτ is the
linear coefﬁcient vector, {ℏt(·)}τ
t=1 denote a set of binary
weak learners and τ is the number of weak learners.
Let us assume that we have already learned a set of
all projection functions. By using the same pAUC loss,
∆(α,β)(·, ·), as in (5), and the same feature mapping,
φζ(·, ·), as in (6), the optimization problem we want to
s.t. w⊤(φζ(H, π∗) −φζ(H, π)) ≥∆(α,β)(π, π∗) −ξ,
∀π ∈Πm,jβ and ξ ≥0. H = (H+, H−) is the projected output for positive and negative training samples.
φζ(H, π) = [φζ(h1:, π), · · · , φζ(hτ:, π)] where φζ(ht:, π) :
(Rm × Rn) × Πm,jβ →R and it is deﬁned as,
φζ(ht:, π) = 1
j=1(1 −πij)
i ) −ℏt(x−
j=1 is any given subsets of negative instances and k
= [k1, . . . , kjβ] is a vector indicating
which elements of S−are included. The only difference
between (7) and (8) is that the original data is now
projected to a new non-linear feature space. The dual
problem of (8) can be written as,
πλ(π)∆(α,β)(π∗, π)−
π,ˆπλ(π)λ(ˆπ)⟨φ∆(H, π), φ∆(H, ˆπ)⟩
where λ is the dual variable, λ(π) denotes the dual
variable associated with the inequality constraint for
π ∈Πm,jβ and φ∆(H, π) = φζ(H, π∗) −φζ(H, π). To
derive the Lagrange dual problem, the following KKT
condition is used,
 φζ(H, π∗) −φζ(H, π)
Finding best weak learners In this section, we show
how one can explicitly learn the projection function,
ℏ(·). We use the idea of column generation to derive
an ensemble-like algorithm similar to LPBoost . The
condition for applying the column generation is that the
duality gap between the primal and dual problem is
zero (strong duality). By inspecting the KKT condition,
at optimality, (11) must hold for all t = 1, · · · , τ. In other
π∈Πm,jβ λ(π)
 φζ(ht:, π∗) −φζ(ht:, π)
must hold for all t.
For weak learners in the current working set, the
corresponding condition in (11) is satisﬁed by the current solution. For weak learners that are not yet selected, they do not appear in the current restricted
optimization problem and their corresponding coefﬁcients are zero (wt
0). It is easy to see that if
π∈Πm,jβ λ(π)
 φζ(ht:, π∗) −φζ(ht:, π)
= 0 for all ℏt(·)
that are not in the current working set, then the current
solution is already the globally optimal one. Hence the
subproblem for selecting the best weak learner is:
ℏ∗(·) = argmax
 φζ(h, π∗) −φζ(h, π)
In other words, we pick the weak learner with the value
 φζ(h, π∗)−φζ(h, π)
| most deviated from zero.
Thus, a stopping condition for our algorithm is
 φζ(h, π∗) −φζ(h, π)
where ε > 0 is a small precision constant (e.g., 10−4). To
ﬁnd the most optimal weak learner in H, we consider the
relative ordering of all positive and negative instances,
π ∈Πm,jβ = {0, 1}m×jβ, whose value for the pair (i, j)
is similar to (4). Given the weak learner ℏ(·), we deﬁne
the joint feature map for the output of the weak learner
φζ(h, π) = 1
j=1(1 −πij)
The subproblem for generating the optimal weak learner
at iteration t considering the relative ordering of all
positive and negative training instances, i.e. (12), can be
re-written as,
t (·) = argmax
 φζ(h, π∗) −φζ(h, π)
lulylℏ(xl)
lulylℏ(xl)
where i, j, l index the positive training samples (i =
1, · · · , m), the negative training samples (j = 1, · · · , jβ)
and the entire training samples (l = 1, 2,· · · ,m + jβ),
respectively. Here yl is equal to +1 if xl is a positive
sample and −1 otherwise, and
π,j λ(π)πlj
if xl is a positive sample
π,i λ(π)πil
otherwise.
For decision stumps and decision trees, the last equation
in (15) is always valid since the weak learner set H is
negation-closed . In other words, if ℏ(·) ∈H, then
[−ℏ](·) ∈H, and vice versa. Here [−ℏ](·) = −ℏ(·). For
decision stumps, one can ﬂip the inequality sign such
that ℏ(·) ∈H and [−ℏ](·) ∈H. In fact, any linear
classiﬁers of the form sign(P
t atxt + a0) are negationclosed. Using (15) to choose the best weak learner is not
heuristic as the solution to (12) decreases the duality gap
the most for the current solution.
Optimizing weak learners’ coefﬁcients We solve for
the optimal w that minimizes our objective function (8).
However, the optimization problem (8) has an exponential number of constraints, one for each matrix π ∈
Πm,jβ. As in , , we use the cutting plane method
to solve this problem. The basic idea of the cutting plane
is that a small subset of the constraints are sufﬁcient to
ﬁnd an ϵ-approximate solution to the original problem.
The algorithm starts with an empty constraint set and it
adds the most violated constraint set at each iteration.
The QP problem is solved using linear SVM and the
process continues until no constraint is violated by more
than ϵ. Since, the quadratic program is of constant size
and the cutting plane method converges in a constant
number of iterations, the major bottleneck lies in the
combinatorial optimization (over Πm,jβ) associated with
ﬁnding the most violated constraint set at each iteration.
Narasimhan and Agarwal show how this combinatorial
problem can be solved efﬁciently in a polynomial time
 . We brieﬂy discuss their efﬁcient algorithm in this
The combinatorial optimization problem associated
with ﬁnding the most violated constraint can be written
¯π = argmax
Qw(π) =∆(α,β)(π∗, π)−
The trick to speed up (17) is to note that any ordering
of the instances that is consistent with π yields the
same objective value, Qw(π) in (18). In addition, one can
break down (17) into smaller maximization problems by
restricting the search space from Πm,jβ to the set Πw
π ∈Πm,jβ| ∀i, j1 < j2 : πi,(j1)w ≥πi,(j2)w
m,jβ represents the set of all matrices π in which
the ordering of the scores of two negative instances,
:j1 and w⊤h−
:j2, is consistent. The new optimization
problem is now easier to solve as the set of negative
instances over which the loss term in (18) is computed is
the same for all orderings in the search space. Interested
reader may refer to . We summarize the algorithm of
our pAUCEnsT in Algorithm 1.
A theoretical analysis of the convergence property for
Algorithm 1 is as follows.
Proposition 1. At each iteration of Algorithm 1, the objective value decreases.
Proposition 2. The decrease of objective value between
iterations t −1 and t is not less than
φζ(ht:, π∗) −φζ(ht:, π⋆
[t] = argmax
∆(α,β)(π, π∗) + w⊤
[t]φζ(H, π)
See supplementary material for the proofs.
Computational complexity Each iteration in Algorithm 1 consists of 5 steps. Step x learns the weak classi-
ﬁer with the minimal weighted error and add this weak
learner to the ensemble set. In this step, we train a weak
classiﬁer using decision trees. We train the decision tree
using the fast implementation of , in which feature
values are quantized into 256 bins. This procedure costs
O((m+n)FK) at each iteration. where m+n is the total
number of samples, F is the number of features and K
is the number of nodes in the tree.
We next analyze the time complexity of step z which
calls Algorithm 2. Algorithm 2 solves the structural SVM
TABLE 1: Computational complexity of our approach and AdaBoost. m, n are the number of positive and negative training samples, F is the
number of features, K is the number of nodes in the decision tree, tmax is the maximum number of weak learners learned and r is the maximum
number of cutting-plane iterations
Training a weak learner (Step x in Algorithm 1)
 (m + n)FK
 (m + n)FK
Solve w (Algorithm 2)
 log(m + n) + tmax
(m + n)(r log(m + n) + rtmax + FK)
Algorithm 1 The training algorithm for pAUCEnsT.
1) A set of training examples {xl, yl}, l = 1, · · · , m + n;
2) The maximum number of weak learners, tmax;
stopping precision constant ε;
3) The regularization parameter, ν;
4) The learning objective based on the partial AUC, α and
Output: The scoring function†, f(x) = Ptmax
t=1 wtℏt(x),
that optimizes the pAUC score in the FPR range
Initialize:
2) Initilaize sample weights: ul = 0.5
m if yl = +1, else
3) Extract low level features and store them in the cache
memory for fast data access;
while t < tmax and (13) is not met do
x Train a new weak learner using (15). The weak
learner corresponds to the weak classiﬁer with the
minimal weighted error (maximal edge);
y Add the best weak learner into the current set;
z Solve the structured SVM problem using the
cutting plane algorithm (Algorithm 2);
{ Update sample weights, u, using (16);
| t ←t + 1;
† For a node in a cascade classiﬁer, we introduce the
threshold, b, and adjust b using the validation set such
achieves the node learning objective;
problem using the efﬁcient cutting-plane algorithm. Step
x in Algorithm 2 costs O
 tmax(m + n)
time since the
linear kernel scales linearly with the number of training
samples . Here tmax is the maximum number of
features (weak classiﬁers). Using the efﬁcient algorithm
of , step y costs O
 n log n + (m + nβ) log(m + nβ)
 (m+n) log(m+n)
time where nβ = nβ and β ≤1. As
shown in , the number of iterations of Algorithm 2 is
upper bounded by the value which is independent of the
number of training samples. Here, we assume that the
number of cutting-plane iterations required is bounded
by r. In total, the time complexity of Algorithm 2 (Step
z in Algorithm 1) is O
 log(m+n)+tmax
{ updates the sample variables which can be executed
in linear time. In summary, the total time complexity for
training tmax boosting iterations using our approach is
(m + n)(r log(m + n) + rtmax + FK)
. From this
analysis, most of the training time is spent on training
weak learners when FK ≫log(m + n). We summarizes
the computational complexity of our approach in Table 1.
Algorithm 2 The cutting-plane algorithm
1) A set of weak learners’ outputs H = (H+, H−);
2) The learning objective based on the partial AUC, α and
3) The regularization parameter, ν;
4) The cutting-plane termination threshold, ϵ;
Output: The weak learners’ coefﬁcients w, the working
set C and the dual variables λ, ρ;
Initialize: C = ∅;
Qw(π) = ∆(α,β)(π∗, π) −
i,j πijw⊤(h+
x Solve the dual problem using linear SVM,
s.t. Qw(π) ≤ξ, ∀π ∈C;
y Compute the most violated constraint,
¯π = argmax
z C ←C ∪{¯π};
Until Qw(¯π) ≤ξ + ϵ;
Table 1 also compares the computational complexity of
our approach with AdaBoost. We discuss the difference
between our approach and AdaBoost in the next section.
Discussion Our ﬁnal ensemble classiﬁer has a similar
form as the AdaBoost-based object detector of . Based
on Algorithm 1, step x and y of our algorithm are
identical to the ﬁrst two steps of AdaBoost adopted in
 . Similar to AdaBoost, ul in step x plays the role of
sample weights associated to each training sample. The
major difference between AdaBoost and our approach is
in step z and { where the weak learner’s coefﬁcient
is computed and the sample weights are updated. In
AdaBoost, the weak learner’s coefﬁcient is calculated as
2 log 1−ϵt
where ϵt = P
 yl ̸= ℏt(xl)
the indicator function. The sample weights are updated
ul exp(−wtylℏt(xl))
l ul exp(−wtylℏt(xl).
We point this out here since a minimal modiﬁcation is required in order to transform the existing implementation
of AdaBoost to pAUCEnsT, due to the high similarity.
We point out here the major difference between the
ensemble classiﬁer proposed in this paper and our earlier
work . In this work, we redeﬁne the joint feature map
(6) over subsets of negative instances. The new feature
map leads to a tighter hinge relaxation on the partial
AUC loss. In other words, the joint feature map deﬁned
in is computed over all the negative instances instead of subsets of negative instances ranked in positions
jα + 1, · · · , jβ (corresponding to the FPR range [α, β] one
is interested in). As a result, the new formulation is not
only faster to train but also perform slightly better on
the pAUC measure than .
Region proposals generation
The evaluation of our pedestrian detector outlined in the
previous subsections can be computed efﬁciently with
the use of integral channel features . However the
detector is still not efﬁcient enough to be used in a
sliding-window based framework. In order to improve
the evaluation time, we adopt a cascaded approach in
which classiﬁers are arranged based on their complexity
 . In this paper, we adopt a two-stage approach, in
which we place the fast to extract features in the ﬁrst
stage and our proposed features with pAUCEnsT in the
second stage. In other words, our proposed detector is
evaluated only on test samples which pass through the
ﬁrst stage.
Recently the binarized normed gradients (BING) feature with a linear SVM classiﬁer has been shown to
speed up the classical sliding window object detection paradigm by discarding a large set of background
patches . On Pascal VOC2007, it achieves a comparable object detection rate to recently proposed Objectness
 and Selective Search , while being three orders of
magnitudes faster than these approaches. The detector of
 is adopted in the ﬁrst stage of our two-stage detector
to ﬁlter out a large number of background patches. The
underlying idea is to reduce the number of samples that
our proposed detector needs to process.
Implementation The original BING detector of 
was trained for generic object detection. We make the
following modiﬁcations to the original BING detector to
improve its performance for pedestrian detection.
1) Instead of resizing the training data to a resolution
of 8×8 pixels, we resize the resolution of pedestrian
samples to 8 × 16 pixels. This template has the
same aspect ratio as the one adopted in . Hence
the BING features we use in our paper is 128 bit
integer instead of 64 bit integer used in the original
paper. The data type UINT128 is adopted to store
BING features. In addition, we replace the Pascal
VOC2007 training data with the Caltech training
data to train the BING detector.
2) The original paper quantizes the test image to a
resolution {(w, h)} where w, h ∈{10, 20, 40, 80, 160,
320}. In this paper, we apply a multi-scale detection
with a ﬁxed aspect ratio. We scan the test image at
8 scales per octave (corresponding to a scale stride
TABLE 2: Log-average miss rate of our features with and without
applying spatial pooling. We observe that spatial pooling improves
the translation invariance of our features
with pooling
with pooling
TUD-Br. 
EXPERIMENTS
Spatially pooled features
We compare the performance of the proposed feature
with and without spatial pooling. Our sp-Cov consists
of 9 low-level image statistics. We exclude the mean and
variance of two image statistics (pixel locations at x and
y co-ordinates) since they do not capture discriminative
information. We also exclude the correlation coefﬁcient
between pixel locations at x and y co-ordinates. Hence
there is a total of 136 channels (7 low-level image statistics + 3 · 7 variances + 3 · 35 correlation coefﬁcients +
3 LUV color channels)2. Experiments are carried out
using AdaBoost with the shrinkage parameter of 0.1
 and level-3 decision trees as weak classiﬁers. We
apply shrinkage to AdaBoost as it has been shown to
improve the ﬁnal classiﬁcation accuracy . We use
the depth-3 decision tree as it offers better generalization performance as shown in . We train three
bootstrapping iterations and the ﬁnal model consists of
2048 weak classiﬁers with soft cascade. We heuristically
set the soft cascade’s rejection threshold to be −10 at
every node. Log-average miss rates of detectors trained
using covariance descriptors and LBP (without and with
spatial pooling) are shown in Table 2. We observe that
it is beneﬁcial to apply spatial pooling as it increases
the robustness of the features against small deformations
and translations. We observe a reduction in miss rate by
more than one percent on the INRIA test set. Since we
did not combine sp-LBP with HOG as in , sp-LBP
performs slightly worse than sp-Cov.
Compared with other pedestrian detectors In this
experiment, we compare the performance of our proposed sp-Cov with the original covariance descriptor
proposed in . calculates the covariance distance
in the Riemannian manifold. As eigen-decomposition
is performed, the approach of is computationally
expensive. We speed up the weak learner training by
proposing our modiﬁed covariance features and train
the weak learner using the decision tree. The new weak
learner is not only simpler than but also highly
effective. We compare our previously trained detector
with the original covariance descriptor in Fig. 3. We
plot HOG and HOG+LBP as the baseline. Similar
to the result reported in , where the authors show
that HOG+Boosting reduces the average miss-rate over
HOG+SVM by more than 30%, we observe that applying
our sp-Cov features as the channel features signiﬁcantly
2. Note here that we extract covariance features at 3 different scales.
INRIA Person Classification
False positives per window (FPPW)
HOG (Dalal and Triggs)
COV (Tuzel et al.)
HOG+LBP (Wang et al.)
Fig. 3: ROC curves of our sp-Cov features and the conventional
covariance detector on INRIA test images.
TABLE 3: Log-average miss rates of various feature combinations
M+O+LUV+LBP
sp-Cov+LUV
sp-Cov+M+O+LUV
sp-Cov+sp-LBP+M+O+LUV
improves the detection performance over the original
covariance detector (a reduction of more than 5% miss
rate at 10−4 false positives per window).
Next we compare the proposed sp-Cov with ACF features (M+O+LUV) . Since ACF uses fewer channels
than sp-Cov, for a fair comparison, we increase ACF’s discriminative power by combining ACF features with LBP3
(M+O+LUV+LBP). The results are reported in Table 3.
We observe that sp-Cov yields competitive results to
M+O+LUV+LBP. From the table, sp-Cov performs better
on the INRIA test set, worse on the ETH test set and
on par with M+O+LUV+LBP on the TUD-Brussels test
set. We observe that the best performance is achieved by
combining sp-Cov and sp-LBP with M+O+LUV.
Ensemble classiﬁer
Synthetic data set In this experiment, we illustrate the
effectiveness of our ensemble classiﬁer on a synthetic
data set similar to the one evaluated in . The radius
and angle of the positive data is drawn from a uniform
distribution [0, 1.5] and [0, 2π], respectively. The radius of
the negative data is drawn from a normal distribution
with mean of 2 and the standard deviation of 0.4. The
angle of the negative data is drawn from a uniform
distribution similar to the positive data. We generate
400 positive data and 400 negative data for training
and validation purposes (200 for training and 200 for
validating the asymmetric parameter). For testing, we
evaluate the learned classiﬁer with 2000 positive and
negative data. We compare pAUCEnsT against the baseline AdaBoost, Cost-Sensitive AdaBoost (CS-AdaBoost)
3. In our implementation, we use an extension of LBP, known as
the uniform LBP, which can better ﬁlter out noises . Each LBP bin
corresponds to each channel.
TABLE 4: The pAUC score on Protein-protein interaction data set. The
higher the pAUC score, the better the classiﬁer. Results marked by †
were reported in . The best classiﬁer is shown in boldface
pAUC(0, 0.1)
Ours (pAUCEnsT)
pAUCEns [0, 0.1] 
pAUC [0, 0.1] 
pAUC [0, 0.1] 
pAUCBoost [0, 0.1]† 
Asym SVM [0, 0.1]† 
 and Asymmetric AdaBoost (AsymBoost) . For
CS-AdaBoost, we set the cost for misclassifying positive
and negative data as follows. We assign the asymmetric
factor k = C1/C2 and restrict 0.5(C1 + C2) = 1. We then
choose the best k which returns the highest partial AUC
from {0.5, 0.6, · · · , 2.4, 2.5}. For AsymBoost, we choose
the best asymmetric factor k which returns the highest
partial AUC from {2−1, 2−0.8, · · · , 21.8, 22}. For our
approach, the regularization parameter is chosen from
{10−5, 10−4.8, · · · , 10−3.2, 10−3}. We use vertical and
horizontal decision stumps as the weak classiﬁer. For
each algorithm, we train a strong classiﬁer consisting of
10 weak classiﬁers. We evaluate the partial AUC of each
algorithm at [0, 0.2] FPRs.
Fig. 4 illustrates the boundary decision4 and the pAUC
score. Our approach outperforms all other asymmetric classiﬁers. We observe that pAUCEnsT places more
emphasis on positive samples than negative samples
to ensure the highest detection rate at the left-most
part of the ROC curve (FPR < 0.2). Even though we
choose the asymmetric parameter, k, from a large range
of values, both CS-AdaBoost and AsymBoost perform
slightly worse than our approach. AdaBoost performs
worst on this toy data set since it optimizes the overall
classiﬁcation accuracy.
Protein-protein interaction prediction In this experiment, we compare our approach with existing algorithms which optimize pAUC in bioinformatics. The
problem we consider here is a protein-protein interaction
prediction , in which the task is to predict whether a
pair of proteins interact or not. We used the data set
labelled ‘Physical Interaction Task in Detailed feature
type’, which is publicly available on the internet5. The
data set contains 2865 protein pairs known to be interacting (positive) and a random set of 237, 384 protein pairs
labelled as non-interacting (negative). We use a subset
of 85 features as in . We randomly split the data
into two groups: 10% for training/validation and 90% for
evaluation. We choose the best regularization parameter
form {1, 1/2, 1/5} by 5-fold cross validation. We repeat
our experiments 10 times using the same regularization
parameter. We train a linear classiﬁer as our weak learner
using LIBLINEAR . We set the maximum number of
4. We set the threshold such that the false positive rate is 0.2.
5. sulp/proteins05 pages/
feature-download.html
AdaBoost (10 stumps) pAUC[0, 0.2]: 0.75
CS−AdaBoost (10 stumps) pAUC[0, 0.2]: 0.80
AsymBoost (10 stumps) pAUC[0, 0.2]: 0.82
Ours (10 stumps) pAUC[0, 0.2]: 0.87
Fig. 4: Decision boundaries on the toy data set where each strong classiﬁer consists of 10 weak classiﬁers (horizontal and vertical decision
stumps). Positive and negative data are represented by ◦and ×, respectively. The partial AUC score in the FPR range [0, 0.2] is also displayed.
Our approach achieves the best pAUC score compared to other asymmetric classiﬁers.
TABLE 5: Average pAUC scores in the FPR range [0, 0.1] and their
standard deviations on vision data sets at various boosting iterations.
Experiments are repeated 20 times. The best average performance is
shown in boldface
0.88 (0.01)
0.72 (0.03)
0.72 (0.02)
0.92 (0.01)
0.78 (0.03)
0.81 (0.01)
0.97 (0.00)
0.87 (0.02)
0.91 (0.01)
0.87 (0.01)
0.70 (0.03)
0.71 (0.02)
0.91 (0.01)
0.77 (0.03)
0.79 (0.01)
0.96 (0.01)
0.85 (0.02)
0.90 (0.01)
0.87 (0.02)
0.70 (0.03)
0.71 (0.02)
0.91 (0.01)
0.77 (0.03)
0.80 (0.01)
0.96 (0.01)
0.85 (0.02)
0.90 (0.01)
0.87 (0.02)
0.71 (0.03)
0.72 (0.02)
0.91 (0.01)
0.77 (0.03)
0.79 (0.01)
0.96 (0.00)
0.85 (0.02)
0.90 (0.01)
boosting iterations to 100 and report the pAUC score
of our approach in Table 4. Baselines include pAUCEns,
SVMpAUC, SVMAUC, pAUCBoost and Asymmetric SVM.
Our approach outperforms all existing algorithms which
optimize either AUC or pAUC . We attribute our improvement over SVMtight
pAUC [0, 0.1] , as a result of introducing a non-linearity into the original problem. This
phenomenon has also been observed in face detection as
reported in .
Comparison to other asymmetric boosting Here we
compare pAUCEnsT against existing asymmetric boosting algorithms, namely, AdaBoost with Fisher LDA postprocessing and AsymBoost . The results of AdaBoost are also presented as the baseline. For each algorithm, we train a strong classiﬁer consisting of 100 weak
classiﬁers (decision trees of depth 2). We then calculate
the pAUC score by varying the threshold value in the
FPR range [0, 0.1]. For each algorithm, the experiment
is repeated 20 times and the average pAUC score is
reported. For AsymBoost, we choose k from {2−0.5,
2−0.4, · · · , 20.5} by cross-validation. For our approach,
the regularization parameter is chosen from {1, 0.5, 0.2,
0.1} by cross-validation. We evaluate the performance of
all algorithms on 3 vision data sets: USPS digits, scenes
and face data sets. For USPS, we use raw pixel values
and categorize the data sets into two classes: even digits
and odd digits. For scenes, we divide the 15-scene data
sets used in into 2 groups: indoor and outdoor
scenes. We use CENTRIST as our feature descriptors
and build 50 visual code words using the histogram
intersection kernel . Each image is represented in a
spatial hierarchy manner. Each image consists of 31 subwindows. In total, there are 1550 feature dimensions per
image. For faces, we use face data sets from and randomly extract 5000 negative patches from background
images. We apply principle component analysis (PCA)
to preserve 95% total variation. The new data set has
a dimension of 93. We report the experimental results
in Table 5. From the table, pAUCEnsT demonstrates the
best performance on all three vision data sets.
Pedestrian detection
We evaluate the performance of our approach on the
pedestrian detection task. We train the pedestrian detector on the KITTI vision benchmark suite and
Caltech-USA pedestrian data set . The KITTI data set
was captured from two high resolution stereo camera
systems mounted to a vehicle. The vehicle was driven
around a mid-size city. All images are color and saved
in the portable network graphics (PNG) format. The
data set consists of 7481 training images and 7518 test
images. To obtain positive training data for the KITTI
data set, we crop 2111 fully visible pedestrians from 7481
training images. We expand the positive training data by
ﬂipping cropped pedestrian patches along the vertical
axis. Negative patches are collected from the KITTI
training set with pedestrians, cyclists and ‘don’t care’ regions cropped out. To train the pAUC-based pedestrian
detector, we set the resolution of the pedestrian model to
32×64 pixels. We extract visual features based on integral
channel features approach. We use ﬁve different types of
features: color (LUV), magnitude, orientation bins ,
the proposed sp-Cov and the proposed sp-LBP. We use
decision trees as weak learners and set the depth of
decision trees to be three. The regularization parameter ν
is cross-validated from {1, 2−1, · · · , 2−4} using the KITTI
training set (dividing the training set into training and
validation splits). For FPR range [α, β], we set the α to 0
and again choose the value of β from {1, 2−1, · · · , 2−4}
on the cross-validation data set. The pAUCEnsT detector
is repeatedly trained with three bootstrapping iterations
and the total number of negative samples collected is
around 33, 000. The ﬁnal classiﬁer consists of 2048 weak
Pedestrian (Easy)
Ours (65.3%)
DA−DPM (56.4%)
LSVM−MDPM−sv (47.7%)
LSVM−MDPM−us (45.5%)
mBoW (44.3%)
Pedestrian (Moderate)
Ours (54.5%)
DA−DPM (45.5%)
LSVM−MDPM−sv (39.4%)
LSVM−MDPM−us (38.4%)
mBoW (31.4%)
Pedestrian (Hard)
Ours (48.6%)
DA−DPM (41.1%)
LSVM−MDPM−sv (36.0%)
LSVM−MDPM−us (34.8%)
mBoW (30.6%)
Fig. 5: Precision-recall curves of our approach and state-of-the-art detectors (DA-PDM , LSVM-MDPM-sv , LSVM-MDPM-us and
mBoW ) on the KITTI pedestrian detection test set.
false positives per image
50.9% MultiFtr+Motion
48.5% MultiResC
48.4% Roerei
48.2% DBN−Mut
46.4% MF+Motion+2Ped
45.5% MOCO
44.2% ACF−Caltech
43.4% MultiResC+2Ped
40.5% MT−DPM
37.6% MT−DPM+Context
37.3% ACF+SDt
21.9% Ours
Fig. 6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt , MT-DPM+Context , MT-DPM , MultiResC+2Ped , ACF-Caltech , MOCO , MF+Motion+2Ped
 , DBN-Mut , Roerei , MultiResC , MultiFtr+Motion ,
ACF , HOG and VJ ) on the Caltech pedestrian test set.
classiﬁers. To obtain ﬁnal detection results, greedy nonmaxima suppression is applied with the default parameter as described in the Addendum of . We submit
our detection results to the KITTI benchmark website
and report the precision-recall curves of our detector
in Fig. 5. The proposed approach outperforms all existing
pedestrian detectors reported so far on the KITTI benchmark
Next, we evaluate our classiﬁer on the Caltech-USA
benchmark data set. The Caltech-USA data set was
collected from a video taken from a vehicle driving
through regular trafﬁc in an urban environment (greater
Los Angeles metropolitan area). Images were captured at
a resoltuion of 640 × 480 pixels at 30 frames per second.
Pedestrian are categorized into three scales. The near
scale includes pedestrians over 80 pixels, the medium
scale includes pedestrians at 30 −80 pixels and the
far scale includes pedestrian under 30 pixels. In this
paper, we use every 30th frame (starting with the 30th
Fig. 7: Left: Spatial distribution of features selected by pAUCEnsT.
White pixels indicate that a large number of low-level visual features
are selected in that area. These regions correspond to human head,
shoulders and feet. Right: The learned linear SVM model from the
BING classiﬁer. Each pixel shows the SVM weight. Note the similarity
between the learned SVM weights and SVM weights of HOG (Fig. 6b
in ), i.e., large SVM weights are near the head and shoulder contour
(∧-shape).
frame) of the Caltech dataset. The data is split into
training and test sets. For the positive training data,
we use 1631 cropped pedestrian patches extracted from
4250 training images. We exclude occluded pedestrians
from the Caltech training set . Pedestrian patches are
horizontally mirrored (ﬂipped along the vertical axis) to
expand the positive training data. Negative patches are
collected from the Caltech-USA training set with pedestrians cropped out. To train the pAUC-based pedestrian
detector, we set the resolution of the pedestrian model
to 32 × 64 pixels. We use six different types of features:
color (LUV), magnitude, orientation bins , histogram
of ﬂow6 , sp-Cov and sp-LBP. We set the depth of
decision trees, the number of bootstrapping iterations
and the number of weak classiﬁers to be the same as
in the previous experiment. We evaluate our pedestrian
detectors on the conditions that pedestrians are at least
50 pixels in height and at least 65% visible. We use the
publicly available evaluation software of Doll´ar et al. ,
We use the optical ﬂow implementation of which can be
downloaded at 
false positives per image
48.4% BING(thresh=0.032)
27.0% Ada+Cascade
23.9% BING(thresh=0.016)
21.9% BING(thresh=0.008)
21.9% BING(thresh=−0.032)
21.9% BING(thresh=0)
21.8% BING(thresh=0.004)
Fig. 8: The change in the detection performance as we vary the threshold value of the BING detector (evaluated on the Caltech pedestrian
test set). BING(thresh=0) represents the proposed two-stage pedestrian
detector, in which the ﬁrst stage is the BING classiﬁer with the
threshold value of zero and and the second stage is the pAUCEnsT
detector described in Section 3.3.
which computes the AUC from 9 discrete points sampled
between [0.01, 1.0] FPPI, to evaluate our experimental
results. Fig. 6 compares the performance of our approach
with other state-of-the-art algorithms.
On the Caltech data set, our approach outperforms all
existing pedestrian detectors by a large margin. Spatial distribution of selected visual features is shown in Fig. 7
(left). Each pixel is normalized such that that white pixels
indicate most frequently chosen regions. We observe
that active regions focus mainly on head, shoulders
and feet. Similar observation has also been reported in
 , in which the authors apply a multi-scale model for
pedestrian detection. The training time of our approach
is under 24 hours on a parallelized quad core Intel Xeon
processor.
Region proposals generation In this section, we train
a two-stage pedestrian detector by placing the efﬁcient
BING classiﬁer in the ﬁrst stage and the previously
trained pAUCEnsT pedestrian detector in the second
stage. To train the BING detector, the resolution of the
pedestrian model is set to 8 × 16 pixels. The learned
linear SVM model using BING features is shown in Fig. 7
(right). We observe that most active pixels (white pixels)
are near the human contour. The SVM weights shown
here are also similar to the learned SVM weights of
HOG (Fig. 6b in ). We compare the performance of our
two-stage pedestrian detector by varying the threshold
value of the BING detector in the ﬁrst stage (varying the
number of region proposals being generated). We plot
ROC curves of our detector with different BING threshold values in Fig. 8. From the ﬁgure, the performance
starts to drop as we increase the BING threshold value
(reducing the number of region proposals generated).
However we observe that setting the BING threshold
value in the range [−0.004, 0.008] results in similar pedestrian detection performance. This clearly demonstrates
that the BING detector can be applied to discard a large
number of background patches while retaining most
pedestrian patches. Table 6 compares the number of
region proposals discarded in the ﬁrst stage, log-average
miss rate and the average scanning time (excluding
feature extraction and post-processing computation, e.g.,
non-maximum suppression) of our two-stage detector by
varying the threshold value of the BING classiﬁer on the
Caltech-USA test set (640 × 480-pixel images). From the
table, setting the threshold value of the BING classiﬁer to
be 0.008 yields similar results to the original pAUCEnsT
detector while reducing the window scanning time by
half. We observe a slight improvement in the log-average
miss rate of 0.1% when we set the BING threshold value
to 0.004. We suspect that the BING detector might have
discarded a few difﬁcult-to-classify background patches
that the pAUCEnsT detector fails to classify.
Next we compare the performance and evaluation
time of our two-stage detector with a soft cascade .
For soft cascade, we train AdaBoost with a combination
of low-level visual features previously used. All other
experimental settings are kept the same (e.g., a number
of weak classiﬁers, a number of bootstrapping iterations,
post-processing computation, etc.). We heuristically set
the soft cascade’s rejection threshold at every node to
be {−160,−80,−40,−20,−10,−1}. The performance and
window scanning time of soft cascade with various
rejection thresholds is shown in Table 7. We observe
that the cascaded classiﬁer performs worse than our
two-stage detector (up to 5% worse in terms of the
log-average miss rate on the Caltech-USA benchmark).
Note that soft cascade (top row in Table 7) has a higher
window scanning time than our two-stage approach (top
row in Table 6). The reason is that, for soft cascade, the
partial sum of weak classiﬁers’ coefﬁcients is repeatedly
compared with the rejection threshold. This additional
comparison increases the window scanning time of soft
cascade when the rejection threshold is set to be small.
It is important to point out that our performance gain
comes at the cost of an increase in window scanning
time. For example, our detector achieves an average miss
rate of 23.9% with an average scan time of 2 seconds
per 640×480-pixel image while soft cascade achieves an
average miss rate of 27.1% with an average scan scan
time of 0.3 seconds per image.
CONCLUSION
In this paper, we have proposed an approach to
strengthen the effectiveness of low-level visual features
and formulated a new ensemble learning method for
object detection. The proposed approach is combined
with the efﬁcient proposal generation, which results
in the effective classiﬁer which optimizes the average
miss rate performance measure. Extensive experiments
demonstrate the effectiveness of the proposed approach
on both synthetic data and visual detection tasks. We
plan to explore the possibility of applying the proposed
approach to the multiple scales detector of in order to improve the detection results of low resolution
pedestrian images.
ACKNOWLEDGEMENTS
This work was in part supported by the Data to Decisions Cooperative Research Centre. C. Shen’s participation was in part supported by an Australian Research
Council Future Fellowship. C. Shen is the corresponding
TABLE 6: Proportion of windows rejected by tuning the threshold of
the BING classiﬁer
Avg. scan time
per image (secs)
TABLE 7: Log-average miss rate and evaluation time of various AdaBoost based pedestrian detectors with different soft cascade’s rejection
thresholds
Soft cascade’s
Avg. scan time
rejection threshold
per image (secs)
Convergence analysis of Algorithm 1
In this Appendix, we provide a theoretical analysis of the convergence property for the structured ensemble learning
in this paper.
The main result is as follows.
Proposition 3. At each iteration of Algorithm 1, the objective value decreases.
We assume that the current solution is a ﬁnite subset of weak learners and their corresponding coefﬁcients are
w. If at the next iteration one more different weak learner is added into the current weak learner subset, and we
re-solve the primal optimization problem, and the corresponding ˆw is zero, then the objective value and the solution
would be unchanged. If this happens, the current solution w is already the optimal solution—one is not able to
ﬁnd another weak learner to decrease the objective value.
Now if the corresponding ˆw is not zero, we have added one more free variable into the primal master problem,
and re-solving it must reduce the objective value.
With the next proposition, we show that the convergence of Algorithm 1 is guaranteed.
Proposition 4. The decrease of objective value between iterations t −1 and t is not less than
φζ(ht:, π∗) −φζ(ht:, π⋆
[t] = argmax
∆(α,β)(π, π∗) + w⊤
[t]φζ(H, π)
and the subscript [t] denotes the index at iteration t.
Recall that the optimization problem we want to solve is:
s.t. w⊤(φζ(H, π∗) −φζ(H, π)) ≥∆(α,β)(π, π∗) −ξ,
∀π ∈Πm,jβ and ξ ≥0.
(H+, H−) is the projected output for positive and negative training samples. φζ(H, π)
[φζ(h1:, π), · · · , φζ(hk:, π)] where φζ(ht:, π) : (Rm × Rn) × Πm,jβ →R and it is deﬁned as,
φζ(ht:, π) = 1
i ) −ℏt(x−
j=1 is any given subsets of negative instances and k = [k1, . . . , kjβ] is a vector indicating which elements
of S−are included.
At iteration t in Algorithm 1, the primal objective in Equation (19) can be reformulated into:
F(w[t]) = 1
[t],τ + ν ·
∆(α,β)(π, π∗) −w⊤
φζ(H, π∗) −φζ(H, π)
[t],τ + ν · max
∆(α,β)(π, π∗) + w⊤
[t]φζ(H, π)
[t]φζ(H, π∗),
where w[t] denotes the optimal solution at iteration t and w[t] =
w[t],1, w[t],2, · · · , w[t],t
Let us deﬁne
[t] = argmax
∆(α,β)(π, π∗) + w⊤
[t]φζ(H, π)
Now the objective function at iteration t is
F(w[t], π⋆
[t],τ + ν · ∆(α,β)(π⋆
[t], π∗) −w⊤
φζ(H, π∗) −φζ(H, π⋆
We know that π⋆
[t] is a sub-optimal maximization solution for iteration (t −1). Therefore the following inequality
must hold:
F(w[t−1]) −F(w[t]) = F(w[t−1], π⋆
[t−1]) −F(w[t], π⋆
[t]) ≥F(w[t−1], π⋆
[t]) −F(w[t], π⋆
Now with an arbitrary value ω, we know that
is a sub-optimal solution for iteration t. Here w[t−1] is the optimal solution for iteration t −1. The inequality in (24)
continues as
F(w[t−1]) −F(w[t]) = · · · ≥· · · ≥F(w[t−1], π⋆
[t]) −F( ˜w[t], π⋆
With the above deﬁnition (23), We can greatly simplify (25), which is
r.h.s. of (25) = −1
φζ(ht:, π∗) −φζ(ht:, π⋆
In summary, the objective decrease is lower bounded as:
F(w[t−1]) −F(w[t]) ≥max
φζ(ht:, π∗) −φζ(ht:, π⋆
φζ(ht:, π∗) −φζ(ht:, π⋆
[t] is calculated by (22).
Note that, since the structured ensemble learning method in , the analysis here can be easily adapted so that
it applies to .