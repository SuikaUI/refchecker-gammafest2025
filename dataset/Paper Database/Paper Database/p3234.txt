Suggestive Annotation: A Deep Active Learning
Framework for Biomedical Image Segmentation
Lin Yang1, Yizhe Zhang1, Jianxu Chen1, Siyuan Zhang2, Danny Z. Chen1
1 Department of Computer Science and Engineering,
University of Notre Dame, Notre Dame, IN 46556, USA
2 Department of Biological Sciences, Harper Cancer Research Institute,
University of Notre Dame, Notre Dame, IN 46556, USA
Abstract. Image segmentation is a fundamental problem in biomedical
image analysis. Recent advances in deep learning have achieved promising results on many biomedical image segmentation benchmarks. However, due to large variations in biomedical images (diﬀerent modalities,
image settings, objects, noise, etc), to utilize deep learning on a new application, it usually needs a new set of training data. This can incur a
great deal of annotation eﬀort and cost, because only biomedical experts
can annotate eﬀectively, and often there are too many instances in images
(e.g., cells) to annotate. In this paper, we aim to address the following
question: With limited eﬀort (e.g., time) for annotation, what instances
should be annotated in order to attain the best performance? We present
a deep active learning framework that combines fully convolutional network (FCN) and active learning to signiﬁcantly reduce annotation eﬀort
by making judicious suggestions on the most eﬀective annotation areas.
We utilize uncertainty and similarity information provided by FCN and
formulate a generalized version of the maximum set cover problem to
determine the most representative and uncertain areas for annotation.
Extensive experiments using the 2015 MICCAI Gland Challenge dataset
and a lymph node ultrasound image segmentation dataset show that, using annotation suggestions by our method, state-of-the-art segmentation
performance can be achieved by using only 50% of training data.
Introduction
Image segmentation is a fundamental task in biomedical image analysis. Recent
advances in deep learning have achieved promising results on many
biomedical image segmentation benchmarks . Due to its accuracy and generality, deep learning has become a main choice for image segmentation. But,
despite its huge success in biomedical applications, deep learning based segmentation still faces a critical obstacle: the diﬃculty in acquiring suﬃcient training
data due to high annotation eﬀorts and costs. Comparing to applications in
natural scene images, it is much harder to acquire training data in biomedical applications for two main reasons. (1) Only trained biomedical experts can
annotate data, which makes crowd leveraging quite diﬃcult. (2) Biomedical images often contain much more object instances than natural scene images, which
 
Apply FCNs
representative
Request new
annotation
Suggest annotation
Train FCNs
Unannotated samples
Add into annotated samples
Fig. 1. Illustrating our overall deep active learning framework.
can incur extensive manual eﬀorts of annotation. For example, public datasets
in biomedical areas have signiﬁcantly fewer spatial annotated images (85 for
MICCAI Gland Challenge ; 30 for ISBI EM Challenge ).
To alleviate the common burden of manual annotation, an array of weakly
supervised segmentation algorithms has been proposed. However, they did
not address well the question that which data samples should be selected for
annotation for high quality performance. Active learning , which allows the
learning model to choose training data, provided a way to answer this need.
As shown in , using active learning, state-of-the-art level performance can
be achieved using signiﬁcantly less training data in natural scene image segmentation. But, this method is based on the pre-trained region proposal model
and pre-trained image descriptor network, which cannot be easily acquired in
biomedical image settings due to large variations in biomedical applications.
In this paper, we present a new framework that combines fully convolutional
network (FCN) and active learning to reduce annotation eﬀort by making judicious suggestions on the most eﬀective annotation areas. To address the
issues in , we exploit FCN to obtain domain speciﬁc image descriptor and
directly generate segmentation without using region proposals. Fig. 1 outlines
the main ideas and steps of our deep active learning framework. Starting with
very little training data, we iteratively train a set of FCNs. At the end of each
stage, we extract useful information (such as uncertainty estimation and similarity estimation) from these FCNs to decide what will be the next batch of
images to annotate. After acquiring the new annotation data, the next stage is
started using all available annotated images. Although the above process seems
straightforward, we need to overcome several challenges in order to integrate
FCNs into this deep active learning framework, as discussed below.
Challenges from the perspective of FCNs. (1) The FCNs need to be fast
to train, so that the time interval between two annotation stages is acceptable.
(2) They need to be of good generality, in order to produce reasonable results
when little training data is available. To make the model fast to train, we utilize
the ideas of batch normalization and residual networks . Then, we use
Conv 3×3, 32
Conv 3×3, 32
Max pooling
Max pooling
Bottleneck 64
Bottleneck 64
Max pooling
Bottleneck 128
Bottleneck 128
Max pooling
Bottleneck 256
Bottleneck 256
Max pooling
Bottleneck 256
Bottleneck 256
Max pooling
Bottleneck 256
Bottleneck 256
Conv 3×3, # classes
Conv 1×1, # classes
1024d image
descriptor
Conv 1×1, NC
Conv 3×3, NC
Conv 1×1, 4×NC
Bottleneck with NC
up-convolution
channel-wise mean
All convolution layers are
followed by BN and ReLU
BN: batch normalization
Fig. 2. Illustrating the detailed structure of our FCN components.
bottleneck design to signiﬁcantly reduce the number of parameters (for better
generality) while maintaining a similar number of feature channels as in .
Challenges from the perspective of active learning. It needs to exploit
well the information provided by the FCNs when determining the next batch of
training data. For this, we ﬁrst demonstrate how to estimate uncertainty of
the FCNs based on the idea of bootstrapping and how to estimate similarity
between images by using the ﬁnal layer of the encoding part of the FCNs. Based
on such information, we formulate a generalized version of the maximum set
cover problem for suggesting the next batch of training data.
Experiments using the 2015 MICCAI Gland Challenge dataset and a
lymph node ultrasound image segmentation dataset show that (1) annotation suggestions by our framework are more eﬀective than common methods
such as random query and uncertainty query, and (2) our framework can achieve
state-of-the-art segmentation performance by using only 50% of training data.
Our proposed method consists of three major components: (1) a new FCN, which
shows state-of-the-art performance on the two datasets used in our experiments;
(2) uncertainty estimation and similarity estimation of the FCNs; (3) an annotation suggestion algorithm for selecting the most eﬀective training data.
A new fully convolutional network
Based on recent advances of deep neural network structures such as batch normalization and residual networks , we carefully design a new FCN that has
better generality and is faster to train.
Fig. 2 shows the detailed structure of our new FCN. Its encoding part largely
follows the structure of DCAN . As shown in both residual networks and
batch normalization , a model with these modiﬁcations can achieve the same
accuracy with signiﬁcantly fewer training steps comparing to its original version.
Fig. 3. (a) An original image; (b) the probability map produced by our FCNs for (a);
(c) uncertainty estimation of the result; (d) relation between uncertainty estimation
and pixel accuracy on the testing data. This shows that the test accuracy is highly
correlated with our uncertainty estimation.
This is essential when combining FCNs and active learning, since training FCNs
usually takes several hours before reaching a reasonable performance. Thus, we
change the original convolution layers into residual modules with batch normalization. Note that, at the start of active learning, since only few training
samples are available, having too many free parameters can make the model
hard to train. Hence, we utilize the bottleneck design to reduce the number
of parameters while maintaining a similar number of feature channels at the end
of each residual module. In the decoding part of the network, we modify the
structure in to gradually enlarge the size of the feature maps to ensure a
smooth result. Finally, a 3 × 3 convolution layer and a 1 × 1 convolution layer
are applied to combine the feature maps from diﬀerent scales together. As the
experiments show, our new FCNs can achieve state-of-the-art performance when
all training data is used while still able to produce reasonable results when very
little training data is available.
Uncertainty estimation and similarity estimation
A straightforward strategy to ﬁnd the most “valuable” annotation areas is to
use uncertainty sampling, with the active learner querying the most uncertain
areas for annotation. However, since deep learning models tend to be uncertain
for similar types of instances, simply using uncertainty sampling will result in
duplicated selections of annotation areas. To avoid this issue, our method aims
to select not only uncertain but also highly representative samples (samples
that are similar to lots of other training samples). To achieve this goal, we need
to estimate the uncertainty of the results and measure the similarity between
images. In this section, we illustrate how to extract such information from FCNs.
Bootstrapping is a standard way for evaluating the uncertainty of learning
models. Its basic idea is to train a set of models while restricting each of them to
use a subset of the training data (generated by sampling with replacement) and
calculate the variance (disagreement) among these models. We follow this procedure to calculate the uncertainty of FCNs. Although the inner variance inside
each FCN can lead to overestimation of the variance, in practice, it can still provide a good estimation of the uncertainty. As shown in Fig. 3(d), the estimated
uncertainty for each pixel has a strong correlation with the testing errors. Thus,
Fig. 4. Illustrating similarity estimation: The 5 images on the right have the highest
similarity scores with respect to the leftmost images among all training images in .
selecting uncertain training samples can help FCNs to correct potential errors.
Finally, the overall uncertainty of each training sample is computed as the mean
uncertainty of its pixels.
CNN based image descriptor has helped produce good results in natural scene
images. The encoding part of FCN is naturally an CNN, and for an input image
Ii, the output of the last convolution layer in the encoding part can be viewed as
high level features If
i of Ii. Next, to eliminate shifting and rotation variances of
the image, we calculate the channel-wise mean of If
i to generate condensed features Ic
i as the domain-speciﬁc image descriptor. This approach has two advantages. (1) There is no need to train another separate image descriptor network.
(2) Because the FCNs are trying to compute the segmentation of the objects, Ic
contains rich and accurate shape information. Finally, we deﬁne the similarity estimation between two images Ii and Ij as: sim(Ii, Ij) = cosine similarity(Ic
Fig. 4 shows an example of the similarity estimation.
Annotation suggestion
To maximize the eﬀectiveness of the annotation data, the annotated areas are
desired to be typical or representative in terms of the following two properties. (1)
Uncertainty: The annotated areas need to be diﬃcult to segment for the network.
(2) Representativeness: The annotated areas need to bear useful characteristics
or features for as many unannotated images as possible. In this section, we show
how to suggest a set of areas for annotation that very well satisfy these two
properties, based on similarity estimation and uncertainty estimation.
In each annotation suggestion stage, among all unannotated images, Su, we
aim to select a subset of k images, Sa ⊆Su, that is both highly uncertain and
representative. Since uncertainty is a more important criterion, in step 1, images
with the top K (K > k) uncertainty scores are extracted and form a candidate
set Sc. In step 2, we ﬁnd Sa ⊆Sc that has the largest representativeness.
To formalize the representativeness of Sa for Su, we ﬁrst deﬁne the representativeness of Sa for an image Ix ∈Su as: f(Sa, Ix) = maxIi∈Sa sim(Ii, Ix), where
sim(·, ·) is the similarity estimation between Ii and Ix. Intuitively, Ix is represented by its most similar image in Sa, measured by the similarity sim(·, ·). Then,
we deﬁne the representativeness of Sa for Su as: F(Sa, Su) = P
Ij∈Su f(Sa, Ij),
which reﬂects how well Sa represents all the images in Su. By ﬁnding Sa ⊆Sc
that maximizes F(Sa, Su), we promote Sa by (1) selecting k “hub” images that
Table 1. Comparison with full training data for gland segmentation.
ObjectDice
ObjectHausdorﬀ
Our method
Multichannel 
Multichannel 
CUMedVision 
Table 2. Results for lymph node ultrasound image segmentation.
Mean IU F1 score
Mean IU F1 score
U-Net 
Uncertainty 50%
CUMedNet 
Our method 50%
CFS-FCN 
Our method full
are similar to many unannotated images and (2) covering diverse cases (since
adding annotation to the same case does not signiﬁcantly increase F(Sa, Su)).
Finding Sa ⊆Sc with k images that maximizes F(Sa, Su) can be formulated
as a generalized version of the maximum set cover problem , as follows. We
ﬁrst show when sim(·, ·) ∈{0, 1}, the problem is an instance of the maximum
set cover problem. For each image Ii ∈Sc, Ii covers a subset SIi ⊆Su, where
Iy ∈SIi if and only if sim(Ii, Iy) = 1. Further, since sim(·, ·) ∈{0, 1}, for any
Ix ∈Su, f(Sa, Ix) is either 1 (covered) or 0 (not covered) and F(Sa, Su) (the
sum of f(Sa, Ix)’s) is the total number of the covered images (elements) in Su
by Sa. Thus, ﬁnding a k-images subset Sa ⊆Sc maximizing F(Sa, Su) becomes
ﬁnding a family F of k subsets from {SIi | Ii ∈Sc} such that ∪Sj∈FSj covers the
largest number of elements (images) in Su (max k-cover ). The maximum set
cover problem is NP-hard and its best possible polynomial time approximation
algorithm is a simple greedy method (iteratively choosing Si to cover the
largest number of uncovered elements). Since our problem is a generalization of
this problem (with sim(·, ·) ∈ , instead of sim(·, ·) ∈{0, 1}), our problem is
clearly NP-hard, and we adopt the same greedy method. Initially, Sa = ∅and
F(Sa, Su) = 0. Then, we iteratively add Ii ∈Sc that maximizes F(Sa ∪Ii, Su)
over Sa, until Sa contains k images. Note that, due to the max operation in
f(·, ·), adding an (almost) duplicated Ii does not increase F(Sa, Su) by much. It
is easy to show that this algorithm achieves an approximation ratio of 1 −1
Experiments and Results
To thoroughly evaluate our method on diﬀerent scenarios, we apply it to the
2015 MICCAI Gland Challenge dataset and a lymph node ultrasound image
segmentation dataset . The MICCAI data have 85 training images and 80
testing images (60 in Part A; 20 in Part B). The lymph node data have 37
% Used data
% Used data
% Used data
% Used data
Fig. 5. Comparison using limited training data for gland segmentation: The black
curves are for the results of random query, the blue curves are for the results of uncertainty query, the red curves are for the results by our annotation suggestion, and the
dashed green lines are for the current state-of-the-art results using full training data.
training images and 37 testing images. In our experiments, we use k = 8, K = 16,
2000 training iterations, and 4 FCNs. The waiting time between two annotation
suggestion stages is 10 minutes on a workstation with 4 NVIDIA Telsa P100
GPU. We use 5% of training data as validation set to select the best model.
Gland segmentation. We ﬁrst evaluate our FCN module using full training data. As Table 1 shows, on the MICCAI dataset, our FCN module achieves
considerable improvement on 4 columns (∼2% better), while has very similar
performance on the other two (∼0.5% worse). Then, we evaluate the eﬀectiveness of our annotation suggestion method, as follows. To simulate the annotation
suggestion process, we reveal training annotation only when the framework suggests it. The annotation cost is calculated as the number of revealed pixels.
Once the annotation cost reaches a given budget, we stop providing more training data. In our experiment, we set this budget as 10%, 30%, and 50% of the
overall labeled pixels. We compare our method with (1) random query: randomly
requesting annotation before reaching the budget, and (2) uncertainty query: selecting annotation areas based only on uncertainty estimation (K = k). Fig. 5
summarizes the results. It shows that our annotation suggestion method is consistently better than random query and uncertainty query, and our framework
can achieve state-of-the-art performance using only 50% of the training data.
Lymph node segmentation. Table 2 summarizes the results on lymph
node segmentation. “Our method full” entry shows the results of our FCN using
all training data. “Our method 50%” and “Uncertainty 50%” entries show the
comparison between uncertainty query and our annotation suggestion method
under the 50% budget. It shows that our framework achieves better performance
in all cases. By using 50% of the training data, our framework attains better
segmentation performance than the state-of-the-art method .
Conclusions
In this paper, we presented a new deep active learning framework for biomedical
image segmentation by combining FCNs and active learning. Our new method
provides two main contributions: (1) A new FCN model that attains state-ofthe-art segmentation performance; (2) an annotation suggestion approach that
can direct manual annotation eﬀorts to the most eﬀective annotation areas.
Acknowledgment. This research was supported in part by NSF Grants CCF-
1217906, CNS-1629914, CCF-1617735, CCF-1640081, NIH Grant 5R01CA194697-
03, and the Nanoelectronics Research Corporation, a wholly-owned subsidiary of
the Semiconductor Research Corporation, through Extremely Energy Eﬃcient
Collective Electronics, an SRC-NRI Nanoelectronics Research Initiative under
Research Task ID 2698.005.