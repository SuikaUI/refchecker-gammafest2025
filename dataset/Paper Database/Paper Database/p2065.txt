Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 567–573
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
 
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 567–573
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
 
Data Augmentation for Low-Resource Neural Machine Translation
Marzieh Fadaee
Arianna Bisazza
Christof Monz
Informatics Institute, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{m.fadaee,a.bisazza,c.monz}@uva.nl
The quality of a Neural Machine Translation system depends substantially on the
availability of sizable parallel corpora. For
low-resource language pairs this is not the
case, resulting in poor translation quality.
Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency
words by generating new sentence pairs
containing rare words in new, synthetically
created contexts. Experimental results on
simulated low-resource settings show that
our method improves translation quality
by up to 2.9 BLEU points over the baseline
and up to 3.2 BLEU over back-translation.
Introduction
In computer vision, data augmentation techniques
are widely used to increase robustness and improve learning of objects with a limited number of
training examples. In image processing the training data is augmented by, for instance, horizontally ﬂipping, random cropping, tilting, and altering the RGB channels of the original images
 .
Since the content of the new image is still the
same, the label of the original image is preserved
(see top of Figure 1). While data augmentation
has become a standard technique to train deep networks for image processing, it is not a common
practice in training networks for NLP tasks such
as Machine Translation.
Neural Machine Translation (NMT) is a sequence-to-sequence architecture
where an encoder builds up a representation of the
source sentence and a decoder, using the previous
A boy is holding a bat.
A boy is holding a backpack.
A boy is holding a bat.
A boy is holding a bat.
Ein Junge hält einen Schläger.
Ein Junge hält einen Rucksack.
computer vision
augmentation
translation
augmentation
Figure 1: Top: ﬂip and crop, two label-preserving
data augmentation techniques in computer vision.
Bottom: Altering one sentence in a parallel corpus
requires changing its translation.
LSTM hidden states and an attention mechanism,
generates the target translation.
To train a model with reliable parameter estimations, these networks require numerous instances
of sentence translation pairs with words occurring
in diverse contexts, which is typically not available in low-resource language pairs. As a result
NMT falls short of reaching state-of-the-art performances for these language pairs . The solution is to either manually annotate
more data or perform unsupervised data augmentation. Since manual annotation of data is timeconsuming, data augmentation for low-resource
language pairs is a more viable approach.
Recently Sennrich et al. proposed a method
to back-translate sentences from monolingual data
and augment the bitext with the resulting pseudo
parallel corpora.
In this paper, we propose a simple yet effective
approach, translation data augmentation (TDA),
that augments the training data by altering existing
sentences in the parallel corpus, similar in spirit to
the data augmentation approaches in computer vision (see Figure 1). In order for the augmentation
process in this scenario to be label-preserving, any
change to a sentence in one language must pre-
serve the meaning of the sentence, requiring sentential paraphrasing systems which are not available for many language pairs. Instead, we propose
a weaker notion of label preservation that allows to
alter both source and target sentences at the same
time as long as they remain translations of each
While our approach allows us to augment
data in numerous ways, we focus on augmenting instances involving low-frequency words, because the parameter estimation of rare words is
challenging, and further exacerbated in a lowresource setting. We simulate a low-resource setting as done in the literature and obtain substantial improvements for translating EnglishÑGerman and
GermanÑEnglish.
Translation Data Augmentation
Given a source and target sentence pair (S,T), we
want to alter it in a way that preserves the semantic
equivalence between S and T while diversifying as
much as possible the training examples. A number
of ways to do this can be envisaged, as for example
paraphrasing (parts of) S or T. Paraphrasing, however, is by itself a difﬁcult task and is not guaranteed to bring useful new information into the training data. We choose instead to focus on a subset of
the vocabulary that we know to be poorly modeled
by our baseline NMT system, namely words that
occur rarely in the parallel corpus. Thus, the goal
of our data augmentation technique is to provide
novel contexts for rare words. To achieve this we
search for contexts where a common word can be
replaced by a rare word and consequently replace
its corresponding word in the other language by
that rare word’s translation:
original pair
augmented pair
S : s1, ..., si, ..., sn
S1 : s1, ..., s1
i, ..., sn
T : t1, ..., tj, ..., tm
T 1 : t1, ..., t1
j, ..., tm
where tj is a translation of si and word-aligned to
si. Plausible substitutions are those that result in a
ﬂuent and grammatical sentence but do not necessarily maintain its semantic content. As an example, the rare word motorbike can be substituted in
different contexts:
Sentence [original / substituted]
My sister drives a [car / motorbike]
My uncle sold his [house / motorbike]
Alice waters the [plant / motorbike]
no (semantics)
John bought two [shirts / motorbike]
no (syntax)
Implausible substitutions need to be ruled out during data augmentation. To this end, rather than relying on linguistic resources which are not available for many languages, we rely on LSTM language models (LM) trained on large
amounts of monolingual data in both forward and
backward directions.
Our data augmentation method involves the following steps:
Targeted words selection:
Following common
practice, our NMT system limits its vocabulary V
to the v most common words observed in the training corpus. We select the words in V that have
fewer than R occurrences and use this as our targeted rare word list VR.
Rare word substitution:
If the LM suggests a
rare substitution in a particular context, we replace
that word and add the new sentence to the training
data. Formally, given a sentence pair pS, Tq and a
position i in S we compute the probability distribution over V by the forward and backward LMs
and select rare word substitutions C as follows:
i P VR : topK PForwardLMSps1
i P VR : topK PBackwardLMSps1
i P ÝÑC ^ s1
where topK returns the K words with highest conditional probability according to the context. The
selected substitutions s1
i, are used to replace the
original word and generate a new sentence.
Translation selection:
Using automatic word
alignments1 trained over the bitext, we replace the
translation of word si in T by the translation of its
substitution s1
i. Following a common practice in
statistical MT, the optimal translation t1
j is chosen
by multiplying direct and inverse lexical translation probabilities with the LM probability of the
translation in context:
j “ arg max
tPtransps1
i | tqPpt | s1
iqPLMT pt | tj´1
If no translation candidate is found because the
word is unaligned or because the LM probability
1We use fast-align to extract word
alignments and a bilingual lexicon with lexical translation
probabilities from the low-resource bitext.
is less than a certain threshold, the augmented sentence is discarded. This reduces the risk of generating sentence pairs that are semantically or syntactically incorrect.
We loop over the original parallel
corpus multiple times, sampling substitution positions, i, in each sentence and making sure that
each rare word gets augmented at most N times so
that a large number of rare words can be affected.
We stop when no new sentences are generated in
one pass of the training data.
Table 1 provides some examples resulting from
our augmentation procedure. While using a large
LM to substitute words with rare words mostly results in grammatical sentences, this does not mean
that the meaning of the original sentence is preserved. Note that meaning preservation is not an
objective of our approach.
Two translation data augmentation (TDA) setups are considered: only one word per sentence
can be replaced (TDAr“1), or multiple words per
sentence can be replaced, with the condition that
any two replaced words are at least ﬁve positions
apart (TDArě1). The latter incurs a higher risk
of introducing noisy sentences but has the potential to positively affect more rare words within the
same amount of augmented data. We evaluate both
setups in the following section.
En: I had been told that you would [not / voluntarily] be
speaking today.
De: mir wurde signalisiert, sie w¨urden heute [nicht / freiwillig] sprechen.
En: the present situation is [indefensible / confusing] and
completely unacceptable to the commission.
De: die situation sei [unhaltbar / verwirrend] und f¨ur die
kommission g¨anzlich unannehmbar.
En: ... agree wholeheartedly with the institution of an ad
hoc delegation of parliament on the turkish [prison /
missile] system.
De: ... ad-hoc delegation des parlaments f¨ur das regime
in den t¨urkischen [gef¨angnissen / ﬂugwaffen] voll und
ganz zustimmen.
Table 1: Examples of augmented data with highlighted [original / substituted] and [original /
translated] words.
Evaluation
In this section we evaluate the utility of our approach in a simulated low-resource NMT scenario.
Data and experimental setup
To simulate a low-resource setting we randomly
sample 10% of the EnglishØGerman WMT15
training data and report results on newstest 2014,
2015, and 2016 . For reference
we also provide the result of our baseline system
on the full data.
As NMT system we use a 4-layer attentionbased encoder-decoder model as described in trained with hidden dimension
1000, batch size 80 for 20 epochs. In all experiments the NMT vocabulary is limited to the most
common 30K words in both languages. Note that
data augmentation does not introduce new words
to the vocabulary. In all experiments we preprocess source and target language data with Bytepair encoding (BPE) using
30K merge operations. In the augmentation experiments BPE is performed after data augmentation.
For the LMs needed for data augmentation, we
train 2-layer LSTM networks in forward and backward directions on the monolingual data provided
for the same task (3.5B and 0.9B tokens in English and German respectively) with embedding
size 64 and hidden size 128. We set the rare word
threshold R to 100, top K words to 1000 and maximum number N of augmentations per rare word
to 500. In all experiments we use the English LM
for the rare word substitutions, and the German
LM to choose the optimal word translation in context. Since our approach is not label preserving we
only perform augmentation during training and do
not alter source sentences during testing.
We also compare our approach to Sennrich et al.
 by back-translating monolingual data and
adding it to the parallel training data. Speciﬁcally,
we back-translate sentences from the target side of
WMT’15 that are not included in our low-resource
baseline with two settings: keeping a one-to-one
ratio of back-translated versus original data (1 : 1)
following the authors’ suggestion, or using three
times more back-translated data (3 : 1).
We measure translation quality by singlereference case-insensitive BLEU computed with the multi-bleu.perl
script from Moses.
All translation results are displayed in Table 2.
As expected, the low-resource baseline performs
much worse than the full data system, re-iterating
Full data (ceiling)
Back-translation1:1 731K 11.4 (+0.8)Ĳ
12.2 (+0.9)Ĳ
14.6 (+1.5)Ĳ
9.0 (+0.8)Ĳ
10.4 (+1.2)Ĳ
12.0 (+1.0)Ĳ
Back-translation3:1 1.5M 11.2 (+0.6)
11.2 (–0.1)
13.3 (+0.2)
7.8 (–0.4)
9.4 (+0.2)
10.7 (–0.3)
4.5M 11.9 (+1.3)Ĳ,-
13.4 (+2.1)Ĳ,Ĳ 15.2 (+2.1)Ĳ,Ĳ
10.4 (+2.2)Ĳ,Ĳ 11.2 (+2.0)Ĳ,Ĳ
13.5 (+2.5)Ĳ,Ĳ
6M 12.6 (+2.0)Ĳ,Ĳ
13.7 (+2.4)Ĳ,Ĳ 15.4 (+2.3)Ĳ,Ĳ
10.7 (+2.5)Ĳ,Ĳ 11.5 (+2.3)Ĳ,Ĳ
13.9 (+2.9)Ĳ,Ĳ
Oversampling
6M 11.9 (+1.3)Ĳ,-
12.9 (+1.6)Ĳ,Ÿ 15.0 (+1.9)Ĳ,-
9.7 (+1.5)Ĳ,Ÿ 10.7 (+1.5)Ĳ,-
12.6 (+1.6)Ĳ,-
Table 2: Translation performance (BLEU) on German-English and English-German WMT test sets in a simulated low-resource setting. Back-translation refers to the work
of Sennrich et al. . Statistically signiﬁcant improvements are marked Ĳ at the p ă .01 and Ÿ at the
p ă .05 level, with the ﬁrst superscript referring to baseline and the second to back-translation1:1.
the importance of sizable training data for NMT.
Next we observe that both back-translation and
our proposed TDA method signiﬁcantly improve
translation quality. However TDA obtains the best
results overall and signiﬁcantly outperforms backtranslation in all test sets. This is an important
ﬁnding considering that our method involves only
minor modiﬁcations to the original training sentences and does not involve any costly translation
process. Improvements are consistent across both
translation directions, regardless of whether rare
word substitutions are ﬁrst applied to the source
or to the target side.
We also observe that altering multiple words in
a sentence performs slightly better than altering
only one. This indicates that addressing more rare
words is preferable even though the augmented
sentences are likely to be noisier.
To verify that the gains are actually due to the
rare word substitutions and not just to the repetition of part of the training data, we perform a ﬁnal experiment where each sentence pair selected
for augmentation is added to the training data unchanged (Oversampling in Table 2). Surprisingly,
we ﬁnd that this simple form of sampled data
replication outperforms both baseline and backtranslation systems,2 while TDArě1 remains the
best performing system overall.
We also observe that the system trained on augmented data tends to generate longer translations.
Averaging on all test sets, the length of translations
generated by the baseline is 0.88 of the average
reference length, while for TDAr“1 and TDArě1
it is 0.95 and 0.94, respectively. We attribute this
effect to the ability of the TDA-trained system to
generate translations for rare words that were left
2Note that this effect cannot be achieved by simply continuing the baseline training for up to 50 epochs.
untranslated by the baseline system.
Analysis of the Results
A desired effect of our method is to increase the
number of correct rare words generated by the
NMT system at test time.
To examine the impact of augmenting the training data by creating contexts for rare words on
the target side, Table 3 provides an example for
GermanÑEnglish translation.
We see that the
baseline model is not able to generate the rare
word centimetres as a correct translation of the
German word zentimeter . However, this word is
not rare in the training data of the TDArě1 model
after augmentation and is generated during translation. Table 3 also provides several instances of
augmented training sentences targeting the word
centimetres.
Note that even though some augmented sentences are nonsensical (e.g. the speed
limit is ﬁve centimetres per hour), the NMT system still beneﬁts from the new context for the rare
word and is able to generate it during testing.
Figure 2 demonstrates that this is indeed the
case for many words: the number of rare words
occurring in the reference translation (VR X Vref)
is three times larger in the TDA system output
than in the baseline output. One can also see that
this increase is a direct effect of TDA as most
of the rare words are not ‘rare’ anymore in the
augmented data, i.e., they were augmented sufﬁciently many times to occur more than 100 times
(see hatched pattern in Figure 2). Note that during
the experiments we did not use any information
from the evaluation sets.
To gauge the impact of augmenting the contexts for rare words on the source side, we examine normalized attention scores of these words
before and after augmentation. When translating
der tunnel hat einen querschnitt von 1,20 meter h¨ohe und 90 zentimeter breite .
Baseline translation the wine consists of about 1,20 m and 90 of the canal .
TDArě1 translation the tunnel has a UNK measuring meters 1.20 metres high and 90 centimetres wide .
the tunnel has a cross - section measuring 1.20 metres high and 90 centimetres across .
Examples of
‚ the average speed of cars and buses is therefore around 20 [kilometres / centimetres] per hour .
augmented data
‚ grab crane in special terminals for handling capacities of up to 1,800 [tonnes / centimetres] per hour .
for the word
‚ all suites and rooms are very spacious and measure between 50 and 70 [m / centimetres]
centimetres
‚ all we have to do is lower the speed limit everywhere to ﬁve [kilometers / centimetres] per hour .
Table 3: An example from newstest2014 illustrating the effect of augmenting rare words on generation
during test time. The translation of the baseline does not include the rare word centimetres, however, the
translation of our TDA model generates the rare word and produces a more ﬂuent sentence. Instances of
the augmentation of the word centimetres in training data are also provided.
Words in VR X Vref generated during translation
Words in VR X Vref not generated during translation
Words in VR X Vref affected by augmentation
Figure 2: Effect of TDA on the number of unique
rare words generated during DeÑEn translation.
VR is the set of rare words targeted by TDArě1
and Vref the reference translation vocabulary.
EnglishÑGerman with our TDA model, the attention scores for rare words on the source side
are on average 8.8% higher than when translating
with the baseline model. This suggests that having more accurate representations of rare words
increases the model’s conﬁdence to attend to these
words when encountered during test time.
En: registered users will receive the UNK newsletter free
[of / yearly] charge.
De: registrierte user erhalten zudem regelm¨aßig [den /
j¨ahrlich] markenticker newsletter.
En: the personal contact is [essential / entrusted] to us
De: pers¨onliche kontakt ist uns sehr [wichtig / betraut]
Table 4: Examples of incorrectly augmented data
with highlighted [original / substituted] and [original / translated] words.
Finally Table 4 provides examples of cases
where augmentation results in incorrect sentences.
In the ﬁrst example, the sentence is ungrammatical after substitution (of / yearly), which can be the
result of choosing substitutions with low probabilities from the English LM topK suggestions.
Errors can also occur during translation selection, as in the second example where betraut is an
acceptable translation of entrusted but would require a rephrasing of the German sentence to be
grammatically correct. Problems of this kind can
be attributed to the German LM, but also to the
lack of a more suitable translation in the lexicon
extracted from the bitext. Interestingly, this noise
seems to affect NMT only to a limited extent.
Conclusion
We have proposed a simple but effective approach to augment the training data of Neural
Machine Translation for low-resource language
By leveraging language models trained
on large amounts of monolingual data, we generate new sentence pairs containing rare words
in new, synthetically created contexts. We show
that this approach leads to generating more rare
words during translation and, consequently, to
higher translation quality.
In particular we report substantial improvements in simulated lowresource EnglishÑGerman and GermanÑEnglish
settings, outperforming another recently proposed
data augmentation technique.
Acknowledgments
Netherlands Organization for Scientiﬁc Research
(NWO) under project numbers 639.022.213 and
639.021.646, and a Google Faculty Research
Award. We also thank NVIDIA for their hardware
support, Ke Tran for providing the neural machine
translation baseline system, and the anonymous
reviewers for their helpful comments.