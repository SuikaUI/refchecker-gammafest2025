CutMix: Regularization Strategy to Train Strong Classiﬁers
with Localizable Features
Sangdoo Yun1
Dongyoon Han1
Seong Joon Oh2
Sanghyuk Chun1
Junsuk Choe1,3
Youngjoon Yoo1
1Clova AI Research, NAVER Corp.
2Clova AI Research, LINE Plus Corp.
3Yonsei University
Regional dropout strategies have been proposed to enhance the performance of convolutional neural network
classiﬁers. They have proved to be effective for guiding
the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby
letting the network generalize better and have better object localization capabilities. On the other hand, current
methods for regional dropout remove informative pixels on
training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefﬁciency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed
proportionally to the area of the patches. By making ef-
ﬁcient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CI-
FAR and ImageNet classiﬁcation tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained
ImageNet classiﬁer, when used as a pretrained model, results in consistent performance gains in Pascal detection
and MS-COCO image captioning benchmarks. We also
show that CutMix improves the model robustness against
input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at 
1. Introduction
Deep convolutional neural networks (CNNs) have shown
promising performances on various computer vision problems such as image classiﬁcation , object de-
Mixup 
Cutout 
Pascal VOC
Table 1: Overview of the results of Mixup, Cutout, and
our CutMix on ImageNet classiﬁcation, ImageNet localization, and Pascal VOC 07 detection (transfer learning with
SSD ﬁnetuning) tasks. Note that CutMix signiﬁcantly
improves the performance on various tasks.
tection , semantic segmentation , and video
analysis . To further improve the training efﬁciency
and performance, a number of training strategies have been
proposed, including data augmentation and regularization techniques .
In particular, to prevent a CNN from focusing too much
on a small set of intermediate activations or on a small region on input images, random feature removal regularizations have been proposed. Examples include dropout 
for randomly dropping hidden activations and regional
dropout for erasing random regions on
the input. Researchers have shown that the feature removal
strategies improve generalization and localization by letting
a model attend not only to the most discriminative parts of
objects, but rather to the entire object region .
 
While regional dropout strategies have shown improvements of classiﬁcation and localization performances to a
certain degree, deleted regions are usually zeroed-out or ﬁlled with random noise , greatly reducing the
proportion of informative pixels on training images. We recognize this as a severe conceptual limitation as CNNs are
generally data hungry . How can we maximally utilize
the deleted regions, while taking advantage of better generalization and localization using regional dropout?
We address the above question by introducing an augmentation strategy CutMix. Instead of simply removing
pixels, we replace the removed regions with a patch from
another image (See Table 1). The ground truth labels are
also mixed proportionally to the number of pixels of combined images. CutMix now enjoys the property that there is
no uninformative pixel during training, making training ef-
ﬁcient, while retaining the advantages of regional dropout
to attend to non-discriminative parts of objects. The added
patches further enhance localization ability by requiring the
model to identify the object from a partial view. The training and inference budgets remain the same.
CutMix shares similarity with Mixup which mixes
two samples by interpolating both the image and labels. While certainly improving classiﬁcation performance,
Mixup samples tend to be unnatural (See the mixed image
in Table 1). CutMix overcomes the problem by replacing
the image region with a patch from another training image.
Table 1 gives an overview of Mixup , Cutout ,
and CutMix on image classiﬁcation, weakly supervised localization, and transfer learning to object detection methods. Although Mixup and Cutout enhance ImageNet classi-
ﬁcation, they decrease the ImageNet localization or object
detection performances. On the other hand, CutMix consistently achieves signiﬁcant enhancements across three tasks.
We present extensive evaluations of CutMix on various
CNN architectures, datasets, and tasks. Summarizing the
key results, CutMix has signiﬁcantly improved the accuracy
of a baseline classiﬁer on CIFAR-100 and has obtained the
state-of-the-art top-1 error 14.47%. On ImageNet , applying CutMix to ResNet-50 and ResNet-101 has improved the classiﬁcation accuracy by +2.28% and +1.70%,
respectively. On the localization front, CutMix improves the
performance of the weakly-supervised object localization
(WSOL) task on CUB200-2011 and ImageNet 
by +5.4% and +0.9%, respectively. The superior localization capability is further evidenced by ﬁne-tuning a detector and an image caption generator on CutMix-ImageNetpretrained models; the CutMix pretraining has improved
the overall detection performances on Pascal VOC 
by +1 mAP and image captioning performance on MS-
COCO by +2 BLEU scores. CutMix also enhances the
model robustness and alleviates the over-conﬁdence issue
 of deep networks.
2. Related Works
Regional dropout: Methods removing random regions in images have been proposed to enhance the generalization performance of CNNs. Object localization methods also utilize the regional dropout techniques for
improving the localization ability of CNNs. CutMix is similar to those methods, while the critical difference is that
the removed regions are ﬁlled with patches from another
training images. DropBlock has generalized the regional
dropout to the feature space and have shown enhanced generalizability as well. CutMix can also be performed on the
feature space, as we will see in the experiments.
Synthesizing training data: Some works have explored
synthesizing training data for further generalizability. Generating new training samples by Stylizing ImageNet 
has guided the model to focus more on shape than texture, leading to better classiﬁcation and object detection performances. CutMix also generates new samples by cutting
and pasting patches within mini-batches, leading to performance boosts in many computer vision tasks; unlike stylization as in , CutMix incurs only negligible additional
cost for training. For object detection, object insertion methods have been proposed as a way to synthesize objects
in the background. These methods aim to train a good represent of a single object samples, while CutMix generates
combined samples which may contain multiple objects.
Mixup: CutMix shares similarity with Mixup in
that both combines two samples, where the ground truth label of the new sample is given by the linear interpolation
of one-hot labels. As we will see in the experiments, Mixup
samples suffer from the fact that they are locally ambiguous
and unnatural, and therefore confuses the model, especially
for localization. Recently, Mixup variants 
have been proposed; they perform feature-level interpolation and other types of transformations. Above works, however, generally lack a deep analysis in particular on the localization ability and transfer-learning performances. We
have veriﬁed the beneﬁts of CutMix not only for an image
classiﬁcation task, but over a wide set of localization tasks
and transfer learning experiments.
Tricks for training deep networks: Efﬁcient training of
deep networks is one of the most important problems in
computer vision community, as they require great amount
of compute and data. Methods such as weight decay,
dropout , and Batch Normalization are widely used
to efﬁciently train deep networks. Recently, methods adding
noises to the internal features of CNNs or adding
extra path to the architecture have been proposed to
enhance image classiﬁcation performance. CutMix is complementary to the above methods because it operates on the
data level, without changing internal representations or architecture.
We describe the CutMix algorithm in detail.
3.1. Algorithm
Let x ∈RW ×H×C and y denote a training image and
its label, respectively. The goal of CutMix is to generate a
new training sample (˜x, ˜y) by combining two training samples (xA, yA) and (xB, yB). The generated training sample
(˜x, ˜y) is used to train the model with its original loss function. We deﬁne the combining operation as
˜x = M ⊙xA + (1 −M) ⊙xB
˜y = λyA + (1 −λ)yB,
where M ∈{0, 1}W ×H denotes a binary mask indicating
where to drop out and ﬁll in from two images, 1 is a binary
mask ﬁlled with ones, and ⊙is element-wise multiplication.
Like Mixup , the combination ratio λ between two data
points is sampled from the beta distribution Beta(α, α). In
our all experiments, we set α to 1, that is λ is sampled from
the uniform distribution (0, 1). Note that the major difference is that CutMix replaces an image region with a patch
from another training image and generates more locally natural image than Mixup does.
To sample the binary mask M, we ﬁrst sample the
bounding box coordinates B = (rx, ry, rw, rh) indicating
the cropping regions on xA and xB. The region B in xA is
removed and ﬁlled in with the patch cropped from B of xB.
In our experiments, we sample rectangular masks M
whose aspect ratio is proportional to the original image. The
box coordinates are uniformly sampled according to:
rx ∼Unif (0, W) , rw = W
ry ∼Unif (0, H) , rh = H
making the cropped area ratio rwrh
W H = 1−λ. With the cropping region, the binary mask M ∈{0, 1}W ×H is decided
by ﬁlling with 0 within the bounding box B, otherwise 1.
In each training iteration, a CutMix-ed sample (˜x, ˜y)
is generated by combining randomly selected two training
samples in a mini-batch according to Equation (1). Codelevel details are presented in Appendix A. CutMix is simple
and incurs a negligible computational overhead as existing
data augmentation techniques used in ; we can efﬁciently utilize it to train any network architecture.
3.2. Discussion
What does model learn with CutMix? We have motivated
CutMix such that full object extents are considered as cues
for classiﬁcation, the motivation shared by Cutout, while
ensuring two objects are recognized from partial views in a
single image to increase training efﬁciency. To verify that
CutMix is indeed learning to recognize two objects from
‘St. Bernard’
Figure 1: Class activation mapping (CAM) visualizations on ‘Saint Bernard’ and ‘Miniature Poodle’ samples
using various augmentation techniques. From top to bottom rows, we show the original images, input augmented
image, CAM for class ‘Saint Bernard’, and CAM for class
‘Miniature Poodle’, respectively. Note that CutMix can take
advantage of the mixed region on image, but Cutout cannot.
Mixup Cutout CutMix
Usage of full image region
Regional dropout
Mixed image & label
Table 2: Comparison among Mixup, Cutout, and CutMix.
their respective partial views, we visually compare the activation maps for CutMix against Cutout and Mixup .
Figure 1 shows example augmentation inputs as well as
corresponding class activation maps (CAM) for two
classes present, Saint Bernard and Miniature Poodle. We
use vanilla ResNet-50 model1 for obtaining the CAMs to
clearly see the effect of augmentation method only.
We observe that Cutout successfully lets a model focus
on less discriminative parts of the object, such as the belly
of Saint Bernard, while being inefﬁcient due to unused pixels. Mixup, on the other hand, makes full use of pixels, but
introduces unnatural artifacts. The CAM for Mixup, as a result, shows that the model is confused when choosing cues
for recognition. We hypothesize that such confusion leads
to its suboptimal performance in classiﬁcation and localization, as we will see in Section 4.
CutMix efﬁciently improves upon Cutout by being able
to localize the two object classes accurately. We summarize
1We use ImageNet-pretrained ResNet-50 provided by PyTorch .
Figure 2: Top-1 test error plot for CIFAR100 (left) and ImageNet (right) classiﬁcation. Cutmix achieves lower test errors than the baseline at the end of training.
the key differences among Mixup, Cutout, and CutMix in
Analysis on validation error:
We analyze the effect
of CutMix on stabilizing the training of deep networks.
We compare the top-1 validation error during the training
with CutMix against the baseline. We train ResNet-50 
for ImageNet Classiﬁcation, and PyramidNet-200 for
CIFAR-100 Classiﬁcation. Figure 2 shows the results.
We observe, ﬁrst of all, that CutMix achieves lower validation errors than the baseline at the end of training. At
epoch 150 when the learning rates are reduced, the baselines suffer from overﬁtting with increasing validation error.
CutMix, on the other hand, shows a steady decrease in validation error; diverse training samples reduce overﬁtting.
4. Experiments
In this section, we evaluate CutMix for its capability to
improve localizability as well as generalizability of a trained
model on multiple tasks. We ﬁrst study the effect of Cut-
Mix on image classiﬁcation (Section 4.1) and weakly supervised object localization (Section 4.2). Next, we show
the transferability of a CutMix pre-trained model when it is
ﬁne-tuned for object detection and image captioning tasks
(Section 4.3). We also show that CutMix can improve the
model robustness and alleviate the model over-conﬁdence
in Section 4.4.
All experiments were implemented and evaluated on
NAVER Smart Machine Learning (NSML) platform
with PyTorch . Source code and pretrained models are
available at 
4.1. Image Classiﬁcation
ImageNet Classiﬁcation
We evaluate on ImageNet-1K benchmark , the dataset
containing 1.2M training images and 50K validation images of 1K categories. For fair comparison, we use the standard augmentation setting for ImageNet dataset such as resizing, cropping, and ﬂipping, as done in .
We found that regularization methods including Stochastic
ResNet-152*
ResNet-101 + SE Layer* 
ResNet-101 + GE Layer* 
ResNet-50 + SE Layer* 
ResNet-50 + GE Layer* 
ResNet-50 (Baseline)
ResNet-50 + Cutout 
ResNet-50 + StochDepth 
ResNet-50 + Mixup 
ResNet-50 + Manifold Mixup 
ResNet-50 + DropBlock* 
ResNet-50 + Feature CutMix
ResNet-50 + CutMix
Table 3: ImageNet classiﬁcation results based on ResNet-50
model. ‘*’ denotes results reported in the original papers.
ResNet-101 (Baseline) 
ResNet-101 + Cutout 
ResNet-101 + Mixup 
ResNet-101 + CutMix
ResNeXt-101 (Baseline) 
ResNeXt-101 + CutMix
Table 4: Impact of CutMix on ImageNet classiﬁcation for
ResNet-101 and ResNext-101.
Depth , Cutout , Mixup , and CutMix require a
greater number of training epochs till convergence. Therefore, we have trained all the models for 300 epochs with
initial learning rate 0.1 decayed by factor 0.1 at epochs
75, 150, and 225. The batch size is set to 256. The hyperparameter α is set to 1. We report the best performances of
CutMix and other baselines during training.
We brieﬂy describe the settings for baseline augmentation schemes. We set the dropping rate of residual blocks to
0.25 for the best performance of Stochastic Depth . The
mask size for Cutout is set to 112×112 and the location
for dropping out is uniformly sampled. The performance of
DropBlock is from the original paper and the difference
from our setting is the training epochs which is set to 270.
Manifold Mixup applies Mixup operation on the randomly chosen internal feature map. We have tried α = 0.5
and 1.0 for Mixup and Manifold Mixup and have chosen 1.0
which has shown better performances. It is also possible to
extend CutMix to feature-level augmentation (Feature Cut-
Mix). Feature CutMix applies CutMix at a randomly chosen
layer per minibatch as Manifold Mixup does.
Comparison against baseline augmentations:
are given in Table 3. We observe that CutMix achieves
PyramidNet-200 (˜α=240)
(# params: 26.8 M)
+ StochDepth 
+ Label smoothing (ϵ=0.1) 
+ Cutout 
+ Cutout + Label smoothing (ϵ=0.1)
+ DropBlock 
+ DropBlock + Label smoothing (ϵ=0.1)
+ Mixup (α=0.5) 
+ Mixup (α=1.0) 
+ Manifold Mixup (α=1.0) 
+ Cutout + Mixup (α=1.0)
+ Cutout + Manifold Mixup (α=1.0)
+ ShakeDrop 
+ CutMix + ShakeDrop 
Table 5: Comparison of state-of-the-art regularization methods on CIFAR-100.
the best result, 21.40% top-1 error, among the considered
augmentation strategies. CutMix outperforms Cutout and
Mixup, the two closest approaches to ours, by +1.53% and
+1.18%, respectively. On the feature level as well, we ﬁnd
CutMix preferable to Mixup, with top-1 errors 21.78% and
22.50%, respectively.
Comparison against architectural improvements:
have also compared improvements due to CutMix versus
architectural improvements (e.g. greater depth or additional
modules). We observe that CutMix improves the performance by +2.28% while increased depth (ResNet-50 →
ResNet-152) boosts +1.99% and SE and GE 
boosts +1.56% and +1.80%, respectively. Note that unlike above architectural boosts improvements due to Cut-
Mix come at little or memory or computational time.
CutMix for Deeper Models: We have explored the performance of CutMix for the deeper networks, ResNet-101 
and ResNeXt-101 (32×4d) , on ImageNet. As seen in
Table 4, we observe +1.60% and +1.71% respective improvements in top-1 errors due to CutMix.
CIFAR Classiﬁcation
We set mini-batch size to 64 and training epochs to 300.
The learning rate was initially set to 0.25 and decayed by
the factor of 0.1 at 150 and 225 epoch. To ensure the effectiveness of the proposed method, we used a strong baseline,
PyramidNet-200 with widening factor ˜α = 240. It has
26.8M parameters and achieves the state-of-the-art performance 16.45% top-1 error on CIFAR-100.
Table 5 shows the performance comparison against other
state-of-the-art data augmentation and regularization methods. All experiments were conducted three times and the
averaged best performances during training are reported.
PyramidNet-110 (˜α = 64) 
PyramidNet-110 + CutMix
ResNet-110 
ResNet-110 + CutMix
Table 6: Impact of CutMix on lighter architectures on
CIFAR-100.
PyramidNet-200 (˜α=240)
Top-1 Error (%)
+ Mixup (α=1.0)
+ Manifold Mixup (α=1.0)
Table 7: Impact of CutMix on CIFAR-10.
Hyper-parameter settings: We set the hole size of
Cutout to 16 × 16. For DropBlock , keep prob and
block size are set to 0.9 and 4, respectively. The drop
rate for Stochastic Depth is set to 0.25. For Mixup ,
we tested the hyper-parameter α with 0.5 and 1.0. For Manifold Mixup , we applied Mixup operation at a randomly
chosen layer per minibatch.
Combination of regularization methods: We have evaluated the combination of regularization methods. Both
Cutout and label smoothing does not improve the
accuracy when adopted independently, but they are effective when used together. Dropblock , the feature-level
generalization of Cutout, is also more effective when label smoothing is also used. Mixup and Manifold
Mixup achieve higher accuracies when Cutout is applied on input images. The combination of Cutout and
Mixup tends to generate locally separated and mixed samples since the cropped regions have less ambiguity than
those of the vanilla Mixup. The superior performance of
Cutout and Mixup combination shows that mixing via cutand-paste manner is better than interpolation, as much evidenced by CutMix performances.
CutMix achieves 14.47% top-1 classiﬁcation error on
CIFAR-100, +1.98% higher than the baseline performance
16.45%. We have achieved a new state-of-the-art performance 13.81% by combining CutMix and ShakeDrop ,
a regularization that adds noise on intermediate features.
CutMix for various models:
Table 6 shows CutMix
also signiﬁcantly improves the performance of the weaker
baseline architectures, such as PyramidNet-110 and
ResNet-110.
CutMix for CIFAR-10:
We have evaluated CutMix on
CIFAR-10 dataset using the same baseline and training setting for CIFAR-100. The results are given in Table 7. On
Figure 3: Impact of α and CutMix layer depth on CIFAR-
100 top-1 error.
PyramidNet-200 (˜α=240)
(# params: 26.8 M)
Proposed (CutMix)
Center Gaussian CutMix
Fixed-size CutMix
One-hot CutMix
Scheduled CutMix
Complete-label CutMix
Table 8: Performance of CutMix variants on CIFAR-100.
CIFAR-10, CutMix also enhances the classiﬁcation performances by +0.97%, outperforming Mixup and Cutout performances.
Ablation Studies
We conducted ablation study in CIFAR-100 dataset using
the same experimental settings in Section 4.1.2. We evaluated CutMix with α ∈{0.1, 0.25, 0.5, 1.0, 2.0, 4.0}; the
results are given in Figure 3, left plot. For all α values considered, CutMix improves upon the baseline (16.45%). The
best performance is achieved when α = 1.0.
The performance of feature-level CutMix is given in
Figure 3, right plot. We changed the layer on which Cut-
Mix is applied, from image layer itself to higher feature
levels. We denote the index as (0=image level, 1=after
ﬁrst conv-bn, 2=after layer1, 3=after layer2, 4=after layer3). CutMix achieves the best performance when
it is applied on the input images. Again, feature-level Cut-
Mix except the layer3 case improves the accuracy over
the baseline (16.45%).
We explore different design choices for CutMix. Table 8
shows the performance of CutMix variations. ‘Center Gaussian CutMix’ samples the box coordinates rx, ry of Equation (2) according to the Gaussian distribution with mean
at the image center, instead of the original uniform distribution. ‘Fixed-size CutMix’ ﬁxes the size of cropping region (rw, rh) at 16 × 16 (i.e. λ = 0.75). ‘Scheduled Cut-
Mix’ linearly increases the probability to apply CutMix as
CUB200-2011
Loc Acc (%)
Loc Acc (%)
VGG-GAP + CAM 
VGG-GAP + ACoL* 
VGG-GAP + ADL* 
GoogLeNet + HaS* 
InceptionV3 + SPG* 
VGG-GAP + Mixup 
VGG-GAP + Cutout 
VGG-GAP + CutMix
ResNet-50 + CAM 
ResNet-50 + Mixup 
ResNet-50 + Cutout 
ResNet-50 + CutMix
Table 9: Weakly supervised object localization results on
CUB200-2011 and ImageNet. * denotes results reported in
the original papers.
training progresses, as done by , from 0 to 1. ‘Onehot CutMix’ decides the mixed target label by committing
to the label of greater patch portion (single one-hot label),
rather than using the combination strategy in Equation (1).
‘Complete-label CutMix’ assigns the mixed target label as
˜y = 0.5yA + 0.5yB regardless of the combination ratio λ.
The results show that above variations lead to performance
degradation compared to the original CutMix.
4.2. Weakly Supervised Object Localization
Weakly supervised object localization (WSOL) task
aims to train the classiﬁer to localize target objects by using only the class labels. To localize the target well, it is
important to make CNNs extract cues from full object regions and not focus on small discriminant parts of the target.
Learning spatially distributed representation is thus the key
for improving performance on WSOL task. CutMix guides
a classiﬁer to attend to broader sets of cues to make decisions; we expect CutMix to improve WSOL performances
of classiﬁers. To measure this, we apply CutMix over baseline WSOL models. We followed the training and evaluation strategy of existing WSOL methods with
VGG-GAP and ResNet-50 as the base architectures. The
quantitative and qualitative results are given in Table 9 and
Figure 4, respectively. Full implementation details are in
Appendix B.
Comparison against Mixup and Cutout: CutMix outperforms Mixup on localization accuracies by +5.51%
and +1.41% on CUB200-2011 and ImageNet, respectively.
Mixup degrades the localization accuracy of the baseline
model; it tends to make a classiﬁer focus on small regions
as shown in Figure 4. As we have hypothesized in Sec-
ImageNet Cls
Top-1 Error (%)
Image Captioning
Faster-RCNN 
ResNet-50 (Baseline)
76.7 (+0.0)
75.6 (+0.0)
61.4 (+0.0)
22.9 (+0.0)
Mixup-trained
76.6 (-0.1)
73.9 (-1.7)
61.6 (+0.2)
23.2 (+0.3)
Cutout-trained
76.8 (+0.1)
75.0 (-0.6)
63.0 (+1.6)
24.0 (+1.1)
CutMix-trained
77.6 (+0.9)
76.7 (+1.1)
64.2 (+2.8)
24.9 (+2.0)
Table 10: Impact of CutMix on transfer learning of pretrained model to other tasks, object detection and image captioning.
Figure 4: Qualitative comparison of the baseline (ResNet-
50), Mixup, Cutout, and CutMix for weakly supervised object localization task on CUB-200-2011 dataset. Ground
truth and predicted bounding boxes are denoted as red and
green, respectively.
tion 3.2, more ambiguity in Mixup samples make a classiﬁer
focus on even more discriminative parts of objects, leading
to decreased localization accuracies. Although Cutout 
improves the accuracy over the baseline, it is outperformed
by CutMix: +2.03% and +0.56% on CUB200-2011 and
ImageNet, respectively.
CutMix also achieves comparable localization accuracies on CUB200-2011 and ImageNet, even when compared against the dedicated state-of-the-art WSOL methods that focus on learning spatially dispersed representations.
4.3. Transfer Learning of Pretrained Model
ImageNet pre-training is de-facto standard practice for
many visual recognition tasks. We examine whether Cut-
Mix pre-trained models leads to better performances in certain downstream tasks based on ImageNet pre-trained models. As CutMix has shown superiority in localizing less discriminative object parts, we would expect it to lead to boosts
in certain recognition tasks with localization elements, such
as object detection and image captioning. We evaluate the
boost from CutMix on those tasks by replacing the backbone network initialization with other ImageNet pre-trained
models using Mixup , Cutout , and CutMix. ResNet-
50 is used as the baseline architecture in this section.
Transferring to Pascal VOC object detection: Two popular detection models, SSD and Faster RCNN , are
considered. Originally the two methods have utilized VGG-
16 as backbones, but we have changed it to ResNet-50. The
ResNet-50 backbone is initialized with various ImageNetpretrained models and then ﬁne-tuned on Pascal VOC 2007
and 2012 trainval data. Models are evaluated on
VOC 2007 test data using the mAP metric. We follow the
ﬁne-tuning strategy of the original methods ; implementation details are in Appendix C. Results are shown in
Table 10. Pre-training with Cutout and Mixup has failed to
improve the object detection performance over the vanilla
pre-trained model. However, the pre-training with CutMix
improves the performance of both SSD and Faster-RCNN.
Stronger localizability of the CutMix pre-trained models
leads to better detection performances.
Transferring to MS-COCO image captioning: We used
Neural Image Caption (NIC) as the base model for image captioning experiments. We have changed the backbone
network of encoder from GoogLeNet to ResNet-50.
The backbone network is initialized with various ImageNet
pre-trained models, and then trained and evaluated on MS-
COCO dataset . Implementation details and evaluation
metrics (METEOR, CIDER, etc.) are in Appendix D. Table 10 shows the results. CutMix outperforms Mixup and
Cutout in both BLEU1 and BLEU4 metrics. Simply replacing backbone network with our CutMix pre-trained model
gives performance gains for object detection and image captioning tasks at no extra cost.
4.4. Robustness and Uncertainty
Many researches have shown that deep models are easily fooled by small and unrecognizable perturbations on the
input images, a phenomenon referred to as adversarial attacks . One straightforward way to enhance robustness and uncertainty is an input augmentation by generating unseen samples . We evaluate robustness and uncertainty improvements due to input augmentation methods
including Mixup, Cutout, and CutMix.
Robustness:
We evaluate the robustness of the trained
models to adversarial samples, occluded samples, and
in-between class samples. We use ImageNet pre-trained
ResNet-50 models with same setting as in Section 4.1.1.
Fast Gradient Sign Method (FGSM) is used to gen-
Top-1 Error (%)
Center occlusion
Top-1 Error (%)
Boundary occlusion
(a) Analysis for occluded samples
Combination ratio
Top-1 Error (%)
Mixup in-between class
Combination ratio
Top-1 Error (%)
Cutmix in-between class
(b) Analysis for in-between class samples
Figure 5: Robustness experiments on the ImageNet validation set.
Top-1 Acc (%)
Table 11: Top-1 accuracy after FGSM white-box attack on
ImageNet validation set.
erate adversarial perturbations and we assume that the adversary has full information of the models (white-box attack). We report top-1 accuracies after attack on ImageNet
validation set in Table 11. CutMix signiﬁcantly improves
the robustness to adversarial attacks compared to other augmentation methods.
For occlusion experiments, we generate occluded samples in two ways: center occlusion by ﬁlling zeros in a center hole and boundary occlusion by ﬁlling zeros outside of
the hole. In Figure 5a, we measure the top-1 error by varying the hole size from 0 to 224. For both occlusion scenarios, Cutout and CutMix achieve signiﬁcant improvements
in robustness while Mixup only marginally improves it. Interestingly, CutMix almost achieves a comparable performance as Cutout even though CutMix has not observed any
occluded sample during training unlike Cutout.
Finally, we evaluate the top-1 error of Mixup and CutMix
in-between samples. The probability to predict neither two
classes by varying the combination ratio λ is illustrated in
Figure 5b. We randomly select 50, 000 in-between samples
in ImageNet validation set. In both experiments, Mixup and
CutMix improve the performance while improvements due
to Cutout are almost negligible. Similarly to the previous
occlusion experiments, CutMix even improves the robustness to the unseen Mixup in-between class samples.
Uncertainty: We measure the performance of the out-ofdistribution (OOD) detectors proposed by which determines whether the sample is in- or out-of-distribution
by score thresholding. We use PyramidNet-200 trained on
CIFAR-100 datasets with same setting as in Section 4.1.2.
In Table 12, we report the averaged OOD detection performances against seven out-of-distribution samples from , including TinyImageNet, LSUN , uniform noise,
TNR at TPR 95%
Detection Acc.
11.8 (-14.5)
49.3 (-38.0)
60.9 (-21.0)
18.8 (-7.5)
68.7 (-18.6)
71.3 (-10.7)
69.0 (+42.7)
94.4 (+7.1)
89.1 (+7.1)
Table 12: Out-of-distribution (OOD) detection results with
CIFAR-100 trained models. Results are averaged on seven
datasets. All numbers are in percents; higher is better.
Gaussian noise, etc. More results are illustrated in Appendix E. Mixup and Cutout augmentations aggravate the
over-conﬁdence of the base networks. Meanwhile, CutMix
signiﬁcantly alleviates the over-conﬁdence of the model.
5. Conclusion
We have introduced CutMix for training CNNs with
strong classiﬁcation and localization ability. CutMix is easy
to implement and has no computational overhead, while being surprisingly effective on various tasks. On ImageNet
classiﬁcation, applying CutMix to ResNet-50 and ResNet-
101 brings +2.28% and +1.70% top-1 accuracy improvements. On CIFAR classiﬁcation, CutMix signiﬁcantly improves the performance of baseline by +1.98% leads to
the state-of-the-art top-1 error 14.47%. On weakly supervised object localization (WSOL), CutMix substantially enhances the localization accuracy and has achieved comparable localization performances as the state-of-the-art WSOL
methods. Furthermore, simply using CutMix-ImageNetpretrained model as the initialized backbone of the object
detection and image captioning brings overall performance
improvements. Finally, we have shown that CutMix results
in improvements in robustness and uncertainty of image
classiﬁers over the vanilla model as well as other regularized models.
Acknowledgement
We would like to thank Clova AI Research team, especially Jung-Woo Ha and Ziad Al-Halah for their helpful
feedback and discussion.