Learning Visual Features from Large Weakly Supervised Data
Armand Joulin∗
 
Laurens van der Maaten∗
 
Allan Jabri
 
Nicolas Vasilache
 
Facebook AI Research
770 Broadway, New York NY 10003
Convolutional networks trained on large supervised
dataset produce visual features which form the basis for
the state-of-the-art in many computer-vision problems. Further improvements of these visual features will likely require
even larger manually labeled data sets, which severely limits the pace at which progress can be made. In this paper, we explore the potential of leveraging massive, weaklylabeled image collections for learning good visual features.
We train convolutional networks on a dataset of 100 million
Flickr photos and captions, and show that these networks
produce features that perform well in a range of vision problems. We also show that the networks appropriately capture
word similarity, and learn correspondences between different languages.
1. Introduction
Recent studies have shown that using visual features extracted from convolutional networks trained on large object
recognition datasets can lead to state-of-theart results on many vision problems including ﬁne-grained
classiﬁcation , object detection , and segmentation . The success of these networks has been largely
fueled by the development of large, manually annotated
datasets such as Imagenet . This suggests that in order
to further improve the quality of visual features, convolutional networks should be trained on larger datasets.
At the same time, this begs the question whether fully
supervised approaches are the right way forward to learning better models. In particular, the manual annotation of
ever larger image datasets is very time-consuming1, which
makes it a non-scalable solution to improving recognition
performances. Moreover, manually selecting and annotating images often introduces a strong bias towards a spe-
* Both authors contributed equally.
1For instance, the development of the COCO dataset took more
than 20, 000 annotator hours spread out over two years.
the veranda hotel
portixol palma
plane approaching zrh
avro regional jet rj
article in the local
paper about all the
unusual things found
at otto s home
student housing by
lungaard tranberg
architects in copenhagen
click here to see where
this photo was taken
this was another one with my old digital
camera i like the way it looks for some things
though slow and lower resolution than new
cameras another problem is that it s a bit of
a brick to carry and is a pain unless you re
carrying a bag with some room it s nearly x x
and weighs ounces new one is x x and weighs
ounces i underexposed this one a bit did
exposure bracketing script underexposure on
that camera looks melty yummy
gold kodak ﬁlm like
not as impressive as
embankment that s for sure
Figure 1. Six randomly picked photos from the Flickr 100M
dataset and the corresponding descriptions.
ciﬁc task . Another problem of fully supervised approaches is that they appear rather inefﬁcient compared to
how humans learn to recognize objects: unsupervised and
weakly supervised learning plays an important role in human vision , as a result of which humans do not need to
see thousands of images of, say, aardvarks to obtain a good
grasp of what an aardvark looks like.
In this paper, we depart from the fully supervised learning paradigm and ask the question: can we learn highquality visual features from scratch without using any fully
supervised data? We perform a series of experiments in
which we train models on a large collection of images and
their associated captions. This type of data is available in
great abundance via photo-sharing websites: speciﬁcally,
we use a publicly available dataset of 100 million Flickr images and captions (see Figure 1 for six randomly picked
Flickr photos and corresponding descriptions).
visual representations on such weakly supervised data has
 
three major advantages: (1) there is a near-inﬁnite amount
of weakly supervised data available2, (2) the training data is
not biased towards solving a speciﬁc task, and (3) it is much
more similar to how humans learn to solve vision.
We present experiments showing that convolutional networks can learn to identify words that are relevant to a particular image, despite being trained on very noisy targets.
More importantly, our experiments show that the visual features learned by weakly-supervised models are as good as
those learned by models that were trained on Imagenet,
suggesting that good visual representations can be learned
without full supervision. Moreover, our experiments reveal
some beneﬁts of training on weakly supervised data such as
the Flickr dataset: our models learn word embeddings that
are both grounded in vision and capture important semantic information, for instance, on word similarity and analogies. Because our training data is multilingual, our models
also relate words from different languages by observing that
they are frequently assigned to similar visual inputs.
2. Related Work
This study is not the ﬁrst to explore alternatives to
training convolutional networks on manually annotated
datasets . In particular, Chen and Gupta 
propose a curriculum-learning approach that trains convolutional networks on “easy” examples retrieved from Google
Images, and then ﬁnetunes the models on weakly labeled
image-hashtag pairs.
Their results suggest that such a
two-stage approach outperforms models trained on solely
image-hashtag data. This result is most likely due to the
limited size of the dataset that was used for training (∼1.2
million images): our results show substantial performance
improvements can be obtained by training on much larger
image-word datasets. Izadinia et al. also ﬁnetune pretrained convolutional networks on a dataset of Flickr images
using a vocabulary of 5, 000 words. By contrast, this study
trains convolutional networks from scratch on 100 million
images associated with 100, 000 words. Ni et al. also
train convolutional networks on tens of millions of imageword pairs, but their study focuses on systems issues and
does not report recognition performances.
Several studies have used weakly supervised data in
image-recognition pipelines that use pre-deﬁned visual features.
In particular, Li and Fei-Fei present a hierarchical topic model that performs simultaneous dataset
construction and incremental learning of object recognition models. Li et al. learn mid-level representations
by training a multiple-instance learning variant of SVMs
on hand-crafted low-level features extracted from images
downloaded using Google Image search. Denton et al. 
2The combined number of photo uploads via various platforms was
estimated to be 1.8 billion photos per day in 2014 .
learn (user-conditional) embeddings of images and hashtags
on a large collection of Instagram photos and hashtags. Torresani et al. train a collection of weak object classiﬁers
and use the classiﬁer outputs as additional image features
(in addition to low-level image features). In contrast to these
studies, we backpropagate the learning signal through the
entire pipeline, allowing us to learn visual features.
In contrast to our work, many prior studies also attempt
to explicitly discard low-quality labels by developing algorithms that identify relevant image-hashtag pairs from a
weakly labeled dataset . These studies solely
aim to create a “clean” dataset and do not explore the training of recognition pipelines on noisy data. By contrast, we
study the training of a full image-recognition pipeline; our
results suggest that “label cleansing” may not be necessary
to learn good visual features if the amount of weakly supervised training data is sufﬁciently large.
Our work is also related to prior studies on multimodal embedding that explore approaches such
as kernel canonical component analysis , restricted
Boltzmann machines , topic models , and logbilinear models . Some works co-embed images and
words , whereas others co-embed images and sentences
or n-grams . Frome et al. show that convolutional networks trained jointly on annotated image data
and a large corpus of unannotated texts can be used for zeroshot learning. Our work is different from those prior studies
in that we train convolutional networks solely on weakly supervised data.
3. Weakly Supervised Learning of Convnets
We train our models on the publicly available Flickr
100M data set . The data set contains approximately
99.2 million photos with associated titles, hashtags, and
captions. We will release our code and models upon publication of the paper.
Preprocessing. We preprocessed the text by removing all
numbers and punctuation (e.g., the # character for hashtags), removing all accents and special characters, and
lower-casing. We then used the Penn Treebank tokenizer3
to tokenize the titles and captions into words, and used
all hashtags and words as targets for the photos. We remove the 500 most common words (e.g., “the”, “of”, and
“and”) and because the tail of the word distribution is very
long , we restrict ourselves to predicting only the K =
{1, 000; 10, 000; 100, 000} most common words. For these
dictionary sizes, the average number of targets per photo is
3.72, 5.62, and 6.81, respectively. The target for each image is a bag of all the words in the dictionary associated
with that image, i.e., a multi-label vector y ∈{0, 1}K. The
images were preprocessed by rescaling them to 256×256
3 
pixels, cropping a central region of 224×224 pixels, subtracting the mean pixel value of each image, and dividing
each image by the standard deviation of its pixel values.
Network architecture. We experimented with two convolutional network architectures, viz., the AlexNet architecture and the GoogLeNet architecture .
AlexNet architecture is a seven-layer architecture that uses
max-pooling and rectiﬁed linear units at each layer; it has
between 15M and 415M parameters depending on the vocabulary size. The GoogLeNet architecture is a narrower,
twelve-layer architecture that has a shallow auxiliary classiﬁer to help learning; it holds the state-of-the-art on the ImageNet ILSVRC2014 dataset . Our GoogLeNet models
had between 4M and 404M parameters depending on vocabulary size. For exact details on both architectures, we
refer the reader to and , respectively—our architectures only deviate from the architectures described there
in the size of their ﬁnal output layer.
Loss functions.
We denote the training set by D
{(xn, yn)}n=1,...,N with the D-dimensional observation
x ∈RD and the multi-label vector y ∈{0, 1}K.
parametrize the mapping f(x; θ) from observation x ∈RD
to some intermediate embedding e ∈RE by a convolutional
network with parameters θ; and the mapping from that embedding e to a label y ∈{0, 1}K by sign(W⊤e), where
W is an E × K matrix. The parameters θ and W are optimized jointly to minimize a one-versus-all or multi-class
logistic loss. The one-versus-all logistic loss sums binary
classiﬁer losses over all classes:
log σ(f(xn; θ)) + 1 −ynk
log(1 −σ(f(xn, θ))),
where σ(x) = 1/(1 + exp(−x)) is the sigmoid function
and Nk is the number of positive examples for the class
k. The multi-class logistic loss minimizes the negative sum
of the log-probabilities over all positive labels. Herein, the
probabilities are computed using a softmax layer:
ℓ(θ, W; D) = −1
k f(xn; θ))
k′=1 exp(w⊤
k′f(xn; θ))
In preliminary experiments, we also considered a pairwise
ranking loss . This loss only updates two columns
of W per training example (corresponding to a positive
and a negative label). We found that when training convolutional networks end-to-end, these sparse updates signiﬁcantly slow down training, which is why we did not consider ranking loss further in this study.
Class balancing. The distribution of words in our dataset
follows a Zipf distribution : much of its probability mass
is accounted for by a few classes. If we are not careful about
how we sample training instances, these classes dominate
the learning, which may lead to poor general-purpose visual features . We follow Mikolov et al. and sample instances uniformly per class. Speciﬁcally, we select a
training example by picking a word uniformly at random
and randomly selecting an image associated with that word.
All the other words are considered negative for the corresponding image, even words that are also associated with
that image. Although this procedure potentially leads to
noisier gradients, it works well in practice.
Training. We trained our models with stochastic gradient
descent (SGD) on batches of size 128. In all experiments,
we set the initial learning rate to 0.1 and after every sweep
through a million images (an “epoch”), we compute the prediction error on a held-out validation set. When the validation error has increased after an “epoch”, we divide the
learning rate by 2 and continue training; but we use each
learning rate for at least 10 epochs. We stopped training
when the learning rate became smaller than 10−6. AlexNet
takes up to two weeks to train on a setup with 4 GPUs, while
training a GoogLeNet takes up to three weeks.
Large dictionary. Training a network on 100, 000 classes
is computationally expensive: a full forward-backward pass
through the last linear layer with a single batch takes
roughly 1, 600ms (compared to 400ms for the rest of the
To circumvent this problem, we only update
the weights that correspond to classes present in a training batch. This means we update at most 128 columns of
W per batch, instead of all 100, 000 columns. We found
such “stochastic gradient descent over targets” to work very
well in practice: it reduced the training time of our largest
models from months to weeks.
Whilst our stochastic approximation is consistent for the
one-versus-all loss, it is not for the multi-class logistic loss:
in the worst-case scenario, the “approximate” logistic loss
can be arbitrarily far from the true loss. However, we observe that the approximation works well in practice, and upper and lower bounds on the expected value of the approximate loss suggest that, indeed, it is closely related to the
true loss. Denoting sk = exp
k f(xn; θ)
and the set of
sampled classes by C (with |C| ≤K) and leaving out constant terms for brevity, it is trivial to see that the expected
approximate loss never overestimates the true loss:
Assuming that ∀k : sk ≥14, we use Markov’s inequality to
obtain a lower bound on the expected approximate loss, too:
4This assumption can always be satisﬁed by adding a constant inside
the exponentials of both the numerator and the denominator of the softmax.
Dictionary size K
1, 000 10, 000 100, 000
Pretrained
End-to-end GoogLeNet
Table 1. Precision@10 on held-out test data of word prediction
models on the Flickr 100M dataset for three different dictionary
We present results for (1) logistic regressors trained
on features extracted from convolutional networks that were pretrained on Imagenet and (2) convolutional networks trained endto-end using multiclass logistic loss. Higher values are better.
Flickr Word Prediction
size of Flickr training set (in millions) →
precision@10 →
Pascal VOC
Figure 2. Lefthand side: Precision@10 of by weakly supervised
AlexNets trained on Flickr datasets of different sizes on a held-out
test set, using K =1, 000 (in red) and a single crop. For reference,
we also show the precision@10 of logistic regression trained on
features from convolutional networks trained on ImageNet with
and without jittering (in blue and black, respectively). Righthand
side: Mean average precision on Pascal VOC 2007 dataset obtained by logistic regressors trained on features extracted from
AlexNet trained on Flickr (in red) and ImageNet with and without jittering (in blue and black). Higher values are better.
This bound relates the sample average of sc to its expected
value, and is exact when |C| →K. The lower bound only
contains an additive constant log(|C|/K), which shows that
the approximate loss is closely related to the true loss.
4. Experiments
To assess the quality of our weakly-supervised convolutional networks, we performed three sets of experiments:
(1) experiments measuring the ability of the models to predict words given an image, (2) transfer-learning experiments measuring the quality of the visual features learned
by our models in a range of computer-vision tasks, and (3)
experiments evaluating the quality of the word embeddings
learned by the networks.
4.1. Experiment 1: Associated Word Prediction
Experimental setup. We measure the ability of our models to predict words that are associated with an image using
the precision@k on a test set of 1 million Flickr images,
rijksmuseum
Figure 3. Six test images with high scores for different words.
The scores were computed using an AlexNet trained on the Flickr
dataset with a dictionary size of K =100, 000.
which we held out until after all our models were trained.
Precision@k is a suitable measure for assessing word prediction performance because (1) it corresponds naturally to
use cases in which a user retrieves images using a text query
and inspects only the top k results and (2) it is robust to the
fact that targets are noisy, i.e., that images may have words
assigned to them that do not describe their visual content.
Results. Table 1 presents the precision@10 of word prediction models trained on the Flickr dataset using dictionaries with 1, 000, 10, 000, and 100, 000 words5. As a baseline, we train L2-regularized logistic regressors on features
produced by convolutional networks trained on the Imagenet dataset6; the regularization parameter was tuned on
a held-out validation set. The results of this experiment
show that end-to-end training of convolutional networks on
the Flickr dataset works substantially better than training a
classiﬁer on features extracted from an Imagenet-pretrained
network: end-to-end training leads to a relative gain of 45
to 110% in precision@10. This suggests that the features
learned by networks on the Imagenet dataset are too tailored to the speciﬁc set of classes in that dataset. The results also show that the relative differences between the
GoogLeNet and AlexNet architectures are smaller on the
Flickr 100M dataset than on the Imagenet dataset, possibly,
because GoogLeNet has less capacity than AlexNet.
In preliminary experiments, we also trained models us-
5Our GoogLeNet networks with K = 100, 000 words did not ﬁnish
training by the submission deadline. We will update the paper with those
results as they become available.
6The Imagenet models were trained 224×224 crops that where randomly selected from 256×256 pixel input images. We applied photometric jittering on the input images , and trained using SGD with batches
of 128 images. Our pretrained networks perform on par with the state-ofthe-art on ImageNet: a single AlexNet obtains a top-5 test error of 24.0%
on a single crop, and our GoogLeNet obtains a top-5 error of 10.7%.
Figure 4. t-SNE map of 20, 000 Flickr test images based on features extracted from the last layer of an AlexNet trained with K = 1, 000.
A full-resolution map is presented in the supplemental material. The inset shows a cluster of sports.
ing one-versus-all logistic loss: using a dictionary of K =
1, 000 words, such a model achieves a precision@10 of
16.43 (compared to 17.98 for multiclass logistic loss). We
surmise this is due to the problems one-versus-all logistic
loss has in dealing with class imbalance: because the number of negative examples is much higher than the number
of positive examples (for the most frequent class, more than
95.0% of the data is still negative), the rebalancing weight
in front of the positive term is very high, which leads to
spikes in the gradient magnitude that hamper SGD training.
We tried various reweighting schemes to counter this effect,
but nevertheless, multiclass logistic loss consistently outperformed one-versus-all logistic loss in our experiments.
To investigate the performance of our models as a function of the amount of training data, we also performed experiments in which we varied the Flickr training set size.
The lefthand side of Figure 2 presents the resulting learning curves for the AlexNet architecture with K = 1, 000.
The ﬁgure shows that there is a clear beneﬁt of training on
larger datasets: the word prediction performance of the networks increases substantially when the training set is increased beyond 1 million images (which is roughly the size
of Imagenet); for our networks, it only levels out after ∼50
million images.
To illustrate the kinds of words for which our models
learn good representations, we show a high-scoring test image for six different words in Figure 3. To obtain more insight into the features learned by the models, we applied
t-SNE to features extracted from the penultimate
layer of an AlexNet trained on 1, 000 words. This produces
maps in which images with similar visual features are close
together; Figure 4 shows such a map of 20, 000 Flickr test
images. The inset shows a “sports” cluster that was formed
by the visual features; interestingly, it contains visually very
dissimilar sports ranging from baseball to ﬁeld hockey, ice
hockey and rollerskating. Whilst all sports are grouped together, the individual sports are still clearly separable: the
model can capture this multi-level structure because the images sometimes occur with the word “sports” and sometimes with the name of the individual sport itself. A model
trained on classiﬁcation datasets such as Pascal VOC is unlikely to learn similar structure unless an explicit target taxonomy is deﬁned (as in the Imagenet dataset). Our results
suggest that such taxonomies can be learned from weakly
labeled data instead.
4.2. Experiment 2: Transfer Learning
Experimental setup. To assess the quality of the visual features learned by our models, we performed transfer-learning
experiments on seven test datasets comprising a range of
computer-vision tasks: (1) the MIT Indoor dataset ,
(2) the MIT SUN dataset , (3) the Stanford 40 Actions
dataset , (4) the Oxford Flowers dataset , (5) the
Sports dataset , (6) the ImageNet ILSVRC 2014 dataset
 , and (7) the Pascal VOC 2007 dataset . We applied
the same preprocessing as before on all datasets: we resized
the images to 224×224 pixels, subtracted their mean pixel
value, and divided by their standard deviation.
Following , we compute the output of the penultimate layer for an input image and use this output as a feature representation for the corresponding image. We evaluate features obtained from Flickr-trained networks as well
as Imagenet-trained networks, and we also perform experiments where we combine both features by concatenating
them. We train L2-regularized logistic regressors on the
features to predict the classes corresponding to each of the
datasets. For all datasets except the Imagenet and Pascal
VOC datasets, we report classiﬁcation accuracies on a separate, held-out test set. For Imagenet, we report classiﬁcation errors on the validation set. For Pascal VOC, we report
average precisions on the test set as is customary for that
dataset. As before, we use convolutional networks trained
on the Imagenet dataset as baseline. Additional details on
the setup of the transfer-learning experiments are presented
in the supplemental material.
Results. Table 3 presents the classiﬁcation accuracies—
averaged over 10 runs—of logistic regressors on six datasets
for both fully supervised and weakly supervised feature-
82.96 70.32 73.28 76.29 32.21 61.84 79.81 72.91 51.56 43.82 60.77 63.32 78.63 67.72 90.26 45.45 53.15 49.14
Combined GoogLeNet
94.09 85.03 89.71 88.47 49.35 81.47
60.51 68.37 71.65 85.81 88.87 85.22 88.69 60.45 77.26 66.61 90.71 74.49
Table 2. Pascal VOC 2007 dataset: Average precision (AP) per class and mean average precision (mAP) of classiﬁers trained on features
extracted with networks trained on the Imagenet and the Flickr dataset (using K =1, 000 words). Higher values are better.
Oxford Flowers
Stanford 40 Actions
Imagenet (no jittering)
Imagenet (jittering)
classiﬁcation accuracy →
size of Flickr training set (in millions) →
Figure 5. Average classiﬁcation accuracy (averaged over ten runs)
of logistic regressors trained on features produced by weakly supervised AlexNets trained on Flickr image-caption datasets of different sizes on six different datasets (in red). For reference, we also
show the classiﬁcation accuracy of classiﬁers trained on features
from convolutional networks trained on ImageNet without jittering (in black) and with jittering (in blue). Dashed lines indicate
the standard deviation across runs. Higher values are better.
production networks, as well as for a combination of both
networks. Table 2 presents the average precision on the Pascal VOC 2007 dataset. Our weakly supervised models were
trained on a dictionary of K = 1, 000 words (we obtained
similar results for models trained on 10, 000 and 100, 000
words; see the supplementary material). The results in the
tables show that using the AlexNet architecture, weakly supervised networks learn visual features of similar quality as
fully supervised networks. This is quite remarkable because
the networks learned these features without any strong supervision.
Admittedly, weakly supervised networks perform poorly
on the ﬂowers dataset: Imagenet-trained networks produce
better features for that dataset, presumably, because the Imagenet dataset itself focuses strongly on ﬁne-grained classi-
ﬁcation. Interestingly, fully supervised networks do learn
better features than weakly supervised networks when a
Indoor SUN Action Flower Sports ImNet
Combined GoogLeNet
Table 3. Classiﬁcation accuracies on held-out test data of L2regularized logistic regressors obtained on six datasets (MIT Indoor, MIT SUN, Stanford 40 Actions, Oxford Flowers, Sports, and
ImageNet) based on feature representations obtained from convolutional networks trained on the Imagenet and the Flickr dataset
(using K = 1, 000 words and a single crop). Errors are averaged
over 10 runs. Higher values are better.
GoogLeNet architecture is used: this result is in line with
the results from 4.1, which suggest that GoogLeNet has too
little capacity to learn optimal models on the Flickr data.
The substantial performance improvements we observe in
experiments in which features from both networks are combined suggest that the features learned by both models complement each other. We note that achieving state-of-the-art
results on these datasets requires the development of tailored pipelines, e.g., using many image transformations and model ensembles, which is out of the scope
of this paper.
We also measured the transfer-learning performance as a
function of the Flickr training set size. The results of these
experiments with the AlexNet architecture and K = 1, 000
are presented in Figure 5 for four of the datasets (Indoor,
MIT SUN, Stanford 40 Actions, and Oxford Flowers); and
in the righthand side of Figure 2 for the Pascal VOC dataset.
The results are in line with those in 4.1: they show that tens
of millions of images are required to learn good featureproduction networks on weakly supervised data.
4.3. Experiment 3: Assessing Word Embeddings
The weights in the last layer of our networks can be
viewed as an embedding of the words. This word embedding is, however, different from those learned by language
models such as word2vec that learn embeddings based
on word co-occurrence: it is constructed without ever observing two words co-occurring (recall that during training,
we use a single, randomly selected word as target for an image). This means that structure in the word embedding can
only be learned when the network notices that two words
are assigned to images with a similar visual structure. We
perform two sets of experiments to assess the quality of the
word embeddings learned by our networks: (1) experiments
investigating how well the word embeddings represent semantic information and (2) experiments investigating the
ability of the embeddings to learn correspondences between
different languages.
Semantic information. We evaluate our word embeddings
on two datasets that capture different types of semantic information: (1) a syntactic-semantic questions dataset 
and (2) the MEN word similarity dataset . The syntacticsemantic dataset contains 8, 869 semantic and 10, 675 syntactic questions of the form “A is to B as C is to D”. Following , we predict D by ﬁnding the word embedding vector wD that has the highest cosine similarity with
wB−wA+wC (excluding A, B, and C from the search), and
measure the number of times we predict the correct word D.
The MEN dataset contains 3, 000 word pairs spanning 751
unique words—all of which appear in the ESP Game image
dataset—with an associated similarity rating. The similarity ratings are averages of ratings provided by a dozen human annotators. Following and others, we measure the
quality of word embeddings by the Spearman’s rank correlation of the cosine similarity of the word pairs and the
human-provided similarity rating for those pairs. In all experiments, we excluded word quadruples / pairs that contained words that are not in our dictionary. We repeated the
experiments for three dictionary sizes. As a baseline, we
measured the performance of word2vec models that were
trained on all comments in the Flickr dataset (using only
the words in the dictionary).
The prediction accuracies of our experiments on the
syntactic-semantic dataset for three dictionary sizes are presented in Table 4. Table 5 presents the rank correlations for
our word embeddings on the MEN dataset (for three vocabulary sizes). As before, we only included word pairs
for which both words appeared in the vocabulary. The results of these experiments show that our weakly supervised
models, indeed, learned meaningful semantic structure. The
results also show that the quality of our word embeddings
is lower than that of word2vec, because unlike our models,
word2vec observes word co-occurrences during training.
We also made t-SNE maps of the embedding of 10, 000
words in Figure 6. The insets highlight six “topics”: (1)
musical performance, (2) sunsets, (3) female and male
ﬁrst names, (4) gardening, (5) photography, and (6) military. These topics were identiﬁed solely because the words
in them are associated with images containing similar visual content: for instance, ﬁrst names are likely to be assigned to photos showing one or a few persons. Interestingly, the “sunset” and “gardening” topics show examples
K=1, 000 K=10, 000 K=100, 000
AlexNet + word2vec
GoogLeNet + word2vec
Table 4. Prediction accuracy of predicting D in questions “A is to
B like C is to D” using convolutional-network word embeddings
and word2vec on the syntactic-semantic dataset, using three dictionary sizes. Questions containing words not in the dictionary
were removed. Higher values are better.
K=1, 000 K=10, 000 K=100, 000
AlexNet + word2vec
GoogLeNet + word2vec
Table 5. Spearman’s rank correlation of cosine similarities between convolutional-network (and word2vec) word embeddings
and human similarity judgements on the MEN dataset. Word pairs
containing words not in the dictionary were removed. Higher values are better.
of grouping of words from different languages.
For instance, “sonne”, “soleil”, “sole” mean “sun” in German,
French, and Italian, respectively; and “garten” and “giardino” are the German and Italian words for garden.
Multi-lingual correspondences.
To further investigate
the ability of our models to ﬁnd correspondences between
words from different languages, we selected pairs of words
from an English-French dictionary7 for which: (1) both the
English and the French word are in the Flickr dictionary and
(2) the English and the French word are different. This produced 309 English-French word pairs for models trained on
K = 10, 000 words, and 3, 008 English-French word pairs
for models trained on K = 100, 000 words. We measured
the quality of the multi-lingual word correspondences in the
embeddings by taking a word in one language and ranking the words in the other language according to their cosine similarity with the query word. We measure the precision@k of the predicted word ranking, using both English
and French words as query words.
Table 6 presents the results of this experiment: for a
non-trivial number of words, our procedure correctly identiﬁed the French translation of an English word, and vice
versa. Finding the English counterpart of a French word
is harder then the other way around, presumably, because
there are more English than French words in the dictionary:
this means the English word embeddings are better optimized than the French ones. In Table 7, we show the ten
7 
nnmt-shared-task/
Figure 6. t-SNE map of 10, 000 words based on their embeddings as learned by a weakly supervised convolutional network trained on the
Flickr dataset. Note that all the semantic information represented in the word embeddings is the result of observing that these words are
assigned to images with similar visual content (the model did not observe word co-occurrences during training). A full-resolution version
of the map is provided in the supplemental material.
Query →Response
k = 1 k = 5 k = 10
English →French
French →English
English →French
100, 000 French →English
Table 6. Precision@k of identifying the French counterpart of an
English word (and vice-versa) for two dictionary sizes, at three
different levels of k. Chance level (with k = 1) is 0.0032 for
K =10, 000 words and 0.00033 for K =100, 000 words. Higher
values are better.
uzbekistan
ouzbekistan
infrarouge
champignons
mauritania
mauritanie
apocalyptique
Table 7. Ten highest-scoring pairs of words, as measured by the
cosine similarity between the corresponding word embeddings.
Correct pairs of words are colored green, and incorrect pairs are
colored red according to the dictionary. The word “oas” is an abbreviation for the Organization of American States.
most similar word pairs, measured by the cosine similarity between their word embeddings. These word pairs suggest that models trained on Flickr data ﬁnd correspondences
between words that have clear visual representations, such
as “tomatoes” or “bookshop”. Interestingly, the identiﬁed
English-French matches appear to span a broad set of domains, including objects such as “pencils”, locations such
as “mauritania”, and concepts such as “infrared”.
5. Discussion and Future Work
This study demonstrates that convolutional networks can
be trained from scratch without any manual annotation and
shows that good features can be learned from weakly supervised data. Indeed, our models learn features that are nearly
on par with those learned from an image collection with
over a million manually deﬁned labels, and achieve good
results on a variety of datasets. (Obtaining state-of-the-art
results requires averaging predictions over many crops and
models, which is outside the scope of this paper.) Moreover, our results show that weakly supervised models can
learn semantic structure from image-word co-occurrences.
In addition, our results lead to three main recommendations for future work in learning models from weakly
supervised data.
First, our results suggest that the bestperforming models on the Imagenet dataset are not optimal for weakly supervised learning. We surmise that current models have insufﬁcient capacity for learning from the
complex Flickr dataset. Second, multi-class logistic loss
performs remarkably well in our experiments even though
it is not tailored to multi-label settings. Presumably, our
approximate multiclass loss works very well on large dictionaries because it shares properties with losses known to
work well in that setting . Third, it is essential
to sample data uniformly per class to learn good visual features . Uniform sampling per class ensures that frequent
classes in the training data do not dominate the learned features, which makes them better suited for transfer learning.
In future work, we aim to combine our weakly supervised vision models with a language model such as
word2vec to perform, for instance, visual question answering . We also intend to further investigate the
ability of our models to learn visual hierarchies, such as the
“sports” example in Section 4.2.
Acknowledgements
We thank Ronan Collobert, Tomas Mikolov, Alexey
Spiridinov, Rob Fergus, Florent Perronnin, L´eon Bottou and
the rest of the FAIR team for code support and helpful discussions.