Distinguishing Enzyme Structures from Non-enzymes
Without Alignments
Paul D. Dobson and Andrew J. Doig*
Department of Biomolecular
Sciences, UMIST, P.O. Box 88
Manchester M60 1QD, UK
The ability to predict protein function from structure is becoming increasingly important as the number of structures resolved is growing more
rapidly than our capacity to study function. Current methods for predicting protein function are mostly reliant on identifying a similar protein of
known function. For proteins that are highly dissimilar or are only similar
to proteins also lacking functional annotations, these methods fail. Here,
we show that protein function can be predicted as enzymatic or not without resorting to alignments. We describe 1178 high-resolution proteins in a
structurally non-redundant subset of the Protein Data Bank using simple
features such as secondary-structure content, amino acid propensities,
surface properties and ligands. The subset is split into two functional
groupings, enzymes and non-enzymes. We use the support vector
machine-learning algorithm to develop models that are capable of assigning the protein class. Validation of the method shows that the function can
be predicted to an accuracy of 77% using 52 features to describe each
protein. An adaptive search of possible subsets of features produces a
simpliﬁed model based on 36 features that predicts at an accuracy of
80%. We compare the method to sequence-based methods that also avoid
calculating alignments and predict a recently released set of unrelated
proteins. The most useful features for distinguishing enzymes from nonenzymes are secondary-structure content, amino acid frequencies, number
of disulphide bonds and size of the largest cleft. This method is applicable
to any structure as it does not require the identiﬁcation of sequence or
structural similarity to a protein of known function.
q 2003 Elsevier Science Ltd. All rights reserved
Keywords: protein function prediction; structure; enzyme; machine
learning; structural genomics
*Corresponding author
Introduction
We aim to demonstrate that protein function can
be predicted as enzymatic or not without resorting
to alignments. Protein function prediction methods
are important as international structural genomics
initiatives are expected to generate thousands of
protein structures in the next decade. The capacity
of laboratories studying protein function is not suf-
ﬁcient to keep pace with the number of structures
being released, with the consequence that many
structures
functional
annotations.
Predictions can help guide the activities of these
laboratories towards functionally more important
proteins. The main approaches to in silico protein
function prediction from structure are neatly summarised by Sung-Ho Kim.1 They are assignment
of a function from a similar fold, sequence, structure or active site of previously known function,
assignment from a structure with a common ligand
prediction
method that does not work by comparison with
another protein of known function).
The most common methods rely on identifying
similarity to a protein of known function and
transferring that function. Sequence alignments
are identiﬁed using approaches such as BLAST2 or
FASTA.3 The power of PSI-BLAST4 has permitted
the detection of sequence similarities that infer
homology down to below 20%. Even when the
likes of PSI-BLAST fail, the sequence can still yield
useful information in the form of sequence motifs,
which can be identiﬁed using PRINTS,5 BLOCKS,6
0022-2836/03/$ - see front matter q 2003 Elsevier Science Ltd. All rights reserved
E-mail address of the corresponding author:
 
Abbreviations used: EC, Enzyme Commission; CE,
Combinatorial Extension.
doi:10.1016/S0022-2836(03)00628-4
J. Mol. Biol. 330, 771–783
PROSITE7 and other similar tools. Using predicted
secondary structures to assign fold class can
expand the information content of a sequence still
further, since fold classes are often associated with
a particular set of functions.8
The next logical step after using predicted structure is to use real structure. As structure is more
highly conserved than sequence, it is often possible
to detect similarities that are beyond the reach of
even the most sophisticated sequence alignment
algorithms. Structural similarity is detected using
Combinatorial
Extension9
VAST,10 which map structures onto each other.
Incomplete structural alignments can still suggest
fold class. A problem encountered when identifying similar folds is that there may not be one
speciﬁc function associated with a fold, making
choosing the correct one non-trivial. The TIM
barrel fold is known to be involved in at least 18
different enzymatic processes1 and while this does
give a narrowing of the number of possible
functions to assign, the precise function remains
Transferring function from a protein that shares
a ligand is a method that can give variable results
if not tempered with some biochemical knowledge.
For example, possession of NADH suggests an
oxidoreductase enzyme. Functionally unimportant
ligands may be shared by many structures, but to
say that these proteins share a common function
would be far from accurate. Ligand data can be
used in conjunction with data concerning the
immediate protein environment that binds the
ligand. Binding-site correspondence is a strong
indicator of functional similarity,11 as is the case
correspondence
near-identical
catalytic triads in the active sites of trypsins and
subtilisins,12 two evolutionarily distant but functionally similar types of protein. The utility of this
approach is demonstrated by the ProCat database.13
For sequences and structures that are highly
similar, the reliability of the predicted function is
good, though in a recent study it has been shown
to be less than previously thought.14 For pair-wise
sequence alignments above 50%, less than 30%
share exact EC numbers. This suggests the level of
sequence/structure
conservation
function conservation is much lower than believed
formerly and demonstrates the pressing need for
protein function prediction methods that are not
dependent upon tools that detect alignments.
Non-alignment-based function predictions have
been made using many different techniques. Text
data mining of scientiﬁc literature15 uses the information in scientiﬁc abstracts to assign subcellular
localisations, which can be used as an indicator of
function. Amino acid compositions have been
used to predict localisation.16,17 The Rosetta Stone18
method allows function predictions to be made for
proteins that do not align to a protein of known
function by examining gene fusions. If the protein
aligns to part of a fused protein and the part of
the fused protein it does not align to matches a
protein of known function, that function can be
transferred to the original protein. Phylogenetic
proﬁling19 functionally relates proteins with similar
proﬁles. The gene neighbour method uses the
observation that if the genes that encode two proteins are close on a chromosome, the proteins tend
to be functionally related.20,21 Neural networks have
been used to combine predicted post-translational
modiﬁcations into sophisticated systems capable of
predicting subcellular location and function.22
While similarity-based methods do provide the
most precise and dependable means of function
prediction, in many cases it is apparent that they
are heavily reliant on being able to identify highly
similar proteins of known function. With one of
the principal objectives of the structural genomics
initiatives being the elucidation of structures from
the more sparsely populated regions of fold space,
the problem of not ﬁnding a similar protein of
known function is more likely to occur. A method
suggested by Stawiski et al.23 that lies between a
similarity-based approach and an ab initio method,
is based on the observation that proteins of similar
function often use basic structural features in a
similar manner. For example, they note that proteases often have smaller than average surface
densities.
Similarly,
O-glycosidases24 deviate from the norm in terms
of features such as the surface roughness (or fractal
dimension). Features identiﬁed as being indicative
of a certain function permit the construction of
machine-learning-based classiﬁcation schemes that
allow function predictions for novel proteins without
conventional
similarity-based
methods. The broad structural similarities that
characterise a functional class of proteins extend
beyond the reach of structural alignments, yet it
has been shown that they can be used for protein
prediction.
demonstrate
identifying
enzymatic or not without resorting to alignments
to proteins of known function. To do this, we
describe each protein in a non-redundant subset
of the Protein Data Bank25 in terms of simple
features such as residue preference, residue surface
fractions, secondary structure fractions, disulphide
bonds, size of the largest surface pocket and
presence of ligands. As we are demonstrating a
method for use when alignment methods do not
yield results, we restrict ourselves to features that
do not rely on alignments. As such, our method is
for use when alignment methods fail. Histograms
illustrate that for some features the distributions
of enzymes and non-enzymes are different. In
order to utilise these differences we combine the
data into a predictive model using the support
vector machine technique. Adaptive programming
is used to ﬁnd a more optimal subset of features,
giving a greater predictive accuracy whilst simultaneously simplifying the model. We validate
these models by leave-out analyses and predicting
a set of unrelated proteins submitted to the Protein
Data Bank since the training set was compiled.
Predicting Protein Function from Structure
Using the same approach, we investigate the utility
of models built only using amino acid propensities.
Being easily calculable from sequence, this provides a method for predicting the function of proteins that cannot be aligned to a protein of known
function, even if we do not have a structure. We
also make a comparison to the ProtFun enzyme/
non-enzyme methods described by Brunak et al.22
The support vector machine works by deducing
the globally optimal position of a hyperplane separating the distribution of two classes of points
scattered in a multi-dimensional space. The number of features used to describe the position of
determines
dimensionality
hyperspace. The 52 features used to describe each
protein are shown in Table 1. All features are easily
calculable from any protein structure. No feature is
based on mapping sequence or structure onto a
known protein, so the model can be said to be
truly independent of alignment-based techniques.
Heterogen data is presented to the machine in
binary form (1 for present, 0 for absent; Table 2).
Table 1. All features and a more optimal subset
Features used to describe the difference between structures of enzymes and non-enzymes. Bold and italicised features are those
selected by the adaptive search of possible subsets as part of the most optimal subset identiﬁed.
Predicting Protein Function from Structure
Some metal ions are present only once in the dataset, in which case the feature was not included in
the set of features on which the machine was
Results from the leave-out analyses of prediction
accuracy are shown in Table 3. These data are
based on a model generated using a radial basis
kernel function. Linear and polynomial kernels
perform less well (not shown).
When using all 52 features, the results indicate a
predictive accuracy for the set of features around
76–77%. Random performance based purely on
the size of classes could give, at best, an accuracy
of 58.7% by predicting “enzyme” every time. The
subset selection strategy selects 36 features from
the total 52. The increase in accuracy is approximately 3–4% to 80%. Bold and italicised features
in Table 1 show the subset that gives the results in
Table 3. On a set of 52 unrelated (by Combinatorial
Extension) proteins submitted to the Protein Data
Bank since the training set was compiled, this 36
feature model predicts with an accuracy of 79%.
Three unrelated proteins with no functional assignment in the Protein Data Bank were predicted as
follows (conﬁdence in parentheses): 1JAL nonenzyme
non-enzyme
1ON0 enzyme (0.548).
Table 4 shows that certain features are selected
more than others. Possibly as a result of its low
abundance at protein surfaces, the surface fraction
of tryptophan is never used. Ligand data are used
probably related to the sparseness of heterogens in
the dataset. Other features, particularly secondarystructure contents (Figure 2) and the volume of
the largest pocket (Figure 3) are selected frequently.
The conﬁdence of a support vector machine
prediction is related to the distance from the
hyperplane. Larger distances equate to higher
conﬁdence. Figure 1 illustrates how the 36 feature
models in a leave-one-out analysis were predicted.
On enzymes, the method is 89.7% accurate. For
non-enzymes, the accuracy is 68.6%.
Models trained only on amino acid propensities
give an indication of how accurate function prediction can be from sequence when a similar protein
of known function cannot be detected (Table 5).
Subsets contain only the features that combine to
more optimally describe the difference between
classes using the support vector machine. To
visualise this, histograms can be used to depict
the underlying probability distribution of classes
for a particular feature. Figures 2–5 show typical
histograms.
Correlations between class distributions for certain features, such as largest pocket volume, helix
and sheet contents, are low. Most other features
have distributions that tend to differ only slightly
for the two classes. However, the support vector
individual
multi-dimensional joint probability distribution, so
it is the manner in which feature distributions
interact that is important. It is for this reason that
wrapper methods were preferred to ﬁlters for subset selection (see Methods). A pre-ﬁlter method
based on a correlation coefﬁcient would have
resulted in a less optimal performance.
For a-helix fraction (Figure 2) it is evident that
there are large differences between enzymes and
non-enzymes, and the coiled-coil state that is
DNA-binding
depicted at the upper end of the scale by a slight
increase in the non-enzyme density. Most other
histograms approximately ﬁt to standard statistical
distributions such as normal, Poisson, etc. as typi-
ﬁed by Figures 4 and 5. Figure 3 shows the volume
of the largest surface pocket. Above 1000 A˚ 3,
enzymes are more prevalent, so a protein with a
large surface pocket is more likely to be enzymatic.
Conversely,
proteins with small surface pockets tend not to
show enzyme activity.
The ProtFun22 prediction servers also predict
whether a protein is an enzyme. These servers
work entirely on features calculated from protein
sequence. Like the method presented here, they
alignments.
sequences from all the proteins in the dataset to
ProtFun analysis. The results indicate that on this
set the ProtFun approach is 75.21% accurate.
Table 2. Ligand groups in the dataset
Hetero group
Non-enzyme
Metals and metal-containing compounds
Iron (not heme)
Numbers of each hetero group type in the dataset and each
class. Metals present only once in the dataset were not used in
the training set.
Table 3. Average percentage accuracies and average
standard errors of models built on all features and the
optimal subset of 36 features in Table 1
Leave-x-out
Number of features
Standard error
Subset (36)
Standard error
Comparison of the prediction accuracy of models generated
from all features and the more optimal subset of features generated by an adaptive search of possible subsets. The accuracy
increase is approximately 3–4%.
Predicting Protein Function from Structure
Table 4. Twenty subsets chosen by adaptive searching
The composition of 20 models selected by the adaptive programming regime. Grey shaded squares indicate that a feature is present
in a subset. The number of times a feature was selected over 20 runs is shown on the left. Percentage accuracies for each model are
shown along the bottom.
Predicting Protein Function from Structure
Discussion
It is apparent that there is a need for methods to
conventional
approaches do not yield results. We demonstrate
the utility of representing proteins not in terms of
the precise locations of residues, but by using
simple features such as residue preference, secondary structure, surface features and ligands. When
these data are combined using the support vector
machine approach, a model is built that can predict
the class of a novel protein as enzymatic or not to
an accuracy of approximately 77%. This accuracy
is based on a model built upon 52 features. A subset of these features selected by a basic adaptive
program gives a much simpler model, capable of
predicting with an increased accuracy of 80%.
Figure 1 suggests that the model works by deﬁning
what constitutes an enzyme rather than a nonenzyme, as it predicts enzymes with much greater
accuracy. This makes sense, as enzymes are truly a
class of proteins, with some similar properties
such as having active sites, whereas non-enzymes
are merely all those proteins that are not enzymes
accumulated
more artiﬁcial and vague. One might expect that
with greater diversity in the non-enzyme class, a
model that isolates enzymes from the rest would
be easier to derive. It is also apparent from
Figure 1 that high-conﬁdence predictions are rarely
incorrect.
The method chosen for modelling the data and
predictions
machine,26,27 a machine-learning tool for two-class
classiﬁcation. The goal of classiﬁcation tools is to
ﬁnd a rule that best maps each member of training
set S (described by the properties X) to the correct
classiﬁcation Y. Each point in the input space is
described by the vector ðxi; yiÞ :
xi [ X , RN
yi [ Y ¼ { þ 1; 21}
The support vector machine is based on separating
the N-dimensional distribution of points in a training set. A hyper-plane is ﬁt to the input space in a
manner that globally optimally separates the two
user-deﬁned classes. The orientation of a test
hyper-plane
predicted class.
Recently, the popularity of the support vector
machine has increased dramatically. It has been
shown to frequently out-perform previous classiﬁcation favourites such as neural networks. There
are two major features of the support vector
machine that make it capable of such high performance, the kernel function and structural risk
minimisation.
It is evident that for most real problems the optimal separating hyper-plane will not ﬁt linearly
between the two classes without signiﬁcant error,
though a non-linear separation may well be feasible. The kernel function permits the mapping of
data in the input space into a higher-dimensional
feature space where non-linear mappings become
linearly possible. The choice of kernel function
determines the nature of the mapping to the
feature space.
Perhaps the most critical component of the support vector machine is the manner in which risk is
minimised. The concept of risk can be deﬁned as
the number of errors the model makes. It is
apparent that a good model should make as few
errors as possible. Traditionally, risk is minimised
by ﬁtting a model to the training set so that it can
Figure 1. Prediction conﬁdence distributions. Distance from the hyperplane is related to the conﬁdence of a
prediction. The distributions of predictions for an 80.9% (36 feature) model are shown.
Predicting Protein Function from Structure
predict the training set as accurately as possible.
This is known as empirical risk minimisation.
However, while the model may be able to predict
the training set almost perfectly, it may not perform so accurately on data unlike what it has been
trained on. The model is said to have lost the
ability to generalise. If the training set can be
guaranteed to represent everything the model
could be asked to predict, then the problem would
not be so great, but this situation is rare. The result
of empirical risk minimisation is often a model
with poor predictive abilities as a consequence of
describing the training set too well. In this situation, we say that the model is over-ﬁt to the data.
Risk is minimised differently by the support vector
Structural
minimisation
measure of the number of different models implementable in the training set to control the point at
which a model can be said to be over-ﬁt to the
training data. In this way, generalisation ability
can be controlled. A rule can be found that
balances the generalisation ability with the empirical risk minimisation method to ensure that the
prediction performance is optimal on the training
set without being over-ﬁt to it.
The adaptive search of possible subsets was preferred to ﬁlter methods that quantify the difference
between classes for each feature, as it explores the
joint probability distributions of possible subsets
in the context of the learning algorithm. This
means that the subset selection strategy investigates features as they interact, rather than in isolation. The ﬁnal subset selected by the adaptive
programming search (what remains upon completion of the adaptive process) is not always the
same. Local performance maxima within the set
can be responsible for this. Without an exhaustive
search it is difﬁcult to show that the adaptive programming approach ﬁnds the most optimal subset
from the 252 possibilities. However, it consistently
ﬁnds simple subsets capable of predicting with
greater accuracy without sacriﬁcing generalisation
performance (by our measure of generalisation, in
which the subset is prevented from over-ﬁtting by
restricting the tolerance of variation in leave-out
results so that any subset of features incapable of
predicting well on all fractions of the dataset is
not permitted).
illustrates
selected consistently, such as the secondary-structure fractions (Figure 2), largest pocket volume
(Figure 3), fractions of phenylalanine (Figure 4),
fractions of surface asparagine and isoleucine, and
ATP, NAD and FAD. Some of these more frequently selected features have class distributions
that differ sufﬁciently to permit slightly improved
(compared to random) function predictions without other features. For example, the volume of the
largest pocket is a feature that one might expect to
differ greatly between classes and, indeed, it
shows little correlation between enzymes and
non-enzymes. Enzymes tend to have large surface
pockets containing their active sites. Therefore, a
protein without a large surface pocket is probably
not an enzyme. This seems to be true, as the
majority of enzymes have a largest pocket volume
above 1000 A˚ 3. Above this volume there are still
non-enzymes as a result of those proteins that
require large surface pockets for binding but have
no enzymatic role (for example, a DNA-binding
protein such as a transcription factor).
The presence of any of the coenzymes ATP, FAD
and NAD in a structure suggests strongly that the
protein is an enzyme. ATP is found in some nonenzyme structures, which is to be expected, as it is
involved in almost all processes that involve
energy transfer. Despite this, in the majority of
cases it is still indicative of an enzyme.
Another feature that exhibits poor correlation is
helix content. A cursory glance at the helix content
Histograms
illustrate
differences
secondary structure contents.
Predicting Protein Function from Structure
distributions suggest that alone it should permit a
better than random prediction, as proteins with
less than 30% helix tend to be non-enzymes,
between 30–60% enzymes tend to dominate, and
non-enzymes
prevalent.
Similar information can be gleaned from the sheet
fractions, with a signiﬁcant proportion of nonenzymes having no b-sheet. Between 10% and
nonenzymes. Greater than 30% sheet content suggests
non-enzymes. That secondary structure contents
have such discriminating power is intriguing and
secondary-structure predictions or experimental
difference between class probability distributions
that suggests they will have any discriminating
power. Yet it is clear that these features do have
some utility, as they are chosen consistently by
adaptive searching. Biological interpretations for
some of the more frequently selected features can
be proposed, though these are difﬁcult to verify.
Many residue types are known to be associated
with certain processes linked to function and/or
subcellular location, such as the role of asparagine
glycosylation
Asn-X-Ser/Thr,
X – Pro). This may explain why surface asparagine is a useful feature, as certain classes of
proteins may require glycosylation (such as certain
extracellular
solubility).
A set of residues that accounts for only a small
fraction of the total surface includes cysteine, isoleucine, leucine, methionine and valine. Surface
hydrophobic
selected frequently. One possible explanation for
this is that a residue type with a preference for
being buried, and so normally present at the surface at only low levels, is signiﬁcant when it is
found unburied. That is, something that introduces
instability, like having a solvent-accessible aliphatic
residue, would not happen without purpose. This
suggests involvement in the protein’s function or
cellular location. Patches of hydrophobic residues
at the surface probably occur in proteins that interact with other proteins, making complex formation
energetically more feasible.
The high frequency of selection of the histidine
fraction feature is interesting. It is not a highly
abundant residue at the surface, yet it has the interesting property of having a pKa of 6–7, which
makes it amenable to mediating proton transfer.
This is a common
process in many enzyme
mechanisms and offers an explanation for the
higher frequency at which it occurs in the enzyme
Several features are selected very rarely, if at all.
Removal of such features allows for a much
simpler model, as the level of abstraction by the
kernel function from the input space to the feature
space is much lower in the absence of noisy or
combination,
cooperate to form a joint probability distribution
Figure 3. Differences in the volume of the largest surface pocket (A˚ 3). Pockets of volume above 1000 A˚ 3 suggest the
protein is an enzyme. (The non-enzyme value in the leftmost bin, excluded here for clarity, is 0.32).
Table 5. Average percentage accuracies and standard
errors of models built on amino acid frequencies
Leave-x-out
The performance of models built on all amino acid frequencies and the associated standard errors.
Predicting Protein Function from Structure
that lends itself well to partitioning, so too a
feature can act to disrupt this and force a less
well-ﬁt model.
Sparse features, such as the metal ion data, seem
to contribute little. It is surprising that some types
of metal are present so infrequently in this dataset.
Calcium is not present infrequently, nor is it distributed equally between classes. However, it does
not get used in any of the models generated.
To summarise the feature selections, it is clear
that some features are useful for predicting protein
function in isolation, as the distributions differ
greatly between classes. In combination, this discriminatory power is heightened. Features that
seem to correlate well across the two classes of
function can bring something to the predictive
method that is not immediately apparent from
their distributions. It is worth noting that the radial
basis function kernel performs better than linear
and polynomial kernels. This is indicative of the
complexity
distribution.
Evidently, different sub-groupings of each class
exist in pockets throughout the total distribution,
forming discontinuous clusters of data. Of the
three kernels used, the radial basis function is
most capable of competently mapping complex
data such as this into higher dimensions, where
the problem becomes linearly tractable.
The construction of the dataset is intended to
avoid the problem of over-representation in the
Protein Data Bank, yet it still suffers the problem
under-representation.
notoriously
structures
membrane proteins and, as a consequence, there
are very few deposited in the Protein Data Bank.
Another source of bias stems from the dataset containing only X-ray crystal structures (as a consequence
programmes
certain features being intolerant of the ambiguity
that is often seen with NMR structures). The structures that are not amenable to crystallisation are
therefore potentially under-represented. The consequence of these biases is that models generated
may be less capable of predicting functions for
histogram.
differences
phenylalanine
and non-enzymes.
histogram.
The differences in surface fractions
of aspartate between classes.
Predicting Protein Function from Structure
under-represented proteins. As under-represented
classes have structures solved, the method should
accommodate
information.
There is an uneven distribution of enzyme
classes in the PDB (with hydrolases being by far
the largest class and ligases much the smallest).
This is reﬂected in the dataset, so one may suspect
that the model may focus on predicting certain
classes more than others. However, errors in a
self-consistent test (using a model to predict the
set on which it was trained) range only from
4–11% over the six top-level enzyme classiﬁcations, suggesting that the model is not overly
directed towards any particular class. For nonenzymes,
equivalent
classiﬁcation
scheme applicable here, but from functional annotations within the PDB ﬁles no bias is immediately
Without structures it is still possible to make
function predictions. Purely working from amino
acid frequencies, models can be built in the same
way that are capable of predicting to approximately 75% accuracy. It seems prudent to validate
this result in future work by using a larger, more
comprehensive dataset based on a non-redundant
sequence dataset (here the results are based on the
structural dataset and so are subject to the biases
listed above). With a dataset that covers a more
diverse selection of proteins the problem changes,
so perhaps the accuracies shown herein should be
treated with a degree of caution. From this dataset
it is clear that predictions to some above-random
extent are certainly possible using only amino acid
frequencies.
A comparison is made with the ProtFun method
described by Brunak et al.22 by running our dataset
through their algorithms. This is not really a comparison on an even footing, as there are more data
available to us in a structure, but more data available to ProtFun in that they have a larger dataset
that may contain sequences similar to sequences
in our dataset. However, the comparison highlights
a greater accuracy for the method herein, and goes
some way to vindicating the inclusion of structure
data in protein function predictions.
We have demonstrated the utility of representing
proteins in terms of simple features as a method of
describing the similarities between proteins with a
common function. Using this information, we
have been able to construct models that can predict
protein function as enzymatic or not to an accuracy
of 80%. We are currently in the process of setting
up a server that will take as input a PDB ﬁle†,
extract data for the features used in our best performing model and make predictions.
Dataset construction
The dataset consists of X-ray crystal structures with a
resolution of less than or equal to 2.5 A˚ and an R-factor
of 0.25 or better. A structurally non-redundant representation of the Protein Data Bank provides a ﬁrmer
grounding for validating results as prediction accuracies
are artiﬁcially high with a redundant dataset (it is easier
to make a correct prediction for an object if the model is
built upon data that is essentially the same). Removing
similarity also avoids the problem of biases in the PDB
that are the result of certain research areas producing
more structures than others.
The Combinatorial Extension9 methods implemented
at UCSC provide structural alignment Z-scores for all
chains in the PDB against all others, or approximations
through a representative structure. This information is
sufﬁcient to build the set of structures required. The
authors of CE propose a biological interpretation of the
Z-scores. Scores of 4.5 and above suggest the same
protein family, scores of 4.0–4.5 are indicative of the
same protein superfamily and/or fold, while scores of
3.5–4.0 imply a possible biologically interesting similarity. In this cull of the PDB, no chain in any protein
structurally aligns to any other chain in the dataset with
a Z-score of 3.5 or above outside of its parent structure.
Aligning to another chain in a parent structure is permitted, as redundancy is only an issue in leave-out
testing, so similarity between chains that are always left
out together does not bias results.
The dataset is split into 691 enzymes and 487 nonenzymes on the basis of EC number, annotations in the
PDB and Medline abstracts. Ten proteins were excluded
incomplete
annotation. The dataset is provided as a supplement.
Features for model building
Easily computed features that differ between classes
are used to describe each protein. Residue preference is
calculated as the number of residues of each type
divided by the total number of residues in the protein.
Secondary structure content is calculated similarly working from STRIDE28 assignments. The helix content is
derived from the total amount of the structure assigned
as a, 310 or p helix. Sheet and turn fractions are calculated similarly. Disulphide bond numbers are derived
from the SSBOND section of the PDB ﬁle. The size of
the largest pocket is the molecular surface29 volume in
A˚ 3, as calculated by the cavity detection algorithm
properties
calculated
attributable to each residue type being features.
Heterogen data are taken from the PDB ﬁle and presented to the machine in binary form (1 for present, 0
for absent). When ligands are used in crystallisation,
often an analogue or a derivative of a structure is used
for technical reasons. This has the potential to make
data very sparse. For example, it is desirable to cluster
adenosine triphosphate with the structures for adenosine
mono- and diphosphate. There are methods available
† 
‡ Hubbard, S. J. & Thornton, J. M. . “NACCESS,”
Department of Biochemistry and Molecular Biology,
University College London.
Predicting Protein Function from Structure
that cluster similar hetero groups together, yet they often
group molecules that are clearly different. After investigating the utility of certain clustering schemes, it was
decided that only a very simple and severe classiﬁcation
scheme would classify rigorously enough to avoid error.
The feature ATP contains all structures that contain one
of the following HET codes: AMP, A, ADP, ATP. These
are codes for adenosine mono-, di- and triphosphate.
Similarly, the feature FAD consists of proteins that contain either FAD or FDA. We consider the oxidised and
reduced forms of FAD as equivalent. The NAD feature
uses the HET codes NAD, NAH, NAP and NDP. The
NAD feature treats NAD and NADP in both oxidised
and reduced states as equivalent. In order to ﬁnd
equivalent HET codes the “stereo smiles” utility in the
Macromolecular Structure Database was used.31 While
this gives some scope for detecting when analogues
have been used, it does not cover the whole range of
alternative
analogues. A looser clustering of hetero groups may
speciﬁcity necessary to prevent noise being introduced.
In terms of restricting noise, the more austere scheme
detailed above performs best. Completeness is sacriﬁced
to preserve accuracy.
All non-binary features are normalised over the range
to take a value between 0 and 1, as recommended with
support vector machines.
The support vector machine
The implementation of the support vector machine
used here is SVMLight†.32 The kernel functions used
were linear, polynomial and radial basis function. Other
than varying the kernel function, the algorithm was run
on default settings.
Performance analysis: leave-out-analysis
In order to validate the method, some measure of
accuracy is required. A simple but reliable method of
assessing accuracy is the leave-out method of cross
validation.33 From a set S a subset of size x is excluded,
with the remaining S–x members forming the training
set. The model built from this training set is used to predict the classes of members of the excluded subset x.
Each of the S/x subsets is left out and predicted in turn.
The sum of the accuracy on each subset divided by the
number of subsets gives an estimate of total accuracy.
At its lower extremity, x can be a portion of size containing only one member (a leave-one-out analysis), though
computationally
expensive and a larger value of x is preferred. The results
from different values of x are not always consistent.
There is debate concerning the value x should take.
Here, results are presented for x being one protein, 5%,
10% and 20% of the set. The percentage leave-out
analyses are each repeated ten times with different partitions to ensure that any result is not purely the consequence of a fortuitous splitting of the dataset. Mean
performance and standard error are averaged over the
ten partitions.
Performance analysis: newly released PDB ﬁles
Since the dataset was compiled the PDB has released
many protein structures. The majority of these proteins
are related to existing structures in some way. By using
the same Z-score criterion as before, 56 new proteins
that are not related to the dataset were identiﬁed. These
split into 29 enzymes and 23 non-enzymes, with 1L4X
being discarded due to it being a de novo designed peptide. Three further structures annotated as having no
known function were 1JAL, 1NY1 and 1ON0.
Performance analysis: comparison to ProtFun22
The ProtFun servers predict protein function from
sequence without resorting to alignments. A sequence
submitted in FASTA format is processed to generate features for a neural network-based prediction. Using the
probabilities
generated,
When a structure contained multiple chains, the average
probability was used.
Feature subset selection by adaptive programming
In the presence of two models of differing complexity
and approximately equivalent performance there is no
reason to presume that the more complex model is any
more valid than the simpler model (by the principle of
Occam’s Razor).34 Choosing a more relevant subset of
features leads to a simpler model that can often be
of greater accuracy. This can be due to noisy features
that introduce unhelpful disturbances to the joint probability distribution and make ﬁtting the hyperplane
more complex.
Methods for choosing a subset of features exist in two
basic forms.35 Filter methods rely on pre-processing the
data to ﬁnd features that differ between the two classes.
They employ techniques that quantify the extent to
which the two class distributions for a feature correlate
(with poorly correlating features being useful for distinguishing between classes). Filter methods take little
account of how the distributions of features interact in
multidimensional space. They are more suited to selection of subsets from a very large number of features,
when the computational expense of wrapper methods
can prove prohibitive. Wrapper methods use the learning algorithm itself to search for a subset of features.
Here, the learning algorithm is used as the selection
criterion in a simple adaptive programming search of
possible feature subsets.36 There are 252 possible subsets
to choose from 52 features, making an exhaustive search
impossible. Adaptive programming is well suited to
querying large search spaces. A basic adaptive programme randomly generates solutions to a problem and
then uses some measure of ﬁtness to select the parents
of the next generation of possible solutions. These
parents then reproduce with mutations. Here, a simple
adaptive programme starts from a random subset of
features. Offspring from this parent are generated by
introducing random mutations at a frequency of 10%
per reproduction (that is, ﬁve changes are made, as this
is approximately 10% of 52. It is permitted for a feature
to change and change back). On the population size
reaching 15, the leave-20%-out analysis forms the basis
of the scoring system that is used to select the model
with greatest accuracy. Here, there is a risk that the subset begins to describe the problem only for the dataset
and in so doing loses the ability to generalise. If the
† 
Predicting Protein Function from Structure
accuracy on each left-out set is similar, then the subset of
features is capable of describing the difference between
classes irrespective of the subset of data it is trained on.
If the average prediction accuracy is high, but the
standard deviation of accuracies over all left-out sets is
also high, then it can be said that the model does not
generalise so well. Low performance on some left-out
sets is indicative of a subset of features that cannot
describe the difference between classes for all data it
could be asked to predict. The selection criteria for the
adaptive programming regime are therefore based on
the leave-20%-out estimated average performance, and
on the standard deviation of these results. The maximum
tolerated standard deviation is that generated by the
whole set of features. A subset capable of predicting
with greater accuracy than the rest of its generation, but
with a standard deviation that is lower than or equivalent to the ancestor of all generations, can be said to
have specialised no more than the common ancestor as
a consequence of subset selection and so is considered
more ﬁt. For example, if the common ancestor predicts
at 75% accuracy with a standard deviation of 2.5%, a
subset that performs with .75% accuracy and #2.5% is
After selection, a precise copy of the parent is always
maintained in the next generation to avoid backwards
steps. This subset gives rise to the next generation. After
a maximum of 150 generations, the algorithm ceases.
Repetition of the algorithm suggests the existence of
different subsets of approximately equivalent performance, which is indicative of the adaptive programme
following different trajectories to multiple local maxima.
The local maximum with best performance is the best
Acknowledgements
This work was funded by a BBSRC Engineering
and Biological Systems committee studentship. We
thank Ben Stapley for helpful discussions and
Kristoffer Rapacki of the Center for Biological
University
Denmark for assistance with the ProtFun results.