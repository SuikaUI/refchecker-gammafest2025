Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2945–2950
Copenhagen, Denmark, September 7–11, 2017. c⃝2017 Association for Computational Linguistics
Zipporah: a Fast and Scalable Data Cleaning System
for Noisy Web-Crawled Parallel Corpora
Philipp Koehn
Department of Compute Science,
Center for Language and Speech Processing,
Johns Hopkins University, U.S.A., 21218
 
 
We introduce Zipporah, a fast and scalable data cleaning system. We propose a
novel type of bag-of-words translation feature, and train logistic regression models
to classify good data and synthetic noisy
data in the proposed feature space. The
trained model is used to score parallel sentences in the data pool for selection. As
shown in experiments, Zipporah selects a
high-quality parallel corpus from a large,
mixed quality data pool. In particular, for
one noisy dataset, Zipporah achieves a 2.1
BLEU score improvement with using 1/5
of the data over using the entire corpus.
Introduction
Statistical machine translation (SMT) systems require the use of parallel corpora for training the
internal model parameters. Data quality is vital
for the performance of the SMT system . To acquire a massive parallel corpus, many
researchers have been using the Internet as a resource, but the quality of data acquired from the
Internet usually has no guarantee, and data cleaning/data selection is needed before the data is used
in actual systems. Usually data cleaning refers to
getting rid of a small amount of very noisy data
from a large data pool, and data selection refers
to selecting a small subset of clean (or in-domain)
data from the data pool; both have the objective of
improving translation performances. For practical purposes, it is highly desirable to perform data
selection in a very fast and scalable manner. In
this paper we introduce Zipporah1, a fast and scalable system which can select an arbitrary size of
good data from a large noisy data pool to be used
in SMT model training.
1 
Prior Work
Many researchers have studied the data cleaning/selection problem. For data selection, there
have been a lot of work on selecting a subset of data based on domain-matching.
 used a neural network based language model trained on a small in-domain corpus to select from a larger data pool. Moore and
Lewis computed cross-entropy between indomain and out-of-domain language models to select data for training language models.
 , an open-source tool, also selects data based on cross-entropy scores on language models. Axelrod et al. utilized partof-speech tags and used a class-based n-gram language model for selecting in-domain data. There
are a few works that utilize other metrics. L¨u et
al. redistributed different weights for sentence pairs/predeﬁned sub-models. Shah and Specia described experiments on quality estimation which, given a source sentence, select the
best translation among several options. The qeclean system uses word alignments and
language models to select sentence pairs that are
likely to be good translations of one another.
For data cleaning, a lot of researchers worked
on getting rid of noising data. Taghipour et al.
 proposed an outlier detection algorithm
which leads to an improved translation quality
when trimming a small portion of data. Cui et al.
 used a graph-based random walk algorithm
to do bilingual data cleaning.
BiTextor utilizes sentence alignment scores and source URL information to ﬁlter
out bad URL pairs and selects good sentence pairs.
In this paper we propose a novel way to evaluate the quality of a sentence pair which runs
efﬁciently.
We do not make a clear distinction
between data selection and data cleaning in this
work, because under different settings, our method
can perform either based on the computed quality
scores of sentence pairs.
The method in this paper works as follows: we
ﬁrst map all sentence pairs into the proposed feature space, and then train a simple logistic regression model to separate known good data and (synthetic) bad data. Once the model is trained, it is
used to score sentence pairs in the noisy data pool.
Sentence pairs with better scores are added to the
selected subset until the desired size constraint is
Since good adequacy and ﬂuency are the major
two elements that constitute a good parallel sentence pair, we propose separate features to address
both of them. For adequacy, we propose bag-ofwords translation scores, and for ﬂuency we use ngram language model scores. For notational simplicity, in this section we assume the sentence pair
is French-English in describing the features, and
we will use subscripts f and e to indicate the languages. In designing the features, we prioritize
efﬁciency as well as performance since we could
be dealing with corpora of huge sizes.
Adequacy scores
We view each sentence as a bag of words, and design a “distance” between the sentence pairs based
on a bag-of-words translation model. To do this,
we ﬁrst generate dictionaries from an aligned corpus, and represent them as sets of triplets. Formally,
Df2e = {(wfi, wei, p(wei|wfi)), i = 1, ..., m}.
Given a sentence pair (sf, se) in the noisy data
pool, we represent the two sentence as two sparse
word-frequency vectors vf and ve.
For example for any French word wf, we have vf[wf] =
l(sf) , where c(wf, sf) is the number of occurrences of wf in sf and l(sf) is the length of sf. We
do the same for ve. Notice that by construction,
both vectors add up to 1 and represent a proper
probability distribution on their respective vocabularies. Then we “translate” vf into v′
e, based on
the probabilistic f2e dictionary, where
vf[wf]p(we|wf)
For a French word w that does not appear in the
dictionary, we keep it as it is in the translated vector, i.e. assume there is an entry of (w, w, 1.0) in
the dictionary. Since the dictionary is probabilistic, the elements in v′
e also add up to 1, and v′
represents another probability distribution on the
English vocabulary. We compute the (smoothed)
cross-entropy between ve and v′
xent(ve, v′
ve[we] log
v′e[we] + c
where c is a smoothing constant to prevent the denominator from being zero, and set c = 0.0001
for all experiments in this paper (more about this
in Section 4).
We perform similar procedures for English-to-
French, and compute xent(vf, v′
f). We deﬁne the
adequacy score as the sum of the two:
adequacy(sf, se) = xent(ve, v′
e) + xent(vf, v′
Fluency scores
We train two n-gram language models with a clean
French and English corpus, and then for each
sentence pair (sf, se), we score each sentence
with the corresponding model, Fngram(sf) and
Fngram(se), each computed as the ratio between
the sentence negative log-likelihood and the
sentence length. We deﬁne the ﬂuency score as
the sum of the two:
ﬂuency(sf, se) = Fngram(sf) + Fngram(se)
Synthetic noisy data generation
We generate synthetic noisy data from good data,
and make sure the generated noisy data include
sentence pairs with a) good ﬂuency and bad adequacy, b) good adequacy and bad ﬂuency and c)
Respectively, we generate 3 types of “noisy”
sentence pairs from a good corpus: a) shufﬂe the
sentences in the target language ﬁle (each sentence
in the source language would be aligned to a random sentence in the target language); b) shufﬂe the
words within each sentence (each sentence will be
bad but the pairs are good translations in the “bagof-words” sense); c) shufﬂe both the sentences and
We emphasize that, while the synthetic
data might not represent “real” noisy data, it has
the following advantages: 1) each type of noisy
data is equally represented so the classiﬁer has to
do well on all of them; 2) the data generated this
way would be among the hardest to classify, especially type a and type b, so if a classiﬁer separates
such hard data with good performance, we expect
it to also be able to do well in real world situations.
Logistic regression feature mapping
Figure 1: newstest09 fr-en data in the feature space
We plot the newstest09 data (original and autogenerated noisy ones as described in Section
3.2) into the proposed feature space in Figure 1.
We observe that the clusters are quite separable,
though the decision function would not be linear.
We map the features into higher order forms of
(xn, yn) in order for logistic regression to train a
non-linear decision boundary.2 We use n = 8 in
this work since it gives the best classiﬁcation performance on the newstest09 fr-en corpus.
Hyper-parameter Tuning
We conduct experiments to determine the value
of the constant c in the smoothed cross-entropy
computation in equation 1. We choose the newstest09 German-English corpus, and shufﬂe the
sentences in the English ﬁle and combine the original (clean) corpus with the shufﬂed (noisy) corpus into a larger corpus, where half of them are
good sentence pairs. We set different values of c
and use the adequacy scores to pick the better half,
2We avoid using multiple mappings of one feature because we want the scoring function to be monotonic both w.r.t
x and y, which could break if we allow multiple higher-order
mappings of the same feature and they end up with weights
with different signs.
and compute the retrieval accuracy. Table 1 shows
that the best value for c is 0.0001, and we use that
in all experiments.
Table 1: Tuning cross-entropy constant c
Evaluation
We evaluate Zipporah on 3 language pairs,
French-English, German-English and Spanish-
English. The noisy web-crawled data comes from
an early version of 
paracrawl. The number of words are (in millions) 340, 487 and 70 respectively.
To generate the dictionaries for computing the
adequacy scores, we use fast align to align the Europarl corpus and generate probabilistic dictionaries from
the alignments. We set the n-gram order to be 5
and use SRILM to train language models on the Europarl corpus and generate
the n-gram scores.
For each language pair, we use scikit-learn to train a logistic regression
model to classify between the original and the synthetic noisy corpus of newstest09, and the trained
model is used to score all sentence pairs in the data
pool. We keep selecting the best ones until the desired number of words is reached.
To evaluate the quality, we train a Moses
 SMT system on selected data,
and evaluate each trained SMT system on 3 test
corpora: newstest2011 which contains 3003 sentence pairs, and a random subset of the TED-talks
corpus and the movie-subtitle corpus from OPUS
 , each of which contains 3000
sentence pairs.
Tables 2, 3 and 4 show the BLEU performance
of the selected subsets of the Zipporah system
compared to the baseline, which selects sentence
pairs at random; for comparison, we also give
the BLEU performance of systems trained on Europarl.
The Zipporah system gives consistently
better performance across multiple datasets and
multiple languages than the baseline.3
3We also point out that the performance of the selected
newstest11
num-words rand zipp rand zipp rand zipp
10 million
21.5 24.4 24.0 27.4 12.3 14.9
20 million
22.8 25.1 25.0 27.9 12.8 15.5
50 million
24.3 26.0 27.4 28.8 14.5 15.8
100 million 25.2 26.6 28.3 30.3 15.0 17.3
200 million 26.1 26.7 29.9 30.0 16.4 17.3
340 mil (all)
Table 2: BLEU Performance, French-English
newstest11
num-words rand zipp rand zipp rand zipp
10 million
13.6 17.6 17.0 22.5 11.4 15.8
20 million
14.8 18.4 18.9 23.7 12.7 16.9
50 million
16.3 19.2 20.8 24.8 13.9 17.8
100 million 16.9 19.5 21.3 25.0 14.0 18.3
200 million 18.0 19.2 22.9 24.2 15.3 17.9
487 mil (all)
Table 3: BLEU Performance, German-English
newstest11
num-words rand zipp rand zipp rand zipp
10 million 24.2 25.5 25.9 28.3 17.9 19.8
20 million 25.3 26.2 28.2 29.7 19.3 21.2
50 million 26.6 26.5 29.9 30.4 21.3 21.4
70 mil (all)
Table 4: BLEU Performance, Spanish-English
In particular, for the Germen-English corpus,
when selecting less than 2% of the data (10 million words), on the TED-talk dataset, Zipporah
achieves a 5.5 BLEU score improvement over the
baseline; by selecting less than 4% of the data
(20 million words) the system gives better performance than using all data. Peak performance is
achieved when selecting 100 million words, where
an improvement of 2.1 BLEU score over all data
is achieved on the movie-subtitle dataset, despite
only using less than 1/5 of the data.
Figure 2: BLEU performance of Zipporah, qeclean and random on TED-talks, French-English
Figure 3: BLEU performance of Zipporah, qeclean and random on newstest11, German-English
Figures 2, 3 and 4 compare the result of Zipporah with that of qe-clean and the random
baseline. We use the same data when running qeclean, with Europarl for training and newstest09
for dev. While they both perform comparably and
better than the baseline, Zipporah achieves a better peak in all the datasets, and the peak is usually achieved when selecting a smaller number of
words compared to qe-clean, Another advantage
of Zipporah is it allows the user to select an arbisubsets of the Zipporah system can surpass that of Europarl,
although the Europarl corpus acts like an “oracle” in the system, upon which the dictionaries and language models for
feature computations are trained.
Figure 4: BLEU performance of Zipporah, qeclean and random on TED-talks, Spanish-English
trary size from the pool.4 We also want to emphasize that unlike qe-clean, which requires running
word-alignments for all sentence pairs in the noisy
corpus, Zipporah’s feature computation is simple,
fast and can easily be scaled for huge datasets.
Conclusion and Future Work
In this paper we introduced Zipporah, a fast data
selection system for noisy parallel corpora. SMT
results demonstrate that Zipporah can select a
high-quality subset of the data and signiﬁcantly
improve SMT performance.
Zipporah currently selects sentences based on
the “individual quality” only, and we plan in future
work to also consider other factors, e.g. encourage
selection of a subset that has a better n-gram coverage.
Acknowledgments
This project was funded by Google Faculty Research Award. The authors would like to thank
Shuoyang Ding, Tongfei Chen, Matthew Wiesner,
Winston Wu, Huda Khayrallah and Adi Renduchintala for their help during this project. The authors would also like to thank Penny Peng for her
moral support.