Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1173–1186,
November 16–20, 2020. c⃝2020 Association for Computational Linguistics
ToTTo: A Controlled Table-To-Text Generation Dataset
Ankur P. Parikh♠
Xuezhi Wang♠
Sebastian Gehrmann♠
Manaal Faruqui♠
Bhuwan Dhingra♣∗Diyi Yang♠♦
Dipanjan Das♠
♠Google Research, New York, NY
♦Georgia Tech, Atlanta, GA
♣Carnegie Mellon University, Pittsburgh, PA
 
We present TOTTO, an open-domain English
table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of
highlighted table cells, produce a one-sentence
description. To obtain generated targets that
are natural but also faithful to the source table,
we introduce a dataset construction process
where annotators directly revise existing candidate sentences from Wikipedia. We present
systematic analyses of our dataset and annotation process as well as results achieved by
several state-of-the-art baselines. While usually ﬂuent, existing methods often hallucinate
phrases that are not supported by the table, suggesting that this dataset can serve as a useful
research benchmark for high-precision conditional text generation.1
Introduction
Data-to-text generation is the task of generating a target textual description y conditioned on source content x in
the form of structured data such as a table. Examples include generating sentences given biographical data , textual descriptions of restaurants given meaning representations , basketball game summaries given boxscore statistics , and generating fun facts from superlative
tables in Wikipedia .
Existing data-to-text tasks have provided an
important test-bed for neural generation models .
Neural models are known to be prone to hallucination, i.e., generating text that is ﬂuent but not
faithful to the source and it is often easier to assess faithfulness
of the generated text when the source content is
structured . Moreover, structured data can also test a
model’s ability for reasoning and numerical inference and for building representations of structured objects ,
providing an interesting complement to tasks that
test these aspects in the NLU setting .
However, constructing a data-to-text dataset can
be challenging on two axes: task design and annotation process.
First, tasks with open-ended
output like summarization lack explicit
signals for models on what to generate, which
can lead to subjective content and evaluation challenges . On the other hand,
data-to-text tasks that are limited to verbalizing
a fully speciﬁed meaning representation do not test a model’s ability to perform inference and thus remove a considerable
amount of challenge from the task.
Secondly, designing an annotation process to
obtain natural but also clean targets is a signiﬁcant challenge. One strategy employed by many
datasets is to have annotators write targets from
scratch which can often lack variety
in terms of structure and style . An alternative is to pair
naturally occurring text with tables . While more diverse,
naturally occurring targets are often noisy and contain information that cannot be inferred from the
source. This can make it problematic to disentangle
modeling weaknesses from data noise.
In this work, we propose TOTTO, an opendomain table-to-text generation dataset that intro-
Table Title: Gabriele Becker
Section Title: International Competitions
Table Description: None
Competition
Representing Germany
World Junior Championships
Seoul, South Korea
10th (semis)
European Junior Championships
San Sebasti´an, Spain
4x100 m relay
12th (semis)
11.66 (wind: +1.3 m/s)
World Junior Championships
Lisbon, Portugal
4x100 m relay
7th (q-ﬁnals)
World Championships
Gothenburg, Sweden
4x100 m relay
Original Text: After winning the German under-23 100 m title, she was selected to run at the 1995 World Championships
in Athletics both individually and in the relay.
Text after Deletion: she at the 1995 World Championships in both individually and in the relay.
Text After Decontextualization: Gabriele Becker competed at the 1995 World Championships
in both individually and in the relay.
Final Text: Gabriele Becker competed at the 1995 World Championships both individually and in the relay.
Table 1: Example in the TOTTO dataset. The goal of the task is given the table, table metadata (such as the title),
and set of highlighted cells, to produce the ﬁnal text. Our data annotation process revolves around annotators
iteratively revising the original text to produce the ﬁnal text.
duces a novel task design and annotation process
to address the above challenges. First, TOTTO
proposes a controlled generation task: given a
Wikipedia table and a set of highlighted cells as
the source x, the goal is to produce a single sentence description y. The highlighted cells identify
portions of potentially large tables that the target
sentence should describe, without specifying an
explicit meaning representation to verbalize.
For dataset construction, to ensure that targets
are natural but also faithful to the source table,
we request annotators to revise existing Wikipedia
candidate sentences into target sentences, instead
of asking them to write new target sentences . Table 1 presents
a simple example from TOTTO to illustrate our annotation process. The table and Original Text were
obtained from Wikipedia using heuristics that collect pairs of tables x and sentences y that likely
have signiﬁcant semantic overlap. This method ensures that the target sentences are natural, although
they may only be partially related to the table. Next,
we create a clean and controlled generation task by
requesting annotators to highlight a subset of the
table that supports the original sentence and revise
the latter iteratively to produce a ﬁnal sentence (see
§5). For instance, in Table 1, the annotator has chosen to highlight a set of table cells (in yellow) that
support the original text. They then deleted phrases
from the original text that are not supported by the
table, e.g., After winning the German under-23 100
m title, and replaced the pronoun she with an entity
Gabriele Becker. The resulting ﬁnal sentence (Final Text) serves as a more suitable generation target
than the original sentence. This annotation process
makes our dataset well suited for high-precision
conditional text generation.
Due to the varied nature of Wikipedia tables,
TOTTO covers a signiﬁcant variety of domains
while containing targets that are completely faithful to the source (see Table 4 and the Appendix for
more complex examples). Our experiments demonstrate that state-of-the-art neural models struggle
to generate faithful results, despite the high quality of the training data. These results suggest that
our dataset could serve as a useful benchmark for
controllable data-to-text generation.
Related Work
TOTTO differs from existing datasets in both task
design and annotation process as we describe below.
A summary is given in Table 2.
Task Design
Most existing table-to-text datasets
are restricted in topic and schema such as WEATH-
ERGOV , ROBOCUP , Rotowire , E2E , KBGen , and Wikibio . In contrast, TOTTO contains tables with
various schema spanning various topical categories
all over Wikipedia. Moreover, TOTTO takes a
different view of content selection compared to
Train Size
Target Quality
Target Source
Content Selection
Wikibio 
Biographies
Not speciﬁed
Rotowire 
Basketball
Not speciﬁed
WebNLG 
15 DBPedia categories
Annotator Generated
Fully speciﬁed
E2E 
Restaurants
Annotator Generated
Partially speciﬁed
LogicNLG 
Wikipedia (open-domain)
Annotator Generated
Columns via entity linking
Wikipedia (open-domain)
Wikipedia (Annotator Revised)
Annotator highlighted
Table 2: Comparison of popular data-to-text datasets. TOTTO combines the advantages of annotator-generated
and fully natural text through a revision process.
existing datasets. Prior to the advent of neural approaches, generation systems typically separated
content selection (what to say) from surface realization (how to say it) .
Thus many generation datasets only focused on
the latter stage . However, this decreases the task complexity, since neural systems have already been quite
powerful at producing ﬂuent text. Some recent
datasets 
have proposed incorporating content selection into
the task by framing it as a summarization problem.
However, summarization is much more subjective,
which can make the task underconstrained and difﬁcult to evaluate . We place
TOTTO as a middle-ground where the highlighted
cells provide some guidance on the topic of the target but still leave a considerable amount of content
planning to be done by the model.
Annotation Process
There are various existing
strategies to create the reference target y. One
strategy employed by many datasets is to have annotators write targets from scratch given a representation of the source . While this will result
in a target that is faithful to the source data, it often
lacks variety in terms of structure and style . Domainspeciﬁc strategies such as presenting an annotator
an image instead of the raw data are not practical for some of the complex
tables that we consider. Other datasets have taken
the opposite approach: ﬁnding real sentences on
the web that are heuristically selected in a way that
they discuss the source content . This strategy typically leads
to targets that are natural and diverse, but they may
be noisy and contain information that cannot be
inferred from the source .To
construct TOTTO, we ask annotators to revise existing candidate sentences from Wikipedia so that
they only contain information that is supported by
the table. This enables TOTTO to maintain the
varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used
in semiparametric generation models and
crowd-sourcing small, iterative changes to text has
been shown to lead to higher-quality data and a
more robust annotation process .
Perez-Beltrachini and Lapata also employed
a revision strategy to construct a cleaner evaluation
set for Wikibio .
Concurrent to this work, Chen et al. proposed LogicNLG which also uses Wikipedia tables,
although omitting some of the more complex structured ones included in our dataset. Their target
sentences are annotator-generated and their task is
signiﬁcantly more uncontrolled due to the lack of
annotator highlighted cells.
Preliminaries
Our tables come from English Wikipedia articles
and thus may not be regular grids.2 For simplicity,
we deﬁne a table t as a set of cells t = {cj}τ
where τ is the number of cells in the table. Each
cell contains: (1) a string value, (2) whether or
not it is a row or column header, (3) the row and
column position of this cell in the table, (4) The
number of rows and columns this cell spans.
Let m = (mpage-title, msection-title, msection-text)
indicate table metadata, i.e, the page title, section title, and up to the ﬁrst 2 sentences of the
section text (if present) respectively. These ﬁelds
can help provide context to the table’s contents.
Let s = (s1, ..., sη) be a sentence of length η. We
deﬁne an annotation example3 d = (t, m, s) a tuple of table, table metadata, and sentence. Here,
n=1 refers to a dataset of annotation
2In Wikipedia, some cells may span multiple rows and
columns. See Table 1 for an example.
3An annotation example is different than a task example
since the annotator could perform a different task than the
examples of size N.
Dataset Collection
We ﬁrst describe how to obtain annotation examples d for subsequent annotation. To prevent any
overlap with the Wikibio dataset , we do not use infobox tables. We employed
three heuristics to collect tables and sentences:
Number matching
We search for tables and sentences on the same Wikipedia page that overlap
with a non-date number of at least 3 non-zero digits.
This approach captures most of the table-sentence
pairs that describe statistics (e.g., sports, election,
census, science, weather).
Cell matching
We extract a sentence if it has
tokens matching at least 3 distinct cell contents
from the same row in the table. The intuition is
that most tables are structured, and a row is usually
used to describe a complete event.
Hyperlinks
The above heuristics only consider
sentences and tables on the same page. We also
ﬁnd examples where a sentence s contains a hyperlink to a page with a title that starts with List (these
pages typically only consist of a large table). If the
table t on that page also has a hyperlink to the page
containing s, then we consider this to be an annotation example. Such examples typically result in
more diverse examples than the other two heuristics, but also add more noise, since the sentence
may only be distantly related to the table.
Using the above heuristics we obtain a set of
examples D. We then sample a random subset of
tables for annotation, excluding tables with formatting issues: 191,693 examples for training, 11,406
examples for development, and 11,406 examples
for test. Among these examples, 35.8% were derived from number matching, 29.4% from cell
matching, and 34.7% from hyperlinks.
Data Annotation Process
The collected annotation examples are noisy since
a sentence s may only be partially supported by the
table t. We thus deﬁne an annotation process that
guides annotators through incremental changes to
the original sentence. This allows us to measure
annotator agreement at every step of the process,
which is atypical in existing generation datasets.
The primary annotation task consists of the following steps: (1) Table Readability, (2) Cell highlighting, (3) Phrase Deletion, (4) Decontextualization. After these steps we employ a ﬁnal secondary
annotation task for grammar correction. Each of
these are described below and more examples are
provided in the Table 3.
Table Readability
If a table is not readable, then
the following steps will not need to be completed.
This step is only intended to remove fringe cases
where the table is poorly formatted or otherwise
not understandable (e.g., in a different language).
99.5% of tables are determined to be readable.
Cell Highlighting
An annotator is instructed to
highlight cells that support the sentence. A phrase
is supported by the table if it is either directly stated
in the cell contents or meta-data, or can be logically
inferred by them. Row and column headers do not
need to be highlighted. If the table does not support
any part of the sentence, then no cell is marked and
no other step needs to be completed. 69.7% of examples are supported by the table. For instance, in
Table 1, the annotator highlighted cells that support
the phrases 1995, World Championships, individually, and relay. The set of highlighted cells are
denoted as a subset of the table: thighlight ∈t.
Phrase Deletion
This step removes phrases in
the sentence unsupported by the selected table cells.
Annotators are restricted such that they are only
able to delete phrases, transforming the original
sentence: s →sdeletion. In Table 1, the annotator
transforms s by removing several phrases such as
After winning the German under-23 100 m title.
On average, sdeletion is different from s for 85.3%
of examples and while s has an average length of
26.6 tokens, this is reduced to 15.9 for sdeletion. We
found that the phrases annotators often disagreed
on corresponded to verbs purportedly supported by
the table.
Decontextualization
A given sentence s may
contain pronominal references or other phrases that
depend on context. We thus instruct annotators to
identify the main topic of the sentence; if it is a
pronoun or other ambiguous phrase, we ask them
to replace it with a named entity from the table or
metadata. To discourage excessive modiﬁcation,
they are instructed to make at most one replacement.4
This transforms the sentence yet again:
4Based on manual examination of a subset of 100 examples, all of them could be decontextualized with only one
replacement. Allowing annotators to make multiple replacements led to excessive clariﬁcation.
After Deletion
After Decontextualization
He later raced a Nissan Pulsar and
then a Mazda 626 in this series, with
a highlight of ﬁnishing runner up to
Phil Morriss in the 1994 Australian
Production Car Championship.
He later raced a Nissan Pulsar and
then a Mazda 626 in this series, with
a highlight of ﬁnishing runner up to
Phil Morriss in the 1994 Australian
Production Car Championship.
Murray Carter raced a Nissan Pulsar and ﬁnished as a runner up in
the 1994 Australian Production Car
Championship.
Murray Carter raced a Nissan Pulsar and ﬁnished as runner up in
the 1994 Australian Production Car
Championship.
On July 6, 2008, Webb failed to qualify for the Beijing Olympics in the
1500 m after ﬁnishing 5th in the US
Olympic Trials in Eugene, Oregon
with a time of 3:41.62.
On July 6, 2008, Webb failed to
qualify for the Beijing Olympics in
the 1500 m after ﬁnishing 5th in the
US Olympic Trials in Eugene, Oregon with a time of 3:41.62.
On July 6, 2008, Webb ﬁnishing
5th in the Olympic Trials in Eugene,
Oregon with a time of 3:41.62.
On July 6, 2008, Webb ﬁnished 5th
in the Olympic Trials in Eugene,
Oregon, with a time of 3:41.62.
Out of the 17,219 inhabitants, 77 percent were 20 years of age or older
and 23 percent were under the age of
Out of the 17,219 inhabitants , 77
percent were 20 years of age or older
and 23 percent were under the age of
Rawdat Al Khail had a population
of 17,219 inhabitants.
Rawdat Al Khail had a population
of 17,219 inhabitants.
Table 3: Examples of annotation process. Deletions are indicated in red strikeouts, while added named entities are
indicated in underlined blue. Signiﬁcant grammar ﬁxes are denoted in orange.
sdeletion →sdecontext. In Table 1, the annotator
replaced she with Gabriele Becker.
Since the previous steps can lead to ungrammatical sentences, annotators are also instructed
to ﬁx the grammar to improve the ﬂuency of the
sentence. We ﬁnd that sdecontext is different than
sdeletion 68.3% of the time, and the average sentence length increases to 17.2 tokens for sdecontext
compared to 15.9 for sdeletion.
Secondary Annotation Task
Due to the complexity of the task, sdecontext may still have grammatical errors, even if annotators were instructed
to ﬁx grammar. Thus, a second set of annotators
were asked to further correct the sentence and were
shown the table with highlighted cells as additional
context. This results in the ﬁnal sentence sﬁnal. On
average, annotators edited the sentence 27.0% of
the time, and the sentence length slightly increased
to 17.4 tokens from 17.2.
Dataset Analysis
Basic statistics of TOTTO are described in Table 5.
The number of unique tables and vocabulary size attests to the open domain nature of our dataset. Furthermore, while the median table is actually quite
large (87 cells), the median number of highlighted
cells is signiﬁcantly smaller (3). This indicates the
importance of the cell highlighting feature of our
dataset toward a well-deﬁned text generation task.
Annotator Agreement
Table 6 shows annotator agreement over the development set for each step of the annotation process.
We compute annotator agreement and Fleiss’ kappa
 for table readability and highlighted
cells, and BLEU-4 score between annotated sentences in different stages.
Entertainment
Literature
Broadcasting
North America
Performing Arts
Figure 1: Topic distribution of our dataset.
As one can see, the table readability task has
an agreement of 99.38%. The cell highlighting
task is more challenging. 73.74% of the time all
three annotators completely agree on the set of
cells which means that they chose the exact same
set of cells. The Fleiss’ kappa is 0.856, which
is regarded as “almost perfect agreement” (0.81 -
1.00) according to .
With respect to the sentence revision tasks, we
see that the agreement slightly degrades as more
steps are performed. We compute single reference
BLEU among all pairs of annotators for examples
in our development set (which only contains examples where both annotators chose thighlight ̸= ∅).
As the sequence of revisions are performed, the
annotator agreement gradually decreases in terms
of BLEU-4: 82.19 →72.56 →68.98. This is
considerably higher than the BLEU-4 between the
original sentence s and sﬁnal (43.17).
Topics and Linguistic Phenomena
We use the Wikimedia Foundation’s topic categorization model to
sort the categories of Wikipedia articles where the
Table Title: Robert Craig (American football)
Section Title: National Football League statistics
Table Description:None
Target Text: Craig ﬁnished his eleven NFL seasons with 8,189 rushing yards and 566 receptions for 4,911 receiving yards.
Table 4: An example in the TOTTO dataset that involves numerical reasoning over the table structure.
Training set size
Number of target tokens
Avg Target Length (tokens)
Target vocabulary size
Unique Tables
Rows per table (Median/Avg)
Cells per table (Median/Avg)
87 / 206.6
No. of Highlighted Cell (Median/Avg)
Development set size
Test set size
Table 5: TOTTO dataset statistics.
Annotation Stage
Table Readability
Agreement / κ
99.38 / 0.646
Cell Highlighting
Agreement / κ
73.74 / 0.856
After Deletion
After Decontextualization
Table 6: Annotator agreement over the development set.
If possible, we measure the total agreement (in %) and
the Fleiss’ Kappa (κ). Otherwise, we report the BLEU-
4 between annotators.
tables come from into a 44-category ontology.5 Figure 1 presents an aggregated topic analysis of our
dataset. We found that the Sports and Countries
topics together comprise 53.4% of our dataset, but
the other 46.6% is composed of broader topics such
as Performing Arts, Politics, and North America.
Our dataset is limited to topics that are present in
Wikipedia.
Table 7 summarizes the fraction of examples
that require reference to the metadata, as well as
some of the challenging linguistic phenomena in
the dataset that potentially pose new challenges to
5 
Wikipedia:WikiProject_Council/Directory
Percentage
Require reference to page title
Require reference to section title
Require reference to table description
Reasoning (logical, numerical, temporal etc.)
Comparison across rows / columns / cells
Require background information
Table 7: Distribution of different linguistic phenomena
among 100 randomly chosen sentences.
current systems. Table 4 gives one example that
requires reasoning (refer to the Appendix for more
examples).
Training, Development, and Test Splits
After the annotation process, we only consider examples where the sentence is related to the table,
i.e., thighlight ̸= ∅. This initially results in a training
set Dorig-train of size 131,849 that we further ﬁlter
as described below. Each example in the development and test sets was annotated by three annotators. Since the machine learning task uses thighlight
as an input, it is challenging to use three different
sets of highlighted cells in evaluation. Thus, we
only use a single randomly chosen thighlight while
using the three sﬁnal as references for evaluation.
We only use examples where at least 2 of the 3
annotators chose thighlight ̸= ∅, resulting in development and test sets of size 7,700 each.
Overlap and Non-Overlap Sets
Without any
modiﬁcation Dorig-train, Ddev, and Dtest may contain many similar tables. Thus, to increase the generalization challenge, we ﬁlter Dorig-train to remove
some examples based on overlap with Ddev, Dtest.
For a given example d, let h(d) denote its set of
header values and similarly let h(D) be the set of
header values for a given dataset D. We remove
examples d from the training set where h(d) is
both rare in the data as well as occurs in either
the development or test sets. Speciﬁcally, Dtrain is
deﬁned as:
Dtrain := {d : h(d) /∈(h(Ddev) ∪h(Dtest)) or
 h(d), Dorig-train
The count(h(d), Dorig-train) function returns the
number of examples in Dorig-train with header h(d).
To choose the hyperparameter α we ﬁrst split the
test set as follows:
Dtest-overlap := {d : h(d) ∈h(Dtrain)}
Dtest-nonoverlap := {d : h(d) /∈h(Dtrain)}
The development set is analogously divided into
Ddev-overlap and Ddev-nonoverlap. We then choose
5 so that Dtest-overlap and Dtest-nonoverlap
have similar size.
After ﬁltering, the size of
Dtrain is 120,761, and Ddev-overlap, Ddev-nonoverlap,
Dtest-overlap, and Dtest-nonoverlap have sizes 3784,
3916, 3853, and 3847 respectively.
Machine Learning Task Construction
In this work, we focus on the following task: Given
a table t, related metadata m (page title, section title, table section text) and a set of highlighted cells
thighlight, produce the ﬁnal sentence sﬁnal. Mathematically this can be described as learning a function f : x →y where x = (t, m, thighlight) and
y = sﬁnal. This task is different from what the annotators perform, since they are provided a starting
sentence requiring revision. Therefore, the task is
more challenging, as the model must generate a
new sentence instead of revising an existing one.
Experiments
We present baseline results on TOTTO by examining three existing state-of-the-art approaches (Note
that since our tables do not have a ﬁxed schema it
is difﬁcult to design a template baseline).
• BERT-to-BERT : A Transformer encoder-decoder model where the encoder and decoder are both
initialized with BERT .
The original BERT model is pre-trained with
both Wikipedia and the Books corpus , the former of which contains
our (unrevised) test targets. Thus, we also
pre-train a version of BERT on the Books corpus only, which we consider a more correct
baseline. However, empirically we ﬁnd that
both models perform similarly in practice (Table 8).
• Pointer-Generator :
Seq2Seq model with attention and copy mechanism. While originally designed for summarization it is commonly used in data-to-text as
well .
• Puduppully et al. : A Seq2Seq model
with an explicit content selection and planning
mechanism designed for data-to-text.
Details about hyperparameter settings are provided
in the Appendix. Moreover, we explore different
strategies of representing the source content that
resemble standard linearization approaches in the
literature 
• Full Table The simplest approach is simply
to use the entire table as the source, adding
special tokens to mark which cells have been
highlighted. However, many tables can be
very large and this strategy performs poorly.
• Subtable Another option is to only use the
highlighted cells thighlight ∈t with the heuristically extracted row and column header for
each highlighted cell. This makes it easier for
the model to only focus on relevant content but
limits the ability to perform reasoning in the
context of the table structure (see Table 11).
Overall though, we ﬁnd this representation
leads to higher performance.
In all cases, the cells are linearized with row and
column separator tokens. We also experiment with
prepending the table metadata to the source table.6
Evaluation metrics
The model output is evaluated using two automatic metrics: BLEU and PARENT .
PARENT is a metric recently proposed speciﬁcally
for data-to-text evaluation that takes the table into
account. We modify it to make it suitable for our
dataset, described in the Appendix. Human evaluation is described in § 8.2.
Table 8 shows our results against multiple references with the subtable input format. Both the
6The table section text is ignored, since it is usually missing or irrelevant.
Overlap Subset
Nonoverlap Subset
BERT-to-BERT (Books+Wiki)
BERT-to-BERT (Books)
Pointer-Generator
Puduppully et al. 
Table 8: Performance compared to multiple references on the test set for the subtable input format with metadata.
Fluency (%)
Faithfulness (%)
Covered Cells (%)
Less/Neutral/More Coverage w.r.t. Ref
18.3 / 61.7 / 20.0
BERT-to-BERT (Books)
49.2 / 36.2 / 14.5
BERT-to-BERT (Books+Wiki)
53.9 / 32.9 / 13.2
19.8 / 62.8 / 17.4
BERT-to-BERT (Books)
42.0 / 43.7 / 14.3
BERT-to-BERT (Books+Wiki)
47.8 / 39.2 / 13.1
Non-overlap
17.0 / 60.9 / 22.1
BERT-to-BERT (Books)
55.5 / 29.8 / 14.7
BERT-to-BERT (Books+Wiki)
60.1 / 26.6 / 13.3
Table 9: Human evaluation over references (to compute Oracle) and model outputs. For Fluency, we report the
percentage of outputs that were completely ﬂuent. In the last column X/Y/Z means X% and Z% of the candidates
were deemed to be less and more informative than the reference respectively and Y% were neutral.
Data Format
subtable w/ metadata
subtable w/o metadata
full table w/ metadata
full table w/o metadata
Table 10: Multi-reference performance of different input representations for BERT-to-BERT Books model.
BERT-to-BERT models perform the best, followed
by the pointer generator model.7 We see that for
all models the performance on the non-overlap set
is signiﬁcantly lower than that of the overlap set,
indicating that slice of our data poses signiﬁcant
challenges for machine learning models. We also
observe that the baseline that separates content selection and planning performs quite poorly. We
attest this to the fact that it is engineered to the
Rotowire data format and schema.
Table 10 explores the effects of the various input representations (subtable vs. full table) on the
BERT-to-BERT model. We see that the full table format performs poorly even if it is the most
knowledge-preserving representation.
Human evaluation
For each of the 2 top performing models in Table 8,
we take 500 random outputs and perform human
evaluation using the following axes:
• Fluency - A candidate sentence is ﬂuent if it
is grammatical and natural. The three choices
are Fluent, Mostly Fluent, Not Fluent.
7Note the BLEU scores are relatively high due to the fact
that our task is more controlled than other text generation tasks
and that we have multiple references.
• Faithfulness (Precision) - A candidate sentence is considered faithful if all pieces of
information are supported by either the table
or one of the references. Any piece of unsupported information makes the candidate
unfaithful.
• Covered Cells (Recall) - Percentage of highlighted cells the candidate sentence covers.
• Coverage with Respect to Reference (Recall) - We ask whether the candidate is strictly
more or less informative than each reference
(or neither, which is referred to as neutral).
We further compute an oracle upper-bound by
treating one of the references as a candidate and
evaluating it compared to the table and other references. The results, shown in Table 9, attest to
the high quality of our human annotations since the
oracle consistently achieves high performance. All
the axes demonstrate that there is a considerable
gap between the model and oracle performance.
This difference is most easily revealed in the last
column when annotators are asked to directly compare the candidate and reference. As expected, the
oracle has similar coverage to the reference (61.7%
neutral) but both baselines demonstrate considerably less coverage. According to an independentsample t-test, this difference is signiﬁcant at a
p < 0.001 level for both baselines. Furthermore,
the baselines are considerably less faithful than
the reference. The faithfulness of both models
is signiﬁcantly lower than the reference (χ2 test
with p < 0.001). The models do not differ signiﬁcantly from each other, except for faithfulness
Decoder output (w/ metadata)
w/o metadata
Full table
in the 1939 currie cup, western
province lost to transvaal by 176 in
cape town.
northern transvaal and western
in 1989 and in 1995, western
province were defeated by the
sharks in durban.
the ﬁrst currie cup was played
in 1939 in transvaal1 at newlands,
with western province winning 176.
the ﬁrst scottish cup was played
transvaal1,
transvaal1
province 176 in the ﬁnal at newlands in cape town.
a second generation of microdrive
was announced by ibm in 2000 with
increased capacities at 512 mb and 1
the microdrive models formed 512
megabyte and 1 gigabyte in 2000.
there were 512 microdrive models
in 2000: 1 gigabyte.
production
the 1956 grand prix motorcycle racing season consisted of six grand
prix races in ﬁve classes:
350cc, 250cc, 125cc and sidecars
the 1966 grand prix motorcycle
racing season consisted of seven
grand prix races in ﬁve classes:
500cc, 350cc, 250cc, 125cc and
sidecars 500cc.
the 1956 grand prix motorcycle racing season consisted of eight grand
prix races in ﬁve classes: 500cc,
350cc, 250cc, 125cc and sidecars
the 1955 grand prix motorcycle racing season consisted of eight grand
prix races in ﬁve classes: 500cc,
350cc, 250cc, 125cc and sidecars
in travis kelce’s last collegiate season, he set personal career highs
in receptions (45), receiving yards
(722), yards per receptions (16.0)
and receiving touchdowns (8).
during the 2011 season, travis kelceum caught 76 receptions for
1,612 yards and 14 touchdowns.
travis kelce ﬁnished the 2012 season with 45 receptions for 722
yards (16.0 avg.) and eight touchdowns.
kelce ﬁnished the 2012 season with
45 catches for 722 yards (16.0 avg.)
and eight touchdowns.
in the 2012 ﬁlm pizza bagel, michael
pillarella portrays tommy.
in 2012, groff played the role of
tommy in the ﬁlm pizza bagel.
pillarella appeared as
tommy in the ﬁlm pizza bagel.
harris played the role of tommy in
the 2012 ﬁlm pizza bagel.
the album shari addison placed at no.
176 on the billboard 200 along with
no. 5 on the gospel albums.
shari addison’s ” 5”, reached number 176 on the billboard 200.
shari addison charted at number 176
on the us chart and at number 5 on
the us billboard 200.
the shari addison peaked at number
176 on the billboard 200 chart.
Table 11: Decoder output examples from BERT-to-BERT Books models on the development set. The “subtable
with metadata” model achieves the highest BLEU. Red indicates model errors and blue denotes interesting reference language not in the model output.
in the non-overlap case, where we see a moderate
effect favoring the book model.
Model Errors and Challenges
Table 11 shows predictions from the BERT-to-
BERT Books model to illustrate challenges existing
models face.
Hallucination
The model sometimes outputs
phrases such as ﬁrst, winning that seem reasonable but are not faithful to the table. This hallucination phenomenon has been widely observed in
other existing data-to-text datasets . However, the noisy
references in these datasets make it difﬁcult to disentangle model incapability from data noise. Our
dataset serves as strong evidence that even when
the reference targets are faithful to the source, neural models still struggle with faithfulness.
Rare topics
Another challenge revealed by the
open domain nature of our task is rare or complex
topics at the tail of the topic distribution (Figure 1).
For instance, example 2 of Table 11 concerns microdrive capacities which is challenging.
Diverse table structure and numerical reasoning
In example 3, inferring six and ﬁve correctly
requires counting table rows and columns. Similarly, in example 4, the phrases last and career
highs can be deduced from the table structure and
with comparisons over the columns. However, the
model is unable to make these inferences from the
simplistic source representation that we used.
Evaluation metrics
Many of the above issues
are difﬁcult to capture with metrics like BLEU
since the reference and prediction may only differ
by a word but largely differ in terms of semantic
meaning. This urges for better metrics possibly
built on learned models . Thus, while we
have a task leaderboard, it should not be interpreted
as the deﬁnitive measure of model performance.
Conclusion
We presented TOTTO, a table-to-text dataset
that presents a controlled generation task and
annotation
iterative sentence revision.
We also provided
several state-of-the-art baselines, and demonstrated TOTTO could serve as a useful research
benchmark for model and metric development.
TOTTO is available at 
google-research-datasets/totto.
Acknowledgements
The authors wish to thank Ming-Wei Chang,
Jonathan H. Clark, Kenton Lee, and Jennimaria
Palomaki for their insightful discussions and support. Many thanks also to Ashwin Kakarla and his
team for help with the annotations.