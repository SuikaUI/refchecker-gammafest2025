Copyright 2002 Psychonomic Society, Inc.
Psychonomic Bulletin & Review
2002, 9 (3), 438-481
Sequentialsamplingmodelsare currentlythemodelsmost
successful in accounting for data from simple two-choice
tasks. Among these, diffusion models have been the ones
most widely applied across a range of experimental procedures, includingmemory ,lexicaldecision ,letter-matching
 , visual search ,
decision making , simple
reaction time , signal detection , and perceptual judgments .
The experimentaldatatowhich the modelsare fit in twochoice tasks are accuracy rates and reaction time distributions for both correct and error responses. The ability of
the models to deal with this range of data sets them apart
from other models for two-choicedecisions.Because multiple dependentvariablesneed to be fit simultaneouslyand
because the data can have contaminants,the fittingprocess
is notstraightforward.Forthesereasons,themodelisa good
testing ground for evaluating fitting methods.
In fitting any sequential sampling model to data, the
aim is to find parameter values for the model that allow it
to produce predicted values for reaction times and accuracy rates that are as close as possible to the empirical data.
But so far in this domain, little attention has been paid to
the methodsfor fitting.Sometimes models have been fit by
eye, by simply observing that they can capture the ordinal
trends in the experimentaldata. Sometimes a standard criterion such as chi square , in
which the difference between the observed and the predicted frequencies of reaction times in the reaction time
distributionis minimized,is used.Nonstandardcriteriahave
also been used. For example, Ratcliff and Rouder 
and Ratcliff et al. fitted an ex-Gaussian, summary,
reaction time distribution to data and to predictions from Ratcliff’s diffusion model. Then the sum of squares for the differences
between the parameters of the ex-Gaussian for predictions
and for data plus the sum of squares for the differences between accuracy rates for predictions and data were minimized. Ratcliff and Rouder used a more direct
sum-of-squares method in which quantile reaction times
were used instead of parameters of the summary (ex-
Gaussian) distribution. The statistical properties for all
these fitting methods have not been examined.
Preparation of this article was supported by NIMH Grant R37-
MH44640, NIDCD Grant R01-DC01240, NIA Grant R01-AG17083,
and NIMH Grant K05-MH01891. We are deeply grateful to Gail Mc-
Koon for extensive comments on this article and for making it somewhat readable. Correspondence concerning this article should be addressed to R.Ratcliff,Department ofPsychology,NorthwesternUniversity,
Evanston, IL 60208 (e-mail: ).
Estimating parameters of the diffusion model:
Approaches to dealing with contaminant
reaction times and parameter variability
ROGER RATCLIFF
Northwestern University, Evanston, Illinois
FRANCIS TUERLINCKX
University of Leuven, Leuven, Belgium
Three methods for fitting the diffusion model to experimental data are examined. Sets
of simulated data were generated with known parameter values, and from fits of the model, we found
that the maximum likelihood method was better than the chi-square and weighted least squares methods by criteria of bias in the parameters relative to the parameter values used to generate the data and
standard deviations in the parameterestimates.The standard deviations in the parametervalues can be
used as measuresof the variabilityin parameterestimatesfrom fits to experimentaldata. We introduced
contaminant reaction times and variability into the other components of processing besides the decision process and found that the maximum likelihood and chi-squaremethods failed, sometimes dramatically.But the weighted least squares method was robust to these two factors. We then present results
from modifications of the maximum likelihood and chi-square methods, in which these factors are explicitlymodeled, and show that the parametervaluesof the diffusion model are recoveredwell. We argue
that explicit modeling is an important method for addressing contaminants and variability in nondecision processes and that it can be applied in any theoretical approach to modeling reaction time.
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
In this article, we will examine three different methods
for fitting the diffusionmodel to two-choice reaction time
and accuracy data and will examine the properties of the
estimators for the parameters of the model. The issues that
arise have implications not only for fitting the diffusion
model, but also for fitting summary models of single reactiontime distributions and for fitting models in other domains. Throughout the article, we
will discuss findings as they apply to the diffusion model
case and also will place them in a more general context.
Examples of the issues that have broad implications are
the following. First, how should contaminant reaction
times be handledempirically (can they be eliminated)and
theoretically (can they be explicitly modeled)? Second,
how robust is a method for estimating parameters eitherin
terms of possible failures of a model’s assumptions or in
terms of contaminateddata? Third, there are practicalconsiderations, including computation speed. Fourth, an optimal fitting method should provide the best possible estimates of parameters. The estimates should be unbiased—
that is, they should converge on the true parameter values
as the number of observations increases (i.e., they should
be consistent). Fifth, the estimates should also have the
smallest possible standard deviations, so that any single
fit of a model to data produces estimates that are close to
the true values. In Appendix A, we present a more formal
discussion of the statistical factors involved in model fitting and parameter estimation.
In the model-fitting enterprise, sometimes there is no
need for extremely accurate estimation of parameter values; finding a set of parameter values that produces predictions reasonably near the experimental data is enough
to show that the model is capable of fitting the data. But
there are many cases in which accurate estimation of parameter values is necessary. For example, any situation in
which individual differences are examined requires reasonably accurate estimates of parameters. Also, if differences among the conditionsin an experiment are to be examined, knowing the standard deviations in parameter
estimates is important. It is toward these ends that this article will providean evaluationof fitting methodsin terms
of their robustnessand flexibilityin fitting data, as well as
in terms of their accuracy in recovering parameter values
from the different fitting methods. In order to explain the
fitting methods we evaluated and the results of the evaluations, we first will need to present the diffusion model,
the model we used as a testing ground and the model for
which we needed good fitting methods.
THE DIFFUSION MODEL
Diffusion and random walk models form one of the
major classes of sequential sampling models in the reaction time domain. The diffusion process is a continuous
variant of therandom walk process. The modelsbest apply
in situationsin which subjectsmake two-choice decisions
that are based on a single, “one-shot” cognitive process,
decisions for which reaction times do not average much
over 1 sec. The basic assumption of the models is that a
stimulus test item provides information that is accumulated over time toward one of two decision criteria, each
criterionrepresenting one of the two response alternatives.
A response is initiatedwhen one of the decision criteria is
reached. The researchers who have developed random
walk and diffusion models take the approach that all the
aspects of experimental data need to be accounted for by
a model. This means that a model should deal with both
correct and error reaction times, with the shapes of the full
distributions of reaction times, and with the probabilities
of correct versus error responses. It shouldbe stressed that
dealing with all these aspects of data is much more of a
challenge than dealing with accuracy alone or with reaction time alone.
Random walk models have been prominent since the
1960s .
Diffusion models appeared in the late 1970s . The random walk and diffusion models are close cousinsand are not competitorsto each other,
as they are with other sequential sampling models .
So, although we will deal with only one diffusion model
in this article, most of the issues and qualitative results
apply to other diffusion and random walk models, and the
general approach applies to other sequential sampling
The earliest random walk models assumed that the accumulation of information occurred at discrete points in
time, each piece of information being either fixed in size
 or variable in size .The modelswere appliedmainlyin choice
reaction time tasks and succeeded in accountingfor accuracy and for mean reaction time for correct responses.
They were also sometimes successful with mean error reaction times, but they rarely addressed the shapes of reaction time distributions.
Ratcliff’s diffusion model is illustrated in Figure 1 in three panels,each showingdifferent aspectsof the
model. Information is accumulated from a starting pointz
toward one or the other of the two response boundaries; a
response is made when the process hits the upper boundary at a or the lower boundary at zero. The mean rate at
which information is accumulated toward a boundary is
called the drift rate. During the accumulation of information, drift varies around its mean with a standard deviation of s. Variabilityis large, and processes wander across
a wide range, sometimes reaching the wrong boundary by
mistake, which results in an error response. The top panel
of the figure shows two processes, one with a drift rate of
v1 (solid arrows) and the otherwith a drift rate ofv2 (dashed
arrows). Variability in drift rate leads to distributions of
finishing times (reaction time distributions), one distrib-
RATCLIFF AND TUERLINCKX
ution for correct response times (at the top boundary for
the processes shown in the figure) and another distribution for error responses (at the bottom boundaryin the figure). The spread of the solid arrows shows the reaction
time distributionfor the process with drift rate v1, and the
spread of the dashed arrows shows the reaction time distribution for the process with drift rate v2. The geometry
of the diffusion process naturally maps out the rightskewed reaction time distributions typically observed in
empirical data. The panel also shows how smaller drift
rates (e.g., v2) lead to slower responses, with more chance
of reachingthewrong boundary, andso tolarger error rates.
The variabilityin drift rate within a trial, representedby
the parameter s, is a scaling parameter; if it were doubled,
for example, all the other parameters could be changed to
produce predictions identical to those before the change.
The s is a fixed, not a free, parameter in fits of the model
to data. It would be possible to fix another of the parameters instead—for example, boundary separation. But
some empirical manipulations would be expected to affect
boundary separation (e.g., speed–accuracy instructions),
so that if such a parameter were fixed, the effects of the
manipulation would show up in the values of other parameters and interpretation would be difficult. The most
natural assumption (and the standard assumption) is to
hold within-trial variability in drift constant, assuming
that it is a constant value across the whole range of different kinds of decisionsin an experiment,from easy to most
difficult. We fixed s at 0.1, a value near those used in previous applicationsof the model .
Variability in Parameters Across Trials
Besides variability in drift rate within each trial, there
are several sources of variabilityacross trials. For one, the
Figure 1. An illustration of the diffusion model and parameters. The top panel shows starting point variability and illustrates how accuracy and reaction time distribution shapes for
correct and error responses change as a function of two different drift rates (v1 and v2). The
bottom right panel illustrates variability in drift across trials (standard deviation h) and the
distribution of contaminants. The bottom left panel shows variability in Ter, the nondecision
component of reaction time.
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
mean drift rate for a givenstimulusvaries across trials (because subjects do not encode a stimulus in exactly the
same way every time they encounterit). This variabilityis
assumed to be normally distributed with a standard deviation of h, and it is illustrated in the bottom right panel of
Another source of variability is variability across trials
in the starting point z (top panel of Figure 1). Variability
across trials comes from a subject’s inability to hold the
startingpoint of the accumulationof informationconstant
across trials. The distributionof startingpointvalues is assumed to be rectangular, with sz as its range. A rectangular distributionwas chosen so that the starting point would
be restricted to lie within the boundaries of the decision
Without variabilityin drift rate and startingpointacross
trials, simple random walk and diffusion models would
predict that reactiontimes will be the same for correct and
error responses when the two response boundaries are
equidistantfrom the starting point; this is contrary to data.
There has been a number of attempts to account for error
reaction times within random walk and accumulator
model frameworks , and some of these were
moderately successful. But the inability of most models
to deal with the full range of effects led to a deemphasis
of error reaction times in the literature: Error reaction
times have relatively rarely been reported, and there has
been relatively little effort to deal with them theoretically
until recently.
However, recent work has shown how variabilityin drift
rate and starting point can produce unequal correct and
error reaction times . Ratcliff showed that variability in
drift across trials produces error reaction times that are
slower, and Laming showed that variability in
starting point across trials produces error reaction times
that are faster. Ratcliff et al. and Ratcliff and
Rouder showed that the combination of the two
types of variability can produce accurate fits to both patterns. Most interestingly, the combination can produce a
crossover, so that errors are slower than correct responses
when accuracy is low and errors are faster than correct responses when accuracy is high. This crossover has been
observed a number of times experimentally .
The diffusionprocessis a model of thedecisionprocess,
and not of the otherprocessesinvolvedin a task, processes
such as stimulus encoding, response output, memory access, retrieval cue assembly, and so on. The times required
for these other processes are combined into one parameter, Ter (bottom right panel, Figure 1). From a theoretical
perspective, it has always been recognized there must be
variability in Ter .But it has neverbeen
clear what the addition of the extra parameter would buy
for the sequential sampling models; it appeared that success or failure of the models was not dependent on it.
However, we recently found sets of data for which the diffusion model fits missed badly in some conditions and
discovered that this was due to large variability in the .1
quantile reaction times across conditions. Adding variability in Ter to the model corrected the fits (Ratcliff et al.,
For purposesof modeling,Ter is assumed tobe uniformly
distributed (bottom left panel of Figure 1). The true distribution for Ter might be skewed or normal, but this distributionis convolved with the distributionfrom the decision process that has a larger standard deviation (by a
factor of at least 4). The distributionof the convolutionis
determined almost completely by the distribution of the
decision process, and so the precise shape of the distribution of Ter has little effect on predicted reaction time distributionshape. The standard deviationof the distribution
of Ter determines the amount of variability in the .1 quantile reaction times across trials that can be accommodated
by the model, and it also determines the size of the separation between the .1 and the .3 quantilereactiontimes relative to the case with no variability in Ter.
Simulating the Diffusion Process
The first step in an examinationof fitting methods is to
produce data to be fit. A computer program was written
that, given input values for all the diffusion model parameters, generated simulated data from the model. That is,
the program generated individual data points, each one a
response choicewith its associated reaction time. The aim
was that the fitting methodsshould recoverthe correct parameter values—in other words, the parameter values
from which the data were generated.
To produce simulated data from the diffusion process,
a random walk approximation was used. Feller derived the diffusion process from the random
walk by using limits in the random walk: As step size becomes small, the number of steps becomes large, and the
probability of taking a step toward one boundary approaches .5. Specifically, if the random walk has a probability of q of taking a step down, a step size in time of h,
and a step size in space of d, the random walk approaches
the diffusion process when d®0, h®0, and q®1/2, so
that ( p 2 q)d/h®v and 4pqd2/h®s2, where v and s2 are
constants and p 5 1 2 q. If these limits are applied to the
expression for reaction time distributions (first-passage
times) and response probabilities, the diffusion process
expressions are obtained .
To scale the random walk approximationso that the diffusion model parameters can be used, the step size and the
probabilityof taking a step up or down need to be scaled,
using the step size in time and the standard deviation in
drift (s). First, we define the parameter h to be the step size
in time (e.g., h might be 0.05 msec). Then, in one time
RATCLIFF AND TUERLINCKX
step, the process can move a step size of d up or down. We
set d 5 sÏh and the probabilityof going distance d to the
lower boundary to be 0.5(1 2 vÏh/s). The simulation
starts from starting point z, and after each unit of time h,
it takes a step of size d until it terminates at 0 or a. With
these definitions, as h®0, then d®0; the mean displacement during time h equals ( p 2 q)d/h, which approaches
drift rate v; the variance of the displacement approaches
4pqd2/h; and so the random walk approachesthe diffusion
To implement this random walk approximation to the
diffusionprocessina computerprogram,it is more efficient
to use integer arithmetic, rescaling distance so that a step
is one unit up or down. This is accomplished by dividing
a, z, and v by d. To produce simulated data from the diffusion process, we reduced the step size in the random
walk, in accordance with the limits stated above, until the
random walk approximated the diffusion process. Steps
with a size of 0.05 msec were used, and these produced
mean reaction times within 0.1 msec and response probabilities within 0.1% of the values produced by explicitsolutions for the diffusion model (see Appendix A).
There are many other ways in which a diffusion process
could be simulated . The advantage of the random walk approximation is generality. There are cases in which the
drift rate is assumed to change over position in the
process, as in the Ornstein Uhlenbeck diffusion model
 , or
over time during the process. For example, if a stimulus
displaywas masked before a response had been produced,
during processing, the drift rate could be assumed to fall
after masking . In these
cases, exact solutionsare usually not available,but it is extremely easy to modify the program that simulatesthe diffusion process with the random walk.
CONTAMINANT REACTION TIMES
In evaluating the three fitting methods, we addressed
the issue of contaminant reaction times. We defined contaminantsas responses that come from some process other
than the diffusion decision process. One class of contaminants is outliers—response times outside the usual range
of responses (either shorter or longer). Outliers are a serious problem in reaction time research. They can cause
major problems in data analysis, because they can distort
estimates of mean reaction time and standard deviationin
reaction time . Also, outliers significantly reduce the power of an analysis of variance . The other class of
contaminants is reaction times that overlap with the distribution of reaction times from the process being examined. These are also a problem for data analysis, although
not as serious as the problem caused by outliers. Contaminantsmight arise, for example,from a guess or from a momentary distraction that is followed by a fast response.
Fast guesses, one kind of outlier, have in themselves
been a topic for modeling. The influential “fast guess”
model was developed to account for speed–accuracy
tradeoffs when these tradeoffs were accomplished by increasing or decreasing the number of fast guesses . Often, fast
error responses are called fast guesses. This is usually not
an accurate description. True fast guesses are guesses—
that is, their accuracy is chance . So, in
a condition in which there are fast error responses, it is
necessary to determine whether all fast responses are at
chance. If many fast responses are accurate and there are
few fast errors, the fast errors are not fast guesses. It cannot be stressed enough that fast errors are not fast guesses
unless all responses below some lower cutoff (e.g., the
fastest 10%, 5%, or 1% of the responses) are at chance responding. This is the signature that is needed to identify
fast guesses and that can be used to eliminate subjects or
devise where to place lower cutoffs.
The method we have adopted for dealing with contaminants in data analyses is, first, to eliminate fast and slow
outliers,using cutoffs. For fast outliers, we place an upper
cutoff at, say, 300 msec and a lower cutoff at zero and examine how many reaction times appear in that range for
each subject, examining the accuracy of these responses.
If a subject has a significant number of responses (e.g.,
over 5% or 10%) that are fast and at chance, the subject is
a candidate for elimination from the experiment (we occasionallyfind such noncooperativesubjects).We then increase the upper cutoff (to, say, 350 msec) to see whether
accuracybeginsto rise abovechance.Repeatingthisprocess
with increasingly larger cutoff values allows us to determine a good choice of a cutoff for fast outliers.
The method just described for setting a cutoff for fast
outliers is workable in most situations.However, when an
experimentaltask biases one response overthe other , then typically, one response will be faster than the other, sometimes by as much
as 100–200 msec in mean reaction time. This means that
the shortest reaction times for responses for the biased response will be up to 100 msec shorter than responses for
the nonbiasedresponse. In this case, the use of cutoffs will
not allow fast guesses of the biased response to be distinguishedfrom genuineresponsesfrom thedecisionprocess.
Examination of the shape of reaction time distributions
might be one way of detecting fast outliers. If the leading
edge of the distribution has a long rise or reaction times
are less than 250 msec, the short reaction times should be
viewed with suspicion. Also, if only fast errors occurred
in a high-accuracy condition when the correct response
was biased against, these fast errors could come from fast
guesses of the biased response.
For slow outliers, we set an upper cutoff not by some
fixed proportion of responses, but rather by determining
a pointabovewhichfew responses fall. The choicedepends
on the research goal; the cutoff might be smaller for hypothesis testing than for model fitting .
For model fitting, cutoffs can eliminate extremely fast
and slow outliers. However, to eliminate all contaminants
is impossible. The solution we adopted to deal with these
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
remaining contaminants was to explicitly represent them
in the fitting method and estimate their proportion ( po).
For the diffusionmodel simulations,we assumed that contaminants were generated only by a delay inserted in the
usual decision process. Specifically, we assumed that on
some proportion of the trials, a random delay was introduced into the response time. Thus, the observed response
times in each condition were a mixture of responses from
a regular diffusion process (with a probability of 1 2 po)
and contaminant responses (with a probability of po; see
Figure 1, bottom right panel).
Our assumptions about contaminants are reasonable if
subjects are cooperative and if they make errors only as a
result of lapses of attentionor other short interruptions.If
subjects produce contaminants in other ways (e.g., they
couldguess in difficultconditions),different assumptions
couldbe incorporatedinto thefittingprogram.Dealingwith
contaminants theoretically in this way can easily be extended to fitting summary distributions for reaction time
distributions,as in Heathcoteet al. , Ratcliff ,
Ratcliff and Murdock , and Van Zandt .
For clarity, it is worth noting that issues of fitting contaminant reaction times will not be addressed until the
methods for fitting the diffusion model have been introduced and evaluated in detail. Issues concerning variability in Ter will be addressed even later in the text.
FITTING THE DIFFUSION MODEL
To fit the diffusion model to a set of data, characteristics of the data have to be compared with the model’s predictionsfor those characteristics.The three different fitting
methods we evaluated each compare different characteristics, and each requires an expressionfor the model’s predictions. Collectively, the comparisons require the predicted probability densities for individual reaction times,
the predicted cumulativeprobabilitydistribution,and predicted values of accuracy for each of the experimental
conditions.The expressionsfor all of these are givenin Appendix B.
Because the expressions do not have closed forms and
because some of the parameters’ values vary across trials
(starting point, drift rate, and Ter), the predictionsmust be
computednumericallyas describedin detailin AppendixB.
Numerical computation allows the accuracy of the predictions to be adjusted by adjusting the number of terms
in infinite series for the reaction time distributions or increasing the number of terms in numerical integration
over startingpointand drift variability(althoughthe more
accuracy desired, the longer the fits take). For the tests of
the fitting methods described below, predicted values of
reaction time and accuracy were computed to within
0.1 msec and .0001, respectively.
Maximum Likelihood Fitting Method
Given simulated data and expressions for predictions
about the data from the diffusion model, we can fit the
model to the data to see how well the maximum likelihood
method does in recoveringparametervalues.For the maximum likelihood method, the predicted defective probability density [ f(ti)] for each simulated reaction time (ti)
for each correct and error response is computed. By a defective density, we mean nothing more than one that does
not integrate to one ; it integrates to the
probability of the response. The product of these defective density values is the likelihood [L 5 Pf(ti)] that is to
be maximized by adjustment of parameter values. Because the product of large numbers of values of densities
can exceed the numerical limits of the computers used to
compute likelihood, log of the likelihood is used (maximizing the likelihood is achieved with the same parameters as maximizing the log of the likelihood).Also, maximizing log likelihood is the same as minimizing minus
the log likelihood,and most routinesare designed for minimization.
Ratcliff’s implementation of the maximum likelihood
method involvesusing the predicted defective cumulative
distribution function to obtain the defective cumulative
probabilityfor each reaction time, F(ti), and the defective
cumulative probability for that reaction time plus an increment, F(ti 1 dt), where dt is small (e.g., 0.5 msec).
Then, by using f(t) 5 [F(t 1 dt) 2 F(t)]/dt, the predicted
defective probability density at point t can be obtained.
Summing the logs of the predicted defective probability
densitiesfor all the reaction times gives the log likelihood.
Again, minus the log likelihood is minimized.
To minimize minus the log likelihood,Ratcliff used the
SIMPLEX routine . This routine takes starting values for each parameter, calculates the value of the function to be minimized, then changes the value of one parameter at a time
(sometimes more than one) to reduce the value of the objective function. This process is repeated until either the
parameters do not changefrom one iteration to the next by
more than some small amount or the value to be minimized does not change by more than some small amount.
Tuerlinckx’s implementation of the maximum likelihood method uses the defective probability density of
each reaction time obtained directly from the predictions
of the model (rather than through the cumulative distribution function). Drift variance is integrated over explicitly, and so there is one less numerical integration for the
density function. (We were unable to integrate over drift
variance for the cumulativedistributionfunction,but only
for the density, and Ratcliffused his expression for the distribution function—used in the chi-square and weighted
least squares programs—to numerically produce the density for use in maximum likelihood partly so that it and
Tuerlinckx’s method couldbe checked againsteach other.)
To minimize minus the log likelihood,Tuerlinckx used
a constrainedoptimizationroutine that searches for theminimum of minus the log likelihood function by using finite difference approximations
to the first derivatives for the objective or target function.
Although the NPSOL computer algorithmallows the user
RATCLIFF AND TUERLINCKX
to supply the theoretical partial derivatives (slopes of the
function as a function of each of the parameter values),
these were not used, because they are very complicated.In
general, in this application,the finite difference method is
faster than the SIMPLEX method, because it uses more
informationaboutthe objectivefunction (derivatives).But
it is less robust and can lead to problems in numerical instability that cause it to fail to converge on the minimum
of the function being minimized. Tuerlinckx’s method is
about five times faster than Ratcliff’s method.
Chi-Square Fitting Method
There are several ways of using a chi-square methodfor
fitting the diffusion model to data; the one we chose was
designed to maximize the speed of its computerimplementation .
The chi-square method we used works as follows. First,
the simulated reaction times are grouped into bins, separately for correct and error responses. The number of bins
we chose was six, with the two extreme bins each containing 10% of the observations and the others each containing 20%. We compute the empirical reaction times
that divide the data into the six bins, and these are the .1,
.3, .5, .7, and .9 quantiles. Inserting the quantile reaction
times for the five quantiles for correct responses into the
cumulativeprobabilityfunction gives the expected cumulative probabilityup to that quantilefor correct responses.
Subtracting the cumulative probabilities for each successive quantile from the next higher quantile gives the proportion of responses expected between each pair of quantiles, and multiplyingby the total number of observations
(total number of correct responses)gives the expected frequencies in each bin for correct responses. Doing the
same thing for the five quantilesfor error responses gives
the expected frequencies in each of the bins for error responses. (If there were fewer than five errors in an experimental condition, five quantiles could not be computed,
and the error reaction times for the condition were excluded from the chi-square computation.) The expected
frequencies (E) are compared with the observed frequencies (O). The chi-square statistic to be minimized is the
sum over the 12 bins, the 6 correct response bins and the
6 error response bins, of (O 2 E)2/E (with these sums for
each condition summed over conditions).This chi-square
statistic is the objective function to be minimized by parameter adjustment.
For each condition,only five evaluationsof F(t) are required for correct responses, and five are required for error
responses (no matter how many observationsin each condition). As compared with Ratcliff’s maximum likelihood
method, the program runs 50 times faster with 250 observations per condition and 200 times faster with 1,000 observationsper condition(because the distributionfunction
has to be computed twice for each density function).
Weighted Least Squares Fitting Method
In this method, the sum of the squared differences between observed and predicted accuracy values plus the
sum of the squared differences between observed and predicted quantile reaction times for correct and error responses is minimized. The expression for the minimized
function is the following: the sum over experimental conditions (for correct and error reaction times separately) of
4( prth 2 prex)2 1 Siwt 3 prex 3 [Qth(i) 2 Qex(i)]2, where
pr is accuracy, Q(i) is the quantilereactiontime in units of
seconds,thstandsfor predicted,ex standsfor experimental,
and wt was 2 for the .1 and .3 quantiles,1 for the .5 and .7
quantiles, and 0.5 for the .9 quantile. Just as with the chisquare method, if there were fewer than five errors in a
condition, five quantiles could not be computed, and the
error reaction time for the condition was excluded from
the least squares computation.
The weights were chosen to roughly approximate the
relative amounts of variability in the quantiles, weighting
more heavily those quantile points for which variability
was smaller. For simple linear regression, an appropriate
weighting scheme is to divide each data point by its variance. This gives the smallest standard deviationsin the estimates of the parameters (for normally distributed residuals). Whether our weights correspond to the relative
variabilities of the quantile reaction times can be determined in two ways: by computingthe theoreticalstandard
deviations for the quantile points and by computing their
standard deviations empirically from experiments. Theoretically, the asymptotic variance of a quantile reaction
time at quantile q is q(1 2 q)/(Nf 2), where N is the number of observations and f is the probability density at the
quantile . We carried out this computationfor two sets of parameter values
(the first and sixth rows of Table 1, with drift rates of .3
and .1, respectively), computing the standard deviations
in the quantilereaction times. We then dividedthe .5 quantile standard deviation by each of the others to give relative weights. For the five quantiles.1, .3, .5, .7, and .9, the
ratios were 2.4, 1.6, 1.0, 0.7, and 0.3 for the first set of parameter values and 2.3, 1.6, 1.0, 0.6, and 0.3 for the sixth
set of parameter values. Thus, the weights we chose are
approximately in the ratio of the standard deviations. The
problem with this theoretical computation is that the expression for the variance is accurate only asymptotically
and our data have too few observations (especially in extreme error conditions) to be asymptotic. So, additional
work would be needed to compute expressions for the
nonasymptoticcase.
Empirically, we calculated the standard deviations in
quantile reaction times across subjects for a letter identification experiment . Again, to represent the standard deviations as weights, we dividedthe standard deviationfor the
.5 quantile by each of the others. For three experimental
conditions that spanned the range of accuracy values in
the experiment (probability correct of .9 to .6), the ratios
were the following: 1.3, 1.2, 1.0, 0.7, 0.4; 1.4, 1.2, 1.0, 0.8,
0.4; and 1.3, 1.1, 1.0, 0.8, 0.6. The squares of these ratios
(relativevariances)are notfar from ourselectionofweights.
Thus, given both the theoretical and the empirical calculations, we conclude that the weights we used (2, 2, 1, 1,
and .5) were not unreasonable.
Another issue concerning the optimality of the least
squares method is that the data entering each term in the
sums of squares should be independentof each other . If the data are not independent,
all possible covariances among the quantities should be
taken into account . In our case, it is
clear that quantile reaction times for correct and error responses and accuracy values all covary. If a single parameter of the model is changed, all the quantile reaction
times and accuracy values will change in a systematic
way. Some of the covariancesare known.For example,the
asymptoticcovariationbetween all the reaction time quantilesfor one response (correct or error) is given by Kendall
and Stuart . But this is an asymptotic expression
and would not apply accurately to error reaction times (for
which there are few data points). Also, theoreticalformulations for the covariations between correct and error reaction time quantiles and accuracy values are not available. It would be very difficult to find expressions for all
the covariances required to produce an optimal version of
the weighted least squares method. The weighted least
squares method we employedshould,therefore, be viewed
as the kind of ad hoc method that is often used in fitting.
To implement our weighted least squares method, the
predicted reaction time for each of the quantiles needs to
be computed. To compute these, the whole predicted cumulativereaction time distributionhas to be obtained.We
used 400 reaction times to obtain cumulative frequencies
and then linear interpolationbetween pairs of values to determine the quantile reaction times. The SIMPLEX routine was used to minimize the weighted least square, and
the implementations ran at about the same speed as Ratcliff’s maximumlikelihoodprogram—that is, much slower
than the chi-square method.
In the weighted least squares method, accuracy is represented explicitlyin the sum of squares, but it is not represented explicitly in the maximum likelihood and chisquare methods.In those methods,accuracy is represented
by the predicted relative frequencies in the reaction time
distributionsfor correct and error responses. For example,
for 250 observations per condition and an accuracy value
of .9, the total observed frequency for errors would be 25,
and the total frequency for correct responses would be
225. In the fitting process, if the model predicted an accuracy of .8, it would overpredict errors (frequency of 50)
and underpredict correct responses (frequency of 200).
Then the fitting program would attempt to adjust these
frequencies, subject to the other experimental conditions
and other constraints.
PARAMETER VALUES
The parameter values were chosen to be representative
of real experiments in which subjects are required to decide between two alternatives—for example,word or nonword in lexicaldecision or brightor dark in brightnessdiscrimination. We simulated data for an experiment with
four conditions,the conditionsrepresenting four levels of
difficultyof a singleindependentvariable, as, for example,
in a lexical decision or recognition memory experiment
with four levels of word frequency or in a two-choice signal detection experiment with four levels of brightness of
the stimulus. We assumed that the levels of the variable
are randomly assigned to trials in each experiment,so that
subjects cannot anticipate which condition is to occur on
any trial . Thismeans that
subjects cannot adjust processing as a function of condition and, therefore, none of the parameters of the model
except the parameter representing stimulus difficulty can
change across levels of the manipulated variable.
The simulated data were generated from 12 different
sets of parameter values, with the values chosen to span
the ranges of values that are typical in fits of the diffusion
modelto real data. For the 6 sets shown in Table1, thestarting point z was symmetric between the two boundaries.
The drift rates are all positive because the same results
wouldbe obtainedwith negativedrift rates, sincethe boundaries are symmetric. (We also performed the same analyses with asymmetric boundaries; the results are qualitatively the same as those for the symmetric boundary case,
and tables displayingthe results can be found on Ratcliff’s
or the Psychonomic Society’s Web pages).
Ter, the parameter for the nondecision components of
processing, was always set at 0.3 sec. This parameter
largely determines the location of the leading edge of the
reaction time distribution. When boundary separation is
small, the leading edge is a little closer to Ter, and when
boundaryseparationis large, the leadingedge is a littlefurther away from Ter. None of the other parameter estimates
or standard deviationsin parameter estimates are changed
by changingthe value of Ter, because a changein Ter shifts
all the reaction times by the same fixed amount.
Drift rate (v) represents the quality of evidence driving
the decision process—that is, the difficulty level of a stimulus. For the four levels of the independent variable, we
selected four drift values that span the range from high accuracy (about 95% correct) to low accuracy (about 50%
correct). The values for the four drift rates are different for
different values of boundary separation because when
boundary separation (a) is small, there is more chance that
a process will hit the wrong boundary by mistake, and so a
highervalue of drift rate was used to producethesame high
accuracy values as when boundary separation was larger.
RATCLIFF AND TUERLINCKX
We selected two values for variabilityin drift across trials (h, which is the standard deviation in a normal distribution) and two values for variability in starting point
across trials (sz, which is the range in a uniform distribution), one relativelylarge and one relativelysmall, as compared with typical values obtained in fits to real data.
We used two values of a, a 5 0.08 and a 5 0.16, both
with symmetric boundaries and asymmetric boundaries.
These values roughly bracket the values of boundary separation typically obtained in fits to real data.
Overall, the selected parameter values cover the range
of valueswe haveobtainedwhen fittingthe diffusionmodel
to experimental data .
EVALUATING FITTING METHODS
Our evaluationof the fitting methodswill start by using
models and simulated data with no contaminants or variability in Ter. Then we will introduce contaminantsand report their effects on the fitting methods. We will then introducecorrectionsfor contaminantsin thefitting methods
and will discuss their performance. Finally, we will introduce variability in Ter and will evaluate performance of
the models without and then with this explicitlymodeled.
This also follows our chronologicalstudy of these issues.
We evaluated the methods by comparing the parameter
values each method recovered from simulated data with
the parameter values that were used to generate the data.
We examined each method’s ability to recover the correct
parameter values, whether the recovered values were biased away from the correct values in some consistentway,
and the size of thestandarddeviationsin theparameter values across fits to multiple simulated data sets. The standard deviations in the estimated parameter values are important for determining how much power is available for
testing hypotheses about differences in parameter values
across experimental conditions or subject populations.
Also, the relativesizes of the standarddeviationsin the estimated parameter values from the different methods provide estimates of the relative efficiencies of the methods.
The diffusion model was used to produce the simulated
empirical data as follows: Given a value for each of the
diffusion model’s parameters, the model produces responses, each with its response time. For each set of parameter values for the model, 100 sets of simulated data
were generated, each data set with either 250 or 1,000 observations for each of the four experimental conditions.
All of the three methods of fitting the model to data involve computing some statistic—that is, some objective
function—to represent how well the model fits the data. A
minimization routine (see Appendix B) begins with some
startingvalues of the parameters of the model and then adjusts them to maximize or minimize (depending on the
method)the objectivefunctionuntilthe best fit is obtained
between predictedaccuracy values and reactiontimes and
simulatedaccuracy valuesand reactiontimes.This process
of finding the parameter values that give the best fit of
predictions to data was repeated for each of the 100 sets
of simulateddata, giving 100 sets of best-fittingparameter
values, from which we calculated the mean and standard
deviation of the 100 estimates for each parameter. This
whole process was repeated for each of the three fitting
methods, and then it was all repeated again with the 100
sets of data generated from a different set of parameters.
Altogether, 12 different sets of parameter values (6 with
symmetric boundaries and 6 with asymmetric boundaries
which are not reported) were used to span the range of parameter values typicalof fits of the diffusionmodel to data
in past studies .
For all the 100 sets of data for all 12 sets of parameter
values, Ratcliff examined the maximum likelihood, chisquare, and the weightedleast squares methods,and Tuerlinckx examinedthemaximum likelihoodmethod.Ratcliff
examined the chi-square methodwith correctionsfor contaminants and variability in Ter, and Tuerlinckx examined
the maximum likelihood method with these corrections.
The mean values of the best-fittingparameters and their
standard deviations allow the three fitting methods to be
compared on the basis of how well they allow recovery of
the parameter values used to generate the simulated data,
how variable the parameter estimates are across sets of
data, and whether they produce systematic biases away
from the true parameter values. For each set of simulations, we will discuss the overall behavior of the methods
in terms of the results from the simulations. But only in
the more important cases will we present tables of the
mean values of the parameter estimates and the standard
deviations in the estimates for the 100 sets of simulated
data that were generated for each set of parameter values.
The conclusions of the fitting exercises are complex;
each fitting method has advantagesand disadvantages.To
anticipate, we will list here several main conclusions, but
with the caveat that any one set of simulations may have
differences from the main results. (1) When the simulated
datacontainedcontaminants,as real dataoftendo,we found
that the maximum likelihood method was extremely sensitive to the contaminants.Although we developedprocedures to correct for some classes of contaminants, the
presence of even a few contaminantsthat could not be corrected for was sufficient to produce poor fits and poor parameter recovery. However, in the absence of contaminants, the maximum likelihoodmethodproduced unbiased
parameter estimates and had the smallest standard deviations in the estimates of any of the methods. (2) The chisquare method was much more robust than the maximum
likelihood method. The presence of a few contaminants
for which we did not correct had little effect on the results
of fitting: parameters were estimated with only small biases away from the values used to generate the data, and
the parameters had standard deviations in their estimated
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
values that were onlysomewhat larger than those obtained
for the maximum likelihood method. In addition, implementationsof the chi-square method are much faster than
implementationsof eitherthemaximumlikelihoodmethod
or the weighted least squares method. (3) The weighted
least squares method produced mean parameter estimates
aboutas biased as those for the chi-square method,but the
standard deviations were larger. However, the weighted
least squares method was the most robust in the face of
contaminants.It was capable of producing reasonable fits
even in situations in which the other methods failed dramatically, although the recovered parameters were not the
same as those used to generate the diffusion process portion of the data. The weighted least squares method is
most useful as a guide to whether the diffusion model is
capable of fitting a data set.
QUANTILE PROBABILITY FUNCTIONS
Fits of the diffusionmodelto data are complicatedto display, because the data include two dependent variables—
accuracy rates and correct and error reaction times—as
well as distributionsof reaction time. Traditionally,accuracy, mean reaction time, and reaction time distributions
are all presented separately as a function of the conditions
of an experiment. Here, we show a method of presenting
all the dependent variables on the same plot so that their
joint behavior can be better examined.
In earlier research, latency probability functions have
been used to display the joint behavior of mean reaction
time and accuracy. They are constructedby plottingmean
reaction time on the y-axis and probabilitiesof correct and
error responses on the x-axis . Responses with probabilitiesgreater than .5 are typicallycorrect responses,and so, datafrom correct responsestypically
fall to the right of the .5 point on the x-axis. Responses
with probabilitiesless than .5 are typically errors and, so,
typicallyfall to the left. Latency probabilityfunctionscapture the jointbehaviorof reaction time and response probability, how fast the two change across experimental conditions, and how fast they change relative to each other.
However, latency probability functions do not display information about reaction time distributions.
Ratcliff generalized latency probability functions to quantile probabilityfunctions, the method of presenting data that we use in this article. A quantileprobability functionplotsquantilesof thereactiontime distribution
on the y-axis against probabilitiesof correct and error responses on the x-axis. In Figure 2, five quantilesare plotted, the .9, .7, .5, .3, and .1 quantiles,as labeled in the vertical rectangle, for four experimental conditions. For a
givenexperimentalconditionwith a probabilityof correct
responses of, say, .8 (to the right of the vertical rectangle),
the five quantilepointsform a vertical line above.8 on the
x-axis. The spread among the points shows the shape of
the distribution.The lower quantile points map the initial
portion of the reaction time distribution, and the higher
quantilesmap the tail of the distribution.Because reaction
time distributions are usually right skewed, the higher
quantilepointsare spread apart more than the lower quantile points. Lines are drawn to connect the quantiles of the
experimental conditions, one line to connect the first
quantiles of all the experimental conditions, another line
to connect the second quantiles,and so on. If, as is usually
the case, responses with a probability of greater than .5
are correct responses and responses with a probability of
Figure 2. An illustration of quantile probability plots. The top
panel shows quantile probability functions, the middle panel illustrates how the quantiles change as drift rate changes, and the
bottom panel illustrates the effect of changing boundary separation.
RATCLIFF AND TUERLINCKX
less than .5 are error responses, the mirror image pointson
the x-axis around the probability of .5 point allow comparisons of the shapes of correct and error response time
distributions.In the example presented in Figure 2, in the
lowest accuracy condition, both correct and error responses have a probabilityof .5, and so their quantilesfall
on top of each other. Also, comparing the quantile points
across different probabilityvalues shows how distribution
shape changes as a function of experimental condition.
For example, if the whole distribution (all quantiles) becomes slower and slower as the difficulty of the experimental conditions increases (and probability of a correct
response decreases), this is easily seen as parallel changes
in all the quantiles. But if instead, the distribution becomes more skewed as the difficulty increases, the first
quantiles for all the conditions will change little across
conditions, and the last (longest) quantiles will change
most, as is the case in the top panel of Figure 2.
The parameters of the diffusion model each have a systematic effect on the quantileprobabilityfunction.Varying
drift rate varies left to right position on the quantile probability function. Changes in drift rate can produce only a
small change in the lowest quantile and a large change in
the highest quantile (Figure 2, middle panel). This corresponds to the distribution’s skewing a lot and shifting its
leading edge a little. Increasing boundary separation results in the distribution’s both shifting and skewing. The
lowest quantile increases, and the highest quantile increases more (Figure 2, bottom panel).
If starting point variability across trials (sz) increases,
the quantilesto the left of a probabilityof .5 (typically, errors) decrease, and they decrease most on the extreme left
side of the plot. This can lead to errors that are faster than
correct responses in the most accurate conditions.If variability in drift rate across trials (h) is increased, the plot
becomes more asymmetric. With h 5 0, sz 5 0 and z 5
a/2, the quantile probability function is symmetric. As h
is increased,the peak is lowered a little,and it moves to the
left as error responses slow relative to correct responses.
When the pointsplottedon the quantileprobabilityfunction are from an experiment in which subjects cannot
change response criteria or strategies between experimental conditions, as in the experiments simulated here for
tests of the fitting methods, then for the diffusion model,
the shapes of the lines that connect the quantiles in the
quantile probability plot are completely determined by
just three parameters: a, h, and sz. This means that, in fitting the model, only drift rate can vary as a function of
condition.The quantileprobabilityfunctionis what is called
a parametric plot, with drift rate the parameter of the plot.
Thus, besides providinga useful summary of the joint behavior of reaction time distribution shape and accuracy,
the quantile probability function provides a stringent visual demonstration of how well the diffusion model fits
the data. 
Variability in Simulated Data
Quantile probability functions provide a vehicle with
which to illustrate variability in the simulated data we
used to test the three fittingmethods.Variabilityin the data
providesa backdrop for understandingwhy the variability
in estimated parameters that we will present later is as
large or as small as it is.
Figure 3 shows quantile probability functions for data
simulated with the set of parameters shown in the first line
of Table 1. Only the quantile probability functions for 40
sets of data, not the full 100 sets, are shown, in order to reduce clutter. The “smears” on the figure are the 40 overlapping lines at each of five quantiles. The figure shows
how much variability there is in the quantile points and
that there is more variabilitywhen thequantilesare derived
from only 250 observations per condition (top panel) as
compared with 1,000 observations per condition (bottom
panel). (The quantile reaction times scale as a function of
the square root of N, so the spread in the bottom panel is
half that in the top panel. The figures provide a visualization of the size of the spread for these sample sizes and
these values of accuracy.)
With 250 observations per condition, the .9 quantile
varies around its mean by as much as 300 msec (for errors
at the extreme left of the figure, which come from conditions with high accuracy), and it varies by as much as
100 msec for correct responses from conditions with intermediate accuracy (in the middle of the figure). The .1
quantile varies little across the 40 sets of data, except for
errors in the conditions with high accuracy. With 1,000
observationsper condition,the locationof the quantilesis
muchtighter,buteven so, the.9 quantilefor error responses
in the high-accuracy conditions varies by as much as
It is possibletounderstandwhichparametershavegreater
or lesser variability associated with them from these figures. Ter and a are determined to a large degree by the position of the .1 quantile.The figures show that the .1 quantile is quite well located without much variability. On the
other hand, h and sz are largely determined by error reaction times, and these have a large amountof variabilityassociated with them. For example, with 250 observations
per condition (e.g., Figure 3, top panel), simulated data
generated from parameters that should produce slow errors can easily,by chance,produceerrors as fast as correct
responses. This would produce a fitted value of variability in drift across trials (h) of zero. This means that with
250 observations per condition and h 5 0.08, it is easily
possible (e.g., 5% or 10% of the time) to obtain a set of
data that producesa fitted value of h near zero. Therefore,
we expect variability in h to be large when fitting simulated data.
Figure 3 shows the variability that simulated data have
associated with them. If the diffusion model is correct—
thatis, if real data are generatedfrom a diffusionprocess—
the same variabilitywill be associated with real data. This
is why we providethe standard deviationsin the parameter
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
values we obtain from fitting simulated data. These standard deviationsgive some reasonable idea of what we can
expect from parameter estimates based on real data.
PARAMETER ESTIMATES FROM THE
MAXIMUM LIKELIHOOD METHOD
For all three methods of fitting, we will report and discuss results for fitting the simulated data that were generated from the six sets of parameter values for which the
startingpointis equidistantfrom the boundaries(Table 1).
We will report the means of the parameter estimates
across the 100 data sets and the standard deviationsin the
estimates. In the series of fits described first, results will
be shown and discussed for the data sets with 250 observations per condition.It turns out that the standard deviations for 1,000 observations per condition are almost exactly twice as small as those for 250 observationsper condition (i.e., the standard deviations change as the square
root ofthenumberofobservations).Therefore,we will later
discuss the results with 1,000 observations per condition
only briefly. The next sections will present simulated data
and fitting methods with no contaminants and with no
variability in Ter.
Means and Standard Deviations
of Parameter Estimates
Recall that Ratcliff and Tuerlinckx obtainedprobability
densities for reaction time distributions in different ways
and used different fitting algorithms (which provides a
Reaction Time (msec)
Reaction Time (msec)
Response Probability
40 sample sets of data for 5
quantiles for the first set of
parameter values (N = 250 per point)
40 sample sets of data for 5
quantiles for the first set of
parameter values (N = 1,000 per point)
Figure 3. Quantile probability functions for 40 sets of simulated data for 250
observations per condition (top panel) and 1,000 observations per condition
(bottom panel). These illustrate the variability in simulated data for the five different quantiles and how the variability changes, going from high-accuracy
correct responses (right-hand side of the figure) to errors on the left-hand side
of the figure.
RATCLIFF AND TUERLINCKX
check on the accuracy of the methods). The means of their
parameter estimates agreed within 1% of each other, except for the variabilityparameters(h and sz), which agreed
within 5%.CorrelationsbetweenRatcliff’s andTuerlinckx’s
means were computed for each parameter for each of the
six sets of parameter values. Averaging across the six sets
of correlations for each parameter value, the correlations
were about .9, except that, for Ter, the correlation was .72
and for sz, the correlation was .58. The low values of the
correlations for the variability parameters were due
Means and Standard Deviations of Parameter Values Recovered
From the Maximum Likelihood Fitting Method (N = 250 per Condition)
Parameter Set
Figure 4. Histograms of the parameter values recovered from fits of the diffusion
model, using the maximum likelihood method to simulated data (using parameters in
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
mainlyto theset ofparametersfor whichh and sz were both
large. Although the correlations for Ter were low, the standard deviationsin Ter were small (3–10 msec), so the low
correlationsdo not indicate large differences between Ratcliff’s and Tuerlinckx’s estimates of Ter. Ratcliff’s values
will be reported in the tables that immediatelyfollow, and
Tuerlinckx’s values will be reported for the investigations
of contaminants and variability in Ter that will be presented later.
Table 2 contains the means and standard deviations of
the parameter estimates that were recovered from the 100
sets of data simulatedfor each set of parameters in Table 1,
for the data sets with 250 observations per condition.
Overall, the means of the parameter estimates are unbiased—that is, they are close to the true values of the parameters with which the simulated data were generated.
The standard deviations in the estimates are small, averaging about 4% of the mean for a, 3–10 msec for Ter,
about 10% of the mean for drift rates (v), but anywhere
from 20% to 70% of the mean for standard deviation in
drift across trials (h). When sz was small (0.02), its standard deviation was about the same size as the mean, but
when it was larger, it was about 30% of the mean. The relative sizes of these differences among standard deviations
are consistent across all three fitting methods. Below, we
will describe the results for each of the parameters in turn.
To show how the parameters vary across the random
samples (the 100 data sets), Figure 4 presents nine histograms that display the parameter values. For two of the
sets of parameter values used to generate the simulated
data, a was 0.08.The first histogram(top left) shows all the
estimates of a from the data generated with both sets. The
next histogram, just below, shows the estimates of a for
the data generated from the four sets of parameter values
with a 5 0.16. Before grouping the two sets of data for
which a 5 0.08 and the four for which a 5 0.16, we made
sure that there were no systematic differences among
them. We also employed groupingsfor the histograms for
the otherparametervalues,alwaysfirst checkingthat there
was no variation as a function of other parameter values.
For the boundary separation parameter a, all the means
of estimates shown in Table 2 are within 4% of the target
value (either 0.08 or 0.16), which is within less than one
standard deviation. When 0.08 was the target value, the
distributionwas skewed onlya littletothe right. When 0.16
was thetargetvalue,thedistributionwas symmetric,roughly
normal. Thus, the fitting method is recovering parameter
values near the true values, with neither large deviations
nor a bias toward one or the other side of the target value.
The means of the estimates for Ter are also near the true
value (300 msec). The histogramfor the valuesof Ter (Figure 4, bottomleftmost panel)is symmetric around the true
value, although there are a few straggling values in the
tail. The standard deviation in the estimates is reasonably
small, always less than about 10 msec. The standard deviation is smaller, about 3 msec, when boundary separation
(a) is small and somewhat larger,5–10 msec, when boundary separation is larger. This is because, when a is small,
the .1 quantile has a smaller value, closer to the 300-msec
value of Ter, and is less variable. Hence, Ter is better located and so has a smaller standard deviation. This same
explanationholds for large sz as opposed to small sz: With
a large sz, more processes have a value of the .1 quantile
near Ter than with a smaller sz.
The means of the estimates for drift rates are typically
within 6% of the true values,with no systematic bias away
from the true values, and they do not vary systematically
with other parameters of the model (we show histograms
for drift rates 0.3 and 0.1 only, because they are representativeof all four drift rates). The standard deviationsin the
estimates of drift rates are about10% of the mean for high
drift rates and about 20% of the mean for low drift rates
(disregarding the drift value of zero). The standard deviation in drift rates is larger for the lower value of boundary
separation a than for the higher value of boundary separation. This is because the quantile probabilityfunction is
flatter for the smaller value of a and, so, drift rates are constrained mainly by their position on the x-axis (accuracy)
and less by their position on the y-axis (reaction times),
because the latter do not vary much across experimental
conditions. The standard deviation in drift rates is also
larger for larger boundary separation when the variability
across trials (h) is larger. This is again because the quantile probability functions are flatter when h is large.
The two variability parameters (h and sz) are considerably less accurately estimated than the other parameters
 . Although there do not appear to
be any systematic biases in h away from the true values
(0.16 and 0.08), large standard deviations mean that, if
there were any systematic bias, it would be hidden in the
variability. The standard deviation in the estimate of h
varies from about 0.03 to about 0.05, 20%–60% the size
of the mean. The reason for large variabilityin h is that its
estimate is based on error reaction times, which themselves have such large variability (see Figure 3) because
there are often few of them .
Thus, for some simulated data sets, the best-fitting value
of h is near zero, and for other simulated data sets, the
best-fitting value is over twice as large as the true value
used to generate the simulated data. This is shown clearly
in the histograms in Figure 4. For h 5 0.08, there are a
number of estimates near zero. These come from simulated data for which boundary separation a is 0.08. For
h 5 0.16, the distribution of estimates is narrower and
more symmetric. The standard deviation varies with
boundary separation a. When a is 0.08, the standard deviation is larger, around 0.05, and when a is 0.16, the standard deviationis smaller, around 0.03.Thus, the larger the
value of a, the better the estimate of h.
The quality of the estimates of the parameter sz depends
on the true value of the parameter. When the value that
was used to generate the data is small, 0.02, two standard
deviation intervals include zero and a value twice as large
RATCLIFF AND TUERLINCKX
as the true value (see the histogram in Figure 4). This extreme variability in estimates comes about because the
size of sz is determined by error response times in conditions with high accuracy, in which there are relatively few
error responses. When the true value of sz is 0.10, the estimates are near the true values,because the effect of sz on
error reaction times is larger and, so, is less likely to be
masked by random variation in the data. The distribution
of estimatedvaluesfor sz 5 0.10 is symmetric (see the histogram in Figure 4), but there are still two values near
In sum, theparametersa, Ter, anddrift rates are quitewell
estimated, within 10% of the mean parameter value. For
each of these parameters, the mean of the estimates from
the 100 data sets falls close to the true mean, there is no
consistent bias toward values larger or smaller than the
true mean, and the standarddeviationamong the estimates
is small relative to the size of the estimated value. But the
variabilityparameters, h and sz, are much more poorly estimated,within 20%–70% of the mean parametervalue.If
it is necessary to obtain good estimates of these parameters, sample size must be increased by running experiments with many sessions per subject.
Correlations Among Parameter Values
A potentially major problem in recovering parameters
for a model is that the value of the estimate for one parameter may be significantly correlated with the value of
another. In attempting to find the best-fitting parameter
values, given the variability in the data, a fitting method
may trade the value of one parameter off against the value
of another.Using results from the 100 fits to the simulated
data, we can investigate how serious this problem is.
This issue is especially critical when testing for differences among parameter values across experimental conditions.A larger value of a parameter in one conditionthan
Figure 5. An illustration of the covariation between parameter estimates for
linear regression. The top panel shows values of slope and intercept from simulated data (falling in an elliptical shape), and the bottom panel shows how
moving one data point up (by random variation) would decrease the slope and
increase the intercept of the best-fitting straight line.
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
in anothercould be due to random variation,or it could be
due to a genuine difference in the values for the two conditions. If other parameters of the model covary with the
parameter at issue in a way that is expected from the
model and the difference is just barely significant,it is less
likely that the observed difference is real than if the other
parameters do not covary in this way.
Before taking up the issue of covariation for the diffusion model, we will illustrate covariation between parameters with an example, linear regression (straight line fitting), for which there are only two parameters. Fitting the
equation for a straight line, y 5 mx 1 c, to data provides
estimates of slope m and intercept c. We generated 100
sets of data for a straight line, with 20 points on the line
per data set. The 100 sets of data were generated with a
slope of 1 and an intercept of zero, and to add variability
to the function, each point had a value added to it from a
normal distributionwith a mean of zero and a standard deviation of 1: y 5 mx 1 c 1 N(0,1). For each data set, a
straight line was fitted to it by the least squares method
(the standard method for linear regression), giving an estimate of slope and intercept for each data set. The slopes
and intercepts are plotted in Figure 5 (top panel).
In linearregression, if the correlationbetween the slope
and the intercept is computed, it usually has a large negative value. The correlation is large if the slope is positive
and the range of y values is more than four times greater
than the standard deviationin the variabilityin y. Figure 5,
top panel, illustratesthis negativecorrelation.Plotting the
slopes versus the intercepts produces a roughly elliptical
shape with a negative slope. For the 100 random samples
in Figure 5, the correlation between the slopes and the intercepts is 2.848.
The explanationfor the negativecorrelation is straightforward. If a data pointnear eitherend of the line is moved
far from the line, as illustratedin the bottom panel of Figure 5, the slope is raised, and the intercept is lowered (or
vice versa). Thisgivesa negativecorrelation.It is much less
likely that the slope and the intercept are both raised in
some random set of data (giving a positive correlation),
because this would require a change in all the data points
in the same direction.
When parameter estimates are evaluated, confidence
intervals are often shown for a single parameter without
reference to the behavior of other parameters. However,
confidenceregions can be drawn for the jointbehaviorsof
parameters. For linear regression, the confidence region
for the slope and the intercept forms an ellipsewith a negative slope for the major axis . In Figure 5, this would be an ellipse around the
points in the top panel. Joint confidence intervals can be
important, as the following examples show: If the slope
for linear regression is higher and the intercept is lower
than the hypothesized population values, this can be the
result of random variation in the data, but if both the intercept and the slope are higher, the values could be different from the populationvalues. For example, in the top
panel of Figure 5, imagine a point at slope 5 1.07 and intercept 5 0.6. This point would lie inside the individual
confidence intervals for both the slope and the intercept
(i.e., 1.07 lies within the vertical scatter of points, and 0.6
lies within the horizontal scatter of points). But the point
would lie outside the joint confidence region (the ellipse
that would contain 95% of the points in the top panel of
Figure 5). So an understandingof the joint confidence regionsprovidesadditionaluseful informationin testing hypotheses about differences among parameter values.
For linearregression,jointconfidenceregionscan be determined analytically, and hypotheses can be tested about
the joint behavior of the slope and the intercept. But for
the diffusion model, such analyticalresults are almost impossible to produce. However, simulation methods can
provide information that can be used more informally to
determine whether joint behavior of parameter values is
somethingthat needsto be examined for a particularset of
fits for a particular data set. This will not allow hypotheses to be tested but will provideenoughinformationto determine whether the issue of correlated estimates needs to
be considered in drawing conclusions about differences
among parameter values across conditions or whether it
can be ignored.
For the diffusion model, Table 3 contains correlations
between each pair of parameters for the maximum likelihood fits shown in Table 3. For each of the 100 data sets
for each of the6 sets of parametervalues,we computedcorrelations between each pair of estimated parameters, and
Table 3 shows the means of these correlations. Figure 6
shows scatterplots for the 100 parameter estimates for the
fits generated from 1 of the 6 sets of parameter values (the
3rd set in Table 1). For the 100 data sets, each scatterplot
Correlations Among Parameter Values for Maximum Likelihood Fits
(N = 250 per Condition)
RATCLIFF AND TUERLINCKX
shows theestimatedvalue of oneparameterplottedagainst
the estimated value of another parameter.
Many of the correlations in Table 3 are positive. This
comes about because of the effects of extra slow error reactiontimes. The occurrence by chance of a few, extraslow
error reaction times moves the estimates of many of the
parameters of the model away from theirtrue values,all in
the same direction (the variability parameters h and sz are
moved more as a percentageof their means than are a, Ter,
and drift rates), and how they change together is what the
correlations show.
To show the effect of slow error response times, we provide a concrete example to illustrate the effect: We took
the third set of parameter values from Table 1 and produced predicted values of response probabilities and
quantilereaction times from the diffusion model. We then
increased the fifth quantile reaction time for errors in the
condition with drift 5 0.1 by 100 msec. We fit the model
to the data with and without this increase. When the original data were fitted using the chi-square method detailed
next, the parameters were recovered to within 0.1% of
their true values. When the functions with the single increased quantile reaction time were fitted, the estimated
values changed by small amounts. The value of a was increased from 0.16 to 0.162, Ter from 0.3 to 0.302, h from
0.08 to 0.086, sz from 0.02 to 0.030, and the three drift
rates that differed from zero increased from 0.300, 0.200,
and 0.100 to 0.307, 0.205, and 0.103, respectively. Figure 7 shows the quantileprobabilityfunctions for the data
with one data pointmoved up and the fittedquantileprob-
Figure 6. Scatterplots among the parameters of the diffusion model for fits to simulated data, using the maximum likelihood
method (using parameters from line 3 in Table 1).
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
ability function for the latter. The result is that the fitted
quantile probabilityfunction is moved up a little, more in
the higher quantilesthan in the lower quantiles.This is reflected in an increase in the estimated values for all the
model’s parameters, as was described above. Making
changes in more of the quantileswould magnify the small
effect of the change in onlythe fifth quantilereaction time
for errors with drift 5 0.1.
This increase in all the parameter values is exactly the
pattern we see in the correlations and scatterplots in Figure 6 and Table 3. The most common and largest random
variation in data is in error reaction times, as is illustrated
in Figure 3, especially in the longest quantiles. As the example above shows, this variation is sufficient to produce
the positivecorrelationsobservedin the parameter values.
The general conclusion for the diffusion model is that
random differences among samples of data produce differences in all the parameters’ estimates in the same direction. This means that if the sizes of differences among
parameter values are important, correlations must be considered. For example, if there is a significant difference
between two values of boundary separation a and if h and
drift rates also vary in the same direction as a, the differences in a could be due to random variation in the data.
The positivecorrelations among estimated parameter values come from variability in error reaction times, and so
they are also obtained with the chi-square and weighted
least squares methods of fitting.
Parameter Estimates for the
Maximum Likelihood Method
When Boundaries Are Asymmetric
For the case with asymmetric boundary separation (all
the parameters were the same as in Table 1, except z 5
.375a, and drift rates were 2.3, 2.1, .1, and .3), the standard errors in the recovered parameters were smaller than
those for the symmetric case. One of the main reasons for
the better estimates is that with the starting point being
asymmetric, reaction times for the response with the
closer boundary are shorter, which makes both Ter and a
better estimated. The correlation matrix for asymmetric
boundaries shows the same pattern as that in Table 3, but
the correlations are 25% smaller because of better parameter estimation.These same differences between the symmetric and the asymmetric cases are found in most of the
fits presented later in this article, so we will discuss the
asymmetric case only if the results are different from the
symmetric case.
PARAMETER ESTIMATES FROM THE
CHI-SQUARE FITTING METHOD
An important advantage of the chi-square method is
that the time needed to producea fit is 25 times shorter per
experimental condition than that for the maximum likelihood method with 250 observationsper conditionand 100
times shorter with 1,000 observations per condition. For
example, when the chi-square method is used, a fit to one
set of data takes about 25 sec on a 500-MHz Compaq
XP1000 with a 21264 Alpha processor and about 14 sec
Figure 7. An illustration of what happens to the fit of the diffusion model if one error quantile is increased (cf. the change in
one data point in Figure 4).
Means and Standard Deviations of Parameter Values Recovered
From the Chi-Square Method (N = 250 per Condition)
Parameter Set
RATCLIFF AND TUERLINCKX
with PC runningLinux,using an Intel compiler on a 1.33-
GHz Athlon processor. For fittingmany sets of data(many
individual subjects or many simulated data sets), the chisquare method runs for minutes or hours, whereas the
maximum likelihoodmethod runs for days.
Table 4 shows the mean parameter estimates and the
standard deviations in the parameter estimates with 250
observations per condition. In order to provide a rough
guide for comparison of the three different fitting methods, we will discuss the sizes of the standard deviationsin
terms of approximate averages. When we want to compare two methods, we compute the ratios in the standard
deviations of their parameter estimates and then compute
the median across the ratios for each of the parameters.
This gives a rough summary of the performance of each
method. For example, the standard deviations in the parameter values given by one method might be anywhere
from 1 to 2 times larger than those for another method,
with a median of about 1.5 times larger. Summary statements like these are only approximate,because the differences in the standard deviations between fitting methods
can be different for different parameters.
The first result of note is that the median standard deviationis 33% larger,on average,for thechi-squaremethod
than for the maximum likelihoodmethod.This means that
parameters recovered with the chi-square method are
likely to be farther away from their true values than those
recovered with the maximum likelihoodmethod. Second,
there are biasesin some of the parameterestimates:in particular, h and the drift rates have estimated values that are
larger than their true values by 5%–10%; sz has estimated
values higherthan the true value when sz is 0.02,but when
it is 0.10, it is estimated to be a little lower than the true
value. However, the means of the estimates for Ter and a
are near their true values, with no systematic direction for
differences from the true values.
We do not present histograms of the estimated parameter values, because they are qualitativelysimilar to those
shown in Figure 4 for maximum likelihood. In general,
they show the same biases for the same parameters. The
only major differences are a few more cases in which h is
estimated to be zero and some cases in which h is estimated to be larger for the chi-square method than for the
maximum likelihood method. The correlation matrix for
the chi- square fits shows almost the same patterns as
those in Table 3, but with slightly higher correlation values. This is because the maximum likelihoodand the chisquare methods produce variation in parameter values in
the same way in response to random variations in the simulated data. So, all the conclusionsthat were presentedfor
the maximum likelihood covariations among parameter
values apply equallyto correlationsamong parameter values derived from the chi-square method.
When boundary positionsare not symmetric, just as for
the maximum likelihood estimates, the standard errors in
the estimates are smaller than when the boundaries are
symmetric. Also, there is slightly less bias than in the
symmetrical case.
In sum, the chi-square method is somewhat worse at recovering accurate parameter estimates, but it is significantly faster than the maximum likelihoodmethod.It also
turns out, as will be seen later, that the chi-square method
is a little more robust to a small proportionof outliersthan
is the maximum likelihood method.
PARAMETER ESTIMATES
FROM THE WEIGHTED LEAST
SQUARES FITTING METHOD
Likethemaximumlikelihoodmethod,theweightedleast
squares method suffers from a speed problem. For the
weighted least squares method, the whole reaction time
distributionhas to be estimatedfor each conditionfor both
correct and error responses, which requires hundreds of
points (recall that we used 400 points). If the weighted
least squares method turned out to be the best estimation
method, it could be speeded up by using search methods
to find the quantiles, but it would still be at least 10 times
slower than the chi-square method.
Table 5 shows the means and standard deviationsin the
recovered parametervalues.The first thingto noticeis that
for the lower value of a (true value 5 0.08), this method
Means and Standard Deviations of Parameter Values Recovered From
the Weighted Least Squares Method (N = 250 per Condition)
Parameter Set
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
is better than the chi-square method; the recovered estimates of all the parameters are nearer the true values, and
standard deviationsin them are smaller. But for the higher
value of a, the oppositeis true. The average across the two
values of a of the standard deviationsis about 29% larger
for the weighted least squares method than for the chisquare method. Overall, the biases in the mean parameter
values away from the true values are about the same for
the chi-square and the weightedleast squares methods,but
the weightedleast squares method has higher standard deviations overall than does the chi-square value. For both
the chi-square and the weighted least squares methods,the
estimates are less accurate than the maximum likelihood
estimates, and their variability is greater. The average of
the standard deviations for the weighted least squares
method is about 63% larger than that for the maximum
likelihoodmethod.
The correlationsamong parametersfor theweightedleast
squares method are a little larger, on average, than those
for the chi-square method (they vary between 5% lower
and 10% larger, except for zero drift, for which the correlationsare near zero). But the correlationsall show exactly
the same patterns: When one is higher than another in
weighted least squares, it is also higher in chi-square and
maximum likelihood. This means that all three methods
produce correlations among parameters, for the reasons
that were discussedabove(using the examplein Figure 7).
In sum, the parameter estimates from the weighted least
squares method are about as biased as those from the chisquare method, but they have larger standard deviations,
and the weighted least squares method has a huge deficit
in computational speed. Both the weighted least squares
and the chi-square methods recover mean parameter values less accurately than does the maximum likelihood
Figure 8. A sample fit of the three methods (maximum likelihood, chisquare, and weighted least squares) to one set of simulated data (the Xs). The
parameters are shown at the top, and the parameters used to generate the simulated data are in the column headings of the table. The top theoretical function (for each of the five quantiles) is from the maximum likelihood method,
the middle function is from the chi-square method, and the lower function is
from the weighted least squares method.
RATCLIFF AND TUERLINCKX
method(by about5%–10%, dependingon the parameter),
and the standarddeviationsin the estimates are larger than
those with the maximum likelihoodmethod.But the maximum likelihoodmethod is slow relative to the chi-square
method and more sensitive to outliers, as will be demonstrated below.
AN EXAMPLE OF THE QUALITY
OF THE FITS OF DIFFUSION MODEL
TO SIMULATED DATA
As was just pointedout, the parameter estimates for the
maximum likelihood method are quite accurate, and for
the chi-square and weighted least squares methods, they
are reasonablyaccurate, within about5%–10% of the true
values. The question we raise here is how good the fit of
the diffusionmodel to the simulateddata is, giventheseestimates.
To provide an example of how good the fit is, we chose
one representativedata set for which the estimated parameter values from all three methods were near to the true
parametervalues from which the simulated data were generated. The true values were the first set of parameter values in Table 1. Each set of estimated parametervalues,one
set from each method, was used to generate predicted values of accuracy and reaction time quantiles for each experimental condition. Figure 8 shows the quantile probability functions for the simulated data (the X points) and
the quantile probabilityfunctions for the predicted values
for each method. In general, the fits all look good, and all
indicatethatthe modelis fittingthedatawell.The weighted
least squares and the chi-square methods produce fits to
the quantileprobabilityfunctionsthat are almost identical
to each other. The chi-square method produces a larger
value of h (50% larger) and a larger value of sz than does
the least squares fit, but the effects of these two larger values lead to only small differences (compared with the
weighted least squares method) in the predicted reaction
time quantiles in the quantile probability functions. The
maximum likelihood method produces fits that do not
match the .9 quantilesquite as well as the other two methods (by visual inspection at least), but ignoring the fits
from the other two methods,the fit looks quiteacceptable.
One would be hard-pressed to determine which method
produced which of the fits in Figure 8 or to work out
which fit was the best or worst simply by looking at the
Figure 9. Predicted functions from the average parameter values from the fits of the
maximum likelihood, chi-square, and weighted least squares methods (third lines of
Tables 3, 6, and 9) to the predictions from the diffusion model for input parameters
from the third line of Table 1. The curve with Xs represents the theoretical predictions
from the input parameters. The bottom right panel shows the average quantiles from
the simulated data (the lines).
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
graphical fit of the predicted functions to the data functions. The maximum likelihood method produces parameter estimates slightly closer to the values used to generate the simulated data, but this may not translate into
better-looking fits to the quantile probability functions.
The fits for one representative data set shown in Figure 8 illustrate that all three fitting methods do well. But
the three methods do show systematic, although small,
differences in their fits. The differences come about because the chi-square and the weighted least squares methods fit data summarized in quantiles, whereas the maximum likelihood method fits all the individual reaction
times. We will explain and illustrate this in the next paragraphs.
Figure 9 shows data and fits for the third set of parameter values in Table 1. The lines in the bottom right-hand
panel of Figure 9 show the average quantiles for the 100
sets of simulated data generated with these parameter values. The Xs are the predicted values of the quantiles. As
the figure shows, the average quantilesfrom the simulated
data do not exactly match the predicted values: For errors
that have very low probability (points to the far left), the
average quantiles for the data are higher than their predicted values. This is an inherent characteristic of the diffusion model becauseofthe variabilityin drift rate. As drift
rate varies around its mean, some responses will come
from slightly higher values of drift rate, and some from
slightly lower values. For slightly higher values of drift
rate, accuracy will be greater, and so there will be relatively fewer errors. For slightly lower values of drift rate,
accuracy will be lower, and there will be relatively more
errors. The errors from low values of drift rate are slower,
and there are more of them, so averaging across all the responses that have the same mean drift rate gives slower errors than would be expected from the mean alone. Also,
for some of the data sets, accuracy in the most accurate
experimental conditions may have been so high that there
were too few errors to compute quantiles.Drift rate would
have been large for these conditions,and so error response
times would have been fast. Eliminating these fast errors
because there were too few of them to compute quantiles
would make the quantiles computed from the simulated
data larger than the predicted quantiles.
The chi-square and weighted least squares methods fit
data as it is summarized by quantiles.Because the extreme
(far left) error quantilesfor data are systematicallyhigher
than predicted, the chi-square and weighted least squares
fits yield parameter estimates that also give slower extreme errors. This is shown in the bottomleft and top right
panels of Figure 9, where the Xs are the same as in the
bottom right panel—that is, they are the quantile values
predicted from parameter values that were used to generate the simulated data—and the lines are the quantile values predicted from the means of the parameters estimated
from fitting the 100 sets of data. In contrast,the maximum
likelihoodmethod fits individualreaction times, so quantiles predicted from the means of the parameters estimated for the 100 data sets with this method do not show
the problem with extreme errors. Because the extreme
quantilesare higher than the predicted values from the parameters, they are fit with higher values of h, thereby producing slightly slower errors in the most accurate condition.When sample size is increased to 1,000 observations
per condition, the bias in h is reduced for the chi-square
and weighted least square methods, because the quantile
error reaction times in the most accurate conditionare better estimated.
If the bias toward estimating higher values of h for the
chi-square and weighted least squares methods results
from the bias in the data, fits of the chi-square method to
the average quantilereaction times should show the same
bias. Fits of the chi-square method to the average data are
shown in the bottom right panel of Figure 9 (the least
squares method shows the same results). Parameter estimates were within a few percent of those in Parameter Set
C of Table 4: for example, the values of a 5 0.1643,Ter 5
0.3091, h 5 0.0952, and v1 5 0.3352 (the other drifts
were equally close to the values in Parameter Set C of
Table 4). Thus, the biases in the chi-square fits do indeed
come from the method’s use of quantile reaction times as
summary statistics for the data.
It should be noted that when quantiles computed from
single data sets are graphed and compared with quantiles
predicted from parameters estimated with the chi-square
and weighted least squares methods, the fits will often appear to the eye to be slightly better than quantiles predicted from parameters estimated with the maximum likelihoodmethod.Thisis because thechi-square andweighted
least squares methods capture the same bias toward slow
extreme errors, as is exhibited by the quantilesof the data
themselves.
In summarizingthe qualityof the fits of the three methods, we pointed out that the estimated values of parameters for the maximum likelihood method were quite accurate and that the estimated values for the chi-square and
weighted least squares methods were about 5%–10% farther away from the true values and about 30%–60% more
variable. The bias in the values of the standard deviation
in drift across trials (h) for these two methods arose from
the fact that error quantile reaction times are biased in the
highest accuracy condition. When the chi-square and
weighted least squares methods were used to fit predicted
quantilereaction times and accuracy rates generated from
the model (and not simulated data), the recovered parameter values matched the values used to generate the predicted values quite accurately.
CHANGING THE RANGE OF DRIFT RATES
In the investigationswe have reported so far, drift rates
spanned a range such that accuracy rates varied widely,
from floor to ceiling.In thissection,we will show what happens for other ranges of accuracy values. To summarize
the conclusions, parameters are recovered about as accu-
RATCLIFF AND TUERLINCKX
rately when the range of accuracy rates is much smaller
than floor to ceiling. The only exception occurs when accuracy rates have a small, very high range, with accuracy
in all the experimental conditions above about 90% (as
would be the case in many lexical decision experiments).
Table 6 shows what happens with the maximum likelihood and chi-square fitting methods when the drift rates
used to generate the simulated data are limited in range.
(Because weighted least squares provides the same quality of fits as the chi square, it would provide the same results here as the chi square.) The results are qualitatively
the same as those obtainedwhen drift rates span the range
from ceiling to floor (Tables 2 and 4). For example, when
the chi-square estimator is biased in Table 6, it is also biased in Table 4. For Sets A and B, the standard deviations
are about the same as those shown in Tables 2 and 4, and
the biases in the parameters are about the same. For Sets
C and D, the standard deviation in Ter is larger, but the
other estimates and standard deviationsare about the same
as those in Tables 2 and 4. The larger standard deviation
in Ter occurs because the .1 quantile and shortest reaction
times are larger and more variable than when there are
larger drift rates that produce shorter reaction times,
which locates Ter better. For Set E, the standard deviations
are larger for both maximum likelihood (Table 2) and chi
square (Table 4; exceptfor Ter, which is better because the
.1 quantileis smaller and less variablethan in the lower accuracy conditions).
These results show that the best estimates of the parameters of the diffusion model are produced when experimentalconditionsspan a moderately wide range of accuracy values, but not necessarily all the way from ceiling
to floor. Only when accuracy is very high (above, say,
90%) for all conditions do the estimates suffer seriously
for each of the three methods. Therefore, when collecting
data for fitting this model (and probably similar stochastic models), it is important to have conditions that span a
moderately wide range of values of accuracy.
CHANGING SAMPLE SIZE:
CONSISTENCY OF ESTIMATORS
So far, we have discussed resultsfor simulateddata with
a sample size of 250 observations per condition. This
would be about one session of data per subject (4 3 250
trials 3 3 sec per trial 5 50 min). The means of the estimated parameter values and their standard deviations
(presented in the tables) can be used to construct confidence intervals around the parameter values estimated for
a single session for a single subject. For the simulations
with 1,000 observations per condition, the results were
exactly what would be expected for the behavior of the
standard deviations as a function of sample size: They
scaled as a function of 1/ÏN (the standarddeviationis the
square root of the sum of squared differences divided by
N ). For example, the average standard deviation in the
maximum likelihoodparameter estimates is decreased by
1.98 (with symmetric boundaries) as N is increased from
250 to 1,000 observations per condition, and for the chisquare estimation method, the standard deviation is decreased by 2.03.
Because the standard deviations scale as expected, tables for standard deviationsfor a sample size of 1,000 are
not necessary. To a good approximation (within 10%),
Maximum Likelihood and Chi-Square Fitting Methods Applied to Samples Where Drift Rates Do Not
Span the Range From Low to High (N = 250 per Condition)
High drift rates (A)
Target value
High drift rates (B)
Target value
Low drift rates (C)
Target value
Low drift rates (D)
Target value
Very high drift rates (E)
Target value
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
standard deviations for sample size N can be computed
from the values in the tables for sample size 250 by multiplying the standard deviations by Ï(250/N).
However, the means of the parameter estimates do
change as a function of sample size. They approach the
input values from Table 1, so that a value of h (for the chisquare method) of 0.1135 in Table 4 (top row) becomes
0.0951 with N 5 1,000. Likewise, a drift rate of 0.4346 in
Table 4 (top row) becomes 0.4116 for N 5 1,000. For the
chi-square and the weighted least squares methods, the
mean parameter values were closer to the parameter values that generated the simulated data for 1,000 observations per condition than to those for 250 observations per
condition (the maximum likelihood method had parameters that had little bias with both 250 and 1,000 observations per condition). This reduction in bias is due to better estimation of the quantiles for error reactions times in
the conditions with few errors.
CONTAMINANTS AND VARIABILITY IN Ter
One of the major problems bedevilingresearch in which
reaction time measures are used is the problem of contaminant reaction times—reaction times that come from some
cognitive process other than the one being studied. Contaminantsinclude both reaction times that overlapthe distributionof reactiontimes from the decisionprocess under
study and outlierreaction times that are outside the distribution of reaction times from the process being studied.
In this section, we will construct new sets of simulated
data for which contaminantreactiontimes are addedto the
reactiontimesgeneratedfrom the diffusionmodel. We will
show that the chi-square and maximum likelihood methods fail badly with these data. We will also show that the
weighted least squares method is robust, although the recovered parameter values are different from those used to
generate the diffusion model portion of the data.
Then we will apply a correction for contaminantsto the
maximum likelihoodand chi-square methods (we did not
applythe correction to the weighted least squares method,
because implementations are slow and it would perform
about the same as the chi-square method). With the correction, the chi-square and maximum likelihood methods
do a good job of recovering parametervalues.Importantly,
however, it is not possible to correct for all possible contaminants. If, after correction, there remain a small number of contaminants not represented in the model (e.g., a
small number of fast responses in one condition),the chisquare method performs adequately, but the maximum
likelihoodmethod does not.
To correct for contaminants,we explicitlyrepresentthem
in the fitting process and estimate their proportions. The
correction leads to an increase in the standard deviations
of the estimates of parameter values, because the contaminants reduce the quality of the data and also increase the
number of parameters to be estimated.
Besidescontaminants,we investigatedvariabilityin the
parameter Ter, the parameter that encodes the time taken
up by processes involvedin the task understudy other than
the decision process itself. In examininghow well the diffusion model fitted data from a recognition memory experiment , the weighted least
squares fitting method produced good fits (by visual inspection). But the chi-square method failed dramatically
 . For example, the predicted .9
quantile reaction times missed the data by as much as
200–400 msec across experimentalconditions.We noticed
that in the data, the .1 quantile was much more variable
than was the case in other data sets and was especially
more variable than the simulated data examined in this article so far. To accommodate the increased variability in
the .1 quantile reaction times, we assumed that Ter was
variable across trials. Previously, variability in Ter was
never considered a necessary addition to sequential samplingmodelsfor two-choicedecisions(when it was thought
about at all). However, the investigations we will present
below indicate that variability in Ter should always be included in models when data are fit.
In the end, the fitting methods we present as optimal
are chi-square and maximum likelihood methods that incorporate the correction for contaminants and also have
variabilityin Ter. But we also argue that the weighted least
squares method is the most robust of the three methods.
Fast and slow outliersthat lie outside the distributionof
reaction times from the decision process under investigation can occurfor many reasons. For example,fast guesses
often occur when subjects anticipate the stimulus and hit
the response key by mistake. Or they can occur when subjects become frustrated or bored and start hittingresponse
keys quickly without trying to perform the task (although
this behaviorcan sometimes be eliminatedby introducing
a time delay after every response time shorter than some
value, such as 250 msec). Slow outliers often occur when
subjectsare momentarily distracted,which leads to a delay
in responding.
We treat fast and slow outliers differently. First, fast
guesses, as was suggestedin the introduction,can be eliminated (in large part) experimentallyor with cutoffs on reactiontimes.Once most fast guesseshave been eliminated,
some of the remaining fast responses that are not from the
decision process under investigation might be subsumed
by variationin Ter. If there are spuriousfast responses still
remaining in the data, but only a few of them, they have
relatively small effects on parameter estimates obtained
with the chi-square method, because they will have little
effect on the .1 quantilereaction time. Slow outlierscan be
eliminated by cutoffs, as was also mentioned in the introduction. Any remaining contaminants, contaminants that
overlap with the distributions of response times from the
cognitive processes under investigation, are accommodated by our correction, as we will show below.
Fast Contaminants and Their Consequences for
the Three Fitting Methods
Fast contaminantsprovide a particularly nasty problem
for the maximum likelihood method. Because each re-
RATCLIFF AND TUERLINCKX
sponse time has to have a probability density assigned to
it [ f(t)], the value of Ter has to be lower than the shortest
reaction time. This usually severely distorts the estimates
of allthe otherparameters.The chi-square andtheweighted
least squares methods avoid this problem to some degree
because they group responses into quantiles, so that the
precise locationof the minimum is lost and not used by the
fitting method. But when the proportion of fast contaminants is greater than a few percent, the chi-square method
producespoorrecovery of parameters,because the.1 quantile is distorted,as we will show below. The weighted least
squares method is the most robust to fast contaminants.
To illustrate the dependence of maximum likelihood
parameter estimates on the fastest responses, we examined the correlations of estimates of Ter with minimum reaction times for the fits for the simulated data generated
from the parameters in Table 1. The average correlation
overthesix groupsof parametervalueswas about.45. This
means that Ter is beingdeterminedto a large degree by the
minimum reaction time. In contrast, the correlations between Ter and the minimum reaction time for the chisquare and weightedleast square estimates averagedabout
zero, as would be expected because the fitting methods
have information only about quantile reaction times.
Slow Outliers and Their Consequences for the
Three Fitting Methods
Slow outlierscannoteasily be eliminatedby cutoffs, except in extreme cases. They can be difficult to detect, because they can hide in the tail of the reaction time distribution . They
spread out the tail, rather than producing a second mode
 . There is always going to be some small number of
responses that are slow outliers, so it is important that
methods be developed that allow their effects to be reduced or minimized. However, little can be done with
really bad data. As a rule of thumb, if more than about 5%
of responses are fast guesses or slow outliers, we should
consider the data unusable for model fitting. This rule of
thumb motivatesthe choice of 5% as the assumed proportion of contaminants in the simulations presented next.
We examined the dependence of the parameter estimates shown in Tables 2, 4, and 5 on long reaction times
for the fits to the simulated data generated from the parameter values from Table 1. The correlations (averaged
over fits to the 100 sets of data and over the six groups of
parameter values) between the estimated values of the parameters and the smallest .1 quantile and the largest .9
quantilewere computedfor the three methods.There were
small correlationsbetween all the parameter estimates and
the .1 quantile for chi square (.1), weighted least squares
(.1), and maximum likelihood (2.08), and there were
larger correlations (.35 for chi square, .40 for weighted
least squares, and .20 for maximum likelihood) between
all the parameters and the .9 quantile.This means that random variationin the recovered parameter values across the
simulated data sets is determined to some degree by the
variation in the largest of the .9 quantiles across the experimental conditions(a conditionwith a large .9 quantile
will often have a large .7 quantile also). This reflects the
point made earlier about correlations among parameter
values, that if one of the .9 quantilesis increased, most of
the estimated parameter valuesincrease.Again, thisshows
how much the estimated values of the parameters are determined by the tail of the reaction time distribution.
If, as we have just shown, parameter estimates depend
significantly on the longest reaction times, then if those
reaction times are contaminants, the parameter estimates
will be altered, sometimes drastically, away from their true
values. We will show this in the next section by explicitly
adding contaminants to simulated data.
FITTING THE DIFFUSION MODEL
IN THE PRESENCE OF CONTAMINANT
RESPONSE TIMES
To examine the behaviors of the three fitting methods
when they are appliedto simulateddatathatcontaina small
proportion of contaminants, we added, with a probability
of po, a random time of between 0 and 2 sec to reaction
times derivedfrom thediffusionmodel.The valueof po was
set to .05, which means that if there are 250 observationsin
a condition,on average12.5 will be contaminants,although
sometimes there might be 0 or 25 contaminants.With contaminants in the data, all three methods fail badly to recover reasonable parameter estimates when a 5 0.08, and
fail, but not as badly, when a 5 0.16. In the former case,
contaminantsoccur well out in the tail, where no reaction
times occur in the data simulated from the diffusion
model. In the latter case, the diffusion model producesreaction time distributions with much longer tails, and so
the contaminants overlap much more with the tails. If we
were to assume that the range of the contaminantreaction
times increased as a increased, then a 5 0.16 fits would
have failed as much as those in the a 5 0.08 case. However, the assumption we made (range from 0 to 2 sec) is
about what might be expected in practice in experimental
data. The next three sections will detail our results, and
then we will go on to discuss corrections for fitting contaminated data.
Maximum Likelihood Method
The maximum likelihood method uses each reaction
time in the fitting computation,and each one is weighted
equally, so contaminants have a serious effect on parameter estimates. We fitted only a small number of representative data sets, because each fit produced extremely bad
estimates of the parameters and each fit took a very long
time (hours). In each case we examined with a 5 0.08, a
was estimated to be about 0.14, h was estimated to be
about 0.5, instead of 0.08, and the drift rates were two to
four times their correct values.
Figure 10 shows quantile probability functions produced from the parameters estimated from the maximum
likelihood method for one representative set of contami-
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
nated data. The method attempts to fit correct responses
at the expense of error responses, because there are more
observationsfor correct responses (hence, the large misses
between the predictions for the error quantiles and the
data). We will discuss the issue of poor fits further in the
next section, with reference to the chi-square method.The
arguments there can be applied equally well to the maximum likelihood method.
Chi-Square Method
Table 7 shows the means and standard deviationsin the
parameter values for the chi-square method applied to the
contaminated data. The result is relatively poor recovery
of the parameter values. The boundary separation (a) is
overestimated in each case by as much as 20%, the drift
rates are overestimated by as much as 40% (especially at
low values of a), and h is overestimatedby a factor of two
to three at low values of a. The standard deviations in the
parameter values also increased from those for simulated
data without contaminants(Table 4), especiallyat the low
value of a. For the conditions with a 5 0.16, some of the
recovered parameters are quite close to the target values.
This is because the contaminants span much the same
range as the real data. The average maximum reaction
Figure 10. Quantile probability functions for fits of the maximum likelihood,
weighted least squares, and the chi-square methods, with and without corrections for
contaminants, to one data set with contaminants. The data set was chosen to illustrate
some of the worst fits of the models without corrections for contaminants. The top
panel shows the parameter values for the fits. The five lines represent the .1, .3, .5, .7,
and .9 quantile reaction times.
RATCLIFF AND TUERLINCKX
time from the simulated data without contaminants is
about 2,500 msec, and the largest maximum contaminant
reaction time is about 4,300 msec, but few of the contaminants are this large. The median reaction time from the
diffusion model plus the maximum contaminant is about
2,700 msec, and extra long reaction times are possible
with and without contaminants being added. In contrast,
for small values of a, all long reaction times are contaminants;the average maximum reaction time from the model
is less than 1,000 msec, and contaminants have up to
2,000 msec added to the reaction time generated from the
model. In practice with real data, some of these reaction
times would be trimmed out as outliers, and the problem
would not be as severe.
Figure 10 shows one of the pathologicalcases in which
the drift rates are estimated to be very high (as compared
with the parameters used to generate the diffusion process
portion of the data), the variability in drift is estimated to
be very high, and the boundary separation is estimated to
be high. The method is attemptingto fit correct responses
at the expense of errors (which are weighted less because
there are fewer of them). One reason that the fits are poor
for this set of data is that there are contaminantsin the four
conditions for correct responses but few contaminants in
the error responses (because there are only 5% contaminants, on average, in any condition; on a particular simulation, there may be 0%, 5%, or 10% contaminants). The
predicted function gets close to the correct responses and
to the intermediate accuracy errors and badly misses the
extreme errors (to the left-hand side of the top panel).
The key point is that parameter estimates are much
larger than the values used to generate the noncontaminant data, because the data set is generated not from a diffusion model with the target parameter values, but from
the diffusion process plus contaminants.
Weighted Least Squares Method
As was discussed earlier, in the weighted least squares
method,the.9 quantileis weightedlessthanthelowerquantiles. This is because the shorter quantiles have less variability, whereas the longerquantileshave more variability.
Because the .9 quantileshows the effects of contaminants
most and is weighted least, the weighted least squares
method is less affected by contaminantsthan are the other
The recovered drift rates are quite near their true values
(except the highest drift rates for a 5 0.08, which may
have had cases in which the number of errors was too
small to providequantiles).The parameter a was betterestimated than with the chi-square method, but Ter was underestimated, which it was not for the chi-square method.
The value of h was overestimated, but by much less than
with the chi-square method. The standard deviations in h
were less for the chi-square method than for the weighted
least squares method, which means that the former produced an incorrect estimate with smaller spread than did
the latter. The value of Ter was more overestimated for
weighted least squares, but all the other parameters were
more overestimated for chi square (on average).
Figure 10 shows the quantileprobabilityfunctionsfrom
the best-fitting parameter values for the weighted least
squares method, using the parameter values in Figure 10.
Unlike the chi-square and maximum likelihood methods,
the data are fit reasonably well, and the speed-up in error
reaction times, going from the middle of Figure 10 to the
left-hand side, is well captured. The parameter values are
much closer to those that generated the diffusion model
portion of the data, and the effects of contaminantson the
parameter estimates are not large.
We concludethat theweightedleastsquaresmethoddoes
better than the chi-square and the maximum likelihood
methods (without corrections for contaminants) at fitting
contaminated data. The fitted quantile probability function comes much closer to all the data for the weighted
least squares method than for the chi-square and the maximum likelihoodmethods (for this single example and for
other data sets). This is true especiallyin cases such as the
one presented in Figure 10, where the contaminantsaffect
correct reaction times more than they affect error reaction
times. The parameter estimates are biased away from the
true parameter values for the diffusion component of the
simulated databecause of the contaminants.Thisis a prob-
Means and Standard Deviations of Parameter Values Recovered
From the Chi-Square Method With No Corrections Applied to Data with
5% Contaminants (N = 250 per Condition)
Parameter Set
0.1631 20.0001
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
lem if the aim is to recover accurate values of the parameters but is not a problem if the aim is to see whether the
model can fit thedata. The examplein Figure10 illustrates
the point that the weighted least squares method is much
more robust than the other two methods to contaminated
CORRECTION FOR CONTAMINANTS
In this section,we will examinea correctionscheme for
the maximum likelihood and chi-square methods. The
correction allows recovery of estimates of parameters that
are about as good as when the data contain no contaminants but the standard deviationsin the estimates are about
10%larger.The qualityof thefit is usuallysimilarorsomewhat better than that of the weighted least squares method
without correction for contaminants.
Contaminants are added to simulated data by adding a
random amount of time of between 0 and 2 sec to a reaction time derived from the diffusion model. This assumption about the distribution of contaminants is our best
guess about what happens when motivated subjects have
occasional distractions. The random amount of time is
added to a small proportion of the responses with probability po, which we set to .05. This sum produces a combined distribution of contaminated reaction times that is
almost a rectangulardistribution.It has a rapid rise, a long
flat asymptote, and a slow fall in the tail .
In order to take contaminantsinto account in the fitting
methods, it is necessary to make an assumption about
what form the distributionof contaminant response times
takes, an assumption that entails few additional parameters and is consistent with what we know about contaminantsin real experimentaldata. The assumption we make
below in modeling is not identical to the assumption we
make in generating the simulated data but is not too different (the assumption in modeling allows contaminants
to be random, as well as the result of a random delay added
to a regular decision). The assumption we make is that
contaminantscome from a rectangulardistributionwith a
range determined by the maximum and the minimum reaction times in each experimental condition and that the
probabilityof a contaminant( po) is the same in each condition(i.e., is independentof the stimulus).The maximum
and minimum reaction times will not determine the true
range of contaminants in any particular condition (there
may not be any contaminantsin some conditions).But our
method of fitting all the conditions with the same parameter values gives successful recovery of the parameters of
the diffusion process and the proportion of contaminants.
To summarize, to fit the diffusion model to simulated
data with the added contaminants,a mixture of two distributions is used. The first componentis the distributionof
reaction times from the diffusion process weighted with
probability 1 2 po, and the second component is a uniform distribution that ranges from the minimum to the
maximum reaction times in each condition, weighted by
probability po. The probability density for each reaction
time is (12 po)f d(t) from the diffusion model plus po fu(t)
from the uniform distribution.
We applied the correction methodto the maximum likelihood and the chi-square fitting methods. The weighted
least squares method is robust, and the contaminant correction methodwould improve performance aboutas much
as it does for the chi-square method, but implementations
would be so slow that it would never be used in practice.
Maximum Likelihood
For the maximum likelihood method, each reaction
time is used to compute the probability density [ f (t)] for
the diffusion process and the probabilitydensityfor a uniform distribution with a range of maximum reaction time
minus minimum reactiontime. These are weightedby 1 2
po and po, respectively, and summed to provide the likelihood of that reaction time. In all other respects, the fitting
method is the same as without correction.
Generally, the parameters of the diffusionmodel are recovered a little less accurately than when the uncorrected
method is appliedto data withoutcontaminants.For all six
sets of parameter values, the value of h is overestimated
by 10%–20%, and drift rates are overestimated by
5%–10%, as compared with the values used to generate
the simulated data (Table 1). For a 5 0.08, the proportion
of contaminants ( po) recovered by the fitting program is
about .049, close to the true value of .05. For a 5 0.16, po
is estimated to be less than .03, and the value of a is overestimated by about 5%. With a 5 0.16, the reaction time
distribution is spread more than in the a 5 0.08 case, and
the fitting method accommodates some proportion of the
contaminantsas thoughthey were generated from the diffusion process. The standard deviations in the parameter
values are increased by approximately15% relativeto fitting the uncontaminateddata.
In practice, it is not possible to know whether data do
or do not contain contaminants (unless they are extreme
enough to be removed by cutoffs). Therefore, it is important to check whether the contaminant correction method
recovers parameter values accurately even when there are
no contaminants in the data. We applied the corrected
maximum likelihood method to the original sets of simulated data that contained no contaminants. The estimated
value of po was less than .01. The parameters a, h, and sz
were slightly underestimated, and drift rates were underestimated by around 5%. The standard deviations were
close to the same as when the method was applied without the correction for contaminants. This means that applicationof the corrected method to data without contaminantsproducesresultssimilartothosewhentheuncorrected
method is applied.
The addition of the correction method to maximum
likelihood allows it to produce parameter estimates that
are close to those used to generate the data, differing from
the true values by only about 20% for h and 10% or less
RATCLIFF AND TUERLINCKX
for the other parameters. Standard deviationsincreased by
about 15%. Even though the distributionassumed for fitting is not quite the same as the distribution used to generate the simulated data (a diffusion process reaction time
plus a uniformly distributed distraction time), the use of
the mixture allows the maximum likelihood method to
produce good fits, in contrast to its failure with contaminated data when no correction is used. Also, the implementation does not suffer in speed, running at about the
same speed as the program without the correction.
Chi Square
Just as for the maximum likelihood method, the chisquare fitting method is corrected with the assumption
that the data are a mixture made up of a distributionof reaction times from the diffusion process and a uniform distributionof contaminants.The rangebetweenthe maximum
and the minimum reaction times for each condition provides the range of the rectangular distribution of contaminants, and po is the probability density within that distribution of contaminants.
To apply the chi-square method to the mixture distribution, we first obtain the quantile points of the observed
reactiontime distributionsfrom the data just as for the chisquare method without corrections for contaminants.
Then we estimate the proportionof contaminantsbetween
the quantiles,and we subtract these away to leave the proportion of responses between the quantilesthat arise from
the diffusion process. Specifically, the rectangular distribution of contaminants is divided into ranges by the observed quantilereaction times (the sum of the proportions
of responses between the quantiles is po). Then the proportions of responses between the quantiles (and outside
the quantiles) assumed to come from contaminants are
subtracted from the observed proportions from the data
(which are .1, .2, .2, .2, .2, and .1). The resulting proportions represent the probabilitydensitiesbetween the quantiles that come from (and are to be fit by) the diffusion
process. (In the computer program used to fit the diffusion model, these proportions are normalized by dividing
by 1 2 po, and the diffusion model is fit as before.)
The correction method gives reasonably good recovery
of parameter values. The h parameter is overestimated by
25% or less, and drift rates are overestimated by less than
10%, as compared with the valuesused to generate the diffusion process portion of the data. The overestimation is
about as large as that obtained with the corrected maximum likelihoodmethod applied to contaminateddata and
about the same, overall, as the uncorrected chi-square
method applied to uncontaminated data. The estimated
proportion of contaminants from the fitting method is
.043 for a 5 0.08, near the correct value of .05, but as for
the maximum likelihood method, for a 5 0.16, the value
is less than the true value, about .03 instead of .05. The
standard deviations in the parameter values increase by
only about 5% over the standard deviations for the chisquare method without corrections applied to uncontaminated data. They are about 38% greater than the standard
deviations for the corrected maximum likelihood method
applied to contaminated data.
When the chi-square method is applied to data without
contaminants, the biases are about the same as those for
the chi-square method without the correction for contaminants,andthe standarddeviationsare about6% smaller.The
proportion of contaminantsestimated is less than 0.9%.
The chi-square method with the correction for contaminants has biases in recovered parameters about the same
as those for the maximum likelihoodmethodwith the correction for contaminants, but it has standard deviations,
on average, 22% higher than those for the maximum likelihood method. The chi-square method also runs 25–100
times faster.
If there are reasons to believe that a particular task has
some other known distributionof contaminants,this could
be used instead of the uniform distribution used here.
VARIABILITY IN Ter
As was mentioned above, it has never been considered
necessary to add variability in Ter to sequential sampling
models; generally, the models fit their target data well
without it. However, given the need for it in recent fits of
Means and Standard Deviations of Parameter Values Recovered From the Maximum Likelihood Method With
Corrections Applied to Data with 5% Contaminants and Variability in Ter (N = 250 per Condition)
Parameter Set
0.1136 20.0020
0.1083 20.0029
0.1051 20.0002
0.1101 20.0023
0.1035 20.0025
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
the diffusion model , we will investigate its effects here. We will explicitly model variability in Ter and will examine the effect of the additional
parameter and additionalvariabilityin data on recovery of
all the parameters of the model.
Variabilityin Ter was modeled by assuming a rectangular distribution of Ter values with range st. A rectangular
distributionwas chosen because it limits the range of values (as compared,for example,with a normal distribution).
The most important consequence of adding variability
in Ter is to increase the robustness of the fitting methods
to variability in fast responses. Because Ter has a distribution of values, probability density or cumulative probabilitiesexist for values of time less than Ter. This increases
robustness,because now st, not Ter alone,is determinedby
the shortest reactiontime (this may also help with fast outliers that are in the range of Ter 2 st/2). However, the assumption of variability in Ter also adds one parameter,
which increases the standard deviations in the other parameter estimates. Also, greater variabilityin the .1 quantile reaction times will reduce the accuracy of the location of Ter, which will increase the variability in all the
parameters.
We chose a value of st 5 200 msec (for the uniform distributionwith range st, the standard deviationis st/Ï12 5
58 msec) for our simulations,because this value was at the
high end of those we obtained when fitting experimental
Adding variability in the onset of the decision process
(Ter) actually changes the predicted quantile reaction
times by a relatively small amount, as compared with the
case in which there is no variability in Ter. For example, a
value of st 5 200 msec (as compared with st 5 0 msec) reduces the .1 quantile reaction time by only 10–40 msec,
and it reduces the .3 quantileby only 0–10 msec (dependingondrift rate).Butvariabilityin Ter producesmuchgreater
variabilityin the .1 quantilereactiontime across conditions
in simulated data sets (and hence, accommodates such
variability in fitting data with large variability in the .1
quantile reaction time across conditions).
We generated simulated data as in the other simulations, with 5% contaminants and with a range in the distribution of Ter of st 5 200 msec, using the six sets of parameter values in Table 1. We will present the results for
the maximum likelihood method with 250 observations
per condition and for the chi-square method with 1,000
observations per condition. The computations for the
maximum likelihoodmethod with 1,000 observationsper
condition would have taken several weeks to run, so we
used the results for 250 observations per condition.
We chose to use 1,000 observations per condition for
the chi-square method because with 250 observationsper
condition,the results for the chi-square method were very
poor. There were large biases away from the parameter values used to generate the diffusion process reaction times.
For example, the estimate of a was 0.094 instead of 0.08,
h was 0.19 instead of 0.08, and drift rates were 20% too
high (for the values in the first row of Table 1). In the 100
sets of fitted parameter values, there were values of h that
were 0.45 instead of 0.08. These large biases are clearly
unacceptable.The standard deviations,on the other hand,
were exactly what was expected as a function of the number of observations: The average standard deviation for
250 observations per condition was 1.96 the standard deviation for 1,000 observations per condition (i.e., scaling
as a function of the square root of N ). We attribute the biases in parameter estimates with 250 observations per
condition to excessive variability in the estimated quantiles for error reaction times when there was variability in
Ter and when there were small numbers of observations.
With 1,000 observations per condition, there were only a
few cases in which there were serious distortions in parameter estimates, because quantile reaction times were
better estimated with this number of observations.For this
reason, we present results for the chi-square fits only for
1,000 observations per condition and note that at least
1,000 observations per condition are needed when applying this version of the chi-square method.
Maximum Likelihood Method With Correction
for Contaminants and Variability in Ter
To examine the effects of slow contaminants and variability in Ter, we simulated data from the diffusion model
with a rectangular distribution for Ter with a range of
Means and Standard Deviations of Parameter Values Recovered From the Chi-Square Method With
Corrections Applied to Data with 5% Contaminants and Variability in Ter (N = 1,000 per Condition)
Parameter Set
RATCLIFF AND TUERLINCKX
200 msec and 5% contaminants from the distributionthat
was used in earlier simulations (i.e., by adding between 0
and 2,000 msec to a reaction time generated from the diffusion model). In generating simulated data, we first
added variabilityto Ter by adding a random number of between 2100 and 1100 msec, and then, on 5% of the responses, we added a random number of between 0 and
2,000 msec to produce the contaminant.
The onlymodificationto the fittingprogram was adding
the computationthat integratedover values of Ter (with the
range of integrationof 2st/2 to 1st/2). Then we fitted the
simulated data with the maximum likelihoodmethod with
the correction for contaminants as before and with the assumption of a uniform distributionof values of Ter.
Table 8 presents the means and standard deviations of
the estimated parameter values. The standard deviations
are about 36% higher than those in Table 2 for the uncorrected method without variability in Ter applied to data
without contaminants.There are biases in some of the parameter values. For example, a, the drift rates for some
conditions,and the values of the variabilityparameters (h
and sz) are usually overestimated (by up to 40% for the
conditionswith a 5 0.08). The estimate of the proportion
of contaminantsis underestimated (.025 instead of .05) at
the larger value of boundary separation a. Values of Ter
and a are estimated within 5% or 6% of the values used to
generate the simulated data. With larger values of the
number of observationsper condition,these biases would
be reduced (but fitting time for 100 data sets would approach weeks).
Chi-Square Method With Correction
for Contaminants and Variability in Ter
First, we determinedthat applicationof the uncorrected
chi-square method without variability in Ter to contaminated data with variabilityin Ter would fail. In the real data
set mentioned above, for which the chi-square method
failed, we believed that the failure was due to excessive
variability in the .1 quantile, and we wanted to check that
this was correct. As was expected, there were severe biases in parameter estimates. Boundary separation, a, was
overestimated by between 10% and 20%, Ter was underestimatedby 30–60 msec, drift rates were underestimated
by 10%–30%, h was one quarter its target value when a 5
0.08 but close to the correct values when a 5 0.16 (probably an overprediction from contaminants was canceled
out by underpredictionfrom variability in Ter). Drift rates
were underestimated by 40% for a 5 0.08 and by 5%–
20% for a 5 0.16.
We then applied the corrected chi-square method with
the correction for contaminantsand with variability in Ter
to the data sets with contaminants and with variability in
Ter. Table 9 presents the results. There are slight biases of
3–15 msec in Ter, and there are biases of about 5% of the
mean in a and about 10% of the mean in h, except when
h and a are small (the first condition). Drift rates are also
within 5% of the target values, except for the first condition. The recovered range in Ter (st) is around 180–
200 msec, which is close to the input value of 200 msec,
and the estimated proportion of contaminants is between
3% and 4%, smaller than the input value of 5%. The standard deviationsin Table 9 are about 50% larger than those
for the uncorrected chi-square method without variability
in Ter applied to data without contaminants or Ter variability for 1,000 observationsper condition.They are also
about 45% greater than standard deviations with the corrected maximum likelihoodmethod with variabilityin Ter
(scaled for 1,000 observations per condition).
These results show that estimates of parameters from
the corrected chi-square method with variabilityin Ter are
no more biased (with 1,000 observations per condition)
than those for the uncorrected method without variability
applied to uncontaminated data without Ter variability.
The standard deviations in the parameter values are increased relative to the case with neither contaminantsnor
Ter variability, but that is to be expected because both contaminants and variability in Ter reduce the quality of the
data and because the number of parameters in the model
is increased. Applying the method to data without contaminants or Ter variability recovers the parameters of the
model as well as does the method that does not have these
factors built into the fitting program.
Means and Standard Deviations of Parameter Values Recovered From the Weighted Least Squares Method
(With No Corrections) for Data With Contaminants and Variability in Ter (N = 1,000 per Condition)
Parameter Set
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
Weighted Least Squares Without
Correction Applied to Data With
Contaminants and Variability in Ter
To illustratethe robustnessof theweightedleast squares
method, Table 10 shows the results of application of the
method to the data set with contaminants and variability
in Ter with 1,000observationsper condition.The weighted
least squares method is not corrected for slow contaminants, and it does not have variability in Ter represented in
the fitting program. The results show large biases in
parameter estimates. There is underestimation (by 30–
60 msec) of Ter, overestimation of a by up to 20%, underestimation of the variance parameters h (by up to 40%)
and sz, and underestimationof the drift rates by up to 25%.
The standard deviations for the fits for the weighted
least squares method without contaminants or variability
in Ter, shown in Table 5 (with N 5 250 observations per
condition),are abouttwice (94% greater) as large as those
for the weightedleast squares method appliedto data with
contaminants and variability in Ter for N 5 1,000 observationsper condition(Table 10). Because the standard deviations scale as the square root of N, this means that the
variability is comparable for the two cases. This means
that the weightedleast squares method is finding solutions
around parameter values that are clustered just as tightly
for the cases with and without contaminantsand variability in Ter, but with large biases in the former cases.
Although we do not present a figure showing this, the
quality of the fits to data (quantile probability functions)
are good for the application of the weighted least squares
method applied to data with contaminants and variability
in Ter. There appear to be no large systematic deviations
of the theoretical fits from the data using the average parameter values from fits to the 100 simulated data sets.
Thus, the weighted least squares method provides a good
method to see whether the diffusion model can fit the data
but does not provide accurate parameter estimates if there
are contaminants or variability in Ter.
REACTION TIME DISTRIBUTIONS WITH
CONTAMINANTS AND VARIABILITY IN Ter
Earlier, we said that adding variability in Ter does not
change reaction time distributionshape very much. Here,
Figure 11. Five reaction time distributions for predictions from the diffusion
model with the various sources of variability present and absent (see the figure
RATCLIFF AND TUERLINCKX
we show how the various sources of variability and contaminants affect distribution shape. Adding variability in
Ter alters the shape of the reaction time distribution, because the leadingedge rises more slowly than it doeswithout variabilityin Ter. But the effect on the shape of the reaction time distribution is relatively small, even though it
affects the fitting methods a great deal because it allows a
lot of variability in the early quantile reaction times (e.g.,
the .1 quantile). Figure 11 shows reaction time distributions with the presence and absence of the various sources
of variability: variability in drift, starting point, Ter, and
contaminants, for one set of parameter values. The first
function has no variability in drift or starting point across
trials. The second adds variability in drift, the third adds
variability in starting point, the fourth adds variability in
the nondecision component of reaction time (Ter), and finally, the fifth adds the distribution of contaminants used
above. The key differences among these functions are the
following. In adding variability in drift across trials, accuracy decreases (dropping from .83 with h 5 0 to approximately .69 for the other four functions with h 5
0.16), and the distributionbecomes more peaked. Adding
starting point variability (sz) shifts the distributiona little
to the left, and adding variability in Ter shifts it even further to the left. Adding contaminantschanges the last distributionlittle,raising the extreme right tail a little.In each
of these latterfour cases, the tail of the distributionchanges
little, and the shape of the distribution remains about the
The changes in quantile reaction times as variability in
Ter is added to the model with the parametervalues in Figure 11can bedescribedasfollows:The .1 quantiledecreases
by 14 msec when st is 200 msec relative to the case in
which st is 0. The .3 quantile decreases by about 4 msec,
and higher quantileschange by less than 5 msec. The size
of the differences is greater at higher drift rates (e.g., the
.1 quantile is lowered by 30 msec when the drift rate is
0.4), but generally, changes in drift rate produce what
seems to be a shift in the distribution by about 10–
40 msec for the range of parameter values we have considered (those in Table 1). This means that with variability in Ter, high drift rates, and small boundary separations,
what would appear to be a shift in the position of the reaction time distribution as a function of moving from an
easier to a harder condition 
could actually be the result of variability in Ter .
The effect of adding variability in Ter is to add a modest shift in the leadingedge of the theoreticalreactiontime
distribution.However, it allows random samples of data to
have large differences in the leading edge (e.g., .1 quantile) from data sample to data sample. Adding variability
in Ter into the diffusion model allows the chi-square and
maximum likelihood methods to accommodate these
Figure 12. Examples of the graphical Monte Carlo method for 100 sets of
simulated data with 5% contaminants with parameters from the third line of
Table 1. The black dots represent the quantile reaction times from the theoretical fits from the average parameter values for the fits to each data set, and the
gray dots represent the Monte Carlo samples.
Reaction Time (msec)
Response Probability
Chi square with contaminant
correction for N = 1,000 per
condition, 5% contaminants.
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
large differencesin the leadingedge. Thisallowsthe methods to fit experimental data in cases in which they would
not fit the data without variability in Ter.
GOODNESS OF FIT: THE GRAPHICAL
MONTE CARLO METHOD
Evaluating goodness of fit for the diffusion model has
not been mentioned up to this point. This is because the
focus is on just this one model. There are no other models
that have been shown to fit all the data that are obtained
from two-choice reaction time experiments. We see four
steps that represent different points in the development
and testing of models, and in the reaction time domain,
the field is still largely at the first step. The first step is to
ask whether there is any model at all that might fit the experimental data. If we are at this point, a graphical Monte
Carlo method for displaying goodness of fit is sufficient,
as will be presented shortly. Second,do we have more than
one model that might fit the data? Then, we can use the
graphical Monte Carlo method to determine whether and
how well all the models fit the data. Third, if we have more
than one model that adequately fits the data, is it possible
to devise experiments that differentiate between the models? Do the models make differential predictions,or do the
models mimic each other? Fourth, if the models do not
mimic each other exactly, but no strong differential predictions can be obtained, standard goodness-of-fit measures can be used to distinguish among them.
When a numerical value to represent the goodnessof fit
isneeded,a chi-squarestatisticcan be used.The use ofa chisquare statistic is a standard method for assessing goodness of fit, and the chi-square fitting method would provide it as a by-product of fitting data (it is the objective
function being minimized).
The graphical Monte Carlo method offers a visual presentationof the variabilityassociatedwith predictionsfrom
a model for the sample size and conditions in the experiment. The idea is to use the best-fittingparameters to generate random samples of simulated data with the same
numberof observationsper conditionas the data. Then the
experimental data can be plotted with the simulated data,
and for 100 random samples of simulated data, the experimental data should lie within limits established by the
middle 95%.
One example of the graphical Monte Carlo method is
presented in Figure 12. We chose one of the sets of parameters with symmetric boundaries(the third set in Table 1).
From this set of parameters, 100 sets of simulated data
were generated with 1,000observationsper condition(we
chose 1,000 observationsper conditionto reduce the overlap between the quantiles;see Figure 2) and with 5% contaminants with no variability in Ter. The diffusion model
with correction for contaminants was fitted to the data,
using the chi-square method,and the data and the two sets
of fits are shown in Figure 12. This example illustrates
how well the model with correctionsfor contaminantsfits
the simulated data with contaminants.
Bothpanelsof Figure12 displaygraydots(thatasgroups
approximately form ellipses) representing the 100 sets of
simulated data. The black points are predictions from the
means oftheestimatedvaluesoftheparameters.The means
of the parameter values a, Ter, h, sz, and four drift rates
were 0.162, 0.301, 0.086, 0.027, 0.308, 0.210, 0.103, and
0.001, respectively, and the estimated probability of contaminantswas .0484.The fits fall in themiddleof therange
of simulated data points, and there appear to be no systematic deviations between the simulated data sets and
predictions. Examples of the use of this method are presented in Ratcliff et al. .
USING THE WEIGHTED LEAST
SQUARES METHOD TO EXPLORE
THE QUALITY OF THE FIT
In examining how well the diffusion model would fit
data from a recognition memory experiment , the weighted least squares fitting method
was used, and it produced quite good fits by visual inspection.As was notedearlier,when the chi-squaremethod
was used to fit the data, it failed dramatically ; for example, the .9 quantile reaction
times missed by as much as 200–400 msec. We noticed
that in the data, the .1 quantile was much more variable
than in other data sets and was especially more variable
than simulated data examined in our simulation. Because
the weighted least squares method fitted well, we looked
for factors that might have caused the chi-square method
to fail, and it was as a result of this search that we added
variability in Ter into the chi-square fitting method. The
use of the weighted least squares methodallowed us to determine that the data couldbe fit adequatelyand prompted
us to search for problems with the chi-square method. In
general, if there are problems with the chi-square method,
but the weighted least squares method fits adequately, the
chi-square method should be examined for the source of
the problems.
If the weighted least squares method does not produce
good fits, it is possible to change the weights on the various components of the sum of squares in order to experiment to see whether better fits can be obtained. For example, if the fits to the higher quantilereactiontimes (e.g.,
.7 and .9 quantiles) are poor, the .9 quantile could be
weighted more and more until the value generated from
the parameter values for the fit comes into line with the
data. Then the .1 quantilecould be weighted more, and accuracy could be weighted less, for example. This might
allow the predicted accuracy values to deviate away from
their best fits (by a few percent) and bring the reaction
time quantiles into better register. The point is that the
weighted least squares method allows more flexibility in
weighting different components of the data than do the
chi-square and the maximum likelihoodmethods.
The weighted least squares method can be seen as an
exploratory tool; the graphical quality of the fits can be
manipulated because each component (accuracy and the
RATCLIFF AND TUERLINCKX
quantiles for both correct and error reaction time quantiles) can be weighteddifferently.Althoughad hoc weighting schemes are not recommended for final presentation
of fits to data, and especially not for model comparison,
they can be used to understandwhere a model is misfitting
data and what specific aspects of the data are providing
the problems.
At this pointin the historyof testingreaction time models and evaluating their goodness of fit, we finally have
models (e.g., diffusion models, accumulator models, etc.)
that appear capable of accounting for the full range of experimentaldata. The data includetwo dependentvariables
for each experimental condition, reaction time and accuracy, as well as reaction time distributionsfor correct and
error responses. In terms of the stages of model testing
that were describedabove,we feel we are aboutat the stage
of trying to determine whether models can handle larger
and more comprehensive data sets than they have so far,
whether the models are capable of mimicking each other,
and whether we can find cases in which the models make
differential predictions .
DISCUSSION
This article has presented the first study aimed at investigating methods for fitting a sequential sampling model,
thediffusionmodel,to experimentaldata. We evaluatedfitting methods, presented examples of parameter estimates
and their standard deviations, and examined the properties of the estimators. We concludethat the fitting methods
providereasonablesolutionsfor estimatingparameters for
the diffusion model even when the data contain contaminants. The results provide measures of bias and standard
deviationsin the recovered parameter values for ranges of
parameter values that match experimental data. The standard deviations can be used as guides to the standard deviations we might expect in parameter values from fits to
group data from single experiments or from fits to data
from single subjects. The standard deviations can be used
in testing hypotheses about differences in parameter values between conditions or groups of subjects—for example, whether one group adoptsmore conservativeresponse
criteria than another. We hope that the work in this article
can serve as a prototypefor investigationsof fitting methods for other models of reaction time and accuracy and for
other models more generally.
The methodthat is usuallythe first choicefor parameter
estimation is the maximum likelihood method. It has attractivestatistical properties: The parameter estimates are
asymptotically unbiased, and the variances in the parameter estimates are the smallest possible for asymptotically
normally distributed estimators. We found in application
to the diffusion model that the maximum likelihood
method provided the smallest standard deviations in parameter estimates and the most unbiased estimates among
the methodswe studied. The methodwas very sensitiveto
contaminated data, and we were able to correct for contaminants that overlapped the reaction time distributions
for the simulateddataby explicitlyrepresentingthem in the
model.With the correction,the methodprovidedbetter parameter estimates (estimates with about the same bias but
lower variance) than did the other method with the same
correction. However, the maximum likelihood method is
very sensitive to spurious fast responses and excessive
variabilityin the fastest responses (i.e., more variablethan
those produced from the model), because it has to place
Ter below the shortest reaction time to determine its probability density, f(t). If it is possible that data contain such
variability or fast outlier responses and they cannot be
eliminated experimentally, the estimated parameters and
fits can be severely distorted. To address the issue of excessive variability in the fastest responses, we added assumptions about variability in Ter and found that the maximum likelihood method again produced better estimates
than did the other methods. However, the method is not
robust: If the assumptionsabout contaminantsor variability in Ter are not reasonably accurate, the method can fail
quite badly, even with just a few deviant data points.
The chi-square method with corrections for contaminants and variability in Ter is very fast (25–100 times
faster than the maximum likelihood method and runs in
minutes, as opposed to hours, on fast workstations). The
chi-square method has higher standard deviations in parameter estimates than the maximum likelihood method
and will often produce biased estimates of the parameter
values. This is the result of biases in the estimates of the
quantile reaction times for errors with small numbers of
observations.However, the chi-square method is robust to
a few fast or slow contaminants, much more robust than
the maximum likelihood method.
The weighted least squares method is about 100 times
slower than the chi-square method, but it is robust in the
face of contaminants and variability in Ter, more robust
than either of the other two methods. When the method is
applied to data with contaminants or with variability in
Ter, as implementedin the studies above, it produces a solution that produces predicted functions near the data.
This is very useful because it shows whether or not the
model is capable of fitting the data. But the estimated parameters are usually biased away from the parameters
used to generate the portion of the data generated by the
diffusion model (unless such corrections were introduced
into the method). The parameter estimates are aboutas biased as those for the chi-square method when applied to
data without contaminants or variability in Ter, but the
standard deviations in parameter values are larger.
Besides being more robust, the weighted least squares
method is easier to manipulate(i.e., to experimentwith) to
determinethe source of misses between the model and the
data. For example, if there are slow contaminants, the .9
quantilecan be weightedless, or if error rates in some conditions are thought to be the result of guessing, accuracy
in these conditions can be weighted less. This allows various guesses about distortions in the data to be evaluated
using the model, and this could (and did) lead to other assumptions (explicit representation of contaminants and
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
variability in Ter) being added to the chi-square and maximum likelihood methods.
It is also important to note that the three different fitting
methodsare fittingdifferentobjectivefunctions.Thismeans
that biases in parameter estimates can be due to the different summaries of the data used in fitting (e.g., biases in
quantile reaction times vs. individual reaction times). We
showed that in the case of the chi-square and weighted
least squares methods, biases in parameter estimates were
due to biases in the data summaries used in fitting the
model (quantilereaction times for error responses). When
the methods are applied to accurate predicted quantile reaction times and accuracy values, the parameters used to
produce the predictions are recovered quite accurately.
When a model is to be fit to data, it is important to consider the aim of the project.We listed four aims: (1) to find
out if a model can produce fits that are near the experimental data, (2) to competitively test between models on
a single data set, (3) to devise manipulationsthat produce
differential testable predictions from the models, and
(4) to use goodness-of-fit methods to discriminate between models that make about the same (but identifiably
different) predictions. At this point of research using the
diffusion model and other models of this class, the usual
aim is to determine whether a model can fit experimental
data, and the methods we have presented here allow this
to be done. We are at the point in this domain of research
where comprehensive projects aimed at testing between
models and examining mimicking between models are
possible. Such projects will require careful consideration
of fitting methods, and the methods presented here will
provide a good starting point for model comparison.
One important question that is often asked is the following: Can the diffusion model fit any pattern of reaction
time and accuracy data, or are there patterns the model is
incapable of fitting? The short answer is that there are
many patterns the model cannot fit but these rarely occur
in experimentaldata.Some examplesof patternsthemodel
cannot fit are the following. First, the model predicts that
the reaction time distribution shape is right skewed and
that it has about the same qualitative shape across a wide
range of drift rates and boundary separations. Data sets
from a variety of tasks all show about the same shape for
reaction time distributions,which matches the shape predicted from the model. If the shapes of empirical reaction
time distributions were considerably more symmetric or
more skewed than they are, the model would fail. Second,
if drift rate changes across conditionsin a within-subjects
designin which all otherparameters are fixed, the reaction
time distribution must skew with only a small change in
the shortest quantile,but with a large changein the longest
quantile (e.g., Figure 2). If instead the whole distribution
shifted (i.e., all quantiles slowed or speeded up equally),
the model would fail. Third, if it is assumed that a speed/
accuracy manipulation affects only the boundary separation parameter, this must be reflected in a moderate increase in the .1 quantile (e.g., 100 msec) for the accuracy
condition relative to the speed condition and a large increase in the .9 quantile (e.g., 500 msec). If the data
showed a large shift in the position of the whole distribution, the model would fail, or it would have to be assumed
that some other parameter (e.g., Ter) is affected by the manipulation. For examples of other patterns that cannot be
fit by the diffusion model, see Ratcliff .
After working with the sequential sampling class of
models for some time, we find that intuition can often be
wrong. Sometimes it seems obvious that a pattern of data
cannot be accommodated by the model, but after application of the model, it turns out that the data are fit in an unexpected way. Conversely,although a set of data might be
qualitativelyconsistent with the behavior of a model, it is
only when the model has been fit to the data that it is possible to say that the model can fit the data. This is because
the model may fail to fit quantitatively.
Limitations
In this section, we will present a discussion of the limitationson what this article has accomplished.We will discuss when the tables can be used in assessing standard
errors in parameters, what needs to be done when the experiments involve more conditions than are presented
above, what happens if the assumptions about contaminants or variabilityin Ter are not correct, and what patterns
of data can make the diffusion model fail.
1. The tables of means and standard deviations presented in this article assume that the number of observations is the same for each condition in an experiment. For
cases in which the number of observations per condition
is the same across conditionsbut the number differs from
those presentedhere, the standard deviationscan be scaled
by the square root of N. However, if numbers of observations differ substantially across conditions, new simulations will be required to compute values of standard deviations.
2. The values of the standard deviations in the tables
cannot be used when the experiments deviate a lot from
those that are mimicked in our simulations, but there are
some generalizationsthat can be made. In our simulations,
each set of fits of the model to MonteCarlo data presented
in the tables represents experiments in which four conditions are tested with one set of boundaries, one value of
Ter, and one value of the between-trial variance parameters. The four drift rates represent the experimental conditions, which might represent number of repetitions of a
stimulusin a memory experimentor several levels of stimulus intensityin a perceptual experiment. But in some experiments, subsets of parameters might be kept constant
across blocks of trials, and others might be allowed to
vary. For example,if we vary speed–accuracy instructions
between blocks of trials, only boundary settings might be
expected to change between speed and accuracy conditions, and all other parameters, including Ter and drift
rates, might be expected to be the same. For model fitting,
it is first necessary to find out what the reasonable hy-
RATCLIFF AND TUERLINCKX
pothesesare and, second,to fit the model to the data, keeping all other parameters constant. The results in the tables
for standard deviations cannot be used to provide estimates for the standard deviations for experiments like
these. However, from our experience, to a rough approximation, the standard deviationsin the tables can be scaled
by the total number of observations. So, for instance, if
there are N observationsper condition for speed trials and
N observations for accuracy trials, a rough scaling factor
(for drift rates, Ter, and variance parameters) is the square
root of 2N instead of N in the first point in this section.
To determine whether speed and accuracy trials can be
fit with the same parameter values, allowing only boundary separation to change, we recommend the following
procedure . First, perform separate fits to the data for the speed and accuracy conditions.
Make sure that the only parameter that has a large change
between speed and accuracy is boundary separation. Then
modify the fitting program to fit the model to both speed
and accuracy conditionssimultaneously, with onlyboundary separation changingbetween speed and accuracy conditions. The same process can be carried out for other manipulations, such as varying the probability of the two
responses.
3. Our choicesof assumptionsfor contaminantsand Ter
are designed to mimic what is likely to occur in real data,
but slightlydifferent assumptionsshould not affect the resultssignificantly. The uniform distributionfor Ter was selected because it is a simple distributionwith two parameters and probability density is spread across the whole
range of values. The distribution is bounded so Ter can
never be negative. The distributionof contaminants is selected as one that plausibly mimics long contaminants.
Usually, fast outliers can be brought under experimental
control by punishingsubjects with a time delay when they
produce a fast outlier (although having a few fast outliers
does not affect the chi-square fitting method). But if fast
outliers are part of what is being examined in the experiment, different assumptionscould be made about contaminants, and they could be explicitlymodeled and included
as part of the fitting program.
4. There are limitationson the qualityof data to be used
in fitting the diffusion model. Averaging over subjects or
sessionscanbe a problem,becauseperformancecanchange
from session to session and different subjectscan produce
different patterns of data. In examining data prior to
model fitting,it is necessary to determinewhether the patterns of results are different across subjects or across sessions for individualsubjects. For example, it would not be
a good idea to average data from subjectswho are fast and
accurate with a small proportion of fast errors together
with data from subjects who are slower and inaccurate
with a large proportion of slow errors . This
would lead to an average that was not representativeof either type of subject. If subjects show different patterns
from each other, their data can be combined into subgroups that show similar patterns and averaged.
Recommendations
Our recommendation for fitting the diffusion model to
data is to use the chi-square method with the corrections
for contaminants and variability. In preparing data for fitting, cutoffs for fast and slow responses should be used.
This reduces both starting point variability and the proportionof slow outliers.Also, thedata from all the subjects
should show the same patterns across experimental conditions. If the plots of the quantile probability functions
for predictionsand data miss each other, the weighted least
squaresmethodshouldbe used to see whether an adequate
fit can be obtained (keeping in mind that this will generally not produce accurate estimates of the parameter values). If the weighted least squares method produces a reasonable fit, the chi-square method may be failing because
some of the assumptionsare not correct (e.g., assumptions
about contaminants). Then an attempt should be made to
modify or add assumptions to the chi-square method to
match hypotheses about what kinds of contamination
there might be in the data.
A number of issues of general importance emerge from
our investigationof fitting methods. We have shown how,
from Monte Carlo studies, it can be determined whether
estimates are biased, how large their standard deviations
are, whether the estimates’ distributionsare normally distributed or not, and whether there are tradeoffs (correlations) among parameters. It can be determined whether
the accuracy and standard deviationsof the estimates vary
as a function of sample size and whetherdata averaging or
grouping introduces biases into the estimates. If a model
fails to fit experimental data, we have illustrated how investigations can be undertaken to determine whether the
miss is the result of contaminants or whether it might be
the result of minor misspecification of the model (e.g., in
our case, failing to include variability in Ter). If a model
fails to fit a set of experimentaldata, it is necessary to determine whether this is a failure of the fitting method because of violation of assumptions or a failure of the
model. If it were a violation of assumptions in the fitting
method, it would be necessary to understand how contaminants or misspecifications affect parameter estimation
over a range of parameter values. Then a choice could be
made aboutwhether to try to model contaminantsor eliminate them or to try to modify assumptions to deal with
misspecifications. A check on the new model or fitting
method would be to generate Monte Carlo data that have
no contaminants and no misspecifications, to make sure
that the fitting program could recover the parameter values used to generate the Monte Carlo data. It might also
be important to compare more than one fitting method for
the same data set, because as we have shown here, different fitting methods can have different properties, so that
one fails where the other succeeds.
The diffusion model and other stochastic models are currently the only
models capable of fittingthe range of correct and error re-
ESTIMATING PARAMETERS OF THE DIFFUSION MODEL
action times, response probabilities,and reactiontime distributions . At this point in
the evolution of this class of models, goodness-of-fit and
model-fittingmethods have taken a back seat to the problem of findinga model that is capable of fittingall aspects
of the experimentaldata. But as the models evolveand are
evaluated, fitting methods and goodness-of-fit measures
will become important.