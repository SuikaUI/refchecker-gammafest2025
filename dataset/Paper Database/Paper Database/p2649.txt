Matterport3D: Learning from RGB-D Data in Indoor Environments
Angel Chang1∗
Angela Dai2∗
Thomas Funkhouser1∗
Maciej Halber1∗
Matthias Nießner3∗
Manolis Savva1∗
Shuran Song1∗
Andy Zeng1∗
Yinda Zhang1∗
1Princeton University
2Stanford University
3Technical University of Munich
Access to large, diverse RGB-D datasets is critical for
training RGB-D scene understanding algorithms. However,
existing datasets still cover only a limited number of views
or a restricted scale of spaces.
In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images
of 90 building-scale scenes. Annotations are provided with
surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and
comprehensive, diverse panoramic set of views over entire
buildings enable a variety of supervised and self-supervised
computer vision tasks, including keypoint matching, view
overlap prediction, normal prediction from color, semantic
segmentation, and region classiﬁcation.
1. Introduction
Scene understanding for RGB-D images of indoor home
environments is a fundamental task for many applications
of computer vision, including personal robotics, augmented
reality, scene modeling, and perception assistance.
Although there has been impressive research progress on
this topic, a signiﬁcant limitation is the availability suitable
RGB-D datasets from which models can be trained. As with
other computer vision tasks, the performance of data-driven
models exceeds that of hand-tuned models and depends directly on the quantity and quality of training datasets. Unfortunately, current RGB-D datasets have small numbers
of images , limited scene coverage , limited viewpoints , and/or motion blurred imagery. Most are restricted to single rooms , synthetic imagery ,
and/or a relatively small number of ofﬁce environments .
No previous dataset provides high-quality RGB-D images
for a diverse set of views in interior home environments.
This paper introduces the Matterport3D dataset and investigates new research opportunities it provides for learning about indoor home environments.
The dataset comprises a set of 194,400 RGB-D images captured in 10,800
∗authors are in alphabetical order
Figure 1: The Matterport3D dataset provides visual data
covering 90 buildings, including HDR color images, depth
images, panoramic skyboxes, textured meshes, region layouts and categories, and object semantic segmentations.
panorama with a Matterport camera1 in home environments.
Unlike previous datasets, it includes both depth
and color 360◦panoramas for each viewpoint, samples
human-height viewpoints uniformly throughout the entire
environment, provides camera poses that are globally consistent and aligned with a textured surface reconstruction,
includes instance-level semantic segmentations into region
and object categories, and provides data collected from living spaces in private homes.
Though the curation of the dataset is interesting in its
own, the most compelling part of the project is the computer vision tasks enabled by it. In this paper, we investigate 5 tasks, each leveraging different properties of the
1 
 
dataset. The precise global alignment over building scale
allows training for state-of-the-art keypoint descriptors that
can robustly match keypoints from drastically varying camera views. The panoramic and comprehensive viewpoint
sampling provides a large number of loop closure instances,
allowing learning of loop closure detection through predicting view overlap. The surface normals estimated from highquality depths in diverse scenes allows training models for
normal estimation from color images that outperform previous ones. The globally consistent registration of images
to a surface mesh facilitates semantic annotation, enabling
efﬁcient 3D interfaces for object and region category annotation from which labels projected into images can train
deep networks for semantic segmentation. For each of these
tasks, we provide baseline results using variants of existing
state-of-the-art algorithms demonstrating the beneﬁts of the
Matterport3D data; we hope that Matterport3D will inspire
future work on many scene understanding tasks2.
2. Background and Related Work
Collecting and analyzing RGB-D imagery to train algorithms for scene understanding is an active area of research with great interest in computer vision, graphics, and
robotics . Existing work on curation of RGB-D datasets
has focused mostly on scans of individual objects , standalone rooms , views of a room , spaces from
academic buildings and small apartments , and small collections of rooms or public spaces . Some of these
datasets provide 3D surface reconstructions and object-level
semantic annotations . However, none have the
scale, coverage, alignment accuracy, or HDR imagery of the
dataset presented in this paper.
These previous RGB-D datasets have been used to train
models for several standard scene understanding tasks, including semantic segmentation , 3D
object detection , normal estimation
 , camera relocalization , and others
 . We add to this body of work by investigating
tasks enabled by our dataset, including learning an image
patch descriptor, predicting image overlaps, estimating normals, semantic voxel labeling, and classifying images by
region category.
The work most closely related to ours is by Armeni et al.
 . They also utilize a 3D dataset collected with Matterport
cameras for scene understanding. However, there are several important differences. First, their data is collected in
only 3 distinct ofﬁce buildings, whereas we have data from
90 distinct buildings with a variety of scene types including
homes (mostly), ofﬁces, and churches. Second, their dataset
contains only RGB images and a coarse surface mesh from
2All data and code is publicly available:
 
Figure 2: Panoramas are captured from viewpoints (green
spheres) on average 2.25m apart.
which they generate a point cloud – we additionally provide the raw depth and HDR images collected by Matterport. Third, their semantic annotations cover only 13 object
categories, half of which are structural building elements –
we collect an open set of category labels which we reduce to
40 categories with good coverage of both building elements
and objects. Finally, their algorithms focus only on tasks
related to semantic parsing of buildings into spaces and elements, while we consider a wide range of tasks enabled by
both supervised and self-supervised learning.
3. The Matterport3D Dataset
This paper introduces a new RGB-D dataset of buildingscale scenes, and describes a set of scene understanding
tasks that can be trained and tested from it. We describe
the data in this section, along with a discussion of how it
differs from prior work.
3.1. Data Acquisition Process
The Matterport data acquisition process uses a tripodmounted camera rig with three color and three depth cameras pointing slightly up, horizontal, and slightly down. For
each panorama, it rotates around the direction of gravity to
6 distinct orientations, stopping at each to acquire an HDR
photo from each of the 3 RGB cameras. The 3 depth cameras acquire data continuously as the rig rotates, which is
integrated to synthesize a 1280x1024 depth image aligned
with each color image. The result for each panorama is 18
RGB-D images with nearly coincident centers of projection
at approximately the height of a human observer.
For each environment in the dataset, an operator captures
a set of panoramas uniformly spaced at approximately 2.5m
throughout the entire walkable ﬂoor plan of the environment
(Figure 2). The user tags windows and mirrors with an iPad
app and uploads the data to Matterport. Matterport then processes the raw data by: 1) stitching the images within each
panorama into a “skybox” suitable for panoramic viewing,
2) estimating the 6 DoF pose for each image with global
Figure 3: Annotator-speciﬁed ﬂoor plans. Floor plans are
used to deﬁne regions for object-level semantic annotation.
Left: ﬂoor plan with textured mesh. Right: ﬂoor plan alone
(colored by region category).
bundle adjustment, and 3) reconstructing a single textured
mesh containing all visible surfaces of the environment.
The result of this process for each scene is a set of RGB-
D images at 1280x1024 (with color in HDR) with a 6 DoF
camera pose estimate for each, plus a skybox for each group
of 18 images in the same panorama, and a textured mesh
for the entire scene. In all, the dataset includes 90 buildings containing a total of 194,400 RGB-D images, 10,800
panorama, and 24,727,520 textured triangles; we provide
textured mesh reconstructions obtained with and .
3.2. Semantic Annotation
We collect instance-level semantic annotations in 3D by
ﬁrst creating a ﬂoor plan annotation for each house, extracting room-like regions from the ﬂoor plan, and then using a crowdsourced painting interface to annotate object instances within each region.
The ﬁrst step of our semantic annotation process is to
break down each building into region components by specifying the 3D spatial extent and semantic category label for
each room-like region. Annotators use a simple interactive
tool in which the annotator selects a category and draws a
2D polygon on the ﬂoor for each region (see Figure 3). The
tool then snaps the polygon to ﬁt planar surfaces (walls and
ﬂoor) and extrudes it to ﬁt the ceiling.
The second step is to label 3D surfaces on objects in each
region. To do that, we extract a mesh for each region using
screened Poisson surface reconstruction . Then, we use
the ScanNet crowd-sourcing interface by Dai et al. to
“paint” triangles to segment and name all object instances
within each region. We ﬁrst collect an initial set of labels on
Amazon Mechanical Turk (AMT), which we complete, ﬁx,
and verify by ten expert annotators. We ensure high-quality
label standards, as well as high annotation coverage.
The 3D segmentations contain a total of 50,811 object
instance annotations. Since AMT workers are allowed to
provide freeform text labels, there were 1,659 unique text
labels, which we then post-processed to establish a canonical set of 40 object categories mapped to WordNet synsets.
Figure 5 shows the distribution of objects by semantic category and Figure 4 shows some examples illustrated as colored meshes.
Figure 4: Instance-level semantic annotations.
rooms annotated with semantic categories for all object instances. Left: 3D room mesh. Middle: object instance labels. Right: object category labels.
3.3. Properties of the Dataset
In comparison to previous datasets, Matterport3D has
unique properties that open up new research opportunities:
RGB-D Panoramas. Previous panorama datasets have provided either no depths at all or approximate depths synthesized from meshes . Matterport3D contains aligned
1280x1024 color and depth images for 18 viewpoints covering approximately 3.75sr (the entire sphere except the north
and south poles), along with “skybox” images reconstructed
for outward looking views aligned with sides of a cube
centered at the panorama center. These RGB-D panorama
provide new opportunities for recognizing scene categories,
estimating region layout, learning contextual relationships,
and more (see Section 4.4).
Precise Global Alignment. Previous RGB-D datasets have
provided limited data about global alignment of camera
poses. Some datasets targeted at SLAM applications 
provide tracked camera poses covering parts of rooms 
or estimated camera poses for individual rooms , and Armeni et al. provides globally-registered camera poses
for 6 ﬂoors of 3 buildings. Ours provides global registered
imagery covering all ﬂoors of 90 reconstructed buildings.
Although we do not have ground-truth camera poses for the
dataset and so cannot measure errors objectively, we subjectively estimate that the average registration error between
corresponding surface points is
1cm or less (see Figure
6). There are some surface misalignments as large as 10cm
or more, but they are rare and usually for pairs of images
whose viewpoints are separated by several meters.
Comprehensive Viewpoint Sampling. Previous datasets
have contained either a small set of images captured for
views around “photograph viewpoints” or a sequence
of video images aimed at up-close scanning of surfaces .
appliances
gym equip.
Figure 5: Semantic annotation statistics. Total number of semantic annotations for the top object categories.
Figure 6: Visualizations of point clouds (left-to-right: color,
diffuse shading, and normals). These images show pixels
from all RGB-D images back-projected into world space
according to the provided camera poses. Please note the
accuracy of the global alignment (no ghosting) and the relatively low noise in surface normals, even without advanced
depth-fusion techniques.
Ours contains panoramic images captured from a comprehensive, sparse sampling of viewpoint space. Panoramic
images are spaced nearly uniformly with separations of
2.25m ± 0.57m, and thus most plausible human viewpoints
are within 1.13m of a panorama center. This comprehensive
sampling of viewpoint space provides new opportunities for
learning about scenes as seen from arbitrary viewpoints that
may be encountered by robots or wearable sensors as they
navigate through them (see Section 4.2).
Stationary Cameras. Most RGB-D image datasets have
been captured mostly with hand-held video cameras and
thus suffer from motion blur and other artifacts typical of
real-time scanning; e.g., pose errors, color-to-depth misalignments, and often contain largely incomplete scenes
with limited coverage. Our dataset contains high dynamic
range (HDR) images acquired in static scenes from stationary cameras mounted on a tripod, and thus has no motion
This property provides new opportunities to study
ﬁne-scale features of imagery in scenes, for example to train
very precise keypoint or boundary detectors.
Figure 7: Visualization of the set of images visible to a selected surface point (shown as red visibility lines). (Please
note that the mesh is highly decimated in this image for convenience of visualization)
Multiple, Diverse Views of Each Surface. Previous RGB-
D datasets have provided a limited range of views for each
surface patch.
Most have expressly attempted to cover
each surface patch once, either to improve the efﬁciency
of scene reconstruction or to reduce bias in scene understanding datasets. Ours provides multiple views of surface
patches from a wide variety of angles and distances (see
Figure 7). Each surface patch is observed by 11 cameras on
average (see Figure 8). The overall range of depths for all
pixels has mean 2.125m and standard deviation 1.4356m,
and the range of angles has mean 42.584◦and standard deviation 15.546◦. This multiplicity and diversity of views enables opportunities for learning to predict view-dependent
surface properties, such as material reﬂectance , and
for learning to factor out view-dependence when learning
view-independent representations, such as patch descriptors
 and normals (see Section 4.3).
Entire Buildings. Previous RGB-D datasets have provided
data for single rooms or small sets of adjacent rooms , or single ﬂoors of a building . Ours provides data
for 90 entire buildings. On average, each scanned building
has 2.61 ﬂoors, covers 2437.761m2 of surface area, and has
517.34m2 of ﬂoorspace. Providing scans of homes in their
entirety enables opportunities for learning about long-range
context, which is critical for holistic scene understanding
and autonomous navigation.
Personal Living Spaces. Previous RGB-D datasets are often limited to academic buildings . Ours contains imagery acquired from private homes (with permissions to dis-
Figure 8: Histogram showing how many images observe
each surface vertex. The mode is 7 and the average is 11.
tribute them for academic research). Data of this type is
difﬁcult to capture and distribute due to privacy concerns,
and thus it is very valuable for learning about the types of
the personal living spaces targeted by most virtual reality,
elderly assistance, home robotics, and other consumer-level
scene understanding applications.
Scale. We believe that Matterport3D is the largest RGB-
D dataset available.
The BuldingParser dataset provides data for 270 rooms spanning 6,020m2 of ﬂoor space.
ScanNet , provides images covering 78,595m2 of surface area spanning 34,453m2 of ﬂoor space in 707 distinct rooms. Our dataset covers 219,399m2 of surface area
in 2056 rooms with 46,561m2 of ﬂoor space. This scale
provides new opportunities for training data-hungry algorithms.
4. Learning from the Data
The following subsections describe several tasks leveraging these unique properties of the Matterport3D dataset
to provide new ways to learn representations of scenes. For
all experiments, we have split the dataset into 61 scenes for
training, 11 for validation, and 18 for testing (see the supplemental materials for details).
4.1. Keypoint Matching
Matching keypoints to establish correspondences between image data is an important task for many applications
including mapping, pose estimation, recognition, and tracking. With the recent success of neural networks, several
works have begun to explore the use of deep learning techniques for training state-of-the-art keypoint descriptors that
can facilitate robust matching between keypoints and their
local image features . To enable training these
deep descriptors, prior works leverage the vast amounts of
correspondences found in existing RGB-D reconstruction
datasets .
With the precise global alignment of RGB-D data and
comprehensive view sampling, our Matterport3D dataset
Figure 9: Example training correspondences (left) and image patches (right) extracted from Matterport3D. Triplets
of matching patches (ﬁrst and second columns) and nonmatching patches (third column) are used to train our deep
local keypoint descriptor.
provides the unique opportunity to retrieve high quality, wide-baselined correspondences between image frames
(see Figure 9). We demonstrate that by pretraining deep
local descriptors over these correspondences, we can learn
useful features to enable training even stronger descriptors.
More speciﬁcally, we train a convolutional neural network
(ResNet-50 ) to map an input image patch to a 512 dimensional descriptor. Similar to state of the art by , we
train the ConvNet in a triplet Siamese fashion, where each
training example contains two matching image patches and
one non-matching image patch. Matches are extracted from
SIFT keypoint locations which project to within 0.02m of
each other in world space and have world normals within
100◦. To supervise the triplet model, we train with an L2
hinge embedding loss.
For evaluation, we train on correspondences from 61
Matterport3D scenes and 17 SUN3D scenes, and test on
ground truth correspondences from 8 held out SUN3D
scenes. The SUN3D ground truth correspondences and registrations are obtained from , using the training and testing scenes split from . As in , we measure keypoint
matching performance with the false-positive rate (error)
at 95% recall, the lower the better. We train three models - one trained on Matterport3D data only, one trained on
SUN3D data only, and another pretrained on Matterport3D
and ﬁne-tuned on SUN3D. Overall, we show that pretraining on Matterport3D yields a descriptor that achieves better
keypoint matching performance on a SUN3D benchmark.
4.2. View Overlap Prediction
Identifying previously visited scenes is a fundamental step for many reconstruction pipelines – i.e., to detect
loop closures. While previous RGB-D video datasets may
only have few instances of loop closures, the Matterport3D
dataset has a large number of view overlaps between image frames due to the panoramic nature and comprehensive
viewpoint sampling of the capturing process. This large
ResNet-50 w/ Matterport3D
ResNet-50 w/ SUN3D
ResNet-50 w/ Matterport3D + SUN3D 9.2%
Table 1: Keypoint matching results. Error (%) at 95% recall on ground truth correspondences from the SUN3D testing scenes. We see an improvement in performance from
pretraining on Matterport3D.
Figure 10: Example overlap views from SUN3D and Matterport3D ranked by their overlap ratio.
In contrast to
RGB-D video datasets captured with hand-held devices like
SUN3D, Matterport3D provides a larger variety of camera
view points and wide baseline correspondences, which enables training a stronger model for view overlap prediction
under such challenging cases.
number of loop closures provides an opportunity to train
a deep model to recognize loop closures, which can be incorporated in future SLAM reconstruction pipelines.
In this work, we formalize loop closure detection as
an image retrieval task.
Given a query image, the goal
is to ﬁnd other images with “as much overlap in surface visibility as possible.”
We quantify that notion as
a real-numbered value modeled after intersection over
union (IOU): overlap(A, B) = min( ˆA, ˆB)/(|A| + |B| −
min( ˆA, ˆB)) where A and B are images, |A| is the number
of pixels with valid depths in A, ˆA is the number of pixels of
image A whose projection into world space lie within 5cm
of any pixel of B.
We train a convolutional neural network (ResNet-50
 ) to map each frame to features, where a closer L2 distance between two features indicates a higher overlap. Similar to keypoint matching, we train this model in a triplet
Siamese fashion, using the distance ratio loss from .
However, unlike the keypoint matching task, where there is
a clear deﬁnition of ”match” and ”non-match,” the overlap
function can be any value ranging from 0 to 1. Therefore we
add a regression loss on top of the triplet loss that directly
regresses the overlap measurement between the ”matching”
image pairs (overlap ratio greater than 0.1).
Table 2 shows an evaluation of this network trained on
the Matterport3D training set and then tested on both the
triplet + regression
Matterport3D
Matterport3D + SUN3D
Matterport3D
Matterport3D
View overlap prediction results.
Results on
SUN3D and Matterport3D dataset measured by normalized
discounted cumulative gain. From the comparison we can
clearly see the performance improvement from training data
with Matterport3D and from adding the extra overlap regression loss. We also note that overlap prediction is much
harder in the Matterport3D dataset due to the wide baselines
between camera poses.
Matterport3D test set and the SUN3D dataset .
each test, we generate a retrieval list sorted by predicted
distance and evaluate it by computing the normalized discounted cumulative gain between predicted list and the best
list from ground truth. To mimic real reconstruction scenarios, we only consider candidate image pairs that have
travel distance greater than 0.5m apart. The experimental
results show that training on the Matterport3D dataset helps
ﬁnd loop closures when testing on SUN3D, and that the extra supervision of overlap ratio regression helps to improve
performance on both test sets. We can also notice that overlap prediction is much harder in our Matterport3D dataset
due to the wide baseline between camera poses, which is
very different from data captured with hand held devices
like SUN3D (Figure 10).
4.3. Surface Normal Estimation
Estimating surface normals is a core task in scene reconstruction and scene understanding. Given a color image,
the task is to estimate the surface normal direction for each
pixel. Networks have been trained to perform that task using RGB-D datasets in the past . However,
the depths acquired from commodity RGB-D cameras are
generally very noisy, and thus provide poor training data.
In contrast, the Matterport camera acquires depth continuously as it rotates for each panorama and synthesizes all the
data into depth images aligned with their color counterparts
which produces normals with less noise.
In this section, we consider whether the normals in the
Matterport3D dataset can be used to train better models for
normal prediction on other datasets. For our study, we use
the model proposed in Zhang et al. , which achieves the
state of the art performance on the NYUv2 dataset. The
model is a fully convolutional neural network consisting of
an encoder, which shares the same architecture as VGG-16
from the beginning till the ﬁrst fully connected layer, and a
purely symmetric decoder. The network also contains shortcut link to copy the high resolution feature from the encoder
Figure 11: Examples of surface normal estimation. We show results of images from NYUv2 testing set. The results from
the model ﬁne-tuned on Matterport3D (SUNCG-MP) shows the best quality visually, as it starts to capture small details
while still produces smooth planar area. The model further ﬁne-tuned on NYUv2 (SUNCG-MP-NYU) achieves the best
quantitatively performance but tends to produce comparatively noisy results.
Train Set 1
Train Set 2
Train Set 3
Median(◦)↓
Table 3: Surface normal estimation results. Impact of
training with Matterport3D (MP) on performance in the
NYUv2 dataset. The columns show the mean and median
angular error on a per pixel level, as well as the percentage
of pixels with error less than 11.25◦, 22.5◦, and 30◦.
Median(◦)↓
Table 4: Surface normal estimation cross dataset validation. We investigate the inﬂuence of training and testing the
model using permutation of datasets. Notice how the Matterport3D dataset is able to perform well on NYUv2, while
the converse is not true.
to the decoder to bring in details, and forces the up pooling
to use the same sampling mask from the corresponding max
pooling layer.
Zhang et al. demonstrate that by pretraining on
a huge repository of high-quality synthetic data rendered
from SUNCG and then ﬁne-tuning on NYUv2, the
network can achieve signiﬁcantly better performance than
directly training on NYUv2. They also point out that the
noisy ground truth on NYUv2 provides inaccurate supervision during the training, yielding results which tend to be
blurry. With an absence of real-world high-quality depths,
their model focuses solely on the improvement from pretraining on synthetic scenes and ﬁne-tuning on real scenes.
We use Matterport3D data as a large-scale real dataset
with high-quality surface normal maps for pretraining, and
train the model with a variety of training strategies. For the
Matterport3D data, we use only the horizontal and downward looking views as they are closer to canonical views a
human observer would choose to look at a scene. Table 3
shows the performance of surface normal estimation. As
can be seen, the model pretrained using both the synthetic
data and Matterport3D data (the last row) outperforms the
one using only the synthetic data (the 2nd row) and achieves
best performance.
We show the cross dataset accuracy in Table 4. We train
models by ﬁrst pretraining on synthetic data and then ﬁnetuning on each dataset; i.e., NYUv2 and Matterport3D, respectively. We evaluate two models on the test set of each
dataset. The model trained on each dataset provides the
best performance when testing on the same dataset. However, the NYUv2 model performs poorly when testing on
Matterport3D, while the Matterport3D model still performs
reasonably well on NYUv2. This demonstrates that model
trained on Matterport3D data generalizes much better, with
its higher quality of depth data and diversity of viewpoints.
Figure 11 shows results on NYUv2 dataset. Compared
to the model only trained on the synthetic data (SUNCG) or
NYUv2 (SUNCG-NYU), the model ﬁne-tuned on Matterport3D shows the best visual quality, as it captures more detail on small objects, such as the paper tower and ﬁre alarm
on the wall, while still producing smooth planar regions.
This improvement on surface normal estimation demonstrates the importance of having high quality depth. The
model further ﬁne-tuned on NYUv2 (SUNCG-MP-NYU)
achieves the best quantitatively performance, but tends to
produce comparatively noisy results since the model is
“contaminated” by the noisy ground truth from NYUv2.
4.4. Region-Type Classiﬁcation
Scene categorization is often considered as the ﬁrst step
for high-level scene understanding and reasoning.
the proposed dataset, which contains a large variety of indoor environments, we focus our problem on indoor region
(room) classiﬁcation – given an image, classify the image
based on the semantic category of the region that contains
its viewpoint (e.g., the camera is in a bedroom, or the camera is in a hallway).
Unlike the semantic voxel labeling problem, region-level
classiﬁcation requires understanding global context that often goes beyond single view observations. While most of
the scene categorization datasets focus on single
view scene classiﬁcation, this dataset provides a unique opportunity to study the relationship between image ﬁeld of
view and scene classiﬁcation performance.
As ground truth for this task, we use the 3D region annotations provided by people as described in Section 3.2.
We choose the 12 most common categories in the dataset
for this experiment. We assign the category label for each
panorama or single image according to the label provided
for the region containing it. We then train a convolutional
neural network (ResNet-50 ) to classify each input image to predict the region type.
Table 5 shows the classiﬁcation accuracy (number of true
positives over the total number of instances per region type).
By comparing the accuracy between [single] and [pano], we
can see an improvement in performance from increased image ﬁeld of view for most region types. The lower performance in lounge and family room is due to confusion with
other adjacent regions (e.g. they are often confused with adjacent hallways and kitchens, which are more visible with
wider ﬁelds of view).
4.5. Semantic Voxel Labeling
Semantic voxel labeling – i.e., predicting a semantic object label for each voxel – is a fundamental task for semantic
scene understanding; it is the analog of image segmentation in 3D space. We follow the description of the semantic
voxel labeling task as introduced in ScanNet .
For training data generation, we ﬁrst voxelize the train-
Figure 12: Semantic voxel labeling results on our Matterport3D test scenes.
ing scenes into a dense voxel grid of 2cm3 voxels, where
each voxel is associated with its occupancy and class label,
using the object class annotations. We then randomly extract subvolumes from the scene of size 1.5m × 1.5m × 3m
(31 × 31 × 62 voxels). Subvolumes are rejected if < 2% of
the voxels are occupied or < 70% of these occupied voxels
have valid annotations. Each subvolume is up-aligned, and
augmented with 8 rotations.
We use 20 object class labels, and a network following
the architecture of ScanNet , and training with 52,355
subvolume samples (418,840 augmented samples). Table 6
shows classiﬁcation accuracy for our semantic voxel labeling on Matterport3D test scenes, with several visual results
show in Figure 12.
5. Conclusion
We introduce Matterport3D, a large RGB-D dataset of 90
building-scale scenes. We provide instance-level semantic
segmentations on the full 3D reconstruction of each building. In combination with the unique data characteristics of
diverse, panoramic RGB-D views, precise global alignment
over a building scale, and comprehensive semantic context
over a variety of indoor living spaces, Matterport3D enables
myriad computer vision tasks. We demonstrate that Matterport3D data can be used to achieve state of the art performance on several scene understanding tasks and release the
dataset for research use.
6. Acknowledgements
The Matterport3D dataset is captured and gifted by Matterport for use by the academic community. We would like
to thank Matt Bell, Craig Reynolds, and Kyle Simek for
their help in accessing and processing the data, as well as
the Matterport photographers who agreed to have their data
be a part of this dataset. Development of tools for processing the data were supported by Google Tango, Intel, Facebook, NSF (IIS-1251217 and VEC 1539014/1539099), and
a Stanford Graduate fellowship.
familyroom
dining room
living room
Table 5: Region-type classiﬁcation results. Each entry lists the prediction accuracy (percentage correct). By comparing the
accuracy between [single] and [pano] we can see an improvement from increased image ﬁeld of view for most regiontypes.
However, the lower performance on lounge and family room may be caused by confusion from seeing multiple rooms in one
% of Test Scenes
Table 6: Semantic voxel label prediction accuracy on our
Matterport3D test scenes.