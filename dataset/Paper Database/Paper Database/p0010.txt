HAL Id: inria-00548513
 
Submitted on 20 Dec 2010
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Hierarchical Part-Based Visual Object Categorization
Guillaume Bouchard, Bill Triggs
To cite this version:
Guillaume Bouchard, Bill Triggs. Hierarchical Part-Based Visual Object Categorization. IEEE Conference on Computer Vision & Pattern Recognition (CPRV ’05), Jun 2005, San Diego, United States.
pp.710–715, ￿10.1109/CVPR.2005.174￿. ￿inria-00548513￿
Hierarchical Part-Based Visual Object Categorization
Guillaume Bouchard
Xerox Research Center Europe
6, chemin de Maupertuis
38240 Meylan, France
 
Bill Triggs
LEAR, GRAVIR-INRIA
655 av. de l’Europe
38330 Montbonnot, France
 
We propose a generative model that codes the geometry and appearance of generic visual object categories as
a loose hierarchy of parts, with probabilistic spatial relations linking parts to subparts, soft assignment of subparts
to parts, and scale invariant keypoint based local features at
the lowest level of the hierarchy. The method is designed to
efﬁciently handle categories containing hundreds of redundant local features, such as those returned by current keypoint detectors. This robustness allows it to outperform constellation style models, despite their stronger spatial models.
The model is initialized by robust bottom-up voting
over location-scale pyramids, and optimized by Expectation-
Maximization. Training is rapid, and objects do not need to
be marked in the training images. Experiments on several
popular datasets show the method’s ability to capture complex natural object classes.
1. Introduction
In object categorization from digital images, existing geometrical models are typically very speciﬁc to a particular
object class (for example 3D human body models). There is
a need for generic models that are suitable for more general
object categories. “Part” or “fragment” based models that
combine local image features or regions into loose geometric
assemblies offer one possible solution to this .
Constellation models provide a probabilistic way to
mix the appearance and location of local descriptors. One
of their major limitations is the fact that they require an
explicit enumeration over possible matchings of model features to image ones. This optimal, but combinatorially expensive, step limits the model to relatively few detected fea-
 
when this work was performed. We gratefully acknowledge the support of
the European Union research projects LAVA and PASCAL.
tures (‘parts’), typically 6 or at most 7. This often means that
a good deal of the available image information must be ignored, especially in cases where the objects have many parts,
either naturally, or because ﬁne grained local visual features
are being used to characterize them. Indeed, such structural
approaches often fail to compete with geometry-free “bag of
features” style approaches because the latter make better use
of the available image information . Hence, it is useful to investigate variants of structural approaches that can
efﬁciently handle models with hundreds of local features.
Secondly, many natural object categories (humans and
animals, man made classes with variable forms) have relatively rigid shape locally, but signiﬁcant shape variability
at larger scales, so that nearby object features have strongly
correlated positions while more distant ones are much more
weakly correlated. This suggests a local parts based representation focused on modelling correlations between feature positions. In fact different parts (groups of features that
move together in 3D) often overlap in the image owing to occlusion and parallax effects, so common movement is a more
important cue than common (image) position for deﬁning
parts. A correlation-based model easily represents this kind
of structure. Also, to do these things well, it is natural to include some levels of part hierarchy, with loosely connected
parts containing more tightly connected subparts. Hence the
overall model becomes a tree-structured graphical model .
The current paper proposes a hierarchical model of this
sort that is capable of handling hundreds of local feature
classes efﬁciently, so that it is suitable for use with very basic
feature detectors. The position of the object in the training
images and the model structure are unknown and treated as
hidden variables, to be estimated using E-M after a suitable
initialization. The method is totally scale-invariant, and all
of the model parameters are learned by maximum likelihood,
so the only tuning needed is the number of parts at each level.
Cross-validation shows that using multi-part models is often
advantageous.
Below, we ﬁrst present the probabilistic model, then explain the learning method, including the initialization and
position/scale displacement
local feature classes
image features
nearest neighbour matching
soft parent assignments
position/scale displacement
Figure 1. The overall structure of our hierarchical object model.
EM steps. Finally experiments on real images show that the
model is effective for object categorization.
2. Model Structure
Our model (see ﬁgure 1) is a hierarchy of parts and
subparts, with the object at the top level, and positionappearance classes of local image features that will be attached to observed image features at the bottom. In each
layer of the hierarchy, the parts are softly assigned to parents from the preceding layer. Soft assignment is included
mainly to help the model structure adapt to the object class
during training. Once the models have been learned, most of
the parts tend to have relatively certain parent assignments.
Spatial structure: Parts and their sub-trees are attached to
their parents by uncertain spatial transformations. In the experiments below, we have used translations and rescalings,
i.e. transformations of the form Tqp =
where s is
the relative scale and (u, v) is the relative translation of part
p relative to its parent q from the previous layer1. We assume
that Tqp is sampled from a Normal distribution over translations and a log-Normal distribution over relative scales. We
write the corresponding mean and variance symbolically as
Tqp and Var (Tqp). These are model parameters that need
to be learned. Formally, Tqp is a non-random transformation and Var (Tqp) can be thought of as a 3 × 3 covariance
matrix for (u, v, log s), which is assumed to be diagonal below.
There is a minor complexity relating to the fact that we
use soft parent assignments. We do not introduce separate
model parameters for the transformation of each part relative to each of its possible parents. This would be a huge
1Tqp is the transformation taking point coordinates in the frame of p to
point coordinates in the frame of q, e.g. a point at the origin of p with scale
1 has scale s and position (u, v) with respect to q.
number of parameters, and those with low parenting probabilities would not be estimated stably. Instead, we expect
parts to represent stable, identiﬁable regions of the object
with their own identities and positions. Parent attributions
are uncertain only because it is unclear before training which
parent best codes the target part’s overall position variability,
i.e. parts are essentially assigned to the parent whose spatial position variations best explain (covary most strongly
over the training set with) their own. To capture this notion, each part p is given just one set of mean transformation parameters Tp, representing the mean position of p
relative the root of the object frame, and a corresponding
set of (reduced2) variance parameters Var (Tp). Given a
parent attribution q for p, the uncertain transformation Tqp
is then sampled with mean Tqp ≡T
q Tp and the correspondingly back-transformed variance, which we can denote by T
q (Var (Tp)) say. (In our case, this is just the
3 × 3 (u, v, log s) covariance Var (Tp) with its (u, v) block
scaled by 1/s2
q). In this way, the same few parameters control the part’s position, whatever its parent assignment. If
we suppose that the (random) parent locations Tq are already known, the part location relative to the object frame is
a mixture of random transformations Tq Tqp, where Tqp is
a random transformation (Gaussian in (u, v, log s)) and the
mixture weights are τ p(q), the model parameters representing the prior probabilities of p’s parent being q:
p (Tp | {Tq}) =
τ p(q) N(TqT
p | Tqp, Var (Tqp))
This mixture has the peculiarity that if all of the possible
parents q are in their mean positions Tq, all of its components coincide exactly — it becomes multimodal only when
several parents have nonzero mixing proportions τ p(q) and
deviate from their means.
Image correspondence: The lowest level of the spatial
hierarchy contains elementary parts representing appearance classes of scale-invariant local features, similar to
those used in other constellation and bag of features models
 . When the model is in use, each elementary part acts as a “bonding site” for a nearby image feature
of similar appearance. Image features are characterized by
their locations (positions and scales) and their appearance
vectors a. In the experiments below they are SIFT descriptors calculated over scale-invariant Harris keypoints ,
but any other feature / descriptor combination could be used.
Each elementary part p has the usual location model Tp, and
also a corresponding feature appearance model — here, a
Gaussian with model parameters ap and Var (ap). When an
image feature is bound to an elementary part, the part’s location is instantiated to the feature’s location and scale, and its
2This is not the variance of the full uncertain transformation Tp, just
the part of this variance that is introduced at the level of part p.
appearance is instantiated to the feature’s appearance. The
model is designed to support large numbers (hundreds) of
elementary parts, only some of which are seen in any given
image. So it is important to allow parts to remain effectively
unassigned. In practice we nominally assign every part to
some feature, but we use a robust assignment probability that
effectively discounts any overly distant assignments:
pp(a, T) = (1 −πp) pbkgd(a, T) + πp papp
Here: a, T are the appearance and location of the assigned
feature f ; πp is a learned inlier proportion for elementary
part / feature class p; pbkgd is a background model, uniform
in appearance and position; papp
is p’s appearance model,
a Gaussian with mean ap and variance Var (ap); and ploc
is the above mentioned spatial mixture over p’s location,
parametrized by Tp, Var (Tp), the corresponding parameters of all p’s parents, grandparents, etc., and the corresponding mixing proportions τ p(q) for all parents q, etc.
When the model is in use, each elementary model part
is bound to the single observed image feature that is most
probable according to the above likelihood model given the
current model parameters. One could also use soft assignments to several nearby image features. This would be more
consistent with our overall philosophy, but at present we do
not do it, mainly because it would make part-feature matching much less efﬁcient.
During testing, we do nothing to prevent several elementary parts from binding to the same image feature. This is
again for efﬁciency reasons — otherwise a combinatorial
matching process would be needed to ﬁnd the best set of correspondences. However during model training we enforce
unique assignments by greedy matching, as otherwise the
learned appearance classes of nearby parts tend to merge.
We effectively ignore any unbound features (sometimes
even the majority of the features detected). This prevents
problems when there are multiple detections of essentially
the same feature, but it also means that the current model
has no efﬁcient means of representing textured regions. We
are currently investigating the use of a Poisson ﬁeld binding model, where elementary parts can bind to many similar
features at once. This also suggests that the model may have
problems with hallucinations in very cluttered regions where
many types of features occur.
3. Training
The model is ﬁtted to a given image, and also trained
over the full training set, using Expectation-Maximization.
The model parameters to be adjusted during training are: for
each part, the mean and variance of its location and scale,
Tp, Var (Tp), and its vector of parent assignment probabilities τ p; and for each elementary part, the mean and variance of its feature appearance ap, Var (ap), and its probability of occurrence πp. In addition, in each image there
are continuous hidden variables to be estimated for the part
locations Tp; and discrete ones for the elementary part-tofeature bindings, the background / foreground decision for
each bound feature, and the parent assignment for each part.
The E-M processes are straightforward to implement.
Once initialized, the method converges in about 5–10 iterations. Training takes around 1 second per image in MAT-
LAB, most of this time being spent in the (currently unoptimized) part-to-feature assignment process. Note that every
parameter of the model is learned using E-M: apart from
the number of parts in each layer and some thresholds used
during initialization, the model has no hand-set parameters,
regularization terms, etc.
3.1. Instantiating the Model in an Image
The effective cost function has many local minima so a
robust instantiation method is needed. We use a hierarchical, heuristic, Hough transform like voting method, based
on voting into a position/scale pyramid of possible locations
(Tp values) for each part.
1. For each part q in the penultimate layer of the hierarchy (the direct parents of the elementary parts), each
image feature f (with appearance af and location Tf)
votes into a position/scale pyramid for q’s location Tq,
essentially by taking the expected mixture distribution
over feature appearances and locations generated by q’s
elementary subparts, and using it backwards as a likelihood for voting:
Voteq(Tq) =
p (Tf | Tq)
where wp ≡P
(af). Here the sum is over features f, and the maximum is over the elementary parts
p whose best parent is q. For speed, the vote uses just
the best elementary part attribution p for f. Note that
we re-weight each elementary part’s vote by the estimated number of image features assigned to its appearance class, wp = P
(af). This heuristic helps
to suppress common background features and enhance
rarer object ones.
2. We work up the spatial tree, combining votes for subpart locations into votes for their parent’s locations using the mean location offsets learned for the model:
Voteq(Tq) = S(
log(1 + Votep(Tq Tqp))
Here, the sum is over subparts p for which q is the most
probable parent (arg maxq′ τ p(q′) = q). Again we use
hard assignments for speed. The log(1 + . . .) nonlinearity makes it harder for high peaks in outlier subparts
to dominate the valid contributions of the other subparts. S is a heuristic smoothing function, currently a
Gaussian convolution. To be more rigorous, we should
smooth by using Tp = Tq Tqp as the argument and
integrating over samples from the uncertain transform
3. Maxima in the voting pyramid for the top-level part 0
give potential object placements T0.
4. For the best (or if necessary, for each) maximum, work
back down the tree assigning part positions. If the part’s
voting pyramid has a good maximum near the resulting
expected part position, use this value. Otherwise, assume that the part was not seen and use its default offset
Although it can not replace a fully-ﬂedged object detector,
this heuristic procedure gives reasonably reliable automatic
model initialization results on test sets such as the Caltech
ones, even when the object has unknown position and scale
and is surrounded by a moderate amount of background clutter. E-M is run after it, so despite the heuristics the resulting
instantiation is at least locally optimal for the full cost function.
3.2. Training — Model Initialization
The above procedure assumes a fully trained model. We
also automatically initialize the entire training process, so
that no manual location or pre-segmentation of the objects in
the training images is required. The number of parts in each
layer of the hierarchy is currently ﬁxed by hand. The method
assumes that each training image contains an instance of the
class, but the instance’s position and scale can be unknown
and some background clutter is tolerated. It works as follows:
1. Heuristically rank the training images according to their
expected “quality” as training examples (see below),
and use just the best image to estimate the initial model
parameters. If this fails we could potentially use the
second, third, etc., images, but this has not been necessary in our experiments.
2. Using K-means, cluster all of the features in the initial
image into n location-appearance classes, where n is
the desired number of elementary parts, and initialize
an elementary part at each cluster center. The experiments below actually assume that n is the number of
observed features, so that there is one elementary part
per observed feature. Some of these elementary parts
Figure 2. Model ﬁtting on test horse toy images. The bottom right
points are the model average positions of the subparts.
will correspond to background clutter. We do not currently attempt to remove these. Some feature classes
may have the same appearance but different locations.
This is intentional: it allows for (small numbers of) repeated features such as eyes or car wheels.
3. Work up the hierarchy, clustering the subpart centers
into the desired number of parent part centres, and initializing one parent for each cluster. Cluster membership gives initial (hard) parent assignments for the subparts. The corresponding τ matrix is initialized to a
slightly softer version of these. The cluster centre gives
the part location, and the median scale of the part’s children gives its initial scale estimate.
The use of a single initial image in the ﬁrst step could
certainly be criticised, but so far our attempts to initialize
from averages over several or many images have given much
worse results. The critical point seems to be to provide an
initial model with cleanly separated appearances and parts,
from which a relatively unambiguous training phase can proceed. Averaging tends to confuse the part relationships and
produce a less effective overall model.
Our method of ranking the training images by their probable quality as model initializers is as follows:
1. Use K-means to cluster the features from all (positive)
training images into about 500 classes. Encode each
image as a 500-D signature vector S (the vector of class
2. Rank the feature classes by an informativeness measure
(see below) and select about the 30 most informative
ones. Rank the images according to the number of these
classes that they contain (i.e. the number of classes c for
which Sc ̸= 0).
For the feature informativeness ranking, we have studied
two methods, one supervised, the other unsupervised.
The supervised method requires a negative training set
(non-class images) as well as the positive one (class images).
It trains a linear classiﬁer to predict the image class (±1
for positive or negative) from the binarized signature vector
(S ̸= 0). The features with the highest weights are chosen
as the most informative ones. Any appropriate classiﬁcation
method can be used: linear SVM or RVM, LASSO, etc.
The unsupervised method is somewhat more heuristic,
but it seems to work equally well and it requires no negative images. For each feature, it counts the number of (positive) images in which it occurs exactly once (or alternatively,
exactly 1–2 times), and chooses the features with the highest counts as the most informative. This works because it
selects distinctive features representing unique object parts.
Many object classes contain such features, whereas background features are much more variable and seldom occur
exactly once per image. This method would fail for object
classes dominated by repetitive texture though.
4. Experiments
Here we will only test a three-layer model (object →part
→feature class). Figure 2 demonstrates the model’s ability to handle local image deformations due to parallax, for
which rigid matching would fail. We learned the model from
6 images of the same toy horse seen from different viewing
positions, using 100 feature classes and 4 parts. The model
was then instantiated on 6 test images with the method described above. The change of viewing angle between views
is considerable, but the model still ﬁnds and locks on to the
correct object parts, even if only a few points are found on
Datasets: We used ﬁve different image classes from the
“Caltech 101 Object Categories” dataset3 , which contains many example images from 101 objects categories,
including faces (435 images), leopards (200), motorbikes
(800), aeroplanes (800) and side views of cars (123). These
datasets have already been used by several groups . Half of the images in each class were held out for testing.
Some examples of the learned models are shown in ﬁgure 3. To test whether the models really managed to learn
the most important appearance parameters and spatial interrelationships, and whether they were sufﬁciently selective
for a given object category, we assessed their discriminative power by ﬁtting several class models to unseen test images, using model likelihoods as decision variables. For each
class, a decision threshold was computed to minimize the average training error rate. We used 10 EM iterations during
training and 5 during testing. Confusion matrices are given
in table 1 for the original 7 class Caltech dataset using 200
feature classes for the one-level and two level (with 3 parts)
hierarchical models. The number of errors depends on the
class, but despite the fact that our models are generative not
3Available at 
one-level, partless model
Motorbikes
Two-level, 3 part model
Motorbikes
Table 1. The confusion matrix for part based multiclass categorization on the original Caltech 7 class dataset.
one-level model
Two-level, three part model
Table 2. Confusion matrix for best-class classiﬁers based on 80 feature classes, on the ﬁrst few classes of the Caltech 101 class dataset.
Figure 3. Examples of ﬁts to images from the motorbikes and leopard datasets. The ﬁrst line shows a close-up of the initialisation
based on location/scale voting.
discriminative, the results seem to be competitive with the
state of the art on these datasets . The basic partless
model is already highly discriminative for these data sets,
but using a 3 part model still reduces the error rates by a
factor of about two.
Figure 4 shows that the results are not too sensitive to the
number of parts R
test error rate
number of subparts K
test error rate
Figure 4. The test error rate of the leaves/faces classiﬁer against the
numbers of parts (left) and feature classes (right).
number of parts, although over-ﬁtting starts to worsen the
results beyond about 8–10 parts. Relatively large numbers of
elementary parts are needed to get optimal results — about
200 in this case.
Soft vs. hard assignments: The matrix τ coding for the
structure can be constrained to have only ones and zeros, so
that a given part can only be generated by a single parent. To
illustrate the advantages of soft parenting, for binary classi-
ﬁcation of motorbikes against background images using 40
training images, 200 feature classes and 4 parts, hard assignment based learning produces a test-set classiﬁcation rate of
83%, while our standard soft assignments gave 88%. Similar
results occur for other datasets and training parameters.
5. Conclusions and Future Work
We have described a multi-layered part-based generative model for category-level visual object recognition using large numbers of local features. The model managed
to adapt well to the object categories tested in our supervised classiﬁcation experiments. Reasons for this include its
well-graded spatial ﬂexibility, and the fact that it can efﬁciently incorporate a large number of interest points, each
carrying a worthwhile amount of discriminant information.
The resulting multiclass object classiﬁer performs well on
the benchmark databases tested. We also note that so long as
the model uses sufﬁciently many detected points, the matching of elementary parts to image features does not need to
be very accurate. We showed how a simple three-layer hierarchy of object, parts and features can give satisfying visual
intuition and probabilistic accuracy.
Future work: The model applies to arbitrary spatial transformations between parts and their subparts, and arbitrary
numbers of layers, although here we applied it only with
translation-scale transformations and 3 layers. Future work
will study the advantages of more general transformations
and additional layers. The main difﬁculty is likely to be
getting a good initialization for these more complex models. Another promising direction is to learn mixtures of such
generative models, for image clustering or to handle more
complex classes such as 3D models viewed from all possible directions.