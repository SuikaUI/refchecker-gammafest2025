Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech
Recognition
Jaeyoung Kim1, Mostafa El-Khamy1, Jungwon Lee1
1Samsung Semiconductor, Inc.
4921 Directors Place, San Diego, CA, USA
 , , 
In this paper, a novel architecture for a deep recurrent neural
network, residual LSTM is introduced. A plain LSTM has an
internal memory cell that can learn long term dependencies of
sequential data. It also provides a temporal shortcut path to
avoid vanishing or exploding gradients in the temporal domain.
The residual LSTM provides an additional spatial shortcut path
from lower layers for efﬁcient training of deep networks with
multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM separates a spatial shortcut path
with temporal one by using output layers, which can help to
avoid a conﬂict between spatial and temporal-domain gradient
ﬂows. Furthermore, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial
information ﬂow instead of additional gate networks, which effectively reduces more than 10% of network parameters. An
experiment for distant speech recognition on the AMI SDM corpus shows that 10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in WER over 3-layer baselines,
respectively. On the contrary, 10-layer residual LSTM networks
provided the lowest WER 41.0%, which corresponds to 3.3%
and 2.8% WER reduction over plain and highway LSTM networks, respectively.
Index Terms: ASR, LSTM, GMM, RNN, CNN
1. Introduction
Over the past years, the emergence of deep neural networks has
fundamentally changed the design of automatic speech recognition (ASR). Neural network-based acoustic models presented
signiﬁcant performance improvement over the prior state-ofthe-art Gaussian mixture model (GMM) . Advanced neural network-based architectures further improved
ASR performance. For example, convolutional neural networks
(CNN) which has been huge success in image classiﬁcation and
detection were effective to reduce environmental and speaker
variability in acoustic features . Recurrent neural
networks (RNN) were successfully applied to learn long term
dependencies of sequential data .
The recent success of a neural network based architecture
mainly comes from its deep architecture . However,
training a deep neural network is a difﬁcult problem due to vanishing or exploding gradients. Furthermore, increasing depth in
recurrent architectures such as gated recurrent unit (GRU) and
long short-term memory (LSTM) is signiﬁcantly more difﬁcult
because they already have a deep architecture in the temporal
There have been two successful architectures for a deep
feed-forward neural network: residual network and highway
network. Residual network was successfully applied to
train more than 100 convolutional layers for image classiﬁcation and detection. The key insight in the residual network is to
provide a shortcut path between layers that can be used for an
additional gradient path. Highway network is an another
way of implementing a shortcut path in a feed-forward neural
network. presented successful MNIST training results with
100 layers.
Shortcut paths have also been investigated for RNN and
LSTM networks.
The maximum entropy RNN (ME-RNN)
model has direct connections between the input and output layers of an RNN layer. Although limited to RNN networks with a single hidden layer, the perplexity improved by
training the direct connections as part of the whole network.
Highway LSTM presented a multi-layer extension of
an advanced RNN architecture, LSTM . LSTM has internal
memory cells that provide shortcut gradient paths in the temporal direction. Highway LSTM reused them for a highway
shortcut in the spatial domain. It also introduced new gate networks to control highway paths from the prior layer memory
cells. presented highway LSTM for far-ﬁeld speech recognition and showed improvement over plain LSTM. However,
 also showed that highway LSTM degraded with increasing
In this paper, a novel highway architecture, residual LSTM
is introduced. The key insights of residual LSTM are summarized as below.
• Highway connection between output layers instead of internal memory cells: LSTM internal memory cells are
used to deal with gradient issues in the temporal domain.
Reusing it again for the spatial domain could make it
more difﬁcult to train a network in both temporal and
spatial domains. The proposed residual LSTM network
uses an output layer for the spatial shortcut connection
instead of an internal memory cell, which can less interfere with a temporal gradient ﬂow.
• Each output layer at the residual LSTM network learns
residual mapping not learnable from highway path.
Therefore, each new layer does not need to waste time
or resource to generate similar outputs from prior layers.
• Residual LSTM reuses an LSTM projection matrix as a
gate network. For an usual LSTM network size, more
than 10% learnable parameters can be saved from residual LSTM over highway LSTM.
The experimental result on the AMI SDM corpus showed
10-layer plain and highway LSTMs had severe degradation
from increased depth: 13.7% and 6.2% increase in WER over 3layer baselines, respectively. On the contrary, a 10-layer residual LSTM presented the lowest WER 41.0%, which outperformed the best models of plain and highway LSTMs.
 
2. Revisiting Highway Networks
In this section, we give a brief review of LSTM and three existing highway architectures.
2.1. Residual Network
A residual network provides an identity mapping by shortcut paths. Since the identity mapping is always on, function
output only needs to learn residual mapping. Formulation of
this relation can be expressed as:
y = F(x; W) + x
y is an output layer, x is an input layer and F(x; W) is a function with an internal parameter W. Without a shortcut path,
F(x; W) should represent y from input x, but with an identity mapping x, F(x; W) only needs to learn residual mapping,
y −x. As layers are stacked up, if no new residual mapping is
needed, a network can bypass identity mappings without training, which could greatly simplify training of a deep network.
2.2. Highway Network
A highway network provides another way of implementing a shortcut path for a deep neural-network. Layer output
H(x; Wh) is multiplied by a transform gate T(x; WT ) and before going into the next layer, a highway path x·(1−T(x; WT ))
is added. Formulation of a highway network can be summarized
y = H(x; Wh) · T(x; WT ) + x · (1 −T(x; WT ))
A transform gate is deﬁned as:
T(x; WT ) = σ(WT x + bT )
Unlike a residual network, a highway path of a highway network is not always turned on. For example, a highway network
can ignore a highway path if T(x; WT ) = 1 , or bypass a output
layer when T(x; WT ) = 0.
2.3. Long Short-Term Memory (LSTM)
Long short-term memory (LSTM) was proposed to resolve
vanishing or exploding gradients for a recurrent neural network.
LSTM has an internal memory cell that is controlled by forget
and input gate networks. A forget gate in an LSTM layer determines how much of prior memory value should be passed into
the next time step. Similarly, an input gate scales new input to
memory cells. Depending on the states of both gates, LSTM
can represent long-term or short-term dependency of sequential
data. The LSTM formulation is as follows:
t · tanh(W l
t · tanh(cl
l represents layer index and il
t are input, forget and
output gates respectively. They are component-wise multiplied
by input, memory cell and hidden output to gradually open or
close their connections. xl
t is an input from (l −1)th layer (or
an input to a network when l is 1), hl
t−1 is a lth output layer at
time t −1 and cl
t−1 is an internal cell state at t −1. W l
projection matrix to reduce dimension of rl
2.4. Highway LSTM
Highway LSTM reused LSTM internal memory
cells for spatial domain highway connections between stacked
LSTM layers. Equations (4), (5), (7), (8), and (9) do not change
for highway LSTM. Equation (6) is updated to add a highway
connection:
t · tanh(W l
t is a depth gate that connects cl−1
in the (l −1)th
layer to cl
t in the lth layer. showed that an acoustic model
based on the highway LSTM network improved far-ﬁeld speech
recognition compared with a plain LSTM network. However,
 also showed that word error rate (WER) degraded when
the number of layers in the highway LSTM network increases
from 3 to 8.
3. Residual LSTM
In this section, a novel architecture for a deep recurrent neural
network, residual LSTM is introduced. Residual LSTM starts
with an intuition that the separation of a spatial-domain shortcut path with a temporal-domain cell update may give better
ﬂexibility to deal with vanishing or exploding gradients. Unlike
highway LSTM, residual LSTM does not accumulate a highway path on an internal memory cell cl
t. Instead, a shortcut
path is added to an LSTM output layer hl
t. For example, cl
keep a temporal gradient ﬂow without attenuation by maintaining forget gate f l
t to be close to one. However, this gradient ﬂow
can directly leak into the next layer cl+1
for highway LSTM in
spite of their irrelevance. On the contrary, residual LSTM has
less impact from cl
t update due to separation of gradient paths.
Figure 1 describes a cell diagram of a residual LSTM layer.
is a shortcut path from (l −1)th output layer that is added
to a projection output ml
t. Although a shortcut path can be
any lower output layer, in this paper, we used a previous output
layer. Equations (4), (5), (6) and (7) do not change for residual
LSTM. The updated equations are as follows:
h can be replaced by an identity matrix if the dimension of xl
t matches that of hl
t. For a matched dimension,
Equation (14) can be changed into:
Since a highway path is always turned on for residual LSTM,
there should be a scaling parameter on the main path output.
For example, linear ﬁlters in the last CNN layer of a residual
network are reused to scale the main path output. For residual LSTM, a projection matrix W l
p is reused in order to scale
the LSTM output. Consequently, the number of parameters for
residual LSTM does not increase compared with plain LSTM.
Simple complexity comparison between residual LSTM and
highway LSTM is as follows. If the size of the internal memory cells is N and the output layer dimension after projection
Memory Cell
Dimension: N
Output Dimension: M
Input Dimension: K
Component-Wise Product
Identity matrix if K=M
Shortcut Path
Projection
Figure 1: Residual LSTM: A shortcut from a prior output layer
is added to a projection output ml
h is a dimension
matching matrix between input and output. If K is equal to M,
it is replaced with an identity matrix.
is N/2, the total number of reduced parameters for a residual LSTM network becomes N 2/2 + 4N. For example, if N
is 1024 and the number of layers is more than 5, the residual
LSTM network has approximately 10% less network parameters compared with the highway LSTM network with same N
and a projection matrix.
One thing to note is that a highway path should be scaled
by an output gate as in Equation (14). The initial design of
residual LSTM was to simply add an input path to an LSTM
output without scaling, which is similar to a ResLSTM block
in . However, it showed signiﬁcant performance loss because highway paths keep accumulated as the number of layers
increase. For example, the ﬁrst layer output without scaling
would be o1
t, which consists of two components. For
the second layer output, however, the number of components increases as three: o2
t. Without proper scaling,
the variance of an residual LSTM output will keep increasing.
The convolutional LSTM network proposed in added
batch normalization layers, which can normalize increased output variance from a highway path. For residual LSTM, output
gate is re-used to act similarly without any additional layer or
parameter. Output gate is a trainable network which can learn
a proper range of an LSTM output. For example, if an output
gate is set as
2, an lth output layer becomes
)(l−k+1)mk
Where, xt is an input to LSTM at time t. If ml
t and xt are
independent each other for all l and have ﬁxed variance of 1,
regardless of layer index l, the variance of layer lth output becomes 1. Since variance of a output layer is variable in the real
scenario, a trainable output gate will better deal with exploding
variance than a ﬁxed scaling factor.
4. Experiments
4.1. Experimental Setup
AMI meeting corpus is used to train and evaluate residual LSTMs.
AMI corpus consists of 100 hours of meeting
recordings. For each meeting, three to four people have free
conversation in English. Frequently, overlapped speaking from
multiple speakers happens and for that case, the training transcript always follows a main speaker. Multiple microphones
are used to synchronously record conversations in different environments.
Individual headset microphone (IHM) recorded
clean close-talking conversation and single distant microphone
(SDM) recorded far-ﬁeld noisy conversation.
In this paper,
SDM is used to train residual LSTMs at Section 4.2 and 4.3
and combined SDM and IHM corpora are used at Section 4.4.
Kaldi is a toolkit for speech recognition that is used
to train a context-dependent LDA-MLLT-GMM-HMM system.
The trained GMM-HMM generates forced aligned labels which
are later used to train a neural network-based acoustic model.
Three neural network-based acoustic models are trained: plain
LSTM network without any shortcut path, highway LSTM network and residual LSTM network. All three LSTM networks
have 1024 memory cells and 512 output nodes for experiments
at Section 4.2, 4.3 and 4.4.
The computational network toolkit (CNTK) is used
to train and decode three acoustic models.
Truncated backpropagation through time (BPTT) is used to train LSTM networks with 20 frames for each truncation. Cross-entropy loss
function is used with L2 regularization.
For decoding, reduced 50k-word Fisher dictionary is used
for lexicon and based on this lexicon, tri-gram language model
is interpolated from AMI training transcript. As a decoding
option, word error rate (WER) can be calculated based on
non-overlapped speaking or overlapped speaking. Recognizing
overlapped speaking is to decode up to 4 concurrent speeches.
Decoding overlapped speaking is a big challenge considering a
network is trained to only recognize a main speaker. Following
sections will provide WERs for both options.
4.2. Training Performance with increasing Depth
Figure 2 compares training and cross-validation (CV) crossentropies for highway and residual LSTMs.
The crossvalidation set is only used to evaluate cross-entropies of trained
In Figure 2a, training and CV cross-entropies for a 10-layer
highway LSTM increased 15% and 3.6% over 3-layer one, respectively. 3.6% CV loss for a 10-layer highway LSTM does
not come from overﬁtting because the training cross-entropy
was increased as well. The training loss from increased network
depth was observed in many cases such as Figure 1 of . A
10-layer highway LSTM revealed the similar training loss pattern, which implies highway LSTM does not completely resolve
this issue.
In Figure 2b, a 10-layer residual LSTM showed that its CV
cross-entropy does not degrade with increasing depth. On the
contrary, the CV cross-entropy improved. Therefore, residual
LSTMs did not show any training loss observed in . One
thing to note is that the 10-layer residual LSTM also showed
6.7% training cross-entropy loss. However, the increased training loss for the residual LSTM network resulted in better generalization performance like regularization or early-stopping techniques. It might be due to better representation of input features
from the deep architecture enabled by residual LSTM.
4.3. WER Evaluation with SDM corpus
Table 1 compares WER for plain LSTM, highway LSTM and
residual LSTM with increasing depth.
All three networks
were trained by SDM AMI corpus. Both overlapped and nonoverlapped WERs are shown. For each layer, internal mem-
Cross Entropy Loss
Highway LSTM
Training Set, 3L
Training Set, 10L
CV Set, 3L
CV Set, 10L
Cross Entropy Loss
Residual LSTM
Training Set, 3L
Training Set, 10L
CV Set, 3L
CV Set, 10L
Figure 2: Training and CV PERs on AMI SDM corpus. (a)
shows training and cross-validation (CV) cross-entropies for 3
and 10-layer highway LSTMs. (b) shows training and crossvalidation (CV) cross-entropies for 3 and 10-layer residual
ory cell size is set to be 1024 and output node size is ﬁxed
as 512. A plain LSTM performed worse with increasing layers. Especially, the 10-layer LSTM degraded up to 13.7% over
the 3-layer LSTM for non-overlapped WER. A highway LSTM
showed better performance over a plain LSTM but still could
not avoid degradation with increasing depth. The 10-layer highway LSTM presented 6.2% increase in WER over the 3-layer
On the contrary, a residual LSTM improved with increasing
layers. 5-layer and 10-layer residual LSTMs have 1.2% and
2.2% WER reductions over the 3-layer network, respectively.
The 10-layer residual LSTM showed the lowest 41.0% WER,
which corresponds to 3.3% and 2.8% WER reduction over 3layer plain and highway LSTMs.
One thing to note is that WERs for 3-layer plain and highway LSTMs are somewhat worse than results reported in .
The main reason might be that forced alignment labels used to
train LSTM networks are not the same as the ones used in .
1-2% WER can easily be improved or degraded depending on
the quality of aligned labels. Since the purpose of our evaluation
is to measure relative performance between different LSTM architectures, small absolute difference of WER would not be any
issue. Moreover, reproduce of highway LSTM is based on the
open source code provided by the author in and therefore,
Table 1: All three LSTM networks have the same size of layer
parameters:1024 memory cells and 512 output nodes. Fixedsize layers are stacked up when the number of layers increases.
WER(over) is overlapped WER and WER (non-over) is nonoverlapped WER.
Acoustic Model
WER (over)
WER (non-over)
Plain LSTM
Highway LSTM
Residual LSTM
Table 2: Highway and residual LSTMs are trained with combined SDM and IHM corpora.
Acoustic Model
WER (over)
WER (non-over)
Highway LSTM
Residual LSTM
it would be less likely to have big experimental mismatch in our
evaluation.
4.4. WER Evaluation with SDM and IHM corpora
Table 2 compares WER of highway and residual LSTMs trained
with combined IHM and SDM corpora. With increased corpus
size, the best performing conﬁguration for a highway LSTM
is changed into 5-layer with 40.7% WER. However, a 10-layer
highway LSTM still suffered from training loss from increased
depth: 6.6% increase in WER (non-over). On the contrary, a
10-layer residual LSTM showed the best WER of 39.3%, which
corresponds to 3.1% WER (non-over) reduction over the 5-layer
one, whereas the prior experiment trained only by SDM corpus
presented 1% improvement. Increasing training data provides
larger gain from a deeper network. Residual LSTM enabled to
train a deeper LSTM network without any training loss.
5. Conclusion
In this paper, we proposed a novel architecture for a deep recurrent neural network: residual LSTM. Residual LSTM provides
a shortcut path between adjacent layer outputs. Unlike highway
network, residual LSTM does not assign dedicated gate networks for a shortcut connection. Instead, projection matrix and
output gate are reused for a shortcut connection, which provides
roughly 10% reduction of network parameters compared with
highway LSTMs.
Experiments on AMI corpus showed that
residual LSTMs improved signiﬁcantly with increasing depth,
meanwhile 10-layer plain and highway LSTMs severely suffered from training loss.
6. References
 G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent
pre-trained deep neural networks for large-vocabulary speech
recognition,” IEEE Transactions on Audio, Speech, and Language
Processing, vol. 20, no. 1, pp. 30–42, 2012.
 G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep
neural networks for acoustic modeling in speech recognition: The
shared views of four research groups,” IEEE Signal Processing
Magazine, vol. 29, no. 6, pp. 82–97, 2012.
 Y. LeCun, Y. Bengio et al., “Convolutional networks for images,
speech, and time series,” The handbook of brain theory and neural
networks, vol. 3361, no. 10, p. 1995, 1995.
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the
IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.
 G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Large vocabulary continuous speech recognition with context-dependent DBN-
HMMs,” in 2011 IEEE international conference on acoustics,
speech and signal processing (ICASSP).
IEEE, 2011, pp. 4688–
 T. N. Sainath, O. Vinyals, A. Senior, and H. Sak, “Convolutional,
long short-term memory, fully connected deep neural networks,”
in 2015 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP).
IEEE, 2015, pp. 4580–4584.
 P. Swietojanski, A. Ghoshal, and S. Renals, “Convolutional neural
networks for distant speech recognition,” IEEE Signal Processing
Letters, vol. 21, no. 9, pp. 1120–1124, 2014.
 O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, and G. Penn, “Applying convolutional neural networks concepts to hybrid NN-HMM
model for speech recognition,” in 2012 IEEE international conference on Acoustics, speech and signal processing (ICASSP).
IEEE, 2012, pp. 4277–4280.
 T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran, “Deep convolutional neural networks for LVCSR,” in 2013
IEEE International Conference on Acoustics, Speech and Signal
Processing.
IEEE, 2013, pp. 8614–8618.
 O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn,
and D. Yu, “Convolutional neural networks for speech recognition,” IEEE/ACM Transactions on audio, speech, and language
processing, vol. 22, no. 10, pp. 1533–1545, 2014.
 A. Graves, N. Jaitly, and A.-r. Mohamed, “Hybrid speech recognition with deep bidirectional LSTM,” in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on.
IEEE, 2013, pp. 273–278.
 H. Sak, A. Senior, and F. Beaufays, “Long short-term memory
based recurrent neural network architectures for large vocabulary
speech recognition,” arXiv preprint arXiv:1402.1128, 2014.
 H. Sak, A. W. Senior, and F. Beaufays, “Long short-term memory recurrent neural network architectures for large scale acoustic
modeling.” in INTERSPEECH, 2014, pp. 338–342.
 M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol. 45,
no. 11, pp. 2673–2681, 1997.
 N. Morgan, “Deep and wide: Multiple layers in automatic speech
recognition,” IEEE Transactions on Audio, Speech, and Language
Processing, vol. 20, no. 1, pp. 7–13, 2012.
 R. K. Srivastava, K. Greff, and J. Schmidhuber, “Training very
deep networks,” in Advances in neural information processing
systems, 2015, pp. 2377–2385.
 K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” arXiv preprint arXiv:1512.03385, 2015.
 R. K. Srivastava, K. Greff, and J. Schmidhuber, “Highway networks,” Deep Learning Workshop , 2015.
 T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ˇCernock`y,
“Strategies for training large scale neural network language
models,” in Automatic Speech Recognition and Understanding
(ASRU), 2011 IEEE Workshop on.
IEEE, 2011, pp. 196–201.
 Y. Zhang, G. Chen, D. Yu, K. Yaco, S. Khudanpur, and J. Glass,
“Highway long short-term memory RNNs for distant speech
recognition,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2016, pp.
5755–5759.
 K. Yao, T. Cohn, K. Vylomova, K. Duh, and C. Dyer, “Depthgated LSTM,” in Presented at Jelinek Summer Workshop on August, vol. 14, 2015, p. 1.
 S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
 J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot,
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal et al.,
“The AMI meeting corpus: A pre-announcement,” in International Workshop on Machine Learning for Multimodal Interaction.
Springer, 2005, pp. 28–39.
 Y. Zhang, W. Chan, and N. Jaitly, “Very deep convolutional
networks for end-to-end speech recognition,” arXiv preprint
 
 D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,
N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz,
J. Silovsky, G. Stemmer, and K. Vesely, “The KALDI speech
recognition toolkit,” in IEEE 2011 Workshop on Automatic Speech
Recognition and Understanding.
IEEE Signal Processing Society, Dec. 2011, iEEE Catalog No.: CFP11SRW-USB.
 D. Yu, A. Eversole, M. Seltzer, K. Yao, Z. Huang, B. Guenter,
O. Kuchaiev, Y. Zhang, F. Seide, H. Wang et al., “An introduction to computational networks and the computational network toolkit,” Technical report, Tech. Rep. MSR, Microsoft Research, 2014, 2014. research. microsoft. com/apps/pubs, Tech.
Rep., 2014.