> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
Abstract— Classification techniques are becoming essential in the
financial world for reducing risks and possible disasters.
Managers are interested in not only high accuracy but also in
interpretability and transparency. It is widely accepted now that
the comprehension of how inputs and output are related to each
other is crucial for taking operative and strategic decisions.
Furthermore, inputs are often affected by contextual factors and
characterized by a high level of uncertainty. In addition, financial
data are usually highly skewed towards the majority class. With
accuracies,
preserving
interpretability and managing uncertain and unbalanced data,
the paper presents a novel method to deal with financial data
classification by adopting type-2 fuzzy rule-based classifiers
(FRBCs) generated from data by a multi-objective evolutionary
algorithm (MOEA). The classifiers employ an approach, denoted
as scaled dominance, for defining rule weights in such a way to
help minority classes to be correctly classified. In particular, we
have extended PAES-RCS, an MOEA-based approach to learn
concurrently the rule and data bases of FRBCs, for managing
both interval type-2 fuzzy sets and unbalanced datasets. To the
best of our knowledge, this is the first work that generates type-2
FRBCs by concurrently maximizing accuracy and minimizing
the number of rules and the rule length with the objective of
producing interpretable models of real-world skewed and
incomplete financial datasets. The rule bases are generated by
exploiting a rule and condition selection (RCS) approach, which
selects a reduced number of rules from a heuristically generated
rule base and a reduced number of conditions for each selected
rule during the evolutionary process. The weight associated with
each rule is scaled by the scaled dominance approach on the
fuzzy frequency of the output class, in order to give a higher
weight to the minority class. As regards the data base learning,
the membership function parameters of the interval type-2 fuzzy
sets used in the rules are learned concurrently to the application
of RCS. Unbalanced datasets are managed by using, in addition
to complexity, selectivity and specificity as objectives of the
MOEA rather than only the classification rate. We tested our
approach, named IT2-PAES-RCS, on eleven financial datasets
and compared our results with the ones obtained by the original
Manuscript received XXXXX; revised XXXXX and XXXXXX; accepted
XXXXXX. Date of publication XXXXX; date of current version XXXXX.
M. Antonelli and F. Marcelloni are with the Dipartimento di Ingegneria
dell’Informazione, University of Pisa, Pisa I-56100, Italy (e-mail:
 ; francesco. ).
D. Bernardo and H. Hagras are with the Computational Intelligence
Centre, School of Computer Science and Electronic Engineering, University
Colchester,
 ).
M. Antonelli is also with Translational Imaging Group, Centre for Medical
Image Computing (CMIC), University College London, London, UK.
PAES-RCS with three objectives and with and without scaled
dominance, the fuzzy rule-based classifiers FARC-HD and
FURIA, the classical C4.5 decision tree algorithm and its costsensitive version. Using non-parametric statistical tests, we will
show that IT2-PAES-RCS generates FRBCs with, on average,
accuracy statistically comparable to and complexity lower than
the ones generated by the two versions of the original PAES-
RCS. Further, the FRBCs generated by FARC-HD and FURIA
and the decision trees computed by C4.5 and its cost-sensitive
version, despite the highest complexity, result to be less accurate
than the FRBCs generated by IT2-PAES-RCS. Finally, we will
highlight how these FRBCs are easily interpretable by showing
and discussing one of them.
Index Terms—type-2 fuzzy rule-based classifiers, multiobjective
evolutionary
unbalanced datasets.
I. INTRODUCTION
HE financial crisis of 2008 demonstrated that lack of
good information can lead to disasters. Financial services
organizations, customers, and particularly regulators quickly
came to understand that clear and relevant information was
key to risk reduction. Therefore, we are now witnessing
ongoing efforts by regulators to ensure that firms operating in
financial services generate comprehensive and comprehensible
information. Superficially, the demands of regulators look
burdensome. In reality, however, they provide an opportunity
for organizations to improve their strategic and operational
activities through risk reduction based on well-managed
information .
Machine learning in financial applications differs from
other domains in how the quality of a model is assessed.
Whereas in most applications, “accuracy of prediction” is
often the only metric used, in financial applications,
interpretability and transparency are also important and
sometimes a requirement. Within financial applications, the
accuracy of the model is not the only crucial issue. There is a
growing interest in having high levels of model transparency,
which is the ability to provide a clear and understandable
explanation of the output result. If advanced analytical
techniques are used, there is now an obligation to manage the
whole process of creating and using the resulting models. It is
no longer enough to create a model, deploy it into production
and leave it unattended without any oversight. A set of
Multi-Objective Evolutionary Optimization of
Type-2 Fuzzy Rule-based Systems for Financial
Data Classification
Michela Antonelli, Dario Bernardo, Hani Hagras, Fellow, IEEE, Francesco Marcelloni, Member, IEEE
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
capabilities and processes are required to ensure that every
aspect of model creation, deployment and performance is well
understood, managed and documented. This implies additional
technology infrastructure and methods, since in large firms the
number of models in use might be measured in the thousands.
This represents a significant shift to much greater
sophistication . Another reason why it is important that we
can understand models is trust. A system that can explain why
a certain decision was taken is more trustworthy in the eyes of
a layman user. This need for transparency is reflected in
legislation that forces financial institutions to disclose the
reasoning behind their financial decisions and models. Left
unchecked, inevitably there will be rogue models that cause
financial harm and breach regulatory requirements .
Furthermore, transparency of a model is important because it
allows users to understand data association by observing why
a specific decision has been taken. This process helps users to
drill-down into their data, understand it, and extract some
useful knowledge that could be a competitive advantage in the
market. Ultimately, a transparent model can become not only a
tool for foresight and prediction but also for analysis and
domain knowledge extraction. As it is often the case,
managers in finance face two conflicting demands. On the one
hand, they need to employ ever more powerful analytical
techniques to remain competitive, while, on the other hand,
the models they use must be transparent and relatively easy to
explain ( - ).
Neural networks, Bayesian networks, support vector
machines are all considered “black box”. This adjective is
applied to systems that, for a given input, are able to output a
class label, but without providing a clear explanation of the
decision process. Logistic regression can provide some
statistic correlations between the inputs and the output, but
this is not enough to understand why, for a given input, a
given label was chosen, or to gain a deep insight of either the
model or the data. On the other hand, “white box” models
usually refer to rule-based systems that are able to provide an
insight of the data on which the models have been trained, and
an explanation of the decision process through their rules.
Decision trees can translate their internal state into a set of
rules and, like any other rule-based system, are able to provide
transparency. Nevertheless in complex real world applications,
such as in the financial domain, the number of generated rules
can explode. It is debatable that a rule base containing
thousands of rules can be considered an understandable and
transparent model. Decision trees - and random forests
 produce associations among sets of data, which are
selected to optimize the classification problem. Thus, the
produced associations could be meaningless in the context of
profiling and knowledge extraction.
Fuzzy logic extends the concepts of association rule
learning by extending the rule antecedent sets to fuzzy
concepts. This technique, in conjunction with genetic and
evolutionary algorithms, is a powerful approach for creating
accurate and interpretable models. Studies such as - 
have shown that accuracy and interpretability are in a trade-off
and it is necessary to sacrifice one in order to increase the
other. It is difficult to define to which extent accuracy or
interpretability can be sacrificed in order to gain in the other.
Usually different applications and specific situations have
different requirements. Multi-objective genetic algorithms are
able to provide an evolution through the two competitive
objectives: accuracy and interpretability . Such
evolutionary algorithms generate a set of solutions, also
known as Pareto front, that optimize both objectives at
different levels. This feature gives the ability to easily identify
the desired level of complexity/accuracy for the specific
application. However, the vast majority of fuzzy systems
employ the type-1 fuzzy sets, which cannot directly handle the
high levels of uncertainty present in financial applications.
Indeed, type-1 fuzzy sets are crisp and precise (i.e., their
membership functions are supposed to be perfectly known)
and do not allow for any uncertainty about membership
values, which is a liability for their use. A type-2 fuzzy set is
characterized by a fuzzy membership function, i.e., the
membership value for each element of this set is itself a fuzzy
set defined on the universe . The membership
functions of the type-2 fuzzy sets are three-dimensional and
include a footprint of uncertainty. The third dimension and the
footprint of uncertainty provide additional degrees of freedom
that make it possible to directly model and handle the high
level of uncertainty affecting the inputs in financial
applications. In addition, it should be noted that using type-2
fuzzy sets to represent the system inputs can result in
reduction of the fuzzy classifier rule base and complexity (as it
will be shown in Section IV) when compared to using type-1
fuzzy sets. Indeed, the footprint of uncertainty, which
characterizes the type-2 fuzzy sets, lets us cover the same
range as type-1 fuzzy sets with a smaller number of labels: of
course, the rule reduction will be greater when the number of
inputs increases .
Previous works have already employed type-2 fuzzy
classifiers in financial domain - and have shown how
these systems outperform their type-1 versions and other state
of the art classifiers. However, to date most of the type-2
fuzzy systems reported in the literature have been generated
from data by optimizing only the accuracy, while neglecting
the complexity - . This aspect is of major importance
to the financial domain since offering compact fuzzy
classifiers with the same accuracy as their counterparts will
help to realize transparent and easy to understand models,
which are becoming essential requirements especially after the
recent economic crisis.
In financial applications, as in many real-world problems,
the data presents challenges that are not often found in
traditional academic datasets. Some of these are: size, noise,
sparsity and uncertainty. Furthermore, in the vast majority of
financial applications, data is highly unbalanced . For
example, in credit card applications the number of good
customers is much higher than that of bad customers, and in
fraud detection the majority of the data is normal transactions
with only a few fraudulent transactions. Most classifiers
designed for minimizing the global error rate perform poorly
on unbalanced datasets, because they misclassify most of the
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
data belonging to the class with few examples. To tackle this
problem, pre-processing techniques like under-sampling or
over-sampling are usually applied, but both of them present
problems. On the one hand, under-sampling techniques may
increment the noise since they could eliminate some important
patterns. On the other hand, over-sampling techniques may
add noise for the original input data or violate the inherent
geometrical structure of the minority and majority classes.
Hence, in financial applications it is not desirable to preprocess or sample the data as this could cause problems. Thus,
there is a need for predictive analytics techniques that can
handle unbalanced financial data sets to give accurate and
interpretable financial models.
In this paper, with the aim of dealing with uncertain and
unbalanced data, and generating accurate and interpretable
classifiers, we employ PAES-RCS , an MOEA-based
approach to learn concurrently the rule and data bases of fuzzy
rule-based classifiers (FRBCs). In PAES-RCS, the learning
process is performed by selecting a set of rules from an initial
rule base and a set of conditions for each selected rule. This
scheme is denoted as rule and condition selection (RCS).
During the multi-objective evolutionary process, PAES-RCS
generates the rule bases of the classifiers by using the RCS
approach and concurrently learns the membership function
parameters of the linguistic values used in the rules. The
original PAES-RCS is extended so as to manage interval type-
2 (IT2) fuzzy sets and unbalanced datasets. We denote this
extension as IT2-PAES-RCS in the following. We modified
both the inference mechanism and the evolutionary process for
coping with the IT2 fuzzy sets. Further, we adopted three
objectives, namely false positive rate (FPR), true positive rate
(TPR) and complexity. In our previous works , we have
verified that the use of FPR and TPR as objectives of the
evolutionary optimization process has proved to be very
effective in managing unbalanced datasets. Indeed, one of the
main strengths of IT2-PAES-RCS is that it can be applied to
unbalanced datasets without any rebalancing.
We tested IT2-PAES-RCS on eleven financial datasets and
compared the results with the ones obtained by the original
PAES-RCS, employing FPR and TPR as objectives, with
(PAES-RCS-SD) and without scaled dominance, the fuzzy
rule-based classifiers FARC-HD and FURIA , the
classical C4.5 decision tree algorithm and its costsensitive version (C4.5-CS) . Using non-parametric
statistical tests, we will show that IT2-PAES-RCS generates
FRBCs with accuracy statistically comparable to the ones
generated by PAES-RCS and PAES-RCS-SD, employing a
lower number of rules and a lower number of conditions in the
antecedent of the rules. The FRBCs generated by IT2-PAES-
RCS results therefore to be less complex and more
interpretable. Further, the FRBCs generated by FARC-HD and
FURIA, and the decision trees computed by C4.5 and its costsensitive version, despite the lowest interpretability, result to
be less accurate than the solutions generated by IT2-PAES-
The paper is organized as follows. In Section II, we provide
a basic description of FRBCs based on IT2 fuzzy sets and
introduce some notations. Section III shows the proposed
MOEA-based learning approach and includes the details of the
initial rule base generation technique, of the chromosome
coding and mating operators, and of the adopted MOEA. In
Section IV, we illustrate the experimental results and in
Section V we draw some final conclusion.
II. INTERVAL TYPE-2 FUZZY RULE-BASED CLASSIFIER
Object classification consists of assigning a class Cj from a
predefined set {C1, …, CK} of classes to an object. Each object
is considered as an F-dimensional point in a feature space ℜF.
Let X = {X1, …, XF} be the set of features and Uf, f = 1,…, F,
be the universe of the f-th feature. Let 𝑃̃f = {𝐴̃𝑓,1, , … , 𝐴̃𝑓,𝑇𝑓},
𝑓= 1, … 𝐹, be a fuzzy partition with Tf IT2 fuzzy sets of the
universe Uf. We recall that an IT2 fuzzy set 𝐴̃ is characterized
by a fuzzy membership function 𝜇𝐴̃(𝑥), that is, the
membership value for each element of this set is a fuzzy set
 . The membership functions of IT2 fuzzy sets include a
footprint of uncertainty, which provides additional degrees of
freedom that make it possible to directly model and handle
uncertainties. In the IT2 fuzzy sets, all the third dimension
values are equal to one. More formally, the membership
function 𝜇𝐴̃(𝑥) of an IT2 fuzzy set 𝐴̃ is defined as:
𝜇𝐴̃(𝑥) = ∫
𝑢∈[𝜇𝐴̃(𝑥),𝜇̅𝐴̃(𝑥) ]
where 𝜇̅𝐴̃(𝑥) and 𝜇𝐴̃(𝑥) represent, respectively, the upper and
lower membership functions of the IT2 fuzzy set 𝐴̃. In this
paper, we use triangular membership functions defined by
three points (a,b,c), where a and c correspond to the endpoints
of the support and b to the core. We build the IT2 fuzzy sets
by using the following procedure. First, we define the upper
triangular membership functions 𝜇̅𝐴̃(𝑥) through the three
points (𝑎̅𝑓,𝑗 , 𝑏̅𝑓,𝑗, 𝑐̅𝑓,𝑗). Then, the left endpoints 𝑎𝑓,𝑗 of the
supports of the lower membership functions 𝜇𝐴̃𝑓(𝑥) are
computed as midpoints
𝑎̅𝑓,𝑗+𝑏̅𝑓,𝑗
between the left endpoints
𝑎̅𝑓,𝑗 of the supports of the upper membership functions
𝜇̅𝐴̃𝑓(𝑥) and their cores 𝑏̅𝑓,𝑗 . Similarly, the right endpoints 𝑐𝑓,𝑗
of the supports of the lower membership functions 𝜇𝐴̃𝑓(𝑥)
correspond to the mid-points
𝑏̅𝑓,𝑗+𝑐̅𝑓,𝑗
between the right
endpoints 𝑐̅𝑓,𝑗 of the supports of the upper membership
functions 𝜇̅𝐴̃𝑓(𝑥) and the cores 𝑏̅𝑓,𝑗. It follows that 𝑎𝑓,𝑗 =
𝑐𝑓,𝑗−1, for 𝑗= 2, … , 𝑇𝑓. The cores 𝑏𝑓,𝑗 coincide with the cores
𝑏̅𝑓,𝑗. Fig. 1 shows an example of IT2 fuzzy partition with Tf =
5. Here, the upper membership functions (thick lines) are
obtained by defining a uniform Ruspini partition with
triangular membership functions on the universe Uf.
The m-th rule Rm (m=1, ..., M) of an IT2 FRBC is typically
expressed as:
Rm: IF X1 is 𝐴̃1,𝑗𝑚,1 and … and XF is 𝐴̃𝐹,𝑗𝑚,𝐹
THEN Y is 𝐶𝑗𝑚 with RWm
where Y is the classifier output, 𝐶𝑗𝑚 is the class label
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
associated with the m-th rule, 𝑗𝑚,𝑓∈[1, 𝑇𝑓] identifies the
index of the IT2 fuzzy set (among the Tf IT2 fuzzy sets of the
partition 𝑃̃f ), which has been selected for Xf in rule Rm, and
RWm is the rule weight, i.e., a certainty degree of the
classification in the class 𝐶𝑗𝑚 for a pattern that fires the
antecedent of the rule.
Let T = {(𝒙1, 𝑦1), … , (𝒙𝑁, 𝑦𝑁)} be a training set composed
of N input-output (𝒙𝑡, 𝑦𝑡) pairs, with 𝒙𝑡= [𝑥𝑡,1, … , 𝑥𝑡,𝐹] ∈ ℜ𝐹
and 𝑦𝑡 ∈{𝐶1, … , 𝐶𝐾}. The strength of activation 𝑤𝑚(𝒙𝑡)
(matching degree of the rule with the input) of the rule Rm is
calculated as:
𝑤𝑚(𝒙𝑡)+𝑤𝑚(𝒙𝑡)
𝑤𝑚(𝒙𝑡) = ∏
𝜇𝐴̃𝑓(𝑥𝑡,𝑓)
𝜇̅𝐴̃𝑓(𝑥𝑡,𝑓)
are the lower and upper bounds of the strength
of activation computed, respectively, on the lower and upper
membership functions. To take the “don’t care” condition into
account, a particular IT2 fuzzy set 𝐴̃𝑓,0 (f = 1,…, F) is added
to all the F partitions 𝑃̃f. This fuzzy set is characterized by
both the lower and upper membership functions equal to 1 on
the overall universe. This means that the condition Xf is 𝐴̃𝑓,0
does not affect the computation of the strength of activation.
In other words, for the specific rule, the variable Xf is not
taken into account and therefore can be removed. The terms
𝐴̃𝑓,0, therefore, allow generating rules, which contain only a
subset of the input variables, thus reducing the total rule length
and consequently increasing the interpretability of the rules.
Fig. 1. An example of IT2 fuzzy partitions with Tf = 5 IT2 fuzzy sets (the
thick and thin lines represent the upper and lower membership functions,
respectively).
As we have pointed out in Section I, financial data is
usually highly unbalanced. To give minority class a fair
chance when competing with majority class, we adopted a
new approach to calculate the rule weight that takes the fuzzy
frequency of the class into account. The approach is called
“scaled dominance”, and has been introduced in - . In
the literature, fuzzy rule weights are traditionally calculated as
fuzzy extension of the confidence and support. Confidence
and support are data mining metrics used in association rule
learning. These metrics, in fuzzy rule-based systems, are
extended by using fuzzy strength instead of crisp counting of
the item-sets. The confidence and support extensions used in
this paper exploit a scaled version 𝑤𝑚
𝑠 of the matching degree.
For a given rule Rm, having a consequent class 𝐶𝑗𝑚, we scale
the matching degree of the rule by dividing the upper and
lower bounds of the strengths of activation by the sum of,
respectively, the upper 𝑤𝑙(𝒙𝑡) and lower 𝑤𝑙(𝒙𝑡) bounds of the
strengths of activation of all the rules Rl, which have 𝐶𝑗𝑚 as
the consequent class. The scaled upper and lower bounds are
therefore computed as follows:
𝑙,𝑜𝑢𝑡= 𝐶𝑗𝑚
𝑙,𝑜𝑢𝑡= 𝐶𝑗𝑚
In IT2 fuzzy rule-based systems, confidence and support of
a rule are determined from the strength of activation and,
therefore, defined by upper and lower bounds. From equations
(4) and (5), we derive the following scaled upper and lower
bounds of the confidence:
𝑠(𝐴𝑛𝑡𝑚⇒𝐶𝑗𝑚) =
𝑠(𝐴𝑛𝑡𝑚⇒𝐶𝑗𝑚) =
where M is the number of rules in the rule base and Antm is the
antecedent of Rm. The confidence can be viewed as a
numerical approximation of the conditional probability
𝑃(𝐶𝑗𝑚|𝐴𝑛𝑡𝑚). The scaled upper and lower bounds of the
support are defined as:
𝑠(𝐴𝑛𝑡𝑚⇒𝐶𝑗𝑚) =
𝑠(𝐴𝑛𝑡𝑚⇒𝐶𝑗𝑚) =
The support can be viewed as a measure of the coverage of
training patterns performed by Rm.
The rule weight is then calculated as product of the scaled
confidence and support. It follows that the rule weight RWm in
(2) becomes a closed interval bounded by the upper 𝑅𝑊
̅̅̅̅̅𝑚 and
𝑅𝑊𝑚 endpoints, calculated as:
̅̅̅̅̅𝑚= 𝑐̅𝑚
The association degree with the class 𝐶𝑗𝑚 will be, in its
turn, a closed interval bounded by the upper ℎ̅𝑚(𝒙𝑡) and lower
ℎ 𝑚(𝒙𝑡) endpoints, which are computed as follows:
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
ℎ̅𝑚(𝒙𝑡) = 𝑤𝑚
̅̅̅̅̅𝑚= 𝑤𝑚
ℎ 𝑚(𝒙𝑡) = 𝑤𝑚
𝑠(𝒙𝑡) ∙𝑅𝑊𝑚= 𝑤𝑚
We adopt the maximum matching method as reasoning
method: an input pattern is classified into the class
corresponding to the rule with the maximum association
degree calculated for the pattern. In the case of tie, we
randomly classify the pattern. The association degree for rule
Rm is computed as:
ℎ̅𝑚(𝒙𝑡)+ℎ 𝑚(𝒙𝑡)
Once fixed the number Tf of IT2 fuzzy sets for each
linguistic variable, we adopt an MOEA-based approach to
learn rules and membership function parameters so as to
generate a set of IT2 FRBCs with different trade-offs between
accuracy and rule base complexity.
III. THE PROPOSED THREE OBJECTIVE EVOLUTIONARY
OPTIMIZATION OF IT2 FUZZY RULE-BASED CLASSIFIERS
MOEAs have been applied in several different domains to
search for optimal solutions to problems characterized by
multiple performance criteria in competition with each other
 . MOEAs do not generate a unique solution, but rather a set
of equally valid solutions, where each solution tends to fulfill a
criterion to a higher extent than another. Comparison between
different solutions is performed by using the notion of Pareto
dominance. A solution x, associated with a performance vector
u, dominates a solution y, associated with a performance vector
v, if and only if, ∀ 𝑖∈ {1, …, I}, with I the number of criteria,
ui performs better than, or equal to, vi and ∃𝑖∈ {1, …, I}, such
that ui performs better than vi, where ui and vi are the i-th
elements of vectors u and v, respectively. The set of solutions,
which are not dominated by any other possible solution, is
denoted as Pareto front. The objective of any MOEA is,
therefore, to search for a set of solutions that are a good
approximation of the Pareto front. In the last years, in
designing fuzzy rule-based systems, developers have not only
considered accuracy, but also interpretability as a crucial
requirement. Since accuracy and interpretability are objectives
in competition with each other, MOEAs have been so
extensively applied that the term multi-objective evolutionary
fuzzy system has been coined to identify fuzzy rule-based
systems generated by MOEAs . While the
accuracy objective has been typically measured in terms of
classification rate and approximation error for, respectively,
classification and regression problems, a number of specific
evaluating
interpretability, taking the rule base complexity and the data
base integrity into account . A large number of
contributions have been recently published under the
framework of multi-objective evolutionary fuzzy systems, with
application mostly to regression - and classification
 - problems. Recently, some taxonomies of the main
contributions have been also introduced in .
In this paper, we extend PAES-RCS, a multi-objective
evolutionary fuzzy system that has been recently proposed by
some of the authors of this paper in . PAES-RCS has
proved to be very effective and efficient in classification
problems . The original PAES-RCS learns concurrently the
rule and data bases of type-1 FRBCs by exploiting the RCS
approach, which selects a reduced number of rules from a
heuristically generated rule base and a reduced number of
conditions for each selected rule during the evolutionary
process. Thus, RCS can be considered a sort of rule learning in
a search space constrained by the heuristically generated rule
base. The membership function parameters of the type-1 fuzzy
sets are learned concurrently to the application of RCS. This
requires an appropriate chromosome coding and properly
defined mating operators. In particular, chromosome C consists
of two parts (CRB, CDB), which define the rule base and the
membership function parameters of the input variables,
respectively. Both crossover and mutation operators are applied
to each part of the chromosome independently. The objectives
used in PAES-RCS are classification rates and complexity
measured in terms of the total number of antecedent conditions
of the rules in the rule base.
In this paper, we extend PAES-RCS along three directions.
First of all, we employ IT2 fuzzy sets rather than type-1 fuzzy
sets. This has required the adoption of a different inference
mechanism. Second, in order to cope with unbalanced datasets,
we split the accuracy into two objectives, namely True Positive
Rate (TPR) and False Positive Rate (FPR). We recall that TPR
and FPR coincide, respectively, with the sensitivity and the
complement to 1 of the specificity. As experimented in 
and using rule learning, this approach allows achieving
high accuracies when dealing with unbalanced datasets without
needing to re-balance the dataset. Third, we use an approach
denoted as scaled dominance, which was introduced in -
 , to handle unbalanced data by trying to give minority
classes a fair chance when competing with a majority class.
This improvement further contributes to manage unbalanced
In the following subsections, we will discuss the method to
generate the initial rule base and summarize the RCS approach
and the membership function parameter learning used in IT2-
A. The initial rule base generation
We generate the initial rule base by first transforming each
continuous variable into a categorical and ordered variable.
Then, we apply the well-known C4.5 algorithm to the
transformed dataset for generating a decision tree. Finally, we
extract the initial rule base from the decision tree.
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
More precisely, for each continuous variable Xf, first we
generate an IT2 fuzzy partition 𝑃̃f = {𝐴̃𝑓,1, , … , 𝐴̃𝑓,𝑇𝑓} of Tf IT2
fuzzy sets as shown in Fig. 1. The number Tf of IT2 fuzzy sets
can be different from an input variable to another. For the sake
of simplicity, in our experiments, we have used the same
number of IT2 fuzzy sets for all the variables Xf. Then, we
compute the α-cut, with α=0.5, of the fuzzy sets defined by the
upper membership functions 𝜇̅𝐴̃𝑓,𝑗 of the IT2 fuzzy sets 𝐴̃𝑓,𝑗,
j=1,…, Tf. The corresponding contiguous intervals, shown in
Fig. 2, are used to discretize the universe Uf of each variable Xf
before applying the C4.5 algorithm. For simplicity, we will
denote the intervals with the index of the corresponding IT2
fuzzy set, which the α-cut is applied to. For instance, interval 1
denotes the interval corresponding to the α-cut of the fuzzy set
defined by 𝜇̅𝐴̃𝑓,1. Then, each input value of the input-output
pairs in the training set is replaced by the interval, which
contains it. Thus, the overall training set is transformed so as to
contain exclusively categorical values. Finally, we apply the
classical C4.5 algorithm to the transformed training set. We
extract the initial rule base from the decision tree generated by
the C4.5 algorithm. Rules are extracted from each path from
the root to a leaf node. Each splitting criterion along a given
path is logically ANDed to form the rule antecedent (“IF”
part). The leaf node holds the class prediction, forming the rule
consequent (“THEN” part). Since each branch is identified by
one of the intervals determined by the discretization process
and an input variable is involved in just one node in a path, the
rules extracted from the decision tree are expressed as in (2).
Each rule is identified by an integer from 1 to MC45, where
MC45 is the number of rules extracted from the tree and
included in the initial rule base.
Fig. 2. Discretization of the universe Uf based on an IT2 fuzzy partition (the
thick and thin lines represent the upper and lower membership functions,
respectively; the dashed lines denote the boundaries of the intervals generated
by the α-cut).
Figure 3 shows an example of a decision tree generated by
the C4.5 algorithm from a training set characterized by six
input variables and two classes (C1, C2). Each input variable Xf
, f = 1,…, 6, has been partitioned with Tf = 5 fuzzy sets. We
observe that only three out of the six original input variables
are included in the decision tree. This is due to the well-known
characteristic of the C4.5 algorithm that can select features
during the generation of the tree. Figure 4 shows the rule base
extracted from the decision tree of Figure 3. We note that the
rule base consists of thirteen rules, which correspond to the
thirteen possible paths from the root to the leaf nodes.
Figure 3. An example of decision tree generated by the C4.5 algorithm applied
to the transformed training set.
A THEN Y is
A THEN Y is
A THEN Y is
Figure 4. The fuzzy rule base extracted from the decision tree shown in Fig. 3.
B. Rule and condition selection
The CRB part of the chromosome is a vector of Mmax pairs pm
= (km,vm), where km identifies the index of the rule in the set of
MC45 rules extracted from the decision tree, and vm = [vm,1, …,
vm,F] is a binary vector, which indicates, for each condition in
the rule, if the condition is present (vm,f = 1) or corresponds to a
“don’t care” (vm,f = 0). Rule bases generated by the C4.5
algorithm could include a high number of rules, especially
when dealing with large and high dimensional training sets.
With the aim of obtaining compact and interpretable FRBCs,
we have set an upper bound Mmax to the number of rules that
can be contained in any rule base generated during the
evolutionary process. In the experiments, we have set Mmax =
50. In our previous works , we have verified that this value
permits us to generate FRBCs with reasonable accuracy,
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
maintaining the complexity at an adequate level. Let MC45 be
the number of rules extracted from the decision tree. If MC45 <
Mmax, then Mmax = MC45. During the evolutionary process, the
MOEA can generate rule bases, which contain a number of
rules lower than Mmax. Indeed, if km = 0, then the mth rule is not
included in the rule base. Further, the number of conditions can
be lower than the number F of features. Indeed, if vm,f = 0, then
the f th condition of the mth rule is replaced by a “don’t care”
condition and, therefore, is not considered in the inference
process. Whenever a condition selection is performed on the
rule, the rule weight associated with the rule is re-computed.
As an example, given a two input fuzzy model, let us assume
that the C4.5 algorithm has generated the following four rules:
R1: IF X1 is 𝐴̃1,1 and X2 is 𝐴̃2,1 THEN Y is 𝐶1
R2: IF X1 is 𝐴̃1,2 and X2 is 𝐴̃2,2 THEN Y is 𝐶2
R3: IF X1 is 𝐴̃1,5 and X2 is 𝐴̃2,3 THEN Y is 𝐶1
R4: IF X2 is 𝐴̃2,1 THEN Y is 𝐶1
Let us suppose that, during the evolutionary process
executed with Mmax = 3, the CRB chromosome part shown in
Figure 5 is generated.
Figure 5. An example of the CRB part of a chromosome.
The first gene of the chromosome selects rule R2 (k1 is equal
to 2) with all the conditions (both v1,1 and v1,2 are equal to 1).
The second gene selects rule R3 (k2 is equal to 3) with only the
first condition (v2,1 is equal to 1, while v2,2 is equal to 0). The
third gene selects no rule (k3 is equal to 0).
The rule base corresponding to the chromosome in Fig. 5
will therefore be:
R2: IF X1 is 𝐴̃1,2 and X2 is 𝐴̃2,2 THEN Y is 𝐶2
R3: IF X1 is 𝐴̃1,5 THEN Y is 𝐶1
We note that, even though Mmax = 3, only two rules have
been selected in the final rule base. Furthermore, for the third
rule, only the first condition has been selected.
The CDB part of the chromosome codifies the upper
membership functions of each variable Xf. Since the lower
membership functions are built, as described in Section II,
from the upper membership functions, the CDB part codifies
exclusively these functions. Since we adopt strong fuzzy
partitions for defining the upper membership functions with,
for j = 2, …, Tf - 1, 𝑏𝑓,𝑗 = 𝑐𝑓,𝑗−1 and 𝑏𝑓,𝑗 = 𝑎𝑓,𝑗+1, each
triangular fuzzy set (𝑎𝑓,𝑗, 𝑏𝑓,𝑗, 𝑐𝑓,𝑗) of the partition is
completely defined by fixing the positions of the cores 𝑏𝑓,𝑗
along the universe Uf of the f th variable (we normalize each
variable in ). Since 𝑏𝑓,1 and 𝑏𝑓,𝑇𝑓 coincide with the lower
and upper extremes of universe Uf, the partition of each
linguistic variable Xf is completely defined by Tf - 2 parameters
{𝑏𝑓,2, … , 𝑏𝑓,𝑇𝑓−1 }, which define the positions of the cores of
the upper membership functions defined on Xf. As shown in
Fig. 6, the CDB chromosome part, therefore, consists of F
vectors of Tf - 2 real numbers. A good level of integrity, in
terms of order, coverage and distinguishability, of the partitions
is ensured by, ∀𝑗∈[2, 𝑇𝑓−1], forcing 𝑏𝑓,𝑗 to vary in the
definition interval [𝑏𝑓,𝑗−
𝑏𝑓,𝑗−𝑏𝑓,𝑗−1
𝑏𝑓,𝑗+1−𝑏𝑓,𝑗
Figure 6. The CDB part of a chromosome.
C. The genetic operators
Both crossover and mutation operators are employed to
generate the offspring population. In particular, we apply the
one-point crossover to the CRB part and the BLX-α crossover,
with α = 0.5, to the CDB part. In applying the one-point
crossover, the common gene between the two mating
chromosomes s1 and s2 is determined by extracting randomly a
number in [1, 𝜌𝑀𝐴𝑋], where 𝜌𝑀𝐴𝑋 is the maximum number of
rules in s1 and s2. The crossover point is always chosen
between two rules and not within a rule.
As regards mutation, two operators are applied to the CRB
part. Both the operators randomly choose a pair pm, i.e. a rule,
in the chromosome. Then, the first operator replaces the rule in
pm with another rule by setting km to an integer value randomly
generated in [1, 𝑀𝐶45]. The second operator modifies the rule
in pm by complementing each gene 𝑣𝑚,𝑓 with a probability
equal to 𝑃𝑐𝑜𝑛𝑑 (𝑃𝑐𝑜𝑛𝑑= 2 𝑓
⁄ in the experiments).
The mutation operator applied to CDB, first, randomly
chooses an input variable 𝑋𝑓, 𝑓∈[1, 𝐹], and a fuzzy set 𝑗∈
[2, 𝑇𝑓−1] and then replaces the value of 𝑏𝑓,𝑗 with a value
randomly chosen within the definition interval of 𝑏𝑓,𝑗.
If, after applying the crossover, the rule base contains one or
more pairs of equal rules, we simply eliminate one of the rules
from each pair setting the corresponding km to zero.
D. Multi-objective evolutionary algorithm
The MOEA used in this paper is the (2+2)M-PAES
algorithm proposed in and adopted in . The
application scheme of the crossover and mutation operators
employed in (2+2)M-PAES for generating the offspring
solutions o1 and o2 from the current solutions s1 and s2 is shown
in Figure 7. Here, PCRB, PCDB, PMRB1 and P MRB2 represent the
probabilities of applying the crossover operators to CRB and
CDB parts and the first and the second mutation operators to
CRB, respectively. PMDB represents the probability of applying
the mutation operator to CDB. Unlike classical (2+2)PAES,
which maintains the current solutions s1 and s2 until they are
not replaced by solutions with particular characteristics, we
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
observe that in (2+2)M-PAES s1 and s2 are randomly extracted
at each iteration.
At the beginning, we generate two current solutions s1 and
s2. While the genes of the CDB part and the km values of the CRB
part of s1 and s2 are randomly generated, all the values vm,f of
the conditions of all the rules are set to 1. An offspring solution
ox is added to the archive only if it is dominated by no solution
contained in the archive; possible solutions in the archive
dominated by ox are removed. If the archive is full and no
solution in the archive can be removed, then the offspring
solution ox is inserted into the archive and the solutions
(possibly ox itself), which belong to the region with the highest
crowding degree, are removed. If the region contains more than
one solution, then, the solution to be removed is randomly
chosen. (2+2)M-PAES concurrently optimizes three objectives,
namely false positive rate (FPR), true positive rate (TPR) and
complexity. The complexity is measured as the sum of the
conditions, which compose the antecedents of the rules in the
rule base. This number is denoted as total rule length (TRL).
Low values of TRL correspond to rule bases characterized by a
low number of rules and a low number of input variables really
used in each rule.
//Generate two new solutions
[s1, s2] = random_selection(archive)
if (rand() <
[o1.CRB,o2.CRB] = crossover_CRB(s1.CRB,s2.CRB);
if (rand() <
[o1.CDB,o2.CDB] = crossover_CDB(s1.CDB,s2.CDB);
loop i=1,2
if (rand() <
oi.CRB = first_mutation_operator(oi.CRB);
if (rand() <
oi.CRB = second_mutation_operator(oi.CRB);
if (rand() <
oi.CDB = DB_mutation_operator(oi.CDB);
Figure 7. Application scheme of the genetic operators.
IV. EXPERIMENTS AND RESULTS
We analysed eleven financial datasets. For each dataset, we
performed a ten-fold cross-validation and executed three trials
for each fold with different seeds for the random function
generator (30 trials in total). We fixed 50,000 evaluations as
stopping criteria.
In the following, we first describe the financial datasets.
Then, we show the results obtained by IT2-PAES-RCS, PAES-
RCS, PAES-RCS-SD, FARC-HD, FURIA, C4.5 and its costsensitive version C4.5-CS. Finally, we analyse the results along
accuracy and interpretability dimensions.
A. The Financial Datasets
In financial applications, as in many real-world problems,
the data is highly unbalanced. For example, in a credit card
application, the number of good customers is much higher than
that of bad customers; in fraud detection, the majority of the
data are normal transactions whereas a few fraudulent
transactions are usually present. Most classifiers designed for
minimizing the global error rate perform poorly on unbalanced
datasets because they misclassify most of the data belonging to
the class represented by few examples. Hence, in our
experiments, in order to evaluate the proposed system for
various financial applications, we have chosen eleven datasets
with various sizes and different levels of imbalance ratios
between the minority and majority classes. The chosen datasets
cover different financial applications, including credit card and
loan authorization, stock market related predictions, insurance,
fraud detection and investment banking.
We have used eleven real-world datasets from various
financial domains. Table I summarizes the main characteristics
of these datasets. For each dataset, we report the name, the
number of instances (#Instances), the number of attributes
(#Attributes), and the imbalance ratio (IR). We recall that IR is
defined as the ratio between the number of instances of the
majority class and of the minority class. The datasets are sorted
for increasing IRs. We do not show the number of classes
because all the datasets represent two classes problems.
FINANCIAL DATASETS USED IN THE EXPERIMENTS
(SORTED FOR INCREASING IRS)
#Instances
#Attributes
In the following, we shortly describe each financial dataset.
 BLA: the dataset is related to the prediction of good
(profitable) or bad (non-profitable) customers for bank
loan authorization.
 CARD: the dataset is used to evaluate if a customer is
going to default on a credit card or no.
 AF: the dataset is related to investment banking and is used
to predict if customers are going to pay back their loans or
if they will default on the given loan.
 ARB: the dataset is used for spotting arbitrage
opportunities in the London International Financial Futures
Exchange (LIFFE) market. The dataset was developed in
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
 - to identify arbitrage situations by analyzing option
and futures prices in the LIFFE market.
 COMM: the dataset is used for the evaluation of customers
(Fraud or No Fraud customer) for commercial loans
applications.
 SL: the dataset is used for the evaluation of customers
(good or bad customers) for personal small loans
applications where there is no knowledge on the customer
full credit history.
 LEN: the dataset is used for evaluation of small companies
(good or bad customer) for business loans applications
when the customer full credit history is known.
 DPKG: the dataset is used to predict whether in an auction,
the customer will be real or fraud.
 BAN: the dataset is used to predict if a customer is eligible
for increasing the credit limits on her/his credit cards.
 GIV: the dataset is used to predict whether an applicant is
eligible to give her/him extra credit on her/his existing loan
 COI: the dataset is used to predict whether a customer will
buy a caravan insurance or not.
B. The Classifiers
In this section, we shortly describe the classifiers applied to
the financial datasets. IT2-PAES-RCS was widely discussed in
Section III. The PAES-RCS algorithm used in this paper is
slightly different from the original version. Indeed, to manage
unbalanced datasets, we use three objectives as in IT2-PAES-
RCS, but generate type-1 FRBCs. PAES-RCS-SD is the
version of PAES-RCS with three objectives and with the scaled
domain approach.
FARC-HD (Fuzzy Association Rule-Based Classification
Model for High Dimensional Datasets) was introduced in 
and is a single objective evolutionary fuzzy classifier, which
exploits association rules mining for generating FRBCs.
FARC-HD is based on three stages. First, it mines all possible
fuzzy association rules building a search tree to list all frequent
fuzzy item sets, limiting the depth of the branches in order to
find a small number of short fuzzy rules. Second, it uses a
pattern weighting scheme to reduce the number of candidate
rules, preselecting the most interesting rules, in order to
decrease the computational costs for the third step. Finally, a
single-objective genetic algorithm, namely CHC, is used to
select and tune a compact set of fuzzy association rules.
FRBCs generated by FARC-HD use the certainty factor and
the additive combination as rule weight and reasoning
method, respectively.
FURIA (Fuzzy Unordered Rules Induction Algorithm) is an
extension of the RIPPER algorithm . Given a classification
problem with K classes, prior to the learning process, RIPPER
sorts the training data by class label in ascending order
according to the corresponding class frequencies. Then, rules
are learned for the first K − 1 classes, starting with the least
frequent. Once a rule has been generated, the instances covered
by that rule are removed from the training data, and this is
repeated until no instance from the target class is left. The
algorithm then proceeds with the next class. Finally, when
RIPPER finds no more rules to learn, a default rule (with
empty antecedent) is added for the last (and hence most
frequent) class. To learn each rule the training set is split into a
growing set and a pruning set: the former is used to specialize
the rule by adding antecedents, while the latter is used to
generalize the rule by removing antecedents. FURIA extends
RIPPER along three directions: i) the use of fuzzy rather than
crisp rules, employing fuzzy intervals with trapezoidal
membership functions instead of crisp intervals, ii) the
exploitation of unordered rather than ordered rule sets, and iii)
the introduction of a novel rule stretching method in order to
manage uncovered examples.
C4.5 builds decision trees from a set of training data using
the concept of information entropy. At each node of the tree,
the C4.5 algorithm chooses one attribute of the training set that
most effectively splits its set of samples into subsets enriched
in one class or the other. The splitting criterion is the
normalized information gain that results from choosing an
attribute for splitting the data. The attribute with the highest
normalized information gain is chosen to make the decision.
The cost-sensitive version of C4.5, denoted as C4.5-CS,
exploits an instance weighting method similar to the one
adopted in the boosting decision tree approach developed by
Quinlan . C4.5-CS changes the class distribution so that the
induced tree is in favour of the class with high weight/cost.
Thus, this version of the C4.5 is less likely to commit errors
with high costs.
Before applying FARC-HD, FURIA and C4.5, the datasets
pre-processed
Oversampling Technique (SMOTE) . In SMOTE, the
minority class is oversampled by taking each minority class
sample and introducing synthetic examples along the line
segments joining any or all of the k minority class nearest
neighbours. Depending upon the amount of oversampling
required, neighbours from the k-nearest neighbours are
randomly chosen.
Table II shows the parameters used for IT2-PAES-RCS,
PAES-RCS and PAES-RCS-SD. The values of the parameters
come, on the one side, from the long experience we maturated
in the application of (2+2)M-PAES for generating fuzzy rulebased systems since our initial paper on this subject . On
the other side, we performed a number of experiments with
different values of these parameters using the datasets in Table
I and realized that the parameters in Table II are effective also
for these datasets. For the other algorithms, we adopted the
implementation in Keel and the default parameters.
C. Analysis of the results
The execution of IT2-PAES-RCS, PAES-RCS and PAES-
RCS-SD generates a set of solutions with different trade-offs
among the three objectives. At the end of each execution of the
algorithms, we verified that the archive of (2+2)M-PAES is
always full for each dataset in Table I. Thus, each execution of
the three algorithms generates 128 different FRBCs. In order to
analyse the results of IT2-PAES-RCS, PAES-RCS and PAES-
RCS-SD, each three-dimensional Pareto front approximation is
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
projected onto the FPR-TPR plane: each FRBC of the Pareto
front approximation is therefore represented as a point
corresponding to the pair (FPR, TPR). We recall that one
classifier in the FPR-TPR plane is better than (dominates)
another if it is located more north-west (higher TPR and/or
lower FPR) than the other . For this reason, in order to
select a set of potentially optimal FRBCs, we extract the nondominated solutions obtained on the training set in the FPR-
TPR plane. Since we do not assume to use any cost function
for selecting a single optimal classifier, we consider all the
non-dominated solutions in the FPR-TPR plane. With the aim
of comparing the outputs of the three multi-objective
evolutionary approaches among them and with the other
algorithms, for each non-dominated solution, we calculate the
Area under the Curve (AUC), defined as 𝐴𝑈𝐶=
100+𝑇𝑃𝑅−𝐹𝑃𝑅
and select the solution with the highest AUC on the training
set. The highest AUC identifies the most north-west solution in
the FPR-TPR plane. Thus, for each comparison algorithm, we
consider just one classifier and compare these classifiers in
terms of AUC computed on the test set.
VALUES OF THE PARAMETERS USED IN THE EXPERIMENTS FOR IT2-PAES-
RCS, PAES-RCS AND PAES-RCS-SD
(2+2)M-PAES archive size
Number of fuzzy sets for each variable Xf, f=1,…, F
Maximum number of rules in a rule base
Probability of applying the crossover operator to
Probability of applying the crossover operator to
Probability of applying the first mutation operator to
Probability of applying the second mutation operator to
Probability of applying the mutation operator to
Table III shows, for each dataset, the average AUC, FPR and
TPR on both the training and the test sets, the average number
of rules and the average TRL for the classifiers with the highest
AUC on the training set generated by IT2-PAES-RCS, PAES-
RCS and PAES-RCS-SD, and for the classifiers generated by
FARC-HD, FURIA, C4.5 and C4.5-CS. For each dataset, we
have shown in bold the best values. We can observe that C4.5
and C4.5-CS suffer very much from overtraining. Indeed, the
value of the AUC is very high on the training set, but is quite
low on the test set. Although it is less evident than for C4.5 and
C4.5-CS, also FURIA suffers from overtraining: the AUC
computed on the test set is at least for some datasets much
lower than on the training set. IT2-PAES-RCS, PAES-RCS
and PAES-RCS-SD do not suffer from overtraining and show
similar performance, thus testifying the validity of the three
objective approach.
To statistically verify these observations, we apply nonparametric statistical tests for multiple comparisons. First, for
each approach, we generate a distribution consisting of the
average values of the AUCs on the test set. Then, we apply the
Friedman test in order to compute a ranking among the
distributions , and the Iman and Davenport test to
evaluate whether there exist statistically relevant differences
among the distributions. If there exists a statistical difference,
we apply a post-hoc procedure, namely the Holm test .
This test allows detecting effective statistical differences
between the control approach, i.e. the one with the lowest
Friedman rank, and the remaining approaches.
Table IV shows the results of the non-parametric statistical
tests: for each algorithm, we show the Friedman rank and the
Iman and Davenport p-value. If the p-value is lower than the
level of significance α (in the experiments α = 0.05), we can
reject the null hypothesis and affirm that there exist statistical
differences between the multiple distributions associated with
each approach. Otherwise, no statistical difference exists
among the distributions and therefore the solutions are
statistically equivalent. We observe that the Iman and
Davenport statistical hypothesis of equivalence is rejected and
so statistical differences among the six approaches are detected.
Thus, we have to apply the Holm post-hoc procedure
considering
PAES-RCS-SD
(associated with the lowest rank and in bold in the Table). In
the part of the table corresponding to the results obtained by the
application of the Holm post-hoc procedure, the algorithms are
sorted by decreasing Friedman ranks. Index i denotes the
position of the algorithm in the sorted list (i = 1 and i = 6
correspond to the lowest and highest Friedman ranks,
respectively). The Holm post-hoc procedure computes the zvalues and p-values shown in the table: if the p-value of the
algorithm in position i is lower than the adjusted α value (α / i),
then the null hypothesis is rejected.
The Holm post-hoc procedure states that the AUCs on the
test set of IT2-PAES-RCS and PAES-RCS are statistically
equivalent to the AUC of PAES-RCS-SD. The null hypothesis
is rejected for all the other algorithms. Thus, we can conclude
that the three versions of PAES-RCS with three objectives
obtain classifiers, which outperform the ones obtained by the
other approaches in terms of AUCs. Also, this result is
obtained without rebalancing the datasets. Further, if we
analyze the Friedman ranks, we realize that the two algorithms
with the highest ranks are just PAES-RCS-SD and IT2-PAES-
RCS. Further, both PAES-RCS-SD and IT2-PAES-RCS obtain
this result with classifiers characterized by a low number of
rules. To verify this observation, we have also applied the nonparametric statistical tests for multiple comparisons to the
number of rules and to the TRL values.
Tables V and VI show the results. Since the null hypothesis
is rejected for both the tests, we apply the Holm post-hoc
procedure by using IT2-PAES-RCS as control algorithm. The
procedure states that, in terms of average number of rules (see
Table V), the classifiers generated by PAES-RCS, PAES-RCS-
SD and FURIA are statistically equivalent to the ones
generated by IT2-PAES-RCS. On the contrary, the null
hypothesis is rejected for FARC-HD, C4.5 and C4.5-CS. As
regards TRL, the Holm post-hoc procedure concludes that the
most accurate classifiers generated by IT2-PAES-RCS result to
be characterized by an average TRL value statistically
equivalent to the most accurate classifiers generated by PAES-
RCS and to the classifiers generated by FURIA. On the
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
contrary, the null hypothesis is rejected for PAES-RCS-SD,
FARC-HD, C4.5 and C4.5-CS.
Among the classifiers used for comparison, only FURIA
shows a complexity comparable to the three versions of PAES-
RCS. We have to highlight, however, that the interpretability of
the classifiers generated by FURIA is limited by the
membership functions computed by the method. Indeed, these
membership functions are hardly describable using linguistic
terms. On the contrary, thanks to the constraints imposed on
the membership function learning during the evolutionary
process, the partitions generated by IT2-PAES-RCS, PAES-
RCS and PAES-RCS-SD can be easily described by linguistic
terms. Just to provide a glimpse of this interpretability, we
consider one of the datasets in Table I, namely ARB. Table VII
describes in detail the meaning of the attributes for the ARB
dataset. We recall that the output here is spotting arbitrage
opportunities in the LIFFE market.
AVERAGE AUC, FPR AND TPR ON BOTH THE TRAINING AND THE TEST SETS, AVERAGE TRL AND NUMBER OF RULES FOR THE CLASSIFIERS WITH THE HIGHEST
AUC ON THE TRAINING SET GENERATED BY IT2-PAES-RCS, PAES-RCS, PAES-RCS-SD, AND FOR THE CLASSIFIERS GENERATED BY FARC-HD, FURIA, C4.5
AND C4.5-CS
IT2-PAES-RCS
PAES-RCS-SD
IT2-PAES-RCS
PAES-RCS-SD
IT2-PAES-RCS
PAES-RCS-SD
IT2-PAES-RCS
PAES-RCS-SD
IT2-PAES-RCS
PAES-RCS-SD
IT2-PAES-RCS
PAES-RCS-SD
IT2-PAES-RCS
PAES-RCS-SD
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
IT2-PAES-RCS
PAES-RCS-SD
IT2-PAES-RCS
PAES-RCS-SD
IT2-PAES-RCS
PAES-RCS-SD
IT2-PAES-RCS
PAES-RCS-SD
RESULTS OF THE NON-PARAMETRIC STATISTICAL TESTS ON THE AUC
COMPUTED ON THE TEST SET AMONG THE CLASSIFIERS WITH THE HIGHEST
AUC ON THE TRAINING SET GENERATED BY IT2-PAES-RCS, PAES-RCS AND
PAES-RCS-SD, AND THE CLASSIFIERS GENERATED BY FARC-HD, FURIA,
C4.5 AND C4.5-CS
Iman and Davenport
Hypothesis
PAES-RCS-SD
IT2-PAES-RCS
Holm post-hoc procedure
Hypothesis
Not Rejected
IT2-PAES-RCS
Not Rejected
Fig. 8 shows an example of partitions generated by IT2-
PAES-RCS for one of the classifiers with the highest AUC on
the training set for the ARB. Here, only 6 out of 7 attributes are
shown since one of the attributes was not used in the final rule
base. We can observe that, although the evolutionary process
has tuned the IT2 fuzzy sets on the specific dataset, the
partitions of the different attributes result to be easily
interpretable.
RESULTS OF THE NON-PARAMETRIC STATISTICAL TESTS ON THE NUMBER OF
RULES AMONG THE CLASSIFIERS WITH THE HIGHEST AUC ON THE TRAINING
SET GENERATED BY IT2-PAES-RCS, PAES-RCS AND PAES-RCS-SD, AND
THE CLASSIFIERS GENERATED BY FARC-HD, FURIA, C4.5 AND C4.5-CS
Iman and Davenport
Hypothesis
IT2-PAES-RCS
PAES-RCS-SD
Holm post-hoc Procedure
Hypothesis
PAES-RCS-SD
Not Rejected
Not Rejected
Not Rejected
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
As regards the interpretability of the rules, Fig. 9 shows the
rule base of the classifier whose data base is shown in Fig. 8.
Here, we do not show the “don’t care” conditions since they do
not contribute to the inference process and penalize the
interpretability of the rule base. The expert can deduce
interesting knowledge from the rules of the classifier. Indeed,
he/she can, for instance, discover that intermediate values of C-
P (C-P is M) lead to conclude that the class is Arbitrage
Opportunity. On the other hand, very high values of Futures
(Futures is VH) allow inferring that the class is non Arbitrage
Opportunity.
RESULTS OF THE NON-PARAMETRIC STATISTICAL TESTS ON THE TRL AMONG
THE CLASSIFIERS WITH THE HIGHEST AUC ON THE TRAINING SET GENERATED
BY IT2-PAES-RCS, PAES-RCS AND PAES-RCS-SD, AND THE CLASSIFIERS
GENERATED BY FARC-HD, FURIA, C4.5 AND C4.5-CS
Iman and Davenport
Hypothesis
IT2-PAES-RCS
PAES-RCS-SD
Holm post-hoc Procedure
Hypothesis
PAES-RCS-SD
Not Rejected
Not Rejected
The non-parametric statistical tests for multiple comparisons
have shown that the classifiers generated by IT2-PAES-RCS,
PAES-RCS and PAES-RCS-SD achieve similar AUC on the
test set and have similar complexity, at least in terms of
number of rules. We observe however that IT2-PAES-RCS is
characterized by the minimum Friedman rank in both Table V
and Table VI. Thus, we decided to perform a statistical analysis
between IT2-PAES-RCS and each of the other two approaches
separately. We applied the Wilcoxon signed-rank test for
pairwise comparison , considering IT2-PAES-RCS as
control algorithm, to the distributions of AUCs calculated on
the test set, average number of rules and average TRL.
MEANING OF THE ATTRIBUTES OF THE ARB DATASET
Description
Strike Price/Underlying Index Level
Basis % (x10000)
Futures price minus spot index level, divided by
futures price, multiplied by 10,000
Spot index level divided by futures price, multiplied
Interest Ask %
The LIBOR ask rate for the maturity closest to the
maturity of futures contract, multiplied by 100
Futures (T-t)
The nave trigger, profit after transaction costs,
divided by futures price, multiplied by 1,000,000
C-P % (x100)
The difference between the call and the put prices,
divided by futures price
Profit after TC (x
1,000,000)
The nave trigger, profit after transaction costs,
divided by futures price, multiplied by 1,000,000
Table VIII shows the results of the test. The null hypothesis
is not rejected for the AUC computed on the test set, but is
rejected for the average number of rules and average TRL. We
can conclude that IT2-PAES-RCS generated classifiers that
achieve the same accuracy in terms of AUC as PAES-RCS and
PAES-RCS-SD, but with a lower number of rules and a lower
TRL. Thus, the classifiers generated by IT2-PAES-RCS result
to be less complex and therefore more interpretable.
Figure 8. An example of partitions generated by IT2-PAES-RCS for one of the classifiers with the highest AUC on the training set for the dataset ARB.
> PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <
R1: IF C-P is VL and ProfitAfterTC is VL THEN Class Y is Non Arbitrage Opportunity
R2: IF Basis is M and InterestAsk is L and Futures is L and ProfitAfterTC is L THEN Class Y is Non Arbitrage Opportunity
R3: IF Basis is H and C-P is L THEN Class Y is Non Arbitrage Opportunity
R4: IF InterestAsk is M and Futures is L THEN Class Y is Non Arbitrage Opportunity
R5: IF Futures is VH THEN Class Y is Non Arbitrage Opportunity
R6: IF ProfitAfterTC is L THEN Class Y is Non Arbitrage Opportunity
R7: IF Basis is M and Futures is VL THEN Class Y is Arbitrage Opportunity
R8: IF InterestAsk is M and Futures is L and C-P is L and ProfitAfterTC is VL THEN Class Y is Arbitrage Opportunity
R9: IF Futures is L and C-P is M THEN Class Y is Arbitrage Opportunity
R10: IF MoneyNess is L and C-P is M THEN Class Y is Arbitrage Opportunity
R11: IF MoneyNess is M and Futures is L and C-P is M and ProfitAfterTC is VL THEN Class Y is Arbitrage Opportunity
Figure 9. The rule base of the classifier whose data base is shown in Fig. 8.
TABLE VIII
RESULTS OF THE WILCOXON SIGNED-RANK TEST ON AUC, TRL AND NUMBER
OF RULES AMONG THE CLASSIFIERS WITH THE HIGHEST AUC ON THE TRAINING
SET GENERATED BY IT2-PAES-RCS, PAES-RCS AND PAES-RCS-SD
Hypothesis
(alpha=0.05)
IT2-PAES-RCS vs. PAES-RCS
Not Rejected
IT2-PAES-RCS vs. PAES-RCS-SD 15.5
Not Rejected
Hypothesis
(alpha=0.05)
IT2-PAES-RCS vs. PAES-RCS
IT2-PAES-RCS vs. PAES-RCS-SD 63.0
Hypothesis
(alpha=0.05)
IT2-PAES-RCS vs. PAES-RCS
IT2-PAES-RCS vs. PAES-RCS-SD 63.0
V. CONCLUSIONS
Financial data
are often strongly unbalanced and
characterized by a high level of uncertainty. In this paper, we
have proposed to deal with financial data classification by
adopting rule-based classifiers generated by a multi-objective
evolutionary algorithm (MOEA). These classifiers have
proved to be very effective in terms of accuracy. Further, they
are generally characterized by a low number of rules and total
rule length, and a good integrity of the partitions, thus making
them very interpretable. Interpretability is considered essential
in the financial context since the comprehension of how inputs
and output are related to each other is crucial to take both
operative and strategic decisions.
We have extended PAES-RCS, an MOEA-based approach
to learn concurrently the rule and data bases of fuzzy rulebased classifiers. In order to cope with unbalanced datasets, we
have split the accuracy into two objectives, namely True
Positive Rate and False Positive Rate, and we have used an
approach denoted as scaled dominance to give minority classes
a fair chance when competing with a majority class. Further,
we have coped with uncertainty by adopting IT2 fuzzy sets
rather than type-1 fuzzy sets. This has required using a
different inference mechanism. We have tested the three
improvements on eleven financial datasets and compared the
results with the ones obtained by the fuzzy rule-based
classifiers FARC-HD and FURIA, the classical C4.5 decision
tree algorithm and its version cost-sensitive. Using nonparametric statistical tests, we have shown that the three
improvements allow generating classifiers, which outperform
the comparison approaches both in terms of accuracy,
computed as area under the curve, and complexity, computed
as number of rules. Finally, the extension of PAES-RCS,
which integrates the three improvements, has proved to
achieve high accuracy with, on average, the lowest number of
rules and total rule length.