The Annals of Statistics
2005, Vol. 33, No. 4, 1538–1579
DOI 10.1214/009053605000000255
© Institute of Mathematical Statistics, 2005
BOOSTING WITH EARLY STOPPING: CONVERGENCE
AND CONSISTENCY
BY TONG ZHANG AND BIN YU1
IBM T. J. Watson Research Center and University of California at Berkeley
Boosting is one of the most signiﬁcant advances in machine learning
for classiﬁcation and regression. In its original and computationally ﬂexible
version, boosting seeks to minimize empirically a loss function in a greedy
fashion. The resulting estimator takes an additive function form and is built
iteratively by applying a base estimator (or learner) to updated samples
depending on the previous iterations. An unusual regularization technique,
early stopping, is employed based on CV or a test set.
This paper studies numerical convergence, consistency and statistical rates
of convergence of boosting with early stopping, when it is carried out over
the linear span of a family of basis functions. For general loss functions, we
prove the convergence of boosting’s greedy optimization to the inﬁnimum
of the loss function over the linear span. Using the numerical convergence
result, we ﬁnd early-stopping strategies under which boosting is shown to
be consistent based on i.i.d. samples, and we obtain bounds on the rates of
convergence for boosting estimators. Simulation studies are also presented
to illustrate the relevance of our theoretical results for providing insights to
practical aspects of boosting.
As a side product, these results also reveal the importance of restricting the
greedy search step-sizes, as known in practice through the work of Friedman
and others. Moreover, our results lead to a rigorous proof that for a linearly
separable problem, AdaBoost with ε →0 step-size becomes an L1-margin
maximizer when left to run to convergence.
1. Introduction.
In this paper we consider boosting algorithms for classiﬁcation and regression. These algorithms represent one of the major advances in
machine learning. In their original version, the computational aspect is explicitly
speciﬁed as part of the estimator/algorithm. That is, the empirical minimization of
an appropriate loss function is carried out in a greedy fashion, which means that
at each step a basis function that leads to the largest reduction of empirical risk
is added into the estimator. This speciﬁcation distinguishes boosting from other
statistical procedures which are deﬁned by an empirical minimization of a loss
function without the numerical optimization details.
Received January 2003; revised September 2004.
1Supported in part by NSF Grant FD01-12731 and ARO Grant DAAD19-01-1-0643.
AMS 2000 subject classiﬁcations. 62G05, 62G08.
Key words and phrases. Boosting, greedy optimization, matching pursuit, early stopping, consistency.
BOOSTING WITH EARLY STOPPING
Boosting algorithms construct composite estimators using often simple base estimators through the greedy ﬁtting procedure. An unusual regularization technique,
early stopping, is employed based on CV or a test set. This family of algorithms
has been known as the stagewise ﬁtting of additive models in the statistics literature . For the squared loss function, they were often referred to in the signal
processing community as matching pursuit . More recently, it was noticed that
the AdaBoost method proposed in the machine learning community can also
be regarded as stagewise ﬁtting of additive models under an exponential loss function . In this paper we use the term boosting to indicate a greedy
stagewise procedure to minimize a certain loss function empirically. The abstract
formulation will be presented in Section 2.
Boosting procedures have drawn much attention in the machine learning
community as well as in the statistics community, due to their superior empirical
performance for classiﬁcation problems. In fact, boosted decision trees are
generally regarded as the best off-the-shelf classiﬁcation algorithms we have today.
In spite of the signiﬁcant practical interest in boosting, a number of theoretical
issues have not been fully addressed in the literature. In this paper we hope to
ﬁll some gaps by addressing three basic issues regarding boosting: its numerical
convergence when the greedy iteration increases, in Section 4.1; its consistency
(after early stopping) when the training sample size gets large, in Sections
3.3 and 5.2; and bounds on the rate of convergence for boosting estimators, in
Sections 3.3 and 5.3.
It is now well known that boosting forever can overﬁt the data (e.g., see ). Therefore, in order to achieve consistency, it is necessary to stop the
boosting procedure early (but not too early) to avoid overﬁtting. In the early
stopping framework, the consistency of boosting procedures has been considered
by Jiang for exponential loss boosting (but the consistency is in terms of
the classiﬁcation loss) and Bühlmann under squared loss for tree-type base
classiﬁers. Jiang’s approach also requires some smoothness conditions on the
underlying distribution, and it is nonconstructive (hence does not lead to an
implementable early-stopping strategy). In Sections 3.3 and 5.2 we present an
early-stopping strategy for general loss functions that guarantees consistency.
A different method of achieving consistency (and obtaining rate of convergence
results) is through restricting the weights of the composite estimator using the
1-norm of its coefﬁcients (with respect to the basis functions). For example, this
point of view is taken up in . In this framework, early stopping is
not necessary since the degree of overﬁtting or regularization is controlled by
the 1-norm of the weights of the composite estimator. Although this approach
simpliﬁes the theoretical analysis, it also introduces an additional control quantity
which needs to be adjusted based on the data. Therefore, in order to select an
optimal regularization parameter, one has to solve many different optimization
problems, each with a regularization parameter. Moreover, if there are an inﬁnite
(or extremely large) number of basis functions, then it is not possible to solve the
T. ZHANG AND B. YU
associated 1-norm regularization problem. Note that in this case greedy boosting
(with approximate optimization) can still be applied.
A question related to consistency and rate of convergence is the convergence
of the boosting procedure as an optimization method. This is clearly one of the
most fundamental theoretical issues for boosting algorithms. Previous studies
have focused on special loss functions. Speciﬁcally, Mallat and Zhang proved the
convergence of matching pursuit in , which was then used in to study
consistency; in Breiman obtained an inﬁnite-sample convergence result of
boosting with the exponential loss function for ±1-trees (under some smoothness
assumptions on the underlying distribution), and the result was used by Jiang to
study the consistency of AdaBoost. In a Bregman divergence-based analysis
was given. A convergence result was also obtained in for a gradient descent
version of boosting.
None of these studies provides any information on the numerical speed of
convergence for the optimization. The question of numerical speed of convergence
has been studied when one works with the 1-norm regularized version of boosting
where we assume that the optimization is performed in the convex hull of the
basis functions. Speciﬁcally, for function estimation under least-squares loss, the
convergence of the greedy algorithm in the convex hull was studied in . For general loss functions, the convergence of greedy algorithms (again,
the optimization is restricted to the convex hull) was recently studied in . In
this paper we apply the same underlying idea to the standard boosting procedure
where we do not limit the optimization to the convex hull of the basis functions.
The resulting bound provides information on the speed of convergence for the
optimization. An interesting observation of our analysis is the important role of
small step-size in the convergence of boosting procedures. This provides some
theoretical justiﬁcation for Friedman’s empirical observation that using small
step-sizes almost always helps in boosting procedures.
Moreover, the combination of numerical convergence results with modern
empirical process bounds (based on Rademacher complexity) provides a way to
derive bounds on the convergence rates of early-stopping boosting procedures.
These results can be found in Sections 3.3 and 5.3. Section 6 contains a simulation
study to show the usefulness of the insights from our theoretical analyses in
practical implementations of boosting. The proofs of the two main results in
the numerical convergence section (Section 4.1) are deferred to Section A.2.
Section A.3 discusses relaxations of the restricted step-size condition used for
earlier results, and Section A.4 uses numerical convergence results to give a
rigorous proof of the fact that for separable problems, AdaBoost with small stepsize becomes an L1 margin maximizer at its limit (see ).
2. Abstract boosting procedure.
We now describe the basics to deﬁne the
boosting procedure that we will analyze in this paper. A similar setup can be found
BOOSTING WITH EARLY STOPPING
in . The main difference is that the authors in use a gradient descent rule
in their boosting procedure while here we use approximate minimization.
Let S be a set of real-valued functions and deﬁne
wjf j :f j ∈S,wj ∈R,m ∈Z+
which forms a linear function space. For all f ∈span(S), we can deﬁne the 1-norm
with respect to the basis S as
∥f ∥1 = inf
wjf j :f j ∈S,m ∈Z+
We want to ﬁnd a function
¯f ∈span(S) that approximately solves the
optimization problem
f ∈span(S)A(f ),
where A is a convex function of f deﬁned on span(S). Note that the optimal value
may not be achieved by any f ∈span(S), and for certain formulations (such as
AdaBoost) it is possible that the optimal value is not ﬁnite. Both cases are still
covered by our results, however.
The abstract form of the greedy boosting procedure (with restricted step-size)
considered in this paper is given by the following algorithm:
ALGORITHM 2.1 (Greedy boosting).
Pick f0 ∈span(S)
for k = 0,1,2,...
Select a closed subset k ⊂R such that 0 ∈k and k = −k
Find ¯αk ∈k and ¯gk ∈S to approximately minimize the function:
(αk,gk) →A(fk + αkgk)
Let fk+1 = fk + ¯αk ¯gk
REMARK 2.1.
The approximate minimization of (∗) in Algorithm 2.1 should
be interpreted as ﬁnding ¯αk ∈k and ¯gk ∈S such that
A(fk + ¯αk ¯gk) ≤
αk∈k,gk∈S A(fk + αkgk) + εk,
where εk ≥0 is a sequence of nonnegative numbers that converges to zero.
REMARK 2.2.
The requirement that 0 ∈k is not crucial in our analysis. It
is used as a convenient assumption in the proof of Lemma 4.1 to simplify the
conditions. Our convergence analysis allows the choice of k to depend on the
previous steps of the algorithm. However, the most interesting k for the purpose
of this paper will be independent of previous steps of the algorithm:
T. ZHANG AND B. YU
(a) k = R,
(b) supk = ˜hk where ˜hk ≥0 and ˜hk →0.
As we will see later, the restriction of αk to the subset k ⊂R is useful in the
convergence analysis.
As we shall see later, the step-size ¯αk plays an important role in our analysis.
A particular interesting case is to restrict the step-size explicitly. That is, we assume
that the starting point f0, as well as quantities εk and k in (3), are sampleindependent, and hk = supk satisﬁes the conditions
The reason for this condition will become clear in the numerical convergence
analysis of Section 4.1.
3. Assumptions and main statistical results.
The purpose of this section
is to state assumptions needed for the analyses to follow, as well as the main
statistical results. There are two main aspects of our analysis. The ﬁrst is the
numerical convergence of the boosting algorithm as the number of iterations
increases, and the second is the statistical convergence of the resulting boosting
estimator, so as to avoid overﬁtting. We list respective assumptions separately. The
statistical consistency result can be obtained by combining these two aspects.
3.1. Assumptions for the numerical convergence analysis.
For all f ∈span(S)
and g ∈S, we deﬁne a real-valued function Af,g(·) as
Af,g(h) = A(f + hg).
DEFINITION 3.1.
Let A(f ) be a function of f deﬁned on span(S). Denote by
span(S)′ the dual space of span(S) [i.e., the space of real-valued linear functionals
on span(S)]. We say that A is differentiable with gradient ∇A ∈span(S)′ if
it satisﬁes the following Fréchet-like differentiability condition for all f,g ∈
A(f + hg) −A(f )
 = ∇A(f )T g,
where ∇A(f )T g denotes the value of the linear functional ∇A(f ) at g. Note that
we adopt the notation f T g from linear algebra, where it is just the scalar product
of the two vectors.
For reference, we shall state the following assumption, which is required in our
BOOSTING WITH EARLY STOPPING
ASSUMPTION 3.1.
Let A(f ) be a convex function of f deﬁned on span(S),
which satisﬁes the following conditions:
1. The functional A is differentiable with gradient ∇A.
2. For all f ∈span(S) and g ∈S, the real-valued function Af,g is second-order
differentiable (as a function of h) and the second derivative satisﬁes
f,g(0) ≤M(∥f ∥1),
where M(·) is a nondecreasing real-valued function.
REMARK 3.1.
A more general form of (5) is A′′
f,g(0) ≤ℓ(g)M(∥f ∥1), where
ℓ(g) is an appropriate scaling factor of g. For example, in the examples given
below, ℓ(g) can be measured by supx |g(x)| or EXg(X)2. In (5) we assume
that functions in S are properly scaled so that ℓ(g) ≤1. This is for notational
convenience only. With more complicated notation techniques developed in
this paper can also handle the general case directly without any normalization
assumption of the basis functions.
The function M(·) will appear in the convergence analysis in Section 4.1.
Although our analysis can handle unbounded M(·), the most interesting boosting
examples have bounded M(·) (as we will show shortly). In this case we will also
use M to denote a real-valued upper bound of supa M(a).
For statistical estimation problems such as classiﬁcation and regression with a
covariate or predictor variable X and a real response variable Y having a joint
distribution, we are interested in the following form of A(f ) in (2):
where φ(·,·) is a loss function that is convex in its ﬁrst argument and ψ is
a monotonic increasing auxiliary function which is introduced so that A(f ) is
convex and M(·) behaves nicely (e.g., bounded). We note that the introduction of ψ
is for proving numerical convergence results using our proof techniques, which
are needed for proving statistical consistency of boosting with early stopping.
However, ψ is not necessary for the actual implementation of the boosting
procedure. Clearly the minimizer of (6) that solves (2) does not depend on the
choice of ψ. Moreover, the behavior of Algorithm 2.1 is not affected by the choice
of ψ as long as εk in (3) is appropriately redeﬁned. We may thus always take
ψ(u) = u, but choosing other auxiliary functions can be convenient for certain
problems in our analysis since the resulting formulation has a bounded M(·)
function (see the examples given below). We have also used EX,Y to indicate the
expectation with respect to the joint distribution of (X,Y).
When not explicitly speciﬁed, EX,Y can denote the expectation either with
respect to the underlying population or with respect to the empirical samples. This
makes no difference as far as our convergence analysis in Section 4.1 is concerned.
T. ZHANG AND B. YU
When it is necessary to distinguish an empirical quantity from its population
counterpart, we shall denote the former by a hat above the corresponding quantity.
For example, ˆE denotes the expectation with respect to the empirical samples,
and ˆA is the function in (6) with EX,Y replaced by ˆEX,Y . This distinction will
become necessary in the uniform convergence analysis of Section 4.2.
An important application of boosting is binary classiﬁcation. In this case it is
very natural for us to use a set of basis functions that satisfy the conditions
|g(x)| ≤1,
For certain loss functions (such as least squares) this condition can be relaxed. In
the classiﬁcation literature φ(f,y) usually has a form φ(fy).
Commonly used loss functions are listed in Section A.1. They show that
for a typical boosting loss function φ, there exists a constant M such that
supa M(a) ≤M.
3.2. Assumptions for the statistical convergence analysis.
In classiﬁcation or
regression problems with a covariate or predictor variable X on Rd and a real
response variable Y, we observe m i.i.d. samples Zm
1 = {(X1,Y1),...,(Xm,Ym)}
from an unknown underlying distribution D. Consider a loss function φ(f,y) and
deﬁne Q(f ) (true risk) and ˆQ(f ) (empirical risk) as
Q(f ) = EDφ
ˆQ(f ) = ˆEφ
f (Xi),Yi
where ED is the expectation over the unknown true joint distribution D of (X,Y)
(denoted by EX,Y previously);
ˆE is the empirical expectation based on the
Boosting estimators are constructed by applying Algorithm 2.1 with respect to
the empirical expectation ˆE with a set S of real-valued basis functions g(x). We
use ˆA(f ) to denote the empirical objective function,
ˆA(f ) = ψ
Similarly, quantities fk, αk and gk in Algorithm 2.1 will be replaced by ˆfk, ˆαk
and ˆgk, respectively.
Techniques from modern empirical process theory can be used to analyze the
statistical convergence of a boosting estimator with a ﬁnite sample. In particular,
we use the concept of Rademacher complexity, which is given by the following
deﬁnition.
DEFINITION 3.2.
Let G = {g(x,y)} be a set of functions of input (x,y).
i=1 be a sequence of binary random variables such that σi = ±1 with
BOOSTING WITH EARLY STOPPING
probability 1/2. The (one-sided) sample-dependent Rademacher complexity of G
is given by
1 ) = Eσ sup
σig(Xi,Yi),
and the expected Rademacher complexity of G is denoted by
Rm(G) = EZm
The Rademacher complexity approach for analyzing boosting algorithms ﬁrst
appeared in , and it has been used by various people to analyze learning
problems, including boosting; for example, see . The analysis using
Rademacher complexity as deﬁned above can be applied both to regression and
to classiﬁcation. However, for notational simplicity we focus only on boosting
methods for classiﬁcation, where we impose the following assumption. This
assumption is not essential to our analysis, but it simpliﬁes the calculations and
some of the ﬁnal conditions.
ASSUMPTION 3.2.
We consider the following form of φ in (8): φ(f,y) =
φ(fy) with a convex function φ(a):R →R such that φ(−a) > φ(a) for all a > 0.
Moreover, we assume that
(i) Condition (7) holds.
(ii) S in Algorithm 2.1 is closed under negation (i.e., f ∈S →−f ∈S).
(iii) There exists a ﬁnite Lipschitz constant γφ(β) of φ in [−β,β]:
∀|f1|,|f2| ≤β
|φ(f1) −φ(f2)| ≤γφ(β)|f1 −f2|.
The Lipschitz condition of a loss function is usually easy to estimate. For
reference, we list γφ for loss functions considered in Section A.1:
(a) Logistic regression φ(f ) = ln(1 + exp(−f )):γφ(β) ≤1.
(b) Exponential φ(f ) = exp(−f ):γφ(β) ≤exp(β).
(c) Least squares φ(f ) = (f −1)2 :γφ(β) ≤2(β + 1).
(d) Modiﬁed least squares φ(f ) = max(1 −f,0)2 :γφ(β) ≤2(β + 1).
(e) p-norm φ(f ) = |f −1|p(p ≥2):γφ(β) ≤p(β + 1)p−1.
3.3. Main statistical results.
We may now state the main statistical results
based on the assumptions and deﬁnitions given earlier. The following theorem
gives conditions for our boosting algorithm so that consistency can be achieved in
the large sample limit. The proof is deferred to Section 5.2, with some auxiliary
T. ZHANG AND B. YU
THEOREM 3.1.
Under Assumption 3.2 let φ be one of the loss functions
considered in Section A.1. Assume further that in Algorithm 2.1 we choose
quantities f0, εk and k to be independent of the sample Zm
1 , such that
j=0 εj < ∞, and hk = supk satisﬁes (4).
Consider two sequences of sample independent numbers km and βm such that
limm→∞km = ∞and limm→∞γφ(βm)βmRm(S) = 0. Then as long as we stop
Algorithm 2.1 at a step ˆk based on Zm
1 such that ˆk ≥km and ∥ˆfˆk∥1 ≤βm, we have
the consistency result
1 Q( ˆfˆk) =
f ∈span(S)Q(f ).
REMARK 3.2.
The choice of (km,βm) in the above theorem should not
be void, in the sense that for all samples Zm
it should be possible to stop
Algorithm 2.1 at a point such that the conditions ˆk ≥km and ∥ˆfˆk∥1 ≤βm are
In particular, if limm→∞Rm(S) = 0, then we can always ﬁnd km ≤k′
m such that
km →∞and γφ(βm)βmRm(S) →0 with βm = ∥f0∥1 + k′m
j=0 hj. This choice of
(km,βm) is valid as we can stop the algorithm at any ˆk ∈[km,k′
Similar to the consistency result, we may further obtain some rate of convergence results. This work does not focus on rate of convergence analysis, and results
we obtain are not necessarily tight. Before stating a more general and more complicated result, we ﬁrst present a version for constant step-size logistic boosting,
which is much easier to understand.
THEOREM 3.2.
Consider the logistic regression loss function, with basis S
which satisﬁes Rm(S) ≤CS
√m for some positive constant CS. For each sample
size m, consider Algorithm 2.1 with f0 = 0, supk k = h0(m) ≤1/√m and
εk ≤h0(m)2/2. Assume that we run boosting for k(m) = βm/h0(m) steps. Then
1 Q( ˆfˆk) ≤
¯f ∈span(S)
Q( ¯f ) + (2CS + 1)βm
+ ∥¯f ∥1 + 1
∥¯f ∥1 + βm
Note that the condition Rm(S) ≤CS/√m is satisﬁed for many basis function classes, such as two-level neural networks and tree basis functions (see
Section 4.3). The bound in Theorem 3.2 is independent of h0(m) [as long as
h0(m) ≤m−1/2]. Although this bound is likely to be suboptimal for practice problems, it does give a worst case guarantee for boosting with the greedy optimization aspect taken into consideration. Assume that there exists ¯f ∈span(S) such
that Q( ¯f ) = inff ∈span Q(f ). Then we may choose βm as βm = O(∥¯f ∥1/2
which gives a convergence rate of EZm
1 Q( ˆfˆk) ≤Q( ¯f ) + O(∥¯f ∥1/2
1 m−1/4). As the
target complexity ∥¯f ∥1 increases, the convergence becomes slower. An example
is provided in Section 6 to illustrate this phenomenon.
BOOSTING WITH EARLY STOPPING
We now state the more general result, on which Theorem 3.2 is based (see
Section 5.3).
THEOREM 3.3.
Under Assumption 3.2, let φ(f ) ≥0 be a loss function such
that A(f ) satisﬁes Assumption 3.1 with the choice ψ(a) = a. Given a sample
size m, we pick a positive nonincreasing sequence {hk} which may depend on m.
Consider Algorithm 2.1 with f0 = 0, supk k = hk and εk ≤h2
kM(sk+1)/2, where
Given training data, suppose we run boosting for ˆk = k(m) steps, and let
βm = sk(m). Then ∀¯f ∈span(S) such that Q( ¯f ) ≤Q(0)
1 Q( ˆfˆk) ≤Q( ¯f ) + 2γφ(βm)βmRm(S)
√mφ(−∥¯f ∥1) + ∥¯f ∥1φ(0)
∥¯f ∥1 + βm
+ δm(∥¯f ∥1),
δm(∥¯f ∥1) =
 sℓ+ ∥¯f ∥1
βm + ∥¯f ∥1
βm + hk(m)
If the target function is ¯f which belongs to span(S), then Theorem 3.3 can
be directly interpreted as a rate of convergence result. However, the expression
of δm may still be quite complicated. For speciﬁc loss function and step-size
choices, the bound can be simpliﬁed. For example, the result for logistic boosting
in Theorem 3.2 follows easily from the theorem (see Section 5.3).
4. Preparatory results.
As discussed earlier, it is well known by now that
boosting can overﬁt if left to run until convergence. In Section 3.3 we stated our
main results that with appropriately chosen stopping rules and under regularity
conditions, results of consistency and rates of convergence can be obtained. In this
section we begin the proof process of these main results by proving the necessary
preparatory results, which are interesting in their own right, especially those on
numerical convergence of boosting in Section 4.1.
Suppose that we run Algorithm 2.1 on the sample Zm
1 and stop at step ˆk. By the
triangle inequality and for any ¯f ∈span(S), we have
1 Q( ˆfˆk) −Q( ¯f ) ≤EZm
1 | ˆQ( ˆfˆk) −Q( ˆfˆk)| + EZm
1 | ˆQ( ¯f ) −Q( ¯f )|
1 [ ˆQ( ˆfˆk) −ˆQ( ¯f )].
The middle term is on a ﬁxed ¯f , and thus it has a rate of convergence O(1/√m)
by the CLT. To study the consistency and rates of convergence of boosting with
early stopping, the work lies in dealing with the ﬁrst and third terms in (9). The
third term is on the empirical performance of the boosting algorithm, and thus a
T. ZHANG AND B. YU
numerical convergence analysis is required and hence proved in Section 4.1. Using
modern empirical process theory, in Section 4.2 we upper bound the ﬁrst term in
terms of Rademacher complexity.
We will focus on the loss functions (such as those in Section A.1) which
satisfy Assumption 3.1. In particular, we assume that ψ is a monotonic increasing
function, so that minimizing A(f ) or ˆA(f ) is equivalent to minimizing Q(f )
or ˆQ(f ). The derivation in Section 4.2 works with Q(f ) and ˆQ(f ) directly,
instead of A(f ) and ˆA(f ). The reason is that, unlike our convergence analysis
in Section 4.1, the relatively simple sample complexity analysis presented in
Section 4.2 does not take advantage of ψ.
4.1. Numerical convergence analysis.
Here we consider the numerical convergence behavior of fk obtained from the greedy boosting procedure as k increases. For notational simplicity, we state the convergence results in terms of the
population boosting algorithm, even though they also hold for the empirical boosting algorithm. The proofs of the two main lemmas are deferred to Section A.2.
In our convergence analysis, we will specify convergence bounds in terms
of ∥¯f ∥1 (where
¯f is a reference function) and a sequence of nondecreasing
numbers sk satisfying the following condition: there exist positive numbers hk
|¯αk| ≤hk ∈k
and let sk = ∥f0∥1 +
where {¯αk} are the step-sizes in (3). Note that hk in (10) can be taken as any number
that satisﬁes the above condition, and it can depend on {¯αk} computed by the
boosting algorithm. However, it is often desirable to state a convergence result that
does not depend on the actual boosting outputs (i.e., the actual ¯αk computed). For
such results we may simply ﬁx hk by letting hk = supk. This gives convergence
bounds for the restricted step-size method which we mentioned earlier.
It can be shown (see Section A.2) that even in the worse case, the value
A(fk+1) −A( ¯f ) decreases from A(fk) −A( ¯f ) by a reasonable quantity.
Cascading this analysis leads to a numerical rate or speed of convergence for the
boosting procedure.
The following lemma contains the one-step convergence bound, which is the
key result in our convergence analysis.
LEMMA 4.1.
Assume that A(f ) satisﬁes Assumption 3.1. Consider hk and sk
that satisfy (10). Let ¯f be an arbitrary reference function in span(S), and deﬁne
A(fk) = max
0,A(fk) −A( ¯f )
2 M(sk+1) + εk.
BOOSTING WITH EARLY STOPPING
Then after k steps, the following bound holds for fk+1 obtained from Algorithm 2.1:
sk + ∥¯f ∥1
A(fk) + ¯εk.
Applying Lemma 4.1 repeatedly, we arrive at a convergence bound for the
boosting Algorithm 2.1 as in the following lemma.
LEMMA 4.2.
Under the assumptions of Lemma 4.1, we have
A(fk) ≤∥f0∥1 + ∥¯f ∥1
sk + ∥¯f ∥1
sj + ∥¯f ∥1
sk + ∥¯f ∥1
The above lemma gives a quantitative bound on the convergence of A(fk)
to the value A( ¯f ) of an arbitrary reference function ¯f ∈span(S). We can see
that the numerical convergence speed of A(fk) to A( ¯f ) depends on ∥¯f ∥1 and
the accumulated or total step-size sk. Speciﬁcally, if we choose
¯f such that
A( ¯f ) ≤A(f0), then it follows from the above bound that
A(fk+1) ≤A( ¯f )
1 −s0 + ∥¯f ∥1
sk+1 + ∥¯f ∥1
+ s0 + ∥¯f ∥1
sk+1 + ∥¯f ∥1
sj+1 + ∥¯f ∥1
sk+1 + ∥¯f ∥1
Note that the inequality is automatically satisﬁed when A(fk+1) ≤A( ¯f ).
Clearly, in order to select ¯f to optimize the bound on the right-hand side, we
need to balance a trade-off: we may select ¯f such that A( ¯f ) (and thus the ﬁrst
term) becomes smaller as we increase ∥¯f ∥1; however, the other two terms will
become large when ∥¯f ∥1 increases. This bound also reveals the dependence of
the convergence on the initial value of the algorithm f0: the closer A(f0) gets
to the inﬁnimum of A, the smaller the bound. To our knowledge, this is the ﬁrst
convergence bound for greedy boosting procedures with quantitative numerical
convergence speed information.
Previous analyses, including matching pursuit for least squares , Breiman’s
analysis of the exponential loss, as well as the Bregman divergence bound
in and the analysis of gradient boosting in , were all limiting results
without any information on the numerical speed of convergence. The key
conceptual difference here is that we do not compare to the optimal value directly,
but instead, to the value of an arbitrary ¯f ∈span(S), so that ∥¯f ∥1 can be used to
measure the convergence speed. This approach is also crucial for problems where
A(·) can take −∞as its inﬁnimum, for which a direct comparison will clearly
T. ZHANG AND B. YU
fail (e.g., Breiman’s exponential loss analysis requires smoothness assumptions to
prevent this −∞inﬁnimum value).
A general limiting convergence result follows directly from the above lemma.
THEOREM 4.1.
Assume that ∞
j=0 ¯εj < ∞and ∞
j=0 hj = ∞; then we
have the following optimization convergence result for the greedy boosting
algorithm (2.1):
k→∞A(fk) =
f ∈span(S)A(f ).
The assumptions imply that limk→∞sk = ∞. We can thus construct a
nonnegative integer-valued function k →j(k) ≤k such that limk→∞sj(k)/sk = 0
and limk→∞sj(k) = ∞.
From Lemma 4.2 we obtain for any ﬁxed ¯f ,
A(fk) ≤∥f0∥1 + ∥¯f ∥1
sk + ∥¯f ∥1
sj + ∥¯f ∥1
sk + ∥¯f ∥1
sj + ∥¯f ∥1
sk + ∥¯f ∥1
sj + ∥¯f ∥1
sk + ∥¯f ∥1
≤o(1) + sj(k) + ∥¯f ∥1
sk + ∥¯f ∥1
¯εj−1 = o(1).
Therefore limk→∞max(0,A(fk) −A( ¯f )) = 0. Since our analysis applies to any
¯f ∈span(S), we can choose ¯fj ∈span(S) such that limjA( ¯fj) = inff ∈span(S)A(f ).
Now from limk→∞max(0,A(fk) −A( ¯fj)) = 0, we obtain the theorem.
COROLLARY 4.1.
For loss functions such as those in Section A.1, we have
supa M(a) < ∞. Therefore as long as there exist hj in (10) and εj in (3) such
j=0 hj = ∞, ∞
j < ∞and ∞
j=0 εj < ∞, we have the following
convergence result for the greedy boosting procedure:
k→∞A(fk) =
f ∈span(S)A(f ).
The above results regarding population minimization automatically apply to the
empirical minimization if we assume that the starting point f0, as well as quantities
εk and k in (3), are sample-independent, and the restricted step-size case where
hk = supk satisﬁes the condition (4).
The idea of restricting the step-size when we compute ¯αj was advocated by
Friedman, who discovered empirically that taking small step-size helps . In
our analysis, we can restrict the search region so that Corollary 4.1 is automatically
BOOSTING WITH EARLY STOPPING
satisﬁed. Since we believe this is an important case which applies for general loss
functions, we shall explicitly state the corresponding convergence result below.
COROLLARY 4.2.
Consider a loss function (e.g., those in Section A.1) such
that supa M(a) < +∞. Pick any sequence of positive numbers hj (j ≥0) such
j=0 hj = ∞, ∞
j < ∞. If we choose k in Algorithm 2.1 such that
hk = supk, and εj in (3) such that ∞
j=0 εj < ∞, then
k→∞A(fk) =
f ∈span(S)A(f ).
Note that the above result requires that the step-size hj be small (∞
but also not too small (∞
j=0 hj = ∞). As discussed above, the ﬁrst condition prevents large oscillation. The second condition is needed to ensure that fk can cover
the whole space span(S).
The above convergence results are limiting results that do not carry any
convergence speed information. Although with speciﬁc choices of hk and sk one
may obtain such information from (14), the second term on the right-hand side is
typically quite complicated. It is thus useful to state a simple result for a speciﬁc
choice of hk and sk, which yields more explicit convergence information.
COROLLARY 4.3.
Assume that A(f ) satisﬁes Assumption 3.1. Pick a sequence of nonincreasing positive numbers hj (j ≥0). Suppose we choose k
in Algorithm 2.1 such that hk = supk, and choose εk in (3) such that εk ≤
kM(sk+1)/2. If we start Algorithm 2.1 with f0 = 0, then
sk + ∥¯f ∥1
A(f0) + inf
ℓ(sℓ+ ∥¯f ∥1)
sk + ∥¯f ∥1
0 + (k −ℓ)h2
Using notation of Lemma 4.1, we have ¯εk ≤h2
ℓM(sk+1). Therefore
each summand in the second term on the right-hand size of Lemma 4.2 is no more
ℓM(sk+1) when j > ℓand is no more than h2
0M(sk+1)(sℓ+ ∥¯f ∥1)/(sk +
∥¯f ∥1) when j ≤ℓ. The desired inequality is now a straightforward consequence
Note that similar to the proof of Theorem 4.1, the term (k−ℓ)h2
ℓin Corollary 4.3
can also be replaced by k
j. A special case of Corollary 4.3 is constant
step-size (hk = h0) boosting, which is the original version of restricted step-size
boosting considered by Friedman . This method is simple to apply since there
is only one step-size parameter to choose. Corollary 4.3 shows that boosting with
constant step-size (also referred to as ε-boosting in the literature) converges to
the optimal value in the limit of h0 →0, as long as we choose the number of
iterations k and step-size h0 such that kh0 →∞and kh2
0 →0. To the best of our
T. ZHANG AND B. YU
knowledge, this is the only rigorously stated convergence result for the ε-boosting
method, which justiﬁes why one needs to use a step-size that is as small as possible.
It is also possible to handle sample-dependent choices of k in Algorithm 2.1,
or allow unrestricted step-size (k = R) for certain formulations. However,
the corresponding analysis becomes much more complicated. According to
Friedman , the restricted step-size boosting procedure is preferable in practice.
Therefore we shall not provide a consistency analysis for unrestricted step-size
formulations in this paper; but see Section A.3 for relaxations of the restricted
step-size condition.
In addition to the above convergence results for general boosting algorithms,
Lemma 4.2 has another very useful consequence regarding the limiting behavior
of AdaBoost in the separable classiﬁcation case. It asserts that the inﬁnitely
small step-size version of AdaBoost, in the convergence limit, is an L1 margin
maximizer. This result has been observed through a connection between boosting
with early stopping and L1 constrained boosting (see ). Our analysis gives
a direct and rigorous proof. This result is interesting because it shows that
AdaBoost shares some similarity (in the limit) with support vector machines
(SVMs) whose goal in the separable case is to ﬁnd maximum margin classiﬁers;
the concept of margin has been popularized by Vapnik who used it to analyze
the generalization performance of SVMs. The detailed analysis is provided in
Section A.4.
4.2. Uniform convergence.
There are a number of possible ways to study
the uniform convergence of empirical processes. In this section we use a
relatively simple approach based on Rademacher complexity. Examples with
neural networks and tree-basis (left orthants) functions will be given to illustrate
our analysis.
The Rademacher complexity approach for analyzing boosting algorithms
appeared ﬁrst in . Due to its simplicity and elegance, it has been used and
generalized by many researchers . The approach used here essentially
follows Theorem 1 of , but without concentration results.
From Lemma 4.2 we can see that the convergence of the boosting procedure is
closely related to ∥¯f ∥1 and ∥fk∥1. Therefore it is natural for us to measure the
learning complexity of Algorithm 2.1 based on the 1-norm of the function family
it can approximate at any given step. We shall mention that this analysis is not
necessarily the best approach for obtaining tight learning bounds since the boosting
procedure may effectively search a much smaller space than the function family
measured by the 1-norm ∥fk∥1. However, it is relatively simple, and sufﬁcient for
our purpose of providing an early-stopping strategy to give consistency and some
rate of convergence results.
Given any β > 0, we now would like to estimate the rate of uniform
convergence,
Q(f ) −ˆQ(f )
BOOSTING WITH EARLY STOPPING
where Q and ˆQ are deﬁned in (8).
The concept of Rademacher complexity used in our analysis is given in
Deﬁnition 3.2. For simplicity, our analysis also employs Assumption 3.2. As
mentioned earlier, the conditions are not essential, but rather they simplify the ﬁnal
results. For example, the condition (7) implies that ∀f ∈span(S), |f (x)| ≤∥f ∥1.
It follows that ∀β ≥∥f ∥1, φ(f,y) ≤φ(−β). This inequality, although convenient,
is certainly not essential.
LEMMA 4.3.
Under Assumption 3.2,
 ≤2γφ(β)βRm(S),
where γφ(β) is a Lipschitz constant of φ in [−β,β]:∀|f1|,|f2| ≤β :|φ(f1) −
φ(f2)| ≤γφ(β)|f1 −f2|.
Using the standard symmetrization argument (e.g., see Lemma 2.3.1
of ), we have
:∥f ∥1 ≤β
Now the one-sided Rademacher process comparison result in , Theorem 7,
which is essentially a slightly reﬁned result (with better constant) of the two-sided
version in , Theorem 4.12, implies that
:∥f ∥1 ≤β
 ≤γφ(β)Rm
{f (X):∥f ∥1 ≤β}
Using the simple fact that g = 
i |αi| = 1) implies g ≤max(supi fi,
supi −fi), and that S is closed under negation, it is easy to verify that Rm(S) =
Rm({f ∈span(S):∥f ∥1 ≤1}). Therefore
{f (X):∥f ∥1 ≤β}
 = βRm(S).
Now by combining the three inequalities, we obtain the lemma.
4.3. Estimating Rademacher complexity.
Our uniform convergence result
depends on the Rademacher complexity Rm(S). For many function classes, it
can be estimated directly. In this section we use a relation between Rademacher
complexity and ℓ2-covering numbers from .
Let X = {X1,...,Xm} be a set of points and let Qm be the uniform probability
measure over these points. We deﬁne the ℓ2(Qm) distance between any two
functions f and g as
ℓ2(Qm)(f,g) =
|f (xi) −g(xi)|2
T. ZHANG AND B. YU
Let F be a class of functions. The empirical ℓ2-covering number of F, denoted
by N(ε,F,ℓ2(Qm)), is the minimal number of balls {g :ℓ2(Qm)(g,f ) ≤ε} of
radius ε needed to cover F. The uniform ℓ2 covering number is given by
N2(ε,F,m) = sup
ε,F,ℓ2(Qm)
where the supremum is over all probability distribution Qm over samples of size m.
If F contains 0, then there exists a universal constant C (see Corollary 2.2.8
in ) such that
logN2(ε,F,m)dε
where we assume that the integral on the right-hand side is ﬁnite. Note that for a
function class F with divergent integration value on the right-hand side, the above
inequality can be easily modiﬁed so that we start the integration from a point ε0 > 0
instead of 0. However, the dependency of Rm(F) on m can be slower than 1/√m.
ASSUMPTION 4.1.
F satisﬁes the condition
logN2(ε,F,m)dε < ∞.
A function class F that satisﬁes Assumption 4.1 is also a Donsker class,
for which the central limit theorem holds. In statistics and machine learning,
one often encounters function classes F with ﬁnite VC-dimension, where the
following condition holds (see Theorem 2.6.7 of ) for some constants C and V
independent of m: N2(ε,F,m) ≤C(1/ε)V . Clearly a function class with ﬁnite
VC-dimension satisﬁes Assumption 4.1.
For simplicity, in this paper we assume that S satisﬁes Assumption 4.1. It
follows that
Rm(S) ≤Rm(S ∪{0}) ≤CS
where CS is a constant that depends on S only. This is the condition used in
Theorem 3.2. We give two examples of basis functions that are often used in
practice with boosting.
Two-level neural networks.
We consider two-level neural networks in Rd,
which form the function space span(S) with S given by
S = {σ(wT x + b):w ∈Rd,b ∈R},
where σ(·) is a monotone bounded continuous activation function.
It is well known that S has a ﬁnite VC-dimension, and thus satisﬁes Assumption 4.1. In addition, for any compact subset U ∈Rd, it is also well known that
span(S) is dense in C(U) (see ).
BOOSTING WITH EARLY STOPPING
Tree-basis functions.
Tree-basis (left orthant) functions in Rd are given by the
indicator function of rectangular regions,
(−∞,a1] × ··· × (−∞,ad]
:a1,...,ad ∈R
Similar to two-level neural networks, it is well known that S has a ﬁnite
VC-dimension, and for any compact set U ∈Rd, span(S) is dense in C(U).
In addition to rectangular region basis functions, we may also consider a basis S
consisting of restricted size classiﬁcation and regression trees (disjoint unions of
constant functions on rectangular regions), where we assume that the number of
terminal nodes is no more than a constant V . Such a basis set S also has a ﬁnite
VC-dimension.
5. Consistency and rates of convergence with early stopping.
section we put together the results in the preparatory Section 4 to prove consistency
and some rate of convergence results for Algorithm 2.1 as stated in the main result
Section 3.3. For simplicity we consider only restricted step-size boosting with
relatively simple strategies for choosing step-sizes. According to Friedman ,
the restricted step-size boosting procedure is preferable in practice. Therefore we
shall not provide a consistency analysis for unrestricted step-size formulations in
this paper. Discussions on the relaxation of the step-size condition can be found in
Section A.3.
5.1. General decomposition.
Suppose that we run the boosting algorithm and
stop at an early stopping point ˆk. The quantity ˆk, which is to be speciﬁed in
Section 5.2, may depend on the empirical sample Zm
1 . Suppose also that the
stopping point ˆk is chosen so that the resulting boosting estimator ˆfˆk satisﬁes
1 Q( ˆfˆk) =
f ∈span(S)Q(f ),
where we use EZm
to denote the expectation with respect to the random
1 . Since Q( ˆfˆk) ≥inff ∈span(S) Q(f ), we also have
Q( ˆfˆk) −
f ∈span(S)Q(f )
 = lim
1 Q( ˆfˆk) −
f ∈span(S)Q(f ) = 0.
If we further assume there is a unique f ∗such that
f ∈span(S)Q(f ),
and for any sequence {fm}, Q(fm) →Q(f ∗) implies that fm →f ∗, then since
Q( ˆfˆk) →Q(f ∗) as m →∞, it follows that
in probability,
T. ZHANG AND B. YU
which gives the usual consistency of the boosting estimator with an appropriate
early stopping if the target function f coincides with f ∗. This is the case, for
example, if the regression function f (x) = ED(Y|x) with respect to the true
distribution D is in span(S) or can be approximated arbitrarily close by functions
in span(S).
In the following, we derive a general decomposition needed for proving (18) or
Theorem 3.1 in Section 3.3. Suppose that Assumption 3.2 holds. Then for all ﬁxed
¯f ∈span(S), we have
1 | ˆQ( ¯f ) −Q( ¯f )| ≤
1 | ˆQ( ¯f ) −Q( ¯f )|21/2
mED|φ( ¯f (X)Y) −Q( ¯f )|2
mEDφ( ¯f (X)Y)2
√mφ(−∥¯f ∥1).
Assume that we run Algorithm 2.1 on the sample Zm
1 and stop at step ˆk. If
the stopping point ˆk satisﬁes P(∥ˆfˆk∥1 ≤βm) = 1 for some sample-independent
βm ≥0, then using the uniform convergence estimate in (16), we obtain
1 Q( ˆfˆk) −Q( ¯f )
1 [Q( ˆfˆk) −ˆQ( ˆfˆk)] + EZm
1 [ ˆQ( ¯f ) −Q( ¯f )]
1 [ ˆQ( ˆfˆk) −ˆQ( ¯f )]
≤2γφ(βm)βmRm(S) +
√mφ(−∥¯f ∥1) + sup
[ ˆQ( ˆfˆk) −ˆQ( ¯f )].
5.2. Consistency with restricted step-size boosting.
We consider a relatively
simple early-stopping strategy for restricted step-size boosting, where we take
hk = supk to satisfy (4).
Clearly, in order to prove consistency, we only need to stop at a point such that
∀¯f ∈span(S), all three terms in (19) become nonpositive in the limit m →∞. By
estimating the third term using Lemma 4.2, we obtain the following proof of our
main consistency result (Theorem 3.1).
PROOF OF THEOREM 3.1.
Obviously the assumptions of the theorem imply
that the ﬁrst two terms of (19) automatically converge to zero. In the following, we
only need to show that ∀¯f ∈span(S):supZm
1 max(0, ˆQ( ˆfˆk) −ˆQ( ¯f )) →0 when
From Section A.1 we know that there exists a distribution-independent number
M > 0 such that M(a) < M for all underlying distributions. Therefore for all
BOOSTING WITH EARLY STOPPING
empirical samples Zm
1 , Lemma 4.2 implies that
ˆA( ˆfˆk) ≤∥f0∥1 + ∥¯f ∥1
sˆk + ∥¯f ∥1
sj + ∥¯f ∥1
sˆk + ∥¯f ∥1
ˆA(f ) = max(0, ˆA(f )−ˆA( ¯f )), sk = ∥f0∥1+k−1
i=0 hi and ¯εk = h2
Now using the inequality
ˆA(f0) ≤max(ψ(φ(−∥f0∥1)) −ψ(φ(∥¯f ∥1)),0) =
c( ¯f ) and ˆk ≥km, we obtain
ˆA( ˆfˆk) ≤sup
∥f0∥1 + ∥¯f ∥1
sk + ∥¯f ∥1
sj + ∥¯f ∥1
sk + ∥¯f ∥1
Observe that the right-hand side is independent of the sample Zm
1 . From the
assumptions of the theorem, we have ∞
j=0 ¯εj < ∞and limk→∞sk = ∞. Now
the proof of Theorem 4.1 implies that as km →∞, the right-hand side of (20)
converges to zero. Therefore limm→∞supZm
ˆA( ˆfˆk) = 0.
The following universal consistency result is a straightforward consequence of
Theorem 3.1.
COROLLARY 5.1.
Under the assumptions of Theorem 3.1, for any Borel set
U ⊂Rd, if span(S) is dense in C(U)—the set of continuous functions under the
uniform-norm topology, then for all Borel measure D on U × {−1,1},
1 Q( ˆfˆk) =
f ∈B(U)Q(f ),
where B(U) is the set of Borel measurable functions.
We only need to show inff ∈span(S) Q(f ) = inff ∈B(U) Q(f ). This
follows directly from Theorem 4.1 of .
For binary classiﬁcation problems where y = ±1, given any real-valued
function f , we predict y = 1 if f (x) ≥0 and y = −1 if f (x) < 0. The
classiﬁcation error is the following 0–1 loss function:
 = I[yf (x) ≤0],
where I[E] is the indicator function of the event E, and the expected loss is
L(f ) = EDℓ
The goal of classiﬁcation is to ﬁnd a predictor f to minimize (21). Using the
notation η(x) = P(Y = 1|X = x), it is well known that L∗, the minimum of L(f ),
can be achieved by setting f (x) = 2η(x) −1. Let D be a Borel measure deﬁned
on U × {−1,1}; it is known (e.g., see ) that if Q(f ) →inff ∈B(U) Q(f ),
then L(f ) →L∗. We thus have the following consistency result for binaryclassiﬁcation problems.
T. ZHANG AND B. YU
COROLLARY 5.2.
Under the assumptions of Corollary 5.1, we have
1 L( ˆfˆk) = L∗.
The stopping criterion given in Theorem 3.1 depends on Rm(S). For S
that satisﬁes Assumption 4.1, this can be estimated from (17). The condition
γφ(βm)βmRm(S) →0 in Theorem 3.1 becomes γφ(βm)βm = o(√m). Using the
bounds for γφ(·) in Section 4.2, we obtain the following condition.
ASSUMPTION 5.1.
The sequence βm satisﬁes:
(i) Logistic regression φ(f ) = ln(1 + exp(−f )):βm = o(m1/2).
(ii) Exponential φ(f ) = exp(−f ):βm = o(logm).
(iii) Least squares φ(f ) = (f −1)2 :βm = o(m1/4).
(iv) Modiﬁed least squares φ(f ) = max(0,1 −f )2 :βm = o(m1/4).
(v) p-norm φ(f ) = |f −1|p(p ≥2):βm = o(m1/2p).
We can summarize the above discussion in the following theorem, which applies
to boosted VC-classes such as boosted trees and two-level neural networks.
THEOREM 5.1.
Under Assumption 3.2, let φ be one of the loss functions
considered in Section A.1. Assume further that in Algorithm 2.1 we choose
the quantities f0, εk and k to be independent of the sample Zm
1 , such that
j=0 εj < ∞, and hk = supk satisﬁes (4).
Suppose S satisﬁes Assumption 4.1 and we choose sample-independent
km →∞, such that βm = ∥f0∥1 + km
j=0 hj satisﬁes Assumption 5.1. If we stop
Algorithm 2.1 at step km, then ∥ˆfkm∥1 ≤βm and the following consistency result
f ∈span(S)Q(f ).
Moreover, if span(S) is dense in C(U) for a Borel set U ⊂Rd, then for all Borel
measures D on U × {−1,1}, we have
f ∈B(U)Q(f ),
Note that in the above theorem the stopping criterion km is sample-independent.
However, similar to Theorem 3.1, we may allow other sample-dependent ˆk such
that ∥ˆfˆk∥1 stays within the βm bound. One may be tempted to interpret the
rates of βm. However, since different loss functions approximate the underlying
distribution in different ways, it is not clear that one can rigorously compare them.
Moreover, our analysis is likely to be loose.
BOOSTING WITH EARLY STOPPING
5.3. Some bounds on the rate of convergence.
In addition to consistency, it is
also useful to study statistical rates of convergence of the greedy boosting method
with certain target function classes. Since our analysis is based on the 1-norm of
the target function, the natural function classes we may consider are those that can
be approximated well using a function in span(S) with small 1-norm.
We would like to emphasize that rate results, that have been stated in Theorems
3.2 and 3.3 and are to be proved here, are not necessarily optimal. There are
several reasons for this. First, we relate the numerical behavior of boosting to
1-norm regularization. In reality, this may not always be the best way to analyze
boosting since boosting can be studied using other complexity measures such
as sparsity (e.g., see for some other complexity measures). Second, even
with the 1-norm regularization complexity measure, the numerical convergence
analysis in Section 4.1 may not be tight. This again will adversely affect our ﬁnal
bounds. Third, our uniform convergence analysis, based on the relatively simple
Rademacher complexity, is not necessarily tight. For some problems there are more
sophisticated methods which improve upon our approach here (e.g., see [2–6, 22,
A related point is that bounds we are interested in here are a priori convergence
bounds that are data-independent. In recent years, there has been much interest
in developing data-dependent bounds which are tighter (see references mentioned
above). For example, in our case we may allow β in (16) to depend on the observed
data (rather than simply setting it to be a value based only on the sample size).
This approach, which can tighten the ﬁnal bounds based on observation, is a quite
signiﬁcant recent theoretical advance. However, as mentioned above, there are
other aspects of our analysis that can be loose. Moreover, we are mainly interested
in worst case scenario upper bounds on the convergence behavior of boosting
without looking at the data. Therefore we shall not develop data-dependent bounds
The statistical convergence behavior of the boosting algorithm relies on its
numerical convergence behavior, which can be estimated using (14). Combined
with statistical convergence analysis, we can easily obtain our main rate of
convergence result in Theorem 3.3.
PROOF OF THEOREM 3.3.
From (19) we obtain
1 Q( ˆfˆk) ≤Q( ¯f )+2γφ(βm)βmRm(S)+
√mφ(−∥¯f ∥1)+sup
[ ˆQ( ˆfˆk)−ˆQ( ¯f )].
Now we simply apply Corollary 4.3 to bound the last term. This leads to the desired
The result for logistic regression in Theorem 3.2 follows easily from Theorem 3.3.
T. ZHANG AND B. YU
PROOF OF THEOREM 3.2.
Consider logistic regression loss and constant
step-size boosting, where hk = h0(m). Note that for logistic regression we have
γφ(β) ≤1, M(a) ≤1, φ(−∥¯f ∥1) ≤1+∥¯f ∥1 and φ(0) ≤1. Using these estimates,
we obtain from Theorem 3.3,
1 Q( ˆfˆk) ≤Q( ¯f ) + 2βmRm(S) + ∥¯f ∥1 + 1
∥¯f ∥1 + βm
+ βmh0(m).
Using the estimate of Rm(S) in (17), and letting h0(m) ≤1/√m, we obtain
1 Q( ˆfˆk) ≤Q( ¯f ) + (2CS + 1)βm
+ ∥¯f ∥1 + 1
∥¯f ∥1 + βm
This leads to the claim.
6. Experiments.
The purpose of this section is not to reproduce the large
number of already existing empirical studies on boosting. Although this paper is
theoretical in nature, it is still useful to empirically examine various implications
of our analysis, so that we can verify they have observable consequences. For this
reason our experiments focus mainly on aspects of boosting with early stopping
which have not been addressed in previous studies.
Speciﬁcally, we are interested in testing consistency and various issues of
boosting with early stopping based on our theoretical analysis. As pointed out
in , experimentally testing consistency is a very challenging task. Therefore,
in this section we have to rely on relatively simple synthetic data, for which we can
precisely control the problem and the associated Bayes risk. Such an experimental
setup serves the purpose of illustrating main insights revealed by our theoretical
6.1. Experimental setup.
In order to fully control the data generation mechanism, we shall use simple one-dimensional examples. A similar experimental setup
was also used in to study various theoretical aspects of voting classiﬁcation
Our goal is to predict Y ∈{±1} based on X ∈ . Throughout the experiments, X is uniformly distributed in . We consider the target conditional
probability of the form P(Y = 1|X) = 2{dX}I({dX} ≤0.5) + 2(1 −{dX}) ×
I({dX} > 0.5), where d ≥1 is an integer which controls the complexity of the
target function, and I denotes the set indicator function. We have also used the
notation {z} = z −⌊z⌋to denote the decimal part of a real number z, with the standard notation of ⌊z⌋for the integer part of z. The Bayes error rate of our model is
always 0.25.
Graphically, the target conditional probability contains d triangles. Figure 1
plots such a target for d = 2.
BOOSTING WITH EARLY STOPPING
Target conditional probability for d = 2.
We use one-dimensional stumps of the form I([0,a]) as our basis functions,
where a is a parameter in . They form a complete basis since each interval
indicator function I((a,b]) can be expressed as I([0,b]) −I([0,a]).
There have been a number of experimental studies on the impact of using
different convex loss functions (e.g., see ). Although our theoretical
analysis applies to general loss functions, it is not reﬁned enough to suggest that
any one particular loss is better than another. For this reason, our experimental
study will not include a comprehensive comparison of different loss functions. This
task is better left to dedicated empirical studies (such as some of those mentioned
We will only focus on consequences of our analysis which have not been
well studied empirically. These include various issues related to early stopping
and their impact on the performance of boosting. For this purpose, throughout
the experiments we shall only use the least-squares loss function. In fact, it is
known that this loss function works quite well for many classiﬁcation problems
(see, e.g., ) and has been widely applied to many pattern-recognition
applications. Its simplicity also makes it attractive.
For the least-squares loss, the target function which the boosting procedure
tries to estimate is f∗(x) = 2P(Y = 1|X = x) −1. In our experiments, unless
otherwise noted, we use boosting with restricted step-size, where at each iteration
we limit the step-size to be no larger than hi = (i +1)−2/3. This choice satisﬁes our
numerical convergence requirement, where we need the conditions 
i hi = ∞and
i < ∞. Therefore it also satisﬁes the consistency requirement in Theorem 3.1.
6.2. Early stopping and overﬁtting.
Although it is known that boosting forever
can overﬁt (e.g., see ), it is natural to begin our experiments by graphically
showing the effect of early-stopping on the predictive performance of boosting.
T. ZHANG AND B. YU
Graphs of boosting estimators after k = 32 and 1024 iterations.
We shall use the target conditional probability described earlier with complexity
d = 2, and training sample-size of 100. Figure 2 plots the graphs of estimators
obtained after k = 32 and 1024 boosting iterations. The dotted lines on the
background show the true target function f∗(x) = 2P(Y = 1|X = x). We can see
that after 32 iterations, the boosting estimator, although not perfect, roughly has the
same shape as that of the true target function. However, after 1024 iterations, the
graph appears quite random, implying that the boosting estimator starts to overﬁt
Figure 3 shows the predictive performance of boosting versus the number of
iterations. The need for early stopping is quite apparent in this example. The
excessive classiﬁcation error quantity is deﬁned as the true classiﬁcation error
of the estimator minus the Bayes error (which is 0.25 in our case). Similarly,
the excessive convex loss quantity is deﬁned as the true least-squares loss of the
estimator minus the optimal least-squares loss of the target function f∗(x). Both
excessive classiﬁcation error and convex loss are evaluated through numerical
Predictive performance of boosting as a function of boosting iterations.
BOOSTING WITH EARLY STOPPING
Training error.
integration for a given decision rule. Moreover, as we can see from Figure 4, the
training error continues to decrease as the number of boosting iterations increases,
which eventually leads to overﬁtting of the training data.
6.3. Early stopping and total step-size.
Since our theoretical analysis favors
restricted step-size, a relevant question is what step-size we should choose. We
are not the ﬁrst authors to look into this issue. For example, Friedman and his coauthors suggested using small steps . In fact, they argued that the smaller
the step-size, the better. They performed a number of empirical studies to support
this claim. Therefore we shall not reinvestigate this issue here. Instead, we focus on
a closely related implication of our analysis, which will be useful for the purpose
of reporting experimental results in later sections.
Let ¯αi be the step-size taken by the boosting algorithm at the ith iteration. Our
analysis characterizes the convergence behavior of boosting after the kth step,
not by the number of iterations k itself, but rather by the quantity sk = 
in (10), as long as ¯αi ≤hi ∈i. Although our theorems are stated with the quantity
i≤k hi, instead of 
i≤k ¯αi, it does suggest that in order to compare the behavior
of boosting under different conﬁgurations, it is more natural to use the quantity
i≤k ¯αi (which we shall call total step-size throughout later experiments) as a
measure of stopping point rather than the actual number of boosting iterations.
This concept of total step-size also appeared in .
Figure 5 shows the predictive performance of boosting versus the total stepsize. We use 100 training examples, with the target conditional probability of
complexity d = 3. The unrestricted step-size method uses exact optimization.
Note that for least-squares loss, as explained in Section A.3, the resulting stepsizes will still satisfy our consistency condition 
i < ∞. The restricted
step-size scheme with step-size ≤h employs a constant step-size restriction of
ˆαi ≤h. This experiment shows that the behavior of these different boosting
T. ZHANG AND B. YU
Predictive performance of boosting as a function of total step-size.
methods is quite similar when we measure the performance not by the number of
boosting iterations, but instead by the total step-size. This observation justiﬁes our
theoretical analysis, which uses quantities closely related to the total step-size to
characterize the convergence behavior of boosting methods. Based on this result, in
the next few experiments we shall use the total step-size (instead of the number of
boosting iterations) to compare boosting methods under different conﬁgurations.
6.4. The effect of sample-size on early stopping.
An interesting issue for
boosting with early stopping is how its predictive behavior changes when the
number of samples increases. Although our analysis does not offer a quantitative
characterization, it implies that we should stop later (and the allowable stopping
range becomes wider) when sample size increases. This essentially suggests that
the optimal stopping point in the boosting predictive performance curve will
increase as the sample size increases, and the curve itself becomes ﬂatter. It follows
that when the sample size is relatively large, we should run boosting algorithms for
a longer time, and it is less necessary to do aggressive early stopping.
The above qualitative characterization of the boosting predictive curve also has
important practical consequences. We believe this may be one reason why in many
practical problems it is very difﬁcult for boosting to overﬁt, and practitioners
often observe that the performance of boosting keeps improving as the number
of boosting iterations increases.
Figure 6 shows the effect of sample size on the behavior of the boosting method.
Since our theoretical analysis applies directly to the convergence of the convex
loss (the convergence of classiﬁcation error follows implicitly as a consequence
of convex loss convergence), the phenomenon described above is more apparent
for excessive convex loss curves. The effect on classiﬁcation error is less obvious,
which suggests there is a discrepancy between classiﬁcation error performance and
convex loss minimization performance.
BOOSTING WITH EARLY STOPPING
Predictive performance of boosting at different sample sizes.
6.5. Early stopping and consistency.
In this experiment we demonstrate that
as sample size increases, boosting with early stopping leads to a consistent
estimator with its error rate approaching the optimal Bayes error. Clearly, it is
not possible to prove consistency experimentally, which requires running a sample
size of ∞. We can only use a ﬁnite number of samples to demonstrate a clear trend
that the predictive performance of boosting with early stopping converges to the
Bayes error when the sample size increases. Another main focus of this experiment
is to compare the performance of different early stopping strategies.
Theoretical results in this paper suggest that for least squares loss, we can
achieve consistency as long as we stop at total step-size approximately mρ with
ρ < 1/4, where m is the sample size. We call such an early stopping strategy
the ρ-strategy. Since our theoretical estimate is conservative, we examine the
ρ-strategy both for ρ = 1/6 and for ρ = 1/4. Instead of the theoretically motivated
(and suboptimal) ρ-strategy, in practice one can use cross validation to determine
the stopping point. We use a sample size of one-third the training data to estimate
the optimal stopping total step-size which minimizes the classiﬁcation error on the
validation set, and then use the training data to compute a boosting estimator which
stops at this cross-validation-determined total step-size. This strategy is referred
to as the cross validation strategy. Figure 7 compares the three early stopping
strategies mentioned above. It may not be very surprising to see that the crossvalidation-based method is more reliable. The ρ-strategies, although they perform
less well, also demonstrate a trend of convergence to consistency. We have also
noticed that the cross validation scheme stops later than the ρ-strategies, implying
that our theoretical results impose more restrictive conditions than necessary.
It is also interesting to see how well cross validation ﬁnds the optimal stopping
point. In Figure 8 we compare the cross validation strategy with two oracle
strategies which are not implementable: one selects the optimal stopping point
which minimizes the true classiﬁcation error (which we refer to as optimal error),
and the other selects the optimal stopping point which minimizes the true convex
T. ZHANG AND B. YU
Consistency and early stopping.
loss (which we refer to as optimal convex risk). These two methods can be regarded
as ideal theoretical stopping points for boosting methods. The experiment shows
that cross validation performs quite well at large sample sizes.
In the log coordinate space, the convergence curve of boosting with the cross
validation stopping criterion is approximately a straight line, which implies that the
excess errors decrease as a power of the sample size. By extrapolating this ﬁnding,
it is reasonable for us to believe that boosting with early stopping converges to
the Bayes error in the limit, which veriﬁes the consistency. The two ρ stopping
rules, even though showing much slower linear convergence trend, also lead to
consistency.
6.6. The effect of target function complexity on early stopping.
Although we
know that boosting with an appropriate early stopping strategy leads to a consistent
estimator in the large sample limit, the rate of convergence depends on the
Consistency and early stopping.
BOOSTING WITH EARLY STOPPING
The effect of target function complexity.
complexity of the target function (see Section 5.3). In our analysis the complexity
can be measured by the 1-norm of the target function. For target functions
considered here, it is not very difﬁcult to show that in order to approximate to an
accuracy within ε, it is only necessary to use a combination of our decision stumps
with the 1-norm Cd/ε. In this formula C is a constant and d is the complexity of
the target function.
Our analysis suggests that the convergence behavior of boosting with early
stopping depends on how easy it is to approximate the target function using a
combination of basis functions with small 1-norm. A target with d = u is u-times
as difﬁcult to approximate as a target with d = 1. Therefore the optimal stopping
point, measured by the total step-size, should accordingly increase as d increases.
Moreover, the predictive performance becomes worse. Figure 9 illustrates this
phenomenon with d = 1,3,5 at the sample size of 300. Notice again that since
our analysis applies to the convex risk, this phenomenon is much more apparent
for the excessive convex loss performance than the excessive classiﬁcation error
performance. Clearly this again shows that although by minimizing a convex loss
we indirectly minimize the classiﬁcation error, these two quantities do not behave
identically.
7. Conclusion.
In this paper we have studied a general version of the boosting
procedure given in Algorithm 2.1. The numerical convergence behavior of this
algorithm has been studied using the so-called averaging technique, which was
previously used to analyze greedy algorithms for optimization problems deﬁned
in the convex hull of a set of basis functions. We have derived an estimate
of the numerical convergence speed and established conditions that ensure the
convergence of Algorithm 2.1. Our results generalize those in previous studies,
such as the matching pursuit analysis in and the convergence analysis of
AdaBoost by Breiman .
T. ZHANG AND B. YU
Furthermore, we have studied the learning complexity of boosting algorithms
based on the Rademacher complexity of the basis functions. Together with the
numerical convergence analysis, we have established a general early stopping
criterion for greedy boosting procedures for various loss functions that guarantees
the consistency of the obtained estimator in the large sample limit. For speciﬁc
choices of step-sizes and sample-independent stopping criteria, we have also been
able to establish bounds on the statistical rate of convergence. We would like to
mention that the learning complexity analysis given in this paper is rather crude.
Consequently, the required conditions in our consistency strategy may be more
restrictive than one actually needs.
A number of experiments were presented to study various aspects of boosting
with early stopping. We speciﬁcally focused on issues that have not been covered
by previous studies. These experiments show that various quantities and concepts
revealed by our theoretical analysis lead to observable consequences. This suggests
that our theory can lead to useful insights into practical applications of boosting
algorithms.
A.1. Loss function examples.
We list commonly used loss functions that
satisfy Assumption 3.1. They show that for a typical boosting loss function φ,
there exists a constant M such that supa M(a) ≤M. All loss functions considered
are convex.
A.1.1. Logistic regression.
This is a traditional loss function used in statistics,
which is given by (in natural log form here)
φ(f,y) = ln
1 + exp(−fy)
We assume that the basis functions satisfy the condition
|g(x)| ≤1,
It can be veriﬁed that A(f ) is convex differentiable. We also have
f,g(0) = EX,Y
(1 + exp(f (X)Y))(1 + exp(−f (X)Y)) ≤1
A.1.2. Exponential loss.
This loss function is used in the AdaBoost algorithm,
which is the original boosting procedure for classiﬁcation problems. It is given by
φ(f,y) = exp(−fy),
ψ(u) = lnu.
Again we assume that the basis functions satisfy the condition
|g(x)| ≤1,
BOOSTING WITH EARLY STOPPING
In this case it is also not difﬁcult to verify that A(f ) is convex differentiable. Hence
we also have
f,g(0) = EX,Y g(X)2Y 2 exp(−f (X)Y)
EX,Y exp(−f (X)Y)
−[EX,Y g(X)Y exp(−f (X)Y)]2
[EX,Y exp(−f (X)Y)]2
A.1.3. Least squares.
The least squares formulation has been widely studied
in regression, but can also be applied to classiﬁcation problems .
A greedy boosting-like procedure for least squares was ﬁrst proposed in the
signal processing community, where it was called matching pursuit . The loss
function is given by
φ(f,y) = 1
We impose the following weaker condition on the basis functions:
EXg(X)2 ≤1,
EY Y 2 < ∞.
It is clear that A(f ) is convex differentiable, and the second derivative is bounded
f,g(0) = EXg(X)2 ≤1.
A.1.4. Modiﬁed least squares.
For classiﬁcation problems we may consider
the following modiﬁed version of the least squares loss, which has a better
approximation property :
φ(f,y) = 1
2 max(1 −fy,0)2,
Since this loss is for classiﬁcation problems, we impose the condition
EXg(X)2 ≤1,
It is clear that A(f ) is convex differentiable, and we have the following bound for
the second derivative:
f,g(0) ≤EXg(X)2 ≤1.
A.1.5. p-norm boosting.
p-norm loss can be interesting both for regression
and for classiﬁcation. In this paper we will only consider the case with p ≥2,
φ(f,y) = |f −y|p,
2(p −1)u2/p.
We impose the condition
EX|g(X)|p ≤1,
EY |Y|p < ∞.
T. ZHANG AND B. YU
Now let u = EX,Y |f (X) + hg(X) −Y|p; we have
p −1u(2−p)/pEX,Y g(X)sign
f (X) + hg(X) −Y
× |f (X) + hg(X) −Y|p−1.
Therefore the second derivative can be bounded as
f,g(h) = u(2−p)/pEX,Y g(X)2|f (X) + hg(X) −Y|p−2
p −1u(2−2p)/p
EX,Y g(X)sign
f (X) + hg(X) −Y
|f (X) + hg(X) −Y|p−12
≤u(2−p)/pEX,Y g(X)2|f (X) + hg(X) −Y|p−2
≤u(2−p)/pE2/p
X,Y |g(X)|p E(p−2)/p
|f (X) + hg(X) −Y|p
X,Y |g(X)|p ≤1,
where the second inequality follows from Hölder’s inequality with the duality pair
(p/2,p/(p −2)).
REMARK A.1.
Similar to the least squares case, we can deﬁne the modiﬁed
p-norm loss for classiﬁcation problems. Although the case p ∈(1,2) can be
handled by the proof techniques used in this paper, it requires a modiﬁed analysis
since in this case the corresponding loss function is not second-order differentiable
at zero. See related discussions in . Note that the hinge loss used in support
vector machines cannot be handled directly with our current technique since its
ﬁrst-order derivative is discontinuous. However, one may approximate the hinge
loss with a continuously differentiable function, which can then be analyzed.
A.2. Numerical convergence proofs.
This section contains two proofs for
the numerical convergence analysis section (Section 4.1).
PROOF OF THE ONE-STEP ANALYSIS OR LEMMA 4.1.
Given an arbitrary
ﬁxed reference function ¯f ∈span(S) with the representation
we would like to compare A(fk) to A( ¯f ). Since ¯f is arbitrary, we use such a
comparison to obtain a bound on the numerical convergence rate.
Given any ﬁnite subset S′ ⊂S such that S′ ⊃{ ¯fj}, we can represent
minimally as
BOOSTING WITH EARLY STOPPING
S′ = ¯wj when g = ¯fj for some j, and ¯wg
S′ = 0 when g /∈{ ¯fj}. A quantity
that will appear in our analysis is ∥¯wS′∥1 = 
g∈S′ | ¯wg
S′|. Since ∥¯wS′∥1 = ∥¯w∥1,
without any confusion, we will still denote ¯wS′ by ¯w with the convention that
¯wg = 0 for all g /∈{ ¯fj}.
Given this reference function ¯f , let us consider a representation of fk as a linear
combination of a ﬁnite number of functions Sk ⊂S, where Sk ⊃{ ¯fj} is to be
chosen later. That is, with g indexing an arbitrary function in Sk, we expand fk in
terms of f g
k ’s which are members of Sk with coefﬁcients βg
With this representation, we deﬁne
Wk = ∥¯w −βk∥1 =
Recall that in the statement of the lemma, the convergence bounds are in terms
of ∥¯w∥1 and a sequence of nondecreasing numbers sk, which satisfy the condition
sk = ∥f0∥1 +
|¯αk| ≤hk ∈k,
where hk can be any real number that satisﬁes the above inequality, which may or
may not depend on the actual step-size ¯αk computed by the boosting algorithm.
Using the deﬁnition of 1-norm for ¯f and since f0 ∈span(S), it is clear that for
all ε > 0 we can choose a ﬁnite subset Sk ⊂S, vector βk and vector ¯w such that
k | ≤sk + ε/2,
∥¯w∥1 ≤∥¯f ∥1 + ε/2.
It follows that with appropriate representation, the following inequality holds for
all ε > 0:
Wk ≤sk + ∥¯f ∥1 + ε.
We now proceed to show that even in the worse case, the value A(fk+1)−A( ¯f )
decreases from A(fk) −A( ¯f ) by a reasonable quantity.
The basic idea is to upper bound the minimum of a set of numbers by an
appropriately chosen weighted average of these numbers. This proof technique,
which we shall call “averaging method,” was used in for analysis of
greedy-type algorithms.
For hk that satisﬁes (10), the symmetry of k implies hk sign( ¯wg −βg
Therefore the approximate minimization step (3) implies that for all g ∈Sk, we
A(fk+1) ≤A(fk + hksgg) + εk,
sg = sign( ¯wg −βg
T. ZHANG AND B. YU
Now multiply the above inequality by | ¯wg −βg
k | and sum over g ∈Sk; we obtain
A(fk+1) −εk
k −¯wg|A(fk + hksgg) =: B(hk).
We only need to upper bound B(hk), which in turn gives an upper bound
on A(fk+1).
We recall a simple but important property of a convex function that follows
directly from the deﬁnition of convexity of A(f ) as a function of f :for all f1,f2
A(f2) ≥A(f1) + ∇A(f1)T (f2 −f1).
If A(fk) −A( ¯f ) < 0, then
A(fk) = 0. From 0 ∈k and (3), we obtain
A(fk+1) −A( ¯f ) ≤A(fk) −A( ¯f ) + εk ≤¯εk,
which implies (13). Hence the lemma holds in this case. Therefore in the following,
we assume that A(fk) −A( ¯f ) ≥0.
Using Taylor expansion, we can bound each term on the right-hand side of (25)
A(fk + hksgg) ≤A(fk) + hksg∇A(fk)T g + h2
fk,g(ξhksg).
Since Assumption 3.1 implies that
fk,g(ξhksg) = sup
fk+ξhk,g(0) ≤M(∥fk∥1 + hk),
A(fk + hksgg) ≤A(fk) + hksg∇A(fk)T g + h2
2 M(∥fk∥1 + hk).
Taking a weighted average, we have
k −¯wg|A(fk + hksgg)
A(fk) + ∇A(fk)T hksgg + h2
2 M(∥fk∥1 + hk)
WkA(fk) + hk∇A(fk)T ( ¯f −fk) + h2
WkM(∥fk∥1 + hk)
WkA(fk) + hk[A( ¯f ) −A(fk)] + h2
WkM(∥fk∥1 + hk).
The last inequality follows from (26). Now using (25) and the bound ∥fk∥1 +hk ≤
sk+1, we obtain
A(fk+1) −A( ¯f )
A(fk) −A( ¯f )
2 M(sk+1).
BOOSTING WITH EARLY STOPPING
Now replace
Wk by the right-hand side of (24) with ε →0; we obtain the lemma.
PROOF OF THE MULTISTEP ANALYSIS OR LEMMA 4.2.
Note that for all
1 −sℓ+1 −sℓ
sk+1 + a .
By recursively applying (13) and using the above inequality, we obtain
sℓ+ ∥¯f ∥1
sℓ+ ∥¯f ∥1
s0 + ∥¯f ∥1
sk+1 + ∥¯f ∥1
sj+1 + ∥¯f ∥1
sk+1 + ∥¯f ∥1
A.3. Discussion of step-size.
We have been deriving our results in the case of
restricted step-size in which the crucial small step-size condition is explicit. In this
section we investigate the case of unrestricted step-size under exact minimization,
for which we show that the small step-size condition is actually implicit if the
boosting algorithm converges. The implication is that the consistency (and rate of
convergence) results can be extended to such a case, although the analysis becomes
more complicated.
Let k = R for all k, so that the size of ¯αk in the boosting algorithm is
unrestricted. For simplicity, we will only consider the case that supa M(a) is upper
bounded by a constant M.
Interestingly enough, although the size of ¯αk is not restricted in the boosting
algorithm itself, for certain formulations the inequality 
j < ∞still holds.
Theorem 4.1 can then be applied to show the convergence of such boosting
procedures. For convenience, we will impose the following additional assumption
for the step-size ¯αk in Algorithm 2.1:
A(fk + ¯αk ¯gk) = inf
αk∈R A(fk + αk ¯gk),
which means that given the selected basis function ¯gk, the corresponding ¯αk is
chosen to be the exact minimizer.
T. ZHANG AND B. YU
LEMMA A.1.
Assume that ¯αk satisﬁes (27). If there exists a positive constant
c such that
ξ∈(0,1)A′′
(1−ξ)fk+ξfk+1, ¯gk(0) ≥c,
j ≤2c−1[A(f0) −A(fk+1)].
Since ¯αk minimizes Afk, ¯gk(α), A′
fk, ¯gk(¯αk) = 0. Using Taylor expansion, we obtain
Afk, ¯gk(0) = Afk, ¯gk(¯αk) + 1
fk, ¯gk(ξk ¯αk)¯α2
where ξk ∈(0,1). That is, A(fk) = A(fk+1) + 1
fk, ¯gk(ξk ¯αk)¯α2
k. By assumption,
we have A′′
fk, ¯gk(ξk ¯αk) ≥c. It follows that, ∀j ≥0, ¯α2
j ≤2c−1[A(fj) −A(fj+1)].
We can obtain the lemma by summing from j = 0 to k.
By combining Lemma A.1 and Corollary 4.1, we obtain:
COROLLARY A.1.
Assume that supa M(a) < +∞and εj in (3) satisﬁes
j=0 εj < ∞. Assume also that in Algorithm 2.1 we let k = R and let ¯αk
satisfy (27). If
ξ∈(0,1)A′′
(1−ξ)fk+ξfk+1, ¯gk(0) > 0,
k→∞A(fk) =
f ∈span(S)A(f ).
If limk→∞A(fk) = −∞, then the conclusion is automatically true.
Otherwise, Lemma A.1 implies that ∞
j < ∞. Now choose hj = |¯αj| +
1/(j + 1) in (10); we have ∞
j=0 hj = ∞, and ∞
j < ∞. The convergence
now follows from Corollary 4.1.
Least squares loss.
The convergence of unrestricted step-size boosting using
least squares loss (matching pursuit) was studied in . Since a scaling of the
basis function does not change the algorithm, without loss of generality we can
assume that EXg(X)2 = 1 for all g ∈S (assume S does not contain function 0). In
this case it is easy to check that for all g ∈S,
f,g(0) = EXg(X)2 = 1.
BOOSTING WITH EARLY STOPPING
Therefore the conditions in Corollary A.1 are satisﬁed as long as ∞
j=0 εj < ∞.
This shows that the matching pursuit procedure converges, that is,
k→∞A(fk) =
f ∈span(S)A(f ).
We would like to point out that for matching pursuit, the inequality in Lemma A.1
can be replaced by the equality
j = 2[A(f0) −A(fk+1)],
which was referred to as “energy conservation” in , and was used there to
prove the convergence.
Exponential loss.
The convergence behavior of boosting with exponential
loss was previously studied by Breiman for ±1-trees under the assumption
infx P(Y = 1|x)P(Y = −1|x) > 0. Using exact computation, Breiman obtained
an equality similar to the matching pursuit energy conservation equation. As part
of the convergence analysis, the equality was used to show ∞
The following lemma shows that under a more general condition, the convergence of unrestricted boosting with exponential loss follows directly from Corollary A.1. This result extends that of , but the condition still constrains the class
of measures that generate the joint distribution of (X,Y).
LEMMA A.2.
Assume that
g∈S EX|g(X)|
P(Y = 1|X)P(Y = −1|X) > 0.
If ¯αk satisﬁes (27), then infk infξ∈(0,1) A′′
(1−ξ)fk+ξfk+1, ¯gk(0) > 0. Hence 
For notational simplicity, we let qX,Y = exp(−f (X)Y). Recall that
the direct computation of A′′
f,g(0) in Section A.1.2 yields
[EX,Y qX,Y ]2A′′
= [EX,Y g(X)2qX,Y ][EX,Y qX,Y ] −[EX,Y g(X)YqX,Y ]2
= [EXg(X)2EY|XqX,Y ][EXEY|XqX,Y ] −[EXg(X)EY|XYqX,Y ]2
≥[EXg(X)2EY|XqX,Y ][EXEY|XqX,Y ]
−[EXg(X)2|EY|XYqX,Y |][EX|EY|XYqX,Y |]
≥[EXg(X)2EY qX,Y ]EX[EY|XqX,Y −|EY|XYqX,Y |]
EY qX,Y (EY|XqX,Y −|EY|XYqX,Y |)
2P(Y = 1|X)P(Y = −1|X)
T. ZHANG AND B. YU
The ﬁrst and the third inequalities follow from Cauchy–Schwarz, and the
last inequality used the fact that (a + b)((a + b) −|a −b|) ≥2ab. Now
observe that EX,Y qX,Y = exp(A(f )). The exact minimization (27) implies that
A(fk) ≤A(f0) for all k ≥0. Therefore, using Jensen’s inequality we know that
∀ξ ∈(0,1),A((1 −ξ)fk + ξfk+1) ≤A(f0). This implies the desired inequality,
(1−ξ)fk+ξfk+1, ¯gk(0)
EX| ¯gk(X)|
2P(Y = 1|X)P(Y = −1|X)
Although unrestricted step-size boosting procedures can be successful in certain
cases, for general problems we are unable to prove convergence. In such cases the
crucial condition of ∞
j < ∞, as required in the proof of Corollary A.1, can
be violated. Although we do not have concrete examples at this point, we believe
boosting may fail to converge when this condition is violated.
For example, for logistic regression we are unable to prove a result similar
to Lemma A.2. The difﬁculty is caused by the near-linear behavior of the loss
function toward negative inﬁnity. This means that the second derivative is so small
that we may take an extremely large step-size when ¯αj is exactly minimized.
Intuitively, the difﬁculty associated with large ¯αj is due to the potential problem
of large oscillation in that a greedy step may search for a suboptimal direction,
which needs to be corrected later on. If a large step is taken toward the suboptimal
direction, then many more additional steps have to be taken to correct the mistake.
If the additional steps are also large, then we may overcorrect and go to some other
suboptimal directions. In general it becomes difﬁcult to keep track of the overall
A.4. The relationship of AdaBoost and L1-margin maximization.
a real-valued classiﬁcation function p(x), we consider the following discrete
prediction rule:
if p(x) ≥0,
if p(x) < 0.
Its classiﬁcation error [for simplicity we ignore the point p(x) = 0, which is
assumed to occur rarely] is given by
Lγ (p(x),y) =
if p(x)y ≤γ ,
if p(x)y > γ ,
with γ = 0. In general, we may consider γ ≥0 and the parameter γ ≥0 is often
referred to as margin, and we shall call the corresponding error function Lγ margin
In the authors proved that under appropriate assumptions on the base
learner, the expected margin error Lγ with a positive margin γ > 0 also
BOOSTING WITH EARLY STOPPING
decreases exponentially. It follows that regularity assumptions of weak learning
for AdaBoost imply the following margin condition: there exists γ > 0 such that
inff ∈span(S),∥f ∥1=1 Lγ (f,y) = 0, which in turn implies the inequality for all s > 0,
f ∈span(S),∥f ∥1=1EX,Y exp
 ≤exp(−γ s).
We now show that under (29) the expected margin errors (with small margin)
from Algorithm 2.1 may decrease exponentially. A similar analysis was given
in . However, the boosting procedure considered there was modiﬁed so that
the estimator always stays in the scaled convex hull of the basis functions. This
restriction is removed in the current analysis:
supk ≤hk,
Note that this implies that ¯εk ≤h2
k for all k.
Now applying (15) with
¯f = sf for any s > 0 and letting f approach the
minimum in (29), we obtain (recall ∥f ∥1 = 1)
A(fk) ≤−sγ
sk + s ¯εj−1 ≤−sγ
Now let s →∞; we have
A(fk) ≤−γ sk +
Assume we pick a constant h < γ and let hk = h; then
−kh(γ −h)
which implies that the margin error decreases exponentially for all margins less
than γ −h. To see this, consider γ ′ < γ −h. Since ∥fk∥1 ≤kh, we have from (30),
Lγ ′fk(x)/∥fk∥1,y
fk(X)Y ≤khγ ′
−fk(X)Y + khγ ′ ≤exp
−kh(γ −h −γ ′)
k→∞Lγ ′fk(x)/∥fk∥1,y
This implies that as h →0, fk(x)/∥fk∥1 achieves a margin that is within h of the
maximum possible. Therefore, when h →and k →∞, fk(x)/∥fk∥1 approaches a
maximum margin separator.
Note that in this particular case we allow a small step-size (h < γ ), which
violates the condition 
k < ∞imposed for the boosting algorithm to converge.
However, this condition that prevents large oscillation from occurring is only a
sufﬁcient condition to guarantee convergence. For speciﬁc problems, especially
T. ZHANG AND B. YU
when inff ∈span(S) A(f ) = −∞, it is still possible to achieve convergence even if
the condition is violated.