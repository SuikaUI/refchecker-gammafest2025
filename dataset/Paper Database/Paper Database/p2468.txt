The Annals of Statistics
2005, Vol. 33, No. 4, 1538â€“1579
DOI 10.1214/009053605000000255
Â© Institute of Mathematical Statistics, 2005
BOOSTING WITH EARLY STOPPING: CONVERGENCE
AND CONSISTENCY
BY TONG ZHANG AND BIN YU1
IBM T. J. Watson Research Center and University of California at Berkeley
Boosting is one of the most signiï¬cant advances in machine learning
for classiï¬cation and regression. In its original and computationally ï¬‚exible
version, boosting seeks to minimize empirically a loss function in a greedy
fashion. The resulting estimator takes an additive function form and is built
iteratively by applying a base estimator (or learner) to updated samples
depending on the previous iterations. An unusual regularization technique,
early stopping, is employed based on CV or a test set.
This paper studies numerical convergence, consistency and statistical rates
of convergence of boosting with early stopping, when it is carried out over
the linear span of a family of basis functions. For general loss functions, we
prove the convergence of boostingâ€™s greedy optimization to the inï¬nimum
of the loss function over the linear span. Using the numerical convergence
result, we ï¬nd early-stopping strategies under which boosting is shown to
be consistent based on i.i.d. samples, and we obtain bounds on the rates of
convergence for boosting estimators. Simulation studies are also presented
to illustrate the relevance of our theoretical results for providing insights to
practical aspects of boosting.
As a side product, these results also reveal the importance of restricting the
greedy search step-sizes, as known in practice through the work of Friedman
and others. Moreover, our results lead to a rigorous proof that for a linearly
separable problem, AdaBoost with Îµ â†’0 step-size becomes an L1-margin
maximizer when left to run to convergence.
1. Introduction.
In this paper we consider boosting algorithms for classiï¬cation and regression. These algorithms represent one of the major advances in
machine learning. In their original version, the computational aspect is explicitly
speciï¬ed as part of the estimator/algorithm. That is, the empirical minimization of
an appropriate loss function is carried out in a greedy fashion, which means that
at each step a basis function that leads to the largest reduction of empirical risk
is added into the estimator. This speciï¬cation distinguishes boosting from other
statistical procedures which are deï¬ned by an empirical minimization of a loss
function without the numerical optimization details.
Received January 2003; revised September 2004.
1Supported in part by NSF Grant FD01-12731 and ARO Grant DAAD19-01-1-0643.
AMS 2000 subject classiï¬cations. 62G05, 62G08.
Key words and phrases. Boosting, greedy optimization, matching pursuit, early stopping, consistency.
BOOSTING WITH EARLY STOPPING
Boosting algorithms construct composite estimators using often simple base estimators through the greedy ï¬tting procedure. An unusual regularization technique,
early stopping, is employed based on CV or a test set. This family of algorithms
has been known as the stagewise ï¬tting of additive models in the statistics literature . For the squared loss function, they were often referred to in the signal
processing community as matching pursuit . More recently, it was noticed that
the AdaBoost method proposed in the machine learning community can also
be regarded as stagewise ï¬tting of additive models under an exponential loss function . In this paper we use the term boosting to indicate a greedy
stagewise procedure to minimize a certain loss function empirically. The abstract
formulation will be presented in Section 2.
Boosting procedures have drawn much attention in the machine learning
community as well as in the statistics community, due to their superior empirical
performance for classiï¬cation problems. In fact, boosted decision trees are
generally regarded as the best off-the-shelf classiï¬cation algorithms we have today.
In spite of the signiï¬cant practical interest in boosting, a number of theoretical
issues have not been fully addressed in the literature. In this paper we hope to
ï¬ll some gaps by addressing three basic issues regarding boosting: its numerical
convergence when the greedy iteration increases, in Section 4.1; its consistency
(after early stopping) when the training sample size gets large, in Sections
3.3 and 5.2; and bounds on the rate of convergence for boosting estimators, in
Sections 3.3 and 5.3.
It is now well known that boosting forever can overï¬t the data (e.g., see ). Therefore, in order to achieve consistency, it is necessary to stop the
boosting procedure early (but not too early) to avoid overï¬tting. In the early
stopping framework, the consistency of boosting procedures has been considered
by Jiang for exponential loss boosting (but the consistency is in terms of
the classiï¬cation loss) and BÃ¼hlmann under squared loss for tree-type base
classiï¬ers. Jiangâ€™s approach also requires some smoothness conditions on the
underlying distribution, and it is nonconstructive (hence does not lead to an
implementable early-stopping strategy). In Sections 3.3 and 5.2 we present an
early-stopping strategy for general loss functions that guarantees consistency.
A different method of achieving consistency (and obtaining rate of convergence
results) is through restricting the weights of the composite estimator using the
1-norm of its coefï¬cients (with respect to the basis functions). For example, this
point of view is taken up in . In this framework, early stopping is
not necessary since the degree of overï¬tting or regularization is controlled by
the 1-norm of the weights of the composite estimator. Although this approach
simpliï¬es the theoretical analysis, it also introduces an additional control quantity
which needs to be adjusted based on the data. Therefore, in order to select an
optimal regularization parameter, one has to solve many different optimization
problems, each with a regularization parameter. Moreover, if there are an inï¬nite
(or extremely large) number of basis functions, then it is not possible to solve the
T. ZHANG AND B. YU
associated 1-norm regularization problem. Note that in this case greedy boosting
(with approximate optimization) can still be applied.
A question related to consistency and rate of convergence is the convergence
of the boosting procedure as an optimization method. This is clearly one of the
most fundamental theoretical issues for boosting algorithms. Previous studies
have focused on special loss functions. Speciï¬cally, Mallat and Zhang proved the
convergence of matching pursuit in , which was then used in to study
consistency; in Breiman obtained an inï¬nite-sample convergence result of
boosting with the exponential loss function for Â±1-trees (under some smoothness
assumptions on the underlying distribution), and the result was used by Jiang to
study the consistency of AdaBoost. In a Bregman divergence-based analysis
was given. A convergence result was also obtained in for a gradient descent
version of boosting.
None of these studies provides any information on the numerical speed of
convergence for the optimization. The question of numerical speed of convergence
has been studied when one works with the 1-norm regularized version of boosting
where we assume that the optimization is performed in the convex hull of the
basis functions. Speciï¬cally, for function estimation under least-squares loss, the
convergence of the greedy algorithm in the convex hull was studied in . For general loss functions, the convergence of greedy algorithms (again,
the optimization is restricted to the convex hull) was recently studied in . In
this paper we apply the same underlying idea to the standard boosting procedure
where we do not limit the optimization to the convex hull of the basis functions.
The resulting bound provides information on the speed of convergence for the
optimization. An interesting observation of our analysis is the important role of
small step-size in the convergence of boosting procedures. This provides some
theoretical justiï¬cation for Friedmanâ€™s empirical observation that using small
step-sizes almost always helps in boosting procedures.
Moreover, the combination of numerical convergence results with modern
empirical process bounds (based on Rademacher complexity) provides a way to
derive bounds on the convergence rates of early-stopping boosting procedures.
These results can be found in Sections 3.3 and 5.3. Section 6 contains a simulation
study to show the usefulness of the insights from our theoretical analyses in
practical implementations of boosting. The proofs of the two main results in
the numerical convergence section (Section 4.1) are deferred to Section A.2.
Section A.3 discusses relaxations of the restricted step-size condition used for
earlier results, and Section A.4 uses numerical convergence results to give a
rigorous proof of the fact that for separable problems, AdaBoost with small stepsize becomes an L1 margin maximizer at its limit (see ).
2. Abstract boosting procedure.
We now describe the basics to deï¬ne the
boosting procedure that we will analyze in this paper. A similar setup can be found
BOOSTING WITH EARLY STOPPING
in . The main difference is that the authors in use a gradient descent rule
in their boosting procedure while here we use approximate minimization.
Let S be a set of real-valued functions and deï¬ne
wjf j :f j âˆˆS,wj âˆˆR,m âˆˆZ+
which forms a linear function space. For all f âˆˆspan(S), we can deï¬ne the 1-norm
with respect to the basis S as
âˆ¥f âˆ¥1 = inf
wjf j :f j âˆˆS,m âˆˆZ+
We want to ï¬nd a function
Â¯f âˆˆspan(S) that approximately solves the
optimization problem
f âˆˆspan(S)A(f ),
where A is a convex function of f deï¬ned on span(S). Note that the optimal value
may not be achieved by any f âˆˆspan(S), and for certain formulations (such as
AdaBoost) it is possible that the optimal value is not ï¬nite. Both cases are still
covered by our results, however.
The abstract form of the greedy boosting procedure (with restricted step-size)
considered in this paper is given by the following algorithm:
ALGORITHM 2.1 (Greedy boosting).
Pick f0 âˆˆspan(S)
for k = 0,1,2,...
Select a closed subset k âŠ‚R such that 0 âˆˆk and k = âˆ’k
Find Â¯Î±k âˆˆk and Â¯gk âˆˆS to approximately minimize the function:
(Î±k,gk) â†’A(fk + Î±kgk)
Let fk+1 = fk + Â¯Î±k Â¯gk
REMARK 2.1.
The approximate minimization of (âˆ—) in Algorithm 2.1 should
be interpreted as ï¬nding Â¯Î±k âˆˆk and Â¯gk âˆˆS such that
A(fk + Â¯Î±k Â¯gk) â‰¤
Î±kâˆˆk,gkâˆˆS A(fk + Î±kgk) + Îµk,
where Îµk â‰¥0 is a sequence of nonnegative numbers that converges to zero.
REMARK 2.2.
The requirement that 0 âˆˆk is not crucial in our analysis. It
is used as a convenient assumption in the proof of Lemma 4.1 to simplify the
conditions. Our convergence analysis allows the choice of k to depend on the
previous steps of the algorithm. However, the most interesting k for the purpose
of this paper will be independent of previous steps of the algorithm:
T. ZHANG AND B. YU
(a) k = R,
(b) supk = Ëœhk where Ëœhk â‰¥0 and Ëœhk â†’0.
As we will see later, the restriction of Î±k to the subset k âŠ‚R is useful in the
convergence analysis.
As we shall see later, the step-size Â¯Î±k plays an important role in our analysis.
A particular interesting case is to restrict the step-size explicitly. That is, we assume
that the starting point f0, as well as quantities Îµk and k in (3), are sampleindependent, and hk = supk satisï¬es the conditions
The reason for this condition will become clear in the numerical convergence
analysis of Section 4.1.
3. Assumptions and main statistical results.
The purpose of this section
is to state assumptions needed for the analyses to follow, as well as the main
statistical results. There are two main aspects of our analysis. The ï¬rst is the
numerical convergence of the boosting algorithm as the number of iterations
increases, and the second is the statistical convergence of the resulting boosting
estimator, so as to avoid overï¬tting. We list respective assumptions separately. The
statistical consistency result can be obtained by combining these two aspects.
3.1. Assumptions for the numerical convergence analysis.
For all f âˆˆspan(S)
and g âˆˆS, we deï¬ne a real-valued function Af,g(Â·) as
Af,g(h) = A(f + hg).
DEFINITION 3.1.
Let A(f ) be a function of f deï¬ned on span(S). Denote by
span(S)â€² the dual space of span(S) [i.e., the space of real-valued linear functionals
on span(S)]. We say that A is differentiable with gradient âˆ‡A âˆˆspan(S)â€² if
it satisï¬es the following FrÃ©chet-like differentiability condition for all f,g âˆˆ
A(f + hg) âˆ’A(f )
 = âˆ‡A(f )T g,
where âˆ‡A(f )T g denotes the value of the linear functional âˆ‡A(f ) at g. Note that
we adopt the notation f T g from linear algebra, where it is just the scalar product
of the two vectors.
For reference, we shall state the following assumption, which is required in our
BOOSTING WITH EARLY STOPPING
ASSUMPTION 3.1.
Let A(f ) be a convex function of f deï¬ned on span(S),
which satisï¬es the following conditions:
1. The functional A is differentiable with gradient âˆ‡A.
2. For all f âˆˆspan(S) and g âˆˆS, the real-valued function Af,g is second-order
differentiable (as a function of h) and the second derivative satisï¬es
f,g(0) â‰¤M(âˆ¥f âˆ¥1),
where M(Â·) is a nondecreasing real-valued function.
REMARK 3.1.
A more general form of (5) is Aâ€²â€²
f,g(0) â‰¤â„“(g)M(âˆ¥f âˆ¥1), where
â„“(g) is an appropriate scaling factor of g. For example, in the examples given
below, â„“(g) can be measured by supx |g(x)| or EXg(X)2. In (5) we assume
that functions in S are properly scaled so that â„“(g) â‰¤1. This is for notational
convenience only. With more complicated notation techniques developed in
this paper can also handle the general case directly without any normalization
assumption of the basis functions.
The function M(Â·) will appear in the convergence analysis in Section 4.1.
Although our analysis can handle unbounded M(Â·), the most interesting boosting
examples have bounded M(Â·) (as we will show shortly). In this case we will also
use M to denote a real-valued upper bound of supa M(a).
For statistical estimation problems such as classiï¬cation and regression with a
covariate or predictor variable X and a real response variable Y having a joint
distribution, we are interested in the following form of A(f ) in (2):
where Ï†(Â·,Â·) is a loss function that is convex in its ï¬rst argument and Ïˆ is
a monotonic increasing auxiliary function which is introduced so that A(f ) is
convex and M(Â·) behaves nicely (e.g., bounded). We note that the introduction of Ïˆ
is for proving numerical convergence results using our proof techniques, which
are needed for proving statistical consistency of boosting with early stopping.
However, Ïˆ is not necessary for the actual implementation of the boosting
procedure. Clearly the minimizer of (6) that solves (2) does not depend on the
choice of Ïˆ. Moreover, the behavior of Algorithm 2.1 is not affected by the choice
of Ïˆ as long as Îµk in (3) is appropriately redeï¬ned. We may thus always take
Ïˆ(u) = u, but choosing other auxiliary functions can be convenient for certain
problems in our analysis since the resulting formulation has a bounded M(Â·)
function (see the examples given below). We have also used EX,Y to indicate the
expectation with respect to the joint distribution of (X,Y).
When not explicitly speciï¬ed, EX,Y can denote the expectation either with
respect to the underlying population or with respect to the empirical samples. This
makes no difference as far as our convergence analysis in Section 4.1 is concerned.
T. ZHANG AND B. YU
When it is necessary to distinguish an empirical quantity from its population
counterpart, we shall denote the former by a hat above the corresponding quantity.
For example, Ë†E denotes the expectation with respect to the empirical samples,
and Ë†A is the function in (6) with EX,Y replaced by Ë†EX,Y . This distinction will
become necessary in the uniform convergence analysis of Section 4.2.
An important application of boosting is binary classiï¬cation. In this case it is
very natural for us to use a set of basis functions that satisfy the conditions
|g(x)| â‰¤1,
For certain loss functions (such as least squares) this condition can be relaxed. In
the classiï¬cation literature Ï†(f,y) usually has a form Ï†(fy).
Commonly used loss functions are listed in Section A.1. They show that
for a typical boosting loss function Ï†, there exists a constant M such that
supa M(a) â‰¤M.
3.2. Assumptions for the statistical convergence analysis.
In classiï¬cation or
regression problems with a covariate or predictor variable X on Rd and a real
response variable Y, we observe m i.i.d. samples Zm
1 = {(X1,Y1),...,(Xm,Ym)}
from an unknown underlying distribution D. Consider a loss function Ï†(f,y) and
deï¬ne Q(f ) (true risk) and Ë†Q(f ) (empirical risk) as
Q(f ) = EDÏ†
Ë†Q(f ) = Ë†EÏ†
f (Xi),Yi
where ED is the expectation over the unknown true joint distribution D of (X,Y)
(denoted by EX,Y previously);
Ë†E is the empirical expectation based on the
Boosting estimators are constructed by applying Algorithm 2.1 with respect to
the empirical expectation Ë†E with a set S of real-valued basis functions g(x). We
use Ë†A(f ) to denote the empirical objective function,
Ë†A(f ) = Ïˆ
Similarly, quantities fk, Î±k and gk in Algorithm 2.1 will be replaced by Ë†fk, Ë†Î±k
and Ë†gk, respectively.
Techniques from modern empirical process theory can be used to analyze the
statistical convergence of a boosting estimator with a ï¬nite sample. In particular,
we use the concept of Rademacher complexity, which is given by the following
deï¬nition.
DEFINITION 3.2.
Let G = {g(x,y)} be a set of functions of input (x,y).
i=1 be a sequence of binary random variables such that Ïƒi = Â±1 with
BOOSTING WITH EARLY STOPPING
probability 1/2. The (one-sided) sample-dependent Rademacher complexity of G
is given by
1 ) = EÏƒ sup
Ïƒig(Xi,Yi),
and the expected Rademacher complexity of G is denoted by
Rm(G) = EZm
The Rademacher complexity approach for analyzing boosting algorithms ï¬rst
appeared in , and it has been used by various people to analyze learning
problems, including boosting; for example, see . The analysis using
Rademacher complexity as deï¬ned above can be applied both to regression and
to classiï¬cation. However, for notational simplicity we focus only on boosting
methods for classiï¬cation, where we impose the following assumption. This
assumption is not essential to our analysis, but it simpliï¬es the calculations and
some of the ï¬nal conditions.
ASSUMPTION 3.2.
We consider the following form of Ï† in (8): Ï†(f,y) =
Ï†(fy) with a convex function Ï†(a):R â†’R such that Ï†(âˆ’a) > Ï†(a) for all a > 0.
Moreover, we assume that
(i) Condition (7) holds.
(ii) S in Algorithm 2.1 is closed under negation (i.e., f âˆˆS â†’âˆ’f âˆˆS).
(iii) There exists a ï¬nite Lipschitz constant Î³Ï†(Î²) of Ï† in [âˆ’Î²,Î²]:
âˆ€|f1|,|f2| â‰¤Î²
|Ï†(f1) âˆ’Ï†(f2)| â‰¤Î³Ï†(Î²)|f1 âˆ’f2|.
The Lipschitz condition of a loss function is usually easy to estimate. For
reference, we list Î³Ï† for loss functions considered in Section A.1:
(a) Logistic regression Ï†(f ) = ln(1 + exp(âˆ’f )):Î³Ï†(Î²) â‰¤1.
(b) Exponential Ï†(f ) = exp(âˆ’f ):Î³Ï†(Î²) â‰¤exp(Î²).
(c) Least squares Ï†(f ) = (f âˆ’1)2 :Î³Ï†(Î²) â‰¤2(Î² + 1).
(d) Modiï¬ed least squares Ï†(f ) = max(1 âˆ’f,0)2 :Î³Ï†(Î²) â‰¤2(Î² + 1).
(e) p-norm Ï†(f ) = |f âˆ’1|p(p â‰¥2):Î³Ï†(Î²) â‰¤p(Î² + 1)pâˆ’1.
3.3. Main statistical results.
We may now state the main statistical results
based on the assumptions and deï¬nitions given earlier. The following theorem
gives conditions for our boosting algorithm so that consistency can be achieved in
the large sample limit. The proof is deferred to Section 5.2, with some auxiliary
T. ZHANG AND B. YU
THEOREM 3.1.
Under Assumption 3.2 let Ï† be one of the loss functions
considered in Section A.1. Assume further that in Algorithm 2.1 we choose
quantities f0, Îµk and k to be independent of the sample Zm
1 , such that
j=0 Îµj < âˆ, and hk = supk satisï¬es (4).
Consider two sequences of sample independent numbers km and Î²m such that
limmâ†’âˆkm = âˆand limmâ†’âˆÎ³Ï†(Î²m)Î²mRm(S) = 0. Then as long as we stop
Algorithm 2.1 at a step Ë†k based on Zm
1 such that Ë†k â‰¥km and âˆ¥Ë†fË†kâˆ¥1 â‰¤Î²m, we have
the consistency result
1 Q( Ë†fË†k) =
f âˆˆspan(S)Q(f ).
REMARK 3.2.
The choice of (km,Î²m) in the above theorem should not
be void, in the sense that for all samples Zm
it should be possible to stop
Algorithm 2.1 at a point such that the conditions Ë†k â‰¥km and âˆ¥Ë†fË†kâˆ¥1 â‰¤Î²m are
In particular, if limmâ†’âˆRm(S) = 0, then we can always ï¬nd km â‰¤kâ€²
m such that
km â†’âˆand Î³Ï†(Î²m)Î²mRm(S) â†’0 with Î²m = âˆ¥f0âˆ¥1 + kâ€²m
j=0 hj. This choice of
(km,Î²m) is valid as we can stop the algorithm at any Ë†k âˆˆ[km,kâ€²
Similar to the consistency result, we may further obtain some rate of convergence results. This work does not focus on rate of convergence analysis, and results
we obtain are not necessarily tight. Before stating a more general and more complicated result, we ï¬rst present a version for constant step-size logistic boosting,
which is much easier to understand.
THEOREM 3.2.
Consider the logistic regression loss function, with basis S
which satisï¬es Rm(S) â‰¤CS
âˆšm for some positive constant CS. For each sample
size m, consider Algorithm 2.1 with f0 = 0, supk k = h0(m) â‰¤1/âˆšm and
Îµk â‰¤h0(m)2/2. Assume that we run boosting for k(m) = Î²m/h0(m) steps. Then
1 Q( Ë†fË†k) â‰¤
Â¯f âˆˆspan(S)
Q( Â¯f ) + (2CS + 1)Î²m
+ âˆ¥Â¯f âˆ¥1 + 1
âˆ¥Â¯f âˆ¥1 + Î²m
Note that the condition Rm(S) â‰¤CS/âˆšm is satisï¬ed for many basis function classes, such as two-level neural networks and tree basis functions (see
Section 4.3). The bound in Theorem 3.2 is independent of h0(m) [as long as
h0(m) â‰¤mâˆ’1/2]. Although this bound is likely to be suboptimal for practice problems, it does give a worst case guarantee for boosting with the greedy optimization aspect taken into consideration. Assume that there exists Â¯f âˆˆspan(S) such
that Q( Â¯f ) = inff âˆˆspan Q(f ). Then we may choose Î²m as Î²m = O(âˆ¥Â¯f âˆ¥1/2
which gives a convergence rate of EZm
1 Q( Ë†fË†k) â‰¤Q( Â¯f ) + O(âˆ¥Â¯f âˆ¥1/2
1 mâˆ’1/4). As the
target complexity âˆ¥Â¯f âˆ¥1 increases, the convergence becomes slower. An example
is provided in Section 6 to illustrate this phenomenon.
BOOSTING WITH EARLY STOPPING
We now state the more general result, on which Theorem 3.2 is based (see
Section 5.3).
THEOREM 3.3.
Under Assumption 3.2, let Ï†(f ) â‰¥0 be a loss function such
that A(f ) satisï¬es Assumption 3.1 with the choice Ïˆ(a) = a. Given a sample
size m, we pick a positive nonincreasing sequence {hk} which may depend on m.
Consider Algorithm 2.1 with f0 = 0, supk k = hk and Îµk â‰¤h2
kM(sk+1)/2, where
Given training data, suppose we run boosting for Ë†k = k(m) steps, and let
Î²m = sk(m). Then âˆ€Â¯f âˆˆspan(S) such that Q( Â¯f ) â‰¤Q(0)
1 Q( Ë†fË†k) â‰¤Q( Â¯f ) + 2Î³Ï†(Î²m)Î²mRm(S)
âˆšmÏ†(âˆ’âˆ¥Â¯f âˆ¥1) + âˆ¥Â¯f âˆ¥1Ï†(0)
âˆ¥Â¯f âˆ¥1 + Î²m
+ Î´m(âˆ¥Â¯f âˆ¥1),
Î´m(âˆ¥Â¯f âˆ¥1) =
 sâ„“+ âˆ¥Â¯f âˆ¥1
Î²m + âˆ¥Â¯f âˆ¥1
Î²m + hk(m)
If the target function is Â¯f which belongs to span(S), then Theorem 3.3 can
be directly interpreted as a rate of convergence result. However, the expression
of Î´m may still be quite complicated. For speciï¬c loss function and step-size
choices, the bound can be simpliï¬ed. For example, the result for logistic boosting
in Theorem 3.2 follows easily from the theorem (see Section 5.3).
4. Preparatory results.
As discussed earlier, it is well known by now that
boosting can overï¬t if left to run until convergence. In Section 3.3 we stated our
main results that with appropriately chosen stopping rules and under regularity
conditions, results of consistency and rates of convergence can be obtained. In this
section we begin the proof process of these main results by proving the necessary
preparatory results, which are interesting in their own right, especially those on
numerical convergence of boosting in Section 4.1.
Suppose that we run Algorithm 2.1 on the sample Zm
1 and stop at step Ë†k. By the
triangle inequality and for any Â¯f âˆˆspan(S), we have
1 Q( Ë†fË†k) âˆ’Q( Â¯f ) â‰¤EZm
1 | Ë†Q( Ë†fË†k) âˆ’Q( Ë†fË†k)| + EZm
1 | Ë†Q( Â¯f ) âˆ’Q( Â¯f )|
1 [ Ë†Q( Ë†fË†k) âˆ’Ë†Q( Â¯f )].
The middle term is on a ï¬xed Â¯f , and thus it has a rate of convergence O(1/âˆšm)
by the CLT. To study the consistency and rates of convergence of boosting with
early stopping, the work lies in dealing with the ï¬rst and third terms in (9). The
third term is on the empirical performance of the boosting algorithm, and thus a
T. ZHANG AND B. YU
numerical convergence analysis is required and hence proved in Section 4.1. Using
modern empirical process theory, in Section 4.2 we upper bound the ï¬rst term in
terms of Rademacher complexity.
We will focus on the loss functions (such as those in Section A.1) which
satisfy Assumption 3.1. In particular, we assume that Ïˆ is a monotonic increasing
function, so that minimizing A(f ) or Ë†A(f ) is equivalent to minimizing Q(f )
or Ë†Q(f ). The derivation in Section 4.2 works with Q(f ) and Ë†Q(f ) directly,
instead of A(f ) and Ë†A(f ). The reason is that, unlike our convergence analysis
in Section 4.1, the relatively simple sample complexity analysis presented in
Section 4.2 does not take advantage of Ïˆ.
4.1. Numerical convergence analysis.
Here we consider the numerical convergence behavior of fk obtained from the greedy boosting procedure as k increases. For notational simplicity, we state the convergence results in terms of the
population boosting algorithm, even though they also hold for the empirical boosting algorithm. The proofs of the two main lemmas are deferred to Section A.2.
In our convergence analysis, we will specify convergence bounds in terms
of âˆ¥Â¯f âˆ¥1 (where
Â¯f is a reference function) and a sequence of nondecreasing
numbers sk satisfying the following condition: there exist positive numbers hk
|Â¯Î±k| â‰¤hk âˆˆk
and let sk = âˆ¥f0âˆ¥1 +
where {Â¯Î±k} are the step-sizes in (3). Note that hk in (10) can be taken as any number
that satisï¬es the above condition, and it can depend on {Â¯Î±k} computed by the
boosting algorithm. However, it is often desirable to state a convergence result that
does not depend on the actual boosting outputs (i.e., the actual Â¯Î±k computed). For
such results we may simply ï¬x hk by letting hk = supk. This gives convergence
bounds for the restricted step-size method which we mentioned earlier.
It can be shown (see Section A.2) that even in the worse case, the value
A(fk+1) âˆ’A( Â¯f ) decreases from A(fk) âˆ’A( Â¯f ) by a reasonable quantity.
Cascading this analysis leads to a numerical rate or speed of convergence for the
boosting procedure.
The following lemma contains the one-step convergence bound, which is the
key result in our convergence analysis.
LEMMA 4.1.
Assume that A(f ) satisï¬es Assumption 3.1. Consider hk and sk
that satisfy (10). Let Â¯f be an arbitrary reference function in span(S), and deï¬ne
A(fk) = max
0,A(fk) âˆ’A( Â¯f )
2 M(sk+1) + Îµk.
BOOSTING WITH EARLY STOPPING
Then after k steps, the following bound holds for fk+1 obtained from Algorithm 2.1:
sk + âˆ¥Â¯f âˆ¥1
A(fk) + Â¯Îµk.
Applying Lemma 4.1 repeatedly, we arrive at a convergence bound for the
boosting Algorithm 2.1 as in the following lemma.
LEMMA 4.2.
Under the assumptions of Lemma 4.1, we have
A(fk) â‰¤âˆ¥f0âˆ¥1 + âˆ¥Â¯f âˆ¥1
sk + âˆ¥Â¯f âˆ¥1
sj + âˆ¥Â¯f âˆ¥1
sk + âˆ¥Â¯f âˆ¥1
The above lemma gives a quantitative bound on the convergence of A(fk)
to the value A( Â¯f ) of an arbitrary reference function Â¯f âˆˆspan(S). We can see
that the numerical convergence speed of A(fk) to A( Â¯f ) depends on âˆ¥Â¯f âˆ¥1 and
the accumulated or total step-size sk. Speciï¬cally, if we choose
Â¯f such that
A( Â¯f ) â‰¤A(f0), then it follows from the above bound that
A(fk+1) â‰¤A( Â¯f )
1 âˆ’s0 + âˆ¥Â¯f âˆ¥1
sk+1 + âˆ¥Â¯f âˆ¥1
+ s0 + âˆ¥Â¯f âˆ¥1
sk+1 + âˆ¥Â¯f âˆ¥1
sj+1 + âˆ¥Â¯f âˆ¥1
sk+1 + âˆ¥Â¯f âˆ¥1
Note that the inequality is automatically satisï¬ed when A(fk+1) â‰¤A( Â¯f ).
Clearly, in order to select Â¯f to optimize the bound on the right-hand side, we
need to balance a trade-off: we may select Â¯f such that A( Â¯f ) (and thus the ï¬rst
term) becomes smaller as we increase âˆ¥Â¯f âˆ¥1; however, the other two terms will
become large when âˆ¥Â¯f âˆ¥1 increases. This bound also reveals the dependence of
the convergence on the initial value of the algorithm f0: the closer A(f0) gets
to the inï¬nimum of A, the smaller the bound. To our knowledge, this is the ï¬rst
convergence bound for greedy boosting procedures with quantitative numerical
convergence speed information.
Previous analyses, including matching pursuit for least squares , Breimanâ€™s
analysis of the exponential loss, as well as the Bregman divergence bound
in and the analysis of gradient boosting in , were all limiting results
without any information on the numerical speed of convergence. The key
conceptual difference here is that we do not compare to the optimal value directly,
but instead, to the value of an arbitrary Â¯f âˆˆspan(S), so that âˆ¥Â¯f âˆ¥1 can be used to
measure the convergence speed. This approach is also crucial for problems where
A(Â·) can take âˆ’âˆas its inï¬nimum, for which a direct comparison will clearly
T. ZHANG AND B. YU
fail (e.g., Breimanâ€™s exponential loss analysis requires smoothness assumptions to
prevent this âˆ’âˆinï¬nimum value).
A general limiting convergence result follows directly from the above lemma.
THEOREM 4.1.
Assume that âˆ
j=0 Â¯Îµj < âˆand âˆ
j=0 hj = âˆ; then we
have the following optimization convergence result for the greedy boosting
algorithm (2.1):
kâ†’âˆA(fk) =
f âˆˆspan(S)A(f ).
The assumptions imply that limkâ†’âˆsk = âˆ. We can thus construct a
nonnegative integer-valued function k â†’j(k) â‰¤k such that limkâ†’âˆsj(k)/sk = 0
and limkâ†’âˆsj(k) = âˆ.
From Lemma 4.2 we obtain for any ï¬xed Â¯f ,
A(fk) â‰¤âˆ¥f0âˆ¥1 + âˆ¥Â¯f âˆ¥1
sk + âˆ¥Â¯f âˆ¥1
sj + âˆ¥Â¯f âˆ¥1
sk + âˆ¥Â¯f âˆ¥1
sj + âˆ¥Â¯f âˆ¥1
sk + âˆ¥Â¯f âˆ¥1
sj + âˆ¥Â¯f âˆ¥1
sk + âˆ¥Â¯f âˆ¥1
â‰¤o(1) + sj(k) + âˆ¥Â¯f âˆ¥1
sk + âˆ¥Â¯f âˆ¥1
Â¯Îµjâˆ’1 = o(1).
Therefore limkâ†’âˆmax(0,A(fk) âˆ’A( Â¯f )) = 0. Since our analysis applies to any
Â¯f âˆˆspan(S), we can choose Â¯fj âˆˆspan(S) such that limjA( Â¯fj) = inff âˆˆspan(S)A(f ).
Now from limkâ†’âˆmax(0,A(fk) âˆ’A( Â¯fj)) = 0, we obtain the theorem.
COROLLARY 4.1.
For loss functions such as those in Section A.1, we have
supa M(a) < âˆ. Therefore as long as there exist hj in (10) and Îµj in (3) such
j=0 hj = âˆ, âˆ
j < âˆand âˆ
j=0 Îµj < âˆ, we have the following
convergence result for the greedy boosting procedure:
kâ†’âˆA(fk) =
f âˆˆspan(S)A(f ).
The above results regarding population minimization automatically apply to the
empirical minimization if we assume that the starting point f0, as well as quantities
Îµk and k in (3), are sample-independent, and the restricted step-size case where
hk = supk satisï¬es the condition (4).
The idea of restricting the step-size when we compute Â¯Î±j was advocated by
Friedman, who discovered empirically that taking small step-size helps . In
our analysis, we can restrict the search region so that Corollary 4.1 is automatically
BOOSTING WITH EARLY STOPPING
satisï¬ed. Since we believe this is an important case which applies for general loss
functions, we shall explicitly state the corresponding convergence result below.
COROLLARY 4.2.
Consider a loss function (e.g., those in Section A.1) such
that supa M(a) < +âˆ. Pick any sequence of positive numbers hj (j â‰¥0) such
j=0 hj = âˆ, âˆ
j < âˆ. If we choose k in Algorithm 2.1 such that
hk = supk, and Îµj in (3) such that âˆ
j=0 Îµj < âˆ, then
kâ†’âˆA(fk) =
f âˆˆspan(S)A(f ).
Note that the above result requires that the step-size hj be small (âˆ
but also not too small (âˆ
j=0 hj = âˆ). As discussed above, the ï¬rst condition prevents large oscillation. The second condition is needed to ensure that fk can cover
the whole space span(S).
The above convergence results are limiting results that do not carry any
convergence speed information. Although with speciï¬c choices of hk and sk one
may obtain such information from (14), the second term on the right-hand side is
typically quite complicated. It is thus useful to state a simple result for a speciï¬c
choice of hk and sk, which yields more explicit convergence information.
COROLLARY 4.3.
Assume that A(f ) satisï¬es Assumption 3.1. Pick a sequence of nonincreasing positive numbers hj (j â‰¥0). Suppose we choose k
in Algorithm 2.1 such that hk = supk, and choose Îµk in (3) such that Îµk â‰¤
kM(sk+1)/2. If we start Algorithm 2.1 with f0 = 0, then
sk + âˆ¥Â¯f âˆ¥1
A(f0) + inf
â„“(sâ„“+ âˆ¥Â¯f âˆ¥1)
sk + âˆ¥Â¯f âˆ¥1
0 + (k âˆ’â„“)h2
Using notation of Lemma 4.1, we have Â¯Îµk â‰¤h2
â„“M(sk+1). Therefore
each summand in the second term on the right-hand size of Lemma 4.2 is no more
â„“M(sk+1) when j > â„“and is no more than h2
0M(sk+1)(sâ„“+ âˆ¥Â¯f âˆ¥1)/(sk +
âˆ¥Â¯f âˆ¥1) when j â‰¤â„“. The desired inequality is now a straightforward consequence
Note that similar to the proof of Theorem 4.1, the term (kâˆ’â„“)h2
â„“in Corollary 4.3
can also be replaced by k
j. A special case of Corollary 4.3 is constant
step-size (hk = h0) boosting, which is the original version of restricted step-size
boosting considered by Friedman . This method is simple to apply since there
is only one step-size parameter to choose. Corollary 4.3 shows that boosting with
constant step-size (also referred to as Îµ-boosting in the literature) converges to
the optimal value in the limit of h0 â†’0, as long as we choose the number of
iterations k and step-size h0 such that kh0 â†’âˆand kh2
0 â†’0. To the best of our
T. ZHANG AND B. YU
knowledge, this is the only rigorously stated convergence result for the Îµ-boosting
method, which justiï¬es why one needs to use a step-size that is as small as possible.
It is also possible to handle sample-dependent choices of k in Algorithm 2.1,
or allow unrestricted step-size (k = R) for certain formulations. However,
the corresponding analysis becomes much more complicated. According to
Friedman , the restricted step-size boosting procedure is preferable in practice.
Therefore we shall not provide a consistency analysis for unrestricted step-size
formulations in this paper; but see Section A.3 for relaxations of the restricted
step-size condition.
In addition to the above convergence results for general boosting algorithms,
Lemma 4.2 has another very useful consequence regarding the limiting behavior
of AdaBoost in the separable classiï¬cation case. It asserts that the inï¬nitely
small step-size version of AdaBoost, in the convergence limit, is an L1 margin
maximizer. This result has been observed through a connection between boosting
with early stopping and L1 constrained boosting (see ). Our analysis gives
a direct and rigorous proof. This result is interesting because it shows that
AdaBoost shares some similarity (in the limit) with support vector machines
(SVMs) whose goal in the separable case is to ï¬nd maximum margin classiï¬ers;
the concept of margin has been popularized by Vapnik who used it to analyze
the generalization performance of SVMs. The detailed analysis is provided in
Section A.4.
4.2. Uniform convergence.
There are a number of possible ways to study
the uniform convergence of empirical processes. In this section we use a
relatively simple approach based on Rademacher complexity. Examples with
neural networks and tree-basis (left orthants) functions will be given to illustrate
our analysis.
The Rademacher complexity approach for analyzing boosting algorithms
appeared ï¬rst in . Due to its simplicity and elegance, it has been used and
generalized by many researchers . The approach used here essentially
follows Theorem 1 of , but without concentration results.
From Lemma 4.2 we can see that the convergence of the boosting procedure is
closely related to âˆ¥Â¯f âˆ¥1 and âˆ¥fkâˆ¥1. Therefore it is natural for us to measure the
learning complexity of Algorithm 2.1 based on the 1-norm of the function family
it can approximate at any given step. We shall mention that this analysis is not
necessarily the best approach for obtaining tight learning bounds since the boosting
procedure may effectively search a much smaller space than the function family
measured by the 1-norm âˆ¥fkâˆ¥1. However, it is relatively simple, and sufï¬cient for
our purpose of providing an early-stopping strategy to give consistency and some
rate of convergence results.
Given any Î² > 0, we now would like to estimate the rate of uniform
convergence,
Q(f ) âˆ’Ë†Q(f )
BOOSTING WITH EARLY STOPPING
where Q and Ë†Q are deï¬ned in (8).
The concept of Rademacher complexity used in our analysis is given in
Deï¬nition 3.2. For simplicity, our analysis also employs Assumption 3.2. As
mentioned earlier, the conditions are not essential, but rather they simplify the ï¬nal
results. For example, the condition (7) implies that âˆ€f âˆˆspan(S), |f (x)| â‰¤âˆ¥f âˆ¥1.
It follows that âˆ€Î² â‰¥âˆ¥f âˆ¥1, Ï†(f,y) â‰¤Ï†(âˆ’Î²). This inequality, although convenient,
is certainly not essential.
LEMMA 4.3.
Under Assumption 3.2,
 â‰¤2Î³Ï†(Î²)Î²Rm(S),
where Î³Ï†(Î²) is a Lipschitz constant of Ï† in [âˆ’Î²,Î²]:âˆ€|f1|,|f2| â‰¤Î² :|Ï†(f1) âˆ’
Ï†(f2)| â‰¤Î³Ï†(Î²)|f1 âˆ’f2|.
Using the standard symmetrization argument (e.g., see Lemma 2.3.1
of ), we have
:âˆ¥f âˆ¥1 â‰¤Î²
Now the one-sided Rademacher process comparison result in , Theorem 7,
which is essentially a slightly reï¬ned result (with better constant) of the two-sided
version in , Theorem 4.12, implies that
:âˆ¥f âˆ¥1 â‰¤Î²
 â‰¤Î³Ï†(Î²)Rm
{f (X):âˆ¥f âˆ¥1 â‰¤Î²}
Using the simple fact that g = 
i |Î±i| = 1) implies g â‰¤max(supi fi,
supi âˆ’fi), and that S is closed under negation, it is easy to verify that Rm(S) =
Rm({f âˆˆspan(S):âˆ¥f âˆ¥1 â‰¤1}). Therefore
{f (X):âˆ¥f âˆ¥1 â‰¤Î²}
 = Î²Rm(S).
Now by combining the three inequalities, we obtain the lemma.
4.3. Estimating Rademacher complexity.
Our uniform convergence result
depends on the Rademacher complexity Rm(S). For many function classes, it
can be estimated directly. In this section we use a relation between Rademacher
complexity and â„“2-covering numbers from .
Let X = {X1,...,Xm} be a set of points and let Qm be the uniform probability
measure over these points. We deï¬ne the â„“2(Qm) distance between any two
functions f and g as
â„“2(Qm)(f,g) =
|f (xi) âˆ’g(xi)|2
T. ZHANG AND B. YU
Let F be a class of functions. The empirical â„“2-covering number of F, denoted
by N(Îµ,F,â„“2(Qm)), is the minimal number of balls {g :â„“2(Qm)(g,f ) â‰¤Îµ} of
radius Îµ needed to cover F. The uniform â„“2 covering number is given by
N2(Îµ,F,m) = sup
Îµ,F,â„“2(Qm)
where the supremum is over all probability distribution Qm over samples of size m.
If F contains 0, then there exists a universal constant C (see Corollary 2.2.8
in ) such that
logN2(Îµ,F,m)dÎµ
where we assume that the integral on the right-hand side is ï¬nite. Note that for a
function class F with divergent integration value on the right-hand side, the above
inequality can be easily modiï¬ed so that we start the integration from a point Îµ0 > 0
instead of 0. However, the dependency of Rm(F) on m can be slower than 1/âˆšm.
ASSUMPTION 4.1.
F satisï¬es the condition
logN2(Îµ,F,m)dÎµ < âˆ.
A function class F that satisï¬es Assumption 4.1 is also a Donsker class,
for which the central limit theorem holds. In statistics and machine learning,
one often encounters function classes F with ï¬nite VC-dimension, where the
following condition holds (see Theorem 2.6.7 of ) for some constants C and V
independent of m: N2(Îµ,F,m) â‰¤C(1/Îµ)V . Clearly a function class with ï¬nite
VC-dimension satisï¬es Assumption 4.1.
For simplicity, in this paper we assume that S satisï¬es Assumption 4.1. It
follows that
Rm(S) â‰¤Rm(S âˆª{0}) â‰¤CS
where CS is a constant that depends on S only. This is the condition used in
Theorem 3.2. We give two examples of basis functions that are often used in
practice with boosting.
Two-level neural networks.
We consider two-level neural networks in Rd,
which form the function space span(S) with S given by
S = {Ïƒ(wT x + b):w âˆˆRd,b âˆˆR},
where Ïƒ(Â·) is a monotone bounded continuous activation function.
It is well known that S has a ï¬nite VC-dimension, and thus satisï¬es Assumption 4.1. In addition, for any compact subset U âˆˆRd, it is also well known that
span(S) is dense in C(U) (see ).
BOOSTING WITH EARLY STOPPING
Tree-basis functions.
Tree-basis (left orthant) functions in Rd are given by the
indicator function of rectangular regions,
(âˆ’âˆ,a1] Ã— Â·Â·Â· Ã— (âˆ’âˆ,ad]
:a1,...,ad âˆˆR
Similar to two-level neural networks, it is well known that S has a ï¬nite
VC-dimension, and for any compact set U âˆˆRd, span(S) is dense in C(U).
In addition to rectangular region basis functions, we may also consider a basis S
consisting of restricted size classiï¬cation and regression trees (disjoint unions of
constant functions on rectangular regions), where we assume that the number of
terminal nodes is no more than a constant V . Such a basis set S also has a ï¬nite
VC-dimension.
5. Consistency and rates of convergence with early stopping.
section we put together the results in the preparatory Section 4 to prove consistency
and some rate of convergence results for Algorithm 2.1 as stated in the main result
Section 3.3. For simplicity we consider only restricted step-size boosting with
relatively simple strategies for choosing step-sizes. According to Friedman ,
the restricted step-size boosting procedure is preferable in practice. Therefore we
shall not provide a consistency analysis for unrestricted step-size formulations in
this paper. Discussions on the relaxation of the step-size condition can be found in
Section A.3.
5.1. General decomposition.
Suppose that we run the boosting algorithm and
stop at an early stopping point Ë†k. The quantity Ë†k, which is to be speciï¬ed in
Section 5.2, may depend on the empirical sample Zm
1 . Suppose also that the
stopping point Ë†k is chosen so that the resulting boosting estimator Ë†fË†k satisï¬es
1 Q( Ë†fË†k) =
f âˆˆspan(S)Q(f ),
where we use EZm
to denote the expectation with respect to the random
1 . Since Q( Ë†fË†k) â‰¥inff âˆˆspan(S) Q(f ), we also have
Q( Ë†fË†k) âˆ’
f âˆˆspan(S)Q(f )
 = lim
1 Q( Ë†fË†k) âˆ’
f âˆˆspan(S)Q(f ) = 0.
If we further assume there is a unique f âˆ—such that
f âˆˆspan(S)Q(f ),
and for any sequence {fm}, Q(fm) â†’Q(f âˆ—) implies that fm â†’f âˆ—, then since
Q( Ë†fË†k) â†’Q(f âˆ—) as m â†’âˆ, it follows that
in probability,
T. ZHANG AND B. YU
which gives the usual consistency of the boosting estimator with an appropriate
early stopping if the target function f coincides with f âˆ—. This is the case, for
example, if the regression function f (x) = ED(Y|x) with respect to the true
distribution D is in span(S) or can be approximated arbitrarily close by functions
in span(S).
In the following, we derive a general decomposition needed for proving (18) or
Theorem 3.1 in Section 3.3. Suppose that Assumption 3.2 holds. Then for all ï¬xed
Â¯f âˆˆspan(S), we have
1 | Ë†Q( Â¯f ) âˆ’Q( Â¯f )| â‰¤
1 | Ë†Q( Â¯f ) âˆ’Q( Â¯f )|21/2
mED|Ï†( Â¯f (X)Y) âˆ’Q( Â¯f )|2
mEDÏ†( Â¯f (X)Y)2
âˆšmÏ†(âˆ’âˆ¥Â¯f âˆ¥1).
Assume that we run Algorithm 2.1 on the sample Zm
1 and stop at step Ë†k. If
the stopping point Ë†k satisï¬es P(âˆ¥Ë†fË†kâˆ¥1 â‰¤Î²m) = 1 for some sample-independent
Î²m â‰¥0, then using the uniform convergence estimate in (16), we obtain
1 Q( Ë†fË†k) âˆ’Q( Â¯f )
1 [Q( Ë†fË†k) âˆ’Ë†Q( Ë†fË†k)] + EZm
1 [ Ë†Q( Â¯f ) âˆ’Q( Â¯f )]
1 [ Ë†Q( Ë†fË†k) âˆ’Ë†Q( Â¯f )]
â‰¤2Î³Ï†(Î²m)Î²mRm(S) +
âˆšmÏ†(âˆ’âˆ¥Â¯f âˆ¥1) + sup
[ Ë†Q( Ë†fË†k) âˆ’Ë†Q( Â¯f )].
5.2. Consistency with restricted step-size boosting.
We consider a relatively
simple early-stopping strategy for restricted step-size boosting, where we take
hk = supk to satisfy (4).
Clearly, in order to prove consistency, we only need to stop at a point such that
âˆ€Â¯f âˆˆspan(S), all three terms in (19) become nonpositive in the limit m â†’âˆ. By
estimating the third term using Lemma 4.2, we obtain the following proof of our
main consistency result (Theorem 3.1).
PROOF OF THEOREM 3.1.
Obviously the assumptions of the theorem imply
that the ï¬rst two terms of (19) automatically converge to zero. In the following, we
only need to show that âˆ€Â¯f âˆˆspan(S):supZm
1 max(0, Ë†Q( Ë†fË†k) âˆ’Ë†Q( Â¯f )) â†’0 when
From Section A.1 we know that there exists a distribution-independent number
M > 0 such that M(a) < M for all underlying distributions. Therefore for all
BOOSTING WITH EARLY STOPPING
empirical samples Zm
1 , Lemma 4.2 implies that
Ë†A( Ë†fË†k) â‰¤âˆ¥f0âˆ¥1 + âˆ¥Â¯f âˆ¥1
sË†k + âˆ¥Â¯f âˆ¥1
sj + âˆ¥Â¯f âˆ¥1
sË†k + âˆ¥Â¯f âˆ¥1
Ë†A(f ) = max(0, Ë†A(f )âˆ’Ë†A( Â¯f )), sk = âˆ¥f0âˆ¥1+kâˆ’1
i=0 hi and Â¯Îµk = h2
Now using the inequality
Ë†A(f0) â‰¤max(Ïˆ(Ï†(âˆ’âˆ¥f0âˆ¥1)) âˆ’Ïˆ(Ï†(âˆ¥Â¯f âˆ¥1)),0) =
c( Â¯f ) and Ë†k â‰¥km, we obtain
Ë†A( Ë†fË†k) â‰¤sup
âˆ¥f0âˆ¥1 + âˆ¥Â¯f âˆ¥1
sk + âˆ¥Â¯f âˆ¥1
sj + âˆ¥Â¯f âˆ¥1
sk + âˆ¥Â¯f âˆ¥1
Observe that the right-hand side is independent of the sample Zm
1 . From the
assumptions of the theorem, we have âˆ
j=0 Â¯Îµj < âˆand limkâ†’âˆsk = âˆ. Now
the proof of Theorem 4.1 implies that as km â†’âˆ, the right-hand side of (20)
converges to zero. Therefore limmâ†’âˆsupZm
Ë†A( Ë†fË†k) = 0.
The following universal consistency result is a straightforward consequence of
Theorem 3.1.
COROLLARY 5.1.
Under the assumptions of Theorem 3.1, for any Borel set
U âŠ‚Rd, if span(S) is dense in C(U)â€”the set of continuous functions under the
uniform-norm topology, then for all Borel measure D on U Ã— {âˆ’1,1},
1 Q( Ë†fË†k) =
f âˆˆB(U)Q(f ),
where B(U) is the set of Borel measurable functions.
We only need to show inff âˆˆspan(S) Q(f ) = inff âˆˆB(U) Q(f ). This
follows directly from Theorem 4.1 of .
For binary classiï¬cation problems where y = Â±1, given any real-valued
function f , we predict y = 1 if f (x) â‰¥0 and y = âˆ’1 if f (x) < 0. The
classiï¬cation error is the following 0â€“1 loss function:
 = I[yf (x) â‰¤0],
where I[E] is the indicator function of the event E, and the expected loss is
L(f ) = EDâ„“
The goal of classiï¬cation is to ï¬nd a predictor f to minimize (21). Using the
notation Î·(x) = P(Y = 1|X = x), it is well known that Lâˆ—, the minimum of L(f ),
can be achieved by setting f (x) = 2Î·(x) âˆ’1. Let D be a Borel measure deï¬ned
on U Ã— {âˆ’1,1}; it is known (e.g., see ) that if Q(f ) â†’inff âˆˆB(U) Q(f ),
then L(f ) â†’Lâˆ—. We thus have the following consistency result for binaryclassiï¬cation problems.
T. ZHANG AND B. YU
COROLLARY 5.2.
Under the assumptions of Corollary 5.1, we have
1 L( Ë†fË†k) = Lâˆ—.
The stopping criterion given in Theorem 3.1 depends on Rm(S). For S
that satisï¬es Assumption 4.1, this can be estimated from (17). The condition
Î³Ï†(Î²m)Î²mRm(S) â†’0 in Theorem 3.1 becomes Î³Ï†(Î²m)Î²m = o(âˆšm). Using the
bounds for Î³Ï†(Â·) in Section 4.2, we obtain the following condition.
ASSUMPTION 5.1.
The sequence Î²m satisï¬es:
(i) Logistic regression Ï†(f ) = ln(1 + exp(âˆ’f )):Î²m = o(m1/2).
(ii) Exponential Ï†(f ) = exp(âˆ’f ):Î²m = o(logm).
(iii) Least squares Ï†(f ) = (f âˆ’1)2 :Î²m = o(m1/4).
(iv) Modiï¬ed least squares Ï†(f ) = max(0,1 âˆ’f )2 :Î²m = o(m1/4).
(v) p-norm Ï†(f ) = |f âˆ’1|p(p â‰¥2):Î²m = o(m1/2p).
We can summarize the above discussion in the following theorem, which applies
to boosted VC-classes such as boosted trees and two-level neural networks.
THEOREM 5.1.
Under Assumption 3.2, let Ï† be one of the loss functions
considered in Section A.1. Assume further that in Algorithm 2.1 we choose
the quantities f0, Îµk and k to be independent of the sample Zm
1 , such that
j=0 Îµj < âˆ, and hk = supk satisï¬es (4).
Suppose S satisï¬es Assumption 4.1 and we choose sample-independent
km â†’âˆ, such that Î²m = âˆ¥f0âˆ¥1 + km
j=0 hj satisï¬es Assumption 5.1. If we stop
Algorithm 2.1 at step km, then âˆ¥Ë†fkmâˆ¥1 â‰¤Î²m and the following consistency result
f âˆˆspan(S)Q(f ).
Moreover, if span(S) is dense in C(U) for a Borel set U âŠ‚Rd, then for all Borel
measures D on U Ã— {âˆ’1,1}, we have
f âˆˆB(U)Q(f ),
Note that in the above theorem the stopping criterion km is sample-independent.
However, similar to Theorem 3.1, we may allow other sample-dependent Ë†k such
that âˆ¥Ë†fË†kâˆ¥1 stays within the Î²m bound. One may be tempted to interpret the
rates of Î²m. However, since different loss functions approximate the underlying
distribution in different ways, it is not clear that one can rigorously compare them.
Moreover, our analysis is likely to be loose.
BOOSTING WITH EARLY STOPPING
5.3. Some bounds on the rate of convergence.
In addition to consistency, it is
also useful to study statistical rates of convergence of the greedy boosting method
with certain target function classes. Since our analysis is based on the 1-norm of
the target function, the natural function classes we may consider are those that can
be approximated well using a function in span(S) with small 1-norm.
We would like to emphasize that rate results, that have been stated in Theorems
3.2 and 3.3 and are to be proved here, are not necessarily optimal. There are
several reasons for this. First, we relate the numerical behavior of boosting to
1-norm regularization. In reality, this may not always be the best way to analyze
boosting since boosting can be studied using other complexity measures such
as sparsity (e.g., see for some other complexity measures). Second, even
with the 1-norm regularization complexity measure, the numerical convergence
analysis in Section 4.1 may not be tight. This again will adversely affect our ï¬nal
bounds. Third, our uniform convergence analysis, based on the relatively simple
Rademacher complexity, is not necessarily tight. For some problems there are more
sophisticated methods which improve upon our approach here (e.g., see [2â€“6, 22,
A related point is that bounds we are interested in here are a priori convergence
bounds that are data-independent. In recent years, there has been much interest
in developing data-dependent bounds which are tighter (see references mentioned
above). For example, in our case we may allow Î² in (16) to depend on the observed
data (rather than simply setting it to be a value based only on the sample size).
This approach, which can tighten the ï¬nal bounds based on observation, is a quite
signiï¬cant recent theoretical advance. However, as mentioned above, there are
other aspects of our analysis that can be loose. Moreover, we are mainly interested
in worst case scenario upper bounds on the convergence behavior of boosting
without looking at the data. Therefore we shall not develop data-dependent bounds
The statistical convergence behavior of the boosting algorithm relies on its
numerical convergence behavior, which can be estimated using (14). Combined
with statistical convergence analysis, we can easily obtain our main rate of
convergence result in Theorem 3.3.
PROOF OF THEOREM 3.3.
From (19) we obtain
1 Q( Ë†fË†k) â‰¤Q( Â¯f )+2Î³Ï†(Î²m)Î²mRm(S)+
âˆšmÏ†(âˆ’âˆ¥Â¯f âˆ¥1)+sup
[ Ë†Q( Ë†fË†k)âˆ’Ë†Q( Â¯f )].
Now we simply apply Corollary 4.3 to bound the last term. This leads to the desired
The result for logistic regression in Theorem 3.2 follows easily from Theorem 3.3.
T. ZHANG AND B. YU
PROOF OF THEOREM 3.2.
Consider logistic regression loss and constant
step-size boosting, where hk = h0(m). Note that for logistic regression we have
Î³Ï†(Î²) â‰¤1, M(a) â‰¤1, Ï†(âˆ’âˆ¥Â¯f âˆ¥1) â‰¤1+âˆ¥Â¯f âˆ¥1 and Ï†(0) â‰¤1. Using these estimates,
we obtain from Theorem 3.3,
1 Q( Ë†fË†k) â‰¤Q( Â¯f ) + 2Î²mRm(S) + âˆ¥Â¯f âˆ¥1 + 1
âˆ¥Â¯f âˆ¥1 + Î²m
+ Î²mh0(m).
Using the estimate of Rm(S) in (17), and letting h0(m) â‰¤1/âˆšm, we obtain
1 Q( Ë†fË†k) â‰¤Q( Â¯f ) + (2CS + 1)Î²m
+ âˆ¥Â¯f âˆ¥1 + 1
âˆ¥Â¯f âˆ¥1 + Î²m
This leads to the claim.
6. Experiments.
The purpose of this section is not to reproduce the large
number of already existing empirical studies on boosting. Although this paper is
theoretical in nature, it is still useful to empirically examine various implications
of our analysis, so that we can verify they have observable consequences. For this
reason our experiments focus mainly on aspects of boosting with early stopping
which have not been addressed in previous studies.
Speciï¬cally, we are interested in testing consistency and various issues of
boosting with early stopping based on our theoretical analysis. As pointed out
in , experimentally testing consistency is a very challenging task. Therefore,
in this section we have to rely on relatively simple synthetic data, for which we can
precisely control the problem and the associated Bayes risk. Such an experimental
setup serves the purpose of illustrating main insights revealed by our theoretical
6.1. Experimental setup.
In order to fully control the data generation mechanism, we shall use simple one-dimensional examples. A similar experimental setup
was also used in to study various theoretical aspects of voting classiï¬cation
Our goal is to predict Y âˆˆ{Â±1} based on X âˆˆ . Throughout the experiments, X is uniformly distributed in . We consider the target conditional
probability of the form P(Y = 1|X) = 2{dX}I({dX} â‰¤0.5) + 2(1 âˆ’{dX}) Ã—
I({dX} > 0.5), where d â‰¥1 is an integer which controls the complexity of the
target function, and I denotes the set indicator function. We have also used the
notation {z} = z âˆ’âŒŠzâŒ‹to denote the decimal part of a real number z, with the standard notation of âŒŠzâŒ‹for the integer part of z. The Bayes error rate of our model is
always 0.25.
Graphically, the target conditional probability contains d triangles. Figure 1
plots such a target for d = 2.
BOOSTING WITH EARLY STOPPING
Target conditional probability for d = 2.
We use one-dimensional stumps of the form I([0,a]) as our basis functions,
where a is a parameter in . They form a complete basis since each interval
indicator function I((a,b]) can be expressed as I([0,b]) âˆ’I([0,a]).
There have been a number of experimental studies on the impact of using
different convex loss functions (e.g., see ). Although our theoretical
analysis applies to general loss functions, it is not reï¬ned enough to suggest that
any one particular loss is better than another. For this reason, our experimental
study will not include a comprehensive comparison of different loss functions. This
task is better left to dedicated empirical studies (such as some of those mentioned
We will only focus on consequences of our analysis which have not been
well studied empirically. These include various issues related to early stopping
and their impact on the performance of boosting. For this purpose, throughout
the experiments we shall only use the least-squares loss function. In fact, it is
known that this loss function works quite well for many classiï¬cation problems
(see, e.g., ) and has been widely applied to many pattern-recognition
applications. Its simplicity also makes it attractive.
For the least-squares loss, the target function which the boosting procedure
tries to estimate is fâˆ—(x) = 2P(Y = 1|X = x) âˆ’1. In our experiments, unless
otherwise noted, we use boosting with restricted step-size, where at each iteration
we limit the step-size to be no larger than hi = (i +1)âˆ’2/3. This choice satisï¬es our
numerical convergence requirement, where we need the conditions 
i hi = âˆand
i < âˆ. Therefore it also satisï¬es the consistency requirement in Theorem 3.1.
6.2. Early stopping and overï¬tting.
Although it is known that boosting forever
can overï¬t (e.g., see ), it is natural to begin our experiments by graphically
showing the effect of early-stopping on the predictive performance of boosting.
T. ZHANG AND B. YU
Graphs of boosting estimators after k = 32 and 1024 iterations.
We shall use the target conditional probability described earlier with complexity
d = 2, and training sample-size of 100. Figure 2 plots the graphs of estimators
obtained after k = 32 and 1024 boosting iterations. The dotted lines on the
background show the true target function fâˆ—(x) = 2P(Y = 1|X = x). We can see
that after 32 iterations, the boosting estimator, although not perfect, roughly has the
same shape as that of the true target function. However, after 1024 iterations, the
graph appears quite random, implying that the boosting estimator starts to overï¬t
Figure 3 shows the predictive performance of boosting versus the number of
iterations. The need for early stopping is quite apparent in this example. The
excessive classiï¬cation error quantity is deï¬ned as the true classiï¬cation error
of the estimator minus the Bayes error (which is 0.25 in our case). Similarly,
the excessive convex loss quantity is deï¬ned as the true least-squares loss of the
estimator minus the optimal least-squares loss of the target function fâˆ—(x). Both
excessive classiï¬cation error and convex loss are evaluated through numerical
Predictive performance of boosting as a function of boosting iterations.
BOOSTING WITH EARLY STOPPING
Training error.
integration for a given decision rule. Moreover, as we can see from Figure 4, the
training error continues to decrease as the number of boosting iterations increases,
which eventually leads to overï¬tting of the training data.
6.3. Early stopping and total step-size.
Since our theoretical analysis favors
restricted step-size, a relevant question is what step-size we should choose. We
are not the ï¬rst authors to look into this issue. For example, Friedman and his coauthors suggested using small steps . In fact, they argued that the smaller
the step-size, the better. They performed a number of empirical studies to support
this claim. Therefore we shall not reinvestigate this issue here. Instead, we focus on
a closely related implication of our analysis, which will be useful for the purpose
of reporting experimental results in later sections.
Let Â¯Î±i be the step-size taken by the boosting algorithm at the ith iteration. Our
analysis characterizes the convergence behavior of boosting after the kth step,
not by the number of iterations k itself, but rather by the quantity sk = 
in (10), as long as Â¯Î±i â‰¤hi âˆˆi. Although our theorems are stated with the quantity
iâ‰¤k hi, instead of 
iâ‰¤k Â¯Î±i, it does suggest that in order to compare the behavior
of boosting under different conï¬gurations, it is more natural to use the quantity
iâ‰¤k Â¯Î±i (which we shall call total step-size throughout later experiments) as a
measure of stopping point rather than the actual number of boosting iterations.
This concept of total step-size also appeared in .
Figure 5 shows the predictive performance of boosting versus the total stepsize. We use 100 training examples, with the target conditional probability of
complexity d = 3. The unrestricted step-size method uses exact optimization.
Note that for least-squares loss, as explained in Section A.3, the resulting stepsizes will still satisfy our consistency condition 
i < âˆ. The restricted
step-size scheme with step-size â‰¤h employs a constant step-size restriction of
Ë†Î±i â‰¤h. This experiment shows that the behavior of these different boosting
T. ZHANG AND B. YU
Predictive performance of boosting as a function of total step-size.
methods is quite similar when we measure the performance not by the number of
boosting iterations, but instead by the total step-size. This observation justiï¬es our
theoretical analysis, which uses quantities closely related to the total step-size to
characterize the convergence behavior of boosting methods. Based on this result, in
the next few experiments we shall use the total step-size (instead of the number of
boosting iterations) to compare boosting methods under different conï¬gurations.
6.4. The effect of sample-size on early stopping.
An interesting issue for
boosting with early stopping is how its predictive behavior changes when the
number of samples increases. Although our analysis does not offer a quantitative
characterization, it implies that we should stop later (and the allowable stopping
range becomes wider) when sample size increases. This essentially suggests that
the optimal stopping point in the boosting predictive performance curve will
increase as the sample size increases, and the curve itself becomes ï¬‚atter. It follows
that when the sample size is relatively large, we should run boosting algorithms for
a longer time, and it is less necessary to do aggressive early stopping.
The above qualitative characterization of the boosting predictive curve also has
important practical consequences. We believe this may be one reason why in many
practical problems it is very difï¬cult for boosting to overï¬t, and practitioners
often observe that the performance of boosting keeps improving as the number
of boosting iterations increases.
Figure 6 shows the effect of sample size on the behavior of the boosting method.
Since our theoretical analysis applies directly to the convergence of the convex
loss (the convergence of classiï¬cation error follows implicitly as a consequence
of convex loss convergence), the phenomenon described above is more apparent
for excessive convex loss curves. The effect on classiï¬cation error is less obvious,
which suggests there is a discrepancy between classiï¬cation error performance and
convex loss minimization performance.
BOOSTING WITH EARLY STOPPING
Predictive performance of boosting at different sample sizes.
6.5. Early stopping and consistency.
In this experiment we demonstrate that
as sample size increases, boosting with early stopping leads to a consistent
estimator with its error rate approaching the optimal Bayes error. Clearly, it is
not possible to prove consistency experimentally, which requires running a sample
size of âˆ. We can only use a ï¬nite number of samples to demonstrate a clear trend
that the predictive performance of boosting with early stopping converges to the
Bayes error when the sample size increases. Another main focus of this experiment
is to compare the performance of different early stopping strategies.
Theoretical results in this paper suggest that for least squares loss, we can
achieve consistency as long as we stop at total step-size approximately mÏ with
Ï < 1/4, where m is the sample size. We call such an early stopping strategy
the Ï-strategy. Since our theoretical estimate is conservative, we examine the
Ï-strategy both for Ï = 1/6 and for Ï = 1/4. Instead of the theoretically motivated
(and suboptimal) Ï-strategy, in practice one can use cross validation to determine
the stopping point. We use a sample size of one-third the training data to estimate
the optimal stopping total step-size which minimizes the classiï¬cation error on the
validation set, and then use the training data to compute a boosting estimator which
stops at this cross-validation-determined total step-size. This strategy is referred
to as the cross validation strategy. Figure 7 compares the three early stopping
strategies mentioned above. It may not be very surprising to see that the crossvalidation-based method is more reliable. The Ï-strategies, although they perform
less well, also demonstrate a trend of convergence to consistency. We have also
noticed that the cross validation scheme stops later than the Ï-strategies, implying
that our theoretical results impose more restrictive conditions than necessary.
It is also interesting to see how well cross validation ï¬nds the optimal stopping
point. In Figure 8 we compare the cross validation strategy with two oracle
strategies which are not implementable: one selects the optimal stopping point
which minimizes the true classiï¬cation error (which we refer to as optimal error),
and the other selects the optimal stopping point which minimizes the true convex
T. ZHANG AND B. YU
Consistency and early stopping.
loss (which we refer to as optimal convex risk). These two methods can be regarded
as ideal theoretical stopping points for boosting methods. The experiment shows
that cross validation performs quite well at large sample sizes.
In the log coordinate space, the convergence curve of boosting with the cross
validation stopping criterion is approximately a straight line, which implies that the
excess errors decrease as a power of the sample size. By extrapolating this ï¬nding,
it is reasonable for us to believe that boosting with early stopping converges to
the Bayes error in the limit, which veriï¬es the consistency. The two Ï stopping
rules, even though showing much slower linear convergence trend, also lead to
consistency.
6.6. The effect of target function complexity on early stopping.
Although we
know that boosting with an appropriate early stopping strategy leads to a consistent
estimator in the large sample limit, the rate of convergence depends on the
Consistency and early stopping.
BOOSTING WITH EARLY STOPPING
The effect of target function complexity.
complexity of the target function (see Section 5.3). In our analysis the complexity
can be measured by the 1-norm of the target function. For target functions
considered here, it is not very difï¬cult to show that in order to approximate to an
accuracy within Îµ, it is only necessary to use a combination of our decision stumps
with the 1-norm Cd/Îµ. In this formula C is a constant and d is the complexity of
the target function.
Our analysis suggests that the convergence behavior of boosting with early
stopping depends on how easy it is to approximate the target function using a
combination of basis functions with small 1-norm. A target with d = u is u-times
as difï¬cult to approximate as a target with d = 1. Therefore the optimal stopping
point, measured by the total step-size, should accordingly increase as d increases.
Moreover, the predictive performance becomes worse. Figure 9 illustrates this
phenomenon with d = 1,3,5 at the sample size of 300. Notice again that since
our analysis applies to the convex risk, this phenomenon is much more apparent
for the excessive convex loss performance than the excessive classiï¬cation error
performance. Clearly this again shows that although by minimizing a convex loss
we indirectly minimize the classiï¬cation error, these two quantities do not behave
identically.
7. Conclusion.
In this paper we have studied a general version of the boosting
procedure given in Algorithm 2.1. The numerical convergence behavior of this
algorithm has been studied using the so-called averaging technique, which was
previously used to analyze greedy algorithms for optimization problems deï¬ned
in the convex hull of a set of basis functions. We have derived an estimate
of the numerical convergence speed and established conditions that ensure the
convergence of Algorithm 2.1. Our results generalize those in previous studies,
such as the matching pursuit analysis in and the convergence analysis of
AdaBoost by Breiman .
T. ZHANG AND B. YU
Furthermore, we have studied the learning complexity of boosting algorithms
based on the Rademacher complexity of the basis functions. Together with the
numerical convergence analysis, we have established a general early stopping
criterion for greedy boosting procedures for various loss functions that guarantees
the consistency of the obtained estimator in the large sample limit. For speciï¬c
choices of step-sizes and sample-independent stopping criteria, we have also been
able to establish bounds on the statistical rate of convergence. We would like to
mention that the learning complexity analysis given in this paper is rather crude.
Consequently, the required conditions in our consistency strategy may be more
restrictive than one actually needs.
A number of experiments were presented to study various aspects of boosting
with early stopping. We speciï¬cally focused on issues that have not been covered
by previous studies. These experiments show that various quantities and concepts
revealed by our theoretical analysis lead to observable consequences. This suggests
that our theory can lead to useful insights into practical applications of boosting
algorithms.
A.1. Loss function examples.
We list commonly used loss functions that
satisfy Assumption 3.1. They show that for a typical boosting loss function Ï†,
there exists a constant M such that supa M(a) â‰¤M. All loss functions considered
are convex.
A.1.1. Logistic regression.
This is a traditional loss function used in statistics,
which is given by (in natural log form here)
Ï†(f,y) = ln
1 + exp(âˆ’fy)
We assume that the basis functions satisfy the condition
|g(x)| â‰¤1,
It can be veriï¬ed that A(f ) is convex differentiable. We also have
f,g(0) = EX,Y
(1 + exp(f (X)Y))(1 + exp(âˆ’f (X)Y)) â‰¤1
A.1.2. Exponential loss.
This loss function is used in the AdaBoost algorithm,
which is the original boosting procedure for classiï¬cation problems. It is given by
Ï†(f,y) = exp(âˆ’fy),
Ïˆ(u) = lnu.
Again we assume that the basis functions satisfy the condition
|g(x)| â‰¤1,
BOOSTING WITH EARLY STOPPING
In this case it is also not difï¬cult to verify that A(f ) is convex differentiable. Hence
we also have
f,g(0) = EX,Y g(X)2Y 2 exp(âˆ’f (X)Y)
EX,Y exp(âˆ’f (X)Y)
âˆ’[EX,Y g(X)Y exp(âˆ’f (X)Y)]2
[EX,Y exp(âˆ’f (X)Y)]2
A.1.3. Least squares.
The least squares formulation has been widely studied
in regression, but can also be applied to classiï¬cation problems .
A greedy boosting-like procedure for least squares was ï¬rst proposed in the
signal processing community, where it was called matching pursuit . The loss
function is given by
Ï†(f,y) = 1
We impose the following weaker condition on the basis functions:
EXg(X)2 â‰¤1,
EY Y 2 < âˆ.
It is clear that A(f ) is convex differentiable, and the second derivative is bounded
f,g(0) = EXg(X)2 â‰¤1.
A.1.4. Modiï¬ed least squares.
For classiï¬cation problems we may consider
the following modiï¬ed version of the least squares loss, which has a better
approximation property :
Ï†(f,y) = 1
2 max(1 âˆ’fy,0)2,
Since this loss is for classiï¬cation problems, we impose the condition
EXg(X)2 â‰¤1,
It is clear that A(f ) is convex differentiable, and we have the following bound for
the second derivative:
f,g(0) â‰¤EXg(X)2 â‰¤1.
A.1.5. p-norm boosting.
p-norm loss can be interesting both for regression
and for classiï¬cation. In this paper we will only consider the case with p â‰¥2,
Ï†(f,y) = |f âˆ’y|p,
2(p âˆ’1)u2/p.
We impose the condition
EX|g(X)|p â‰¤1,
EY |Y|p < âˆ.
T. ZHANG AND B. YU
Now let u = EX,Y |f (X) + hg(X) âˆ’Y|p; we have
p âˆ’1u(2âˆ’p)/pEX,Y g(X)sign
f (X) + hg(X) âˆ’Y
Ã— |f (X) + hg(X) âˆ’Y|pâˆ’1.
Therefore the second derivative can be bounded as
f,g(h) = u(2âˆ’p)/pEX,Y g(X)2|f (X) + hg(X) âˆ’Y|pâˆ’2
p âˆ’1u(2âˆ’2p)/p
EX,Y g(X)sign
f (X) + hg(X) âˆ’Y
|f (X) + hg(X) âˆ’Y|pâˆ’12
â‰¤u(2âˆ’p)/pEX,Y g(X)2|f (X) + hg(X) âˆ’Y|pâˆ’2
â‰¤u(2âˆ’p)/pE2/p
X,Y |g(X)|p E(pâˆ’2)/p
|f (X) + hg(X) âˆ’Y|p
X,Y |g(X)|p â‰¤1,
where the second inequality follows from HÃ¶lderâ€™s inequality with the duality pair
(p/2,p/(p âˆ’2)).
REMARK A.1.
Similar to the least squares case, we can deï¬ne the modiï¬ed
p-norm loss for classiï¬cation problems. Although the case p âˆˆ(1,2) can be
handled by the proof techniques used in this paper, it requires a modiï¬ed analysis
since in this case the corresponding loss function is not second-order differentiable
at zero. See related discussions in . Note that the hinge loss used in support
vector machines cannot be handled directly with our current technique since its
ï¬rst-order derivative is discontinuous. However, one may approximate the hinge
loss with a continuously differentiable function, which can then be analyzed.
A.2. Numerical convergence proofs.
This section contains two proofs for
the numerical convergence analysis section (Section 4.1).
PROOF OF THE ONE-STEP ANALYSIS OR LEMMA 4.1.
Given an arbitrary
ï¬xed reference function Â¯f âˆˆspan(S) with the representation
we would like to compare A(fk) to A( Â¯f ). Since Â¯f is arbitrary, we use such a
comparison to obtain a bound on the numerical convergence rate.
Given any ï¬nite subset Sâ€² âŠ‚S such that Sâ€² âŠƒ{ Â¯fj}, we can represent
minimally as
BOOSTING WITH EARLY STOPPING
Sâ€² = Â¯wj when g = Â¯fj for some j, and Â¯wg
Sâ€² = 0 when g /âˆˆ{ Â¯fj}. A quantity
that will appear in our analysis is âˆ¥Â¯wSâ€²âˆ¥1 = 
gâˆˆSâ€² | Â¯wg
Sâ€²|. Since âˆ¥Â¯wSâ€²âˆ¥1 = âˆ¥Â¯wâˆ¥1,
without any confusion, we will still denote Â¯wSâ€² by Â¯w with the convention that
Â¯wg = 0 for all g /âˆˆ{ Â¯fj}.
Given this reference function Â¯f , let us consider a representation of fk as a linear
combination of a ï¬nite number of functions Sk âŠ‚S, where Sk âŠƒ{ Â¯fj} is to be
chosen later. That is, with g indexing an arbitrary function in Sk, we expand fk in
terms of f g
k â€™s which are members of Sk with coefï¬cients Î²g
With this representation, we deï¬ne
Wk = âˆ¥Â¯w âˆ’Î²kâˆ¥1 =
Recall that in the statement of the lemma, the convergence bounds are in terms
of âˆ¥Â¯wâˆ¥1 and a sequence of nondecreasing numbers sk, which satisfy the condition
sk = âˆ¥f0âˆ¥1 +
|Â¯Î±k| â‰¤hk âˆˆk,
where hk can be any real number that satisï¬es the above inequality, which may or
may not depend on the actual step-size Â¯Î±k computed by the boosting algorithm.
Using the deï¬nition of 1-norm for Â¯f and since f0 âˆˆspan(S), it is clear that for
all Îµ > 0 we can choose a ï¬nite subset Sk âŠ‚S, vector Î²k and vector Â¯w such that
k | â‰¤sk + Îµ/2,
âˆ¥Â¯wâˆ¥1 â‰¤âˆ¥Â¯f âˆ¥1 + Îµ/2.
It follows that with appropriate representation, the following inequality holds for
all Îµ > 0:
Wk â‰¤sk + âˆ¥Â¯f âˆ¥1 + Îµ.
We now proceed to show that even in the worse case, the value A(fk+1)âˆ’A( Â¯f )
decreases from A(fk) âˆ’A( Â¯f ) by a reasonable quantity.
The basic idea is to upper bound the minimum of a set of numbers by an
appropriately chosen weighted average of these numbers. This proof technique,
which we shall call â€œaveraging method,â€ was used in for analysis of
greedy-type algorithms.
For hk that satisï¬es (10), the symmetry of k implies hk sign( Â¯wg âˆ’Î²g
Therefore the approximate minimization step (3) implies that for all g âˆˆSk, we
A(fk+1) â‰¤A(fk + hksgg) + Îµk,
sg = sign( Â¯wg âˆ’Î²g
T. ZHANG AND B. YU
Now multiply the above inequality by | Â¯wg âˆ’Î²g
k | and sum over g âˆˆSk; we obtain
A(fk+1) âˆ’Îµk
k âˆ’Â¯wg|A(fk + hksgg) =: B(hk).
We only need to upper bound B(hk), which in turn gives an upper bound
on A(fk+1).
We recall a simple but important property of a convex function that follows
directly from the deï¬nition of convexity of A(f ) as a function of f :for all f1,f2
A(f2) â‰¥A(f1) + âˆ‡A(f1)T (f2 âˆ’f1).
If A(fk) âˆ’A( Â¯f ) < 0, then
A(fk) = 0. From 0 âˆˆk and (3), we obtain
A(fk+1) âˆ’A( Â¯f ) â‰¤A(fk) âˆ’A( Â¯f ) + Îµk â‰¤Â¯Îµk,
which implies (13). Hence the lemma holds in this case. Therefore in the following,
we assume that A(fk) âˆ’A( Â¯f ) â‰¥0.
Using Taylor expansion, we can bound each term on the right-hand side of (25)
A(fk + hksgg) â‰¤A(fk) + hksgâˆ‡A(fk)T g + h2
fk,g(Î¾hksg).
Since Assumption 3.1 implies that
fk,g(Î¾hksg) = sup
fk+Î¾hk,g(0) â‰¤M(âˆ¥fkâˆ¥1 + hk),
A(fk + hksgg) â‰¤A(fk) + hksgâˆ‡A(fk)T g + h2
2 M(âˆ¥fkâˆ¥1 + hk).
Taking a weighted average, we have
k âˆ’Â¯wg|A(fk + hksgg)
A(fk) + âˆ‡A(fk)T hksgg + h2
2 M(âˆ¥fkâˆ¥1 + hk)
WkA(fk) + hkâˆ‡A(fk)T ( Â¯f âˆ’fk) + h2
WkM(âˆ¥fkâˆ¥1 + hk)
WkA(fk) + hk[A( Â¯f ) âˆ’A(fk)] + h2
WkM(âˆ¥fkâˆ¥1 + hk).
The last inequality follows from (26). Now using (25) and the bound âˆ¥fkâˆ¥1 +hk â‰¤
sk+1, we obtain
A(fk+1) âˆ’A( Â¯f )
A(fk) âˆ’A( Â¯f )
2 M(sk+1).
BOOSTING WITH EARLY STOPPING
Now replace
Wk by the right-hand side of (24) with Îµ â†’0; we obtain the lemma.
PROOF OF THE MULTISTEP ANALYSIS OR LEMMA 4.2.
Note that for all
1 âˆ’sâ„“+1 âˆ’sâ„“
sk+1 + a .
By recursively applying (13) and using the above inequality, we obtain
sâ„“+ âˆ¥Â¯f âˆ¥1
sâ„“+ âˆ¥Â¯f âˆ¥1
s0 + âˆ¥Â¯f âˆ¥1
sk+1 + âˆ¥Â¯f âˆ¥1
sj+1 + âˆ¥Â¯f âˆ¥1
sk+1 + âˆ¥Â¯f âˆ¥1
A.3. Discussion of step-size.
We have been deriving our results in the case of
restricted step-size in which the crucial small step-size condition is explicit. In this
section we investigate the case of unrestricted step-size under exact minimization,
for which we show that the small step-size condition is actually implicit if the
boosting algorithm converges. The implication is that the consistency (and rate of
convergence) results can be extended to such a case, although the analysis becomes
more complicated.
Let k = R for all k, so that the size of Â¯Î±k in the boosting algorithm is
unrestricted. For simplicity, we will only consider the case that supa M(a) is upper
bounded by a constant M.
Interestingly enough, although the size of Â¯Î±k is not restricted in the boosting
algorithm itself, for certain formulations the inequality 
j < âˆstill holds.
Theorem 4.1 can then be applied to show the convergence of such boosting
procedures. For convenience, we will impose the following additional assumption
for the step-size Â¯Î±k in Algorithm 2.1:
A(fk + Â¯Î±k Â¯gk) = inf
Î±kâˆˆR A(fk + Î±k Â¯gk),
which means that given the selected basis function Â¯gk, the corresponding Â¯Î±k is
chosen to be the exact minimizer.
T. ZHANG AND B. YU
LEMMA A.1.
Assume that Â¯Î±k satisï¬es (27). If there exists a positive constant
c such that
Î¾âˆˆ(0,1)Aâ€²â€²
(1âˆ’Î¾)fk+Î¾fk+1, Â¯gk(0) â‰¥c,
j â‰¤2câˆ’1[A(f0) âˆ’A(fk+1)].
Since Â¯Î±k minimizes Afk, Â¯gk(Î±), Aâ€²
fk, Â¯gk(Â¯Î±k) = 0. Using Taylor expansion, we obtain
Afk, Â¯gk(0) = Afk, Â¯gk(Â¯Î±k) + 1
fk, Â¯gk(Î¾k Â¯Î±k)Â¯Î±2
where Î¾k âˆˆ(0,1). That is, A(fk) = A(fk+1) + 1
fk, Â¯gk(Î¾k Â¯Î±k)Â¯Î±2
k. By assumption,
we have Aâ€²â€²
fk, Â¯gk(Î¾k Â¯Î±k) â‰¥c. It follows that, âˆ€j â‰¥0, Â¯Î±2
j â‰¤2câˆ’1[A(fj) âˆ’A(fj+1)].
We can obtain the lemma by summing from j = 0 to k.
By combining Lemma A.1 and Corollary 4.1, we obtain:
COROLLARY A.1.
Assume that supa M(a) < +âˆand Îµj in (3) satisï¬es
j=0 Îµj < âˆ. Assume also that in Algorithm 2.1 we let k = R and let Â¯Î±k
satisfy (27). If
Î¾âˆˆ(0,1)Aâ€²â€²
(1âˆ’Î¾)fk+Î¾fk+1, Â¯gk(0) > 0,
kâ†’âˆA(fk) =
f âˆˆspan(S)A(f ).
If limkâ†’âˆA(fk) = âˆ’âˆ, then the conclusion is automatically true.
Otherwise, Lemma A.1 implies that âˆ
j < âˆ. Now choose hj = |Â¯Î±j| +
1/(j + 1) in (10); we have âˆ
j=0 hj = âˆ, and âˆ
j < âˆ. The convergence
now follows from Corollary 4.1.
Least squares loss.
The convergence of unrestricted step-size boosting using
least squares loss (matching pursuit) was studied in . Since a scaling of the
basis function does not change the algorithm, without loss of generality we can
assume that EXg(X)2 = 1 for all g âˆˆS (assume S does not contain function 0). In
this case it is easy to check that for all g âˆˆS,
f,g(0) = EXg(X)2 = 1.
BOOSTING WITH EARLY STOPPING
Therefore the conditions in Corollary A.1 are satisï¬ed as long as âˆ
j=0 Îµj < âˆ.
This shows that the matching pursuit procedure converges, that is,
kâ†’âˆA(fk) =
f âˆˆspan(S)A(f ).
We would like to point out that for matching pursuit, the inequality in Lemma A.1
can be replaced by the equality
j = 2[A(f0) âˆ’A(fk+1)],
which was referred to as â€œenergy conservationâ€ in , and was used there to
prove the convergence.
Exponential loss.
The convergence behavior of boosting with exponential
loss was previously studied by Breiman for Â±1-trees under the assumption
infx P(Y = 1|x)P(Y = âˆ’1|x) > 0. Using exact computation, Breiman obtained
an equality similar to the matching pursuit energy conservation equation. As part
of the convergence analysis, the equality was used to show âˆ
The following lemma shows that under a more general condition, the convergence of unrestricted boosting with exponential loss follows directly from Corollary A.1. This result extends that of , but the condition still constrains the class
of measures that generate the joint distribution of (X,Y).
LEMMA A.2.
Assume that
gâˆˆS EX|g(X)|
P(Y = 1|X)P(Y = âˆ’1|X) > 0.
If Â¯Î±k satisï¬es (27), then infk infÎ¾âˆˆ(0,1) Aâ€²â€²
(1âˆ’Î¾)fk+Î¾fk+1, Â¯gk(0) > 0. Hence 
For notational simplicity, we let qX,Y = exp(âˆ’f (X)Y). Recall that
the direct computation of Aâ€²â€²
f,g(0) in Section A.1.2 yields
[EX,Y qX,Y ]2Aâ€²â€²
= [EX,Y g(X)2qX,Y ][EX,Y qX,Y ] âˆ’[EX,Y g(X)YqX,Y ]2
= [EXg(X)2EY|XqX,Y ][EXEY|XqX,Y ] âˆ’[EXg(X)EY|XYqX,Y ]2
â‰¥[EXg(X)2EY|XqX,Y ][EXEY|XqX,Y ]
âˆ’[EXg(X)2|EY|XYqX,Y |][EX|EY|XYqX,Y |]
â‰¥[EXg(X)2EY qX,Y ]EX[EY|XqX,Y âˆ’|EY|XYqX,Y |]
EY qX,Y (EY|XqX,Y âˆ’|EY|XYqX,Y |)
2P(Y = 1|X)P(Y = âˆ’1|X)
T. ZHANG AND B. YU
The ï¬rst and the third inequalities follow from Cauchyâ€“Schwarz, and the
last inequality used the fact that (a + b)((a + b) âˆ’|a âˆ’b|) â‰¥2ab. Now
observe that EX,Y qX,Y = exp(A(f )). The exact minimization (27) implies that
A(fk) â‰¤A(f0) for all k â‰¥0. Therefore, using Jensenâ€™s inequality we know that
âˆ€Î¾ âˆˆ(0,1),A((1 âˆ’Î¾)fk + Î¾fk+1) â‰¤A(f0). This implies the desired inequality,
(1âˆ’Î¾)fk+Î¾fk+1, Â¯gk(0)
EX| Â¯gk(X)|
2P(Y = 1|X)P(Y = âˆ’1|X)
Although unrestricted step-size boosting procedures can be successful in certain
cases, for general problems we are unable to prove convergence. In such cases the
crucial condition of âˆ
j < âˆ, as required in the proof of Corollary A.1, can
be violated. Although we do not have concrete examples at this point, we believe
boosting may fail to converge when this condition is violated.
For example, for logistic regression we are unable to prove a result similar
to Lemma A.2. The difï¬culty is caused by the near-linear behavior of the loss
function toward negative inï¬nity. This means that the second derivative is so small
that we may take an extremely large step-size when Â¯Î±j is exactly minimized.
Intuitively, the difï¬culty associated with large Â¯Î±j is due to the potential problem
of large oscillation in that a greedy step may search for a suboptimal direction,
which needs to be corrected later on. If a large step is taken toward the suboptimal
direction, then many more additional steps have to be taken to correct the mistake.
If the additional steps are also large, then we may overcorrect and go to some other
suboptimal directions. In general it becomes difï¬cult to keep track of the overall
A.4. The relationship of AdaBoost and L1-margin maximization.
a real-valued classiï¬cation function p(x), we consider the following discrete
prediction rule:
if p(x) â‰¥0,
if p(x) < 0.
Its classiï¬cation error [for simplicity we ignore the point p(x) = 0, which is
assumed to occur rarely] is given by
LÎ³ (p(x),y) =
if p(x)y â‰¤Î³ ,
if p(x)y > Î³ ,
with Î³ = 0. In general, we may consider Î³ â‰¥0 and the parameter Î³ â‰¥0 is often
referred to as margin, and we shall call the corresponding error function LÎ³ margin
In the authors proved that under appropriate assumptions on the base
learner, the expected margin error LÎ³ with a positive margin Î³ > 0 also
BOOSTING WITH EARLY STOPPING
decreases exponentially. It follows that regularity assumptions of weak learning
for AdaBoost imply the following margin condition: there exists Î³ > 0 such that
inff âˆˆspan(S),âˆ¥f âˆ¥1=1 LÎ³ (f,y) = 0, which in turn implies the inequality for all s > 0,
f âˆˆspan(S),âˆ¥f âˆ¥1=1EX,Y exp
 â‰¤exp(âˆ’Î³ s).
We now show that under (29) the expected margin errors (with small margin)
from Algorithm 2.1 may decrease exponentially. A similar analysis was given
in . However, the boosting procedure considered there was modiï¬ed so that
the estimator always stays in the scaled convex hull of the basis functions. This
restriction is removed in the current analysis:
supk â‰¤hk,
Note that this implies that Â¯Îµk â‰¤h2
k for all k.
Now applying (15) with
Â¯f = sf for any s > 0 and letting f approach the
minimum in (29), we obtain (recall âˆ¥f âˆ¥1 = 1)
A(fk) â‰¤âˆ’sÎ³
sk + s Â¯Îµjâˆ’1 â‰¤âˆ’sÎ³
Now let s â†’âˆ; we have
A(fk) â‰¤âˆ’Î³ sk +
Assume we pick a constant h < Î³ and let hk = h; then
âˆ’kh(Î³ âˆ’h)
which implies that the margin error decreases exponentially for all margins less
than Î³ âˆ’h. To see this, consider Î³ â€² < Î³ âˆ’h. Since âˆ¥fkâˆ¥1 â‰¤kh, we have from (30),
LÎ³ â€²fk(x)/âˆ¥fkâˆ¥1,y
fk(X)Y â‰¤khÎ³ â€²
âˆ’fk(X)Y + khÎ³ â€² â‰¤exp
âˆ’kh(Î³ âˆ’h âˆ’Î³ â€²)
kâ†’âˆLÎ³ â€²fk(x)/âˆ¥fkâˆ¥1,y
This implies that as h â†’0, fk(x)/âˆ¥fkâˆ¥1 achieves a margin that is within h of the
maximum possible. Therefore, when h â†’and k â†’âˆ, fk(x)/âˆ¥fkâˆ¥1 approaches a
maximum margin separator.
Note that in this particular case we allow a small step-size (h < Î³ ), which
violates the condition 
k < âˆimposed for the boosting algorithm to converge.
However, this condition that prevents large oscillation from occurring is only a
sufï¬cient condition to guarantee convergence. For speciï¬c problems, especially
T. ZHANG AND B. YU
when inff âˆˆspan(S) A(f ) = âˆ’âˆ, it is still possible to achieve convergence even if
the condition is violated.