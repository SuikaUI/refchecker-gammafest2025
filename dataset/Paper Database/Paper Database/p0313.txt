J. Austral. Math. Soc. Sen B 37 , 430-450
GRADIENT ALGORITHMS FOR PRINCIPAL COMPONENT
R. E. MAHONY1, U. HELMKE2 and J. B. MOORE1
 
The problem of principal component analysis of a symmetric matrix (finding a
p-dimensional eigenspace associated with the largest p eigenvalues) can be viewed as
a smooth optimization problem on a homogeneous space. A solution in terms of the limiting value of a continuous-time dynamical system is presented. A discretization of the
dynamical system is proposed that exploits the geometry of the homogeneous space. The
relationship between the proposed algorithm and classical methods are investigated.
1. Introduction
The problem of principal component analysis of a symmetric matrix N = N1 is that
of finding an eigenspace of specified dimension p > 1 which corresponds to the
maximal p eigenvalues of N. There are a number of classical algorithms available
for computing dominant eigenspaces (principal components) of a symmetric matrix.
A good reference for standard numerical methods is Golub and Van Loan .
There has been considerable interest in the last decade in using dynamical systems
to solve linear algebra problems (see the review and the recent monograph ). It
is desirable to consider the relationship between such methods and classical algebraic
methods. For example, Deift et al. investigated a matrix differential equation
based on the Toda flow, the solution of which (evaluated at integer times) is exactly the
sequence of iterates generated by the standard QR algorithm. In general, dynamical
system solutions of linear algebra problems do not interpolate classical methods
exactly. Discrete computational methods based on dynamical system solutions to
a given problem provide a way of comparing classical algorithms with dynamical
system methods. Recent work on developing numerical methods based on dynamical
1 Dept of Systems Eng, Research School of Phys. Sciences and Eng, ANU, Canberra ACT 0200.
2Dept of Mathematics, University of Regensburg, 8400 Regensburg, F.R.G.
© Australian Mathematical Society, 1996, Serial-fee code 0334-2700/96
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
systems insight is contained Brockett and Moore et al. .
Concentrating on the problem of principal component analysis, Ammar and Martin
 have studied the power method (for determining the dominant p-dimensional
eigenspace of a symmetric matrix) as a recursion on the Grassmannian manifold
GP(W), the set of all p-dimensional subspaces of K". Using local coordinate charts
on GP(R") Ammar and Martin show that the power method is closely related
to the solution of a matrix Riccati differential equation. Unfortunately, the solution
to a matrix Riccati equation may diverge to infinity in finite time. Such solutions
correspond to solutions that do not remain in the original local coordinate chart. In
his review paper, Chu derives the gradient flow of the Rayleigh quotient (for a
symmetric matrix) on the sphere. Chu's result is a vector differential equation in K"
whose limiting solution is the maximal eigenvector of the matrix considered. It turns
out that Chu's result is a simple case of a matrix differential equation proposed by Oja
 for the analysis of learning performance of single-layer neural networks with
n inputs and p neurons. The differential equation that Oja considers evolves on K"xp
and corresponds to the 'learning' procedure of the neural network. The columns of the
limiting solution span the principal component of the covariance matrix N = E[ukuJ)
(where E{ukuJ] is the expectation of ukuj) of the vector random process uk e K",
k = 1,2,..., with which the network was 'trained'. Recent work by Yan et al.
 has provided a rigorous analysis of the learning equation proposed by Oja. Not
surprisingly, it is seen that the solution to Oja's learning equations is closely related
to the solution of a Riccati differential equation [11, page 27].
In this paper we follow Yan et al. and study the properties of Oja's learning
equation restricted to the Stiefel manifold (the set of all n x p real matrices with
orthonormal columns). However, we differ from earlier treatments by considering a
homogeneous geometric structure on the Stiefel manifold. Oja's flow is derived as the
gradient flow of a generalised Rayleigh quotient and explicit proofs of convergence
for the flow are presented which extend the results of Yan et al. and Helmke and
Moore [11, page 26] so that no genericity assumption is required on the eigenvalues
of N. The homogeneous nature of the Stiefel manifold is exploited to develop an
explicit numerical method (a discrete-time system evolving on the Stiefel manifold) for
principal component analysis. The method proposed is a gradient ascent algorithm
modified to evolve explicitly on St(p, n). A step-size must be selected for each
iteration and a suitable selection scheme is proposed. A proof of convergence for
the proposed algorithm is given as well as modifications and observations aimed at
reducing the computational cost of implementing the algorithm on a digital computer.
The discrete method proposed is similar to the classical power method and steepest
ascent methods for determining the dominant p -eigenspace of a matrix N. Indeed, in
the case where p = 1 (for a particular choice of time-step) the discretization is shown
to be the power method. When p > 1, however, there are subtle differences between
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore
the methods.
The paper is organised into five sections including the introduction. Section 2
reviews the derivation of the Oja flow and gives a general proof of convergence.
In Section 3 a discrete-time iteration based on the results in Section 2 is proposed,
along with a suitable choice of time-step. Section 4 considers two modifications of
the scheme to reduce the computational cost of implementing the proposed numerical
algorithm. Finally Section 5 considers the relationship of the proposed algorithm to
classical methods.
2. Continuous-time gradient flow
In this section a dynamical systems solution to the problem of finding the principal
component of a matrix is developed, based on computing the gradient flow associated
with a generalised Rayleigh quotient function on the Stiefel manifold. The Stiefel
manifold is given the structure of a homogeneous space (rather than just an embedded
submanifold of K"xp). This approach differs from the approach previous authors have
taken in related work , and we take the time to present the geometry
before the principal result of this section is presented. The reader is referred to Warner
 for general technical details on Lie-groups and homogeneous spaces.
Let N = NT be a real symmetric n x n matrix with eigenvalues X{ > X2 >
... > Xn and an associated set of orthonormal eigenvectors V\,... , vn. A maximal
p-dimensional
eigenspace,
or maximal
p-eigenspace
of N, is sp{i>i,... , vp] the
subspace of K" spanned by {vi,... , vp). lfkp > kp+x then the maximal p-eigenspace
of N is unique. If kp = kp+i = • • • = kp+r, for some r > 0, then any subspace
sp{u!,... , up_i, w], where w e sp{up, vp+\,... , vp+r], is a maximal p-eigenspace
For p an integer with 1 < p < n, let
where Ip is the p x p identity matrix, denote the Stiefel manifold of real orthogonal
n x p matrices. For X e St(p, n), the columns of X are orthonormal basis vectors for
a p-dimensional subspace of K". The Stiefel manifold St(p, n) is a smooth compact
np — \p(p + 1)-dimensional submanifold of K"xp [11, page 25]. The proof given
in Helmke and Moore exploits the fact that lp is a regular point of the map
X i-> X^X. One can also think of St(p, n) as a homogeneous space and it is this
property of St(p, n) that is exploited later to develop a numerical method.
Let G = O(n) x O (p) be the topological product of the set of n x n and p x p real
orthogonal matrices O(n) = [U € W \ UJU = UUT = !„}. Then G is a compact
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
Lie-group [11, page 348]. It is easily verified that y : G x St(p, n) —> St(p, n) given
y«U,V),X):=UXVJ
is a smooth, transitive, group action of G on St(p, n). Since G is compact it follows
that St(p, n) is a compact embedded submanifold of W*p [11, page 352]. The tangent
space Tx St(p, n) of St(p, n) at a point X e St(p, n) is given by the image of the
linearization of yx • G —>• St(p, n),
yx(U):=y(U,X),
at the identity element of G [9, page 75]. Recall that the tangent space of 0{n) at the
identity is [11, page 349]
ThO(.n) = Sk(n) =
and consequently that the tangent space at the identity of G is T^nJp)G = Sk(n) x
Sk(p). It follows that
Tx St(p, n) = {QX - XTl | £2 € SJfc(/i), n € Sik(/>)}.
The natural Riemannian metric to use with the homogeneous structure of St(p, ri)
is the normal metric derived from a right invariant metric on the Lie-group G [13, page
127]. To construct this metric consider the Euclidean inner product on 1R"X" x Kpxp
„ n,), (Q2, n2)> - tr(J2[fi2) + tr(n|n2).
This induces a nondegenerate inner product on Tynjp)G. Given X e St(p, n), then
the linearization T^n,ip)yx of yx can be used to decompose the identity tangent space
= kQTTd»,ip)Yx
domTUnJi>)yx,
where ker T(inJp)yx is the kernel of TanJp)yx and
dom T(U,lf)yx = {(«,, nO € r(/<i/j>)G I <(fi,, n,), (fi, n)> =0, («, n) €kerr(/o,/;;)yx}
is the domain of T^,ip)yx (the subspace orthogonal to ker Tiln, ^yx using the Euclidean
inner product provided on T(injp)G). By construction, T(,n,p)yx restricts to a vector
space isomorphism T^, }>/x,
r(t./,)^ : d o m T<.ir.ip)Yx ->• Tx St(p, n),
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore
The normal Riemannian metric (see Helmke and Moore [11, page 52]) on St(p, n) is
the nondegenerate bilinear map on each tangent space Tx St(/?, n)
- xnu Q2X - xn2))x = tr((af )Tn£) + tr((nf )Tnx),
where fi,X - Xn, € Tx St(p, n) for i = 1, 2 and
(£2,, n,) = ((fi.Oj., (n,-)j.) e (n,x, nf)
is the decomposition of (£2,, Fl,) into components in kerT^j ^yx and dom r(/n>/ }yx
respectively. It is easily verified that {{•,•))x varies smoothly with X and defines a
Riemannian metric.
The manifold St(p, n) provides a smooth constraint set on which the problem of
principal component analysis may be considered. In the case p = 1 (where only a
single maximal eigenvector is desired) one may consider optimising the cost index
rN : R" - {0} -» R,
known as the classical Rayleigh quotient. Of course when x e St(l, n) then rN(x) =
xTNx. The generalised Rayleigh quotient on St(p, n) is denned as
RN : St(p, n) -> R,
RN(X) = tr(XJNX).
To confirm that optimizing RN on St(p, n) provides a solution to the problem of
principal component analysis, recall the Ky-Fan minimax principle [12, page 191],
which states
max RN(X) = A., + ... + kp,
min RN(X) = kn+i_p + ... + Xn.
Moreover, if X 6 St(p, n), such that RN(X) = Yl"j=\ *<> then the columns of X will
generate a basis for a maximal p-dimensional eigenspace of N.
We proceed by computing the gradient associated with RN on St(p, n) and showing
that the limiting solution of the continuous-time gradient ascent differential equation
converges to a maximum of RN.
THEOREM 2.1. Let N = NT be a real symmetric n x n matrix and p an integer
with 1 < p < n. Denote the eigenvalues of N by A., > ... > Xq with algebraic
multiplicities nu...
, nq such that Yll=\ni — n- F°r X e St(p,n), define the
generalised Rayleigh quotient RN : St(p, n) -+ R, RN(X) = tT(XJNX). Then we
have the following.
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
(i) The gradient of RN(X), on the Stiefel manifold St(p, n), with respect to the
normal Riemannian metric (4), is
gradRN(X) = (/„ - XXy)NX
= [XXT, N]X
(where the Lie bracket of two matrices A, B € IRnxn is
[A,B] := AB-
(ii) The critical points of RN (X) on St(p, n) are characterised by
[XX1, N] = 0
and correspond to points X € St(p, n), such that the columns of X span a
p-dimensional eigenspace of N.
(iii) For all initial conditions Xo e St(/?, n), the solution X(t) e St(/>, n) of
= (In-XXT)NX,
exists for all t € OS and converges to some matrix X^ e St(p, n) as t -*•
oo. For almost all initial conditions the solution X(t) of (7) converges
exponentially fast to a matrix whose columns form a basis for the maximal
p-eigenspace of N.
(iv) When p = 1 the exact solution to (7) is given by
where x0 6 S""1 =St(l,«).
PROOF. The gradient of RN is computed using the identities [11, page 356]
!=eTxSt(p,n)
ii) grad/?N(X)
eTxSt(p,n),
where DRN\x(t;) is the Fr6chet derivative of RN(X) in direction £ e
evaluated at the point X € St(p, n). Computing the Fre"chet derivative of RN in
direction SIX - XU € Tx St(p, n) gives
DRN\X(S2X - XU) = 2tT(XrN(QX - XU))
= 2ti(XXJNQ) -
2ti(XJNXn).
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore
Observe that tr(XTiVXn) = 0 since XJNX is symmetric and FI is skew symmetric.
Similarly only the skew symmetric part of XXT/V contributes to tr(XXTN £2). Thus,
- XTl) = tr([XXT,
= {([N,XXT]X,QX-Xn)))x,
using the Riemannian metric (4). The second line follows since any component of
[N, XXT] that lies in ker T(LJp)yx does not contribute to the value of tr([XXT, N]S2)
and of course [N, XXJ] e Sk(n) which ensures [N, XXT]X e Tx St(p, n). This
proves part (i).
At critical points of RN the gradient gradRN(X) is zero, [N, XXT]X = 0. It
follows that NX = X(XJNX)
and thus the columns of X € St(/?, n) span a peigenspace of N. Moreover [N, XXJ] = X(XJNX)XT
- X(XJNX)XT
= 0, which
proves part (ii).
Infinite-time existence of solutions to (7) follows from the compact nature of
St(p, n). By applying La'Salle's invariance principle, it is easily verified that X(t)
converges to a level set of RN for which grad RN(X) = 0. These sets are termed
critical sets. To show convergence to a single point (rather than just to a critical set)
requires a little extra effort.
An important property of the critical sets of RN is that they are a disjoint union
of smooth closed manifolds. An explicit proof of this result is contained in ; it
also follows from the more general development given in . Given X e St(p, n) a
critical point of RN let ^fx Q St(/?, n) denote the critical set containing X. Since ££x
is a submanifold of St(/?, ri) it has a well defined tangent space,
TxSfx = fa* - X n | f i e Sk(n), U e Skip), and [N, [Q, XXT]] = 0}, (9)
at the point X e St(p, n). To see this, observe that any curve Y(t), Y(0) = X, lying
in JS?X will satisfy [N, Y(t)Y(t)T]Y(t)
= 0. Similarly it is easily verified that any
curve (passing through Y(0) = X) satisfying this equality must lie in Jfx. Setting
Y(0) = S2X - X n € Tx St(p, n) and then evaluating
gives the above algebraic characterisation for TxJfx C Tx St(p, n).
Now at a critical point X e RN, the Hessian J f RN is a well-defined bilinear map
from Tx St(/J, n) to the reals [11, page 344]. Let (fi,X - XT\{) € Tx St(p, n) and
- Xn2) € Tx St(p, n) be arbitrary; then
, Q2X - Xn2) = Dn,x_xn, {DQ2X_x
= On.x-xn, tr([XXT,
= tr([[QuXXJ],N]Q2).
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
Observe that [£2i, XXT] is skew symmetric since XX1 is symmetric and £2] is skew
symmetric. Similarly, [[£2i, XXJ], N] is skew symmetric. Since fii and £22 are
arbitrary then JVRN is degenerate in exactly those tangent directions (SIX — XYl) e
Tx St(p, n) for which [[S2, XX1], N] = 0. But this corresponds exactly to (9) and one
concludes that the Hessian 3^RN degenerates only on the tangent space of the critical
set J£x- The above argument shows that RN is a Morse-Bott function on St(/?, n) [11,
page 361]. Applying Proposition C.12.3 from Appendix C completes the proof
of part (iii).
Part (iv) of the theorem is verified by explicitly evaluating the derivative of (8).
REMARK 2.2. In the case 1 < p < n no exact solution to (7) is known; however, for
X(t) a solution to (7) the solution for H(t) = X(f)X(0T is known, since
H(t) = XX1 + XX1
= NXXT + XX1N - 2XXJNXXJ
= NH(t) + H(t)N -2H(t)NH(t),
//(0) = X0Xl and this equation is a Riccati differential equation .
3. A gradient ascent algorithm
In this section a numerical algorithm for solving (7) is proposed. The algorithm
is based on a gradient ascent algorithm modified to ensure that each iteration lies in
Let Xo e St(p, n) and consider the recursive algorithm generated by
Xk+l = e-at[XkX^N]Xk,
for a sequence of positive real numbers ak, termed time-steps. The algorithm generated
by (11) is referred to as the Rayleigh gradient algorithm. The Lie-bracket [XkXj, N]
is skew symmetric and consequently e~aklXtXi /V] is orthogonal and Xk+\ e St(/?, n).
Observe also that
= (/, - XkXJ
k)NXk = grad
the gradient of RN at Xk. Thus, e z^XkXt 'N]Xk represents a curve in St(p, n), passing
through Xk at time T = 0, and with first derivative equal to grad^?w(^)- The
linearization of X*+I(r) = e~t[XkXt tN]Xk around r = 0 is
Xk+i(*) = Xk + r grad RN(Xk) + (higher order terms).
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore
The higher order terms modify the basic gradient ascent algorithm on K"xp to ensure
that the interpolation occurs along curves in St(p, n). For suitably small time-steps
ak, it is clear that (11) will closely approximate the gradient ascent algorithm on R"xp.
To implement the Rayleigh gradient algorithm it is necessary to choose a time-step
ak, for each step of the recursion. A convenient criteria for determining suitable
time-steps is to maximise the change in potential
ARN(Xk, ak) = RN(Xk+l) - RN(Xk).
It is possible to use line-search techniques to determine the optimal time-step for each
iteration of the algorithm. Completing a line search at each step of the iteration,
however, is computationally expensive and often results in worse stability properties
for the overall algorithm. Instead, a simple deterministic formulae for the time-step
based on maximising a lower bound AR'N(Xk, r) for (12) is provided.
LEMMA 3.1. For any Xk e St(p, n) such that gra.dRN(Xk) ^ 0, the recursive estimate
Xk+l = e-atlXkX?-N]Xk, where
2J\\N[XXjNn
satisfies ARN(Xk, ak) = RN(Xk+1) - RN(Xk) > 0.
PROOF. Denote Xk+i(z) = e~nXkXt tN]Xk for an arbitrary time-step r. Direct calculations show
-2tT(Xj+l(z)N[XkXr
x) = +4tT(XJ
k+l(z)N[Xkx],
N]2Xk+i(r)).
Taylor's formula for ARN(Xk, r) gives
ARN(Xk, t) = -2ztr(XjN[XkXJ,
+ 4T2 / tr(X]+i(z)N[XkXJ
k, N]2Xk+l(x))(l - s)ds
>2z\\[XkXj,N]\\2
- 4 T 2 / \\Xk+l(z)Xj+l(z)\\\\N[XkXj,
N]2\\(l - s)ds
= 2z\\[XkXJ, AHH2 - 2r2 Jp\\N[XkXj,
N]2\\ =: AR'N(Xk, z).
The quadratic nature of R'N(Xk, z) yields a unique maximum occurring at r = ak
given by (13). Observe that if grad/?„(**) ^ 0, then \\[XkXj, N]\\2 ^ 0 and thus
R'N(Xk, r) > 0. The result follows since ARN(Xk, z) > AR'N(Xk, z) > 0.
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
THEOREM 3.2. Let N = NT be a real symmetric n x n matrix, and p an integer with
1 < P < n. Denote the eigenvalues of N by kt > ... > Xn. For a given estimate
Xk € St(p, n), let ak be given by (13). The Rayleigh gradient algorithm,
has the following properties.
(i) The algorithm defines an iteration on St(p, n).
(ii) Fixed points of the algorithm are critical points of RN, X e St(p, n) such
that [XXT, N] = 0. The columns of a fixed point of (11) form a basis for a
p-dimensional eigenspace of N.
(iii) IfXk,for k = 1, 2,..., is a solution to the algorithm, then the real sequence
RN(Xk) is strictly monotonic increasing unless there is some k € N with Xk
a fixed point of the algorithm.
(iv) Let Xk,for k = 1, 2,..., be a solution to the algorithm. Then Xk converges
to a critical level set of RN on St(p, n).
(v) All critical level sets ofRN are unstable except the set for which the Rayleigh
quotient is maximised. The columns of an element of the maximal critical
level set form a basis for the maximal eigenspace of N.
PROOF. Part (i) follows from the observation that e~"k[XkXk -N] is orthogonal. Part (ii)
is a direct consequence of Lemma 3.1 (since ARN(Xk, ak) = 0 if and only if Xk is a
fixed point) and Theorem 2.1. Part (iii) also follows directly from Lemma 3.1.
To prove part (iv), observe that since St(/?, n) is a compact set, RN (Xk) is a bounded
monotonically increasing sequence which must converge. As a consequence Xk
converges to some level set of RN such that for any X in this set ARN(X, a(X)) = 0.
Lemma 3.1 ensures that any X in this set is a fixed point of the recursion.
If X is a fixed point of the recursion whose columns do not span the maximal
p-dimensional subspace of N, then it is clear that there exists an orthogonal matrix
U € O(n), with \\U - ln\\ arbitrarily small and such that RN(UX) > RN(X). As
a consequence, the initial condition Xo = UX (\\X0 — X\\ small) will give rise to a
sequence of matrices Xk that diverges from the level set containing X, Lemma 3.1.
This proves the first statement of (v), while the attractive nature of the remaining
fixed points follows from La'Salle's principle of invariance along with the Lyapunov
function V(X) = (£f=1 A,) -
REMARK 3.3. It is difficult to characterise the exact basin of attraction for the set of
matrices whose columns span the maximal p -eigenspace of N. It is conjectured that
the attractive basin for this set is all of St(p, n) except for other critical points.
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore
REMARK 3.4. For a fixed initial condition XQ e St(p, n) let Xk be the solution to (11).
Define Hk = XkXj and observe
Hk+i=e-a>lH»N*HkJ"M-N\
Thus Hk can be written as a recursion on the set of symmetric rank p projection
matrices {// e Knx" | H = HT, H2 = H, rank// = p). The algorithm generated
in this manner is known as the double-bracket algorithm , a discretization of the
continuous-time double-bracket equation (10).
To illustrate the Rayleigh gradient algorithm a simulation has been included. A
positive definite symmetric matrix N was randomly generated,
2.1180 -3.8054
-3.8054 0.3498
and its maximal two-dimensional eigenspace (denoted D2(N)) was computed using
a standard eigenvalue decomposition algorithm. A matrix U2 € St(4, 2) was chosen
such that D2(N) = sp((/2) was the span of the columns of U2. The Rayleigh gradient
algorithm (11) was initialised with a randomly generated matrix Xo e St(4, 2) and
run until the distance between the estimated and desired subspace was of order 10~4.
The distance between sp(X^) and the true maximal 2-dimensional eigenspace D2(N)
was computed by
dist(D2(A0, sp(X*)) = \\Xk - U2UjXk\\2 = p-
the Frobenius norm of the projection of Xk onto the complement of the span of U2.
A plot of the distance versus iteration for this example is given by Figure 1. Plotting
the distance on a logarithmic scale displays the linear convergence behaviour of the
algorithm.
4. Computational considerations
In this section, two issues related to implementing (11) in a digital environment
are discussed. Results in both the following subsections are aimed at reducing the
computational cost associated with estimating the matrix exponential e~"k[XkXt >N\ a
transcendental n x n matrix function. The result presented in Subsection 4.1 is also
important in Section 5.
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
FIGURE 1. Plot of dist (D2(N), sp (X*)) versus the iteration k for Xk a typical solution to (11).
4.1. An equivalent formulation
To implement (11) on conventional computer architecture, the main computational cost for each step of the algorithm lies in computing
the n x n matrix exponential e~"k[XtXk -N]. The following result provides an equivalent
formulation of the algorithm which involves the related p x p transcendental matrix
functions "cos" and "sine". In many applications it is only required to compute the
eigenspace associated with a couple of dominant eigenvalues, p << n, and considerable computational advantage can be obtained by using the following formulation.
Define the matrix function sine : Rpxp -> Kpxp by the convergent infinite sum
sinc(A) = / P - ^ 7 + T 7 "
Observe that A sinc(/4) = sin(A) and thus, if A is invertible, sinc(A) = A ' sin(A).
Define the matrix function cos(A) by an analogous power series expansion. The
matrix functions cos and sine are related by cos2(A) = Ip — A2 sinc2(A).
LEMMA 4.1. Let N = NJ be a real symmetric n x n matrix with eigenvalues kx >
... > kn, and at, for k = 1,2,..., be a sequence of real positive numbers. If
Xo e St (p, n) is an initial condition that is not a critical point of RN(X), then
= Xk (cos(akYk) - akXjNXk sinc(akYk)J + ockNXk sinc(akYk),
where the power expansions for
positive semi-definite matrix Yj € Kpx/7
are determined by the
Y2 = xjN(In - XkXj)NXk
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore
REMARK 4.2. The matrix Yk need not be explicitly calculated as the power series
expansions of sine and cos depend only on Y%.
PROOF. The proof follows from a power series expansion of e~"klXtXt 'N]Xk,
Xk+1 = (jT^i-adXtXl,
N])l\ • Xk.
Simple algebraic manipulations lead to the relation
[XkX], NfXk = -XkY2
where Y% is defined by (16).
Using (18) it is possible to rewrite (17) as a power series in (—Yf)
where the first and second terms in the summation follow from the odd and the even
powers of [XkXj, N]lXk respectively. On rewriting this as two separate power series
c y2im — (x (XJNX
} — NX i V^
k J/_^t (2m)\
= Xk cos(akYk) — a^ (xk(XjNXk)
— NXkj sinc(akYk),
the result follows by rearranging terms.
4.2. Pade approximations of the exponential It is also of interest to consider approximate methods for calculating matrix exponentials. In particular, one is interested
in methods that will not violate the constraint Xk+i e St(p, n). A standard approximation used for calculating the exponential function is a Pade" approximation of order
(n, m) where n > 0 and m > 0 are integers [10, page 557]. For example, a (1,1) Pad6
approximation of the exponential is
A key observation is that when n = m and the exponent is skew-symmetric, the
resulting Pad6 approximate is orthogonal. Thus
Xk+l = (/„ + °j[XkXJ, TV])"' (/„ - ^[XkXJ
k, W]) Xk,
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
with initial condition Xo e St(p, n), defines an iteration on St(p, n) which approximates the Rayleigh gradient algorithm (11). Of course, in practice one would use
an algorithm such as Gaussian elimination [10, page 92] to solve the linear system
j Xk+l = (/„ - j[XkXT
for Xk+1 rather than computing the inverse explicitly.
The algorithm defined by (20) can also be rewritten in a similar form to that obtained
in Lemma 4.1. Consider the power series expansion
(/„ + °^[XkxJ, N])'1 = JT (-^-[XkXJ, N])' .
From here it is easily shown that
where Y\ € Rpxp is given by (16).
5. Comparison with classical algorithms
In this section the relationship between the Rayleigh gradient algorithm (11) and
some classical algorithms for determining the maximal eigenspace of a symmetric
matrix are investigated. A good discussion of the power method and the steepest
ascent method for determining a single maximal eigenvalue of a symmetric matrix is
given by Faddeev and Faddeeva . Practical issues arising in implementing these
algorithms along with direct generalizations to eigenspace methods are covered by
Golub and Van Loan .
5.1. The power method In this subsection, the algorithm (11) in the case where
p = 1 is considered. It is shown that for a certain choice of time-step ak, the
algorithm (11) is the classical power method.
In the case where p = 1 then St(l, n) = [x e R" | ||JC|| = 1} = S""1, the (n - 1)dimensional sphere in K". The usual representation of the tangent space of S"~l is
TxS"~l = {f e R" | | Tx = 0} and the Riemannian metric induced from the standard
metriconR"is((£,r?)) = fTr?,for^, r) in TxSn~l [11,page 25]. It is easily verified that
these constructions are equivalent to considering St(l, n) as a homogeneous space.
THEOREM 5.1. Let N = NJ be a real symmetric n x n matrix with eigenvalues
A-i > • • • > K- For xk € S"~' let ak be given by
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore
where ^ e K is given by
For x0 e 5/(1, n) = S""1 an arbitrary initial condition, the following hold.
(i) The formula
e-'\*3>wXk
defines a recursive algorithm on S"~1.
(ii) Fixed points of the rank-1 Rayleigh gradient algorithm are the critical points
ofrN on S"~\ and are exactly the eigenvectors of N.
(iii) If xk,for k = 1,2,... is a solution to the Rayleigh gradient algorithm, then
the real sequence rN(xk) is strictly monotonic increasing, unless xk is an
eigenvector ofN.
(iv) For a given xk e S"~l which is not an eigenvector of N, then yk ^ 0 and
T», sin(a*y*)\
sin(a*;yt)
xk+i - I cos(akyk) - xk Nxk
(v) Let xk, for k = 1,2,... be a solution to the rank-1 Rayleigh gradient algorithm. Then xk converges to an eigenvector of N.
(vi) All eigenvectors of N, considered as fixed points of (24), are unstable, except the eigenvector corresponding to the maximal eigenvalue ku which is
exponentially stable.
PROOF. Parts (i)—(iii) follow directly from Theorem 3.2. To see part (iv) observe that
yk = || grad rN (xk) || and yk = 0 if and only if grad rN (xk) — 0 and xk is an eigenvector
of N. The recursive iteration (24) now follows directly from Lemma 4.1, with the
substitution sine((**)>*) = sin(akyk)/(akyk).
Parts (v) and (vi) again follow directly
from Theorem 3.2.
REMARK 5.2. Equation (24) involves only Nxk, x]Nxk and (Nxk)y(Nxk) vector computations. This structure is especially of interest when sparse or structured matrices
N are considered.
A geodesic (or great circle) on 5""1, passing through x at time t = 0, can be written
y (0 = COS(/)JC - sin(0 V,
where V = y(0) is a unit vector orthogonal to x.
Choosing V(xk) =
gradrw(*t)/|| gradrN(xk)\\, x = xk and evaluating y(t) at time t = ak\\ grad/>,(**)||
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
gives (24). Thus, (24) is a geodesic interpolation of (8), the solution to the rank-1
Rayleigh gradient flow (7).
For a symmetric n x n matrix N = NJ the classical power method is computed
using the recursive formulae [10, page 351]
xk+l =zk/\\zk\\.
The renormalisation operation is necessary if the algorithm is to be numerically stable.
The following lemma shows that for N positive semi-definite and a particular choice
of ctk the rank-1 Rayleigh gradient algorithm (24) is exactly the power method (26).
LEMMA 5.3. Let N = NJ be a positive semi-definite n xn matrix. For xk e S"~x (not
an eigenvector of N) then || gradrN(xk)\\ < ||Af;c||. Let ak be given by
. _, (\\ff*drN(xk)\\\
where sin"1 ( J L i ^ p i ) 6 (0, n/2). Then
cos(akyk) - xk Nxk
where yk is given by (23).
PROOF. Observe that || gradr^^)!!2 = yj = \\Nxk\\2 - (x]Nx2)2 > 0 and thus
|| grad/>(**)|| < IliVocjfc||. Consider the 2-dimensional linear subspace sp{xk, Nxk] of
1". The new estimate xk+x generated using either (24) or (26) will lie in sp{;q, Nxk]
(see Figure 2).
sp{xk,Nxtl
FIGURE 2. The geometric relationship between the power-method iterate and the iterate generated by
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore
iiA^ii= \cos(ryk) ~Xk
NXk^r)Xk+~^rNXk'
for r > 0 and observe that xk and Nxk are linearly independent. Then
cos(ryk)-xjNxk—-^-
Since N > 0 is positive definite, a real solution to the first relation exists for which
xyk e (0, jr/2).
The time-step value is now obtained by computing the smallest
positive root of the second relation.
Choosing N > 0 positive definite in Lemma 5.3 ensures that (24) and (26) converge
'generically' to the same eigenvector. Conversely, if N is symmetric with eigenvalues
X} > • • • > Xn, 0 > kn and \kn\ > |A.,|, then the power method will converge to the
eigenvector associated with Xn while (24) (equipped with time-step (22)) will converge
to the eigenvector associated with A.]. Nevertheless, one may still choose ak using
(27), with the inverse sine operation chosen to lie in the interval
such that (24) and (26) are equivalent. In this case the geodesies corresponding to
each iteration of (24) are describing great circles travelling almost from pole to pole
of the sphere.
5.2. The steepest ascent algorithm The gradient ascent algorithm for the
Rayleigh quotient rN is the recursion [8, page 430]
sk grad rN (xk)
xk+i = —k—
where sk > 0 is a real number termed the step-size, and gradrN(xk) is the Euclidiean
gradient computed in W. It is easily verified that the k + 1-th iterate of (28) will
also lie on the 2-dimensional linear subspace sp{xk, Nxk} of OS". Indeed, for xk not an
eigenvector of N, (24) and (28) are equivalent when
\cos(akyk)
The optimal step-size for the steepest ascent algorithm (that is, rN(xk+i(sk
rN(xk+i (sk)) for any sk e K) is [8, page 433]
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
It follows directly that the optimal time-step selection for (24) is given by
Substituting directly into (24) and analytically computing the composition of cos
and sin with cos"1 gives
pt given by (30). This recursion provides an optimal steepest ascent algorithm
with scaling factor
2, which converges to one as xk converges to an eigenvector
5.3. The generalised power method In both the power method and the steepest
ascent algorithm the rescaling operation preserves the computational stability of the
calculation. To generalise classical methods to the case where p > 1, (that is,
Xk € St(p, «)), one must decide on a procedure to renormalise new estimates to lie
on St(p, n). Thus a generalised power method may be written abstractly
Xk+l = rescale(ZA).
Since the span of the columns of Xk (denoted sp(X*)) is the quantity in which one is
interested, the rescaling operation is usually computed by generating an orthonormal
basis for sp(Zk) (that is, using the Gram-Schmidt algorithm [10, page 218]). Thus
Xk+1 = ZkG, and Xj+lXk+l = Ip, where G € ti&pxp contains the coefficients which
orthonormalise the columns of Zk. When Zk is full rank then G is invertible and the
factorization Zk = X^G'1
can be computed as a QR factorisation of Zk [10, page
211]. The matrix G depends on the particular algorithm employed in computing an
orthonormal basis for Zk.
When N > 0 is positive definite, the power method will act to maximise the generalised Rayleigh quotient RN (5). Different choices of G in the rescaling operation,
however, will affect the performance of the power method with respect to the relative
change in RN at each iteration. The optimal choice of G (for maximising the increase
in Rayleigh quotient) for the £-th step of (32) is given by a solution of the optimization
[G 6 R'*' | G^Z]ZkG = /„}
where Zk = NXk. The cost criterion tr (GTZjNZkG)
= RzJNZt(G) is a Rayleigh
quotient while the constraint set is similar in structure to St(p, n). Indeed, it appears
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore
that this optimization problem is qualitatively the same as explicitly solving for the
principal components of N.
One may still hope to obtain a similar result to Lemma 5.3 relating the generalised
power method to the Rayleigh gradient algorithm (11). Unfortunately, this not the
case except in nongeneric cases.
LEMMA 5.4. Let N = NT be a symmetric n x n matrix. For any Xk e St(p, n) let
Yk be the unique symmetric, positive semi-definite square root of Y2 = XjN2Xt —
There exists a matrix G € \&pxp and scalar ak > 0 such that
NXkG = e-"k[XtX^N]Xk,
if and only if one can solve
sin2(akYk)X]NXk
= cos(akYk) sin(akYk)Yk
PROOF. Assume that there exists a matrix G and a scalar ak > 0 such that (33)
holds. Observe that rank(e-ak[XtX?N]Xk)
= p and thus rank(NX^) = p. Similarly
G € Kpxp is non-singular.
Premultiplication (33) by G^XjN and use of the constraint relation
= Ip gives
kNe-ak[X^-N]Xk.
Since one need only consider the case where G is invertible, it follows that
G"1 = X]ea>[X>x?-N]NXk.
Lengthy matrix manipulations yield
k, NfNXk = (-1)'Yfx]NXk,
for / = 0, 1,... ,
= {-\)lYk^+2 for/ = 0, 1,... .
On expanding e"k[XkX* -N] as a power series in Yk2 and then grouping terms suitably (see
Subsection 4.1), one obtains
G~l = cos(akYk)XT
kNXk + s\n(akYk)Yk.
If we use (15) for e-a'[X'x?N]Xk then (33) becomes
e-"k[XkX^N]XkG-1
(xkcos(aYk)-ak[XkXT
 Published online by Cambridge University Press
Gradient algorithms for principal component analysis
Premultiplying this by Xj yields
xjNXk = cos2(akYk)XjNXj
+ cos(akYk) sin(akYk)Yk,
sin2(a* Yk)XJ
cos(akYk)sin(akYk)Yk.
This shows that (33) implies (34).
If ak solves (34) then defining G~l =
X]eatlXtX* -N]NXk ensures (33) also holds which completes the proof.
Write Yk = £]f=1 fryiyj, where {ji,... , yp} is a set of orthonormal eigenvectors
for Yk, whose eigenvalues are denoted # > 0 for / = 1,... , p. Then (34) becomes
Fixing / and premultiplying by yj while also postmultiplying by yt gives the following
p equations for ak:
sin(akf}i) = 0 or
cot(a,t/$,) =
—yJxjNXkyi,
for / = 1,... , p. It follows that either from the first relation akfii = mn for some
integer m or from the second relation, that
cot (at) =
for each / = 1, ... , p. One can easily confirm from this that the p equations will
fail to have a consistent solution for arbitrary choices of Xk and N. Thus, generically,
the Rayleigh gradient algorithm (11) does not correspond to the generalised power
method (32) for any choice of rescaling operation or time-step selection.
Acknowledgement
The authors wish to acknowledge the funding of the activities of the Cooperative
Research Centre for Robust and Adaptive Systems by the Australian Commonwealth
Government under the Cooperative Research Centres Program, and separate support
by Boeing Commercial Aircraft Cooperation Inc.
 Published online by Cambridge University Press
R. E. Mahony, U. Helmke and J. B. Moore