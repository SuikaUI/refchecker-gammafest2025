HAL Id: inria-00583818
 
Submitted on 7 Apr 2011
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Action Recognition by Dense Trajectories
Heng Wang, Alexander Kläser, Cordelia Schmid, Liu Cheng-Lin
To cite this version:
Heng Wang, Alexander Kläser, Cordelia Schmid, Liu Cheng-Lin. Action Recognition by Dense Trajectories. CVPR 2011 - IEEE Conference on Computer Vision & Pattern Recognition, Jun 2011,
Colorado Springs, United States. pp.3169-3176, ￿10.1109/CVPR.2011.5995407￿. ￿inria-00583818￿
Action Recognition by Dense Trajectories
Heng Wang†
Alexander Kl¨aser‡
Cordelia Schmid‡
Cheng-Lin Liu†
†National Laboratory of Pattern Recognition
‡ LEAR, INRIA Grenoble, LJK
Institute of Automation, Chinese Academy of Sciences
Grenoble, France
{hwang, liucl}@nlpr.ia.ac.cn
{Alexander.Klaser, Cordelia.Schmid}@inria.fr
Feature trajectories have shown to be efﬁcient for representing videos.
Typically, they are extracted using the
KLT tracker or matching SIFT descriptors between frames.
However, the quality as well as quantity of these trajectories is often not sufﬁcient. Inspired by the recent success
of dense sampling in image classiﬁcation, we propose an
approach to describe videos by dense trajectories. We sample dense points from each frame and track them based on
displacement information from a dense optical ﬂow ﬁeld.
Given a state-of-the-art optical ﬂow algorithm, our trajectories are robust to fast irregular motions as well as shot
boundaries. Additionally, dense trajectories cover the motion information in videos well.
We, also, investigate how to design descriptors to encode
the trajectory information. We introduce a novel descriptor
based on motion boundary histograms, which is robust to
camera motion. This descriptor consistently outperforms
other state-of-the-art descriptors, in particular in uncontrolled realistic videos. We evaluate our video description
in the context of action classiﬁcation with a bag-of-features
approach. Experimental results show a signiﬁcant improvement over the state of the art on four datasets of varying
difﬁculty, i.e. KTH, YouTube, Hollywood2 and UCF sports.
1. Introduction
Local features are a popular way for representing videos.
They achieve state-of-the-art results for action classiﬁcation
when combined with a bag-of-features representation. Recently, interest point detectors and local descriptors have
been extended from images to videos. Laptev and Lindeberg introduced space-time interest points by extending the Harris detector. Other interest point detectors include detectors based on Gabor ﬁlters or on the determinant of the spatio-temporal Hessian matrix . Feature descriptors range from higher order derivatives (local
jets), gradient information, optical ﬂow, and brightness information to spatio-temporal extensions of image
Dense trajectories
Figure 1. A comparison of the KLT tracker and dense trajectories.
Red dots indicate the point positions in the current frame. Dense
trajectories are more robust to irregular abrupt motions, in particular at shot boundaries (second row), and capture more accurately
complex motion patterns.
descriptors, such as 3D-SIFT , HOG3D , extended
SURF , or Local Trinary Patterns .
However, the 2D space domain and 1D time domain in
videos have very different characteristics. It is, therefore,
intuitive to handle them in a different manner than via interest point detection in a joint 3D space. Tracking interest
points through video sequences is a straightforward choice.
Some recent methods show impressive results
for action recognition by leveraging the motion information
of trajectories. Messing et al. extracted feature trajectories by tracking Harris3D interest points with the KLT
tracker . Trajectories are represented as sequences of
log-polar quantized velocities. Matikainen et al. used
Trajectory description
Dense sampling
in each spatial scale
Tracking in each spatial scale separately
Figure 2. Illustration of our dense trajectory description. Left: Feature points are sampled densely for multiple spatial scales. Middle:
Tracking is performed in the corresponding spatial scale over L frames. Right: Trajectory descriptors are based on its shape represented by
relative point coordinates as well as appearance and motion information over a local neighborhood of N × N pixels along the trajectory.
In order to capture the structure information, the trajectory neighborhood is divided into a spatio-temporal grid of size nσ × nσ × nτ.
a standard KLT tracker. Trajectories in a video are clustered, and an afﬁne transformation matrix is computed for
each cluster center. The elements of the matrix are used to
represent the trajectories. Sun et al. extracted trajectories by matching SIFT descriptors between two consecutive
frames. They imposed a unique-match constraint among the
descriptors and discarded matches that are too far apart.
Dense sampling has shown to improve results over
sparse interest points for image classiﬁcation . The
same has been observed for action recognition in a recent
evaluation by Wang et al. , where dense sampling at regular positions in space and time outperforms state-of-the-art
space-time interest point detectors. In contrast, trajectories
are often obtained by the KLT tracker, which is designed to
track sparse interest points . Matching dense SIFT descriptors is computationally very expensive and, thus,
infeasible for large video datasets.
In this paper, we propose an efﬁcient way to extract
dense trajectories. The trajectories are obtained by tracking
densely sampled points using optical ﬂow ﬁelds. The number of tracked points can be scaled up easily, as dense ﬂow
ﬁelds are already computed. Furthermore, global smoothness constraints are imposed among the points in dense optical ﬂow ﬁelds, which results in more robust trajectories than
tracking or matching points separately, see Figure 1. Dense
trajectories have not been employed previously for action
recognition. Sundaram et al. accelerated dense trajectories computation on a GPU. Brox et al. segmented objects by clustering dense trajectories. A similar approach is
used in for video object extraction.
Motion is the most informative cue for action recognition. It can be due to the action of interest, but also be
caused by background or the camera motion. This is inevitable when dealing with realistic actions in uncontrolled
settings. How to separate action motion from irrelevant motion is still an open problem. Ikizler-Cinbis et al. applied
video stabilization via a motion compensation procedure,
where most camera motion is removed. Uemura et al. 
segmented feature tracks to separate the motion characterizing the actions from the dominant camera motion.
To overcome the problem of camera motion, we introduce a local descriptor that focuses on foreground motion.
Our descriptor extends the motion coding scheme based
on motion boundaries developed in the context of human
detection to dense trajectories. We show that motion
boundaries encoded along the trajectories signiﬁcantly outperform state-of-the-art descriptors.
This paper is organized as follows. In section 2, we introduce the approach for extracting dense trajectories. We,
then, show how to encode feature descriptors along the trajectories in section 3. Finally, we present the experimental
setup and discuss the results in sections 4 and 5 respectively.
The code to compute dense trajectories and their description
is available online1.
2. Dense trajectories
Dense trajectories are extracted for multiple spatial
scales, see Figure 2. Feature points are sampled on a grid
spaced by W pixels and tracked in each scale separately.
Experimentally, we observed that a sampling step size of
W = 5 is dense enough to give good results. We used
8 spatial scales spaced by a factor of 1/
2. Each point
Pt = (xt, yt) at frame t is tracked to the next frame t+1 by
median ﬁltering in a dense optical ﬂow ﬁeld ω = (ut, vt).
Pt+1 = (xt+1, yt+1) = (xt, yt) + (M ∗ω)|(¯xt,¯yt),
where M is the median ﬁltering kernel, and (¯xt, ¯yt) is the
rounded position of (xt, yt). This is more robust than bilinear interpolation used in , especially for points near
motion boundaries.
Once the dense optical ﬂow ﬁeld is
computed, points can be tracked very densely without additional cost. Points of subsequent frames are concatenated
to form a trajectory: (Pt, Pt+1, Pt+2, . . .). To extract dense
1 
Figure 3. Illustration of the information captured by HOG, HOF, and MBH descriptors. For each image, gradient/ﬂow orientation is
indicated by color (hue) and magnitude by saturation. Motion boundaries are computed as gradients of the x and y optical ﬂow components
separately. Compared to optical ﬂow, motion boundaries suppress most camera motion in the background and highlight the foreground
motion. Unlike gradient information, motion boundaries eliminate most texture information from the static background.
optical ﬂow, we use the algorithm by F¨arneback as implemented in the OpenCV library2. We found this algorithm
to be a good compromise between accuracy and speed.
A common problem in tracking is drifting. Trajectories
tend to drift from their initial location during tracking. To
avoid this problem, we limit the length of a trajectory to L
frames. As soon as a trajectory exceeds length L, it is removed from the tracking process, see Figure 2 (middle). To
assure a dense coverage of the video, we verify the presence
of a track on our dense grid in every frame. If no tracked
point is found in a W × W neighborhood, this feature point
is sampled and added to the tracking process. Experimentally, we chose a trajectory length of L = 15 frames.
In homogeneous image areas without any structure, it is
impossible to track points. Here, we use the same criterion
as Shi and Tomasi . When a feature point is sampled,
we check the smaller eigenvalue of its autocorrelation matrix. If it is below a threshold, this point will not be included
in the tracking process. Since for action recognition we are
mainly interested in dynamic information, static trajectories
are pruned in a pre-processing stage. Trajectories with sudden large displacements, most likely to be erroneous, are
also removed. Figure 1 compares dense and KLT trajectories. We can observe that dense trajectories are more robust
and denser than the trajectories obtained by the KLT tracker.
The shape of a trajectory encodes local motion patterns.
Given a trajectory of length L, we describe its shape by a
sequence S = (∆Pt, . . . , ∆Pt+L−1) of displacement vectors ∆Pt = (Pt+1 −Pt) = (xt+1 −xt, yt+1 −yt). The
resulting vector is normalized by the sum of the magnitudes
2 
of the displacement vectors:
S′ = (∆Pt, . . . , ∆Pt+L−1)
We refer to this vector by trajectory descriptor. We have
also evaluated representing trajectories at multiple temporal
scales, in order to recognize actions with different speeds.
However, this did not improve the results in practice. Therefore, we use trajectories with a ﬁxed length L in our experiments.
3. Trajectory-aligned descriptors
Local descriptors computed in a 3D video volume
around interest points have become a popular way for video
representation . To leverage the motion
information in our dense trajectories, we compute descriptors within a space-time volume around the trajectory, see
Figure 2 (right). The size of the volume is N ×N pixels and
L frames. To embed structure information in the representation, the volume is subdivided into a spatio-temporal grid
of size nσ ×nσ ×nτ. The default parameters for our experiments are N = 32, nσ = 2, nτ = 3 , which has shown to
be optimal based on cross validation on the training set of
the Hollywood2. We give results using different parameter
settings in section 5.3.
Among the existing descriptors for action recognition,
HOGHOF has shown to give excellent results on a variety of datasets . HOG (histograms of oriented gradients) focuses on static appearance information, whereas
HOF (histograms of optical ﬂow) captures the local motion
information. We compute HOGHOF along our dense trajectories. For both HOG and HOF, orientations are quantized
into 8 bins using full orientations, with an additional zero
Walking dog
AnswerPhone
Skateboarding
High-Bar-Swinging
Figure 4. Sample frames from video sequences of KTH (ﬁrst row), YouTube (second row), Hollywood2 (third row) and UCF sports (last
row) action datasets.
bin for HOF (i.e., in total 9 bins). Both descriptors are normalized with their L2 norm. Figure 3 shows a visualization
of HOGHOF.
Optical ﬂow computes the absolute motion, which inevitably includes camera motion . Dalal et al. proposed the MBH (motion boundary histogram) descriptor
for human detection, where derivatives are computed separately for the horizontal and vertical components of the
optical ﬂow. This descriptor encodes the relative motion
between pixels, as shown in Figure 3. Here we use MBH to
describe our dense trajectories.
The MBH descriptor separates the optical ﬂow ﬁeld
Iω = (Ix, Iy) into its x and y component. Spatial derivatives are computed for each of them and orientation information is quantized into histograms, similarly to the HOG
descriptor. We obtain an 8-bin histogram for each component, and normalize them separately with the L2 norm.
Since MBH represents the gradient of the optical ﬂow, constant motion information is suppressed and only information about changes in the ﬂow ﬁeld (i.e., motion boundaries)
is kept. Compared to video stabilization and motion
compensation , this is a simple way to eliminate noise
due to background motion. This descriptor yields excellent results when combined with our dense trajectories. For
instance, on the YouTube dataset , MBH signiﬁcantly
outperforms HOF, see section 5.
For both HOF and MBH descriptors, we reuse the dense
optical ﬂow that is already computed to extract dense trajectories. This makes our feature computation process very
4. Experimental setup
In this section, we ﬁrst describe the datasets used for
action recognition. We, then, brieﬂy present the bag-offeatures model used for evaluating our dense trajectory features as well as the KTL tracking baseline.
4.1. Datasets
Our dense trajectories are extensively evaluated on four
standard action datasets: KTH, YouTube, Hollywood2, and
UCF sports, see Figure 4.
These datasets are very diverse. The KTH dataset views actions in front of a uniform
background, whereas the Hollywood2 dataset contains real
movies with signiﬁcant background clutter. The YouTube
videos are low quality, whereas UCF sport videos are high
resolution.
The KTH dataset 3 consists of six human action
classes: walking, jogging, running, boxing, waving and
clapping. Each action is performed several times by 25 subjects. The sequences were recorded in four different scenarios: outdoors, outdoors with scale variation, outdoors with
different clothes and indoors. The background is homogeneous and static in most sequences. In total, the data consists of 2391 video samples. We follow the original experimental setup of the authors, e.g., divide the samples into test
set (9 subjects: 2, 3, 5, 6, 7, 8, 9, 10, and 22) and training set
(the remaining 16 subjects). As in the initial paper , we
train and evaluate a multi-class classiﬁer and report average
accuracy over all classes as performance measure.
3 
The YouTube dataset 4 contains 11 action categories: basketball shooting, biking/cycling, diving, golf
swinging, horse back riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and
walking with a dog. This dataset is challenging due to large
variations in camera motion, object appearance and pose,
object scale, viewpoint, cluttered background and illumination conditions. The dataset contains a total of 1168 sequences. We follow the original setup using leave one
out cross validation for a pre-deﬁned set of 25 folds. Average accuracy over all classes is reported as performance
The Hollywood2 dataset 5 has been collected from
69 different Hollywood movies. There are 12 action classes:
answering the phone, driving car, eating, ﬁghting, getting
out of car, hand shaking, hugging, kissing, running, sitting
down, sitting up, and standing up. In our experiments, we
used the clean training dataset. In total, there are 1707 action samples divided into a training set (823 sequences) and
a test set (884 sequences). Train and test sequences come
from different movies. The performance is evaluated by
computing the average precision (AP) for each of the action
classes and reporting the mean AP over all classes (mAP)
as in .
The UCF sport dataset 6 contains ten human actions: swinging (on the pommel horse and on the ﬂoor),
diving, kicking (a ball), weight-lifting, horse-riding, running, skateboarding, swinging (at the high bar), golf swinging and walking. The dataset consists of 150 video samples
which show a large intra-class variability. To increase the
amount of data samples, we extend the dataset by adding a
horizontally ﬂipped version of each sequence to the dataset.
Similar to the KTH actions dataset, we train a multi-class
classiﬁer and report the average accuracy over all classes.
We use a leave-one-out setup and test on each original sequence while training on all other sequences together with
their ﬂipped versions (i.e., the ﬂipped version of the tested
sequence is removed from the training set).
4.2. Bag of features
To evaluate the performance of our dense trajectories, we
use a standard bag-of-features approach. We ﬁrst construct
a codebook for each descriptor (trajectory, HOG, HOF,
MBH) separately. We ﬁx the number of visual words per descriptor to 4000 which has shown to empirically give good
results for a wide range of datasets. To limit the complexity,
we cluster a subset of 100,000 randomly selected training
features using k-means. To increase precision, we initialize
k-means 8 times and keep the result with the lowest error.
4 
_dataset.html
5 
6 
Descriptors are assigned to their closest vocabulary word
using Euclidean distance. The resulting histograms of visual word occurrences are used as video descriptors.
For classiﬁcation we use a non-linear SVM with a χ2kernel . Different descriptors are combined in a multichannel approach as in :
K(xi, xj) = exp(−
where D(xc
j) is the χ2 distance between video xi and
xj with respect to the c-th channel. Ac is the mean value
of χ2 distances between the training samples for the c-th
channel . In the case of multi-class classiﬁcation, we
use a one-against-rest approach and select the class with
the highest score.
4.3. Baseline KLT trajectories
To compare our dense trajectories with the standard KLT
tracker , we use the implementation of the KLT tracker
from OpenCV. In each frame 100 interest points are detected, and added to the tracker, which is somewhat denser
than space-time interest points .
Interest points are
tracked through the video for L frames. This is identical to
the procedure used for our dense trajectories. We also use
the same descriptors for the KLT trajectories, e.g. the trajectory shape is represented by normalized relative point coordinates, and HOG, HOF, MBH descriptors are extracted
around the trajectories.
5. Experimental results
In this section, we evaluate the performance of our description and compare to state-of-the-art methods. We also
determine the inﬂuence of different parameter settings.
5.1. Evaluation of our dense trajectory descriptors
In this section we compare dense and KLT trajectories
as well as the different descriptors. We use our default parameters for this comparison. To compute the descriptors,
we set N = 32, nσ = 2, nτ = 3 for both baseline KLT and
dense trajectories. We ﬁx the trajectory length to L = 15,
and the dense sampling step size to W = 5.
Results for the four datasets are presented in Table 1.
Overall, our dense trajectories outperform the KLT trajectories by 2% to 6%. Since the descriptors are identical, this
demonstrates that our dense trajectories describe the video
structures more accurately.
Trajectory descriptors, which only describe the motion
of the trajectories, give surprisingly good results by themselves, e.g. 90.2% on KTH and 47.7% on Hollywood2 for
dense trajectories.
This conﬁrms the importance of motion information contained in the local trajectory patterns.
We report only 67.2% on YouTube because the trajectory
Hollywood2
UCF sports
Dense trajectories
Dense trajectories
Dense trajectories
Dense trajectories
Trajectory
Table 1. Comparison of KLT and dense trajectories as well as different descriptors on KTH, YouTube, Hollywood2 and UCF sports. We
report average accuracy over all classes for KTH, YouTube and UCF sports and mean AP over all classes for Hollywood2.
Hollywood2
UCF sports
Laptev et al. 
Liu et al. 
Wang et al. 
Wang et al. 
Yuan et al. 
Ikizler-Cinbis et al. 
Gilbert et al. 
Kovashka et al. 
Gilbert et al. 
Ullah et al. 
Kl¨aser et al. 
Kovashka et al. 
Taylor et al. 
Our method
Our method
Our method
Our method
Table 2. Comparison of our dense trajectories characterized by our combined descriptor (Trajectory+HOG+HOF+MBH) with state-of-theart methods in the literature.
descriptors capture lots of motions from camera. Generally, HOF outperforms HOG as motion is more discriminative than static appearance for action recognition. However, HOG gets better results both on YouTube and UCF
sports. The HOF descriptors computed on YouTube videos
are heavily polluted by camera motions, since many videos
are collected by hand-held cameras. Static scene context is
very important for UCF sports actions which often involve
speciﬁc equipment and scene types. MBH consistently outperforms the other descriptors on all four datasets. The improvement is most signiﬁcant on the uncontrolled realistic
datasets YouTube and Hollywood2. For instance, MBH is
11.1% better than HOF on YouTube. This conﬁrms the advantage of suppressing background motion when dealing
with optical ﬂow.
5.2. Comparison to the state of the art
Table 2 compares our results to state of the art. On KTH,
we obtain 94.2% which is comparable to the state of the
art, i.e., 94.53% . Note that on this dataset several authors use a leave-one-out cross-validation setting. Here, we
only compare to those using the standard setting . Interestingly, MBH alone obtains a slightly better performance
on KTH, i.e., 95.0%, than combining all the descriptors
together. Ullah et al. also found that a combination
of descriptors performed worse than a subset of them. On
YouTube, we signiﬁcantly outperform the current state-ofthe-art method by 9%, where video stabilization is used
to remove camera motion. We report 58.3% on Hollywood2
which is an improvement of 5% over . Note that Ullah
et al. achieved better results by using additional images
collected from Internet. The difference between all methods
is rather small on UCF sports, which is largely due to the
leave-one-out setting, e.g. 149 videos are used for training
and only one for testing. Nevertheless, we outperform the
state of the art by 1%.
We also compare the results per action class for YouTube
and Hollywood2. On YouTube, our dense trajectories give
best results for 8 out of 11 action classes when compare
with the KLT baseline and the approach of , see Table 3.
On Hollywood2, we compare the AP of each action class
with the KLT baseline and the approach of , i.e., a combination of 24 spatio-temporal grids, see Table 4. Our dense
trajectories yield best results for 8 out of 12 action classes.
Dense trajectories
Ikizler-Cinbis 
Table 3. Accuracy per action class for the YouTube dataset. We
compare with the results reported in .
Dense trajectories
Ullah 
AnswerPhone
FightPerson
Table 4. Average precision per action class for the Hollywood2
dataset. We compare with the results reported in .
Mean Average Precision −− Hollywood2
Trajectory Length L (frames)
Average Accuracy −− YouTube
Hollywood2
Mean Average Precision −− Hollywood2
Sampling Stride W (pixels)
Average Accuracy −− YouTube
Hollywood2
Mean Average Precision −− Hollywood2
Neighborhood Size N (pixels)
Average Accuracy −− YouTube
Hollywood2
1x1x1 1x1x2 1x1x3 2x2x1 2x2x2 2x2x3 3x3x1 3x3x2 3x3x3
Mean Average Precision −− Hollywood2
Cell Grid Structure nσ x nσ x nτ
1x1x1 1x1x2 1x1x3 2x2x1 2x2x2 2x2x3 3x3x1 3x3x2 3x3x3
Average Accuracy −− YouTube
Hollywood2
Figure 5. Results for different parameter settings on the Hollywood2 and YouTube datasets.
5.3. Evaluation of trajectory parameters
To evaluate the different parameter settings for dense trajectories, we report results on YouTube and Hollywood2, as
they are larger and more challenging than the other two. We
study the impact of the trajectory length, sampling step size,
neighborhood size and cell grid structure. We evaluate the
performance for a parameter at the time. The other parameters are ﬁxed to the default values, i.e., trajectory length
L = 15, sampling step size W = 5, neighborhood size
N = 32 and cell grid structure nσ = 2, nτ = 3.
Figure 5 (top, left) evaluates the impact of the trajectory
length L. For both datasets an increase of length L improves
performance up to a certain point (L=15 or 20), and then
decreases slightly, since longer trajectories have a higher
chance to drift from the initial position. We achieve the best
results with a trajectory length of 15 or 20 frames.
With respect to the sampling step size W, Figure 5 (top,
right) shows that dense sampling improves the results as the
step size decreases. This is consistent with dense sampling
at regular positions , where more features in general improve the results. We report 58.9% (58.3%) on Hollywood2
and 84.4% (84.2%) on YouTube for a step size of 2 (5) pixels. A sampling step of 2 pixels is extremely dense, i.e.,
every other pixel is sampled, and does not justify the minor
gain obtained.
The results are relatively stable with regard to the neighborhood size N, see Figure 5 (bottom left).
On Hollywood2, results are almost the same when N changes from
24 pixels to 48 pixels. The best result on YouTube is 84.7%
with a neighborhood size of 40 pixels. Dividing the video
volume into cells improves the results on both Hollywood2
and YouTube. In particular, the performance increases signiﬁcantly when the spatial cell grid nσ is increased from 1
to 2, see Figure 5 (bottom right). However, further increasing the number of cells, i.e., beyond nσ = 2, nτ = 3, does
not improve the results.
6. Conclusions
This paper has introduced an approach to model videos
by combining dense sampling with feature tracking. Our
dense trajectories are more robust than previous video descriptions.
They capture the motion information in the
videos efﬁciently and show improved performance over
state-of-the-art approaches for action classiﬁcation.
have also introduced an efﬁcient solution to remove camera
motion by computing motion boundaries descriptors along
the dense trajectories. This successfully segments the relevant motion from background motion, and outperforms previous video stabilization methods. Our descriptors combine
trajectory shape, appearance, and motion information. Such
a representation has shown to be efﬁcient for action classiﬁcation, but could also be used in other areas, such as action
localization and video retrieval.
Acknowledgments. This work was partly supported by National Natural Science Foundation of China (NSFC) under
grant no.60825301 as well as the joint Microsoft/INRIA
project and the European integrated project AXES.