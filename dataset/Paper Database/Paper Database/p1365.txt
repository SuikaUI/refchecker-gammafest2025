User Model User-Adap Inter manuscript No.
(will be inserted by the editor)
Evaluating the eﬀectiveness of explanations for
recommender systems
Methodological issues and empirical studies on the impact of
personalization
Nava Tintarev · Judith Masthoﬀ
Received: date / Accepted: date
Abstract When recommender systems present items, these can be accompanied by explanatory information. Such explanations can serve seven aims:
eﬀectiveness, satisfaction, transparency, scrutability, trust, persuasiveness, and
eﬃciency. These aims can be incompatible, so any evaluation needs to state
which aim is being investigated and use appropriate metrics. This paper focuses particularly on eﬀectiveness (helping users to make good decisions) and
its trade-oﬀwith satisfaction. It provides an overview of existing work on
evaluating eﬀectiveness and the metrics used. It also highlights the limitations
of the existing eﬀectiveness metrics, in particular the eﬀects of under- and
overestimation and recommendation domain. In addition to this methodological contribution, the paper presents four empirical studies in two domains:
movies and cameras. These studies investigate the impact of personalizing
simple feature-based explanations on eﬀectiveness and satisfaction. Both approximated and real eﬀectiveness is investigated. Contrary to expectation,
personalization was detrimental to eﬀectiveness, though it may improve user
satisfaction. The studies also highlighted the importance of considering opt-out
rates and the underlying rating distribution when evaluating eﬀectiveness.
Keywords Recommender systems · Metrics · Item descriptions · Explanations · Empirical studies
N. Tintarev
University of Aberdeen
Tel.: +44-1224-274620
Fax: +44-1224-273422
E-mail: 
J. Masthoﬀ
University of Aberdeen
Tel.: +44-1224-272299
E-mail: 
Nava Tintarev, Judith Masthoﬀ
1 Introduction
While recommender systems have traditionally been evaluated in terms of recommendation accuracy, in recent years, interest has increased in more usercentered evaluation metrics such as user satisfaction . These metrics can be
inﬂuenced by more than just the recommendations, such as by explanations,
the way recommendations are presented, and the method of interacting with
recommendations (Tintarev and Masthoﬀ2010). This paper investigates the
role of explanations. It is sometimes erroneously assumed that explanations
should always justify why items have been recommended. A popular deﬁnition
of explain is “to justify”. However, to explain also means “to make clear by
giving a detailed description” [Oxford concise dictionary]. So, an explanation
can be an item description that helps the user to understand the qualities of
the item well enough to decide whether it is relevant to them or not.
In their work on explanations, Herlocker et al noted that many
recommender systems provided no transparency into the working of the recommendation process, nor oﬀered any additional information to accompany
the recommendations. Since then, the body of research on explanations in
recommender systems has continued to grow .1
As indicated above, explanations can serve multiple aims. For example, explanations can provide transparency, exposing the reasoning and data behind
a recommendation. This is the case with some of the explanations hosted on
Amazon.com, such as: “Customers Who Bought This Item Also Bought . . . ”.
Alternatively, explanations can be more focused on helping users make decisions (about the items) that they are happy with: eﬀectiveness. An eﬀective
explanation may be formulated along the lines of “You might (not) like this
item because...”. In contrast to the Amazon example above, this explanation
does not necessarily describe how the recommendation was selected - in which
case it is not transparent: “It is a funny comedy” could be an eﬀective explanation even when the recommendation was based on collaborative ﬁltering.
Table 1 shows seven possible aims for explanations. These aims can be
complementary (e.g. eﬀectiveness may increase trust) or contradictory (e.g.
persuasiveness may decrease eﬀectiveness). Relations between aims are not always clear-cut. For example, transparency may lead to an increase or decrease
in trust, depending how much conﬁdence users have in the internal working
of the system shown to them. The main explanatory aims need to be decided
prior to the design and evaluation of (optimal) explanations. As optimization
in one criterion may damage another, it is important to consider how the
criteria relate to one other (Tintarev and Masthoﬀ2009).
1 The current interest in explanation-aware computing extends beyond recommender systems .
Eﬀectiveness of personalized explanations
Table 1 Explanatory aims
Transparency
Explain how the system works
Scrutability
Allow users to tell the system it is wrong
Increase users’ conﬁdence in the system
Eﬀectiveness
Help users make good decisions
Persuasiveness
Convince users to try or buy
Help users make decisions faster
Satisfaction
Increase the ease of use or enjoyment
Table 2 provides an overview of recommender systems with explanations,
focusing on papers that evaluated the generated explanations. See Tintarev
and Masthoﬀ for more detailed deﬁnitions, examples, and a discussion
on how factors such as the degree of interaction with the recommender system
and the way in which recommendations are presented may impact evaluations
on these aims.
This paper investigates the eﬀectiveness of explanations: what makes an
explanation eﬀective (in the sense of helping users to make good decisions),
and how eﬀectiveness can best be evaluated. However, optimizing eﬀectiveness
may negatively impact other criteria. Therefore, two criteria (eﬀectiveness and
satisfaction) will be considered in our studies below, to investigate potential
trade-oﬀs.
We consider explanations for all items, not just for items predicted to be
liked by the user. As discussed in Tintarev and Masthoﬀ , there are
alternative ways recommender systems can present their results instead of, or
in addition to, a top-N list (where only the highest recommended items are
presented). For example, systems such as MovieLens2 show all items with a
number of stars signifying how much the user is predicted to like each item.
Showing all items may improve users’ sense of control, transparency and trust.
Similarly, recommender systems that use critiquing may show items with positive and less positive feature
values, such as cameras that are reasonably priced but have low resolution.
Group recommender systems may
present items that are disliked by a user, even when presented as a top-N list,
because they are liked by other group members. In all of these cases, an eﬀective recommender system may need to highlight relevant positive and negative
information about items.
We consider a particular type of explanations, namely feature-based explanations. Feature-based explanations will not be able to mention all item
features. Firstly, there are likely space restrictions, for example when a user
is surveying a list of recommendations or using a portable device such as a
mobile phone. Secondly, mentioning many item features would be detrimental
to the speed with which users can make decisions (i.e. eﬃciency). To provide
good decision support, the selection of features may need to be personalized,
2 retrieved May 2011
Nava Tintarev, Judith Masthoﬀ
as users may diﬀer in terms of which features they ﬁnd important, and have
individual tastes with regard to these features. For example, people may diﬀer
in the degree to which the author is a decisive factor for reading or enjoying a
book. A study by Herlocker et al on explanations found a strong persuasive eﬀect for an explanation referring to a particular movie feature, namely
“favorite actor or actress”. This feature may have been more important to
some users than others, since a high variance in acceptance for this type of
explanation was found. In the real-estate domain, Carenini and Moore 
found that user-tailored evaluative arguments (such as including “the house
has a good location” for a user who cares a lot about location) increased users’
likelihood to adopt a particular house compared to non-tailored arguments.
There has already been substantial research on generating personalized
feature-based item descriptions, for example for the ILEX system in the museum domain . Integrating recommendations with personalized item descriptions has also been proposed, for
example for the INTRIGUE system in the tourism domain . Billsus and Pazzani also looked at tailored explanations (including some feature-based ones), for a news recommender. The novelty of the
work presented in this paper is in the analysis of the impact of these kinds of
explanations on the user, in particular on eﬀectiveness.
While similar, our work also diﬀers from previous studies on the impact
of explanations which primarily considered the persuasive power of arguments
and explanations, but did not study eﬀectiveness . Arguably, Carenini and Moore varied the polarity
(i.e. good vs. bad) of the evaluative arguments, but given the domain (realestate) it was diﬃcult for them to consider the ﬁnal valuation of the item, i.e.
whether the user would really like the house once they bought it. Others evaluated eﬀectiveness but did not consider the role
of personalization. We will investigate how personalization of item features
can aﬀect explanation eﬀectiveness and user satisfaction.
Using a user-centered approach, we have conducted user studies to elicit
which features users use to make decisions about whether or not to watch
movies, or buy digital cameras. We then used the elicited item features in a
prototype natural language generation system, using commercial meta-data,
to dynamically generate explanations. We inquired whether personalization
helps increase eﬀectiveness. Sections 4 and 5 describe two experiments, in the
movie and camera domains, using approximated eﬀectiveness (based on reading online reviews). To check the validity of these results, Section 6 presents a
study in the movie domain in which participants saw the movies in question.
Section 7 concludes with comments on the eﬀects of personalization in both
domains and whether it makes sense to explain at all. It also reﬂects on lessons
learned about measuring eﬀectiveness, and presents future work inspired by
the limitations of the work presented.
Eﬀectiveness of personalized explanations
Table 2 The goals for which explanations in recommender systems have been evaluated. System names are mentioned if available. Works that have
no clear explanation goal stated, or have not evaluated the system on the stated goal, are omitted. Note that while a system may have been evaluated
for several goals, it may not have achieved all of them. Also, for completeness a distinction is made between multiple studies using the same system.
System: type of items
Eﬀectiveness
Persuasiveness
Internet providers 
 
 
Music 
Movies (Tintarev and Masthoﬀ2008b)
restaurants
 
ACORN: movies 
CHIP: artworks 
CHIP: artworks 
Fringe: people 
iSuggest-Usability: music 
LIBRA: books 
MovieLens: movies 
Moviexplain: movies 
myCameraAdvisor:
 
Qwikshop: cameras 
SASY: holidays 
Tagsplanations: movies 
Social software items 
Nava Tintarev, Judith Masthoﬀ
2 Eﬀectiveness
Getting the metric right is a key part of any evaluation . Sometimes when adaptive systems are evaluated, there is a shortage of
information about what exactly they are being evaluated on . This section discusses related work in evaluating the eﬀectiveness of
explanations, the metric that will be used in the studies in this paper, and the
potential impact of recommendation domain on eﬀectiveness.
2.1 Related work on eﬀectiveness and metrics
As mentioned above, the focus of this paper is on explanations which aim to
help users make qualiﬁed decisions, i.e. eﬀective explanations. Eﬀectiveness is
by deﬁnition dependent on the accuracy of the recommendation algorithm,
i.e. it is hard for users to make correct decisions if the recommendations are
poor. However, an eﬀective explanation may help the user evaluate the quality
of suggested items according to their own preferences. This increases the likelihood that users discard irrelevant options while helping them to recognize
good ones. For example, a book recommender system with eﬀective explanations helps users to buy books they actually enjoy reading. Previous work
emphasizes the importance of measuring the ability of a system to assist the
user in making accurate decisions about recommendations, and compared different explanation types for eﬀectiveness . Eﬀective
explanations could also serve the purpose of introducing a new domain, or the
range of products, to a novice user, thereby helping them to understand the
full range of options .
Several studies that seem to investigate eﬀectiveness have in fact investigated something subtly diﬀerent:
– For their conversational recommender system, Thompson et al calculated the percentage of conversations in which the ﬁrst item presented
was acceptable to the user (hit-rate). They also considered the proportion of features about which the system asked but the user did not care
(rejection-rate). This muddles the distinction between eﬀectiveness on the
one hand and eﬃciency and satisfaction on the other. Thompson et al’s
metrics do not really measure how good a decision the system helps the
user to make, but rather how fast the user can make a decision, and what
they may (dis)like about the interaction.
– Cramer et al tested the same system with and without an explanation facility, and compared the system’s perceived and actual competence.
Actual competence was established by comparing the item features used
by the recommender system, with the item features mentioned by users
when asked why they had chosen their top items. So, actual competence
Eﬀectiveness of personalized explanations
focused on the quality of the user modeling rather than on whether the
explanations helped users to make good decisions. Similarly, perceived effectiveness focused on whether the recommendations were good rather than
on whether the explanations helped users to make good decisions.
– Symeonidis et al compared three justiﬁcation styles. Users rated
which justiﬁcation style they preferred, which seems to measure satisfaction. Additionally, they used a metric called ‘coverage ratio’, to measure
the quality of the justiﬁcations objectively. For this, they measured how
well the item features revealed in the justiﬁcations covered the relevant
item features in the user’s proﬁle. So, similarly to the work by Cramer
et al , they measure how good the user modeling underlying the
justiﬁcations is rather than whether the explanations helped users to make
good decisions.
Several metrics for measuring eﬀectiveness have been proposed:
– Perceived eﬀectiveness before consumption. Vig et al let participants
judge the eﬀectiveness of explanations by rating how much they agreed
with statements such as “This explanation helps me determine how well I
will like this movie.”. Using a similar metric, Hingston studied the
perceived eﬀectiveness of explanations for a number of interfaces and recommendation algorithms. Participants were asked how useful (and understandable) they perceived the explanations to be, and to rank explanations
in order of usefulness.
– Perceived eﬀectiveness after consumption. A variation of the metric used
by Vig et al would be for participants to rate how much they agree
with statements such as “This explanation helped me determine how well
I would like this item.”.
– Success rate in ﬁnding the best item. Chen and Pu used a decision
quality metric from marketing which measures the
fraction of participants that switched their choice to another option after
viewing all of the items. This metric considers how eﬀectively the system
supports users to ﬁnd the single best possible item (rather than “good
enough items” as above). Participants interacted with the system until
they found the item they would buy. Next, they viewed all items and were
able to change their choice. Eﬀectiveness was measured by the fraction of
participants who changed their choice (so, a lower fraction meant better
eﬀectiveness).
– Acceptance of items known to the user. Guy et al counted the number of accepted recommendations. In their domain, this meant the number
of recommended people that were added to a social network. As a general metric, this seems to measure persuasiveness rather than eﬀectiveness.
However, if the recommended items (people in their domain) are already
known to the user – as was the case in their study – then the fraction of
accepted recommendations may indeed measure eﬀectiveness.
– Use of the explanations. McCarthy et al investigated compound
critique-based explanations that make users more aware of the items avail-
Nava Tintarev, Judith Masthoﬀ
able beyond the currently suggested item (e.g. other cameras with more
memory but heavier). They measured among other things how often users
selected compound critiques, regarding an explanation as more eﬀective
if the critiques were used more often. This measured whether the explanations helped users in making a choice, but not whether this actually
resulted in users being happy with the item chosen after trying it. They
also measured how satisﬁed users were with the camera they decided to
purchase. However, this was done without the user actually trying the camera, and was merely done to ensure that the users who made little use of
the compound critiques were not simply more diﬃcult to satisfy.
– Similarity between liking items before and after consumption. Bilgic and
Mooney used as metric the absence of a diﬀerence between the
liking of the recommended item prior to, and after, consumption. The
metric compares two item ratings: one after receiving an explanation, and
a second after experiencing the item. If the opinion on the item did not
change much, the explanation was considered eﬀective.
These metrics diﬀer on three dimensions:
– Timing: Measuring eﬀectiveness before or after consumption. Afterconsumption eﬀectiveness takes into account the user’s opinion after experiencing the item. The metrics used by Vig et al , Hingston ,
and McCarthy et al measure before-consumption eﬀectiveness,
while Bilgic and Mooney’s metric approximates after-consumption eﬀectiveness (they used an approximation of really experiencing the item). Chen
and Pu’s metric is somewhere in between, as participants could explore the
whole catalog of items, presumably getting more information about the
items, but not really fully experiencing them.
– Items considered: Measuring eﬀectiveness using only the top recommended
item or all items. Bilgic and Mooney’s metric considers all items across
the rating spectrum (or a representative set of such items). Cramer et al’s
metric considers a subset of items, namely the six items most preferred by
users. Chen and Pu’s metric considers the item users deemed best before
having tried it.
– Type of measurement: Measuring objective or perceived eﬀectiveness. Perceived eﬀectiveness measures the user’s perception of the system’s eﬀectiveness (e.g. through self-reporting), while objective eﬀectiveness measures effectiveness directly (e.g. through comparing the diﬀerence between before
and after ratings for items)3. Vig et al’s and Hingston’s metrics measure
perceived eﬀectiveness, while the metrics used by Bilgic and Mooney, Chen
and Pu, and McCarthy et al measure objective eﬀectiveness. The dimensions are not independent: objective eﬀectiveness tends to be measured
after consumption, though McCarthy et al show that it is possible
to measure it before consumption. Perceived eﬀectiveness can be measured
both before and after consumption.
3 Parallels can be drawn with the distinction made between perceived and actual accuracy
of recommendations as discussed in the usability framework proposed by Pu et al .
Eﬀectiveness of personalized explanations
2.2 Metric used in this paper
We will use Bilgic and Mooney’s metric, as it measures objective
eﬀectiveness after consumption, and considers all items. We measure afterconsumption eﬀectiveness, as before-consumption eﬀectiveness may overlap
with persuasiveness. We measure objective eﬀectiveness, as perceived eﬀectiveness may overlap with satisfaction. We consider all items (across the rating
spectrum, so including items the user may not like), as this provides a more
comprehensive view of eﬀectiveness. A metric considering a subset of items
lacks granularity. For example, suppose that in a particular recommender system users tend to select item A when no explanations are present, and item B
when explanations are present. Suppose that the item most suited was in fact
item C. According to the metric used by Chen and Pu , these systems
have equal eﬀectiveness, independent of how suitable items A and B were. So,
even if item B were a lot more suitable – suggesting that explanations led to a
decision a user was happier with – the eﬀectiveness metric would not show this.
It could be argued that this could be solved by creating a new metric which
takes the diﬀerence between ratings for the best item before consumption and
the best item after consumption (so, using an adapted version of Bilgic and
Mooney’s metric). However, this would still assume that only the best item(s)
matter. As discussed in the introduction, in some circumstances, it is important that users get an accurate impression even of items less suitable to their
tastes. This is for example the case in a group recommender system, where
users may have to compromise given varying tastes in the group :
1. (Rating1) The user rates the item on the basis of the explanation
2. The user tries the item
3. (Rating2) The user re-rates the item
Eﬀectiveness can then be measured by the discrepancy between steps 1 and
3 (Rating1 −Rating2). According to this metric, an eﬀective explanation is
one which minimizes the gap between these two ratings. If an explanation helps
users make good decisions, getting more (accurate and balanced) information
or trying the product should not change their valuation of the product greatly.
Bilgic and Mooney approximated step 2, by letting the users view reviews of
the items (books) online. For other domains, other types of approximation
may be possible, such as trailers for movies.
When the user rates several items, one possibility is to study the mean
of the diﬀerence of these two ratings . In this case,
0 is the best possible mean. In a normal distribution, with as much over- as
underestimation, this eﬀectiveness metric will be close to 0, but this does not
mean the explanations are eﬀective. Bilgic and Mooney remedy this by also
looking at the correlation between the ﬁrst and second rating, with a high and
signiﬁcant correlation reﬂecting strong eﬀectiveness.
Nava Tintarev, Judith Masthoﬀ
Alternatively, one can take the mean of the unsigned diﬀerence (as well
as studying the correlation) between the two ratings. We have included this
additional analysis to our experiments, and survey the signed mean to see if
there is a greater degree of over or underestimation.
2.3 Over- and underestimation and the role of domain
The metric introduced above does not distinguish between under- and overestimation. For example, a diﬀerence of 2 between an item’s rating before
consumption and after (overestimation) will have the same magnitude of contribution to the eﬀectiveness score as a diﬀerence of -2 (underestimation). The
question arises whether an explanation leading to an overestimation is indeed
equally bad as one leading to a similarly big underestimation. Likewise, one
wonders if the location on the scale matters. For example, should the impact
on eﬀectiveness of a pre-rating of 3 and a post-rating of 5 (on a scale from 1
to 5) really be equal to the impact of a pre-rating of 1 and a post-rating of
3? There may also be an impact of the recommender domain: the expected
duration of experiencing the items and the expected impact of experiencing
a disliked item is likely to diﬀer between domains, and may well aﬀect how
eﬀectiveness is perceived.
In economics, there has been a great deal of debate about classiﬁcation
of products into diﬀerent categories. There is a distinction between experience goods, or goods that consumers learn about through experience, and
“search goods” which they do not need to learn about through direct experience . Similarly, there has been a distinction between sensory
products and non-sensory products . In Tintarev and Masthoﬀ
 , we proposed an interpretation of these categories which distinguishes
between products which are easy to evaluate objectively (e.g. light bulbs and
cameras) and those which commonly require an experiential and subjective
judgment (e.g. holidays and movies).
Another common categorization in economics involves investment or cost.
Often this is a complex construct. For example, Murphy and Enis discuss perceived price in terms of the dimensions of risk and eﬀort. This construct
of risk includes ﬁnancial risk but also psychological, physical, functional and
social risk. The construct of eﬀort considers purchase price, but also time that
the purchase takes. Perceived price has also been deﬁned in terms of nonmonetary eﬀort and degree of involvement . Others narrow
down the deﬁnition of cost to the objective measure of the purchase price of an
item . For simplicity, we will also use a deﬁnition of investment
which only considers purchase price (for example, light bulbs and movies are
low-investment, while holidays and cameras are high-investment).
In our previous work, we found that users considered overestimation to
be less helpful than underestimation (Tintarev and Masthoﬀ2008a). We also
found that the negative impact of a discrepancy between pre- and post-ratings
on perceived eﬀectiveness was signiﬁcantly higher for high investment domains
Eﬀectiveness of personalized explanations
than for low investment ones. In particular overestimation had a signiﬁcantly
higher negative impact on perceived eﬀectiveness in high investment domains.
There was also a trend towards over- and underestimation having a higher
negative impact on perceived eﬀectiveness in objective compared to subjective
domains. This trend was conﬁrmed by participant comments (e.g. a wrong
suggestion about subjective evaluations of products, such as for movies or
holidays, should not determine a severe bad judgment of the website.). We
also found that the eﬀect of prediction errors on perceived eﬀectiveness varied
depending on where on the scale the prediction error occurred. Gaps on the
negative end of the scale (e.g. a pre-rating of 1 and a post-rating of 3 and vice
versa) had a higher negative impact on perceived eﬀectiveness than gaps on
the positive end (e.g. 3 ↔5), and gaps which cross over between the positive
and negative ends of the scale (e.g., 2 ↔4) for both over- and underestimation. Gaps which cross over in turn were perceived less eﬀective than positive
In this paper, we will keep the metric as deﬁned in Section 2.2. Because
of the ﬁndings in this subsection, we will consider over- and underestimations
in the experiments, and we will present studies in two diﬀerent domains: a
low investment and subjective domain (movies) and a high investment and
objective domain (cameras).
3 Experimental setup
The four experiments outlined in the next sections share a common experimental setup, following that of our study in Tintarev and Masthoﬀ .
This section discusses this setup. Small divergences will be highlighted in the
experiments when they occur.
3.1 Methodology
3.1.1 Experimental design
Participants were randomly allocated to one of three conditions in a betweensubjects design. The conditions diﬀered in the type of explanations provided,
in particular on whether the explanations were personalized and feature-based.
We used the layered evaluation framework, which advocates that adaptation needs to be decomposed and assessed in layers in order to be evaluated
eﬀectively . Instead of evaluating a full recommender
system, we focused the evaluation on the combination of the Decide Upon
Adaptation (DA), and Apply Adaptation (AA) layers. So, we wanted to know
whether the decision to personalize the explanations (DA) and the way this was
done (AA) would indeed lead to increased eﬀectiveness and satisfaction. Following the advice in Paramythis et al , we needed to guarantee accurate
input to the layers under evaluation, so an accurate user model. Participants
Nava Tintarev, Judith Masthoﬀ
therefore provided the user model directly, using the features that were available for a commercial recommender.
We also did not use a real recommender algorithm to provide recommendations. There are two reasons for this. Firstly, we wanted to evaluate the
eﬀectiveness of the explanations, rather than the recommendation algorithm.
Secondly, as discussed above, we were interested in the eﬀectiveness of explanations across the spectrum: not just how well explanations would support
decisions related to good items, but also for weaker items. So, instead of using
a recommender system to select the items, we used a random selection from
a ﬁxed set of items. Isolating explanations is in accordance with the ’dicing’
approach, as advocated in Masthoﬀ . While layered evaluation ’slices’
the adaptation process into its components (e.g. Decide Upon Adaptation and
Apply Adaptation), dicing isolates the various functionalities that are being
adapted (e.g. recommendation algorithm and explanations).
3.1.2 Independent variables
Type of explanation. Three types of explanations were used:
1. Baseline. The explanation was neither personalized, nor described item
features. E.g. “This movie is one of the top 100 movies in the Internet
Movie Database.” In all studies, baseline explanations were chosen so that
they could be automatically retrieved and were comparable to common
practice on commercial sites.
2. Non-personalized, feature-based. The explanation described item features, but the features were not tailored to the user. E.g. “This movie
belongs to the genre(s): Drama. Kasi Lemmons directed this movie.”
3. Personalized, feature-based. The explanation described item features,
and tailored them to the user’s interests. E.g. “Although this movie does
not belong to any of your preferred genres(s), it belongs to the genre(s):
Documentary. This movie stars Ben Kingsley, Ralph Fiennes and Liam
Neeson your favorite actor(s).” For this user, the most important feature
is leading actors and these actors were mentioned as favorites.
Details of the explanations used will be given separately for each study.
3.1.3 Dependent variables
Eﬀectiveness. Eﬀectiveness was measured using the metric described in Section 2.2. Participants rated the items twice (e.g., “How much do you think you
would like this camera?”), using a 7-point Likert scale (from “not at all” to
“a lot”), both before and after experiencing them. The metric considers how
the user’s valuation of the items changes.
Satisfaction. Satisfaction was measured through rating the explanations
(“How good do you think this explanation is?”) on a 7-point Likert scale (from
really bad to really good).
Eﬀectiveness of personalized explanations
3.1.4 Procedure
The procedure consisted of the following steps:
1. Participants provided background information about themselves, such as
demographic information.
2. Participants rated the importance of diﬀerent product features and entered
their preferences for each feature, resulting in a simple user model. For
example, for experiments 1, 2 and 4, participants rated the importance of
the following features: actors, director, MPAA rating (suitable for children,
adult content etc), genre and average rating by other users. They also
selected their favorite actors and directors and indicated which genres they
were in the mood for and for which ones they were not.
3. A number of items were selected at random from a pre-selected set. If
participants were already familiar with an item, they could request another
one by clicking on a button (e.g., “I might know this movie, please skip to
another one”). Participants evaluated the items and their explanations.
For each item in turn:
(a) Participants were shown the item and explanation, and rated:
– How much they would like this item.
– How good the explanation was.
They could opt out by saying they had “no opinion”, and could give
qualitative comments to justify their response.
(b) Participants tried the item. In the ﬁrst three experiments this step
was approximated by participants reading user and expert reviews on
Amazon.com; care was taken to diﬀerentiate between our explanation
facility and Amazon.
(c) They re-rated the item, and the explanation.
3.1.5 Statistical Analysis
To analyze the results for eﬀectiveness and satisfaction, mixed linear eﬀects
models were ﬁtted, with participant as random factor, trial as repeated factor
(as each participant evaluated multiple items), AR(1) as co-variance structure4, type of explanation as ﬁxed factor, and absolute eﬀectiveness and satisfaction as dependent variables respectively. We used the SPSS v19 procedure
mixed models linear, with Bonferroni correction for the pair-wise comparisons
between explanation types.
For the analysis of eﬀectiveness, opt-outs (“no opinion”) were treated as
missing values. However, opt-outs are not arbitrary missing data; they indicate
that participants did not have enough information to decide how much they
would like the item. One could argue that treating opt-outs as missing values
biases the data towards better eﬀectiveness, and that this may bias our results
when the opt-out rates diﬀer per condition. This is not always a problem: if a
4 The statistical inferences about the explanation types remained the same regardless of
the particular variance structure selected.
Nava Tintarev, Judith Masthoﬀ
condition with better eﬀectiveness also has a lower opt-out rate, then we can
assume that the better eﬀectiveness is a real result. However, if a condition
with better eﬀectiveness has a higher opt-out rate, then the result is less solid.
A solution may be to replace the missing values with the worst possible
eﬀectiveness value (6). However, the assumption that opt-outs result in arti-
ﬁcially better eﬀectiveness is not necessarily correct. If participants had been
forced to make a decision instead of being allowed to opt-out, they are likely
to have chosen the neutral middle of the scale rating. This may in fact have led
to better eﬀectiveness, as middle of the scale ratings would result in a worst
possible eﬀectiveness score of 3 rather than 6 (see Section 7.3.1 for more on
middle of the scale ratings). So, replacing missing values with 6 is not representative of participant behavior, and artiﬁcially diminishes the eﬀectiveness
of explanations with higher opt-out rates. This may lead to such explanations
looking worse than “misleading” explanations (where participants think they
can judge the item, but judge it completely wrongly). Replacing missing values with 3 is not ideal either. Participants opting out means that they had
insuﬃcient information to decide, which is a clear indication of poor eﬀectiveness. Replacing missing values with 3 may hide this, and make explanations
with high opt-out rates look better than they deserve. Therefore, we have decided not to replace missing values, but to analyze opt-out rates separately
and discuss their potential implications.
To analyze the opt-out rates, generalized linear mixed eﬀect models were
ﬁtted, with participant as random factor, trial as repeated factor, AR(1) as
co-variance structure, type of explanation as ﬁxed factor, and opt-out5 as
dependent variable with a binomial distribution and logit link function. We
used the SPSS v19 procedure GLMM, with sequential Bonferroni correction
for the pair-wise comparisons between explanation types.
3.2 Hypotheses
We hypothesized that:
– H1: Personalized feature-based explanations will be more eﬀective than
non-personalized feature-based and baseline explanations.
– H2: Users will be more satisﬁed with personalized feature-based explanations compared to non-personalized feature-based and baseline explanations.
5 Opt-outs were represented as a binary variable: “0” for opt-outs, and “1” when a rating
was given.
Eﬀectiveness of personalized explanations
Table 3 Participants in the experiments who were included in the analyses
Experiment
Mean (StD)
1. MoviesI
26.54 (8.13)
25 male, 21 female
2. MoviesII
24.58 (6.58)
26 male, 7 female
3. Cameras
24.17 (5.85)
31 male, 16 female
4. Final Evaluation
26.17 (7.24)
21 male, 27 female
3.3 Participants
Participants were recruited from university staﬀand students. They received a
gift voucher (£5-10 depending on the study duration) to compensate them for
their time. The studies took place in a lab under controlled conditions. Table
3 shows the number of participants and demographics for the four studies
discussed in this paper. These are the participants whose data was used in the
analyses. There were some additional participants that were excluded from
the analyses. The number of participants excluded and the reasons for their
exclusion will be discussed in each study.
3.4 Motivation of the choice of explanations
The explanations used in these experiments are short and simple. There are
three good reasons for this. Firstly, brevity is important in a context where
the user has to review many possible options. Secondly, the features that are
currently available in existing commercial services are limited in both diversity
and depth6. Thirdly, algorithms which consider simple features such as actor
and director names already exist .
Thus, the questions we are investigating are if it makes sense for the developers of explanations in recommender systems to change their algorithms
to explain by using item features, and if it makes sense to personalize the
features to present. Even a simplistic change can be a large investment, and
so an experiment of this type saves a considerable potential cost. As this sort
of change would not make much sense unless it made a diﬀerence, let us now
see if this is the case.
4 Experiments 1 and 2: Approximated eﬀectiveness for movies
The aims of the initial experiments in the movie domain was to see if using
movie features (e.g. lead actors), and personalization in explanations could
aﬀect their eﬀectiveness and user satisfaction. We chose to use item features
that realistically could be extracted from a real world system such as Amazon
Web Services as these were freely available via an API for a number of domains,
6 In these experiments we have chosen to use Amazon Web Services as a representative
example, although similar limitations are likely to occur with other commercial services.
Nava Tintarev, Judith Masthoﬀ
while considering the features extracted from our user studies (e.g. Tintarev
and Masthoﬀ2007a). This resulted in the features: genre, cast, director, MPAA
rating (e.g. rated R) and average rating. Our user studies also suggested that
genre information is important to most if not all users, so both feature based
conditions contain a sentence regarding the genre.
Experiment 1 has been reported in full in Tintarev and Masthoﬀ ,
and is therefore only summarized here. Since there was no signiﬁcant difference between conditions w.r.t. eﬀectiveness (and the trend was even for
non-personalized explanations to be more eﬀective), we considered potentially
confounding factors and repeated the experiment with some modiﬁcations.
This section shows the results of this new experiment (Experiment 2) in the
context of the results of Experiment 1.
4.1 Example explanations for Experiment 2
Table 4 summarizes the types of explanations that were given in the three
conditions in Experiment 2. Explanations could contain negative as well as
positive information, and the genre information could vary in terms of polarity:
positive if the movie belongs to only preferred genres, negative if it belonged
to any genre the participant did not want to see, and neutral if it contained
neither preferred nor ‘disliked’ genres.7.
4.2 Diﬀerences between Experiments 1 and 2
– Use of cover image. In Experiment 1, the movie cover was shown in all conditions, and this may have provided unintentional additional information
about the movie. In Experiment 2, the cover was not shown.
– Distinction between personalized and non-personalized conditions. In Experiment 1, the information about genre would always say whether the
genre was a preferred or non-preferred genre. This can be seen as a form
of personalization. In Experiment 2, this was only mentioned in the personalized condition.8
– Genre information. We ensured that the information about genres was
more detailed and complete in Experiment 2, as participants in Experiment
1 complained that genre information automatically retrieved from Amazon
was occasionally incorrect and incomplete9. The genre information was
annotated by hand, and the generated explanations describe all the genres
a movie belongs to.
7 While not included in the results here, polarity was found to signiﬁcantly correlate with
users’ initial ratings of movies, but there was no signiﬁcant correlation between polarity and
eﬀectiveness (Tintarev and Masthoﬀ2008b).
8 While this was a ﬂaw, the personalized condition in Experiment 1 was still more personalized than the other feature-based condition.
9 This will have aﬀected both feature-based conditions to the same extent, so does not
invalidate the results regarding the eﬀect of personalization.
Eﬀectiveness of personalized explanations
Table 4 Experiment 2: Example explanations per condition in the movie domain. The
general interface was similar in appearance in all three conditions, as exempliﬁed by the
screenshot for the baseline condition. Both feature-based explanations describe the movie
genres, but the personalized explanation also relates them to the user’s preferences. The
personalized explanation also describes the most important feature for this user (actors),
while the non-personalized explanation used a random feature (average rating).
Non-personalized:
“This movie belongs to the genre(s): Action & Adventure and
Comedy. On average other users rated this movie 4/5.0”
Personalized:
“Unfortunately, this movie belongs to at least one genre you
do not want to see: Action & Adventure. It also belongs to the
genre(s): Comedy. This movie stars Jo Marr and Robert Redford.”
– Number of trials. Experiment 1 typically took around 45 minutes to complete, which may have led to participant fatigue. For this reason, we reduced
the number of trials from ten in Experiment 1 to three in Experiment 2.
– Baseline condition The baseline condition in Experiment 2 mentions movies
in the top 250 (rather than top 100) in the Internet Movie Database
4.3 Materials
In Experiment 2, the 85 movies were distributed evenly among 17 genres.
As a movie belongs to multiple genres, they were balanced according to the
main genre. In this experiment, movies were also selected for having a high
degree of variation of rating. High variation is more likely to lead to polarized
views leading to an even distribution of initial ratings of movies. We used the
measure of rating variation (entropy) described in Rashid et al , based
on the MovieLens 100k ratings data set10, which considers both variation and
number of ratings for that movie (to avoid very obscure movies). Fourteen of
the movies were in the top 250 of IMDB.
10 
Nava Tintarev, Judith Masthoﬀ
4.4 Results
In Experiment 2, seven participants (out of twelve) in the baseline condition
had to be removed from analysis for clicking through the experiment (ﬁnishing in under 90 seconds) or dropping out altogether. We also found that the
remaining participants in the baseline condition opted out of giving Movie
Before ratings in 56% of cases. There was a signiﬁcant eﬀect of condition on
opt-out rates (F(2,94)=17.31, p<0.001), with the baseline condition having
signiﬁcantly more opt-outs than the other conditions (sequential Bonferroni
corrected, p<0.05). This suggests that explanations such as our baseline without cover images could damage user satisfaction considerably. Because of these
problems, the baseline condition is not included in the analysis of Experiment
Are personalized explanations more eﬀective (H1)? As in Experiment 1, explanations in the non-personalized feature-based condition appear to be most
eﬀective in Experiment 2 (see Table 5), but this diﬀerence was not signiﬁcant (F(1,57.61)=2.23, p=.14). It is also in this condition that participants
opted out the least (4.3% compared to 15.2% in the personalized condition,
sequential Bonferroni corrected, p<0.05).
Table 5 Experiments 1 and 2: Means (StD) of the two movie ratings (excluding opt-outs)
and eﬀectiveness per condition per experiment. “Before” and “After” denote the two movie
ratings before and after viewing Amazon reviews. Eﬀectiveness is better the closer it is to
Eﬀectiveness
(absolute)
Eﬀectiveness
3.45 (1.26)
4.11 (1.85)
1.38 (1.20)
-0.69 (1.69)
Non-personalized
3.85 (1.87)
4.43 (2.02)
1.14 (1.30)
-0.57 (1.64)
Personalized
3.61 (1.65)
4.37 (1.93)
1.40 (1.20)
-0.77 (1.68)
Non-personalized
3.84 (1.95)
3.93 (1.95)
0.96 (0.81)
-0.09 (1.25)
Personalized
3.75 (2.05)
4.00 (1.87)
1.33 (1.27)
-0.25 (1.85)
Are users more satisﬁed with personalized explanations (H2)? Table 6 shows
that the Explanation After ratings are mostly higher than the Explanation
Before ratings. This may be due to participants confounding our explanations
with the Amazon reviews when rating Explanation After. Since participants’
comments corroborate this, we did not include Explanation After in analyses for the ﬁrst three experiments (where Amazon reviews are used). The
mean rating for Explanation Before is low overall. In Experiment 1, there was
a signiﬁcant diﬀerence in ratings between the conditions (F(2,449.36)=9.95,
p<0.001), with participants rating the ﬁrst explanation signiﬁcantly highest in the personalized condition (Bonferroni corrected, p<0.01). In Experiment 2, the mean satisfaction for personalized explanations is again highest
(Explanation Before, see Table 6), though the diﬀerence between the nonpersonalized and personalized conditions is not statistically signiﬁcant this
Eﬀectiveness of personalized explanations
time (F=(1,75.15)=1.11, p=0.3). This suggests that while the personalized
explanations may not help users make better decisions, users may still be
more satisﬁed. This was conﬁrmed by the qualitative comments given by participants.
Table 6 Experiments 1 and 2: Means (StD) of the two explanation ratings, excluding
opt-outs, (on a scale from 1-7, 1=really bad, 7=really good) per condition per experiment.
“Before” and “After” denote the two explanation ratings before and after viewing Amazon
Explanation Before
Explanation After
Experiment 1
2.38 (1.54)
2.85 (1.85)
Non-personalized
2.50 (1.62)
2.66 (1.89)
Personalized
3.09 (1.70)
3.15 (1.99)
Experiment 2
Non-personalized
2.72 (1.68)
2.83 (1.74)
Personalized
3.31 (1.55)
2.97 (1.33)
General comments In all conditions, there was more underestimation than
overestimation. In light of this underestimation we reconsider the fact that
movie ratings, and Amazon reviews, may lean toward positive ratings. If Amazon reviews are overly positive, this may have aﬀected our results.
5 Experiment 3: Approximated eﬀectiveness for cameras
To investigate whether the results were due to the domain, or were more general, we repeated the experiment in another domain. For this purpose we chose
a domain which was more objective and higher investment: digital cameras.
Our intuition was that the movie domain suﬀers from being subjective in nature. So while it is possible to talk about the user’s favorite actor starring
in a movie, the actor’s performance may be deemed as both good and bad
depending on the user. Nor is an actor’s performance likely to be consistent
across their career, deeming this feature (most commonly selected by our participants) a poor indicator for decision support. We expected this eﬀect to be
smaller in a more objective domain such as digital cameras.
We have also seen that participants may be less forgiving of overestimation
(persuasion) in high investment and (relatively) objective domains (Tintarev
and Masthoﬀ2008a). We are interested to see how additional and personalized
information in explanations inﬂuences users in the camera domain: whether
this impacts eﬀectiveness and user satisfaction.
5.1 Modiﬁcations
To perform the experiment in a second domain, several changes were needed.
Nava Tintarev, Judith Masthoﬀ
5.1.1 Procedure
– Participants are less likely to be consumers of cameras than movies. To
exclude participants that would never use or buy a camera, they indicated
their photography expertise and likelihood of purchasing a camera.
– People buy fewer cameras than they see movies. This means that participants are unlikely to be familiar with a particular camera, especially
because explanations were accompanied with a generic image of a camera
(see below) and the name of the camera was not used. For this reason, we
did not need a “I might know this item” button as in the movie experiments.
– Participants evaluated 4 cameras and explanations.
– No polarity was applied to the explanations. In the previous experiments,
a movie could belong to a user’s preferred genres (positive polarity) as
well as disliked genres (negative polarity). Even though polarity may not
impede eﬀectiveness11, in this experiment we chose to control for polarity
5.1.2 Explanations used
Table 7 provides an overview of the explanations given in the three conditions
described below:
– Non-personalized condition. This explanation describes the three most commonly selected camera features (as described in Section 5.1.4), which were
camera type, brand, and price. Three features are mentioned to make these
explanations comparable in length to the explanations in the movie domain
which mentioned genre as well as a feature.
– Personalized condition. The explanation describes the three camera features that are most important to this participant. For example, if features
’price’, ’brand’ and ’zoom’ are most important the explanation may be:
“This camera costs 679.95£. This camera is a Nikon. It has an optical
zoom of 11.0x.”
– Baseline condition. There is no equivalent to IMDB for cameras, so the
baseline was changed to a bar chart which summarizes review ratings of
the camera categorized into good, ok, and bad (see Table 7). This sort of
bar chart exists on the Amazon website, and is similar to the explanations
given on several commercial sites. It is similar to the bar chart used in
 , but considers all reviews rather than similar users
11 In the previous experiments, polarity was found to signiﬁcantly correlate with users’
initial ratings of movies, but there was no signiﬁcant correlation between polarity and eﬀectiveness (Tintarev and Masthoﬀ2008b).
Eﬀectiveness of personalized explanations
Table 7 Experiment 3: example explanations for three conditions in the camera domain.
The general interface was similar in appearance in all three conditions, as exempliﬁed by
the screenshot for the baseline condition. In the personalized condition, the explanation
describes the three camera features that are most important to this participant, but in the
non-personalized the same features (type, brand and price) are always used. While simple,
the baseline is similar (but not identical) to information supplied by Amazon and used in
the study by Herlocker et al 2000.
Non-personalized:
e.g. “This camera costs 179.0£. This camera is a Panasonic. This
camera is a ’point and shoot camera’.”
Personalized:
e.g. “This camera costs 679.95£. This camera is a Nikon. It has
an optical zoom of 11.0x.”
5.1.3 Hypotheses
The changed baseline results in a third hypothesis. Camera reviews are strongly
biased toward positive ratings: there are more positive ratings than negative
and neutral. Bilgic and Mooney found that a positively biased bar chart
is likely to lead to overestimation of items. For this reason we also hypothesize
– H3: Users are more likely to overestimate their rating of the camera in the
baseline condition compared to the two feature-based explanations (persuasion).
5.1.4 Materials
We wanted to elicit which features are generally considered important when
purchasing a camera. As a starting point, we surveyed which features existing
recommender systems in the camera domain have used . We shortlisted the following features: brand, optical zoom, price, resolution, weight, memory and type
of camera (SLR or point and shoot). From these memory was excluded as
modern cameras usually have external memory that can be added on. The re-
Nava Tintarev, Judith Masthoﬀ
Table 8 Experiment 3: Range of features over the 22 cameras used in this experiment.
Mean (StD)
448.40 (489.23)
Resolution
5-12 megapixels
9.45 (2.21)
5.77 (4.80)
421.59 (286.86)
Camera ‘type’
SLR (9), ”point-and-shoot“ (13)
Panasonic (4), Nikon (4), Canon (4), Olympus (4), Fujiﬁlm (3), Sony (3)
Table 9 Experiment 3: Total number of ratings, and mean number of reviews per camera
(StD), by category of rating.
Mean (StD)
Mean (StD)
Mean (StD)
16.09 (13.10)
0.86 (1.22)
1.32 (2.69)
maining six features are all readily available on Amazon’s Web Service. Next,
11 members of university staﬀor members of the university photography club
(1 female, 10 male; average age 44.67, range 29-62) rated the importance of
these six features. The purpose of the questionnaire was twofold. Firstly, we
wanted to know whether there are features that are commonly considered important. Secondly, we wanted to ﬁnd out if there was a case for personalization,
i.e. do diﬀerent people ﬁnd diﬀerent features important. Overall, type of camera, brand, and price were found to be most important. However, this is not a
complete consensus, people do rate the features diﬀerently. It is not the case
that any one feature was rated highest by each participant.
Twenty-two cameras have been hand-picked from the Amazon website.
Speciﬁcations for SLR cameras were deﬁned by the lens that came with them
per default. Table 8 summarizes the range for each of the features. It is also
possible to select the cameras automatically via an API, but handpicking the
items enabled us to control the range for each feature better.
As seen in Table 9, there were by far more good ratings (4’s and 5’s) than
ok (3s) and bad (1s and 2s), which is a pre-existing bias for the cameras which
had at least 3 reviews.
The explanation was accompanied with an identical image of a camera,
with a semitransparent letter (A-D) superimposed to diﬀerentiate the four
5.2 Results
Five participants were removed from analysis: one for not completing the experiment, three for being “unlikely to buy a camera” and one for saying they
“knew nothing about photography”.
Eﬀectiveness of personalized explanations
5.2.1 Enough to form an opinion?
As in the previous experiments we inquire if the short explanations are suﬃcient for users to form an opinion. Table 10 shows that in the baseline there was
a large percentage (23.9%) of opt-outs for the ﬁrst camera rating. There was
a signiﬁcant eﬀect of condition on opt-out rates (F(2,185)=18.30, p<0.001).
The baseline and non-personalized conditions had signiﬁcantly more opt-outs
than the personalized condition (sequential Bonferroni corrected, p<0.01).
Table 10 Experiment 3: Percentage of opt-outs for the two camera ratings and the two explanation ratings. “Before” and “After” denote the ratings before and after viewing Amazon
Explanation
Explanation
Non-personalized
Personalized
Table 11 Experiment 3: Means (StD) of the two camera ratings (excluding opt-outs) and
eﬀectiveness per condition. “Before” and “After” denote the two camera ratings before and
after viewing Amazon reviews.
Eﬀectiveness
(absolute value)
Eﬀectiveness
(signed value)
3.94 (1.47)
4.75 (1.73)
1.77 (1.50)
-0.77 (2.20)
Non-personalized
3.88 (1.62)
4.78 (1.75)
1.14 (1.32)
-0.78 (1.57)
Personalized
3.83 (1.86)
4.95 (1.77)
1.88 (1.34)
-1.08 (2.05)
Fig. 1 Experiment 3: Distribution of (signed) eﬀectiveness per condition for cameras (excluding opt-outs).
Nava Tintarev, Judith Masthoﬀ
Fig. 2 Experiment 3: Camera ratings before and after, per condition.
5.2.2 Are personalized explanations more eﬀective? (H1)
Table 11 shows the ratings of the cameras and eﬀectiveness per condition.
Eﬀectiveness appears to be best (lowest value) in the non-personalized condition. Comparing between conditions we found a signiﬁcant diﬀerence in (absolute values for) eﬀectiveness (F(2,151.42)=4.83, p < 0.01). Post-hoc tests
showed that eﬀectiveness was signiﬁcantly better in the non-personalized condition than the personalized condition (Bonferroni corrected, p < 0.05) and
marginally better in the non-personalized condition than in the baseline condition (Bonferroni corrected, p = 0.056). That is, non-personalized explanations
were more eﬀective. Figure 1 shows that almost 45% of explanations in this
condition lead to perfect eﬀectiveness (i.e. Rating1 - Rating2 = 0). In other
words there is no support for H1, personalized explanations are not most eﬀective. The lower opt-out rates in the personalized condition indicate that that
the personalized explanations did help participants form an opinion, though
not necessarily an accurate one.
5.2.3 Do baseline explanations lead to more overestimation? (H3)
We also hypothesized that participants would be more likely to overestimate
their ratings of cameras in the baseline condition. Firstly, in Table 10 we see
that a large number of participants in this condition have opted out. In Table
Eﬀectiveness of personalized explanations
11, we see however that with the opt-out ratings omitted, the initial ratings
for cameras are comparable between the three conditions. Figure 2 also shows
the distribution of these initial camera ratings per condition. Moreover, the
signed value of eﬀectiveness in Table 11 suggests a small underestimation in
the baseline condition. These ﬁndings are contrary to our third hypothesis,
It is surprising that the baseline has reasonably good eﬀectiveness. The
distribution of initial ratings in the baseline condition (Figure 2) suggests that
users are less susceptible to persuasion than one might initially think. We
return to this when discussing users’ qualitative comments.
5.2.4 Are users more satisﬁed with personalized explanations? (H2)
We compared the users’ ratings for the initial explanations (Explanation Before, see Table 12), and found a signiﬁcant diﬀerence between conditions
(F(2,168.38)=6.04, p<0.01). Post-hoc tests support H2; participants were signiﬁcantly more satisﬁed with personalized explanations than non-personalized
(Bonferroni corrected, p<0.01). There was, however, no signiﬁcant diﬀerent
between the baseline and the personalized condition.
Table 12 Experiment 3: Means (StD) of the two explanation ratings, excluding opt-outs,
(on a scale from 1-7, 1=really bad, 7=really good) per condition. “Before” and “After”
denote the two explanation ratings before and after viewing Amazon reviews.
Explanation Before
Explanation After
2.83 (1.44)
3.80 (1.87)
Non-personalized
2.38 (1.64)
2.87 (1.94)
Personalized
3.27 (1.27)
2.67 (1.56)
5.2.5 Distribution of features
Participants in the non-personalized condition were all shown the same three
features: price, brand and type. We were interested in the distribution of features chosen in the personalized condition to see how it compared to these
three features. The combination of features used in the non-personalized condition only received 8% of the votes, and there is a great deal of variability in
the features participants found most important.
5.2.6 Limitations and qualitative comments
As we mentioned in our description of the three conditions, the bar chart we
used is not identical to the bar chart used in . This is
also noted in user comments; “...doesn’t give you information about what kind
of customers rated it (a complete newbie wanting to buy a ’point-and-shoot’
will rate things diﬀerently than amateur buying a SLR)”;“No clear indication
Nava Tintarev, Judith Masthoﬀ
of the audience that’s declared it ’good’. ”
Participants reacted to the number of ratings and polarity of the baseline
explanations, which may explain why the baseline performed surprisingly well
yet again. Participants were not easily persuaded, and did not rate cameras
in this condition consistently highly. For example, a bar chart using too few
ratings was considered insuﬃcient information: “...since it only has the review
from six people, it’s hard to base my decision on just this explanation....”;“Too
small a test group for a clear set of results”.
The majority of reviews were positive even for cameras with many reviews,
which by some participants was perceived as poor information as well: “There
are no other opinions except for the people’s who are in favor of the camera.
This is a poor display of statistics”;“everybody cannot possibly rate this good,
there has to be some opposers.”. Explanations were taken more seriously when
the distribution of ratings was more even; “The ratings have a larger review
base, with some dissenting into “ok”, broader review”.
There were also two small issues, but neither should have aﬀected the
comparison between conditions: some participants were inﬂuenced by the accompanying image (which was always the same) and some compared cameras
with each other.
5.3 Summary of Experiment 3
Participants made better decisions in the non-personalized condition, but preferred the personalized explanations. This result is similar to what we found
in the movie domain, although we did not expect to ﬁnd this in an objective,
high investment domain.
6 Experiment 4: True eﬀectiveness for movies
So far, we have been looking at explanation eﬀectiveness in two domains:
movies and cameras. These experiments were limited by the approximation
we used for evaluating the items: reading online reviews. In particular, the
reviews used are likely to have been positively biased, and so this raises the
question if the same results would be found if users actually tried the items.
It is possible that non-personalized explanations cause an overestimation that
correlates well with the positive bias of the reviews, but that these explanations
are not in actual fact eﬀective. In this experiment, participants experience the
items. This experiment is conducted in the movie domain as it is easier, and
less costly, to collect suitable materials. This also simpliﬁes the users’ valuation
of items, as it is easier to let users watch movies than evaluate digital cameras.
Eﬀectiveness of personalized explanations
6.1 Modiﬁcations
6.1.1 Procedure
Given the strong result for baseline explanations in the previous experiments
in the movie domain, we wanted to know how much the title inﬂuenced users’
ratings of a movie and so included an initial rating (Movie Title). So, the
procedure is revised such that:
1. The user rates the item on the basis of the title only (Movie Title)
2. The user rates the item on the basis of the explanation (Movie Before)
3. The user tries the item
4. The user re-rates the item (Movie After)
Other changes are:
– In addition to the usual “I might know this movie, please skip to another
one” button, for ethical reasons an “I don’t want to watch this movie”
button was also included so that participants would not be forced to watch
a movie they did not want to see.
– Participants were asked if they disliked the genres children, comedy or
animation, as most of our movies were (for ethical reasons) from those
genres. This resulted in three participants being omitted. Participants were
not explicitly told about the selection criteria for movies.
– Participants were asked to watch as many short movies as was feasible
within the duration of an hour, but no more than 3.
– For ethical reasons, participants had full control over the movies (e.g. they
could press the stop and pause buttons), and could terminate the experiment at any time.
6.1.2 Experimental design
The baseline condition was modiﬁed to adjust for the fact that we were using
short movies (see Section 6.1.4): e.g. “This movie is (not) in the top 50 short
movies on IMDB (the Internet Movie Database).”. The other conditions were
the same as in the second experiment.
6.1.3 Hypotheses
Hypotheses H1 and H2 are as before, while H3 regards the camera experiment
only. The addition of an initial movie rating leads to an additional hypothesis:
– H4: Users will able to form an opinion more often after the ﬁrst explanation
(Movie Before) than after just seeing the title (Movie Title).
We believe that the title is not enough information for a user to form an
opinion, while the additional information supplied in explanations will help to
make a decision.
Nava Tintarev, Judith Masthoﬀ
Table 13 Experiment 4: Short movies used, with genres and duration.
Wrong Trousers
Animation, Children, Comedy, Crime
Close Shave
Animation, Children, Comedy, Crime
Grand Day Out
Animation, Children, Comedy, Crime
Feed the Kitty
Animation, Children, Comedy
Animation, Children, Fantasy
For the Birds
Animation, Children, Comedy
Mickey’s Trailer
Animation,
Adventure,
Animation, Comedy, Fantasy
Mr. Bean’s Christmas
Rabbit Seasoning
Animation, Children, Comedy
Hedgehog in the Fog
Animation, Children, Drama, Fantasy,
Animation, Action, Adventure, Children, Comedy, Drama, Thriller
Strange to Meet You
Comedy, Drama
Jack Shows Meg His Tesla Coil
Comedy, Drama
Somewhere in California
Comedy, Drama
6.1.4 Materials
To decrease the duration of the experiment, short movies were chosen over full
length features. Movies were selected as to be non-oﬀensive. Fifteen movies
were selected, out of which eleven are in the top 50 short movies in IMDB. The
durations of the movies vary from 3-30 minutes (mean=11.73, StD=10.36). Table 13 summarizes the selected movies. The majority of movies belong to the
genres comedy, animation, and children. Most of the movies have an international certiﬁcation rating, and some have actors (e.g. Rowan Atkinson) or
directors (e.g. Tim Burton) that are likely to be known. The movies also vary
w.r.t. other factors, for example some are in foreign languages (English subtitles), the animations diﬀer in style, and three of the movies are black and
The selection of non-oﬀensive movies could result in users’ ratings of movies
being less well distributed. However, for an interesting analysis, it suﬃces that
the distribution diﬀers suﬃciently from the mid-point, even if it does not make
full use of the scale. This was conﬁrmed in two pilot sessions.
6.2 Results
In this section we survey the eﬀectiveness of, and satisfaction with, explanations when participants were able to trial the items (watch short movies).
Eﬀectiveness of personalized explanations
6.2.1 How do titles compare to explanations? (H4)
Opt-outs. Looking at the percentages of opt-outs in Table 14 we see that, on
average, participants opted out 35.6% of the time for Movie Title, compared
to 15.9% after receiving an explanation (Movie Before). In Figure 3 we see the
change in opt-outs for the three movie ratings across the conditions, and the
noteworthy decrease of opt-outs from Movie Title to Movie Before in all three
conditions. This suggests that explanations do help users to make decisions.
We investigated whether the diﬀerence in opt-outs between Movie Title and
Movie Before was signiﬁcant. We used a repeated measure “opt-out point” with
two levels (one for Movie Title and one Movie Before) each associated with a
binary opt-out value. We ﬁtted a generalized estimating equation regression
model, with participant as random factor, trial and opt-out point as repeated
factors, AR(1) as co-variance structure, type of explanation and opt-out point
as ﬁxed factors, and opt-out as dependent variable with a binomial distribution
and logit link function. We included the main eﬀects of type of explanation
and opt-out point, their interaction, and a random intercept in the model.
We used the SPSS v19 procedure GEE. There was a signiﬁcant eﬀect of optout point (Wald Chi-Square=15.20, p < 0.001). Surveying the direction of
changes, H4 is conﬁrmed - more participants were able to make decisions with
the explanations than with just the title.
At ﬁrst glance the opt-out rates for Movie Title seem to diﬀer per condition.
However, further statistical analysis (GLM) shows that this diﬀerence is not
signiﬁcant (F(2,129)=2.03, p=0.14). Nevertheless, in Section 6.2.5 we discuss
if the large number of opt-outs for Movie Title in the baseline could be an
artifact of the movies shown to the participants or individual diﬀerences.
Table 14 Experiment 4: Percentage of opt-outs for the three movie ratings.“Title”, “Before” and “After” denote the three movie ratings based on the movie title only, and with
the explanation before and after viewing the movie.
Movie Title
Movie Before
Movie After
Non-personalized
Personalized
Mid-scale ratings. It is also worth surveying the proportion of ratings in the
middle of the scale (value=4). While these are not as strong an indicator of
(lack of) informativeness as opt-outs, a larger proportion in the middle of the
scale could suggest that participants had diﬃculty in forming a strong opinion.
Movie Before and Movie After were distributed beyond the mean rating of 4,
suggesting that participants are able to form opinions across the scale. Movie
Before and Movie After ratings are also skewed toward the higher end of the
Nava Tintarev, Judith Masthoﬀ
Fig. 3 Experiment 4: Change in opt-outs between the three movie ratings.
Table 15 Experiment 4: Means (StD) of the three movie ratings (excluding opt-outs).
“Title”, “Before” and “After” denote the three movie ratings based on the movie title only,
and with the explanation before and after viewing the movie.
Movie Title
Movie Before
Movie After
4.36 (0.95)
4.28 (0.81)
4.76 (1.67)
Non-personalized
4.12 (1.67)
4.45 (1.53)
4.58 (1.88)
Personalized
3.86 (1.23)
4.31 (1.26)
4.93 (1.86)
scale; there are more ratings of value 4 or above. This skew is not completely
surprising given that we had selected movies that were unlikely to cause offense, as well as avoided genres and movies that participants did not want to
see. There were more mid-scale ratings of 4 in the non-personalized and personalized conditions for Movie Title than for Movie Before and Movie After.
The large number of opt-outs and mid-scale ratings suggest that users struggled to specify an opinion with the title alone.
6.2.2 Do explanations diﬀer on how much they help to form an opinion?
We have seen that explanations help people form an opinion. We also investigated whether explanations diﬀered on how much they helped. Table 14 shows
the opt-out rates for Movie Before in the diﬀerent conditions. There was a signiﬁcant eﬀect of condition on these opt-out rates (F(2,129)=3.65, p<0.05). The
baseline condition had signiﬁcantly more opt-outs than the other two conditions (sequential Bonferroni corrected, p<0.05). So, baseline explanations were
the least helpful.
6.2.3 Are personalized explanations more eﬀective? (H1)
The mean eﬀectiveness in each condition is summarized in Table 16. Looking at the signed eﬀectiveness we see that in all conditions the explanations
Eﬀectiveness of personalized explanations
led to a slight underestimation. Comparing between conditions we found a
signiﬁcant diﬀerence in (absolute values for) eﬀectiveness (F(2,103.32)=4.39,
p < 0.05). Surprisingly, explanations in the baseline condition led to the “best”
eﬀectiveness (Bonferroni corrected, p < 0.05 for baseline compared to nonpersonalized, p = 0.05 for baseline compared to personalized). This ﬁnding
is in stark contrast to the large number of opt-outs in this condition, which
indicate that baseline explanations are clearly not helpful more than half of
Table 16 Experiment 4: Means (StD) of eﬀectiveness (excluding opt-outs) per condition.
Eﬀectiveness
(absolute value)
Eﬀectiveness
(signed value)
1.09 (1.00)
-0.41 (1.43)
Non-personalized
1.78 (1.37)
-0.08 (2.26)
Personalized
1.69 (1.08)
-0.41 (1.98)
Fig. 4 Experiment 4: Distribution of eﬀectiveness, excluding opt-outs
One possible explanation for the comparatively high eﬀectiveness of baseline explanations is that the baseline explanations biased the participants toward high ratings, as most of the short movies were in the top 50 in IMDB.
As the selection of movies was guided by being acceptable to users, this also
was likely to lead to a large proportion of high ratings. There was a skew
toward high ratings in all conditions for Movie Before, but this skew was not
the most severe in the baseline. Movie Before ratings are comparable for all
conditions (no signiﬁcant diﬀerence), suggesting that the baseline is not more
strongly skewed toward positive ratings. We also surveyed how many of the
shown movies in the baseline were in the top 50, and found that the relative
proportion was comparable (44.4% were in the top 50, and 55.6% were not).
So, it seems that the large number of mid-range ratings is a more plausible ex-
Nava Tintarev, Judith Masthoﬀ
planation for good baseline eﬀectiveness than a skew toward positive ratings.
(We explain why a rating distribution with many mid-range ratings may lead
to misleading measurements of eﬀectiveness in Section 6.3.)
6.2.4 Are users more satisﬁed with personalized explanations? (H2)
We hypothesized that participants would prefer personalized explanations to
non-personalized and baseline explanations. First, we look at the opt-out rates.
In Table 17 we see that while the opt-out rates for the explanations in the two
feature based conditions are comparable, the opt-out rate for explanations in
the baseline is much higher.
Next, we investigate if participants preferred the personalized explanations
Table 17 Experiment 4: Means (StD) and percentage of opt-outs of the two explanation
ratings, (on a scale from 1-7, 1=really bad, 7=really good) per condition. “Before” and
“After” denote the two explanation ratings before and after viewing the movie. Means
exclude opt-outs.
Explanation Before
Explanation After
Mean (StD)
Mean (StD)
2.55 (1.43)
2.89 (1.60)
Non-personalized
3.51 (1.61)
3.53 (2.00)
Personalized
3.21 (1.46)
3.16 (1.83)
over the explanations in the other two conditions. First, we look at the initial
explanation ratings (Explanation Before). There was signiﬁcant eﬀect of condition (F(2,113.93)=4.05, p < 0.05). There was signiﬁcant diﬀerence between
the ratings for the baseline and non-personalized explanations (Bonferroni corrected, p < 0.05), but not for the ratings between the personalized condition
and the other two conditions (although the mean rating for Explanation Before indicates that participants preferred non-personalized explanations over
personalized ones). This contradicts our previous ﬁndings, where personalized
explanations were preferred in both the camera and movie domain. We will
discuss this further in Section 6.2.6.
In our previous experiments we were not able to compare the ratings for
Explanation After as participants confused our test bed with Amazon itself.
As this confusion no longer is a factor in this experiment, we can study the
participants’ opinion of the explanations after watching the movie. To our surprise we found no signiﬁcant diﬀerence between the conditions, but note that
the non-personalized explanations still have the highest mean rating, followed
by the personalized explanations.
Eﬀectiveness of personalized explanations
Table 18 Experiment 4: Number of times used and number of opt-outs for each movie per
condition, and overall opt-out rate (%) for each movie across conditions. Movies have been
sorted by overall opt-out rate, from lowest to highest.
N opt-outs
N opt-outs
N opt-outs
Mr. Bean’s Christmas‡
Wrong Trousers‡
Close Shave‡
Mickey’s Trailer⋄
Hedgehog in the Fog†
Feed the Kitty⋄
Rabbit Seasoning
Jack shows Meg his Tesla coil
For the Birds†
Strange to Meet You
Somewhere in California‡
Grand Day Out†
The Rocks†
‡ = identical opt-outs, † = very similar opt-outs, ⋄= diﬀerence due to 1 participant
6.2.5 Why did more participants opt out for Movie Title in the baseline
condition and does it matter?
A larger proportion of participants opted out for Movie Title –before explanations had been given – in the baseline condition. We also see that the mean
Movie Title rating is lowest in the personalized condition, despite users only
being shown the title. We imagine two possible reasons for this: either the
titles diﬀer somehow, or there are notable individual diﬀerences between participants.
Diﬀerence in shown titles? If the movie titles shown to participants in the
baseline were less informative (revealing less information such as the character in “Mr. Bean’s Christmas”) than in the other conditions, this could
explain the presence of more opt-outs. Table 18 shows how often each movie
was shown in each condition.12 In general, the distribution of titles between
the three conditions is comparable. However, there are some small diﬀerences
which may have contributed to the higher opt-out rate for Movie Title in the
baseline condition. For example, the baseline used The Rocks (which has a
high overall opt-out rate) a bit more and Mickey’s Trailer (which has a low
overall opt-out rate) a bit less. Assuming that the overall opt-out rates per
movie are representative and that participants are similar between conditions,
the diﬀerence in movie distribution would result in a slightly higher expected
12 Remember that movies shown were chosen randomly, but with participants requesting
another movie if they had already seen it. It would have been diﬃcult to ensure the same
movies were used in each condition given we could not control what participants had already
Nava Tintarev, Judith Masthoﬀ
opt-out rate13 for the baseline (38%) compared to the non-personalized (35%)
and personalized (34%) conditions. So, the slightly diﬀerent distribution of
movies may have contributed, but is clearly not the only factor.
Diﬀerences between participants? It is harder to discuss the eﬀects of individual diﬀerences as participants only rated up to three movies, but we did survey
the rating patterns of individual participants. While participants in the baseline on average opted out more for Movie Title, this was not due to participants
opting-out for all the titles they rated: there were many cases where participants opted out for some of the movies, but not all. Table 18 shows that for
most movies Movie Title opt-out ratios are quite similar between conditions:
they are identical for four movies, very similar for four movies, and diﬀerences
are caused by only one participant for three movies. So, there does not seem
to be a diﬀerence in general participant decisiveness between conditions. We
conjecture that small deviations in participants’ reactions to individual titles
(e.g. somebody not thinking of Mickey Mouse when seeing the title Mickey’s
Trailer) caused diﬀerences in opt-outs.
So, overall, a likely explanation is that the diﬀerence in opt-outs for Movie
Title may have been caused by a combination of a small diﬀerence in movies
shown and some small individual diﬀerences.
Does the diﬀerence in Movie Title opt-outs matter? We do not expect the
small diﬀerence in movies shown and small individual diﬀerences to have had
much impact on our results. While it is likely that the higher opt-out rate in
the baseline condition contributed to the high opt-out rate for Movie Before in
the baseline, there is plenty of evidence to suggest that this high opt-out rate
for Movie Before was also caused by the baseline explanations. Participants
considered baseline explanations less informative, as evidenced by the high
number of opt-outs for Explanation Before and the low ratings for Explanation
Before (see Section 6.2.4). This is also corroborated by the large proportion
of opt-outs for Movie Before in the baseline for all our previous experiments
(see Table 21).
6.2.6 Qualitative comments
Participants may use the movie title to inform their opinion. Indeed, this is
reﬂected in the participants’ comments: “..I considered what it might be like
based on the title and the genre (personalized condition)”,“I thought the title
(Rabbit Seasoning) suggested rabbits being killed but the light hearted nature of
the ﬁlm made me enjoy it more than I thought I would.”
We were surprised that personalized explanations in this experiment did
not lead to higher satisfaction than non-personalized explanations. A large
13 A condition’s expected number of opt-outs has been calculated by summing over the
movies, the product of the movie’s overall opt-out rate and the number of times it was
Eﬀectiveness of personalized explanations
proportion of the explanations in the personalized condition described the actors, but the information is not as useful to the participants as they might
have anticipated. The names used were largely unknown to participants, as it
was harder to ﬁnd short movies with known actors. For animations in particular, participants saw the names of actors whose voices were recorded, rather
than the character they played (e.g. Bugs Bunny or Gromit). This could have
decreased the satisfaction with the personalized explanations as participants
were less likely to recognize them: ‘‘Also without knowing who the star is, this
could still not mean a lot to the description.”. Likewise, some participants
complained they did not recognize the director: “the director’s name is even
less recognizable than the actors’ names...”;“As I don’t know the director, the
rest of the description could easily belong to a totally diﬀerent movie.”
Participants also commented on factors that were not considered so important in our focus groups (Tintarev and Masthoﬀ2007a), but which may
have been identifying for the movies they were shown. For example, while our
focus groups participants said they did not care about movie studio, this does
aﬀect the style of animation: “...pretty much what I’d expect from a Pixar
movie.”;“Unlike the last movie I was not expecting a Walt Disney ﬁlm...”.
6.3 Summary of Experiment 4
The results of this experiment reinforce the importance of selecting relevant
evaluation criteria. While the baseline explanations were found to be the most
eﬀective, they also had the lowest satisfaction, and led to most opt-outs and
ratings in the middle of the scale.
The diﬀerence between opt-outs for the Movie Title and Movie Before
ratings oﬀers an argument in favor of explanations in recommendations: participants in all three conditions opted out a lot less after receiving even a
simple explanation.
Both feature-based explanations were initially (Explanation Before) rated
higher than baseline explanations, but only the diﬀerence between
non-personalized and baseline explanations was signiﬁcant. We believe that
the weaker result for personalized explanations in this experiment compared
to our previous experiments is due to the restricted choice of materials.
This experiment also brings into light two situations where our evaluation
metric for eﬀectiveness could fail: a) if a large proportion of ratings fall on the
middle of the scale and b) if the explanations are biased in the same direction
as the data. We discuss these further in our conclusion in Section 7.3.
We consider the eﬀect of letting users watch the movies contra reading
movie reviews on Amazon. In our case it is diﬃcult to separate the eﬀects of
material choice from the eﬀects of the change in design. The baseline explanations were the most eﬀective in this experiment, but this does not seem to
be due to an initial overestimation because Movie Before ratings were comparable between conditions (see Section 6.2.3). It is possible that the popularity
of short movies is a better predictor than for long movies.
Nava Tintarev, Judith Masthoﬀ
Our previous experiments in two domains used reading Amazon reviews
as an approximation for real experience and led to repeated results. One
could therefore also argue that using Amazon reviews leveraged the results
for feature-based explanations w.r.t. eﬀectiveness in our previous experiments.
That is, reading reviews on Amazon caused an overestimation that correlated
well with the (also overestimated) valuation of items after reading explanations. However, the average Movie After ratings in this experiment are also
high, even though real experience replaced the reading of Amazon reviews.
This bias would therefore also beneﬁt from a presumed positive skew caused
by feature-based explanations, but in this experiment no such bias is evident
as the feature-based explanations show worse eﬀectiveness. Our suggestion is
therefore that the selection of movies was more likely to have aﬀected our
results than the change of design, in particular with regard to satisfaction. It
is also worth considering that the baseline was more eﬀective simply because
IMDB is a good data source for short movies. Another alternative explanation
for our results is that while the baseline did not oﬀer the best possible explanations (inferring from the large number of opt-outs), the type of personalization we used in the personalized condition does not contribute to eﬀectiveness.
Naturally, further similar experiments with alternative item selection would
be required to conﬁrm that this really is the case.
7 Conclusions and future work
This paper has presented an overview of seven evaluation criteria for explanations in recommender systems and considerations related to choosing between them. We have discussed in detail the aim of eﬀectiveness and how
it can be measured, also highlighting limitations of the current metrics. We
have presented a series of experiments in two domains investigating the impact of personalization and feature-based explanations on eﬀectiveness. While
there were issues related to the individual experiments (as discussed above), a
meta-review of the series of experiments, presented below, allows us to draw
overall conclusions with relative conﬁdence. First, we address the question of
why we should explain, or whether there is any point in explaining recommendations at all (Section 7.1). Then, we discuss if the type of explanations
matters, or rather if our personalization of explanations increased their eﬀectiveness (Section 7.2). Next, we summarize the lessons we have learned from
the experiments about the used metric for eﬀectiveness, and its relation to the
underlying data (Section 7.3). Finally, we conclude with suggestions for future
7.1 The value of explanations
For a recommender system aiming at user satisfaction rather than decision
support, well formed explanations can contribute positively: in Experiments
Eﬀectiveness of personalized explanations
1-3 we saw that personalization of explanations does increase satisfaction compared to a baseline. Table 19 summarizes participant’s satisfaction with the
explanation, by condition, for each of the four experiments: MoviesI, MoviesII,
Cameras and Final Evaluation.
Table 19 Summary: Means (StD) of initial satisfaction with explanations, excluding optouts, (on a scale from 1-7, 1=really bad, 7=really good) per condition per experiment.
Final Evaluation
2.38 (1.54)
2.83 (1.44)
2.55 (1.43)
Non-personalized
2.50 (1.62)
2.72 (1.68)
2.38 (1.64)
3.51 (1.61)
Personalized
3.09 (1.70)
3.31 (1.55)
3.27 (1.27)
3.21 (1.46)
Table 20 summarizes the change of opinion, where we hope to minimize this
value14. We see that the average change (mean absolute eﬀectiveness) for all
explanations and experiments is reasonable: on the magnitude of 1 scale point
on a 7 point scale. This suggests that explanations, our baselines included,
oﬀer relevant (albeit limited and imperfect) information, with the caveat that
baseline explanations have led to more opt-outs for the initial rating in all four
experiments (see Table 21 for a summary of opt-outs15). Part of the strong
Table 20 Summary: Means (StD) of absolute eﬀectiveness (excluding opt-outs), per condition per experiment.
Final Evaluation
1.38 (1.20)
1.77 (1.50)
1.09 (1.00)
Non-personalized
1.14 (1.30)
0.96 (0.81)
1.14 (1.32)
1.78 (1.37)
Personalized
1.40 (1.20)
1.33 (1.27)
1.88 (1.34)
1.69 (1.08)
result for baseline explanations (when participants did not opt out) may have
been due to the presence of a title for movies, but the replicated ﬁnding for
cameras is promising (see also Table 20): explanations (with or without titles)
can help in making decisions.
In the ﬁnal evaluation, we allowed participants to rate the title alone, and
then rate the item again once they saw the explanation. The number of optouts decreased signiﬁcantly once participants received an explanation. That
is, explanations also add to eﬀectiveness in terms of increasing the number of
items that users feel that they can evaluate.
14 The lack of data for the baseline condition for MoviesII in Table 20 reﬂects a large
opt-out rate, and participants removed for extremely short duration time.
15 The smaller diﬀerence in opt-outs between the baseline and feature-based conditions in
MoviesI is caused by the movie cover image being shown.
Nava Tintarev, Judith Masthoﬀ
Table 21 Summary: Percentage of opt-outs for Item Before per condition per experiment
Final Evaluation
Non-personalized
Personalized
7.2 The value of personalized explanations
Table 22 summarizes the results related to eﬀectiveness contra satisfaction
across all experiments. In three initial experiments in two domains, we found
that our method of personalization hindered eﬀectiveness, but increased satisfaction with explanations. However, these experiments were based on an approximation of eﬀectiveness where participants read reviews for items rather
than trying them.
In our ﬁnal evaluation, participants were able to watch the movies. In this
case, the opt-out rate for the baseline explanation (Movie Before) was much
higher than for the other two conditions (see also Table 21). For the remaining
movie ratings, both feature-based explanation types were less eﬀective than
baseline explanations. Again, personalization did not improve eﬀectiveness.
Feature-based explanations led to higher satisfaction than baseline explanations. Contrary to our earlier results, personalized explanations were not preferred to non-personalized ones (the trend is even in the opposite direction).
As discussed in 6.2.6, qualitative user comments suggest that the satisfaction
with personalized explanations was decreased because participants recognized
actors and directors less often (given that these tend to be more obscure in
short movies). So, personalization of explanations only seems to increase satisfaction if it results in information that is meaningful to a user (rather than
e.g. mentioning names of unknown actors).
Table 22 Overview of the results related to eﬀectiveness and satisfaction across all experiments.
Experiment
Eﬀectiveness
Satisfaction
Trend: non-personalized best
Signiﬁcant: personalized highest
Trend: non-personalized best
Trend: personalized highest
Signiﬁcant: non-personalized had
least opt-outs
Signiﬁcant: non-personalized
best, personalized had least
Signiﬁcant: personalized higher
than non-personalized
Trend: personalized higher than
Final Eval.
Signiﬁcant:
had most opt-outs
Signiﬁcant:
non-personalized
higher than baseline
Trend: personalized higher than
Eﬀectiveness of personalized explanations
While the results for the approximated and true eﬀectiveness experiments
are not entirely consistent, three conclusions can be drawn for all experiments:
1. Contrary to our initial hypothesis, personalization was in most cases clearly
detrimental to eﬀectiveness.
2. Users are likely to be more satisﬁed with feature-based than baseline explanations. If the personalization is perceived as relevant to them, then personalized feature-based explanations are preferred over non-personalized.
3. User satisfaction is also reﬂected in the proportion of opt-outs, which is
highest for the baseline explanations in all experiments. This was the case
despite the diﬀerent types of baselines used in the two domains.
7.3 Lessons learned for eﬀectiveness
Through our series of experiments, we have learned a few things about measuring eﬀectiveness. Firstly, this metric does not consider opt-outs. As mentioned
previously, the baseline explanations in our experiments suﬀered from a large
number of opt-outs even if the eﬀectiveness (measured as the absolute mean
of the diﬀerence between the before and after ratings) was seemingly comparable with other conditions. For an explanation to be eﬀective, it has to at
the very least elicit some sort of rating (preferably one that reﬂects the user’s
preferences). An explanation that cannot help elicit any rating, by deﬁnition
leads to poor eﬀectiveness, and moreover is likely to result in user satisfaction
so low that the system is likely to lose the user.
Secondly, the validity of the used metric (see Section 2.2) is dependent on
the underlying data set. In Section 6.3 we highlighted two weaknesses of the
used metric in relation to the underlying data set. We elaborate on them here.
7.3.1 If a large proportion of ratings fall on the middle of the scale.
Firstly, mid-scale ratings are ambiguous, we do not know if users are selecting
this option because they cannot make a decision, or because they feel neutral
(i.e. neither good, nor bad) about an item. The presence of an opt-out option
helps clarify this, but only partially, as participants may supply a neutral value
when they do not have enough information to form a polarized opinion. For
example, in the ﬁnal evaluation for the baseline Movie Before ratings we saw
that participants gave more mid-scale (“4”) ratings than in other conditions:
this was more likely due to a lack of information than a large proportion of
movies that were precisely ok.
Secondly, explanations that cause a majority of mid-scale ratings (for the
rating before trying the item, e.g. Movie Before) are likely to lead to better
eﬀectiveness than explanations that result in more evenly distributed ratings.
When the before rating is mid-scale, then even the most extreme change in
opinion can only be as big as half of the scale. A wider distribution of the before ratings, assuming that the after ratings are normally distributed around
Nava Tintarev, Judith Masthoﬀ
the middle of the scale, is likely to lead to greater divergences. Thus, smaller
diﬀerences between before and after ratings might then (at least in part) be
due to the poor distribution of the before ratings, rather than better eﬀectiveness. So, explanations that lead to more polarized opinions may be more
detrimental to the metric of eﬀectiveness. This may have contributed to the
downfall of the personalized explanations, which are likely to result in more
polarized opinions. For example, personalized explanations may have led to
users expecting to really like a movie when favorite actors were mentioned,
only to be disappointed when those actors only had a minor role.
It is also arguable that if the initial ratings (Movie Before) are random but
also follow a normal distribution (our current assumption for Movie After), the
signed average diﬀerence would be 0. This is why we highlight the importance
of measuring the absolute value of the diﬀerence.
7.3.2 If the explanations are biased in the same direction as the data.
Following on from the previous point, we can imagine an extreme scenario
where a recommender system gives explanations that result in a large proportion of neutral ratings, and only recommends “safe” items that usually score
around the middle of the scale. These explanations might appear to be eﬀective16, but are not likely to be particularly informative. This does not mean
that the explanations are generally eﬀective, as they would be misleading for
non-neutral recommendations.
In addition, in our experiments we have seen that neither the before nor after ratings were centered around the middle of the scale. In this case, it makes
more sense to consider the mean rating (e.g. 5 out of 7) for the after distribution (Movie After) rather than the middle of the scale (e.g. 4 out of 7). That
is, false eﬀectiveness may be found if there are many initial ratings (Movie
Before) around the value that is the mean of the after ratings (Movie After).
We can imagine explanations that inﬂate the initial valuation of items for a
system that only recommends the most popular items; or explanations that
devalue items and the system only presents unpopular items. In these cases our
metric for eﬀectiveness may result in high correlations, and a mean diﬀerence
of 0 between the before and after ratings. However, this does not mean that
the explanations are eﬀective. For this reason, the underlying distribution of
ratings should be presented alongside any measurement of eﬀectiveness.
We caution that none of these situations per default imply a failed metric.
The items may in fact be just ok, and a system that helps to identify this
correctly should not be classiﬁed as faulty. Likewise, “slanted” explanations
may be suitable if this ﬁts the data e.g. positive explanations for items that
the user is predicted to like. Baseline explanations like ours may make sense
16 Assume the after ratings are normally distributed around the middle of the scale. Compare before ratings that are ﬁxed at the middle of the scale, with before ratings that are ﬁxed
at another value (e.g. if all Movie Before ratings were equal to 5). The average diﬀerence
between the before and after ratings would be smaller in the former case.
Eﬀectiveness of personalized explanations
if they are based on many previous user opinions as is the case with the
Internet Movie Database (IMDB). However, it would be prudent to assume
that explanations cannot be ported between data sets, or domains without
careful consideration. Any study using the same metrics for eﬀectiveness would
need to study the underlying distribution as well. For these reason we would
encourage replication of this experiment with other materials and in diﬀerent
domains, to conﬁrm which of our ﬁndings carry beyond our small selection of
materials.
7.4 Future work
We found that while personalization could increase satisfaction in two domains, contrary to our hypothesis, it was detrimental to eﬀectiveness. It may
be the case that personalization in general does not increase eﬀectiveness. We
considered if this result is more speciﬁc to our studies, and discussed how our
choice of experimental design, and type of explanations generated may have
led to these surprising results. Of course, the outcome of the experiments may
be dependent on the particular personalization chosen and the domains used.
Further studies are needed with more complex or simply longer explanations (e.g. based on deeper user models), using a diﬀerent design (e.g. diﬀerent
materials), other domains, larger and less homogeneous participants’ samples,
and trying alternative ways of personalization. Of particular interest may be
to compare explanations based on user models built using the diﬀerent compositional preference elicitation methods described in Pommeranz et al .
Our suggestions for related future work also include using an implicitly learned
user model given that users may not always know what information they need
to make accurate decisions.
While the independence from a particular recommender system has allowed
us to run controlled experiments, it would also be interesting to conduct studies with a live recommender system. That way one could for example conduct
longitudinal studies, investigating e.g. the eﬀect of explanations on trust, and
see in which situations trust increases and decreases over time.
In addition, other researchers are starting to ﬁnd that explanations are
part of a cyclical process. The explanations aﬀect a user’s mental model of the
recommender system, and in turn the way they interact with the explanations.
In fact this may also impact the recommendation accuracy negatively . For example, Ahn et al saw that recommendation accuracy decreased as users removed keywords from their proﬁle
for a news recommender system. Understanding this cycle will likely be one
of the future strands of research.
Acknowledgements The authors would like to thank the anonymous reviewers for their
time and constructive comments which aided in improving this article. They would also
like to thank Dr. G. Prescott for invaluable advice on statistics, though they assume full
responsibility for any remaining shortcomings. The experiments reported were funded by
EPSRC platform grant EP/E011764/1.
Nava Tintarev, Judith Masthoﬀ