Event-Driven Visual-Tactile Sensing
and Learning for Robots
Tasbolat Taunyazov∗, Weicong Sng∗, Hian Hian See†, Brian Lim†‡,
Jethro Kuan∗, Abdul Fatir Ansari∗, Benjamin C.K. Tee†‡, and Harold Soh∗
∗Dept. of Computer Science, National University of Singapore
†Dept. of Materials Science and Engineering, National University of Singapore
‡Institute for Health Technology and Innovation, National University of Singapore
Email: , , , ,
 , , , 
Abstract—This work contributes an event-driven visual-tactile
perception system, comprising a novel biologically-inspired tactile
sensor and multi-modal spike-based learning. Our neuromorphic
ﬁngertip tactile sensor, NeuTouch, scales well with the number
of taxels thanks to its event-based nature. Likewise, our Visual-
Tactile Spiking Neural Network (VT-SNN) enables fast perception
when coupled with event sensors. We evaluate our visual-tactile
system (using the NeuTouch and Prophesee event camera) on
two robot tasks: container classiﬁcation and rotational slip
detection. On both tasks, we observe good accuracies relative to
standard deep learning methods. We have made our visual-tactile
datasets freely-available to encourage research on multi-modal
event-driven robot perception, which we believe is a promising
approach towards intelligent power-efﬁcient robot systems.
Index Terms—Event-Driven Perception, Multi-Modal Learning, Tactile Sensing, Spiking Neural Networks.
I. INTRODUCTION
Many everyday tasks require multiple sensory modalities
to perform successfully. For example, consider fetching a
carton of soymilk from the fridge ; humans use vision
to locate the carton and can infer from a simple grasp how
much liquid the carton contains. They can then use their sense
of sight and touch to lift the object without letting it slip.
These actions (and inferences) are performed robustly using a
power-efﬁcient neural substrate—compared to the multi-modal
deep neural networks used in current artiﬁcial systems, human
brains require far less energy , .
In this work, we take crucial steps towards efﬁcient visualtactile perception for robotic systems. We gain inspiration
from biological systems, which are asynchronous and eventdriven. In contrast to resource-hungry deep learning methods,
event-driven perception forms an alternative approach that
promises power-efﬁciency and low-latency—features that are
ideal for real-time mobile robots. However, event-driven systems remain under-developed relative to standard synchronous
perception methods , .
This paper1 makes multiple contributions that advance
event-driven visual-tactile perception. First, to enable richer
1The original paper version is published in the RSS 2020 proceedings. This
version includes the complete appendix, updated results for power utilization
on the Loihi, and new results for a larger container & weight classiﬁcation
tactile sensing, we contribute the 39-taxel NeuTouch ﬁngertip
sensor. Compared to existing commercially-available tactile
sensors, NeuTouch’s neuromorphic design enables scaling to
a larger number of taxels while retaining low latencies.
Next, we investigate multi-modal learning with NeuTouch
and the Prophesee event camera. Speciﬁcally, we develop
a visual-tactile spiking neural network (VT-SNN) that uses
both sensory modalities for supervised-learning tasks. Different from conventional deep artiﬁcial neural network (ANN)
models , SNNs process discrete spikes asynchronously and
thus, are arguably better suited to the event data generated
by our neuromorphic sensors. In addition, SNNs can be used
on efﬁcient low-power neuromorphic chips such as the Intel
Loihi . We train our VT-SNN using recent spike-based
backpropagation and introduce a weighted spike-count loss
to encourage early classiﬁcation.
Our experiments center on two robot tasks: object classiﬁcation and (rotational) slip detection. In the former, we tasked
the robot to determine the type of container being handled and
amount of liquid held within. The containers were opaque
with differing stiffness, and hence, both visual and tactile
sensing are relevant for accurate classiﬁcation. We show that
relatively small differences in weight (≈30g across 20 objectweight classes) can be distinguished by our prototype sensors
and spiking models. The slip detection experiment indicates
rotational slip can be accurately detected within 0.08s (visualtactile spikes processed every ≈1ms). In both experiments,
SNNs achieved competitive (and sometimes superior) performance relative to ANNs with similar architecture. When
tested on the Intel Loihi, the SNN achieved inference speeds
similar to a GPU in a real-time simulation, but consumed
approximately 1900 times less power.
Taking a broader perspective, event-driven perception represents an exciting opportunity to enable power-efﬁcient intelligent robots. This work suggests that an “end-to-end” eventdriven perception framework is promising and warrants further
research. We have made the data from our two experiments
(and a third dataset that expands the number of grasped items
to 36 different objects using a similar protocol) freely-available
 
to the research community2. To our knowledge, these represent
the ﬁrst publicly-available event-based visual-tactile datasets,
and we hope their availability will spur research on eventdriven robotics. To summarize, our primary contributions are:
• NeuTouch, a scalable event-based tactile sensor for robot
end-effectors;
• A Visual-Tactile Spiking Neural Network that leverages
multiple event sensor modalities and trained using a new
temporally-weighted spike-count loss;
• Systematic experiments demonstrating the effectiveness
of our event-driven perception system on object classi-
ﬁcation and slip detection, with comparisons to conventional ANN methods;
• Visual-tactile event sensor datasets, which also includes
RGB images and proprioceptive data from the robot.
II. BACKGROUND AND RELATED WORK
In the following, we give a brief overview of related work on
visual-tactile perception for robotics, and event-driven sensing
and learning. Both areas are broad and thus, we focus on the
core concepts and provide links to articles that cover these
topics in greater detail.
A. Visual-Tactile Perception for Robots
General recognition of the importance of multi-modal sensing for robotics has led to innovations in both sensing and
perception methods. Of late, there has been a ﬂurry of papers
on combining vision and touch sensing, e.g., – . However, work on visual-tactile learning of objects dates back to
(at least) 1984 when vision and tactile data was used to create
a surface description of primitive objects ; in this early
work, tactile sensing played a supportive role for vision due
to the low resolution of tactile sensors at the time.
Recent advancements in tactile technology , have
encouraged the use of tactile sensing for more complex tasks,
including object exploration and classiﬁcation – ,
shape completion , and slip detection , . One
popular sensor is the BioTac; similar to a human ﬁnger, it
uses textured skin, allowing vibration signatures to be used
for high accuracy material and object identiﬁcation and slip
detection . The BioTac has also been used in visualtactile learning, e.g., combined tactile data with RGB
images to recognize objects via deep learning. Other recent
works have used the Gelsight —an optical-based tactile
sensor—for visual-tactile slip detection , , grasp stability, and texture recognition . Very recent work has
used unsupervised learning to generate neural representations
of visual-tactile data (with proprioception) for reinforcement
learning . Compared to the prior work above, we do not
leverage synchronous sensors or conventional deep learning
methods. Rather, our sensor and learning method are both
event-driven.
2Available at 
B. Event-based Perception: Sensors and Learning
Work on event-based perception has focused primarily on
vision (see for a comprehensive survey). This emphasis
on vision can be attributed both to its applicability across many
tasks, as well as the recent availability of event cameras such as
the DVS and Prophesee Onboard. Unlike conventional optical
sensors, event cameras capture pixel changes asynchronously.
There are very few event-based tactile sensors; recent variants
used in robotics are optical-based sensors that are constructed
by attaching event-based cameras to an elastic material ,
 . The camera captures deformations of the elastic material
when it physically interacts with an object.
Event-based sensors have been successfully used in conjunction with deep learning techniques . The binary events
are ﬁrst converted into real-valued tensors, which are processed downstream by deep ANNs. This approach generally
yields good models (e.g., for motion segmentation , optical
ﬂow estimation , and car steering prediction ), but at
high compute cost.
Neuromorphic learning, speciﬁcally Spiking Neural Networks (SNNs) , , provide a competing approach for
learning with event data. Similar to event-based sensors, SNNs
work directly with discrete spikes and hence, possess similar
characteristics, i.e., low latency, high temporal resolution and
low power consumption. Historically, SNNs have been hampered by the lack of a good training procedure. Gradient-based
methods such as backpropagation were not available because
spikes are non-differentiable. Recent developments in effective
SNN training , , and the nascent availability of
neuromorphic hardware (e.g., IBM TrueNorth and Intel
Loihi ) have renewed interest in neuromorphic learning
for various applications, including robotics. SNNs do not yet
consistently outperform their deep ANN cousins on pseudoevent image datasets, and the research community is actively
exploring better training methods for real event-data.
In this work, we develop a visual-tactile SNN trained using
SLAYER , a recent (approximate) backpropagation method
for SNNs. There has been relatively little work on multi-modal
SNNs and prior work has focused on audio-video data ,
 (e.g., for emotion detection ). Very recent work 
has investigated turn-taking in human-robot collaboration using a range of non-neuromorphic sensors (Kinect video, EEG
etc.). In contrast, we develop a multi-modal SNN for visual
and tactile event data; note that the latter is an “active” sense
with different characteristics from audio.
III. NEUTOUCH: AN EVENT-BASED TACTILE SENSOR
Although there are numerous applications for tactile sensors (e.g., minimal invasive surgery and smart prosthetics ), tactile sensing technology lags behind vision. In
particular, current tactile sensors remain difﬁcult to scale and
integrate with robot platforms. The reasons are twofold: ﬁrst,
many tactile sensors are interfaced via time-divisional multiple
access (TDMA), where individual taxels are periodically and
sequentially sampled. The serial readout nature of TDMA
inherently leads to an increase of readout latency as the
3D-printed Part
Transducer
Fig. 1. (a) NeuTouch compared to a human ﬁnger. (b) Cross-sectional view
of NeuTouch and constituent components. NeuTouch performs tactile sensing
using an electrode layer with 39 taxels and a graphene-based piezoresistive
thin ﬁlm that is embedded beneath the protective Ecoﬂex “skin”. (c) Spatial
distribution of the 39 taxels on NeuTouch. (d) Pressure response curve of the
graphene-based pressure transducer.
number of taxels in the sensor is increased. Second, high
spatial localization accuracy is typically achieved by adding
more taxels in the sensor; this invariably leads to more wiring,
which complicates integration of the skin onto robot endeffectors and surfaces.
Motivated by the limitations of the existing tactile sensing technology, we developed a novel Neuro-inspired Tactile
sensor (NeuTouch) for use on robot end-effectors (Fig. 1).
The structure of NeuTouch is akin to a human ﬁngertip: it
comprises “skin”, and “bone”, and has a physical dimension
of 37 × 21 × 13 mm. This design facilitates integration with
anthropomorphic end-effectors (for prosthetics or humanoid
robots) and standard multi-ﬁnger grippers; in our experiments,
we use NeuTouch with a Robotiq 2F-140 gripper. We focused
on a ﬁngertip design in this paper, but alternative structures
can be developed to suit different applications.
Different from earlier work , tactile sensing is achieved
via a layer of electrodes with 39 taxels and a graphene-based
piezoresistive thin ﬁlm. The taxels are elliptically-shaped to
resemble the human ﬁngertips fast-adapting (FA) mechanoreceptors , and are radially-arranged with density varied
from high to low, from the center to the periphery of the sensor.
During typical grasps, NeuTouch (with its convex surface)
tends to make initial contact with objects at its central region
where the taxel density is the highest. Correspondingly, rich
tactile data can be captured in the earlier phase of tactile
sensing, which may help accelerate inference (e.g., for early
classiﬁcation). The graphene-based pressure transducer forms
an effective tactile sensor , due to its high Youngs
modulus, which helps to reduce the transducers hysteresis and
response time. Figure 1d shows the pressure response of the
transducer, and a low hysteresis can be observed from the
loading and unloading curves.
A 3D-printed component serves the role of the ﬁngertip
bone, and Ecoﬂex 00-30 emulates skin for NeuTouch. The
Ecoﬂex offers protection for the electrodes for a longer uselife and ampliﬁes the stimuli exerted on NeuTouch. The latter
enables more tactile features to be collected, since the transient
phase of contact encodes much of the physical description of
a grasped object, such as stiffness or surface roughness .
The NeuTouch exhibits a slight delay of ≈300ms when
recovering from a deformation due to the soft nature of
Ecoﬂex. Nevertheless, our experiments show this effect did not
impede the NeuTouch’s sensitivity to various tactile stimuli.
Compared to available tactile sensors, NeuTouch is eventbased and scales well with the number of taxels; NeuTouch
can accommodate 240 taxels while maintaining an exceptionally low constant readout latency of 1ms for rapid tactile
perception . We achieve this by leveraging upon the
Asynchronously Coded Electronic Skin (ACES) platform 
— an event-based neuro-mimetic architecture that enables
asynchronous transmission of tactile information. With ACES,
the taxels of NeuTouch mimic the function of the FA mechanoreceptors of a human ﬁngertip, which capture dynamic pressure (i.e., dynamic skin deformations) . FA responses are
crucial for dexterous manipulation tasks that require rapid detection of object slippage, object hardness, and local curvature.
Crucially, transmission of the stimuli information is in the
form of asynchronous spikes (i.e., electrical pulses), similar
to biological systems; data is transmitted by individual taxels
only when necessary. This is made possible by encoding the
taxels of NeuTouch with unique electrical pulse signatures.
These signatures are robust to overlap and permit multiple
taxels to transmit data without speciﬁc time synchronization.
Therefore, stimuli information of all the activated taxels can
be combined and propagated upstream to the decoder via a
single electrical conductor. This yields lower readout latency
and simpler wiring. The decoder correlates the received pulses
(i.e., the combined pulse signatures) against each taxels known
signature to retrieve the spatio-temporal tactile information. In
this work, this decoding is performed in real-time on an FPGA,
rather than ofﬂine .
IV. VISUAL-TACTILE SPIKING NEURAL NETWORK
As motivated in the introduction, the successful completion
of many tasks is contingent upon using multiple sensory
modalities. In this work, we focus on touch and sight, i.e., we
fuse tactile and visual data from NeuTouch and an event-based
camera via a spiking neural model. This Visual-Tactile Spiking
Neural Network (VT-SNN) enables learning and perception
using both these modalities, and can be easily extended to
incorporate other event sensors.
Model Architecture. From a bird’s-eye perspective, the VT-
SNN employs a simple architecture (Fig. 2) that ﬁrst encodes
the two modalities into individual latent (spiking) representations, that are combined and further processed through
additional layers to yield a task-speciﬁc output.
We now detail the precise network structures used in our
experiments, but VT-SNN may use alternative network struc-
Combination
Prophesee Spikes
NeuTouch Spikes
Output Spikes
Input Spikes
Refractory
Fig. 2. The Visual-Tactile Spiking Neural Network (VT-SNN) which comprises two “spiking encoders” for each modality. The spikes from these two
encoders are combined via a ﬁxed-width combination layer and propagated to
a task SNN that outputs a task-speciﬁc output spike-train. VT-SNN employs
the Spike Response Model (SRM) neuron that integrates incoming spikes and
spikes when a threshold is breached.
tures for the Tactile, Vision and Task SNNs. The Tactile SNN
employs a fully connected (FC) network consisting of 2 dense
spiking layers3. It has an input size of 156 (two ﬁngers, each
with the 39 taxels with a positive and negative polarity channel
per taxel) and a hidden layer size of 32. The Vision SNN
uses 3 layers; the ﬁrst layer is a pooling layer with kernel
size and stride length of 4. The pooled spike train is passed as
input to a 2-layer FC architecture identical to the Tactile SNN.
The tactile and vision encoders have output sizes of 50 and
10, respectively4. The encoded spike-trains of both modalities
are concatenated, and passed into a dense spiking layer that
generates output spikes. Note that the output dimensionality is
dependent on the task: 20 for container & weight classiﬁcation,
and 2 for rotational slip classiﬁcation. The model architectures
are agnostic to the size of the input time dimension.
Neuron Model. We use the Spike Response Model (SRM) ,
 . In the SRM, spikes are generated whenever a neuron’s
internal state (“membrane potential”) u(t) exceeds a prede-
ﬁned threshold ϕ. Each neuron’s internal state is affected by
incoming spikes and a refractory response:
wi(ϵ ∗si)(t) + (ν ∗o)(t)
where wi is a synaptic weight, ∗indicates convolution, si(t)
are the incoming spikes from input i, ϵ(·) is the response kernel, ν(·) is the refractory kernel, and o(t) is the neuron’s output
spike train. In words, incoming spikes si(t) are convolved with
a response kernel ϵ(·) to yield a spike response signal that is
scaled by a synaptic weight wi.
Model Training. We optimized our spiking networks using
SLAYER . As mentioned in Sec. II, the derivative of a
3In preliminary experiments, we also tested spiking convolutional layers
but it resulted in poorer performance.
4Several different dimension sizes were tested and a 50-10 encoding gave
the best results.
spike is undeﬁned, which prohibits a direct application of
backpropagation to SNNs. SLAYER overcomes this problem
by using a stochastic spiking neuron approximation to derive
an approximate gradient, and a temporal credit assignment
policy to distribute errors. SLAYER trains models “ofﬂine”
on GPU hardware. Hence, the spiking data needs to be binned
into ﬁxed-width intervals during the training process, but the
resultant SNN model can be run on neuromorphic hardware.
We used a straight-forward binning process where the (binary)
value for each bin window Vw was 1 whenever the total spike
count in that window P
w S exceeded a threshold value Smin:
otherwise.
Similar to prior work , class prediction is determined
by the number of spikes in the output layer spike train; each
output neuron is associated with a speciﬁc class and the neuron
that generates the most spikes represents the winning class.
The spike-count loss is given by:
which captures the difference between the observed output
spike count PT
t=0 s(t) and the desired spike count PT
across the output neurons (indexed by n).
We introduce a generalization of the spike-count loss above
to incorporate temporal weighting:
ω(t)sn(t) −
ω(t)˜sn(t)
We refer to Lω as the weighted spike-count loss. In our
experiments, we set ω(t) to be monotonically decreasing,
which encourages early classiﬁcation by down-weighting later
spikes. Speciﬁcally, we used a simple quadratic function,
ω(t) = βt2 + γ with β < 0, but other forms may be used.
For both L and Lω, appropriate counts have to be speciﬁed
for the correct and incorrect classes and are task-speciﬁc
hyperparameters. We tuned them manually and found that
setting the positive class count to ≈50% of the maximum
number of spikes (across each input within the considered
time interval) worked well. In initial trials, we observed that
training solely with the losses above led to rapid over-ﬁtting
and poor performance on a validation set. We explored several
techniques to mitigate this issue (e.g., ℓ1 regularization and
dropout), but found that simple ℓ2 regularization led to the
best results.
V. ROBOT AND SENSORS SETUP
In this section, we describe the robot hardware setup used
across our experiments (Fig. 3). We used a 7-DoF Franka
Emika Panda arm with a Robotiq 2F-140 gripper and collected
data from four primary sensors types: NeuTouch, Prophesee
Onboard, RGB cameras, and the Optitrack motion capture
system. The latter two are non-event sensors and their data
streams were not used in VT-SNN.
Robotiq 2F-140
ACES Encoder
RGB Camera
Fig. 3. Robot Experiment Setup. (Left) Close-up of the Franka Emika Panda
arm and sensors; a NeuTouch sensor was attached on each Robotiq gripper
ﬁnger. The Prophesee event and Realsense cameras were mounted on the arm
and pointed towards the center of the gripper’s grasp area. Our prototype
ACES encoder was mounted on top of the arm’s control panel. (Right) A
view of the object classiﬁcation experiment showing three OptiTrack cameras
(11 were used, and out of scene), the RGB scene camera, and the object (soy
milk carton) to be grasped and lifted.
NeuTouch Tactile Sensor. We mounted two NeuTouch sensors to the Robotiq 2F-140 gripper and the ACES encoder
on the Panda arm (Fig. 3, left). To ensure consistent data,
we performed a sensor warm-up before each data collection
session and obtained baseline results to check for sensor drift.
Speciﬁcally, we repeated 100 cycles of: closing the gripper
onto a ﬂat stiff object (the ‘9 hole peg test’ from the YCB
dataset ) for 3 seconds, opening the gripper, and pausing
for 2 seconds. We then collected a set of benchmark data,
i.e., 20 repetitions of closing the gripper onto the same ‘9
hole peg test’ for 3 seconds. Throughout our experiments, we
periodically tested for sensor drift by repeating the closing test
on the ‘9 hole peg test’ and then examining the sensor data;
no signiﬁcant drift was found throughout our experiments.
Prophesee Event Camera. Event-based vision data was captured using the Prophesee Onboard. Similar to the tactile
sensor, each camera pixel ﬁres asynchronously and a positive (negative) spike is obtained when there is an increase
(decrease) in luminosity. The Onboard was mounted on the
arm and pointed towards the gripper to obtain information
about the object of interest (Fig. 3). Although the camera has
a maximum resolution of 640 x 480, we captured spikes from
a cropped 200 x 250 rectangular window to minimize noise
from irrelevant regions. The event camera bias parameters
were tuned following recommended guidelines5 and we use
the same parameters throughout all experiments. During preliminary experiments, we found the Onboard to be sensitive to
high frequency (≥100Hz) luminosity changes; in other words,
ﬂickering light bulbs triggered undesirable spikes. To counter
this effect, we used six Philips 12W LED White light bulbs
mounted around the experiment setup to provide consistent
non-ﬂickering illumination.
RGB Cameras. We used two Intel RealSense D435s to
5See 
provide additional non-event image data6. The ﬁrst camera was
mounted on the end-effector with the camera pointed towards
the gripper (providing a view of the grasped object), and the
second camera was placed to provide a view of the scene.
The RGB images were used for visualization and validation
purposes, but not as input to our models; future work may
look into the integration of these standard sensors to provide
even better model performance.
OptiTrack. The OptiTrack motion capture system was used to
collect object movement data for the slip detection experiment.
We attached 6 reﬂective markers on the rigid parts of the
end-effector and 14 markers on the object of interest. Eleven
OptiTrack Prime 13 cameras were placed strategically around
the experimental area to minimize tracking error (Fig. 3,
right); each marker was visible to most if not all cameras
at any instance, which resulted in continuous and reliable
tracking. We used Motive Body v1.10.0 for marker tracking
and manually annotated the detected markers. Initial tests
indicated the OptiTrack gave reliable position estimates with
error ≤1mm at 120Hz.
Further Details. In addition to the above sensors, we also
collected proprioceptive data for the Panda arm and Robotiq
gripper; these are not currently used in our models but can
be included in future work. Additional information (including
speciﬁc parameter settings, 3D-printed attachments, and multinode data collection) is available in the online supplementary
material7.
VI. CONTAINER & WEIGHT CLASSIFICATION
Our ﬁrst experiment applies our event-driven perception
framework—comprising NeuTouch, the Onboard camera, and
the VT-SNN—to classify containers with varying amounts of
liquid. Our primary goal was to determine (i) if our multimodal system was effective at detecting differences in objects
that were difﬁcult to isolate using a single sensor, and (ii)
whether the weight spike-count loss resulted in better early
classiﬁcation performance. Note that our objective was not
to derive the best possible classiﬁer; indeed, we did not
include proprioceptive data which would likely have improved
results , nor conduct an exhaustive (and computationally
expensive) search for the best architecture. Rather, we sought
to understand the potential beneﬁts of using both visual and
tactile spiking data in a reasonable setup.
A. Methods and Procedure
Objects. We used four different containers: an aluminium
coffee can, a plastic Pepsi bottle, a cardboard soy milk carton
and a metal tuna can (Fig. 5). These objects have different
degrees of hardness; the soy milk container was the softest, and
the tuna can was the most rigid. Because of size differences,
each container was ﬁlled with differing amounts of liquid;
the four objects contained a maximum of 250g, 400g, 300g,
6The infrared emitters were disabled as they increased noise for the event
camera and hence, no depth data was recorded.
7Available at 
Visual Spike
Visual Spike
Visual Spike
Fig. 4. Data collection procedure for the container & weight classiﬁcation task. The robot end-effector starts at 10 cm above the grasp point. It then approaches
the coffee can, stopping at the 2-second mark. The gripper then starts to close, grasping it at around the 4-second mark, resulting in tactile spikes. At the
6-second mark, the robot lifts it by 5cm above the table. The robot then holds it in the same position till the 8.5-second mark. For container and weight
classiﬁcation, we only use data starting from the 2-second mark.
(Left) Containers used for container classiﬁcation task: coffee can,
plastic soda bottle, soy milk carton, and metal tuna can. (Right) The objects
in our expanded dataset (36 object classes with various visual and tactile
proﬁles) collected using the same protocol.
and 140g, respectively8. For each object, we collected data
for {0%, 25%, 50%, 75%, 100%} of the respective maximum
amount. This resulted in 20 object classes comprising the four
containers with ﬁve different weight levels each9.
Robot Motion. The robot would grasp and lift each object
class ﬁfteen times, yielding 15 samples per class. Trajectories
for each part of the motion was computed using the MoveIt
Cartesian Pose Controller . Brieﬂy, the robot gripper was
initialized 10cm above each object’s designated grasp point.
The end-effector was then moved to the grasp position (2
seconds) and the gripper was closed using the Robotiq grasp
controller (4 seconds). The gripper then lifted the object by
5cm (2 seconds) and held it for 0.5 seconds.
Data Pre-processing. For both modalities, we selected data
from the grasping, lifting and holding phases (corresponding
to the 2.0s to 8.5s window in Figure 4), and set a bin duration
of 0.02s (325 bins) and a binning threshold value Smin = 1. We
used stratiﬁed K-folds to create 5 splits; each split contained
240 training and 60 test examples with equal class distribution.
8The can did not have a cover and we ﬁlled it with a packet of rice to
avoid spills and possible liquid damage. The tuna can was placed with the
open side facing downwards so, the rice was not visible.
9We have an updated dataset (Version 2.0) which contains 40 samples for
each class. Please see the Appendix for results and the online website for
download links.
CONTAINER & WEIGHT CLASSIFICATION (ENTIRE INPUT SEQUENCE):
AVERAGE ACCURACY WITH STANDARD DEVIATION IN BRACKETS
0.71 (0.045)
0.73 (0.064)
0.81 (0.039)
0.71 (0.023)
0.72 (0.065)
0.80 (0.048)
ANN (MLP-GRU)
0.50 (0.059)
0.43 (0.054)
0.44 (0.062)
ANN (CNN-3D)
0.75 (0.061)
0.68 (0.022)
0.80 (0.041)
Classiﬁcation Models. We compared the SNNs against conventional deep learning, speciﬁcally Multi-layer Perceptrons
(MLPs) with Gated Recurrent Units (GRUs) and 3D
convolutional neural networks (CNN-3D) . We trained
each model using (i) the tactile data only, (ii) the visual
data only, and (iii) the combined visual-tactile data. Note
that the SNN model on the combined data corresponds to
the VT-SNN. When training on a single modality, we use
Visual or Tactile SNN as appropriate. We implemented all
the models using PyTorch. The SNNs were trained with
SLAYER to minimize the (weighted) spike-count loss, and
the ANNs were trained to minimize the cross-entropy loss
using RMSProp. All models were trained for 500 epochs.
Source code implementing our models is available online at
 
B. Results and Analysis
Model Comparisons. The test accuracies of the models (using
the entire input sequence) are summarized in Table I. The
multi-modal SNN model achieves the highest score of 81%,
an improvement of ≈10% compared to the single-modality
variants. Using either modality alone results in comparable
performance. We were initially surprised by the accuracy
attained by the vision-only model; we expected performance
≈20% since since the containers were easily distinguishable
by sight, but the weight category was not. However, a closer
examination of the data showed that (i) the Pepsi bottle
was not fully opaque and the water level was observable by
Pepsi Bottle
Coffee Can
Pepsi Bottle
Coffee Can
Pepsi Bottle
Coffee Can
Output spikes (blue lines) for the models trained with different
modalities with correct and incorrect predictions in green and red, respectively,
while grasping a coffee can with 100% weight. The weight categories are
arranged from 0% to 100% (bottom to top) for each container. The tactile
model is unable to distinguish between a coffee can and a tuna can while
the vision model is uncertain about the weight. The combined visual-tactile
model predicts the correct class with high certainty.
Onboard on some trials, and (ii) the Onboard was able to see
object deformations as the gripper closed, which revealed the
“fullness” of the softer containers.
Figure 6 shows an instructive example showing the advantage of fusing both modalities. It shows the output spikes from
the different SNN models for a coffee can with 100% weight.
The models trained on tactile and vision data are uncertain of
the container and the weight category, respectively. We see the
tactile model is unable to discern between tuna can and coffee
can. On the other hand, the vision model correctly predicts
the container but is unsure about the weight category. The
combined visual-tactile model incorporates information from
both the modalities and is able to predict the correct class
(both container and weight categories) with high certainty.
The SNN models performed far better than the MLP-
GRU model, and similar to the 3D CNNs. We had expected
comparable performance since MLP-GRU models are known
to perform well on a variety of tasks. The poor performance
was possibly due to the relatively long sample durations (325
time-steps) and the large number of parameters in the ANN
models, relative to the size of our dataset.
Early Classiﬁcation. Instead of waiting for all the output
spikes to accumulate, we can perform early classiﬁcation based
on the number of spikes seen up to time t. Fig. 7 shows
the accuracies of the different models over time. Between
0.5 −3.0s, the vision model was already able to distinguish
between certain objects. We posit this was due to small
movements (of the mounted camera) as the gripper closed,
which resulted in changes perceived by the Onboard. As
expected, output tactile spikes do not emerge until after contact
is made with the object at ≈2s.
Although the two losses L and Lω have similar “ﬁnal”
Test Accuracy
Spike Count
Weighted Spike Count
Fig. 7. Container and weight classiﬁcation accuracy over time. Lines show
average test accuracy and shaded regions represent the standard deviations.
Vision-only classiﬁcation results in higher early accuracy as visual spikes
are obtained as the gripper is closing, and tactile events arise only upon
contact with the object. Combining both vision and tactile event data results
in signiﬁcantly higher accuracy, compared to using each modality separately.
Using the weighted spike count loss results in better early classiﬁcation.
Fig. 8. (Left) Object for the Slip Classiﬁcation Task with attached OptiTrack
markers. (Right) Object during a stable grasp (top) and unstable grasp with
rotational slip (bottom); the bottom object has additional mass transferred to
the left leg (from the right leg) that causes it to rotate during lifting. Note
that both objects had equal mass.
accuracies in Table I, we see that Lω has a signiﬁcant impact
on test accuracies over time. This effect is most clearly seen
for the combined visual-tactile model; the Lω variant has a
similar early accuracy proﬁle as vision, but achieves better
performance as tactile information is accumulated.
VII. ROTATIONAL SLIP CLASSIFICATION
In this second experiment, we task our perception system to
classify rotational slip, which is important for stable grasping;
stable grasp points can be incorrectly predicted for objects
with center-of-mass that are not easily determined by sight,
e.g., a hammer and other irregularly-shaped items. Accurate
detection of rotational slip will allow the controller to re-grasp
the object and remedy poor initial grasp locations. However,
to be effective, slip detection needs to be performed accurately
and rapidly.
A. Method and Procedure
Objects. Our test object was constructed using Lego Duplo
blocks (Fig. 8) with a hidden 10g mass in each leg. The
“control” object was designed to be balanced at the grasp
point. To induce rotational slip, we modiﬁed the object by
transferring the hidden mass from the right leg to the left. As
SLIP DETECTION ACCURACY (ENTIRE INPUT SEQUENCE): AVERAGE
ACCURACY WITH STANDARD DEVIATION IN BRACKETS
0.82 (0.045)
1.00 (0.000)
1.00 (0.000)
0.91 (0.020)
1.00 (0.000)
1.00 (0.000)
ANN (MLP-GRU)
0.87 (0.059)
1.00 (0.000)
1.00 (0.000)
ANN (CNN-3D)
0.44 (0.086)
0.55 (0.100)
0.77 (0.117)
such, the stable and unstable objects were visually identical
and had the same overall weight.
Robot Motion. The robot would grasp and lift both object
variants 50 times, yielding 50 samples per class. Similar to
the previous experiment, motion trajectories were computed
using MoveIt . The robot was instructed to close upon
the object, lift by 10cm off the table (in 0.75 seconds) and
hold it for an additional 4.25 seconds. We tuned the gripper’s
grasping force to enable the object to be lifted, yet allow for
rotational slip for the off-center object (Fig. 8, right).
Data Preprocessing. Instead of training our models across the
entire movement period, we extracted a short time period in the
lifting stage. The exact start time was obtained by analyzing
the OptiTrack data; speciﬁcally, we obtained the baseline
orientation distribution (for 1 second) and deﬁned rotational
slip as a deviation larger than 98% of the baseline frames. We
found that slip occurred almost immediately during the lifting.
Since we were interested in rapid detection, we extracted a
0.15s window around the start of the lift, and set a bin duration
of 0.001s (150 bins) with binning threshold Smin = 1. We used
stratiﬁed K-folds to obtain 5 splits, where each split contained
80 training samples and 20 testing samples.
Classiﬁcation Models. The model setup and optimization
procedure were identical to the previous task, with slight
modiﬁcations. The output size was reduced to 2 given the
binary labels and the sequence length for the MLP-GRUs was
set to 150. Finally, the desired true and false spike counts were
set to 80 and 5, respectively.
B. Results and Analysis
Model Comparisons. Test accuracy scores are shown in Table
II. For both the SNN and MLP-GRU, the vision and multimodal models achieve perfect accuracy using the entire input
sequence; this was unsurprising as rotational slip produced
a visually distinctive signature. Using only tactile events,
the SNN and MLP-GRU achieved 91% (with Lω) and 87%
accuracy, respectively. The CNN-3D performed poorly for this
task, possibly due to overﬁtting.
Early Slip Detection. Fig. 9 summarizes slip test accuracy
at different time points. The object starts being lifted at
approximately 0.01s, and we see that by 0.1s, the multimodal VT-SNN is able to classify slip perfectly. Again,
we see vision and touch possess different accuracy proﬁles;
tactile-only classiﬁcation is more accurate early on (between
0.01−0.05s), while vision-based classiﬁcation is better after ≈
Test Accuracy
Spike Count
Weighted Spike Count
Fig. 9. Slip classiﬁcation accuracy over time. Lines show average test accuracy and shaded regions represent the standard deviations. Classiﬁcation using
tactile only data results in higher early accuracy, but vision-only classiﬁcation
becomes more accurate as sensory data is accumulated. Combining both
modalities results in higher accuracy, and the models trained with weighted
spike count loss achieves better early classiﬁcation.
LATENCY AND POWER UTILIZATION (REAL-WORLD SIMULATION)
Latency (µs)
Total Power (mW)
0.6s. Fusing both modalities (under L) results in an accuracy
proﬁle similar to vision but shifted towards higher accuracies.
As before, early classiﬁcation performance is signiﬁcantly
improved when using the weighted spike-count loss Lw.
VIII. DISCUSSION: SPEED AND POWER EFFICIENCY
We ran two benchmarking experiments comparing the VT-
SNN on a Nvidia RTX 2080Ti and the neuromorphic Intel
Loihi chip: (1) ofﬂine batch processing and (2) a simulated
real-world setting. In the interest of brevity, we discuss the
experimental setup and summarize the results for the realworld simulation setting here, and detail the procedure and
full benchmark results in the appendix.
The models were tasked to perform forward passes on 1000
samples. Each sample has a length of 0.15s, which were
binned every 1ms forming 150 time steps. To simulate the
real-world setting, data is presented to the hardware in the
different ways. For the Loihi, the model was artiﬁcially slowed
down by the primary x86 core to match the 1ms timestep
of the slip data. For the GPU, an artiﬁcial delay of 0.15s
is introduced during the dataset fetch, to simulate the GPU
waiting the length of the full window before being able to
perform the inference.
The benchmark results are shown in Table III, where latency
is the time taken to process 1 timestep. We observe that the
latency on the Loihi is slightly lower, because it is able to
perform the inference as the spiking data arrives. The power
consumption on the Loihi is signiﬁcantly lower than
on the GPU.
IX. CONCLUSION
In this work, we propose an event-based perception framework that combines vision and touch to achieve better performance on two robot tasks. In contrast to conventional synchronous systems, our event-driven framework asynchronously
processes discrete events. Our results suggest that the eventdriven paradigm is a promising line of enquiry; we believe
event-based sensing and learning will form essential parts of
next-generation real-time autonomous robots that are powerefﬁcient. We hope that the results in this paper and our datasets
will encourage research in this area.
ACKNOWLEDGEMENTS
This work was supported by the SERC, A*STAR, Singapore, through the National Robotics Program under Grant
No. 172 25 00063, a NUS Startup Grant 2017-01, and the
Singapore National Research Foundation (NRF) Fellowship
NRFF2017-08. Thank you to Intel for access to the Neuromorphic Research Cloud, and Garrick Orchard in particular
for providing valuable guidance in performing the latency and
power utilization benchmarks.