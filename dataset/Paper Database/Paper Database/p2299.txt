Information Filtering via Self-Consistent Reﬁnement
Jie Ren1,2, Tao Zhou1,3,4,∗and Yi-Cheng Zhang1,4
1 Department of Physics, University of Fribourg, Chemin du Muse 3, 1700 Fribourg Switzerland
2 Department of Physics and Centre for Computational Science and Engineering,
National University of Singapore, Singapore 117542, Republic of Singapore
3 Department of Modern Physics, University of Science and Technology of China, Hefei 230026, PR China
4 Information Economy and Internet Research Laboratory,
University of Electronic Science and Technology of China, Chengdu 610054, PR China
Recommender systems are signiﬁcant to help people deal with the world of information explosion
and overload.
In this Letter, we develop a general framework named self-consistent reﬁnement
and implement it be embedding two representative recommendation algorithms: similarity-based
and spectrum-based methods. Numerical simulations on a benchmark data set demonstrate that
the present method converges fast and can provide quite better performance than the standard
PACS numbers: 89.75.-K, 89.20.Hh, 89.65.Gh
Introduction.—The last few years have witnessed an
explosion of information that the Internet and World
Wide Web have brought us into a world of endless possibilities: people may choose from thousands of movies,
millions of books, and billions of web pages. The amount
of information is increasing more quickly than our processing ability, thus evaluating all these alternatives and
then making choice becomes infeasible.
As a consequence, an urgent problem is how to automatically extract the hidden information and do a personal recommendation. For example, Amazon.com uses one’s purchase record to recommend books , and Adaptive-
Info.com uses one’s reading history to recommend news
 . Motivated by the signiﬁcance in economy and society,
the design of an eﬃcient recommendation algorithm becomes a joint focus from engineering science to marketing practice , from mathematical analysis 
to physics community .
A recommender system, consisted of N users and M
items, can be fully described by an N × M rating matrix
R, with Riα ̸= 0 the rating user i gives to item α. If i
has not yet evaluated α, Riα is set as zero. The aim of a
recommender system, or of a recommendation algorithm,
is to predict ratings for the items have not been voted.
To evaluate the algorithmic accuracy, the given data set
is usually divided into two parts: one is the training set,
and the other one is the testing set.
Only the information contained in the training set can be used in the
prediction. Denoting the predicted rating matrix as ˜R,
the most commonly used measurement for the algorithmic accuracy, namely the mean average error (MAE), is
deﬁned as:
| ˜Riα −R∗
where the subscript (i, α) runs over all the elements cor-
∗Electronic address: 
responding to the non-zero ratings in testing set, R∗denotes the rating matrix for testing set, and S is the number of non-zero ratings in R∗.
Thus far, the most accurate algorithms are contentbased . However, those methods are practical only
if the items have well-deﬁned attributes, and those attributes can be extracted automatically.
Besides the
content-based algorithms, the recommendation methods
can be classiﬁed into two main categories: similaritybased and spectrum-based . In this Letter, we propose a generic framework of self-consistent re-
ﬁnement (SCR) for the personal recommendation, which
is implemented by embedding the similarity-based and
spectrum-based methods, respectively. Numerical simulations on a benchmark data set demonstrate the significant improvement of algorithmic performance via SCR
compared with the standard methods.
Generic framework of SCR.—The similarity-based and
spectrum-based algorithms, including their extensions,
can be expressed in a generic matrix formula
˜R = D(R),
where R is the rating matrix obtained from the training set, ˜R the predicted rating matrix, and D a matrix
This operator, D, may be extremely simple
as a left-multiplying matrix used in the basic similaritybased method, or very complicated, usually involving a
latent optimization process, like the case of rank-k singular value decomposition (see below for details). Most
previous works concentrated on the design of the operator D. In contrast, we propose a completely new scenario
where Eq. (2) is replaced by a SCR via iterations. Denoting the initial conﬁguration R(0) = R, and the initial
time step k = 0, a generic framework of SCR reads:
(i) Implement the operation D(R(k));
(ii) Set the elements of R(k+1) as
D(R(k))iα, Riα = 0,
Then, set k = k + 1.
(iii) Repeat (i)(ii) until the diﬀerence between R(k) and
R(k−1) (or, more practical, the diﬀerence |MAE(k) −
MAE(k−1)|) is smaller than a given terminate threshold.
Consider the matrix series R(0), R(1), · · · , R(T ) (T denotes the last time step) as a certain dynamics driven
by the operator D, all the elements corresponding to the
voted items (i.e. Riα ̸= 0) can be treated as the boundary
conditions giving expression to the known information.
If ˜R is an ideal prediction, consider itself as the known
rating matrix, is should satisfy the self-consistent condition ˜R = D( ˜R). However, this equation is not hold for
the standard methods. Correspondingly, the convergent
matrix R(T ) is self-consistent. Though the simplicity of
SCR, it leads to a great improvement compared with the
traditional case shown in Eq. (2).
Similarity-based SCR.—The basic idea behind the
similarity-based method is that: a user who likes a item
will also like other similar items . Taking into account
the diﬀerent evaluation scales of diﬀerent users ,
we subtract the corresponding user average from each
evaluated entry in the matrix R and get a new matrix
R′. The similarity between items α and β is given by:
where ⟨R⟩i is the average evaluation of user i and R′
Riα −⟨R⟩i. U denotes the set of users who evaluated
both items α and β. Ωαβ →1 means the items α and
β are very similar, while Ωαβ →−1 means the opposite
In the most widely applied similarity-based algorithm,
namely collaborative ﬁltering , the predicted rating is calculated by using a weighted average, as:
β Ωαβ · R′
The contribution of Ωαβ · R′
iβ is positive if the signs of
Ωαβ and R′
iβ are the same. That is to say, a person i like
item α may result from the situations (i) the person i likes
the item β which is similar to item α, or (ii) the person
i dislikes the item β which is opposite to item α (i.e.
Ωαβ < 0). Note that, when computing the predictions
to a speciﬁc user i, we have to add the average rating of
this user, ⟨R⟩i, back to ˜Riα.
Obviously, Eq. (5) can be rewritten in a matrix form
for any given user i, as
Ri = P · R′
i are M-dimensional column vectors denoting the predicted and known ratings for user i, and
β |Ωαβ|, acting as the transfer matrix.
For simplicity, hereinafter, without confusion, we cancel
the subscript i and superscript - a comma. Since for each
user, the predicting operation can be expressed in a matrix form, we can get the numerical results by directly
using the general framework of SCR, as shown in Eq.
(3). However, we have to perform the matrix multiplying for every user, which takes long time in computation
especially for huge-size recommender systems.
To get the analytical expression and reduce the computational complexity, for a given user, we group its known
ratings (as boundary conditions) and unknown ratings
into RB and RU, respectively. Correspondingly, matrix
P is re-arranged by the same order as R. For this user,
we can rewrite Eq. (6) in a sub-matrix multiplying form:
In the standard collaborative ﬁltering , as shown
(5), the unknown vector, RU, is set as a zero
vector. Therefore, the predicted vector, ˜RU, can be expressed by a compact form:
˜RU = PUB · RB.
Clearly, it only takes into account the direct correlations
between the unknown and known sets.
The solution Eq. (8) does not obey the self-consistent
condition, for the free sub-vector ˜RU is not equal to RU.
Considering the self-consistent condition (i.e ˜RU = RU),
the predicted vector should obey the following equation:
˜RU = PUBRB + PUU ˜RU,
whose solution reads:
˜RU = (I −PUU)−1PUBRB.
This solution diﬀers from the standard collaborative ﬁltering by an additional item (I −PUU)−1.
Since it may not be practical to directly inverse (I −
PUU) especially for huge-size PUU, we come up with a
simple and eﬃcient iterative method: Substitute the ﬁrst
results ˜RU for RU, on the right term of Eq. (6), and
take RB as the ﬁxed boundary conditions. Then, get the
second step results about ˜RU, and substitute it for RU
again. Do it repeatedly, at the nth step, we get:
˜RU = (I + PUU + P 2
UU + · · · + P n−1
UU )PUBRB.
Since the dominant eigenvalue of PUU is smaller than 1,
UU converges exponentially fast , and we can get the
stable solution quickly within several steps.
In addition, besides the item-item similarity used introduced here, the similarity-based method can also be
implemented analogously via using the user-user similarity . The SCR can also be embedded in that case, and
gain much better algorithmic accuracy.
Spectrum-based SCR.—We here present a spectrumbased algorithm, which relies on the Singular Value Decomposition (SVD) of the rating matrix. Analogously,
we use the matrix with subtraction of average ratings,
R′, instead of R. The SVD of R′ is deﬁned as :
R′ = U · S · V T ,
number of iterations
Similarity based method
Self -Consistent Ref inement
FIG. 1: (a) Prediction error vs. iteration step, with p = 0.9
ﬁxed. (b) The comparison of algorithmic accuracy between
the standard similarity-based method and the similaritybased SCR for diﬀerent p.
where U is an N ×N unitary matrix formed by the eigenvectors of R′R′T , S is an N × M singular value matrix with nonnegative numbers in decreasing order on
the diagonal and zeros oﬀthe diagonal, and V T is an
M × M unitary matrix formed by the eigenvectors of
R′T R′. The number of positive diagonal elements in S
equals rank(R′).
We keep only the k largest diagonal elements (also the
k largest singular values) to obtain a reduced k×k matrix
Sk, and then, reduce the matrices U and V accordingly.
That is to say, only the k column vectors of U and k
row vectors of V T corresponding to the k largest singular
values are kept. The reconstructed matrix reads:
k = Uk · Sk · V T
where Uk, Sk and V T
k have dimensions N × k, k × k and
k × M, respectively. Note that, Eq. (13) is no longer
the exact decomposition of the original matrix R′ (i.e.,
k ̸= R′), but the closest rank-k matrix to R . In
other words, R′
k minimizes the Frobenius norm ∥R′−R′
 over all rank-k matrices. Previous studies found that
 the reduced dimensional approximation sometimes
performs better than the original matrix in information
retrieval since it ﬁlters out the small singular values that
may be highly distorted by the noise.
Actually, each row of the N × k matrix Uk
√Sk represents the vector of the corresponding agent’s tastes, and
each row of the M × k matrix Vk
√Sk characterizes the
features of the corresponding item. Therefore, the prediction of the evaluation a user i gives to an item α can
be obtained by computing the inner product of the i-th
√Sk and the α-th row of Vk
= Uk · Sk · V T
This derivation reproduces the Eq. (13), and illuminates
the reason why using SVD to extract hidden informa-
number of iterations
Spectrum based method
Self -Consistent Ref inement
FIG. 2: (a) Prediction error vs. iteration step, with p = 0.9
ﬁxed. (b) The comparison of algorithmic accuracy between
the standard spectrum-based method and the spectrum-based
SCR for diﬀerent p.
tion in user-item rating matrix.
The entry ˜Riα is the
predicted rating of user i on item α.
An underlying assumption in the k-truncated SVD
method is the existence of k principle attributes in both
the user’s tastes and the item’s features. For example, a
movie’s attributes may include the director, hero, heroine, gut, music, etc., and a user has his personal tastes
on each attribute. If a movie is well ﬁt his tastes, he will
give a high rating, otherwise a low rating. Denote the
states of a user i and an item α as:
⟨ui| = (u1
i , · · · , uk
i ); ⟨vα| = (v1
α, · · · , vk
then we can estimate the evaluation of i on α as the
matching extent between their tastes and features:
˜Riα = ⟨ui|vα⟩.
Therefore, we want to ﬁnd a matrix ˜R that can be decomposed to N k-dimensional taste vectors and M kdimensional feature vectors so that the corresponding
entries are exactly the same as the known ratings and
consequently, the other entries are the predicted ratings.
However, the k-truncated SVD matrix is not selfconsistent for the elements corresponding to the known
ratings in R′
k are not exactly the same as those in R′. A
self-consistent prediction matrix can be obtained via an
iterative k-truncated SVD process by resetting those elements back to the known values at each step. Referring
to Eq. (3), the Spectrum-based SCR treats the known
ratings as the boundary conditions, and use k-truncated
SVD as the matrix operator D. The iteration will converge to a stable matrix ˜R, namely the predicted matrix.
Numerical results.—To test the algorithmic accuracy,
we use a benchmark data set, namely MovieLens .
The data consists of N = 3020 users, M = 1809 movies,
and 2.24 × 105 discrete ratings 1-5. All the ratings are
sorted according to their time stamps. We set a fraction
p of earlier ratings as the training set, and the remain
ratings (with later time stamps) as the testing set.
As shown in Figs.
1 and 2, both the similaritybased and spectrum-based SCRs converge very fast, and
sharply improve the algorithmic accuracy of the standard
methods. In spectrum-based methods, the parameter k
is not observable in the real system, thus we treat it as a
tunable parameter. The results displayed in Fig. 2 correspond to the optimal k that minimizes the prediction
error. For diﬀerent p, the optimal k is diﬀerent. Denoting
the data density as ρ = E/NM, where E is the number
of ratings in the training set. The spectrum-based SCR
will converge only if k is smaller than a threshold
kc = N + M −2
s„N + M −2
So that the searching horizon of optimal k can be reduced to the natural numbers not larger than kc. The
mathematical derivation and numerical results about this
threshold behavior, as well as the sensitivity of algorithmic performance to k will be discussed elsewhere.
Conclusions.—In this Letter, we proposed a algorithmic framework for recommender systems, namely selfconsistent reﬁnement.
This general framework is implemented by embedding two representative recommendation algorithms: similarity-based and spectrum-based
methods. Numerical simulations on a benchmark data
set demonstrate the signiﬁcant improvement of algorithmic accuracy compared with the standard algorithms.
Actually, the spectrum-based SCR has higher accuracy
than the similarity-based one, but it requires an optimizing process on the selection of the parameter k, thus
takes longer computational time.
similarity-based
spectrum-based
methods, very recently, some new kinds of recommendation algorithms that mimic certain physics dynamics,
such as heat conduction and mass diﬀusion , are
suggested to be the promising candidates in the next generation of recommender systems for they provide better algorithmic accuracy while have lower computational
complexity. It is worthwhile to emphasize that those two
algorithms also belong to the framework of SCR
- they are just two speciﬁc realizations of SCR if considering the matrix operator D as the conduction of heat or
the exchange of mass during one step. In fact, the SCR
framework is of great generality, and any algorithm that
can be expressed in the form of Eq. (2) has the opportunity being improved via iterative SCR. Furthermore, the
present method can be applied in not only the recommender systems, but also many other subjects, such as
data clustering, miss data mining, detection of community structure, pattern recognition, predicting of protein
structure, and so on.
This work is partially supported by SBF (Switzerland) for ﬁnancial support through project C05.0148
(Physics of Risk), and the Swiss National Science Foundation (205120-113842). T.Z. acknowledges NNSFC under Grant No. 10635040 and 60744003, as well as the 973
Project 2006CB705500.
 G. Linden et al., IEEE Internet Computing 7, 76 .
 D. Billsus et al., Commun. ACM 45, 34 .
 J. L. Herlocker et al., ACM Trans. Inform. Syst. 22, 5
 G. Adomavicius et al., IEEE Trans. Knowl. Data Eng.
17, 734 .
 A. Ansari et al., J. Mark. Res. 37, 363 .
 Y. P. Ying et al., J. Mark. Res. 43, 355 .
 R. Kumar et al., J. Comput. Syst. Sci. 63, 42 .
 J. O’Donovan et al., Proc. 10th Int’l Conf. Intell. User
Interfaces .
 S. Maslov et al., Phys. Rev. Lett. 87, 248701 .
 P. Laureti et al., EPL 75, 1006 .
 Y.-C. Zhang et al., Phys. Rev. Lett. 99, 154301 .
 Y.-C. Zhang et al., EPL 80, 68003 .
 T. Zhou et al., Phys. Rev. E 76, 046115 .
 T. Zhou et al., EPL 81, 58004 .
 C.-K. Yu et al., Physica A 371, 732 .
 M. Blattner et al., Physica A 373, 753 .
 M. J. Pazzani et al., Lect. Notes Comput. Sci. 4321, 325
 J. A. Konstan et al., Commun. ACM 40, 77 .
 B. Sarwar et al., Proc. 10th Int’l WWW Conf. .
 D. Billsus et al., Proc. Int’l Conf. Machine Learning
 B. Sarwar et al., Proc. ACM WebKDD Workshop .
 P. Resnick et al., Proc. Comput. Supported Cooperative
Work Conf. .
 J. S. Breese et al., Proc. 14th Conf. Uncertainty in Arti-
ﬁcial Intelligence .
 G. H. Golub et al., Matrix Computation .
 X. Zhang, Matrix Analysis and Applications .
 R. A. Horn et al., Matrix analysis .
 The Fribenius norm (also called Euclidean norm, Schui
norm or Hilbert-Schmidt norm) of a matrix {aij}, is de-
ﬁned as ∥A∥=
 M. W. Berry et al., SIAM Rev. 37, 573 .
 The MovieLens data can be download from the website
of GroupLens Research (