HAL Id: hal-00271141
 
Submitted on 21 Jan 2010
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
A review of image denoising algorithms, with a new one
Antoni Buades, Bartomeu Coll, Jean-Michel Morel
To cite this version:
Antoni Buades, Bartomeu Coll, Jean-Michel Morel. A review of image denoising algorithms, with
a new one.
Multiscale Modeling and Simulation: A SIAM Interdisciplinary Journal, 2005, 4 (2),
pp.490-530. ￿10.1137/040616024￿. ￿hal-00271141￿
A REVIEW OF IMAGE DENOISING ALGORITHMS, WITH A NEW
A. BUADES †
‡, B. COLL †, AND J.M. MOREL ‡
The search for eﬃcient image denoising methods still is a valid challenge, at the
crossing of functional analysis and statistics. In spite of the sophistication of the recently proposed
methods, most algorithms have not yet attained a desirable level of applicability. All show an outstanding performance when the image model corresponds to the algorithm assumptions, but fail in
general and create artifacts or remove image ﬁne structures. The main focus of this paper is, ﬁrst, to
deﬁne a general mathematical and experimental methodology to compare and classify classical image
denoising algorithms, second, to propose an algorithm (Non Local Means) addressing the preservation
of structure in a digital image. The mathematical analysis is based on the analysis of the “method
noise”, deﬁned as the diﬀerence between a digital image and its denoised version. The NL-means
algorithm is proven to be asymptotically optimal under a generic statistical image model. The denoising performance of all considered methods are compared in four ways; mathematical: asymptotic
order of magnitude of the method noise under regularity assumptions; perceptual-mathematical: the
algorithms artifacts and their explanation as a violation of the image model; quantitative experimental: by tables of L2 distances of the denoised version to the original image. The most powerful
evaluation method seems, however, to be the visualization of the method noise on natural images.
The more this method noise looks like a real white noise, the better the method.
Key words.
Image restoration, non parametric estimation, PDE smoothing ﬁlters, adaptive
ﬁlters, frequency domain ﬁlters
AMS subject classiﬁcations. 62H35
1. Introduction.
1.1. Digital images and noise. The need for eﬃcient image restoration methods has grown with the massive production of digital images and movies of all kinds,
often taken in poor conditions. No matter how good cameras are, an image improvement is always desirable to extend their range of action.
A digital image is generally encoded as a matrix of grey level or color values. In
the case of a movie, this matrix has three dimensions, the third one corresponding
to time. Each pair (i, u(i)) where u(i) is the value at i is called pixel, for “picture
element”. In the case of grey level images, i is a point on a 2D grid and u(i) is a
real value. In the case of classical color images, u(i) is a triplet of values for the red,
green and blue components. All of what we shall say applies identically to movies,
3D images and color or multispectral images. For a sake of simplicity in notation
and display of experiments, we shall here be contented with rectangular 2D grey-level
The two main limitations in image accuracy are categorized as blur and noise.
Blur is intrinsic to image acquisition systems, as digital images have a ﬁnite number of
samples and must satisfy the Shannon-Nyquist sampling conditions . The second
main image perturbation is noise.
†Universitat de les Illes Balears, Anselm Turmeda, Ctra. Valldemossa Km. 7.5, 07122 Palma de
Mallorca, Spain. Author ﬁnanced by the Ministerio de Ciencia y Tecnologia under grant TIC2002-
02172. During this work, the ﬁrst author had a fellowship of the Govern de les Illes Balears for the
realization of his PhD.
‡Centre de Mathmatiques et Leurs Applications. ENS Cachan 61, Av du Prsident Wilson 94235
Cachan, France.
Author ﬁnanced by the Centre National d’Etudes Spatiales (CNES), the Oﬃce
of Naval Research under grant N00014-97-1-0839, the Direction Gnrale des Armements (DGA), the
Ministre de la Recherche et de la Technologie.
A. BUADES, B. COLL AND J.M MOREL
Each one of the pixel values u(i) is the result of a light intensity measurement,
usually made by a CCD matrix coupled with a light focusing system. Each captor
of the CCD is roughly a square in which the number of incoming photons is being
counted for a ﬁxed period corresponding to the obturation time.
When the light
source is constant, the number of photons received by each pixel ﬂuctuates around
its average in accordance with the central limit theorem.
In other terms one can
expect ﬂuctuations of order √n for n incoming photons. In addition, each captor, if
not adequately cooled, receives heat spurious photons. The resulting perturbation is
usually called “obscurity noise”. In a ﬁrst rough approximation one can write
v(i) = u(i) + n(i),
where i ∈I, v(i) is the observed value, u(i) would be the “true” value at pixel i,
namely the one which would be observed by averaging the photon counting on a long
period of time, and n(i) is the noise perturbation. As indicated, the amount of noise
is signal-dependent, that is n(i) is larger when u(i) is larger. In noise models, the
normalized values of n(i) and n(j) at diﬀerent pixels are assumed to be independent
random variables and one talks about “white noise”.
1.2. Signal and noise ratios. A good quality photograph (for visual inspection) has about 256 grey level values, where 0 represents black and 255 represents
white. Measuring the amount of noise by its standard deviation, σ(n), one can deﬁne
the signal noise ratio (SNR) as
SNR = σ(u)
where σ(u) denotes the empirical standard deviation of u,
(u(i) −u)2
i∈I u(i) is the average grey level value. The standard deviation of the
noise can also be obtained as an empirical measurement or formally computed when
the noise model and parameters are known. A good quality image has a standard
deviation of about 60.
The best way to test the eﬀect of noise on a standard digital image is to add
a gaussian white noise, in which case n(i) are i.i.d. gaussian real variables. When
σ(n) = 3, no visible alteration is usually observed. Thus, a 60
3 ≃20 signal to noise
ratio is nearly invisible.
Surprisingly enough, one can add white noise up to a
ratio and still see
everything in a picture ! This fact is illustrated in Figure 1.1
and constitutes a major enigma of human vision. It justiﬁes the many attempts to
deﬁne convincing denoising algorithms. As we shall see, the results have been rather
deceptive. Denoising algorithms see no diﬀerence between small details and noise, and
therefore remove them. In many cases, they create new distortions and the researchers
are so much used to them as to have created a taxonomy of denoising artifacts:
“ringing”, “blur”, “staircase eﬀect”, “checkerboard eﬀect”, “wavelet outliers”, etc.
This fact is not quite a surprise. Indeed, to the best of our knowledge, all denoising
algorithms are based on
• a noise model
• a generic image smoothness model, local or global.
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Fig. 1.1. A digital image with standard deviation 55, the same with noise added (standard
deviation 3), the signal noise ratio being therefore equal to 18, and the same with signal noise ratio
slightly larger than 2. In this second image, no alteration is visible. In the third, a conspicuous
noise with standard deviation 25 has been added but, surprisingly enough, all details of the original
image still are visible.
In experimental settings, the noise model is perfectly precise. So the weak point of the
algorithms is the inadequacy of the image model. All of the methods assume that the
noise is oscillatory, and that the image is smooth, or piecewise smooth. So they try
to separate the smooth or patchy part (the image) from the oscillatory one. Actually,
many ﬁne structures in images are as oscillatory as noise is; conversely, white noise
has low frequencies and therefore smooth components. Thus a separation method
based on smoothness arguments only is hazardous.
1.3. The “method noise”. All denoising methods depend on a ﬁltering parameter h. This parameter measures the degree of ﬁltering applied to the image. For
most methods, the parameter h depends on an estimation of the noise variance σ2.
One can deﬁne the result of a denoising method Dh as a decomposition of any image
v = Dhv + n(Dh, v),
1. Dhv is more smooth than v
2. n(Dh, v) is the noise guessed by the method.
Now, it is not enough to smooth v to ensure that n(Dh, v) will look like a noise.
The more recent methods are actually not contented with a smoothing, but try to
recover lost information in n(Dh, v) , . So the focus is on n(Dh, v).
Definition 1.1 (Method noise). Let u be a (not necessarily noisy) image and
Dh a denoising operator depending on h. Then we deﬁne the method noise of u as
the image diﬀerence
n(Dh, u) = u −Dh(u).
This method noise should be as similar to a white noise as possible. In addition,
since we would like the original image u not to be altered by denoising methods, the
method noise should be as small as possible for the functions with the right regularity.
According to the preceding discussion, four criteria can and will be taken into
account in the comparison of denoising methods:
• a display of typical artifacts in denoised images.
A. BUADES, B. COLL AND J.M MOREL
• a formal computation of the method noise on smooth images, evaluating how
small it is in accordance with image local smoothness.
• a comparative display of the method noise of each method on real images
with σ = 2.5. We mentioned that a noise standard deviation smaller than 3 is
subliminal and it is expected that most digitization methods allow themselves
this kind of noise.
• a classical comparison receipt based on noise simulation : it consists of taking
a good quality image, add gaussian white noise with known σ and then compute the best image recovered from the noisy one by each method. A table
of L2 distances from the restored to the original can be established. The L2
distance does not provide a good quality assessment. However, it reﬂects well
the relative performances of algorithms.
On top of this, in two cases, a proof of asymptotic recovery of the image can be
obtained by statistical arguments.
1.4. Which methods to compare ?. We had to make a selection of the denoising methods we wished to compare.
Here a diﬃculty arises, as most original
methods have caused an abundant literature proposing many improvements. So we
tried to get the best available version, but keeping the simple and genuine character
of the original method : no hybrid method. So we shall analyze :
1. the Gaussian smoothing model (Gabor ), where the smoothness of u is
measured by the Dirichlet integral
2. the anisotropic ﬁltering model (Perona-Malik , Alvarez et al. );
3. the Rudin-Osher-Fatemi total variation model and two recently proposed
iterated total variation reﬁnements ;
4. the Yaroslavsky ( , ) neighborhood ﬁlters and an elegant variant, the
SUSAN ﬁlter (Smith and Brady) ;
5. the Wiener local empirical ﬁlter as implemented by Yaroslavsky ;
6. the translation invariant wavelet thresholding , a simple and performing
variant of the wavelet thresholding ;
7. DUDE, the discrete universal denoiser and the UINTA, Unsupervised
Information-Theoretic, Adaptive Filtering , two very recent new approaches;
8. the non local means (NL-means) algorithm, which we introduce here.
This last algorithm is given by a simple closed formula. Let u be deﬁned in a bounded
domain Ω⊂R2, then
NL(u)(x) =
(Ga∗|u(x+.)−u(y+.)|2)(0)
where x ∈Ω, Ga is a Gaussian kernel of standard deviation a, h acts as a ﬁltering
parameter and C(x) =
e−(Ga∗|u(x+.)−u(z+.)|2)(0)
dz is the normalizing factor. In order
to make clear the previous deﬁnition, we recall that
(Ga ∗|u(x + .) −u(y + .)|2)(0) =
R2 Ga(t)|u(x + t) −u(y + t)|2dt.
This amounts to say that NL(u)(x), the denoised value at x, is a mean of the values
of all pixels whose gaussian neighborhood looks like the neighborhood of x.
1.5. What is left. We do not draw into comparison the hybrid methods, in
particular the total variation + wavelets ( , , ). Such methods are signiﬁcant
improvements of the simple methods but are impossible to draw into a benchmark :
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
their eﬃciency depends a lot upon the choice of wavelet dictionaries and the kind of
Second, we do not draw into the comparison the method introduced recently by
Y. Meyer , whose aim it is to decompose the image into a BV part and a texture
part (the so called u+v methods), and even into three terms, namely u+v +w where
u is the BV part, v is the “texture” part belonging to the dual space of BV , denoted
by G, and w belongs to the Besov space ˙B∞
−1,∞, a space characterized by the fact that
the wavelet coeﬃcients have a uniform bound. G is proposed by Y. Meyer as the right
space to model oscillatory patterns such as textures. The main focus of this method
is not denoising, yet.
Because of the diﬀerent and more ambitious scopes of the
Meyer method, , which makes it parameter and implementation-dependent,
we could not draw it into the discussion.
Last but not least, let us mention the
bandlets and curvelets transforms for image analysis. These methods also
are separation methods between the geometric part and the oscillatory part of the
image and intend to ﬁnd an accurate and compressed version of the geometric part.
Incidentally, they may be considered as denoising methods in geometric images, as the
oscillatory part then contains part of the noise. Those methods are closely related to
the total variation method and to the wavelet thresholding and we shall be contented
with those simpler representatives.
1.6. Plan of the paper. Section 2 computes formally the method noise for the
best elementary local smoothing methods, namely gaussian smoothing, anisotropic
smoothing (mean curvature motion), total variation minimization and the neighborhood ﬁlters. For all of them we prove or recall the asymptotic expansion of the ﬁlter
at smooth points of the image and therefore obtain a formal expression of the method
noise. This expression permits to characterize places where the ﬁlter performs well
and where it fails. In section 3, we treat the Wiener-like methods, which proceed by
a soft or hard threshold on frequency or space-frequency coeﬃcients. We examine
in turn the Wiener-Fourier ﬁlter, the Yaroslavsky local adaptive DCT based ﬁlters
and the wavelet threshold method. Of course the gaussian smoothing belongs to both
classes of ﬁlters. We also describe the universal denoiser DUDE, but we cannot draw
it into the comparison as its direct application to grey level images is unpractical
so far (we discuss its feasibility). Finally, we examine the UINTA algorithms whose
principles stand close to the NL-means algorithm. In section 5, we introduce the Non
Local means (NL-means) ﬁlter. This method is not easily classiﬁed in the preceding terminology, since it can work adaptively in a local or non local way. We ﬁrst
give a proof that this algorithm is asymptotically consistent (it gives back the conditional expectation of each pixel value given an observed neighborhood) under the
assumption that the image is a fairly general stationary random process. The works
of Efros and Leung and Levina have shown that this assumption is sound
for images having enough samples in each texture patch. In section 6, we compare
all algorithms from several points of view, do a performance classiﬁcation and explain why the NL-means algorithm shares the consistency properties of most of the
aforementioned algorithms.
2. Local smoothing ﬁlters. The original image u is deﬁned in a bounded
domain Ω⊂R2, and denoted by u(x) for x = (x, y) ∈R2. This continuous image is
usually interpreted as the Shannon interpolation of a discrete grid of samples, and
is therefore analytic. The distance between two consecutive samples will be denoted
The noise itself is a discrete phenomenon on the sampling grid. According to
A. BUADES, B. COLL AND J.M MOREL
the usual screen and printing visualization practice, we do not interpolate the noise
samples ni as a band limited function, but rather as a piecewise constant function,
constant on each pixel i and equal to ni.
We write |x| = (x2 + y2)
2 and x1.x2 = x1x2 + y1y2 their scalar product and
denote the derivatives of u by ux = ∂u
∂x∂y. The gradient of u
is written as Du = (ux, uy) and the Laplacian of u as ∆u = uxx + uyy.
2.1. Gaussian smoothing. By Riesz theorem, image isotropic linear ﬁltering
boils down to a convolution of the image by a linear radial kernel. The smoothing
requirement is usually expressed by the positivity of the kernel. The paradigm of such
kernels is of course the gaussian x →Gh(x) =
(4πh2)e−|x|2
4h2 . In that case, Gh has
standard deviation h and it is easily seen that
Theorem 2.1 . The image method noise of the convolution with a
gaussian kernel Gh is
u −Gh ∗u = −h2∆u + o(h2).
A similar result is actually valid for any positive radial kernel with bounded variance,
so one can keep the gaussian example without loss of generality.
The preceding
estimate is valid if h is small enough. On the other hand, the noise reduction properties
depend upon the fact that the neighborhood involved in the smoothing is large enough,
so that the noise gets reduced by averaging. So in the following we assume that h = kε,
where k stands for the number of samples of the function u and noise n in an interval of
length h. The spatial ratio k must be much larger than 1 to ensure a noise reduction.
The eﬀect of a gaussian smoothing on the noise can be evaluated at a reference
pixel i = 0. At this pixel,
Gh ∗n(0) =
Gh(x)n(x)dx =
ε2Gh(i)ni,
where we recall that n(x) is being interpolated as a piecewise constant function, the
Pi square pixels centered in i have size ε2 and Gh(i) denotes the mean value of the
function Gh on the pixel i.
Denoting by V ar(X) the variance of a random variable X, the additivity of
variances of independent centered random variables yields
V ar(Gh ∗n(0)) =
ε4Gh(i)2σ2 ≃σ2ε2
Gh(x)2dx = ε2σ2
So we have proved
Theorem 2.2. Let n(x) be a piecewise constant white noise, with n(x) = ni on
each square pixel i. Assume that the ni are i.i.d. with zero mean and variance σ2.
Then the “noise residue” after a gaussian convolution of n by Gh satisﬁes
V ar(Gh ∗n(0)) ≃ε2σ2
In other terms, the standard deviation of the noise, which can be interpreted as the
noise amplitude, is multiplied by
Theorems 2.1 and 2.2 traduce the delicate equilibrium between noise reduction
and image destruction by any linear smoothing. Denoising does not alter the image
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
at points where it is smooth at a scale h much larger than the sampling scale ε.
The ﬁrst theorem tells us that the method noise of the gaussian denoising method is
zero in harmonic parts of the image. A Gaussian convolution is optimal on harmonic
functions, and performs instead poorly on singular parts of u, namely edges or texture,
where the Laplacian of the image is large. See Figure 2.2.
2.2. Anisotropic ﬁlters and curvature motion. The anisotropic ﬁlter (AF)
attempts to avoid the blurring eﬀect of the gaussian by convolving the image u at x
only in the direction orthogonal to Du(x). The idea of such ﬁlter goes back to Perona
and Malik and actually again to Gabor . Set
Gh(t)u(x + tDu(x)⊥
|Du(x)| )dt,
for x such that Du(x) ̸= 0 and where (x, y)⊥= (−y, x) and Gh(t) =
the one-dimensional Gauss function with variance h2. At points where Du(x) = 0 an
isotropic gaussian mean is usually applied and the result of Theorem 2.1 holds at those
points. If one assumes that the original image u is twice continuously diﬀerentiable
(C2) at x, it is easily shown by a second order Taylor expansion that
Theorem 2.3. The image method noise of an anisotropic ﬁlter AFh is
u(x) −AFhu(x) ≃−1
2h2D2u(Du⊥
|Du| , Du⊥
|Du| ) = −1
2h2|Du|curv(u)(x),
where the relation holds when Du(x) ̸= 0.
By curv(u)(x), we denote the curvature, i.e. the signed inverse of the radius of
curvature of the level line passing by x. When Du(x) ̸= 0, this means
curv(u) = uxxu2
y −2uxyuxuy + uyyu2
(u2x + u2y)
This method noise is zero wherever u behaves locally like a one variable function,
u(x, y) = f(ax + by + c). In such a case, the level line of u is locally the straight
line with equation ax + by + c = 0 and the gradient of f may instead be very large.
In other terms, with anisotropic ﬁltering, an edge can be maintained. On the other
hand, we have to evaluate the gaussian noise reduction.
This is easily done by a
one-dimensional adaptation of Theorem 2.2. Notice that the noise on a grid is not
isotropic ; so the gaussian average when Du is parallel to one coordinate axis is made
roughly on
2 more samples than the gaussian average in the diagonal direction.
Theorem 2.4. By anisotropic gaussian smoothing, when ε is small enough with
respect to h, the noise residue satisﬁes
Var (AFh(n)) ≤
In other terms, the standard deviation of the noise n is multiplied by a factor at most
equal to (
2πh)1/2, this maximal value being attained in the diagonals.
Proof. Let L be the line x + t Du⊥(x)
|Du(x)| passing by x, parameterized by t ∈R and
denote by Pi, i ∈I the pixels which meet L, n(i) the noise value, constant on pixel
A. BUADES, B. COLL AND J.M MOREL
Pi, and εi the length of the intersection of L ∩Pi. Denote by g(i) the average of
Gh(x + t Du⊥(x)
|Du(x)| ) on L ∩Pi. Then, one has
εin(i)g(i).
The n(i) are i.i.d. with standard variation σ and therefore
V ar(AFh(n)) =
i σ2g(i)2 ≤σ2 max(εi)
This yields
Var (AFh(n)) ≤
Gh(t)2dt =
There are many versions of AFh, all yielding an asymptotic estimate equivalent to
the one in Theorem 2.3 : the famous median ﬁlter , an inf-sup ﬁlter on segments
centered at x , and the clever numerical implementation of the mean curvature
equation in . So all of those ﬁlters have in common the good preservation of edges,
but they perform poorly on ﬂat regions and are worse there than a gaussian blur. This
fact derives from the comparison of the noise reduction estimates of Theorems 2.1 and
2.4, and is experimentally patent in Figure 2.2.
2.3. Total variation. The Total variation minimization was introduced by Rudin,
Osher and Fatemi . The original image u is supposed to have a simple geometric description, namely a set of connected sets, the objects, along with their
smooth contours, or edges. The image is smooth inside the objects but with jumps
across the boundaries. The functional space modelling these properties is BV (Ω), the
space of integrable functions with ﬁnite total variation TVΩ(u) =
|Du|, where Du
is assumed to be a Radon measure. Given a noisy image v(x), the above mentioned
authors proposed to recover the original image u(x) as the solution of the constrained
minimization problem
subject to the noise constraints
(u(x) −v(x))dx = 0
|u(x) −v(x)|2dx = σ2.
The solution u must be as regular as possible in the sense of the total variation,
while the diﬀerence v(x) −u(x) is treated as an error, with a prescribed energy. The
constraints prescribe the right mean and variance to u −v, but do not ensure that
it be similar to a noise (see a thorough discussion in ). The preceding problem is
naturally linked to the unconstrained problem
u TVΩ(u) + λ
|v(x) −u(x)|2dx,
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
for a given Lagrange multiplier λ. The above functional is strictly convex and lower
semicontinuous with respect to the weak-star topology of BV. Therefore the minimum
exists, is unique and computable (see (e.g.) .) The parameter λ controls the trade
oﬀbetween the regularity and ﬁdelity terms. As λ gets smaller the weight of the
regularity term increases.
Therefore λ is related to the degree of ﬁltering of the
solution of the minimization problem.
Let us denote by TVFλ(v) the solution of
problem (2.2) for a given value of λ. The Euler Lagrange equation associated with
the minimization problem is given by
(u(x) −v(x)) −1
2λcurv(u)(x) = 0,
(see ). Thus,
Theorem 2.5. The image method noise of the total variation minimization (2.2)
u(x) −TVFλ(u)(x) = −1
2λcurv(TVFλ(u))(x).
As in the anisotropic case, straight edges are maintained because of their small
curvature. However, details and texture can be over smoothed if λ is too small, as is
shown in Figure 2.2.
2.4. Iterated Total Variation reﬁnement. In the original TV model the
removed noise, v(x)−u(x), is treated as an error and is no longer studied. In practice,
some structures and texture are present in this error. Several recent works have tried
to avoid this eﬀect .
2.4.1. The Tadmor et al. approach. In , the authors have proposed to
use the Rudin-Osher-Fatemi iteratively. They decompose the noisy image, v = u0+n0
by the total variation model. So taking u0 to contain only geometric information, they
decompose by the very same model n0 = u1 + n1, where u1 is assumed to be again
a geometric part and n1 contains less geometric information than n0. Iterating this
process, one obtains u = u0+u1+u2+...+uk as a reﬁned geometric part and nk as the
noise residue. This strategy is in some sense close to the matching pursuit methods
 . Of course, the weight parameter in the Rudin-Osher-Fatemi has to grow at each
iteration and the authors propose a geometric series λ, 2λ, ...., 2kλ. In that way, the
extraction of the geometric part nk becomes twice more asking at each step. Then,
the new algorithm is as follows:
1. Starting with an initial scale λ = λ0,
v = u0 + n0,
[u0, n0] = arg min
|v(x) −u(x)|2dx.
2. Proceed with successive applications of the dyadic reﬁnement nj = uj+1 +
[uj+1, nj+1] = arg
|Du| + λ02j+1
|nj(x) −u(x)|2dx.
3. After k steps, we get the following hierarchical decomposition of v
v = u0 + n0
= u0 + u1 + n1
= u0 + u1 + ... + uk + nk.
A. BUADES, B. COLL AND J.M MOREL
The denoised image is given by the partial sum Pk
j=0 uj and nk is the noise residue.
This is a multilayered decomposition of v which lies in an intermediate scale of spaces,
in between BV and L2. Some theoretical results on the convergence of this expansion
are presented in .
2.4.2. The Osher et al.
approach. The second algorithm due to Osher et
al. , also consists of an iteration of the original model. The new algorithm is as
1. First solve the original TV model
u1 = arg min
|∇u(x)|dx + λ
(v(x) −u(x))2dx
to obtain the decomposition v = u1 + n1.
2. Perform a correction step to obtain
u2 = arg min
|∇u(x)|dx + λ
(v(x) + n1(x) −u(x))2dx
where n1 is the noise estimated by the ﬁrst step. The correction step adds
this ﬁrst estimate of the noise to the original image and raises the following
decomposition v + n1 = u2 + n2.
3. Iterate : compute uk+1 as a minimizer of the modiﬁed total variation minimization,
uk+1 = arg min
|∇u(x)|dx + λ
(v(x) + nk(x) −u(x))2dx
v + nk = uk+1 + nk+1.
Some results are presented in which clarify the nature of the above sequence:
• {uk}k converges monotonically in L2 to v, the noisy image, as k →∞.
• {uk}k approaches the noisy free image monotonically in the Bregman distance
associated with the BV seminorm, at least until ∥u¯k −u∥≤σ2, where u is
the original image and σ is the standard deviation of the added noise.
These two results indicate how to stop the sequence and choose u¯k. It is enough
to proceed iteratively until the result gets noisier or the distance ∥u¯k−u∥2 gets smaller
than σ2. The new solution has more details preserved, as Figure 2.2 shows.
The above iterated denoising strategy being quite general, one can make the
computations for a linear denoising operator T as well. In that case, this strategy
T(v + n1) = T(v) + T(n1)
amounts to say that the ﬁrst estimated noise n1 is ﬁltered again and its smooth
components added back to the original, which is in fact the Tadmor et al. strategy.
2.5. Neighborhood ﬁlters. The previous ﬁlters are based on a notion of spatial neighborhood or proximity. Neighborhood ﬁlters instead take into account grey
level values to deﬁne neighboring pixels. In the simplest and more extreme case, the
denoised value at pixel i is an average of values at pixels which have a grey level value
close to u(i). The grey level neighborhood is therefore
B(i, h) = {j ∈I | u(i) −h < u(j) < u(i) + h}.
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
This is a fully non-local algorithm, since pixels belonging to the whole image are used
for the estimation at pixel i. This algorithm can be written in a more continuous form
|u(y)−u(x)|2
where Ω⊂R2 is an open and bounded set, and C(x) =
|u(y)−u(x)|2
normalization factor. The ﬁrst question to address here is the consistency of such a
ﬁlter, namely how close the denoised version is to the original when u is smooth.
Lemma 2.6. Suppose u is Lipschitz in Ωand h > 0, then C(x) ≥O(h2).
Proof. Given x, y ∈Ω, by the Mean Value Theorem, |u(x) −u(y)| ≤K|x −y| for
some real constant K. Then, C(x) =
|u(y)−u(x)|2
|u(y)−u(x)|2
e−K2O(h2).
Proposition 2.7. (Method noise estimate). Suppose u is a Lipschitz bounded
function on Ω, where Ωis an open and bounded domain of R2. Then |u(x)−NFhu(x)| =
O(h√−log h), for h small, 0 < h < 1, x ∈Ω.
Proof. Let x be a point of Ωand for a given B and h, B, h ∈R, consider the set
Dh = {y ∈Ω| |u(y) −u(x)| ≤Bh}. Then
|u(x) −NFhu(x)| ≤1
|u(y)−u(x)|2
|u(y) −u(x)|dy
|u(y)−u(x)|2
|u(y) −u(x)|dy.
By one hand, considering that
|u(y)−u(x)|2
dy ≤C(x) and |u(y) −u(x)| ≤
Bh for y ∈Dh one one sees that the ﬁrst term is bounded by Bh. On the other
hand, considering that e−
|u(y)−u(x)|2
≤e−B2 for y /∈Dh,
h |u(y) −u(x)|dy is
bounded and by Lemma 2.6, C ≥O(h2), one deduces that the second term has an
order O(h−2e−B2). Finally, choosing B such that B2 = −3 log h yields
|u(x) −NFhu(x)| ≤Bh + O(h−2e−B2) = O(h
−log h) + O(h)
and so the method noise has order O(h√−log h).
The Yaroslavsky ( ) neighborhood ﬁlters consider mixed neighborhoods
B(i, h) ∩Bρ(i), where Bρ(i) is a ball of center i and radius ρ. So the method takes an
average of the values of pixels which are both close in grey level and spatial distance.
This ﬁlter can be easily written in a continuous form as,
YNFh,ρ(x) =
|u(y)−u(x)|2
where C(x) =
|u(y)−u(x)|2
dy is the normalization factor. In the authors
present a similar approach where they do not consider a ball of radius ρ but they
weigh the distance to the central pixel, obtaining the following close formula,
|u(y)−u(x)|2
A. BUADES, B. COLL AND J.M MOREL
where C(x) =
|u(y)−u(x)|2
dy is the normalization factor.
First, we study the method noise of the YNFh,ρ in the 1D case. In that case, u
denotes a one dimensional signal.
Theorem 2.8. Suppose u ∈C2((a, b)), a, b ∈R. Then, for 0 < ρ << h and
u(s) −YNFh,ρu(s) ≃−ρ2
h|u′(s)|) u′′(s),
Proof. Let s ∈(a, b) and h, ρ ∈R+. Then
u(s) −YNFh,ρ(s) = −
−ρ e−(u(s+t)−u(s))2
(u(s + t) −u(s))e−(u(s+t)−u(s))2
If we take the Taylor expansion of u(s + t) and the exponential function e−x2 and we
integrate, then we obtain that
u(s) −YNFh,ρ(s) ≃−
−3ρ5u′2u′′
2h −2ρ3u′2
for ρ small enough. The method noise follows from the above expression.
The previous result shows that the neighborhood ﬁltering method noise is proportional to the second derivative of the signal. That is, it behaves like a weighted
heat equation. The function f gives the sign and the magnitude of this heat equation.
Where the function f takes positive values, the method noise behaves as a pure heat
equation, while where it takes negative values, the method noise behaves as a reverse
heat equation. The zeros and the discontinuity points of f represent the singular
points where the behavior of the method changes. The magnitude of this change is
much larger near the discontinuities of f producing an ampliﬁed shock eﬀect. Figure
2.1 displays one experiment with the one dimensional neighborhood ﬁlter. We iterate
the algorithm on a sine signal and illustrate the shock eﬀect. For the two intermediate
iterations un+1, we display the signal f( ρ
n|) which gives the sign and magnitude of
the heat equation at each point. We can see that the positions of the discontinuities of
n|) describe exactly the positions of the shocks in the further iterations and the
ﬁnal state. These two examples corroborate Theorem 2.8 and show how the function
f totally characterizes the performance of the one dimensional neighborhood ﬁlter.
Next we give the analogous result for 2D images.
Theorem 2.9. Suppose u ∈C2(Ω), Ω⊂R2. Then, for 0 < ρ << h and h →0
u(x) −YNFh,ρ(x) ≃−ρ2
h|Du|) uηη + h( ρ
h|Du|) uξξ),
uηη = D2u( Du
uξξ = D2u(Du⊥
|Du| , Du⊥
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Fig. 2.1. One dimensional neighborhood ﬁltering experience. We iterate the ﬁlter on the sine
signal until it converges to a steady state. We show the input signal (a) and the ﬁnal state (b). The
ﬁgures (e) and (f) display two intermediate states. Figures (c) and (d) display the signal f( ρ
which gives the magnitude and signal of the heat equation leading to ﬁgures (e) and (f). These two
signals describe exactly the positions of the shocks in the further iterations and the ﬁnal state.
g(t) = 1 −3
h(t) = 1 −1
Proof. Let x ∈Ωand h, ρ ∈R+. Then
u(x)−YNFh,ρ(x) = −
Bρ(0) e−(u(x+t)−u(x))2
(u(x+t)−u(x))e−(u(x+t)−u(t))2
We take the Taylor expansion of u(x + t), and the exponential function e−y2. Then,
we take polar coordinates and integrate, obtaining
u(x) −YNFh,ρ(x) ≃
4h2 (u2x + u2y)
xuxx + 2uxuyuxy + u2
A. BUADES, B. COLL AND J.M MOREL
for ρ small enough. By grouping the terms of above expression, we get the desired
The neighborhood ﬁltering method noise can be written as the sum of a diﬀusion
term in the tangent direction uξξ, plus a diﬀusion term in the normal direction, uηη.
The sign and the magnitude of both diﬀusions depend on the sign and the magnitude
of the functions g and h. Both functions can take positive and negative values. Therefore, both diﬀusions can appear as a directional heat equation or directional reverse
heat equation depending on the value of the gradient. As in the one dimensional case,
the algorithm performs like a ﬁltering / enhancing algorithm depending on the value
of the gradient. If B1 =
3 and B2 =
2 respectively denote the zeros of the
functions g and h, we can distinguish the following cases:
• When 0 < |Du| < B2 h
ρ the algorithm behaves like the Perona-Malik ﬁlter
 . In a ﬁrst step, a heat equation is applied but when |Du| > B1 h
normal diﬀusion turns into a reverse diﬀusion enhancing the edges, while the
tangent diﬀusion stays positive.
• When |Du| > B2 h
ρ the algorithm diﬀers from the Perona-Malik ﬁlter. A heat
equation or a reverse heat equation is applied depending on the value of the
gradient. The change of behavior between these two dynamics is marked by
an asymptotical discontinuity leading to an ampliﬁed shock eﬀect.
Fig. 2.2. Denoising experience on a natural image. From left to right and from top to bottom: noisy image (standard deviation 20), gaussian convolution, anisotropic ﬁlter, total variation
minimization, Tadmor et al. iterated total variation, Osher et al. iterated total variation and the
Yaroslavsky neighborhood ﬁlter.
3. Frequency domain ﬁlters. Let u be the original image deﬁned on the grid
I. The image is supposed to be modiﬁed by the addition of a signal independent white
noise N. N is a random process where N(i) are i.i.d, zero mean and have constant
variance σ2. The resulting noisy process depends on the random noise component,
and therefore is modelled as a random ﬁeld V ,
V (i) = u(i) + N(i).
Given a noise observation n(i), v(i) denotes the observed noisy image,
v(i) = u(i) + n(i).
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Let B = {gα}α∈A be an orthogonal basis of R|I|. The noisy process is transformed
VB(α) = uB(α) + NB(α),
VB(α) = ⟨V, gα⟩,
uB(α) = ⟨u, gα⟩,
NB(α) = ⟨N, gα⟩
are the scalar products of V, u and N with gα ∈B. The noise coeﬃcients NB(α)
remain uncorrelated and zero mean, but the variances are multiplied by ∥gα∥2,
E[NB(α)NB(β)] =
gα(m)gβ(n)E[N(m)N(n)]
= ⟨gα, gβ⟩σ2 = σ2∥gα∥2δ[α −β].
Frequency domain ﬁlters are applied independently to every transform coeﬃcient
VB(α) and then the solution is estimated by the inverse transform of the new coef-
ﬁcients. Noisy coeﬃcients VB(α) are modiﬁed to a(α)VB(α). This is a non linear
algorithm because a(α) depends on the value VB(α). The inverse transform yields the
a(α) VB(α) gα.
D is also called a diagonal operator. Let us look for the frequency domain ﬁlter D
which minimizes a certain estimation error. This error is based on the square euclidean
distance, and it is averaged over the noise distribution.
Definition 3.1. Let u be the original image, N a white noise and V = u + N.
Let D be a frequency domain ﬁlter. Deﬁne the risk of D as
r(D, u) = E{||u −DV ||2},
where the expectation is taken over the noise distribution.
The following theorem, which is easily proved, gives the diagonal operator Dinf
that minimizes the risk,
Dinf = arg min
D r(D, u).
Theorem 3.2. The operator Dinf which minimizes the risk is given by the family
{a(α)}α, where
|uB(α)|2 + ∥gα∥2σ2 ,
and the corresponding risk is
rinf (u) =
|uB(α)|2σ2
|uB(α)|2 + ∥gα∥2σ2 .
A. BUADES, B. COLL AND J.M MOREL
The previous optimal operator attenuates all noisy coeﬃcients in order to minimize the risk. If one restricts a(α) to be 0 or 1, one gets a projection operator. In
that case, a subset of coeﬃcients is kept and the rest gets cancelled. The projection
operator that minimizes the risk r(D, u) is obtained by the following family {a(α)}α,
|uB(α)|2 ≥||gα||2σ2
and the corresponding risk is
||gα||2 min(|uB(α)|2, ||gα||2σ2).
Note that both ﬁlters are ideal operators because they depend on the coeﬃcients
uB(α) of the original image, which are not known.
We call, as classical, Fourier
Wiener Filter the optimal operator (3.6) where B is a Fourier Basis. This is an ideal
ﬁlter, since it uses the (unknown) Fourier transform of the original image. By the
use of the Fourier basis (see Figure 3.1), global image characteristics may prevail over
local ones and create spurious periodic patterns. To avoid this eﬀect, the basis must
take into account more local features, as the wavelet and local DCT transforms do.
The search for the ideal basis associated with each image still is open. At the moment,
the way seems to be a dictionary of basis instead of one single basis, .
Fig. 3.1. Fourier Wiener ﬁlter experiment. Top Left: Degraded image by an additive white
noise of σ = 15. Top Right: Fourier Wiener ﬁlter solution. Down: Zoom on three diﬀerent zones
of the solution. The image is ﬁltered as a whole and therefore a uniform texture is spread all over
the image.
3.1. Local adaptive ﬁlters in transform Domain. The local adaptive ﬁlters
have been introduced by L. Yaroslavsky . In this case, the noisy image is
analyzed in a moving window and in each position of the window its spectrum is
computed and modiﬁed. Finally, an inverse transform is used to estimate only the
signal value in the central pixel of the window.
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Let i ∈I be a pixel and W = W(i) a window centered in i. Then the DCT
transform of W is computed and modiﬁed.
The original image coeﬃcients of W,
uB,W (α) are estimated and the optimal attenuation of Theorem 3.2 is applied. Finally,
only the center pixel of the restored window is used.
This method is called the
Empirical Wiener Filter. In order to approximate uB,W (α) one can take averages on
the additive noise model, that is,
E|VB,W (α)|2 = |uB,W (α)|2 + σ2||gα||2.
Denoting by µ = σ||gα||, the unknown original coeﬃcients can be written as
|uB,W (α)|2 = E|VB,W (α)|2 −µ2.
The observed coeﬃcients |vB,W (α)|2 are used to approximate E|VB,W (α)|2, and the
estimated original coeﬃcients are replaced in the optimal attenuation, leading to the
family {a(α)}α, where
a(α) = max
0, |vB,W (α)|2 −µ2
|vB,W (α)|2
Denote by EWFµ(i) the ﬁlter given by the previous family of coeﬃcients. The method
noise of the EWFµ(i) is easily computed, as proved in the following theorem.
Theorem 3.3. Let u be an image deﬁned in a grid I and let i ∈I be a pixel.
Let W = W(i) be a window centered in the pixel i. Then the method noise of the
EWFµ(i) is given by
u(i) −EWFµ(i) =
vB,W(α) gα(i) +
|vB,W(α)|2 vB,W(α) gα(i).
where Λ = {α | |vB,W(α)| < µ}.
The presence of an edge in the window W will produce a great amount of large
coeﬃcients and as a consequence, the cancelation of these coeﬃcients will produce
oscillations. Then, spurious cosines will also appear in the image under the form of
chessboard patterns, see Figure 3.2.
3.2. Wavelet thresholding. Let B = {gα}α∈A be an orthonormal basis of
wavelets . Let us discuss two procedures modifying the noisy coeﬃcients, called
wavelet thresholding methods (D. Donoho et al. ). The ﬁrst procedure is a projection operator which approximates the ideal projection (3.6). It is called hard thresholding, and cancels coeﬃcients smaller than a certain threshold µ,
|vB(α)| > µ
Let us denote this operator by HWTµ(v). This procedure is based on the idea that
the image is represented with large wavelet coeﬃcients, which are kept, whereas the
noise is distributed across small coeﬃcients, which are canceled. The performance
of the method depends on the capacity of approximating u by a small set of large
coeﬃcients. Wavelets are for example an adapted representation for smooth functions.
Theorem 3.4. Let u be an image deﬁned in a grid I. The method noise of a
hard thresholding HWTµ(u) is
u −HWTµ(u) =
{α||uB(α)|<µ}
A. BUADES, B. COLL AND J.M MOREL
Unfortunately, edges lead to a great amount of wavelet coeﬃcients lower than the
threshold, but not zero. The cancelation of these wavelet coeﬃcients causes small
oscillations near the edges, i.e.
a Gibbs-like phenomenon.
Spurious wavelets can
also be seen in the restored image due to the cancelation of small coeﬃcients : see
Figure 3.2. This artifact will be called: wavelet outliers, as it is introduced in . D.
Donoho showed that these eﬀects can be partially avoided with the use of a soft
thresholding,
vB(α)−sgn(vB(α))µ
|vB(α)| ≥µ
which will be denoted by SWTµ(v). The continuity of the soft thresholding operator
better preserves the structure of the wavelet coeﬃcients, reducing the oscillations
near discontinuities. Note that a soft thresholding attenuates all coeﬃcients in order
to reduce the noise, as an ideal operator does. As we shall see at the end of this
paper, the L2 norm of the method noise is lessened when replacing the hard by a soft
threshold. See Figures 3.2 and 6.3 for a comparison of the both method noises.
Theorem 3.5. Let u be an image deﬁned in a grid I. The method noise of a soft
thresholding SWTµ(u) is
u −SWTµ(u) =
{α||uB(α)|<µ}
uB(α)gα + µ
{α||uB(α)|>µ}
sgn(uB(α)) gα
A simple example can show how to ﬁx the threshold µ. Suppose the original
image u is zero, then vB(α) = nB(α), and therefore the threshold µ must be taken
over the maximum of noise coeﬃcients to ensure their suppression and the recovery
of the original image. It can be shown that the maximum amplitude of a white noise
has a high probability of being smaller than σ
2 log |I|. It can be proved that the
risk of a wavelet thresholding with the threshold µ = σ
2 log |I| is near the risk rp
of the optimal projection, see .
Theorem 3.6. The risk rt(u) of a hard or soft thresholding with the threshold
2 log |I| is such that for all |I| ≥4
rt(u) ≤(2 log |I| + 1)(σ2 + rp(u)).
The factor 2 log |I| is optimal among all the diagonal operators in B, that is,
E{||u −DV ||2}
σ2 + rp(u)
2log|I| = 1.
In practice the optimal threshold µ is very high and cancels too many coeﬃcients
not produced by the noise. A threshold lower than the optimal is used in the experiments and produces much better results, see Figure 3.2. For a hard thresholding the
threshold is ﬁxed to 3 ∗σ. For a soft thresholding this threshold still is too high ; it
is better ﬁxed at 3
3.3. Translation invariant wavelet thresholding. R. Coifman and D. Donoho
 improved the wavelet thresholding methods by averaging the estimation of all translations of the degraded signal. Calling vp(i) the translated signal v(i−p), the wavelet
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
coeﬃcients of the original and translated signals can be very diﬀerent, and they are
not related by a simple translation or permutation,
B(α) = ⟨v(n −p), gα(n)⟩= ⟨v(n), gα(n + p)⟩.
The vectors gα(n + p) are not in general in the basis B = {gα}α∈A, and therefore the
estimation of the translated signal is not related to the estimation of v. This new
algorithm yields an estimate ˆup for every translated vp of the original image,
ˆup = Dvp =
The translation invariant thresholding is obtained by averaging all these estimators
after a translation in the inverse sense,
ˆup(i + p),
and will be denoted by TIWT(v).
The Gibbs eﬀect is considerably reduced by the translation invariant wavelet
thresholding, (see Figure 3.2), because the average of diﬀerent estimations of the image
reduces the oscillations. This is therefore the version we shall use in the comparison
section. Recently, S. Durand and M. Nikolova have actually proposed an eﬃcient
variational method ﬁnding the best compromise to avoid the three common artifacts
in TV methods and wavelet thresholding, namely the stair-casing, the Gibbs eﬀect and
the wavelet outliers. Unfortunately, we couldn’t draw the method into the comparison.
Fig. 3.2. Denoising experiment on a natural image. From left to right and from top to bottom:
noisy image (standard deviation 20), Fourier Wiener ﬁlter (ideal ﬁlter), the DCT empirical Wiener
ﬁlter, the wavelet hard thresholding, the soft wavelet thresholding and the translation invariant
wavelet hard thresholding.
4. Statistical neighborhood approaches. The methods we are going to consider are very recent attempts to take advantage of an image model learned from the
image itself. More speciﬁcally, these denoising methods attempt to learn the statistical relationship between the image values in a window around a pixel and the pixel
value at the window center.
A. BUADES, B. COLL AND J.M MOREL
4.1. DUDE, a universal denoiser. The recent work by Ordentlich et al. 
has led to the proposition of a “universal denoiser” for digital images. The authors
assume that the noise model is fully known, namely the probability transition matrix
Π(a, b) , where a, b ∈A, the ﬁnite alphabet of all possible values for the image. In
order to ﬁx ideas, we shall assume as in the rest of this paper that the noise is additive
gaussian, in which case one simply has Π(a, b) =
2πσe−(a−b)2
for the probability of
observing b when the real value was a. The authors also ﬁx an error cost Λ(a, b)
which, to ﬁx ideas, we can take to be a quadratic function Λ(a, b) = (a −b)2, namely
the cost of mistaking a for b.
The authors ﬁx a neighborhood shape, say, a square discrete window deprived of
its center i, ˜Ni = Ni \ {i} around each pixel i. Then the question is : once the image
has been observed in the window ˜Ni, what is the best estimate we can make from the
observation of the full image ?
The authors propose the following algorithm:
• Compute, for each possible value b of u(i) the number of windows Nj in
the image such the restrictions of u to ˜Nj and ˜Ni coincide and the observed
value at the pixel j is b. This number is called m(b, Ni) and the line vector
(m(b, Ni))b∈A is denoted by m(Ni).
• Then, compute the denoised value of u at i as
˜u(i) = arg min
b∈A m(Ni)Π−1(Λb ⊗Πu(i)),
where w ⊗v = (w(b)v(b)) denotes the vector obtained by multiplying each
component of u by each component of v, u(i) is the observed value at i, and
we denote by Xa the a-column of a matrix X.
The authors prove that this denoiser is universal in the sense “of asymptotically
achieving, without access to any information on the statistics of the clean signal, the
same performance as the best denoiser that does have access to this information”. In
 the authors present an implementation valid for binary images with an impulse
noise, with excellent results. The reason of these limitations in implementation are
clear : ﬁrst the matrix Π is of very low dimension and invertible for impulse noise. If
instead we consider as above a gaussian noise, then the application of Π−1 amounts
to deconvolve a signal by a gaussian, which is a rather ill-conditioned method. All the
same, it is doable, while the computation of m certainly is not for a large alphabet, like
the one involved in grey tone images (256 values). Even supposing that the learning
window Ni has the minimal possible size of 9, the number of possible such windows is
about 2569 which turns out to be much larger than the number of observable windows
in an image (whose typical size amounts to 106 pixels).
Actually, the number of
samples can be made signiﬁcantly smaller by quantizing the grey level image and by
noting that the window samples are clustered. Anyway, the direct observation of the
number m(Ni) in an image is almost hopeless, particularly if it is corrupted by noise.
4.2. The UINTA algorithm. Awate and Whitaker have proposed a method
whose principles stand close to the the NL-means algorithm, since, as in Dude, the
method involves comparison between subwindows to estimate a restored value. The
objective of the algorithm ”UINTA, for unsupervised information theoretic adaptive
ﬁlter”, is to denoise the image decreasing the randomness of the image. The algorithm
proceeds as follows:
• Assume that the (2d + 1) × (2d + 1) windows in the image are realizations of
a random vector Z. The probability distribution function of Z is estimated
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
from the samples in the image,
Gσ(z −zi),
where z ∈R(2d+1)×(2d+1), Gσ is the gaussian density function in dimension
n with variance σ2,
2 σ e−||x||2
and A is a random subset of windows in the image.
• Then the authors propose an iterative method which minimizes the entropy
of the density distribution,
Ep log p(Z) = −
p(z) log p(z)dz.
This minimization is achieved by a gradient descent algorithm of the previous
energy function.
The denoising eﬀect of this algorithm can be understood, as it forces the probability density to concentrate. Thus, groups of similar windows tend to assume a more
and more similar conﬁguration which is less noisy. The diﬀerences of this algorithm
with NL-means are patent, however. This algorithm creates a global interaction between all windows. In particular, it tends to favor big groups of similar windows and
to remove small groups. In that extent, it is a global homogenization process and is
quite valid if the image consists of a periodic or quasi periodic texture, as is patent
in the successful experiments shown in this paper. The spirit of this method is to
deﬁne a new, information theoretically oriented scale space. In that sense, the gradient descent must be stopped before a steady state. The time at which the process is
stopped gives us the scale of randomness of the ﬁltered image.
5. Non local means algorithm (NL-means). The local smoothing methods
and the frequency domain ﬁlters aim at a noise reduction and at a reconstruction of
the main geometrical conﬁgurations but not at the preservation of the ﬁne structure,
details and texture. Due to the regularity assumptions on the original image of previous methods, details and ﬁne structures are smoothed out because they behave in
all functional aspects as noise. The NL-means algorithm we shall now discuss tries to
take advantage of the high degree of redundancy of any natural image. By this, we
simply mean that every small window in a natural image has many similar windows
in the same image. This fact is patent for windows close by, at one pixel distance
and in that case we go back to a local regularity assumption. Now in a very general
sense inspired by the neighborhood ﬁlters, one can deﬁne as “neighborhood of a pixel
i” any set of pixels j in the image such that a window around j looks like a window
around i. All pixels in that neighborhood can be used for predicting the value at i,
as was ﬁrst shown in for 2D images. This ﬁrst work has inspired many variants
for the restoration of various digital objects, in particular 3D surfaces . The fact
that such a self-similarity exists is a regularity assumption, actually more general and
more accurate than all regularity assumptions we have considered in section 2. It also
generalizes a periodicity assumption of the image.
Let v be the noisy image observation deﬁned on a bounded domain Ω⊂R2 and
let x ∈Ω. The NL-means algorithm estimates the value of x as an average of the
A. BUADES, B. COLL AND J.M MOREL
values of all the pixels whose gaussian neighborhood looks like the neighborhood of
NL(v)(x) =
(Ga∗|v(x+.)−v(y+.)|2)(0)
where Ga is a Gaussian kernel with standard deviation a, h acts as a ﬁltering parameter and C(x) =
Ωe−(Ga∗|v(x+.)−v(z+.)|2)(0)
dz is the normalizing factor. We recall
(Ga ∗|v(x + .) −v(y + .)|2)(0) =
R2 Ga(t)|v(x + t) −v(y + t)|2dt.
Since we are considering images deﬁned on a discrete grid I, we shall give a discrete
description of the NL-means algorithm and some consistency results. This simple and
generic algorithm and its application to the improvement of the performance of digital
cameras is the object of an European patent application .
5.1. Description. Given a discrete noisy image v = {v(i) | i ∈I}, the estimated
value NL(v)(i) is computed as a weighted average of all the pixels in the image,
NL(v)(i) =
w(i, j)v(j),
where the weights {w(i, j)}j depend on the similarity between the pixels i and j, and
satisfy the usual conditions 0 ≤w(i, j) ≤1 and P
j w(i, j) = 1.
In order to compute the similarity between the image pixels, we deﬁne a neighborhood system on I.
Definition 5.1 (Neighborhoods).
A neighborhood system on I is a family
N = {Ni}i∈I of subsets of I such that for all i ∈I,
(i) i ∈Ni,
(ii) j ∈Ni ⇒i ∈Nj.
The subset Ni is called the neighborhood or the similarity window of i. We set ˜
The similarity windows can have diﬀerent sizes and shapes to better adapt to the
image. For simplicity we will use square windows of ﬁxed size. The restriction of v to
a neighborhood Ni will be denoted by v(Ni),
v(Ni) = (v(j), j ∈Ni).
The similarity between two pixels i and j will depend on the similarity of the
intensity gray level vectors v(Ni) and v(Nj). The pixels with a similar grey level
neighborhood to v(Ni) will have larger weights in the average, see Figure 5.1.
In order to compute the similarity of the intensity gray level vectors v(Ni) and
v(Nj), one can compute a gaussian weighted Euclidean distance, ∥v(Ni) −v(Nj)∥2
Efros and Leung showed that the L2 distance is a reliable measure for the comparison of image windows in a texture patch.
Now, this measure is so much the
more adapted to any additive white noise as such a noise alters the distance between
windows in a uniform way. Indeed,
E||v(Ni) −v(Nj)||2
2,a = ||u(Ni) −u(Nj)||2
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Fig. 5.1. q1 and q2 have a large weight because their similarity windows are similar to that
of p. On the other side the weight w(p,q3) is much smaller because the intensity grey values in the
similarity windows are very diﬀerent.
where u and v are respectively the original and noisy images and σ2 is the noise
variance. This equality shows that, in expectation, the Euclidean distance preserves
the order of similarity between pixels. So the most similar pixels to i in v also are
expected to be the most similar pixels to i in u. The weights associated with the
quadratic distances are deﬁned by
||v(Ni)−v(Nj )||2
where Z(i) is the normalizing factor Z(i) = P
||v(Ni)−v(Nj )||2
and the parameter
h controls the decay of the exponential function and therefore the decay of the weights
as a function of the Euclidean distances.
5.2. A consistency theorem for NL-means. The NL-means algorithm is
intuitively consistent under stationarity conditions, saying that one can ﬁnd many
samples of every image detail.
In fact, we shall be assuming that the image is a
fairly general stationary random process. Under these assumptions, for every pixel
i, the NL-means algorithm converges to the conditional expectation of i knowing its
neighborhood. In the case of an additive or multiplicative white noise model, this
expectation is in fact the solution to a minimization problem.
Let X and Y denote two random vectors with values on Rp and R respectively. Let
fX, fY denote the probability distribution functions of X, Y and let fXY denote the
joint probability distribution function of X and Y . Let us recall brieﬂy the deﬁnition
of the conditional expectation.
Definition 5.2.
i) Deﬁne the probability distribution function of Y conditioned to X as
f(y | x) =
if fX(x) > 0
for all x ∈Rp and y ∈R.
ii) Deﬁne the conditional expectation of Y given {X = x} as the expectation
with respect to the conditional distribution f(y | x)
E[Y | X = x] =
y f(y | x) dy,
A. BUADES, B. COLL AND J.M MOREL
for all x ∈Rp.
The conditional expectation is a function of X and therefore a new random variable g(X) which is denoted by E[Y | X].
Let now V be a random ﬁeld and N a neighborhood system on I. Let Z denote
the sequence of random variables Zi = {Yi, Xi}i∈I where Yi = V (i) is real valued and
Xi = V ( ˜
Ni) is Rp valued. Recall that ˜
Ni = Ni\{i}.
Let us restrict Z to the n ﬁrst elements {Yi, Xi}n
i=1. Let us deﬁne the function
rn(x) = Rn(x)/ ˆfn(x)
φ(Yi)K(Xi −x
φ is an integrable real valued function, K is a nonnegative kernel and x ∈Rp.
Let X and Y be distributed as X1 and Y1.
Under this form the NL-means
algorithm can be seen as an instance for the exponential operator of the Nadaraya-
Watson estimator . This is an estimator of the conditional expectation r(x) =
E[φ(Y ) | X = x]. Some deﬁnitions are needed for the statement of the main result.
Definition 5.3.
A stochastic process {Zt | t = 1, 2, . . .}, with Zt deﬁned on
some probability space (Ω, A, P), is said to be (strict-sense) stationary if for any ﬁnite
partition {t1, t2, · · · , tn} the joint distributions Ft1,t2,···,tn(x1, x2, · · · , xn) are the same
as the joint distributions Ft1+τ,t2+τ,···,tn+τ(x1, x2, · · · , xn) for any τ ∈N .
In the case of images, this stationary condition amounts to say that as the size
of the image grows, we are able to ﬁnd in the image many similar patches for all the
details of the image. This is a crucial point to understand the performance of the
NL-means algorithm. The following mixing deﬁnition is a rather technical condition.
In the case of images, it amounts to say that regions become more independent as
their distance increases. This is intuitively true for natural images.
Definition 5.4. Let Z be a stochastic and stationary process {Zt | t = 1, 2, · · · , n},
and, for m < n, let Fn
m be the σ −field induced in Ωby the r.v.’s Zj, m ≤j ≤n.
Then, the sequence Z is said to be β −mixing if for every A ∈Fk
1 and every B ∈F∞
|P(A ∩B) −P(A)P(B)| ≤β(n)
with β(n) →0, as n →∞.
The following theorem establishes the convergence of rn to r, see Roussas .
The theorem is established under the stationary and mixing hypothesis of {Yi, Xi}∞
and asymptotic conditions on the decay of φ, β(n) and K. This set of conditions will
be denoted by H and it is more carefully detailed in the Appendix.
Theorem 5.5 (Conditional expectation theorem). Let Zj = {Xj, Yj} for j =
1, 2, · · · be a strictly stationary and mixing process. For i ∈I, let X and Y be distributed as Xi and Yi. Let J be a compact subset J ⊂Rp such that
inf{fX(x); x ∈J} > 0.
Then, under hypothesis H,
sup[ψn|rn(x) −r(x)|; x ∈J] →0
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
where ψn are positive norming factors.
Let v be the observed noisy image and let i be a pixel. Taking for φ the identity,
we see that rn(v( ˜
Ni)) converges to E[V (i) | V ( ˜
Ni) = v( ˜
Ni)] under stationary and
mixing conditions of the sequence {V (i), V ( ˜
In the case where an additive or multiplicative white noise model is assumed, the
next result shows that this conditional expectation is in fact the function of V ( ˜
that minimizes the mean square error with the original ﬁeld U.
Theorem 5.6. Let V, U, N1, N2 be random ﬁelds on I such that V = U + N1 +
g(U)N2, where N1 and N2 are independent white noises. Let N be a neighborhood
system on I . Then,
(i) E[V (i) | V ( ˜
Ni) = x] = E[U(i) | V ( ˜
Ni) = x] for all i ∈I and x ∈Rp.
(ii) The real value E[U(i) | V ( ˜
Ni) = x] minimizes the following mean square
g∗∈R E[(U(i) −g∗)2 | V ( ˜
for all i ∈I and x ∈Rp.
(iii) The expected random variable E[U(i) | V ( ˜
Ni)] is the function of V ( ˜
minimizes the mean square error
E[U(i) −g(V ( ˜
Given a noisy image observation v(i) = u(i) + n1(i) + g(u(i))n2(i), i ∈I, where
g is a real function and n1 and n2 are white noise realizations, then the NL-means
algorithm is the function of v( ˜
Ni) that minimizes the mean square error with the
original image u(i).
5.3. Experiments with NL-means. The NL-means algorithm chooses for each
pixel a diﬀerent average conﬁguration adapted to the image. As we explained in the
previous sections, for a given pixel i, we take into account the similarity between
the neighborhood conﬁguration of i and all the pixels of the image. The similarity
between pixels is measured as a decreasing function of the Euclidean distance of the
similarity windows. Due to the fast decay of the exponential kernel, large Euclidean
distances lead to nearly zero weights, acting as an automatic threshold. The decay of
the exponential function and therefore the decay of the weights is controlled by the
parameter h. Empirical experimentation shows that one can take a similarity window
of size 7 × 7 or 9 × 9 for grey level images and 5 × 5 or even 3 × 3 in color images
with little noise. These window sizes have shown to be large enough to be robust to
noise and at the same time to be able to take care of the details and ﬁne structure.
Smaller windows are not robust enough to noise. Notice that in the limit case, one can
take the window reduced to a single pixel i and get therefore back to the Yaroslavsky
neighborhood ﬁlter. We have seen experimentally that the ﬁltering parameter h can
take values between 10 ∗σ and 15 ∗σ, obtaining a high visual quality solution. In all
experiments this parameter has been ﬁxed to 12∗σ. For computational aspects, in the
following experiments the average is not performed in all the image. In practice, for
each pixel p, we only consider a squared window centered in p and size 21 × 21 pixels.
The computational cost of the algorithm and a fast multiscale version is addressed in
section 5.5.
Due to the nature of the algorithm, the most favorable case for the NL-means is
the periodic case. In this situation, for every pixel i of the image one can ﬁnd a large
A. BUADES, B. COLL AND J.M MOREL
Fig. 5.2. NL-means denoising experiment with a nearly periodic image. Left: Noisy image with
standard deviation 30. Right: NL-means restored image.
set of samples with a very similar conﬁguration, leading to a noise reduction and a
preservation of the original image, see Figure 5.2 for an example.
Fig. 5.3. NL-means denoising experiment with a Brodatz texture image. Left: Noisy image
with standard deviation 30. Right: NL-means restored image. The Fourier transform of the noisy
and restored images show how main features are preserved even at high frequencies.
Another case which is ideally suitable for the application of the NL-means algorithm is the textural case. Texture images have a large redundancy. For a ﬁxed
conﬁguration many similar samples can be found in the image. In Figure 5.3 one
can see an example with a Brodatz texture. The Fourier transform of the noisy and
restored images shows the ability of the algorithm to preserve the main features even
in the case of high frequencies.
NL-means is not only able to restore periodic or texture images. Natural images
also have enough redundancy to be restored. For example in a ﬂat zone, one can
ﬁnd many pixels lying in the same region and similar conﬁgurations. In a straight or
curved edge a complete line of pixels with a similar conﬁguration is found. In addition,
the redundancy of natural images allows us to ﬁnd many similar conﬁgurations in far
away pixels. Figures 5.4 and 5.5 show two examples on two well known standard
processing images. The same algorithm applies to the restoration of color images and
ﬁlms, see Figure 5.6.
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Fig. 5.4. NL-means denoising experiment with a natural image. Left: Noisy image with standard deviation 20. Right: Restored image.
Fig. 5.5. NL-means denoising experiment with a natural image. Left: Noisy image with standard deviation 35. Right: Restored image.
5.4. Testing stationarity : a soft threshold optimal correction. In this
section, we describe a simple and useful statistical improvement of NL-means, with a
technique similar to the wavelet thresholding. The stationarity assumption of Theorem 5.5 is not true everywhere, as each image may contain exceptional, non repeated
structures. Such structures can be blurred out by the algorithm. The NL-means algorithm, and actually every local averaging algorithm, must involve a detection phase
and special treatment of non stationary points. The principle of such a correction
is quite simple and directly derived from other thresholding methods, like the SWT
Let us estimate the original value at a pixel i, u(i), as the mean of the noisy grey
levels v(j) for j ∈J ⊂I. In order to reduce the noise and restore the original value,
pixels j ∈J should have a non noisy grey level u(j) similar to u(i). Assuming this
u(i) + n(j) →u(i) as |J| →∞,
because the average of noise values tends to zero. In addition,
(v(j) −ˆu(i))2 ≃1
n(j)2 →σ2 as |J| →∞.
If the averaged pixels have a non noisy grey level value close to u(i), as expected, then
the variance of the average should be close to σ2. If it is a posteriori observed that
A. BUADES, B. COLL AND J.M MOREL
Fig. 5.6. NL-means denoising experiment with a color image. Left: Noisy image with standard
deviation 15 in every color component. Right: Restored image.
this variance is much larger than σ2, this fact can hardly be caused only by the noise.
This means that NL is averaging pixels whose original grey level values were very
diﬀerent in the original. At those pixels, a more conservative estimate is required,
and therefore the estimated value should be averaged with the noisy one. The next
result tells us how to compute this average.
Theorem 5.7.
Let X and Y be two real random variables. Then, the linear
estimate ˆY ,
ˆY = EY + Cov(X, Y )
minimizes the square error
a,b∈R E[(Y −(a + bX))2].
In our case, X = Y + N where N is independent of Y, with zero mean and variance
V arY + σ2 (X −EY ),
which is equal to
ˆY = EX + max
0, V arX −σ2
This strategy can be applied to correct any local smoothing ﬁlter. However, a good
estimate of the mean and the variance at every pixel is needed. That is not the case
for the local smoothing ﬁlters of Section 2. This strategy can instead be satisfactorily
applied to the NL-means algorithm. As we have shown in the previous section, the
NL-means algorithm converges to the conditional mean. The conditional variance can
be also computed by the NL-means, by taking φ(x) = x2 in Theorem 5.5, and then
computing the variance as EX2 −(EX)2. In Figure 5.7 one can see an application of
this correction.
5.5. Fast multiscale versions.
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Optimal correction experience.
Left: Noisy image.
Middle: NL-means solution.
Right: NL-means corrected solution. The average with the noisy image makes the solution to be
noisier, but details and ﬁne structure are better preserved.
5.5.1. Plain multiscale. Let us now make some comments on the complexity
of NL-means, and how to accelerate it. One can estimate the complexity of an unsophisticated version as follows. If we take a similarity window of size (2f + 1)2 and
since we can restrict the search of similar windows in a larger ”search window” of size
(2s + 1)2, the overall complexity of the algorithm is N 2 × (2f + 1)2 × (2s + 1)2, where
N 2 is the number of pixels of the image. As for practical numbers, we took in all
experiments f = 3, s = 10, so that the ﬁnal complexity is about 49 × 441 × N 2. For
a 512 × 512 images, this takes about 30 seconds on a normal PC. It is quite desirable
to expand the size of the search window as much as possible and it is therefore useful
to give a fast version. This is easily done by a multiscale strategy, with little loss in
Multiscale algorithm
1. Zoom out the image u0 by a factor 2, by a standard Shannon subsampling
procedure. This yields a new image u1. For convenience, we denote by (i, j)
the pixels of u1 and by (2i, 2j) the even pixels of the original image u0.
2. Apply NL-means to u1, so that with each pixel (i, j) of u1, a list of windows
centered in (i1, j1), ..., (ik, jk) is associated.
3. For each pixel of u0, (2i+r, 2j +s) with r, s ∈{0, 1}, we apply the NL-means
algorithm. But instead of comparing with all the windows in a searching zone,
we compare only with the nine neighboring windows of each pixel (2il, 2jl)
for l = 1, · · · , k.
4. This procedure can be applied in a pyramid fashion by subsampling u1 into
u2, and so on. In fact, it is not advisable to zoom down more than twice.
By zooming down by just a factor 2, the computation time is divided by approximately
5.5.2. By blocks. Let I be the 2D grid of pixels and let {i1, . . . , in} be a subset
of I. For each ik, let Wk ⊂I be a neighborhood centered in ik, Wk = ik + Bk, where
Bk gives the size and shape of the neighborhood. Let us suppose that each Wk is a
connected subset of I, such that I = W1 ∪W2 ∪. . . ∪Wn, and where we allow the
intersections between the neighborhoods to be non empty.
Then, for each Wk we deﬁne the vectorial NL-means as
NL(Wk) = 1
v(j + Bk)e−
||v(ik+Bk)−v(j+Bk)||2
where Ck = P
||v(ik+Bk)−v(j+Bk)||2
and h acts as a ﬁltering parameter. We note
that NL(Wk) is a vector of the same size as Wk. In contrast with the NL-means
A. BUADES, B. COLL AND J.M MOREL
algorithm, we compute a non weighted L2 distance, since we restore at the same
time a whole neighborhood and we do not want to give privilege to any point of the
neighborhood.
In order to restore the value at a pixel i, we take into account all Wk containing
i, Ai = {k | i ∈Wk}, and deﬁne
NL(Wk)(i).
The overlapping of these neighborhoods permits a regular transition in the restored
image and avoids block eﬀects.
This variant by blocks of NL-means allows a better adaptation to the local image
conﬁguration of the image and, at the same time, a reduction of the complexity. In
order to illustrate this reduction, let us describe the simplest implementation:
• Let N × N be the size of the image and set ik = (kn, kn) for k = 1, . . . , (N −
• Consider the subset B = {i = (xi, yi) | |xi| ≤m and |yi| ≤m} and Wk =
ik + B for all k. We take m > n/2 in order to have a non empty intersection
between neighboring subsets Wk.
• If we take a squared neighborhood B of size (2m+1)2 and since we can restrict
the search of similar windows in a larger ”search window” of size (2s + 1)2,
the overall complexity of the algorithm is (2m + 1)2 × (2s + 1)2 × ( N−n
Taking n = 9 reduces the computation time of the original algorithm by more
6. Discussion and Comparison.
6.1. NL-means as an extension of previous methods. As was said before,
the gaussian convolution only preserves ﬂat zones while contours and ﬁne structure
are removed or blurred. Anisotropic ﬁlters instead preserve straight edges but ﬂat
zones present many artifacts. One could think of combining these two methods to
improve both results. A gaussian convolution could be applied in ﬂat zones while an
anisotropic ﬁlter could be applied on straight edges. Still, other types of ﬁlters should
be designed to speciﬁcally restore corners or curved edges and texture. The NL-means
algorithm seems to provide a feasible and rational method to automatically take the
best of each mentioned algorithm, reducing for every possible geometric conﬁguration
the image method noise. Although we have not computed explicitly the image method
noise, Figure 6.1 illustrates how the NL-means algorithm chooses in each case a weight
conﬁguration corresponding to one of the previously analyzed ﬁlters. In particular,
according to this set of experiments, we can consider that the consistency results given
in Theorems 2.1, 2.3 and 2.5 are all valid for this algorithm.
In Figure 6.2 we display the probability distributions used to restore a noisy pixel.
The images are the same of Figure 6.1. The comparison of both ﬁgures illustrates
how the probability distribution is perturbed by the addition of a white noise. In
this case, the probability distribution is still adapted to the local conﬁguration of the
image, and the main structures of Figure 6.1 are well preserved.
6.2. Comparison. In this section we shall compare the diﬀerent algorithms
based on three well deﬁned criteria: the method noise, the mean square error and the
visual quality of the restored images. Note that every criterion measures a diﬀerent
aspect of the denoising method.
It is easy to show that only one criterion is not
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Fig. 6.1. On the right-hand side of each pair, we display the weight distribution used to estimate
the central pixel of the left image by the NL-means algorithm. Image a: In ﬂat zones, the weights
are distributed as a convolution ﬁlter (as a gaussian convolution). Image b: In straight edges, the
weights are distributed in the direction of the level line (as the mean curvature motion).
c: On curved edges, the weights favor pixels belonging to the same contour or level line, which is
a strong improvement w.r. to the mean curvature motion. Image d: In a ﬂat neighborhood, the
weights are distributed in a grey level neighborhood (as with a neighborhood ﬁlter). In the case of
Images e and f, the weights are distributed across the more similar conﬁgurations, even though they
are far away from the observed pixel. This shows a behavior similar to a nonlocal neighborhood ﬁlter
or to an ideal Wiener ﬁlter.
enough to judge the restored image, and so one expects a good solution to have a
high performance under the three criteria.
6.2.1. Method noise comparison. In previous sections we have deﬁned the
method noise and computed it for the diﬀerent algorithms. Remember that the denoising algorithm is applied on the original (slightly noisy) image. A ﬁltering parameter,
depending mainly on the standard deviation of the noise, must be ﬁxed for the most
part of algorithms. Let us ﬁx σ = 2.5: we can suppose that any digital image is
aﬀected by this amount of noise since it is not visually noticeable.
The method noise tells us which geometrical features or details are preserved by
the denoising process and which are eliminated. In order to preserve as much features
as possible of the original image the method noise should look as much as possible
like white noise. Figures 6.3-6.5 display the method noise of the diﬀerent methods for
a set of standard natural images. Let us comment them brieﬂy.
• The gaussian ﬁlter method noise highlights all important features of the image
like texture, contours and details. All these features have a large Laplacian
and are therefore modiﬁed by the application of the algorithm, see Theorem
A. BUADES, B. COLL AND J.M MOREL
Fig. 6.2. On the right-hand side of each pair, we display the weight distribution used to estimate
the central pixel of the left image by the NL-means algorithm. Images are obtained by adding a
gaussian noise of standard deviation 12.5 to those of Figure 6.1.
• As announced in Theorem 2.3, the anisotropic ﬁlter method noise displays
the corners and high frequency features. The straight edges are instead not
to be seen : they have a low curvature.
• The Total Variation method modiﬁes most structures and details of the image.
Even straight edges are not well preserved.
• The iterated Total Variation reﬁnements improve the total variation method
noise. Both strategies try to reduce the geometry present in the removed noise
adding it back to the restored image, and therefore reducing the method noise.
• The neighborhood ﬁlter preserves ﬂat objects and contrasted edges, while
edges with a low contrast are not kept. In any case, the contours, texture
and details seem to be well preserved.
• The TIHWT method noise is concentrated on the edges and high frequency
features. These structures lead to coeﬃcients of large enough value but lower
than the threshold. They are removed by the algorithm. The average of the
application to all translated versions reduces the method noise, and structures
are hardly noticeable.
• The TISWT method noise presents much more structure than the hard thresholding. Indeed, the method noise is not only based on the small coeﬃcients
but also on an attenuation of the large ones, leading to a high alteration of
the original image.
• It is diﬃcult to ﬁnd noticeable structure in the DCT empirical Wiener ﬁlter
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Image method noise.
From left to right and from top to bottom: original image,
Gaussian convolution, Mean curvature motion, Total Variation, Tadmor et al. Iterated Total Variation, Osher et al. Total variation, Neighborhood ﬁlter, Soft TIWT, Hard TIWT, DCT empirical
Wiener ﬁlter and NL-means.
method noise. Only some contours are noticeable. In general, this ﬁlter seems
to perform much better than all local smoothing ﬁlters and other frequency
domain ﬁlters. Its results are similar to those of a hard stationary wavelet
thresholding.
• The NL-means method noise looks the more like a white noise.
A. BUADES, B. COLL AND J.M MOREL
Fig. 6.4. Image method noise. From left to right and from top to bottom: original image, Total
Variation, Neighborhood ﬁlter, Hard TIWT, DCT empirical Wiener ﬁlter and NL-means.
6.2.2. Visual quality comparison. As commented before, the visual quality
of the restored image is another important criterion to judge the performance of a
denoising algorithm. Let us present some experiments on a set of standard natural
images. The objective is to compare the visual quality of the restored images, the
non presence of artifacts and the correct reconstruction of edges, texture and ﬁne
structure. Figures 6.6-6.9 present these experiences comparing the visual quality of
previous methods.
Figure 6.6 illustrates the fact that a non local algorithm is needed for the correct
reconstruction of periodic images. Local smoothing ﬁlters and local frequency ﬁlters
are not able to reconstruct the wall pattern. Only the NL-means algorithm and the
global Fourier Wiener ﬁlter reconstruct the original texture. The Fourier Wiener ﬁlter
is based on a global Fourier transform which is able to capture the periodic structure
of the image in a few coeﬃcients. Now, in practice, this is an ideal ﬁlter because
the Fourier transform of the original image is used.
Figure 6.1 e) shows how the
NL-means method chooses the correct weight conﬁguration and explains the correct
reconstruction of the wall pattern.
Figure 6.7 and 6.8 illustrate the diﬃculty of local smoothing ﬁlters for recovering
stochastic patterns. The high degree of noise present in the image makes the local
comparisons of the neighborhood ﬁlter noise dependent. As a consequence, noise and
texture are not well diﬀerentiated. The regularity assumption involved in the bounded
variation makes it unsuitable for the restoration of textures which are ﬁltered as noise.
Iterated Total Variation reﬁnements improve the total variation minimization and
recover part of the excessively ﬁltered texture.
Figure 6.9 shows that the frequency domain ﬁlters are well adapted to the recovery
of oscillatory patterns. Although some artifacts are noticeable in both solutions, the
stripes are well reconstructed.
The DCT transform seems to be more adapted to
this type of texture and stripes are a little better reconstructed. For a much more
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Fig. 6.5. Image method noise. From left to right and from top to bottom: original image, Total
Variation, Neighborhood ﬁlter, Hard TIWT, DCT empirical Wiener ﬁlter and NL-means.
Fig. 6.6. Denoising experience on a periodic image. From left to right and from top to bottom:
noisy image (standard deviation 35), Gauss ﬁltering, Total variation, Neighborhood ﬁlter, Wiener
ﬁlter (ideal ﬁlter), Hard TIWT, DCT empirical Wiener ﬁltering, NL-means algorithm.
detailed comparison between sliding window transform domain ﬁltering methods and
wavelet threshold methods, we refer to . Figure 6.10 shows that although the
total variation minimization is not adapted to the restoration of oscillatory patterns,
the iterated total variation approaches improve the restored image and reduce this
drawback. The NL-means also performs well on this type of texture, due to its high
degree of redundancy.
Finally in Figures 6.11 and 6.12 we show a real denoising experiment. The NLmeans algorithm is applied to a natural image taken in poor light conditions. The
NL-means algorithm seems to denoise the image keeping the main structures and
A. BUADES, B. COLL AND J.M MOREL
Fig. 6.7. Denoising experience on a natural image. From left to right and from top to bottom:
noisy image (standard deviation 35), Neighborhood ﬁlter, Total variation, Tadmor et al. Iterated
Total Variation, Osher et al. Iterated Total Variation and the NL-means algorithm.
Fig. 6.8. Denoising experience on a natural image. From left to right and from top to bottom:
noisy image (standard deviation 35), Neighborhood ﬁlter, Total variation, NL-means algorithm.
6.2.3. Mean square error comparison. The mean square error is the square
of the Euclidean distance between the original image and its estimate. This numerical
quality measurement is the more objective one, since it does not rely on any visual
interpretation. Table 6.1 shows the mean square error of the diﬀerent denoising methods with the images presented in this paper. This error table seems to corroborate
the observations made for the other criteria. One sees for example how the frequency
domain ﬁlters have a lower mean square error than the local smoothing ﬁlters. One
also sees that in presence of periodic or textural structures the Empirical Wiener
Filter based on a DCT transform performs better than the wavelet thresholding, see
also Figures 6.6 and 6.9. Note that, in presence of periodic or stochastic patterns, the
NL-means mean square error is signiﬁcantly more precise than the other algorithms.
Of course, the errors presented in this table cannot be computed in a real denoising
problem. Let us remark that a small error does not guarantee a good visual quality of
the restored image. The mean square error by itself would not be meaningful and all
previous quality criteria are also necessary to evaluate the performance of denoising
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Fig. 6.9. Denoising experience on a natural image. From left to right and from top to bottom:
noisy image (standard deviation 25), the DCT empirical Wiener ﬁlter, Hard TIWT and NL-means
algorithm.
Fig. 6.10. Denoising experience on a natural image. From left to right and from top to bottom:
noisy image (standard deviation 25), the total variation minimization, the Tadmor et al. iterated
total variation, the Osher et al. iterated total variation.
Acknowledgments. We thank Franois Malgouyres, Stphane Mallat, Yves Meyer,
Stanley Osher, Guillermo Sapiro and Luminita Vese for valuable conversations and
Several experiments in this paper have been performed thanks to the
public software libraries MegaWave, Wavelab and Advanced Image Processing Lab.
The set H of assumptions necessary for the statement of Theorem 5.5 are:
(H1) The sequence of random vectors Zi = {Yi, Xi}∞
i=1 where Yi is real valued and
Xi is Rp valued form a strictly stationary sequence.
(H2) The sequence {Zi} is β−mixing and the sequence β(n) satisﬁes the following
summability requirement: β∗= P∞
j=1 β(n) < ∞.
(H3) Let α = α(n) be a positive integer and let µ = µ(n) be the largest positive
integer for which 2αµ ≤n. Then
lim sup[1 + 6e
2 β1/(µ+1)(α)]µ < ∞.
(H4) ∥x∥pK(x) →0, as x →∞, where the norm ∥x∥of x = (x1, . . . , xp) is deﬁned
by ∥x∥= max(|x1|, . . . , |xp|).
i) φ is a real valued Borel function deﬁned on R such that E|φ(Y )|s < ∞
for some s > 1.
|φ(y)|sfXY (x, y)dy; x ∈Rp] = C < ∞.
A. BUADES, B. COLL AND J.M MOREL
Fig. 6.11. Denoising experience on a natural noisy image. Left: Original image, Dei village
(Mallorca). Right: NL-means ﬁltered image.
i) For any point x and x′ in Rp and for some positive constant C(independent
of these points):
|K(x) −K(x′)| ≤C∥x −x′∥
∥x∥K(x)dx < ∞.
(H7) For any point x in Rp, there are positive constants C(x) such that, for all
x′ ∈Rp and with J being as in (H8):
A REVIEW OF IMAGE DENOISING ALGORIHTMS WITH A NEW ONE
Fig. 6.12. Detail of Figure 6.11. Left: Original detail. Right: NL-means ﬁltered detail.
Mean square error table. A smaller mean square error indicates that the estimate is closer to
the original image. The numbers have to be compared on each row. The square of the number on
the left hand column gives the real variance of the noise. By comparing this square to the values
on the same row, it is quickly checked that all studied algorithms indeed perform some denoising. A
sanity check! In general, the comparison performance corroborates the previously mentioned quality
||fX(x) −fX(x′)|| ≤C(x)||x −x′||,
sup[C(x); x ∈J] < ∞.
||ψ(x) −ψ(x′)|| ≤C(x)||x −x′||,
sup[C(x); x ∈J] < ∞,
where r(x) = E[φ(Y ) | X = x] and ψ(x) = r(x)fX(x).
(H8) There exists a compact subset J of Rp such that
inf[fX(x); x ∈J] > 0.