EXPLORATION BY RANDOM NETWORK DISTILLATION
Yuri Burda∗
Harrison Edwards∗
Amos Storkey
Univ. of Edinburgh
Oleg Klimov
We introduce an exploration bonus for deep reinforcement learning methods that
is easy to implement and adds minimal overhead to the computation performed.
The bonus is the error of a neural network predicting features of the observations
given by a ﬁxed randomly initialized neural network. We also introduce a method
to ﬂexibly combine intrinsic and extrinsic rewards. We ﬁnd that the random
network distillation (RND) bonus combined with this increased ﬂexibility enables
signiﬁcant progress on several hard exploration Atari games. In particular we
establish state of the art performance on Montezuma’s Revenge, a game famously
difﬁcult for deep reinforcement learning methods. To the best of our knowledge,
this is the ﬁrst method that achieves better than average human performance on this
game without using demonstrations or having access to the underlying state of the
game, and occasionally completes the ﬁrst level.
INTRODUCTION
Reinforcement learning (RL) methods work by maximizing the expected return of a policy. This
works well when the environment has dense rewards that are easy to ﬁnd by taking random sequences
of actions, but tends to fail when the rewards are sparse and hard to ﬁnd. In reality it is often
impractical to engineer dense reward functions for every task one wants an RL agent to solve. In
these situations methods that explore the environment in a directed way are necessary.
Intrinsic reward
Figure 1: RND exploration bonus over the course of the ﬁrst episode where the agent picks up the
torch (19-21). To do so the agent passes 17 rooms and collects gems, keys, a sword, an amulet, and
opens two doors. Many of the spikes in the exploration bonus correspond to meaningful events: losing
a life (2,8,10,21), narrowly escaping an enemy (3,5,6,11,12,13,14,15), passing a difﬁcult obstacle
(7,9,18), or picking up an object (20,21). The large spike at the end corresponds to a novel experience
of interacting with the torch, while the smaller spikes correspond to relatively rare events that the
agent has nevertheless experienced multiple times. See here for videos.
∗Alphabetical ordering; the ﬁrst two authors contributed equally.
 
Recent developments in RL seem to suggest that solving the most challenging tasks requires processing large numbers of samples obtained from running many copies of the
environment in parallel. In light of this it is desirable to have exploration methods that scale well
with large amounts of experience. However many of the recently introduced exploration methods
based on counts, pseudo-counts, information gain or prediction gain are difﬁcult to scale up to large
numbers of parallel environments.
This paper introduces an exploration bonus that is particularly simple to implement, works well with
high-dimensional observations, can be used with any policy optimization algorithm, and is efﬁcient
to compute as it requires only a single forward pass of a neural network on a batch of experience.
Our exploration bonus is based on the observation that neural networks tend to have signiﬁcantly
lower prediction errors on examples similar to those on which they have been trained. This motivates
the use of prediction errors of networks trained on the agent’s past experience to quantify the novelty
of new experience.
As pointed out by many authors, agents that maximize such prediction errors tend to get attracted
to transitions where the answer to the prediction problem is a stochastic function of the inputs.
For example if the prediction problem is that of predicting the next observation given the current
observation and agent’s action (forward dynamics), an agent trying to maximize this prediction error
will tend to seek out stochastic transitions, like those involving randomly changing static noise on a
TV, or outcomes of random events such as coin tosses. This observation motivated the use of methods
that quantify the relative improvement of the prediction, rather than its absolute error. Unfortunately,
as previously mentioned, such methods are hard to implement efﬁciently.
We propose an alternative solution to this undesirable stochasticity by deﬁning an exploration bonus
using a prediction problem where the answer is a deterministic function of its inputs. Namely we
predict the output of a ﬁxed randomly initialized neural network on the current observation.
Atari games have been a standard benchmark for deep reinforcement learning algorithms since the
pioneering work by Mnih et al. . Bellemare et al. identiﬁed among these games the hard
exploration games with sparse rewards: Freeway, Gravitar, Montezuma’s Revenge, Pitfall!, Private
Eye, Solaris, and Venture. RL algorithms tend to struggle on these games, often not ﬁnding even a
single positive reward.
In particular, Montezuma’s Revenge is considered to be a difﬁcult problem for RL agents, requiring a
combination of mastery of multiple in-game skills to avoid deadly obstacles, and ﬁnding rewards that
are hundreds of steps apart from each other even under optimal play. Signiﬁcant progress has been
achieved by methods with access to either expert demonstrations , special access to the underlying emulator state , or both . However without such aids, progress
on the exploration problem in Montezuma’s Revenge has been slow, with the best methods ﬁnding
about half the rooms . For these reasons we provide extensive ablations of
our method on this environment.
We ﬁnd that even when disregarding the extrinsic reward altogether, an agent maximizing the RND
exploration bonus consistently ﬁnds more than half of the rooms in Montezuma’s Revenge. To
combine the exploration bonus with the extrinsic rewards we introduce a modiﬁcation of Proximal
Policy Optimization ) that uses two value heads for the two reward
streams. This allows the use of different discount rates for the different rewards, and combining
episodic and non-episodic returns. With this additional ﬂexibility, our best agent often ﬁnds 22 out of
the 24 rooms on the ﬁrst level in Montezuma’s Revenge, and occasionally (though not frequently)
passes the ﬁrst level. The same method gets state of the art performance on Venture and Gravitar.
EXPLORATION BONUSES
Exploration bonuses are a class of methods that encourage an agent to explore even when the
environment’s reward et is sparse. They do so by replacing et with a new reward rt = et + it, where
it is the exploration bonus associated with the transition at time t.
To encourage the agent to visit novel states, it is desirable for it to be higher in novel states than
in frequently visited ones. Count-based exploration methods provide an example of such bonuses.
In a tabular setting with a ﬁnite number of states one can deﬁne it to be a decreasing function
of the visitation count nt(s) of the state s. In particular it = 1/nt(s) and it = 1/
nt(s) have
been used in prior work . In non-tabular cases it is
not straightforward to produce counts, as most states will be visited at most once. One possible
generalization of counts to non-tabular settings is pseudo-counts which uses
changes in state density estimates as an exploration bonus. In this way the counts derived from the
density model can be positive even for states that have not been visited in the past, provided they are
similar to previously visited states.
An alternative is to deﬁne it as the prediction error for a problem related to the agent’s transitions.
Generic examples of such problems include forward dynamics and inverse dynamics . Non-generic prediction problems can also be used if specialized information about the
environment is available, like predicting physical properties of objects the agent interacts with . Such prediction errors tend to decrease as the agent collects more experience similar
to the current one. For this reason even trivial prediction problems like predicting a constant zero
function can work as exploration bonuses .
RANDOM NETWORK DISTILLATION
This paper introduces a different approach where the prediction problem is randomly generated.
This involves two neural networks: a ﬁxed and randomly initialized target network which sets the
prediction problem, and a predictor network trained on data collected by the agent. The target network
takes an observation to an embedding f : O →Rk and the predictor neural network ˆf : O →Rk
is trained by gradient descent to minimize the expected MSE ∥ˆf(x; θ) −f(x)∥2 with respect to its
parameters θ ˆ
f. This process distills a randomly initialized neural network into a trained one. The
prediction error is expected to be higher for novel states dissimilar to the ones the predictor has been
trained on.
To build intuition we consider a toy model of this process on MNIST. We train a predictor neural
network to mimic a randomly initialized target network on training data consisting of a mixture of
images with the label 0 and of a target class, varying the proportion of the classes, but not the total
number of training examples. We then test the predictor network on the unseen test examples of
the target class and report the MSE. In this model the zeros are playing the role of states that have
been seen many times before, and the target class is playing the role of states that have been visited
infrequently. The results are shown in Figure 2. The ﬁgure shows that test error decreases as a
function of the number of training examples in the target class, suggesting that this method can be
used to detect novelty. Figure 1 shows that the intrinsic reward is high in novel states in an episode of
Montezuma’s Revenge.
One objection to this method is that a sufﬁciently powerful optimization algorithm might ﬁnd a
predictor that mimics the target random network perfectly on any input (for example the target
network itself would be such a predictor). However the above experiment on MNIST shows that
standard gradient-based methods don’t overgeneralize in this undesirable way.
SOURCES OF PREDICTION ERRORS
In general, prediction errors can be attributed to a number of factors:
1. Amount of training data. Prediction error is high where few similar examples were seen by
the predictor (epistemic uncertainty).
2. Stochasticity. Prediction error is high because the target function is stochastic (aleatoric uncertainty). Stochastic transitions are a source of such error for forward dynamics prediction.
3. Model misspeciﬁcation. Prediction error is high because necessary information is missing,
or the model class is too limited to ﬁt the complexity of the target function.
4. Learning dynamics. Prediction error is high because the optimization process fails to ﬁnd a
predictor in the model class that best approximates the target function.
Factor 1 is what allows one to use prediction error as an exploration bonus. In practice the prediction
error is caused by a combination of all of these factors, not all of them desirable.
For instance if the prediction problem is forward dynamics, then factor 2 results in the ‘noisy-TV’
problem. This is the thought experiment where an agent that is rewarded for errors in the prediction
of its forward dynamics model gets attracted to local sources of entropy in the environment. A TV
showing white noise would be such an attractor, as would a coin ﬂip.
To avoid the undesirable factors 2 and 3, methods such as those by Schmidhuber ; Oudeyer
et al. ; Lopes et al. ; Achiam & Sastry instead use a measurement of how much
the prediction model improves upon seeing a new datapoint. However these approaches tend to be
computationally expensive and hence difﬁcult to scale.
RND obviates factors 2 and 3 since the target network can be chosen to be deterministic and inside
the model-class of the predictor network.
RELATION TO UNCERTAINTY QUANTIFICATION
RND prediction error is related to an uncertainty quantiﬁcation method introduced by Osband et al.
 . Namely, consider a regression problem with data distribution D = {xi, yi}i. In the Bayesian
setting we would consider a prior p(θ∗) over the parameters of a mapping fθ∗and calculate the
posterior after updating on the evidence.
Let F be the distribution over functions gθ = fθ + fθ∗, where θ∗is drawn from p(θ∗) and θ is given
by minimizing the expected prediction error
θ = arg min
E(xi,yi)∼D∥fθ(xi) + fθ∗(xi) −yi∥2 + R(θ),
where R(θ) is a regularization term coming from the prior ).
Osband et al. argue (by analogy to the case of Bayesian linear regression) that the ensemble F
is an approximation of the posterior.
If we specialize the regression targets yi to be zero,
then the optimization problem
arg minθ E(xi,yi)∼D∥fθ(xi) + fθ∗(xi)∥2 is equivalent to distilling a randomly drawn function from
the prior. Seen from this perspective, each coordinate of the output of the predictor and target networks would correspond to a member of an ensemble (with parameter sharing amongst the ensemble),
and the MSE would be an estimate of the predictive variance of the ensemble (assuming the ensemble
is unbiased). In other words the distillation error could be seen as a quantiﬁcation of uncertainty in
predicting the constant zero function.
COMBINING INTRINSIC AND EXTRINSIC RETURNS
In preliminary experiments that used only intrinsic rewards, treating the problem as non-episodic
resulted in better exploration. In that setting the return is not truncated at “game over”. We argue that
this is a natural way to do exploration in simulated environments, since the agent’s intrinsic return
should be related to all the novel states that it could ﬁnd in the future, regardless of whether they all
occur in one episode or are spread over several. It is also argued in that using
episodic intrinsic rewards can leak information about the task to the agent.
We also argue that this is closer to how humans explore games. For example let’s say Alice is playing
a videogame and is attempting a tricky maneuver to reach a suspected secret room. Because the
maneuver is tricky the chance of a game over is high, but the payoff to Alice’s curiosity will be high
if she succeeds. If Alice is modelled as an episodic reinforcement learning agent, then her future
return will be exactly zero if she gets a game over, which might make her overly risk averse. The real
cost of a game over to Alice is the opportunity cost incurred by having to play through the game from
the beginning (which is presumably less interesting to Alice having played the game for some time).
However using non-episodic returns for extrinsic rewards could be exploited by a strategy that ﬁnds a
reward close to the beginning of the game, deliberately restarts the game by getting a game over, and
repeats this in an endless cycle.
It is not obvious how to estimate the combined value of the non-episodic stream of intrinsic rewards
it and the episodic stream of extrinsic rewards et. Our solution is to observe that the return is linear in
Figure 2: Novelty detection on MNIST: a predictor network mimics a randomly initialized target
network. The training data consists of varying
proportions of images from class “0” and a target
class. Each curve shows the test MSE on held out
target class examples plotted against the number
of training examples of the target class (log scale).
Figure 3: Mean episodic return and number
of rooms found by pure exploration agents on
Montezuma’s Revenge trained without access
to the extrinsic reward. The agents explores
more in the non-episodic setting (see also Section 2.3)
the rewards and so can be decomposed as a sum R = RE + RI of the extrinsic and intrinsic returns
respectively. Hence we can ﬁt two value heads VE and VI separately using their respective returns,
and combine them to give the value function V = VE + VI. This same idea can also be used to
combine reward streams with different discount factors.
Note that even where one is not trying to combine episodic and non-episodic reward streams, or
reward streams with different discount factors, there may still be a beneﬁt to having separate value
functions since there is an additional supervisory signal to the value function. This may be especially
important for exploration bonuses since the extrinsic reward function is stationary whereas the
intrinsic reward function is non-stationary.
REWARD AND OBSERVATION NORMALIZATION
One issue with using prediction error as an exploration bonus is that the scale of the reward can
vary greatly between different environments and at different points in time, making it difﬁcult to
choose hyperparameters that work in all settings. In order to keep the rewards on a consistent scale
we normalized the intrinsic reward by dividing it by a running estimate of the standard deviations of
the intrinsic returns.
Observation normalization is often important in deep learning but it is crucial when using a random
neural network as a target, since the parameters are frozen and hence cannot adjust to the scale of
different datasets. Lack of normalization can result in the variance of the embedding being extremely
low and carrying little information about the inputs. To address this issue we use an observation
normalization scheme often used in continuous control problems whereby we whiten each dimension
by subtracting the running mean and then dividing by the running standard deviation. We then clip
the normalized observations to be between -5 and 5. We initialize the normalization parameters by
stepping a random agent in the environment for a small number of steps before beginning optimization.
We use the same observation normalization for both predictor and target networks but not the policy
EXPERIMENTS
We begin with an intrinsic reward only experiment on Montezuma’s Revenge in Section 3.1 to isolate
the inductive bias of the RND bonus, follow by extensive ablations of RND on Montezuma’s Revenge
in Sections 3.2-3.4 to understand the factors that contribute to RND’s performance, and conclude with
a comparison to baseline methods on 6 hard exploration Atari games in Section 3.6. For details of
hyperparameters and architectures we refer the reader to Appendices A.3 and A.4. Most experiments
are run for 30K rollouts of length 128 per environment with 128 parallel environments, for a total of
1.97 billion frames of experience.
PURE EXPLORATION
In this section we explore the performance of RND in the absence of any extrinsic reward. In Section
2.3 we argued that exploration with RND might be more natural in the non-episodic setting. By
comparing the performance of the pure exploration agent in episodic and non-episodic settings we
can see if this observation translates to improved exploration performance.
We report two measures of exploration performance in Figure 3: mean episodic return, and the
number of rooms the agent ﬁnds over the training run. Since the pure exploration agent is not aware
of the extrinsic rewards or number of rooms, it is not directly optimizing for any of these measures.
However obtaining some rewards in Montezuma’s Revenge (like getting the key to open a door)
is required for accessing more interesting states in new rooms, and hence we observe the extrinsic
reward increasing over time up to some point. The best return is achieved when the agent interacts
with some of the objects, but the agent has no incentive to keep doing the same once such interactions
become repetitive, hence returns are not consistently high.
We clearly see in Figure 3 that on both measures of exploration the non-episodic agent performs best,
consistent with the discussion in Section 2.3. The non-episodic setting with γI = 0.999 explores
more rooms than γI = 0.99, with one of the runs exploring 21 rooms. The best return achieved by 4
out 5 runs of this setting was 6,700.
COMBINING EPISODIC AND NON-EPISODIC RETURNS
In Section 3.1 we saw that the non-episodic setting resulted in more exploration than the episodic
setting when exploring without any extrinsic rewards. Next we consider whether this holds in the case
where we combine intrinsic and extrinsic rewards. As discussed in Section 2.3 in order to combine
episodic and non-episodic reward streams we require two value heads. This also raises the question
of whether it is better to have two value heads even when both reward streams are episodic. In Figure
4 we compare episodic intrinsic rewards to non-episodic intrinsic rewards combined with episodic
extrinsic rewards, and additionally two value heads versus one for the episodic case. The discount
factors are γI = γE = 0.99.
(a) RNN policies
(b) CNN policies
Figure 4: Different ways of combining intrinsic and extrinsic rewards. Combining non-episodic
stream of intrinsic rewards with the episodic stream of extrinsic rewards outperforms combining
episodic versions of both steams in terms of number of explored rooms, but performs similarly in
terms of mean return. Single value estimate of the combined stream of episodic returns performs a
little better than the dual value estimate. The differences are more pronounced with RNN policies.
CNN runs are more stable than the RNN counterparts.
In Figure 4 we see that using a non-episodic intrinsic reward stream increases the number of rooms
explored for both CNN and RNN policies, consistent with the experiments in Section 3.1, but that the
difference is less dramatic, likely because the extrinsic reward is able to preserve useful behaviors.
We also see that the difference is less pronounced for the CNN experiments, and that the RNN results
tend to be less stable and perform worse for γE = 0.99.
Contrary to our expectations (Section 2.3) using two value heads did not show any beneﬁt over a
single head in the episodic setting. Nevertheless having two value heads is necessary for combining
reward streams with different characteristics, and so all further experiments use two value heads.
Figure 5: Performance of different discount factors for intrinsic and extrinsic reward streams. A
higher discount factor for the extrinsic rewards
leads to better performance, while for intrinsic
rewards it hurts exploration.
Figure 6: Mean episodic return improves as the
number of parallel environments used for collecting the experience increases for both the CNN
policy (left) and the RNN policy (right). The
runs have processed 0.5,2, and 16B frames.
DISCOUNT FACTORS
Previous experiments 
solving Montezuma’s Revenge using expert demonstrations used a high discount factor to achieve
the best performance, enabling the agent to anticipate rewards far into the future. We compare the
performance of the RND agent with γE ∈{0.99, 0.999} and γI = 0.99. We also investigate the
effect of increasing γI to 0.999. The results are shown in Figure 5.
In Figure 5 we see that increasing γE to 0.999 while holding γI at 0.99 greatly improves performance.
We also see that further increasing γI to 0.999 hurts performance. This is at odds with the results in
Figure 3 where increasing γI did not signiﬁcantly impact performance.
SCALING UP TRAINING
In this section we report experiments showing the effect of increased scale on training. The intrinsic
rewards are non-episodic with γI = 0.99, and γE = 0.999.
To hold the rate at which the intrinsic reward decreases over time constant across experiments with
different numbers of parallel environments, we downsample the batch size when training the predictor
to match the batch size with 32 parallel environments (for full details see Appendix A.4). Larger
numbers of environments results in larger batch sizes per update for training the policy, whereas
the predictor network batch size remains constant. Since the intrinsic reward disappears over time
it is important for the policy to learn to ﬁnd and exploit these transitory rewards, since they act as
stepping-stones to nearby novel states.
Figure 6 shows that agents trained with larger batches of experience collected from more parallel
environments obtain higher mean returns after similar numbers of updates. They also achieve better
ﬁnal performance. This effect seems to saturate earlier for the CNN policy than for the RNN policy.
We allowed the RNN experiment with 32 parallel environments to run for more time, eventually
reaching a mean return of 7,570 after processing 1.6 billion frames over 1.6 million parameter updates.
One of these runs visited all 24 rooms, and passed the ﬁrst level once, achieving a best return of
17,500. The RNN experiment with 1024 parallel environments had mean return of 10,070 at the end
of training, and yielded one run with mean return of 14,415.
RECURRENCE
Montezuma’s Revenge is a partially observable environment even though large parts of the game state
can be inferred from the screen. For example the number of keys the agent has appears on the screen,
but not where they come from, how many keys have been used in the past, or what doors have been
opened. To deal with this partial observability, an agent should maintain a state summarizing the past,
for example the state of a recurrent policy. Hence it would be natural to hope for better performance
from agents with recurrent policies. Contrary to expectations in Figure 4 recurrent policies performed
worse than non-recurrent counterparts with γE = 0.99. However in Figure 6 the RNN policy with
γE = 0.999 outperformed the CNN counterpart at each scale1. Comparison of Figures 7 and 9 shows
that across multiple games the RNN policy outperforms the CNN more frequently than the other way
COMPARISON TO BASELINES
In this section we compare RND to two baselines: PPO without an exploration bonus and an
alternative exploration bonus based on forward dynamics error. We evaluate RND’s performance on
six hard exploration Atari games: Gravitar, Montezuma’s Revenge, Pitfall!, Private Eye, Solaris, and
Venture. We ﬁrst compare to the performance of a baseline PPO implementation without intrinsic
reward. For RND the intrinsic rewards are non-episodic with γI = 0.99, while γE = 0.999 for both
PPO and RND. The results are shown in Figure 7 for the RNN policy and summarized in Table 1 (see
also Figure 9 for the CNN policy).
Figure 7: Mean episodic return of RNN-based policies: RND, dynamics-based exploration method,
and PPO with extrinsic reward only on 6 hard exploration Atari games. RND achieves state of the art
performance on Gravitar, Montezuma’s Revenge, and Venture, signiﬁcantly outperforming PPO on
the latter two.
In Gravitar we see that RND does not consistently exceed the performance of PPO. However both
exceed average human performance with an RNN policy, as well as the previous state of the art. On
Montezuma’s Revenge and Venture RND signiﬁcantly outperforms PPO, and exceeds state of the art
performance and average human performance. On Pitfall! both algorithms fail to ﬁnd any positive
rewards. This is a typical result for this game, as the extrinsic positive reward is very sparse. On
Private Eye RND’s performance exceeds that of PPO. On Solaris RND’s performance is comparable
to that of PPO.
Next we consider an alternative exploration bonus based on forward dynamics error. There are
numerous previous works using such a bonus . Fortuitously Burda et al. show that
training a forward dynamics model in a random feature space typically works as well as any other
feature space when used to create an exploration bonus. This means that we can easily implement
an apples to apples comparison and change the loss in RND so the predictor network predicts the
random features of the next observation given the current observation and action, while holding ﬁxed
all other parts of our method such as dual value heads, non-episodic intrinsic returns, normalization
1The results in Figure 5 for the CNN policy were obtained as an average of 5 random seeds. When we ran
10 different seeds for the best performing setting for Figure 6 we found a large discrepancy in performance. This
discrepancy is likely explained by the fact that the distribution of results on Montezuma’s Revenge dominated by
effects of discrete choices (such as going left or right from the ﬁrst room), and hence contains a preponderance
of outliers. In addition, the results in Figure 5 were run with an earlier version of our code base and it is possible
that subtle differences between that version and the publicly released one have contributed to the discrepancy.
The results in Figure 6 were reproduced with the publicly released code and so we suggest that future work
compares against these results.
schemes etc. This provides an ablation of the prediction problem deﬁning the exploration bonus,
while also being representative of a class of prior work using forward dynamics error. Our expectation
was that these methods should be fairly similar except where the dynamics-based agent is able to
exploit non-determinism in the environment to get intrinsic reward.
Figure 7 shows that dynamics-based exploration performs signiﬁcantly worse than RND with the
same CNN policy on Montezuma’s Revenge, PrivateEye, and Solaris, and performs similarly on
Venture, Pitfall, and Gravitar. By analyzing agent’s behavior at convergence we notice that in
Montezuma’s Revenge the agent oscillates between two rooms. This leads to an irreducibly high
prediction error, as the non-determinism of sticky actions makes it impossible to know whether, once
the agent is close to crossing a room boundary, making one extra step will result in it staying in
the same room, or crossing to the next one. This is a manifestation of the ‘noisy TV’ problem, or
aleatoric uncertainty discussed in Section 2.2.1. Similar behavior emerges in PrivateEye and Pitfall!.
In Table 1 the ﬁnal training performance for each algorithm is listed, alongside the state of the art
from previous work and average human performance.
Gravitar Montezuma’s Revenge Pitfall! PrivateEye
Avg. Human
Table 1: Comparison to baselines results. Final mean performance for various methods. State of
the art results taken from: (Horgan et al.,
QUALITATIVE ANALYSIS: DANCING WITH SKULLS
By observing the RND agent, we notice that frequently once it obtains all the extrinsic rewards that
it knows how to obtain reliably (as judged by the extrinsic value function), the agent settles into a
pattern of behavior where it keeps interacting with potentially dangerous objects. For instance in
Montezuma’s Revenge the agent jumps back and forth over a moving skull, moves in between laser
gates, and gets on and off disappearing bridges. We also observe similar behavior in Pitfall!. It might
be related to the very fact that such dangerous states are difﬁcult to achieve, and hence are rarely
represented in agent’s past experience compared to safer states.
RELATED WORK
Exploration. Count-based exploration bonuses are a natural and effective way to do exploration
 and a lot of work has studied how to tractably generalize count bonuses to
large state spaces .
Another class of exploration methods rely on errors in predicting dynamics . As discussed in
Section 2.2, these methods are subject to the ‘noisy TV’ problem in stochastic or partially-observable
environments. This has motivated work on exploration via quantiﬁcation of uncertainty or prediction improvement measures .
Other methods of exploration include adversarial self-play , maximizing
empowerment , parameter noise ,
identifying diverse policies , and using ensembles of
value functions .
Montezuma’s Revenge. Early neural-network based reinforcement learning algorithms that were
successful on a signiﬁcant portion of Atari games failed
to make meaningful progress on Montezuma’s Revenge, not ﬁnding a way out of the ﬁrst room
reliably. This is not necessarily a failure of exploration, as even a random agent ﬁnds the key in the
ﬁrst room once every few hundred thousand steps, and escapes the ﬁrst room every few million steps.
Indeed, a mean return of about 2,500 can be reliably achieved without special exploration methods
 .
Combining DQN with a pseudo-count exploration bonus Bellemare et al. set a new state of
the art performance, exploring 15 rooms and getting best return of 6,600. Since then a number of
other works have achieved similar performance , without exceeding it.
Special access to the underlying RAM state can also be used to improve exploration by using it to
hand-craft exploration bonuses .
Even with such access previous work achieves performance inferior to average human performance.
Expert demonstrations can be used effectively to simplify the exploration problem in Montezuma’s
Revenge, and a number of works have achieved performance comparable to or better than that of human
experts. Learning from expert demonstrations beneﬁts from the game’s determinism. The suggested
training method to prevent an agent from simply memorizing the correct
sequence of actions is to use sticky actions (i.e. randomly repeating previous action) has not been
used in these works. In this work we use sticky actions and thus don’t rely on determinism.
Random features. Features of randomly initialized neural networks have been extensively studied
in the context of supervised learning . More recently they have been used in the context of exploration . The work Osband et al. provides motivation for random network
distillation as discussed in Section 2.2.
Vectorized value functions. Pong et al. ﬁnd that a vectorized value function (with coordinates
corresponding to additive factors of the reward) improves their method. Bellemare et al. 
parametrize the value as a linear combination of value heads that estimate probabilities of discretized
returns. However the Bellman backup equation used there is not itself vectorized.
DISCUSSION
This paper introduced an exploration method based on random network distillation and experimentally
showed that the method is capable of performing directed exploration on several Atari games with
very sparse rewards. These experiments suggest that progress on hard exploration games is possible
with relatively simple generic methods, especially when applied at scale. They also suggest that
methods that are able to treat the stream of intrinsic rewards separately from the stream of extrinsic
rewards (for instance by having separate value heads) can beneﬁt from such ﬂexibility.
We ﬁnd that the RND exploration bonus is sufﬁcient to deal with local exploration, i.e. exploring the
consequences of short-term decisions, like whether to interact with a particular object, or avoid it.
However global exploration that involves coordinated decisions over long time horizons is beyond
the reach of our method.
To solve the ﬁrst level of Montezuma’s Revenge, the agent must enter a room locked behind two
doors. There are four keys and six doors spread throughout the level. Any of the four keys can open
any of the six doors, but are consumed in the process. To open the ﬁnal two doors the agent must
therefore forego opening two of the doors that are easier to ﬁnd and that would immediately reward it
for opening them.
To incentivize this behavior the agent should receive enough intrinsic reward for saving the keys to
balance the loss of extrinsic reward from using them early on. From our analysis of the RND agent’s
behavior, it does not get a large enough incentive to try this strategy, and only stumbles upon it rarely.
Solving this and similar problems that require high level exploration is an important direction for
future work.