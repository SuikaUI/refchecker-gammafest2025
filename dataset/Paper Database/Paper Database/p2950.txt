NOTICE WARNING CONCERNING COPYRIGHT RESTRICTIONS:
The copyright law of the United States (title 17, U.S. Code) governs the making
of photocopies or other reproductions of copyrighted material. Any copying of this
document without permission of its author may be prohibited by law.
Near-Optimal Sensor Placements in Gaussian Processes
Carlos Guestrin
Andreas Krause
Ajit Singh
CMU-CS-05-120, CMU-CALD-05-102-
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
When monitoring spatial phenomena selecting the best locations for sensors is a fundamental task. To avoid strong
assumptions, such as fixed sensing radii, and to tackle noisy measurements, Gaussian processes (GPs) are often used
to model the underlying phenomena. A commonly used placement strategy is to select the locations that have highest
entropy with respect to the GP model. Unfortunately, this criterion is indirect, since prediction quality for unsensed
positions is not considered, and thus suboptimal positions are often selected. In this paper, we propose a mutual
information criterion that chooses sensor locations that most reduce uncertainty about unsensed locations. We first
show that choosing a set of A: sensors that maximizes the mutual information is NP-complete. By exploiting the
submodularity of mutual information we propose a polynomial-time algorithm that guarantees a placement within
(1 — 1/e) of the maxima. This algorithm is extended to exploit local structure in the Gaussian process, significantly
improving performance. Finally, we show that the sensor placements chosen by our algorithm can lead to significantly
better predictions through extensive experimental validation on two real-world datasets.
Keywords: Gaussian Processes; Experimental Design; Active Learning; Spatial Learning; Sensor Networks
(a) 54 node sensor network deployment
(b) Temperature correlations
(c) Precipitation correlations
Figure 1: Correlation between a sensor placed on the blue square and other possible locations for: (b) temperature data
from the sensor network deployment in Fig. l(a); (c) precipitation data from measurements made across the Pacific
Northwest, Fig. 6(a).
1 Introduction
When monitoring spatial phenomena, such as temperatures in an indoor environment as shown in Fig. l(a), using a limited number of sensing devices, deciding where to place the sensors is a fundamental task. One approach is to assume
that sensors have a fixed sensing radius and to solve the task as an instance of the art-gallery problem ). In practice, however, this assumption is too strong; sensors make
noisy measurements about the nearby environment, and this "sensing area" is not usually characterized by a regular
disk, as illustrated by the temperature correlations in Fig. l(b). Furthermore, note that correlations can be both positive
and negative, as shown in Fig. l(c). Often, correlations may be too weak to enable prediction from a single sensor,
suggesting the need for combining data from multiple sensors to obtain accurate predictions.
An alternative approach from spatial statistics , making weaker assumptions, is to use a pilot deployment or expert knowledge to learn a Gaussian process (GP) model for the phenomena, a non-parametric generalization
of linear regression that allows to represent uncertainty about the sensed field. The learned GP model can then be used
to predict the effect of placing sensors at particular locations, and thus optimize their positions. This initial GP is, of
course, a rough model, and a sensor placement strategy can be viewed as an inner-loop step for an active learning
algorithm .
Typical sensor placement techniques greedily add sensors where uncertainty about the phenomena is highest, i.e., the
highest entropy location of the GP . Unfortunately, this criterion suffers from a significant flaw: entropy
is an indirect criterion, not considering the prediction quality of the selected placements. The highest entropy set, i.e.,
the sensors that are most uncertain about each other's measurements, is usually characterized by sensor locations that
are as far as possible from each other. Thus, the entropy criterion tends to place sensors along the borders of the area of
interest , e.g., Fig. 2(c). Since a sensor usually provides information about the area around
it, a sensor on the boundary "wastes" sensed infonnation.
In this paper, we address this issue by proposing a new optimization criterion, mutual information, that seeks to find
sensor placements that are most informative about unsensed locations. Our optimization criterion directly measures the
effect of sensor placements on the posterior uncertainty of the GP. We first prove that maximizing mutual information
is an NP-complete problem. Then, by exploiting the fact that mutual information is a submodular function , we design an approximation algorithm that guarantees a constant-factor approximation of the best set
of sensor locations in polynomial time. To the best of our knowledge, no such guarantee exists for other GP-based
sensor placement approaches. Though polynomial, the complexity of our basic algorithm is relatively high - O(kn*)
to select k out of n possible sensor locations. If we trim low covariance entries, exploiting locality in sensing areas,
we reduce the complexity to O(kn).
2 Gaussian processes
Consider, for example, a sensor network, such as the one we deployed as shown in Fig. l(a), that measures a temperature field at 54 discrete locations. In order to predict the temperature at one of these locations from the other sensor
readings, we need the joint distribution over temperatures at the 54 locations. A simple, yet often effective (Deshpande
(a) Temperature prediction using GP
(b) Variance of temperature prediction
(c) Entropy vs. MI placements
Figure 2: Using all sensors (a) Predicted temperature (b) predicted variance. An example of placements chosen using
entropy and mutual information in (c). Diamonds indicate the positions chosen using entropy; squares the positions
chosen using MI.
et al., 2004), approach is to assume that the temperatures have a (multivariate) Gaussian joint distribution. Here, we
have a set of n random variables X with joint distribution:
P(X = X) =
-±<~-^Tr-if—..\
where \i is the mean vector and £ is the covariance matrix. If we consider indexing each variable Xi G X by i, then
we will have a finite set of indices V, in our sensor network example | V| = 5 4 . Interestingly, if we consider any subset
of our random variables, A C V, then their joint distribution will also be Gaussian.
In our sensor network example, we are not just interested in temperatures at sensed locations, but also at locations
where no sensors were placed. In such cases, we can use regression techniques to perform such prediction . Although linear regression often gives excellent predictions, there is usually no notion of uncertainty,
e.g., for Fig. l(a), we are likely to have better temperature estimates at points near existing sensors, than in the two
central areas that were not instrumented. A Gaussian process (GP) is a natural generalization of linear regression that
allows us to consider uncertainty about predictions.
Intuitively, a GP generalizes multivariate Gaussians to consider an infinite number of random variables. In analogy to
the multivariate Gaussian above where the the index set V was finite, we now have an (possibly uncountably) infinite
index set V. In our temperature example, V would be a subset of R2, and each index would correspond to a position
in the lab. GPs have been widely studied, c.f, .
An important property of GPs is that for every finite subset A of the indices V, the joint distribution over these random
variables is Gaussian, e.g., the joint distribution over temperatures at a finite number of sensor locations is Gaussian.
In order to specify this distribution, a GP is associated with a mean function M{-), and a symmetric positive-definite
kernel function /C(-, •), often called the covariance function. In this paper, we do not make other limiting assumptions,
such as /C(-, •) being stationary or isotropic. For each random variable with index u G V, its mean \iu is given by
Analogously, for each pair of indices u, v G V, their covariance auv is given by K{u, v). For simplicity of
notation, we denote the mean vector of some set of variables A by HA* where the entry for element w of /x^ is M{u).
Similarly, we denote their covariance matrix by £AA» where the entry for u, v is IC(u, v).
The GP representation is extremely powerful. For example, if we observe a set of sensor measurements XA corresponding to the finite subset A C V, we can predict the value at every point y G V conditioned on these measurements,
P(Xy \XA)- The distribution of Xy given these observations is a Gaussian whose conditional mean ^y\A and variance
(Ty,A are given by:
') — E JE" 1 Ej
where Y,yA is a covariance vector with one entry for each u G A with value /C(y, ix). Figures Fig. 2(a) and Fig. 2(b)
show the posterior mean and variance derived using these equations on 54 sensors at Intel Labs Berkeley. Note that
two areas in the center of the lab are not instrumented. These areas have higher posterior variance, as expected. An
important property of GPs is that the posterior variance (2) does not depend on the actual observed values XA- Thus,
for a given kernel function, the variances in Fig. 2(b) will not depend on the observed temperatures.
3 Optimizing sensor placements
Usually, we are limited to deploying a small number of sensors, and thus must carefully choose where to place them.
In spatial statistics this is called sampling design: finding the k best sensor locations out of a finite subset V of possible
locations, e.g., out of a grid discretization of R2.
An often used heuristic is to start from an empty set of locations, A = 0, and greedily add placements until |.4| = k.
Usually, at each iteration, the greedy rule used is to add the location y €V\A
that has highest variance according to
Eq. (2) , i.e., the location that we are most uncertain about given the sensors placed
thus far. Since, for a fixed kernel function, the variance does not depend on the observed values, this optimization can
be done before deploying the sensors.
Note that the (differential) entropy of a Gaussian random variable Y conditioned on some set of variables A is a
monotonic function of its variance:
= - \og(2neallA)
= - l o g a ^ + -(log(27r) + 1),
where, with some abuse of notation, we use A to refer to a set of indices and the corresponding set of random variables.
If we define the set of selected locations as A — {Y\,...,
Yk }, using the chain-rule of entropies, we have that:
Thus, we can view the greedy variance heuristic as an approximation to the problem
aigmaxH(A),
that is, finding the set of sensor locations that has maximum joint entropy. This is an intuitive criterion for finding
sensor placements, since the sensors that are most uncertain about each other should cover the space well.
Unfortunately, this entropy criterion suffers from the problem shown in Fig. 2(c), where sensors are placed far apart
along the boundary of the space and information is "wasted". This phenomenon has been noticed previously by
Ramakrishnan et al. , who proposed a weighting heuristic. Intuitively, this problem arises because the entropy
criterion is indirect: rather than considering prediction quality over the space of interest, the entropy criterion only
considers the entropy of the selected sensor locations. In this paper, we propose a new optimization criterion that
addresses this problem: we search for the subset of sensor locations that most significantly reduces the uncertainty
about the estimates in the rest of the space.
More formally, we define our space as a discrete set of locations V = S U U composed of two parts: a set S of possible
positions where we can place sensors, and an additional set U of positions of interest, where no sensor placements are
possible. The goal is to place a set of k sensors that will give us good predictions throughout V. Specifically,
H(V\A)-H(V\A\
that is, the set A that maximally reduces the entropy over the rest of the space V \ A. Note that our criterion H(V \
A) — H(V \A\A)is equivalent to finding the set that maximizes the mutual information I (A] V \ A) between
A and the rest of the space V \ A. On the same simple example in Fig. 2(c), this mutual information criterion
leads to intuitively appropriate central sensor placements that do not have the "wasted information" property of the
entropy criterion. Our experimental results in Sec. 6 further demonstrate the advantages in performance of our mutual
information criterion.
Entropy and mutual information are both hard to optimize. Maximizing either criteria is NP-complete.
Theorem 1 ). Given rational M and rational covariance matrix Eyv over Gaussian random variables V, deciding whether there exists a subset A C V of cardinality k such that H(A) > M is NP-complete.
Theorem 2. Given rational M and a rational covariance matrix Eyv over Gaussian random variables V = S U U,
deciding whether there exists a subset AQS of cardinality k such that I(A; V\A) > M is NP-complete.
Proofs of all results are given in the appendix.
Approximation algorithm
Optimizing our mutual information criterion is an NP-complete problem. We now describe a poly-time algorithm with
a constant-factor approximation guarantee.
4.1 The algorithm
Our algorithm is greedy, simply adding sensors in sequence, choosing the next sensor which provides the maximum
increase in mutual information. More formally, using F(A) = I (A; V \ A), our goal is to greedily select the next
sensor Y that maximizes:
F(A U Y) - F(A) = H{A U 7 ) - H(A U Y\A) - [H(A) - H(A\A U Y)],
= H{A U Y) - H(V) + H(A) - [H{A) - H(V) + H{A U Y)]
H{Y\A)-H{Y\A),
where, to simplify notation, we write A U Y to denote the set A U {Y}, and use A to mean V\(AUY). Note that the
greedy rule for entropy only considers the H(Y\A) part. In contrast, our greedy mutual information trades off uncertainty with — H(Y\A), which forces us to pick a Y that is "central" with respect to the unselected locations A. Using
the definition of conditional entropy in Eq. (3), Algorithm 1 shows our greedy sensor placement algorithm.
4.2 An approximation bound
We now prove that, if the discretization V of locations of interest in the Gaussian process is fine enough, our greedy
algorithm gives a (1 — 1/e) approximation to the optimal sensor placement: If the algorithm returns set A, then
* fl — 1/e)
for some small e > 0.
To prove this result, we use submodularity , a property of set functions that intuitively
represents "diminishing returns": adding a sensor Y when we only have a small set of sensors A gives us more
Input: Covariance matrix £yv» k,V = SUU
Output: Sensor selection ACS
for; = 1 to k do
Y*^- argmax -f—— **
Algorithm 1: Approximation algorithm for maximizing mutual information.
advantage than adding Y to a larger set of sensors A!. Using the "information never hurts" bound, H(Y\A) >
H(Y\A U B) , note that our greedy update rule maximizing H(Y\A) - H{Y\A) implies
F(A' U Y) - F{A!) < F{A U Y) - F{A),
when A C A\ i.e., adding Y to A helps more than adding Y to A!. This is exactly the definition of a submodular
function. Hence we have the following result:
Lemma 3. The function A *-* I(A; V \ A) is submodular. •
A submodular set function F is called nondecreasing if F(A U Y) > F(A) for Y € V. In ,
it is proven that for nondecreasing submodular set functions F with F(0) = 0, the greedy algorithm guarantees a
performance guarantee of (1 - \/e)OPT, where OPT is the value of the optimal subset of size k. This greedy
algorithm is defined by selecting in each step the element Y* = argmaxy F(A L)Y) - F(A). This is exactly the
algorithm we propose in the previous section.
It is clear that F(0) = 7(0; V) = 0, as required by Nemhauser et al. . However, the monotonicity of our
objective function is not apparent, since F(V) = J( V, 0) = 0, our objective function will increase and then decrease,
and, thus, is not monotonic. Fortunately, the proof of Nemhauser et al. does not use monotonicity for all
possible sets, it is sufficient to prove that F is monotonic for all sets of size up to 2k. Intuitively, mutual information is
not monotonic when the set of sensor selected locations approaches V. If the discretization level is significantly larger
than 2k points, then mutual information should meet the conditions of the proof of Nemhauser et al. .
Thus the heart of our analysis of Algorithm 1 will be to prove that if the discretization of the Gaussian process is fine
enough, then mutual information is approximately non-decreasing for sets of size up to 2k. More precisely we prove
the following result:
Lemma 4. Let Gbea Gaussian process on a compact subset C ofW71 with a positive-definite, continuous covariance
kernel K : C x C —• RQ. Assume the sensors have a measurement error with variance at least a2. Then, for any
e > 0, and any finite maximum number k of sensors to place there exists a discretization V = S U U, S and U having
mesh widths such that YY eV\A,F{AUY) - F(A) > -e for all AC S, \A\ <2k.
If the covariance function is Lipschitz-continuous, such as the Gaussian RBF kernel, the following corollary gives a
bound on the required discretization level with respect to the Lipschitz constant:
Corollary S.IffC is Lipschitz-continuous with constant L, then the required discretization is
6< AkLM {a2 + 2k2 M + 6k2 a2)'
where M = max x €c £(x, x), for e < min(M, 1).
Corollary 5 guarantees that for any e > 0, a polynomial discretization level is sufficient to guarantee that mutual
information is e—approximately non-decreasing. These bounds on the discretization are, of course, worst case bounds
considering sensor placements that are arbitrarily close to each other. We expect the bounds to be very loose in the
situations that arise during normal operation of the greedy algorithm, since the algorithm is unlikely to place sensors
at such a close proximity.
Combining our Lemmas 3 and 4 with the Nemhauser et al. result, we obtain our constant-factor approximation
bound on the quality of the sensor placements obtained by our algorithm:
Theorem 6. Under the assumptions of Lemma 4, Algorithm 1 is guaranteed to select a set Aofk sensors for which
I(A; V \ A) > (1 - l/e)(OPT - he),
where OPT is the value of the optimal placement.
Note that our bound has two implications: First, it shows that our greedy algorithm has a guaranteed minimum performance level of 1 — 1/e when compared to the optimal solution. Second, our approach also provides an upper-bound
on the value of the optimal placement, which can be used to bound the quality of the placements by other heuristic
approaches, such as local search, that may perform better than our greedy algorithm on specific problems.
In many real-world settings, the cost of placing a sensor depends on the specific location. Such cases can often be
formalized by specifying a total budget, and the task is to select placements whose total cost is within our budget.
We have recently extended the submodular function maximization approach of Nemhauser et al. to address
this budgeted case . The combination of the analysis in this paper with this new result
also yields a constant-factor (1 — 1/e) approximation guarantee for the sensor placement problem with non-uniform
4.3 A note on maximizing the entropy
As noted by Ko et al. , entropy is also a submodular set function, suggesting a possible application of the result
of Nemhauser et al. to the entropy criterion for sensor placement The corresponding greedy algorithm adds
the sensor Y maximizing H(A U Y) — H(A) = H(Y\A). Unfortunately, our analysis of approximate monotonicity
does not extend to the entropy case: Consider H(Y\A) for A = {Z}, for sufficiently small measurement noise cr2,
we show that H(Y\Z) can become arbitrarily negative as the mesh width of the discretization decreases. Thus, (even
approximate) non-decreasingness does not hold for entropy, suggesting that the direct application of the result of
Nemhauser et al. is not possible. More precisely, our negative result about the entropy criterion is:
Remark 7. Under the same assumptions as in Lemma 4, for any e > 0, there exists a mesh discretization width S > 0
such that entropy violates the monotonicity criteria by at least s, if a2 < -^.
5 Local kernels
Greedy updates for both entropy and mutual information require the computation of conditional entropies using
Eq. (3), which involves solving a system of |*4| linear equations. For entropy maximization, where we consider
H(Y\A) alone, the complexity of this operation is O(k3). To maximize the mutual information, we also need H(Y\A)
requiring <9(n3), for n = |V|. Since we need to recompute the score of all possible position at every iteration of
Algorithm 1, the complexity of our greedy approach for selecting k sensors is O(kn4), which is not computationally
feasible for very fine discretizations (large n). We address this issue by exploiting locality in the kernel function: First,
we note that, for many GPs, correlation decreases exponentially with the distance between points. Often, variables
which are far apart are actually independent These weak dependencies can be modeled using a covariance function
/C for which /C(x, •) has compact support, i.e., that has non-zero value only for a small portion of the space. For
example, consider the following isotropic covariance function proposed by Storkey :
(27r-A)(l + (cosA)/2) +
for A <2?r and 0 otherwise, where A=(3\\x — y H2, for /? > 0. This covariance function closely resembles the Gaussian
kernel /C(x,y) = exp(—(3\\x — 2/111/(2^)) as shown in Fig. 3, but has zero values for distances larger than 2TT//3.
Even if the covariance function does not have compact support, it can be appropriate to compute H(Y \B) « H(Y \B)
where B results from removing all elements X from B for which | K,(X, Y)\ < e for some small value of e. This
---Gaussian
Euclidean distance
Figure 3: Comparison of local and Gaussian kernels
Input: Covariance EVv, k,V = SUU,e>0
Output: Sensor selection ACS
foreach Y € S do
SY^H(Y)-H£(Y\V\Y)',
forj = 1 to k do
Y * <— arg maxy 5y;
Algorithm 2: Approximation algorithm for maximising mutual information using local kernels
truncation is motivated by noting that:
aY\B\X ~ aY\B ^
This implies that the decrease in entropy H(Y\B\X)- H(Y\B) is bounded by e2/(a2ajc) (using a similar argument
as the proof of Lemma 4), assuming that each sensor has independent Gaussian measurement error of at least a2. The
total decrease of entropy H(Y\B) - H(Y\B) is bounded by ne2/a*. This truncation allows to compute H(Y\A)
much more efficiently, at the expense of this small absolute error. In the special case of isotropic covariance functions,
the number d of variables X with fC(X, Y) > e can be computed as a function of the discretization and the covariance
kernel. This reduces the complexity of computing H(Y \A) from O(n3) to O(d?), which is a constant
Our truncation approach leads to the more efficient optimization algorithm shown in Algorithm 2. Here, He refers to
the truncated computation of entropy as described above, and N(Y*; e) < d refers to the set of elements X e S for
which | JC(Y*, X)\ > e. Using this approximation, our algorithm is significantly faster: Initialization (Line 1) requires
O(nd3) operations. For each one of the k iterations, finding the next sensor (Line 2) requires O(n) comparisons, and
adding the new sensor Y* can only change the score of its neighbors (N(Y*; e) < d), thus Line 3 requires O(d • d3)
operations. The total running time of Algorithm 2 is O(nd3 + kn + JW4), which can be significantly lower than the
O(kn4) operations required by Algorithm 1. We summarize our analysis with the following theorem:
Theorem 8. Under the assumptions of Lemma 4, guaranteeing s\-approximate non-decreasingness and truncation
parameter e2, Algorithm 2 selects A^S such that
; V\A)>(1-
l/e)(OPT - kex
- 2kne2/crA),
in time O(nd3 + nk + kd4).
It is possible to slightly improve the performance of Algorithm 2 under certain conditions by using a priority queue
to maintain the advantages 5Y . Using for example a Relaxed Heaps data structure, the running time can be decreased
Mutual Information
Mutual Information
Mutual Information
Mutual Information
(a) Morning RMS
(b) Noon RMS
(c) Morning log-likelihood
(d) M?on log-likelihood
Figure 4: Prediction error on test data for temperatures in sensor network deployment
(a) Comparison of heuristics
(b) Objective values
(c) Isotropic model
(d) Predicted variance
(e) Temperature (entropy)
(f) Temperature (mutual inf.)
(g) Variance (entropy)
(h) Variance (MI)
Figure 5: Comparison of predictive behavior of subsets selected using mutual information and entropy.
to O(nd3 + kdlogn + fcd4): Line 1 uses the insert operation with complexity O(l), Line 2 calls deletemax with
complexity O(log n), and Line 3 uses delete and insert, again with complexity O(log n). This complexity improves
on Algorithm 2 if d log n < n . This assumption is frequently met in practice, since d can be considered a constant as
the size n of the sensing area grows.
6 Experiments
6.1 Indoor temperature measurements
In our first set of experiments, we analyzed temperature measurements from the sensor network shown in Fig. l(a). We
used our algorithms to learn informative subsets of the sensors during two times of the day: between 8 am and 9 am and
between 12 pm and 1 pm. Our training data consisted of samples collected at 30 sec. intervals on 5 consecutive days
 , the testing data consisted of the corresponding samples on the two following days.
In our initial experiments, we used an estimate of the empirical covariance matrix as the input to the algorithms.
We first compared the mutual information of the sets selected by our greedy algorithm to random selections and to
a hill climbing method that uses a pairwise exchange heuristic. Fig. 5(a) shows that the greedy algorithm provided
significantly better results than the random selections, and even the maximum of a hundred random placements did
not reach the quality of the greedy placements. Furthermore, we enhanced the random and greedy selections with the
pairwise exchange (PE) heuristic, which iteratively finds exchanges improving the mutual information score. Fig. 5(b)
presents objective values of these enhanced selection methods for a subset size of 12, for which the maximum over
100 random selections enhanced with PE actually exceeded the greedy score (unlike with most other subset sizes,
where random + PE did about as well as our algorithm). Typically, the objective values of random + PE, greedy + PE
and greedy did not differ much. Note that as mentioned in Sec. 4, the performance guarantee for the greedy algorithm
always provides an online approximation guarantee for the other heuristics.
• Mututf Information
o Variant*
(a) Placements of rain sensors
(b) Precipitation RMS
(c) Running time
(d) Approximation error
Figure 6: Placements (diamonds correspond to entropy, squares to MI), prediction error and running times for precipitation data.
Secondly, we computed the greedy subsets of sizes up to 30, using entropy and mutual information as objective functions. For testing the prediction accuracy, we provided the sensor measurements of the selected sensors from the
test set, and used Gaussian inference to predict the temperature at the remaining sensor locations. We only let the
algorithms choose from the locations where sensors are actually deployed in order to have test set data available for
comparison. Figures 4(a) and 4(b) show the average root-mean-squares error (RMS) for these predictions, whereas
Figures 4(c) and 4(d) show the average log-likelihoods. Mutual information exhibited superior performance in comparison to the entropy heuristic for increasing set sizes.
To gain further insight into the qualitative behavior of the selection criteria we learned a GP model using all sensors
over one hour starting at noon. The model was fit with a isotropic Gaussian kernel and quadratic trend for the mean,
using the geoR Toolkit . Figures 5(c) and 5(d) show the posterior mean and variance for
the model. Using our algorithms, 22 sensors were chosen using the entropy and mutual information criteria. For each
set of selected sensors, additional models were trained using only the measurements of the selected sensors. Predicted
temperature surfaces for the entropy and mutual information configurations are presented in Figures 5(e) and 5(f).
Entropy tends to favor placing sensors near the boundary as observed in Sec. 3, while mutual information tends to
place the sensors on the top and bottom sides, which exhibited the most complexity and should have a higher sensor
density. The predicted variances for each model are shown in figures 5(g) and 5(h). The mutual information version
has significantly lower variance than the entropy version almost everywhere, displaying, as expected, higher variance
in the unsensed areas in the center of the lab.
6.2 Precipitation data
Our second data set consisted of precipitation data collected during the years 1949 -1994 in the states of Washington
and Oregon . Overall 167 regions of equal area, approximately 50 km apart, reported
the daily precipitation.
To ensure the data could be reasonably modelled using a Gaussian process we applied a
log-transformation, removed the daily mean, and only considered days during which rain was reported. After this
preprocessing, we selected the initial two thirds of the data as training instances, and the remaining samples for testing
purposes. From the training data, we estimated the empirical covariance matrix, regularized it by adding independent
measurement noise1 of a2 = 0 . 1 , and used our approximation algorithms to compute the sensor placements optimizing
entropy and mutual information. We then used the test set to test prediction accuracy. Fig. 6(b) shows the average RMS
prediction error. Mutual information significantly outperforms entropy as a selection criteria - often several sensors
would have to be additionally placed for entropy to reach the same level of prediction accuracy as mutual information.
Fig. 6(a) shows where both objective values would place sensors to measure precipitation. It can be seen that entropy
is again much more likely to place sensors around the border of the sensing area than mutual information.
To provide estimates on the covariance structure, we report the 25, 50 and 75 percentiles of the absolute covariance
entries as 0.122,0.263 and 0.442, the maximum was 3.51, the minimum S.7SE-6. For the diagonal entries, the median
was 1.70, and the minimum was 0.990. Fig. 6(c) shows that the computation time can be drastically decreased as we
increase the truncation parameter e from 0 to the maximum variance. Fig. 6(d) shows the RMS prediction accuracy
for the 20 element subsets selected by Algorithm 2. According to the graphs, the range e e [0.5,1] seems to provide
the appropriate trade-off between computation time and prediction accuracy.
'The measurement noise a2 was chosen by cross-validation
Related Work
Our work falls under the rubric of discrete design. In the spatial setting notes that minimizing either the
average or maximum predicted variance is used, but that the optimization requires evaluating (£) placements or simulated annealing. Other methods ignore the GP, attempting to cover the space using various forms of random sampling,
which are compared in . Pairwise exchange has been used, albeit using the Fedorov delta . Such algorithms often fall into local minima, and lack quality-of-solution guarantees.
Most closely related to our work is , which selects the best set of k sensors using the entropy (variance)
criteria. Ko et al. formulate a branch-and-bound algorithm that finds the placement corresponding to a global maxima.
This approach, however, does not have a polynomial running time guarantee.
Conclusions
In this paper we (i) propose mutual information as a criterion for sensor placement in Gaussian processes, (ii) show the
exact optimization in NP-complete, (iii) provide an approximation algorithm that is within (1 — 1/e) of the maximum
mutual information configuration, (iv) show how to exploit local structure in GPs for significant speed-ups. Our empirical results indicate the mutual information criteria is often better than entropy, both qualitatively and in prediction
accuracy. This work can be used to increase the efficacy of monitoring systems, and is a step towards well-founded
active learning algorithms for spatial and structured data.
Acknowledgements: This work was partially supported by a gift from Intel Corporation. A. Singh acknowledges the
support of NSERC.
Proof of Theorem 2. Our reduction builds on the proof by Ko et al. , who show that for any graph G, there
exists a polynomially related, symmetric positive-definite matrix E such that E has a subdeterminant (of a submatrix
resulting from the selection of A: rows and columns ii, ...,£*) greater than some M if G has a clique of size at least
k, and E does not have a subdeterminant greater than M — e for some (polynomially-large) e > 0 if G does not have
such a clique. Let G be a graph, and let E be the matrix constructed in Ko et al. . We will consider E as
the covariance matrix of a multivariate Gaussian distribution with variables U = {Xi,..., Xn}. Introduce additional
variables S = {Yu..., Yn} such that Yi\Xi = x ~ M(x, a2). Note that a subset ACS, \A\ = fc, has maximum
entropy of all such subsets if and only if the parents T^ C U of A have maximum entropy among all such subsets
of U. Now note that I (A] (UUS)\A) = H{A) - H(A\(U US)\A) = H(A) - H(A\U), because Yt and
Yj are conditionally independent given U. Furthermore, again because of independence, H(A\U) is a constant only
depending on the cardinality of A. Assume we could decide efficiently whether there is a subset A C S such that
I{A\ V \ A) > M'. If we choose a2 small enough, then this would allow us to decide whether G has a clique of size
k, utilizing the gap e.
Proof of Lemma 4. Define K,(x, y) = £(x, y) for x ^ y and KL(x, x) = /C(x, x) + a2 to include the sensor noise a2.
Since C is compact and /C continuous, /C is uniformly continuous over C. Hence, for any e\, there exists a 8\ such
that for all x,x',y,y',\\x - x'\\2 <5u\\y-y'\\2 < Si it holds that |/C(x,y) - /C(x/,y/)l < d- Assume d cC
is a finite mesh grid with mesh width 25\. We allow sensor placement only on grid C\. Let C2 C C be a mesh grid
of mesh width 2<5i, which is derived by translating C\ by 8\ in Euclidean norm, and let G\,G2 denote the restriction
of the GP G to C\, Oi. We assume C\, C2 cover C in the sense of compactness. We use the notation ~ to refer to the
translated version in G2 of the random variable • in G\. K, is a symmetric strictly positive definite covariance function
and \fc(X, Y)—K,(X, Y)\ < e\ for all X, Y G G\. Moreover, since K is positive semidefinite, the smallest eigenvalue
of any covariance matrix derived from K, is at least a2.
Let A be a subset of C\ and X eC\\A. Using (3), we first consider the conditional variance cr2
x^A. By definition,
\\Y - Y\\2 < Su and hence \£{X,Y) - K{X,Y)\ < ei for all Y € A. Hence we know that
< k2ex. We furthermore note that | | E ^ | | 2 = A m a i ( E ^ ) = A m i n(Ex4)" 1 < cr"2, and hence
We derive \\LxA - Ex4||2 < ||eilT||2 = eiVk, hence
kx|^ - <4|.AI = ISx^S^S^x - S x
< 2\\HXA- E^HallE^HallEx^lla + ||E^ - E^||2||EX>1||2 + O(e2),
< 2eiv/fcCT-2M%/fc + CT-4fc2eiM2fc + O(e?),
< exka-2M (2 + a~2k2M) + O{e\),
where M = max x 6c IC(x, x). We choose S such that the above difference is bounded by a2e. We note that (assuming
w.l.o.g. H(X\A) > H{X\A))
which concludes the argument.
Proof of Corollary 5.
The higher order terms O(e\) can be worked out as ko~2e2(\+Mk2a~2+ek2a~2). Assuming
that e < min(M, 1), this is bounded by Sk3Ma~4e. Using the Lipschitz assumption, we can directly compute S\
from e\ in the above proof, by letting S = e\jL. Let R = ka~2M (2 + a~2k2M) + Sk3Ma~4. We want to choose
5 such that eiR < a2e. Hence if we choose 8 < f^f, then \H(X\A) - H(X\A)\ < e uniformly as required. Note
that in order to apply the result from Nemhauser et al. , the approximate monotonicity has to be guaranteed for
subsets of size 2k, which results in the stated bound.
Proof of Remark 7. We have that H(Y\Z) < 0 <=> K{Y, Y) + a2 - K{¥/)1*> < db- U s i ng a s i m i l a r argument as
the proof of Lemma 4, for very fine discretizations, there exists a Y arbitrarily close to Z, such that for any a > 0,
| /C(Z, Z) - K{Y, Y) | < a and | /C(Z, Z) - K{Z, Y)\<a. Plugging these bounds into the definition of H{Y\Z) and
some algebraic manipulation proves the claim.