Squeeze-and-Attention Networks for Semantic Segmentation
Zilong Zhong1,4, Zhong Qiu Lin2, Rene Bidart2, Xiaodan Hu2, Ibrahim Ben Daya2, Zhifeng Li5,
Wei-Shi Zheng1,3,4, Jonathan Li2, Alexander Wong2
1School of Data and Computer Science, Sun Yat-Sen Univeristy, China
2University of Waterloo, Waterloo, Canada
3Peng Cheng Laboratory, Shenzhen 518005, China
4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China
5Mstar Technologies, Hangzhou, China
{zlzhong, wszheng}@ieee.org, {zq2lin, x226hu, ibendaya, junli, a28wong}@uwaterloo.ca
The recent integration of attention mechanisms into segmentation networks improves their representational capabilities through a great emphasis on more informative features.
However, these attention mechanisms ignore an
implicit sub-task of semantic segmentation and are constrained by the grid structure of convolution kernels. In this
paper, we propose a novel squeeze-and-attention network
(SANet) architecture that leverages an effective squeezeand-attention (SA) module to account for two distinctive
characteristics of segmentation: i) pixel-group attention,
and ii) pixel-wise prediction.
Speciﬁcally, the proposed
SA modules impose pixel-group attention on conventional
convolution by introducing an ‘attention’ convolutional
channel, thus taking into account spatial-channel interdependencies in an efﬁcient manner. The ﬁnal segmentation
results are produced by merging outputs from four hierarchical stages of a SANet to integrate multi-scale contexts
for obtaining an enhanced pixel-wise prediction. Empirical
experiments on two challenging public datasets validate the
effectiveness of the proposed SANets, which achieves 83.2%
mIoU (without COCO pre-training) on PASCAL VOC and
a state-of-the-art mIoU of 54.4% on PASCAL Context.
1. Introduction
Segmentation networks become the key recognition elements for autonomous driving, medical image analysis,
robotic navigation and virtual reality. The advances of segmentation methods are mainly driven by improving pixelwise representation for accurate labeling.
However, semantic segmentation is not fully equivalent to pixel-wise
prediction. In this paper, we argue that semantic segmentation can be disentangled into two independent dimen-
Figure 1: Semantic segmentation can be disentangled into
two sub-tasks: explicit pixel-wise prediction and implicit
pixel grouping. These two tasks separate semantic segmentation from image classiﬁcation. Motivated by designing a
module that accounts for pixel grouping, we design a novel
squeeze-and-attention (SA) module along with a SANet to
improve the performance of dense prediction and account
for the largely ignored pixel grouping.
sions: pixel-wise prediction and pixel grouping. Speciﬁcally, pixel-wise prediction addresses the prediction of each
pixel, while pixel grouping emphasizes the connection between pixels. Previous segmentation works mainly focus
on improving segmentation performance from the pixellevel but largely ignore the implicit task of pixel grouping
 .
The largely ignored task of pixel grouping can be discovered by disentangling semantic segmentation into two
sub-tasks. As shown in Figure 1, the ﬁrst sub-task requires
precise pixel-wise annotation and introduces spatial constraints to image classiﬁcation. Recent segmentation models achieved signiﬁcant advances by aggregating contextual
features using pyramid pooling and dilated convolution layers for pixel-wise labeling . However, the grid strucarXiv:1909.03402v4 [cs.CV] 1 Apr 2020
tures of these kernels restrict the shapes of spatial features
learned in segmentation networks.
The feature aggregation strategy enhances pixel-wise prediction results, but the
global perspective of understanding images remains underexploited.
To this end, we introduce the second sub-task of pixel
grouping that directly encourages pixels that belong to the
same class being grouped together without spatial limitation. Pixel grouping involves translating images sampled
from a range of electromagnetic spectrum to pixel groups
deﬁned in a task-speciﬁc semantic spectrum, where each
entry of the semantic spectrum corresponds to a class. Motivated by designing a module that accounts for pixel grouping, we design a novel squeeze-and-attention (SA) module
to alleviate the local constraints of convolution kernels. The
SA module contains down-sampled but not fully squeezed
attention channels to efﬁciently produce non-local spatial
attention, while avoiding the usage of heavy dilated convolution in output heads. Speciﬁcally, An attention convolution are used to generate attention masks because each convolution kernel sweeps across input feature maps. Different
from SE modules that enhance backbones, SA modules
integrate spatial attentions and are head units, the outputs
of which are aggregated to improve segmentation performance. The spatial attention mechanism introduced by the
SA modules emphasizes the attention of pixel groups that
belong to the same classes at different spatial scales. Additionally, the squeezed channel works as global attention
We design SANets with four SA modules to approach
the above two tasks of segmentation. The SA modules learn
multi-scale spatial features and non-local spectral features
and therefore overcome the constraints of convolution layers for segmentation. We use dilated ResNet and Ef-
ﬁcient Nets as backbones to take advantage of their
strong capacity for image recognition. To aggregate multistage non-local features, we adopt SA modules on the multistage outputs of backbones, resulting in better object boundaries and scene parsing outcomes. This simple but effective
innovation makes it easier to generalize SANets to other related visual recognition tasks. We validate the SANets using
two challenging segmentation datasets: PASCAL context
and PASCAL VOC 2012 .
The contributions of this paper are three-fold:
• We disentangle semantic segmentation into two subtasks: pixel-wise dense prediction and pixel grouping.
• We design a squeeze-and-attention (SA) module that
accounts for both the multi-scale dense prediction of
individual pixels and the spatial attention of pixel
• We propose a squeeze-and-attention network (SANet)
with multi-level heads to exploit the representational
Figure 2: (a) Residual Block; (b) Squeeze-and-excitation
(SE) module; (c) Squeeze-and-attention (SA) module; and
For simplicity, we show convolution (CONV), fully connected (FC), average pooling (Avg.
Pool) layers, while
omitting normalization and activation layers. The SA module has a similar structure as the SE module that contains
an additional path to learn weights for re-calibrating channels of output feature maps Xout. The difference lies in that
the attention channel of SA modules uses average pooling
to down sample feature maps but not fully squeeze as in the
SE modules. Therefore, we term this channel the attention
convolution (ACONV) channel.
boost from SA modules, and to integrate multi-scale
contextual features and image-level categorical information.
2. Related Works
Multi-scale contexts.
Recent improvements for semantic segmentation have mostly been made possible by
incorporating multi-scale contextual features to facilitate
segmentation models to extract discriminative features. a
Laplacian pyramid structure is introduced to combine multiscale features introduced. A multi-path ReﬁneNet explicitly integrate features extracted from multi-scale inputs
to boost segmentation outputs. Encoder-decoder architectures have been used to fuse features that have different levels of semantic meaning . The most popular methods
adopt pooling operations to collect spatial information from
different scales . Similarly, EncNet employs an encoding module that projects different contexts in a Gaussian
kernel space to encode multi-scale contextual features .
Graphical models like CRF and MRF are used to impose
smoothness constraints to obtain better segmentation results
 . Recently, a gather-excite module is designed to
alleviate the local feature constraints of classic convolution
by gathering features from long-range contexts . We
improve the multi-scale dense prediction by merging outputs from different stages of backbone residual networks.
Channel-wise attention.
Selectively weighting the
channels of feature maps effectively increases the representational power of conventional residual modules. A good
example is the squeeze-and-excitation (SE) module because
it emphasizes attention on the selected channels of feature
maps. This module signiﬁcantly improves classiﬁcation accuracy of residual networks by grouping related classes together . EncNet also uses the categorical recognition
capacity of SE modules . Discriminative Feature Network (DFN) utilize the channel-weighting paradigm in its
smooth sub-network. .
Although re-calibrating the spectral weights of feature
map channels has been proved effective for improving the
representational power of convolution layers, but the implementation (e.g. squeeze-and-excitation modules) leads
to excessive model parameters. In contrast to SE module
 , we design a novel squeeze-and-attention (SA) module
with a down-sampled but not fully squeezed convolutional
channel to produce a ﬂexible module. Speciﬁcally, this additional channel generates categorical speciﬁc soft attention
masks for pixel grouping, while adding scaled spatial features on top of the classical convolution channels for pixellevel prediction.
Pixel-group attention. The success of attention mechanism in neural language processing foster its adoption for
semantic segmentation.
Spatial Transform Networks explicitly learn spatial attention in the form of afﬁne transformation to increase feature invariance . Since machine
translation and image translation share many similarities,
RNN and LSTM have been used for semantic segmentation
by connecting semantic labeling to translation . 
employed a scale-sensitive attention strategy to enable networks to focus on objects of different scales. designed
a speciﬁc spatial attention propagation mechanism, including a collection channel and a diffusion channel. used
self-attention masks by computing correlation metrics. 
designed a gather-and-excite operation via collecting local
features to generate hard masks for image classiﬁcation.
Also, has proved that not-fully-squeezed module is effective for image classiﬁcation with marginal computation
cost. Since the weights generated by spatially-asymmetric
recalibration (SAR) modules are vectors, they cannot be directly used for segmentation.Different from exiting attention modules, we use the down-sampled channels that implemented by pooling layers to aggregate multi-scale features and generate soft global attention masks simultaneously. Therefore, the SA models enhance the objective of
pixel-level dense prediction and consider the pixel-group attention that has largely been ignored.
3. Framework
Classical convolution mainly focuses on spatial local
feature encoding and Squeeze-and-Excitation (SE) modules
enhance it by selectively re-weighting feature map channels
through the use of global image information . Inspired
by this simple but effective SE module for image-level categorization, we design a Squeeze-and-Attention (SA) module that incorporates the advantages of fully convolutional
layers for dense pixel-wise prediction and additionally adds
an alternative, more local form of feature map re-weighting,
which we call pixel-group attention. Similar to the SE module that boosts classiﬁcation performance, the SA module is
designed speciﬁcally for improving segmentation results.
3.1. Squeeze-and-excitation module
Residual networks (ResNets) are widely used as the
backbones of segmentation networks because of their strong
performance on image recognition, and it has been shown
that ResNets pre-trained on the large image dataset ImageNet transfer well to other vision tasks, including semantic segmentation . Since classical convolution can
be regarded as a spatial attention mechanism, we start from
the residual blocks that perform as the fundamental components of ResNets. As shown in Figure 2 (a), conventional
residual blocks can be formulated as:
Xout = Xin + Xres = Xin + F(Xin; Θ, Ω)
where F(·) represents the residual function, which is parameterized by Θ and Ωdenotes the structure of two convolutional layers. Xin ∈RC′×H′×W ′ and Xout ∈RC×H×W
are input and output feature maps. The SE module improve
residual block by re-calibrating feature map channels, It is
worth noting that we adopt the updated version of SE module, which perform equivalently to original one in . As
shown in Figure 2 (b), the SE module can be formulated as:
Xout = w ∗Xin + F(Xin; Θ, Ω)
where the learned weights w for re-calibrating the channels
of input feature map Xin is calculated as:
w = Φ(W2 ∗σ(W1 ∗APool(Xin))),
where the Φ(·) represents the sigmoid function and σ(·) denotes the ReLU activation function. First, an average pooling layer is used to ‘squeeze’ input feature map Xin. Then,
two fully connected layers parameterized by W1 and W2 are
adopted to get the ‘excitation’ weights. By adding such a
simple re-weighting mechanism, the SE module effectively
increases the representational capacity of residual blocks.
Figure 3: Squeeze-and-attention Network. The SANet aggregates outputs from multiple hierarchical SA heads to generate
multi-scale class-wise masks accounting for the largely ignored pixel grouping task of semantic segmentation. The training
of these masks are supervised by corresponding categorical regions in ground truth annotation. Also, the masks are used
to guide the pixel-wise prediction, which is the output from a FCN head. In this way, we utilize the pixel-group attention
extraction capacity of SA modules and integrate multi-scale contextual features simultaneously.
3.2. Squeeze-and-attention module
Useful representation for semantic segmentation appears
at both global and local levels of an image. At the pixel
level, convolution layers generate feature maps conditional
on local information, as convolution is computed locally
around each pixel. Pixel level convolution lays the foundation of all semantic segmentation modules, and increased
receptive ﬁeld of convolution layers in various ways boost
segmentation performance , showing larger context
is useful for semantic segmentation.
At the global image level, context can be exploited to determine which parts of feature maps are activated, because
the contextual features indicate which classes likely to appear together in the image. Also, shows that the global
context provides a broader ﬁeld of view which is beneﬁcial
for semantic segmentation. Global context features encode
these areas holistically, rather than learning a re-weighting
independently for each portion of the image.
there remains little investigation into encoding context at a
more ﬁne-grained scale, which is needed because different
sections of the same image could contain totally different
environments.
To this end, we design a squeeze-and-attention (SA)
module to learn more representative features for the task of
semantic segmentation through a re-weighting mechanism
that accounts for both local and global aspects. The SA
module expands the re-weighting channel of SE module, as
shown in Figure 2 (b), with spatial information not fully
squeezed to adapt the SE modules for scene parsing. Therefore, as shown in Figure 2 (c), a simple squeeze-attention
module is proposed and can be formulated as:
Xout = Xattn ∗Xres + Xattn
where Xattn = Up(σ( ˆ
Xattn)) and Up(·) is a up-sampled
function to expand the output of the attention channel:
Xattn = Fattn(APool(Xin); Θattn, Ωattn)
where ˆXattn represents the output of the attention convolution channel Fattn(·), which is parameterized by Θattn
and the structure of attention convolution layers Ωattn. A
average pooling layer APool(·) is used to perform the notfully-squeezed operation and then the output of the attention
channel ˆXattn is up-sampled to match the output of main
convolution channel Xres.
In this way, the SA modules extend SE modules with
preserved spatial information and the up-sampled output of
the attention channel Xattn aggregates non-local extracted
features upon the main channel.
3.3. Squeeze-and-attention network
We build a SA network (SANet) for semantic segmentation on top of the SA modules. Speciﬁcally, we use SA
modules as heads to extract features from the four stages
of backbone networks to fully exploit their multi-scale. As
illustrated in Figure 3, the total loss involves three parts:
dense loss(CE loss), mask loss(CE loss), and categorical
loss(binary CE loss). ynj is the average pooled results of
Y den” Therefore, the total loss of SANets can be represented as:
Figure 4: Ablation study of α and β that weight the categorical loss and dense prediction loss, respectively. We test
SANets using ResNet50 as backbones and train 20 epochs
for each case. Left: mIoUs of SANets with ﬁxed β = 0.8
for selecting α. Right mIoUs of SANets with ﬁxed α = 0.2
for selecting β.
LSANet = Lmask + α ∗Lcat + β ∗Lden
where α and β are weighting parameters of categorical loss
and auxiliary loss, respectively. Each component of the total
loss can be formulated as follows:
j=1 Ynij log ˆY mask
j=1 ynj log ˆycat
+(1 −ynj) log (1 −ˆycat
j=1 Ynij log ˆY den
where N is number of training data size for each epoch, M
represents the spaital locations, and C denotes the number
of classes for a dataset. ˆYnij and Ynij are the predictions
of SANets and ground truth, ˆynj and ynj are the categorical
predictions and targets to calculate the categorical loss Lcat.
The Lcat takes a binary cross entropy form. Lmask and
Lden are typical cross entropy losses. The auxiliary head
is similar to the strategy of deep supervision , but
its input comes from the fourth stage of backbone ResNet
instead of the commonly used third stage. The prediction of
SANets integrates the pixel-wise prediction and is regularized by the fourth SA feature map. Hence, the regularized
dense segmentation prediction of a SANet is ˆY den + ˆY SA4.
Dilated FCNs have been used as the backbones of
SANets. Suppose that the input image has a size of 3×512×
512. The main channel of SA modules has the same channel
numbers as their attention counterparts and the same spatial
sizes as the input features. Empirically, we reduce the channel sizes of inputs to a fourth in both main and attention
channels, set the downsample (max pooling) and upsample
ratio of attention channels to 8, and set the channel number of the intermediate fully connected layer of SE modules
to 4 in both datasets. We adopt group convolution using 2
Figure 5: Sample semantic segmentation results on PAS-
CAL Context validation set. Example of semantic segmentation results on PASCAL VOC validation set. (a) Raw images. (b) Groud truth images. (c) Results of a FCN baseline.
(d) Results of a SANet. SANet generates more accurate results, especially for object boundaries. The last raw shows a
failed example with relative complex contexts, which bring
challenges for segmentation models.
Table 1: Ablation study results of SANets on PASCAL
Context dataset (59 classes without background).
Squeeze-and-attention heads. Cat: Categorical loss. Den:
Dense prediction Loss. PAcc: Pixel accuracy (%). mIoU:
Mean intersection of union (%).
groups for the ﬁrst convolution operations in both main and
attention channels. Also, we adapt outputs of SA heads to
the class number of segmentation datasets.
4. Experimental Results
In this section, we ﬁrst compare SA module to SE modules, then conduct an ablation study using the PASCAL
Context dataset to test the effectiveness of each component of the total training loss, and further validate SANets
on the challenging PASCAL VOC dataset . Following the convention for scene parsing , we paper both
mean intersection and union (mIoU) and pixel-wise accuracy (PAcc) on PASCAL Context, and mIoU only on PAS-
CAL VOC dataset to assess the effectiveness of segmenta-
CRF-RNN 
ParseNet 
BoxSup 
HighOrder-CRF 
Piecewise 
Deeplab-v2 
ReﬁneNet 
EncNet 
SANet (ours)
SANet (ours)
Table 2: Mean intersection over union (%) results on PAS-
CAL Context dataset (60 classes with background).
SANet50 (ours)
SANet101 (ours)
Table 3: Pixel accuracy (PAcc) and mIoUs of baseline
dilated FCNs, dilated FCNs with SE modules (FCN-SE),
and SANets using ResNet50 or ResNet101 as backbones
on PASCAL Context. SANet signiﬁcanly output their SE
counterparts and baseline models. Each model is trained
for 20 epochs
tion models.
4.1. Implementation
We use Pytorch to implement SANets and conduct ablation studies. For the training process, we adopt a
poly learning rate decreasing schedule as in previous works
The starting learning rates for PASCAL Context and PASCAL VOC are 0.001 and 0.0001, respectively.
Stochastic gradient descent and poly learning rate annealing schedule are adopted for both datasets. For PASCAL
Context dataset, we train SANets for 80 epochs. As for the
PASCAL VOC dataset, we pretrain models on the COCO
dataset. Then, we train networks for 50 epochs on the validation set. We adopt the ResNet50 and ResNet101 as the
backbones of SANets because these networks have been
widely used for mainstream segmentation benchmarks. We
set the batch-size to 16 in all training cases and use sync
batch normalization across multiple gpus recentely implemented by . We concatenate four SA head outputs to
exploit the multi-scale features of different stages of backbones and also to regularize the training of deep networks.
4.2. Results on PASCAL Context
The Pascal Context dataset contains 59 classes, 4998
training images, and 5105 test images. Since this dataset is
relatively small in size, we use it as the benchmark to design
module architectures and select hyper-parameters including
α and β. To conduct an ablation study, we explore each
component of SA modules that contribute to enhancing the
segmentation results of SANets.
The ablation study includes three parts. First, we test
the impacts of the weights α and β of the total training
loss. As shown in Figure 4, we test α from 0 to 1.0, and
ﬁnd that the SANet with α = 0.2 works the best. Similarly, we ﬁx α = 0.2 to ﬁnd that β = 0.8 yields the best
segmentation performance. Second, we study the impacts
of categorical loss and dense prediction loss of in equation
(7) using selected hyper-parameters. Table 1 shows that the
SANet, which contains the four dual-usage SA modules,
using ResNet50 as the backbone improves signiﬁcantly (a
2.7% PAcc and 6.0% mIoU increase) compared to the FCN
baseline. Also, the categorical loss and auxiliary loss boost
the segmentation performance.
We compare SANets with state-of-the-art models to validate their effectiveness, as shown in Table 2, the SANet using ResNet101 as its backbone achieves 53.0% mIoU. The
mIoU equals to 52.1% when including the background class
this result and outperforms other competitors. Also, we use
the recently published Efﬁcient Net (EffNet) as backbones. Then, the EffNet version SANet achieved state-ofthe-art 54.4% mIoU that sets new records for the PASCAL
Context dataset. Figure 5 shows the segmentation results
of a dilated ResNet50 FCN and a SANet using the same
backbone. In the ﬁrst three rows, SANets generate better
object boundaries and higher segmentation accuracy. However, for complex images like the last row, both models fail
to generate clean parsing results. In general, the qualitative
assessment is in line with quantitative papers.
We also validate the effectiveness of SA modules by
comparing them with SE modules on top of the baseline
dilated FCNs, including ResNet50 and ResNet101. Table
3 shows that the SANets achieve the best accuracy with
signiﬁcant improvement (4.1% and 4.5% mIoU increase)
in both settings, while FCN-SE models barely improve the
segmentation results.
4.3. Attention and Feature Maps
The classic convolution already yields inherent global
attention because each convolutional kernel sweeps across
spatial locations over input feature maps. Therefore, we visualize the attention and feature maps of a example of PAS-
CAL VOC set and conduct a comparison between Head1
and Head4 within a SANet To better understand the effect
of attention channels in SA modules. We use L2 distance
to show the attention maps of the attention channel within
Figure 6: Attention and feature map visualization of SA
head1 and head4 of a trained SANet on PASCAL VOC
dataset. For each head, the feature maps of main channel,
attention channel, and output are demonstrated. (a) Raw
image and its ground truth; the pixel group visualization of
(b) blue point; (c) yellow point; and (d) magenta point.
SA module, and select the most activated feature map channels for the outputs of the main channel within the same SA
module. The activated areas (red color) of the output feature
maps of SA modules can be regarded as the pixel groups of
selected points. For the sake of visualization, we scale all
feature maps illustrated in Figure 6 to the same size. we select three points (red, blue, and magenta) in this examples to
show that the attention channel emphasizes the pixel-group
attention, which is complementary to the main channels of
SA modules that focus on pixel-level prediction.
Interestingly, as shown in Figure 6, the attention channels in low-level (SA head1) and high-level (SA head4) play
different roles. For the low-level stage, the attention maps
of the attention channel have broad ﬁeld of view, and feature
maps of the main channel focus on local feature extraction
with object boundary being preserved. In contrast, for the
high-level stage, the attention maps of the attention channel mainly focus on the areas surrounding selected points,
and feature maps of the main channel present more homogeneous with clearer semantic meaning than those of head1.
Figure 7: Example of semantic segmentation results on
PASCAL VOC validation set. (a) Raw images. (b) Groud
truth images. (c) FCN baseline. (d) A SANet. SANet generates more accurate parsing results compared to the baseline.
4.4. Results on PASCAL VOC
The PASCAL VOC dataset is the most widely studied segmentation benchmark, which contains 20 classes and
is composed of 10582 training images, and 1449 validation
images, 1456 test images. We train the SANet using augmented data for 80 epochs as previous works .
First, we test the SANet without COCO pretraining. As
shown in Table 4, the SANet achieves 83.2% mIoU which is
higher than its competitors and dominates multiple classes,
including aeroplane, chair, cow, table, dog, plant, sheep,
and tv monitor. This result validates the effectiveness of the
dual-usage SA modules. Models use extra datasets
like JFT other than PASCAL VOC or COCO are not
included in Table 4.
Then, we test the the SANet with COCO pretraining. As
shown in Table 5, the SANet achieves an evaluated result
of 86.1% mIoU using COCO data for pretraining, which is
comparable to top-ranking models including PSPNet ,
and outperforms the ReﬁneNet that is built on a heavy
ResNet152 backbone. Our SA module is more computationally efﬁcient than the encoding module of EncNet .
As shown in Figure 6, the prediction of SANets yields
clearer boundaries and better qualitative results compared
to those of the baseline model.
4.5. Complexity Analysis
Instead of pursing SOTA without considering computation costs, our objective is to design lightweight modules for
segmentation inspired by this intuition. We use MACs and
model parameters to analyze the complexity of SANet. As
shown in Table 6, both Deeplab V3+ (our implementation)
DeepLabv2 
CRF-RNN 
DeconvNet 
Piecewise 
ResNet38 
PSPNet 
DANet 
EncNet 
SANet(ours)
Table 4: Class-wise IoUs and mIoU of PASCAL VOC dataset without pretraining on COCO dataset. The SANet achieves
83.2% mIoU that outperforms other models and dominates multiple classes. The best two entries of each column are highlighted. To make a fair comparison, modelsuse extra datasets (e.g. JFT) are not included like .
CRF-RNN 
BoxSup 
DilatedNet 
PieceWise 
Deeplab-v2 
ReﬁneNet 
PSPNet 
DeeplabV3 
EncNet 
SANet (ours)
Table 5: Mean intersection over union (%) results on PAS-
CAL VOC dataset with pretraining on COCO dataset. The
SANet achieves 86.1% mIoU that is comparable results to
state-of-the-art models.
and SAN use ResNet101 backbone and are evaluated on
PASCAL VOC dataset to enablea a fair comparison. Without using COCO dataset for pretraining, our SANet surpasses Deeplab V3+ with an increase of 1.7% mIoU. Compared to heavy-weight models like SDN (238.5M params),
SANet achieves slightly under-performed results with less
than a fourth number of parameters (55.5M params). The
comparison results demonstrate the SANet is effective and
5. Conclusion
In this paper, we rethink semantic segmentation from
two independent dimensions — pixel-wise prediction and
pixel grouping. We design a SA module to account for the
Dilated FCN
APCNet 
Deeplab V3+† 
SANet (ours)
† Our implementation
Table 6: MIoUs (%), Multiply-Accumulate operation per
second (MACs) and network parameters (Params) using
ResNet101 as backbones evaluated on PASCAL VOC test
set without COCO pretraining. We re-implement Deeplab
V3+ using dilated ResNet101 as its backbone to enable a
fair comparison.
implicit sub-task of pixel grouping. The SA module enhances the pixel-wise dense prediction and accounts for the
largely ignored pixel-group attention. More importantly, we
propose SANets that achieve promising segmentation performance on two challenging benchmarks. We hope that
the simple yet effective SA modules and the SANets built
on top of SA modules can facilitate the segmentation research of other groups.
Acknowledgement
This work was supported partially by the National
Development
(2018YFB1004903), Research Projects of Zhejiang Lab
(No. 2019KD0AB03), International Postdoctoral Exchange
Fellowship Program (Talent-Introduction Program), and
Google Cloud Platform research credits program.