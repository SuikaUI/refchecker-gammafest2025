Proceedings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pages 483–498
June 6–11, 2021. ©2021 Association for Computational Linguistics
mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
Linting Xue∗
Noah Constant∗
Adam Roberts∗
Mihir Kale
Rami Al-Rfou
Aditya Siddhant
Aditya Barua
Colin Raffel
Google Research
“Text-to-Text
Transformer” (T5) leveraged a uniﬁed text-to-text
format and scale to attain state-of-the-art results on a wide variety of English-language
NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained
on a new Common Crawl-based dataset covering 101 languages. We detail the design and
modiﬁed training of mT5 and demonstrate its
state-of-the-art performance on many multilingual benchmarks. We also describe a simple
technique to prevent “accidental translation”
in the zero-shot setting, where a generative
model chooses to (partially) translate its prediction into the wrong language. All of the
code and model checkpoints used in this work
are publicly available.1
Introduction
processing
pipelines often make use of transfer learning, where
a model is pre-trained on a data-rich task before
being ﬁne-tuned on a downstream task of interest
 . The success of this paradigm
is partially thanks to the release of parameter checkpoints for pre-trained models. These checkpoints
allow members of the NLP community to quickly
attain strong performance on many tasks without
needing to perform expensive pre-training themselves. As one example, the pre-trained checkpoints for the “Text-to-Text Transfer Transformer”
(T5) model released by Raffel et al. have
been used to achieve state-of-the-art results on
many benchmarks .
Unfortunately, many of these language models
were pre-trained solely on English-language text.
∗Equal Contribution. Please direct correspondence to
 , ,
 , and 
1 
This signiﬁcantly limits their use given that roughly
80% of the world population does not speak English . One way the community
has addressed this English-centricity has been to
release dozens of models, each pre-trained on a
single non-English language . A more general solution is to produce
multilingual models that have been pre-trained on
a mixture of many languages. Popular models of
this type are mBERT , mBART , and XLM-R ,
which are multilingual variants of BERT , BART , and
RoBERTa , respectively.
In this paper, we continue this tradition by releasing mT5, a multilingual variant of T5. Our goal
with mT5 is to produce a massively multilingual
model that deviates as little as possible from the
recipe used to create T5. As such, mT5 inherits
all of the beneﬁts of T5 (described in section 2),
such as its general-purpose text-to-text format, its
design based on insights from a large-scale empirical study, and its scale. To train mT5, we introduce a multilingual variant of the C4 dataset
called mC4. mC4 comprises natural text in 101
languages drawn from the public Common Crawl
web scrape. To validate the performance of mT5,
we include results on several benchmark datasets,
showing state-of-the-art results in many cases. Finally, we characterize a problematic behavior of
pre-trained generative multilingual language models in the zero-shot setting, where they erroneously
translate part of their prediction into the wrong language. To address this “accidental translation”, we
describe a simple procedure that involves mixing
in unlabeled pre-training data during ﬁne-tuning
and demonstrate that it dramatically alleviates this
issue. We release our pre-trained models and code
so that the community can leverage our work.1
Background on T5 and C4
In this section, we provide a short overview of T5
and the C4 pre-training dataset. Further details are
available in Raffel et al. .
T5 is a pre-trained language model whose primary distinction is its use of a uniﬁed “text-totext” format for all text-based NLP problems. This
approach is natural for generative tasks (such as
machine translation or abstractive summarization)
where the task format requires the model to generate text conditioned on some input. It is more
unusual for classiﬁcation tasks, where T5 is trained
to output the literal text of the label (e.g. “positive” or “negative” for sentiment analysis) instead
of a class index. The primary advantage of this
approach is that it allows the use of exactly the
same training objective (teacher-forced maximumlikelihood) for every task, which in practice means
that a single set of hyperparameters can be used for
effective ﬁne-tuning on any downstream task. Similar unifying frameworks were proposed by Keskar
et al. and McCann et al. . Given the
sequence-to-sequence structure of this task format,
T5 uses a basic encoder-decoder Transformer architecture as originally proposed by Vaswani et al.
 . T5 is pre-trained on a masked language
modeling “span-corruption” objective, where consecutive spans of input tokens are replaced with a
mask token and the model is trained to reconstruct
the masked-out tokens.
An additional distinguishing factor of T5 is its
scale, with pre-trained model sizes available from
60 million to 11 billion parameters. These models
were pre-trained on around 1 trillion tokens of data.
Unlabeled data comes from the C4 dataset, which
is a collection of about 750GB of English-language
text sourced from the public Common Crawl web
scrape. C4 includes heuristics to extract only natural language (as opposed to boilerplate and other
gibberish) in addition to extensive deduplication.
The pre-training objective, model architecture, scaling strategy, and many other design choices for T5
were chosen based on a large-scale empirical study
described in detail in Raffel et al. .
mC4 and mT5
Our goal in this paper is to create a massively multilingual model that follows T5’s recipe as closely
as possible. Towards this end, we develop an extended version of the C4 pre-training dataset that
covers 101 languages and introduce changes to T5
to better suit this multilinguality.
The C4 dataset was explicitly designed to be
English only:
any page that was not given a
probability of at least 99% of being English by
langdetect2 was discarded. In contrast, for
mC4 we use cld33 to identify over 100 languages.
Since some of these languages are relatively scarce
on the internet, we make use of all of the 71
monthly web scrapes released so far by Common
This is dramatically more source data
than was used for C4, for which the April 2019
web scrape alone was enough to provide plenty of
English-language data.
An important heuristic ﬁltering step in C4 was
the removal of lines that did not end in an English
terminal punctuation mark. Since many languages
do not use English terminal punctuation marks, we
instead apply a “line length ﬁlter” that requires
pages to contain at least three lines of text with 200
or more characters. Otherwise, we follow C4’s ﬁltering by deduplicating lines across documents and
removing pages containing bad words.4 Finally, we
detect each page’s primary language using cld3
and remove those with a conﬁdence below 70%.
After these ﬁlters are applied, we group the remaining pages by language and include in the corpus all languages with 10,000 or more pages. This
produces text in 107 “languages” as deﬁned by
cld3. However, we note that six of these are
just script variants of the same spoken language
(e.g. ru is Russian in Cyrillic script and ru-Latn
is Russian in Latin script). A histogram of the
page counts for each language is shown in ﬁg. 1.
Detailed dataset statistics including per-language
token counts are shown in Appendix A.
The model architecture and training procedure that
we use for mT5 closely follows that of T5. Speciﬁcally, we base mT5 on the “T5.1.1” recipe,5 which
improves upon T5 by using GeGLU nonlinearities
 , scaling both dmodel and dﬀinstead
2 
3 
4 
5 
text-to-text-transfer-transformer/blob/
master/released_checkpoints.md#t511
Pages of mC4 training text
% of mT5 training examples
Figure 1: Page counts per language in mC4 (left axis), and percentage of mT5 training examples coming from
each language, for different language sampling exponents α (right axis). Our ﬁnal model uses α=0.3.
Architecture
Parameters
# languages
Data source
mBERT 
Encoder-only
XLM 
Encoder-only
XLM-R 
Encoder-only
270M – 550M
Common Crawl (CCNet)
mBART 
Encoder-decoder
Common Crawl (CC25)
MARGE 
Encoder-decoder
Wikipedia or CC-News
mT5 (ours)
Encoder-decoder
300M – 13B
Common Crawl (mC4)
Table 1: Comparison of mT5 to existing massively multilingual pre-trained language models. Multiple versions of
XLM and mBERT exist; we refer here to the ones that cover the most languages. Note that XLM-R counts ﬁve
Romanized variants as separate languages, while we ignore six Romanized variants in the mT5 language count.
of just dﬀin the larger models, and pre-training on
unlabeled data only with no dropout. We refer to
Raffel et al. for further details on T5.
A major factor in pre-training multilingual models is how to sample data from each language.
Ultimately, this choice is a zero-sum game: If
low-resource languages are sampled too often, the
model may overﬁt; if high-resource languages are
not trained on enough, the model will underﬁt. We
therefore take the approach used in and
boost lower-resource languages by sampling examples according to the probability p(L) ∝|L|α,
where p(L) is the probability of sampling text from
a given language during pre-training and |L| is the
number of examples in the language. The hyperparameter α (typically with α < 1) allows us to
control how much to “boost” the probability of
training on low-resource languages. Values used
by prior work include α = 0.7 for mBERT , α = 0.3 for XLM-R ,
and α = 0.2 for MMNMT . We tried all three of these values (ablation
results in section 4.2) and found α = 0.3 to give a
reasonable compromise between performance on
high- and low-resource languages.
The fact that our model covers over 100 languages necessitates a larger vocabulary. Following
XLM-R , we increase the vocabulary size to 250,000 wordpieces. As in T5, we
use SentencePiece models trained with the language sampling rates used during pre-training. To accommodate languages with large character sets like
Chinese, we use a character coverage of 0.99999
and enable SentencePiece’s “byte-fallback” feature
to ensure that any string can be uniquely encoded.
Comparison to Related Models
To contextualize our new model, we provide a brief
comparison with existing massively multilingual
pre-trained language models. For brevity, we focus
on models that support more than a few dozen languages. Table 1 gives a high-level comparison of
mT5 to the most similar models.
mBERT is a multilingual version of BERT . Similar to our
approach with mT5, mBERT follows the BERT
recipe as closely as possible (same architecture, objective, etc.). The primary difference is the training
set: Instead of training on English Wikipedia and
the Toronto Books Corpus, mBERT is trained on
up to 104 languages from Wikipedia. XLM is also based on BERT but
applies improved methods for pre-training multilingual language models including explicitly crosslingual pre-training objectives. Many pre-trained
versions of XLM have been released; the most
massively-multilingual variant was trained on 100
languages from Wikipedia.
XLM-R (Conneau
Sentence pair
Structured
Question answering
WikiAnn NER
TyDi QA-GoldP
Cross-lingual zero-shot transfer (models ﬁne-tuned on English data only)
64.5 / 49.4
61.4 / 44.2
59.7 / 43.9
59.8 / 44.3
48.5 / 32.6
43.6 / 29.1
73.6 / 55.2
77.2 / 61.3
72.3 / 53.5
76.0 / 59.5
76.6 / 60.8
71.6 / 53.2
65.1 / 45.0
77.3 / 61.8
71.7 / 53.2
67.6 / 49.1
79.6 / 64.0
73.1 / 55.0
77.0 / 63.0
58.1 / 42.5
54.6 / 37.1
36.4 / 24.4
67.0 / 49.0
64.6 / 45.0
59.1 / 42.4
77.8 / 61.5
71.2 / 51.7
68.4 / 50.9
79.5 / 63.6
73.5 / 54.5
77.8 / 61.8
82.5 / 66.8
76.0 / 57.4
82.0 / 67.3
Translate-train (models ﬁne-tuned on English data plus translations in all target languages)
80.2 / 65.9
72.8 / 54.3
66.5 / 47.7
FILTER + Self-Teaching
82.4 / 68.0
76.2 / 57.7
68.3 / 50.9
79.9 / 66.3
73.1 / 54.9
75.0 / 58.9
64.3 / 49.5
56.6 / 38.8
49.8 / 35.6
75.3 / 59.7
67.6 / 48.5
66.4 / 51.0
81.2 / 65.9
73.9 / 55.2
75.7 / 60.1
82.7 / 68.1
75.1 / 56.6
80.1 / 65.0
85.2 / 71.3
76.9 / 58.3
83.3 / 69.4
In-language multitask (models ﬁne-tuned on gold data in all target languages)
77.6 / 68.0
74.0 / 62.7
79.7 / 68.4
85.3 / 75.3
87.6 / 78.4
88.7 / 79.5
Table 2: Results on XTREME sentence-pair classiﬁcation, structured prediction and question answering tasks.
mBERT metrics are from Hu et al. . Metrics for XLM, InfoXLM, X-STILTs and XLM-R are from Fang
et al. , though Conneau et al. report better performance of XLM-R on XNLI (80.9). All other metrics
are from the original sources: FILTER , VECO and RemBERT . For the “translate-train” setting, we include English training data, so as to be comparable with Fang et al.
 and Luo et al. . This differs from the XTREME “translate-train” setup of Hu et al. . For mT5
results on TyDi QA zero-shot, we report the median across ﬁve ﬁne-tuning runs, as we observed high variance
across runs.6 Full results for all languages in all tasks are provided in the appendix.
et al., 2020) is an improved version of XLM based
on the RoBERTa model . XLM-R
is trained with a cross-lingual masked language
modeling objective on data in 100 languages from
Common Crawl. To improve the pre-training data
quality, pages from Common Crawl were ﬁltered
by an n-gram language model trained on Wikipedia
 . mBART 
is a multilingual encoder-decoder model that is
based on BART . mBART is
trained with a combination of span masking and
sentence shufﬂing objectives on a subset of 25 languages from the same data as XLM-R. MARGE
 is a multilingual encoderdecoder model that is trained to reconstruct a document in one language by retrieving documents in
other languages. It uses data in 26 languages from
Wikipedia and CC-News .
Experiments
To validate the performance of mT5, we evaluate
our models on 6 tasks from the XTREME multilingual benchmark : the XNLI entailment task covering 14 languages; the XQuAD , MLQA
 , and TyDi QA reading comprehension benchmarks with 10,
6Standard deviations of mT5 models on TyDi QA zeroshot across ﬁve runs are: Small: 0.44, Base: 1.38, Large: 3.66,
XL: 1.29, XXL: 0.20.
7, and 11 languages respectively; the Named Entity Recognition (NER) dataset of WikiAnn restricted to the 40 languages from
XTREME , and the PAWS-X paraphrase identiﬁcation dataset with
7 languages. We cast all tasks into the text-to-text
format, i.e. generating the label text (XNLI and
PAWS-X), entity tags and labels (WikiAnn NER),
or answer (XQuAD, MLQA, and TyDi QA) directly in a generative fashion. For NER, if there are
multiple entities, they are concatenated in the order
they appear, and if there are no entities then the
target text is “None”. We consider three variants
of these tasks: (1) “zero-shot”, where the model
is ﬁne-tuned only on English data, (2) “translatetrain”, adding machine translations from English
into each target language, and (3) “in-language multitask”, training on gold data in all target languages.
For brevity, we refer to Hu et al. for further
details on these benchmarks.
Following the original T5 recipe, we consider
ﬁve model sizes: Small (≈300M parameters),
Base (580M), Large (1.2B), XL (3.7B), and XXL
(13B). The increase in parameter counts compared to the corresponding T5 model variants
comes from the larger vocabulary used in mT5.
Note that, because mT5 is an encoder-decoder
model, it has roughly twice as many parameters as
correspondingly-sized encoder-only models such
as XLM-R. For example, the “Large” variant of
XLM-R has 550 million parameters whereas mT5-
Large has around 1 billion. However, the computational cost for text classiﬁcation is roughly the
same: In both cases, the model processes a length-
T input sequence with an encoder of approximately
equal size. In an encoder-only model like XLM-R,
the encoder processes one additional “CLS” token,
which is used to generate the representation for classiﬁcation. In mT5, the decoder typically produces
two additional tokens: the class label and an endof-sequence token. Since the decoder has the same
architecture (ignoring encoder-decoder attention)
as the encoder, the computational cost of classiﬁcation with mT5 typically amounts to the cost of
processing T + 2 tokens compared to T + 1 for
an encoder-only model. However, encoder-decoder
architectures have the additional beneﬁt of being
applicable to generative tasks like abstractive summarization or dialog.
We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input
sequences, corresponding to roughly 1 trillion input tokens total. This is the same amount of pretraining as T5 and about 1
6 as much as XLM-R.7
Note that our pre-training dataset is large enough
that we only complete a fraction of an epoch for
high-resource languages (e.g. only covering 2% of
the English data). While XLM-R’s pre-training corpus CC-100 is 20 times smaller than mC4, XLM-R
nevertheless pre-trains for more steps, and sees
over 6 times more tokens in pre-training.
We use the same inverse square-root learning
rate schedule used by T5 during pre-training, with
the learning rate set to 1/
max(n, k) where n is
the current training iteration and k = 104 is the
number of warm-up steps. Following the T5.1.1
recipe, we do not apply dropout during pre-training.
We use the same self-supervised objective as T5,
with 15% of tokens masked and an average noise
span length of 3. We ablate some of these experimental details in section 4.2.
For ﬁne-tuning, we use a constant learning rate
of 0.001 and dropout rate of 0.1 for all tasks. We
use a batch size of 217 for most tasks, but decrease
to 216 for WikiAnn NER zero-shot, due to the small
size of the training, and increase to 220 tokens for
XNLI, which we found gave better performance.
For early stopping, we save checkpoints every 200
steps and choose the checkpoint with the highest
performance on the standard validation sets speci-
ﬁed by XTREME.
Table 2 presents our main results, with perlanguage breakdowns for each task given in Appendix B. Our largest model mT5-XXL exceeds
state-of-the-art on all classiﬁcation and QA tasks
and is near SOTA on NER (69.2 vs. 70.1). Note
that unlike our model, InfoXLM 
and VECO beneﬁt from parallel training data, while X-STILTs leverages labeled data from tasks similar to
the target task. Overall, our results highlight the
importance of model capacity in cross-lingual representation learning and suggest that scaling up a
simple pre-training recipe can be a viable alternative to more complex techniques relying on LM
ﬁltering, parallel data, or intermediate tasks.
In the “translate-train” setting, we exceed state-
7XLM-R Large sees 6.3 trillion tokens during pre-training
(1.5 million batches of 8192 sequences of 512 tokens), and
uses a packing mechanism similar to T5 to minimize the number of “wasted” padding tokens.
87.2 / 79.1
84.7 / 76.4
92.1 / 85.4
89.6 / 83.8
93.8 / 86.7
93.0 / 87.0
95.0 / 88.5
94.5 / 88.9
96.2 / 91.3
95.6 / 90.4
Table 3: Comparison of T5 vs. mT5 on SQuAD question answering (F1/EM).
# Parameters
In-Language Multitask
Translate-Train
Figure 2: Average F1 on the TyDi QA GoldP task
across languages. Performance improves with increasing model capacity.
The importance of in-language
training data (whether gold In-Language Multitask or
synthetic Translate-Train) decreases with model scale,
as seen by Zero-Shot closing the quality gap.
of-the-art on all XTREME classiﬁcation and QA
tasks. For these tasks, we ﬁne-tune on the combination of the labeled English data and machine translations thereof.8 This allows direct comparison
with both FILTER as well as the
XLM-R baseline of Fang et al. . Note that
this setup differs from XTREME “translate-train”
 , which excludes English.
Figure 2 shows that model capacity is key to improving performance on variants of the TyDi QA
GoldP task in the absence of “gold” multilingual
data: For the smallest model, training on gold
datasets (in-language multitask) achieves dramatically better performance than using weakly supervised data (translate-train) or English-only data
(zero-shot), whereas the gap between these three
settings is much smaller for the largest model. For
our two largest models, zero-shot and translatetrain performance is nearly the same, showing that
machine translations of the monolingual dataset
bring diminishing returns as model capacity in-
8We use the translation data provided by Hu et al. 
throughout. On the PAWS-X task, FILTER used translation
data from the original task instead. Switching to this data
would improve our scores slightly (mT5-XXL 91.5 →92.0).
creases. Overall, these trends point to the possibility of avoiding the costly step of annotating data in
more than one language when using large models.
Massively multilingual models have been observed to underperform on a given language when
compared to a similarly-sized “dedicated” model
trained speciﬁcally for that language . To quantify this effect, we compare
the performance of mT5 and T5 when ﬁne-tuned
on the SQuAD reading comprehension benchmark
 . The results are shown in
table 3, with results for T5 reproduced from Raffel
et al. . While the Small and Base mT5 models fall short of their English T5 counterparts, we
ﬁnd that the larger models close the gap. This suggests there may be a turning point past which the
model has enough capacity to effectively learn 101
languages without signiﬁcant interference effects.
Looking at the per-language breakdowns in Appendix B, we ﬁnd that mT5 performs well on both
high- and low-resource languages. For example,
in table 7, we see mT5-XXL outperforms XLM-R
by between +3 (English) and +9 (Swahili) points
on each individual language on XNLI zero-shot.
In table 12 we see similarly strong performance
across languages on TyDi QA GoldP (including
lower-resource languages like Swahili and Telugu),
with mT5-XXL surpassing human performance in
four of nine languages on the “in-language” setting.
We run six ablations, modifying various settings,
using our Large model as a baseline: (i) increase
dropout to 0.1 in hopes of mitigating overﬁtting
on low-resource languages, (ii) decrease sequence
length to 512 (as was used in T5), (iii) increase the
average noise span length in the pre-training objective to 10 since we observe fewer characters per
token than T5, (iv) adjust the language sampling
exponent α to {0.2, 0.7} as used in MMNMT and mBERT ,
respectively, (v) turn off the “line length ﬁlter” in
the mC4 data pipeline, and (vi) supplement mC4
with Wikipedia data9 from 103 languages.
The effect of these ablations on XNLI zero-shot
accuracy is shown in table 4. In each case, the
average XNLI score is lower than the mT5-Large
baseline, justifying our chosen settings. The line
9We use the 2020 Wikipedia data from TensorFlow
 
catalog/wikipedia
Baseline (mT5-Large)
Dropout 0.1
Sequence length 512
Span length 10
No line length ﬁlter
Add Wikipedia data
Table 4: Average XNLI zero-shot accuracy of various
ablations on our mT5-Large model. Per-language metrics are shown in Appendix C.
length ﬁlter provides a +2 point boost, corroborating the ﬁndings of Conneau et al. and
Raffel et al. that ﬁltering low-quality pages
from Common Crawl is valuable. Increasing the
language sampling exponent α to 0.7 has the expected effect of improving performance in highresource languages (e.g. Russian 81.5 →82.8),
while hurting low-resource languages (e.g. Swahili
75.4 →70.6), with the average effect being negative. Conversely, lowering α to 0.2 boosts one
tail language slightly (Urdu 73.5 →73.9) but is
harmful elsewhere. Detailed per-language metrics
on XNLI and the results of our ablations on zeroshot XQuAD are provided in Appendix C, showing
similar trends.
Zero-Shot Generation
Since mT5 is a generative model, it can output
arbitrary text predictions in a free form fashion.
This is in contrast to “encoder-only” models like
mBERT and XLM(-R) that make a prediction by either extracting it from the input or producing a class
label. We found that the lack of constraints during
prediction caused mT5 to sometimes have trouble
generating a well-formed prediction in a language
unseen during ﬁne-tuning. Focusing on XQuAD
zero-shot, we ﬁnd that many of these errors are
due to “accidental translation” into the ﬁne-tuning
language (English). In this section, we characterize
this behavior and demonstrate that it can be counteracted by mixing a small amount of our multilingual
pre-training task into the ﬁne-tuning stage.
Illegal Predictions
In using a generative model for span selection (as
in extractive QA tasks), we hope the model learns
to generate “legal” spans that are substrings of the
provided context. However, unlike encoder-based
models like BERT, this is not a hard constraint of
Prediction
Explanation
จํานวนเฉพาะ
จํานวนเฉพาะ
Decomposed Thai ํา into ํ + า
लोथर डे माइिज़यर
लोथर डे माइिज़यर
Decomposed Hindi ज़ into ज + ◌़
Replaced full-width percent sign
Removed superscript
اﻟﻼھﻮاﺋﯿﺔ ﻟﻠﺒﻜﺘﺮﯾﺎ اﻟﻼھﻮاﺋﯿﺔ اﻟﺒﻜﺘﺮﯾﺎArabic “for anaerobic bacteria”
⇒ “anaerobic bacteria”
строками битов
строки битов
Russian “bit strings (instrumental)”
⇒ “bit strings (nominative)”
Translated from Spanish
Zweiten Weltkrieg
the Second World War
Translated from German
New英格兰爱国者队
Partially translated Chinese
“New England Patriots”
хлоропласт
chlorопласт
Partially translated Russian
“chloroplast”
Table 5: Illegal mT5-XXL predictions on XQuAD zeroshot, illustrating normalization (top), grammatical adjustment (middle) and translation (bottom).
the model. Notably, T5 learns to always output
legal spans on SQuAD, suggesting this is not a
major issue for generative models in simple cases.
A more challenging case for generative models is
zero-shot cross-lingual span selection. Here, a pretrained multilingual model is ﬁne-tuned on English
but tested on other languages. We want the model
to generate legal non-English predictions despite
having only seen English targets in ﬁne-tuning.
In practice, while mT5 achieves SOTA on
the zero-shot variants of XQuAD, MLQA and
TyDi QA, illegal predictions are still a problem. For
example, on zero-shot XQuAD, a non-trivial portion of mT5 mistakes are in fact illegal spans, for all
model sizes (cf. ﬁg. 4 “Baseline”). Through inspection, we ﬁnd these illegal predictions mainly fall
into three categories: (i) normalization, (ii) grammatical adjustment, and (iii) accidental translation.
Table 5 provides examples of each type.
Normalization indicates predictions that would
be legal, except that “equivalent” Unicode characters have been substituted, so a legal span may be
recovered through Unicode NFKC normalization.
This is particularly common in Thai, Chinese and
Hindi, where most mT5-XXL illegal predictions
are resolved by normalization, as seen in ﬁg. 3b.
Grammatical adjustment involves minor morphological changes to the original text. We frequently observe these adjustments when the target
span cannot stand as a well-formed answer on its
own. For example, mT5-XXL’s Arabic and Russian
predictions in the middle rows of table 5 are judged
by native speakers as correct and grammatical answers to the posed XQuAD questions, while the
gold targets are judged as ungrammatical answers.
This type of illegal prediction is most common in
Illegal after norm
(a) mT5-Small
(b) mT5-XXL
Figure 3: Per-language error rates on XQuAD zeroshot, sorted by illegal rate. Incorrect: Not matching
the target span. Illegal: Missing from the input context.
Illegal after norm: Illegal even after Unicode NFKC
normalization is applied to the prediction and context.
languages with extensive grammatical case marking, such as Russian, Turkish and German.
Accidental translation involves the model
translating part or all of a contextual span into English (the language of all ﬁne-tuning data). On
the one hand, it is remarkable that mT5 performs
“spontaneous” translation despite never seeing parallel training data. On the other, as practitioners we
would ideally be able to control this behavior.
We observe accidental translation across all
model sizes and all XQuAD languages. The problem is most prevalent in mT5-Small and mT5-Base,
where from manual inspection, half or more of the
illegal predictions within each language exhibit
accidental translation, with many of the illegal predictions coming from Greek and Russian, as shown
in ﬁg. 3a. While we do observe full phrase translations, a more common occurrence is partial translation, where the model outputs a token or two of
English before reverting to the correct target language. The transition may even occur mid-word,
as in the prediction “chlorопласт”, where the ﬁrst
half of the target “хлоропласт” (Russian: chloroplast) has been translated to English.
Preventing Accidental Translation
The most direct solution to avoiding accidental
translation on span selection tasks would be to modify our inference procedure. As is common practice
with encoder-based models, we could devise a taskspeciﬁc ﬁne-tuning mechanism that restricts the
model to perform ranking over legal spans, removing the possibility of illegal predictions entirely.
While this would likely improve our zero-shot metrics, it is unsatisfying for two reasons: First, it
implies taking a step backward from the general
text-to-text interface, as different tasks would demand different types of inference. Second, this
solution won’t extend to more “open-ended” zeroshot generative tasks like summarization, where
the legal output space can’t be easily delimited.
For these reasons, we consider a more general
solution that remains within the text-to-text framework and can apply to all zero-shot generation
tasks. Our motivating intuition is that the reason the
model outputs English when given a non-English
test input is that it has never observed a non-English
target during ﬁne-tuning. As English-only ﬁnetuning proceeds, the model’s assigned likelihood
of non-English tokens presumably decreases, eventually reaching the point where English becomes
the most likely answer to any question.
To prevent the model from “forgetting” how to
generate other languages, we use a strategy inspired
by domain/task-adaptive pre-training : We simply
mix in our unsupervised multilingual pre-training
task during ﬁne-tuning. A similar approach was
explored by Liu et al. . We use the same
mC4 task deﬁnition as in pre-training, with two
adjustments: First, we remove all “sentinel” tokens
(corresponding to non-masked spans in the input
text) from the target sequence, as otherwise we
observe occasional sentinels in downstream predictions. Second, we reduce the language sampling
parameter α from 0.3 to 0.1. This produces a nearuniform distribution of languages, encouraging the
model to treat all languages as equally likely.10
With these changes, we mix a small amount of
our unsupervised task (covering 101 languages)
into XQuAD ﬁne-tuning, at a ratio of just 1:100.
Figure 4 shows the results on XQuAD zero-shot error rates. The addition of even this small amount of
multilingual data has a marked effect on the mT5-
Small and mT5-Base models (where accidental
translation was most rampant), reducing the illegal
prediction rates by more than 70% (relative), and
contributing to an overall reduction in errors.
10Alternatively, one could mix in unlabeled data only for a
single language at a time. However, we believe this is contrary
Baseline Incorrect
Baseline Illegal
Baseline Illegal after norm
DPT Incorrect
DPT Illegal
DPT Illegal after norm
Figure 4: Error rates of mT5 on XQuAD zero-shot.
Baseline: Fine-tuning on XQuAD alone. Domain Preserving Training (DPT): Mixing in the unsupervised
mC4 task with ﬁne-tuning.
Conclusion
In this paper, we introduced mT5 and mC4: massively multilingual variants of the T5 model and
C4 dataset. We demonstrated that the T5 recipe is
straightforwardly applicable to the multilingual setting, and achieved strong performance on a diverse
set of benchmarks. We also characterized illegal
predictions that can occur in zero-shot evaluation
of multilingual pre-trained generative models, and
described a simple technique to avoid this issue.
We release all code and pre-trained datasets used in
this paper to facilitate future work on multilingual
language understanding.11
Acknowledgements
We thank Melvin Johnson for tips on the translatetrain procedure for XTREME and Itai Rolnick for
help with infrastructure.