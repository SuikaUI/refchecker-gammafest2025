Journal of Machine Learning Research (XXXX)
Submitted XX; Published XXX
Universal Kernels for Multi-Task Learning
Andrea Caponnetto
 
Department of Computer Science
The University of Chicago
1100 East 58th Street, Chicago, IL 60637,
and DISI, Universit`a di Genova
Via Dodecaneso 35, 16146 Genova, Italy
Charles A. Micchelli
 
Department of Mathematics and Statistics
State University of New York
The University at Albany
Albany, New York 12222, USA
Massimiliano Pontil
 
Department of Computer Science
University College London
Gower Street, London, WC1E 6BT, UK
Yiming Ying
 
Department of Engineering Mathematics
University of Bristol
Queen’s Building, Bristol, BS8 1TR, UK
In this paper we are concerned with reproducing kernel Hilbert spaces HK of functions
from an input space into a Hilbert space Y, an environment appropriate for multi-task
learning. The reproducing kernel K associated to HK has its values as operators on Y.
Our primary goal here is to derive conditions which ensure that the kernel K is universal.
This means that on every compact subset of the input space, every continuous function with
values in Y can be uniformly approximated by sections of the kernel. We provide various
characterizations of universal kernels and highlight them with several concrete examples
of some practical importance. Our analysis uses basic principles of functional analysis and
especially the useful notion of vector measures which we describe in suﬃcient detail to
clarify our results.
Keywords. Multi-task learning, operator-valued kernels, universal approximation, vectorvalued reproducing kernel Hilbert spaces.
c⃝XXXX Andrea Caponnetto, Charles A. Micchelli, Massimiliano Pontil, and Yiming Ying.
Caponnetto, Micchelli, Pontil, and Ying
1. Introduction
The problem of studying representations and methods for learning vector-valued functions
has received increasing attention in Machine Learning in the recent years. This problem is
motivated by several applications in which it is required to estimate a vector-valued function
from a set of input/output data. For example, one is frequently confronted with situations
in which multiple supervised learning tasks must be learned simultaneously. This problem
can be framed as that of learning a vector-valued function f = (f1, f2, . . . , fn), where each of
its components is a real-valued function and corresponds to a particular task. Often, these
tasks are dependent on each other in that they share some common underlying structure.
By making use of this structure, each task is easier to learn. Empirical studies indicate that
one can beneﬁt signiﬁcantly by learning the tasks simultaneously as opposed to learning
them one by one in isolation, see e.g. and references therein.
In this paper, we build upon the recent work of Micchelli and Pontil concerning
the study of reproducing kernel Hilbert spaces (RKHS) of vector-valued functions . These
functions are deﬁned on a Hausdorﬀspace X and take values on a Hilbert space Y with
inner product (·, ·) (non necessarily separable or ﬁnite dimensional). The kernel associated
to an RKHS is a function K deﬁned on X × X which takes values as operators on Y and
satisﬁes certain properties which shall be reviewed in Section 2. The RKHS is formed by
taking the closure of the linear span of kernel sections {K(·, x)y, x ∈X, y ∈Y}, relative to
the RKHS norm. We refer to such a kernel as an operator-valued kernel and matrix-valued
kernel if Y is a ﬁnite dimensional Hilbert space. In , it was
emphasized that the algebra of operator-valued kernels is, in general, non-Abelian. This
implies that the structure of the RKHS is richer than that obtained by considering each
coordinate of the vector–valued function to be in an RKHS space of real-valued functions.
The ﬂexibility in designing operator-valued kernels makes it possible to model smoothness
properties relative to both the input space X and output space Y. For example, returning
to the application to multi-task learning, in which Y = Rn, it is possible to choose the
kernel so that certain relationships are modeled across the tasks.
In this paper, we study conditions on the kernel K which ensure that its associated
RKHS is dense in the space of continuous functions. This problem was proposed by Poggio
et al. and, in the particular scalar case (Y = R), studied by Micchelli and Pontil
 ; Micchelli et al. ; Steinwart ; Zhou .
The condition that the RKHS with kernel K is dense in C(Z, Y) means that we can
approximate arbitrarily well any continuous function with a function in the RKHS. That
is, for every continuous function f : Z →Y and any ϵ > 0 there exists a function g in
span{K(·, x)y : x ∈Z, y ∈Y} such that max{∥f(x) −g(x)∥: x ∈Z} ≤ϵ. Then, we say
that the kernel K is a universal kernel if, for every compact subset Z of X, its associated
RKHS is dense in the space of continuous functions.
Because of our discussion above, the density problem for an RKHS of vector-valued
functions cannot be merely reduced to study the density problems of each scalar component.
Hence, in this paper we aim at studying a uniﬁed framework for this problem which includes
the scalar case as a special case.
Universal operator-valued kernels
The density problem is important both practically and theoretically. In practice, it is
useful to know whether the kernel is dense in the space of continuous functions or in some
linear subspace of it. The techniques that we present in this paper can be used to study the
density of problem relative to an arbitrary such a subspace. From a theoretical point of view
the density problem is important in order to study the consistency of learning algorithms
based on RKHS. For example in a detailed analysis of
the regularized least-squares algorithm over vector-valued RKHS was developed, showing
in particular that the density property of the RKHS implies universal consistency of the
algorithm.
The paper is organized as follows.
In Section 2, we review the basic deﬁnition and
properties of operator-valued kernels and deﬁne the notion of universal kernel. Moreover,
we describe some examples of such kernels. In Section 3, we introduce the notion of feature
map associated to an operator-valued kernel and show its relevance to the density problem.
The main result in this section is Theorem 5, which establishes that the closure of the RKHS
in the space of continuous functions is the same as the closure of the space generated by
the feature map. Our analysis, which uses basic principles of functional analysis, is useful
because in many circumstances it may be easier to verify universality through the feature
representation than the kernel sections themselves. In Section 4 we provide an alternate
proof of Theorem 5 which use the notion of vector measures and derive Corollary 12, which
establishes the main tool used to verify the universality of a kernel. Finally, In section 5 we
highlight our result with several concrete examples of some practical importance.
2. RKHS of Vector-Valued Functions
In this section, we review the theory of reproducing kernel Hilbert spaces of vector-valued
functions as in and introduce the notion of universal kernels.
We begin by introducing some notation. We let Y be a Hilbert space with inner product
(·, ·)Y (we drop the subscript Y when confusion does not arise). The vector-valued functions
will take values on Y. We denote by L(Y) the space of all bounded linear operators from
Y into itself, with the operator norm ∥L∥:= sup∥y∥=1 ∥Ly∥, and by L+(Y) the set of all
bounded, positive semi-deﬁnite linear operators, that is, L ∈L+(Y) provided that, for any
y ∈Y, (y, Ly) ≥0. We also denote, for any L ∈L(Y), by L∗its adjoint.
Deﬁnition 1. We say that a function K : X × X →L(Y) is an operator-valued kernel on
X if K(x, t)∗= K(t, x) for any x, t ∈X, and it is positive semi-deﬁnite, that is, for any
m ∈N, {xj : j ∈Nm} ⊆X and {yj : j ∈Nm} ⊆Y there holds
(yi, K(xi, xj)yj) ≥0.
For any t ∈X and y ∈Y, we introduce the mapping Kty : X →Y deﬁned, for every
x ∈X by (Kty)(x) := K(x, t)y. We denote by HK the RKHS of vector-valued functions
associated with the operator-valued kernel K. In the spirit of Moore-Aronszjain’s theorem,
HK is the completion of the linear span of kernel sections {Kty : t ∈X, y ∈Y} with inner
product ⟨·, ·⟩K induced by the equation
⟨Kty, f⟩K = (y, f(t)),
∀f ∈HK, t ∈X, y ∈Y.
Caponnetto, Micchelli, Pontil, and Ying
The following proposition collects some useful properties of the kernel which we shall
use in our development.
Proposition 2. Let K be an operator-valued kernel and, for any x ∈X, Kx be deﬁned as
above. The following properties hold true for every f ∈HK and x, t ∈X.
(a) ∥K(x, t)∥≤∥K(x, x)∥
2 ∥K(t, t)∥
(b) ∥f(x)∥≤∥f∥K∥K(x, x)∥
(c) ∥f(x) −f(t)∥≤∥K(x, x) + K(t, t) −K(x, t) −K(t, x)∥
Proof. For the proof of (a) and (b) see . As for the last
property, we observe by equation (2) that
(y, f(x) −f(t)) = ⟨(Kx −Kt)y, f⟩K ≤∥Kxy −Kty∥K∥f∥K.
Moreover, we have that
∥Kxy −Kty∥2
= ((K(x, x) + K(t, t) −K(x, t) −K(t, x))y, y)
≤∥K(x, x) + K(t, t) −K(x, t) −K(t, x)∥∥y∥2.
Therefore, combining this equation with equation (3) and setting y∥f(x) −f(t)∥=
f(x) −f(t), the result follows.
Throughout this paper, we assume that the kernel K is continuous relative to the operator norm and, for any compact set Z ⊆X, we introduce the ﬁnite quantity κ(Z) :=
supx∈Z ∥K(x, x)∥
2 . This hypothesis implies that any function f ∈HK is continuous by
property (c) in Proposition 2, and that it is uniformly bounded by κ(Z)∥f∥K on any compact set Z, by property (b) in Proposition 2.
We now return to the formulation of the density problem. For this purpose, we recall
that C(Z, Y) is the Banach space of continuous Y-valued continuous function on a compact
subset Z of X with the maximum norm, deﬁned by ∥f∥∞,Z := supx∈Z ∥f(x)∥Y. We also
deﬁne, for every operator-valued kernel K, the subspace of C(Z, Y)
CK(Z, Y) := span{Kxy : x ∈Z, y ∈Y},
where the closure is relative to the norm in the space C(Z, Y).
Deﬁnition 3. We say that an operator-valued kernel K is a universal kernel if, for any
compact subset Z of X, CK(Z, Y) = C(Z, Y).
In the special case that Y = Rn, the kernel function K takes values as n × n matrices.
The corresponding matrix elements can be identiﬁed by the formula
(K(x, t))pq = ⟨Kxep, Kteq⟩K,
where ep, eq are the standard coordinate bases in Rn, for p, q ∈Nn.
Universal operator-valued kernels
In order to describe some of the examples of operator-valued kernels below, it is useful
to ﬁrst present the following generalization of Schur product of scalar kernels to matrixvalued kernels. For this purpose, for any i ∈Nm we let yi = (y1i, y2i, . . . , yni) ∈Rn so that
equation (1) is equivalent to
ypi(K(xi, xj))pqyqj ≥0.
From the above observation, we conclude that K is a kernel if and only if
(K(xi, xj)p,q
a new matrix with row index (p, i) and column index (q, j) is positive semi-deﬁnite.
Proposition 4. Let G and K be n × n matrix-valued kernels and the element-wise product
kernel K ◦G : X × X →Rn × Rn be deﬁned, for any x, t ∈X and p, q ∈Nn, by
pq. Then, K ◦G is a matrix-valued kernel.
Proof. We have to check the semi-positiveness of K ◦G. To see this, for any m ∈N,
{yi ∈Rn : i ∈Nm} and {xi ∈X : i ∈Nm} we observe that
(yi, K ◦G(xi, xj)yj) =
is positive semi-deﬁnite as a new matrix with (p, i) and (q, j) as
row and column indices, and so is
. Applying the Schur Lemma to these new matrices implies that equation (5) is nonnegative, and hence proves the
assertion.
In the following, we present some examples of operator-valued kernels. They will be
used in Section 5 to illustrate the general results in Sections 3 and 4.
The ﬁrst example is adapted from .
Example 1. If, for every j ∈Nm the function Gj : X × X →R is a scalar kernel and
Bj ∈L+(Y), then the function
BjGj(x, t),
is an operator-valued kernel.
The operators Bj model smoothness across the components of the vector-valued function. For example, in the context of multi-task learning , we set Y = Rn, hence Bj are n × n matrices1. These matrices model
the relationships across the tasks. Evgeniou et al. considered kernels of the form (6)
with m = 2, B1 a multiple of the identity matrix and B2 a low rank matrix. A speciﬁc case
for X = Rd is
(K(x, t))p,q = λx · t + (1 −λ)δpq(x · t)2,
1. In the context of multi-task learning the notation multi-task kernel has been used in place of matrixvalued kernel .
Caponnetto, Micchelli, Pontil, and Ying
where x·t is the standard inner product in Rd and λ ∈ . This kernel has an interesting
interpretation. Using only the ﬁrst term in the right hand side of the above equation (λ = 1)
corresponds to learning all tasks as the same task, that is, all components of the vectorvalued function f = (f1, . . . , fn) are the same function, which will be a linear function since
the kernel G1 is linear. Whereas, using only the second term (λ = 0) corresponds to learn
independent tasks, that is, the component of the function f will be in general diﬀerent
functions. These functions will be quadratic since G2 is a quadratic polynomial kernel.
Thus, the above kernel combines two “opposite” kernels to form a more ﬂexible one. By
choosing the parameter λ appropriately, the learning model can be tailored to the data at
We note that if the kernel K is a diagonal matrix-valued function, each component
of a vector-valued function in the RKHS of K can be represented, independently of the
other components, as a function in the RKHS of a scalar kernel. However, in general, an
operator-valued kernel will not be diagonal and, more importantly, will not be reduced to a
diagonal one by linearly transforming the output space. For example, the kernel in equation
(6) cannot be reduced to a diagonal kernel, unless all the matrices Bj, j ∈Nm can all be
transformed into a diagonal matrix by the same transformation.
Therefore, in general,
the component functions share some underling structure which is contained in the kernel
and cannot be treated as independent objects. This fact is further illustrated by the next
Example 2. Let X0 be a compact Hausdroﬀspace, Tp be a map from X from X0 (not
necessary linear) for p ∈Nn and G : X0 × X0 →R be a scalar kernel. Then,
K(x, t) :=
G(Tpx, Tqt)
is a matrix-valued kernel on X.
A speciﬁc instance of the above example is described by Vazquez and Walter in
the context of system identiﬁcation. It corresponds to the choices that X0 = X = R and
Tp(x) = x + τp, where τp ∈R. In this case, the kernel K models “delays” between the
components of the vector-valued function. Indeed, it is easy to verify that, for this choice,
for all f ∈HK and p ∈Nn,
fp(x) := (f(x), ep) = h(x −τp),
where h is a scalar-valued function in the RKHS of kernel G.
Other choices of matrix Tp are possible and provide interesting extensions of scalar
kernels. For example, we make the choice K(x, t) := (eσpq⟨x,t⟩: p, q ∈Nn), where σ = (σpq)
a positive semi-deﬁnite matrix.
In view of the eigenvalue decomposition of the matrix
i=1 λiuiuT
i , for each i, p ∈Nn we can deﬁne the map T (i)
p x := √λiuipx for
any x ∈X. Therefore, we obtain that K(x, t) = (Qn
i=1 e⟨T (i)
t⟩: p, q ∈Nn) and, so, by
Proposition 4, we conclude that K is a matrix-valued kernel.
It is interesting to note, in passing, that, although one would expect the function
K(x, t) :=
e−σpq∥x−y∥2´n
Universal operator-valued kernels
to be a kernel over X = Rd, we will show later in Section 5 that this is not true, unless all
entries of the matrix σ are the same.
The next example is taken from and was motivated by the construction
of divergence free vector-valued radial basis functions.
Example 3. We let Y = X = Rn, and, for any x = (xp : p ∈Nn) ∈X, G(x) = exp(−∥x∥2
with σ > 0. Then, the Hessian matrix given by
K(x, t) := (−(∂p∂qG)(x −t) : p, q ∈Nn) ∀x, t ∈X
is a matrix-valued kernel.
To illustrate our ﬁnal example we let L2(R) be the Hilbert space of square integrable
functions on R with the norm ∥h∥L2 =
R h2(x)dx. Moreover, we denote by W 1(R) the
Sobolev space of order 1, which is deﬁned as the space of real-valued functions h on R
whose norm
L2 + ∥h′∥2
Example 4. Let Y = L2(R), X = R and consider the linear space of functions from R to
Y which have ﬁnite norm
∥f(x, ·)∥2
Then this is an RKHS and the corresponding operator-valued kernel is given, every x, t ∈X
(K(x, t)y)(r) = e−π|x−t|
e−π|r−s|y(s)ds,
∀y ∈Y, r ∈R.
We note that the above example is of the same form as Example 1.
More general
examples for the choice that Y = L2(Rd) will be provided in Section 5.
As an application of the above example, consider the case that the input valuable x
represents time and the output is the heat distribution in a medium. Another application,
which generalizes the multi-task example above is one in which the input represents a
parameterized task (e.g. the proﬁle identifying a customer) and the output is the regression
function associated to that task. So, this example is amenable for learning a continuity of
We postpone the proofs of these examples and the discussion about the universality of
the kernels described therein to Section 5.
3. Kernels by Feature Maps
In this section, we prove that an operator-valued kernel is universal if and only if its feature
representation is universal. To explain what we have in mind, we require some additional
We let W be a Hilbert space and L(Y, W) be the set of all bounded linear
operators from Y to W. A feature representation associated with an operator-valued kernel
K is a continuous function
Φ : X →L(Y, W)
Caponnetto, Micchelli, Pontil, and Ying
such that, for every x, t ∈X
K(x, t) = Φ∗(x)Φ(t).
where, we recall, for each x ∈X, Φ∗(x) is the adjoint of Φ(x) and, therefore, it is in L(W, Y).
Hence, we call W the feature space later on. In the case that Y = R, the condition that
Φ(x) ∈L(Y, W) can be merely viewed as saying that Φ(x) in an element of W. Therefore,
at least in this case we can rewrite equation (10) as
K(x, t) = (Φ(x), Φ(t))W.
Another example of practical importance corresponds to the choice W = Rm and Y = Rn,
both ﬁnite dimensional Euclidean spaces. Here we can identify Φ(x) relative to standard
basis of W and Y with the m × n matrix Φ(x) = (Φrp(x) : r ∈Nm, p ∈Nn), where
Φrp are scalar-valued continuous functions on X. Therefore, according to (10) the matrix
representation of the operator-valued kernel K is, for each x, t ∈X,
(K(x, t))pq =
Φrp(x)Φrq(t),
Returning to the general case, we emphasize that we assume that the kernel K has the
representation in equation (10), although if it corresponds to a compact integral operator
such a representation will follow from the spectral theorem and Mercer Theorem, see e.g.
 .
Associated with a feature representation as described above is the following closed linear
subspace of C(Z, Y)
CΦ(Z, Y) :=
Φ∗(·)w : w ∈W
where the closure is taken relative to the norm of C(Z, Y), and the continuity of the functions
Φ∗(·)w follows from the assumed continuity of K(·, ·) by the inequality
∥Φ∗(x)w −Φ∗(t)w∥2
≤∥Φ∗(x) −Φ∗(t)∥2∥w∥2
= ∥(Φ∗(x) −Φ∗(t))(Φ(x) −Φ(t))∥∥w∥2
= ∥K(x, x) + K(t, t) −K(x, t) −K(t, x)∥∥w∥2
The meaning of the phrase “the feature representation is universal” is that CΦ(Z, Y) =
C(Z, Y) for every compact subset Z of the input space X. The theorem below demonstrates,
as we mention above, that the kernel K is universal if and only if its feature representation
is universal.
Theorem 5. If K is a continuous operator-valued kernel with feature representation Φ,
then for every compact subset Z of X, we have that CK(Z, Y) = CΦ(Z, Y).
Proof. The theorem follows straightforwardly from Lemmas 6, 7 and 8.
As we know, the feature representation of a given kernel is not unique, therefore we conclude
by Theorem 5 that if some feature representation of an operator-valued kernel is universal
then every feature representation is universal.
We shall give two diﬀerent proofs of this general theorem.
The ﬁrst one will use a
technique highlighted in and will be given in this section. The
Universal operator-valued kernels
second proof will be given in the next section and uses the notion of vector measure. Both
approaches adopt the point of view of Micchelli et al. , in which Theorem 5 is proved
in the special case that Y = R.
We now begin to explain in detail our ﬁrst proof. We denote the unit ball in Y by
B1 := {y : y ∈Y, ∥y∥≤1} and let Z be a prescribed compact subset of X. Recall that B1
is not compact in the norm topology on Y unless Y is ﬁnite dimensional, but it is compact
in the weak topology on Y since Y is a Hilbert space . Remember that
a basis for the open neighborhood of the origin in the weak topology is a set of the form
{y : y ∈Y, |(y, yi)| ≤1, i ∈Nm}, where y1, . . . , ym are arbitrary vectors in Y. We put on
B1 the weak topology and conclude, by Tychonoﬀtheorem ,
that the set Z × B1 is also compact in the product topology.
The above observation allows us to associate Y-valued functions deﬁned on Z to scalarvalued functions deﬁned on Z × B1. Speciﬁcally, we introduce the map γ : C(Z, Y) →
C(Z × B1) which maps any function f ∈C(Z, Y) to the function γ(f) ∈C(Z × B1) deﬁned
by the action (x, y) 7→(f(x), y)Y. Consequently, it follows that the map γ is isometric,
∥f(x)∥Y = sup
|(f(x), y)Y| = sup
|γ(f)(x, y)|,
where the ﬁrst equality follows by Cauchy-Schwarz inequality. Moreover, we will denote by
Cγ(Z × B1) the image of C(Z, Y) under the mapping γ. In particular, this space is a closed
linear subspace of C(Z × B1) and, hence, a Banach space.
Similarly, to any operator-valued kernel K on Z we associate a scalar kernel J on Z ×B1
deﬁned, for every (x, y), (t, u) ∈X × B1, as
J((x, y), (t, u)) := (K(x, t)u, y).
Moreover, we denote by CJ(Z × B1) the closure in C(Z × B1) of the set of the sections of
the kernel, {J((x, y), (·, ·)) : (x, y) ∈Z × B1}. It is important to realize that whenever K is
a valid operator-valued kernel, then J is a valid scalar kernel.
The lemma below relates the set CK(Z, Y) to the corresponding set CJ(Z × B1) for the
kernel J on Z × B1.
Lemma 6. If Z is a compact subset of X and K is a continuous operator-valued kernel
then γ(CK(Z, Y)) = CJ(Z × B1).
Proof. The assertion follows by equation (14) and the continuity of the map γ.
In order to prove Theorem 5, we also need to provide a similar lemma for the set
Before we state the next lemma, we note that knowing the features of the
operator-valued kernel K leads us to the features for the scalar-kernel J associated to K.
Speciﬁcally, for every (x, y), (t, u) ∈X × B1, we have that
J((x, y), (t, u)) = (Ψ(x, y), Ψ(t, u))W,
where the continuous function Ψ : X × B1 →W is deﬁned as
Ψ(x, y) = Φ(x)y,
x ∈X, y ∈B1.
Caponnetto, Micchelli, Pontil, and Ying
Thus, equation (15) parallels equation (11) except that X is replaced by X × B1. We also
denote by CΨ(Z × B1) the closed linear subspace of C(Z × B1),
(Ψ(·), w)W : w ∈W
Lemma 7. If Z is a compact subset of X and K is a continuous operator-valued kernel
with feature representation Φ then γ(CΦ(Z, Y)) = CΨ(Z × B1).
Proof. The proof is immediate. Indeed, for each x ∈X, w ∈W, y ∈Y, we have that
(Φ∗(x)w, y)Y = (w, Φ(x)y)W = (Ψ(x, y), w)W.
Aimed at proving the last result used in the proof of Theorem 5, we review some facts
about signed measures and bounded linear functionals on continuous functions. Let Ωbe
any prescribed compact Hausdorﬀspace and C(Ω) the space of all real-valued continuous
functions with norm ∥· ∥∞,Ω. We also use the notation B(Ω) to denote the Borel σ-algebra
on Ω. Now, we recall the description of the dual space of C(Ω). By the Riesz representation
theorem the linear functional L in the dual space of C(Ω) is identiﬁed as a regular signed
Borel measure ν on Ω ), that is,
g(x)dν(x),
The variation of ν is given, for any E ∈B(Ω), by
|ν|(E) := sup
|ν(Aj)| : {Aj : j ∈N} pairwise disjoint and ∪j∈N Aj = E
and ∥L∥= ∥ν∥where ∥ν∥= |ν|(Ω) and ∥L∥is the operator norm of L deﬁned by ∥L∥=
sup∥g∥∞,Ω=1 |L(g)|. Recall that a Borel measure ν is regular if, for any E ∈B(X),
ν(E) = inf
ν(U) : E ⊆U, U open
ν( ¯U) : ¯U ⊆E, ¯U compact
In particular, every ﬁnite Borel measure on Ωis regular, see Folland . We
denote by M(Ω) the space of all regular signeded measures on Ωwith total variation norm.
Proposition 8. For any compact set Z ⊆X, and any continuous operator-valued kernel
K with feature representation Φ, we have that CΨ(Z × B1) = CJ(Z × B1).
Proof. For any compact set Z ⊆X, recall that Z ×B1 is compact if B1 is endowed with the
weak topology of Y. Hence, the result follows by applying to the scalar kernel J on the set Z ×B1 with the feature representation given by equation
(15). However, for the convenience of the reader we review the step of the argument used
to prove that theorem. The basic idea is the observation that two closed subspaces of a
Banach space are equal if and only if whenever a continuous linear functional vanishes on
either one of the subspaces, it must also vanish on the other one. This is a consequence of
the Hahn-Banach Theorem . In the case at hand, we know by the Riesz
Representation Theorem that all continuous linear functionals L on C(Z ×B1) are given by
a regular signed Borel measure ν, that is for every F ∈C(Z × B1), we have that
F(x, y)dν(x, y).
Universal operator-valued kernels
Now, suppose that L vanishes on CJ(Z × B1), then we conclude, by (15), that
(Ψ(x, y), Ψ(t, u))Wdν(x, y)dν(t, u).
However, we also have that
(Ψ(x, y), Ψ(t, u))Wdν(x, y)dν(t, u) =
Ψ(x, y)dν(x, y)
and, so, we conclude that
Ψ(x, y)dν(x, y) = 0.
The proof of equation (16) and the interpretation of the W-valued integral appearing in
equation (17) is explained in detail in Micchelli et al. .
So, we conclude that L
vanishes on CΨ(Z × B1).
Conversely, if L vanishes on CΨ(Z × B1) then, for any x ∈Z, y ∈B1, we have that
J((x, y), (t, u))dν(t, u) =
Ψ(t, u)ν(t, u)
that is, L vanishes on CJ(Z × B1).
4. Characterizations of Universality
In this section, we provide an alternate proof of Theorem 5.
We feel that this task is
worthwhile as it seems appropriate to work directly in the space C(Z, Y) rather than bypass
it and retreat to more familiar space C(Z ×B1). The basic principle is the same, namely, we
know that two linear subspaces of a Banach space are equal as long as whenever a bounded
linear functional vanishes on one of them, it vanishes on the other. Thus, to implement
this principle we are led to consider the dual space of C(Z, Y). To this end, we introduce
the notion of vector measures and rely upon the information about them from . Before we get to the deﬁnition of vector measures we mention in passing
that the need to have a variable description of the dual space of C(Z, Y) arose also in the
context of the feature space perspective for learning the kernel, see (Micchelli and Pontil,
To introduce our ﬁrst deﬁnition, recall that throughout this paper X denotes a Hausdorﬀ
space, Z ⊆X any compact subset of X and B(Z) the Borel σ-algebra of Z.
Deﬁnition 9. A map µ : B(Z) →Y is called a Borel vector measure if µ is countably
additive, that is, µ(∪∞
j=1Ej) = P∞
j=1 µ(Ej) in the norm of Y, for all sequences {Ej : j ∈N}
of pairwise disjoint sets in B(Z)
It is important to note that the deﬁnition of vector measures given in Diestel and Uhl, Jr.
 only requires it to be ﬁnitely additive. For our purpose here, we only use countably
additive measures and thus do not require the general concept of Diestel and Uhl, Jr. .
Caponnetto, Micchelli, Pontil, and Ying
For any vector measure µ, the variation of µ is deﬁned, for any E ∈B(Z), by the
|µ|(E) := sup
∥µ(Aj)∥: {Aj : j ∈N} pairwise disjoint and ∪j∈N Aj = E
In our terminology we conclude from that µ is a vector
measure if and only if the corresponding variation |µ| is a scalar measure as explained in
Section 3. Whenever |µ|(Z) < ∞, we call µ a vector measure of bounded variation on Z.
Moreover, we say that a Borel vector measure µ on Z is regular if its variation measure |µ|
is regular as deﬁned in Section 3. We denote by M(Z, Y) the Banach space of all vector
measures with bounded variation and norm ∥µ∥:= |µ|(Z).
For any scalar measure ν ∈M(Z ×B1), we deﬁne a Y-valued function on B(Z), by the
ydν(x, y),
Let us conﬁrm that µ is indeed a vector measure. For this purpose, choose any sequence of
pairwise disjoint subsets {Ej : j ∈N} ⊆B(Z), and observe that
∥µ(Ej)∥Y ≤
dν(x, y)∥Y ≤|ν|(Z × B1),
which implies that |µ|(Z) is ﬁnite and, hence, µ is a regular vector measure. This observation
suggests that we deﬁne, for any f ∈C(Z, Y), the integral of f relative to µ as
(f(x), dµ(x)) :=
(f(x), y)dν(x, y).
By the standard techniques of measure theory, for any vector measure µ ∈M(Z, Y) the
integral on the right-hand side is well-deﬁned. One of our goals below is to show that given
any vector measure µ, there corresponds a scalar measure ν such that equation (19) still
holds. Before doing so, let us point out a useful property about the integral appearing in the
left hand side of equation (19). Speciﬁcally, for any y ∈Y, we associate to any µ ∈M(Z, Y),
a scalar measure µy deﬁned, for any E ∈B(Z), by the equation µy(E) := (y, µ(E)).
Therefore, we conclude, for any f ∈C(Z), that
(yf(x), dµ(x)) =
f(x)dµy(x).
To prepare for our description of the dual space of C(Z, Y), we introduce, for each
f ∈C(Z, Y), a linear functional L deﬁned by,
(f(x), dµ(x)).
Lemma 10. If µ ∈M(Z, Y) then Lµ ∈C∗(Z, Y) and ∥Lµ∥= ∥µ∥.
Universal operator-valued kernels
Proof. By the deﬁnition of the integral appearing in the right-hand side of equation it
follows (20) , for any f ∈C(Z, Y), that
|Lµf| ≤∥µ∥sup
Therefore, we obtain that ∥Lµ∥≤∥µ∥, and thus Lµ ∈C∗(Z, Y).
To show that ∥L∥= ∥µ∥, it remains to establish that ∥µ∥≤∥Lµ∥. To this end, for
any ε > 0 and, by the deﬁnition of ∥µ∥, we conclude that there exist pairwise disjoint sets
{Aj : j ∈Nn} such that ∪j∈NnAj ⊆Z and ∥µ∥:= |µ|(Z) ≤ε + P
j∈Nn ∥µ(Aj)∥Y. We
introduce the function g = P
∥µ(Aj)∥Y χAj which satisﬁes, for any x ∈Z, the bound
∥g(x)∥Y ≤1. Since |µ| is a regular measure on Z, applying Lusin’s theorem to the function χAj, there exists a real-valued continuous function fj ∈C(Z) such
that |fj(x)| ≤1 for any x ∈Z and fj = χAj, except on a set Ej with |µ|(Ej) ≤
(n+1)2j+1 .
We now deﬁne a function h : Y →Y by setting h(y) = y, if ∥y∥Y ≤1 and h(y) =
if ∥y∥Y ≥1, and introduce another function in C(Z, Y) given by ¯f := P
∥µ(Aj)∥Y fj.
Therefore, the function f = h ◦¯f is in C(Z, Y) as well, because ¯f ∈C(Z, Y) and, for any
y, z ∈Y, ∥h(y) −h(z)∥Y ≤2∥y −z∥Y. Moreover, we observe, for any x ∈
f(x) = g(x) and, for any x ∈Z, that ∥f(x)∥Y ≤1.
We are now ready to estimate the total variation of µ. First, observe that
∥f(x) −g(x)∥Y d|µ|(x) ≤
(n + 1)|µ|(Ej) ≤ε,
and consequently we obtain the inequality
j∈Nn ∥µ(Aj)∥Y + ε =
(g(x), dµ(x)) + ε
(f(x) −g(x), dµ(x))
(f(x), dµ(x))
∥f(x) −g(x)∥Yd|µ|(x) + ∥Lµ∥+ ε ≤2ε + ∥Lµ∥.
This ﬁnishes the proof of the lemma.
We will use a vector-valued form of the Riesz representation theorem called Dinculeanu-
Singer theorem, see e.g. .
Lemma 11. For any compact set Z ⊆X, the map µ 7→Lµ is an isomorphism of Banach
spaces from M(Z, Y) to C∗(Z, Y).
Proof. For each µ ∈M(X, Y), there exists an Lµ ∈C∗(Z, Y) given by equation (20). The
isometry of the map µ 7→Lµ follows from Lemma 10.
Therefore, it suﬃces to prove, for every ¯L ∈C∗(Z, Y), that there is a µ ∈M(Z, Y)
such that Lµ = ¯L. To this end, note that ¯L ◦γ−1 ∈C∗
γ(Z × B1) since γ is an isometric
map from C(Z, Y) onto Cγ(Z × B1). Since Cγ(Z × B1) is a closed subspace of C(Z × B1),
applying the Hahn-Banach extension theorem yields that,
Caponnetto, Micchelli, Pontil, and Ying
for any L ∈C∗
γ(Z × B1), there exists an extension functional ˜L ∈C∗(Z × B1) such that
˜L(F) = L ◦γ−1(F) for any F ∈Cγ(Z × B1). Moreover, recalling that Z × B1 is compact
if B1 is equipped with the weak topology, by the Riesz representation theorem, for any ˜L,
there exists a scalar measure ν on Z × B1 such that
F(x, y)dν(x, y),
∀F ∈C(Z × B1).
Equivalently, for any f ∈C(Z, Y) there holds
¯Lf = ¯L ◦γ−1(F) =
F(x, y)dν(x, y) =
(f(x), dµ(x)) = Lµf,
where µ is deﬁned i terms of ν as in Equation (18).
This ﬁnishes the identiﬁcation between functionals in C(Z, Y) and vector measures with
bounded variation.
It is interesting to remark that, as the consequence of the proof of Lemma 11, for every
regular vector measure there corresponds a scalar measure ν on Z × B1 for which equation
(18) holds true. Indeed, for any µ ∈M(Z, Y) we have established in the proof of Lemma
11 that there exists a regular scalar measure ν on Z × B1 such that
(f(x), y)dν(x, y).
Since we established the isometry between C∗(Z, Y) and M(Z, Y), it follows that equation
(18) holds true.
In order to provide our alternate proof of Theorem 5, we need to attend to one further
issue. Speciﬁcally, we need to deﬁne the integral
K(t, x)(dµ(x)) as an element in Y. For
this purpose, for any µ ∈M(Z, Y) and t ∈Z we deﬁne a linear functional Lt on Y at y ∈Y
(K(x, t)y, dµ(x)).
Since its norm has the property ∥Lt∥≤
supx∈Z ∥K(x, t)∥
∥µ∥, by the Riesz representation
lemma, we conclude that there exists a unique element ¯y in Y such that
(K(x, t)y, dµ(x)) = (¯y, y).
It is this vector ¯y which we denote by the integral
K(t, x)(dµ(x)).
Similarly, we deﬁne the integral
Φ(x)(dµ(x)) as an element in W. To do this, we
note that ∥Φ(x)∥= ∥Φ∗(x)∥and ∥Φ∗(x)y∥2 = ⟨K(x, x)y, y⟩. Hence, we conclude that
Universal operator-valued kernels
∥Φ(x)∥≤∥K(x, x)∥
2 ≤κ(Z). Consequently, the linear functional L on W deﬁned, for any
Φ∗(x)w, dµ(x)
satisﬁes the inequality ∥L∥≤κ(Z)∥µ∥. Hence, we conclude that there exists a unique element ¯w ∈W such that L(w) = ( ¯w, w) for any w ∈W. Now, we denote ¯w by
Φ(x)(dµ(x))
which means that
Φ(x)(dµ(x)), w
Φ∗(x)w, dµ(x)
We have now assembled all the necessary properties of vector measures to provide an
alternate proof of Theorem 5 in Section 3.
Proof of Theorem 5. We see from the feature representation (10) that
K(t, x)(dµ(x)) =
Φ∗(t)Φ(x)(dµ(x)) = Φ∗(t)
Φ(x)(dµ(x))
From this equation, we easily see that if
Φ(x)(dµ(x)) = 0 then, for every t ∈Z,
K(t, x)(dµ(x)) = 0. On the other hand, applying (23) with the choice w =
Φ(x)(dµ(x))
Φ(x)(dµ(x)), dµ(t)
Φ(x)(dµ(x))
Therefore, if, for any t ∈Z,
K(t, x)(dµ(x)) = 0 then
Φ(x)(dµ(x)) = 0, or equivalently,
by equation (23),
(Φ∗(x)w, dµ(x)) = 0,
Consequently, a linear functional vanishes on CK(Z, Y) if and only if it vanishes on CΦ(Z, Y)
and thus, we obtained that CK(Z, Y) = CΦ(Z, Y).
We end this section with a comment that will be useful to us in developing several
examples of universal kernels in Section 5. This remark is a basic principle of function
analysis which has guided our approach to the study of the problem addressed in this
To this end, we recall the notion of the annihilator of a set V which is deﬁned by
µ ∈M(Z, Y) :
(v(x), dµ(x)) = 0, ∀v ∈V
Notice that the annihilator of the closed linear span of V is the same as that of V. Consequently, by applying the basic principle stated at the beginning of this section , we conclude
that the linear span of V is dense in C(Z, Y), that is, span(V) = C(Z, Y) if and only if
the annihilator V⊥= {0}. Hence, applying this observation to the set of kernel sections
K(Z) := {K(·, x)y :
x ∈Z, y ∈Y} or to the set of its corresponding feature sections
Φ(Z) := {Φ∗(·)w : w ∈W}, we obtain from Lemma 11 and Theorem 5, the following useful
Caponnetto, Micchelli, Pontil, and Ying
Corollary 12. Let Z be a compact subset of X, K a continuous operator-valued kernel,
and Φ its feature representation. Then, CK(Z, Y) = C(Z, Y) if and only if either one of the
following statements is valid.
µ ∈M(Z, Y) :
K(t, x)(dµ(x)) = 0,
µ ∈M(Z, Y) :
Φ(x)(dµ(x)) = 0
5. Universal Kernels
In this section, we prove the universality of some kernels, based on the theorems developed
above. Speciﬁcally, the examples highlighted in Section 2 will be discussed in detail.
5.1 Product of Scalar Kernels and Operators
Our ﬁrst example is produced by coupling a scalar kernel with an operator in L+(Y). Given a
scalar kernel G on X and an operator B ∈L+(Y), we deﬁne the function K : X ×X →L(Y)
K(x, t) = G(x, t)B,
For any {xj ∈X : j ∈Nm} and {yj ∈Y : j ∈Nm}, we know that (G(xi, xj))i,j∈Nm
and ((Byi, yj))i,j∈Nm are positive semi-deﬁnite. Applying Schur’s lemma implies that the
matrix (G(xi, xj)(Byi, yj))i,j∈Nm is positive semi-deﬁnite and hence, K is positive semideﬁnite. Moreover, K∗(x, t) = K(x, t) = K(t, x) for any x, t ∈X. Therefore, we conclude
by Deﬁnition 1 that K is an operator-valued kernel.
Our goal below is to use the feature representation of the scalar kernel G to introduce
the corresponding one of kernel K. To this end, we ﬁrst let W be a Hilbert space and
φ : X →W a feature map of the scalar kernel G, so that
G(x, t) = (φ(x), φ(t))W,
Then, we introduce the tensor vector space W N Y whose elements are tensors w ⊗y with
w ∈W and y ∈Y satisfying, for any w1, w2 ∈W, y1, y2 ∈Y and c ∈R, the multi-linear
cw ⊗y = w ⊗cy = c(w ⊗y),
(w1 + w2) ⊗y = w1 ⊗y + w2 ⊗y,
w ⊗(y1 + y2) = w ⊗y1 + w ⊗y2.
This vector space W N Y becomes a Hilbert space provided that it is endowed with inner
product norm deﬁned, for any w1 ⊗y1, w2 ⊗y2 ∈W N Y, by
⟨w1 ⊗y1, w2 ⊗y2⟩= (w1, w2)W(y1, y2)Y.
The above tensor product suggests us to deﬁne the map Φ : X →L(Y, W ⊗Y) of kernel K
Φ(x)y := φ(x) ⊗
∀x ∈X, y ∈Y,
Universal operator-valued kernels
and it follows that Φ∗: X →L(W ⊗Y, Y) is given by
Φ∗(x)(w ⊗y) := (φ(x), w)W
∀x ∈X, w ∈W, and y ∈Y.
From the above observation, it is easy to check, for any x, t ∈X and y, u ∈Y, that
(K(x, t)y, u) = ⟨Φ(x)y, Φ(t)u⟩. Therefore, we conclude that Φ is a feature map for the
operator-valued kernel K.
Finally, we say that an operator B ∈L+(Y) is positive deﬁnite if (By, y) is positive
whenever y is nonzero. We are now ready to present the result on universality of kernel K.
Theorem 13. Let G : X × X →R be a continuous scalar kernel, B ∈L+(Y) and K be
deﬁned by equation (24). Then, K is an operator-valued universal kernel if and only if the
scalar kernel G is universal and the operator B is positive deﬁnite.
Proof. By Corollary 12 and the feature representation (26), we only need to show that
Φ(Z)⊥= {0} if and only if G is universal and the operator B is positive deﬁnite.
We begin with the suﬃciency. Suppose that there exists a nonzero vector measure µ
such that, for any w ⊗y ∈W ⊗Y, there holds
(Φ∗(x)(w ⊗y), dµ(x)) =
(φ(x), w)W(
By, dµ(x)) = 0.
Here, with a little abuse of notation we interpret, for a ﬁxed y ∈Y, (
By, dµ(x)) as a
scalar measure deﬁned, for any E ∈B(Z), by
By, dµ(x)) = (
By, µ(E)).
Since µ ∈M(Z, Y), (
By, dµ(x)) is a regular signed scalar measure. Therefore, we see
from (27) that (
By, dµ(x)) ∈φ(Z)⊥for any y ∈Y. Remember that G is universal if and
only if φ(Z)⊥= {0}, and thus we conclude from (27) that (
By, dµ(x)) = 0 for any y ∈Y.
It follows that (
By, µ(E)) = 0 for any y ∈Y and E ∈B(Z). Thus, for any ﬁxed set E
taking the choice y =
Bµ(E) implies that (Bµ(E), µ(E)) = 0. Since E is arbitrary, this
means µ = 0 and thus ﬁnishes the proof for the suﬃciency.
To prove the necessity, suppose ﬁrst that G is not universal and hence, we know that,
for some compact subset Z of X, there exists a nonzero scalar measure ν ∈M(Z) such
that ν ∈φ(Z)⊥, that is,
(φ(x), w)dν(x) = 0 for any w ∈W. This suggests us to choose,
for a nonzero y0 ∈Y, the nonzero vector measure µ = y0ν deﬁned by µ(E) := y0ν(E) for
any E ∈B(Z). Hence, the integral in equation (27) equals
(Φ∗(x)(w ⊗y), dµ(x)) = (
(φ(x), w)Wdν(x) = 0.
Therefore, we conclude that there exists a nonzero vector measure µ ∈Φ(Z)⊥, which implies
that K is not universal.
If B is not positive deﬁnite, namely, there exists a nonzero element y1 ∈Y such that
(By1, y1) = 0. However, we observe that ∥
By1∥2 = (By1, y1) which implies that
Caponnetto, Micchelli, Pontil, and Ying
0. This suggests us to choose a nonzero vector measure µ = y1ν with some nonzero scalar
measure ν. Therefore, we conclude, for any w ∈W and y ∈Y, that
(Φ∗(x)(w ⊗y), dµ(x))
(φ(x), w)Wdν(x)
(φ(x), w)Wdν(x) = 0,
which implies that the nonzero vector measure µ ∈Φ(Z)⊥. This ﬁnishes the proof of the
In the special case Y = Rn, the operator B is an n × n positive semi-deﬁnite matrix.
Then, Theorem 13 tells us that the matrix-valued kernel K(x, t) := G(x, t)B is universal if
and only if G is universal and the matrix B is of full rank.
We proceed to consider kernels produced by a ﬁnite combination of scalar kernels and
operators. Speciﬁcally, we consider, for any j ∈Nm, that Gj : X × X →R be a scalar
kernel and Bj ∈L+(Y). We are interested in the kernel deﬁned, for any x, t ∈X, by
K(x, t) :=
Gj(x, t)Bj.
Suppose also, for each scalar kernel Gj, that there exists feature Hilbert spaces Wj and
feature maps φj : X →Wj.
To explain the associated feature maps of kernel K, we need to deﬁne a Hilbert feature
space. For this purpose, let Hj be a Hilbert space with inner products (·, ·)j for j ∈Nm
and we introduce the direct sum Hilbert space ⊕j∈NmHj as follows. The elements in this
space are of the form (h1, . . . , hm) with hj ∈Hj, and its inner product is deﬁned, for any
(h1, . . . , hm), (h′
1, . . . , h′
m) ∈⊕j∈NmHj, by
⟨(h1, . . . , hm), (h′
1, . . . , h′
This observation suggests us to deﬁne the feature space of kernel K by the direct sum
Hilbert space W := ⊕j∈Nm(Wj ⊗Y), and its the map Φ : X →L(Y, W), for any x ∈X and
Φ(x)y := (φ1(x) ⊗
B1 y, . . . , φm(x) ⊗
Hence, its adjoint operator Φ∗: X →L(⊕j∈Nm(Wj ⊗Y), Y) is given, for any (w1 ⊗
y1, . . . , wm ⊗ym) ∈⊕j∈Nm(Wj ⊗Y), by
Φ∗(x)((w1 ⊗y1, . . . , wm ⊗ym)) :=
(φj(x), wj)Wj
Using the above observation, it is easy to see that, for any x, t ∈X, K(x, t) = Φ∗(x)Φ(t).
Thus K is an operator-valued kernel and Φ is a feature map of K.
We are now in a position to state the result about the universality of the kernel K.
Universal operator-valued kernels
Theorem 14. Suppose that Gj : X × X →R is a continuous scalar universal kernel, and
Bj ∈L+(Y) for j ∈Nm. Then, K(x, t) := P
j∈Nm Gj(x, t)Bj is universal if and only if
j∈Nm Bj is positive deﬁnite.
Proof. We only need to prove that Φ(Z)⊥= {0} for any compact set Z if and only if
j∈Nm Bj is positive deﬁnite. To see this let us notice that, by the deﬁnition of Φ∗and
Φ(Z)⊥associated to kernel K, µ ∈Φ(Z)⊥implies, for any (w1 ⊗y1, . . . , wm ⊗ym) ∈
⊕j∈Nm(Wj ⊗Y), that
(φj(x), wj)Wj(
Bj yj, dµ(x)) = 0.
Since wj ∈Wj is arbitrary, the above equation is equivalent to
(φj(x), wj)Wj(
Bj yj, dµ(x)) = 0,
∀wj ∈Wj, yj ∈Y and j ∈Nm,
which implies that (
Bj yj, dµ(x)) ∈φ(Z)⊥for any j ∈Nm. Recall that Gj is universal if
and only if φ(Z)⊥= {0}. Therefore, equation (30) holds true if and only if
Bj yj) = (
Bjµ(E), yj) = 0,
To move on to the next step, we will show that equation (31) is true if and only if
(µ(E), Bjµ(E)) = 0,
To see this, we observe, for any j ∈Nm, that ∥
Bjµ(E)∥2 = (µ(E), Bjµ(E)). Hence,
equation (32) implies equation (31). Conversely, applying equation (31) with the choice
yj = µ(E) directly yields equation (32).
However, we know, for any y ∈Y and j ∈Nm, that (Bjy, y) is nonnegative. Therefore,
equation (32) is equivalent to that
Bj)µ(E), µ(E)
Therefore, we conclude that µ ∈Φ(Z)⊥if and only if the above equation holds true.
Obviously, by equation (33), we see that if P
j∈Nm Bj is positive deﬁnite then µ = 0.
This means that kernel K is universal. Suppose that P
j∈Nm Bj is not positive deﬁnite, that
is, there exists a nonzero y0 ∈Y such that ∥
j∈Nm Bj)y0, y0
Hence, choosing a nonzero vector measure µ := y0ν, with ν any nonzero scalar measure,
implies that equation (33) holds true and, thus kernel K is not universal. This ﬁnishes the
proof of the theorem.
We are in a position to analyze Examples 1 and 4 given in the Section 2.
Example 1 (continued) Since the function K considered in Example 1 has the feature
representation (29), it is an operator-valued kernel.
Moreover, necessary and suﬃcient
conditions for the universality of K are given in Theorem 14
Caponnetto, Micchelli, Pontil, and Ying
We now discuss a class of kernel which include that in Example 4. To this end, we
use the notation Z+ = {0} ∪N and, for any smooth function f : Rm →R and index
α = (α1, . . . , αm) ∈Zm
+, we denote its α partial derivative by (∂αf)(x) :=
Then, recall that the Sobolev space W k with integer order k is the space of
real valued functions with norm deﬁned by
Rm |∂αf(x)|2dx.
This space can be extended to any fractional index s > 0. To see this, we need the Fourier
transform deﬁned for any f ∈L1(Rm) as
Rm e−2π⟨x,ξ⟩f(x)dx,
It has a natural extension to L2(Rm) satisfying the Plancherel formula ∥f∥L2(Rm) = ∥ˆf∥L2(Rm).
In particular, we observe, for any α ∈Zd and ξ ∈Rm, that d
∂αf(ξ) = (2πiξ)α ˆf(ξ). Hence, by
Plancherel formula, we see, for any f ∈W k with k ∈N, that its norm ∥f∥W k is equivalent
Rm(1 + 4π|ξ|2)k| ˆf(ξ)|2dξ
This observation suggests us to introduce fractional Sobolev space W s )
with any order s > 0 with norm deﬁned, for any real function f, by
Rm(1 + 4π2|ξ|2)s| ˆf(ξ)|2dξ.
Finally, we need the Sobolev embedding lemma ; Stein ) which
states that, for any s > m
2 , there exists an absolute constant c such that, for any f ∈W s
and any x ∈Rm, there holds
|f(x)| ≤c∥f∥W s.
The next result applies to Example 4 in the high dimensional case.
Proposition 15. Let Y = L2(Rd), X = R and let H be the space of real-valued functions
°°°f(x, ·)
Then this is an RKHS and the corresponding operator-valued kernel is given, every x, t ∈X
(K(x, t)y)(r) = e−π|x−t|
Rd e−π∥r−s∥y(s)ds,
∀y ∈Y, r ∈Rd
is a universal operator-valued kernel.
Proof. By applying Sobolev embedding theorem there exists a constant c such that, for any
y ∈B1 and f ∈H,
|(y, f(x))Y| ≤c∥f∥.
Universal operator-valued kernels
Hence, by the Riesz representation lemma there exists a reproducing kernel that H is an
RKHS .
In the following, our target is to show that K given by equation (35) is the kernel
associated to H. To see this, it suﬃces to show that the reproducing property holds, that
is, for any f ∈H, y ∈Y and x ∈X
(y, f(x))Y = ⟨Kxy, f⟩,
where ⟨·, ·⟩denotes the inner product in H.
By the Plancherel formula, we observe that the left-hand side of equation (36) equals
ˆy(τ)e−2πi⟨x,ξ⟩ˆf(ξ, τ)dξdτ.
On the other hand, note that Kxy(x′)(·) := (K(x′, x)y)(·) ∈Y, and we denote its Fourier
transform by
bK(·, x)y)(·)(ξ, τ) =
e−2πi⟨x′,ξ⟩e−2πi⟨r,τ⟩(K(x′, x)y)(r)drdx′
which by Plancherel formula again, is equal to
e−2πi⟨x,ξ⟩
(1 + 4π2|ξ|2)
(1 + 4π2|τ|2).
However, the right-hand side of equation (36) is identical to
bK(·, x)y)(·)(ξ, τ) ˆf(ξ, τ)(1 + 4π2|ξ|2)
2 (1 + 4π2|τ|2).
Putting (37) into the above equation, we immediately know that the reproducing property
(36) holds true. This veriﬁes that K is the kernel of the Hilbert space H.
To prove the universality of this kernel, let Z be any prescribed compact subset of
X, we deﬁne the Poisson kernel, for any x, t ∈R, by G(x, t) := e−|x−t| and the operator
B : L2(Rd) →L2(Rd) by
Rd e−σ∥r−s∥g(s)ds,
Then, K(x, t) = e−σ|x−t|B. Note that
Bg(·)(ξ) = cd
(1 + 4π2|ξ|2)
Hence, by Theorem 13 it suﬃces to prove that G is universal and B is positive deﬁnite.
The universality of G follows from Steinwart .
To show the positive deﬁniteness of B, applying the Plancherel formula implies that the
equation (Bg, g) = 0 is equivalent to
(1 + 4π2|ξ|2)
Caponnetto, Micchelli, Pontil, and Ying
Therefore, ˆg = 0 and, so, g = 0 almost everywhere, which means that B is positive deﬁnite.
Hence, we conclude by Theorem 13, that K is universal.
Let us proceed to discuss continuous parameterized kernels. Let Σ be a locally compact
Hausdorﬀspace and, for any σ ∈Σ, B(ω) is an n × n positive semi-deﬁnite matrix. We are
interested in the following kernel
G(σ)(x, t)B(σ)dp(σ),
To investigate this kind of kernels, for any σ ∈Σ we assume that G(σ) be a scalar kernel
with feature representation given by, for any x, t ∈X, G(x, t) = ⟨φσ(x), φgs(t)⟩W. We
introduce the Hilbert space W = L2(Σ, W ⊗Y, p) with norm, for any f : Σ →W ⊗Y,
Next, we deﬁne a map Φ : X →L(Y, L2(Σ, W ⊗Y, p)), for any x ∈X and σ ∈Σ, by
Φ(x)y(σ) := φσ(x) ⊗(
By the exact argument as that before Theorem 14, we know that K is a kernel and has the
feature map Φ with feature space W.
We are ready to present a suﬃcient condition on the universality of K.
Theorem 16. Let G(σ) be a continuous universal kernel and B(σ) be positive deﬁnite in
the support of the probability measure p. Suppose K be deﬁned by equation (38). Then, the
operator-valued kernel K is universal.
Proof. Notice the feature W = L2(Σ, W ⊗Y, p). By Corollary 12, for any compact set
Z ⊆X we need to check the equation
B(σ)(dµ(x)) = 0
which the above equation equals zero should be interpreted as
support(p)
B(σ)(dµ(x))
Wdp(σ) = 0
Therefore, there exists a σ0 ∈support(p) such that
B(σ)(dµ(x)) = 0. Equivalently,
B(σ0)dµ(x), y) = 0 for any y ∈Y.
But G(σ0) is universal, appealing to the feature characterization in the scalar case implies that the scalar measure
B(σ0)dµ(x), y) = 0. Since y ∈Y is arbitrary, we have that µ ≡0 which completes the
proof of this theorem.
An example in the continuously parameterized scenario can be described as below.
Universal operator-valued kernels
Example 5. Suppose the probability measure p over [0, ∞) does not concentrate on zero
and B(σ) be a positive deﬁnite n×n matrix for each σ ∈(0, ∞). Then the kernel K(x, t) =
e−σ∥x−t∥2B(σ)dp(σ) is a multi-task universal kernel.
As a special of the above construction, we choose B(σ) in the following manner. Let
A, B be n × n symmetric matrices. For every σ > 0, we deﬁne the (i, j) entry of B(σ) as
e−σAij where i, j ∈Nn. Recall that a matrix A is conditionally negative deﬁnite if, for any
{ci ∈R : i ∈Nn} with P
i∈Nn ci = 0, the quadratic form P
i,j∈Nn ciAijcj ≤0. A well-known
theorem of I. J. Schoenberg ) state that B(σ) is positive deﬁnite
for all σ > 0 if and only if A is conditionally negative deﬁnite. Moreover, if the elements of
the matrix A satisfy, for any i, j ∈Nn, the inequalities Aij > 1
2(Aii +Ajj) and Aii > 0, then
A is conditionally negative deﬁnite. With this choice, the above integral can be computed
and we see that the kernel above is given by
K(x, y)ij =
∥x −t∥2 + Aij
∀i, j ∈Nn.
Hence, we conclude that K is a universal operator-valued kernel provided that, for any
i, j ∈Nn, Aii > 0 and Aij > 1
2(Aii + Ajj).
5.2 Transformable Kernels
In this subsection we explore matrix-valued kernels produced by transforming scalar kernels.
To introduce this type of kernels, let Y = Rn , X be a Hausdroﬀspace and Tp be a map
from X to X (not necessary linear) for p ∈Nn. Then, given a continuous scalar kernel
G : X × X →R, we consider the matrix-valued kernel on X deﬁned by
K(x, t) :=
G(Tpx, Tqt)
Proposition 17. Let G be a scalar kernel and K be deﬁned by (39). Then, K is a matrixvalued kernel.
Proof. For any m ∈N, {yi = (y1i, y2i, · · · , yni) ∈Rn : i ∈Nm} and {xi ∈X : i ∈Nm} then
(yi, K(xi, xj)yj) =
ypiyqjG(Tpxi, Tqxj).
Since G is a scalar reproducing kernel on Z, the last term of the above equality is nonnegative, and hence K is positive semi-deﬁnite matrix-valued kernel. This completes the proof
of the assertion.
Our target below is to use our main result in Sections 3 and 4 to characterize the
universality of K.
To this end, we assume that the scalar kernel G has a feature map
φ : X →W and deﬁne the, for every x ∈X let Φ(x) : Rn →W be the the linear operator
deﬁned, for any y = (y1, . . . , yn) ∈Rn, as Φ(x)y = P
p∈Nn ypφ(Tpx). Its adjoint operator
Φ(x)∗: W →Rn is given, for any w ∈W, as Φ∗(x)w = (⟨φ(T1x), w⟩W, . . . , ⟨φ(Tnx), w⟩W).
Caponnetto, Micchelli, Pontil, and Ying
Then, we can easily see, for any x, t ∈X, that K(x, t) = Φ∗(x)Φ(t). Hence, we conclude
that W is the feature space of K and Φ is its feature map.
We also need some notations and deﬁnitions. For a map T : X →X, we denote its
range space by TX := {Tx : x ∈X} and T −1(E) := {x : Tx ∈E} for any E ⊂X.
In addition, we say that T is continuous if T −1(U) is open whenever U is a open set in
X. Finally, for any scalar Borel measure ν on X and a continuous map T from X to X,
we induce the image measure ν ◦T −1 on X which is deﬁned by, for any E ∈B(X), as
ν ◦T −1(E) := ν({x ∈X : Tx ∈E}).
We are ready to state the result about universality of the kernel K in equation (39).
Proposition 18. Let Tp : X →X be continuous for each p ∈Nn and deﬁne the kernel K
by equation (39). Then K is universal if and only the sets TqX, with q ∈Nn, are pairwise
disjoint and Tq is one-to-one for each q ∈Nn.
Proof. For any compact set Z ⊆X, by Corollary 12 it suﬃces to verity that the vector
measures µ ∈Φ(Z)⊥, or equivalently, that
Φ(x)(dµ(x)) = 0.
By Lemma 11 and the remark which followed it, for any vector measure µ ∈M(Z, Rn),
there exists a scalar regular measure ν ∈M(Z × B1) such that
y1dν(t, y), . . . ,
yndν(t, y)
Hence, any vector measure µ can be represented as µ = (µ1, . . . , µn) with each µi a scalar
measure. Then, equation (40) becomes
φ(Tqt)dµq(t) = 0.
Equivalently, for Z := ∪p∈NnTpZ,
Since Tp is continuous for any p ∈Nn, the range space TpZ is compact and so is Z. By
Corollary 12 with Y = R G is universal on Z if and only if its feature φ is universal on Z.
Therefore, the above equation is identical to the equation
Consequently, we conclude that K is universal if and only if
(µ1, · · · , µq) :
Universal operator-valued kernels
With the above derivation, we can now prove the necessity. Suppose that {TqX : q ∈Nn}
is not pairwise disjoint. Without loss of generality, we assume that T1X ∩T2X ̸= ∅. That
means there exists x1, x2 ∈X such that T1x1 = T2x2 = z0. Let µq ≡0 for q ≥3, and select
µ1 = δx=x1, µ2 = −δx=x2 which implies that (42) holds true. By Corollary 12 in Section 4,
we know that K is not universal. This completes the ﬁrst assertion.
Now suppose that there is a map, for example Tp, which is not one-to-one. This implies
that there exists x1, x2 ∈X, x1 ̸= x2, such that Tpx1 = Tpx2. Hence, if we let µq = 0 for
any q ̸= p and µp = δx=x1 −δx=x2 then P
q∈Nn µq ◦T −1
= 0, but µp ̸= 0. By Corollary 12,
K is not universal. This completes the our assertion.
Finally, we prove the suﬃciency. Since µq ◦T −1
only lives on TqX and {TqX : q ∈Nn}
is pairwise disjoint, then P
q∈Nn µq ◦T −1
= 0 is equivalent to µq ◦T −1
= 0 for each q ∈Nn.
However, since Tq is one-to-one, E = T −1
(Tq(E)) for each Borel set E ∈B(X). This means
that µq(E) = µq ◦T −1
(Tq(E)) = 0 for any E ∈B(X). This concludes the proof of the
proposition.
We end this subsection with the detailed proofs of our claims about the examples presented in Section 2. Indeed, we already proved the positive semi-deﬁniteness of the kernel
in Example 2 by Proposition 17. Below, we prove the claim that the function K given by
equation (8) is not a kernel in general.
Proposition 19. Let σpq > 0 and σpq = σqp for any p, q ∈Nn. Then, the matrix-valued
function deﬁned by
K(x, t) :=
e−σpq∥x−y∥2´n
is an operator-valued kernel if and only if for some constant σ, σpq = σ for any p, q ∈Nn.
Proof. By the observation just before Proposition 4, it suﬃces to prove, for any m ∈N and
xi ∈X with i ∈Nm, that the double-indexed nm × nm matrix
G((i, p), (j, q)) = e−σpq∥xi−xj∥2´
is positive semi-deﬁnite.
Since m is arbitrary, we can assume that m ≥n. Now, for any p0 ̸= q0 ∈Nn, we
consider the principal matrix with the both column and row indices taking from the set
{(i, p) : i, p ∈{p0, q0}}. Note again that xp0, xq0 are arbitrary, and thus we assume that
c := ∥xp0 −xq0∥2 > 0. It follows that the principal matrix mentioned above is exactly
exp{−cσp0p0}
exp{−cσp0q0}
exp{−cσq0p0}
exp{−cσq0q0}
exp{−cσp0p0}
exp{−cσp0q0}
exp{−cσq0p0}
exp{−cσq0q0}
Caponnetto, Micchelli, Pontil, and Ying
The determinant of the subprincipal matrix of the matrix above is given by
exp{−cσp0p0}
exp{−cσq0p0}
exp{−cσp0p0}
exp{−cσp0q0}
exp{−cσp0p0} −exp{−cσq0p0}
This means that if K is positive semi-deﬁnite then σp0p0 = σq0p0. Remember that p0, q0 are
arbitrary, it follows that
p,q=1 is a constant matrix whenever K is an operator-valued
Obviously, we can see that if
p,q=1 is a constant matrix then K is positive semideﬁnite, and hence ﬁnishes the assertion.
5.3 Hessian of Gaussian Kernels
In this subsection we investigate the Hessian of scalar Gaussian kernels (Example 3 in
Section 2) which is mainly motivated by the study of divergence free vector-valued functions
in Lowitzsh .
To introduce this type of kernels, we let X = Y = Rn and G be a standard Gaussian
kernel with variance σ, that is, for any x ∈Rn, G(x) = e−∥x∥2
with σ > 0. Then, we deﬁne
the Hessian matrix by
K(x, t) :=
−(∂p∂qG)(x −t)
∀x, t ∈Rn.
Observe that
−(∂p∂qG)(x) = 4π
Rn e2πi⟨x,ξ⟩ξpξqe−σ∥ξ∥2dξ,
which implies that
K(x, t) = 4π
Rn e2πi⟨x−t,ξ⟩ξξT e−σ∥ξ∥2dξ.
Theorem 20. Let n ≥1 and K : Rn × Rn →Rn×n be deﬁned by (43). Then, K is a
matrix-valued kernel. Moreover, K is universal for n = 1 but not universal for n ≥2.
Proof. First we show that K is positive semi-deﬁnite, and hence it is a matrix-valued kernel.
To this end, we need to verify, for any m ∈N, {yi = (y1i, y2i, · · · , yni) ∈Rn : i ∈Nm} and
{xi ∈X : i ∈Nm}, that
(yi, K(xi, xj)yj) ≥0.
By (44), the above summation term can be written as
Rn e2πi⟨xi−xj,ξ⟩ξξT yje−σ∥ξ∥2dξ)
⟨yi, ξ⟩e2πi⟨xi,ξ⟩°°2e−σ∥ξ∥2dξ,
Universal operator-valued kernels
which is nonnegative. This proves the ﬁrst assertion.
In order to proof the second assertion, by Corollary 12 we need to verify that, for any
compact set Z ⊆Rn be a compact set, K(Z)⊥= {0}. For this purpose, we assume that
µ ∈K(Z)⊥, that is,
K(x, t)(dµ(t)) = 0
By (44), the above equation is equivalent to
Rn e2πi⟨x,ξ⟩ξe−σ∥ξ∥2 Z
e−2πi⟨t,ξ⟩(ξ, dµ(t))dξ = 0,
which implies, by integrating with respect to x, that
Rn e−σ∥ξ∥2°°
e−2πi⟨t,ξ⟩(ξ, dµ(t))
°°2dξ = 0.
Consequently, equation (45) is equivalent to
e−2πi⟨t,ξ⟩(ξ, dµ(t)) = 0,
We now analyze this equation in the case that n = 1 or n ≥2.
If n = 1, the above equation is equivalent to
e−2πitξdµ(t) = 0,
Taking the k-th derivative with respect to ξ, we have, for every k ∈N, that
e−2πitξtkdµ(t) = 0,
Since the set of all polynomials is dense in the continuous function space C(Z), we conclude
from the above equation that µ = 0. Hence, by Corollary 12, the kernel K is universal if
If n ≥2, we choose µq ≡0 for q ≥3 and dµ1(t) = dt1(δ(t2 = 1)−δ(t2 = −1)) Qn
p=3 δ(tp =
0) and dµ2(t) = dt2(δ(t1 = −1) −δ(t1 = 1)) Qn
p=3 δ(tp = 0). Note that
[−1,1]n e−2πi⟨t,ξ⟩dµ1(t)
= (e−2πiξ2−e2πiξ2
e−2πt1ξ1dt1 = (−2πi sin(2πξ2))sin(2πξ1)
[−1,1]n e−2πi⟨t,ξ⟩dµ2(t)
= (e−2πiξ1−e2πiξ1
e−2πt2ξ2dt2 = (2πi sin(2πξ1))sin(2πξ2)
Therefore, we conclude that
[−1,1]n e−2πi⟨t,ξ⟩(ξ, dµ(t)) = ξ1
[−1,1]n e−2πi⟨t,ξ⟩dµ1(t) + ξ2
[−1,1]n e−2πi⟨t,ξ⟩dµ2(t) = 0.
Hence, the kernel K is not universal when n ≥2.
Caponnetto, Micchelli, Pontil, and Ying
5.4 Projection Kernels
In this ﬁnal subsection we introduce a class of operator-valued kernels associated with
projection operators of scalar kernels.
Let us start with some notations and deﬁnitions. Let X ⊆Rd, Ω⊆Rm be a compact
set and Y = L2(Ω). We also need a continuous scalar kernel G : (X × Ω) × (X × Ω) :→R
with a feature representation given, for any x, x′ ∈X and t, s ∈Ω, by
G((x, t), (x′, s)) = ⟨φ(x, t), φ(x′, s)⟩W.
Then, the projection operator K : X × X →L(Y, Y) is deﬁned, for any f ∈L2(Ω), by
(K(x, x′)f)(t) :=
G((x, t), (x′, s))f(s)ds,
∀x, x′ ∈X,
We ﬁrst show that K is an operator-valued kernel. To see this, for any m ∈N, {xi ∈
X : i ∈Nm}, and {yi ∈L2(Ω) : i ∈Nm} there holds
i,j∈Nm(K(xi, xj)yj, yi)
G((xi, t), (xj, s))yj(s)yi(t)dtds
φ(xi, s)yi(s)ds
which implies that K is a kernel.
To investigate its universality from the feature perspective, we deﬁne Φ : X →L(Y, W),
for any x ∈X and y ∈Y, by
φ(x, s)y(s)ds,
and thus it is easy to see that its adjoint operator Φ∗is given, for any w ∈W, by Φ∗(x)w =
⟨φ(x, ·), w⟩W. Hence, for any x, x′ ∈X, we conclude that K(x, x′) = Φ∗(x)Φ(x′) which
implies that K is an operator-valued kernel and Φ is its associated feature map.
We move on to the proof of the universality of K.
Theorem 21. Let G and K be deﬁned as in equations (47) and (48). If G is a universal
scalar kernel then K is a universal operator-valued kernel.
Proof. By Corollary 12, it suﬃces to show that, for any compact Z ⊆X, whenever there
exists a vector measure µ such that
Φ(x)(dµ(x)) = 0,
then µ = 0. Note that µ is L2(Ω)-valued measure. Hence, µ can be interpreted as a new
measure µ(·, ·) on Z × Ωsatisfying, for any E ∈B(Z) and E′ ∈B(Ω), that µ(E, E′) :=
E′ µ(E)(s)ds. By this observation, we know that
Φ(x)(dµ(x)) =
φ(x, s)dµ(x, s).
Universal operator-valued kernels
Since Z and Ωare both compact, then Z×Ωis also compact by Tychonoﬀtheorem . By assumption, G is universal on X × Ωand φ is its feature map, and thus
we conclude that the scalar measure dµ(x, s) is the zero measure. This means that, for any
E ∈B(Z) and E′ ∈B(Ω),
E′ µ(E)(s)ds = 0.
Since E, E′ are arbitrary, we immediately conclude that the vector measure µ = 0 which
completes the assertion.
Acknowledgments
This work was supported by EPSRC Grants GR/T18707/01 and EP/D052807/1 by the
IST Programme of the European Community, under the PASCAL Network of Excellence
IST-2002-506778. The ﬁrst author was supported by the NSF grant 0325113, the FIRB
project RBIN04PARL and the EU Integrated Project Health-e-Child IST-2004-027749. We
are grateful to Alessandro Verri, Head of the Department of Computer Science at the
University of Genova for providing us with the opportunity to complete part of this work
in a scientiﬁcally stimulating and friendly environment.