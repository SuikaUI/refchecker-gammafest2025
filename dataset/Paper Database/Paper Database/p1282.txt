Enhancing Time Series Momentum Strategies
Using Deep Neural Networks
Bryan Lim, Stefan Zohren, Stephen Roberts
Abstract—While time series momentum is a wellstudied phenomenon in ﬁnance, common strategies require
the explicit deﬁnition of both a trend estimator and a
position sizing rule. In this paper, we introduce Deep
Momentum Networks – a hybrid approach which injects
deep learning based trading rules into the volatility scaling
framework of time series momentum. The model also
simultaneously learns both trend estimation and position
sizing in a data-driven manner, with networks directly
trained by optimising the Sharpe ratio of the signal. Backtesting on a portfolio of 88 continuous futures contracts, we
demonstrate that the Sharpe-optimised LSTM improved
traditional methods by more than two times in the absence
of transactions costs, and continue outperforming when
considering transaction costs up to 2-3 basis points. To
account for more illiquid assets, we also propose a turnover
regularisation term which trains the network to factor in
costs at run-time.
I. INTRODUCTION
Momentum as a risk premium in ﬁnance has been
extensively documented in the academic literature,
with evidence of persistent abnormal returns demonstrated across a range of asset classes, prediction
horizons and time periods . Based on the
philosophy that strong price trends have a tendency
to persist, time series momentum strategies are
typically designed to increase position sizes with
large directional moves and reduce positions at
other times. Although the intuition underpinning
the strategy is clear, speciﬁc implementation details
can vary widely between signals with a plethora
of methods available to estimate the magnitude of
price trends and map them to actual traded
positions .
In recent times, deep neural networks have been
increasingly used for time series prediction, outperforming traditional benchmarks in applications
B. Lim, S. Zohren and S. Roberts are with the Department
Engineering
Oxford-Man
of Quantitative Finance, University of Oxford, Oxford, United
Kingdom (email: , ,
 ).
such as demand forecasting , medicine 
and ﬁnance . With the development of modern
architectures such as convolutional neural networks
(CNNs) and recurrent neural networks (RNNs) ,
deep learning models have been favoured for their
ability to build representations of a given dataset 
– capturing temporal dynamics and cross-sectional
relationships in a purely data-driven manner. The
adoption of deep neural networks has also been
facilitated by powerful open-source frameworks such
as TensorFlow and PyTorch – which
use automatic differentiation to compute gradients
for backpropagation without having to explicitly
derive them in advance. In turn, this ﬂexibility
has allowed deep neural networks to go beyond
standard classiﬁcation and regression models. For
instance, the creation of hybrid methods that combine
traditional time-series models with neural network
components have been observed to outperform pure
methods in either category e.g. the exponential
smoothing RNN , autoregressive CNNs 
and Kalman ﬁlter variants – while also
making outputs easier to interpret by practitioners.
Furthermore, these frameworks have also enabled
the development of new loss functions for training
neural networks, such as adversarial loss functions
in generative adversarial networks (GANs) .
While numerous papers have investigated the
use of machine learning for ﬁnancial time series
prediction, they typically focus on casting the underlying prediction problem as a standard regression
or classiﬁcation task –
with regression models forecasting expected returns,
and classiﬁcation models predicting the direction
of future price movements. This approach, however, could lead to suboptimal performance in the
context time-series momentum for several reasons.
Firstly, sizing positions based on expected returns
alone does not take risk characteristics into account
such as the volatility or skew of the predictive
 
returns distribution - which could inadvertently
expose signals to large downside moves. This is
particularly relevant as raw momentum strategies
without adequate risk adjustments, such as volatility
scaling , are susceptible to large crashes during
periods of market panic . Furthermore, even
with volatility scaling
which leads to positively
skewed returns distributions and long-option-like
behaviour – trend following strategies can
place more losing trades than winning ones and still
be proﬁtable on the whole – as they size up only
into large but infrequent directional moves. As such,
 argue that the fraction of winning trades is a
meaningless metric of performance, given that it
cannot be evaluated independently from the trading
style of the strategy. Similarly, high classiﬁcation
accuracies may not necessarily translate into positive
strategy performance, as proﬁtability also depends
on the magnitude of returns in each class. This
is also echoed in betting strategies such as the
Kelly criterion , which requires both win/loss
probabilities and betting odds for optimal sizing
in binomial games. In light of the deﬁciencies of
standard supervised learning techniques, new loss
functions and training methods would need to be
explored for position sizing – accounting for tradeoffs between risk and reward.
In this paper, we introduce a novel class of
hybrid models that combines deep learning-based
trading signals with the volatility scaling framework
used in time series momentum strategies –
which we refer to as the Deep Momentum Networks (DMNs). This improves existing methods
from several angles. Firstly, by using deep neural
networks to directly generate trading signals, we
remove the need to manually specify both the
trend estimator and position sizing methodology –
allowing them to be learnt directly using modern time
series prediction architectures. Secondly, by utilising
automatic differentiation in existing backpropagation
frameworks, we explicitly optimise networks for
risk-adjusted performance metrics, i.e. the Sharpe
ratio , improving the risk proﬁle of the signal on
the whole. Lastly, retaining a consistent framework
with other momentum strategies also allows us to
retain desirable attributes from previous works –
speciﬁcally volatility scaling, which plays a critical
role in the positive performance of time series
momentum strategies . This consistency also helps
when making comparisons to existing methods, and
facilitates the interpretation of different components
of the overall signal by practitioners.
II. RELATED WORKS
A. Classical Momentum Strategies
Momentum strategies are traditionally divided
into two categories – namely (multivariate) cross
sectional momentum and (univariate) time
series momentum . Cross sectional momentum
strategies focus on the relative performance of
securities against each other, buying relative winners
and selling relative losers. By ranking a universe
of stocks based on their past return and trading the
top decile against the bottom decile, ﬁnd that
securities that recently outperformed their peers over
the past 3 to 12 months continue to outperform on
average over the next month. The performance of
cross sectional momentum has also been shown to
be stable across time , and across a variety of
markets and asset classes .
Time series momentum extends the idea to focus
on an asset’s own past returns, building portfolios
comprising all securities under consideration. This
was initially proposed by , who describe a
concrete strategy which uses volatility scaling and
trades positions based on the sign of returns over
the past year – demonstrating proﬁtability across
58 different liquid instruments individually over
25 years of data. Since then, numerous trading
rules have been proposed – with various trend
estimation techniques and methods map them to
traded positions. For instance, documents a wide
range of linear and non-linear ﬁlters to measure
trends and a statistic to test for its signiﬁcance
– although methods to size positions with these
estimates are not directly discussed. adopt a
similar approach to , regressing the log price
over the past 12 months against time and using
the regression coefﬁcient t-statistics to determine
the direction of the traded position. While Sharpe
ratios were comparable between the two, t-statistic
based trend estimation led to a 66% reduction in
portfolio turnover and consequently trading costs.
More sophisticated trading rules are proposed in
 and , taking volatility-normalised moving
average convergence divergence (MACD) indicators
as inputs. Despite the diversity of options, few
comparisons have been made between the trading
rules themselves, offering little clear evidence or
intuitive reasoning to favour one rule over the next.
We hence propose the use of deep neural networks
to generate these rules directly, avoiding the need for
explicit speciﬁcation. Training them based on riskadjusted performance metrics, the networks hence
learn optimal training rules directly from the data
B. Deep Learning in Finance
Machine learning has long been used for ﬁnancial
time series prediction, with recent deep learning
applications studying mid-price prediction using
daily data , or using limit order book data
in a high frequency trading setting .
While a variety of CNN and RNN models have
been proposed, they typically frame the forecasting
task as a classiﬁcation problem, demonstrating the
improved accuracy of their method in predicting
the direction of the next price movement. Trading
rules are then manually deﬁned in relation to class
probabilities – either by using thresholds on classi-
ﬁcation probabilities to determine when to initiate
positions , or incorporating these thresholds into
the classiﬁcation problem itself by dividing price
movements into buy, hold and sell classes depending
on magnitude . In addition to restricting the
universe of strategies to those which rely on high
accuracy, further gains might be made by learning
trading rules directly from the data and removing
the need for manual speciﬁcation – both of which
are addressed in our proposed method.
Deep learning regression methods have also been
considered in cross-sectional strategies ,
ranking assets on the basis of expected returns over
the next time period. Using a variety of linear, treebased and neural network models demonstrate
the outperformance of non-linear methods, with deep
neural networks – speciﬁcally 3-layer multilayer
perceptrons (MLPs) – having the best out-of-sample
predictive R2. Machine learning portfolios were
then built by ranking stocks on a monthly basis
using model predictions, with the best strategy
coming from a 4-layer MLP that trades the top
decile against the bottom decile of predictions. In
other works, adopt a similar approach using
autoencoder and denoising autoencoder architectures,
incorporating volatility scaling into their model
as well. While the results with basic deep neural
networks are promising, they do not consider more
modern architectures for time series prediction, such
as the LSTM and WaveNet architectures
which we evaluate for the DMN. Moreover, to the
best of our knowledge, our paper is the ﬁrst to
consider the use of deep learning within the context
of time series momentum strategies – opening up
possibilities in an alternate class of signals.
Popularised by success of DeepMind’s AlphaGo
Zero , deep reinforcement learning (RL) has
also gained much attention in recent times – prized
for its ability to recommend path-dependent actions
in dynamic environments. RL is particularly of
interest within the context of optimal execution
and automated hedging for example, where
actions taken can have an impact on future states
of the world (e.g. market impact). However, deep
RL methods generally require a realistic simulation
environment (for Q-learning or policy gradient methods), or model of the world (for model-based RL)
to provide feedback to agents during training – both
of which are difﬁcult to obtain in practice.
III. STRATEGY DEFINITION
Adopting the terminology of , the combined
returns of a time series momentum (TSMOM)
strategy can be expressed as below – characterised
by a trading rule or signal Xt ∈[−1, 1]:
Here rTSMOM
is the realised return of the strategy
from day t to t + 1, Nt is the number of included
assets at t, and r(i)
t,t+1 is the one-day return of asset i.
We set the annualised volatility target σtgt to be 15%
and scale asset returns with an ex-ante volatility
estimate σ(i)
– computed using an exponentially
weighted moving standard deviation with a 60-day
span on r(i)
A. Standard Trading Rules
In traditional ﬁnancial time series momentum
strategies, the construction of a trading signal Xt
is typically divided into two steps: 1) estimating
future trends based on past information, and 2)
computing the actual positions to hold. We illustrate
this in this section using two examples from the
academic literature , which we also include as
benchmarks into our tests.
Moskowitz et al. 2012 : In their original paper
on time series momentum, a simple trading rule is
adopted as below:
Trend Estimation:
Position Sizing:
= sgn(Y (i)
This broadly uses the past year’s returns as a
trend estimate for the next time step - taking a
maximum long position when the expected trend
is positive (i.e. sgn(r(i)
t−252,t)) and a maximum short
position when negative.
Baz et al. 2015 : In practice, more sophisticated methods can be used to compute Y (i)
– such as the model of described below:
Trend Estimation:
= MACD(i, t, S, L) / std(p(i)
MACD(i, t, S, L) = m(i, S) −m(i, L).
Here std(p(i)
t−63:t) is the 63-day rolling standard
deviation of asset i prices p(i)
t−63:t = [p(i)
t−63, . . . , p(i)
m(i, S) is the exponentially weighted moving average of asset i prices with a time-scale S that translates into a half-life of HL = log(0.5)/ log(1 −1
The moving average crossover divergence (MACD)
signal is deﬁned in relation to a short and a long
time-scale S and L respectively.
The volatility-normalised MACD signal hence
measures the strength of the trend, which is then
translated in to a position size as below:
Position Sizing:
where φ(y) =
y exp( −y2
. Plotting φ(y) in Exhibit 1,
we can see that positions are increased until |Y (i)
2 ≈1.41, before decreasing back to zero for larger
moves. This allows the signal to reduces positions
in instances where assets are overbought or oversold
– deﬁned to be when |q(i)
t | is observed to be larger
than 1.41 times its past year’s standard deviation.
Exhibit 1: Position Sizing Function φ(y)
Increasing the complexity even further, multiple
signals with different times-scales can also be averaged to give a ﬁnal position:
t (Sk, Lk),
where Y (i)
t (Sk, Lk) is as per Equation (4) with
explicitly deﬁned short and long time-scales – using
Sk ∈{8, 16, 32} and Lk ∈{24, 48, 96} as deﬁned
B. Machine Learning Extensions
As can be seen from Section III-A, many
explicit design decisions are required to deﬁne a
sophisticated time series momentum strategy. We
hence start by considering how machine learning
methods can be used to learn these relationships
directly from data – alleviating the need for manual
speciﬁcation.
Standard Supervised Learning: In line with numerous previous works (see Section II-B), we can
cast trend estimation as a standard regression or
binary classiﬁcation problem, with outputs:
Trend Estimation:
where f(·) is the output of the machine learning
model, which takes in a vector of input features
and model parameters θθθ to generate predictions.
Taking volatility-normalised returns as targets, the
following mean-squared error and binary crossentropy losses can be used for training:
Lbinary(θθθ)
+ (1 −I) log
T−1,T/σ(N)
is the set of all M possible
prediction and target tuples across all N assets and
T time steps. For the binary classiﬁcation case, I is
the indicator function I
t,t+1/σ(i)
the estimated probability of a positive return.
This still leaves us to specify how trend estimates
map to positions, and we do so using a similar form
to Equation 3:
Position Sizing:
Regression
= sgn(Y (i)
Classiﬁcation
= sgn(Y (i)
As such, we take a maximum long position when
the expected returns are positive in the regression
case, or when the probability of a positive return is
greater than 0.5 in the classiﬁcation case.
Direct Outputs: An alternative approach is to
use machine learning models to generate positions
directly – simultaneously learning both trend estimation and position sizing in the same function,
Direct Outputs:
Given the lack of direct information on the optimal
positions to hold at each step – which is required
to produce labels for standard regression and classi-
ﬁcation models – calibration would hence need to
be performed by directly optimising performance
metrics. Speciﬁcally, we focus on optimising the
average return and the annualised Sharpe ratio via
the loss functions below:
Lreturns(θθθ) =
Lsharpe(θθθ) = −
ΩR(i, t)2) /M −µ2
where µR is the average return over Ω, and R(i, t)
is the return captured by the trading rule for asset i
at time t.
IV. DEEP MOMENTUM NETWORKS
In this section, we examine a variety of architectures that can be used in Deep Momentum Networks
– all of which can be easily reconﬁgured to generate
the predictions described in Section III-B. This is
achieved by implementing the models using the
Keras API in Tensorflow , where output
activation functions can be ﬂexibly interchanged
to generate the predictions of different types (e.g.
expected returns, binary probabilities, or direct positions). Arbitrary loss functions can also be deﬁned
for direct outputs, with gradients for backpropagation
being easily computed using the built-in libraries for
automatic differentiation.
A. Network Architectures
Lasso Regression: In the simplest case, a
standard linear model could be used to generate
predictions as below:
where Z(i)
depending on the prediction task, w is a weight vector for the linear
model, and b is a bias term. Here g(·) is a activation
function which depends on the speciﬁc prediction
type – linear for standard regression, sigmoid for
binary classiﬁcation, and tanh-function for direct
Additional regularisation is also provided during
training by augmenting the various loss functions to
include an additional L1 regulariser as below:
˜L(θθθ) = L(θθθ) + α||w||1,
where L(θθθ) corresponds to one of the loss functions
described in Section III-B, ||w||1 is the L1 norm
of w, and α is a constant term which we treat as
an additional hyperparameter. To incorporate recent
history into predictions as well, we concatenate
inputs over the past τ-days into a single input vector
– i.e. u(i)
t−τ:t = [u(i) T
t−τ , . . . , u(i) T
]T. This was ﬁxed
to be τ = 5 days for tests in Section V.
Multilayer Perceptron (MLP): Increasing the
degree of model complexity slightly, a 2-layer neural
network can be used to incorporated non-linear
t−τ:t + bh
where h(i)
is the hidden state of the MLP using
an internal tanh activation function, tanh(·), and
W. and b. are layer weight matrices and biases
respectively.
WaveNet: More modern techniques such as
convolutional neural networks (CNNs) have been
used in the domain of time series prediction – particularly in the form of autoregressive architectures
e.g. . These typically take the form of 1D causal
convolutions, sliding convolutional ﬁlters across time
to extract useful representations which are then
aggregated in higher layers of the network. To
increase the size of the receptive ﬁeld – or the length
of history fed into the CNN – dilated CNNs such as
WaveNet have been proposed, which skip over
inputs at intermediate levels with a predetermined
dilation rate. This allows it to effectively increase
the amount of historical information used by the
CNN without a large increase in computational cost.
Let us consider a dilated convolutional layer with
residual connections take the form below:
ψ(u) = tanh(Wu) ⊙σ(Vu)
Gated Activation
Skip Connection
Here W and V are weight matrices associated with
the gated activation function, and A and b are the
weights and biases used to transform the u to match
dimensionality of the layer outputs for the skip
connection. The equations for WaveNet architecture
used in our investigations can then be expressed as:
monthly(t)
weekly(t −5)
weekly(t −10)
weekly(t −15)
quarterly(t)
monthly(t)
monthly(t −21)
monthly(t −42)
Here each intermediate layer s(i)
. (t) aggregates
representations at weekly, monthly and quarterly
frequencies respectively. Intermediate layers are then
concatenated at each layer before passing through a
2-layer MLP to generate outputs, i.e.:
monthly(t)
quarterly(t)
= tanh(Whs(i)
State sizes for each intermediate layers s(i)
weekly(t),
monthly(t), s(i)
quarterly(t) and the MLP hidden state
are ﬁxed to be the same, allowing us to use a
single hyperparameter to deﬁne the architecture. To
independently evaluate the performance of CNN
and RNN architectures, the above also excludes the
LSTM block (i.e. the context stack) described in
 , focusing purely on the merits of the dilated
CNN model.
Long Short-term Memory (LSTM): Traditionally used in sequence prediction for natural language
processing, recurrent neural networks – speciﬁcally
long short-term memory (LSTM) architectures 
– have been increasing used in time series prediction
tasks. The equations for the LSTM in our model are
provided below:
= σ(Wfu(i)
t + Vfh(i)
= σ(Wiu(i)
t + Vih(i)
= σ(Wou(i)
t + Voh(i)
⊙tanh(Wcu(i)
t−1 + bc) (31)
⊙tanh(c(i)
where ⊙is the Hadamard (element-wise) product,
σ(.) is the sigmoid activation function, W. and
V. are weight matrices for the different layers,
correspond to the forget, input and
output gates respectively, c(i)
is the cell state, and
is the hidden state of the LSTM. From these
equations, we can see that the LSTM uses the cell
state as a compact summary of past information,
controlling memory retention with the forget gate
and incorporating new information via the input gate.
As such, the LSTM is able to learn representations
of long-term relationships relevant to the prediction
task – sequentially updating its internal memory
states with new observations at each step.
B. Training Details
Model calibration was undertaken using minibatch
stochastic gradient descent with the Adam optimiser
 , based on the loss functions deﬁned in Section
III-B. Backpropagation was performed up to a
maximum of 100 training epochs using 90% of a
given block of training data, and the most recent
10% retained as a validation dataset. Validation
data is then used to determine convergence – with
early stopping triggered when the validation loss
has not improved for 25 epochs – and to identify
the optimal model across hyperparameter settings.
Hyperparameter optimisation was conducted using
50 iterations of random search, with full details
provided in Appendix B. For additional information
on the deep neural network calibration, please refer
Dropout regularisation was a key feature
to avoid overﬁtting in the neural network models
– with dropout rates included as hyperparameters
during training. This was applied to the inputs and
hidden state for the MLP, as well as the inputs,
Equation (22), and outputs, Equation (26), of the
convolutional layers in the WaveNet architecture.
For the LSTM, we adopted the same dropout masks
as in – applying dropout to the RNN inputs,
recurrent states and outputs.
V. PERFORMANCE EVALUATION
A. Overview of Dataset
The predictive performance of the different architectures was evaluated via a backtest using 88 ratioadjusted continuous futures contracts downloaded
from the Pinnacle Data Corp CLC Database .
These contracts spanned across a variety of asset
classes – including commodities, ﬁxed income and
currency futures – and contained prices from 1990
to 2015. A full breakdown of the dataset can be
found in Appendix A.
B. Backtest Description
Throughout our backtest, the models were recalibrated from scratch every 5 years – re-running
the entire hyperparameter optimisation procedure
using all data available up to the recalibration point.
Model weights were then ﬁxed for signals generated
over the next 5 year period, ensuring that tests were
performed out-of-sample.
For the Deep Momentum Networks, we incorporate a series of useful features adopted by standard
time series momentum strategies in Section III-A to
generate predictions at each step:
1) Normalised Returns – Returns over the past
day, 1-month, 3-month, 6-month and 1-year
periods are used, normalised by a measure of
daily volatility scaled to an appropriate time
scale. For instance, normalised annual returns
were taken to be r(i)
t−252,t/(σ(i)
2) MACD Indicators – We also include the
MACD indicators – i.e. trend estimates Y (i)
as in Equation (4), using the same short timescales Sk ∈{8, 16, 32} and long time-scales
Lk ∈{24, 48, 96}.
For comparisons against traditional time series momentum strategies, we also incorporate the following
reference benchmarks:
1) Long Only with Volatility Scaling (X(i)
2) Sgn(Returns) – Moskowitz et al. 2012 
3) MACD Signal – Baz et al. 2015 
Finally, performance was judged based on the
following metrics:
1) Proﬁtability – Expected returns (E[Returns])
and the percentage of positive returns observed
across the test period.
2) Risk – Daily volatility (Vol.), downside deviation and the maximum drawdown (MDD) of
the overall portfolio.
3) Performance Ratios – Risk adjusted performance was measured by the Sharpe ratio
E[Returns]
, Sortino ratio
E[Returns]
Downside Deviation
Calmar ratio
E[Returns]
, as well as the average
proﬁt over the average loss
C. Results and Discussion
Aggregating the out-of-sample predictions from
1995 to 2015, we compute performance metrics
for both the strategy returns based on Equation (1)
(Exhibit 2), as well as that for portfolios with an
additional layer of volatility scaling – which brings
overall strategy returns to match the 15% volatility
target (Exhibit 3). Given the large differences in
returns volatility seen in Table 2, this rescaling
also helps to facilitate comparisons between the
cumulative returns of different strategies – which
are plotted for various loss functions in Exhibit
4. We note that strategy returns in this section
are computed in the absence of transaction costs,
allowing us to focus on the raw predictive ability of
the models themselves. The impact of transaction
costs is explored further in Section VI, where we
undertake a deeper analysis of signal turnover. More
detailed results can also be found in Appendix C,
which echo the ﬁndings below.
Focusing on the raw signal outputs, the Sharpe
ratio-optimised LSTM outperforms all benchmarks
as expected, improving the best neural network
model (Sharpe-optimised MLP) by 44% and the
best reference benchmark (Sgn(Returns)) by more
than two times. In conjunction with Sharpe ratio
improvements to both the linear and MLP models,
this highlights the beneﬁts of using models which
capture non-linear relationships, and have access
to more time history via an internal memory state.
Additional model complexity, however, does not
necessarily lead to better predictive performance, as
demonstrated by the underperformance of WaveNet
compared to both the reference benchmarks and
simple linear models. Part of this can be attributed
to the difﬁculties in tuning models with multiple
design parameters - for instance, better results could
possibly achieved by using alternative dilation rates,
number of convolutional layers, and hidden state
sizes in Equations (22) to (24) for the WaveNet. In
contrast, only a single design parameter is sufﬁcient
to specify the hidden state size in both the MLP and
LSTM models. Analysing the relative performance
within each model class, we can see that models
which directly generate positions perform the best –
demonstrating the beneﬁts of simultaneous learning
both trend estimation and position sizing functions.
In addition, with the exception of a slight decrease
in the MLP, Sharpe-optimised models outperform
returns-optimised ones, with standard regression and
classiﬁcation benchmarks taking third and fourth
place respectively.
From Exhibit 3, while the addition of volatility
scaling at the portfolio level improved performance
ratios on the whole, it had a larger beneﬁcial effect on
machine learning models compared to the reference
benchmarks – propelling Sharpe-optimised MLPs to
outperform returns-optimised ones, and even leading
to Sharpe-optimised linear models beating reference
benchmarks. From a risk perspective, we can see that
both volatility and downside deviation also become
a lot more comparable, with the former hovering
close to 15.5% and the latter around 10%. However,
Sharpe-optimised LSTMs still retained the lowest
MDD across all models, with superior risk-adjusted
performance ratios across the board. Referring to the
cumulative returns plots for the rescaled portfolios in
Exhibit 4, the beneﬁts of direct outputs with Sharpe
ratio optimisation can also be observed – with larger
cumulative returns observed for linear, MLP and
LSTM models compared to the reference benchmarks. Furthermore, we note the general underperformance of models which use standard regression and
classiﬁcation methods for trend estimation – hinting
at the difﬁculties faced in selecting an appropriate
position sizing function, and in optimising models
to generate positions without accounting for risk.
This is particularly relevant for binary classiﬁcation
Exhibit 2: Performance Metrics – Raw Signal Outputs
Sgn(Returns)
Ave. Returns
Ave. Returns
Ave. Returns
Ave. Returns
Exhibit 3: Performance Metrics – Rescaled to Target Volatility
Sgn(Returns)
Ave. Returns
Ave. Returns
Ave. Returns
Ave. Returns
Exhibit 4: Cumulative Returns - Rescaled to Target Volatility
(a) Sharpe Ratio
(b) Average Returns
(d) Binary
methods, which produce relatively ﬂat equity lines
and underperform reference benchmarks in general.
Some of these poor results can be explained by
the implicit decision threshold adopted. From the
percentage of positive returns captured in Exhibit
3, most binary classiﬁcation models have about a
50% accuracy which, while expected of a classiﬁer
with a 0.5 probability threshold, is far below the
accuracies seen in other benchmarks. Furthermore,
performance is made worse by the fact that the
model’s magnitude of gains versus losses
is much smaller than competing methods – with
average loss magnitudes even outweighing proﬁts for
the MLP classiﬁer
Ave. L = 0.986
. As such, these
observations lend support to the direct generation of
positions sizes with machine learning methods, given
the multiple considerations (e.g. decision thresholds
and proﬁt/loss magnitudes) that would be required
to incorporate standard supervising learning methods
into a proﬁtable trading strategy.
Strategy performance could also be aided by
diversiﬁcation across a range of assets, particularly
when the correlation between signals is low. Hence,
to evaluate the raw quality of the underlying signal,
we investigate the performance constituents of the
time series momentum portfolios – using box plots
for a variety of performance metrics, plotting the
minimum, lower quartile, median, upper quartile, and
maximum values across individual futures contracts.
We present in Exhibit 5 plots of one metric per
category in Section V-B, although similar results can
be seen for other performance ratios are documented
in Appendix C. In general, the Sharpe ratio plots
in Exhibit 5a echo previous ﬁndings, with direct
output methods performing better than indirect trend
estimation models. However, as seen in Exhibit 5c,
this is mainly attributable to signiﬁcant reduction in
signal volatility for the Sharpe-optimised methods,
despite a comparable range of average returns in
Exhibit 5b. The beneﬁts of retaining the volatility
scaling can also be observed, with individual signal
volatility capped near the target across all methods
– even with a naive sgn(.) position sizer. As such,
the combination of volatility scaling, direct outputs
and Sharpe ratio optimisation were all key to
performance gains in Deep Momentum Networks.
Exhibit 5: Performance Across Individual Assets
(a) Sharpe Ratio
(b) Average Returns
(c) Volatility
VI. TURNOVER ANALYSIS
To investigate how transaction costs affect strategy
performance, we ﬁrst analyse the daily position
changes of the signal – characterised for asset i
by daily turnover ζ(i)
as deﬁned in :
Which is broadly proportional to the volume of
asset i traded on day t with reference to the updated
portfolio weights.
Exhibit 6a shows the average strategy turnover
across all assets from 1995 to 2015, focusing on
positions generated by the raw signal outputs. As the
box plots are charted on a logarithm scale, we note
that while the machine learning-based models have
a similar turnover, they also trade signiﬁcantly more
than the reference benchmarks – approximately 10
times more compared to the Long Only benchmark.
This is also reﬂected in Exhibit 6a which compares
the average daily returns against the average daily
turnover – with ratios from machine learning models
lying close to the x-axis.
To concretely quantify the impact of transaction
costs on performance, we also compute the excost Sharpe ratios – using the rebalancing costs
deﬁned in to adjust our returns for a variety
of transaction cost assumptions . For the results
in Exhibit 7, the top of each bar chart marks the
maximum cost-free Sharpe ratio of the strategy,
with each coloured block denoting the Sharpe ratio
reduction for the corresponding cost assumption.
In line with the turnover analysis, the reference
benchmarks demonstrate the most resilience to high
transaction costs (up to 5bps), with the proﬁtability
across most machine learning models persisting only
up to 4bps. However, we still obtain higher costadjusted Sharpe ratios with the Sharpe-optimised
LSTM for up to 2-3 bps, demonstrating its suitability
for trading more liquid instruments.
A. Turnover Regularisation
One simple way to account for transaction costs is
to use cost-adjusted returns ˜rTSMOM
directly during
training, augmenting the strategy returns deﬁned in
Equation (1) as below:
where c is a constant reﬂecting transaction cost
assumptions. As such, using ˜rTSMOM
ratio loss functions during training corresponds to
optimising the ex-cost risk-adjusted returns, and
can also be interpreted as a regularisation term for turnover.
Given that the Sharpe-optimised LSTM is still
proﬁtable in the presence of small transactions costs,
we seek to quantify the effectiveness of turnover
regularisation when costs are prohibitively high –
considering the extreme case where c = 10bps in
our investigation. Tests were focused on the Sharpeoptimised LSTM with and without the turnover
regulariser (LSTM + Reg. for the former) – including
the additional portfolio level volatility scaling to
bring signal volatilities to the same level. Based on
the results in Exhibit 8, we can see that the turnover
regularisation does help improve the LSTM in the
presence of large costs, leading to slightly better
performance ratios when compared to the reference
benchmarks.
VII. CONCLUSIONS
We introduce Deep Momentum Networks – a
hybrid class of deep learning models which retain
the volatility scaling framework of time series momentum strategies while using deep neural networks
to output position targeting trading signals. Two
approaches to position generation were evaluated
here. Firstly, we cast trend estimation as a standard
supervised learning problem – using machine learning models to forecast the expected asset returns or
probability of a positive return at the next time step –
and apply a simple maximum long/short trading rule
based on the direction of the next return. Secondly,
trading rules were directly generated as outputs
from the model, which we calibrate by maximising
the Sharpe ratio or average strategy return. Testing
this on a universe of continuous futures contracts,
we demonstrate clear improvements in risk-adjusted
performance by calibrating models with the Sharpe
Exhibit 6: Turnover Analysis
(a) Average Strategy Turnover
(b) Average Returns / Average Turnover
Exhibit 7: Impact of Transaction Costs on Sharpe Ratio
Exhibit 8: Performance Metrics with Transaction Costs (c = 10bps)
Sgn(Returns)
LSTM + Reg.
ratio – where the LSTM model achieved best results.
Incorporating transaction costs, the Sharpe-optimised
LSTM outperforms benchmarks up to 2-3 basis
points of costs, demonstrating its suitability for
trading more liquid assets. To accommodate high
costs settings, we introduce a turnover regulariser to
use during training, which was shown to be effective
even in extreme scenarios (i.e. c = 10bps).
Future work includes extensions of the framework
presented here to incorporate ways to deal better with
non-stationarity in the data, such as using the recently
introduced Recurrent Neural Filters . Another
direction of future work focuses on the study of time
series momentum at the microstructure level.
VIII. ACKNOWLEDGEMENTS
We would like to thank Anthony Ledford, James
Powrie and Thomas Flury for their interesting
comments as well the Oxford-Man Institute of
Quantitative Finance for ﬁnancial support.