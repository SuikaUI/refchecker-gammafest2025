Research-paper recommender systems: a literature survey
Joeran Beel1 · Bela Gipp2 · Stefan Langer3 · Corinna Breitinger4
Abstract In the last 16years, more than 200 research articles were published about research-paper recommender systems.Wereviewedthesearticlesandpresentsomedescriptive
statistics in this paper, as well as a discussion about the major
advancements and shortcomings and an overview of the most
common recommendation concepts and approaches. We
found that more than half of the recommendation approaches
applied content-based ﬁltering (55%). Collaborative ﬁltering
was applied by only 18% of the reviewed approaches, and
graph-based recommendations by 16%. Other recommendation concepts included stereotyping, item-centric recommendations, and hybrid recommendations. The content-based
ﬁltering approaches mainly utilized papers that the users
had authored, tagged, browsed, or downloaded. TF-IDF was
the most frequently applied weighting scheme. In addition to simple terms, n-grams, topics, and citations were
utilized to model users’ information needs. Our review
revealed some shortcomings of the current research. First,
it remains unclear which recommendation concepts and
approaches are the most promising. For instance, researchers
reported different results on the performance of contentbased and collaborative ﬁltering. Sometimes content-based
ﬁltering performed better than collaborative ﬁltering and
B Joeran Beel
 
 
Corinna Breitinger
 
Docear, Magdeburg, Germany
University of Konstanz, Konstanz, Germany
Otto-von-Guericke University, Magdeburg, Germany
Linnaeus University, Kalmar, Sweden
sometimes it performed worse. We identiﬁed three potential reasons for the ambiguity of the results. (A) Several
evaluations had limitations. They were based on strongly
pruned datasets, few participants in user studies, or did not
use appropriate baselines. (B) Some authors provided little
information about their algorithms, which makes it difﬁcult
to re-implement the approaches. Consequently, researchers
use different implementations of the same recommendations approaches, which might lead to variations in the
results. (C) We speculated that minor variations in datasets,
algorithms, or user populations inevitably lead to strong variations in the performance of the approaches. Hence, ﬁnding
the most promising approaches is a challenge. As a second limitation, we noted that many authors neglected to
take into account factors other than accuracy, for example overall user satisfaction. In addition, most approaches
(81%) neglected the user-modeling process and did not infer
information automatically but let users provide keywords,
text snippets, or a single paper as input. Information on
runtime was provided for 10% of the approaches. Finally,
few research papers had an impact on research-paper recommender systems in practice. We also identiﬁed a lack
of authority and long-term research interest in the ﬁeld:
73% of the authors published no more than one paper
on research-paper recommender systems, and there was
little cooperation among different co-author groups. We
concluded that several actions could improve the research
landscape: developing a common evaluation framework,
agreement on the information to include in research papers,
a stronger focus on non-accuracy aspects and user modeling, a platform for researchers to exchange information, and
an open-source framework that bundles the available recommendation approaches.
Konstanzer Online-Publikations-System (KOPS)
URL: 
Erschienen in: International Journal on Digital Libraries ; 17 , 4. - S. 305-338
 
Keywords Recommender system · User modeling ·
Research paper recommender systems · Content based
ﬁltering · Review · Survey
1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3 Related research ﬁelds . . . . . . . . . . . . . . . . . . . . . . .
4 Survey of the evaluations
. . . . . . . . . . . . . . . . . . . . .
4.1 Evaluation methods and their adequacy . . . . . . . . . . . .
4.1.1 User studies . . . . . . . . . . . . . . . . . . . . . . .
4.1.2 Online evaluations
. . . . . . . . . . . . . . . . . . .
4.1.3 Ofﬂine evaluations . . . . . . . . . . . . . . . . . . .
4.2 The operator’s perspective
. . . . . . . . . . . . . . . . . .
4.3 Coverage
. . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Baselines
. . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Ofﬂine evaluation metrics . . . . . . . . . . . . . . . . . . .
4.6 Datasets and architectures . . . . . . . . . . . . . . . . . . .
4.7 Reproducibility and the butterﬂy effect . . . . . . . . . . . .
5 Survey of the recommendation classes
. . . . . . . . . . . . . .
5.1 Stereotyping . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Content-based ﬁltering
. . . . . . . . . . . . . . . . . . . .
5.3 Collaborative ﬁltering . . . . . . . . . . . . . . . . . . . . .
5.4 Co-occurrence recommendations . . . . . . . . . . . . . . .
5.5 Graph based . . . . . . . . . . . . . . . . . . . . . . . . . .
5.6 Global relevance . . . . . . . . . . . . . . . . . . . . . . . .
5.7 Hybrid recommendation approaches . . . . . . . . . . . . .
6 Survey of the research ﬁeld and shortcomings . . . . . . . . . . .
6.1 Neglect of user modeling . . . . . . . . . . . . . . . . . . .
6.2 Focus on accuracy . . . . . . . . . . . . . . . . . . . . . . .
6.2.1 Users’ tasks . . . . . . . . . . . . . . . . . . . . . . .
6.2.2 Diversity
. . . . . . . . . . . . . . . . . . . . . . . .
6.2.3 Layout
. . . . . . . . . . . . . . . . . . . . . . . . .
6.2.4 User characteristics . . . . . . . . . . . . . . . . . . .
6.2.5 Usage duration . . . . . . . . . . . . . . . . . . . . .
6.2.6 Recommendation medium . . . . . . . . . . . . . . .
6.2.7 Relevance and proﬁle feedback . . . . . . . . . . . . .
6.3 Translating research into practice . . . . . . . . . . . . . . .
6.4 Persistence and authorities
. . . . . . . . . . . . . . . . . .
6.5 Cooperation . . . . . . . . . . . . . . . . . . . . . . . . . .
6.6 Information scarcity . . . . . . . . . . . . . . . . . . . . . .
7 Summary and outlook . . . . . . . . . . . . . . . . . . . . . . .
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1 Introduction
In 1998, Giles et al. introduced the ﬁrst research-paper recommender system as part of the CiteSeer project . Since
then, at least 216 articles relating to 120 research-paper
recommendation approaches were published . The
amount of literature and approaches represents a problem
for new researchers: they do not know which of the articles
are most relevant, and which recommendation approaches
are most promising. Even researchers familiar with researchpaper recommender systems would ﬁnd it difﬁcult to keep
track of the current developments, since the yearly number of
articles steadily increases: 66 of the 217 articles (30%) were
published just in 2012 and 2013 alone (Fig. 1; Table 1). The
few existing literature surveys in the ﬁeld cover
just a fraction of the articles, or focus on selected aspects,
such as recommender-system evaluation . Thus, they
do not provide an overview of the research ﬁeld, or identify
the most promising approaches.
We survey the ﬁeld of research-paper recommender systems with the goal of enabling researchers and developers
to (a) learn about the status-quo of research-paper recommender systems, (b) identify promising ﬁelds of research,
and (c) motivate the community to solve the most urgent
problems that currently hinder the effective use of researchpaper recommender systems in practice. For clarity, we use
the term “article” to refer to the reviewed journal articles,
patents, websites, etc., and the term “paper” to refer to documents being recommended by research-paper recommender
systems.1 When referring to a large number of recommender
systems with certain properties, we cite three exemplary articles. For instance, when we report how many recommender
systems apply content-based ﬁltering, we report the number
and provide three references .
To identify relevant literature for our survey, we conducted
a literature search on Google Scholar, ACM Digital Library,
Springer Link, and ScienceDirect. We searched for [paper |
article | citation] [recommender | recommendation] [system
| systems] and downloaded all articles that had relevance for
research-paper recommender systems. Our relevance judgment made use of the title and the abstract if the title alone
did not indicate a recognizable relevance to research-paper
recommender systems. We examined the bibliography of
each article. If an entry in the bibliography pointed to a
relevant article not yet downloaded, we downloaded that
article. In addition, we checked on Google Scholar which
articles cited the relevant article. If one of the citing articles
seemed relevant, we also downloaded it. We expanded our
search to websites, blogs, patents, and presentations on major
academic recommender systems. These major academic services include the academic search engines CiteSeer(x),2
Google Scholar (Scholar Update),3 and PubMed;4 the social
network ResearchGate;5 and the reference managers CiteU-
Like,6 Docear,7 and Mendeley.8 While these systems offer
1 Some recommender systems also recommended “citations” but in
our opinion, differences between recommending papers and citations
are marginal, which is why we do not distinguish between these two
terms in this paper.
2 
3 
4 
5 researchgate.net/.
6 
7 
8 
-- •.. ·•·
···~····; ··; ;i-~/·mlt , ......•
1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013
=:JCumulated
100 121 137
151 177 217
... <> ... New (per year)
Fig. 1 Annual publications in the field of research-paper recommender
systems. Numbers are based on our literature search. Although, we
believe our survey to be the most comprehensive survey about researchpaper recommender systems, we may have missed a few articles. In
addition, most likely, more than 40 papers were published in 2013 since
recommender systems along with their main services, there
are also a few stand-alone recommender systems, namely
BibTip,9 bX, 10 RefSeer,1 1 TheAdvisor12 and an experimental
system called SarkantoP
The first literature search was conducted in June 2013 and
found 188 relevant articles [l- 188]. Three of the 188 articles
were literature surveys , which were ignored in our
survey. The remaining 185 articles consist of peer-reviewed
conference articles (59%), journal articles ( 16% ), pre-prints
(5% ), and other formats such as Ph.D. theses, patents, presentations, and web pages (Table 2). Overall, the reviewed
articles were comprehensive, with a median page count of
eight. More than one-third (36%) had 10 or more pages
(Fig. 2). Another 23% had eight or nine pages, while only
26% of the articles had four or less pages.
Citation counts follow a typical power- law distribution:
a few articles gained many citations (the maximum was
528 citations for ) and many articles had few citations,
see Fig. 3. The mean citation count was 30, and median
was seven. From the reviewed articles, 31% had no citations. Citation counts were retrieved from Google Scholar in
early 2014. Some researchers have reservations about using
Google Scholar as source for citation counts , but
the numbers can give a rough idea of a paper's popularity.
We reviewed the 185 articles, which contained information on 96 research-paper recommendation approaches
 . In an initial review, we focused on the evaluation
of the approaches. The review included an analysis of which
9 
10 
I I 
12 
13 
we conducted the literature search in January 2014. Articles presented
at conferences in late 2013 most likely had not been published in conferences proceedings by January 2014, and hence were not found through
our search. Hence, the total number of papers published is probably
higher than 217
evaluation methods were applied (e.g., user-studies or offline
evaluations), which evaluation metrics were used (e.g., precision or recall), how many participants the user studies bad,
and how strongly datasets were pruned.
Of the 96 research-paper recommendation approaches
presented in 185 articles, 62 approaches were chosen for
an in-depth analysis, presented in 127 articles . We
chose these 62 approaches, because we classified the remaining 34 approaches, i.e., 58 articles, as not sufficiently related
to 'research-paper recommender systems' . We
classified articles as insufficiently related if they provided
no evaluation, or if their approach did not differ significantly
from that of previous authors. We also excluded articles that
could not be clearly interpreted due to grammar and language use, or when they were outside of the scope (even
if the article's title suggested relevance to research-paper
recommender systems). One example of an article outside
of the research scope was 'Research Paper Recommender
Systems- A Subspace Clustering Approach' . The title
appears relevant for this survey, but the article presents a
collaborative filtering approach that is not intended for recommender systems for research papers. Instead, the paper
used the Movielens dataset, which contains ratings of movies.
In January 2014, we conducted a second literature search
and found 29 additional articles of relevance . The
goal of this search was to identify the overall number of
articles published in 2013; see Fig. 1. However, time limitations prevented us from broadening the scope of the initially
planned survey. Therefore, our review concentrates on the
two subsets of the 217 articles: the 185 articles identified
during the first round of the literature search, and the 127
articles that we chose for an in-depth review.
In the remaining paper, we present definitions (Sect. 2),
followed by an introduction to related research fields (Sect. 3).
Table 1 List of reviewed articles by year
References 
We then present the survey of the 96 approaches’ evaluations
(Sect.4),followedbyananalysisofthe62approachesthatwe
chose for the in-depth review (Sect. 5). Finally, we examine
the ﬁeld of research-paper recommender systems in general,
and point out some shortcomings, such as the neglect of user
modeling, a strong focus on accuracy alone at the expense
of other aspects being ignored and scarce information on the
details of the algorithms used (Sect. 6).
2 Deﬁnitions
We use the term “idea” to refer to a hypothesis about how
recommendations could be effectively generated. To differentiate how speciﬁc the idea is, we distinguish between
recommendation classes, approaches, algorithms, and implementations (Fig. 4).
We deﬁne a “recommendation class” as the least speciﬁc idea, namely a broad concept that broadly describes
how recommendations might be given. For instance, the
recommendation classes collaborative ﬁltering (CF) and
content-based ﬁltering (CBF) fundamentally differ in their
underlying ideas: the underlying idea of CBF is that users
are interested in items that are similar to items the users previously liked. In contrast, the idea of CF is that users like
items that the users’ peers liked. However, these ideas are
rather vague and leave room for different approaches.
A “recommendation approach” is a model of how to bring
a recommendation class into practice. For instance, the idea
behind CF can be realized with user-based CF , contentboosted CF , and various other approaches . These
approaches are quite different, but are each consistent with
the central idea of CF. Nevertheless, these approaches to represent a concept are still vague and leave room for speculation
on how recommendations are calculated.
A “recommendation algorithm” precisely speciﬁes a recommendation approach. For instance, an algorithm of a CBF
approach would specify whether terms were extracted from
the title of a document or from the body of the text, and how
terms are processed (e.g., stop-word removal or stemming)
and weighted (e.g., TF-IDF). Algorithms are not necessarily complete. For instance, pseudo-code might contain only
the most important information and ignore basics, such as
weighting schemes. This means that for a particular recommendation approach there might be several algorithms.
Finally, the “implementation” is the actual source code
of an algorithm that can be compiled and applied in a recommender system. It fully details how recommendations are
generated and leaves no room for speculation. It is, therefore,
the most speciﬁc idea about how recommendations might be
generated.
A “recommender system” is a fully functional software
system that applies at least one implementation to make rec-
Table 2 Percentage of different
article types reviewed
Page Count
Fig. 2 Page count of reviewed articles
Conference
s:~"'"' $' ~0, -;!'
,.,~ ,_<§1
Number of citations
Fig. 3 Citation counts overview for reviewed articles
ommendations. In addition, recommender systems feature
several other components, such as a user interface, a corpus of
recommendation candidates, and an operator that owns/runs
the system. Some recommender systems also use two or more
recommendation approaches: CiteULike, a service for discovering and managing scholarly references, lets their users
choose between two approaches , and Docear randornl y selects one of three approaches each time users request
recommendations .
The "recommendation scenario" describes the entire
setting of a recommender system, including the recommender system and the recommendation environment, i.e.,
the domain and user characteristics.
By "effectiveness," we refer to the degree to which a recommender system achieves its objective. The objective of a
recommender system from a broad perspective is to provide
"good" and "useful" recommendations that make
users "happy" by satisfying user needs . The needs
of users vary. Consequently, some users might be interested in
novel research-paper recommendations, while others might
be interested in authoritative research-paper recommendations. Of course, users require recommendations specific to
their fields of research . When we use the term "effectiveness," we refer to the specific objective the evaluator
Pre-prints/
unpublished
wanted to measure. We use the terms "performance" and
"effectiveness" interchangeably.
"Evaluation" describes any kind of assessment that measures the effectiveness or merit of a concrete idea or approach.
More details about research paper recommender system evaluation methods follow in Sect. 4.
3 Related research fields
Several research fields are related to user modeling and
(research-paper) recommender systems. Although we do not
survey these fields, we introduce them so interested readers
can broaden their research.
Research on academic search engines deals with calculating relevance between research papers and search queries
 . The techniques are often similar to those used by
research-paper recommender systems. In some cases, recommender systems and academic search engines are even
identical. As described later, some recommender systems
require their users to provide keywords that represent their
interests. In these cases, research-paper recommender systems do not differ from academic search engines where
users provide keywords to retrieve relevant papers. Consequently, these fields are highly related and most approaches
for academic search engines are relevant for research-paper
recommender systems.
The reviewer assignment problem
information-retrieval and information-filtering techniques to
automate the assignment of conference papers to reviewers
 . The differences from research-paper recommendations are minimal: in the reviewer assignment problem,
a relatively small number of paper submissions must be
assigned to a small number of users, i.e., reviewers; researchpaper recommender systems recommend a few papers out of
a large corpus to a relatively large number of users. However,
the techniques are usually identical. The reviewer assignment
problem was first addressed by Dumais and Nielson in 1992
 ; 6 years before Giles et al. introduced the first researchpaper recommender system. A good survey on the reviewer
assignment problem was published by Wang et al. .
Scietttometrics deals with analyzing the impact of
researchers, research articles and the links between them. Scientometrics researchers use several techniques to calculate
document relatedness or to rank a collection of articles. Some
of the measures- h-index , co-citation strength 
and bibliographic coupling strength - have also been
applied by research-paper recommender systems [13, 123,
Envirorvnent
Recommender System
User Interface
~mplementation J
Fig. 4 mustration of recommendation system terminology and concepts
126]. However, there are many more metrics in scientometrics that might be relevant for research-paper recommender
systems .
User modeling evolved from the field of Human Computer Interaction. One thing user modeling focuses on is
reducing users' information overload making use of users'
current tasks and backgrounds . User modeling shares
this goal with recommender systems, and papers published
at the major conferences in both fields (UMAP14 and Rec-
Sys 15) often overlap. User modeling is a central component of
recommender systems because modeling the users' information needs is crucial for providing useful recommendations.
For some comprehensive surveys about user modeling in the
context of web personaliza6on, refer to .
Other related research fields include book recommender
systems , educational recommender systems , academic aler6ng services , expert search , automatic
summarization of academic articles , academic
news feed recommenders , academic event recommenders , venue recommendations , citation
recommenders for patents , recommenders for academic datasets , and plagiarism detection. Plagiarism
detection, like many research-paper recommenders, uses text
and citation analysis to identify similar documents . Additionally, research relating to crawling the web
and analyzing academic ar6cles can be useful for building
research-paper recommender systems, for instance, author
14 
15 
More SpecWic
Recommendation 1
name extraction and disambiguation , title extraction
 , or citation extraction and matching . Finally,
most of the research on content-based or collaborative
filtering from other domains, such as movies or
news, can also be relevant for research-paper recommender
4 Survey of the evaluations
Recommender-systems research heavily relies on evaluations to assess the effectiveness of recommendation
approaches. Among the key prerequisites for thorough evaluations are appropriate evaluation methods, a sufficient
number of study participants, and a comparison of the novel
approach against one or more state-of-the-art approaches
 . The novel approach and its evaluation must be
clearly described. The soundness of the evaluation, the reirnplementation of the approach, and the reproducibility and
replicability of the results are guaranteed only if a clear
description is given.
We reviewed the evaluation methods, metrics, and datasets
used; the number of participants in the user studies; the baselines used for comparing the novel approaches; and several
other factors to judge the appropriateness of the evaluations
of the 96 approaches. Originally, our goal was to identify
the approaches for which evaluations were thoroughly conducted. The further review would have then concentrated
on these thoroughly evaluated approaches to identify the
most promising approaches. However, as we will show in
the following sections, the majority of evaluations contained
limitations, which made it impossible to determine a number
of promising approaches.
4.1 Evaluation methods and their adequacy
Of the 96 reviewed recommendation approaches, 21 (22%)
were not evaluated by their authors . In other
cases, an evaluation was attempted, but the methods were
questionable and were insufﬁciently described to be understandable or reproducible . Of the remaining
75 evaluated approaches, 53 (71%) were evaluated using
ofﬂine evaluations, 25 (33%) using quantitative user studies, two (3%) using qualitative user studies, and ﬁve (7%)
using an online evaluation (Table 3). The different evaluation
methods and their application in the ﬁeld of research-paper
recommender systems are introduced in the next sections.
4.1.1 User studies
User studies typically measure user satisfaction through
explicit ratings. Users receive recommendations generated
by different recommendation approaches, users rate the recommendations, and the approach with the highest average
rating is considered most effective . Study participants
are typically asked to quantify their overall satisfaction with
the recommendations. However, they might also be asked
to rate individual aspects of a recommender system, for
instance, how novel or authoritative recommendations are
 , or how suitable they are for non-experts . A user
study can also collect qualitative feedback, but qualitative
feedback is rarely used in the ﬁeld of (research-paper) recommender systems .
We distinguish between “lab” and “real-world” user studies. In lab studies, participants are aware that they are part of
a user study, which together with other factors might affect
user behavior and thereby the evaluation’s results .
In real-world studies, participants are not aware of the study
and rate recommendations for their own beneﬁt, for instance
because the recommender system improves recommendations based on user ratings (i.e., relevance feedback ),
or user ratings are required to generate recommendations
(i.e., collaborative ﬁltering ). All reviewed user studies
were lab-based.
Table 3 Evaluation methods used by reviewed recommendation
approaches
User study
User study
Some approaches were evaluated using several methods. As a result,
percentages do not add up to 100%
Often, user studies are considered the optimal evaluation
method . However, the outcome of user studies often
depends on the questions asked. Cremonesi et al. found that
it makes a difference if users are asked for the “perceived
relevance” or the “global satisfaction” of recommendations
 . Similarly, it made a difference whether users were
asked to rate the novelty or the relevance of recommendations
 . A large number of participants are also crucial to user
study validity, which makes user studies relatively expensive
to conduct. The number of required participants, to receive
statistically signiﬁcant results, depends on the number of
approaches being evaluated, the number of recommendations
being displayed, and the variations in the results .
However, as a rough estimate, at least a few dozen participants are required, often more.
Most participants in the reviewed user studies rated only
a few recommendations and four studies (15%) were conducted with fewer than ﬁve participants ; ﬁve
studies (19%) had ﬁve to ten participants ; three
studies (12%) had 11–15 participants ; and
ﬁve studies (19%) had 16–50 participants . Six
studies (23%) were conducted with more than 50 participants . Three studies (12%) failed to mention
the number of participants (Table 4). Given these
ﬁndings, we conclude that most user studies were not large
enough to arrive at meaningful conclusions.
4.1.2 Online evaluations
Online evaluations were ﬁrst used by the online advertising
and e-commerce ﬁelds. They measure the acceptance rates
of recommendations in real-world recommender systems.
Acceptance rates are typically measured by click-through
rates (CTR), i.e., the ratio of clicked recommendations
to displayed recommendations. For instance, if a recommender system displays 10,000 recommendations and 120
are clicked, the CTR is 1.2%. Other metrics include the ratio
of downloaded or bought items to the number of items displayed. Acceptance rate is typically interpreted as an implicit
measure for user satisfaction. The assumption is that when a
user clicks, downloads, or buys a recommended item, the user
liked the recommendation. Of course, this assumption is not
alwaysreliablebecauseusersmightbuyabookbutrateitnegatively after reading it. However, metrics such as CTR can be
Table 4 Number of participants in user studies of reviewed recommendation approaches
Number of participants
an explicit measures of effectiveness, namely when the operator receives money, e.g., for clicks on recommendations.
Online evaluations are not without drawbacks. Zheng et
al. showed that CTR and relevance do not always correlate
and concluded that “CTR may not be the optimal metric
for online evaluation of recommender systems” and “CTR
should be used with precaution” . In addition, conducting online evaluations requires signiﬁcantly more time than
ofﬂine evaluations, they are more expensive, and they can
only be conducted by researchers who have access to a realworld recommender system.
Among the 75 approaches that included some form of
evaluation, only six (8%) were evaluated using an online
evaluation . Despite the active experimentation in
the ﬁeld with a large number or evaluations being performed
on research-paper recommender systems, we observed that
many researchers have no access to real-world systems to
evaluate their approaches. Interestingly, the researchers who
do have access to real-world recommender systems often do
not make use of this resource to conduct online evaluations,
but rather perform ofﬂine evaluations or lab user studies. For
instance,Gilesandhisco-authors,whoaresomeofthelargest
contributors in the ﬁeld, could have conducted online evaluations with their academic search engine CiteSeer. However,
they chose primarily to use ofﬂine evaluations. The reason
for this might be that ofﬂine evaluations are more convenient
than conducting online evaluations or user studies. Results
are available within minutes or hours and not within days or
weeks as is the case for user studies and online evaluations.
However, ofﬂine evaluations have a set of serious drawbacks,
as shown in the next section.
4.1.3 Ofﬂine evaluations
Ofﬂine evaluations typically measure the accuracy of a recommender system based on a ground truth. To measure
accuracy, precision at position n (P@n) is often used to
express how many items of the ground truth are recommended within the top n recommendations. Other common
evaluation metrics include recall, F-measure, mean reciprocal rank (MRR), normalized discounted cumulative gain
(nDCG), mean absolute error, and root mean square error.
Ofﬂine evaluations are also sometimes used to evaluate
aspects such as novelty or serendipity of recommendations
 . For a comprehensive overview of ofﬂine evaluations,
refer to .
Ofﬂine evaluations were originally meant to identify a
number of promising recommendation approaches . These approaches should then be evaluated in
detail with a user study or online evaluation to identify
the most effective approaches. However, criticism has been
raised on the assumption that ofﬂine evaluation could predict an algorithm’s effectiveness in online evaluations or
user studies. More precisely, several researchers have shown
that results from ofﬂine evaluations do not necessarily correlate with results from user studies or online evaluations
 . This means that approaches that are
effective in ofﬂine evaluations are not necessarily effective
in real-world recommender systems. McNee et al. observed
“the research community’s dependence on ofﬂine
experiments [has] created a disconnect between algorithms that score well on accuracy metrics and algorithms that users will ﬁnd useful.” 
Other researchers also voiced criticism of ofﬂine evaluations. Jannach et al. stated that “the results of ofﬁne
[evaluations] may remain inconclusive or even misleading”
and “real-world evaluations and, to some extent, lab studies represent probably the best methods to evaluate systems”
 . Knijnenburg et al. reported that “the presumed link
between algorithm accuracy […] and user experience […] is
all, but evident” . Said et al. consider “on-line evaluation
[as] the only technique able to measure the true user satisfaction” . Rashid et al. observed that biases in the ofﬂine
datasets may cause bias in the evaluation . The main
reason for the criticism in the literature is that ofﬂine evaluations focus on accuracy yet ignore human factors; however,
human factors strongly affect overall user satisfaction for
recommendations. Despite the criticism, ofﬂine evaluations
are the predominant evaluation method in the recommender
community and “surprisingly few studies [evaluate]
algorithms in live experiments with real users” .
Our review indicates that the voiced criticism of ofﬂine
evaluations also applies to the ﬁeld of research-paper recommender systems. Some of the approaches were evaluated using both an ofﬂine evaluation and a user study.
In two evaluations, results from the ofﬂine evaluations
were indeed similar to results of the user studies . However, the user studies had ﬁve and 19 participants, respectively, which led to statistically insigniﬁcant results. Three other studies reported contradicting
results for ofﬂine evaluations and user studies (two of
these studies had more than 100 participants) .
This means that ofﬂine evaluations could not reliably predict the effectiveness in the real-world use case. Interestingly, the three studies with the most participants were all
conducted by the authors of TechLens , who
are also the only authors in the ﬁeld of research-paper
recommender systems who discuss the potential shortcomings of ofﬂine evaluations . It seems that other
researchers in this ﬁeld are not aware of—or chose not
to address—problems associated with ofﬂine evaluations,
although there has been quite a discussion outside the
research-paper recommender-system community .
4.2 The operator’s perspective
It is commonly assumed that the objective of a recommender
system is to make users “happy” by satisfying their
needs . However, there is another important stakeholder
who is often ignored: the operator of a recommender system
 . It is often assumed that operators of recommender systems are satisﬁed when their users are satisﬁed, but this is not
always the case. Operators may also want to keep down costs
of labor, disk storage, memory, computing power, and data
transfer . Therefore, for operators, an effective recommender system may be one that can be developed, operated,
and maintained at a low cost. Operators may also want to
generate a proﬁt from the recommender system . Such
operators might prefer to recommend items with higher proﬁt
margins, even if user satisfaction is not optimal. For instance,
publishers might be more interested in recommending papers
the user must pay for than papers the user can freely download.
The operator’s perspective has been widely ignored in the
reviewed articles. Costs of building a recommender system,
or implementing an approach were not reported in any article.
Costs to run a recommender system were reported by Jack
from Mendeley . He stated that the costs on Amazon’s S3
were $66 a month plus $30 to update the recommender system that served 20 requests per second generated by 2million
Runtime information is crucial to estimate costs, and
hence to estimate how feasible an approach will be to apply
in practice. In one paper, the runtimes of two approaches
differed by a factor of 600 . For many operators, an
approach that requires 600 times more computing power than
another would probably not be an option. While this example
is extreme, other runtime comparisons showed differences by
a factor of ﬁve or more, which can also affect algorithm selection. However, information on runtime was provided only for
10% of the approaches.
Reporting on computational complexity is also important.
Foroperatorswhowanttooffertheirsystemtoalargenumber
of users, computational complexity is important for estimating the long-term suitability of an approach. An approach
may perform well enough for a few users, but it might
not scale well. Approaches with exponentially increasing
complexity most likely will not be applicable in practice.
However, computational complexity was reported for even
fewer approaches than runtime.
4.3 Coverage
Coverage describes how many papers of those in the recommender’s database might potentially be recommended
 . As such, coverage is an important metric to judge
the usefulness of a recommender system. For text-based
approaches, coverage is usually 100%. For other approaches,
coverage is typically lower. For instance, in collaborative ﬁltering not all items are rated by users. Although the unrated
items might be relevant, they cannot be recommended. High
coverage is important because it increases the number of
recommendations a user can receive. Of the reviewed articles, few considered coverage in their evaluations. He et al.
judge the effectiveness of their approaches based on which
approach provides the best tradeoff between accuracy and
coverage . The BibTip developers report that 80% of
all documents have been co-viewed and can be used for
generating recommendations . Pohl et al. report that
co-download coverage on arXiv is close to 100% while cocitation coverage is only around 30% . The TechLens
authors report that all of their hybrid and CBF approaches
have 100% coverage, except pure CF which has a coverage
of 93% .
4.4 Baselines
Another important factor in evaluating recommender systems is the baseline against which an algorithm is compared.
For instance, knowing that a certain approach has a particular CTR is not useful if the CTRs of alternative approaches
are unknown. Therefore, novel approaches should be compared against a baseline representative of the state-of-the-art
approaches. This way it is possible to quantify whether, and
when, a novel approach is more effective than the state-ofthe-art and by what margin.
Of the 75 evaluated approaches, 15 (20%) were not compared against a baseline . Another 53 (71%)
approaches were compared against trivial baselines, such
as simple content-based ﬁltering without any sophisticated
adjustments. These trivial baselines do not represent the
state-of-the-art and are not helpful for deciding whether a
novel approach is promising. This is particularly troublesome
since the reviewed approaches were not evaluated against
the same trivial baselines. Even for a simple CBF baseline,
there are many variables, such as whether stop words are
ﬁltered, which stemmer is applied, or from which document ﬁeld the text is extracted. This means that almost all
reviewed approaches were compared against different baselines, and results cannot be compared with each other. Seven
approaches (9%) were evaluated against approaches proposed by other researchers in the ﬁeld. Only these evaluations
allow drawing some conclusions on which approaches may
be most effective.
It is interesting to note that in all evaluations, at least one
of the novel approaches performed better than the baseline
(if the approach was evaluated against a baseline). No article
reported on a non-effective approach. We can just speculate about the reasons: First, authors may intentionally select
baselines such that their approaches appear favorable. Sec-
Table 5 Evaluation metrics of
reviewed recommendation
approaches
Some approaches’ effectiveness was measured with multiple metrics; therefore, numbers do not add up to
ond, the simple baselines used in most evaluations achieve
relatively poor results, so that any alternative easily performs
better. Third, authors do not report failures. Lastly, journals
and conferences might not accept publications that report on
failures. Whatever the reasons are, we advocate that reporting
failures is desirable since it could prevent other researchers
from doing the same experiments, and hence wasting time.
4.5 Ofﬂine evaluation metrics
Precision was used as an evaluation metric in 38 ofﬂine evaluations (72%) (Table 5). Recall was used in 12 evaluations
(23%); F-measure in 6 evaluations (11%); nDCG in 11 evaluations (21%); MRR in 10 evaluations (19%); and other
measures in 12 evaluations (23%). Overall, results of the
different measures highly correlated. That is, an algorithm
that performed well measured by precision tended to perform
well measured by nDCG, for instance. However, there were
exceptions. Zarrinkalam and Kahani tested the effectiveness
of abstract and title against abstract, title, and citation context
 . When co-citation probability was used as an evaluation metric, title and abstract were most effective. Based on
recall, the most effective ﬁeld combination was abstract, title,
and citation context. With the nDCG measure, results varied
depending on how the candidate set was generated and which
ranking approach was used.
4.6 Datasets and architectures
Researchers and developers in the ﬁeld of recommender systems can beneﬁt from publicly available architectures and
datasets.16 Architectures help with the understanding and
building of recommender systems, and are available in various recommendation domains, such as e-commerce ,
marketing , and engineering . Datasets enable the
evaluation of recommender systems by allowing researchers
to evaluate their systems with the same data. Datasets are
available in several recommendation domains, including
movies,17 music,18 and baby names.19 Notable are also the
16 Recommendation frameworks such as LensKit or Mahout may also
be helpful for researchers and developers, but frameworks are not the
topic of this paper.
17 
18 
19 
various TREC datasets that facilitated and standardized evaluations in several domains.20
Architectures of research-paper recommender systems
were published by few authors. The developers of Cite-
Seer(x) published an architecture that focused on crawling
and searching academic PDFs . This architecture
has some relevance for recommender systems, since many
tasks in academic search are related to recommender systems
(e.g., crawling and indexing PDFs, and matching user models or search-queries with research papers). Bollen and van
de Sompel published an architecture that later served as the
foundation for the research-paper recommender system bX
 . This architecture focuses on recording, processing, and
exchanging scholarly usage data. The developers of BibTiP
 also published an architecture that is similar to the architecture of bX (both bX and BibTip exploit usage data to
generate recommendations).
Several academic services published datasets that eased
the process of researching and developing research-paper
recommender systems. CiteULike21 and Bibsonomy22 published datasets containing the social tags that their users
added to research articles. The datasets were not originally intended for recommender-system research but are
frequently used for this purpose . CiteSeer made
its corpus of research papers public,23 as well as the citation
graph of the articles, data for author name disambiguation,
and the co-author network . CiteSeer’s dataset has been
frequently used by researchers for evaluating research-paper
recommender systems .
Jack et al. compiled a dataset based on the reference management software Mendeley . The dataset includes 50,000
randomly selected personal libraries from 1.5million users.
These 50,000 libraries contain 4.4million articles of which
3.6million are unique. Due to privacy concerns, Jack et al.
publish only the unique IDs of articles and no title or author
names. Additionally, only those libraries with at least 20
articles were included in the dataset. Sugiyama and Kan
released two small datasets,24 which they created for their
academic recommender system . The datasets include
20 
21 
22 
23 
24 
Table 6 Source of datasets for reviewed recommendation approaches
that performed ofﬂine evaluations
some research papers, and the interests of 50 researchers.
The CORE project released a dataset25 with enriched metadata and full-texts of academic articles that could be helpful
in building a recommendation candidate corpus.
Of the 53 reviewed ofﬂine evaluations, 17 (32%) were
evaluated using data from CiteSeer and 6 (11%) were evaluated using data from CiteULike (Table 6). Other data sources
included ACM (9%), DBLP (8%), and a variety of others, often not publicly available datasets (51%). Even when
data originated from the same sources, it did not guarantee that the same datasets were used. For instance, 32%
of the approaches were evaluated with data from CiteSeer
but no single CiteSeer dataset exists. Authors collected Cite-
Seer data at different times and pruned datasets differently.
Some authors removed documents with fewer than two citations from the CiteSeer corpus , others with fewer than
three citations , and others with fewer than four citations . Other datasets were pruned even more heavily.
Caragea et al. removed papers with fewer than ten and more
than 100 citations, as well as papers citing fewer than 15 and
more than 50 papers . From 1.3million papers in the corpus, around 16,000 remained (1.2%). Pennock et al. removed
documents from the corpus with fewer than 15 implicit ratings : from originally 270,000 papers, 1575 remained
(0.58%). It is, therefore, safe to say that no two studies,
performed by different authors, used the same dataset. This
raises the question to what extent results based on different
datasets are comparable.
Naturally, recommendation approaches perform differently on different datasets . This is particularly true for the absolute effectiveness of recommendation
approaches. For instance, an algorithm that achieved a recall
of 4% on an IEEE dataset achieved a recall of 12% on
an ACM dataset . The relative effectiveness of two
approaches is also not necessarily the same with different
datasets. For instance, because approach A is more effective than approach B on dataset I, it does not mean that A
is also more effective than B on dataset II. However, among
the few reviewed approaches that were evaluated on different
datasets, the effectiveness was surprisingly consistent.
Of the evaluated approaches, seven were evaluated on
multiple ofﬂine datasets. Dataset combinations included
CiteSeer and some blogs , CiteSeer and Web-kd ,
25 
Table 7 MRR on different datasets used for ofﬂine evaluations
CiteSeer and CiteULike , CiteSeer and Eachmovie ,
and IEEE, ACM and ScienceDirect . Results differed
notably among the different datasets only in one study. However, the absolute ranking of the approaches remained the
same (Table 7). In that article, the proposed approach
(CTM) performed best on two datasets (CiteULike and Cite-
Seer), with a MRR of 0.529 and 0.467, respectively. Three
of the four baselines performed similarly on the CiteSeer
dataset (all with a MRR between 0.238 and 0.288). However, for the CiteULike dataset, the TM approach performed
four times as well as CRM. Consequently, if TM had been
compared with CRM, rankings would have been similar on
the CiteSeer dataset but different on the CiteULike dataset.
Overall, a sample size of seven is small, but it gives at
least some indication that the impact of the chosen dataset is
rather low in the domain of research-paper recommender systems. This ﬁnding is interesting because in other ﬁelds it has
been observed that different datasets lead to different results
 . Nevertheless, we doubt that pruning datasets drastically should be considered good practice, especially if just
a fraction of the original data remains.
4.7 Reproducibility and the butterﬂy effect
The reproducibility of experimental results is the “fundamental assumption” in science , and the “cornerstone” that
allows drawing meaningful conclusions about the generalizability of ideas . Reproducibility describes the situation
when (slightly) different ideas, scenarios, and evaluations
lead to similar experimental results , where we deﬁne
“similar results” as results that allow the same conclusions
to be drawn. Reproducibility should not be confused with
replicability. Replicability describes an exact copy of an
experiment that uses the same tools, follows the same steps,
and produces the same results . Therefore, replicability
is important when analyzing whether the original experiment
was conducted thoroughly and whether the results can be
Conversely, if changes in the ideas, scenarios, or evaluations cause dissimilar results, i.e., results that do not
allow the same conclusions to be drawn, we speak of
non-reproducibility. Non-reproducibility is expected when
signiﬁcant changes are made to the ideas, scenarios, or evaluations. However, if minor changes are made but results are
unexpectedly dissimilar, then we speak of what we term the
“butterﬂy effect”.
During the review, we found several examples of this butterﬂy effect, i.e., variations in experimental results that we
considered unexpected and non-reproducible. For instance,
the developers of the recommender system bx report that
the effectiveness of their recommender system varied by a
factor of three at different institutions, although the same
recommendation approach was used . Lu et al. reported
that the translation model had twice the accuracy of the
language model , but in another evaluation, accuracy
was only 18% higher . Huang et al. report that the
Context-aware Relevance Model (CRM) and cite-LDA performed similarly, but in another evaluation by the same
authors, CRM performed signiﬁcantly worse than cite-LDA
 . Lu et al. found that sometimes terms from the abstract
performed better than terms from the body-text, while sometimes the opposite was true . Zarrinkalam and Kahani
found that sometimes terms from the title and abstract were
most effective, while sometimes terms from the title, abstract,
and citation context were most effective . Bethard and
Jurafsky reported that citation counts strongly increased the
effectiveness of their recommendation approach , while
He et al. reported that citation counts slightly increased the
effectiveness of their approach .
Most interesting with respect to the butterﬂy effect, there
were some evaluations by the TechLens team (Table 8). The
TechLens team evaluated several content-based (CBF) and
collaborative ﬁltering (CF) approaches for research-paper
recommendations.In2002,McNeeetal.conductedanofﬂine
evaluation in which CF and CBF performed similarly .
However,theiradditionaluserstudyledtoadifferentresult—
CBF outperformed CF. A user study by Torres et al. in 2004
reports results similar to the user study by McNee et al.
(CBF outperformed CF) . However, the ofﬂine evaluation from Torres et al. contradicted the previous results—this
time, CF outperformed CBF. In 2006, another user study by
McNee et al. indicated that CF (slightly) outperforms CBF
 , which showed the opposite of the previous user studies.
In 2009, Dong et al., who are not afﬁliated with TechLens,
evaluated the approaches of Torres et al. with an ofﬂine
evaluation . In this evaluation, CBF outperformed CF,
contradicting the previous ofﬂine results from Torres et al. In
2010, Ekstrand et al. found that CBF performed worse than
CF in both an ofﬂine evaluation and a user study, which again
did not align with the previous ﬁndings .
The authors of the studies provide some potential reasons
for the variations, such as different datasets (as discussed
in Sect. 4.6) differences in user populations, and variations
in the implementations. However, these reasons can only
explain some of the variations. Overall, we consider most
of the different outcomes to be unexpected. We view this
as a problem, since we see the primary purpose of evaluations in aiding developers and researchers to identify the most
effective recommendation approaches (for a given scenario).
Consequently, a developer looking for an effective recommendation approach, or a researcher needing an appropriate
baseline to compare a novel approach against, would not ﬁnd
much guidance in the existing evaluations. Similarly, the currently existing evaluations do not help to identify whether CF
or CBF is more promising for research-paper recommender
Interestingly, reproducibility is widely ignored by the
(research-paper) recommender-system community, even by
researchers focusing on recommender-systems evaluation.
For instance, Al-Maskari et al. analyzed how well classic
IR evaluation metrics correlated with user satisfaction in
recommender systems . Gunawardana and Shani published a survey about accuracy metrics . Herlocker
et al. wrote an article on how to evaluate collaborative
ﬁltering approaches . Various authors showed that
ofﬂine and online evaluations often provide contradictory
results . Many papers about various aspects
of recommender-system evaluation have been published
 . However, while many of
the ﬁndings in these papers are important with respect to
reproducibility, none of the authors mentioned or discussed
their ﬁndings in the context of reproducibility.
The neglect of reproducibility in recommender-systems
evaluationisalsoobservedbyEkstrandetal.andKonstanand
Adomavicius.Theystatethat“itiscurrentlydifﬁculttoreproduce and extend recommender systems research results,”
evaluations are “not handled consistently” , and many
research papers “contribute little to collective knowledge,”
primarily due to non-reproducibility of the results .
They concluded:
Table 8 Results of different CBF and CF evaluations
McNee et al. 
Torres et al. 
McNee et al. 
Dong et al. 
Ekstrand et al. 
“[T]he Recommender Systems research community is
facing a crisis where a signiﬁcant number of papers
present results that contribute little to collective knowledge […] often because the research lacks the […]
evaluation to be properly judged and, hence, to provide meaningful contributions”.
Not all researchers agree that the primary purpose of evaluations is to aid developers and researchers identify the most
effective recommendation approaches. When we submitted
a paper on reproducibility to the ACM RecSys conference
observingthattheevaluationswerelargelynon-reproducible,
one reviewer commented:
“I think that it is widely agreed in the community that
this [non-reproducibility] is just the way things are –
if you want a recsys for a speciﬁc application, there is
no better way than just test and optimize a number of
alternatives. This probably cannot be avoided – there
will never be a sufﬁcient set of experiments that would
allow “practitioners” to make decisions without running through this optimization process for their speciﬁc
app and dataset”.
When it comes to the lack of reproducibility, we hope that
the research community can move beyond the observation
that “this is just the way things are”, given that reproducibility is a “bedrock principle in the conduct and validation
of experimental science” . The view of the reviewer
above also leads to the question: How should the “number of [promising] alternatives” be determined? At least for
research-paper recommender systems, there is no small number of promising alternatives; there are only all alternatives,
because nearly all the evaluated approaches were most effective in at least one evaluation. Practitioners could hardly
implement all approaches to ﬁnd the most effective approach
for their scenario. Even if a few promising approaches were
identiﬁed, how should they be optimized? There is no list
of parameters that might be worth optimizing, and even if
there were, there would probably be dozens of parameters,
each with dozens or even hundreds of possible values, that
wouldrequiretesting.Again,thiswouldhardlybefeasiblefor
someone who wanted to implement a recommender system.
In addition, datasets and “speciﬁc features” of recommender
systems change over time. What does this mean for the operation of a recommender system? Would the operator have
to reevaluate the “number of alternatives” every time documents are added to the recommendation corpus, or whenever
minor features of the recommender system were changed?
5 Survey of the recommendation classes
Aside from collaborative and content-based ﬁltering, which
were brieﬂy introduced in Sect. 2, there are feature-based,
knowledge-based, behavior-based, citation-based, contextbased, ruse-based, and many more recommendation classes
 . We consider the following seven
classes to be most appropriate for distinguishing the
approaches in the ﬁeld of research-paper recommender systems:
1. Stereotyping
2. Content-based Filtering
3. Collaborative Filtering
4. Co-Occurrence
5. Graph-based
6. Global Relevance
Originally, we planned to review the most promising
approach or approaches of each recommendation class.
However, as the review of the evaluations showed, most
approaches were evaluated in ways making them nearly
impossible to compare. Therefore, the most promising
approaches could not be determined. Instead, we provide an
overview of the most important aspects and techniques that
have been used in the ﬁeld. The analysis is based on the “indepth” dataset, i.e., the 127 articles on 62 recommendation
approaches that we classiﬁed as signiﬁcant.
5.1 Stereotyping
Stereotyping is one of the earliest user modeling and recommendation classes. It was introduced by Rich in the
recommender system Grundy, which recommended novels
to its users . Rich was inspired by stereotypes from psychology that allowed psychologists to quickly judge people
based on a few characteristics. Rich deﬁned stereotypes—
which she called “facets”—as collections of characteristics.
For instance, Grundy assumed that male users have “a fairly
high tolerance for violence and suffering, as well as a preference for thrill, suspense, fast plots, and a negative interest in
romance” . Consequently, Grundy recommended books
that had been manually classiﬁed to match the facets.
One major problem with stereotypes is that they can
pigeonhole users. While many men may have a negative
interest in romance, this is not true for all men. In addition, building stereotypes is often labor intensive, since the
items typically need to be manually classiﬁed for each facet.
This limits the number of items, for example, books that can
reasonably be personalized .
Advocates of stereotype approaches argue that once the
stereotypes are created the recommender system needs little
computing power and may perform quite well in practice. For
instance, Weber and Castillo observed that female users were
usually searching for the composer Richard Wagner when
they entered the search query ‘Wagner’ on Yahoo! . In
contrast, male users entering the same query were usually
looking for the Wagner paint sprayer. Weber and Castillo
modiﬁed the search algorithm to show the Wikipedia page
for Richard Wagner to female users, and the homepage of
the Wagner paint sprayer company to male users searching
for ‘Wagner.’ As a result, user satisfaction increased. Similarly, the travel agency Orbitz observed that Macintosh users
were “40% more likely to book a four- or ﬁve-star hotel
than PC users” and when booking the same hotel, Macintosh
users booked the more expensive rooms . Consequently,
Orbitz assigned their website visitors to either the “Mac
User” or “PC user” stereotype, and Mac users received recommendations for pricier hotels than PC users. All parties
beneﬁted—users received more relevant search results, and
Orbitz received higher commissions.
In the domain of research-paper recommender systems,
only Beel et al. applied stereotypes . The authors
assume that all users of their reference-management software Docear are researchers or students. Hence, papers and
books are recommended that are potentially interesting for
researchers and students (for example, papers about optimizing scholarly literature for Google Scholar ). Beel et al.
used stereotypes as a fallback model when other recommendation approaches could not deliver recommendations. They
report mediocre performance of the stereotype approach with
click-through rates (CTR) around 4%, while their contentbased ﬁltering approaches achieved CTRs over 6%.
5.2 Content-based ﬁltering
Content-based ﬁltering (CBF) is one of the most widely
used and researched recommendation class . One central component of CBF is the user modeling process, in
which the interests of users are inferred from the items
that users interacted with. “Items” are usually textual, for
instance emails or webpages . “Interaction” is
typically established through actions, such as downloading,
buying, authoring, or tagging an item. Items are represented
by a content model containing the items’ features. Features
are typically word-based, i.e., single words, phrases, or ngrams. Some recommender systems also use non-textual
features, such as writing style , layout information
 , and XML tags . Typically, only the most
descriptive features are used to model an item and users and
these features are commonly weighted. Once the most discriminative features are identiﬁed, they are stored, often as a
vector that contains the features and their weights. The user
model typically consists of the features of a user’s items. To
generate recommendations, the user model and recommendation candidates are compared, for example using the vector
space model and the cosine similarity coefﬁcient.
In the research-paper recommender-system community,
CBF is the predominant recommendation class: of the 62
reviewed approaches, 34 (55%) applied the idea of CBF . “Interaction” between users and items was typically
established through authorship , having papers in
one’s personal collection , adding social tags ,
or downloading , reading , and browsing papers
 .
Most of the reviewed approaches use plain words as features, although some use n-grams , topics (words
and word combinations that occurred as social tags on
CiteULike) , and concepts that were inferred from the
Anthology Reference Corpus (ACL ARC) via Latent Dirichlet Allocation , and assigned to papers through machine
learning. A few approaches utilize non-textual features, and
if they did then these non-textual features were typically utilized in addition to words. Giles et al. used citations in the
same way as words were used and weighted the citations with
the standard TF-IDF measure (they called this method CC-
IDF) . Others adopted the idea of CC-IDF or used it as
a baseline . However, Beel recently provided some
initial evidence that CC-IDF might not be an ideal weighting
scheme .ZarrinkalamandKahaniconsideredauthorsas
features and determined similarities by the number of authors
two items share .
The approaches extracted words from the title , abstract , header , introduction ,
foreword , author-provided keywords , and
bibliography , as well as from the papers’ body text
 . The approaches further extracted words from
external sources, such as social tags , ACM classiﬁcation tree and DMOZ categories , and citation context
 . Using citation context is similar to the way search
engines use anchor analysis for webpage indexing since the
1990s . Citation context analysis was also used in
academic search before it was used by research-paper recommender systems .
Words from different document ﬁelds have different discriminative powers . For instance, a word occurring in
the title is usually more meaningful than a word occurring
in the body text. Nascimento et al. accounted for this and
weighted terms from the title three times stronger than terms
from the text body, and text from the abstract twice as strong
 . This weighting scheme was arbitrarily selected and
not based on empirical evidence. Huang et al. experimented
withdifferentweightsforpapers’contentandcitationcontext
 . They found that an equal weight for both ﬁelds achieved
the highest precision. The other reviewed approaches that
used text from different ﬁelds did not report on any ﬁeld
weighting.
The most popular model to store item representations and
user models was the Vector Space Model (VSM), which was
used by 9 (64%) of those 14 approaches that reported how
they stored the user and item models. Other approaches modeled their users as a graph , as a list of topics
that were assigned through machine learning , or as an
ACM hierarchy . Of those who used the VSM, all but
one used the cosine measure to calculate similarities between
user models and recommendation candidates. In 1998, Giles
et al. compared headers of documents with a string distance
measure , but neither they nor others mentioned this technique again, which might imply that string edit distance was
not effective.
TF-IDF was the most popular weighting scheme (70%)
among those approaches for which the scheme was speciﬁed.
Other weighting schemes included plain term frequency (TF)
 , and techniques that the authors called “phrase
depth” and “life span” .
CBF has a number of advantages compared to stereotypes.
CBF allows a user-based personalization so that the recommender system can determine the best recommendations for
each user individually, rather than being limited by stereotypes. CBF also requires less up-front classiﬁcation work,
since user models can be created automatically.
On the downside, content-based ﬁltering requires more
computing power than stereotyping. Each item must be analyzedforitsfeatures,usermodelsmustbebuilt,andsimilarity
calculations must be performed. If there are many users and
many items, these calculations require signiﬁcant resources.
The weakness of content-based ﬁltering is its low serendipity and overspecialization leading it to recommend items as
similar as possible to the ones a user already knows .
Content-based ﬁltering also ignores quality and popularity
of items . For instance, two research papers may be considered equally relevant by a CBF recommender system if
the papers share the same terms with the user model. This
relevance might not always be justiﬁed, for example if one
paper was written by an authority in the ﬁeld and presents
original results, while another paper was written by a student who paraphrases the results of other research papers.
Ideally, a recommender system should recommend only the
ﬁrst paper but a CBF system would fail to do so. Another
criticism of content-based ﬁltering is that it is dependent on
access to the item’s features . For research-paper recommendations, usually PDFs must be processed and converted
to text, document ﬁelds must be identiﬁed, and features, such
as terms must be extracted. None of these tasks are trivial and they may introduce errors into the recommendations
 .
5.3 Collaborative ﬁltering
The term “collaborative ﬁltering” (CF) was coined in 1992
by Goldberg et al., who proposed that “information ﬁltering can be more effective when humans are involved in the
ﬁltering process” . The concept of collaborative ﬁltering as it is understood today was introduced 2years later by
Resnick et al. . Their theory was that users like what
like-minded users like, where two users were considered
like-minded when they rated items alike. When like-minded
users were identiﬁed, items that one user rated positively
were recommended to the other user, and vice versa. Compared to CBF, CF offers three advantages. First, CF is content
independent, i.e., no error-prone item processing is required
 . Second, because humans do the ratings, CF
takes into account real quality assessments . Finally,
CF is supposed to provide serendipitous recommendations
because recommendations are not based on item similarity
but on user similarity .
From the reviewed approaches, only 11 (18%) applied
collaborative ﬁltering . Yang et al. intended to
let users rate research papers, but users were “too lazy to
provide ratings” . Naak et al. faced the same problem
and created artiﬁcial ratings for their evaluation . This
illustrates one of the main problems of CF: CF requires user
participation, but often the motivation to participate is low.
This problem is referred to as the “cold-start” problem, which
may occur in three situations : new users, new items,
and new communities or disciplines. If a new user rates few
or no items, the system cannot ﬁnd like-minded users and,
therefore, cannot provide recommendations. If an item is new
in the system and has not been rated yet by at least one user,
it cannot be recommended. In a new community, no users
have rated items, so no recommendations can be made and
as a result, the incentive for users to rate items is low.
To overcome the cold-start problem, implicit ratings may
be inferred from the interactions between users and items.
Yang et al. inferred implicit ratings from the number of pages
the users read: the more pages users read, the more the users
were assumed to like the documents . Pennock et al.
interpreted interactions, such as downloading a paper, adding
it to ones’ proﬁle, editing paper details, and viewing its bibliography as positive votes . McNee et al. assumed that
an author’s citations indicate a positive vote for a paper .
They postulated that when two authors cite the same papers,
they are like-minded. Similar, if a user reads or cites a paper
the citations of the cited paper are supposed to be liked by
Using inferred ratings voids CF’s advantage of being
based on real user quality assessments. This criticism applies
to citations as well as to other types of implicit ratings . For example, we cite papers in this survey that had
inadequate evaluations, or were written in barely understandable English. Thus, interpreting citations always as a positive
vote can be misguiding. Similarly, when a user spends a lot of
time reading a paper this could mean that the paper contains
interesting information that the user would rate positively,
but it could also mean that the paper is difﬁcult to understand
and requires a lot of effort to read. Consequently, CF’s advantage of explicit human quality assessments mostly vanishes
when implicit ratings are used.
Using citations as inferred ratings might also void CF’s
second advantage of being content-independent. Typically,
reliable citation data are not widely available. Therefore,
access to the paper’s content is required to build a citation
network, but this process is even more fault-prone than word
extraction in CBF. In CBF, the text of the papers must be
extracted, and maybe ﬁelds such as the title or abstract must
be identiﬁed. For citation-based CF, the text must also be
extracted but in the text, the bibliography and its individual
references must be identiﬁed, including their various ﬁelds
including title and author. This is an error-prone task .
A general problem of collaborative ﬁltering in the domain
of research-paper recommender systems is sparsity. Vellino
compared the implicit ratings on Mendeley (research papers)
and Netﬂix (movies), and found that sparsity on Netﬂix was
three orders of magnitude lower than on Mendeley .
This is caused by the different ratio of users and items. In
domains like movie recommendations, there are typically
few items and many users. For instance, the movie recommender MovieLens has 65,000 users and 5,000 movies .
Typically, many users watched the same movies. Therefore,
like-minded users can be found for most users and recommendations can be given effectively. Similarly, most movies
have been watched by at least some users and hence most
movies can be recommended. The situation is different in
the domain of research papers. There are typically few users
but millions of papers, and very few users have rated the
same papers. Finding like-minded users is often not possible. In addition, many papers are not rated by any users and,
therefore, cannot be recommended.
There are further critiques of CF. Computing time for CF
tends to be higher than for content-based ﬁltering . Collaborative ﬁltering is generally less scalable and requires
more ofﬂine data processing than CBF . Torres et al.
note that collaborative ﬁltering creates similar users 
and Sundar et al. observe that collaborative ﬁltering dictates
opinions . Lops makes the criticism that collaborative
ﬁltering systems are black boxes that cannot explain why
an item is recommended except that other users liked it
 . Manipulation is also a problem: since collaborative
ﬁltering is based on user opinions, blackguards might try
to manipulate ratings to promote their products so they are
recommended more often .
5.4 Co-occurrence recommendations
Togiveco-occurrencerecommendations,thoseitemsarerecommended that frequently co-occur with some source items.
One of the ﬁrst applications of co-occurrence was co-citation
analysis introduced by Small . Small proposed that two
papers are more related to each other, the more often they are
co-cited. Many others adopted this concept, the most popular example being Amazon’s “Customers Who Bought This
Item Also Bought….” Amazon analyzes which items are frequently bought together, and when a customer browses a
product, items frequently bought with that item are recommended.
One advantage of co-occurrence recommendations is
the focus on relatedness instead of similarity. Similarity
expresses how many features two items have in common.
Recommending similar items, as CBF is doing, is often
not ideal because similar items are not serendipitous .
In contrast, relatedness expresses how closely coupled two
items are, not necessarily dependent on their features. For
instance, two papers sharing the same features (words) are
similar. In contrast, paper and pen are not similar but related,
because both are required for writing letters. Hence, cooccurrence recommendations provide more serendipitous
recommendations and, in this way, are comparable to collaborative ﬁltering. In addition, no access to content is needed
and complexity is rather low. It is also rather easy to generate anonymous recommendations, and hence to assure users’
privacy. On the downside, recommendations are not highly
personalized and items can only be recommended if they
co-occur at least once with another item.
approaches
co-occurrences (10%). Three of those approaches analyze
how often papers are co-viewed during a browsing session
 . Whenever a user views a paper, those papers that
were frequently co-viewed with the browsed paper are recommended. Another approach uses proximity of co-citations
to calculate document relatedness : the closer the proximity of two references within a paper, the more related the
cited papers are assumed to be. Pohl et al. compared the
effectiveness of co-citations and co-downloads and found
that co-downloads are more effective than co-citations only
in the ﬁrst 2years after a paper is published .
Calculatingco-occurrencerecommendationsisnotalways
feasible. For instance, on arXiv.org, two-thirds of all papers
have no co-citations, and those that do usually have no more
than one or two . Despite its limitations, co-occurrence
recommendations seem to perform quite well. Two popular
research-paper recommender systems, bX and BibTip, both
rely on co-occurrence recommendations and deliver millions
of recommendations every month .
5.5 Graph based
Ten of the reviewed approaches utilize the inherent connections that exist in academia (16%). Based on these
connections, the approaches build graph networks that typically show how papers are connected through citations
 . Sometimes, graphs include authors ,
users/customers , venues , genes and proteins , and the years the papers were published .
Lao et al. even included terms from the papers’ titles in
the graph, which makes their approach a mixture of the
graph and content based approach . Depending on the
entities in the graph, connections can be citations , purchases , “published in” relations, ,
authorship , relatedness between genes26 , or
occurrences of genes in papers . Some authors connected entities based on non-inherent relations. For instance,
Huang et al. and Woodruff et al. calculated text similarities between items and used the text similarity to connect
papers in the graph . Other connections were based
on attribute similarity,27 bibliographic coupling, co-citation
strength , or demographic similarity . Once
a graph was built, graph metrics were used to ﬁnd recommendation candidates. Typically, one or several input papers
were given and from this input random walks with restarts
were conducted to ﬁnd the most popular items in the graph
 .
5.6 Global relevance
In its simplest form, a recommender system adopts a one-
ﬁts-all approach and recommends items that have the highest
global relevance. In this case, the relevance is not calculated
speciﬁc to a user. Instead, some global measures are used,
such as overall popularity. For instance, a movie-rental system could recommend those movies that were most often
rented or that had the highest average rating over all users. In
this case, the basic assumption would be that users like what
most other users like.
From the reviewed approaches, none used global relevance exclusively but many used it as an additional ranking
factor. For instance, ﬁve CBF approaches used global popularity metrics in their rankings . They ﬁrst
determined a list of recommendation candidates with a
user-speciﬁc CBF approach. Then, the recommendation candidates were re-ranked based on the global relevance metrics.
Popular metrics were PageRank , HITS , Katz metric , citation counts , venues’ citation counts
 , citation counts of the authors’ afﬁliations ,
authors’ citation count , h-index , recency of
articles , title length , number of co-authors ,
number of afﬁliations , and venue type .
Strohman et al. report that the Katz metric, which quanti-
ﬁes relevance as a function of the paths between two nodes
(the shorter the paths the higher the relevance), strongly
improved precision . All variations that included Katz
were about twice as good as those variations without. Bethard
and Jurafsky report that a simple citation count was the most
Relatedness between genes was retrieved from an external data
source that maintained information about gene relatedness.
Attribute similarity was calculated, e.g., based on the number of
important factor, and age (recency) and h-index were even
counterproductive . They also report that considering
these simple metrics doubled mean average precision compared to a standard content-based ﬁltering approach.
5.7 Hybrid recommendation approaches
Approaches of the previously introduced recommendation
classes may be combined in hybrid approaches. Many of the
reviewed approaches have some hybrid characteristics. For
instance, several of the CBF approaches use global relevance
attributes to rank the candidates, or graph methods are used to
extend or restrict potential recommendation candidates. This
type of hybrid recommendation technique is called “feature
augmentation” . It is a weak form of hybrid recommendation technique, since the primary technique is still
dominant. In true hybrids, the combined concepts are similarly important . From the reviewed approaches,
only some of the TechLens approaches may be considered
true hybrid approaches.
TechLens is one of the most inﬂuential research-paper recommender systems, although it was
not the ﬁrst like some have claimed (e.g., ). TechLens
was developed by the GroupLens28 team. Currently Tech-
Lens is not publicly available, although the GroupLens team
is still active in the development and research of recommender systems in other ﬁelds. Between 2002 and 2010,
Konstan, Riedel, McNee, Torres, and several others published six articles related to research-paper recommender
systems. Often, McNee et al.’s article from 2002 is considered to be the original TechLens article . However,
the 2002 article ‘only’ introduced some algorithms for recommending citations, which severed as a foundation for
TechLens, which was introduced in 2004 by Torres et
al. . Two articles about TechLens followed in 2005
and 2007 but added nothing new with respect to recommendations . In 2006, McNee et al. analyzed potential
pitfalls of recommender systems . In 2010, Ekstrand et
al. published another article on the approaches of TechLens
and suggested enhancements for the approaches .
TechLens’ algorithms were adopted from Burke and
consisted of three CBF variations, two CF variations, and ﬁve
hybrid approaches.
Content-Based Filtering: Pure-CBF served as a baseline in the form of standard CBF in which a term-based
user model was compared with the recommendation
candidates. In the case of TechLens, terms from a single input paper were used. In CBF-Separated, for each
paper being cited by the input paper, similar papers are
determined separately and at the end the different recommendationlistsaremergedandpresentedtotheuser.
28 
In CBF-Combined, terms of the input paper and terms
of all papers being cited by the input paper are combined in the user model. Then, the papers most similar
to this user model are recommended.
Collaborative Filtering: Pure-CF served as another
baseline and represented the collaborative ﬁltering
approach from McNee et al., in which papers were
interpreted as users and citations were interpreted as
votes . In Denser-CF, citations of the input paper
were additionally included in the user model.
Hybrid: With Pure-CF->CBF Separated, recommendations were ﬁrst created with Pure-CF. These recommendations were then used as input documents
for CBF-Separated. Similarly, Pure-CF->CBF Combined,
Separated->
Combined->Pure-CF were used to generate recommendations. Fusion created recommendations with
both CBF and CF independently and then merged the
recommendation lists.
Despite various evaluations of the approaches, it remains
unclear which are most promising (refer to the explanations
in Sect. 4.7).
6 Survey of the research ﬁeld and shortcomings
Our surveyalreadyrevealedthat therearesomeshortcomings
and challenges in the ﬁeld of research-paper recommender
systems. This is especially the case when it comes to evaluations, which are often non-reproducible and incomparable.
However, during the review, we identiﬁed more challenges.
In the next sections, we introduce these challenges, and hope
to stimulate a discussion about future research directions to
enhance research-paper recommender systems.
6.1 Neglect of user modeling
A fundamental part of generating recommendations is the
user modeling process that identiﬁes a user’s information
needs . Ideally, a recommender system identiﬁes the
needs automatically by inferring the needs from the user’s
item interactions. Alternatively, the recommender system
asks users to specify their needs by providing a list of keywords or through some other method. However, in this case
a recommender system becomes very much like a search
engine and loses one of its main features, namely the capability to recommend items even if users do not know exactly
what they need.
Ofthe62reviewedapproaches,50(81%)requiredusersto
either explicitly provide keywords , or to provide
text snippets (e.g., an abstract) , or to provide a
single paper as input , or the approaches ignored
the user modeling process entirely. These approaches neglect
one of the most important parts of a recommender system,
which makes the approaches very similar to classic search,
or related document search , where users provide
search terms or an input paper, and receive a list of search
results or similar papers. Of course, neither classic search
nor related-document search are trivial tasks in themselves,
but they neglect the user modeling process and we see little
reason to label such systems as recommender systems.
Of the reviewed approaches, 12 (19%) inferred information from the items the users interacted with. However, most
approaches that inferred information automatically used all
papers that a user authored, downloaded, etc. .
This is not ideal. When inferring information automatically,
a recommender system should determine the items that are
currently relevant for the user-modeling process . For
example, 10-year-old user-authored papers are probably not
suitable to describe a user’s current information needs. This
aspect is called “concept drift” and it is important for creating
meaningful user models. In the research-paper recommender
systems community, concept drift is widely ignored: a mere
three approaches considered concept drift in detail. Middleton et al. weight papers by the number of days since the
user last accessed them . Watanabe et al. use a similar approach . Sugiyama and Kan, who use the user’s
authored papers, weight each paper based on the difference
between a paper’s publication year, and the year of the most
recently authored paper . In addition, they found that
it makes sense to include only those papers that the user
authored in the past 3years .
Another important question in user modeling is the usermodel size. While in search, user models (i.e., search queries)
typicallyconsistofafewwords;usermodelsinrecommender
systems may consist of hundreds or even thousands of words.
Of the reviewed CBF approaches, 30 (88%) did not report
the user-model size. The few authors who reported the usermodel size, usually stored fewer than 100 terms. For instance,
Giles et al. made us in the top 20 words of the papers .
6.2 Focus on accuracy
The research-paper recommender-system community places
a strong focus on accuracy, and seems to assume that an accurate recommender system will lead to high user satisfaction.
However, outside the research-paper recommender-system
community, it is agreed that many aspects beyond accuracy
affect user satisfaction. For instance, users might become
dissatisﬁed with accurate recommendations when they have
no trust in the recommender system’s operator , their
privacy is not ensured , they need to wait too long for
recommendations , or they ﬁnd the user interfaces unappealing . Other factors that affect user satisfaction are
conﬁdence in a recommender system , data security
 , diversity , user tasks , item’s lifespan 
and novelty , risk of accepting recommendations ,
robustness against spam and fraud , transparency and
explanations , time to ﬁrst recommendation , and
interoperability .
Among the reviewed articles, a few authors considered
aspects beyond accuracy, as shown in the following sections.
6.2.1 Users’ tasks
Torres et al. from TechLens’ considered a user’s current task
in the recommendation process. The authors distinguished
between users who wanted to receive authoritative recommendations and novel recommendations . They showed
that different recommendation approaches were differently
effective for these tasks. The developers of TheAdvisor let
users specify whether they are interested in classic or recent
papers . Uchiyama et al. found that students are typically not interested in ﬁnding papers that are “similar” to
their input paper . This ﬁnding is interesting because
content-based ﬁltering is based on the assumption that user
want similar papers. However, the study by Uchiyama et al.
was based on 16 participants. As such, it remains uncertain
how signiﬁcant the results are.
6.2.2 Diversity
Diversity of recommendations was mentioned in a few articles, but considered in depth only by two author groups
(Vellino et al. and Küçüktunç et al.). Vellino et al. measured
diversity as the number of journals from which articles were
recommended . If recommendations were all from the
same journals, diversity was zero. They compared diversity
of a CF approach with the co-occurrence approach from bX
and found that CF had a diversity of 60% while diversity of
bX was 34%. Küçüktunç et al. from TheAdvisor published
two articles about diversity in research-paper recommender
systems . They provided a survey on diversiﬁcation
techniques in graphs and proposed some new techniques to
measure diversity.
6.2.3 Layout
Farooq et al. from CiteSeer analyzed which information users
wanted to see when receiving recommendations in RSS feeds
 . They found that the information to display varies for
the type of recommendation. In one approach, Farooq et
al. recommended papers that cited the user’s papers. In this
case, users preferred to see the citing paper’s bibliographic
data (e.g., title, author) and the context of the citation—
the sentence in which the citation appeared. When papers
were recommended that were co-cited with the users’ papers,
citation context was not that important. Rather, the users
preferred to see the bibliographic data and abstract of the
co-cited paper. When papers were recommended that had a
similar content to the users’ papers, users preferred to see
bibliographic data and abstract. These ﬁndings are interesting because from the reviewed recommender systems the
majority display only the title and not the abstract.
As part of our work, we researched the impact of labeling
and found that papers labeled as ‘sponsored recommendation’ performed worse than recommendations with a label
that indicated that the recommendations were ‘organic,’
though the recommended papers were identical . It also
made a difference if paper recommendations were labeled as
‘Sponsored’ or ‘Advertisement’ although both labels indicate
that recommendations are displayed for commercial reasons.
6.2.4 User characteristics
For our own recommender system Docear, we found that
researchers who registered tended to have higher clickthrough rates than unregistered users (6.95 vs. 4.97%)
 . In addition, older users seem to have higher average
click-through rates (40–44years: 8.46%) than younger users
(20–24years: 2.73%) . Middleton et al. also report differences for different user groups. Click-through rates in their
recommender system, Quickstep, was around 9%, but only
around 3.5% for Foxtrot, although both systems applied very
similar approaches. However, Quickstep users were recruited
from a computer science laboratory, while Foxtrot was a
real-world system offered to 260 faculty members and students (although only 14% of users used Foxtrot at least three
Click-through rates from the bX recommender are also
interesting . They varied between 3 and 10% depending
on the university in which recommendations were shown (bX
provides more than 1000 institutions with recommendations)
 . This could have been caused by different layouts of the
recommendations, but it might also have been caused by the
different backgrounds of students.
6.2.5 Usage duration
Middleton et al. reported that the longer someone used
the recommender system; the lower click-through rates
became . Jack reports the opposite, namely that precision increased over time (p = 0.025 in the beginning,
p = 0.4 after 6months) and depended on a user’s library
size (p = 0.08 for 20 articles and p = 0.40 for 140 articles)
 . We showed that it might make sense to be “persistent”
and show the same recommendations to the same users multiple times—even recommendations that users had clicked
before were often clicked again .
6.2.6 Recommendation medium
User satisfaction also depends on the medium through which
recommendations are made. Middleton et al. report that recommendations via email received half the click-through rate
as the same recommendations delivered via a website .
Of the reviewed recommender systems, only Docear and
Mendeley provide recommendations through a desktop software; CiteSeer provided recommendations in a news
feed ; and all others deliver their recommendations
through websites. If and how click rates differ when recommendations are delivered via desktop software compared
to a website remains unknown.
6.2.7 Relevance and proﬁle feedback
Relevance feedback is a common technique to improve
recommendations but it is widely ignored in the
research-paper recommender-system community. Middleton
et al. showed that proﬁle feedback is better than relevance
feedback: allowing users to edit their user models is more
effective than just learning from relevance feedback .
Bollacker et al. from CiteSeer allowed their users to edit
their proﬁles but conducted no research on the effectiveness
of this feature .
6.3 Translating research into practice
Translating research into practice is a current challenge in
the research paper recommender system community. Out of
the large number of proposed approaches in the ﬁeld, 24
research-paper recommender systems could be used by users
in practice (Table 9).29 Of these 24 recommender systems,
eight (33%) never left the prototyping stage—and today only
one of the prototypes is still publicly available. Of the remaining recommender systems, four are ofﬂine (25%), ﬁve are no
longer actively maintained (31%),30 while seven are running and actively maintained (44%). Of the seven active
recommender systems, four operators are involved with the
recommender-system research community (see footnote 29)
and publish information about their systems.
Most of the real-world recommender systems apply simple recommendation approaches that are not based on recent
research. For instance, as far as we could tell, PubMed was
still using an approach introduced in 2007; ResearchGate is
using a simple content-based ﬁltering approach similar to
29 The recommender systems of Mendeley, CiteULike, and CiteSeer
are counted twice because they offer or offered two independent recommender systems.
30 We classiﬁed a recommender system as not actively maintained if
no article was published or no changes were made to the system for a
classic search31; CiteULike apparently uses two approaches
from 2008/2009; and BibTip and bX are using simple cooccurrence approaches. Whether the RefSeer system applies
all the results from their research remains unclear. In other
words, the reviewed research typically did not affect realworld recommender systems.
6.4 Persistence and authorities
One reason the research is not transferred directly into practice might be a lack of persistence and authorities in the ﬁeld.
Of the 276 authors who authored the 185 articles, 201 (73%)
published just one article (Fig. 5). 15 authors (5%) published
ﬁve or more articles, but of these authors, several were coauthors publishing the same articles. This means that there
are only a few groups that consistently publish research in
the ﬁeld of research-paper recommender systems.
The most productive authors are C. Lee Giles and his coauthor P. Mitra from CiteSeer/RefSeer (Tables 10, 11). No
other authors have published as many articles (17) over as
long a period of time (16years) on as many different aspects
of research-paper recommender systems. Other productive
authors are A. Geyer-Schulz and his co-authors M. Hahsler,
and M. Jahn from BibTip. They published 13 articles, but
these were less often cited in the community than those of
Giles et al. The articles are also narrower in scope than those
of the CiteSeer authors. Our research group, i.e., J. Beel, S.
Langer, M. Genzmehr, and B. Gipp from Docear, authored
eight papers between 2009 and 2013, including posters or
short papers. The research concentrated on aspects beyond
accuracy, such as the impact of labeling recommendations
and the impact of demographics on click-through rates. O.
Küçüktunç and his co-authors E. Saule and K. Kaya from
TheAdvisor published six articles focusing on diversity and
graph-based recommendations. J. A. Konstan, S. M. McNee,
R. Torres, and J.T. Riedel, who are highly recognized authors
in the ﬁeld of recommender systems, developed TechLens
and authored six articles relating to research-paper recommender systems during 2002 and 2010. Two of their articles
inﬂuenced the work of several others and are among the most
cited articles we reviewed . W. W. Cohen and his
PhD student N. Lao are also two productive authors. They
authored six articles from 2008 to 2012 (some of which are
unpublished). SE. Middleton and his co-authors published
ﬁve articles. It is interesting to note that ﬁve of the six most
productive research groups have access to real-world recommender systems.
ResearchGate also applied other recommender systems, e.g., for
people or news, and it seems that these approaches are more sophisticated.
Table 9 Overview of
recommender systems surveyed
Fig. 5 Papers published per
author in the research paper
recommender field
Related Papers
Scholar Update
Item-Centric
PubMedPRMA
Research Gate
TheAdvisor
Who shoud I Cite?
Related Documents
Sarkanto & Syntbese
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Real system
Presentation
Stand-alone
Stand-alone
Stand-alone
Stand-alone
Stand-alone
Stand-alone
Webpage, Email
Stand-alone
Stand-alone
Stand-alone
Stand-alone
6.5 Cooperation
Most articles were authored by multiple authors: the majority of articles had two (26% ), three (26%) or four authors
(18 %) (Fig. 6).32 17% of articles were authored by a single researcher. On first glance, these numbers indicate a high
32 Median author count was three, maximum count eleven.
Number of published papers
degree of collaboration. However, we noticed that between
different co-author groups little cooperation took place. The
closest cooperation we could identify was that Giles was part
of a committee for a thesis that Cohen supervised . Leading authors of different research groups did not co-author
articles together.
Co-author groups frequently seemed to work alone and did
not always build on the results of the work done by their peers.
Table IO Most productive
authors in the research paper
recommender field
C. Lee Giles
A. Geyer-Schulz
0 . Kiiytiktuny
J. A. Konstan
W. W.Cohen
M. Genzmehr
S. E. Middleton
D. C. De Raure
Table I I Most productive author groups
C. Lee Giles; P. Mitra
(CiteSeer/RefSeer)
A. Geyer-Schulz; M. Hahsler; M.
Jahn (BibTip)
J. Beel; S. Langer, M. Genzmehr,
B. Gipp (Docear)
J. A. Konstan; S.M. McNee; R.
Torres, J. T. Riedl (TechLens)
0. Kiiyiiktun9; E. Saule; K. Kaya
(TheAdvisor)
W. W. Cohen; N. Lao
S. E. Middleton; D. C. De Raure,
N. R. Shadbolt , the authors did not report the weighting
scheme they used (e.g., TF-IDF). The feature representation
model (e.g., vector space model) was not reported for 20
approaches (59%). Whether stop words were removed was
not reported for 23 approaches ( 68 %) . For 16 approaches
(47 %), no information was given on the fields the terms
were extracted from (e.g., title or abstract). This information scarcity means, when an evaluation reports promising
results for an approach, that other researchers would not
know how to re-irnplement the approach in detail. If they
tried, and guessed the specifics of an approach, the outcome
would probably differ significantly from the original. This
might cause problems in replicating evaluations, and reproducing research results and hinders the re-implementation
and application of promising approaches in real-word recommender systems .
7 Summary and outlook
In the 16years from 1998 to 2013, more than 200 research
articles were published in the field of research-paper recommender systems. The articles consisted primarily of peerreviewed conference papers (59%), journal articles (16%),
pre-prints (5 %), and oilier documents such as presentations
and web pages (15 %). The few existing literature surveys
in this field cover only a fraction of these articles, which is
why we conducted a comprehensive survey of research-paper
recommender systems. The review revealed the following
information.
Content-based ﬁltering (CBF) is the predominant recommendation class. From 62 reviewed approaches, 34 used
CBF (55%). From these CBF approaches, the majority utilized plain terms contained in the documents. Some used
n-grams, or topics based on LDA. A few approaches also
utilized non-textual features, such as citations or authors.
The most popular model to store item representations was
the Vector Space Model. Other approaches modeled their
users as graphs, as lists with topics that were assigned
through machine learning, or as ACM classiﬁcation hierarchies. The reviewed approaches extracted text from the title,
abstract, header, introduction, foreword, author-provided
keywords, bibliography, body text, social tags, and citation
Only eleven approaches applied collaborative ﬁltering,
and none of them successfully used explicit ratings. Yang
et al. intended to develop such a system, but their users were
“too lazy to provide ratings” . Hence, implicit instead of
explicit ratings were used. Implicit ratings were inferred from
the number of pages the users read, users’ interaction with
the papers (e.g., downloads, edits, views) and citations. The
main problem of collaborative ﬁltering for research papers
seems to be scarcity. Vellino compared implicit ratings on
Mendeley (research papers) and Netﬂix (movies), and found
that scarcity on Mendeley differs from Netﬂix by a magnitude of three.
Although stereotyping is one of the oldest user modeling approaches, and is successfully applied by services, such
as Yahoo!, only one of the reviewed approaches applied
stereotypes as a fallback model when other recommendation
approaches could not deliver recommendations. The authors
reported reasonable performance of the stereotype approach
with click-through rates (CTR) around 4% while the CBF
approach achieves CTRs around 6%. Given these results,
we believe that a more thorough analysis of stereotype recommendations could be interesting.
Six of the reviewed approaches were co-occurrence based.
Three approaches analyzed how often papers were co-viewed
during a browsing session: whenever users browsed a paper,
the system recommended those papers that had frequently
been co-viewed with the browsed paper in the past. Another
approach used co-citations to calculate document relatedness. The higher the proximity of two references within a
paper, the more related they are assumed to be. Pohl et al.
compared the effectiveness of co-citations and co-downloads
and found that co-downloads are more effective than cocitations only in the ﬁrst 2years after a paper was published
Ten recommendation approaches built graphs to generate recommendations. Such graphs typically included papers
that were connected via citations. Some graphs included
authors, users/customers, venues, genes and proteins, and
publishing years of the papers. Lao et al. even included terms
from the papers’ titles in the graph. Depending on the entities
in the graph, connections were citations, purchases, “published in” relations, authorship, relatedness between genes,
and occurrences of genes in papers. Some authors connected
the entities based on non-inherent relations. For instance,
Huang et al. and Woodruff et al. calculated text similarities
between items and used the text similarity to connect papers
in the graph. Other connections were based on attribute
similarity, bibliographic coupling, co-citation strength, and
demographic similarity.
Despite a lot of research about research-paper recommender systems, we identiﬁed two main problems in the
research ﬁeld.
First, it is currently not possible to determine the most
effective recommendation approaches. If we were asked
which recommendation approach to apply in practice or to
use as baseline, there is no deﬁnite answer. We do not even
have a clue as to which of the approaches might be most
promising. This problem mainly relates to poor experimental design and lack of information, which includes inadequate
evaluations and too little information given by the authors:
22% of the approaches were not evaluated. Of the remaining approaches, 20% were not evaluated against a baseline;
the majority of the remaining approaches were compared to
simple baselines but not to approaches of other researchers
in the ﬁeld. The majority (71%) of approaches were evaluated using ofﬂine evaluations, which are subject to various
shortcomings. Some claim that ofﬂine evaluations should
not be used for evaluating research-paper recommender systems . If that is true, most of the reviewed evaluations
would be of little signiﬁcance. Even if this criticism is unjustiﬁed, some problems remain. Many authors pruned their
ofﬂine datasets in drastic ways. For instance, Pennock et al.
removed all documents with fewer than 15 implicit ratings
from the corpus. Therefore, 1575 papers remained from the
original 270,000 (0.58%). Results based on such datasets
do not allow drawing reliable conclusions about how the
approaches might perform in real-world recommender systems. In addition, the majority of the user studies (58%) had
15 or less participants, which questions the signiﬁcance of
these evaluations. Only 8% of the approaches were evaluated with online evaluations in real-world recommender
systems with real users. Additionally, many authors provided
little information about their approaches, which hinders reimplementation. For instance, most authors did not report
on the text ﬁelds they utilized, or which weighting schemes
were used.
To solve this problem, we believe it is crucial that the
community discusses and develops frameworks and bestpractice guidelines for the evaluation of research-paper
recommender-systems. This should include an analysis and
discussion of how suitable ofﬂine evaluations are; to what
extent datasets should be pruned; the creation of datasets
comparable to existing TREC datasets; the minimum number of participants in user studies; and which factors inﬂuence
the results of evaluations (e.g., user demographics). Ideally, a
set of reference approaches would be implemented that could
be used as baselines. In addition, more details on implementations are needed, based on a discussion of the information
needed in research articles.
It is crucial to ﬁnd out why seemingly minor differences in
algorithms or evaluations lead to major variations in the evaluation results. As long as the reasons for these variations are
not found, scholars cannot rely on existing research results
because it is not clear whether the results can be reproduced
in a new recommendation scenario.
Second, we identiﬁed unused potential in recommender
systems research. This problem has two root causes.
(A) Research results are often not transferred into practice,
or considered by peers. Despite the large number of
research articles, just a handful of active recommender
systems exist, and most of them apply simple recommendation approaches that are not based on recent
research results. As such, the extensive research conducted from 1998 to 2013 apparently had a rather
minor impact on research-paper recommender systems
in the real world. Additionally, developers of several of
the active recommender systems do not engage in the
research community or publish information about their
systems. Some researchers also seem to be unaware of
developments in related research domains, such as user
modeling, scientometrics, and the reviewer-assignment
problem. In addition, the major co-author groups in the
domain of research-paper recommender systems do not
cooperate much with each other. One reason for some of
these problems might be a relatively short-lived interest
in the research ﬁeld. Most authors (73%) published only
a single paper on research-paper recommender systems.
(B) The majority of authors did not take into account that
user satisfaction might depend not only on accuracy but
also on factors such as privacy, data security, diversity,
serendipity, labeling, and presentation. The operator
perspective was widely neglected. Information about
runtime was provided for 10% of the approaches. Complexity was covered by very few authors and the costs of
running a recommender system were reported by a single article. We also observed that many authors neglect
the user-modeling process: 81% of the approaches
made their users provide some keyword, text snippets,
or a single input paper to represent their information
need. Few approaches automatically inferred informationneedsfromtheusers’authored,tagged,orotherwise
connected papers.
One step towards using the full potential of research-paper
recommender systems could be to establish a platform for
researchers to collaborate, work on joint publications, communicate ideas, or to establish conferences or workshops
focusing solely on research-paper recommender systems.
An open-source recommender framework containing the
most promising approaches could help transfer the research
results into practice. Such a framework would also help
new researchers in the ﬁeld access a number of baselines
they could compare their own approaches with. A framework could either be built from scratch, or be based on
existing frameworks such as MyMediaLite,33 LensKit,34
Mahout,35 Duine,36 RecLab Core,37 easyrec,38 or Recommender101.39 Finally, the community could beneﬁt from
considering research results from related disciplines. In
particular, research in the area of user modeling and scientometrics appears promising, as well as research from
the general recommender-systems community about aspects
beyond accuracy.