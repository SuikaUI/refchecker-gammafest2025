JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
Graph Self-Supervised Learning: A Survey
Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, Philip S. Yu, Life Fellow, IEEE
Abstract—Deep learning on graphs has attracted signiﬁcant interests recently. However, most of the works have focused on (semi-)
supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address
these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying
on manual labels, has become a promising and trending learning paradigm for graph data. Different from SSL on other domains like
computer vision and natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under
the umbrella of graph self-supervised learning, we present a timely and comprehensive review of the existing approaches which
employ SSL techniques for graph data. We construct a uniﬁed framework that mathematically formalizes the paradigm of graph SSL.
According to the objectives of pretext tasks, we divide these approaches into four categories: generation-based, auxiliary
property-based, contrast-based, and hybrid approaches. We further describe the applications of graph SSL across various research
ﬁelds and summarize the commonly used datasets, evaluation benchmark, performance comparison and open-source codes of graph
SSL. Finally, we discuss the remaining challenges and potential future directions in this research ﬁeld.
Index Terms—Self-supervised learning, graph analytics, deep learning, graph representation learning, graph neural networks.
INTRODUCTION
N recent years, deep learning on graphs , , , 
has become increasingly popular for the artiﬁcial intelligence research community since graph-structured data is
ubiquitous in numerous domains, including e-commerce
 , trafﬁc , chemistry , and knowledge base . Most
deep learning studies on graphs focus on (semi-) supervised
learning scenarios, where speciﬁc downstream tasks (e.g.,
node classiﬁcation) are exploited to train models with wellannotated manual labels. Despite the success of these studies, the heavy reliance on labels brings several shortcomings.
Firstly, the cost of the collection and annotation of manual
labels is prohibitive, especially for the research areas which
have large-scale datasets (e.g., citation and social networks
 ) or demand on domain knowledge (e.g., chemistry and
medicine ). Secondly, a purely supervised learning scenario usually suffers from poor generalization owing to
the over-ﬁtting problem, particularly when training data is
scarce . Thirdly, supervised graph deep learning models
are vulnerable to label-related adversarial attacks, causing
the weak robustness of graph supervised learning .
To address the shortcomings of (semi-) supervised learning, self-supervised learning (SSL) provides a promising
Y. Liu, M. Jin, and S. Pan are with Department of Data Science &
AI, Faculty of IT, Monash University, VIC 3800, Australia (E-mail:
 ; ; ;).
C. Zhou is with Academy of Mathematics and Systems Science, Chinese
Academy of Sciences, China (Email: ).
Y. Zheng is with the Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia (E-mail:
 ).
Engineering,
Information
Technology
Federation
University,
 ).
P. S. Yu is with Department of Computer Science, University of Illinois
at Chicago, Chicago, IL 60607-7053, USA (Email: )
Corresponding author: Shirui Pan.
Y. Liu and M. Jin contributed equally to this work. This work was supported
in part by an ARC Future Fellowship (FT210100097), NSF under grants
III-1763325, III-1909323, III-2106758, and SaTC-1930941.
learning paradigm that reduces the dependence on manual labels. In SSL, models are learned by solving a series
of handcrafted auxiliary tasks (so-called pretext tasks), in
which the supervision signals are acquired from data itself automatically without the need for manual annotation.
With the help of well-designed pretext tasks, SSL enables
the model to learn more informative representations from
unlabeled data to achieve better performance , ,
generalization , , and robustness , on
various downstream tasks.
Described as “the key to human-level intelligence” by
Turing Award winners Yoshua Bengio and Yann LeCun, SSL
has recently achieved great success in the domains of computer vision (CV) and natural language processing (NLP).
Early SSL methods in CV domain design various semanticsrelated pretext tasks for visual representation learning ,
such as image inpainting , image colorizing , and
jigsaw puzzle , etc. Lately, self-supervised contrastive
learning frameworks (e.g., MoCo , SimCLR and
BYOL ) leverage the invariance of semantics under
image transformation to learn visual features. In the NLP
domain, early word embedding methods , share
the same idea with SSL which learns from data itself. Pretrained by linguistic pretext tasks, recent large-scale language models (e.g., BERT and XLNet ) achieve stateof-the-art performance on multiple NLP tasks.
Following the immense success of SSL on CV and NLP,
very recently, there has been increasing interest in applying
SSL to graph-structured data. However, it is non-trivial to
transfer the pretext tasks designed for CV/NLP for graph
data analytics. The main challenge is that graphs are in irregular non-Euclidean data space. Compared to the 2D/1D
regular-grid Euclidean spaces where image/language data
reside in, non-Euclidean spaces are more general but more
complex. Therefore, some pretext tasks for grid-structure
data cannot be mapped to graph data directly. Furthermore,
the data examples (nodes) in graph data are correlated with
the topological structure naturally, while the examples in
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
(a) SSL pretext tasks
in CV: image colorizing (upper) and image contrastive learning (bottom).
[MASK] is impossible.
Nothing is impossible.
I’m going outside.
I’ll be back soon.
I got up late.
(b) SSL pretext tasks
prediction
(c) SSL pretext tasks in
graph analytics: masked
graph generation (upper)
contrastive
learning (bottom).
Fig. 1: Toy examples of different SSL pretext tasks in CV,
NLP and graph analytics. In generative tasks, graph SSL
should consider the topological structure in an irregular grid
as well as node features, while SSL in CV/NLP just needs to
recover the information in 2D/1D grid space. In contrastive
tasks, the dependency between nodes is non-negligible in
graph SSL, while the samples in CV/NLP are independent.
CV (image) and NLP (text) are often independent. Hence,
how to deal with such dependency in graph SSL becomes
a challenge for pretext task designs. Fig. 1 illustrates such
differences with some toy examples. Considering the significant difference between SSL in graph analytics and other
research areas, exclusive deﬁnitions and taxonomies are
required for graph SSL.
The history of graph SSL goes back to at least the
early studies on unsupervised graph embedding , 
1. These methods learn node representations by maximizing
the agreement between contextual nodes within truncated
random walks. A classical unsupervised learning model,
graph autoencoder (GAE) , can also be regarded as a
graph SSL method that learns to rebuild the graph structure.
Since 2019, the recent wave of graph SSL has brought about
various designs of pretext tasks, from contrastive learning
 , to graph property mining , . Considering
the increasing trend of graph SSL research and the diversity
of related pretext tasks, there is an urgent need to construct a
uniﬁed framework and systematic taxonomy to summarize
the methodologies and applications of graph SSL.
To ﬁll the gap, this paper conducts a comprehensive and
up-to-date overview of the rapidly growing area of graph
SSL, and also provides abundant resources and discussions
of related applications. The intended audiences for this
article are general machine learning researchers who would
like to know about self-supervised learning on graph data,
graph learning researchers who want to keep track of the
most recent advances on graph neural networks (GNNs),
and domain experts who would like to generalize graph
SSL approaches to new applications or other ﬁelds. The core
contributions of this survey are summarized as follows:
Uniﬁed framework and systematic taxonomy. We
propose a uniﬁed framework that mathematically
formalizes graph SSL approaches. Based on our
framework, we systematically categorize the existing
1. A timeline of milestone works are summarized in Appendix A.
works into four groups: generation-based, auxiliary
property-based, contrast-based, and hybrid methods.
We also build the taxonomies of downstream tasks
and SSL learning schemes.
Comprehensive and up-to-date review. We conduct
a comprehensive and timely review for classical and
latest graph SSL approaches. For each type of graph
SSL approach, we provide ﬁne-grained classiﬁcation,
mathematical description, detailed comparison, and
high-level summary.
applications. We collect abundant resources on graph SSL, including
datasets, evaluation benchmark, performance comparison, and open-source codes. We also summarize
the practical applications of graph SSL in various
research ﬁelds.
Outlook on future directions. We point out the
technical limitations of current research. We further
suggest six promising directions for future works
from different perspectives.
Comparison with related survey articles. Some existing surveys mainly review from the perspectives of general SSL
 , SSL for CV , or self-supervised contrastive learning
 , while this paper purely focuses on SSL for graphstructured data. Compared to the recent surveys on graph
self-supervised learning , , our survey has a more
comprehensive overview on this topic and provides the
following differences: (1) a uniﬁed encoder-decoder framework to deﬁne graph SSL; (2) a systematical and more
ﬁne-grained taxonomy from a mathematical perspective;
(3) more up-to-date review; (4) more detailed summary
of resources including performance comparison, datasets,
implementations, and practical applications; and (5) more
forward-looking discussion for challenges and future directions.
The remainder of this article is organized as follows.
Section 2 deﬁnes the related concepts and provides notations used in the remaining sections. Section 3 describes the
framework of graph SSL and provides categorization from
multiple perspectives. Section 4-7 review four categories of
graph SSL approaches respectively. Section 8 summarizes
the useful resources for empirical study of graph SSL,
including performance comparison, datasets, and opensource implementations. Section 9 surveys the real-world
applications in various domains. Section 10 analyzes the
remaining challenges and possible future directions. Section
11 concludes this article in the end.
DEFINITION AND NOTATION
In this section, we outline the related term deﬁnitions of
graph SSL, list commonly used notations, and deﬁne graphrelated concepts.
Term Deﬁnitions
In graph SSL, we provide the following deﬁnitions of related
essential concepts.
Manual Labels Versus Pseudo Labels. Manual labels, a.k.a.
human-annotated labels in some papers , indicate the
labels that human experts or workers manually annotate.
Pseudo labels, in contrast, denote the labels that can be
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
acquired automatically from data by machines without any
human knowledge. In general, pseudo labels require lower
acquisition costs than manual labels so that they have advantages when manual labels are difﬁcult to obtain or the
amount of data is vast. In self-supervised learning settings,
speciﬁc methods can be designed to generate pseudo labels,
enhancing the representation learning.
Downstream Tasks Versus Pretext Tasks. Downstream tasks
are the graph analytic tasks used to evaluate the quality
or performance of the feature representation learned by
different models. Typical applications include node classiﬁcation and graph classiﬁcation. Pretext tasks refer to the predesigned tasks for models to solve (e.g., graph reconstruction), which helps models to learn more generalized representations from unlabeled data, and thus beneﬁts downstream tasks by providing a better initialization or more effective regularization. In general, solving downstream tasks
needs manual labels, while pretext tasks are usually learned
with pseudo labels.
Supervised Learning, Unsupervised Learning and Self-
Supervised Learning. Supervised learning refers to the learning paradigm that leverages well-deﬁned manual labels
to train machine learning models. Conversely, unsupervised
learning refers to the learning paradigm without using any
manual labels. As a subset of unsupervised learning, selfsupervised learning indicates the learning paradigm where
supervision signals are generated from data itself. In selfsupervised learning methods, models are trained with pretext tasks to obtain better performance and generalization
on downstream tasks.
We provide important notations used in this paper (which
are summarized in Appendix B.1) and the deﬁnitions of
different types of graphs and GNNs in this subsection.
Deﬁnition 1 (Plain Graph). A plain graph2 is represented
as G = (V, E), where V = {v1, . . . , vn} (|V| = n) is
the set of nodes and E (|E| = m) is the set of edges,
and naturally we have E ⊆V × V. The neighborhood
of a node vi is denoted as N(vi) = {vj ∈V|ei,j ∈E}.
The topology of the graph is represented as an adjacency
matrix A ∈Rn×n, where Ai,j = 1 means ei,j ∈E, and
Ai,j = 0 means ei,j /∈E.
Deﬁnition 2 (Attributed Graph). An attributed graph refers
to a graph where nodes and/or edges are associated with
their own features (a.k.a attributes). The feature matrices
of nodes and edges are represented as Xnode ∈Rn×dnode
and Xedge ∈Rm×dedge respectively. In a more common
scenario where only nodes have features, we use X ∈
Rn×d to denote the node feature matrix for short, and
denote the attributed graph as G = (V, E, X).
There are also some dynamic graphs and heterogeneous
graphs whose deﬁnitions are given in Appendix B.2.
Most of the reviewed methods leverage GNNs as backbone encoders to transform the input raw node features X
into compact node representations H by leveraging the rich
underlying node connectivity, i.e., adjacency matrix A, with
2. A plain graph is an unattributed, static, and homogeneous graph.
Self-supervised
Generation
Generation
Clusteringbased
Pair Relationbased
Generationbased
Propertybased
Contrastbased
Classification
Regression
Same-Scale
Node-Level
Graph-Level
Cross-Scale
Fig. 2: Categorization of graph SSL methods.
learnable parameters. Furthermore, readout functions R(·)
are often employed to generate a graph-level representation
hG from node-level representations H. The formulation of
GNNs and readout functions are introduced in Appendix
B.3. Besides, in Appendix B.4, we formulate the commonly
used loss functions in this survey.
FRAMEWORK AND CATEGORIZATION
In this section, we provide a uniﬁed framework of graph
SSL, and further categorize it from different perspectives,
including pretext tasks, downstream tasks, and the combination of both (i.e., self-supervised training schemes).
Uniﬁed Framework and Mathematical Formulation
of Graph Self-Supervised Learning
We construct an encoder-decoder framework to formalize
graph SSL. The encoder fθ (parameterized by θ) aims to
learn a low-dimensional representation (a.k.a. embedding)
hi ∈H for each node vi from graph G. In general, the
encoder fθ can be GNNs , , or other types
of neural networks for graph learning , , . The
pretext decoder pφ (parameterized by φ) takes H as its input
for the pretext tasks. The architecture of pφ depends on
speciﬁc downstream tasks.
Under this framework, graph SSL can be formulated as:
θ∗, φ∗= arg min
Lssl (fθ, pφ, D) ,
where D denotes the graph data distribution that satisﬁes
(V, E) ∼D in an unlabeled graph G, and Lssl is the SSL
loss function that regularizes the output of pretext decoder
according to speciﬁc crafted pretext tasks.
By leveraging the trained graph encoder fθ∗, the generated representations can then be used in various downstream tasks. Here we introduce a downstream decoder qψ
(parameterized by ψ), and formulate the downstream task
as a graph supervised learning task:
θ∗∗, ψ∗= arg min
Lsup (fθ∗, qψ, G, y) ,
where y denotes the downstream task labels, and Lsup is the
supervised loss that trains the model for downstream tasks.
In the following subsections, we specify four graph
SSL variants based on Equation (1) in Section 3.2, three
graph self-supervised training schemes in Section 3.3 by
combining Equations (1) and (2) differently, and three types
of downstream tasks based on Equation (2) in Section 3.4.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
Perturbed Graph
Representations
Reconstructed Graph
Input Graph
Perturbation
Reconstruction Loss
(a) Generation-based graph SSL methods. The model input
is generated by a (optional) graph perturbation. In the pretext task, a generative decoder tries to recover the original
graph from representation H, with a loss function aiming
to minimize the difference between the reconstructed and
original graphs.
Input Graph
Representations
Predicted Properties
Extraction
Prediction Loss
Auxiliary Properties
(b) Auxiliary property-based graph SSL methods. The
auxiliary properties are extracted from graphs freely. A
classiﬁcation- or regression- based decoder aims to predict
the extracted properties under the training of CE/MSE loss.
Augmentation
Augmented Graph
Augmentation
Augmented Graph
Agreements
Agreements
(Positive/
Contrastive
Representations
Representations
(c) Contrast-based graph SSL methods. Two different augmented views are constructed from the original graph.
Then, the model is trained by maximizing the MI between
two views. The MI estimator with (optional) projection
often serves as the pretext decoder and SSL loss.
Input Graph
Representations
Pretext Tasks
(d) Hybrid graph SSL methods. Multiple pretext tasks
are designed to train the model together in a multi-task
learning manner.
Fig. 3: Four categories of graph SSL.
Taxonomy of Graph Self-supervised Learning
Graph SSL can be divided into four types conceptually, including generation-based, auxiliary property-based,
contrastive-based and hybrid methods, by leveraging different designs of pretext decoders and objective functions.
The categorizations of these methods are brieﬂy discussed
below and shown in Fig. 2, and the concept map of each
type of methods is given in Fig. 3.
Generation-based Methods form the pretext task as the
graph data reconstruction from two perspectives: feature
and structure. Speciﬁcally, they focus on the node/edge
features or/and graph adjacency reconstructions. In such
a case, Equation (1) can be further derived as:
θ∗, φ∗= arg min
pretext decoder. ˜G denotes the graph data with perturbed
node/edge features or/and adjacency matrix. For most of
the generation-based approaches, the self-supervised objective function Lssl is typically deﬁned to measure the
difference between the reconstructed and the original graph
data. One of the representative approaches is GAE 
which learns embeddings by rebuilding the graph adjacency
Auxiliary Property-based Methods enrich the supervision
signals by capitalizing on a larger set of attributive and topological graph properties. In particular, for different crafted
auxiliary properties, we further categorize these methods
into two types: regression- and classiﬁcation-based. Formally, they can be formulated as:
θ∗, φ∗= arg min
where c denotes the speciﬁc crafted auxiliary properties. For
regression-based approaches, c can be localized or global
graph properties, such as the node degree or distance
to clusters within G. For classiﬁcation-based methods, on
the other hand, the auxiliary properties are typically constructed as pseudo labels, such as the graph partition or
cluster indices. Regarding to the objective function, Lssl
can be mean squared error (MSE) for regression-based and
cross-entropy (CE) loss for classiﬁcation-based methods.
As a pioneering work, M3S uses node clustering to
construct pseudo labels that provide supervision signals.
Contrast-based Methods are usually developed based on
the concept of mutual information (MI) maximization,
where the estimated MI between augmented instances of the
same object (e.g., node, subgraph, and graph) is maximized.
For contrastive-based graph SSL, Equation (1) is reformulated as:
θ∗, φ∗= arg min
fθ( ˜G(1)), fθ( ˜G(2))
where ˜G(1) and ˜G(2) are two differently augmented instances
of G. In these methods, the pretext decoder pφ indicates
the discriminator that estimates the agreement between two
instances (e.g., the bilinear function or the dot product),
and Lssl denotes the contrastive loss. By combining them
and optimizing Lssl, the pretext tasks aim to estimate and
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
Graph for Fine-tuning
1 0 1 1 0 1 0 1
Downstream
Stage 1: Pre-training
Stage 2: Fine-tuning
Param Transfer
Graph for Pre-training
(a) Pre-training and Fine-tuning (PF). First, the encoder is pretrained with pretext tasks in an unsupervised manner. Then, the
pre-trained parameters are leveraged as the initial parameters in
the ﬁne-tuning phase where encoder is trained by downstream
tasks independently.
Input Graph
1 0 1 1 0 1 0 1
Downstream
(b) Joint Learning (JL). The model is trained with pretext and
downstream tasks in a multi-task learning manner.
Input Graph
1 0 1 1 0 1 0 1
Downstream
(Freeze Param)
Stage 1: Unsupervised Learning
Stage 2: Downstream Task Learning
Param Transfer
(c) Unsupervised Representation Learning (URL). It ﬁrst trains
encoder with pretext tasks, and uses the ﬁxed representations to
learn the downstream decoder in Stage 2.
Fig. 4: Three types of learning schemes for SSL.
maximize the MI between positive pairs (e.g., augmented
instances of the same object) and minimize the MI between
negative samples (e.g., instances derived from different
objects), which is implicitly included in Lssl. Representative works include cross-scale methods (e.g., DGI ) and
same-scale methods (e.g., GraphCL and GCC ).
Hybrid Methods take advantage of previous categories and
consist of more than one pretext decoder and/or training
objective. We formulate this branch of methods as the
weighted or unweighted combination of two or more graph
SSL schemes based on formulas from Equation (3) to (5).
GMI , which jointly considers edge-level reconstruction
and node-level contrast, is a typical hybrid method.
Discussion. Different graph SSL methods have different
properties. Generation-based methods are simple to implement since the reconstruction task is easy to build, but sometimes recovering input data is memory-consuming for largescale graphs. Auxiliary property-based methods enjoy the
uncomplicated design of decoders and loss functions; however, the selection of helpful auxiliary properties often needs
domain knowledge. Compared to other categories, contrastbased methods have more ﬂexible designs and boarder
applications. Nevertheless, the designs of contrastive frameworks, augmentation strategies, and loss functions usually rely on time-consuming empirical experiments. Hybrid
methods beneﬁt from multiple pretext tasks, but a main
challenge is how to design a joint learning framework to
balance each component.
Taxonomy of Self-Supervised Training Schemes
According to the relationship among graph encoders, selfsupervised pretext tasks, and downstream tasks, we investigate three types of graph self-supervised training
schemes: Pre-training and Fine-tuning (PF), Joint Learning (JL), and Unsupervised Representation Learning (URL).
Brief pipelines of them are given in Fig. 4.
Pre-training and Fine-tuning (PF). In PF scheme, the encoder fθ is ﬁrst pre-trained with pretext tasks on pretraining datasets, which can be viewed as an initialization
for the encoder’s parameters. After that, the pre-trained
encoder fθinit is ﬁne-tuned together on ﬁne-tuning datasets
(with labels) with a downstream decoder qψ under the
supervision of speciﬁc downstream tasks. Note that the
datasets for pre-training and ﬁne-tuning could be the same
or different. The formulation of PF scheme is deﬁned as
θ∗, φ∗= arg min
Lssl (fθ, pφ, D) ,
θ∗∗, φ∗= arg min
Lsup (pθ∗, qψ, G, y) .
Joint Learning (JL). In JL scheme, the encoder is jointly
trained with the pretext and downstream tasks. The
loss function consists of both the self-supervised and
downstream task loss functions, where a trade-off hyperparameter α controls the contribution of self-supervision
term. This can be considered as a kind of multi-task learning
where the pretext task is served as a regularization of the
downstream task:
θ∗, φ∗, ψ∗= arg min
αLssl (fθ, pφ, D)+Lsup (fθ, qψ, G, y)
Unsupervised Representation Learning (URL). The ﬁrst
stage of the URL scheme is similar to that of PF. The
differences are: (1) In the second stage, the encoder’s parameters are frozen (i.e., θ∗) when the model is trained
with the downstream task; (2) The training of two stages
is performed on the same dataset. The formulation of URL
is deﬁned as:
θ∗, φ∗= arg min
Lssl (fθ, pφ, D) ,
ψ∗= arg min
Lsup (fθ∗, qψ, G, y) .
Compared with other schemes, URL is more challenging
since there is no supervision during the encoder training.
Taxonomy of Downstream Tasks
According to the scale of prediction target, we divide
downstream tasks into node-, link-, and graph-level tasks.
Speciﬁcally, node-level tasks aim to predict the property of
nodes in graph(s) according to node representations. Linklevel tasks infer the property of edges or pairs of nodes,
where downstream decoders map the embeddings of two
nodes into link-level predictions. Besides, graph-level tasks
learn from a dataset with multiple graphs and forecast
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
Perturbed Graph
Representations
Reconstructed Features
Input Graph
Feature Masking
Reconstruction
(a) Graph Completion is a representative approach of feature
generation-based graph SSL. The features of certain nodes are
masked and then fed into the model, and the learning target is to
reconstruct the masked features. An MSE loss is used to recover
the features.
Perturbed Graph
Representations
Reconstructed Structure
Input Graph
Edge Masking
Reconstruction Target
Reconstruction
(b) The objective of Denoising Link Reconstruction is to rebuild
the masked edges. A binary cross-entropy (BCE) loss is employed
to train the model where existing edges are the positive samples
and the unrelated node pairs are the negative samples. A negative
sampling (Neg. Samp.) strategy is used to balance the classes.
Fig. 5: Examples of two categories of generation-based methods: Graph Completion and Denoising Link Reconstruction.
the property of each graph. Based on Equation (2), we
provide the speciﬁc deﬁnitions of downstream decoders qψ,
downstream objectives Lsup, and downstream task labels y
of three types of tasks, which are detailed in Appendix C.
GENERATION-BASED METHODS
The generation-based methods aim to reconstruct the input
data and use the input data as their supervision signals.
The origin of this category of methods can be traced back to
Autoencoder which learns to compress data vectors into
low-dimensional representations with the encoder network
and then try to rebuild the input vectors with the decoder
network. Different from generic input data represented in
vector formats, graph data are interconnected. As a result,
generation-based graph SSL approaches often take the full
graph or a subgraph as the model input, and reconstruct
one of the components, i.e. feature or structure, individually.
According to the objects of reconstruction, we divide these
works into two sub-categories: (1) feature generation that
learns to reconstruct the feature information of graphs,
and (2) structure generation that learns to reconstruct the
topological structure information of graphs. The pipelines
of two example methods are given in Fig. 5, and a summary
of the generation-based works is illustrated in Table 1.
Feature Generation
Feature generation approaches learn by recovering feature
information from the perturbed or original graphs. Based
on Equation (3), the feature generation approaches can be
further formalized as:
θ∗, φ∗= arg min
where pφ(·) is the decoder for feature regression (e.g., a
fully connected network that maps the representations to
reconstructed features), Lmse is the Mean Squared Error
(MSE) loss function, and ˆX is a general expression of various
kinds of feature matrices, e.g., node feature matrix, edge
feature matrix, or low-dimensional feature matrix.
To leverage the dependency between nodes, a representative branch of feature generation approaches follows
the masked feature regression strategy, which is motivated
by image inpainting in CV domain . Speciﬁcally, the
features of certain nodes/edges are masked with zero or
speciﬁc tokens in the pre-processing phase. Then, the model
tries to recover the masked features according to the unmasked information. Graph Completion is a representative method. It ﬁrst masks certain nodes of the input graph
by removing their features. Then, the learning objective is
to predict the masked node features from the features of
neighboring nodes with a GCN encoder. We can consider
Graph Completion as an implement of Equation (9) where
ˆX = X and ˜G = (A, ˜X). Similarly, AttributeMask aims
to reconstruct the dense feature matrix processed by Principle Component Analysis (PCA) ( ˆX = PCA(X)) instead
of the raw features due to the difﬁculty of rebuilding highdimensional and sparse features. AttrMasking rebuilds
not only node attributes but also the edge one, which can be
written as ˆX = [X, Xedge].
Another branch of methods aims to generate features
from noisy features. Inspired by denoising autoencoder ,
MGAE recovers raw features from noisy input features
with each GNN layer. Here we also denote ˜G = (A, ˜X)
but here ˜X is corrupted with random noise. Proposed
in , Corrupted Features Reconstruction and Corrupted
Embeddings Reconstruction aim to reconstruct raw features
and hidden embeddings from corrupted features.
Besides, directly rebuilding features from the clean data
is also an available solution. GALA trains a Laplacian
smoothing-sharpening graph autoencoder model with the
objective that rebuilds the raw feature matrix according to
the clean input graph. Similarly, autoencoding reconstructs the raw features from clean inputs. For these two
methods, we can formalize that ˜G = (A, X) and ˆX = X.
Structure Generation
Different from the feature generation approaches that rebuild the feature information, structure generation approaches learn by recovering the structural information. In
most cases, the objective is to reconstruct the adjacency
matrix, since the adjacency matrix can brieﬂy represent the
topological structure of graphs. Based on Equation (3), the
structure generation methods can be formalized as follows:
θ∗, φ∗= arg min
where pφ(·) is a decoder for structure reconstruction, and A
is the (full or partial) adjacency matrix.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
TABLE 1: Main characteristics of generation-based graph SSL approaches. “FG” and “SG” mean “Feature Generation” and
“Structure Generation”, respectively. Missing values (“-”) in Input Data Perturbation indicate that the method takes the
original graph data as input.
Pretext Task
Downstream
Task Level
Input Data
Perturbation
Generation
Graph Completion 
Attributed
Feature Masking
Node Feature
AttributeMask 
Attributed
Feature Masking
PCA Node Feature
AttrMasking 
Attributed
Feature Masking
Node/Edge Feature
Attributed
Feature Noising
Node Feature
Corrupted Features Reconstruction 
Attributed
Feature Noising
Node Feature
Corrupted Embeddings Reconstruction 
Attributed
Embedding Noising
Node Embedding
Attributed
Node Feature
Autoencoding 
Attributed
Node Feature
GAE/VGAE 
Attributed
Adjacency Matrix
SIG-VAE 
Plain/Attributed
Adjacency Matrix
ARGA/ARVGA 
Attributed
Adjacency Matrix
SuperGAT 
Attributed
Partial Edge
Denoising Link Reconstruction 
Node/Link/Graph
Attributed
Edge Masking
Masked Edge
EdgeMask 
Attributed
Edge Masking
Masked Edge
Zhu et al. 
Attributed
Feature Masking/Edge Masking
Partial Edge
GAE is the simplest instance of the structure generation method. In GAE, a GCN-based encoder ﬁrst generates
node embeddings H from the original graph ( ˜G = G). Then,
an inner production function with sigmoid activation serves
as its decoder to recover the adjacency matrix from H. Since
adjacency matrix A is usually binary and sparse, a BCE loss
function is employed to maximize the similarity between the
recovered adjacency matrix and the original one, where positive and negative samples are the existing edges (Ai,j = 1)
and unconnected node pairs (Ai,j = 0), respectively. To
avoid the imbalanced training sample problem caused by
extremely sparse adjacency, two strategies can be used to
prevent trivial solution: (1) re-weighting the terms with
Ai,j = 1; or (2) sub-sampling terms with Ai,j = 0.
As a classic learning paradigm, GAE has a series of
derivative works. VGAE further integrates the idea
of variational autoencoder into GAE. It employs
model-based
mean and deviation with two parallel output layers
and uses Kullback-Leibler divergence between the prior
distribution and the estimated distribution. Following
VGAE, SIG-VAE considers hierarchical variational
inference to learn more generative representations for graph
data. ARGA/ARVGA regularizes the GAE/VGAE
model with generative adversarial networks (GANs) .
Speciﬁcally, a discriminator is trained to distinguish the
fake and real data, which forces the distribution of latent
embeddings closer to the Gaussian prior. SuperGAT 
further extends this idea to every layers in the encoder.
Concretely, it rebuilds the adjacency matrix from the latent
representations of every layer in the encoder.
Instead of rebuilding the full graph, another solution
is to reconstruct the masked edges. Denoising Link Reconstruction randomly drops existing edges to obtain
the perturbed graph ˜G. Then, the model aims to recover
the discarded connections with a pairwise similarity-based
decoder trained by a BCE loss. EdgeMask also has a
similar perturbation strategy, where a non-parametric MAE
function minimizes the difference between the embeddings
of two connected nodes. Zhu et al. apply two perturbing
strategies, i.e. Randomly Removing Links and Randomly
Covering Features, to the input graph ( ˜G = ( ˜A, ˜X)), while
its target is to recover the masked link by a decoder.
Discussion. Due to the different learning targets, two
branches of generation-based methods have distinct designs
of the decoder and loss functions. The learned representations by structure generation usually contain more node
pair-level information since structure generation focuses on
edge reconstruction; by contrary, feature generation methods often capture node-level knowledge.
AUXILIARY PROPERTY-BASED METHODS
The auxiliary property-based methods acquire supervision
signals from the node-, link- and graph- level properties
which can be obtained from the graph data freely. These
methods have a similar training paradigm with supervised
learning since both of them learn with “sample-label” pairs.
Their difference lies in how the label is obtained: In supervised learning, the manual label is human-annotated
which often needs expensive costs; in auxiliary propertybased SSL, the pseudo label is self-generated automatically
without any cost.
Following the general taxonomy of supervised learning,
we divide auxiliary property-based methods into two subcategories: (1) auxiliary property classiﬁcation which leverages classiﬁcation-based pretext tasks to train the encoder
and (2) auxiliary property regression which performs SSL via
regression-based pretext tasks. Fig. 6 provides the pipelines
of them, and Table 2 summarizes the auxiliary propertybased methods.
Auxiliary Property Classiﬁcation
Borrowing the training paradigm from supervised classiﬁcation tasks, the methods of auxiliary property classiﬁcation
create discrete pseudo labels automatically, build a classiﬁer
as the pretext decoder, and use a cross entropy (CE) loss
Lce to train the model. Originated from Equation (4), we
provide the formalization of this branch of methods as:
θ∗, φ∗= arg min
where pφ is the neural network classiﬁer-based decoder
which outputs a k-dimensional probability vector (k is
the number of classes), and c ∈C = {c1, · · · , ck} is the
corresponding pseudo label which belongs to a discrete and
ﬁnite label set C. According to the deﬁnition of pseudo
label set C, we further construct two sub-categories under
auxiliary property classiﬁcation, i.e., clustering-based and
pair relation-based methods.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
Classifier
Input Graph
Representations
Classification Results
Pseudo Labels
Discrete Properties
I. Clustering-based
II. Pair Relation-based
(a) Auxiliary property classiﬁcation methods extract discrete properties as pseudo
labels, and the pretext decoder is used to predict the classiﬁcation results. A CE loss
function is used to train the models. Two types of properties (clustering-based and
pair relation-based) can be used to deﬁne the pseudo labels.
Input Graph
Representations
Regression Results
Regression Targets
Continuous Properties
(b) Auxiliary property regression methods aim to predict the continuous auxiliary properties with the decoder, where models are trained by the MSE loss.
Fig. 6: Two categories of auxiliary property-based graph SSL.
TABLE 2: Main characteristics of auxiliary property-based graph SSL approaches. “CAPC”, “PAPC” and “APR” mean
clustering-based auxiliary property classiﬁcation, pair relation-based auxiliary property classiﬁcation and auxiliary property regression, respectively.
Pretext Task
Downstream
Task Level
Mapping Function
Node Clustering 
Attributed
Feature-based Clustering
Attributed
Feature-based Clustering
Graph Partitioning 
Attributed
Structure-based Clustering
Cluster Preserving 
Node/Link/Graph
Attributed
Structure-based Clustering
CAGNN 
Attributed
Feature-based Clustering
with Structural Reﬁnement
S2GRL 
Attributed
Shortest Distance Function
PairwiseDistance 
Attributed
Shortest Distance Function
Centrality Score Ranking 
Node/Link/Graph
Attributed
Centrality Scores Comparison
NodeProperty 
Attributed
Degree Calculation
Distance2Cluster 
Attributed
Distance to Cluster Center
PairwiseAttrSim 
Attributed
Cosine Similarity of Feature
SimP-GCN 
Attributed
Cosine Similarity of Feature
Clustering-based Methods
A promising way to construct pseudo label is to divide
nodes into different clusters according to their attributive
or structural characteristics. To achieve that, a mapping
function Ω: V →C is introduced to acquire the pseudo
label for each node, which is built on speciﬁc unsupervised
clustering/partitioning algorithms , , , . Then,
the learning objective is to classify each node into its corresponding cluster. Following Equation (11), the learning
objective is reﬁned as:
θ∗, φ∗= arg min
 [fθ(G)]vi
where [·]vi is the picking function that extracts the representation of vi.
Node Clustering is a representative approach that
utilizes attributive information to generate pseudo labels.
Speciﬁcally, it leverages a feature-based clustering algorithm
(which is an instance of Ω) taking X as input to divide node
set into k clusters, and each cluster indicates a pseudo label
for classiﬁcation. The intuition behind Node Clustering is
that nodes with similar features tend to have consistent
semantic properties. M3S introduces a multi-stage selftraining mechanism for SSL using DeepCluster algorithm. In each stage, it ﬁrst runs K-means clustering on node
embedding H. After that, an alignment is executed to map
each cluster to a class label. Finally, the unlabeled nodes
with high conﬁdence are given the corresponding (pseudo)
labels and used to train the model. In M3S, C is borrowed
from the manual label set Y, and Ωis composed of the Kmeans and alignment algorithms.
In addition to feature-based clustering, Graph Partitioning divides the nodes according to the structural characteristics of nodes. Concretely, it groups nodes into multiple
subsets by minimizing the connections across subsets ,
deﬁning Ωas the graph partitioning algorithm. Cluster
Preserving ﬁrst leverages graph clustering algorithm
 to acquire non-overlapping clusters, and then calculates
the representation of each cluster via an attention-based
aggregator. After that, a vector representing the similarities between each node and the cluster representations is
assigned as the soft pseudo label for each node. Besides,
CAGNN ﬁrst runs feature-based clusters to generate
pseudo labels and then reﬁnes the clusters by minimizing
inter-cluster edges, which absorbs the advantages of both
attributive and structural clustering algorithms.
Pair Relation-based Methods
Apart from the clustering and graph properties, an alternative supervision signal is the relationship between each pair
of nodes within a graph. In these methods, the input of the
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
decoder is not a single node or graph but a pair of nodes. A
mapping function Ω: V × V →C is utilized to deﬁne the
pseudo label according to pair-wise contextual relationship.
We write the objective function as:
θ∗, φ∗= arg min
 [fθ(G)]vi,vj
, Ω(vi, vj)
where P ⊆V × V is the node pair set deﬁned by speciﬁc
pretext tasks, and [·]vi,vj is the picking function that extracts
and concatenates the node representations of vi and vj.
Some approaches regard the distance between two nodes
as the auxiliary property. For instance, S2GRL learns by
predicting the shortest path between two nodes. Speciﬁcally,
the label for a pair of nodes is deﬁned as the shortest
distance between them. Formally, we can write the mapping
function as Ω(vi, vj) = dist(vi, vj). The decoder is built
to measure the interaction between pairs of nodes, which
is deﬁned as an element-wise distance between two embedding vectors. The node pair set P collects all possible
node pairs including the combination of all nodes with
their 1 to K hops neighborhoods. PairwiseDistance has
a very similar learning target and decoder with S2GRL,
but introduces an upper bound of distance, which can be
represented as Ω(vi, vj) = max(dist(vi, vj), 4).
Centrality Score Ranking presents a pretext task that
predicts the relative order of centrality scores between a pair
of nodes. For each node pair (vi, vj), it ﬁrst calculates four
types of centrality scores si, sj (eigencentrality, betweenness, closeness, and subgraph centrality), and then creates
its pseudo label by comparing the value of si and sj. We
formalize the mapping function as: Ω(vi, vj) = I(si > sj),
where I(·) is the identity function.
Auxiliary Property Regression
Auxiliary property regression approaches construct the pretext tasks on predicting extensive numerical properties of
graphs. Compared to auxiliary property classiﬁcation, the
most signiﬁcant difference is that the auxiliary properties are
continuous values within a certain range instead of discrete
pseudo labels in a limited set. We reﬁne Equation (4) into a
regression version:
θ∗, φ∗= arg min
where Lmse is the MSE loss function for regression, and
c ∈R is a continuous property value.
NodeProperty is a node-level pretext task that predicts the property for each node. The available choices of
node properties include their degree, local node importance,
and local clustering coefﬁcient. Taking node degree as an
example, the objective function is illustrated as follows:
θ∗, φ∗= arg min
]vi, Ω(vi)
where Ω(vi) = Pn
j=1 Aij is the mapping function that
calculates the degree of node vi. Distance2Cluster aims
to regress the distances from each node to predeﬁned graph
clusters. Speciﬁcally, it ﬁrst partitions the graph into several
clusters with the METIS algorithm and deﬁnes the node
with the highest degree within each cluster as its cluster
center. Then, the target is to predict the distances between
each node and all cluster centers.
Another type of methods take the pair-wise property
as their regression targets. For instance, the target of PairwiseAttrSim is to predict the feature similarity of two
nodes according to their embeddings. We formalize its objective function as follows:
θ∗, φ∗= arg min
 [fθ(G)]vi,vj
, Ω(vi, vj)
where mapping function Ω(vi, vj) = cosine(xi, xj) is the
cosine similarity of raw features. In PairwiseAttrSim, the
node pairs with the highest similarity and dissimilarity are
selected to form the node pair set P. Similar to PairwiseAttrSim, SimP-GCN also considers predicting the cosine
similarity of raw features as a self-supervised regularization
for downstream tasks.
Discussion. As we can observe, the auxiliary property
classiﬁcation methods are more diverse than the regression
methods, since the discrete pseudo labels can be acquired by
various algorithms. In future works, more continuous properties are expected to be leveraged for regression methods.
CONTRAST-BASED METHODS
The contrast-based methods are built on the idea of mutual
information (MI) maximization , which learns by predicting the agreement between two augmented instances.
Speciﬁcally, the MI between graph instances with the similar
semantic information (i.e., positive samples) is maximized,
while the MI between those with unrelated information
(i.e., negative samples) is minimized. Similar to the visual
domain , , there exist various graph augmentations
and contrastive pretext tasks on multiple granularities to
enrich the supervision signals.
Following the taxonomy of contrast-based graph SSL
deﬁned in Section 3.2.3, we survey this branch of methods
from three perspectives: (1) Graph augmentations that generate various graph instances; (2) Graph contrastive learning
which forms various contrastive pretext tasks on the non-
Euclidean space; (3) Mutual information estimation that measures the MI between instances and forms the contrastive
learning objective together with speciﬁc pretext tasks.
Graph Augmentations
Recent success of contrastive learning on the visual domain
relies heavily on well-crafted image augmentations, which
reveals that data augmentations beneﬁt the model to explore
richer underlying semantic information by making pretext
tasks more challenging to solve . However, due to the
nature of graph-structured data, it is difﬁcult to apply the
augmentations from the Euclidean to the non-Euclidean
space directly. Motivated by image augmentations (e.g.,
image cutout and cropping ), existing graph augmentations can be categorized into three types: attributive-based,
topological-based, and the combination of both ,
Node Feature Shufﬂe (NFS), Edge Modiﬁcation (EM), Graph
Diffusion (GD), and Subgraph Sampling (SS).
augmentations). The examples of ﬁve representative augmentation strategies are demonstrated in Fig. 7. Formally, given
a graph G, we deﬁne the i-th augmented graph instance as
˜G(i) = ti(G), where ti ∼τ is a selected graph augmentation
and τ is a set of available augmentations.
Attributive augmentations
This category of augmentations is typically placed on node
attributes. Given G = (A, X), the augmented graph is
represented as:
˜G(i) = (A, ˜X(i)) =
where ti(·) is placed on the node feature matrix only, and
˜X(i) denotes the augmented node features. Speciﬁcally, attributive augmentations have two variants. The ﬁrst type
is Node feature masking (NFM) , , , , ,
which randomly masks the features of a portion of nodes
within the given graph. In particular, we can completely
(i.e., row-wisely) mask selected feature vectors with zeros
 , , or partially (i.e., column-wisely) mask a number of
selected feature channels with zeros , . We formulate
the node feature masking operation as:
ti(X) = M ◦X,
where M is the masking matrix with the same shape of
X, and ◦denotes the Hadamard product. For a given
masking matrix, its elements have been initialized to one
and masking entries are assigned to zero. In addition to
randomly sampling a masking matrix M, we can also calculate it adaptively , . For example, GCA keeps
important node features unmasked while assigning a higher
masking probability for those unimportant nodes, where the
importance is measured by node centrality.
On the other hand, instead of masking a part of the
feature matrix, node feature shufﬂe (NFS) , , 
partially and row-wisely perturbs the node feature matrix.
In other words, several nodes in the augmented graph are
placed to other positions when compared with the input
graph, as formulated below:
ti(X) = [X]eV,
where [·]vi is a picking function that indexes the feature
vector of vi from the node feature matrix, and eV denotes
the partially shufﬂed node set.
Topological augmentations
Graph augmentations from the structural perspectives
mainly work on the graph adjacency matrix, which is formulated as follows:
˜G(i) = ( ˜A(i), X) =
where ti(·) is typically placed on the graph adjacency matrix. For this branch of methods, edge modiﬁcation (EM)
 , , , , , is one of the most common
approaches, which partially perturbs the given graph adjacency by randomly dropping and inserting a portion of
edges. We deﬁne this process as follows:
ti(A) = M1 ◦A + M2 ◦(1 −A),
where M1 and M2 are edge dropping and insertion matrices. Speciﬁcally, M1 and M2 are generated by randomly
masking a portion of elements with the value equal to one in
A and (1−A). Similar to node feature masking, M1 and M2
can also be calculated adaptively . Furthermore, edge
modiﬁcation matrices can be generated based on adversarial
learning , , which increases the robustness of learned
representations.
Different from the edge modiﬁcation, graph diffusion
(GD) is another type of structural augmentations ,
 , which injects the global topological information to
the given graph adjacency by connecting nodes with their
indirectly connected neighbors with calculated weights:
where Θ and T are weighting coefﬁcient and transition matrix, respectively. Speciﬁcally, the above diffusion formula
has two instantiations . Let Θk = e−ιtk
and T = AD−1,
we have the heat kernel-based graph diffusion:
ti(A) = exp (ιAD−1 −ι),
where ι denotes the diffusion time. Similarly, the Personalized PageRank-based graph diffusion is deﬁned below by
letting Θk = β(1 −β)k and T = D−1
 I −(1 −β)D−1
where β denotes the tunable teleport probability.
Hybrid augmentations
It is worth noting that a given graph augmentation may
involve not only the attributive but also the topological
augmentations simultaneously, where we deﬁne it as the
hybrid augmentation and formulate as:
˜G(i) = ( ˜A(i), ˜X(i)) =
In such a case, the augmentation ti(·) is placed on both
the node feature and graph adjacency matrices. Subgraph
sampling (SS) , , , is a typical hybrid graph
augmentation which is similar to image cropping. Specifically, it samples a portion of nodes and their underlying
linkages as augmented graph instances:
ti(A, X) = [(A, X)]V′∈V,
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
where V′ denotes a subset of V, and [·]V′ is a picking function that indexes the node feature and adjacency matrices of
the subgraph with node set V′. Regarding the generation of
V′, several approaches have been proposed, such as uniform
sampling , random walk-based sampling , and top-k
importance-based sampling .
Apart from the subgraph sampling, most of graph contrastive methods heavily rely on hybrid augmentations
by combining the aforementioned strategies. For example,
GRACE applies the edge dropping and node feature
masking, while MVGRL adopts the graph diffusion and
subgraph sampling to generate different contrastive views.
Graph Contrastive Learning
As contrastive learning aims to maximize the MI between
instances with similar semantic information, various pretext
tasks can be constructed to enrich the supervision signals
from such information. Regarding the formulation of pretext
decoder pφ(·) in Equation (5), we classify existing works
into two mainstreams: same-scale and cross-scale contrastive
learning. The former branch of methods discriminates graph
instances in an equal scale (e.g., node versus node), while
the second type of methods places the contrasting across
multiple granularities (e.g., node versus graph). Fig. 8 and
Table 3 provide the pipelines and summaries of contrastbased methods, respectively.
Same-Scale Contrast
According to the scale for contrast, we further divide the
same-scale contrastive learning approaches into two subtypes: node-level and graph-level.
Node-Level Same-Scale Contrast: Early methods , , , under this category are mainly to
learn node-level representations and built on the idea that
nodes with similar contextual information should share the
similar representations. In other word, these methods are
trying to pull the representation of a node closer to its
contextual neighborhood without relying on complex graph
augmentations. We formulate them as below:
θ∗= arg min
 [fθ(A, X)]vi, [fθ(A, X)]vc
where vc denotes the contextual node of vi, for example,
a neighboring node in a random walk starting from vi. In
those methods, the pretext discriminator (i.e., decoder) is
typically the dot product and thus we omit its parameter in
equation. Speciﬁcally, DeepWalk introduces a random
(RW)-based
contextual
information around a selected node in an unattributed
graph. It maximizes the co-occurrence (i.e., MI measured
by the binary classiﬁer) of nodes within the same walk
as in the Skip-Gram model , . Similarly, node2vec
 adopts biased RWs to explore richer node contextual
information and yields a better performance. GraphSAGE
 , on the other hand, extends aforementioned two
methods to attributed graphs, and proposes a novel GNN
to calculate node embedding in an inductive manner, which
applies RW as its internal sampling strategy as well. On
heterogeneous graphs, SELAR samples meta-paths to
capture the contextual information. It consists of a primary
link prediction task and several meta-paths prediction
auxiliary tasks to enforce nodes within the same meta-path
to share closer semantic information.
Different from the aforementioned approaches, modern
node-level same-scale contrastive methods are exploring
richer underlying semantic information via various graph
augmentations, instead of limiting on subgraph sampling:
θ∗, φ∗= arg min
 fθ( ˜A(1), ˜X(1)), fθ( ˜A(2), ˜X(2))
where ˜A(1) and ˜A(2) are two augmented graph adjacency
matrices. Similarly, ˜X(1) and ˜X(2) are two node feature
matrices under different augmentations. The discriminator
pφ(·) in above equation can be parametric with Φ (e.g.,
bilinear transformation) or not (e.g., cosine similarity where
Φ = ∅). In those methods, most of them deal with attributed graphs: GRACE adopts two graph augmentation strategies, namely node feature masking and edge
dropping, to generate two contrastive views, which then
pulls the representations of the same nodes closer between
two graph views while pushing the rest of nodes away (i.e.,
intra- and inter-view negatives). Based on this framework,
GCA further introduces an adaptive augmentation for
graph-structured data based on underlying graph properties, which results in a more competitive performance. Differently, GROC proposes an adversarial augmentation
on graph linkages to increase the robustness of learned node
representations. Because of the success of SimCLR in the
visual domain, GraphCL(N) 3 further extends this idea
to graph-structured data, which relies on the node feature
masking and edge modiﬁcation to generate two contrastive
views, and then the MI between two target nodes within
different views is maximized. CGPN introduces Poisson
learning to node-level contrastive learning, which beneﬁts
node classiﬁcation task under extremely limited labeled
data. On plain graphs, GCC utilizes RW as augmentations to extract the contextual information of a node, which
then contrasts the representation of it with its counterparts
by leveraging the contrastive framework of MoCo . On
the other hand, HeCo is contrasting on heterogeneous
graphs, where two contrastive views are generated from
two perspectives, i.e., network schema and meta-path, while
the encoder is trained by maximizing the MI between the
embeddings of the same node in two views.
Apart from those methods relying on carefully-crafted
negative samples, approaches like BGRL propose to
contrast on graph instances themselves and thus alleviate
the reliance on deliberately designed negative sampling
strategies. BGRL takes the advantage of knowledge distillation in BYOL , where a momentum-driven Siamese architecture has been introduced to guide the extraction of supervision signals. Speciﬁcally, it uses node feature masking
and edge modiﬁcation as augmentations, and the objective
of BGRL is the same as in BYOL where the MI between
node representations from online and target networks is
maximized. SelfGNN adopts the same technique while
3. The approaches proposed in and have the same name
“GraphCL”. For distinction, we denote the node-level approach as
GraphCL(N) and the graph-level approach as GraphCL(G).
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
Augmentation
Augmented Graph
Augmentation
Augmented Graph
Node-Level
Same-Scale
Node Representations
Node Representations
Representation
Representation
Graph-Level
Same-Scale
(a) In same-scale contrast, different views are generated with
various graph augmentations based on the input graph at ﬁrst.
Then, the contrast can be placed on node or graph scales.
Augmentation
Augmented Graph
Augmentation
Augmented Graph
Node Representations
Node Representations
Representation
Representation
Patch-Global
Cross-Scale
Context-Global
Cross-Scale
(b) In cross-scale contrast, the augmented graph views are generated ﬁrstly. Then, the cross-scale contrast aims to discriminate
patch- or context- level embeddings with global representations.
Fig. 8: Two categories of contrast-based graph SSL.
the difference is that SelfGNN uses other graph augmentations, such as graph diffusion , node feature split, standardization, and pasting. Apart from BYOL, Barlow Twins
 is another similar yet powerful method without using
negative samples to prevent the model from collapsing.
G-BT extends the redundancy-reduction principle for
graph data analytics, where the optimization objective is to
minimize the dissimilarity between the identity and crosscorrelation metrics generated via node embeddings of two
augmented graph views. MERIT , on the other hand,
proposes to combine the advantages of Siamese knowledge
distillation and conventional graph contrastive learning.
It leverages a self-distillation framework in SimSiam 
while introducing extra node-level negatives to further exploit the underlying semantic information and enrich the
supervision signals.
Graph-Level Same-Scale Contrast: For graphlevel representation learning under same-scale contrasting,
the discrimination is typically placed on graph representations:
θ∗, φ∗= arg min
 ˜g(1), ˜g(2)
where ˜g(i) = R
 fθ( ˜A(i), ˜X(i))
denotes the representation
of augmented graph ˜G(i), and R(·) is a readout function to
generate the graph-level embedding based on node representations. Methods under Equation (29) may share similar
augmentations and backbone contrastive frameworks with
the aforementioned node-level approaches. For example,
GraphCL(G) adopts SimCLR to form its contrastive
pipeline which pulls the graph-level representations of two
views closer. Similarly, DACL is also built on SimCLR
but it designs a general yet effective augmentation strategy, namely mixup-based data interpolation. AD-GCL 
proposes an adversarial edge dropping mechanism as augmentations to reduce the amount of redundant information
taken by encoders. JOAO proposes the concept of joint
augmentation optimization, where a bi-level optimization
problem is formulated by jointly optimizing the augmentation selection together with the contrastive objectives.
Similar to GCC , CSSL is built on MoCo but
it contrasts graph-level embeddings. A similar design can
also be found in LCGNN . On the other hand, regarding
to the knowledge-distillation, IGSD is leveraging the
concept of BYOL and similar to MERIT .
Cross-Scale Contrast
Different from contrasting graph instances in an equivalent scale, this branch of methods places the discrimination across various graph topologies (e.g., node versus
graph). We further build two sub-classes under this category, namely patch-global and context-global contrast.
Patch-Global Cross-Scale Contrast: For nodelevel representation learning, we deﬁne this contrast as
[fθ(A, X)]vi, R
where R(·) denotes the readout function as we mentioned
in previous subsection. Under this category, DGI is the
ﬁrst method that proposes to contrast node-level embeddings with the graph-level representation, which aims to
maximize the MI between such two representations from
different scales to assist the graph encoder to learn both
localized and global semantic information. Based on this
idea, GIC ﬁrst clusters nodes within a graph based
on their embeddings, and then pulls nodes closer to their
corresponding cluster summaries, which is optimized with a
DGI objective simultaneously. Apart from attributed graphs,
some works on heterogeneous graphs are based on the
similar schema: HDGI can be regarded as a version
of DGI on heterogeneous graphs, where the difference is
that the ﬁnal node embeddings of a graph are calculated
by aggregating node representations under different metapaths. Similarly, ConCH shares the same objective with
DGI and aggregates meta-path-based node representations
to calculate node embeddings of a heterogeneous graph.
Differently, DMGI considers a multiplex graph as the
combination of several attributed graphs. For each of them,
given a selected target node and its associated relation type,
the relation-speciﬁc node embedding is ﬁrstly calculated.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
TABLE 3: Main characteristics of contrast-based graph SSL approaches. “NSC”, “GSC”, “PGCC” and “CGCC” mean nodelevel same-scale, graph-level same-scale, patch-global cross-scale, and context-global cross-scale contrast, respectively.
Pretext Task
Downstream
Task Level
Augmentation
DeepWalk 
node2vec 
GraphSAGE 
Attributed
SELAR 
Heterogeneous
Meta-path sampling
GRACE 
Attributed
Attributed
NFM+Adversarial EM
Attributed
Adaptive NFM+Adaptive EM
GraphCL(N) 
Attributed
Attributed
Node/Graph
Heterogeneous
Attributed
SelfGNN 
Attributed
GD+Node attributive transformation
Attributed
Barlow Twins
MERIT 
Attributed
SS+GD+NFM+EM
BYOL+InfoNCE
GraphCL(G) 
Attributed
Attributed
Noise Mixing
AD-GCL 
Attributed
Adversarail EM
Attributed
Attributed
SS+Node insertion/deletion+EM
LCGNN 
Attributed
Attributed
BYOL+InfoNCE
Attributed
Attributed
Heterogeneous
ConCH 
Attributed
Heterogeneous
Attributed
STDGI 
Spatial-temporal
Node feature shufﬂing
MVGRL 
Node/Graph
Attributed
SUBG-CON 
Attributed
SS+Node representation shufﬂing
SLiCE 
Heterogeneous
InfoGraph 
Attributed
Robinson et al. 
Attributed
Heterogeneous
Attributed
MICRO-Graph 
Attributed
SUGAR 
Attributed
The MI between the graph-level representation and such an
node embedding is maximized as in DGI. EGI extracts
high-level transferable graph knowledge by enforcing node
features to be structure-respecting and then maximizing the
MI between the embedding of a node and its surrounding
ego-graphs. On spatial-temporal graphs, STDGI maximizes the agreement between the node representations at
timestep t with the raw node features at t + 1 to guide
the graph encoder to capture rich semantic information to
predict future node features.
Note that aforementioned methods are not explicitly using any graph augmentations. For patch-global contrastive
approaches based on augmentations, we reformulate Equation (30) as follows:
θ∗, φ∗= arg min
i , ˜g(2)
where ˜h(1)
= [fθ( ˜A(1), ˜X(1))]vi is the representation of node
vi in augmented view 1, and ˜g(2) = R
 fθ( ˜A(2), ˜X(2))
denotes the representation of differently augmented view
2. Under the umbrella of this deﬁnition, MVGRL ﬁrst
generates two graph views via graph diffusion and subgraph sampling. Then, it enriches the localized and global
supervision signals by maximizing the MI between the node
embeddings in a view and the graph-level representation of
another view. SUBG-CON , on the other hand, inherits
the objective of MVGRL while it adopts different graph
augmentations. Speciﬁcally, it ﬁrst extracts the top-k most
informative neighbors of a central node from a large-scale
input graph. Then, the encoded node representations are
further shufﬂed to increase the difﬁculty of pretext task. On
heterogeneous graphs, SLiCE pulls nodes closer to their
closest contextual graphs, instead of explicitly contrasting
nodes with the entire graph. In addition, SLiCE enriches the
localized information of node embeddings via a contextual
translation mechanism.
For graph-level representation learning based on patchglobal contrast, we can formulate it by using Equation (30).
InfoGraph shares a similar schema with DGI . It contrasts the graph representation directly with node embeddings to discriminate whether a node belongs to the given
graph. To further boost contrastive methods like InfoGraph,
Robinson et al. propose a general yet effective hard
negative sampling strategy to make the underlying pretext
task more challenging to solved.
Context-Global Cross-Scale Contrast: Another
popular design under the category of cross-scale graph contrastive learning is context-global contrast, which is deﬁned
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
θ∗, φ∗= arg min
where S denotes a set of contextual subgraphs in an augmented input graph ˜G, where augmentations are typically
based on graph sampling under this category. In above
formula, ˜hs is the representation of augmented contextual subgraph s, and ˜g represents the graph-level representation over all subgraphs in S. Speciﬁcally, we let
 [fθ( ˜A, ˜X)]vi∈s
, and ˜g = R
 fθ( ˜A, ˜X)
. However,
for some methods, such as and , the graph-level
representation is calculated on the original input graph,
where ˜g = R
. Among them, BiGI is a nodelevel representation learning approach on bipartite graphs,
inheriting the contrasting schema of DGI . Speciﬁcally, it
ﬁrst calculates graph-level representation of the input graph
by aggregating two types of node embeddings. Then, it
samples the original graph, and then calculates the local
contextual representation of a target edge between two
nodes. The optimization objective of BiGI is to maximize
the MI between such a local contextual and global representations, where the trained graph encoder can then be
used in various edge-level downstream tasks. Aiming to
learn graph-level embedding, HTC maximizes the
MI between full-graph representation and the contextual
embedding which is the aggregation of sampled subgraphs.
Similar to but different from HTC, MICRO-Graph proposes a different yet novel motif learning-based sampling as
the implicit augmentation to generate several semanticallyinformative subgraphs, where the embedding of each subgraph is pulled closer to the representation of entire graph.
Considering a scenario that the graph-level representation
is based on the augmented input graph, as the default
setting shown in Equation (32), SUGAR ﬁrst samples
n subgraphs from the given graph, and then proposes a
reinforcement learning-based top-k sampling strategy to
select the n′ informative subgraphs among the candidate
set with size n. Finally, the contrast of SUGAR is established
between the subgraph embedding and the representation
of sketched graph, i.e., the generated graph by combining
these n′ subgraphs.
Mutual Information Estimation
Most of contrast-based methods rely on the MI estimation
between two or more instances. Speciﬁcally, the representations of a pair of instances sampled from the positive pool
are being pulled closer while the counterparts from negative
sets are pushed away. Given a pair of instances (xi, xj),
we let (hi, hj) to denote their representations. Thus, the MI
between (i, j) is given by :
MI(hi, hj) = KL
 P(hi, hj)||P(hi)P(hj)
= EP (hi,hj)
P(hi)P(hj)
where KL(·) denotes the Kullback-Leibler divergence, and
the end goal is to train the encoder to be discriminative
between a pair of instances from the joint density P(hi, hj)
and negatives from marginal densities P(hi) and P(hj).
In this subsection, we deﬁne two common forms of lower
bound and three speciﬁc forms of non-bound MI estimators
derived from Equation (33).
Jensen-Shannon Estimator
Although Donsker-Varadhan representation provides a
tight lower bound of KL divergence , Jensen-Shannon
divergence (JSD) is more common on graph contrastive
learning, which provides a lower bound and more efﬁcient
estimation on MI. We deﬁne the contrastive loss based on it
as follows:
= −MIJSD(hi, hj)
 1 −pφ(hi, h′
 pφ(hi, hj)
In above equation, hi and hj are sampled from the
same distribution P, and h′j is sampled from a different distribution eP. For the discriminator pφ(·), it can be
taken from various forms, where a bilinear transformation is typically adopted, i.e., pφ(hi, hj) = hT
Speciﬁcally,
pφ(hi, hj) = sigmoid
, Equation (34) can be presented in another form as in InfoGraph .
Noise-Contrastive Estimator
Similar to JSD, noise-contrastive estimator (a.k.a. InfoNCE)
provides a lower bound MI estimation that naturally consists of a positive and N negative pairs . An InfoNCEbased contrasitve loss is deﬁned as follows:
= −MINCE(hi, hj)
epφ(hi,hj)
epφ(hi,hj) + P
n∈N epφ(hi,h′n)
where the discriminator pφ(·) can be the dot product with a
temperature parameter τ, i.e., p(hi, hj) = hT
i hj/τ, such as
in GRACE and GCC .
Triplet Loss
Apart from aforementioned two lower bound MI estimators,
a triplet margin loss can also be adopted to estimate the MI
between data instances. However, minimizing this loss can
not guarantee that the MI is being maximized because it
cannot represent the lower bound of MI. Formally, Jiao et al.
 deﬁne this loss function as follows:
pφ(hi, hj)−pφ(hi, h′
discriminator
p(hi, hj) = 1/1 + e(−hT
For the methods inspired by BYOL and not relying on
negative samples, such as BGRL , their objective functions can also be interpreted as a non-bound MI estimator.
Given hi, hj ∼P, we deﬁne this loss as in below:
∥pψ(hi)∥∥hj∥
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
where pψ denotes an online predictor parameterized by ψ in
Siamese networks, which prevents the model from collapsing with other mechanisms such as momentum encoders,
stop gradient, etc. In particular, the pretext decoder in this
case denotes the mean square error between two instances,
which has been expanded in the above equation.
Barlow Twins Loss
Similar to BYOL, this objective alleviates the reliance on
negative samples but much simpler in implementation,
which is motivated by the redundancy-reduction principle.
Speciﬁcally, given the representations of two views H(1)
and H(2) for a batch of data instances sampled from a
distribution P, we deﬁne this loss function as below :
 H(1), H(2) =EB∼P|B|
where a and b index the dimension of a representation
vector, and i indexes the samples within a batch B.
Discussion. In general, same-scale methods usually follow the two-stream contrastive learning frameworks (e.g.,
SimCLR and BYOL ), where InfoNCE and BYOL
losses are widely used; In contrast, cross-scale methods
often derive from DGI , hence they prefer the JSD loss.
HYBRID METHODS
Compared to the aforementioned methods that only utilize
a single pretext task to train models, hybrid methods adopt
multiple pretext tasks to better leverage the advantages of
various types of supervision signals. The hybrid methods integrate various pretext tasks together in a multi-task learning
fashion, where the objective function is the weighted sum of
two or more self-supervised objectives. The formulation of
hybrid graph SSL methods is:
θ∗, φ∗= arg min
αiLssli (fθ, pφi, Di) ,
where N is the number of pretext tasks, αi, Lssli, pφ1 and
Di are the trade-off weight, loss function, pretext decoder
and data distribution of the i-th pretext task, respectively.
A common idea of hybrid graph SSL is to combine
different generation-based tasks together. GPT-GNN integrates feature and structure generation into a pre-training
framework for GNNs. Speciﬁcally, for each sampled input
graph, it ﬁrst randomly masks a certain amount of edges
and nodes. Then, two generation tasks are used to train the
encoder simultaneously: Attribute Generation that rebuilds
the masked features with MSE loss, and Edge Generation
that predicts the masked edges with a contrastive loss.
Graph-Bert combines attributive and structural pretext
tasks to pre-train a graph transformer model. Concretely,
Node Raw Attribute Reconstruction reconstructs the raw
features from the node’s embedding, while Graph Structure
Recovery aims to recover the graph diffusion value between
two nodes with a cosine similarity decoder. PT-DGNN 
TABLE 4: Main characteristics of hybrid graph SSL approaches. “M. et al.” and “Hetero.” are the abbreviations
for “Manessi et al.” and “Heterogeneous”, respectively. The
abbreviations for pretext tasks categories please refer to
Table 1, 2 and 3.
Pretext Task
Categories
Downstream
Task Level
GPT-GNN 
Graph-Bert 
Attributed
PT-DGNN 
M. et al. 
Attributed
Attributed
Attributed
MVMI-FT 
Attributed
GraphLoG 
Attributed
HDMI 
G-Zoom 
Attributed
LnL-GNN 
Attributed
Hu et al. 
Node/Link/
Attributed
GROVER 
Node/Link/
Attributed
Kou et al. 
Attributed
extends the idea of combining attributive and structural
generation to pre-train GNNs for dynamic graphs. Besides,
Manessi et al. propose to train GNNs with three types
of feature generation tasks.
Another idea is to integrate generative and contrastive
pretext tasks together. GMI adopts a joint learning
objective for graph representation learning. In GMI, the
contrastive learning target (i.e., feature MI) is to maximize
the agreement between node embeddings and neighbors’
features with a JSD estimator, and the generative target
(i.e., edge MI) is to minimize the reconstruction error of
the adjacency matrix with a BCE loss. CG3 considers
contrastive and generative SSL jointly for semi-supervised
node classiﬁcation problem. In CG3, two parallel encoders
(GCN and HGCN) are established to provide local and
global views for graphs. In contrastive learning, an Info-
NCE contrastive loss is used to maximize the MI between
the node embeddings from two views. In generative learning, a generative decoder is used to rebuild the topological
structure from the concatenation of two views’ embeddings.
MVMI-FT presents a cross-scale contrastive learning
framework that learns node representation from different
views, and also uses a graph reconstruction module to learn
the cross-view sharing information.
Since different types of contrasts can provide supervision
signals from different views, some approaches integrate
multiple contrast-based tasks together. GraphLoG consists of three contrastive objectives: the subgraph versus
subgraph, graph versus graph, and graph versus contextual.
The InfoNCE loss serves as the MI estimator for three types
of contrasts. HDMI mixes both same-scale and crossscale contrastive learning, which dissects a given multiplex
network into multiple attributed graphs. For each of them,
HDMI proposes three different objectives to maximize the
MI between raw node features, node embeddings, and
graph-level representations. G-Zoom uses same-scale
contrasts in three scales to learns representations, which extracts valuable clues from multiple perspectives. LnL-GNN
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
 leverages a bi-level MI maximization to learn from
local and non-local neighborhoods obtained by community
detection and feature-based clustering respectively.
Different auxiliary property-based tasks can also be integrated into a hybrid method. Hu et al. present to pretrain GNNs with multiple tasks simultaneously to capture
transferable generic graph structures, including Denoising
Link Reconstruction, Centrality Score Ranking, and Cluster
Preserving. In GROVER , the authors pre-train the GNN
Transformer model with auxiliary property classiﬁcation
tasks in node level (Contextual Property Prediction) and
graph level (Motif Prediction) simultaneously. Kou et al.
 mix structure generation, feature generation, and auxiliary property classiﬁcation tasks into a clustering model.
EMPIRICAL STUDY
In this section, we summarize essential resources for empirical study of graph SSL. Speciﬁcally, we conduct an
experimental comparison of the representative methods on
two commonly used downstream tasks on graph learning,
i.e., node classiﬁcation and graph classiﬁcation. We also
collect useful resources for empirical research, including
benchmark datasets and open-source implementations.
Performance Comparison of Node Classiﬁcation. We consider two learning settings for node classiﬁcation, i.e., semisupervised transductive learning and supervised inductive
learning. For transductive learning, we consider three citation network datasets, including Cora, Citeseer and Pubmed
 , for performance evaluation. The standard split of
train/valid/test often follows , where 20 nodes per class
are used for training, 500/1000 nodes are used for validation/testing. For inductive learning, we use PPI dataset 
to evaluate the performance. Following , 20 graphs are
employed to train the model, while 2 graphs are used to
validate and 2 graphs are used to test. In both setting, the
performance is measured by classiﬁcation accuracy.
We compare the performance of two groups of graph
SSL methods. In URL group, the encoder is purely trained
by SSL pretext tasks, and the learned representations are
directly fed into classiﬁcation decoders. In PF/JL group,
the training labels are accessible for encoders’ learning. We
consider two conventional (semi-) supervised classiﬁcation
methods (i.e., GCN and GAT ) as baselines.
The results of performance comparison are illustrated
in Table 5. According to the results, we have the following observations and analysis: (1) Early random walkbased contrastive methods (e.g., DeepWalk and Graph-
SAGE) and autoencoder-based generative methods (e.g.,
GAE and SIG-VAE) perform worse than the majority of
graph SSL methods. The possible reason is that they train
encoders with simple unsupervised learning targets instead
of well-designed self-supervised pretext tasks, hence failing
to fully leverage the original data to acquire supervision
signals. For example, DeepWalk only maximizes the MI
among nodes within a random walk, ignoring the global
structural information of graphs. (2) The methods employing advanced contrastive objectives from visual contrastive
learning (e.g., BGRL which uses BYOL loss and G-
BT which uses Barlow Twins loss ) do not show a
superior performance like their prototypes performing on
visual data. Such an observation indicates that directly
TABLE 5: A summary of experimental results for node
classiﬁcation in four benchmark datasets.
SIG-VAE 
S2GRL 
DeepWalk 
GraphSAGE 
GRACE 
GraphCL(N) 
MERIT 
MVGRL 
SubG-Con 
MVMI-FT 
G-Zoom 
G. Comp. 
SuperGAT 
N. Clu. 
G. Part. 
SimP-GCN 
Graph-Bert 
M. et al. 
borrowing self-supervised objectives from other domains
does not always bring enhancement. (3) Some representative
contrast-based methods (e.g., MVGRL, MERIT, and SubG-
Con) perform better than the generalization-based and auxiliary property-based methods, which reﬂects the effectiveness of contrastive pretext tasks and the potential room
for improvement of other methods. (4) The hybrid methods have competitive performance and some of them even
outperform the supervised baselines. The outperformance
suggests that integrating multiple pretext tasks can provide
supervision signals from diverse perspectives, which brings
signiﬁcant performance gain. For instance, G-Zoom 
achieves excellent results by combining contrastive pretext
tasks in three different levels. (5) The performance of methods in PF/JL groups is generally better than that in URL
groups, which demonstrates that the accessibility of label
information leads to further improvement for graph SSL.
More resources. For the evaluation and performance comparison of graph classiﬁcation, we please readers refer to
Appendix D. We also collect widely applied benchmark
datasets and divide them into four groups. The description
and statistics of the selected benchmark datasets are detailed
in Appendix E. Besides, we provide a collection of the opensource implementations of the surveyed works in Appendix
F, which can facilitate the reproduction, improvement, and
baseline experiments in further research.
PRACTICAL APPLICATIONS
Graph SSL has also been applied to a wide range of disciplines. We summarize the applications of graph SSL in three
research ﬁelds. More can be found in Appendix G.
Recommender Systems. Graph-based recommender system
has drawn great research attention since it can model items
and users with networks and leverage their underlying
linkages to produce high-quality recommendations . Recently, researchers introduce graph SSL in recommender
systems to deal with several issues, including the coldstart problem, pre-training for recommendation model, se-
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
lection bias, etc. For instance, Hao et al. present a
reconstruction-based pretext task to pre-train GNNs on the
cold-start users and items. S2-MHCN and DHCN
 employ contrastive tasks for hypergraph representation learning for social- and session- based recommendation, respectively. Liu et al. overcome the message
dropout problem and reduce the selection bias in GNNbased recommender system by introducing a graph contrastive learning module with a debiased loss. PMGT 
utilizes two generation-based tasks to capture multimodal
side information for recommendation.
Anomaly Detection. Graph anomaly detection is often
performed under an unsupervised scenario due to the
lack of annotated anomalies, which is naturally consistent
with the setting of SSL . Hence, various works apply
SSL to graph anomaly detection problem. To be concrete,
DOMINANT , SpecAE and AEGIS employ
hybrid SSL frameworks that combine structure and feature
generation to capture the patterns of anomalies. CoLA 
and ANEMONE utilize contrastive learning to detect
anomalies on graphs. SL-GAD applies hybrid graph
SSL to anomaly detection. HCM introduces an auxiliary property classiﬁcation task that predicts the hop-count
of each node pair for graph anomaly detection.
Chemistry. In the domain of chemistry, researchers usually
model molecules or compounds as graphs where atoms and
chemical bonds are denoted as nodes and edges respectively. Note that GROVER and Hu et al. also focus
on graph SSL for molecule data, which have been reviewed
before. Additionally, MolCLR and CKGNN learn
molecular representations with graph-level contrast-based
pretext tasks. Besides, GraphMS and MIRACLE 
employ contrastive learning to solve the drug–target and
drug-drug interaction prediction problems.
FUTURE DIRECTIONS
In this section, we analyze existing challenges in graph SSL
and pinpoint a few future research directions aiming to
address the shortcomings.
Theoretical Foundation. Despite its great success in various
tasks and datasets, graph SSL still lacks a theoretical foundation to prove its usefulness. Most existing methods are
mainly designed with intuition and evaluated by empirical
experiments. Although MI estimation theory supports
some of the works on contrastive learning, the choice of the
MI estimator still relies on empirical studies . Setting
up a solid theoretical foundation for graph SSL is urgently
needed. It is desirable to bridge the gap between empirical
SSL and fundamental graph theories, including the graph
signal processing and spectral graph theory.
Interpretability and Robustness. Graph SSL applications
may be risk-sensitive and privacy-related (e.g., fraud detection), an explainable and robust SSL framework is of great
signiﬁcance to adapt to such learning scenarios. However,
most existing graph SSL methods only aim to reach a higher
performance on downstream tasks with black-box models,
ignoring the explainability of learned representations and
predicted results. Moreover, except for a few pioneering
works , that consider the robustness problem, most
graph SSL methods assume input data is perfect, despite
the fact that real-world data is often noisy and GNNs
are vulnerable to adversarial attacks . It would be an
interesting and practical direction to explore explainable
and robust graph SSL methods in future.
Pretext Tasks for Complex Types of Graphs. Most current works concentrate on SSL for attributed graphs, and
only a few focus on complex graph types, e.g., heterogeneous or spatial-temporal graphs. For complex graphs, the
main challenge is how to design pretext tasks to capture
unique data characteristics of these complex graphs. Some
existing methods use MI maximization for complex
graph learning, which is limited in its ability to leverage
rich information from data, e.g., the temporal dynamics in
spatial-temporal/dynamic graphs. A future opportunity is
to produce various SSL tasks for complex graph data, where
speciﬁc data characteristics are the main focus. Furthermore,
extending SSL to more ubiquitous graph types (e.g., hypergraphs) would be a feasible direction for further exploration.
Augmentation for Graph Contrastive Learning. In contrastive learning for CV , a large amount of augmentation strategies (including rotation, color distort, crop, etc.)
provide diverse views of image data, maintaining the representation invariance during contrastive learning. However,
due to the nature of graph-structured data (e.g., complex
and non-Euclidean structure), data augmentation schemes
on graphs are not well explored and thus compromise
the effectiveness of graph augmentation-based approaches
as discussed in Section 6.1. Most of the existing graph
augmentations consider uniformly masking/shufﬂing node
features, modifying edges, or other alternative ways like
subgraph sampling and graph diffusion , which provides limited diversity and uncertain invariance when generating multiple graph views. To bridge the gaps, adaptively performing graph augmentations , automatically
selecting augmentations or jointly considering stronger
augmented samples by mining the rich underlying
structural and attributive information would be interesting
directions for further investigation.
Learning with Multiple Pretext Tasks. Most existing graph
SSL approaches learn representations by solving one pretext
task, while only a few hybrid methods explore the combination of multiple pretext tasks. As shown in previous NLP
pre-training models and the reviewed hybrid methods,
the integration of diverse pretext tasks can provide different
supervision signals from various perspectives, which facilitates graph SSL methods to produce more informative representations. Therefore, more advanced hybrid approaches
that consider a diverse and adaptive combination of multiple pretext tasks deserve further studies.
Broader Scope of Applications. Graphs are ubiquitous data
structures in numerous domains; nevertheless, acquiring
manual labels is often costly in most application ﬁelds.
In that case, graph SSL has a promising prospect on a
wide range of applications, especially those that highly
depend on domain knowledge to annotate data. However,
most current practical applications merely concentrate on a
few areas (e.g., recommender systems, anomaly detection,
and chemistry), indicating that graph SSL holds untapped
potential for most application ﬁelds. It is promising to
extend graph SSL to more expansive ﬁelds of applications,
for instance, ﬁnancial networks, cybersecurity, community
detection , and federated learning.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
CONCLUSION
This paper conducts a comprehensive overview of selfsupervised learning on graphs. We present a uniﬁed framework and further provide a systematic taxonomy that
groups graph SSL into four categories: generation-based,
auxiliary property-based, contrast-based, and hybrid approaches. For each category, we provide mathematical summary, up-to-date review, and comparison between methods.
More importantly, we collect abundant resources including
datasets, evaluation methods, performance comparison and
open-source codes for graph SSL approaches. A wide range
of practical applications of graph SSL are also introduced in
our paper. Finally, we suggest open challenges and promising research directions of graph SSL in the future.
APPENDIX A
TIMELINE OF GRAPH SELF-SUPERVISED LEARNING
Fig. 9 provides a timeline of some of the most representative
graph SSL methods since 2014. A group of early works is the
random walk-based node embedding methods including
DeepWalk , LINE , and node2vec . Another line
of early works is the graph autoencoder-based generation
methods, i.e. GAE/VGAE , MGAE , ARGA ,
and SIG-VAE . In 2019, a representation contrast-based
work, DGI , was proposed, bringing the ﬂourishing
development of graph contrastive learning. In 2020, more
types graph SSL methods were introduced, containing the
ﬁrst auxiliary property-based method (M3S ) and hybrid
method (GMI ). In 2021, more advanced techniques are
integrated with graph SSL, such as adaptive augmentation
(GCA ), automatic machine learning (JOAO ), and
adversarial augmentation (AD-GCL ).
APPENDIX B
NOTATIONS AND DEFINITIONS
Throughout this paper, we use bold uppercase letters (e.g.
X), bold lowercase letters (e.g. x), and calligraphic fonts
(e.g. V) to denote matrices, vectors and sets, respectively.
Unless speciﬁed otherwise, the notations used in this paper
are summarized in Table 6.
Dynamic and Heterogeneous Graphs
Deﬁnition 3 (Dynamic Graph). A dynamic graph indicates
a graph where nodes and edges change with respect to
time, i.e., G(t) = (V(t), E(t)), which denotes a dynamic
graph at time t ∈R+ that consists of a node set V(t) and
an edge set E(t) indexed by time. In practice, a dynamic
graph is also known as temporal dependency interaction
graph, where an interaction at time t is represented as
ei,j,t ∈E(t), where node vi ∈V(t) and vj ∈V(t).
For an attributed dynamic graph with node and edge
features, we can also let G(t) = (A(t), X(t)
node, X(t)
where the adjacency matrix A(t) ∈Rn×n, node feature
matrix X(t)
∈Rn×dnode and edge feature matrix
edge ∈Rn×dnode are evolving with respect to time.
TABLE 6: Commonly used notations with explanations.
Explanation
The length of a set.
Hadamard product.
The set of nodes in a graph.
The set of edges in a graph.
A node in the node set V.
An edge in the edge set E.
The number of nodes in a graph.
The number of edges in a graph.
The adjacency matrix of a graph.
The dimension of node features.
The dimension of edge features.
X, Xnode ∈Rn×d
The node feature matrix of a graph.
Xedge ∈Rm×dedge
The edge feature matrix of a graph.
xi = [X]vi
The node feature of vi.
The dimension of node representation.
The node representation matrix of a graph.
hi = [H]vi
The node representation vector of vi.
The graph representation vector of a graph.
Manual label set of downstream task.
Pseudo label set of pretext task.
Manual label of downstream task.
Pseudo label of pretext task.
Augmented/Perturbed graph.
Encoder parameterized by θ.
Pretext decoder parameterized by φ.
Downstream decoder parameterized by ψ.
Loss function.
Readout function.
Auxiliary property mapping function.
Mutual information function.
Special type: Spatial-temporal graph can be regarded as an
special type of attributed dynamic graph with ﬁxed graph
adjacency and dynamic features at different time steps.
Concretely, at each time step t, the dynamic feature matrix
is denoted as X(t) ∈Rn×d.
Deﬁnition 4 (Heterogeneous Graph). A heterogeneous graph
denotes a graph consisting of different types of nodes
and/ or edges. For a heterogeneous graph G = (V, E),
each node vi ∈V and each edge ei,j ∈E are associated with a corresponding mapping function, i.e.,
φv(vi) : V →Sv and φe(ei,j) : E →Se. Sv and Se
are the node types and link types, respectively, and they
satisfy |Sv| + |Se| > 2.
Special types: Bipartite graph and multiplex graph can be
viewed as two special types of heterogeneous graphs.
Speciﬁcally, a bipartite graph is a heterogeneous graph with
two types of nodes (|Sv| = 2) and a single type of edge
(|Se| = 1); a multiplex graph has one type of node (|Sv| = 1)
and multiple types of edges (|Se| > 1).
Graph Neural Networks and Readout Layers
A general deﬁnition of Graph Neural Networks (GNNs) is
represented as:
Deﬁnition 5 (Graph Neural Networks). Given an attributed
graph G with its feature matrix X where xi = X[i, :]T
is a d-dimensional feature vector of the node vi, a GNN
learns a node representation hi for each node vi ∈V
with two core functions: the aggregation function and
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
Contrast-based
Generation-based
Auxiliary property-based
Jin et al.
Fig. 9: A timeline of graph self-supervised learning (SSL) development.
combination function. Considering a K-layer GNN, the
k-th layer performs:
= aggregate(k) n
: vj ∈N(vi)
= combine(k) 
where h(k)
is the latent vector of node vi at the k-th iteration/layer with h(0)
= xi and h(K)
= hi, aggregate(k)(·)
and combine(k)(·) are the aggregation and combination
function of the k −th layer, respectively. The design of
the core functions is crucial to different GNNs, and we
refer the reader to the recent survey for a thorough
To learn the property of each node, the node representation hi is used for downstream tasks directly. Nevertheless,
when learning the representation of the entire graph, an
extra readout layer is needed, which is deﬁned as:
Deﬁnition 6 (Readout layer). Given a graph G = (V, E) with
its node representation h(K)
, · · · , h(K)
, a readout layer
generates the graph representation hG by:
(K) | vi ∈V
where R(·) is the readout function that fuses the nodelevel representations into a graph-level representation.
It can be a simple permutation invariant function like
summation or complex pooling algorithms like DiffPool
 and HaarPool .
Commonly Used Loss Functions
We formulate two commonly used loss functions, i.e., mean
squared error (MSE) loss and cross-entropy (CE) loss.
Deﬁnition 7 (Mean squared error (MSE) loss).
Given a predicted vector ˆy ∈Rdy and a target vector y ∈
Rdy, the formulation of MSE loss is deﬁned as follows:
Lmse(ˆy, y) = 1
(ˆyi −yi)2,
where ˆyi and yi are the i-th element of ˆy and y, respectively. Note that Lmse can also be applied to matrices
and scalars.
Deﬁnition 8 (Cross-entropy (CE) loss).
Given a predicted vector ˆy ∈Rdy after Softmax activation and a one-hot target vector y ∈Rdy, the formulation
of CE loss is deﬁned as follows:
Lce(ˆy, y) = −
yi log ˆyi.
Specially, the binary version of CE loss is denoted as
binary cross-entropy (BCE) loss, which is expressed by:
Lbce(ˆy, y) = −y log ˆy −(1 −y) log (1 −ˆy),
where ˆy ∈ and y ∈{0, 1} are the predicted and
target scalars, respectively.
APPENDIX C
DOWNSTREAM TASKS
Node-level tasks
Node-level tasks includes node regression and node classiﬁcation. As an representative task, node classiﬁcation aims to
predict the label yi ∈Y for each node vi ∈V, where Y is a
ﬁnite and discrete label set. A typical downstream decoder
qψ for node classiﬁcation is an MLP layer with Softmax
activation that takes node representation as its input. The
cross-entropy (CE) loss Lce is typically adopted for model
training. The formulation of node classiﬁcation is deﬁned as
follows based on Equation (2):
fθ∗, qψ∗= arg min
 [fθ(G)]vi
where VL is the training node set (where each node vi ∈VL
has a known label yi for training), and [·]vi is a picking
function that index the embedding of vi from the whole
embedding matrix.
Link-level tasks
Link-level tasks includes edge classiﬁcation and link prediction. Taking edge classiﬁcation as an example, given an edge
(vi, vj), the goal is to predict its label yi,j ∈Y from a given
label set. A downstream decoder qψ could be a classiﬁer
with the embeddings of two nodes as input, while CE
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
Lce serves as the loss function. We formalize the objective
function as follows:
fθ∗, qψ∗= arg min
 [fθ(G)]vi,vj
where EL is the training edge set, and [·]vi,vj is a picking
function that index the embedding of vi and vj from the
whole embedding matrix.
Graph-level Tasks
Graph-level tasks, including graph classiﬁcation and graph
regression, often rely on graph-level representations. For
instance, in graph classiﬁcation task, each graph Gi has its
target value yi ∈Y, and the objective is to train a model to
predict the labels of input graphs. In general, an encoder fθ
ﬁrst learns node embeddings and then aggregates them into
a graph embedding via a readout function. After that, the
graph embedding is fed into the decoder qψ composed by a
classiﬁer with Softmax function. The model is trained by CE
loss Lce, which can be formalized as:
fθ∗= arg min
where UL is the training graph set.
APPENDIX D
PERFORMANCE COMPARISON OF GRAPH CLASSI-
The task of graph classiﬁcation often adapts an inductive
supervised learning setting. We collect the experimental
results on nine benchmark datasets, including IMDB-B,
IMDB-M, RDT-B, RDT-M, COLLAB, MUTAG, PROTEINS,
PTC, and NCI-1 . We consider the dataset split which
is utilized in . Classiﬁcation accuracy is leveraged to
evaluate the performance. Similar to node classiﬁcation, we
divide the approaches into two groups: URL and PF/JL. A
powerful GNN for graph classiﬁcation, GIN , is employed
as our baseline.
The evaluation results are demonstrated in Table 7.
Based on the results, we have the following observations:
(1) The overall performance of the methods in URL group
is lower than that of the supervised baseline (i.e., GIN).
The reason is that current SSL methods cannot learn optimal representations as supervised methods do. Considering
the URL methods for node classiﬁcation usually achieve
higher results than baselines, future SSL methods for graphlevel representation learning are expected to reach higher
performance.
(2) The cross-scale contrast-based methods,
in general, have better performance than the same-scale
contrast-based methods. A possible reason is that cross-scale
contrastive learning can improve the quality of global-wise
(i.e., graph-level) representations, which beneﬁts graphlevel learning tasks.
(3) Taking GCC as an example, the
involvement of graph labels (i.e., PF/JL schemes) does not
necessarily lead to better encoders on all datasets compared to pure unsupervised learning (URL) scheme. We
conclude that pre-trained encoders with SSL sometimes
have a negative effect on downstream tasks. In this case,
how to circumvent such a negative transfer phenomenon
in graph-level SSL deserves further investigation. (4) Most
of these methods are measured on different sets of graphlevel datasets due to the lack of general benchmarks for
evaluation. Therefore, uniﬁed benchmarks for graph-level
SSL evaluation are signiﬁcantly needed in future works.
APPENDIX E
In this section, we conduct an introduction and a summary
of commonly used datasets in four categories, including
citation networks, co-purchase networks, social networks,
and bio-chemical graphs. A statistic of these datasets is
given in Table 8.
Citation Networks
In citation networks, nodes often represent the published
papers and/or authors, while edges denote the relationship between nodes, such as citation, authorship, and coauthorship. The features of each node usually contain the
context of the papers or authors, and the labels denote
the ﬁelds of study for each paper or author. Speciﬁcally,
in Cora, Citeseer and Pubmed , the nodes are papers,
the edges are citation relationships, and the features are the
bag-of-word representation for papers. In Coauthor CS and
Coauthor Physics , the nodes are authors, the edges
are co-authorship between authors, and the features are the
keywords for each author’s papers. In Wiki-CS , nodes
represent papers about computer science, edges represent
the hyperlinks between papers, and node features are the
mean vectors of GloVe word embeddings of articles. The
deﬁnition in ogbn-arxiv is similar to Wiki-CS.
Co-purchase Networks
In the two co-purchase graphs from Amazon (Amazon
Computers and Amazon Photo ), the nodes indicate
goods, and the edges indicate that two goods are frequently
bought together. The features of each node are the bag-ofwords encoded product reviews, while the class labels are
obtained by the category of goods.
Social Networks
The social network datasets are often formed by users
and their interactions on online services or communities.
For instance, in Reddit , the data is collected from a
large online discussion forum named Reddit. The nodes are
the users post of the forum, the edges are the comments
between users, and the class labels is the communities. The
features are composed by: (1) the average embedding of
the post title, (2) the average embedding of all the post’s
comments (3) the post’s score, and (4) the number of comments made on the post. IMDB-B and IMDB-M are two
movie-collaboration datasets. Each graph includes the actors/actresses and genre information of a movie. In a graph,
nodes indicate actors/actresses, while edges indicate if them
if they play the same movie. In REDDIT-BINARY (RDT-B)
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
TABLE 7: A summary of experimental results for graph classiﬁcation in nine benchmark datasets.
GraphCL(G) 
AD-GCL 
MVGRL 
InfoGraph 
LCGNN 
TABLE 8: A summary of selected benchmark datasets. The markers “*” in the “# Classes” column indicate that it is a
multi-label classiﬁcation dataset.
# Nodes(Avg.)
# Edges (Avg.)
Cora 
 , , , , , , 
 , , , , , , 
 , , , , , , 
 , , , , , , 
 , , , 
Citeseer 
 , , , , , , 
 , , , , , , 
 , , , , , , 
 , , , , , , 
 , , 
Pubmed 
 , , , , , , 
 , , , , , , 
 , , , , , , 
 , , , , , , 
 , , , , , , 
Physics 
 , , , , , 
Wiki-CS 
 , , , , 
ogbn-arxiv 
 , , 
Copurchase
Photo 
 , , , , , , 
 , , , 
Computers 
 , , , , , , 
Reddit 
11,606,919
 , , , , , , 
 , 
IMDB-B 
 , , , , , , 
 , , 
IMDB-M 
 , , , , , , 
 , , 
RDT-B 
 , , , , , , 
RDT-M 
 , , , , 
COLLAB 
 , , , , 
Biochemical
 , , , , , , 
 , , 
MUTAG 
 , , , , , , 
 , , 
PROTEINS 
 , , , , , , 
 , , , , , 
 , , , 
PTC-MR 
 , , , 
NCI-1 
 , , , , , , 
NCI-109 
 , 
BBBP 
 , , , , , 
Tox21 
 , , , , , 
ToxCast 
 , , , , , 
SIDER 
 , , , , , 
ClinTox 
 , , , , , 
 , , , 
 , , , , 
BACE 
 , , , , , 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
TABLE 9: A summary of open-source implementations.
Github Link
You et al. 
 
Jin et al. 
 
Hu et al. 
 
MGAE 
 
GAE/VGAE 
 
SIG-VAE 
 
ARGA/ARVGA 
 
SuperGAT 
 
M3S 
 
SimP-GCN 
 
DeepWalk 
 
node2vec 
 
GraphSAGE 
 
SELAR 
 
LINE 
 
GRACE 
 
GCA 
 
GCC 
 
HeCo 
 
BGRL 
 Pytorch
SelfGNN 
 
G-BT 
 
MERIT 
 
GraphCL(G) 
 
AD-GCL 
 
JOAO 
 Automated
LCGNN 
 
DGI 
 
GIC 
 
HDGI 
 
ConCH 
 
DMGI 
 
MVGRL 
 
SUBG-CON 
 
InfoGraph 
 
SLiCE 
 
Robinson et al. 
 
EGI 
 pvI6ap5Mn&name=supplementary material
BiGI 
 
HTC 
 
MICRO-Graph 
 Msv1GP&name=supplementary material
SUGAR 
 
GPT-GNN 
 
Graph-Bert 
 
PT-DGNN 
 
GMI 
 
MVMI-FT 
 
GraphLoG 
 material
HDMI 
 
LnL-GNN 
 
GROVER 
 
and REDDIT-MULTI-5K (RDT-M) , each graph denotes
an online discussion thread. In each graph, the nodes are
the users, and the edges are the comments between users.
COLLAB is a scientiﬁc-collaboration dataset, where
each graph is a ego-network of an researcher, and the label
is the ﬁeld of this researcher.
Bio-chemical Graphs
Biochemical graphs are related to biology and chemistry
domains. The Protein-Protein Interaction (PPI) dataset
contains 24 biological graphs where nodes are proteins and
edges are the interactions between proteins. In MUTAG
 , each graph indicates a nitro compounds, and its label
denotes whether they are aromatic or heteroaromatic. PRO-
TEINS and D&D are also protein datasets, where
graphs indicate proteins, and labels represent whether they
are enzymes or non-enzymes. The PTC and PTC-
MR datasets contain a series of chemical compounds,
while the labels indicate whether they are carcinogenic for
male and female rats. NCI-1 and NCI-109 also consist
of chemical compounds, labeled as to whether they are
active to hinder the growth of human cancer cell lines. The
Open Graph Benchmark (OGB) molecule property prediction benchmark contains 8 molecule graph datasets
from different sources, including BBBP, Tox21, ToxCast,
SIDER, ClinTox, MUV, HIB, and BACE. In these datasets, the
graphs indicate different types of molecules, while the labels
express their speciﬁc properties. For detailed information of
the domains please refer to .
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
APPENDIX F
OPEN-SOURCE IMPLEMENTATIONS
We collect the implementations of graph SSL approaches
reviewed in this survey if there exists an open-source code
for this method. The hyperlinks of the source codes are
provided in Table 9.
APPENDIX G
OTHER GRAPH SSL APPLICATIONS
For zero-shot expert linking problem in the ﬁeld of expert
ﬁnding, COAD leverages a same-scale contrastive
learning framework to pre-train the expert encoder model.
SLAPS integrates a denoising node feature generation
task into a classiﬁcation model for graph structure learning.
SCRL applies a prototype-based pretext tasks for fewlabel graph learning. SDGE is a community detection
approach where the model is trained by a same-scale contrastive learning objective. Aiming to repair program from
diagnostic feedback, DrRepair pre-trains the model by
a repaired line prediction task which is learned with the
automatically corrupted programs from online dataset. C-
SWM introduces a same-scale node-level contrastive
learning strategy to train structured world models that
simulate multi-object systems. Sehanobish et al. use
a clustering-based auxiliary property classiﬁcation task to
train a GAT model and regard the learned edge weights
as the edge feature for downstream tasks learned on biological datasets including SARS-CoV-2 and COVID-19. To
learn representations for medical images, Sun et al. apply a context-based hybrid contrastive learning model that
maximizes patch- and graph- level agreement on anatomyaware graphs extracted from medical images. For federated
learning , , FedGL introduces an auxiliary
property classiﬁcation task to provide a global view for
the local training in clients. Speciﬁcally, the pseudo labels
for each node is acquired by a weighted average fusion of
clients’ prediction results in a server.