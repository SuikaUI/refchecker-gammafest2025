Shape Matching and Object Recognition using Low Distortion Correspondences
Alexander C. Berg
Tamara L. Berg
Jitendra Malik
Department of Electrical Engineering and Computer Science
U.C. Berkeley
{aberg,millert,malik}@eecs.berkeley.edu
We approach recognition in the framework of deformable
shape matching, relying on a new algorithm for ﬁnding correspondences between feature points. This algorithm sets
up correspondence as an integer quadratic programming
problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors
as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers,
and thus enables matching of exemplars to query images
in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically
a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is
then handled in a nearest neighbor framework where the
distance between exemplar and query is the matching cost
between corresponding points.
We show results on two
datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus
and Perona), an extremely challenging dataset with large
intraclass variation. Our approach yields a 48% correct
classiﬁcation rate, compared to Fei-Fei et al’s 16%. We also
show results for localizing frontal and proﬁle faces that are
comparable to special purpose approaches tuned to faces.
1. Introduction
Our thesis is that recognizing object categories, be they
ﬁsh or bicycles, is fundamentally a problem of deformable
shape matching. Back in the 1970s, at least three different research groups working in different communities initiated such an approach: in computer vision, Fischler and
Elschlager , in statistical image analysis, Grenander
( and earlier), and in neural networks, von der Malsburg
( and earlier). The core idea that related but not identical shapes can be deformed into alignment using simple coordinate transformations dates even further back, to D’Arcy
Thompson, in the 1910’s with, On Growth and Form .
The basic subroutine in deformable matching takes as
input an image with an unknown object (shape) and compares it to a model by: solving the correspondence problem between the model and the object, using the correspondences to estimate and perform an aligning transformation
and computing a similarity based on both the aligning transform and the residual difference after applying the aligning transformation. This subroutine can be used for object
recognition by using stored exemplars for different object
categories as models, possibly with multiple exemplars for
different 2D aspects of a 3D object.
Practically speaking, the most difﬁcult step is the correspondence problem: how do we algorithmically determine
which points on two shapes correspond? The correspondence problem in this setting is more difﬁcult than in the
setting of binocular stereopsis, for a number of reasons:
1. Intra-category variation: the aligning transform between instances of a category is not a simple parameterized transform. It is reasonable to assume that the
mapping is a smooth, but it may be difﬁcult to characterize by a small number of parameters as in a rigid or
afﬁne transform.
2. Occlusion and clutter: while we may assume that the
stored prototype shapes are present in a clean, isolated
version, the shape that we have to recognize in an image is in the context of multiple other objects, possibly
occluding each other.
3. 3D pose changes: since the stored exemplars represent
multiple 2D views of a 3D object, we could have variation in image appearance which is purely pose-related,
the 3D shapes could be identical
The principal contribution of this paper is a novel algorithm for solving the correspondence problem for shape
We represent shape by a set of points sampled from contours on the shape. Typically 50-100 pixel locations sampled from the output of an edge detector are used; as we use
more samples we get better approximations. Note that there
is nothing special about these points – they are not required
to be keypoints such as those found using a Harris/Forstner
Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)
1063-6919/05 $20.00 © 2005 IEEE
type of operator or scale-space extrema of a Laplacian of
Gaussian operator, such as used by Lowe .
We exploit three kinds of constraints to solve the correspondence problem between shapes:
1. Corresponding points on the two shapes should have
similar local descriptors.
There are several choices
here: SIFT , Shape contexts , and Geometric
blur . We use geometric blur.
2. Minimizing geometric distortion: If i and j are points
on the model corresponding to i′ and j′ respectively,
then the vector from i to j, ⃗rij should be consistent
with the vector from i′ to j′, ⃗ri′j′. If the transformation
from one shape to another is a translation accompanied
by pure scaling, then these vectors must be scalar multiples. If the transformation is a pure Euclidean motion, then the lengths must be preserved. Etc.
3. Smoothness of the transformation from one shape to
the other. This enables us to interpolate the transformation to the entire shape, given just the knowledge of
the correspondences for a subset of the sample points.
We use regularized thin plate splines to characterize
the transformations.
The similarity of point descriptors and the geometric
distortion is encoded in a cost function deﬁned over the
space of correspondences. We purposely construct this to
be an integer quadratic programming problem (cf. Maciel
and Costeira ) and solve it using fast-approximate techniques.1
We address two object recognition problems, multiclass
recognition and face detection. In the multiple object class
recognition problem, given an image of an object we must
identify the class of the object and ﬁnd a correspondence
with an exemplar.
We use the Caltech 101 object class
dataset consisting of images from 101 classes of objects:
from accordion to kangaroo to yin-yang, available at .
This dataset includes signiﬁcant intra class variation, a wide
variety of classes, and clutter. On average we achieve 48%
accuracy on object classiﬁcation with quite good localization on the correctly classiﬁed objects. This compares favorably with the state of the art of 16% from .
We also consider face detection for large faces, suitable
for face recognition experiments. Here the task is to detect
and localize a number of faces in an image. The face dataset
we use is sampled from the very large dataset used in 
consisting of news photographs collected from yahoo.com.
With only 20 exemplar faces our generic system provides a
ROC curve with slightly better generalization, and slightly
worse false detection rate than the quite effective specialized face detector of Mikolajczyk used in .
1It is worth noting that this formulation is amenable to various probabilistic models, maximum likelihood estimation for a product of Gaussians
among others, but we do not address this further in this paper.
2. Related Work
There have been several approaches to shape recognition based on spatial conﬁgurations of a small number of
keypoints or landmarks. In geometric hashing , these
conﬁgurations are used to vote for a model without explicitly solving for correspondences. Amit et al. train decision trees for recognition by learning discriminative spatial
conﬁgurations of keypoints. Leung et al. , Schmid and
Mohr , and Lowe additionally use gray level information at the keypoints to provide greater discriminative
power. Lowe’s SIFT descriptor has been shown in various
studies e.g. to perform very well particularly at tasks
where one is looking for identical point features. Recent
work extends this approach to category recognition ,
and to three-dimensional objects .
It should be noted that not all objects have distinguished
key points (think of a circle for instance), and using key
points alone sacriﬁces the shape information available in
smooth portions of object contours. Approaches based on
extracting edge points are, in our opinion, more universally
applicable. Huttenlocher et al. developed methods based on
the Hausdorff distance . A drawback for our purposes is
that the method does not return correspondences. Methods
based on Distance Transforms, such as , are similar in
spirit and behavior in practice. Work based on shape contexts is indeed aimed at ﬁrst ﬁnding correspondences 
and is close to the spirit of this work. Another approach
is the non-rigid point matching of based on thin plate
splines and “softassign”.
One can do without extracting either keypoints or edge
points: Ullman et al propose using intermediate complexity
features, a collection of image patches, .
For faces and cars the class speciﬁc detectors of have been very successful. These techniques use simple local features, roughly based on image gradients, and a
cascade of classiﬁers for efﬁciency. Recent work on sharing
features has extended this to multiclass problems.
3. Geometric Blur Descriptor
Figure 1. A sparse signal S (a.) and the geometric blur
of S around the feature point marked in red (b.) We only
sample the geometric blur of a signal at small number of
locations {si}, indicated in (b.)
Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)
1063-6919/05 $20.00 © 2005 IEEE
Figure 2. Two images (a. and c.) and four oriented edge
channels derived from the images using the boundary detector of (b. and d. respectively). A feature point descriptor is the concatenation of the subsampled geometric
blur descriptor at the feature point for each of the channels.
We use features based on a subsampled version of the geometric blur descriptor of . This descriptor is a smoothed
version of the signal around a feature point, blurred by
a spatially varying kernel.
The objective is to provide
discriminative information by using an extended patch of
signal, while providing robustness to geometric distortion.
There are two design choices when using geometric blur
descriptors: the signal, and the blur. The signal should be
sparse and indicate the presence of some interesting feature
such as an edge in the image. The blur is determined by the
nature of geometric distortion expected.
The experiments in this paper use one of two types of
sparse channels from which to compute geometric blur descriptors: the oriented boundary detector output of or
oriented edge energy computed using quadrature pairs, following . See Figure 2 for an example of the former. In
each case the edge detector is used to produce four channels
of oriented edge responses.
Throughout we use a spatially varying Gaussian kernel
to compute geometric blur. Given one of the oriented channels discussed above as the signal, S, we compute blurred
versions, Sd = S ∗Gd, by convolving with a Gaussian of
standard deviation d. The geometric blur descriptor around
location x0 is then
Bx0(x) = Sα|x|+β(x0 −x)
Where α and β are constants that determine the amount
of blur. The intuition is that under an afﬁne transform that
ﬁxes a feature point, the distance a piece of the signal moves
is linearly proportional to the distance that piece was from
the feature point.
In practice the geometric blur of a signal is usually rather
smooth far from a feature point, we take advantage of this
by subsampling the geometric blur, as shown in ﬁgure 1. We
sample Bxo(x) at a sparse set of points x = si as shown in
ﬁgure 1, so we need only compute Sd for a few distinct values of d = α|si| + β. Since the Gaussian is a separable kernel and we can subsample the signal for larger standard deviations, extracting geometric blur descriptors is quite fast,
taking less than a second per image in our experiments.
The feature descriptor at a point is the concatenation of
the subsampled geometric blur descriptor computed at that
point in each of the channels. We compare geometric blur
descriptors using (L2) normalized correlation.
4. Geometric Distortion Costs
We consider correspondences between feature points
{pi} in model image P and {qj} in image Q. A correspondence is a mapping σ indicating that pi corresponds to
qσ(i). To reduce notational clutter we will sometimes abbreviate σ(i) as i′, so σ maps pi to qi′.
The quality of a correspondence is measured in two
ways: how similar feature points are to their corresponding feature points, and how much the spatial arrangement
of the feature points is changed. We refer to the former as
the match quality, and the later as the distortion of a correspondence.
We express the problem of ﬁnding a good correspondence as minimization of a cost function deﬁned over correspondences. This cost function has a term for the match
quality and for the geometric distortion of a correspondence: cost(σ) = ωmCmatch(σ) + ωdCdistortion(σ)
Where constants ωm and ωd weigh the two terms. The
match cost for a correspondence is:
Cmatch(σ) =
Where c(i, j) is the cost of matching i to j in a correspondence. We use the negative of the correlation between
the feature descriptors at i and j as c(i, j).
We use a distortion measure computed over pairs of
model points in an image. This will allow the cost minimization to be expressed as an integer quadratic programming problem.
Cdistortion(σ) =
H(i, i′, j, j′)
Where H(i, j, k, l) is the distortion cost of mapping
model points i and j to k to l respectively. While there are a
wide variety of possible distortion measures, including the
possibility of using point descriptors and other features, in
addition to location, we concentrate on geometric distortion
and restrict ourselves to measures based on the two offset
vectors rij = pj −pi and si′j′ = qj′ −qi′.
Cdistortion(σ) =
distortion(rij, si′j′)
Our distortion cost is made up of two components:
Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)
1063-6919/05 $20.00 © 2005 IEEE
Figure 3. An exemplar with a subset of feature points marked (left), the novel “probe” image with all feature points in white, and
the feature points found to correspond with the exemplar feature points marked in corresponding colors (left center), the exemplar
with all its feature points marked in color, coded by location in the image (right center), and the probe with the exemplar feature
points mapped by a thin plate spline transform based on the correspondences, again colored by position in the exemplar (far right).
See Figure 7 for more examples
Cdistortion(σ) =
γda(σ) + (1 −γ)dl(σ)
|rij| + βd
 arcsin
si′j′ × rij
|si′j′||rij|
dl(σ) = |si′j′| −|rij|
(|rij| + µd)
where da penalizes the change in direction, and dl penalizes change in length. A correspondence σ resulting from
pure scale and translation will result in da(σ) = 0, while
σ resulting from pure translation and rotation will result in
dl(σ) = 0. The constants αd, βd, µd, are all terms allowing
slightly more ﬂexibility for nearby points in order to deal
with local “noise” factors such as sampling, localization,
etc. They should be set relative to the scale of these local phenomena. The constant γ weighs the angle distortion
term against the length distortion term.
Outliers Each point pi, in P, is mapped to a qσ(i), in Q.
This mapping automatically allows outliers in Q as it is not
necessarily surjective – points qj may not be the image any
point pi under σ. We introduce an additional point qnull and
use σ(i) = null to allow a point pi to be an outlier. We limit
the number of points pi which can be assigned to qnull, thus
allowing for outliers in both P and Q.
5. Correspondence Algorithm
Finding an assignment to minimize a cost function described by the terms in Equations 3 and 2 above can be
written as an Integer Quadratic Programming (IQP) problem.
H(a, b)xaxb +
Where the binary indicator variable x has entries xa, that
if 1, indicate σ(ai) = aj.
We then have H(a, b) =
H(ai, aj, bi, bj), and c(a) = c(ai, aj) from Equations 3
We constrain x to represent an assignment. Write xij in
place of xaiaj. We require 
j xij = 1 for each i. Futhermore if we allow outliers as discussed in Section 4, then we
i xinull ≤k, where k is the maximum number of
outliers allowed. Using outliers does not increase the cost
in our problems, so this is equivalent to 
i xinull = k.
Each of these linear constraints are encoded in one row of
A and an entry of b. Replacing H with a matrix having
entries Hab = H(a, b) and c with a vector having entries
ca = c(a). We can now write the IQP in matrix form:
min cost(x) =x′Hx + c′x subject to,
x ∈{0, 1}n
5.1. Approximation
Integer Quadratic Programming is NP-Complete, however speciﬁc instances may be easy to solve. We follow a
two step process that results in good solutions to our problem. We ﬁrst ﬁnd the minimum of a linear bounding problem, an approximation to the quadratic problem, then follow
local gradient descent to ﬁnd a locally minimal assignment.
Although we do not necessarily ﬁnd global minima of the
cost function in practice the results are quite good.
We deﬁne a linear objective function over assignments
that is a lower bound for our cost function in two steps. First
compute qa = min 
b Habxb. Note that from here on we
will omit writing the constraints Ax = b and x ∈{0, 1}n
for brevity.
If xa represents σ(i) = j then qa is a lower bound for
the cost contributed to any assignment by using σ(i) = j.
Now we have L(x) = 
a(qa + ca)xa as a lower bound for
cost(x) from Equation 8. This construction follows ,
and is a standard bound for a quadratic program. Of note is
the operational similarity to geometric hashing.
The equations for qa and L are both integer linear programming problems, but since the vertices of the constraint
polytopes lie only on integer coordinates, they can be relaxed to linear programming problems without changing the
optima, and solved easily. In fact due to the structure of the
problems in our setup they can be solved explicitly by construction. If n is the length of x, each problem takes O(n)
operations with a very small constant. Computing qa for
a = 1 . . . n requires O(n2) time.
Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)
1063-6919/05 $20.00 © 2005 IEEE
We then perform gradient descent changing up to two elements of the assignment at each step. This takes O(n2) operations per step, and usually requires a very small number
of steps (we put an upper bound on the number of steps). In
practice we can solve problems with m = 50 and n = 2550,
50 possible matches for each of 50 model points with outliers, in less than 5 seconds.
6. Correspondence results
Given a model image P of an object, and a target image
Q, possibly containing an instance of a similar object we
ﬁnd a correspondence between the images as follows:
1. Extract sparse oriented edge maps from each image.
2. Compute features based on geometric blur descriptors
at locations with high edge energy.
3. Allow each of m feature points from P to potentially
match any of the k most similar points in Q based on
feature similarity and or proximity.
4. Construct cost matrices H and c as in Section 4.
5. Approximate the resulting Binary Quadratic Optimization to obtain a correspondence. Store the cost of the
correspondence as well.
6. Extend the correpondence on m points to a smooth
map using a regularized thin plate spline .
See Figures 3 and 7 for a number of examples. In the leftmost column of the ﬁgures is the image, P, shown with m
points marked in color. In the middle left column is the
target image Q with the corresponding points found using
our algorithm. A regularized thin plate spline is ﬁt to this
correspondence to map the full set of feature points on the
object in P, shown in the middle right column, to the target, as shown on the far right column. Corresponding points
are colored the same and points are colored based on their
position (or corresponding position) in P – in P colors are
assigned in uniform diagonal stripes, the distortion of these
striped in the far right column of the ﬁgure gives some idea
of the distortion in the correspondence.
7. Recognition Experiments
Our recognition framework is based on nearest neighbors.
Preprocessing: For each object class we store a number of exemplars, possibly replicated at multiple scales, and
compute features for all of the exemplars.
Indexing: Extract features from a query image. For each
feature point in an exemplar, ﬁnd the best matching feature
point in the query based on normalized correlation of the
geometric blur descriptors. The mean of these best correlations is the similarity of the exemplar to the query. We
form a shortlist of the exemplars with highest similarity to
the query image.
Correspondence: Find a correspondence from each exemplar in the shortlist to the query as described abive. Pick
the exemplar with the least cost.
We apply our technique to two different data sets, the
Caltech set of 101 object categories (available here ) and
a collection of news photographs containing faces gathered
from yahoo.com (provided by the authors of ). In the
experiments that follow, we utilize the same parameters for
both datasets except for those speciﬁcally mentioned.
For all images edges are extracted at four orientations
and a ﬁxed scale. For the Caltech dataset where signiﬁcant
texture and clutter are present, we use the boundary detector
of at a scale of 2% of the image diagonal. With the
face dataset, a quadrature pair of even and odd symmetric
gaussian derivatives sufﬁces. We use a scale of σ = 2 pixels
and elongate the ﬁlter by a factor of 4 in the direction of the
putative edge orientation.
Geometric blur features are computed at 400 points sampled randomly on the image with the blur pattern shown in
Figure 1. We use a maximum radius of 50 pixels (40 for
faces), and blur parameters α = 0.5 and β = 1.
For correspondence we use 50 (40 for faces) points, sampled randomly on edge points, in the correspondence problem. Each point is allowed to match to any of the most similar 40 points on the query image based on feature similarity.
In addition for the caltech 101 dataset we use γ = 0.9 allowing correspondences with signiﬁcant variation in scale,
while for the faces dataset we handle scale variation partly
by repeating exemplars at multiple scales and use γ = 0.5.
# Exemplars in the Shortlist
% Probes with a Correct Retrieval
Retrieval Rate vs Shortlist Length
# Exemplars in the Shortlist
% Probes with a Correct Retrieval
Retrieval Rate vs Shortlist Length
Figure 4. For a probe or query image exemplars are
ranked according to feature similarity. We plot the percentage of probes for which an exemplar of the correct class
was found in the shortlist. Here the ﬁrst exemplar is correct 41% of the time. Left Full curve. Right Curve up to
shortlist length 100 for detail.
8. Caltech 101 Results
Basic Setup: Fifteen exemplars were chosen randomly
from each of the 101 object classes and the background
class, yeilding a total 1530 exemplars. For each class, we
select up to 50 testing images, or “probes” excluding those
used as exemplars.
Results for each class are weighted
evenly so there is no bias toward classes with more images.
Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)
1063-6919/05 $20.00 © 2005 IEEE
The spatial support of the objects in exemplars is acquired from human labeling. The top entry in the shortlist is
corect 41% of the time. One of the top 20 entries is correct
75% of the time. (Figure 4). 2
Recognition and localization: Using each of the top ten
exemplars from the shortlist we ﬁnd a good correspondence
in the probe image. We do this by ﬁrst sampling 50 locations on the exemplar object and allowing each to be
matched to its 50 best matching possibilities in the probe
with up to 15% outliers. This results in a quadratic programming problem of dimension 2550. We use a distortion
cost based mainly on the change in angle of edges between
vertices (γ = 0.9). This allows matches with relatively different scales (Figure 7 line 3). The exemplar with the lowest
distortion correspondence gives 48% correct classiﬁcation,
at the same time providing localization. A baseline experiment comparing grayscale images using SSD and 1-nearest
neighbor classiﬁcation gives 16%. At press, results from the
Caltech group are 27% using discriminative methods .
Multiscale: We compute exemplar edge responses and features at a second scale for each exemplar resulting in twice
as many exemplars. This improves shortlist performance by
1% or less, and does not change recognition performance.
This illustrates the lack of scale variation in Caltech 101.
The face dataset exhibits a large range of scale variation.
9. Face Detection Results
We apply the same technique to detecting medium to
large scale faces for possible use in face recognition experiments. The face dataset is sampled from the very large
dataset in consisting of A.P. news photographs. A set of
20 exemplar faces split between front, left, and right facing, was chosen from the database by hand, but without
care. The test set was selected randomly from the remaining images on which the face detector of found at least
one 86×86 pixels or larger face. We use the generic object recognition framework described above, but after ﬁnding the lowest cost correspondence we continue to look for
others. A comparison of the ROC curves for our detector
and that of is found in Figure 6. Our detector has an
advatage in generalization, while producing more false positives. While not up the the level of specialized face detectors, these are remarkably good results for a face detector
using 20 exemplars and a generative model for classiﬁcation, without any negative training examples.
2We note that these results are on the Caltech 101 dataset as presented
in 8, which contains some duplicates. Using the currently available dataset
 which has no duplicates the performance drops by approximately 3%
across all experiments, in this case to 38% and 72% respectively. For
the recognition results using correspondence performance drops from 48%
with duplicates to 45% without duplicates.
# Exemplars in the Shortlist
% Probes with a Correct Retrieval
Retrieval Rate vs Shortlist Length
Hand Segmentation
Automatic Segmentation
Figure 5. Illustrating automatic model segmentation:
One training image (a.) the remaining 14 training images
colors indicate how well on average feature points
match after aligning transforms to each of the other training images (c.) At lower right, the percentage of probes
for which an exemplar of the correct class was found in the
shortlist. The blue curve shows performance with hand segmented exemplars, the red Curve shows performance with
automatically segmented exemplars. For hand segmented
exemplars the ﬁrst exemplar is correct 41% of the time, for
automatically segmented exemplars 45%. (d.)
10. Automatic Model Building
In the recognition experiments above, exemplar objects
were hand segmented from their backgrounds.
show how this can be automated by ﬁnding the repetitive
aspects of objects in the example images. Ideally this would
be computed for all images simultaneously. We show that
in many cases it is sufﬁcient to ﬁnd the similar parts in pairs
of images independantly.
Starting with a set of example images {Ii} from an object class ﬁnd the support of the object in an image Ii0 as
follows. For each image Ij where j ̸= i0 : 1) ﬁnd a correspondence from Ii0 to Ij. 3 2) Use a regularized thin plate
spline to map all of the feature points in Ii0 to Ij. 3) Foreach mapped feature from Ii0, the quality of the match is the
similarity to the best matching nearby feature in Ij. The
median quality of match for a feature is the measure of how
common that feature is in the training images.
Feature points with median quality within 90% of the
best for that image are considered part of the object. Repeating the recognition experiments in Section 8, the shortlist
accuracy improves by 1-4% (Fig. 5). While the estimated
support is usually not perfect, recognition performance is
similar to that using hand segmented images, 48%.
The learned models of support reﬂect a region of the image that is consistent across training images, as opposed to
3Here we allow 40% outliers instead of 15% as used in the recognition
experiments.
Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)
1063-6919/05 $20.00 © 2005 IEEE
Detections (out of 117)
False Positives
Mikolajczyk’s Face Detector
Figure 6. Left ROC curves for our face detector using 20
exemplar images of faces (split between frontal and proﬁle)
and the detector of Mikolajczyk (similar to ) evaluated on a dataset of ap news photos. Mikolajczyk’s detector
has proven to be effective on this dataset . Our detector
works by simply ﬁnding sets of feature points in an image
that have a good correspondence, based on distortion cost,
to an exemplar. Good correspondences allow detection and
localization of faces using a simple generative model, no
negative examples were used. Right Detections from our
face detector marked with rectangles.
individual discriminative features. For instance the cheek
on a face is not by itself discriminative for faces, but when
considering faces transformed into alignment the cheek is
usually consistent. More at www.cs.berkeley.edu/˜aberg
Acknowledgements We thank Charless Fowlkes for mentioning the work of Maciel and Costeira, David Forsyth for
excellent discussion on quadratic programming, and David
Martin and Charless Fowlkes for their boundary detection
code. ONR N00014-01-1-0890, NSF ITR IIS-00-85864.