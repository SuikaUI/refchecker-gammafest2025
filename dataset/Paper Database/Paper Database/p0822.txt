The Annals of Statistics
2009, Vol. 37, No. 4, 1705–1732
DOI: 10.1214/08-AOS620
© Institute of Mathematical Statistics, 2009
SIMULTANEOUS ANALYSIS OF LASSO AND
DANTZIG SELECTOR1
BY PETER J. BICKEL, YA’ACOV RITOV AND ALEXANDRE B. TSYBAKOV
University of California at Berkeley, The Hebrew University and
Université Paris VI and CREST
We show that, under a sparsity scenario, the Lasso estimator and the
Dantzig selector exhibit similar behavior. For both methods, we derive, in parallel, oracle inequalities for the prediction risk in the general nonparametric
regression model, as well as bounds on the ℓp estimation loss for 1 ≤p ≤2
in the linear model when the number of variables can be much larger than the
sample size.
1. Introduction.
During the last few years, a great deal of attention has been
focused on the ℓ1 penalized least squares (Lasso) estimator of parameters in highdimensional linear regression when the number of variables can be much larger
than the sample size and . Quite recently, Candes
and Tao have proposed a new estimate for such linear models, the Dantzig selector, for which they establish optimal ℓ2 rate properties under a sparsity scenario;
that is, when the number of nonzero components of the true vector of parameters
Lasso estimators have also been studied in the nonparametric regression setup
 and . In particular, Bunea, Tsybakov and Wegkamp obtain sparsity oracle inequalities for the prediction loss in this context and point out
the implications for minimax estimation in classical nonparametric regression settings, as well as for the problem of aggregation of estimators. An analog of Lasso
for density estimation with similar properties (SPADES) is proposed in . Modiﬁed versions of Lasso estimators (nonquadratic terms and/or penalties slightly
different from ℓ1) for nonparametric regression with random design are suggested
and studied under prediction loss in and . Sparsity oracle inequalities for
the Dantzig selector with random design are obtained in . In linear ﬁxed design regression, Meinshausen and Yu establish a bound on the ℓ2 loss for the
coefﬁcients of Lasso that is quite different from the bound on the same loss for the
Dantzig selector proven in .
The main message of this paper is that, under a sparsity scenario, the Lasso
and the Dantzig selector exhibit similar behavior, both for linear regression and
Received August 2007; revised April 2008.
1Supported in part by NSF Grant DMS-06-05236, ISF grant, France-Berkeley Fund, the Grant
ANR-06-BLAN-0194 and the European Network of Excellence PASCAL.
AMS 2000 subject classiﬁcations. Primary 60K35, 62G08; secondary 62C20, 62G05, 62G20.
Key words and phrases. Linear models, model selection, nonparametric statistics.
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
for nonparametric regression models, for ℓ2 prediction loss and for ℓp loss in the
coefﬁcients for 1 ≤p ≤2. All the results of the paper are nonasymptotic.
Let us specialize to the case of linear regression with many covariates,
y = Xβ + w, where X is the n × M deterministic design matrix, with M possibly
much larger than n, and w is a vector of i.i.d. standard normal random variables.
This is the situation considered most recently by Candes and Tao and Meinshausen and Yu . Here, sparsity speciﬁes that the high-dimensional vector β
has coefﬁcients that are mostly 0.
We develop general tools to study these two estimators in parallel. For the ﬁxed
design Gaussian regression model, we recover, as particular cases, sparsity oracle
inequalities for the Lasso, as in Bunea, Tsybakov and Wegkamp , and ℓ2 bounds
for the coefﬁcients of Dantzig selector, as in Candes and Tao . This is obtained
as a consequence of our more general results, which are the following:
• In the nonparametric regression model, we prove sparsity oracle inequalities for
the Dantzig selector; that is, bounds on the prediction loss in terms of the best
possible (oracle) approximation under the sparsity constraint.
• Similar sparsity oracle inequalities are proved for the Lasso in the nonparametric
regression model, and this is done under more general assumptions on the design
matrix than in .
• We prove that, for nonparametric regression, the Lasso and the Dantzig selector
are approximately equivalent in terms of the prediction loss.
• We develop geometrical assumptions that are considerably weaker than those of
Candes and Tao for the Dantzig selector and Bunea, Tsybakov and Wegkamp
 for the Lasso. In the context of linear regression where the number of variables is possibly much larger than the sample size, these assumptions imply the
result of for the ℓ2 loss and generalize it to ℓp loss 1 ≤p ≤2 and to prediction loss. Our bounds for the Lasso differ from those for Dantzig selector only
in numerical constants.
We begin, in the next section, by deﬁning the Lasso and Dantzig procedures and
the notation. In Section 3, we present our key geometric assumptions. Some sufﬁcient conditions for these assumptions are given in Section 4, where they are also
compared to those of and , as well as to ones appearing in and . We
note a weakness of our assumptions, and, hence, of those in the papers we cited,
and we discuss a way of slightly remedying them. Sections 5 and 6 give some
equivalence results and sparsity oracle inequalities for the Lasso and Dantzig estimators in the general nonparametric regression model. Section 7 focuses on the
linear regression model and includes a ﬁnal discussion. Two important technical
lemmas are given in Appendix B as well as most of the proofs.
2. Deﬁnitions and notation.
Let (Z1,Y1),...,(Zn,Yn) be a sample of independent random pairs with
Yi = f (Zi) + Wi,
i = 1,...,n,
LASSO AND DANTZIG SELECTOR
where f :Z →R is an unknown regression function to be estimated, Z is a Borel
subset of Rd, the Zi’s are ﬁxed elements in Z and the regression errors Wi are
Gaussian. Let FM = {f1,...,fM} be a ﬁnite dictionary of functions fj :Z →R,
j = 1,...,M. We assume throughout that M ≥2.
Depending on the statistical targets, the dictionary FM can contain qualitatively
different parts. For instance, it can be a collection of basis functions used to approximate f in the nonparametric regression model (e.g., wavelets, splines with
ﬁxed knots, step functions). Another example is related to the aggregation problem, where the fj are estimators arising from M different methods. They can also
correspond to M different values of the tuning parameter of the same method.
Without much loss of generality, these estimators fj are treated as ﬁxed functions.
The results are viewed as being conditioned on the sample that the fj are based
The selection of the dictionary can be very important to make the estimation
of f possible. We assume implicitly that f can be well approximated by a member
of the span of FM. However, this is not enough. In this paper, we have in mind the
situation where M ≫n, and f can be estimated reasonably only because it can
approximated by a linear combination of a small number of members of FM, or, in
other words, it has a sparse approximation in the span of FM. But, when sparsity
is an issue, equivalent bases can have different properties. A function that has a
sparse representation in one basis may not have it in another, even if both of them
span the same linear space.
Consider the matrix X = (fj(Zi))i,j, i = 1,...,n, j = 1,...,M and the vectors y = (Y1,...,Yn)T, f = (f (Z1),...,f (Zn))T, w = (W1,...,Wn)T. With the
y = f + w,
we will write |x|p for the ℓp norm of x ∈RM, 1 ≤p ≤∞. The notation ∥· ∥n
stands for the empirical norm
for any g :Z →R. We suppose that ∥fj∥n ̸= 0, j = 1,...,M. Set
fmax = max
1≤j≤M ∥fj∥n,
fmin = min
1≤j≤M ∥fj∥n.
For any β = (β1,...,βM) ∈RM, deﬁne fβ = M
j=1 βjfj or, explicitly, fβ(z) =
j=1 βjfj(z) and fβ = Xβ. The estimates we consider are all of the form f ˜β(·),
where ˜β is data determined. Since we consider mainly sparse vectors ˜β, it will be
convenient to deﬁne the following. Let
I{βj̸=0} = |J(β)|
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
denote the number of nonzero coordinates of β, where I{·} denotes the indicator
function J(β) = {j ∈{1,...,M}:βj ̸= 0} and |J| denotes the cardinality of J.
The value M(β) characterizes the sparsity of the vector β. The smaller M(β), the
“sparser” β. For a vector δ ∈RM and a subset J ⊂{1,...,M}, we denote by δJ
the vector in RM that has the same coordinates as δ on J and zero coordinates on
the complement J c of J.
Introduce the residual sum of squares
{Yi −fβ(Zi)}2
for all β ∈RM. Deﬁne the Lasso solution βL = (β1,L,..., βM,L) by
βL = argmin
S(β) + 2r
where r > 0 is some tuning constant, and introduce the corresponding Lasso estimator
fL(x) = fβL(x) =
βj,Lfj(z).
The criterion in (2.1) is convex in β, so that standard convex optimization procedures can be used to compute βL. We refer to and for
detailed discussion of these optimization problems and fast algorithms.
A necessary and sufﬁcient condition of the minimizer in (2.1) is that 0 belongs
to the subdifferential of the convex function β
→n−1|y −Xβ|2
2 + 2r|D1/2β|1.
This implies that the Lasso selector βL satisﬁes the constraint
nD−1/2XT(y −XβL)
where D is the diagonal matrix
D = diag{∥f1∥2
n,...,∥fM∥2
More generally, we will say that β ∈RM satisﬁes the Dantzig constraint if β belongs to the set
nD−1/2XT(y −Xβ)
The Dantzig estimator of the regression function f is based on a particular
solution of (2.3), the Dantzig selector βD, which is deﬁned as a vector having the
smallest ℓ1 norm among all β satisfying the Dantzig constraint
βD = argmin
nD−1/2XT(y −Xβ)
LASSO AND DANTZIG SELECTOR
The Dantzig estimator is deﬁned by
fD(z) = fβD(z) =
βj,Dfj(z),
where βD = (β1,D,..., βM,D) is the Dantzig selector. By the deﬁnition of Dantzig
selector, we have |βD|1 ≤|βL|1.
The Dantzig selector is computationally feasible, since it reduces to a linear
programming problem .
Finally, for any n ≥1, M ≥2, we consider the Gram matrix
fj(Zi)fj′(Zi)
and let φmax denote the maximal eigenvalue of n.
3. Restricted eigenvalue assumptions.
We now introduce the key assumptions on the Gram matrix that are needed to guarantee nice statistical properties
of the Lasso and Dantzig selectors. Under the sparsity scenario, we are typically
interested in the case where M > n, and even M ≫n. Then, the matrix n is
degenerate, which can be written as
(δTnδ)1/2
Clearly, ordinary least squares does not work in this case, since it requires positive
deﬁniteness of n; that is,
It turns out that the Lasso and Dantzig selector require much weaker assumptions.
The minimum in (3.1) can be replaced by the minimum over a restricted set of
vectors, and the norm |δ|2 in the denominator of the condition can be replaced by
the ℓ2 norm of only a part of δ.
One of the properties of both the Lasso and the Dantzig selectors is that, for the
linear regression model, the residuals δ = ˆβL −β and δ = ˆβD −β satisfy, with
probability close to 1,
0 |1 ≤c0|δJ0|1,
where J0 = J(β) is the set of nonzero coefﬁcients of the true parameter β of the
model. For the linear regression model, the vector of Dantzig residuals δ satisﬁes
(3.2) with probability close 1 if c0 = 1 and M is large [cf. (B.9) and the fact that
β of the model satisﬁes the Dantzig constraint with probability close to 1 if M is
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
large]. A similar inequality holds for the vector of Lasso residuals δ = βL −β, but
this time with c0 = 3 [cf. Corollary B.2].
Now, for example, consider the case where the elements of the Gram matrix n are close to those of a positive deﬁnite (M × M)-matrix . Denote, by
△= maxi,j |(n −)i,j|, the maximal difference between the elements of the
two matrices. Then, for any δ satisfying (3.2), we get
= δTδ + δT(n −)δ
(1 + c0)|δJ0|1
−εn(1 + c0)2|J0|.
Thus, for δ satisfying (3.2), which are the vectors that we have in mind, and for
εn|J0| small enough, the LHS of (3.3) is bounded away from 0. This means that we
have a kind of “restricted” positive deﬁniteness, which is valid only for the vectors
satisfying (3.2). This suggests the following conditions, which will sufﬁce for the
main argument of the paper. We refer to these conditions as restricted eigenvalue
(RE) assumptions.
ASSUMPTION RE(s,c0).
For some integer s such that 1 ≤s ≤M and a positive number c0, the following condition holds:
J0⊆{1,...,M},
0 |1≤c0|δJ0|1
The integer s here plays the role of an upper bound on the sparsity M(β) of a
vector of coefﬁcients β.
Note that, if Assumption RE(s,c0) is satisﬁed with c0 ≥1, then
min{|Xδ|2 :M(δ) ≤2s,δ ̸= 0} > 0.
In other words, the square submatrices of size ≤2s of the Gram matrix are necessarily positive deﬁnite. Indeed, suppose that, for some δ ̸= 0, we have simultaneously M(δ) ≤2s and Xδ = 0. Partition J(δ) in two sets J(δ) = I0 ∪I1, such that
|Ii| ≤s, i = 0,1. Without loss of generality, suppose that |δI1|1 ≤|δI0|1. Since,
clearly, |δI1|1 = |δI c
0 |1 and c0 ≥1, we have |δI c
0 |1 ≤c0|δI0|1. Hence, κ(s,c0) = 0,
a contradiction.
LASSO AND DANTZIG SELECTOR
To introduce the second assumption, we need some notation. For integers s,m
such that 1 ≤s ≤M/2 and m ≥s, s +m ≤M, a vector δ ∈RM and a set of indices
J0 ⊆{1,...,M} with |J0| ≤s; denote by J1 the subset of {1,...,M} corresponding to the m largest in absolute value coordinates of δ outside of J0, and deﬁne
△= J0 ∪J1. Clearly, J1 and J01 depend on m, but we do not indicate this in our
notation for the sake of brevity.
ASSUMPTION RE(s,m,c0).
J0⊆{1,...,M},
0 |1≤c0|δJ0|1
Note that the only difference between the two assumptions is in the denominators, and κ(s,m,c0) ≤κ(s,c0). As written, for ﬁxed n, the two assumptions are
equivalent. However, asymptotically for large n, Assumption RE(s,c0) is less restrictive than RE(s,m,c0), since the ratio κ(s,m,c0)/κ(s,c0) may tend to 0 if s
and m depend on n. For our bounds on the prediction loss and on the ℓ1 loss of the
Lasso and Dantzig estimators, we will only need Assumption RE(s,c0). Assumption RE(s,m,c0) will be required exclusively for the bounds on the ℓp loss with
Note also that Assumptions RE(s′,c0) and RE(s′,m,c0) imply Assumptions
RE(s,c0) and RE(s,m,c0), respectively, if s′ > s.
4. Discussion of the RE assumptions.
There exist several simple sufﬁcient
conditions for Assumptions RE(s,c0) and RE(s,m,c0) to hold. Here, we discuss
some of them.
For a real number 1 ≤u ≤M, we introduce the following quantities that we
will call restricted eigenvalues:
x∈RM:1≤M(x)≤u
x∈RM:1≤M(x)≤u
Denote by XJ the n × |J| submatrix of X obtained by removing from X the
columns that do not correspond to the indices in J, and, for 1 ≤m1,m2 ≤M,
introduce the following quantities called restricted correlations:
θm1,m2 = max
n|c1|2|c2|2
:I1 ∩I2 = ∅,|Ii| ≤mi,ci ∈RIi \ {0},i = 1,2
In Lemma 4.1, below, we show that a sufﬁcient condition for RE(s,c0) and
RE(s,s,c0) to hold is given, for example, by the following assumption on the
Gram matrix.
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
ASSUMPTION 1.
Assume that
φmin(2s) > c0θs,2s
for some integer 1 ≤s ≤M/2 and a constant c0 > 0.
This condition with c0 = 1 appeared in , in connection with the Dantzig selector. Assumption 1 is more general, in that we can have an arbitrary constant
c0 > 0 that will allow us to cover not only the Dantzig selector but also the Lasso
estimators and to prove oracle inequalities for the prediction loss when the model
is nonparametric.
Our second sufﬁcient condition for RE(s,c0) and RE(s,m,c0) does not need
bounds on correlations. Only bounds on the minimal and maximal eigenvalues of
“small” submatrices of the Gram matrix n are involved.
ASSUMPTION 2.
Assume that
mφmin(s + m) > c2
for some integers s,m, such that 1 ≤s ≤M/2, m ≥s and s + m ≤M, and a
constant c0 > 0.
Assumption 2 can be viewed as a weakening of the condition on φmin in .
Indeed, taking s + m = s logn (we assume, without loss of generality, that s logn
is an integer and n > 3) and assuming that φmax(·) is uniformly bounded by a
constant, we get that Assumption 2 is equivalent to
φmin(s logn) > c/logn,
where c > 0 is a constant. The corresponding, slightly stronger, assumption in 
is stated in asymptotic form, for s = sn →∞, as
φmin(sn logn) > 0.
The following two constants are useful when Assumptions 1 and 2 are considered:
κ1(s,c0) =
1 −c0θs,2s
κ2(s,m,c0) =
φmin(s + m)
mφmin(s + m)
The next lemma shows that if Assumptions 1 or 2 are satisﬁed, then the quadratic
form xTnx is positive deﬁnite on some restricted sets of vectors x. The construction of the lemma is inspired by Candes and Tao and covers, in particular, the
corresponding result in .
LASSO AND DANTZIG SELECTOR
LEMMA 4.1.
Fix an integer 1 ≤s ≤M/2 and a constant c0 > 0.
(i) Let Assumption 1 be satisﬁed. Then, Assumptions RE(s,c0) and RE(s,s,
c0) hold with κ(s,c0) = κ(s,s,c0) = κ1(s,c0). Moreover, for any subset J0 of
{1,...,M}, with cardinality |J0| ≤s, and any δ ∈RM such that
0 |1 ≤c0|δJ0|1,
√n|P01Xδ|2 ≥κ1(s,c0)|δJ01|2,
where P01 is the projector in RM on the linear span of the columns of XJ01.
(ii) Let Assumption 2 be satisﬁed. Then, Assumptions RE(s,c0) and RE(s,m,
c0) hold with κ(s,c0) = κ(s,m,c0) = κ2(s,m,c0). Moreover, for any subset J0 of
{1,...,M}, with cardinality |J0| ≤s, and any δ ∈RM such that (4.1) holds, we
√n|P01Xδ|2 ≥κ2(s,m,c0)|δJ01|2.
The proof of the lemma is given in Appendix A.
There exist other sufﬁcient conditions for Assumptions RE(s,c0) and RE(s,m,
c0) to hold. We mention here three of them implying Assumption RE(s,c0). The
ﬁrst one is the following .
ASSUMPTION 3.
For an integer s such that 1 ≤s ≤M, we have
φmin(s) > 2c0θs,1
where c0 > 0 is a constant.
To argue that Assumption 3 implies RE(s,c0), it sufﬁces to remark that
J0XTXδJ0 −2
≥φmin(s)|δJ0|2
and, if (4.1) holds,
0 |/n ≤|δJ c
≤θs,1|δJ c
0 |1|δJ0|2
Another type of assumption related to “mutual coherence” is discussed in
connection to Lasso in . We state it in two different forms, which are given
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
ASSUMPTION 4.
For an integer s such that 1 ≤s ≤M, we have
φmin(s) > 2c0θ1,1s,
where c0 > 0 is a constant.
It is easy to see that Assumption 4 implies RE(s,c0). Indeed, if (4.1) holds,
J0XTXδJ0 −2θ1,1|δJ c
0 |1|δJ0|1
≥φmin(s)|δJ0|2
2 −2c0θ1,1|δJ0|2
φmin(s) −2c0θ1,1s
If all the diagonal elements of matrix XTX/n are equal to 1 (and thus θ1,1 coincides with the mutual coherence ), then a simple sufﬁcient condition for Assumption RE(s,c0) to hold is stated as follows.
ASSUMPTION 5.
All the diagonal elements of the Gram matrix n are equal
to 1, and for an integer s, such that 1 ≤s ≤M, we have
(1 + 2c0)s ,
where c0 > 0 is a constant.
In fact, separating the diagonal and off-diagonal terms of the quadratic form, we
J0XTXδJ0/n ≥|δJ0|2
2 −θ1,1|δJ0|2
2(1 −θ1,1s).
Combining this inequality with (4.2), we see that Assumption RE(s,c0) is satisﬁed
whenever (4.3) holds.
Unfortunately, Assumption RE(s,c0) has some weakness. Let, for example, fj,
j = 1,...,2m −1, be the Haar wavelet basis on (M = 2m), and consider
Zi = i/n, i = 1,...,n. If M ≫n, then it is clear that φmin(1) = 0, since there are
functions fj on the highest resolution level whose supports (of length M−1) contain no points Zi. So, none of Assumptions 1–4 hold. A less severe, although similar, situation is when we consider step functions fj(t) = I{t<j/M} for t ∈ .
It is clear that φmin(2) = O(1/M), although sparse representation in this basis is
very natural. Intuitively, the problem arises only because we include very high resolution components. Therefore, we may try to restrict the set J0 in RE(s,c0) to
low resolution components, which is quite reasonable, because the “true” or “interesting” vectors of parameters β are often characterized by such J0. This idea is
formalized in Section 6 (cf. Corollary 6.2, see also a remark after Theorem 7.2 in
Section 7).
LASSO AND DANTZIG SELECTOR
5. Approximate equivalence.
In this section, we prove a type of approximate equivalence between the Lasso and the Dantzig selector. It is expressed as
closeness of the prediction losses ∥
n when the number of
nonzero components of the Lasso or the Dantzig selector is small as compared to
the sample size.
THEOREM 5.1.
Let Wi be independent N (0,σ 2) random variables with
σ 2 > 0. Fix n ≥1, M ≥2. Let Assumption RE(s,1) be satisﬁed with 1 ≤s ≤M.
Consider the Dantzig estimator 
fD deﬁned by (2.5)–(2.4) with
where A > 2
2, and consider the Lasso estimator 
fL deﬁned by (2.1)–(2.2) with
the same r.
If M(βL) ≤s, then, with probability at least 1 −M1−A2/8, we have
≤16A2 M(βL)σ 2
κ2(s,1) logM.
Note that the RHS of (5.1) is bounded by a product of three factors (and a
numerical constant which, unfortunately, equals at least 128). The ﬁrst factor
M(βL)σ 2/n ≤sσ 2/n corresponds to the error rate for prediction in regression
with s parameters. The two other factors, logM and f 2
max/κ2(s,1), can be regarded as a price to pay for the large number of regressors. If the Gram matrix
n equals the identity matrix (the white noise model), then there is only the logM
factor. In the general case, there is another factor f 2
max/κ2(s,1) representing the
extent to which the Gram matrix is ill-posed for estimation of sparse vectors.
We also have the following result that we state, for simplicity, under the assumption that ∥fj∥n = 1, j = 1,...,M. It gives a bound in the spirit of Theorem 5.1
but with M(βD) rather than M(βL) on the right-hand side.
THEOREM 5.2.
Let the assumptions of Theorem 5.1 hold, but with RE(s,5)
in place of RE(s,1), and let ∥fj∥n = 1, j = 1,...,M. If M(βD) ≤s, then, with
probability at least 1 −M1−A2/8, we have
n + 81A2 M(βD)σ 2
The approximate equivalence is essentially that of the rates as Theorem 5.1 exhibits. A statement free of M(β) holds for linear regression, see discussion after Theorems 7.2 and 7.3 below.
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
6. Oracle inequalities for prediction loss.
Here, we prove sparsity oracle
inequalities for the prediction loss of the Lasso and Dantzig estimators. These inequalities allow us to bound the difference between the prediction errors of the
estimators and the best sparse approximation of the regression function (by an oracle that knows the truth but is constrained by sparsity). The results of this section,
together with those of Section 5, show that the distance between the prediction
losses of the Dantzig and Lasso estimators is of the same order as the distances
between them and their oracle approximations.
A general discussion of sparsity oracle inequalities can be found in . Such
inequalities have been recently obtained for the Lasso type estimators in a number
of settings and . In particular, the regression model with ﬁxed design
that we study here is considered in . The assumptions on the Gram matrix
n in are more restrictive than ours. In those papers, either n is positive
deﬁnite, or a mutual coherence condition similar to (4.3) is imposed.
THEOREM 6.1.
Let Wi be independent N (0,σ 2) random variables with
σ 2 > 0. Fix some ε > 0 and integers n ≥1, M ≥2, 1 ≤s ≤M. Let Assumption RE(s,3+4/ε) be satisﬁed. Consider the Lasso estimator 
fL deﬁned by (2.1)–
(2.2) with
for some A > 2
2. Then, with probability at least 1 −M1−A2/8, we have
n + C(ε)f 2
κ2(s,3 + 4/ε)
where C(ε) > 0 is a constant depending only on ε.
We now state, as a corollary, a softer version of Theorem 6.1 that can be used to
eliminate the pathologies mentioned at the end of Section 4. For this purpose, we
J0 ⊂{1,...,M}:|J0| ≤s and
0 |1≤c0|δJ0|1
where γ > 0 is a constant, and set
s,γ,c0 = {β :J(β) ∈Js,γ,c0}.
In similar way, we deﬁne Js,γ,m,c0 and
s,γ,m,c0 corresponding to Assumption RE(s,m,c0).
LASSO AND DANTZIG SELECTOR
COROLLARY 6.2.
Let Wi, s and the Lasso estimator 
fL be the same as in
Theorem 6.1. Then, for all n ≥1, ε > 0, and γ > 0, with probability at least 1 −
M1−A2/8 we have
n ≤(1 + ε)
n + C(ε)f 2
s,γ,ε = {β ∈
s,γ,3+4/ε :M(β) ≤s}.
To obtain this corollary, it sufﬁces to observe that the proof of Theorem 6.1
goes through if we drop Assumption RE(s,3 + 4/ε), but we assume instead that
s,γ,3+4/ε, and we replace κ(s,3 + 4/ε) by γ .
We would like now to get a sparsity oracle inequality similar to that of Theorem 6.1 for the Dantzig estimator 
fD. We will need a mild additional assumption
on f . This is due to the fact that not every β ∈RM obeys the Dantzig constraint;
thus, we cannot assure the key relation (B.9) for all β ∈RM. One possibility would
be to prove inequality as (6.1), where the inﬁmum on the right hand side is taken
over β satisfying not only M(β) ≤s but also the Dantzig constraint. However, this
seems not to be very intuitive, since we cannot guarantee that the corresponding
fβ gives a good approximation of the unknown function f . Therefore, we choose
another approach (cf. ), in which we consider f satisfying the weak sparsity
property relative to the dictionary f1,...,fM. That is, we assume that there exist
an integer s and constant C0 < ∞such that the set
β ∈RM :M(β) ≤s,∥fβ −f ∥2
κ2(s,3 + 4/ε)M(β)
is nonempty. The second inequality in (6.2) says that the “bias” term ∥fβ −f ∥2
cannot be much larger than the “variance term” ∼f 2
maxr2κ−2M(β) [cf. (6.1)].
Weak sparsity is milder than the sparsity property in the usual sense. The latter
means that f admits the exact representation f = fβ∗, for some β∗∈RM, with
hopefully small M(β∗) = s.
PROPOSITION 6.3.
Let Wi be independent N (0,σ 2) random variables with
σ 2 > 0. Fix some ε > 0 and integers n ≥1, M ≥2. Let f obey the weak sparsity
assumption for some C0 < ∞and some s such that 1 ≤s max{C1(ε),1} ≤M,
C1(ε) = 4[(1 + ε)C0 + C(ε)]φmaxf 2
and C(ε) is the constant in Theorem 6.1. Suppose, further, that Assumption RE(s max{C1(ε),1},3 + 4/ε) is satisﬁed. Consider the Dantzig estimator
fD deﬁned by (2.5)–(2.4) with
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
2. Then, with probability at least 1 −M1−A2/8, we have
β∈RM:M(β)=s ∥fβ −f ∥2
n + C2(ε)f 2
Here, C2(ε) = 16C1(ε) + C(ε) and κ0 = κ(max(C1(ε),1)s,3 + 4/ε).
Note that the sparsity oracle inequality (6.3) is slightly weaker than the analogous inequality (6.1) for the Lasso. Here, we have infβ∈RM:M(β)=s instead of
infβ∈RM:M(β)≤s in (6.1).
7. Special case. Parametric estimation in linear regression.
In this section,
we assume that the vector of observations y = (Y1,...,Yn)T is of the form
y = Xβ∗+ w,
where X is an n × M deterministic matrix β∗∈RM and w = (W1,...,Wn)T.
We consider dimension M that can be of order n and even much larger. Then,
β∗is, in general, not uniquely deﬁned. For M > n, if (7.1) is satisﬁed for β∗= β0,
then there exists an afﬁne space U = {β∗:Xβ∗= Xβ0} of vectors satisfying (7.1).
The results of this section are valid for any β∗such that (7.1) holds. However, we
will suppose that Assumption RE(s,c0) holds with c0 ≥1 and that M(β∗) ≤s.
Then, the set U ∩{β∗:M(β∗) ≤s} reduces to a single element (cf. Remark 2 at
the end of this section). In this sense, there is a unique sparse solution of (7.1).
Our goal in this section, unlike that of the previous ones, is to estimate both Xβ∗
for the purpose of prediction and β∗itself for purpose of model selection. We will
see that meaningful results are obtained when the sparsity index M(β∗) is small.
It will be assumed throughout this section that the diagonal elements of the
Gram matrix n = XTX/n are all equal to 1 (this is equivalent to the condition
∥fj∥n = 1,j = 1,...,M, in the notation of previous sections). Then, the Lasso
estimator of β∗in (7.1) is deﬁned by
βL = argmin
2 + 2r|β|1
The correspondence between the notation here and that of the previous sections is
n = |X(β −β∗)|2
n = |X(βL −β∗)|2
The Dantzig selector for linear model (7.1) is deﬁned by
βD = argmin
LASSO AND DANTZIG SELECTOR
nXT(y −Xβ)
is the set of all β satisfying the Dantzig constraint.
We ﬁrst get bounds on the rate of convergence of Dantzig selector.
THEOREM 7.1.
Let Wi be independent N (0,σ 2) random variables with
σ 2 > 0, let all the diagonal elements of the matrix XTX/n be equal to 1 and
M(β∗) ≤s, where 1 ≤s ≤M, n ≥1, M ≥2. Let Assumption RE(s,1) be satisﬁed. Consider the Dantzig selector βD deﬁned by (7.3) with
2. Then, with probability at least 1 −M1−A2/2, we have
|βD −β∗|1 ≤
|X(βD −β∗)|2
κ2(s,1)σ 2s logM.
If Assumption RE(s,m,1) is satisﬁed, then, with the same probability as above,
simultaneously for all 1 < p ≤2, we have
|βD −β∗|p
Note that, since s ≤m, the factor in curly brackets in (7.6) is bounded by a
constant independent of s and m. Under Assumption 1 in Section 4, with c0 = 1
[which is less general than RE(s,s,1), cf. Lemma 4.1(i)], a bound of the form (7.6)
for the case p = 2 is established by Candes and Tao .
Bounds on the rate of convergence of the Lasso selector are quite similar to
those obtained in Theorem 7.1. They are given by the following result.
THEOREM 7.2.
Let Wi be independent N (0,σ 2) random variables with
σ 2 > 0. Let all the diagonal elements of the matrix XTX/n be equal to 1, and
let M(β∗) ≤s, where 1 ≤s ≤M, n ≥1, M ≥2. Let Assumption RE(s,3) be satisﬁed. Consider the Lasso estimator βL deﬁned by (7.2) with
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
2. Then, with probability at least 1 −M1−A2/8, we have
|βL −β∗|1 ≤
|X(βL −β∗)|2
κ2(s,3)σ 2s logM,
M(βL) ≤64φmax
If Assumption RE(s,m,3) is satisﬁed, then, with the same probability as above,
simultaneously for all 1 < p ≤2, we have
|βL −β∗|p
Inequalities of the form similar to (7.7) and (7.8) can be deduced from the results
of under more restrictive conditions on the Gram matrix (the mutual coherence
assumption, cf. Assumption 5 of Section 4).
Assumptions RE(s,1) and RE(s,3), respectively, can be dropped in Theorems
7.1 and 7.2 if we assume β∗∈
s,γ,c0 with c0 = 1 or c0 = 3 as appropriate. Then,
(7.4) and (7.5) or, respectively, (7.7) and (7.8) hold with κ = γ . This is analogous
to Corollary 6.2. Similarly, (7.6) and (7.10) hold with κ = γ if β∗∈
s,γ,m,c0 with
c0 = 1 or c0 = 3 as appropriate.
Observe that, combining Theorems 7.1 and 7.2, we can immediately get
bounds for the differences between Lasso and Dantzig selector |βL −βD|p
|X(βL−βD)|2
2. Such bounds have the same form as those of Theorems 7.1 and 7.2,
up to numerical constants. Another way of estimating these differences follows directly from the proof of Theorem 7.1. It sufﬁces to observe that the only property
of β∗used in that proof is the fact that β∗satisﬁes the Dantzig constraint on the
event of given probability, which is also true for the Lasso solution βL. So, we can
replace β∗by βL and s by M(βL) everywhere in Theorem 7.1. Generalizing a bit
more, we easily derive the following fact.
THEOREM 7.3.
The result of Theorem 7.1 remains valid if we replace |βD −
p by sup{|βD −β|p
,M(β) ≤s} for 1 ≤p ≤2 and |X(βD −β∗)|2
sup{|X(βD −β)|2
,M(β) ≤s}, respectively. Here,
is the set of all vectors
satisfying the Dantzig constraint.
1. Theorems 7.1 and 7.2 only give nonasymptotic upper bounds on the loss,
with some probability and under some conditions. The probability depends on M
and the conditions depend on n and M. Recall that Assumptions RE(s,c0) and
RE(s,m,c0) are imposed on the n × M matrix X. To deduce asymptotic conver-
LASSO AND DANTZIG SELECTOR
gence (as n →∞and/or as M →∞) from Theorems 7.1 and 7.2, we would need
some very strong additional properties, such as simultaneous validity of Assumption RE(s,c0) or RE(s,m,c0) (with one and the same constant κ) for inﬁnitely
many n and M.
2. Note that neither Assumption RE(s,c0) or RE(s,m,c0) implies identiﬁability of β∗in the linear model (7.1). However, the vector β∗appearing in the
statements of Theorems 7.1 and 7.2 is uniquely deﬁned, because we additionally suppose that M(β∗) ≤s and c0 ≥1. Indeed, if there exists a β′ such that
Xβ′ = Xβ∗, and M(β′) ≤s, then, in view of assumption RE(s,c0) with c0 ≥1,
we necessarily have β∗= β′ [cf. discussion following the deﬁnition of RE(s,c0)].
On the other hand, Theorem 7.3 applies to certain values of β that do not come
from the model (7.1) at all.
3. For the smallest value of A (which is A = 2
2) the constants in the bound of
Theorem 7.2 for the Lasso are larger than the corresponding numerical constants
for the Dantzig selector given in Theorem 7.1, again, for the smallest admissible
2. On the contrary, the Dantzig selector has certain defects as compared to Lasso when the model is nonparametric, as discussed in Section 6. In
particular, to obtain sparsity oracle inequalities for the Dantzig selector, we need
some restrictions on f , for example, the weak sparsity property. On the other hand,
the sparsity oracle inequality (6.1) for the Lasso is valid with no restriction on f .
4. The proofs of Theorems 7.1 and 7.2 differ mainly in the value of the tuning
constant, which is c0 = 1 in Theorem 7.1 and c0 = 3 in Theorem 7.2. Note that,
since the Lasso solution satisﬁes the Dantzig constraint, we could have obtained a
result similar to Theorem 7.2, but with less accurate numerical constants, by simply conducting the proof of Theorem 7.1 with c0 = 3. However, we act differently,
and we deduce (B.30) directly from (B.1) and not from (B.25). This is done only
for the sake of improving the constants. In fact, using (B.25) with c0 = 3 would
yield (B.30) with the doubled constant on the right-hand side.
5. For the Dantzig selector in the linear regression model and under Assumptions 1 or 2, some further improvement of constants in the ℓp bounds for the coefﬁcients can be achieved by applying the general version of Lemma 4.1 with the
projector P01 inside. We do not pursue this issue here.
6. All of our results are stated with probabilities at least 1 −M1−A2/2 or 1 −
M1−A2/8. These are reasonable (but not the most accurate) lower bounds on the
probabilities P(B) and P(A), respectively. We have chosen them for readability.
Inspection of (B.4) shows that they can be reﬁned to 1 −2M
(A√logM) and
(A√logM/2), respectively, where
(·) is the standard normal c.d.f.
APPENDIX A
PROOF OF LEMMA 4.1.
Consider a partition J c
0 into subsets of size m,
with the last subset of size ≤m: J c
k=1 Jk, where K ≥1, |Jk| = m for
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
k = 1,...,K −1 and |JK| ≤m, such that Jk is the set of indices corresponding to m largest in absolute value coordinates of δ outside k−1
j=1 Jj (for k < K)
and JK is the remaining subset. We have
|P01Xδ|2 ≥|P01XδJ01|2 −
= |XδJ01|2 −
≥|XδJ01|2 −
|P01XδJk|2.
We will prove ﬁrst part (ii) of the lemma. Since for k ≥1 the vector δJk has only m
nonzero components, we obtain
√n|P01XδJk|2 ≤1
√n|XδJk|2 ≤
φmax(m)|δJk|2.
Next, as in , we observe that |δJk+1|2 ≤|δJk|1/√m, k = 1,...,K −1. Therefore,
√m ≤c0|δJ0|1
m|δJ0|2 ≤c0
where we used (4.1). From (A.1)–(A.3), we ﬁnd
√n|Xδ|2 ≥1
√n|XδJ01|2 −c0
φmin(s + m) −c0
which proves part (ii) of the lemma.
The proof of part (i) is analogous. The only difference is that we replace, in the
above argument, m by s, and instead of (A.2), we use the bound (cf. )
√n|P01XδJk|2 ≤
√φmin(2s)|δJk|2.
APPENDIX B: TWO LEMMAS AND THE PROOFS OF THE RESULTS
LEMMA B.1.
Fix M ≥2 and n ≥1. Let Wi be independent N (0,σ 2) random
variables with σ 2 > 0, and let 
fL be the Lasso estimator deﬁned by (2.2) with
LASSO AND DANTZIG SELECTOR
for some A > 2
2. Then, with probability at least 1 −M1−A2/8, we have, simultaneously for all β ∈RM,
∥fj∥n|βj,L −βj|
≤∥fβ −f ∥2
∥fj∥n|βj,L −βj|
≤∥fβ −f ∥2
∥fj∥2n|βj,L −βj|2,
nXT(f −XβL)
≤3rfmax/2.
Furthermore, with the same probability,
M(βL) ≤4φmaxf −2
where φmax denotes the maximal eigenvalue of the matrix XTX/n.
PROOF OF LEMMA B.1.
The result (B.1) is essentially Lemma 1 from . For
completeness, we give its proof. Set rn,j = r∥fj∥n. By deﬁnition,
S(βL) + 2
rn,j|βj,L| ≤S(β) + 2
for all β ∈RM, which is equivalent to
rn,j|βj,L|
≤∥fβ −f ∥2
rn,j|βj| + 2
fL −fβ)(Zi).
Deﬁne the random variables Vj = n−1 n
i=1 fj(Zi)Wi, 1 ≤j ≤M, and the event
{2|Vj| ≤rn,j}.
Using an elementary bound on the tails of Gaussian distribution, we ﬁnd that the
probability of the complementary event Ac satisﬁes
√n|Vj| > √nrn,j/2
|η| ≥r√n/(2σ)
= M1−A2/8,
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
where η ∼N (0,1). On the event A we have
n ≤∥fβ −f ∥2
rn,j|βj,L −βj| +
2rn,j|βj| −
2rn,j|βj,L|.
Adding the term M
j=1 rn,j|βj,L −βj| to both sides of this inequality yields, on A,
rn,j|βj,L −βj|
≤∥fβ −f ∥2
rn,j(|βj,L −βj| + |βj| −|βj,L|).
Now, |βj,L −βj| + |βj| −|βj,L| = 0 for j /∈J(β), so that, on A, we get (B.1).
To prove (B.2) it sufﬁces to note that, on A, we have
Now, y = f + w, and (B.2) follows from (2.3) and (B.5).
We ﬁnally prove (B.3). The necessary and sufﬁcient condition for βL to be the
Lasso solution can be written in the form
(j)(y −XβL) = r∥fj∥n sign(βj,L)
if βj,L ̸= 0,
(j)(y −XβL)
if βj,L = 0,
where x(j) denotes the jth column of X, j = 1,...,M. Next, (B.5) yields that,
on A, we have
≤r∥fj∥n/2,
j = 1,...,M.
Combining (B.6) and (B.7), we get
(j)(f −XβL)
if βj,L ̸= 0.
Therefore,
n2 (f −XβL)TXXT(f −XβL) = 1
(j)(f −XβL)
j:βj,L̸=0
(j)(f −XβL)
= M(βL)r2∥fj∥2
minM(βL)r2/4.
LASSO AND DANTZIG SELECTOR
Since the matrices XTX/n and XXT/n have the same maximal eigenvalues,
n2 (f −XβL)TXXT(f −XβL) ≤φmax
|f −XβL|2
2 = φmax∥f −
and we deduce (B.3) from the last two displays.
COROLLARY B.2.
Let the assumptions of Lemma B.1 be satisﬁed and
∥fj∥n = 1,j = 1,...,M. Consider the linear regression model y = Xβ +w. Then,
with probability at least 1 −M1−A2/8, we have
0 |1 ≤3|δJ0|1,
where J0 = J(β) is the set of nonzero coefﬁcients of β and δ = βL −β.
Use the ﬁrst inequality in (B.1) and the fact that f = fβ for the linear
regression model.
LEMMA B.3.
Let β ∈RM satisfy the Dantzig constraint
nD−1/2XT(y −Xβ)
and set δ = βD −β, J0 = J(β). Then,
0 |1 ≤|δJ0|1.
Further, let the assumptions of Lemma B.1 be satisﬁed with A >
2. Then, with
probability of at least 1 −M1−A2/2, we have
nXT(f −XβD)
PROOF OF LEMMA B.3.
Inequality (B.9) follows immediately from the deﬁnition of Dantzig selector (cf. ). To prove (B.10), consider the event
{|Vj| ≤rn,j}.
Analogously to (B.4), P{Bc} ≤M1−A2/2. On the other hand, y = f+w, and, using
the deﬁnition of Dantzig selector, it is easy to see that (B.10) is satisﬁed on B.
PROOF OF THEOREM 5.1.
Set δ = βL −βD. We have
n|f −XβL|2
n|f −XβD|2
nδTXT(f −XβD) + 1
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
This and (B.10) yield
nXT(f −XβD)
n + 4fmaxr|δ|1 −1
where the last inequality holds with probability at least 1 −M1−A2/2. Since the
Lasso solution βL satisﬁes the Dantzig constraint, we can apply Lemma B.3 with
β = βL, which yields
0 |1 ≤|δJ0|1
with J0 = J(βL). By Assumption RE(s,1), we get
√n|Xδ|2 ≥κ|δJ0|2,
where κ = κ(s,1). Using (B.12) and (B.13), we obtain
|δ|1 ≤2|δJ0|1 ≤2M1/2(βL)|δJ0|2 ≤2M1/2(βL)
Finally, from (B.11) and (B.14), we get that, with probability at least 1−M1−A2/2,
n + 8fmaxrM1/2(βL)
maxr2M(βL)
where the RHS follows (B.2), (B.10) and another application of (B.14). This
proves one side of the inequality.
To show the other side of the bound on the difference, we act as in (B.11), up
to the inversion of roles of βL and βD, and we use (B.2). This yields that, with
probability at least 1 −M1−A2/8,
nXT(f −XβL)
n + 3fmaxr|δ|1 −1
This is analogous to (B.11). Now, paralleling the proof leading to (B.15), we obtain
maxr2M(βL)
The theorem now follows from (B.15) and (B.17).
LASSO AND DANTZIG SELECTOR
PROOF OF THEOREM 5.2.
Set, again, δ = βL −βD. We apply (B.1) with
β = βD, which yields that, with probability at least 1 −M1−A2/8,
|δ|1 ≤4|δJ0|1 + ∥
where, now, J0 = J(βD). Consider the following two cases: (i) ∥
2r|δJ0|1 and (ii) ∥
n ≤2r|δJ0|1. In case (i), inequality (B.16) with fmax = 1
immediately implies
and the theorem follows. In case (ii), we get, from (B.18), that
|δ|1 ≤6|δJ0|1
and thus |δJ c
0 |1 ≤5|δJ0|1. We can therefore apply Assumption RE(s,5), which
yields, similarly to (B.14),
|δ|1 ≤6M1/2(βD)|δJ0|2 ≤6M1/2(βD)
where κ = κ(s,5). Plugging (B.19) into (B.16) we ﬁnally get that, in case (ii),
n + 18rM1/2(βD)
n + 81r2M(βD)
PROOF OF THEOREM 6.1.
Fix an arbitrary β ∈RM with M(β) ≤s. Set δ =
D1/2(βL −β), J0 = J(β). On the event A, we get, from the ﬁrst line in (B.1), that
n + r|δ|1 ≤∥fβ −f ∥2
∥fj∥n|βj,L −βj|
= ∥fβ −f ∥2
n + 4r|δJ0|1,
and from the second line in (B.1) that
n ≤∥fβ −f ∥2
M(β)|δJ0|2.
Consider, separately, the cases where
4r|δJ0|1 ≤ε∥fβ −f ∥2
ε∥fβ −f ∥2
n < 4r|δJ0|1.
In case (B.23), the result of the theorem trivially follows from (B.21). So, we will
only consider the case (B.24). All of the subsequent inequalities are valid on the
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
event A ∩A1, where A1 is deﬁned by (B.24). On this event, we get, from (B.21),
|δ|1 ≤4(1 + 1/ε)|δJ0|1,
which implies |δJ c
0 |1 ≤(3 + 4/ε)|δJ0|1. We now use Assumption RE(s,3 + 4/ε).
This yields
n(βK −β)TD1/2XTXD1/2(βL −β)
n (βL −β)TXTX(βL −β) = f 2
where κ = κ(s,3 + 4/ε). Combining this with (B.22), we ﬁnd
n ≤∥fβ −f ∥2
n + 4rfmaxκ−1
≤∥fβ −f ∥2
n + 4rfmaxκ−1
fL −f ∥n + ∥fβ −f ∥n).
This inequality is of the same form as (A.4) in . A standard decoupling argument
as in , using inequality 2xy ≤x2/b + by2 with b > 1, x = rκ−1√M(β) and y
being either ∥
fL −f ∥n or ∥fβ −f ∥n, yields that
b −1∥fβ −f ∥2
n + 8b2f 2
(b −1)κ2 r2M(β)
Taking b = 1 + 2/ε in the last display ﬁnishes the proof of the theorem.
PROOF OF PROPOSITION 6.3.
Due to the weak sparsity assumption, there exists ¯β ∈RM with M( ¯β) ≤s such that ∥f ¯β −f ∥2
maxr2κ−2M( ¯β), where
κ = κ(s,3 + 4/ε) is the same as in Theorem 6.1. Using this together with Theorem 6.1 and (B.3), we obtain that, with probability at least 1 −M1−A2/8,
M(βL) ≤C1(ε)M( ¯β) ≤C1(ε)s.
This and Theorem 5.1 imply
n + 16C1(ε)f 2
where κ0 = κ(max(C1(ε),1)s,3 + 4/ε). Once Again, applying Theorem 6.1, we
get the result.
PROOF OF THEOREM 7.1.
Set δ = βD −β∗and J0 = J(β∗). Using Lemma B.3 with β = β∗, we get that, on the event B (i.e., with probability at least
LASSO AND DANTZIG SELECTOR
1−M1−A2/2), the following are true: (i) 1
n|XTXδ|∞≤2r, and (ii) inequality (4.1)
holds with c0 = 1. Therefore, on B we have
n|XTXδ|∞|δ|1
≤2r(|δJ0|1 + |δJ c
≤2(1 + c0)r|δJ0|1
≤2(1 + c0)r√s|δJ0|2 = 4r√s|δJ0|2
since c0 = 1. From Assumption RE(s,1), we get that
2 ≥κ2|δJ0|2
where κ = κ(s,1). This and (B.25) yield that, on B,
2 ≤16r2s/κ2,
|δJ0|2 ≤4r√s/κ2.
The ﬁrst inequality in (B.26) implies (7.5). Next, (7.4) is straightforward in view
of the second inequality in (B.26) and of the relations (with c0 = 1)
|δ|1 = |δJ0|1 + |δJ c
0 |1 ≤(1 + c0)|δJ0|1 ≤(1 + c0)√s|δJ0|2
that hold on B. It remains to prove (7.6). It is easy to see that the kth largest in
absolute value element of δJ c
0 satisﬁes |δJ c
0 |(k) ≤|δJ c
0 |1/k. Thus,
and, since (4.1) holds on B (with c0 = 1), we ﬁnd
01|2 ≤c0|δJ0|1
m ≤c0|δJ01|2
Therefore, on B,
On the other hand, it follows from (B.25) that
2 ≤4r√s|δJ01|2.
Combining this inequality with Assumption RE(s,m,1), we obtain that, on B,
|δJ01|2 ≤4r√s/κ2.
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
Recalling that c0 = 1 and applying the last inequality together with (B.28), we get
2r√s/κ22.
It remains to note that (7.6) is a direct consequence of (7.4) and (B.29). This follows from the fact that inequalities M
j=1 aj ≤b1 and M
j ≤b2 with aj ≥0
∀1 < p ≤2.
PROOF OF THEOREM 7.2.
Set δ = βL −β∗and J0 = J(β∗). Using (B.1),
where we put β = β∗, rn,j ≡r and ∥fβ −f ∥n = 0, we get that, on the event A,
2 ≤4r√s|δJ0|2
and (4.1) holds with c0 = 3 on the same event. Thus, by Assumption RE(s,3) and
the last inequality, we obtain that, on A,
2 ≤16r2s/κ2,
|δJ0|2 ≤4r√s/κ2,
where κ = κ(s,3). The ﬁrst inequality here coincides with (7.8). Next, (7.9) follows immediately from (B.3) and (7.8). To show (7.7), it sufﬁces to note that on
the event A the relations (B.27) hold with c0 = 3, to apply the second inequality
in (B.31) and to use (B.4).
Finally, the proof of (7.10) follows exactly the same lines as that of (7.6). The
only difference is that one should set c0 = 3 in (B.28) and (B.29), as well as in the
display preceding (B.28).