The Annals of Statistics
2009, Vol. 37, No. 4, 1705â€“1732
DOI: 10.1214/08-AOS620
Â© Institute of Mathematical Statistics, 2009
SIMULTANEOUS ANALYSIS OF LASSO AND
DANTZIG SELECTOR1
BY PETER J. BICKEL, YAâ€™ACOV RITOV AND ALEXANDRE B. TSYBAKOV
University of California at Berkeley, The Hebrew University and
UniversitÃ© Paris VI and CREST
We show that, under a sparsity scenario, the Lasso estimator and the
Dantzig selector exhibit similar behavior. For both methods, we derive, in parallel, oracle inequalities for the prediction risk in the general nonparametric
regression model, as well as bounds on the â„“p estimation loss for 1 â‰¤p â‰¤2
in the linear model when the number of variables can be much larger than the
sample size.
1. Introduction.
During the last few years, a great deal of attention has been
focused on the â„“1 penalized least squares (Lasso) estimator of parameters in highdimensional linear regression when the number of variables can be much larger
than the sample size and . Quite recently, Candes
and Tao have proposed a new estimate for such linear models, the Dantzig selector, for which they establish optimal â„“2 rate properties under a sparsity scenario;
that is, when the number of nonzero components of the true vector of parameters
Lasso estimators have also been studied in the nonparametric regression setup
 and . In particular, Bunea, Tsybakov and Wegkamp obtain sparsity oracle inequalities for the prediction loss in this context and point out
the implications for minimax estimation in classical nonparametric regression settings, as well as for the problem of aggregation of estimators. An analog of Lasso
for density estimation with similar properties (SPADES) is proposed in . Modiï¬ed versions of Lasso estimators (nonquadratic terms and/or penalties slightly
different from â„“1) for nonparametric regression with random design are suggested
and studied under prediction loss in and . Sparsity oracle inequalities for
the Dantzig selector with random design are obtained in . In linear ï¬xed design regression, Meinshausen and Yu establish a bound on the â„“2 loss for the
coefï¬cients of Lasso that is quite different from the bound on the same loss for the
Dantzig selector proven in .
The main message of this paper is that, under a sparsity scenario, the Lasso
and the Dantzig selector exhibit similar behavior, both for linear regression and
Received August 2007; revised April 2008.
1Supported in part by NSF Grant DMS-06-05236, ISF grant, France-Berkeley Fund, the Grant
ANR-06-BLAN-0194 and the European Network of Excellence PASCAL.
AMS 2000 subject classiï¬cations. Primary 60K35, 62G08; secondary 62C20, 62G05, 62G20.
Key words and phrases. Linear models, model selection, nonparametric statistics.
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
for nonparametric regression models, for â„“2 prediction loss and for â„“p loss in the
coefï¬cients for 1 â‰¤p â‰¤2. All the results of the paper are nonasymptotic.
Let us specialize to the case of linear regression with many covariates,
y = XÎ² + w, where X is the n Ã— M deterministic design matrix, with M possibly
much larger than n, and w is a vector of i.i.d. standard normal random variables.
This is the situation considered most recently by Candes and Tao and Meinshausen and Yu . Here, sparsity speciï¬es that the high-dimensional vector Î²
has coefï¬cients that are mostly 0.
We develop general tools to study these two estimators in parallel. For the ï¬xed
design Gaussian regression model, we recover, as particular cases, sparsity oracle
inequalities for the Lasso, as in Bunea, Tsybakov and Wegkamp , and â„“2 bounds
for the coefï¬cients of Dantzig selector, as in Candes and Tao . This is obtained
as a consequence of our more general results, which are the following:
â€¢ In the nonparametric regression model, we prove sparsity oracle inequalities for
the Dantzig selector; that is, bounds on the prediction loss in terms of the best
possible (oracle) approximation under the sparsity constraint.
â€¢ Similar sparsity oracle inequalities are proved for the Lasso in the nonparametric
regression model, and this is done under more general assumptions on the design
matrix than in .
â€¢ We prove that, for nonparametric regression, the Lasso and the Dantzig selector
are approximately equivalent in terms of the prediction loss.
â€¢ We develop geometrical assumptions that are considerably weaker than those of
Candes and Tao for the Dantzig selector and Bunea, Tsybakov and Wegkamp
 for the Lasso. In the context of linear regression where the number of variables is possibly much larger than the sample size, these assumptions imply the
result of for the â„“2 loss and generalize it to â„“p loss 1 â‰¤p â‰¤2 and to prediction loss. Our bounds for the Lasso differ from those for Dantzig selector only
in numerical constants.
We begin, in the next section, by deï¬ning the Lasso and Dantzig procedures and
the notation. In Section 3, we present our key geometric assumptions. Some sufï¬cient conditions for these assumptions are given in Section 4, where they are also
compared to those of and , as well as to ones appearing in and . We
note a weakness of our assumptions, and, hence, of those in the papers we cited,
and we discuss a way of slightly remedying them. Sections 5 and 6 give some
equivalence results and sparsity oracle inequalities for the Lasso and Dantzig estimators in the general nonparametric regression model. Section 7 focuses on the
linear regression model and includes a ï¬nal discussion. Two important technical
lemmas are given in Appendix B as well as most of the proofs.
2. Deï¬nitions and notation.
Let (Z1,Y1),...,(Zn,Yn) be a sample of independent random pairs with
Yi = f (Zi) + Wi,
i = 1,...,n,
LASSO AND DANTZIG SELECTOR
where f :Z â†’R is an unknown regression function to be estimated, Z is a Borel
subset of Rd, the Ziâ€™s are ï¬xed elements in Z and the regression errors Wi are
Gaussian. Let FM = {f1,...,fM} be a ï¬nite dictionary of functions fj :Z â†’R,
j = 1,...,M. We assume throughout that M â‰¥2.
Depending on the statistical targets, the dictionary FM can contain qualitatively
different parts. For instance, it can be a collection of basis functions used to approximate f in the nonparametric regression model (e.g., wavelets, splines with
ï¬xed knots, step functions). Another example is related to the aggregation problem, where the fj are estimators arising from M different methods. They can also
correspond to M different values of the tuning parameter of the same method.
Without much loss of generality, these estimators fj are treated as ï¬xed functions.
The results are viewed as being conditioned on the sample that the fj are based
The selection of the dictionary can be very important to make the estimation
of f possible. We assume implicitly that f can be well approximated by a member
of the span of FM. However, this is not enough. In this paper, we have in mind the
situation where M â‰«n, and f can be estimated reasonably only because it can
approximated by a linear combination of a small number of members of FM, or, in
other words, it has a sparse approximation in the span of FM. But, when sparsity
is an issue, equivalent bases can have different properties. A function that has a
sparse representation in one basis may not have it in another, even if both of them
span the same linear space.
Consider the matrix X = (fj(Zi))i,j, i = 1,...,n, j = 1,...,M and the vectors y = (Y1,...,Yn)T, f = (f (Z1),...,f (Zn))T, w = (W1,...,Wn)T. With the
y = f + w,
we will write |x|p for the â„“p norm of x âˆˆRM, 1 â‰¤p â‰¤âˆ. The notation âˆ¥Â· âˆ¥n
stands for the empirical norm
for any g :Z â†’R. We suppose that âˆ¥fjâˆ¥n Ì¸= 0, j = 1,...,M. Set
fmax = max
1â‰¤jâ‰¤M âˆ¥fjâˆ¥n,
fmin = min
1â‰¤jâ‰¤M âˆ¥fjâˆ¥n.
For any Î² = (Î²1,...,Î²M) âˆˆRM, deï¬ne fÎ² = M
j=1 Î²jfj or, explicitly, fÎ²(z) =
j=1 Î²jfj(z) and fÎ² = XÎ². The estimates we consider are all of the form f ËœÎ²(Â·),
where ËœÎ² is data determined. Since we consider mainly sparse vectors ËœÎ², it will be
convenient to deï¬ne the following. Let
I{Î²jÌ¸=0} = |J(Î²)|
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
denote the number of nonzero coordinates of Î², where I{Â·} denotes the indicator
function J(Î²) = {j âˆˆ{1,...,M}:Î²j Ì¸= 0} and |J| denotes the cardinality of J.
The value M(Î²) characterizes the sparsity of the vector Î². The smaller M(Î²), the
â€œsparserâ€ Î². For a vector Î´ âˆˆRM and a subset J âŠ‚{1,...,M}, we denote by Î´J
the vector in RM that has the same coordinates as Î´ on J and zero coordinates on
the complement J c of J.
Introduce the residual sum of squares
{Yi âˆ’fÎ²(Zi)}2
for all Î² âˆˆRM. Deï¬ne the Lasso solution Î²L = (Î²1,L,..., Î²M,L) by
Î²L = argmin
S(Î²) + 2r
where r > 0 is some tuning constant, and introduce the corresponding Lasso estimator
fL(x) = fÎ²L(x) =
Î²j,Lfj(z).
The criterion in (2.1) is convex in Î², so that standard convex optimization procedures can be used to compute Î²L. We refer to and for
detailed discussion of these optimization problems and fast algorithms.
A necessary and sufï¬cient condition of the minimizer in (2.1) is that 0 belongs
to the subdifferential of the convex function Î²
â†’nâˆ’1|y âˆ’XÎ²|2
2 + 2r|D1/2Î²|1.
This implies that the Lasso selector Î²L satisï¬es the constraint
nDâˆ’1/2XT(y âˆ’XÎ²L)
where D is the diagonal matrix
D = diag{âˆ¥f1âˆ¥2
n,...,âˆ¥fMâˆ¥2
More generally, we will say that Î² âˆˆRM satisï¬es the Dantzig constraint if Î² belongs to the set
nDâˆ’1/2XT(y âˆ’XÎ²)
The Dantzig estimator of the regression function f is based on a particular
solution of (2.3), the Dantzig selector Î²D, which is deï¬ned as a vector having the
smallest â„“1 norm among all Î² satisfying the Dantzig constraint
Î²D = argmin
nDâˆ’1/2XT(y âˆ’XÎ²)
LASSO AND DANTZIG SELECTOR
The Dantzig estimator is deï¬ned by
fD(z) = fÎ²D(z) =
Î²j,Dfj(z),
where Î²D = (Î²1,D,..., Î²M,D) is the Dantzig selector. By the deï¬nition of Dantzig
selector, we have |Î²D|1 â‰¤|Î²L|1.
The Dantzig selector is computationally feasible, since it reduces to a linear
programming problem .
Finally, for any n â‰¥1, M â‰¥2, we consider the Gram matrix
fj(Zi)fjâ€²(Zi)
and let Ï†max denote the maximal eigenvalue of n.
3. Restricted eigenvalue assumptions.
We now introduce the key assumptions on the Gram matrix that are needed to guarantee nice statistical properties
of the Lasso and Dantzig selectors. Under the sparsity scenario, we are typically
interested in the case where M > n, and even M â‰«n. Then, the matrix n is
degenerate, which can be written as
(Î´TnÎ´)1/2
Clearly, ordinary least squares does not work in this case, since it requires positive
deï¬niteness of n; that is,
It turns out that the Lasso and Dantzig selector require much weaker assumptions.
The minimum in (3.1) can be replaced by the minimum over a restricted set of
vectors, and the norm |Î´|2 in the denominator of the condition can be replaced by
the â„“2 norm of only a part of Î´.
One of the properties of both the Lasso and the Dantzig selectors is that, for the
linear regression model, the residuals Î´ = Ë†Î²L âˆ’Î² and Î´ = Ë†Î²D âˆ’Î² satisfy, with
probability close to 1,
0 |1 â‰¤c0|Î´J0|1,
where J0 = J(Î²) is the set of nonzero coefï¬cients of the true parameter Î² of the
model. For the linear regression model, the vector of Dantzig residuals Î´ satisï¬es
(3.2) with probability close 1 if c0 = 1 and M is large [cf. (B.9) and the fact that
Î² of the model satisï¬es the Dantzig constraint with probability close to 1 if M is
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
large]. A similar inequality holds for the vector of Lasso residuals Î´ = Î²L âˆ’Î², but
this time with c0 = 3 [cf. Corollary B.2].
Now, for example, consider the case where the elements of the Gram matrix n are close to those of a positive deï¬nite (M Ã— M)-matrix . Denote, by
â–³= maxi,j |(n âˆ’)i,j|, the maximal difference between the elements of the
two matrices. Then, for any Î´ satisfying (3.2), we get
= Î´TÎ´ + Î´T(n âˆ’)Î´
(1 + c0)|Î´J0|1
âˆ’Îµn(1 + c0)2|J0|.
Thus, for Î´ satisfying (3.2), which are the vectors that we have in mind, and for
Îµn|J0| small enough, the LHS of (3.3) is bounded away from 0. This means that we
have a kind of â€œrestrictedâ€ positive deï¬niteness, which is valid only for the vectors
satisfying (3.2). This suggests the following conditions, which will sufï¬ce for the
main argument of the paper. We refer to these conditions as restricted eigenvalue
(RE) assumptions.
ASSUMPTION RE(s,c0).
For some integer s such that 1 â‰¤s â‰¤M and a positive number c0, the following condition holds:
J0âŠ†{1,...,M},
0 |1â‰¤c0|Î´J0|1
The integer s here plays the role of an upper bound on the sparsity M(Î²) of a
vector of coefï¬cients Î².
Note that, if Assumption RE(s,c0) is satisï¬ed with c0 â‰¥1, then
min{|XÎ´|2 :M(Î´) â‰¤2s,Î´ Ì¸= 0} > 0.
In other words, the square submatrices of size â‰¤2s of the Gram matrix are necessarily positive deï¬nite. Indeed, suppose that, for some Î´ Ì¸= 0, we have simultaneously M(Î´) â‰¤2s and XÎ´ = 0. Partition J(Î´) in two sets J(Î´) = I0 âˆªI1, such that
|Ii| â‰¤s, i = 0,1. Without loss of generality, suppose that |Î´I1|1 â‰¤|Î´I0|1. Since,
clearly, |Î´I1|1 = |Î´I c
0 |1 and c0 â‰¥1, we have |Î´I c
0 |1 â‰¤c0|Î´I0|1. Hence, Îº(s,c0) = 0,
a contradiction.
LASSO AND DANTZIG SELECTOR
To introduce the second assumption, we need some notation. For integers s,m
such that 1 â‰¤s â‰¤M/2 and m â‰¥s, s +m â‰¤M, a vector Î´ âˆˆRM and a set of indices
J0 âŠ†{1,...,M} with |J0| â‰¤s; denote by J1 the subset of {1,...,M} corresponding to the m largest in absolute value coordinates of Î´ outside of J0, and deï¬ne
â–³= J0 âˆªJ1. Clearly, J1 and J01 depend on m, but we do not indicate this in our
notation for the sake of brevity.
ASSUMPTION RE(s,m,c0).
J0âŠ†{1,...,M},
0 |1â‰¤c0|Î´J0|1
Note that the only difference between the two assumptions is in the denominators, and Îº(s,m,c0) â‰¤Îº(s,c0). As written, for ï¬xed n, the two assumptions are
equivalent. However, asymptotically for large n, Assumption RE(s,c0) is less restrictive than RE(s,m,c0), since the ratio Îº(s,m,c0)/Îº(s,c0) may tend to 0 if s
and m depend on n. For our bounds on the prediction loss and on the â„“1 loss of the
Lasso and Dantzig estimators, we will only need Assumption RE(s,c0). Assumption RE(s,m,c0) will be required exclusively for the bounds on the â„“p loss with
Note also that Assumptions RE(sâ€²,c0) and RE(sâ€²,m,c0) imply Assumptions
RE(s,c0) and RE(s,m,c0), respectively, if sâ€² > s.
4. Discussion of the RE assumptions.
There exist several simple sufï¬cient
conditions for Assumptions RE(s,c0) and RE(s,m,c0) to hold. Here, we discuss
some of them.
For a real number 1 â‰¤u â‰¤M, we introduce the following quantities that we
will call restricted eigenvalues:
xâˆˆRM:1â‰¤M(x)â‰¤u
xâˆˆRM:1â‰¤M(x)â‰¤u
Denote by XJ the n Ã— |J| submatrix of X obtained by removing from X the
columns that do not correspond to the indices in J, and, for 1 â‰¤m1,m2 â‰¤M,
introduce the following quantities called restricted correlations:
Î¸m1,m2 = max
n|c1|2|c2|2
:I1 âˆ©I2 = âˆ…,|Ii| â‰¤mi,ci âˆˆRIi \ {0},i = 1,2
In Lemma 4.1, below, we show that a sufï¬cient condition for RE(s,c0) and
RE(s,s,c0) to hold is given, for example, by the following assumption on the
Gram matrix.
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
ASSUMPTION 1.
Assume that
Ï†min(2s) > c0Î¸s,2s
for some integer 1 â‰¤s â‰¤M/2 and a constant c0 > 0.
This condition with c0 = 1 appeared in , in connection with the Dantzig selector. Assumption 1 is more general, in that we can have an arbitrary constant
c0 > 0 that will allow us to cover not only the Dantzig selector but also the Lasso
estimators and to prove oracle inequalities for the prediction loss when the model
is nonparametric.
Our second sufï¬cient condition for RE(s,c0) and RE(s,m,c0) does not need
bounds on correlations. Only bounds on the minimal and maximal eigenvalues of
â€œsmallâ€ submatrices of the Gram matrix n are involved.
ASSUMPTION 2.
Assume that
mÏ†min(s + m) > c2
for some integers s,m, such that 1 â‰¤s â‰¤M/2, m â‰¥s and s + m â‰¤M, and a
constant c0 > 0.
Assumption 2 can be viewed as a weakening of the condition on Ï†min in .
Indeed, taking s + m = s logn (we assume, without loss of generality, that s logn
is an integer and n > 3) and assuming that Ï†max(Â·) is uniformly bounded by a
constant, we get that Assumption 2 is equivalent to
Ï†min(s logn) > c/logn,
where c > 0 is a constant. The corresponding, slightly stronger, assumption in 
is stated in asymptotic form, for s = sn â†’âˆ, as
Ï†min(sn logn) > 0.
The following two constants are useful when Assumptions 1 and 2 are considered:
Îº1(s,c0) =
1 âˆ’c0Î¸s,2s
Îº2(s,m,c0) =
Ï†min(s + m)
mÏ†min(s + m)
The next lemma shows that if Assumptions 1 or 2 are satisï¬ed, then the quadratic
form xTnx is positive deï¬nite on some restricted sets of vectors x. The construction of the lemma is inspired by Candes and Tao and covers, in particular, the
corresponding result in .
LASSO AND DANTZIG SELECTOR
LEMMA 4.1.
Fix an integer 1 â‰¤s â‰¤M/2 and a constant c0 > 0.
(i) Let Assumption 1 be satisï¬ed. Then, Assumptions RE(s,c0) and RE(s,s,
c0) hold with Îº(s,c0) = Îº(s,s,c0) = Îº1(s,c0). Moreover, for any subset J0 of
{1,...,M}, with cardinality |J0| â‰¤s, and any Î´ âˆˆRM such that
0 |1 â‰¤c0|Î´J0|1,
âˆšn|P01XÎ´|2 â‰¥Îº1(s,c0)|Î´J01|2,
where P01 is the projector in RM on the linear span of the columns of XJ01.
(ii) Let Assumption 2 be satisï¬ed. Then, Assumptions RE(s,c0) and RE(s,m,
c0) hold with Îº(s,c0) = Îº(s,m,c0) = Îº2(s,m,c0). Moreover, for any subset J0 of
{1,...,M}, with cardinality |J0| â‰¤s, and any Î´ âˆˆRM such that (4.1) holds, we
âˆšn|P01XÎ´|2 â‰¥Îº2(s,m,c0)|Î´J01|2.
The proof of the lemma is given in Appendix A.
There exist other sufï¬cient conditions for Assumptions RE(s,c0) and RE(s,m,
c0) to hold. We mention here three of them implying Assumption RE(s,c0). The
ï¬rst one is the following .
ASSUMPTION 3.
For an integer s such that 1 â‰¤s â‰¤M, we have
Ï†min(s) > 2c0Î¸s,1
where c0 > 0 is a constant.
To argue that Assumption 3 implies RE(s,c0), it sufï¬ces to remark that
J0XTXÎ´J0 âˆ’2
â‰¥Ï†min(s)|Î´J0|2
and, if (4.1) holds,
0 |/n â‰¤|Î´J c
â‰¤Î¸s,1|Î´J c
0 |1|Î´J0|2
Another type of assumption related to â€œmutual coherenceâ€ is discussed in
connection to Lasso in . We state it in two different forms, which are given
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
ASSUMPTION 4.
For an integer s such that 1 â‰¤s â‰¤M, we have
Ï†min(s) > 2c0Î¸1,1s,
where c0 > 0 is a constant.
It is easy to see that Assumption 4 implies RE(s,c0). Indeed, if (4.1) holds,
J0XTXÎ´J0 âˆ’2Î¸1,1|Î´J c
0 |1|Î´J0|1
â‰¥Ï†min(s)|Î´J0|2
2 âˆ’2c0Î¸1,1|Î´J0|2
Ï†min(s) âˆ’2c0Î¸1,1s
If all the diagonal elements of matrix XTX/n are equal to 1 (and thus Î¸1,1 coincides with the mutual coherence ), then a simple sufï¬cient condition for Assumption RE(s,c0) to hold is stated as follows.
ASSUMPTION 5.
All the diagonal elements of the Gram matrix n are equal
to 1, and for an integer s, such that 1 â‰¤s â‰¤M, we have
(1 + 2c0)s ,
where c0 > 0 is a constant.
In fact, separating the diagonal and off-diagonal terms of the quadratic form, we
J0XTXÎ´J0/n â‰¥|Î´J0|2
2 âˆ’Î¸1,1|Î´J0|2
2(1 âˆ’Î¸1,1s).
Combining this inequality with (4.2), we see that Assumption RE(s,c0) is satisï¬ed
whenever (4.3) holds.
Unfortunately, Assumption RE(s,c0) has some weakness. Let, for example, fj,
j = 1,...,2m âˆ’1, be the Haar wavelet basis on (M = 2m), and consider
Zi = i/n, i = 1,...,n. If M â‰«n, then it is clear that Ï†min(1) = 0, since there are
functions fj on the highest resolution level whose supports (of length Mâˆ’1) contain no points Zi. So, none of Assumptions 1â€“4 hold. A less severe, although similar, situation is when we consider step functions fj(t) = I{t<j/M} for t âˆˆ .
It is clear that Ï†min(2) = O(1/M), although sparse representation in this basis is
very natural. Intuitively, the problem arises only because we include very high resolution components. Therefore, we may try to restrict the set J0 in RE(s,c0) to
low resolution components, which is quite reasonable, because the â€œtrueâ€ or â€œinterestingâ€ vectors of parameters Î² are often characterized by such J0. This idea is
formalized in Section 6 (cf. Corollary 6.2, see also a remark after Theorem 7.2 in
Section 7).
LASSO AND DANTZIG SELECTOR
5. Approximate equivalence.
In this section, we prove a type of approximate equivalence between the Lasso and the Dantzig selector. It is expressed as
closeness of the prediction losses âˆ¥
n when the number of
nonzero components of the Lasso or the Dantzig selector is small as compared to
the sample size.
THEOREM 5.1.
Let Wi be independent N (0,Ïƒ 2) random variables with
Ïƒ 2 > 0. Fix n â‰¥1, M â‰¥2. Let Assumption RE(s,1) be satisï¬ed with 1 â‰¤s â‰¤M.
Consider the Dantzig estimator 
fD deï¬ned by (2.5)â€“(2.4) with
where A > 2
2, and consider the Lasso estimator 
fL deï¬ned by (2.1)â€“(2.2) with
the same r.
If M(Î²L) â‰¤s, then, with probability at least 1 âˆ’M1âˆ’A2/8, we have
â‰¤16A2 M(Î²L)Ïƒ 2
Îº2(s,1) logM.
Note that the RHS of (5.1) is bounded by a product of three factors (and a
numerical constant which, unfortunately, equals at least 128). The ï¬rst factor
M(Î²L)Ïƒ 2/n â‰¤sÏƒ 2/n corresponds to the error rate for prediction in regression
with s parameters. The two other factors, logM and f 2
max/Îº2(s,1), can be regarded as a price to pay for the large number of regressors. If the Gram matrix
n equals the identity matrix (the white noise model), then there is only the logM
factor. In the general case, there is another factor f 2
max/Îº2(s,1) representing the
extent to which the Gram matrix is ill-posed for estimation of sparse vectors.
We also have the following result that we state, for simplicity, under the assumption that âˆ¥fjâˆ¥n = 1, j = 1,...,M. It gives a bound in the spirit of Theorem 5.1
but with M(Î²D) rather than M(Î²L) on the right-hand side.
THEOREM 5.2.
Let the assumptions of Theorem 5.1 hold, but with RE(s,5)
in place of RE(s,1), and let âˆ¥fjâˆ¥n = 1, j = 1,...,M. If M(Î²D) â‰¤s, then, with
probability at least 1 âˆ’M1âˆ’A2/8, we have
n + 81A2 M(Î²D)Ïƒ 2
The approximate equivalence is essentially that of the rates as Theorem 5.1 exhibits. A statement free of M(Î²) holds for linear regression, see discussion after Theorems 7.2 and 7.3 below.
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
6. Oracle inequalities for prediction loss.
Here, we prove sparsity oracle
inequalities for the prediction loss of the Lasso and Dantzig estimators. These inequalities allow us to bound the difference between the prediction errors of the
estimators and the best sparse approximation of the regression function (by an oracle that knows the truth but is constrained by sparsity). The results of this section,
together with those of Section 5, show that the distance between the prediction
losses of the Dantzig and Lasso estimators is of the same order as the distances
between them and their oracle approximations.
A general discussion of sparsity oracle inequalities can be found in . Such
inequalities have been recently obtained for the Lasso type estimators in a number
of settings and . In particular, the regression model with ï¬xed design
that we study here is considered in . The assumptions on the Gram matrix
n in are more restrictive than ours. In those papers, either n is positive
deï¬nite, or a mutual coherence condition similar to (4.3) is imposed.
THEOREM 6.1.
Let Wi be independent N (0,Ïƒ 2) random variables with
Ïƒ 2 > 0. Fix some Îµ > 0 and integers n â‰¥1, M â‰¥2, 1 â‰¤s â‰¤M. Let Assumption RE(s,3+4/Îµ) be satisï¬ed. Consider the Lasso estimator 
fL deï¬ned by (2.1)â€“
(2.2) with
for some A > 2
2. Then, with probability at least 1 âˆ’M1âˆ’A2/8, we have
n + C(Îµ)f 2
Îº2(s,3 + 4/Îµ)
where C(Îµ) > 0 is a constant depending only on Îµ.
We now state, as a corollary, a softer version of Theorem 6.1 that can be used to
eliminate the pathologies mentioned at the end of Section 4. For this purpose, we
J0 âŠ‚{1,...,M}:|J0| â‰¤s and
0 |1â‰¤c0|Î´J0|1
where Î³ > 0 is a constant, and set
s,Î³,c0 = {Î² :J(Î²) âˆˆJs,Î³,c0}.
In similar way, we deï¬ne Js,Î³,m,c0 and
s,Î³,m,c0 corresponding to Assumption RE(s,m,c0).
LASSO AND DANTZIG SELECTOR
COROLLARY 6.2.
Let Wi, s and the Lasso estimator 
fL be the same as in
Theorem 6.1. Then, for all n â‰¥1, Îµ > 0, and Î³ > 0, with probability at least 1 âˆ’
M1âˆ’A2/8 we have
n â‰¤(1 + Îµ)
n + C(Îµ)f 2
s,Î³,Îµ = {Î² âˆˆ
s,Î³,3+4/Îµ :M(Î²) â‰¤s}.
To obtain this corollary, it sufï¬ces to observe that the proof of Theorem 6.1
goes through if we drop Assumption RE(s,3 + 4/Îµ), but we assume instead that
s,Î³,3+4/Îµ, and we replace Îº(s,3 + 4/Îµ) by Î³ .
We would like now to get a sparsity oracle inequality similar to that of Theorem 6.1 for the Dantzig estimator 
fD. We will need a mild additional assumption
on f . This is due to the fact that not every Î² âˆˆRM obeys the Dantzig constraint;
thus, we cannot assure the key relation (B.9) for all Î² âˆˆRM. One possibility would
be to prove inequality as (6.1), where the inï¬mum on the right hand side is taken
over Î² satisfying not only M(Î²) â‰¤s but also the Dantzig constraint. However, this
seems not to be very intuitive, since we cannot guarantee that the corresponding
fÎ² gives a good approximation of the unknown function f . Therefore, we choose
another approach (cf. ), in which we consider f satisfying the weak sparsity
property relative to the dictionary f1,...,fM. That is, we assume that there exist
an integer s and constant C0 < âˆsuch that the set
Î² âˆˆRM :M(Î²) â‰¤s,âˆ¥fÎ² âˆ’f âˆ¥2
Îº2(s,3 + 4/Îµ)M(Î²)
is nonempty. The second inequality in (6.2) says that the â€œbiasâ€ term âˆ¥fÎ² âˆ’f âˆ¥2
cannot be much larger than the â€œvariance termâ€ âˆ¼f 2
maxr2Îºâˆ’2M(Î²) [cf. (6.1)].
Weak sparsity is milder than the sparsity property in the usual sense. The latter
means that f admits the exact representation f = fÎ²âˆ—, for some Î²âˆ—âˆˆRM, with
hopefully small M(Î²âˆ—) = s.
PROPOSITION 6.3.
Let Wi be independent N (0,Ïƒ 2) random variables with
Ïƒ 2 > 0. Fix some Îµ > 0 and integers n â‰¥1, M â‰¥2. Let f obey the weak sparsity
assumption for some C0 < âˆand some s such that 1 â‰¤s max{C1(Îµ),1} â‰¤M,
C1(Îµ) = 4[(1 + Îµ)C0 + C(Îµ)]Ï†maxf 2
and C(Îµ) is the constant in Theorem 6.1. Suppose, further, that Assumption RE(s max{C1(Îµ),1},3 + 4/Îµ) is satisï¬ed. Consider the Dantzig estimator
fD deï¬ned by (2.5)â€“(2.4) with
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
2. Then, with probability at least 1 âˆ’M1âˆ’A2/8, we have
Î²âˆˆRM:M(Î²)=s âˆ¥fÎ² âˆ’f âˆ¥2
n + C2(Îµ)f 2
Here, C2(Îµ) = 16C1(Îµ) + C(Îµ) and Îº0 = Îº(max(C1(Îµ),1)s,3 + 4/Îµ).
Note that the sparsity oracle inequality (6.3) is slightly weaker than the analogous inequality (6.1) for the Lasso. Here, we have infÎ²âˆˆRM:M(Î²)=s instead of
infÎ²âˆˆRM:M(Î²)â‰¤s in (6.1).
7. Special case. Parametric estimation in linear regression.
In this section,
we assume that the vector of observations y = (Y1,...,Yn)T is of the form
y = XÎ²âˆ—+ w,
where X is an n Ã— M deterministic matrix Î²âˆ—âˆˆRM and w = (W1,...,Wn)T.
We consider dimension M that can be of order n and even much larger. Then,
Î²âˆ—is, in general, not uniquely deï¬ned. For M > n, if (7.1) is satisï¬ed for Î²âˆ—= Î²0,
then there exists an afï¬ne space U = {Î²âˆ—:XÎ²âˆ—= XÎ²0} of vectors satisfying (7.1).
The results of this section are valid for any Î²âˆ—such that (7.1) holds. However, we
will suppose that Assumption RE(s,c0) holds with c0 â‰¥1 and that M(Î²âˆ—) â‰¤s.
Then, the set U âˆ©{Î²âˆ—:M(Î²âˆ—) â‰¤s} reduces to a single element (cf. Remark 2 at
the end of this section). In this sense, there is a unique sparse solution of (7.1).
Our goal in this section, unlike that of the previous ones, is to estimate both XÎ²âˆ—
for the purpose of prediction and Î²âˆ—itself for purpose of model selection. We will
see that meaningful results are obtained when the sparsity index M(Î²âˆ—) is small.
It will be assumed throughout this section that the diagonal elements of the
Gram matrix n = XTX/n are all equal to 1 (this is equivalent to the condition
âˆ¥fjâˆ¥n = 1,j = 1,...,M, in the notation of previous sections). Then, the Lasso
estimator of Î²âˆ—in (7.1) is deï¬ned by
Î²L = argmin
2 + 2r|Î²|1
The correspondence between the notation here and that of the previous sections is
n = |X(Î² âˆ’Î²âˆ—)|2
n = |X(Î²L âˆ’Î²âˆ—)|2
The Dantzig selector for linear model (7.1) is deï¬ned by
Î²D = argmin
LASSO AND DANTZIG SELECTOR
nXT(y âˆ’XÎ²)
is the set of all Î² satisfying the Dantzig constraint.
We ï¬rst get bounds on the rate of convergence of Dantzig selector.
THEOREM 7.1.
Let Wi be independent N (0,Ïƒ 2) random variables with
Ïƒ 2 > 0, let all the diagonal elements of the matrix XTX/n be equal to 1 and
M(Î²âˆ—) â‰¤s, where 1 â‰¤s â‰¤M, n â‰¥1, M â‰¥2. Let Assumption RE(s,1) be satisï¬ed. Consider the Dantzig selector Î²D deï¬ned by (7.3) with
2. Then, with probability at least 1 âˆ’M1âˆ’A2/2, we have
|Î²D âˆ’Î²âˆ—|1 â‰¤
|X(Î²D âˆ’Î²âˆ—)|2
Îº2(s,1)Ïƒ 2s logM.
If Assumption RE(s,m,1) is satisï¬ed, then, with the same probability as above,
simultaneously for all 1 < p â‰¤2, we have
|Î²D âˆ’Î²âˆ—|p
Note that, since s â‰¤m, the factor in curly brackets in (7.6) is bounded by a
constant independent of s and m. Under Assumption 1 in Section 4, with c0 = 1
[which is less general than RE(s,s,1), cf. Lemma 4.1(i)], a bound of the form (7.6)
for the case p = 2 is established by Candes and Tao .
Bounds on the rate of convergence of the Lasso selector are quite similar to
those obtained in Theorem 7.1. They are given by the following result.
THEOREM 7.2.
Let Wi be independent N (0,Ïƒ 2) random variables with
Ïƒ 2 > 0. Let all the diagonal elements of the matrix XTX/n be equal to 1, and
let M(Î²âˆ—) â‰¤s, where 1 â‰¤s â‰¤M, n â‰¥1, M â‰¥2. Let Assumption RE(s,3) be satisï¬ed. Consider the Lasso estimator Î²L deï¬ned by (7.2) with
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
2. Then, with probability at least 1 âˆ’M1âˆ’A2/8, we have
|Î²L âˆ’Î²âˆ—|1 â‰¤
|X(Î²L âˆ’Î²âˆ—)|2
Îº2(s,3)Ïƒ 2s logM,
M(Î²L) â‰¤64Ï†max
If Assumption RE(s,m,3) is satisï¬ed, then, with the same probability as above,
simultaneously for all 1 < p â‰¤2, we have
|Î²L âˆ’Î²âˆ—|p
Inequalities of the form similar to (7.7) and (7.8) can be deduced from the results
of under more restrictive conditions on the Gram matrix (the mutual coherence
assumption, cf. Assumption 5 of Section 4).
Assumptions RE(s,1) and RE(s,3), respectively, can be dropped in Theorems
7.1 and 7.2 if we assume Î²âˆ—âˆˆ
s,Î³,c0 with c0 = 1 or c0 = 3 as appropriate. Then,
(7.4) and (7.5) or, respectively, (7.7) and (7.8) hold with Îº = Î³ . This is analogous
to Corollary 6.2. Similarly, (7.6) and (7.10) hold with Îº = Î³ if Î²âˆ—âˆˆ
s,Î³,m,c0 with
c0 = 1 or c0 = 3 as appropriate.
Observe that, combining Theorems 7.1 and 7.2, we can immediately get
bounds for the differences between Lasso and Dantzig selector |Î²L âˆ’Î²D|p
|X(Î²Lâˆ’Î²D)|2
2. Such bounds have the same form as those of Theorems 7.1 and 7.2,
up to numerical constants. Another way of estimating these differences follows directly from the proof of Theorem 7.1. It sufï¬ces to observe that the only property
of Î²âˆ—used in that proof is the fact that Î²âˆ—satisï¬es the Dantzig constraint on the
event of given probability, which is also true for the Lasso solution Î²L. So, we can
replace Î²âˆ—by Î²L and s by M(Î²L) everywhere in Theorem 7.1. Generalizing a bit
more, we easily derive the following fact.
THEOREM 7.3.
The result of Theorem 7.1 remains valid if we replace |Î²D âˆ’
p by sup{|Î²D âˆ’Î²|p
,M(Î²) â‰¤s} for 1 â‰¤p â‰¤2 and |X(Î²D âˆ’Î²âˆ—)|2
sup{|X(Î²D âˆ’Î²)|2
,M(Î²) â‰¤s}, respectively. Here,
is the set of all vectors
satisfying the Dantzig constraint.
1. Theorems 7.1 and 7.2 only give nonasymptotic upper bounds on the loss,
with some probability and under some conditions. The probability depends on M
and the conditions depend on n and M. Recall that Assumptions RE(s,c0) and
RE(s,m,c0) are imposed on the n Ã— M matrix X. To deduce asymptotic conver-
LASSO AND DANTZIG SELECTOR
gence (as n â†’âˆand/or as M â†’âˆ) from Theorems 7.1 and 7.2, we would need
some very strong additional properties, such as simultaneous validity of Assumption RE(s,c0) or RE(s,m,c0) (with one and the same constant Îº) for inï¬nitely
many n and M.
2. Note that neither Assumption RE(s,c0) or RE(s,m,c0) implies identiï¬ability of Î²âˆ—in the linear model (7.1). However, the vector Î²âˆ—appearing in the
statements of Theorems 7.1 and 7.2 is uniquely deï¬ned, because we additionally suppose that M(Î²âˆ—) â‰¤s and c0 â‰¥1. Indeed, if there exists a Î²â€² such that
XÎ²â€² = XÎ²âˆ—, and M(Î²â€²) â‰¤s, then, in view of assumption RE(s,c0) with c0 â‰¥1,
we necessarily have Î²âˆ—= Î²â€² [cf. discussion following the deï¬nition of RE(s,c0)].
On the other hand, Theorem 7.3 applies to certain values of Î² that do not come
from the model (7.1) at all.
3. For the smallest value of A (which is A = 2
2) the constants in the bound of
Theorem 7.2 for the Lasso are larger than the corresponding numerical constants
for the Dantzig selector given in Theorem 7.1, again, for the smallest admissible
2. On the contrary, the Dantzig selector has certain defects as compared to Lasso when the model is nonparametric, as discussed in Section 6. In
particular, to obtain sparsity oracle inequalities for the Dantzig selector, we need
some restrictions on f , for example, the weak sparsity property. On the other hand,
the sparsity oracle inequality (6.1) for the Lasso is valid with no restriction on f .
4. The proofs of Theorems 7.1 and 7.2 differ mainly in the value of the tuning
constant, which is c0 = 1 in Theorem 7.1 and c0 = 3 in Theorem 7.2. Note that,
since the Lasso solution satisï¬es the Dantzig constraint, we could have obtained a
result similar to Theorem 7.2, but with less accurate numerical constants, by simply conducting the proof of Theorem 7.1 with c0 = 3. However, we act differently,
and we deduce (B.30) directly from (B.1) and not from (B.25). This is done only
for the sake of improving the constants. In fact, using (B.25) with c0 = 3 would
yield (B.30) with the doubled constant on the right-hand side.
5. For the Dantzig selector in the linear regression model and under Assumptions 1 or 2, some further improvement of constants in the â„“p bounds for the coefï¬cients can be achieved by applying the general version of Lemma 4.1 with the
projector P01 inside. We do not pursue this issue here.
6. All of our results are stated with probabilities at least 1 âˆ’M1âˆ’A2/2 or 1 âˆ’
M1âˆ’A2/8. These are reasonable (but not the most accurate) lower bounds on the
probabilities P(B) and P(A), respectively. We have chosen them for readability.
Inspection of (B.4) shows that they can be reï¬ned to 1 âˆ’2M
(AâˆšlogM) and
(AâˆšlogM/2), respectively, where
(Â·) is the standard normal c.d.f.
APPENDIX A
PROOF OF LEMMA 4.1.
Consider a partition J c
0 into subsets of size m,
with the last subset of size â‰¤m: J c
k=1 Jk, where K â‰¥1, |Jk| = m for
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
k = 1,...,K âˆ’1 and |JK| â‰¤m, such that Jk is the set of indices corresponding to m largest in absolute value coordinates of Î´ outside kâˆ’1
j=1 Jj (for k < K)
and JK is the remaining subset. We have
|P01XÎ´|2 â‰¥|P01XÎ´J01|2 âˆ’
= |XÎ´J01|2 âˆ’
â‰¥|XÎ´J01|2 âˆ’
|P01XÎ´Jk|2.
We will prove ï¬rst part (ii) of the lemma. Since for k â‰¥1 the vector Î´Jk has only m
nonzero components, we obtain
âˆšn|P01XÎ´Jk|2 â‰¤1
âˆšn|XÎ´Jk|2 â‰¤
Ï†max(m)|Î´Jk|2.
Next, as in , we observe that |Î´Jk+1|2 â‰¤|Î´Jk|1/âˆšm, k = 1,...,K âˆ’1. Therefore,
âˆšm â‰¤c0|Î´J0|1
m|Î´J0|2 â‰¤c0
where we used (4.1). From (A.1)â€“(A.3), we ï¬nd
âˆšn|XÎ´|2 â‰¥1
âˆšn|XÎ´J01|2 âˆ’c0
Ï†min(s + m) âˆ’c0
which proves part (ii) of the lemma.
The proof of part (i) is analogous. The only difference is that we replace, in the
above argument, m by s, and instead of (A.2), we use the bound (cf. )
âˆšn|P01XÎ´Jk|2 â‰¤
âˆšÏ†min(2s)|Î´Jk|2.
APPENDIX B: TWO LEMMAS AND THE PROOFS OF THE RESULTS
LEMMA B.1.
Fix M â‰¥2 and n â‰¥1. Let Wi be independent N (0,Ïƒ 2) random
variables with Ïƒ 2 > 0, and let 
fL be the Lasso estimator deï¬ned by (2.2) with
LASSO AND DANTZIG SELECTOR
for some A > 2
2. Then, with probability at least 1 âˆ’M1âˆ’A2/8, we have, simultaneously for all Î² âˆˆRM,
âˆ¥fjâˆ¥n|Î²j,L âˆ’Î²j|
â‰¤âˆ¥fÎ² âˆ’f âˆ¥2
âˆ¥fjâˆ¥n|Î²j,L âˆ’Î²j|
â‰¤âˆ¥fÎ² âˆ’f âˆ¥2
âˆ¥fjâˆ¥2n|Î²j,L âˆ’Î²j|2,
nXT(f âˆ’XÎ²L)
â‰¤3rfmax/2.
Furthermore, with the same probability,
M(Î²L) â‰¤4Ï†maxf âˆ’2
where Ï†max denotes the maximal eigenvalue of the matrix XTX/n.
PROOF OF LEMMA B.1.
The result (B.1) is essentially Lemma 1 from . For
completeness, we give its proof. Set rn,j = râˆ¥fjâˆ¥n. By deï¬nition,
S(Î²L) + 2
rn,j|Î²j,L| â‰¤S(Î²) + 2
for all Î² âˆˆRM, which is equivalent to
rn,j|Î²j,L|
â‰¤âˆ¥fÎ² âˆ’f âˆ¥2
rn,j|Î²j| + 2
fL âˆ’fÎ²)(Zi).
Deï¬ne the random variables Vj = nâˆ’1 n
i=1 fj(Zi)Wi, 1 â‰¤j â‰¤M, and the event
{2|Vj| â‰¤rn,j}.
Using an elementary bound on the tails of Gaussian distribution, we ï¬nd that the
probability of the complementary event Ac satisï¬es
âˆšn|Vj| > âˆšnrn,j/2
|Î·| â‰¥râˆšn/(2Ïƒ)
= M1âˆ’A2/8,
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
where Î· âˆ¼N (0,1). On the event A we have
n â‰¤âˆ¥fÎ² âˆ’f âˆ¥2
rn,j|Î²j,L âˆ’Î²j| +
2rn,j|Î²j| âˆ’
2rn,j|Î²j,L|.
Adding the term M
j=1 rn,j|Î²j,L âˆ’Î²j| to both sides of this inequality yields, on A,
rn,j|Î²j,L âˆ’Î²j|
â‰¤âˆ¥fÎ² âˆ’f âˆ¥2
rn,j(|Î²j,L âˆ’Î²j| + |Î²j| âˆ’|Î²j,L|).
Now, |Î²j,L âˆ’Î²j| + |Î²j| âˆ’|Î²j,L| = 0 for j /âˆˆJ(Î²), so that, on A, we get (B.1).
To prove (B.2) it sufï¬ces to note that, on A, we have
Now, y = f + w, and (B.2) follows from (2.3) and (B.5).
We ï¬nally prove (B.3). The necessary and sufï¬cient condition for Î²L to be the
Lasso solution can be written in the form
(j)(y âˆ’XÎ²L) = râˆ¥fjâˆ¥n sign(Î²j,L)
if Î²j,L Ì¸= 0,
(j)(y âˆ’XÎ²L)
if Î²j,L = 0,
where x(j) denotes the jth column of X, j = 1,...,M. Next, (B.5) yields that,
on A, we have
â‰¤râˆ¥fjâˆ¥n/2,
j = 1,...,M.
Combining (B.6) and (B.7), we get
(j)(f âˆ’XÎ²L)
if Î²j,L Ì¸= 0.
Therefore,
n2 (f âˆ’XÎ²L)TXXT(f âˆ’XÎ²L) = 1
(j)(f âˆ’XÎ²L)
j:Î²j,LÌ¸=0
(j)(f âˆ’XÎ²L)
= M(Î²L)r2âˆ¥fjâˆ¥2
minM(Î²L)r2/4.
LASSO AND DANTZIG SELECTOR
Since the matrices XTX/n and XXT/n have the same maximal eigenvalues,
n2 (f âˆ’XÎ²L)TXXT(f âˆ’XÎ²L) â‰¤Ï†max
|f âˆ’XÎ²L|2
2 = Ï†maxâˆ¥f âˆ’
and we deduce (B.3) from the last two displays.
COROLLARY B.2.
Let the assumptions of Lemma B.1 be satisï¬ed and
âˆ¥fjâˆ¥n = 1,j = 1,...,M. Consider the linear regression model y = XÎ² +w. Then,
with probability at least 1 âˆ’M1âˆ’A2/8, we have
0 |1 â‰¤3|Î´J0|1,
where J0 = J(Î²) is the set of nonzero coefï¬cients of Î² and Î´ = Î²L âˆ’Î².
Use the ï¬rst inequality in (B.1) and the fact that f = fÎ² for the linear
regression model.
LEMMA B.3.
Let Î² âˆˆRM satisfy the Dantzig constraint
nDâˆ’1/2XT(y âˆ’XÎ²)
and set Î´ = Î²D âˆ’Î², J0 = J(Î²). Then,
0 |1 â‰¤|Î´J0|1.
Further, let the assumptions of Lemma B.1 be satisï¬ed with A >
2. Then, with
probability of at least 1 âˆ’M1âˆ’A2/2, we have
nXT(f âˆ’XÎ²D)
PROOF OF LEMMA B.3.
Inequality (B.9) follows immediately from the deï¬nition of Dantzig selector (cf. ). To prove (B.10), consider the event
{|Vj| â‰¤rn,j}.
Analogously to (B.4), P{Bc} â‰¤M1âˆ’A2/2. On the other hand, y = f+w, and, using
the deï¬nition of Dantzig selector, it is easy to see that (B.10) is satisï¬ed on B.
PROOF OF THEOREM 5.1.
Set Î´ = Î²L âˆ’Î²D. We have
n|f âˆ’XÎ²L|2
n|f âˆ’XÎ²D|2
nÎ´TXT(f âˆ’XÎ²D) + 1
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
This and (B.10) yield
nXT(f âˆ’XÎ²D)
n + 4fmaxr|Î´|1 âˆ’1
where the last inequality holds with probability at least 1 âˆ’M1âˆ’A2/2. Since the
Lasso solution Î²L satisï¬es the Dantzig constraint, we can apply Lemma B.3 with
Î² = Î²L, which yields
0 |1 â‰¤|Î´J0|1
with J0 = J(Î²L). By Assumption RE(s,1), we get
âˆšn|XÎ´|2 â‰¥Îº|Î´J0|2,
where Îº = Îº(s,1). Using (B.12) and (B.13), we obtain
|Î´|1 â‰¤2|Î´J0|1 â‰¤2M1/2(Î²L)|Î´J0|2 â‰¤2M1/2(Î²L)
Finally, from (B.11) and (B.14), we get that, with probability at least 1âˆ’M1âˆ’A2/2,
n + 8fmaxrM1/2(Î²L)
maxr2M(Î²L)
where the RHS follows (B.2), (B.10) and another application of (B.14). This
proves one side of the inequality.
To show the other side of the bound on the difference, we act as in (B.11), up
to the inversion of roles of Î²L and Î²D, and we use (B.2). This yields that, with
probability at least 1 âˆ’M1âˆ’A2/8,
nXT(f âˆ’XÎ²L)
n + 3fmaxr|Î´|1 âˆ’1
This is analogous to (B.11). Now, paralleling the proof leading to (B.15), we obtain
maxr2M(Î²L)
The theorem now follows from (B.15) and (B.17).
LASSO AND DANTZIG SELECTOR
PROOF OF THEOREM 5.2.
Set, again, Î´ = Î²L âˆ’Î²D. We apply (B.1) with
Î² = Î²D, which yields that, with probability at least 1 âˆ’M1âˆ’A2/8,
|Î´|1 â‰¤4|Î´J0|1 + âˆ¥
where, now, J0 = J(Î²D). Consider the following two cases: (i) âˆ¥
2r|Î´J0|1 and (ii) âˆ¥
n â‰¤2r|Î´J0|1. In case (i), inequality (B.16) with fmax = 1
immediately implies
and the theorem follows. In case (ii), we get, from (B.18), that
|Î´|1 â‰¤6|Î´J0|1
and thus |Î´J c
0 |1 â‰¤5|Î´J0|1. We can therefore apply Assumption RE(s,5), which
yields, similarly to (B.14),
|Î´|1 â‰¤6M1/2(Î²D)|Î´J0|2 â‰¤6M1/2(Î²D)
where Îº = Îº(s,5). Plugging (B.19) into (B.16) we ï¬nally get that, in case (ii),
n + 18rM1/2(Î²D)
n + 81r2M(Î²D)
PROOF OF THEOREM 6.1.
Fix an arbitrary Î² âˆˆRM with M(Î²) â‰¤s. Set Î´ =
D1/2(Î²L âˆ’Î²), J0 = J(Î²). On the event A, we get, from the ï¬rst line in (B.1), that
n + r|Î´|1 â‰¤âˆ¥fÎ² âˆ’f âˆ¥2
âˆ¥fjâˆ¥n|Î²j,L âˆ’Î²j|
= âˆ¥fÎ² âˆ’f âˆ¥2
n + 4r|Î´J0|1,
and from the second line in (B.1) that
n â‰¤âˆ¥fÎ² âˆ’f âˆ¥2
M(Î²)|Î´J0|2.
Consider, separately, the cases where
4r|Î´J0|1 â‰¤Îµâˆ¥fÎ² âˆ’f âˆ¥2
Îµâˆ¥fÎ² âˆ’f âˆ¥2
n < 4r|Î´J0|1.
In case (B.23), the result of the theorem trivially follows from (B.21). So, we will
only consider the case (B.24). All of the subsequent inequalities are valid on the
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
event A âˆ©A1, where A1 is deï¬ned by (B.24). On this event, we get, from (B.21),
|Î´|1 â‰¤4(1 + 1/Îµ)|Î´J0|1,
which implies |Î´J c
0 |1 â‰¤(3 + 4/Îµ)|Î´J0|1. We now use Assumption RE(s,3 + 4/Îµ).
This yields
n(Î²K âˆ’Î²)TD1/2XTXD1/2(Î²L âˆ’Î²)
n (Î²L âˆ’Î²)TXTX(Î²L âˆ’Î²) = f 2
where Îº = Îº(s,3 + 4/Îµ). Combining this with (B.22), we ï¬nd
n â‰¤âˆ¥fÎ² âˆ’f âˆ¥2
n + 4rfmaxÎºâˆ’1
â‰¤âˆ¥fÎ² âˆ’f âˆ¥2
n + 4rfmaxÎºâˆ’1
fL âˆ’f âˆ¥n + âˆ¥fÎ² âˆ’f âˆ¥n).
This inequality is of the same form as (A.4) in . A standard decoupling argument
as in , using inequality 2xy â‰¤x2/b + by2 with b > 1, x = rÎºâˆ’1âˆšM(Î²) and y
being either âˆ¥
fL âˆ’f âˆ¥n or âˆ¥fÎ² âˆ’f âˆ¥n, yields that
b âˆ’1âˆ¥fÎ² âˆ’f âˆ¥2
n + 8b2f 2
(b âˆ’1)Îº2 r2M(Î²)
Taking b = 1 + 2/Îµ in the last display ï¬nishes the proof of the theorem.
PROOF OF PROPOSITION 6.3.
Due to the weak sparsity assumption, there exists Â¯Î² âˆˆRM with M( Â¯Î²) â‰¤s such that âˆ¥f Â¯Î² âˆ’f âˆ¥2
maxr2Îºâˆ’2M( Â¯Î²), where
Îº = Îº(s,3 + 4/Îµ) is the same as in Theorem 6.1. Using this together with Theorem 6.1 and (B.3), we obtain that, with probability at least 1 âˆ’M1âˆ’A2/8,
M(Î²L) â‰¤C1(Îµ)M( Â¯Î²) â‰¤C1(Îµ)s.
This and Theorem 5.1 imply
n + 16C1(Îµ)f 2
where Îº0 = Îº(max(C1(Îµ),1)s,3 + 4/Îµ). Once Again, applying Theorem 6.1, we
get the result.
PROOF OF THEOREM 7.1.
Set Î´ = Î²D âˆ’Î²âˆ—and J0 = J(Î²âˆ—). Using Lemma B.3 with Î² = Î²âˆ—, we get that, on the event B (i.e., with probability at least
LASSO AND DANTZIG SELECTOR
1âˆ’M1âˆ’A2/2), the following are true: (i) 1
n|XTXÎ´|âˆâ‰¤2r, and (ii) inequality (4.1)
holds with c0 = 1. Therefore, on B we have
n|XTXÎ´|âˆ|Î´|1
â‰¤2r(|Î´J0|1 + |Î´J c
â‰¤2(1 + c0)r|Î´J0|1
â‰¤2(1 + c0)râˆšs|Î´J0|2 = 4râˆšs|Î´J0|2
since c0 = 1. From Assumption RE(s,1), we get that
2 â‰¥Îº2|Î´J0|2
where Îº = Îº(s,1). This and (B.25) yield that, on B,
2 â‰¤16r2s/Îº2,
|Î´J0|2 â‰¤4râˆšs/Îº2.
The ï¬rst inequality in (B.26) implies (7.5). Next, (7.4) is straightforward in view
of the second inequality in (B.26) and of the relations (with c0 = 1)
|Î´|1 = |Î´J0|1 + |Î´J c
0 |1 â‰¤(1 + c0)|Î´J0|1 â‰¤(1 + c0)âˆšs|Î´J0|2
that hold on B. It remains to prove (7.6). It is easy to see that the kth largest in
absolute value element of Î´J c
0 satisï¬es |Î´J c
0 |(k) â‰¤|Î´J c
0 |1/k. Thus,
and, since (4.1) holds on B (with c0 = 1), we ï¬nd
01|2 â‰¤c0|Î´J0|1
m â‰¤c0|Î´J01|2
Therefore, on B,
On the other hand, it follows from (B.25) that
2 â‰¤4râˆšs|Î´J01|2.
Combining this inequality with Assumption RE(s,m,1), we obtain that, on B,
|Î´J01|2 â‰¤4râˆšs/Îº2.
P. J. BICKEL, Y. RITOV AND A. B. TSYBAKOV
Recalling that c0 = 1 and applying the last inequality together with (B.28), we get
2râˆšs/Îº22.
It remains to note that (7.6) is a direct consequence of (7.4) and (B.29). This follows from the fact that inequalities M
j=1 aj â‰¤b1 and M
j â‰¤b2 with aj â‰¥0
âˆ€1 < p â‰¤2.
PROOF OF THEOREM 7.2.
Set Î´ = Î²L âˆ’Î²âˆ—and J0 = J(Î²âˆ—). Using (B.1),
where we put Î² = Î²âˆ—, rn,j â‰¡r and âˆ¥fÎ² âˆ’f âˆ¥n = 0, we get that, on the event A,
2 â‰¤4râˆšs|Î´J0|2
and (4.1) holds with c0 = 3 on the same event. Thus, by Assumption RE(s,3) and
the last inequality, we obtain that, on A,
2 â‰¤16r2s/Îº2,
|Î´J0|2 â‰¤4râˆšs/Îº2,
where Îº = Îº(s,3). The ï¬rst inequality here coincides with (7.8). Next, (7.9) follows immediately from (B.3) and (7.8). To show (7.7), it sufï¬ces to note that on
the event A the relations (B.27) hold with c0 = 3, to apply the second inequality
in (B.31) and to use (B.4).
Finally, the proof of (7.10) follows exactly the same lines as that of (7.6). The
only difference is that one should set c0 = 3 in (B.28) and (B.29), as well as in the
display preceding (B.28).