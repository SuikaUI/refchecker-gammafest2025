Holistic Adversarial Robustness of Deep Learning Models
Pin-Yu Chen1, Sijia Liu2
1 IBM Research
2 Michigan State University
 , 
Adversarial robustness studies the worst-case performance
of a machine learning model to ensure safety and reliability. With the proliferation of deep-learning-based technology,
the potential risks associated with model development and
deployment can be ampliﬁed and become dreadful vulnerabilities. This paper provides a comprehensive overview of research topics and foundational principles of research methods
for adversarial robustness of deep learning models, including
attacks, defenses, veriﬁcation, and novel applications.
Introduction
Deep learning is a core
engine that drives recent advances in artiﬁcial intelligence
(AI) and machine learning (ML), and it has broad impacts
on our society and technology. However, there is a growing
gap between AI technology’s creation and its deployment
in the wild. One critical example is the lack of robustness,
including natural robustness to data distribution shifts, ability in generalization and adaptation to new tasks, and worstcase robustness when facing an adversary (also known as adversarial robustness). According to a recent Gartner report1,
30% of cyberattacks by 2022 will involve data poisoning,
model theft or adversarial examples. However, the industry
seems underprepared. In a survey of 28 organizations spanning small as well as large organizations, 25 organizations
did not know how to secure their AI/ML systems . Moreover, various practical vulnerabilities and
incidences incurred by AI-empowered systems have been reported in real life, such as Adversarial ML Threat Matrix2
and AI Incident Database3.
To prepare deep-learning enabled AI systems for the real
world and to familiarize researchers with the error-prone
risks hidden in the lifecycle of AI model development and
deployment – spanning from data collection and processing, model selection and training, to model deployment and
system integration – this paper aims to provide a holistic
Copyright © 2023, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
1 
2 
3 
Figure 1: Holistic view of adversarial attack categories and
capabilities (threat models) in the training and deployment
phases. The three types of attacks highlighted in colors (poisoning/backdoor/evasion attack) are the major focus of this
paper. In the deployment phase, the target (victim) can be an
access-limited black-box system (e.g. a prediction API) or a
transparent white-box model.
overview of adversarial robustness for deep learning models. The research themes include (i) attack (risk identiﬁcation and demonstration), (ii) defense (threat detection and
mitigation), (iii) veriﬁcation (robustness certiﬁcate), and (iv)
novel applications. In each theme, the fundamental concepts
and key research principles will be presented in a uniﬁed
and organized manner. This paper takes an overarching and
holistic approach to introduce adversarial robustness of deep
learning models based on the terminology of an AI lifecycle
in development and deployment, which differs from existing
survey papers that provide an in-depth discussion on a speciﬁc threat model. The main goal of this paper is to deliver a
primer that provides basic concepts, systematic knowledge,
and categorization of this rapidly evolving research ﬁeld to
the general audience and the broad AI/ML research community.
Figure 1 shows the lifecycle of AI development and deployment and different adversarial threats corresponding to
attackers’ capabilities (also known as threat models). The
lifecycle is further divided into two phases. The training
phase includes data collection and pre-processing, as well as
model selection (e.g. architecture search and design), hyperparameter tuning, model parameter optimization, and validation. After model training, the model is “frozen” (ﬁxed
The Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)
fθ : Rd 7→ K
K-way neural network classiﬁcation model parameterized by θ
logit : Rd 7→RK
logit (pre-softmax) representation
data sample x and its associated groundtruth label y
ˆyθ(x) ∈[K]
top-1 label prediction of x by fθ
adversarial example of x
adversarial perturbation to x for evasion attack
universal trigger pattern for backdoor attack
target label for targeted attack
loss(fθ(x), y)
classiﬁcation loss (e.g. cross entropy)
attcker’s loss function
Dtrain / Dtest
original training / testing dataset
data transformation function
Table 1: Mathematical notation.
Design a poisoned dataset Dpoison such that models trained
on Dpoison fail to generalize on Dtest (i.e. ˆyθ(xtest) ̸= ytest)
Embed a trigger ∆with a target label t to Dtrain such that
ˆyθ(xtest) = ytest but ˆyθ(xtest + ∆) = t
Evasion (untargeted)
Given fθ, ﬁnd xadv such that xadv is similar to x but ˆyθ(xadv) ̸= y
Evasion (targeted)
Given fθ, ﬁnd xadv such that xadv is similar to x but ˆyθ(xadv) = t
Table 2: Objectives of adversarial attacks.
model architecture and parameters) and is ready for deployment. Before deployment, there are possibly some posthoc model adjustment steps such as model compression and
quantiﬁcation for memory/energy reduction, calibration or
risk mitigation. The frozen model providing inference/prediction can be deployed in a white-box or black-box manner. The former means the model details are transparent to
a user (e.g. releasing the model architecture and pre-trained
weights for neural networks), while the latter means a user
can access model predictions but does not know what the
model is (i.e., an access-limited model), such as a prediction
API. The gray-box setting is a mediocre scenario that assumes a user knows partial information about the deployed
model. In some cases, a user may have knowledge of the
training data and the deployed model is black-box, such as
in the case of an AI automation service that only returns
a model prediction portal based on user-provided training
data. We also note that these two phases can be recurrent: a
deployed model can re-enter the training phase with continuous model/data updates.
Throughout this paper, we focus on adversarial robustness of neural networks for classiﬁcation tasks. Many principles in classiﬁcation can be naturally extended to other machine learning tasks, which will be discussed in Section 4.
Based on Figure 1, this paper will focus on training-phase
and deployment-phase attacks driven by the limitation of
current ML techniques. While other adversarial threats concerning model/data privacy and integrity are also crucial,
such as model stealing, membership inference, data leakage, and model injection, which will not be covered in this
paper. We also note that adversarial robustness of non-deeplearning models such as support vector machines has been
investigated. We refer the readers to 
for the research evolution in adversarial machine learning.
Table 1 summarizes the main mathematical notations. We
use [K] = {1, 2, . . . , K} to denote the set of K class labels.
Without loss of generality, we assume the data inputs are
vectorized (ﬂattened) as d-dimensional real-valued vectors,
and the output (class conﬁdence) of the K-way neural network classiﬁer fθ is nonnegative and sum to 1 (e.g. softmax
as the ﬁnal layer), that is, PK
k=1[fθ(·)]k = 1. The adversarial robustness of real-valued continuous data modalities such
as image, audio, time series, and tabular data can be studied
based on a uniﬁed methodology. For discrete data modalities
such as texts and graphs, one can leverage their real-valued
embeddings , latent representations, or continuous relaxation of the problem formulation ). Unless speciﬁed, in what follows
we will not further distinguish data modalities.
This section will cover mainstream adversarial threats that
aim to manipulate the prediction and decision-making of an
AI model through training-phase or deployment-phase attacks. Table 2 summarizes their attack objectives.
Training-Phase Attacks
Training-phase attacks assume the ability to modify the
training data to achieve malicious attempts on the resulting model, which can be realized through noisy data collection such as crowdsourcing. Speciﬁcally, the memorization
effect of deep learning models can be leveraged as vulnerabilities. We note
that sometimes the term “data poisoning” entails both poisoning and backdoor attacks, though their attack objectives
are different.
Poisoning attack
aims to design a poisoned dataset
Dpoison such that models trained on Dpoison will fail to generalize on Dtest (i.e. ˆyθ(xtest) ̸= ytest) while the training loss
remains similar to clean data. The poisoned dataset Dpoison
can be created by modifying the original training dataset
Dtrain, such as label ﬂipping, data addition/deletion, and feature modiﬁcation. The rationale is that training on Dpoison
will land on a “bad” local minimum of model parameters.
To control the amount of data modiﬁcation and reduce
the overall accuracy on Dtest (i.e. test accuracy), poisoning
attack often assumes the knowledge of the target model and
its training method . 
proposes black-box poisoning with additional conditions on
the training loss function. Targeted poisoning attack aims
at manipulating the prediction of a subset of data samples
in Dtest, which can be accomplished by clean-label poisoning (small perturbations to a subset of Dtrain while keeping
their labels intact) or
gradient-matching poisoning .
Backdoor attack
is also known as Trojan attack. The central idea is to embed a universal trigger ∆to a subset of
data samples in Dtrain with a modiﬁed target label t . Examples of trigger patterns are a small patch
in images and a speciﬁc text string in sentences. Typically,
backdoor attack only assumes access to the training data and
does not assume the knowledge of the model and its training.
The model fθ trained on the tampered data is called a backdoored (Trojan) model. Its attack objective has two folds:
(i) High standard accuracy in the absence of trigger – the
backdoored model should behave like a normal model (same
model trained on untampered data), i.e., ˆyθ(xtest) = ytest. (ii)
High attack success rate in the presence of trigger – the backdoored model will predict any data input with the trigger as
the target label t, i.e., ˆyθ(xtest+∆) = t. Therefore, backdoor
attack is stealthy and insidious. The trigger pattern can also
be made input-aware and dynamic .
There is a growing concern of backdoor attacks in emerging machine learning systems featuring collaborative model
training with local private data, such as federated learning
 . Backdoor
attacks can be made more insidious by leveraging the innate local model/data heterogeneity and diversity . proposes distributed backdoor
attacks by trigger pattern decomposition among malicious
clients to make the attack more stealthy and effective. We
also refer the readers to the detailed survey of these two attacks in .
number of modiﬁed features
total changes in modiﬁed features
Euclidean distance between x and xadv
maximal change in modiﬁed features
Table 3: ℓp norm similarity measures for additive perturbation δ = xadv −x. The change in each feature (dimension)
between x and xadv is measured in absolute value.
Deployment-Phase Attacks
The objective of deployment-phase attacks is to ﬁnd a “similar” example T (x) of x such that the ﬁxed model fθ
will evade its prediction from the original groundtruth label y. The evasion condition can be further separated into
two cases: (i) untargeted attack such that fθ(x) = y but
fθ(T (x)) ̸= y, or (ii) targeted attack such that fθ(x) = y
but fθ(T (x)) = t, t ̸= y. Such T (x) is known as an adversarial example4 of x , and it can be interpreted as out-of-distribution sample or generalization error
 .
Data Similarity.
Depending on data characteristics, specifying a transformation function T (·) that preserves data
similarity between an original sample x and its transformed
sample T (x) is a core mission for evasion attacks. The transformation can also be a composite function of semanticpreserving changes . A common practice to select T (·) is through a simple additive perturbation
δ such that xadv = x + δ, or through domain-speciﬁc knowledge such as rotation, object translation, and color changes
for image data . For additive perturbation (either on data input
or parameter(s) simulating semantic changes), the ℓp norm
(p ≥1) of δ deﬁned as ∥δ∥p ≜
i=1 |δi|p1/p
pseudo norm ℓ0 are surrogate metrics for measuring similarity distance. Table 3 summarizes popular choices of ℓp
norms and their meanings. Take image as an example, ℓ0
norm is used to design few-pixel (patch) attacks , ℓ1 norm is used to generate sparse
and small perturbations , ℓ∞norm is
used to conﬁne maximal changes in pixel values , and mixed ℓp norms can also be used .
Figure 2: Taxonomy and illustration of evasion attacks.
differentiation function offered by deep learning packages,
such as backpropagation (input gradient) from the model
output to the model input, to craft adversarial examples.
Black-box attack assumes an attacker can only observe
the model prediction of a data input (that is, a query) and
does not know any other information. The target model can
be viewed as a black-box function and thus backpropagation
for computing input gradient is infeasible without knowing
the model details. In the soft-label black-box attack setting,
an attacker can observe (parts of) class predictions and their
associated conﬁdence scores. In the hard-label black-box attack (decision-based) setting, an attacker can only observe
the top-1 label prediction, which is the least information required to be returned to remain the utility of the model. In
addition to attack success rate, query efﬁciency is also an important metric for the performance evaluation of black-box
Transfer attack is a branch of black-box attack that uses
adversarial examples generated from a white-box surrogate
model to attack the target model. The surrogate model can
be either pre-trained or distilled from a set
of data samples with soft labels given by the target model for
training .
Attack formulation.
The process of ﬁnding an adversarial perturbation δ can be formulated as a constrained optimization problem with a speciﬁed attack loss g(δ|x, y, t, θ)
reﬂecting the attack objective (t is omitted for untargeted
attack). The variation in problem formulations and solvers
will lead to different attack algorithms. We specify three examples below. Without loss of generality, we use ℓp norm as
the similarity measure (distortion) and untargeted attack as
the objective, and assume that all feasible data inputs lie in
the scaled space S = d.
• Minimal-distortion formulation:
Minimizeδ:x+δ∈S ∥δ∥p subject to ˆyθ(x + δ) ̸= ˆyθ(x) (1)
• Penalty-based formulation:
Minimizeδ:x+δ∈S ∥δ∥p + λ · g(δ|x, y, θ)
• Budget-based (norm bounded) formulation:
Minimizeδ:x+δ∈S g(δ|x, y, θ) subject to ∥δ∥p ≤ϵ
For untargeted attacks, the attacker’s loss can be the negative classiﬁcation loss g(δ|x, y, θ) = −loss(fθ(x + δ), y)
or the truncated class margin loss (using either logit or softmax output) deﬁned as g(δ|x, y, θ) = max{[logit(x+δ)]y −
maxk∈[K],k̸=y[logit(x + δ)]k + κ, 0}. The margin loss suggests that g(δ|x, y, θ) achieves minimal value (i.e. 0) when
the top-1 class conﬁdence score excluding the original class
y satisﬁes maxk∈[K],k̸=y[logit(x+δ)]k ≥logit(x+δ)]y+κ,
where κ ≥0 is a tuning parameter governing their conﬁdence gap. Similarly, for targeted attacks, the attacker’s loss
can be g(δ|x, y, t, θ) = loss(fθ(x+δ), t) or g(δ|x, y, t, θ) =
max{maxk∈[K],k̸=t[logit(x + δ)]k −[logit(x + δ)]t + κ, 0}.
When implementing black-box attacks, the logit margin loss
can be replaced with the observable model output log fθ.
The attack formulation can be generalized to the universal perturbation setting such that it simultaneously evades
all model predictions. The universality can be w.r.t. data
samples , model ensembles
 , or various data transformations . shows that
min-max optimization can yield effective universal perturbations.
Selected Attack Algorithms.
We show some white-box
and black-box attack algorithms driven by the three aforementioned attack formulations. For the minimal-distortion
formulation, the attack constraint ˆyθ(x + δ) ̸= ˆyθ(x) can
be rewritten as maxk∈[K],k̸=y[fθ(x + δ)]k ≥[fθ(x + δ)]y,
which can be used to linearize the local decision boundary
around x and allow for efﬁcient projection to the closest linearized decision boundary, leading to white-box attack algorithms such as DeepFool and fast adaptive boundary (FAB) attack
 . For the penalty-based formulation,
one can use change-of-variable on δ to convert to an unconstrained optimization problem and then use binary search on
λ to ﬁnd the smallest λ leading to successful attack (i.e.,
g = 0), known as Carlini-Wagner (C&W) white-box attack
 . For the budget-based formulation, one can apply projected gradient descent (PGD), leading to the white-box PGD attack . Attack algorithms using input gradients of the loss function
are called gradient-based attacks.
Black-box attack algorithms often adopt either the
penalty-based or budget-based formulation. Since the input gradient of the attacker’s loss is unavailable to obtain
in the black-box setting, one principal approach is to perform gradient estimation using model queries and then use
the estimated gradient to replace the true gradient in whitebox attack algorithms, leading to the zeroth-order optimization (ZOO) based black-box attacks . The
choices in gradient estimators and
ZOO solvers will give rise to different attack algorithms.
Hard-label black-box attacks can still adopt ZOO principles by spending extra queries to explore local loss landscapes for gradient estimation , which is more queryefﬁcient than random exploration . We refer the readers to for
more details on ZOO methods and applications.
Physical adversarial example
is a prediction-evasive
physical adversarial object. Examples include stop sign
 , eyeglass , physical patch , 3D printing , T-shirt , and facial
makeup .
Defenses and Veriﬁcation
Defenses are adversarial threat detection and mitigation
strategies, which can be divided into empirical and certi-
ﬁed defenses. We note that the interplay between attack and
defense is essentially a cat-and-mouse game. Many seemingly successful empirical defenses were later weakened by
advanced attacks that are defense-aware, which gives a false
sense of adversarial robustness due to information obfuscation . Consequently, defenses are expected to be fully
evaluated against the best possible adaptive attacks that are
defense-aware .
While empirical robustness refers to the model performance against a set of known attacks, it may fail to serve as
a proper robustness indicator against advanced and unseen
attacks. To address this issue, certiﬁed robustness is used to
ensure the model is provably robust given a set of attack conditions (threat models). Veriﬁcation can be viewed as a passive certiﬁed defense in the sense that its goal is to quantify
a given model’s (local) robustness with guarantees.
Empirical Defenses
Empirical defenses are hardening methods applied during
the training/deployment phase to improve adversarial robustness without provable guarantees of their effectiveness.
For training-phase attacks, data ﬁltering and model ﬁnetuning are major approaches. For instance, shows removing outliers using learned latent
representations and retraining the model can reduce the poison effect. To inspect whether a pre-trained model has a
backdoor or not, Neural Cleanse reverseengineers potential trigger patterns for detection. proposes data-efﬁcient detectors that require
only one sample per class and are made data-free for convolutional neural networks. exploits the
mode connectivity in the loss landscape to recover a backdoored model using limited clean data.
For deployment-phase attacks, adversarial input detection
schemes that exploit data characteristics such as spatial or
temporal correlations are shown to be effective, such as
the detection of audio adversarial example using temporal
dependency . For training adversarially
robust models, adversarial training that aims to minimize
the worst-case loss evaluated by perturbed examples generated during training is so far the strongest empirical defense
 . Speciﬁcally, the standard formulation of
adversarial training can be expressed as the following minmax optimization over training samples {xi, yi}n
∥δi∥p≤ϵ loss(fθ(xi + δ), y)
The worst-case loss corresponding to the inner maximization step is often evaluated by gradient-based attacks such
as PGD attack . Variants of adversarial
training methods such as TRADES and
customized adversarial training (CAT) 
have been proposed for improved robustness. proposes attack-independent robust training based on
self-progression. On 18 different ImageNet pre-trained models, unveils an undesirable trade-off between
standard accuracy and adversarial robustness. This trade-off
can be improved with unlabeled data . A similar study on vision transformers is presented in . 
extends the analysis to a variety of robustness aspects beyond adversarial robustness.
Certiﬁed Defenses
Certiﬁed defenses provide performance guarantees on hardened models. Adversarial attacks are ineffective if their
threat models fall within the provably robust conditions.
For training-phase attacks, proposes certiﬁed data sanitization against poisoning attacks. proposes randomized data
training for certiﬁed defense against backdoor attacks. For
deployment-phase attacks, randomized smoothing is an effective and model-agnostic approach that adds random
noises to data input to “smooth” the model and perform
majority voting on the model predictions. The certiﬁed radius (region) in ℓp-norm perturbation ensuring consistent
class prediction can be computed by information-theoretical
approach , differential privacy , Neyman-Pearson lemma , or higher-order certiﬁcation . The certiﬁed defense is also recently extended to robustify black-box victim models by leveraging
the technique of denoised randomized smoothing .
Veriﬁcation
Veriﬁcation is often used in certifying local robustness
against evasion attacks. Given a neural network fθ and
a data sample x, veriﬁcation (in its simplest form) aims
to maximally certify an ℓp-norm bounded radius r on the
perturbation δ to ensure the model prediction on the perturbed sample x + δ is consistent as long as δ is within
the certiﬁed region. That is, for any δ such that ∥δ∥p ≤r,
ˆyθ(x + δ) = ˆyθ(x). The certiﬁed radius is a robustness certiﬁcate relating to the distance to the closest decision boundary, which is computationally challenging (NP-complete)
for neural networks . However, its estimate (hence not a certiﬁcate) can be efﬁciently computed
and used as a model-agnostic robustness metric, such as
the CLEVER score . To address the
non-linearity induced by layer propagation in neural networks, solving for a certiﬁed radius is often cast as a relaxed
optimization problem. The methods include convex polytope , semideﬁnite programming
 , dual optimization , layer-wise linear bounds , and interval bound propagation . The veriﬁcation tools are also expanded to support
general network architectures and semantic adversarial examples . The intermediate certiﬁed
results can be used to train a more certiﬁable model . However, scalability to
large-sized neural networks remains a major challenge in
veriﬁcation.
Remarks and Discussion
Here we make several concluding remarks and discussions.
Novel Applications.
The insights from studying adversarial robustness have led to several new use cases. Adversarial
perturbation and data poisoning are used in generating contrastive explanations , personal privacy protection , data/model watermarking and ﬁngerprinting , data-limited transfer learning , and visual prompting . Adversarial examples with proper design are also
efﬁcient data augmentation tools to simultaneously improve
model generalization and adversarial robustness . Other noteworthy applications include image synthesis generating
contrastive explanations , robust
text CAPTCHAs , reverse engineering of
deception , uncertainty calibration , and molecule discovery and
image captioning . Beyond input perturbation, the robustness of model parameter perturbation also relates to model quantiﬁcation and energy-efﬁcient inference .
Instilling Adversarial Robustness into Foundation Models.
As foundation models adapt
task-independent pre-training for general representation
learning followed by task-speciﬁc ﬁne-tuning for fast adaptation, it is of utmost importance to understand (i) how to incorporate adversarial robustness into foundation model pretraining and (ii) how to maximize adversarial robustness
transfer from pre-training to ﬁne-tuning. show promising results in adversarial robustness preservation and transfer in meta learning and contrastive learning. The rapid growth and intensifying demand
on foundation models create a unique opportunity to advocate adversarial robustness as a necessary native property
in next-generation trustworthy AI tools and call for novel
methods for evaluating representational robustness, such as
in .
Practical Adversarial Robustness at Scale.
From an industrial viewpoint, current solutions to strengthen adversarial robustness may not be ideal because of the unacceptable
performance drop on the original task and the poor scalability of effective defenses to industry-scale large deep learning models and systems. While there are some efforts for
enabling adversarial training at scale, such as , the notable tradeoff between standard accuracy and
robust accuracy may not be a favorable solution for business adoption. An alternative can be rethinking the evaluation methodology of adversarial robustness. For example,
instead of aiming to mitigate the robustness-accuracy tradeoff, we can compare the unilateral robustness gain under the
constraint of making minimal (or even zero) harm to the
original model utility (e.g. test accuracy). Moreover, an ideal
defense should be lightweight and deployable in a plug-andplay manner for any given model, instead of demanding to
train a model from scratch for improved robustness.