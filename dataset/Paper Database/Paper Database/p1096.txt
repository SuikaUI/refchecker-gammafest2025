Jointly Attentive Spatial-Temporal Pooling Networks for Video-based
Person Re-Identiﬁcation
Shuangjie Xu1∗
Yu Cheng2∗
Yang Yang3
Shiyu Chang4
1Huazhong University of Science and Technology
2AI Foundations, IBM Research
4Northwestern University
3IBM T.J. Watson Research Center
Person Re-Identiﬁcation (person re-id) is a crucial task
as its applications in visual surveillance and humancomputer interaction. In this work, we present a novel joint
Spatial and Temporal Attention Pooling Network (ASTPN)
for video-based person re-identiﬁcation, which enables the
feature extractor to be aware of the current input video sequences, in a way that interdependency from the matching
items can directly inﬂuence the computation of each other’s
representation.
Speciﬁcally, the spatial pooling layer is
able to select regions from each frame, while the attention
temporal pooling performed can select informative frames
over the sequence, both pooling guided by the information
from distance matching. Experiments are conduced on the
iLIDS-VID, PRID-2011 and MARS datasets and the results
demonstrate that this approach outperforms existing stateof-art methods. We also analyze how the joint pooling in
both dimensions can boost the person re-id performance
more effectively than using either of them separately 1.
1. Introduction
Person Re-Identiﬁcation has been viewed as one of the
key subproblems of the generic object recognition task. It
is also important due to its applications in surveillance, and
human-computer interaction communities. Given a query
image, the task is to identify a set of matching person images from a pool, usually captured from the same/different
cameras, from different viewpoints, at the same/different
time points.
It is a very challenging task due to the
large variations of lighting conditions, viewing angles, body
poses and occlusions.
Methods for re-identiﬁcation in still images setting have
been extensively investigated, including feature represen-
∗indicates equal contributions.
 
Temporal-Pooling-Networks-ReID
Figure 1. Sample video frames from one person captured by three
cameras a, b and c, simulating how human compare different video
pairs. The regions under the cycles are the parts which visual attentions are drawn on.
tation learning , distance metric learning
 and CNN-based schemes . Very recently, researchers began to explore solving
this problem in video-based setting, which is a more natural
way to perform re-identiﬁcation. The intuition of this kind
of methods is that temporal information related to person
motion can be captured from video. Moreover, sequences of
images provide rich samples of persons’ appearances, helping boosting the re-identiﬁcation performance with more
discriminative features.
In , a temporal deep neural
network architecture combines optical ﬂow, recurrent layers and mean-pooling and achieves reasonable success. The
work in exploited a novel recurrent feature aggregation framework, which is capable of learning discriminative
sequence level representation from simple frame-wise features.
The main idea of video-based methods is ﬁrst to extract useful representations from video images with RNN
(or CNN-RNN) models. Then exploit a distance function to
judge their extent of matching. However, most of these ap-
 
proaches derive each sequence’s representation separately,
rarely considering the impact of the others, which neglect
the mutual inﬂuence of the two video sequences in the context of the matching task. Let’s think about how human
visual processing works when comparing video sequences.
For example, the pair-wise case described in Figure 1, when
comparing video frames a with two other b and c separately,
as b and c are different, it is natural for our brain to draw
different focuses on different frames of a. On the other
hand, the interaction of compared sequences should also
have effect on the spatial dimension, which guides human
to pay attentions on different regions of the input a. This is
extremely important for the scenario with large viewpoint
changes or fast moving object. The example demonstrates
why we should draw different attention when comparing
different pairs of video frames.
Motivated by recent success of attention models , we proposed jointly Attentive Spatial-Temporal
Pooling Networks (ASTPN), a powerful mechanism for
learning the representation of video sequences by taking
into account the interdependence among them. Speciﬁcally,
ASTPN ﬁrst learns a similarity measure over the features
extracted from recurrent-convolutional networks of the two
input items, and uses the similarity scores between the features to compute attention vectors in both spatial (regions in
each frame) and temporal (frames over sequences) dimensions. Next, the attention vectors are used to perform pooling. Finally, a Siamese network architecture is deployed
over the attention vectors. The proposed architecture can be
trained efﬁciently with the end-to-end training schema.
We perform extensive experiments on three datasets,
iLIDS-VID, PRID-2011 and MARS. The results clearly
demonstrate that our proposed method for person reidentiﬁcation outperforms well established baselines significantly and offers new state-of-the-art performance. The
cross dataset test also derives the same conclusion. ASTPN
is also a general component that can handle a wide variety
of person re-identiﬁcation tasks.
2. Related Work
Person re-id, a challenging task which has been explored
for several years, still remains to be further focused on to
overcome the problems of viewpoint difference, illumination change, occlusions and even similar appearance of different people. A majority of recent works mainly develop
their solutions from two aspects: extracting reliable feature
representations or learning a robust
distance metric . To be speciﬁc, features including color histograms , texture
histograms , Local Binary Patterns , Color Names
 and so on are widely utilized for person re-id to address identity information in the existence of challenges like
lighting change. In the meantime, metric learning methods
Spatial Pyramid
Attentive Temporal
Figure 2. Our video-based person re-identiﬁcation system. We
adopt Siamese network architecture for spatial-temporal feature
extraction, and jointly attentive spatial-temporal pooling for interdependence information learning.
such as large margin nearest neighbor (LMNN) , Mahalanobis distance metric (RCA) , Locally Adaptive Decision Function (LADF) and RankSVM have also
been applied to person re-id task. Despite the prominent
progress in recent years, most of these works are still based
on image-to-image level. Video setting is intuitively more
close to the practical scenario as video is the ﬁrst-hand material captured by surveillance camera . Besides, temporal information relevant to a person’s motion, gait for instance, may help to discriminate similar pedestrians. Moreover, video provides abundant samples of the target for us
with the cost of increasing computation.
Gradually, more and more works began to explore videoto-video matching problem in person re-id.
Discriminative Video Ranking model used discriminative video
fragments selection to capture more accurate space-time information, while simultaneously learning a video ranking
function for person re-id. Bag-of-words method aimed
to encode frame-wise features into a global vector. However, neither of these models could be considered effective
for ignoring the rich temporal information contained in the
videos. However, video-based person re-id raises new challenges: some inter-class difference of video-based representation can be much more ambiguous compared with the
one when using image-based representation, since it’s likely
that different people could not only have similar appearance but also similar motions, making alignment tough to
achieve. Therefore, space-time information must be fully
utilized to solve those extra problems. Besides, a top-push
distance learning (TDL) model has been proposed to effectively make use of space-time information, with a top-push
constraint to quantify ambiguous video representation .
Deep learning offers an approach to solve feature representation and metric learning problem at the same time.
The typical architecture is composed of two parts: a feature
extracting network, usually a CNN or RNN, and multiple
metric learning layers to make ﬁnal prediction. The ﬁrst
Siamese-CNN (SCNN) structure proposed for person
re-id leveraged a set of 3 SCNNs to the three overlapped
parts of the image. exploited a novel recurrent feature aggregation framework, which is capable of learning
discriminative sequence level representation for frame-wise
features. A recent work used CNN to obtain feature
representation from the multiple frames of the video, then
applied RNN to learn the interaction between them. Temporal pooling layer followed the recurrent layer, aiming to
capture sequential interdependence (the pooling might be
max-pooling or mean-pooling). Those layers were jointly
trained to function as a feature extractor. However, the maxpooling and mean-pooling adapted by them were not robust
enough to compress and produce the person’s appearance
over a period of time, since max-pooling only employed the
most active feature map at one temporal step of whole sequence and mean-pooling, which produced a representation
averaged over all time steps, thus couldn’t preclude the impact of ineffective features well.
More importantly, the re-id frameworks usually take the
form of similarity measure with other inputs. Most of prior
works ignored the mutual inﬂuence of other items when performing representation learning. Thus we would like to ﬁll
this gap by introducing the attention mechanism, which already achieved great success in image caption generation
 , machine translation , question-answering as
well as action recognition . presented an comparative attention architecture and addressed the problem in the
spatial dimension. proposed a two-way attention mechanism to matching the text sequence, which is exploited in
our framework as the temporal pooling component.
3. The Proposed Model Architecture
This work builds a recurrent-convolutional network with
jointly attentive spatial-temporal pooling (ASTPN) for
video-based person re-identiﬁcation. Our ASTPN architecture works by passing a pair of video sequences through
a Siamese networks to obtain two representations and producing the Euclidean distance between them. As shown in
Figure 2, each input (one frame from a video with optic
ﬂow involved) is passed through a CNN network to extract
feature maps from the last convolutional layer. Then those
feature maps are fed into our spatial pooling layer to obtain
image-level representation at one time step. After that, we
take temporal information into consideration by utilizing a
recurrent network to generate the feature set of a video sequence. Finally, all time steps resulting from recurrent network are combined by attentive temporal pooling to form
Feature maps
from Conv_3
Image-level
Representation
Figure 3. Jointly attentive spatial pooling architecture.
Conv 3 is the last convolutional layer. In the spatial pooling layer,
we use a spatial pyramid pooling structure with multi-level spatial
bins (8×8, 4×4, 2×2 and 1×1). The image-level representation
is then generated by joining all pooling outputs with those spatial
the sequence-level representation.
The crucial part of ASTPN relies on the jointly attentive
spatial-temporal pooling (ASTP) layers. Instead of using
general max (mean) pooling and over-time temporal pooling layers, this pooling mechanism could take information
to form the distance at each step, allowing our model to be
more attentive both on region of interests in image level and
on effective time step in sequence level. Moreover, attentive spatial-temporal pooling also makes our model adaptive
to image sequence of arbitrary resolution/length. Detailed
techniques about the attentive spatial and temporal pooling
will be presented in following subsections.
3.1. Spatial Pooling Layer
In person re-identiﬁcation, due to the overlooking angle of most surveillance equipment, pedestrians only take
a part in whole spatial images. Therefore, local spatial attention is necessary for deep networks. The design of such
layer should 1) generate multi-scales region patches of each
image and feed them into RNN/attention pooling layer; 2)
make the model robust to image sequence of arbitrary resolution/length. In this work, we use spatial pyramid pooling
(SPP) layer as the component attentive spatial pooling to
concentrate our model on important region in spatial dimension. Shown in Figure 3, the SPP layer has multi-level spatial bins to generate multi-level spatial representations, and
then those representations are combined into a ﬁxed-length
image-level representation. Since the image-level representations involve pedestrian position and multi-scale spatial
information, our joint attentive spatial pooling mechanism
is able to select regions from each frame.
Given the input sequence v =
v1, . . . , vT
, we obtain the feature maps set C =
C1, . . . , CT
by utilizing the convolutional network shown in Table 1.
Ci ∈Rc×w×h is then fed into spatial pooling layer to get
image-level representation ri. Assuming that the size set
of spatial bins is
(mwj, mhj)|j = 1, . . . , n
, the window
size winj =
and pooling stride strj =
for the j-th spatial bin are determined.
Then the result vector ri is obtained by formula:
 Ci; winj, strj
ri = bi,1 ⊕bi,2 ⊕· · · ⊕bi,n
where fp represents the max pooling function with window
size win and stride str. ⌈·⌉and ⌊·⌋denote ceiling and ﬂoor
operations. fR means the reshape operation which reshapes
a matrix to a vector. Besides, ⊕denotes vector connection operation. Let r =
ri ∈RL|i = 1, . . . , T
be a sequence representation, where L = P
h, we then pass
r forward to the recurrent network to extract information
between time-steps. The recurrent layer is formulized by:
ot = Urt + Wst−1,
st = tanh (ot)
where st−1 ∈RN is the hidden state containing information from previous time step, and ot is the output as time t.
Fully-connected weight U ∈RL×N projects the recurrent
layer input rt from RL to RN, and W ∈RN×N projects
hidden state st−1 from RN to RN. Notice that the recurrent
layer embeds the feature vector into a lower-dimensional
feature by matrix U. The hidden state s0 is initialized to
zero at ﬁrst time step, and between time steps the hidden
state is passed through tanh activation function.
3.2. Attentive Temporal Pooling Layer
Although the recurrent layer is able to capture temporal
information with hidden states, those raw temporarily contains much redundant information. For instance, there are
only minor changes in a series of continuous frames, shown
in Figure 1, thus features learned from these sequence input
involve a lot of redundant information such as the ambiguous background and clothing. In order to avoid the ”Bad
money drives out good” issue, we propose an attentive temporal pooling architecture to enable our model to concentrate on effective information. Attentive temporal pooling
reinforces the pooling layer to perceive the input probe and
gallery data pair, and allows the probe input sequence IP
to directly inﬂuence the computation of gallery sequence
representation vg. We put attentive temporal pooling layer
between the recurrent layer and the distance computation
layer. In the training phase, attentive temporal pooling is
jointly learning with recurrent-convolutional network and
the spatial pooling layer, guiding our model for effective
information extraction in temporal dimension.
In Figure 4, we illustrate our attentive temporal pooling
architecture, which follows the design in . Given the
matrices P ∈RT ×N and G ∈RT ×N, whose i-th row represents the output of the recurrent layer in the i-th time step
with probe data and gallery data respectively, we compute
the attention matrix A ∈RT ×T as follows:
where U ∈RN×N is a intent information sharing matrix
to be learned by networks. When the convolution and recurrent layer are employed to obtain matrix P and G, the
attention matrix is able to have a sight on both probe and
gallery sequence features, and computes weight scores in
temporal dimension. In the gradient descent phase, U is
updated by back propagation and inﬂuences parameters of
convolution and hidden state to guide our model to focus on
effective information.
Next, we apply column-wise and row-wise max pooling
on A respectively to obtain temporal weight vector tp ∈RT
and tg ∈RT . The i-th element of tp represents importance
score for the i-th frame in the probe sequence, which is the
same with tg. Due to the participation of P in the computation of tg, the vector tg can capture the attentive scores of
gallery features related to probe data.
After that, we apply softmax function on temporal
weight vectors tp and tg to generate attention vectors ap ∈
RT and ag ∈RT . The softmax function transforms the i-th
weight [tp]i and [tg]i to the attention ratio [ap]i and [ag]i.
For instance, the i-th element in ag is computed as follows:
[ag]i = e[tg]i
Finally, we apply dot product between the feature matrices P, G and attention vectors ap, ag to obtain the
sequence-level representation vp ∈RN and vg ∈RN, respectively:
vp = P T ap,
vg = GT ag
3.3. Model Details
The main thought of our work is to construct a feature
extracting network which is able to map the sequence data
into feature vector in a low dimensional space, where the
feature vectors from the sequences of the same person are
close, and the feature vectors from the sequences of different persons are separated by a margin. More details on the
components of our proposed network will be explained in
Input: The input to our network consists of three color
channels and two optical ﬂow. The color channels provide
spatial information such as clothing and background, while
optical ﬂow channels provide the temporal motion information. Compared with only use color channels as input, there
Image-level
Representation
Sequence Outputs
Gallery Set
Column-wise
Max Pooling
Max Pooling
Sequence-level
Representation
𝑣𝑝= 𝑃·𝑆𝑜𝑓𝑡𝑚𝑎𝑥(𝑡𝑝)
𝑣𝑔= 𝐺·𝑆𝑜𝑓𝑡𝑚𝑎𝑥(𝑡𝑔)
Figure 4. Attentive temporal pooling architecture. With the RNNs output matrices P and G, we compute attention matrix by introduce a
parameter matrix U to capture attentive score in temporal dimension. With column/row-wise max pooling operation and softmax function,
the attention vector is obtained, which contains the attentive weight for each time step. The sequence-level representation is computed by
dot product between the feature matrices P, G and attention vectors ap, ag.
should be a promotion for person re-id when utilizing both
of color channels and optical ﬂow channels .
The Siamese Network: We use a Siamese network architecture as shown in Figure 2. As mentioned above, our
network architecture is grouped by four functional parts:
convolutional layers, the attentive spatial pooling layer, the
recurrent layer and the attentive temporal pooling layer. As
for convolutional layers, we use a convolutional architecture
with parameters shown in Table 1, and the pooling layer in
the ﬁnal layer is replaced by the attentive spatial pooling
layer. Notice that convolutional layers are unrolled along
with the recurrent layer, and these layers share their parameters in all time steps, which means all frames are passed
through the same spatial feature extractor. Similarly, the
two recurrent layers also share their parameters to process a
pair sequences input.
The Training Objective: Given a pair of sequences
(Ip, Ig) of persons p and g, the sequence-level representations (vp, vg) are obtained by our Siamese network. After
that, we use the Euclidean distance Hinge loss to train our
model as follows:
E (vp, vg) =
0, m −∥vp −vg∥2
where m denotes the margin to separate features of different
persons in Hinge loss. In the training phase, the network is
shown positive and negative input pairs alternately. While
in the testing phase for a new sequence input, we copy the
sequence to form a new pair and pass the pair through our
Siamese network to obtain identity feature. By computing
the distance between the identity feature with previously
saved features of other identities, the most similar identity
is indicated with the lowest distance. In addition, we also
Table 1. layer parameter of the CNN network
Conv(size,
channel, pad, stride)
Max Pooling
5×5, 16, 4, 1
5×5, 32, 4, 1
5×5, 32, 4, 1
c: Convolutional layer; t: Tanh layer; p: Pooling layer
take identity classiﬁcation loss into consideration, following the work . We apply softmax regression on the ﬁnal
features (vp, vg) to predict the identity of persons. By using the cross-entropy loss, we obtain the identity loss I (vp)
and I (vg). Since that the joint learning of Siamese loss and
identity loss brings about a great promotion, the ﬁnal training objective is the combination of the Siamese loss and the
identity loss L (vp, vg) = E (vp, vg) + I (vp) + I (vg).
4. Experimental Results
We evaluate our model for video-based person re-id on
three different datasets: iLIDS-VID , PRID-2011 
and MARS . We also investigate how the joint pooling strategy can bring beneﬁt to the proposed network, the
difference between adapting attentive temporal pooling and
other common temporal pooling strategies, and the use of
attentive spatial pooling.
4.1. iLIDS-VID & PRID-2011
The iLIDS-VID dataset contains 300 people in total,
where each person is represented by two image sequences
recorded by a pair of non-overlapping cameras. The length
of frames forming each image sequence ranges from 23 to
192, with an average length of 73. The challenging dataset
was created at an airport arrival hall under a multi-camera
CCTV network, whose image sequences were accompanied
by clothing similarities among people, lighting and viewpoint variations, cluttered background and occlusions.
The PRID-2011 re-id dataset consists of 400 image
sequences for 200 people captured by two cameras that are
adjacent to each other. Each image sequence is composed
of frames of length from 5 to 675, with an average number
of 100. It’s captured in relatively simple environments with
rare occlusions, compared with the iLIDS-VID dataset.
Experiment Settings
Following , we split the whole set of human sequence
pairs of iLIDS-VID and PRID-2011 randomly into two subsets with equal size. One is used for training, and the other
is used for testing. We report the performance of the average Cumulative Matching Characteristics (CMC) curves
over 10 trials with different train/test splits. Data augmentation was done in several forms. Firstly, since the probe
and gallery sequences are of variable-length, sub-sequences
of k = 16 consecutive frames were chosen randomly at
each epoch during training process. Yet we considered the
ﬁrst camera as probe and the second camera as gallery during testing. Secondly, positive pair was composed of a subsequence from camera 1 and a sub-sequence from camera 2
containing the same person A, and negative pair was composed of a sub-sequence from camera 1 of person A and a
sub-sequence from camera 2 of person B, who was selected
arbitrarily from the rest of people in training set. Positive
and negative sequence pairs were sent to our system successively so that the model is capable of distinguishing correct
match and wrong match. Lastly, the image level augmentation was performed by cropping and mirroring. Sub-image
of both width and length 8 pixels less than its progenitor
was produced after cropping, and then we ﬁxed the cropping area within the same sequence. Mirroring operation
was randomly applied to a whole sequence together with a
probability of p = 0.5. Test data also underwent the augmentation to eliminate bias.
Preprocessing steps included the following actions :
Images were converted to YUV color space ﬁrstly, and each
color channel was normalized to have zero mean and unit
variance; Optical ﬂow, both vertical and horizontal, were
extracted between each pair of adjoining images using the
Lucas-Kanade method , and then optical ﬂow channels
were normalized to the range -1 to 1; The learning rate,
when network was trained with stochastic gradient descent,
was 0.001 at the beginning, with batch size set as one.
The initialization of hyper-parameters of convolutional
network was performed based on , optimized already on
Table 2. Comparison of our model with other state-of-the-art methods on iLIDS-VID and PRID-2011 according to CMC curves (%).
RNN-CNN 
the challenging VIPeR person re-identiﬁcation dataset .
Besides, the margin in the Siamese cost function was set to
3, and the dimension of feature space was set to 128. We
alternately showed our Siamese network positive and negative sequence pairs, and a full epoch consisted of the equal
number of both. As the training set contains 150 people
with a maximum sequence length of 192, it takes approximately 3 hours to train for 700 epochs, using the Nvidia
GTX-1080 GPU.
We display the results on iLIDS-VID and PRID-2011 in Table 2. The competitor methods are introduced as follows:
• RNN-CNN: A recurrent convolutional network (RCN)
 with temporal pooling.
• RFA: Recurrent feature aggregation network 
based on LSTM, which aggregates the frame-wise human region representation at each time stamp and produces a sequence-level representation.
• STA: Spatio-temporal body-action model that takes the
video of a walking person as input and builds a spatiotemporal appearance representation for pedestrian reidentiﬁcation.
• VR: A DVR framework presented in for person
re-id uses discriminative space-time feature selection
to automatically discover and exploit the most reliable
video fragments.
• AFDA: An algorithm that hierarchically clusters
image sequences and uses the representative data samples to learn a feature subspace maximizing the Fisher
criterion.
Comparing the CMC results of our proposed architecture
with the RNN-CNN method and other systems on iLIDS-
VID, we can conclude that the attentive mechanism enables
our network to outperform all mentioned networks by a
large margin. Note that even for the rank-1 matching rate,
our method also achieves 62%, exceeding the RNN-CNN
method by more than 4%. We further notice that even without attentive spatial pooling layer, the utilization of attentive temporal pooling can still lead to a fairly good performance. Thus our proposed network is capable of capturing frame-level human features and then fusing them into a
discriminative representation. Another point is that the performances of DNNs seem to apparently surpass the existing
state-of-the-art algorithms , proving the power of
DNNs when sufﬁcient training data is available.
Less challenging than iLIDS-VID as PRID-2011 is, we
can observe overall increments in matching rate. Our model
still outperforms other methods prominently in terms of Table 2, with rank-1 accuracy achieving 77%—transcending
the RNN-CNN method by 7%.
Besides, our system is
more efﬁcient and robust since its CMC rank rate reaches
95% at level of rank 5 and further goes up to the summit of 99% quickly at the level of rank 10. The tendency
of accuracy demonstrates that our system is an effective
space-time feature extractor, able to obtain more discriminative sequence-level representation through learning process. DNNs still exhibit distinctive capability of
capturing human features on the whole, with accuracy converging at earlier point.
This is a dataset introduced in , which is also claimed
to be the largest video re-id dataset to date. MARS consists
of 1261 different pedestrians, each of whom was captured
by at least two cameras. Compared with iLIDS-VID and
PRID-2011, MARS is 4 times larger in the number of identities and 30 times larger in total tracklets. The tracklets of
MARS are generated automatically by DPM detector and
GMMCP tracker, whose error makes MARS more realistic
and of course more challenging than previous dataset. Each
identity has 13.2 tracklets on average. For instance, most
identities are captured by 2-4 cameras, and most identities
have 5-15 tracklets, most of which contain 25-50 frames.
To perform our experiments on MARS, simpliﬁcation
should be done in two steps. Firstly, as pedestrians were
recorded by at least 2 cameras, we randomly chose 2 camera
viewpoints of the same person out of the ensemble. Then
one of them was set as probe set and the other was set as
gallery set. Here the case was reduced to our previous experiences with iLIDS-VID and PRID-2011.
The performances of our models are displayed in Table 3, compared with the baseline RNN-CNN. ASTPN still
achieves the best accuracy while general results dropping
obviously in contrast with Table 2. Compared with iLIDS-
VID and PRID-2011, the improvement is larger (around 4%
in all ranks). The reasons may be attributed to that a considerable part of image sequences of MARS are accompanied
9 10 11 12 13 14 15 16 17 18 19 20
9 10 11 12 13 14 15 16 17 18 19 20
9 10 11 12 13 14 15 16 17 18 19 20
Figure 5. The variants of our model are tested on three datasets respectively. ATPN refers to attentive temporal pooling network and
ASP refers to attentive spatial pooling network. Finally ASTPN
stands for the combination of ATPN and ASPN.
by cluttered backgrounds, ambiguity in visual appearance,
or drastic viewpoint changes between sequence pairs.
4.3. Control Experiments with Different Pooling
Strategies
We investigate the effects of attentive temporal pooling
(ATPN), attentive spatial pooling (ASPN), and the coexistence of them (ASTPN). The related CMC curves on iLIDS-
VID, PRID-2011 and MARS are presented in Figure 5 re-
Table 3. Performance comparison with CMC Rank accuracy on
spectively.
ATPN: the overall performance of ATPN curve is obviously better than the RNN-CNN method in Figure 5(a) and
5(b). For example, ATPN curve exceeds the RNN-CNN
method by almost 10% at the rank 2 accuracy on PRID-
2011. Meanwhile, on iLIDS-VID ATPN curve also outperforms the RNN-CNN method by 5% at the rank 3 accuracy.
We can safely conclude that ATPN can efﬁciently utilize
temporal human appearance to form powerful sequencelevel representation, which is more subtle and discriminative than the output of simple pooling strategies (maxpooling and mean-pooling).
ASPN exhibits equally prominent capability of matching
compared with ATPN on iLIDS-VID. Although it is less robust than ATPN on PRID-2011, distinct margin still exists
between ASPN curve and the RNN-CNN method. We may
reason that ASPN, attentive spatial pooling network, mainly
leverages relevant contextual information to enhance the
discriminative power of the ﬁnal representation. However,
as we have mentioned about these two datasets, iLIDS-VID
was created in a rather complicated environment, which
means the contextual information could be more valuable
clue due to the ambiguity of human appearance. On the
contrary, ASPN thus doesn’t perform as competitively as
ATPN in Figure 5(b).
ASTPN: combining ATPN and ASPN together, ASTPN
can capitalize on frame-wise interactions effectively as well
as selectively propagate additional contextual information
through the network. Based on Figure 5(c), where apparent distinction between ATPN curve and ASTPN curve can
be observed with overall accuracy decreasing caused by
MARS, ASTPN exceeds ATPN by about 5% at rank 3 point.
It’s proven that ASTPN is a more robust joint method especially on dataset as challenging as MARS.
4.4. Cross-Dataset Testing
Data bias is inevitable since a particular dataset only represents a small fragment of data of whole real world. The
machine-learning model trained on A dataset would perform much worse when tested on B dataset. It can be regarded as over-ﬁtting to the particular scenario, thus reducing the generality of the model. Cross-data testing is designed to evaluate the model’s potentials in practical application.
The settings are introduced as follows: Both ASTPN and
Table 4. Cross-dataset testing matching rate on PRID-2011 (%). *
indicates that both probe set and gallery set are composed of single
image during test.
Trained on
RNN-CNN are trained on diverse iLIDS-VID dataset, and
then are tested on 50% of the PRID-2011 dataset. Apart
from distinction brought out by cross-dataset training, the
contrast of single-shot method and multi-shot method is
also shown in Table 4. Although results are much worse
than Table 2, ASTPN still achieves 30% on rank 1 accuracy,
close to SRID which is trained on PRID-2011 with the
rank 1 accuracy of 35% . Moreover, using video-based re-id
seems to improve the scores of both models by 100% than
using single-shot re-id in terms of rank 1 score. It can be
concluded that the valuable temporal information provided
by video-based re-id really enhance the generalization performance of re-id networks greatly.
5. Conclusion
We proposed ASTPN, a novel deep architecture with
jointly attentive spatial-temporal pooling for video-based
person re-identiﬁcation, enabling a joint learning of the representations of the inputs as well as their similarity measurement. ASTPN extends the standard RNN-CNNs by decomposing pooling into two steps: a spatial-pooling on feature
map from CNN and an attentive temporal-pooling on the
output of RNN. In effect, explicit or implicitly attention is
performed at each pooling stage which select key regions
or frames over the sequences for the feature representation
Extensive experiments on iLIDS-VID, PRID-2011 and
MARS have demonstrated that ASTPN signiﬁcantly outperforms standard max and temporal pooling approaches. In
particular, by executing control experiments, we show the
joint pooling power than either of spatial/temporl pooling
separately. Additionally, ASTPN is simple to implement
and introduces little computational overhead compared to
general max pooling, which makes it a desirable design
choice for deep RNN-CNNs used in person re-identiﬁcation
in future.
We would also consider to apply the current
method into target tracking/detection systems .
6. Acknowledgment
This research is supported by NSFC No. 61401169. The
corresponding author is Pan Zhou.