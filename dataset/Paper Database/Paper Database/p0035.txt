Foundations and Trends R⃝in Machine Learning
An Introduction to
Variational Autoencoders
Suggested Citation: Diederik P. Kingma and Max Welling , “An Introduction to
Variational Autoencoders”, Foundations and Trends
⃝in Machine Learning: Vol. xx, No.
xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.
Diederik P. Kingma
 
Max Welling
Universiteit van Amsterdam, Qualcomm
 
This article may be used only for the purpose of research, teaching,
and/or private study. Commercial use or systematic downloading
(by robots or other automatic processes) is prohibited without explicit Publisher approval.
Boston — Delft
 
Introduction
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Probabilistic Models and Variational Inference . . . . . . .
Parameterizing Conditional Distributions with Neural Networks
Directed Graphical Models and Neural Networks . . . . . .
Learning in Fully Observed Models with Neural Nets
Learning and Inference in Deep Latent Variable Models . .
Intractabilities . . . . . . . . . . . . . . . . . . . . . . . .
Variational Autoencoders
Encoder or Approximate Posterior
. . . . . . . . . . . . .
Evidence Lower Bound (ELBO) . . . . . . . . . . . . . . .
Stochastic Gradient-Based Optimization of the ELBO . . .
Reparameterization Trick . . . . . . . . . . . . . . . . . .
Factorized Gaussian posteriors
. . . . . . . . . . . . . . .
Estimation of the Marginal Likelihood
. . . . . . . . . . .
Marginal Likelihood and ELBO as KL Divergences . . . . .
Challenges . . . . . . . . . . . . . . . . . . . . . . . . . .
Related prior and concurrent work
. . . . . . . . . . . . .
Beyond Gaussian Posteriors
Requirements for Computational Tractability . . . . . . . .
Improving the Flexibility of Inference Models . . . . . . . .
Inverse Autoregressive Transformations . . . . . . . . . . .
Inverse Autoregressive Flow (IAF)
. . . . . . . . . . . . .
Related work . . . . . . . . . . . . . . . . . . . . . . . . .
Deeper Generative Models
Inference and Learning with Multiple Latent Variables . . .
Alternative methods for increasing expressivity . . . . . . .
Autoregressive Models . . . . . . . . . . . . . . . . . . . .
Invertible transformations with tractable Jacobian determinant 53
Follow-Up Work . . . . . . . . . . . . . . . . . . . . . . .
Conclusion
Acknowledgements
Appendices
A Appendix
Notation and deﬁnitions . . . . . . . . . . . . . . . . . . .
Alternative methods for learning in DLVMs . . . . . . . . .
Stochastic Gradient Descent
. . . . . . . . . . . . . . . .
References
An Introduction to
Variational Autoencoders
Diederik P. Kingma1 and Max Welling2,3
1Google; 
2Universiteit van Amsterdam
3Qualcomm; 
Variational autoencoders provide a principled framework
for learning deep latent-variable models and corresponding
inference models. In this work, we provide an introduction
to variational autoencoders and some important extensions.
Diederik P. Kingma and Max Welling , “An Introduction to
Variational Autoencoders”, Foundations and Trends
⃝in Machine Learning: Vol. xx,
No. xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.
Introduction
Motivation
One major division in machine learning is generative versus discriminative modeling. While in discriminative modeling one aims to learn a
predictor given the observations, in generative modeling one aims to
solve the more general problem of learning a joint distribution over all
the variables. A generative model simulates how the data is generated
in the real world. “Modeling” is understood in almost every science as
unveiling this generating process by hypothesizing theories and testing
these theories through observations. For instance, when meteorologists
model the weather they use highly complex partial diﬀerential equations
to express the underlying physics of the weather. Or when an astronomer
models the formation of galaxies s/he encodes in his/her equations of
motion the physical laws under which stellar bodies interact. The same
is true for biologists, chemists, economists and so on. Modeling in the
sciences is in fact almost always generative modeling.
There are many reasons why generative modeling is attractive. First,
we can express physical laws and constraints into the generative process
while details that we don’t know or care about, i.e. nuisance variables,
are treated as noise. The resulting models are usually highly intuitive
1.1. Motivation
and interpretable and by testing them against observations we can
conﬁrm or reject our theories about how the world works.
Another reason for trying to understand the generative process of
data is that it naturally expresses causal relations of the world. Causal
relations have the great advantage that they generalize much better to
new situations than mere correlations. For instance, once we understand
the generative process of an earthquake, we can use that knowledge
both in California and in Chile.
To turn a generative model into a discriminator, we need to use
Bayes rule. For instance, we have a generative model for an earthquake
of type A and another for type B, then seeing which of the two describes
the data best we can compute a probability for whether earthquake A
or B happened. Applying Bayes rule is however often computationally
expensive.
In discriminative methods we directly learn a map in the same
direction as we intend to make future predictions in. This is in the
opposite direction than the generative model. For instance, one can
argue that an image is generated in the world by ﬁrst identifying the
object, then generating the object in 3D and then projecting it onto an
pixel grid. A discriminative model takes these pixel values directly as
input and maps them to the labels. While generative models can learn
eﬃciently from data, they also tend to make stronger assumptions on
the data than their purely discriminative counterparts, often leading
to higher asymptotic bias when the model is wrong.
For this reason, if the model is wrong (and it almost always is to some
degree!), if one is solely interested in learning to discriminate, and
one is in a regime with a suﬃciently large amount of data, then purely
discriminative models typically will lead to fewer errors in discriminative
tasks. Nevertheless, depending on how much data is around, it may pay
oﬀto study the data generating process as a way to guide the training
of the discriminator, such as a classiﬁer. For instance, one may have
few labeled examples and many more unlabeled examples. In this semisupervised learning setting, one can use the generative model of the data
to improve classiﬁcation .
Generative modeling can be useful more generally. One can think
of it as an auxiliary task. For instance, predicting the immediate future
Introduction
may help us build useful abstractions of the world that can be used
for multiple prediction tasks downstream. This quest for disentangled,
semantically meaningful, statistically independent and causal factors
of variation in data is generally known as unsupervised representation
learning, and the variational autoencoder (VAE) has been extensively
employed for that purpose. Alternatively, one may view this as an
implicit form of regularization: by forcing the representations to be
meaningful for data generation, we bias the inverse of that process, which
maps from input to representation, into a certain mould. The auxiliary
task of predicting the world is used to better understand the world at
an abstract level and thus to better make downstream predictions.
The VAE can be viewed as two coupled, but independently parameterized models: the encoder or recognition model, and the decoder or
generative model. These two models support each other. The recognition model delivers to the generative model an approximation to its
posterior over latent random variables, which it needs to update its
parameters inside an iteration of “expectation maximization” learning.
Reversely, the generative model is a scaﬀolding of sorts for the recognition model to learn meaningful representations of the data, including
possibly class-labels. The recognition model is the approximate inverse
of the generative model according to Bayes rule.
One advantage of the VAE framework, relative to ordinary Variational Inference (VI), is that the recognition model (also called inference
model) is now a (stochastic) function of the input variables. This in
contrast to VI where each data-case has a separate variational distribution, which is ineﬃcient for large data-sets. The recognition model uses
one set of parameters to model the relation between input and latent
variables and as such is called “amortized inference”. This recognition
model can be arbitrary complex but is still reasonably fast because
by construction it can be done using a single feedforward pass from
input to latent variables. However the price we pay is that this sampling
induces sampling noise in the gradients required for learning. Perhaps
the greatest contribution of the VAE framework is the realization that
we can counteract this variance by using what is now known as the
“reparameterization trick”, a simple procedure to reorganize our gradient
computation that reduces variance in the gradients.
1.1. Motivation
The VAE is inspired by the Helmholtz Machine 
which was perhaps the ﬁrst model that employed a recognition model.
However, its wake-sleep algorithm was ineﬃcient and didn’t optimize a
single objective. The VAE learning rules instead follow from a single
approximation to the maximum likelihood objective.
VAEs marry graphical models and deep learning. The generative
model is a Bayesian network of the form p(x|z)p(z), or, if there are
multiple stochastic latent layers, a hierarchy such as p(x|zL)p(zL|zL−1)
...p(z1|z0). Similarly, the recognition model is also a conditional Bayesian
network of the form q(z|x) or as a hierarchy, such as q(z0|z1)...q(zL|X).
But inside each conditional may hide a complex (deep) neural network,
e.g. z|x ∼f(x, ϵ), with f a neural network mapping and ϵ a noise
random variable. Its learning algorithm is a mix of classical (amortized,
variational) expectation maximization but through the reparameterization trick ends up backpropagating through the many layers of the
deep neural networks embedded inside of it.
Since its inception, the VAE framework has been extended in many
directions, e.g. to dynamical models , models with
attention , models with multiple levels of stochastic
latent variables , and many more. It has proven
itself as a fertile framework to build new models in. More recently,
another generative modeling paradigm has gained signiﬁcant attention:
the generative adversarial network (GAN) .
VAEs and GANs seem to have complementary properties: while GANs
can generate images of high subjective perceptual quality, they tend
to lack full support over the data , as opposed to
likelihood-based generative models. VAEs, like other likelihood-based
models, generate more dispersed samples, but are better density models
in terms of the likelihood criterion. As such many hybrid models have
been proposed to try to represent the best of both worlds .
As a community we seem to have embraced the fact that generative
models and unsupervised learning play an important role in building
intelligent machines. We hope that the VAE provides a useful piece of
that puzzle.
Introduction
The framework of variational autoencoders (VAEs) provides a principled method for jointly
learning deep latent-variable models and corresponding inference models
using stochastic gradient descent. The framework has a wide array
of applications from generative modeling, semi-supervised learning to
representation learning.
This work is meant as an expanded version of our earlier work
 , allowing us to explain the topic in ﬁner
detail and to discuss a selection of important follow-up work. This is
not aimed to be a comprehensive review of all related work. We assume
that the reader has basic knowledge of algebra, calculus and probability
In this chapter we discuss background material: probabilistic models,
directed graphical models, the marriage of directed graphical models
with neural networks, learning in fully observed models and deep latentvariable models (DLVMs). In chapter 2 we explain the basics of VAEs.
In chapter 3 we explain advanced inference techniques, followed by an
explanation of advanced generative models in chapter 4. Please refer to
section A.1 for more information on mathematical notation.
Probabilistic Models and Variational Inference
In the ﬁeld of machine learning, we are often interested in learning probabilistic models of various natural and artiﬁcial phenomena from data.
Probabilistic models are mathematical descriptions of such phenomena.
They are useful for understanding such phenomena, for prediction of
unknowns in the future, and for various forms of assisted or automated
decision making. As such, probabilistic models formalize the notion of
knowledge and skill, and are central constructs in the ﬁeld of machine
learning and AI.
As probabilistic models contain unknowns and the data rarely paints
a complete picture of the unknowns, we typically need to assume some
level of uncertainty over aspects of the model. The degree and nature
of this uncertainty is speciﬁed in terms of (conditional) probability dis-
1.3. Probabilistic Models and Variational Inference
tributions. Models may consist of both continuous-valued variables and
discrete-valued variables. The, in some sense, most complete forms of
probabilistic models specify all correlations and higher-order dependencies between the variables in the model, in the form of a joint probability
distribution over those variables.
Let’s use x as the vector representing the set of all observed variables
whose joint distribution we would like to model. Note that for notational
simplicity and to avoid clutter, we use lower case bold (e.g. x) to denote
the underlying set of observed random variables, i.e. ﬂattened and
concatenated such that the set is represented as a single vector. See
section A.1 for more on notation.
We assume the observed variable x is a random sample from an
unknown underlying process, whose true (probability) distribution p∗(x)
is unknown. We attempt to approximate this underlying process with a
chosen model pθ(x), with parameters θ:
Learning is, most commonly, the process of searching for a value of
the parameters θ such that the probability distribution function given
by the model, pθ(x), approximates the true distribution of the data,
denoted by p∗(x), such that for any observed x:
pθ(x) ≈p∗(x)
Naturally, we wish pθ(x) to be suﬃciently ﬂexible to be able to
adapt to the data, such that we have a chance of obtaining a suﬃciently
accurate model. At the same time, we wish to be able to incorporate
knowledge about the distribution of data into the model that is known
Conditional Models
Often, such as in case of classiﬁcation or regression problems, we are not
interested in learning an unconditional model pθ(x), but a conditional
model pθ(y|x) that approximates the underlying conditional distribution
p∗(y|x): a distribution over the values of variable y, conditioned on the
value of an observed variable x. In this case, x is often called the input
Introduction
of the model. Like in the unconditional case, a model pθ(y|x) is chosen,
and optimized to be close to the unknown underlying distribution, such
that for any x and y:
pθ(y|x) ≈p∗(y|x)
A relatively common and simple example of conditional modeling is
image classiﬁcation, where x is an image, and y is the image’s class, as
labeled by a human, which we wish to predict. In this case, pθ(y|x) is
typically chosen to be a categorical distribution, whose parameters are
computed from x.
Conditional models become more diﬃcult to learn when the predicted variables are very high-dimensional, such as images, video or
sound. One example is the reverse of the image classiﬁcation problem: prediction of a distribution over images, conditioned on the class
label. Another example with both high-dimensional input, and highdimensional output, is time series prediction, such as text or video
prediction.
To avoid notational clutter we will often assume unconditional modeling, but one should always keep in mind that the methods introduced
in this work are, in almost all cases, equally applicable to conditional
models. The data on which the model is conditioned, can be treated as
inputs to the model, similar to the parameters of the model, with the
obvious diﬀerence that one doesn’t optimize over their value.
Parameterizing Conditional Distributions with Neural Networks
Diﬀerentiable feed-forward neural networks, from here just called neural
networks, are a particularly ﬂexible and computationally scalable type
of function approximator. Learning of models based on neural networks
with multiple ’hidden’ layers of artiﬁcial neurons is often referred to
as deep learning
 . A
particularly interesting application is probabilistic models, i.e. the use of
neural networks for probability density functions (PDFs) or probability
mass functions (PMFs) in probabilistic models. Probabilistic models
based on neural networks are computationally scalable since they allow
for stochastic gradient-based optimization which, as we will explain,
1.5. Directed Graphical Models and Neural Networks
allows scaling to large models and large datasets. We will denote a deep
neural network as a vector function: NeuralNet(.).
At the time of writing, deep learning has been shown to work well for
a large variety of classiﬁcation and regression problems, as summarized
in . In case of neuralnetwork based image classiﬁcation LeCun et al., 1998, for example,
neural networks parameterize a categorical distribution pθ(y|x) over a
class label y, conditioned on an image x.
p = NeuralNet(x)
pθ(y|x) = Categorical(y; p)
where the last operation of NeuralNet(.) is typically a softmax() function
such that P
Directed Graphical Models and Neural Networks
We work with directed probabilistic models, also called directed probabilistic graphical models (PGMs), or Bayesian networks. Directed graphical models are a type of probabilistic models where all the variables
are topologically organized into a directed acyclic graph. The joint
distribution over the variables of such models factorizes as a product of
prior and conditional distributions:
pθ(x1, ..., xM) =
pθ(xj|Pa(xj))
where Pa(xj) is the set of parent variables of node j in the directed graph.
For non-root-nodes, we condition on the parents. For root nodes, the set
of parents is the empty set, such that the distribution is unconditional.
Traditionally, each conditional probability distribution pθ(xj|Pa(xj))
is parameterized as a lookup table or a linear model . As we explained above, a more ﬂexible way to parameterize
such conditional distributions is with neural networks. In this case,
neural networks take as input the parents of a variable in a directed
Introduction
graph, and produce the distributional parameters η over that variable:
η = NeuralNet(Pa(x))
pθ(x|Pa(x)) = pθ(x|η)
We will now discuss how to learn the parameters of such models, if
all the variables are observed in the data.
Learning in Fully Observed Models with Neural Nets
If all variables in the directed graphical model are observed in the data,
then we can compute and diﬀerentiate the log-probability of the data
under the model, leading to relatively straightforward optimization.
We often collect a dataset D consisting of N ≥1 datapoints:
D = {x(1), x(2), ..., x(N)} ≡{x(i)}N
i=1 ≡x(1:N)
The datapoints are assumed to be independent samples from an unchanging underlying distribution. In other words, the dataset is assumed
to consist of distinct, independent measurements from the same (unchanging) system. In this case, the observations D = {x(i)}N
i=1 are said
to be i.i.d., for independently and identically distributed. Under the
i.i.d. assumption, the probability of the datapoints given the parameters factorizes as a product of individual datapoint probabilities. The
log-probability assigned to the data by the model is therefore given by:
log pθ(D) =
Maximum Likelihood and Minibatch SGD
The most common criterion for probabilistic models is maximum loglikelihood (ML). As we will explain, maximization of the log-likelihood
criterion is equivalent to minimization of a Kullback Leibler divergence
between the data and model distributions.
Under the ML criterion, we attempt to ﬁnd the parameters θ that
maximize the sum, or equivalently the average, of the log-probabilities
1.6. Learning in Fully Observed Models with Neural Nets
assigned to the data by the model. With i.i.d. dataset D of size ND,
the maximum likelihood objective is to maximize the log-probability
given by equation (1.10).
Using calculus’ chain rule and automatic diﬀerentiation tools, we can
eﬃciently compute gradients of this objective, i.e. the ﬁrst derivatives
of the objective w.r.t. its parameters θ. We can use such gradients to
iteratively hill-climb to a local optimum of the ML objective. If we
compute such gradients using all datapoints, ∇θ log pθ(D), then this
is known as batch gradient descent. Computation of this derivative is,
however, an expensive operation for large dataset size ND, since it scales
linearly with ND.
A more eﬃcient method for optimization is stochastic gradient
descent (SGD) (section A.3), which uses randomly drawn minibatches
of data M ⊂D of size NM. With such minibatches we can form an
unbiased estimator of the ML criterion:
log pθ(D) ≃
log pθ(M) =
The ≃symbol means that one of the two sides is an unbiased estimator
of the other side. So one side (in this case the right-hand side) is a
random variable due to some noise source, and the two sides are equal
when averaged over the noise distribution. The noise source, in this case,
is the randomly drawn minibatch of data M. The unbiased estimator
log pθ(M) is diﬀerentiable, yielding the unbiased stochastic gradients:
∇θ log pθ(D) ≃
∇θ log pθ(M) =
∇θ log pθ(x)
These gradients can be plugged into stochastic gradient-based optimizers;
see section A.3 for further discussion. In a nutshell, we can optimize the
objective function by repeatedly taking small steps in the direction of
the stochastic gradient.
Bayesian inference
From a Bayesian perspective, we can improve upon ML through maximum a posteriori (MAP) estimation (section section A.2.1), or, going
Introduction
even further, inference of a full approximate posterior distribution over
the parameters (see section A.1.4).
Learning and Inference in Deep Latent Variable Models
Latent Variables
We can extend fully-observed directed models, discussed in the previous
section, into directed models with latent variables. Latent variables are
variables that are part of the model, but which we don’t observe, and
are therefore not part of the dataset. We typically use z to denote such
latent variables. In case of unconditional modeling of observed variable
x, the directed graphical model would then represent a joint distribution
pθ(x, z) over both the observed variables x and the latent variables z.
The marginal distribution over the observed variables pθ(x), is given
pθ(x, z) dz
This is also called the (single datapoint) marginal likelihood or the model
evidence, when taken as a function of θ.
Such an implicit distribution over x can be quite ﬂexible. If z is
discrete and pθ(x|z) is a Gaussian distribution, then pθ(x) is a mixtureof-Gaussians distribution. For continuous z, pθ(x) can be seen as an
inﬁnite mixture, which are potentially more powerful than discrete mixtures. Such marginal distributions are also called compound probability
distributions.
Deep Latent Variable Models
We use the term deep latent variable model (DLVM) to denote a latent
variable model pθ(x, z) whose distributions are parameterized by neural networks. Such a model can be conditioned on some context, like
pθ(x, z|y). One important advantage of DLVMs, is that even when each
factor (prior or conditional distribution) in the directed model is relatively simple (such as conditional Gaussian), the marginal distribution
pθ(x) can be very complex, i.e. contain almost arbitrary dependen-
1.8. Intractabilities
cies. This expressivity makes deep latent-variable models attractive for
approximating complicated underlying distributions p∗(x).
Perhaps the simplest, and most common, DLVM is one that is
speciﬁed as factorization with the following structure:
pθ(x, z) = pθ(z)pθ(x|z)
where pθ(z) and/or pθ(x|z) are speciﬁed. The distribution p(z) is often
called the prior distribution over z, since it is not conditioned on any
observations.
Example DLVM for multivariate Bernoulli data
A simple example DLVM, used in
 for
binary data x, is with a spherical Gaussian latent space, and a factorized
Bernoulli observation model:
p(z) = N(z; 0, I)
p = DecoderNeuralNetθ(z)
log p(x|z) =
log p(xj|z) =
log Bernoulli(xj; pj)
xj log pj + (1 −xj) log(1 −pj)
where ∀pj ∈p : 0 ≤pj ≤1 (e.g. implemented through a sigmoid
nonlinearity as the last layer of the DecoderNeuralNetθ(.)), where D
is the dimensionality of x, and Bernoulli(.; p) is the probability mass
function (PMF) of the Bernoulli distribution.
Intractabilities
The main diﬃculty of maximum likelihood learning in DLVMs is that
the marginal probability of data under the model is typically intractable.
This is due to the integral in equation (1.13) for computing the marginal
likelihood (or model evidence), pθ(x) =
R pθ(x, z) dz, not having an
analytic solution or eﬃcient estimator. Due to this intractability, we
Introduction
cannot diﬀerentiate it w.r.t. its parameters and optimize it, as we can
with fully observed models.
The intractability of pθ(x), is related to the intractability of the
posterior distribution pθ(z|x). Note that the joint distribution pθ(x, z)
is eﬃcient to compute, and that the densities are related through the
basic identity:
pθ(z|x) = pθ(x, z)
Since pθ(x, z) is tractable to compute, a tractable marginal likelihood
pθ(x) leads to a tractable posterior pθ(z|x), and vice versa. Both are
intractable in DLVMs.
Approximate inference techniques (see also section A.2) allow us to
approximate the posterior pθ(z|x) and the marginal likelihood pθ(x) in
DLVMs. Traditional inference methods are relatively expensive. Such
methods, for example, often require a per-datapoint optimization loop,
or yield bad posterior approximations. We would like to avoid such
expensive procedures.
Likewise, the posterior over the parameters of (directed models
parameterized with) neural networks, p(θ|D), is generally intractable
to compute exactly, and requires approximate inference techniques.
Variational Autoencoders
In this chapter we explain the basics of variational autoencoders (VAEs).
Encoder or Approximate Posterior
In the previous chapter, we introduced deep latent-variable models
(DLVMs), and the problem of estimating the log-likelihood and posterior
distributions in such models. The framework of variational autoencoders
(VAEs) provides a computationally eﬃcient way for optimizing DLVMs
jointly with a corresponding inference model using SGD.
To turn the DLVM’s intractable posterior inference and learning
problems into tractable problems, we introduce a parametric inference
model qφ(z|x). This model is also called an encoder or recognition model.
With φ we indicate the parameters of this inference model, also called
the variational parameters. We optimize the variational parameters φ
such that:
qφ(z|x) ≈pθ(z|x)
As we will explain, this approximation to the posterior help us optimize
the marginal likelihood.
Variational Autoencoders
Like a DLVM, the inference model can be (almost) any directed
graphical model:
qφ(z|x) = qφ(z1, ..., zM|x) =
qφ(zj|Pa(zj), x)
where Pa(zj) is the set of parent variables of variable zj in the directed
graph. And also similar to a DLVM, the distribution qφ(z|x) can be
parameterized using deep neural networks. In this case, the variational
parameters φ include the weights and biases of the neural network. For
(µ, log σ) = EncoderNeuralNetφ(x)
qφ(z|x) = N(z; µ, diag(σ))
Typically, we use a single encoder neural network to perform posterior inference over all of the datapoints in our dataset. This can be
contrasted to more traditional variational inference methods where
the variational parameters are not shared, but instead separately and
iteratively optimized per datapoint. The strategy used in VAEs of sharing variational parameters across datapoints is also called amortized
variational inference . With amortized
inference we can avoid a per-datapoint optimization loop, and leverage
the eﬃciency of SGD.
Evidence Lower Bound (ELBO)
The optimization objective of the variational autoencoder, like in other
variational methods, is the evidence lower bound, abbreviated as ELBO.
An alternative term for this objective is variational lower bound. Typically, the ELBO is derived through Jensen’s inequality. Here we will
use an alternative derivation that avoids Jensen’s inequality, providing
greater insight about its tightness.
For any choice of inference model qφ(z|x), including the choice of
2.2. Evidence Lower Bound (ELBO)
Encoder: qφ(z|x)
Decoder: pθ(x|z)
Prior distribution: pθ(z)
Dataset: D
Figure 2.1: A VAE learns stochastic mappings between an observed x-space, whose
empirical distribution qD(x) is typically complicated, and a latent z-space, whose
distribution can be relatively simple (such as spherical, as in this ﬁgure). The
generative model learns a joint distribution pθ(x, z) that is often (but not always)
factorized as pθ(x, z) = pθ(z)pθ(x|z), with a prior distribution over latent space
pθ(z), and a stochastic decoder pθ(x|z). The stochastic encoder qφ(z|x), also called
inference model, approximates the true but intractable posterior pθ(z|x) of the
generative model.
Variational Autoencoders
variational parameters φ, we have:
log pθ(x) = Eqφ(z|x) [log pθ(x)]
= Eqφ(z|x)
= Eqφ(z|x)
= Eqφ(z|x)
+ Eqφ(z|x)
=DKL(qφ(z|x)||pθ(z|x))
The second term in eq. (2.8) is the Kullback-Leibler (KL) divergence
between qφ(z|x) and pθ(z|x), which is non-negative:
DKL(qφ(z|x)||pθ(z|x)) ≥0
and zero if, and only if, qφ(z|x) equals the true posterior distribution.
The ﬁrst term in eq. (2.8) is the variational lower bound, also called
the evidence lower bound (ELBO):
Lθ,φ(x) = Eqφ(z|x) [log pθ(x, z) −log qφ(z|x)]
Due to the non-negativity of the KL divergence, the ELBO is a lower
bound on the log-likelihood of the data.
Lθ,φ(x) = log pθ(x) −DKL(qφ(z|x)||pθ(z|x))
≤log pθ(x)
So, interestingly, the KL divergence DKL(qφ(z|x)||pθ(z|x)) determines
two ’distances’:
1. By deﬁnition, the KL divergence of the approximate posterior
from the true posterior;
2. The gap between the ELBO Lθ,φ(x) and the marginal likelihood
log pθ(x); this is also called the tightness of the bound. The better
qφ(z|x) approximates the true (posterior) distribution pθ(z|x), in
terms of the KL divergence, the smaller the gap.
2.3. Stochastic Gradient-Based Optimization of the ELBO
Generative Model
Inference Model
ELBO = log p(x,z) - log q(z|x)
Figure 2.2: Simple schematic of computational ﬂow in a variational autoencoder.
Two for One
By looking at equation 2.11, it can be understood that maximization
of the ELBO Lθ,φ(x) w.r.t. the parameters θ and φ, will concurrently
optimize the two things we care about:
1. It will approximately maximize the marginal likelihood pθ(x).
This means that our generative model will become better.
2. It will minimize the KL divergence of the approximation qφ(z|x)
from the true posterior pθ(z|x), so qφ(z|x) becomes better.
Stochastic Gradient-Based Optimization of the ELBO
An important property of the ELBO, is that it allows joint optimization
w.r.t. all parameters (φ and θ) using stochastic gradient descent (SGD).
We can start out with random initial values of φ and θ, and stochastically
optimize their values until convergence.
Given a dataset with i.i.d. data, the ELBO objective is the sum (or
average) of individual-datapoint ELBO’s:
The individual-datapoint ELBO, and its gradient ∇θ,φLθ,φ(x) is, in
general, intractable. However, good unbiased estimators ˜∇θ,φLθ,φ(x)
exist, as we will show, such that we can still perform minibatch SGD.
Variational Autoencoders
Unbiased gradients of the ELBO w.r.t. the generative model parameters θ are simple to obtain:
∇θLθ,φ(x) = ∇θEqφ(z|x) [log pθ(x, z) −log qφ(z|x)]
= Eqφ(z|x) [∇θ(log pθ(x, z) −log qφ(z|x))]
≃∇θ(log pθ(x, z) −log qφ(z|x))
= ∇θ(log pθ(x, z))
The last line (eq. (2.17)) is a simple Monte Carlo estimator of the second
line (eq. (2.15)), where z in the last two lines (eq. (2.16) and eq. (2.17))
is a random sample from qφ(z|x).
Unbiased gradients w.r.t. the variational parameters φ are more
diﬃcult to obtain, since the ELBO’s expectation is taken w.r.t. the
distribution qφ(z|x), which is a function of φ. I.e., in general:
∇φLθ,φ(x) = ∇φEqφ(z|x) [log pθ(x, z) −log qφ(z|x)]
̸= Eqφ(z|x) [∇φ(log pθ(x, z) −log qφ(z|x))]
In the case of continuous latent variables, we can use a reparameterization trick for computing unbiased estimates of ∇θ,φLθ,φ(x), as
we will now discuss. This stochastic estimate allows us to optimize the
ELBO using SGD; see algorithm 1. See section 2.9.1 for a discussion of
variational methods for discrete latent variables.
Reparameterization Trick
For continuous latent variables and a diﬀerentiable encoder and generative model, the ELBO can be straightforwardly diﬀerentiated w.r.t. both
φ and θ through a change of variables, also called the reparameterization
trick .
Change of variables
First, we express the random variable z ∼qφ(z|x) as some diﬀerentiable
(and invertible) transformation of another random variable ϵ, given z
z = g(ϵ, φ, x)
where the distribution of random variable ϵ is independent of x or φ.
2.4. Reparameterization Trick
Algorithm 1: Stochastic optimization of the ELBO. Since noise
originates from both the minibatch sampling and sampling of p(ϵ),
this is a doubly stochastic optimization procedure. We also refer
to this procedure as the Auto-Encoding Variational Bayes (AEVB)
algorithm.
D: Dataset
qφ(z|x): Inference model
pθ(x, z): Generative model
θ, φ: Learned parameters
(θ, φ) ←Initialize parameters
while SGD not converged do
M ∼D (Random minibatch of data)
ϵ ∼p(ϵ) (Random noise for every datapoint in M)
Compute ˜Lθ,φ(M, ϵ) and its gradients ∇θ,φ ˜Lθ,φ(M, ϵ)
Update θ and φ using SGD optimizer
Gradient of expectation under change of variable
Given such a change of variable, expectations can be rewritten in terms
Eqφ(z|x) [f(z)] = Ep(ϵ) [f(z)]
where z = g(ϵ, φ, x). and the expectation and gradient operators become
commutative, and we can form a simple Monte Carlo estimator:
∇φEqφ(z|x) [f(z)] = ∇φEp(ϵ) [f(z)]
= Ep(ϵ) [∇φf(z)]
where in the last line, z = g(φ, x, ϵ) with random noise sample ϵ ∼p(ϵ).
See ﬁgure 2.3 for an illustration and further clariﬁcation, and ﬁgure 3.2
for an illustration of the resulting posteriors for a 2D toy problem.
Variational Autoencoders
= g(φ,x,ε)
Original form
Reparameterized form
: Deterministic node
: Random node
: Evaluation of f
: Diﬀerentiation of f
Figure 2.3: Illustration of the reparameterization trick. The variational parameters
φ aﬀect the objective f through the random variable z ∼qφ(z|x). We wish to
compute gradients ∇φf to optimize the objective with SGD. In the original form
(left), we cannot diﬀerentiate f w.r.t. φ, because we cannot directly backpropagate
gradients through the random variable z. We can ’externalize’ the randomness in z
by re-parameterizing the variable as a deterministic and diﬀerentiable function of φ,
x, and a newly introduced random variable ϵ. This allows us to ’backprop through
z’, and compute gradients ∇φf.
2.4. Reparameterization Trick
Gradient of ELBO
Under the reparameterization, we can replace an expectation w.r.t.
qφ(z|x) with one w.r.t. p(ϵ). The ELBO can be rewritten as:
Lθ,φ(x) = Eqφ(z|x) [log pθ(x, z) −log qφ(z|x)]
= Ep(ϵ) [log pθ(x, z) −log qφ(z|x)]
where z = g(ϵ, φ, x).
As a result we can form a simple Monte Carlo estimator ˜Lθ,φ(x) of
the individual-datapoint ELBO where we use a single noise sample ϵ
from p(ϵ):
z = g(φ, x, ϵ)
˜Lθ,φ(x) = log pθ(x, z) −log qφ(z|x)
This series of operations can be expressed as a symbolic graph in
software like TensorFlow, and eﬀortlessly diﬀerentiated w.r.t. the parameters θ and φ. The resulting gradient ∇φ ˜Lθ,φ(x) is used to optimize
the ELBO using minibatch SGD. See algorithm 1. This algorithm was
originally referred to as the Auto-Encoding Variational Bayes (AEVB)
algorithm by Kingma and Welling, 2014. More generally, the reparameterized ELBO estimator was referred to as the Stochastic Gradient
Variational Bayes (SGVB) estimator. This estimator can also be used
to estimate a posterior over the model parameters, as explained in the
appendix of .
Unbiasedness
This gradient is an unbiased estimator of the exact single-datapoint
ELBO gradient; when averaged over noise ϵ ∼p(ϵ), this gradient equals
the single-datapoint ELBO gradient:
∇θ,φ ˜Lθ,φ(x; ϵ)
= Ep(ϵ) [∇θ,φ(log pθ(x, z) −log qφ(z|x))] (2.30)
= ∇θ,φ(Ep(ϵ) [log pθ(x, z) −log qφ(z|x)]) (2.31)
= ∇θ,φLθ,φ(x)
Variational Autoencoders
Computation of log qφ(z|x)
Computation of the (estimator of) the ELBO requires computation of
the density log qφ(z|x), given a value of x, and given a value of z or
equivalently ϵ. This log-density is a simple computation, as long as we
choose the right transformation g().
Note that we typically know the density p(ϵ), since this is the density
of the chosen noise distribution. As long as g(.) is an invertible function,
the densities of ϵ and z are related as:
log qφ(z|x) = log p(ϵ) −log dφ(x, ϵ)
where the second term is the log of the absolute value of the determinant
of the Jacobian matrix (∂z/∂ϵ):
log dφ(x, ϵ) = log
We call this the log-determinant of the transformation from ϵ to z. We
use the notation log dφ(x, ϵ) to make explicit that this log-determinant,
similar to g(), is a function of x, ϵ and φ. The Jacobian matrix contains
all ﬁrst derivatives of the transformation from ϵ to z:
∂ϵ = ∂(z1, ..., zk)
∂(ϵ1, ..., ϵk) =
As we will show, we can build very ﬂexible transformations g() for which
log dφ(x, ϵ) is simple to compute, resulting in highly ﬂexible inference
models qφ(z|x).
Factorized Gaussian posteriors
A common choice is a simple factorized Gaussian encoder
qφ(z|x) = N(z; µ, diag(σ2)):
(µ, log σ) = EncoderNeuralNetφ(x)
qφ(zi|x) =
N(zi; µi, σ2
2.5. Factorized Gaussian posteriors
where N(zi; µi, σ2
i ) is the PDF of the univariate Gaussian distribution.
After reparameterization, we can write:
ϵ ∼N(0, I)
(µ, log σ) = EncoderNeuralNetφ(x)
z = µ + σ ⊙ϵ
where ⊙is the element-wise product. The Jacobian of the transformation
from ϵ to z is:
∂ϵ = diag(σ),
i.e. a diagonal matrix with the elements of σ on the diagonal. The
determinant of a diagonal (or more generally, triangular) matrix is the
product of its diagonal terms. The log determinant of the Jacobian is
therefore:
log dφ(x, ϵ) = log
and the posterior density is:
log qφ(z|x) = log p(ϵ) −log dφ(x, ϵ)
log N(ϵi; 0, 1) −log σi
when z = g(ϵ, φ, x).
Full-covariance Gaussian posterior
The factorized Gaussian posterior can be extended to a Gaussian with
full covariance:
qφ(z|x) = N(z; µ, Σ)
A reparameterization of this distribution is given by:
ϵ ∼N(0, I)
z = µ + Lϵ
Variational Autoencoders
where L is a lower (or upper) triangular matrix, with non-zero entries on the diagonal. The oﬀ-diagonal elements deﬁne the correlations
(covariances) of the elements in z.
The reason for this parameterization of the full-covariance Gaussian,
is that the Jacobian determinant is remarkably simple. The Jacobian
in this case is trivial: ∂z
∂ϵ = L. Note that the determinant of a triangular matrix is the product of its diagonal elements. Therefore, in this
parameterization:
log | det(∂z
And the log-density of the posterior is:
log qφ(z|x) = log p(ϵ) −
This parameterization corresponds to the Cholesky decomposition
Σ = LLT of the covariance of z:
(z −E [z|])(z −E [z|])T i
Note that E
= I since ϵ ∼N(0, I).
One way to build a matrix L with the desired properties, namely
triangularity and non-zero diagonal entries, is by constructing it as
(µ, log σ, L′) ←EncoderNeuralNetφ(x)
L ←Lmask ⊙L′ + diag(σ)
and then proceeding with z = µ + Lϵ as described above. Lmask is
a masking matrix with zeros on and above the diagonal, and ones
below the diagonal. Note that due to the masking L, the Jacobian
matrix (∂z/∂ϵ) is triangular with the values of σ on the diagonal. The
log-determinant is therefore identical to the factorized Gaussian case:
2.5. Factorized Gaussian posteriors
More generally, we can replace z = Lϵ+µ with a chain of (diﬀerentiable
and nonlinear) transformations; as long as the Jacobian of each step in
the chain is triangular with non-zero diagonal entries, the log determinant remains simple. This principle is used by inverse autoregressive
ﬂow (IAF) as explored by Kingma et al., 2016 and discussed in chapter
Algorithm 2: Computation of unbiased estimate of singledatapoint ELBO for example VAE with a full-covariance Gaussian
inference model and a factorized Bernoulli generative model. Lmask
is a masking matrix with zeros on and above the diagonal, and
ones below the diagonal.
x: a datapoint, and optionally other conditioning information
ϵ: a random sample from p(ϵ) = N(0, I)
θ: Generative model parameters
φ: Inference model parameters
qφ(z|x): Inference model
pθ(x, z): Generative model
˜L: unbiased estimate of the single-datapoint ELBO Lθ,φ(x)
(µ, log σ, L′) ←EncoderNeuralNetφ(x)
L ←Lmask ⊙L′ + diag(σ)
ϵ ∼N(0, I)
˜Llogqz ←−P
i + log(2π) + log σi))i
▷= qφ(z|x)
˜Llogpz ←−P
i + log(2π)))
p ←DecoderNeuralNetθ(z)
˜Llogpx ←P
i(xi log pi + (1 −xi) log(1 −pi))
▷= pθ(x|z)
˜L = ˜Llogpx + ˜Llogpz −˜Llogqz
Variational Autoencoders
Estimation of the Marginal Likelihood
After training a VAE, we can estimate the probability of data under the
model using an importance sampling technique, as originally proposed
by Rezende et al., 2014. The marginal likelhood of a datapoint can be
written as:
log pθ(x) = log Eqφ(z|x) [pθ(x, z)/qφ(z|x)]
Taking random samples from qφ(z|x), a Monte Carlo estimator of this
log pθ(x) ≈log 1
pθ(x, z(l))/qφ(z(l)|x)
where each z(l) ∼qφ(z|x) is a random sample from the inference model.
By making L large, the approximation becomes a better estimate of the
marginal likelihood, and in fact since this is a Monte Carlo estimator,
for L →∞this converges to the actual marginal likelihood.
Notice that when setting L = 1, this equals the ELBO estimator of the VAE. We can also use the estimator of eq. (2.57) as our
objective function; this is the objective used in importance weighted
autoencoders (IWAE). In that paper, it was also
shown that the objective has increasing tightness for increasing value
of L. It was later shown by Cremer et al., 2017 that the IWAE objective can be re-interpreted as an ELBO objective with a particular
inference model. The downside of these approaches for optimizing a
tighter bound, is that importance weighted estimates have notoriously
bad scaling properties to high-dimensional latent spaces.
Marginal Likelihood and ELBO as KL Divergences
One way to improve the potential tightness of the ELBO, is increasing
the ﬂexibility of the generative model. This can be understood through
a connection between the ELBO and the KL divergence.
2.7. Marginal Likelihood and ELBO as KL Divergences
With i.i.d. dataset D of size ND, the maximum likelihood criterion is:
log pθ(D) =
= EqD(x) [log pθ(x)]
where qD(x) is the empirical (data) distribution, which is a mixture
distribution:
where each component q(i)
D (x) typically corresponds to a Dirac delta
distribution centered at value x(i) in case of continuous data, or a discrete distribution with all probability mass concentrated at value x(i)
in case of discrete data. The Kullback Leibler (KL) divergence between
the data and model distributions, can be rewritten as the negative
log-likelihood, plus a constant:
DKL(qD(x)||pθ(x)) = −EqD(x) [log pθ(x)] + EqD(x) [log qD(x)]
= −log pθ(D) + constant
where constant = −H(qD(x)). So minimization of the KL divergence
above is equivalent to maximization of the data log-likelihood log pθ(D).
Taking the combination of the empirical data distribution qD(x)
and the inference model, we get a joint distribution over data x and
latent variables z: qD,φ(x, z) = qD(x)q(z|x).
The KL divergence of qD,φ(x, z) from pθ(x, z) can be written as the
negative ELBO, plus a constant:
DKL(qD,φ(x, z)||pθ(x, z))
Eqφ(z|x) [log pθ(x, z) −log qφ(z|x)] −log qD(x)
= −Lθ,φ(D) + constant
where constant = −H(qD(x)). So maximization of the ELBO, is equivalent to the minimization of this KL divergence DKL(qD,φ(x, z)||pθ(x, z)).
The relationship between the ML and ELBO objectives can be summa-
Variational Autoencoders
rized in the following simple equation:
DKL(qD,φ(x, z)||pθ(x, z))
= DKL(qD(x)||pθ(x)) + EqD(x) [DKL(qD,φ(z|x)||pθ(z|x))]
≥DKL(qD(x)||pθ(x))
One additional perspective is that the ELBO can be viewed as a maximum likelihood objective in an augmented space. For some ﬁxed choice
of encoder qφ(z|x), we can view the joint distribution pθ(x, z) as an augmented empirical distribution over the original data x and (stochastic)
auxiliary features z associated with each datapoint. The model pθ(x, z)
then deﬁnes a joint model over the original data, and the auxiliary
features. See ﬁgure 2.4.
Challenges
Optimization issues
In our work, consistent with ﬁndings in and
 , we found that stochastic optimization with the
unmodiﬁed lower bound objective can gets stuck in an undesirable stable
equilibrium. At the start of training, the likelihood term log p(x|z) is
relatively weak, such that an initially attractive state is where q(z|x) ≈
p(z), resulting in a stable equilibrium from which it is diﬃcult to escape.
The solution proposed in and is to use an optimization schedule where the weights of the
latent cost DKL(q(z|x)||p(z)) is slowly annealed from 0 to 1 over many
An alternative proposed in is the method of
free bits: a modiﬁcation of the ELBO objective, that ensures that on
average, a certain minimum number of bits of information are encoded
per latent variable, or per group of latent variables.
The latent dimensions are divided into the K groups. We then use
the following minibatch objective, which ensures that using less than λ
nats of information per subset j (on average per minibatch M) is not
2.8. Challenges
Decoder: pθ(x|z)
Prior distribution: pθ(z)
Marginal: pθ(x)
Encoder: qφ(z|x)
Marginal: qφ(z)
Data distribution: qD(x)
ML objective = - DKL( qD(x) || pθ(x) )
ELBO objective = - DKL( qD,φ(x,z) || pθ(x,z) )
qD,φ(x,z) = qD(x) qφ(z|x)
pθ(x,z) = pθ(z) pθ(x|z)
Figure 2.4: The maximum likelihood (ML) objective can be viewed as the minimization of DKL(qD,φ(x)||pθ(x)), while the ELBO objective can be viewed as the
minimization of DKL(qD,φ(x, z)||pθ(x, z)), which upper bounds DKL(qD,φ(x)||pθ(x)).
If a perfect ﬁt is not possible, then pθ(x, z) will typically end up with higher variance
than qD,φ(x, z), because of the direction of the KL divergence.
Variational Autoencoders
advantageous:
eLλ = Ex∼M
Eq(z|x) [log p(x|z)]
maximum(λ, Ex∼M [DKL(q(zj|x)||p(zj))]
Since increasing the latent information is generally advantageous for
the ﬁrst (unaﬀected) term of the objective (often called the negative
reconstruction error), this results in Ex∼M [DKL(q(zj|x)||p(zj))] ≥λ
for all j, in practice. In Kingma et al., 2016 it was found that the method
worked well for a fairly wide range of values (λ ∈[0.125, 0.25, 0.5, 1, 2]),
resulting in signiﬁcant improvement in the resulting log-likelihood on a
benchmark result.
Blurriness of generative model
In section 2.7 we saw that optimizing the ELBO is equivalent to minimizing DKL(qD,φ(x, z)||pθ(x, z)). If a perfect ﬁt between qD,φ(x, z) and
pθ(x, z) is not possible, then the variance of pθ(x, z) and pθ(x) will end
up larger than the variance qD,φ(x, z) and the data qD,φ(x). This is due
to the direction of the KL divergence; if there are values of (x, z) which
are likely under qD,φ but not under pθ, the term EqD,φ(x,z) [log pθ(x, z)]
will go to inﬁnity. However, the reverse is not true: the generative model
is only slightly penalized when putting probability mass on values of
(x, z) with no support under qD,φ.
Issues with ’blurriness’ can thus can be countered by choosing
a suﬃciently ﬂexible inference model, and/or a suﬃciently ﬂexible
generative model. In the next two chapters we will discuss techniques
for constructing ﬂexible inference models and ﬂexible generative models.
Related prior and concurrent work
Here we brieﬂy discuss relevant literature prior to and concurrent with
the work in .
The wake-sleep algorithm is another on-line
learning method, applicable to the same general class of continuous
latent variable models. Like our method, the wake-sleep algorithm
2.9. Related prior and concurrent work
employs a recognition model that approximates the true posterior. A
drawback of the wake-sleep algorithm is that it requires a concurrent
optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood. An
advantage of wake-sleep is that it also applies to models with discrete
latent variables. Wake-Sleep has the same computational complexity as
AEVB per datapoint.
Variational inference has a long history in the ﬁeld of machine learning. We refer to for a comprehensive
overview and synthesis of ideas around variational inference for exponential family graphical models. Among other connections, shows how various inference algorithms (such as
expectation propagation, sum-product, max-product and many others) can be understood as exact or approximate forms of variational
inference.
Stochastic variational inference Hoﬀman et al., 2013 has received
increasing interest. Blei et al., 2012 introduced a control variate schemes
to reduce the variance of the score function gradient estimator, and
applied the estimator to exponential family approximations of the
posterior. In some general methods, e.g. a
control variate scheme, were introduced for reducing the variance of the
original gradient estimator. In , a similar
reparameterization as in this work was used in an eﬃcient version of
a stochastic variational inference algorithm for learning the natural
parameters of exponential-family approximating distributions.
In Graves, 2011 a similar estimator of the gradient is introduced;
however the estimator of the variance is not an unbiased estimator w.r.t.
the ELBO gradient.
The VAE training algorithm exposes a connection between directed
probabilistic models (trained with a variational objective) and autoencoders. A connection between linear autoencoders and a certain class
of generative linear-Gaussian models has long been known. In it was shown that PCA corresponds to the maximum-likelihood
(ML) solution of a special case of the linear-Gaussian model with a prior
p(z) = N(0, I) and a conditional distribution p(x|z) = N(x; Wz, ϵI),
speciﬁcally the case with inﬁnitesimally small ϵ. In this limiting case,
Variational Autoencoders
the posterior over the latent variables p(z|x) is a Dirac delta distribution: p(z|x) = δ(z −W′x) where W′ = (WT W)−1WT , i.e., given W
and x there is no uncertainty about latent variable z. Roweis, 1998
then introduces an EM-type approach to learning W. Much earlier
work showed that optimization of linear
autoencoders retrieves the principal components of data, from which
it follows that learning linear autoencoders correspond to a speciﬁc
method for learning the above case of linear-Gaussian probabilistic
model of the data. However, this approach using linear autoencoders
is limited to linear-Gaussian models, while our approach applies to a
much broader class of continuous latent variable models.
When using neural networks for both the inference model and the
generative model, the combination forms a type of autoencoder with a speciﬁc regularization term:
˜Lθ,φ(x; ϵ) =
log pθ(x|z)
Negative reconstruction error
+ log pθ(z) −log qφ(z|x)
Regularization terms
In an analysis of plain autoencoders it was
shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound ) of the mutual information between input X and
latent representation Z. Maximizing (w.r.t. parameters) of the mutual
information is equivalent to maximizing the conditional entropy, which
is lower bounded by the expected log-likelihood of the data under the
autoencoding model , i.e. the negative reconstruction error. However, it is well known that this reconstruction criterion is
in itself not suﬃcient for learning useful representations . Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and
sparse autoencoder variants . The VAE objective
contains a regularization term dictated by the variational bound, lacking
the usual nuisance regularization hyper-parameter required to learn
useful representations. Related are also encoder-decoder architectures
such as the predictive sparse decomposition (PSD) , from which we drew some inspiration. Also relevant are the recently introduced Generative Stochastic Networks 
2.9. Related prior and concurrent work
where noisy autoencoders learn the transition operator of a Markov
chain that samples from the data distribution. In a recognition model was employed for eﬃcient learning with Deep Boltzmann Machines. These methods are targeted at
either unnormalized models (i.e. undirected models like Boltzmann
machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic
The proposed DARN method , also learns a
directed probabilistic model using an autoencoding structure, however
their method applies to binary latent variables. In concurrent work,
Rezende et al., 2014 also make the connection between autoencoders,
directed probabilistic models and stochastic variational inference using the reparameterization trick we describe in . Their work was developed independently of ours and provides
an additional perspective on the VAE.
Score function estimator
An alternative unbiased stochastic gradient estimator of the ELBO is
the score function estimator :
∇φEqφ(z|x) [f(z)] = Eqφ(z|x) [f(z)∇φ log qφ(z|x)]
≃f(z)∇φ log qφ(z|x)
where z ∼qφ(z|x).
This is also known as the likelihood ratio estimator and the REINFORCE gradient estimator .
The method has been successfully used in various methods like neural
variational inference , black-box variational
inference , automated variational inference
 , and variational stochastic search , often in combination with various novel control variate
techniques for variance reduction. An advantage
of the likelihood ratio estimator is its applicability to discrete latent
variables.
Variational Autoencoders
We do not directly compare to these techniques, since we concern
ourselves with continuous latent variables, in which case we have (computationally cheap) access to gradient information ∇z log pθ(x, z), courtesy
of the backpropagation algorithm. The score function estimator solely
uses the scalar-valued log pθ(x, z), ignoring the gradient information
about the function log pθ(x, z), generally leading to much higher variance. This has been experimentally conﬁrmed by e.g. , which ﬁnds that a sophisticated score function estimator requires
two orders of magnitude more samples to arrive at the same variance
as a reparameterization based estimator.
The diﬀerence in eﬃciency of our proposed reparameterization-based
gradient estimator, compared to score function estimators, can intuitively be understood as removing an information bottleneck during the
computation of gradients of the ELBO w.r.t. φ from current parameters
θ: in the latter case, this computation is bottlenecked by the scalar
value log pθ(x, z), while in the former case it is bottlenecked by the
much wider vector ∇z log pθ(x, z).
Beyond Gaussian Posteriors
In this chapter we discuss techniques for improving the ﬂexibility of the
inference model qφ(z|x). Increasing the ﬂexibility and accuracy of the
inference model wel generally improve the tightness of the variational
bound (ELBO), bringing it closer the true marginal likelihood objective.
Requirements for Computational Tractability
Requirements for the inference model, in order to be able to eﬃciently
optimize the ELBO, are that it is (1) computationally eﬃcient to
compute and diﬀerentiate its probability density qφ(z|x), and (2) computationally eﬃcient to sample from, since both these operations need
to be performed for each datapoint in a minibatch at every iteration
of optimization. If z is high-dimensional and we want to make eﬃcient
use of parallel computational resources like GPUs, then parallelizability
of these operations across dimensions of z is a large factor towards
eﬃciency. This requirement restricts the class of approximate posteriors
q(z|x) that are practical to use. In practice this often leads to the use
of simple Gaussian posteriors. However, as explained, we also need the
density q(z|x) to be suﬃciently ﬂexible to match the true posterior
p(z|x), in order to arrive at a tight bound.
Beyond Gaussian Posteriors
Improving the Flexibility of Inference Models
Here we will review two general techniques for improving the ﬂexibility
of approximate posteriors in the context of gradient-based variational
inference: auxiliary latent variables, and normalizing ﬂows.
Auxiliary Latent Variables
One method for improving the ﬂexibility of inference models, is through
the introduction of auxiliary latent variables, as explored by Salimans
et al., 2015, and Maaløe et al., 2016.
The methods work by augmenting both the inference model and
the generative model with a continuous auxiliary variable, here denoted
The inference model deﬁnes a distribution over both u and and z,
which can, for example, factorize as:
qφ(u, z|x) = qφ(u|x)qφ(z|u, x)
This inference model augmented with u, implicitly deﬁnes a potentially
powerful implicit marginal distribution:
qφ(u, z|x) du
Likewise, we introduce an additional distribution in the generative
model: such that our generative model is now over the joint distribution
pθ(x, z, u). This can, for example, factorize as:
pθ(x, z, u) = pθ(u|x, z)pθ(x, z)
The ELBO objective with auxiliary variables, given empirical distribution qD(x), is then (again) equivalent to minimization of a KL
divergence:
Eqφ(u,z|x) [log pθ(x, z, u) −log qφ(u, z|x)]
= DKL(qD,φ(x, z, u)||pθ(x, z, u)
Recall that maximization of the original ELBO objective, without
auxiliary variables, is equivalent to minimization of DKL(qD,φ(x, z)||
3.2. Improving the Flexibility of Inference Models
pθ(x, z)), and that maximization of the expected marginal likelihood is
equivalent to minimization of DKL(qD,φ(x)||pθ(x)).
We can gain additional understanding into the relationship between
the objectives, through the following equation:
DKL(qD,φ(x, z, u)||pθ(x, z, u))
(= ELBO loss with auxiliary variables)
= DKL(qD,φ(x, z)||pθ(x, z)) + EqD(x,z) [DKL(qD,φ(u|x, z)||pθ(u|x, z))]
≥DKL(qD,φ(x, z)||pθ(x, z))
(= original ELBO objective))
= DKL(qD(x)||pθ(x)) + EqD(x) [DKL(qD,φ(z|x)||pθ(z|x))]
≥DKL(qD(x)||pθ(x))
(= Marginal log-likelihood objective)
From this equation it can be seen that in principle, the ELBO gets
worse by augmenting the VAE with an auxiliary variable u:
DKL(qD,φ(x, z, u)||pθ(x, z, u)) ≥DKL(qD,φ(x, z)||pθ(x, z))
But because we now have access to a much more ﬂexible class of inference
distributions, qφ(z|x), the original ELBO objective DKL(qD,φ(x, z)||
pθ(x, z)) can improve, potentially outweighing the additional cost of
EqD(x,z) [DKL(qD,φ(u|x, z)||pθ(u|x, z))]. In , and it was shown that auxiliary
variables can indeed lead to signiﬁcant improvements in models.
The introduction of auxiliary latent variables in the graph, are a
special case of VAEs with multiple layers of latent variables, which are
discussed in chapter 4. In our experiment with CIFAR-10, we make use
of multiple layers of stochastic variables.
Normalizing Flows
An alternative approach towards ﬂexible approximate posteriors is
Normalizing Flow (NF), introduced by 
in the context of stochastic gradient variational inference. In normalizing
ﬂows, we build ﬂexible posterior distributions through an iterative
procedure. The general idea is to start oﬀwith an initial random variable
Beyond Gaussian Posteriors
with a relatively simple distribution with a known (and computationally
cheap) probability density function, and then apply a chain of invertible
parameterized transformations ft, such that the last iterate zT has a
more ﬂexible distribution1:
for t = 1...T :
ϵt = ft(ϵt−1, x)
The Jacobian of the transformation factorizes:
So its determinant also factorizes as well:
As long as the Jacobian determinant of each of the transformations ft
can be computed, we can still compute the probability density function
of the last iterate:
log qφ(z|x) = log p(ϵ0) −
Rezende and Mohamed, 2015 experimented with a transformation
of the form:
ft(ϵt−1) = ϵt−1 + uh(wT ϵt−1 + b)
where u and w are vectors, wT is w transposed, b is a scalar and h(.)
is a nonlinearity, such that uh(wT zt−1 + b) can be interpreted as a
MLP with a bottleneck hidden layer with a single unit. This ﬂow does
not scale well to a high-dimensional latent space: since information
goes through the single bottleneck, a long chain of transformations is
required to capture high-dimensional dependencies.
1where x is the context, such as the value of the datapoint. In case of models
with multiple levels of latent variables, the context also includes the value of the
previously sampled latent variables.
3.3. Inverse Autoregressive Transformations
Inverse Autoregressive Transformations
In order to ﬁnd a type of normalizing ﬂow that scales well to a highdimensional space, Kingma et al., 2016 consider Gaussian versions of
autoregressive autoencoders such as MADE and
the PixelCNN . Let y be a variable modeled by
such a model, with some chosen ordering on its elements y = {yi}D
We will use [µ(y), σ(y)] to denote the function of the vector y, to the
vectors µ and σ. Due to the autoregressive structure, the Jacobian
matrix is triangular with zeros on the diagonal: ∂[µi, σi]/∂yj = 
for j ≥i. The elements [µi(y1:i−1), σi(y1:i−1)] are the predicted mean
and standard deviation of the i-th element of y, which are functions of
only the previous elements in y.
Sampling from such a model is a sequential transformation from a
noise vector ϵ ∼N(0, I) to the corresponding vector y: y0 = µ0+σ0⊙ϵ0,
and for i > 0, yi = µi(y1:i−1) + σi(y1:i−1) · ϵi. The computation involved
in this transformation is clearly proportional to the dimensionality D.
Since variational inference requires sampling from the posterior, such
models are not interesting for direct use in such applications. However,
the inverse transformation is interesting for normalizing ﬂows. As long
as we have σi > 0 for all i, the sampling transformation above is a
one-to-one transformation, and can be inverted:
ϵi = yi −µi(y1:i−1)
σi(y1:i−1)
Kingma et al., 2016 make two key observations, important for
normalizing ﬂows. The ﬁrst is that this inverse transformation can be
parallelized, since (in case of autoregressive autoencoders) computations
of the individual elements ϵi do not depend on each other. The vectorized
transformation is:
ϵ = (y −µ(y))/σ(y)
where the subtraction and division are element-wise.
The second key observation, is that this inverse autoregressive operation has a simple Jacobian determinant. Note that due to the autoregressive structure, ∂[µi, σi]/∂yj = for j ≥i. As a result, the
Beyond Gaussian Posteriors
transformation has a lower triangular Jacobian (∂ϵi/∂yj = 0 for j > i),
with a simple diagonal: ∂ϵi/∂yi = 1
σi . The determinant of a lower triangular matrix equals the product of the diagonal terms. As a result, the
log-determinant of the Jacobian of the transformation is remarkably
simple and straightforward to compute:
−log σi(y)
The combination of model ﬂexibility, parallelizability across dimensions,
and simple log-determinant, makes this transformation interesting for
use as a normalizing ﬂow over high-dimensional latent space.
For the following section we will use a slightly diﬀerent, but equivalently ﬂexible, transformation of the type:
ϵ = σ(y) ⊙y + µ(y)
With corresponding log-determinant:
Inverse Autoregressive Flow (IAF)
Kingma et al., 2016 propose inverse autoregressive ﬂow (IAF) based
on a chain of transformations that are each equivalent to an inverse
autoregressive transformation of eq. (3.19) and eq. (3.21). See algorithm
3 for pseudo-code of an approximate posterior with the proposed ﬂow.
We let an initial encoder neural network output µ0 and σ0, in addition
to an extra output h, which serves as an additional input to each
subsequent step in the ﬂow. The chain is initialized with a factorized
Gaussian qφ(z0|x) = N(µ0, diag(σ0)2):
ϵ0 ∼N(0, I)
(µ0, log σ0, h) = EncoderNeuralNet(x; θ)
z0 = µ0 + σ0 ⊙ϵ0
3.4. Inverse Autoregressive Flow (IAF)
Algorithm 3:
Pseudo-code of an approximate posterior with
Inverse Autoregressive Flow (IAF).
Input: EncoderNN(x; θ) is an encoder neural network, with
additional output h.
Input: AutoregressiveNN(z; h, t, θ) is a neural network that is
autoregressive over z, with additional inputs h and t.
Input: T signiﬁes the number of steps of ﬂow.
x: a datapoint, and optionally other conditioning information
θ: neural network parameters
z: a random sample from q(z|x), the approximate posterior
distribution
l: the scalar value of log q(z|x), evaluated at sample ’z’
[µ, σ, h] ←EncoderNN(x; θ)
ϵ ∼N(0, I)
z ←σ ⊙ϵ + µ
i(log σi + 1
2 log(2π))
for t ←1 to T do
[m, s] ←AutoregressiveNN(z; h, t, θ)
σ ←(1 + exp(−s))−1
z ←σ ⊙z + (1 −σ) ⊙m
IAF then consists of a chain of T of the following transformations:
(µt, σt) = AutoregressiveNeuralNett(ϵt−1, h; θ)
ϵt = µt + σt ⊙ϵt−1
Each step of this ﬂow is an inverse autoregressive transformation of
the type of eq. (3.19) and eq. (3.21), and each step uses a separate
autoregressive neural network. Following eq. (3.16), the density under
Beyond Gaussian Posteriors
Approximate Posterior with Inverse Autoregressive Flow (IAF)
Encoder Neural Net
Autoregressive Neural Net
Figure 3.1: Like other normalizing ﬂows, drawing samples from an approximate
posterior with Inverse Autoregressive Flow (IAF) starts with
a distribution with tractable density, such as a Gaussian with diagonal covariance,
followed by a chain of nonlinear invertible transformations of z, each with a simple
Jacobian determinant. The ﬁnal iterate has a ﬂexible distribution.
the ﬁnal iterate is:
log q(z|x) = −
2 log(2π) +
The ﬂexibility of the distribution of the ﬁnal iterate ϵT , and its
ability to closely ﬁt to the true posterior, increases with the expressivity
of the autoregressive models and the depth of the chain. See ﬁgure 3.1
for an illustration of the computation.
A numerically stable version, inspired by the LSTM-type update,
is where we let the autoregressive network output (mt, st), two uncon-
3.4. Inverse Autoregressive Flow (IAF)
(a) Prior distribution (b) Factorized posteriors
(c) IAF posteriors
Figure 3.2: Best viewed in color. We ﬁtted a variational autoencoder (VAE) with
a spherical Gaussian prior, and with factorized Gaussian posteriors (b) or inverse
autoregressive ﬂow (IAF) posteriors (c) to a toy dataset with four datapoints. Each
colored cluster corresponds to the posterior distribution of one datapoint. IAF greatly
improves the ﬂexibility of the posterior distributions, and allows for a much better
ﬁt between the posteriors and the prior.
strained real-valued vectors, and compute ϵt as:
(mt, st) = AutoregressiveNeuralNett(ϵt−1, h; θ)
σt = sigmoid(st)
ϵt = σt ⊙ϵt−1 + (1 −σt) ⊙mt
This version is shown in algorithm 3. Note that this is just a particular
version of the update of eq. (3.27), so the simple computation of the
ﬁnal log-density of eq. (3.29) still applies.
It was found beneﬁcial for results to parameterize or initialize the
parameters of each AutoregressiveNeuralNett such that its outputs st
are, before optimization, suﬃciently positive, such as close to +1 or +2.
This leads to an initial behavior that updates ϵ only slightly with each
step of IAF. Such a parameterization is known as a ’forget gate bias’ in
LSTMs, as investigated by Jozefowicz et al., 2015.
It is straightforward to see that a special case of IAF with one
step, and a linear autoregressive model, is the fully Gaussian posterior
discussed earlier. This transforms a Gaussian variable with diagonal
covariance, to one with linear dependencies, i.e. a Gaussian distribution
with full covariance.
Autoregressive neural networks form a rich family of nonlinear
Beyond Gaussian Posteriors
transformations for IAF. For non-convolutional models, the family of
masked autoregressive network introduced in 
was used as the autoregressive neural networks. For CIFAR-10 experiments, which beneﬁts more from scaling to high dimensional latent
space, the family of convolutional autoregressive autoencoders introduced by
 was
It was found that results improved when reversing the ordering of the
variables after each step in the IAF chain. This is a volume-preserving
transformation, so the simple form of eq. (3.29) remains unchanged.
Related work
As we explained, inverse autoregressive ﬂow (IAF) is a member of the
family of normalizing ﬂows, ﬁrst discussed in in the context of stochastic variational inference. In two speciﬁc types of ﬂows are introduced: planar ﬂow
(eq. (3.17)) and radial ﬂow. These ﬂows are shown to be eﬀective to
problems with a relatively low-dimensional latent space. It is not clear,
however, how to scale such ﬂows to much higher-dimensional latent
spaces, such as latent spaces of generative models of larger images, and
how planar and radial ﬂows can leverage the topology of latent space,
as is possible with IAF. Volume-conserving neural architectures were
ﬁrst presented in in , as a form of nonlinear
independent component analysis.
Another type of normalizing ﬂow, introduced by 
(NICE), uses similar transformations as IAF. In contrast with IAF,
NICE was directly applied to the observed variables in a generative
model. NICE is type of transformations that updates only half of the
variables z1:D/2 per step, adding a vector f(zD/2+1:D) which is a neural
network based function of the remaining latent variables zD/2+1:D.
Such large blocks have the advantage of computationally cheap inverse
transformation, and the disadvantage of typically requiring longer chains.
In experiments, found that this type of
transformation is generally less powerful than other types of normalizing
ﬂow, in experiments with a low-dimensional latent space. Concurrently
3.5. Related work
to our work, NICE was extended to high-dimensional spaces in (Real NVP).
A potentially powerful transformation is the Hamiltonian ﬂow used
in Hamiltonian Variational Inference . Here, a
transformation is generated by simulating the ﬂow of a Hamiltonian
system consisting of the latent variables z, and a set of auxiliary momentum variables. This type of transformation has the additional beneﬁt
that it is guided by the exact posterior distribution, and that it leaves
this distribution invariant for small step sizes. Such a transformation
could thus take us arbitrarily close to the exact posterior distribution
if we can apply it a suﬃcient number of times. In practice, however,
Hamiltonian Variational Inference is very demanding computationally.
Also, it requires an auxiliary variational bound to account for the auxiliary variables, which can impede progress if the bound is not suﬃciently
An alternative method for increasing the ﬂexibility of variational
inference is the introduction of auxiliary latent variables , discussed in
3.2.1, and corresponding auxiliary inference models. Latent variable
models with multiple layers of stochastic variables, such as the one
used in our experiments, are often equivalent to such auxiliary-variable
methods. We combine deep latent variable models with IAF in our
experiments, beneﬁting from both techniques.
Deeper Generative Models
In the previous chapter we explain advanced strategies for improving
inference models. In this chapter, we review strategies for learning
deeper generative models, such as inference and learning with multiple
latent variables or observed variables, and techniques for improving the
ﬂexibility of the generative models pθ(x, z).
Inference and Learning with Multiple Latent Variables
The generative model pθ(x, z), and corresponding inference model
qφ(z|x) can be parameterized as any directed graph. Both x and z
can be composed of multiple variables with some topological ordering.
It may not be immediately obvious how to optimize such models in the
VAE framework; it is, however, quite straightforward, as we will now
Let z = {z1, ..., zK}, and qφ(z|x) = qφ(z1, ..., zK|x) where the subscript corresponds with the topological ordering of each variable. Given
a datapoint x, computation of the ELBO estimator consists of two steps:
1. Sampling z ∼qφ(z|x). In case of multiple latent variables, this
means ancestral sampling the latent variables one by one, in
4.1. Inference and Learning with Multiple Latent Variables
topological ordering deﬁned by the inference model’s directed
graph. In pseudo-code, the ancestral sampling step looks like:
for i = 1...K :
zi ∼qφ(zi|Pa(zi))
where Pa(zi) are the parents of variable zi in the inference model,
which may include x. In reparameterized (and diﬀerentiable) form,
for i = 1...K :
zi = gi(ϵi, Pa(zi), φ)
2. Evaluating the scalar value (log pθ(x, z) −log qφ(z|x)) at the resulting sample z and datapoint x. This scalar is the unbiased
stochastic estimate lower bound on log pθ(x). It is also diﬀerentiable and optimizable with SGD.
Choice of ordering
It should be noted that the choice of latent variables’ topological ordering
for the inference model can be diﬀerent from the choice of ordering for
the generative model.
Since the inference model has the data as root node, while the
generative model has the data as leaf node, one (in some sense) logical
choice would be to let the topological ordering of the latent variables
in the inference model be the reverse of the ordering in the generative
In multiple works it has been shown that it can be advantageous to let the
generative model and inference model share the topological ordering
of latent variables. The two choices of ordering are illustrated in ﬁgure
4.1. One advantage of shared ordering, as explained in these works, is
that this allows us to easily share parameters between the inference and
generative models, leading to faster learning and better solutions.
Deeper Generative Models
Deep generative model
Bottom-up inference model
VAE with bottom-up inference
(a) VAE with bottom-up inference.
Deep generative model
Top-down inference model
VAE with top-down inference
(b) VAE with top-down inference.
Figure 4.1: Illustration, taken from Kingma et al., 2016, of two choices of directionality of the inference model. Sharing directionality of inference, as in (b), has
the beneﬁt that it allows for straightforward sharing of parameters between the
generative model and the inference model.
To see why this might be a good idea, note that the true posterior
over the latent variables, is a function of the prior:
pθ(z|x) ∝pθ(z)pθ(x|z)
Likewise, the posterior of a latent variable given its parents (in the
generative model), is:
pθ(zi|x, Pa(zi)) ∝pθ(zi|Pa(zi))pθ(x|zi, Pa(zi))
Optimization of the generative model changes both pθ(zi|Pa(zi)) and
pθ(x|zi, Pa(zi)). By coupling the inference model qφ(zi|x, Pa(zi)) and
4.2. Alternative methods for increasing expressivity
prior pθ(zi|Pa(zi)), changes in pθ(zi|Pa(zi)) can be directly reﬂected
in changes in qφ(zi|Pa(zi)).
This coupling is especially straightforward when pθ(zi|Pa(zi)) is
Gaussian distributed. The inference model can be directly speciﬁed
as the product of this Gaussian distribution, with a learned quadratic
pseudo-likelihood term:
qφ(zi|Pa(zi), x) = pθ(zi|Pa(zi))˜l(zi; x, Pa(zi))/Z,
where Z is tractable to compute. This idea is explored by . In principle this idea could be
extended to a more general class of conjugate priors, but no work on
this is known at the time of writing.
A less constraining variant, explored by , is
to simply let the neural network that parameterizes qφ(zi|Pa(zi), x) be
partially speciﬁed by a part of the neural network that parameterizes
pθ(zi|Pa(zi)). In general, we can let the two distributions share parameters. This allows for more complicated posteriors, like normalizing ﬂows
Alternative methods for increasing expressivity
Typically, especially with large data sets, we wish to choose an expressive
class of directed models, such that it can feasibly approximate the true
distribution. Popular strategies for specifying expressive models are:
• Introduction of latent variables into the directed models, and optimization through (amortized) variational inference, as explained
in this work.
• Full autoregression: factorization of distributions into univariate
(one-dimensional) conditionals, or at least very low-dimensional
conditionals (section 4.3).
• Speciﬁcation of distributions through invertible transformations
with tractable Jacobian determinant (section 4.4).
Synthesis from fully autoregressive models models is relatively slow,
since the length of computation for synthesis from such models is linear in the dimensionality of the data. The length of computation of
Deeper Generative Models
the log-likelihood of fully autoregressive models does not necesarilly
scale with the dimensionality of the data. In this respect, introduction of latent variables for improving expressivity is especially interesting when x is very high-dimensional. It is relatively straightforward and computationally attractive, due to parallelizability, to specify directed models over high-dimensional variables where each conditional factorizes into independent distributions. For example, if we let
pθ(xj|Pa(xj)) = Q
k pθ(xj,k|Pa(xj)), where each factor is a univariate
Gaussian whose means and variance are nonlinear functions (speciﬁed
by a neural network) of the parents Pa(xj), then computations for both
synthesis and evaluation of log-likelihood can be fully parallelized across
dimensions k. See for experiments demonstrating
a 100x improvement in speed of synthesis.
The best models to date, in terms of attained log-likelihood on test
data, employ a combination of the three approaches listed above.
Autoregressive Models
A powerful strategy for modeling high-dimensional data is to divide up
the high-dimensional observed variables into small constituents (often
single dimensional parts, or otherwise just parts with a small number of
dimensions), impose a certain ordering, and to model their dependencies
as a directed graphical model. The resulting directed graphical model
breaks up the joint distribution into a product of a factors:
pθ(x) = pθ(x1, ..., xD) = pθ(x1)
pθ(xj|Pa(xj))
where D is the dimensionality of the data. This is known as an autoregressive (AR) model. In case of neural network based autoregressive
models, we let the conditional distributions be parameterized with a
neural network:
pθ(xj|x<j) = pθ(xj|NeuralNetj
θ(Pa(xj)))
In case of continuous data, autoregressive models can be interpreted
as a special case of a more general approach: learning an invertible
transformation from the data to a simpler, known distribution such as
4.4. Invertible transformations with tractable Jacobian determinant
a Gaussian or Uniform distribution; this approach with invertible transformations is discussed in section 4.4. The techniques of autoregressive
models and invertible transformations can be naturally combined with
variational autoencoders, and at the time of writing, the best systems
use a combination Rezende and Mohamed, 2015; Kingma et al., 2016;
Gulrajani et al., 2017.
A disadvantage of autoregressive models, compared to latent-variable
models, is that ancestral sampling from autoregressive models is a
sequential operation computation of O(D) length, i.e. proportional
to the dimensionality of the data. Autoregressive models also require
choosing a speciﬁc ordering of input elements (equation (4.8)). When no
single natural one-dimensional ordering exists, like in two-dimensional
images, this leads to a model with a somewhat awkward inductive bias.
Invertible transformations with tractable Jacobian determinant
In case of continuous data, autoregressive models can be interpreted
as a special case of a more general approach: learning an invertible
transformation with tractable Jacobian determinant (also called normalizing ﬂow) from the data to a simpler, known distribution such as a
Gaussian or Uniform distribution. If we use neural networks for such
invertible mappings, this is a powerful and ﬂexible approach towards
probabilistic modeling of continuous data and nonlinear independent
component analysis .
Such normalizing ﬂows iteratively update a variable, which is constrained to be of the same dimensionality as the data, to a target distribution. This constraint on the dimensionality of intermediate states
of the mapping can make such transformations more challenging to
optimize than methods without such constraint. An obvious advantage,
on the other hand, is that the likelihood and its gradient are tractable.
In , particularly interesting ﬂows
(NICE and Real NVP) were introduced, with equal computational cost
and depth in both directions, making it both relatively cheap to optimize
and to sample from such models. At the time of writing, no such model
has yet been demonstrated to lead to the similar performance as purely
autoregressive or VAE-based models in terms of data log-likelihood, but
Deeper Generative Models
this remains an active area of research.
Follow-Up Work
Some important applications and motivations for deep generative models
and variational autoencoders are:
• Representation learning: learning better representations of the
data. Some uses of this are:
– Data-eﬃcient learning, such as semi-supervised learning
– Visualisation of data as low-dimensional manifolds
• Artiﬁcial creativity: plausible interpolation between data and
extrapolation from data.
Here we will now highlight some concrete applications to representation learning and artiﬁcial creativity.
Representation Learning
In the case of supervised learning, we typically aim to learn a conditional
distribution: to predict the distribution over the possible values of a
variable, given the value of some another variable. One such problem
is that of image classiﬁcation: given an image, the prediction of a
distribution over the possible class labels. Through the yearly ImageNet
competion , it has become clear that deep
convolutional neural networks (CNNs), given a large amount of labeled images, are extraordinarily
good at solving the image classiﬁcation task. Modern versions of CNNs
based on residual networks, which is a variant of LSTM-type neural
networks , now arguably achieves
human-level classiﬁcation accuracy on this task .
When the number of labeled examples is low, solutions found with
purely supervised approaches tend to exhibit poor generalization to
new data. In such cases, generative models can be employed as an
eﬀective type of regularization. One particular strategy, presented in
4.5. Follow-Up Work
Kingma et al., 2014, is to optimize the classiﬁcation model jointly with
a variational autoencoder over the input variables, sharing parameters
between the two. The variational autoencoder, in this case, provides an
auxiliary objective, improving the data eﬃciency of the classiﬁcation
solution. Through sharing of statistical strength between modeling
problems, this can greatly improve upon the supervised classiﬁcation
error. Techniques based on VAEs are now among state of the art for semisupervised classiﬁcation , with on average under 1%
classiﬁcation error in the MNIST classiﬁcation problem, when trained
with only 10 labeled images per class, i.e. when more than 99.8% of the
labels in the training set were removed. In concurrent work , it was shown that VAE-based semi-supervised learning
can even do well when only a single sample per class is presented.
A standard supervised approach, GoogLeNet ,
which normally achieves near state-of-the-art performance on the ImageNet validation set, achieves only around 5% top-1 classiﬁcation accuracy when trained with only 1% of the labeled images, as shown by Pu
et al., 2016. In contrast, they show that a semi-supervised approach
with VAEs achieves around 45% classiﬁcation accuracy on the same
task, when modeling the labels jointly with the labeled and unlabeled
input images.
Understanding of data, and artiﬁcial creativity
Generative models with latent spaces allow us to transform the data
into a simpler latent space, explore it in that space, and understand it
better. A related branch of applications of deep generative models is
the synthesis of plausible pseudo-data with certain desirable properties,
sometimes coined as artiﬁcial creativity.
Chemical Design
One example of a recent scientiﬁc application of artiﬁcial creativity,
is shown in
Gómez-Bombarelli et al., 2018. In this paper, a fairly
straightforward VAE is trained on hundreds of thousands of existing
chemical structures. The resulting continuous representation (latent
space) is subsequently used to perform gradient-based optimization
Deeper Generative Models
Figure 4.2: (a) Application of a VAE to chemical design in . A latent continuous representation z of molecules is learned on a large
dataset of molecules. (b) This continuous representation enables gradient-based
search of new molecules that maximizes f(z), a certain desired property.
towards certain properties; the method is demonstrated on the design
of drug-like molecules and organic light-emitting diodes. See ﬁgure 4.2.
Natural Language Synthesis
A similar approach was used to generating natural-language sentences
from a continuous space by Bowman et al., 2015. In this paper, it is
shown how a VAE can be successfully trained on text. The model is
shown to succesfully interpolate between sentences, and for imputation
of missing words. See ﬁgure 4.3.
4.5. Follow-Up Work
Figure 4.3: An application of VAEs to interpolation between pairs of sentences,
from . The intermediate sentences are grammatically correct,
and the topic and syntactic structure are typically locally consistent.
In , VAEs are applied to simulate observations
of distant galaxies. This helps with the calibration of systems that need
to indirectly detect the shearing of observations of distant galaxies,
caused by weak gravitational lensing in the presence of dark matter
between earth and those galaxies. Since the lensing eﬀects are so weak,
such systems need to be calibrated with ground-truth images with a
known amount of shearing. Since real data is still limited, the proposed
solution is to use deep generative models for synthesis of pseudo-data.
Image (Re-)Synthesis
A popular application is image (re)synthesis. One can optimize a VAE to
form a generative model over images. One can synthesize images from the
generative model, but the inference model (or encoder) also allows one
to encode real images into a latent space. One can modify the encoding
in this latent space, then decode the image back into the observed
space. Relatively simple transformations in the observed space, such
as linear transformations, often translate into semantically meaningful
modiﬁcations of the original image. One example, as demonstrated by
White, 2016, is the modiﬁcation of images in latent space along a "smile
vector" in order to make them more happy, or more sad looking. See
ﬁgure 4.4 for an example.
Deeper Generative Models
Figure 4.4: VAEs can be used for image resynthesis. In this example by White,
2016, an original image (left) is modiﬁed in a latent space in the direction of a smile
vector, producing a range of versions of the original, from smiling to sadness.
Other relevant follow-up work
We unfortunately do not have space to discuss all follow-up work in
depth, but will here highlight a selection of relevant recent work.
In addition to our original publication ,
two later papers have proposed equivalent algorithms , where the latter work applies the same
reparameterization gradient method to the estimation of parameter
posteriors, rather than amortized latent-variable inference.
In the appendix of we proposed to apply
the reparameterization gradients to estimation of parameter posteriors.
In this method, with a mixture-of-Gaussians prior
and named Bayes by Backprop, was used in experiments with some
promising early results. In we describe a reﬁned
method, the local reparameterization trick, for further decreasing the
variance of the gradient estimator, and applied it to estimation of Gaussian parameter posteriors. Further results were presented in with
increasingly sophisticated choices of priors and approximate posteriors.
In , a similar reparameterization was used to analyze Dropout as a Bayesian method, coined
4.5. Follow-Up Work
Variational Dropout. In this method was further
analyzed and reﬁned. Various papers have applied reparameterization
gradients for estimating parameter posteriors, including in the context of recurrent neural networks and more generally for Bayesian models and in 
for deep probabilistic programming. A Bayesian nonparametric variational family based in the Gaussian Process using reparameterization
gradients was proposed in .
Normalizing ﬂows were proposed as a
framework for improving the ﬂexibility of inference models. In Kingma
et al., 2016, the ﬁrst normalizing ﬂow was proposed that scales well to
high-dimensional latent spaces. The same principle was later applied
in for density estimation, and further reﬁned
in . Various other ﬂows were proposed in and .
As an alternative to (or in conjunction with) normalizing ﬂows, one
can use auxiliary variables to improve posterior ﬂexibility. This principle
was, to the best of our knowledge, ﬁrst proposed in Salimans et al., 2015.
In this paper, the principle was used in a combination of variational
inference with Hamiltonian Monte Carlo (HMC), with the momentum
variables of HMC as auxiliary variables. Auxiliary variables were more
elaborately discussed in in as Auxiliary Deep
Generative Models. Similarly, one can use deep models with multiple
stochastic layers to improve the variational bound, as demonstrated in
 and as Ladder VAEs.
There has been plenty of follow-up work on gradient variance reduction for the variational parameters of discrete latent variables, as
opposed to continuous latent variables for which reparameterization
gradients apply. These proposals include NVIL
 , MuProp
 , Variational inference for Monte
Carlo objectives , the Concrete distribution and Categorical Reparameterization with
Gumbel-Softmax .
The ELBO objective can be generalized into an importance-weighted
objective, as proposed in (Importance-Weighted
Autoencoders). This potentially reduces the variance in the gradient,
Deeper Generative Models
but has not been discussed in-depth here since (as often the case with
importance-weighted estimators) it can be diﬃcult to scale to highdimensional latent spaces. Other objectives have been proposed such as
Rényi divergence variational inference , Generative
Moment Matching Networks , objectives based on normalizing such as NICE and RealNVP ﬂows , black-box α-divergence minimization and Bi-directional Helmholtz Machines .
Various combinations with adversarial objectives have been proposed. In , the "adversarial autoencoder" (AAE)
was proposed, a probabilistic autoencoder that uses a generative adversarial network (GAN) to perform variational
inference. In Adversarially Learned Inference
(ALI) was proposed, which aims to minimize a GAN objective between
the joint distributions qφ(x, z) and pθ(x, z). Other hybrids have been
proposed as well (Deep convolutional inverse graphics network), a convolutional VAE was applied to
modeling images with some success, building on work by proposing convolutional networks for image synthesis. In
 (DRAW), an attention mechanism was combined
with a recurrent inference model and recurrent generative model for
image synthesis. This approach was further extended in (Towards Conceptual Compression) with convolutional networks,
scalable to larger images, and applied to image compression. In , deep convolutional inference models and generative models
were also applied to images. Furthermore,
 
(PixelVAE) and (Variational Lossy Autoencoder)
combined convolutional VAEs with the PixelCNN model . Methods and VAE architectures
for controlled image generation from attributes or text were studied
in . Predicting the color of pixels based on a
4.5. Follow-Up Work
grayscale image is another promising application . The application to semi-supervised learning has been studied
in among other
Another prominent application of VAEs is modeling of text and
or sequential data . VAEs have also been applied to speech
and handwriting Chung et al., 2015. Sequential models typically use
recurrent neural networks, such as LSTMs , as encoder and/or decoder. When modeling sequences, the validity
of a sequence can sometimes be constrained by a context-free grammar.
In this case, incorporation of the grammar in VAEs can lead to better
models, as shown in
 (Grammar VAEs), and
applied to modeling molecules in textual representations.
Since VAEs can transform discrete observation spaces to continuous
latent-variable spaces with approximately known marginals, they are
interesting for use in model-based control . In (Stochastic Value Gradients) it was
shown that the re-parameterization of the observed variables, together
with an observation model, can be used to compute novel forms of policy
gradients. Variational inference and reparameterization gradients have
also been used for variational information maximisation for intrinsically
motivated reinforcement learning and
VIME for improved exploration. Variational
autoencoders have also been used as components in models that perform
iterative reasoning about objects in a scene .
In (β-VAE) it was proposed to strengthen the
contribution of DKL(qφ(z|x)||pθ(z)), thus restricting the information
ﬂow through the latent space, which was shown to improve disentanglement of latent factors, further studied in .
Other applications include modeling of graphs (Variational Graph Autoencoders), learning of 3D structure from
images , one-shot learning ,
learning nonlinear state space models , voice
Deeper Generative Models
conversion from non-parallel corpora , discriminationaware (fair) representations and transfer learning
 .
The reparameterization gradient estimator discussed in this work
has been extended in various directions , including
acceptance-rejection sampling algorithms . The
gradient variance can in some cases be reduced by ’carving up the
ELBO’ and using
a modiﬁed gradient estimator. A second-order gradient estimator has
also been proposed in .
All in all, this remains an actively researched area with frequently
exciting developments.
Conclusion
Directed probabilistic models form an important aspect of modern
artiﬁcial intelligence. Such models can be made incredibly ﬂexible by
parameterizing the conditional distributions with diﬀerentiable deep
neural networks.
Optimization of such models towards the maximum likelihood objective is straightforward in the fully-observed case. However, one is often
more interested in ﬂexible models with latent variables, such as deep
latent-variable models, or Bayesian models with random parameters. In
both cases one needs to perform approximate posterior estimation for
which variational inference (VI) methods are suitable. In VI, inference
is cast as an optimization problem over newly introduced variational parameters, typically optimized towards the ELBO, a lower bound on the
model evidence, or marginal likelihood of the data. Existing methods for
such posterior inference were either relatively ineﬃcient, or not applicable to models with neural networks as components. Our main contribution is a framework for eﬃcient and scalable gradient-based variational
posterior inference and approximate maximum likelihood learning.
In this work we describe the variational autoencoder (VAE) and
some of its extensions. A VAE is a combination of a deep latent-variable
Conclusion
model (DLVM) with continuous latent variables, and an associated
inference model. The DLVM is a type of generative model over the
data. The inference model, also called encoder or recognition model,
approximates the posterior distribution of the latent variables of the
generative model. Both the generative model and the inference model
are directed graphical models that are wholly or partially parameterized
by deep neural networks. The parameters of the models, including the
parameters of the neural networks such as the weights and biases, are
jointly optimized by performing stochastic gradient ascent on the socalled evidence lower bound (ELBO). The ELBO is a lower bound on
the marginal likelihood of the data, also called the variational lower
bound. Stochastic gradients, necessary for performing SGD, are obtained
through a basic reparameterization trick. The VAE framework is now a
commonly used tool for various applications of probabilistic modeling
and artiﬁcial creativity, and basic implementations are available in most
major deep learning software libraries.
For learning ﬂexible inference models, we proposed inverse autoregressive ﬂows (IAF), a type of normalizing ﬂow that allows scaling
to high-dimensional latent spaces. An interesting direction for further
exploration is comparison with transformations with computationally
cheap inverses, such as NICE and Real NVP . Application of such transformations in the VAE framework
can potentially lead to relatively simple VAEs with a combination of
powerful posteriors, priors and decoders. Such architectures can potentially rival or surpass purely autoregressive architectures , while allowing much faster synthesis.
The proposed VAE framework remains the only framework in the
literature that allows for both discrete and continuous observed variables,
allows for eﬃcient amortized latent-variable inference and fast synthesis,
and which can produce close to state-of-the-art performance in terms
of the log-likelihood of data.
Acknowledgements
We are grateful for the help of Tim Salimans, Alec Radford, Rif A.
Saurous and others who have given us valuable feedback at various
stages of writing.
Appendices
Notation and deﬁnitions
Example(s)
Description
With characters in bold we typically denote random vectors. We also use this notation for collections of random variables variables.
With characters in italic we typically denote
random scalars, i.e. single real-valued numbers.
With bold and capitalized letters we typically
denote random matrices.
The parents of random variable z in a directed
Diagonal matrix, with the values of vector x on
the diagonal.
Element-wise multiplication of two vectors. The
resulting vector is (x1y1, ..., xKyK)T .
Parameters of a (generative) model are typically
denoted with the Greek lowercase letter θ (theta).
Variational parameters are typically denoted
with the bold Greek letter φ (phi).
p(x), p(z)
Probability density functions (PDFs) and probability mass functions (PMFs), also simply called
distributions, are denoted by p(.), q(.) or r(.).
p(x, y, z)
Joint distributions are denoted by p(., .)
Conditional distributions are denoted by p(.|.)
p(.; θ), pθ(x)
The parameters of a distribution are denoted
with p(.; θ) or equivalently with subscript pθ(.).
We may use an (in-)equality sign within a probability distribution to distinguish between function arguments and value at which to evaluate.
So p(x = a) denotes a PDF or PMF over variable
x evaluated at the value of variable a. Likewise,
p(x ≤a) denotes a CDF evaluated at the value
p(.), q(.)
We use diﬀerent letters to refer to diﬀerent
probabilistic models, such as p(.) or q(.). Conversely, we use the same letter across diﬀerent
marginals/conditionals to indicate they relate to
the same probabilistic model.
Deﬁnitions
Description
A.1. Notation and deﬁnitions
Probability
function (PDF)
A function that assigns a probability density to
each possible value of given continuous random
variables.
Cumulative
distribution
A function that assigns a cumulative probability
density to each possible value of given univariate
continuous random variables.
Probability
mass function
A function that assigns a probability mass to
given discrete random variable.
Distributions
We overload the notation of distributions (e.g. p(x) = N(x; µ, Σ)) with
two meanings: (1) a distribution from which we can sample, and (2)
the probability density function (PDF) of that distribution.
Description
Categorical(x; p)
Categorical distribution, with parameter p
such that P
Bernoulli(x; p)
Multivariate
distribution
independent
Bernoulli.
Bernoulli(x; p) = Q
i Bernoulli(xi; pi) with
∀i : 0 ≤pi ≤1.
Normal(x; µ, Σ) =
N(x; µ, Σ)
Multivariate Normal distribution with mean
µ and covariance Σ.
Chain rule of probability
p(a, b) = p(a)p(b|a)
Bayes’ Rule
p(a|b) = p(b|a)p(a)/p(b)
Bayesian Inference
Let p(θ) be a chosen marginal distribution over its parameters θ, called
a prior distribution. Let D be observed data, p(D|θ) ≡pθ(D) be the
probability assigned to the data under the model with parameters θ.
Recall the chain rule in probability:
p(θ, D) = p(θ|D)p(D) = p(θ)p(D|θ)
Simply re-arranging terms above, the posterior distribution over the
parameters θ, taking into account the data D, is:
p(θ|D) = p(D|θ)p(θ)
∝p(D|θ)p(θ)
where the proportionality (∝) holds since p(D) is a constant that is
not dependent on parameters θ. The formula above is known as Bayes’
rule, a fundamental formula in machine learning and statistics, and is
of special importance to this work.
A principal application of Bayes’ rule is that it allows us to make
predictions about future data x′, that are optimal as long as the prior
p(θ) and model class pθ(x) are correct:
p(x = x′|D) =
pθ(x = x′)p(θ|D)dθ
Alternative methods for learning in DLVMs
Maximum A Posteriori
From a Bayesian perspective, we can improve upon the maximum
likelihood objective through maximum a posteriori (MAP) estimation,
which maximizes the log-posterior w.r.t. θ. With i.i.d. data D, this is:
LMAP (θ) = log p(θ|D)
= log p(θ) + LML(θ) + constant
A.2. Alternative methods for learning in DLVMs
The prior p(θ) in equation (A.5) has diminishing eﬀect for increasingly
large N. For this reason, in case of optimization with large datasets,
we often choose to simply use the maximum likelihood criterion by
omitting the prior from the objective, which is numerically equivalent
to setting p(θ) = constant.
Variational EM with local variational parameters
Expectation Maximization (EM) is a general strategy for learning
parameters in partially observed models . See
section A.2.3 for a discussion of EM using MCMC. The method can be
explained as coordinate ascent on the ELBO .
In case of of i.i.d. data, traditional variational EM methods estimate
local variational parameters φ(i), i.e. a separate set of variational
parameters per datapoint i in the dataset. In contrast, VAEs employ a
strategy with global variational parameters.
EM starts out with some (random) initial choice of θ and φ(1:N). It
then iteratively applies updates:
∀i = 1, ..., N :
φ(i) ←argmax
L(x(i); θ, φ)
L(x(i); θ, φ)
until convergence. Why does this work? Note that at the E-step:
L(x; θ, φ)
[log pθ(x) −DKL(qφ(z|x)||pθ(z|x))]
DKL(qφ(z|x)||pθ(z|x))
so the E-step, sensibly, minimizes the KL divergence of qφ(z|x) from
the true posterior.
Secondly, note that if qφ(z|x) equals pθ(z|x), the ELBO equals the
marginal likelihood, but that for any choice of qφ(z|x), the M-step
optimizes a bound on the marginal likelihood. The tightness of this
bound is deﬁned by DKL(qφ(z|x)||pθ(z|x)).
Another Bayesian approach towards optimizing the likelihood pθ(x)
with DLVMs is Expectation Maximization (EM) with Markov Chain
Monte Carlo (MCMC). In case of MCMC, the posterior is approximated
by a mixture of a set of approximately i.i.d. samples from the posterior,
acquired by running a Markov chain. Note that posterior gradients
in DLVMs are relatively aﬀordable to compute by diﬀerentiating the
log-joint distribution w.r.t. z:
∇z log pθ(z|x) = ∇z log[pθ(x, z)/pθ(x)]
= ∇z[log pθ(x, z) −log pθ(x)]
= ∇z log pθ(x, z) −∇z log pθ(x)
= ∇z log pθ(x, z)
One version of MCMC which uses such posterior for relatively fast
convergence, is Hamiltonian MCMC . A disadvantage of
this approach is the requirement for running an independent MCMC
chain per datapoint.
Stochastic Gradient Descent
We work with directed models where the objective per datapoint is
scalar, and due to the diﬀerentiability of neural networks that compose
them, the objective is diﬀerentiable w.r.t. its parameters θ. Due to the
remarkable eﬃciency of reverse-mode automatic diﬀerentiation ), the
value and gradient (i.e. the vector of partial derivatives) of diﬀerentiable
scalar objectives can be computed with equal time complexity. In SGD,
we iteratively update parameters θ:
θt+1 ←θt + αt · ∇θ ˜L(θ, ξ)
where αt is a learning rate or preconditioner, and ˜L(θ, ξ) is an unbiased
estimate of the objective L(θ), i.e. Eξ∼p(ξ)
= L(θ). The random variable ξ could e.g. be a datapoint index, uniformly sampled from
{1, ..., N}, but can also include diﬀerent types of noise such posterior
sampling noise in VAEs. In experiments, we have typically used the
A.3. Stochastic Gradient Descent
Adam and Adamax optimization methods for choosing αt ; these methods are invariant to constant rescaling of the objective, and invariant to constant re-scalings of the individual gradients.
As a result, ˜L(θ, ξ) only needs to be unbiased up to proportionality. We
iteratively apply eq. (A.15) until a stopping criterion is met. A simple
but eﬀective criterion is to stop optimization as soon as the probability
of a holdout set of data starts decreasing; this criterion is called early
References
Banerjee, A. . “An analysis of logistic models: Exponential family
connections and online performance”. In: Proceedings of the 2007
SIAM International Conference on Data Mining. SIAM. 204–215.
Bayer, J. and C. Osendorfer . “Learning stochastic recurrent
networks”. In: NIPS 2014 Workshop on Advances in Variational
Inference.
Bengio, Y., A. Courville, and P. Vincent . Representation Learning: A Review and New Perspectives. IEEE.
Bengio, Y., E. Laufer, G. Alain, and J. Yosinski . “Deep generative stochastic networks trainable by backprop”. In: International
Conference on Machine Learning. 226–234.
Berg, R. v. d., L. Hasenclever, J. M. Tomczak, and M. Welling .
“Sylvester Normalizing Flows for Variational Inference”. Conference
on Uncertainty in Artiﬁcial Intelligence.
Blei, D. M., M. I. Jordan, and J. W. Paisley . “Variational
Bayesian inference with stochastic search”. In: International Conference on Machine Learning. 1367–1374.
Blundell, C., J. Cornebise, K. Kavukcuoglu, and D. Wierstra .
“Weight Uncertainty in Neural Networks”. In: International Conference on Machine Learning. 1613–1622.
References
Bornschein, J., S. Shabanian, A. Fischer, and Y. Bengio . “Bidirectional Helmholtz machines”. In: Proceedings of the 33rd International
Conference on International Conference on Machine Learning. 2511–
Bourlard, H. and Y. Kamp . “Auto-association by multilayer perceptrons and singular value decomposition”. Biological Cybernetics.
59(4-5): 291–294.
Bowman, S. R., L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and
S. Bengio . “Generating sentences from a continuous space”.
arXiv preprint arXiv:1511.06349.
Brock, A., T. Lim, J. M. Ritchie, and N. J. Weston . “Neural photo
editing with introspective adversarial networks”. In: International
Conference on Learning Representations.
Burda, Y., R. Grosse, and R. Salakhutdinov . “Importance
weighted autoencoders”. arXiv preprint arXiv:1509.00519.
Chen, R. T., X. Li, R. Grosse, and D. Duvenaud . “Isolating
sources of disentanglement in VAEs”. In: Proceedings of the 32nd
International Conference on Neural Information Processing Systems.
Curran Associates Inc. 2615–2625.
Chen, X., D. P. Kingma, T. Salimans, Y. Duan, P. Dhariwal, J. Schulman, I. Sutskever, and P. Abbeel . “Variational lossy autoencoder”. International Conference on Learning Representations.
Chung, J., K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio
 . “A recurrent latent variable model for sequential data”. In:
Advances in neural information processing systems. 2980–2988.
Cremer, C., Q. Morris, and D. Duvenaud . “Re-interpreting
importance weighted autoencoders”. International Conference on
Learning Representations.
Dayan, P., G. E. Hinton, R. M. Neal, and R. S. Zemel . “The
Helmholtz machine”. Neural computation. 7(5): 889–904.
Deco, G. and W. Brauer . “Higher order statistical decorrelation without information loss”. Advances in Neural Information
Processing Systems: 247–254.
Dempster, A. P., N. M. Laird, and D. B. Rubin . “Maximum
likelihood from incomplete data via the EM algorithm”. Journal of
the Royal Statistical Society. Series B (Methodological): 1–38.
References
Deshpande, A., J. Lu, M.-C. Yeh, M. Jin Chong, and D. Forsyth .
“Learning diverse image colorization”. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 6837–
Dinh, L., D. Krueger, and Y. Bengio . “NICE: Non-linear independent components estimation”. arXiv preprint arXiv:1410.8516.
Dinh, L., J. Sohl-Dickstein, and S. Bengio . “Density estimation
using Real NVP”. arXiv preprint arXiv:1605.08803.
Dosovitskiy, A., J. Tobias Springenberg, and T. Brox . “Learning
to generate chairs with convolutional neural networks”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. 1538–1546.
Dumoulin, V., I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville . “Adversarially learned inference”.
International Conference on Learning Representations.
Edwards, H. and A. Storkey . “Towards a neural statistician”.
International Conference on Learning Representations.
Eslami, S. A., N. Heess, T. Weber, Y. Tassa, D. Szepesvari, G. E. Hinton,
et al. . “Attend, infer, repeat: Fast scene understanding with
generative models”. In: Advances In Neural Information Processing
Systems. 3225–3233.
Fan, K., Z. Wang, J. Beck, J. Kwok, and K. A. Heller . “Fast
second order stochastic backpropagation for variational inference”.
In: Advances in Neural Information Processing Systems. 1387–1395.
Fortunato, M., C. Blundell, and O. Vinyals . “Bayesian recurrent
neural networks”. arXiv preprint arXiv:1704.02798.
Fraccaro, M., S. K. Sønderby, U. Paquet, and O. Winther .
“Sequential neural models with stochastic layers”. In: Advances in
Neural Information Processing Systems. 2199–2207.
Fu, M. C. . “Gradient estimation”. Handbooks in Operations
Research and Management Science. 13: 575–616.
Gal, Y. and Z. Ghahramani . “A theoretically grounded application of dropout in recurrent neural networks”. In: Advances in
neural information processing systems. 1019–1027.
References
Germain, M., K. Gregor, I. Murray, and H. Larochelle . “Made:
Masked autoencoder for distribution estimation”. In: International
Conference on Machine Learning. 881–889.
Gershman, S. and N. Goodman . “Amortized inference in probabilistic reasoning.” In: CogSci.
Glasserman, P. . Monte Carlo methods in ﬁnancial engineering.
Vol. 53. Springer Science & Business Media.
Glynn, P. W. . “Likelihood ratio gradient estimation for stochastic
systems”. Communications of the ACM. 33(10): 75–84.
Gómez-Bombarelli, R., J. N. Wei, D. Duvenaud, J. M. Hernández-
Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre,
T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik . “Automatic
chemical design using a data-driven continuous representation of
molecules”. ACS central science. 4(2): 268–276.
Goodfellow, I., Y. Bengio, and A. Courville . Deep learning. MIT
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
Ozair, A. Courville, and Y. Bengio . “Generative adversarial
nets”. In: Advances in Neural Information Processing Systems. 2672–
Graves, A. . “Practical variational inference for neural networks”.
In: Advances in Neural Information Processing Systems. 2348–2356.
Gregor, K., F. Besse, D. J. Rezende, I. Danihelka, and D. Wierstra
 . “Towards conceptual compression”. In: Advances In Neural
Information Processing Systems. 3549–3557.
Gregor, K., I. Danihelka, A. Graves, D. Rezende, and D. Wierstra .
“DRAW: A Recurrent Neural Network For Image Generation”. In:
International Conference on Machine Learning. 1462–1471.
Gregor, K., I. Danihelka, A. Mnih, C. Blundell, and D. Wierstra .
“Deep AutoRegressive Networks”. In: International Conference on
Machine Learning. 1242–1250.
Grover, A., M. Dhar, and S. Ermon . “Flow-GAN: Combining
maximum likelihood and adversarial learning in generative models”.
In: AAAI Conference on Artiﬁcial Intelligence.
References
Gu, S., S. Levine, I. Sutskever, and A. Mnih . “MuProp: Unbiased
backpropagation for stochastic neural networks”. arXiv preprint
 
Gulrajani, I., K. Kumar, F. Ahmed, A. A. Taiga, F. Visin, D. Vazquez,
and A. Courville . “PixelVAE: A latent variable model for
natural images”. International Conference on Learning Representations.
He, K., X. Zhang, S. Ren, and J. Sun . “Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation”. In:
Proceedings of the IEEE International Conference on Computer
Vision. 1026–1034.
He, K., X. Zhang, S. Ren, and J. Sun . “Deep residual learning
for image recognition”. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. 770–778.
Heess, N., G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa
 . “Learning continuous control policies by stochastic value
gradients”. In: Advances in Neural Information Processing Systems.
2944–2952.
Hernández-Lobato, J. M., Y. Li, M. Rowland, D. Hernández-Lobato,
T. Bui, and R. E. Turner . “Black-box α-divergence minimization”.
Higgins, I., L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S.
Mohamed, and A. Lerchner . “beta-vae: Learning basic visual
concepts with a constrained variational framework”. International
Conference on Learning Representations.
Hinton, G. E., P. Dayan, B. J. Frey, and R. M. Neal . “The
"Wake-Sleep" algorithm for unsupervised neural networks”. Science:
1158–1158.
Hochreiter, S. and J. Schmidhuber . “Long Short-Term Memory”.
Neural Computation. 9(8): 1735–1780.
Hoﬀman, M. D., D. M. Blei, C. Wang, and J. Paisley . “Stochastic
variational inference”. The Journal of Machine Learning Research.
14(1): 1303–1347.
Hoﬀman, M. D. and M. J. Johnson . “Elbo surgery: yet another
way to carve up the variational evidence lower bound”. In: Workshop
in Advances in Approximate Bayesian Inference, NIPS.
References
Houthooft, R., X. Chen, Y. Duan, J. Schulman, F. De Turck, and P.
Abbeel . “Vime: Variational information maximizing exploration”. In: Advances in Neural Information Processing Systems.
1109–1117.
Hsu, C.-C., H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang .
“Voice conversion from non-parallel corpora using variational autoencoder”. In: Signal and Information Processing Association Annual
Summit and Conference (APSIPA), 2016 Asia-Paciﬁc. IEEE. 1–6.
Hsu, C.-C., H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang .
“Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks”. arXiv preprint
 
Hu, Z., Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing .
“Controllable text generation”. arXiv preprint arXiv:1703.00955.
Huang, C.-W., D. Krueger, A. Lacoste, and A. Courville . “Neural
Autoregressive Flows”. In: International Conference on Machine
Learning. 2083–2092.
Jang, E., S. Gu, and B. Poole . “Categorical Reparameterization with Gumbel-Softmax”. International Conference on Learning
Representations.
Johnson, M., D. K. Duvenaud, A. Wiltschko, R. P. Adams, and S. R.
Datta . “Composing graphical models with neural networks
for structured representations and fast inference”. In: Advances in
Neural Information Processing Systems. 2946–2954.
Jozefowicz, R., W. Zaremba, and I. Sutskever . “An empirical
exploration of recurrent network architectures”. In: International
Conference on Machine Learning. 2342–2350.
Karl, M., M. Soelch, J. Bayer, and P. van der Smagt . “Deep variational bayes ﬁlters: Unsupervised learning of state space models from
raw data”. International Conference on Learning Representations.
Kavukcuoglu, K., M. Ranzato, and Y. LeCun . “Fast inference in
sparse coding algorithms with applications to object recognition”.
Tech. rep. No. CBLL-TR-2008-12-01. Computational and Biological
Learning Lab, Courant Institute, NYU.
References
Kingma, D. P., S. Mohamed, D. J. Rezende, and M. Welling .
“Semi-supervised learning with deep generative models”. In: Advances in Neural Information Processing Systems. 3581–3589.
Kingma, D. P., T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and
M. Welling . “Improved variational inference with inverse
autoregressive ﬂow”. In: Advances in Neural Information Processing
Systems. 4743–4751.
Kingma, D. P., T. Salimans, and M. Welling . “Variational dropout
and the local reparameterization trick”. In: Advances in Neural
Information Processing Systems. 2575–2583.
Kingma, D. P. and M. Welling . “Auto-Encoding Variational
Bayes”. International Conference on Learning Representations.
Kingma, D. and J. Ba . “Adam: A Method for Stochastic Optimization”. International Conference on Learning Representations.
Kipf, T. N. and M. Welling . “Variational graph auto-encoders”.
arXiv preprint arXiv:1611.07308.
Kleijnen, J. P. and R. Y. Rubinstein . “Optimization and sensitivity analysis of computer simulation models by the score function
method”. European Journal of Operational Research. 88(3): 413–427.
Koller, D. and N. Friedman . Probabilistic graphical models: Principles and techniques. MIT press.
Krishnan, R. G., U. Shalit, and D. Sontag . “Structured Inference
Networks for Nonlinear State Space Models.” In: AAAI. 2101–2109.
Kucukelbir, A., D. Tran, R. Ranganath, A. Gelman, and D. M. Blei
 . “Automatic diﬀerentiation variational inference”. The Journal of Machine Learning Research. 18(1): 430–474.
Kulkarni, T. D., W. F. Whitney, P. Kohli, and J. Tenenbaum .
“Deep convolutional inverse graphics network”. In: Advances in
Neural Information Processing Systems. 2539–2547.
Kusner, M. J., B. Paige, and J. M. Hernández-Lobato . “Grammar
variational autoencoder”. In: Proceedings of the 34th International
Conference on Machine Learning-Volume 70. 1945–1954.
Larsen, A. B. L., S. K. Sønderby, H. Larochelle, and O. Winther .
“Autoencoding beyond pixels using a learned similarity metric”. In:
International Conference on Machine Learning. 1558–1566.
References
Lázaro-Gredilla, M. . “Doubly stochastic variational Bayes for
non-conjugate inference”. In: International Conference on Machine
LeCun, Y., Y. Bengio, and G. Hinton . “Deep Learning”. Nature.
521(7553): 436–444.
LeCun, Y., L. Bottou, Y. Bengio, and P. Haﬀner . “Gradientbased learning applied to document recognition”. Proceedings of the
IEEE. 86(11): 2278–2324.
Li, Y. and R. E. Turner . “Rényi divergence variational inference”.
In: Advances in Neural Information Processing Systems. 1073–1081.
Li, Y., K. Swersky, and R. S. Zemel . “Generative moment matching networks”. In: International Conference on Machine Learning.
1718–1727.
Linsker, R. . An Application of the Principle of Maximum Information Preservation to Linear Systems. Morgan Kaufmann Publishers
Louizos, C., K. Swersky, Y. Li, M. Welling, and R. Zemel . “The
variational fair autoencoder”. arXiv preprint arXiv:1511.00830.
Louizos, C., K. Ullrich, and M. Welling . “Bayesian compression
for deep learning”. In: Advances in Neural Information Processing
Systems. 3288–3298.
Louizos, C. and M. Welling . “Structured and eﬃcient variational
deep learning with matrix gaussian posteriors”. In: International
Conference on Machine Learning. 1708–1716.
Louizos, C. and M. Welling . “Multiplicative normalizing ﬂows for
variational Bayesian neural networks”. In: International Conference
on Machine Learning. 2218–2227.
Maaløe, L., C. K. Sønderby, S. K. Sønderby, and O. Winther .
“Auxiliary deep generative models”. In: International Conference on
Machine Learning.
Maddison, C. J., A. Mnih, and Y. W. Teh . “The concrete
distribution: A continuous relaxation of discrete random variables”.
International Conference on Learning Representations.
Makhzani, A., J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey .
“Adversarial autoencoders”. arXiv preprint arXiv:1511.05644.
References
Mansimov, E., E. Parisotto, J. L. Ba, and R. Salakhutdinov .
“Generating images from captions with attention”. arXiv preprint
 
Miao, Y., L. Yu, and P. Blunsom . “Neural variational inference for
text processing”. In: International Conference on Machine Learning.
1727–1736.
Mnih, A. and K. Gregor . “Neural variational inference and learning in belief networks”. In: International Conference on Machine
Mnih, A. and D. Rezende . “Variational Inference for Monte Carlo
Objectives”. In: International Conference on Machine Learning.
2188–2196.
Mohamed, S. and D. J. Rezende . “Variational information maximisation for intrinsically motivated reinforcement learning”. In:
Advances in Neural Information Processing Systems. 2125–2133.
Molchanov, D., A. Ashukha, and D. Vetrov . “Variational dropout
sparsiﬁes deep neural networks”. In: International Conference on
Machine Learning. 2498–2507.
Naesseth, C., F. Ruiz, S. Linderman, and D. Blei . “Reparameterization gradients through acceptance-rejection sampling algorithms”.
In: Artiﬁcial Intelligence and Statistics. 489–498.
Neal, R. . “MCMC Using Hamiltonian Dynamics”. Handbook of
Markov Chain Monte Carlo: 113–162.
Neal, R. M. and G. E. Hinton . “A view of the EM algorithm
that justiﬁes incremental, sparse, and other variants”. In: Learning
in Graphical Models. Springer. 355–368.
Paisley, J., D. Blei, and M. Jordan . “Variational Bayesian Inference with stochastic search”. In: International Conference on
Machine Learning. 1367–1374.
Papamakarios, G., I. Murray, and T. Pavlakou . “Masked autoregressive ﬂow for density estimation”. In: Advances in Neural
Information Processing Systems. 2335–2344.
Pritzel, A., B. Uria, S. Srinivasan, A. P. Badia, O. Vinyals, D. Hassabis,
D. Wierstra, and C. Blundell . “Neural episodic control”. In:
International Conference on Machine Learning. 2827–2836.
References
Pu, Y., Z. Gan, R. Henao, X. Yuan, C. Li, A. Stevens, and L. Carin
 . “Variational autoencoder for deep learning of images, labels and captions”. In: Advances in Neural Information Processing
Systems. 2352–2360.
Ranganath, R., S. Gerrish, and D. Blei . “Black Box Variational
Inference”. In: International Conference on Artiﬁcial Intelligence
and Statistics. 814–822.
Ranganath, R., D. Tran, and D. Blei . “Hierarchical variational
models”. In: International Conference on Machine Learning. 324–
Ravanbakhsh, S., F. Lanusse, R. Mandelbaum, J. Schneider, and B.
Poczos . “Enabling dark energy science with deep generative models of galaxy images”. In: AAAI Conference on Artiﬁcial
Intelligence.
Rezende, D. J., S. Mohamed, I. Danihelka, K. Gregor, and D. Wierstra
 . “One-shot generalization in deep generative models”. In:
International Conference on International Conference on Machine
Learning. 1521–1529.
Rezende, D. J., S. Mohamed, and D. Wierstra . “Stochastic backpropagation and approximate inference in deep generative models”.
In: International Conference on Machine Learning. 1278–1286.
Rezende, D. J., S. A. Eslami, S. Mohamed, P. Battaglia, M. Jaderberg,
and N. Heess . “Unsupervised learning of 3d structure from
images”. In: Advances In Neural Information Processing Systems.
4997–5005.
Rezende, D. and S. Mohamed . “Variational inference with normalizing ﬂows”. In: International Conference on Machine Learning.
1530–1538.
Roeder, G., Y. Wu, and D. K. Duvenaud . “Sticking the landing:
Simple, lower-variance gradient estimators for variational inference”.
In: Advances in Neural Information Processing Systems. 6925–6934.
Rosca, M., B. Lakshminarayanan, and S. Mohamed . “Distribution
matching in variational inference”. arXiv preprint arXiv:1802.06847.
Roweis, S. . “EM algorithms for PCA and SPCA”. Advances in
Neural Information Processing Systems: 626–632.
References
Ruiz, F. R., M. T. R. AUEB, and D. Blei . “The generalized
reparameterization gradient”. In: Advances in Neural Information
Processing Systems. 460–468.
Rumelhart, D. E., G. E. Hinton, and R. J. Williams . “Learning
representations by back-propagating errors”. Cognitive Modeling.
Russakovsky, O., J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, et al. . “Imagenet
large scale visual recognition challenge”. International Journal of
Computer Vision. 115(3): 211–252.
Salakhutdinov, R. and H. Larochelle . “Eﬃcient learning of deep
Boltzmann machines”. In: International Conference on Artiﬁcial
Intelligence and Statistics. 693–700.
Salimans, T. . “A structured variational auto-encoder for learning
deep hierarchies of sparse features”. arXiv preprint arXiv:1602.08734.
Salimans, T., D. P. Kingma, and M. Welling . “Markov Chain
Monte Carlo and Variational Inference: Bridging the Gap.” In:
International Conference on Machine Learning. Vol. 37. 1218–1226.
Salimans, T. and D. A. Knowles . “Fixed-Form variational posterior approximation through stochastic linear regression”. Bayesian
Analysis. 8(4).
Semeniuta, S., A. Severyn, and E. Barth . “A hybrid convolutional variational autoencoder for text generation”. arXiv preprint
 
Serban, I. V., A. Sordoni, R. Lowe, L. Charlin, J. Pineau, A. Courville,
and Y. Bengio . “A hierarchical latent variable encoderdecoder model for generating dialogues”. In: Proceedings of the
Thirty-First AAAI Conference on Artiﬁcial Intelligence. AAAI Press.
3295–3301.
Sohl-Dickstein, J., E. Weiss, N. Maheswaranathan, and S. Ganguli
 . “Deep unsupervised learning using nonequilibrium thermodynamics”. In: International Conference on Machine Learning. 2256–
References
Sønderby, C. K., T. Raiko, L. Maaløe, S. K. Sønderby, and O. Winther
 . “How to train deep variational autoencoders and probabilistic ladder networks”. In: International Conference on Machine
Sønderby, C. K., T. Raiko, L. Maaløe, S. K. Sønderby, and O. Winther
 . “Ladder variational autoencoders”. In: Advances in Neural
Information Processing Systems. 3738–3746.
Szegedy, C., W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D.
Erhan, V. Vanhoucke, and A. Rabinovich . “Going deeper with
convolutions”. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 1–9.
Tomczak, J. M. and M. Welling. “Improving variational auto-encoders
using convex combination linear inverse autoregressive ﬂow”. In:
Benelearn 2017: Proceedings of the Twenty-Sixth Benelux Conference
on Machine Learning, Technische Universiteit Eindhoven, 9-10 June
2017. 162.
Tomczak, J. M. and M. Welling . “Improving variational autoencoders using householder ﬂow”. arXiv preprint arXiv:1611.09630.
Tran, D., M. D. Hoﬀman, R. A. Saurous, E. Brevdo, K. Murphy, and
D. M. Blei . “Deep probabilistic programming”. International
Conference on Learning Representations.
Tran, D., R. Ranganath, and D. M. Blei . “The variational
Gaussian process”. arXiv preprint arXiv:1511.06499.
Van den Oord, A., N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves,
et al. . “Conditional image generation with PixelCNN decoders”. In: Advances in neural information processing systems.
4790–4798.
Van Oord, A., N. Kalchbrenner, and K. Kavukcuoglu . “Pixel Recurrent Neural Networks”. In: International Conference on Machine
Learning. 1747–1756.
Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol
 . “Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion”. Journal
of Machine Learning Research. 11(Dec): 3371–3408.