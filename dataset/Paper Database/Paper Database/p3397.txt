Do Better ImageNet Models Transfer Better?
Simon Kornblith∗, Jonathon Shlens, and Quoc V. Le
Google Brain
{skornblith,shlens,qvl}@google.com
Transfer learning is a cornerstone of computer vision,
yet little work has been done to evaluate the relationship
between architecture and transfer. An implicit hypothesis
in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other
vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16
classiﬁcation networks on 12 image classiﬁcation datasets.
We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between
ImageNet accuracy and transfer accuracy (r = 0.99 and
0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are
trained on ImageNet; many common forms of regularization
slightly improve ImageNet accuracy but yield penultimate
layer features that are much worse for transfer learning.
Additionally, we ﬁnd that, on two small ﬁne-grained image
classiﬁcation datasets, pretraining on ImageNet provides
minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together,
our results show that ImageNet architectures generalize well
across datasets, but ImageNet features are less general than
previously suggested.
1. Introduction
The last decade of computer vision research has pursued academic benchmarks as a measure of progress. No
benchmark has been as hotly pursued as ImageNet .
Network architectures measured against this dataset have
fueled much progress in computer vision research across
a broad array of problems, including transferring to new
datasets , object detection , image segmentation
 and perceptual metrics of images . An implicit
assumption behind this progress is that network architectures that perform better on ImageNet necessarily perform
better on other vision tasks. Another assumption is that bet-
∗Work done as a member of the Google AI Residency program (g.co/
airesidency).
ImageNet Top-1 Accuracy (%)
Transfer Accuracy (Log Odds)
MobileNet v1
NASNet Large
Inception-ResNet v2
Logistic Regression
ImageNet Top-1 Accuracy (%)
MobileNet v1
NASNet Large
Inception v4
Fine-Tuned
Figure 1. Transfer learning performance is highly correlated with
ImageNet top-1 accuracy for ﬁxed ImageNet features (left) and
ﬁne-tuning from ImageNet initialization (right). The 16 points in
each plot represent transfer accuracy for 16 distinct CNN architectures, averaged across 12 datasets after logit transformation (see
Section 3). Error bars measure variation in transfer accuracy across
datasets. These plots are replicated in Figure 2 (right).
ter network architectures learn better features that can be
transferred across vision-based tasks. Although previous
studies have provided some evidence for these hypotheses
(e.g. ), they have never been systematically
explored across network architectures.
In the present work, we seek to test these hypotheses by investigating the transferability of both ImageNet features and
ImageNet classiﬁcation architectures. Speciﬁcally, we conduct a large-scale study of transfer learning across 16 modern
convolutional neural networks for image classiﬁcation on
12 image classiﬁcation datasets in 3 different experimental
settings: as ﬁxed feature extractors , ﬁne-tuned from
ImageNet initialization , and trained from random
initialization. Our main contributions are as follows:
• Better ImageNet networks provide better penultimate
layer features for transfer learning with linear classi-
ﬁcation (r = 0.99), and better performance when the
entire network is ﬁne-tuned (r = 0.96).
• Regularizers that improve ImageNet performance are
highly detrimental to the performance of transfer learning based on penultimate layer features.
• Architectures transfer well across tasks even when
 
weights do not. On two small ﬁne-grained classiﬁcation datasets, ﬁne-tuning does not provide a substantial
beneﬁt over training from random initialization, but
better ImageNet architectures nonetheless obtain higher
2. Related work
ImageNet follows in a succession of progressively larger
and more realistic benchmark datasets for computer vision.
Each successive dataset was designed to address perceived
issues with the size and content of previous datasets. Torralba and Efros showed that many early datasets were
heavily biased, with classiﬁers trained to recognize or classify objects on those datasets possessing almost no ability to
generalize to images from other datasets.
Early work using convolutional neural networks (CNNs)
for transfer learning extracted ﬁxed features from ImageNettrained networks and used these features to train SVMs and
logistic regression classiﬁers for new tasks . These
features could outperform hand-engineered features even for
tasks very distinct from ImageNet classiﬁcation . Following this work, several studies compared the performance
of AlexNet-like CNNs of varying levels of computational
complexity in a transfer learning setting with no ﬁne-tuning.
Chatﬁeld et al. found that, out of three networks, the two
more computationally expensive networks performed better
on PASCAL VOC. Similar work concluded that deeper networks produce higher accuracy across many transfer tasks,
but wider networks produce lower accuracy . More recent
evaluation efforts have investigated transfer from modern
CNNs to medical image datasets , and transfer of sentence embeddings to language tasks .
A substantial body of existing research indicates that, in
image tasks, ﬁne-tuning typically achieves higher accuracy
than classiﬁcation based on ﬁxed features, especially for
larger datasets or datasets with a larger domain mismatch
from the training set . In object detection, ImageNet-pretrained networks are used as
backbone models for Faster R-CNN and R-FCN detection
systems . Classiﬁers with higher ImageNet accuracy achieve higher overall object detection accuracy ,
although variability across network architectures is small
compared to variability from other object detection architecture choices. A parallel story likewise appears in image
segmentation models , although it has not been as systematically explored.
Several authors have investigated how properties of the
original training dataset affect transfer accuracy. Work examining the performance of ﬁxed image features drawn from
networks trained on subsets of ImageNet have reached con-
ﬂicting conclusions regarding the importance of the number
of classes vs. number of images per class . Yosinski et
al. showed that the ﬁrst layer of AlexNet can be frozen
when transferring between natural and manmade subsets
of ImageNet without performance impairment, but freezing
later layers produces a substantial drop in accuracy. Other
work has investigated transfer from extremely large image
datasets to ImageNet, demonstrating that transfer learning
can be useful even when the target dataset is large .
Finally, a recent work devised a strategy to transfer when
labeled data from many different domains is available .
3. Statistical methods
Much of the analysis in this work requires comparing accuracies across datasets of differing difﬁculty. When ﬁtting
linear models to accuracy values across multiple datasets,
we consider effects of model and dataset to be additive.
In this context, using untransformed accuracy as a dependent variable is problematic: The meaning of a 1% additive increase in accuracy is different if it is relative to a
base accuracy of 50% vs. 99%. Thus, we consider the
log odds, i.e., the accuracy after the logit transformation
logit(p) = log(p/(1 −p)) = sigmoid−1(p). The logit transformation is the most commonly used transformation for
analysis of proportion data, and an additive change ∆in
logit-transformed accuracy has a simple interpretation as a
multiplicative change exp ∆in the odds of correct classiﬁcation:
ncorrect + nincorrect
 ncorrect
nincorrect
 ncorrect
nincorrect
We plot all accuracy numbers on logit-scaled axes.
We computed error bars for model accuracy averaged
across datasets, using the procedure from Morey to
remove variance due to inherent differences in dataset dif-
ﬁculty. Given logit-transformed accuracies xmd of model
m ∈M on dataset d ∈D, we compute adjusted accuracies
acc(m, d) = xmd −P
n∈M xnd/|M|. For each model, we
take the mean and standard error of the adjusted accuracy
across datasets, and multiply the latter by a correction factor
|M|/(|M| −1).
When examining the strength of the correlation between
ImageNet accuracy and accuracy on transfer datasets, we
report r for the correlation between the logit-transformed
ImageNet accuracy and the logit-transformed transfer accuracy averaged across datasets. We report the rank correlation
(Spearman’s ρ) in Appendix A.1.2.
We tested for signiﬁcant differences between pairs of
networks on the same dataset using a permutation test or
equivalent binomial test of the null hypothesis that the predictions of the two networks are equally likely to be correct,
described further in Appendix A.1.1. We tested for signiﬁcant differences between networks in average performance
across datasets using a t-test.
Size (train/test)
Accuracy metric
Food-101 
75,750/25,250
CIFAR-10 
50,000/10,000
CIFAR-100 
50,000/10,000
Birdsnap 
47,386/2,443
SUN397 
19,850/19,850
Stanford Cars 
8,144/8,041
FGVC Aircraft 
6,667/3,333
mean per-class
PASCAL VOC 2007 Cls. 
5,011/4,952
11-point mAP
Describable Textures (DTD) 
3,760/1,880
Oxford-IIIT Pets 
3,680/3,369
mean per-class
Caltech-101 
3,060/6,084
mean per-class
Oxford 102 Flowers 
2,040/6,149
mean per-class
Table 1. Datasets examined in transfer learning
4. Results
We examined 16 modern networks ranging in ImageNet
 top-1 accuracy from 71.6% to
80.8%. These networks encompassed widely used Inception architectures ; ResNets ;
DenseNets ; MobileNets ; and NASNets .
For fair comparison, we retrained all models with scale parameters for batch normalization layers and without label
smoothing, dropout, or auxiliary heads, rather than relying
on pretrained models. Appendix A.3 provides training hyperparameters along with further details of each network,
including the ImageNet top-1 accuracy, parameter count,
dimension of the penultimate layer, input image size, and
performance of retrained models. For all experiments, we
rescaled images to the same image size as was used for
ImageNet training.
We evaluated models on 12 image classiﬁcation datasets
ranging in training set size from 2,040 to 75,750 images
(20 to 5,000 images per class; Table 1). These datasets
covered a wide range of image classiﬁcation tasks, including superordinate-level object classiﬁcation ; ﬁne-grained object classiﬁcation (Food-101 , Birdsnap , Stanford Cars , FGVC Aircraft , Oxford-
IIIT Pets ); texture classiﬁcation (DTD ); and scene
classiﬁcation (SUN397 ).
Figure 2 presents correlations between the top-1 accuracy
on ImageNet vs. the performance of the same model architecture on new image tasks. We measure transfer learning
performance in three settings: (1) training a logistic regression classiﬁer on the ﬁxed feature representation from the
penultimate layer of the ImageNet-pretrained network, (2)
ﬁne-tuning the ImageNet-pretrained network, and (3) training the same CNN architecture from scratch on the new
image task.
4.1. ImageNet accuracy predicts performance of logistic regression on ﬁxed features, but regularization settings matter
We ﬁrst examined the performance of different networks
when used as ﬁxed feature extractors by training an L2regularized logistic regression classiﬁer on penultimate layer
activations using L-BFGS without data augmentation.1
As shown in Figure 2 (top), ImageNet top-1 accuracy was
highly correlated with accuracy on transfer tasks (r = 0.99).
Inception-ResNet v2 and NASNet Large, the top two models
in terms of ImageNet accuracy, were statistically tied for
ﬁrst place.
Critically, results in Figure 2 were obtained with models
that were all trained on ImageNet with the same training
settings. In experiments conducted with publicly available
checkpoints, we were surprised to ﬁnd that ResNets and
DenseNets consistently achieved higher accuracy than other
models, and the correlation between ImageNet accuracy
and transfer accuracy with ﬁxed features was low and not
statistically signiﬁcant (Appendix B). Further investigation
revealed that the poor correlation arose from differences in
regularization used for these public checkpoints.
Figure 3 shows the transfer learning performance of Inception models with different training settings. We identify 4
choices made in the Inception training procedure and subsequently adopted by several other models that are detrimental
to transfer accuracy: (1) The absence of scale parameter (γ)
for batch normalization layers; the use of (2) label smoothing and (3) dropout ; and (4) the presence of an
auxiliary classiﬁer head . These settings had a small
(< 1%) impact on the overall ImageNet top-1 accuracy of
each model (Figure 3, inset). However, in terms of average
transfer accuracy, the difference between the default and
1We also repeated these experiments with support vector machine classiﬁers in place of logistic regression, and when using data augmentation for
logistic regression; see Appendix G. Findings did not change.
Logistic Regression
Fine-Tuned
Trained from Random Initialization
Figure 2. ImageNet accuracy is a strong predictor of transfer accuracy for logistic regression on penultimate layer features and ﬁne-tuning.
Each set of panels measures correlations between ImageNet accuracy and transfer accuracy across ﬁxed ImageNet features (top), ﬁne-tuned
networks (middle) and networks trained from scratch (bottom). Left: Relationship between classiﬁcation accuracy on transfer datasets
(y-axis) and ImageNet top-1 accuracy (x-axis) in different training settings. Axes are logit-scaled (see text). The regression line and a 95%
bootstrap conﬁdence interval are plotted in blue. Right: Average log odds of correct classiﬁcation across datasets. Error bars are standard
error. Points corresponding to models not signiﬁcantly different from the best model (p > 0.05) are colored green.
optimal training settings was approximately equal to the difference between the worst and best ImageNet models trained
with optimal settings. This difference was visible not only
in transfer accuracy, but also in t-SNE embeddings of the
features (Figure 4). Differences in transfer accuracy between
settings were apparent earlier in training than differences
in ImageNet accuracy, and were consistent across datasets
(Appendix C.1).
Label smoothing and dropout are regularizers in the traditional sense: They are intended to improve generalization
accuracy at the expense of training accuracy. Although auxiliary classiﬁer heads were initially proposed to alleviate
Training Settings
Average Transfer Accuracy (Log Odds)
+ Label Smooth
+ BN Scale
+ Label Smooth
+ BN Scale
−Label Smooth
+ BN Scale
−Label Smooth
+ BN Scale
−Label Smooth
Inception-ResNet v2
Inception v4
Inception v3
BN-Inception
Inception v1
Training Settings
Figure 3. ImageNet training settings have a large effect upon performance of logistic regression classiﬁers trained on penultimate
layer features. In the main plot, each point represents the logittransformed transfer accuracy averaged across the 12 datasets, measured using logistic regression on penultimate layer features from
a speciﬁc model trained with the training conﬁguration labeled at
the bottom. "+" indicates that a setting was enabled, whereas "−"
indicates that a setting was disabled. The leftmost, most heavily
regularized conﬁguration is typically used for Inception models
 ; the rightmost is typically used for ResNets and DenseNets.
The inset plot shows ImageNet top-1 accuracy for the same training
conﬁgurations. See also Appendix C.1. Best viewed in color.
Default Training Settings
Optimal Training Settings
Figure 4. The default Inception training settings produce a suboptimal feature space. Low dimensional embeddings of Oxford 102
Flowers using t-SNE on features from the penultimate layer of
Inception v4, for 10 classes from the test set. Best viewed in color.
issues related to vanishing gradients , Szegedy et al.
 instead suggest that they also act as regularizers. The
improvement in transfer performance when incorporating
batch normalization scale parameters may relate to changes
in effective learning rates .
4.2. ImageNet accuracy predicts ﬁne-tuning performance
We also examined performance when ﬁne-tuning ImageNet networks (Figure 2, middle). We initialized each network from the ImageNet weights and ﬁne-tuned for 20,000
Avg. Transfer Accuracy (Log Odds)
+ Label Smooth
+ Aux Head
+ BN Scale
+ Label Smooth
+ Aux Head
+ BN Scale
−Label Smooth
+ Aux Head
+ BN Scale
−Label Smooth
+ Aux Head
+ BN Scale
−Label Smooth
Inception v4
Inception-ResNet v2
Training Settings
Figure 5. ImageNet training settings have only a minor impact
on ﬁne-tuning performance. Each point represents transfer accuracy for a model pretrained and ﬁne-tuned with the same training
conﬁguration, labeled at the bottom. Axes follow Figure 3. See
Appendix C.2 for performance of models pretrained with regularization but ﬁne-tuned without regularization.
steps with Nesterov momentum and a cosine decay learning
rate schedule at a batch size of 256. We performed grid
search to select the optimal learning rate and weight decay
based on a validation set (for details, see Appendix A.5).
Again, we found that ImageNet top-1 accuracy was highly
correlated with transfer accuracy (r = 0.96).
Compared with the logistic regression setting, regularization and training settings had smaller effects upon the
performance of ﬁne-tuned models. Figure 5 shows average
transfer accuracy for Inception v4 and Inception-ResNet v2
models with different regularization settings. As in the logistic regression setting, introducing a batch normalization
scale parameter and disabling label smoothing improved
performance. In contrast to the logistic regression setting,
dropout and the auxiliary head sometimes improved performance, but only if used during ﬁne-tuning. We discuss these
results further in Appendix C.2.
Overall, ﬁne-tuning yielded better performance than classiﬁers trained on ﬁxed ImageNet features, but the gain differed by dataset. Fine-tuning improved performance over
logistic regression in 179 out of 192 dataset and model combinations (Figure 6; see also Appendix E). When averaged
across the tested architectures, ﬁne-tuning yielded signiﬁcantly better results on all datasets except Caltech-101 (all
p < 0.01, Wilcoxon signed rank test; Figure 6). The improvement was generally larger for larger datasets. However,
ﬁne-tuning provided substantial gains on the smallest dataset,
102 Flowers, with 102 classes and 2,040 training examples.
4.3. ImageNet accuracy predicts performance of
networks trained from random initialization
One confound of the previous results is that it is not clear
whether ImageNet accuracy for transfer learning is due to
the weights derived from the ImageNet training or the archi-
Caltech-101
Fine Tuned
Random Init
Inception v1
BN-Inception
Inception v3
Inception v4
Inception-ResNet v2
ResNet-50 v1
ResNet-101 v1
ResNet-152 v1
DenseNet-121
DenseNet-169
DenseNet-201
MobileNet v1
MobileNet v2
MobileNet v2 (1.4)
NASNet-A Mobile
NASNet-A Large
Inception v4 @ 448px
Figure 6. Performance comparison of logistic regression, ﬁne-tuning, and training from random initialization. Bars reﬂect accuracy across
models (excluding VGG) for logistic regression, ﬁne-tuning, and training from random initialization. Error bars are standard error. Points
represent individual models. Lines represent previous state-of-the-art. Best viewed in color.
tecture itself. To remove the confound, we next examined
architectures trained from random initialization, using a similar training setup as for ﬁne-tuning (see Appendix A.6). In
this setting, the correlation between ImageNet top-1 accuracy and accuracy on the new tasks was more variable than
in the transfer learning settings, but there was a tendency
toward higher performance for models that achieved higher
accuracy on ImageNet (r = 0.55; Figure 2, bottom).
Examining these results further, we found that a single
correlation averages over a large amount of variability. For
the 7 datasets with <10,000 examples, the correlation was
low and did not reach statistically signiﬁcance (r = 0.29;
see also Appendix D). However, for the larger datasets, the
correlation between ImageNet top-1 accuracy and transfer
learning performance was markedly stronger (r = 0.86).
Inception v3 and v4 were among the top-performing models
across all dataset sizes.
4.4. Beneﬁts of better models are comparable to
specialized methods for transfer learning
Given the strong correlation between ImageNet accuracy
and transfer accuracy, we next sought to compare simple
approaches to transfer learning with better ImageNet models
with baselines from the literature. We achieve state-of-theart performance on half of the 12 datasets if we evaluate
using the same image sizes as the baseline methods (Figure
6; see full results in Appendix F). Our results suggest that the
ImageNet performance of the pretrained model is a critical
factor in transfer performance.
Several papers have proposed methods to make better use
of CNN features and thus improve the efﬁcacy of transfer
learning . On the datasets
we examine, we outperform all such methods simply by ﬁnetuning state-of-the-art CNNs (Appendix F). Moreover, in
some cases a better CNN can make up for dataset deﬁciencies: By ﬁne-tuning ImageNet-pretrained Inception v4, we
outperform the best reported single-model results for networks pretrained on the Places dataset , which more
closely matches the domain of SUN397.
7075 80 85
Trained from Random Init
Fine Tuned
Stanford Cars
FGVC Aircraft
Oxford-IIIT Pets
Caltech-101
102 Flowers
Figure 7. For some datasets and networks, the gap between ﬁnetuning and training from random initialization is small. Each point
represents a dataset/model combination. Axes are logit-scaled. See
Figure 6 for network legend and Appendix E for scatter plots of
other settings. Best viewed in color.
It is likely that improvements obtained with better models, specialized transfer learning methods, and pretraining
datasets with greater domain match are complementary.
Combining these approaches could lead to even better performance. Nonetheless, it is surprising that simply using
a better model can yield gains comparable to specialized
techniques.
4.5. ImageNet pretraining does not necessarily improve accuracy on ﬁne-grained tasks
Fine-tuning was more accurate than training from random
initialization for 189 out of 192 dataset/model combinations,
but on Stanford Cars and FGVC Aircraft, the improvement
was unexpectedly small (Figures 6 and 7). In both settings,
Inception v4 was the best model we tested on these datasets.
When trained at the default image size of 299 × 299, it
achieved 92.7% on Stanford Cars when trained from scratch
on vs. 93.3% when ﬁne-tuned, and 88.8% on FGVC Aircraft
when trained from scratch vs. 89.0% when ﬁne-tuned.
ImageNet pretraining thus appears to have only marginal
accuracy beneﬁts for ﬁne-grained classiﬁcation tasks where
Train Epochs
Train Steps
Figure 8. Networks pretrained on ImageNet converge faster, even when ﬁnal accuracy is the same as training from random initialization.
Each point represents an independent Inception v4 model trained with optimized hyperparameters. For ﬁne-tuning, we initialize with the
public TensorFlow Inception v4 checkpoint. Axes are logit-scaled.
labels are not well-represented in ImageNet. At 100+ classes
and <10,000 examples, Stanford Cars and FGVC Aircraft
are much smaller than most datasets used to train CNNs .
In fact, the ImageNet training set contains more car images
than Stanford Cars (12,455 vs. 8,144). However, ImageNet
contains only 10 high-level car classes (e.g., sports car),
whereas Stanford Cars contains 196 car classes by make,
model, and year. Four other datasets (Oxford 102 Flowers,
Oxford-IIIT Pets, Birdsnap, and Food-101) require similarly
ﬁne-grained classiﬁcation, but the classes contained in the
latter three datasets are much better-represented in ImageNet.
Most of the cat and dog breeds present in Oxford-IIIT Pets
correspond directly to ImageNet classes, and ImageNet contains 59 classes of birds and around 45 classes of fruits,
vegetables, and prepared dishes.
4.6. ImageNet pretraining accelerates convergence
Given that ﬁne-tuning and training from random initialization achieved similar performance on Stanford Cars and
FGVC Aircraft, we next asked whether ﬁne-tuning still
posed an advantage in terms of training time. In Figure 8,
we examine performance of Inception v4 when ﬁne-tuning
or training from random initialization for different numbers
of steps. Even when ﬁne-tuning and training from scratch
achieved similar ﬁnal accuracy, we could ﬁne-tune the model
to this level of accuracy in an order of magnitude fewer steps.
To quantify this acceleration, we computed the number of
epochs and steps required to reach 90% of the maximum
odds of correct classiﬁcation achieved at any number of
steps, and computed the geometric mean across datasets.
Fine-tuning reached this threshold level of accuracy in an
average of 26 epochs/1151 steps (inter-quartile ranges 267-
4882 steps, 12-58 epochs), whereas training from scratch
required 444 epochs/19531 steps (inter-quartile ranges 9765-
39062 steps, 208-873 epochs) corresponding to a 17-fold
speedup on average.
4.7. Accuracy beneﬁts of ImageNet pretraining fade
quickly with dataset size
Although all datasets beneﬁt substantially from ImageNet
pretraining when few examples are available for transfer,
for many datasets, these beneﬁts fade quickly when more
examples are available. In Figure 9, we show the behavior
of logistic regression, ﬁne-tuning, and training from random
initialization in the regime of limited data, i.e., for dataset
subsets consisting of different numbers of examples per class.
When data is sparse (47-800 total examples), logistic regression is a strong baseline, achieving accuracy comparable to
or better than ﬁne-tuning. At larger dataset sizes, ﬁne-tuning
achieves higher performance than logistic regression, and,
for ﬁne-grained classiﬁcation datasets, the performance of
training from random initialization begins to approach results of pre-trained models. On FGVC Aircraft, training
from random initialization achieved parity with ﬁne-tuning
at only 1600 total examples (16 examples per class).
5. Discussion
Has the computer vision community overﬁt to ImageNet
as a dataset? In a broad sense, our results suggest the answer
is no: We ﬁnd that there is a strong correlation between
ImageNet top-1 accuracy and transfer accuracy, suggesting
that better ImageNet architectures are capable of learning
better, transferable representations. But we also ﬁnd that a
number of widely-used regularizers that improve ImageNet
performance do not produce better representations. These
Total Number of Examples
Examples per Class
Figure 9. Pretraining on ImageNet improves performance on ﬁne-grained tasks with small amounts of data, but the gap narrows quickly
as dataset size increases. Performance of transfer learning with the public Inception v4 model at different dataset sizes. Error bars reﬂect
standard error over 3 subsets. Note that the maximum dataset size shown is not the full dataset. Best viewed in color.
regularizers are harmful to the penultimate layer feature
space, and have mixed effects when networks are ﬁne-tuned.
More generally, our results reveal clear limits to transferring features, even among natural image datasets. ImageNet pretraining accelerates convergence and improves
performance on many datasets, but its value diminishes with
greater training time, more training data, and greater divergence from ImageNet labels. For some ﬁne-grained classiﬁcation datasets, a few thousand labeled examples, or a few
dozen per class, are all that are needed to make training from
scratch perform competitively with ﬁne-tuning. Surprisingly,
however, the value of architecture persists.
The last decade of computer vision research has demonstrated the superiority of image features learned from data
over generic, hand-crafted features. Before the rise of convolutional neural networks, most approaches to image understanding relied on hand-engineered feature descriptors
 . Krizhevsky et al. showed that, given the
training data provided by ImageNet , features learned
by convolutional neural networks could substantially outperform these hand-engineered features. Soon after, it became
clear that intermediate representations learned from ImageNet also provided substantial gains over hand-engineered
features when transferred to other tasks .
Is the general enterprise of learning widely-useful features
doomed to suffer the same fate as feature engineering? Given
differences between datasets , it is not entirely surprising that features learned on one dataset beneﬁt from some
amount of adaptation when applied to another. However,
given the history of attempts to build general natural-image
feature descriptors, it is surprising that common transfer
learning approaches cannot always proﬁtably adapt features
learned from a large natural-image to a much smaller naturalimage dataset.
ImageNet weights provide a starting point for features
on a new classiﬁcation task, but perhaps what is needed is
a way to learn adaptable features. This problem is closely
related to few-shot learning , but
these methods are typically evaluated with training and test
classes from the same distribution. Common few-shot learning methods do not seem to outperform classiﬁers trained on
ﬁxed features when domain shift is present , but it may
be possible to obtain better results with specialized methods or by combining few-shot learning methods with
ﬁne-tuning . It thus remains to be seen whether methods
can be developed or repurposed to adapt visual representations learned from ImageNet to provide larger beneﬁts across
natural image tasks.
Acknowledgements
We thank George Dahl, Boyang Deng, Sara Hooker,
Pieter-jan Kindermans, Rafael Müller, Jiquan Ngiam, Ruoming Pang, Daiyi Peng, Kevin Swersky, Vishy Tirumalashetty,
Vijay Vasudevan, and Emily Xue for comments on the experiments and manuscript, and Aliza Elkin and members of
the Google Brain team for support and ideas.