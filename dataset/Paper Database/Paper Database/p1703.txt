Random Vector Functional Link Neural Network based
Ensemble Deep Learning
Rakesh Katuwala, P.N. Suganthana,∗, M. Tanveerb
aSchool of Electrical and Electronic Engineering, Nanyang Technological University,
Singapore 639798, Singapore
bDiscipline of Mathematics, Indian Institute of Technology Indore, Simrol, Indore, 453552,
In this paper, we propose a deep learning framework based on randomized
neural network.
In particular, inspired by the principles of Random Vector
Functional Link (RVFL) network, we present a deep RVFL network (dRVFL)
with stacked layers. The parameters of the hidden layers of the dRVFL are
randomly generated within a suitable range and kept ﬁxed while the output
weights are computed using the closed form solution as in a standard RVFL
network. We also propose an ensemble deep network (edRVFL) that can be regarded as a marriage of ensemble learning with deep learning. Unlike traditional
ensembling approaches that require training several models independently from
scratch, edRVFL is obtained by training a single dRVFL network once. Both
dRVFL and edRVFL frameworks are generic and can be used with any RVFL
variant. To illustrate this, we integrate the deep learning networks with a recently proposed sparse-pretrained RVFL (SP-RVFL). Extensive experiments on
benchmark datasets from diverse domains show the superior performance of our
proposed deep RVFL networks.
Random Vector Functional Link (RVFL), deep RVFL, multi-layer
RVFL, ensemble deep learning, randomized neural network.
∗Corresponding author
Email addresses: (Rakesh Katuwal), 
(P.N. Suganthan), (M. Tanveer)
 
July 2, 2019
 
1. Introduction
Deep Learning, also known as representational learning, has sparked a surging interest in neural networks amongst the machine learning enthusiasts with
the state-of-the-art results in diverse applications ranging from image/video
classiﬁcation to segmentation, action recognition and many others. The superiority of a deep learning model emanates from its potential ability to extract
meaningful representations at diﬀerent levels of the hierarchical model while
disentangling a complex task into several simpler ones .
Deep neural networks typically consist of multiple hidden layers stacked
together. Each hidden layer builds an internal representation of the data with
the hidden layers closer to the input layer learning simple features such as edges
and layers above them learning sophisticated (complex) features . With
such stacked layers, deep learning models typically have thousands of model
parameters that need to be optimized during the training phase. These networks
are typically trained using back-propagation (BP) technique so as to minimize
the loss function (cross-entropy or mean square error or others depending on
the particular task). In addition to be time-consuming, such models may fail to
converge to a global minimum, thus, giving sub-optimal performance or lower
generalization .
Also, such deep learning models require large amount of
training data. While the usual image and speech datasets that are commonly
used with deep learning models have abundant data, there are datasets from a
wide variety of domains, such as agriculture, credit scoring, health outcomes,
ecology and others, with very limited data size. The performance of the stateof-the-art deep learning models on such datasets are far from superior .
Apart from the conventional BP-trained neural networks, there has also been
a growing interest in the class of randomization based neural networks .
Randomization based neural networks with closed form solution avoid the pitfalls of conventional BP-trained neural networks . They are faster to train
and have demonstrated good learning performance . Among the randomization based methods, Random Vector Functional Link (RVFL) network
has rapidly gained signiﬁcant traction because of its superior performance in
several diverse domains ranging from visual tracking , classiﬁcation ,
regression , to forecasting . RVFL is a single layer feed-forward neural network (SLFN) in which the weights and biases of the hidden neurons are
randomly generated within a suitable range and kept ﬁxed while the output
weights are computed via a simple closed form solution . Randomization
based neural networks greatly beneﬁt from the presence of direct links from the
input layer to the output layer as in RVFL network . The original
features are reused or propagated to the output layer via the direct links. The
direct links act as a regularization for the randomization . It also helps
to keep the model complexity low with the RVFL network being thinner and
simpler compared to its other counterparts. With the Occam’s Razor principle
and PAC learning theory advocating for simpler and less complex models, this makes the RVFL network attractive to use compared to other similar
randomized neural networks.
Ensembles of neural networks are known to be much more robust and accurate than individual networks . Because of the existence of
several randomization operations in their training procedure, neural networks
are regarded as unstable algorithms whose performance greatly vary even when
there is a small perturbation in training set or random seed. It is therefore not
surprising that two neural networks with identical architectures optimized with
diﬀerent initialization or slightly perturbed training data will converge to different solutions. This diversity can be exploited through ensembling, in which
multiple neural networks are trained with slightly diﬀerent training set or parameters and then combined with majority voting or averaging. Ensembling
often leads to drastic reductions in error rates. However, this comes with an obvious trade oﬀ: computational cost. While ensembling shallow neural networks
doesn’t incur great computational cost, the same is not true for the ensembling
of deep networks.
With the current trend of building deep networks, there have also been several attempts in the literature to build deep or multi-layer networks based on
randomized neural networks . Even though there exist several deep
learning models with randomized neural networks, there are limited works in the
context of RVFL network. In this paper, we investigate the performance of deep
learning and ensemble deep learning models based on RVFL networks. To the
best of our knowledge, is one of the pioneering paper to propose multi-layer
RVFL network. However, the performance of the multi-layer RVFL network
compared to a shallow RVFL network (with 1 hidden layer) is sub-optimal and
non-persuasive. A deep model enriched with complex feature learning capabilities should achieve good generalization.
Thus, in this paper, we propose
deep neural networks based on RVFL while maintaining its advantages of lower
complexity, training eﬃciency and good generalization. We also propose an ensemble of such deep networks without incurring any signiﬁcant training costs.
Speciﬁcally, we propose an ensemble deep RVFL network which can be regarded
as a marriage of ensemble and deep learning that is simple and straight-forward
to implement. The key contributions of this paper are summarized as follows:
• We propose a deep RVFL network (dRVFL), an extension of RVFL for
representational learning. The dRVFL network consists of several hidden
layers stacked on top of each other. The parameters of the hidden layers are randomly generated and kept ﬁxed while only the output weights
need to be computed. Thus, the deep RVFL network emanates from the
standard RVFL network.
• We also propose an implicit ensembling approach called ensemble deep
RVFL framework (edRVFL), a marriage of ensembling learning with deep
learning. Instead of training L neural networks independently from scratch
as in traditional ensembling method, we only train a single deep RVFL
network. The ensemble consists of L models equivalent to the number of
hidden layers in the single deep RVFL network. The ensemble is trained
in such a way that the higher models (equivalent to higher layers in deep
RVFL network) utilize both the original features (from direct links as in
standard RVFL network) and non-linearly transformed features from the
preceding layers. Thus, the framework is consistent with the tenets of
both ensemble learning and deep learning at the same time. The training
cost of edRVFL is slightly higher than that of a single dRVFL network
while it is signiﬁcantly lower than that of traditional ensembles.
• The deep learning models proposed in this paper (dRVFL and edRVFL)
are generic and are applicable with any RVFL variant. We create deep
learning models using both standard RVFL and recently proposed sparse
pre-trained RVFL (SP-RVFL) .
• With extensive experiments on several real-world classiﬁcation datasets,
we show that our proposed deep RVFL models (dRVFL and edRVFL)
have superior performance compared to other relevant neural networks.
The rest of this paper is structured as follows. Section 2 gives a brief overview
of related works on shallow randomized neural networks followed by randomization based multi-layer neural network. Section 3 details our proposed deep
RVFL method followed by its ensemble. In Section 4, we compare the performance of our proposed methods with other relevant neural networks. Finally,
the conclusion is presented in Section 5.
2. Related Works
In this section, we present a brief overview of the fundamentals of a standard
RVFL network, extreme learning machine (ELM) as a variant of RVFL, stateof-the-art sparse pre-trained RVFL (SP-RVFL) network and hierarchical ELM
(HELM), a randomization based multi-layer neural network.
2.1. Random Vector Functional Link Network (RVFL)
A basic framework of the standard RVFL network is shown in Fig. 1(a).
The inputs to the output layer in RVFL consist of both non-linearly transformed
features H from the hidden layer and original input features X. If d be the input
data features and N be the number of hidden nodes, then there are total d + N
inputs for each output node. Since the hidden layer parameters are randomly
generated and kept ﬁxed during the training phase, only the output weights
βs need to be computed.
Thus, the resulting optimization problem can be
mathematically represented as:
βs ∥Dβs −Y ∥2 + λ∥βs∥2 ,
where D = [H X] is the concatenation of hidden features and original features,
λ is the regularization parameter and Y is the target vector.
Typically, Eq. 1 can be solved via a closed form solution using either ridge
regression (i.e. λ ̸= 0) or Moore-Penrose pseudoinverse (i.e. λ = 0) . Using
Moore-Penrose pseudoinverse, the solution is given by: βs = D+Y while using
the regularized least squares (or ridge regression), the closed form solution is
Primal Space:
βs = (DT D + λI)−1DT Y ,
Dual Space:
βs = DT (DDT + λI)−1Y .
The training complexity in the RVFL network introduced by the matrix
inversion operation can be circumvented by using either primal or dual solution
depending on the sample size or total feature dimensions (i.e. input features
plus total number of hidden neurons) .
2.2. Extreme Learning Machine (ELM)
ELM , developed in 2004, can be viewed as a variant of RVFL without
direct links and bias term (see Figure 1(b)). Thus, Eq. 1 becomes
βE ∥HβE −Y ∥2 + λ∥βE∥2 .
Output Layer
Hidden Layer
Input Layer
D = [H, X]
Output Layer
Hidden Layer
Input Layer
Figure 1: Framework of RVFL and ELM networks. The structure of RVFL and
ELM diﬀer in the presence (absence) of direct links and bias term (not shown in the ﬁgure).
The red lines represent the direct links (original features) from the input to the output layer.
The weights for the blue lines are randomly generated from a certain range and kept ﬁxed.
Only the output weights (associated with red and black lines) need to be computed. Best
viewed in color.
Its solution is:
Primal Space:
βE = (HT H + λI)−1HT Y ,
Dual Space:
βE = HT (HHT + λI)−1Y .
2.3. Sparse pre-trained RVFL (SP-RVFL)
In a standard RVFL network, the hidden layer parameters (w and b) are
randomly generated within a suitable range and kept ﬁxed thereafter. Even
though RVFL has demonstrated its eﬃcacy in various domains, its performance
is often challenged by randomly assigned hidden layer parameters. To alleviate
this issue, the authors in proposed an unsupervised parameter learning
based RVFL known as sparse pre-trained RVFL (SP-RVFL). In an SP-RVFL
network, an autoencoder with l1 regularization is employed to learn the hidden
layer parameters. Speciﬁcally, the optimization problem for the autoencoder is
∥˜Hϖ −X∥2 + ∥ϖ∥1 ,
where X is the input, ˜H is the hidden layer matrix obtained via random
mapping and ϖ is the output weight matrix of the autoencoder. The above optimization problem, Eq. 7 is solved using a fast iterative shrinkage-thresholding
algorithm (FISTA) . The ϖ pre-trained by sparse-autoencoder is then used
as the weights of the hidden layer of a standard RVFL. The hidden biases are
then computed as:
i = 1, 2, . . . , N .
With the pre-trained hidden layer parameters, the output of the hidden layer
H of RVFL is computed as:
H = g(Xϖ + ˆb) ,
where g(·) is a non-linear activation function. Only the Eq. 9 in SP-RVFL
diﬀers from a standard RVFL network with H of a standard RVFL given by
H = g(Xw + b) where w and b are randomly generated. The optimization
problem of SP-RVFL then becomes similar to the optimization problem given
in Eq. 1. Eqs. 2 and 3 are then used to compute the output weights βs as in a
standard RVFL network.
2.4. Hierarchical ELM (HELM)
The HELM is a randomized multi-layer neural network based on ELM.
It consists of two components: feature encoding using ELM and an ELM based
classiﬁer. For feature extraction, it uses sparse autoencoder as deﬁned by Eq. 7
in the preceding section. Multiple hidden layers are then stacked on top of each
other for the feature extraction part. The extracted features are then used by
ELM classiﬁer for ﬁnal decision making.
3. Deep RVFL for representational learning
In this section, we introduce our proposed deep learning frameworks based
on RVFL. We ﬁrst describe the deep RVFL network in Section 3.1. We then
elucidate our proposed ensemble deep RVFL network in Section 3.2.
3.1. Deep Random Vector Functional Link Network
The Deep Random Vector Functional Link (dRVFL) network is an extension
of the shallow RVFL network in the context of representation learning or deep
learning. The dRVFL network is typically characterized by a stacked hierarchy
of hidden layers as shown in Fig. 2. The input to each layer in the stack is the
output of the preceding layer wherein each layer builds an internal representation
of the input data.
Although the stacked hierarchical organization of hidden
layers in dRVFL network allows a general ﬂexibility in the size (both in width
and depth) of the network, for the sake of simplicity here we consider a stack of
L hidden layers each of which contains the same number of hidden nodes N.
For the ease of notation, we omit the bias term in the formulas. The output
of the ﬁrst hidden layer is then deﬁned as follows:
H(1) = g(XW(1)) ,
while for every layer > 1 it is deﬁned as:
H(L) = g(H(L−1)W(L)) ,
where W(1) ∈Rd×N and W(L) ∈RN×N are the weight matrices between the
input-ﬁrst hidden layer and inter hidden layers respectively. These parameters
(weights and biases) of the hidden neurons are randomly generated within a
suitable range and kept ﬁxed during the training. g(·) is the non-linear activation
function. The input to the output layer is then deﬁned as:
D = [H(1) H(2) . . . H(L−1) H(L) X] .
This design structure is very similar to the standard shallow RVFL network
wherein the input to the output layer consists of non-linear features from the
stacked hidden layers along with the original features. The output of the dRVFL
network is then deﬁned as follows:
The output weight βd ∈R(NL+d)×K (K: the number of classes) is then solved
using Eqs. 2 or 3.
12 and 13, one can see that in dRVFL there exists a linear
combination between the features and the output layer weight matrix βd i.e. a
weighted sum of the features from the hidden layers including the input layer.
In the training stage, this directly enables the model to weigh diﬀerently the
contribution of each type of features originating from diﬀerent layers.
It is also worth mentioning that our proposed dRVFL network diﬀerentiates itself from the deep learning architecture proposed in in threefold: 1)
dRVFL is inspired by a shallow RVFL network with the output layer consisting
of both non-linearly transformed features and original features via direct links.
In contrast, the deep learning architecture proposed in does not consider
the original features during the output weight computation, 2) we investigate
the performance of the dRVFL network in classiﬁcation problems while 
explores time-series problems, and 3) the deep learning framework, dRVFL is
generic and can be used with any RVFL variant.
3.2. Ensemble Deep Random Vector Functional Link Network
The framework of the ensemble deep RVFL network (edRVFL) is shown
in Fig. 3. It serves three purposes: 1) instead of using only the higher level
representations (features extracted from the ﬁnal hidden layer) of the data as
in a conventional deep learning model for classiﬁcation, it employs rich
intermediate features also for ﬁnal decision making. 2) the ensemble is obtained
by training a single dRVFL network once with a training cost slightly higher
than that of a single dRVFL but cheaper than training several independent
models of dRVFL. 3) like dRVFL, the edRVFL framework is generic and any
RVFL variant can be used with it.
W(L-1),b(L-1)
Direct Links
Figure 2: Framework of a dRVFL network. It consists of several hidden layers stacked on top
of each other whose parameters (weights and biases between the hidden layers) are randomly
generated and kept ﬁxed during the training. Only the output weights βd need to be computed
as in the shallow RVFL network.
The computation of the output weight βd in the dRVFL network described
in Section 3.1 requires matrix inversion of size either (T × T) or ((NL + d) ×
(NL + d)), whichever is small (refer to primal and dual solutions, Eqs. 2 and
3) where T is the training data size, L is the number of hidden layers, N is the
number of hidden nodes at each layer and d is the dimension of the data. For
simplicity, we omit the bias terms. In a standard implementation, the matrix
inversion of a matrix of size (T × T) requires O(T 3) time and O(T 2) memory
 . In case of dRVFL, this is equivalent to either O(T 3) or O((NL + d)3)
time and either O(T 2) or O((NL + d)2) memory. Such scaling is prohibitive
when all N, L, T and d are large. Inversion of a large matrix can also result in
out-of-memory failures thus, requiring powerful and high performance hardware
 . Depending on the dataset, all these parameters can be actually large. For
example, one dataset used in this paper has 7000 training samples with 5000
features. A dRVFL network with 10 hidden layers and 100 hidden nodes in each
layer, requires matrix inversion of size (6000×6000) using primal solution (using
dual solution would require matrix inversion of size (7000 × 7000)). Thus, we
decompose the computation of the ﬁnal output weight βd of dRVFL into several
small βed in edRVFL. Speciﬁcally, each small βed is independently computed
(treated as independent model) and the ﬁnal output is obtained by using either
majority voting or averaging of the models. Each small βed requires the matrix
inversion of size either (T × T) or ((N + d) × (N + d)). Without direct links,
it would require an inversion of size either (T × T) or (N × N). However, as
discussed in the preceding sections, direct links are essential parts of randomized
neural networks . The signiﬁcance of such direct links is also
discussed later in Section 4.4.1.
The input to each hidden layer is the non-linearly transformed features from
the preceding layer as in dRVFL along with the original input features (direct
links) as in standard RVFL. The direct links act as a regularization for the
randomization. The input of the ﬁrst hidden layer is then deﬁned as follows:
H(1) = g(XW(1)) ,
while for every layer > 1 it is deﬁned as:
H(L) = g([H(L−1)X]W(L)) .
The output weights βed are then solved independently using Eqs. 2 or 3.
The mechanism of obtaining several models while training only a single
model (implicit ensembles) as in ensemble deep RVFL network (edRVFL) is
related to the snapshot ensembling of which trains a neural network using
a cyclic learning rate schedule to converge to diﬀerent local minima. Instead
of training several neural networks independently (true ensembles), the method
saves (snapshots the parameters) each time the model converges and adds the
corresponding network to the ensemble (also known as implicit ensemble). However, such approach is only applicable to neural networks trained with stochastic gradient descent (SGD). Since RVFL neural networks can be trained using
closed form solutions, no learning rate mechanism is required. Like snapshot
ensembling, edRVFL can even be ensembled if enough resources are available
Hidden Layer L-1
Hidden Layer 1
Hidden Layer L
Hidden Layer 2
W(L-1),b(L-1)
Final Output
Averaging)
Figure 3: Framework of ensemble deep RVFL network (edRVFL). It diﬀers from dRVFL
in that the computation of ﬁnal output weight βd is decomposed into several small βed.
Speciﬁcally, each small βed is independently computed (treated as independent model) and
the ﬁnal output is obtained by using either majority voting or averaging of the models. Each
higher level model is fed with original input data and the non-linearly transformed features
from the preceding model. O1, . . . , OL represents the output of each model.
during training.
4. Experiments
4.1. Datasets
The experiments are performed on 13 publicly available real-world classiﬁcation datasets from various domains used in which include two biomedical
datasets (Carcinom and Lung), two human face image datasets (ORL and Yale),
four hand-written digit datasets (Binary Alphabet(BA), Gisette, a portion of
MNIST and USPS), two object recognition datsets (COIL20, COIL100) and
three text datasets (BASEHOCK, RCV1 and TDT2). Table 1 gives an overview
of the 13 real-world application datasets.
4.2. Compared Methods
To verify the eﬀectiveness of our proposed deep learning frameworks, we
perform comparisons against relevant algorithms (shallow RVFL networks, ran-
Table 1: Overview of the datasets used in this paper.
Handwritten Digits
For datasets preprocessing and further details, please refer to .
domization based multi-layer neural networks and ensembles of RVFL). The
compared methods are enumerated as follows:
1. ELM: extreme learning machine ; shallow RVFL without direct links
2. RVFL: standard shallow RVFL network .
3. SP-RVFL: sparse pre-trained RVFL ; state-of-the-art RVFL network.
4. HELM: hierarchical ELM , a multi-layer network based on ELM; has
superior performance compared to other relevant deep learning methods such as Stacked Auto-Encoders (SAE) , Stacked Denoising Auto-
Encoders (SDA) , Deep Belief Networks (DBN) , Deep Boltzmann
Machines (DBM) .
5. dRVFL: deep RVFL proposed in this paper.
6. dRVFL(-O): dRVFL without direct links but with bias.
7. edRVFL: ensemble deep RVFL proposed in this paper.
8. edRVFL(-O): edRVFL without direct links but with bias.
9. dSP-RVFL: SP-RVFL based dRVFL
10. edSP-RVFL: SP-RVFL based edRVFL
4.3. Experimental Settings
To compare the diﬀerent algorithms, we follow the experimental settings of
 . The number of hidden neurons N is set to 100 . For deep RVFL based
methods, the same number of hidden neurons is used at each layer with the
maximum number of hidden layers L set to 10 for each dataset. The HELM
algorithm is implemented using the source code1 available online. Meanwhile,
the regularization parameter λ in each layer is set as (1/C) where C is tuned
over the range 2x, {x = −6, −4, −2, . . . , 12}. The widely used sigmoid function is
used as the activation function in each type of RVFL network. The experimental
results reported are obtained by averaging results from 10-fold cross-validation.
4.4. Performance Comparison and Analysis
In this section, we compare our proposed deep RVFL based frameworks
against pertinent methods. Speciﬁcally, we ﬁrst compare standard RVFL based
methods in Section 4.4.1 and SP-RVFL based methods in Section 4.4.2.
4.4.1. Comparison between standard RVFL based methods
The classiﬁcation accuracies of each algorithm in each dataset is presented in
Table 2. From the table, one can see that the ensemble deep RVFL (edRVFL)
proposed in this paper has the best accuracy in 12 out of 13 datasets. The
edRVFL has comparable performance to dRVFL in 3 datasets while it outperforms dRVFL in all other datasets. We follow the procedure of and use
the Friedman rank of each classiﬁer to assess its performance. Depending on
the performance, each classiﬁer is ranked, with the highest performing classiﬁer
ranked 1, the second highest ranked 2, and so on in each dataset. From the
same table, one can see that edRVFL is the top ranked algorithm followed by
We also perform a statistical comparison of the algorithms using the Friedman test . The Friedman test compares the average ranks of the classi-
ﬁers, Rj = P
i where, rj
i is the rank of the j-th of the m classiﬁer on the i-th
1 
Table 2: Accuracy (%) of the standard RVFL based methods on all the datasets
dRVFL(-O)†
edRVFL(-O)†
62.94±12.46
97.05±3.42
90.85±7.13
98.86±2.41
80.46±10.94
97.12±4.01
98.86±2.41
95.57±5.45
97.05±4.19
92.52±8.91
96.57±4.03
97.05±4.19
91.25±3.58
59.38±12.5
77.5±11.51
71.51±5.78
87.17±7.83
70.85±12.83
86.03±7.96
88.58±6.92
52.21±5.21
57.42±4.27
67.09±3.31
65.33±4.11
55.13±2.17
59.97±3.48
66.11±3.34
83.04±2.11
92.07±1.17
95.17±0.93
98.16±0.48
98.17±0.55
98.21±0.55
79.13±1.74
88.11±1.15
88.12±1.32
84.07±1.71
86.52±2.06
92.78±1.43
93.41±2.18
98.54±0.51
99.65±0.49
98.54±0.69
98.82±0.93
99.86±0.29
64.07±2.09
75.28±1.07
90.06±0.91
81.65±0.72
87.51±0.77
73.37±3.63
88.19±3.18
96.39±1.22
98.04±1.07
83.59±2.54
97.34±1.11
98.04±0.76
79.51±1.74
87.57±0.73
88.69±1.25
93.75±0.71
79.52±2.18
92.34±0.76
93.86±0.69
61.32±2.21
81.92±0.83
80.31±1.28
94.73±0.43
96.51±0.53
73.56±4.83
86.95±3.01
87.29±2.65
92.69±2.15
82.34±4.09
91.26±2.29
93.35±2.01
Avg. Friedman Rank
The results for ELM and RVFL are directly copied from . † are the methods introduced in this paper. The best results for each dataset is
given in bold. Lower rank reﬂects better performance.
of M datasets. The null hypothesis is that the performance of all the classiﬁers
are similar with their ranks Rj being equal.
Let M and m denote the number of datasets and classiﬁers respectively.
When M and m are large enough, the Friedman statistic
j −m(m + 1)2
is distributed according to χ2
F with m-1 degrees of freedom under the null hypothesis. However, in this case, χ2
F is undesirably conservative. A better statistics is given by
M(m −1) −χ2
which is distributed according to F-distribution with (m-1) and (m-1)(M-1)
degrees of freedom. If the null-hypothesis is rejected, the Nemenyi post-hoc test
 can be used to check whether the performance of two among m classiﬁers is
signiﬁcantly diﬀerent. The performance of two classiﬁers is signiﬁcantly diﬀerent
if the corresponding average ranks of the classiﬁers diﬀer by at least the critical
diﬀerence (CD)
where critical values qα are based on the Studentized range statistic divided by
2. α is the signiﬁcance level and is equal to 0.05 in this paper.
Based on simple calculations we obtain, χ2
F = 68.11 and FF = 82.64. With
7 classiﬁers and 13 datasets, FF is distributed according to the F-distribution
with 7 −1 = 6 and (7 −1)(13 −1) = 72 degrees of freedom.
The critical
value for F(6,72) for α = 0.05 is 2.22, so we reject the null-hypothesis. Based
on the Nemenyi test, the critical diﬀerence is CD = qα
(m(m + 1))/(6M) =
7 ∗8/(6 ∗13) ≃2.49. From Fig. 4, we can see that edRVFL is statistically signiﬁcantly better than ELM, RVFL, HELM and dRVFL(-O) while
dRVFL is statistically signiﬁcantly better than ELM, RVFL, and dRVFL(-O).
The diﬀerence of ranks between the randomized multi-layer networks, HELM
and dRVFL is 2.35 (0.14 less than CD). The dRVFL has superior performance
(around 5.4% times more accurate) in almost all the datasets compared to
HELM except BA dataset. Some of the biggest improvements of dRVFL over
HELM in average are in Carcinom (8.01%), ORL (7.75%), Yale (15.66%),
COIL100 (14.7%), RCV1 (5.06%) and TDT2 (10.91%) datasets.
This indicates the superior generalization ability (representational capability) of dRVFL
over HELM.
In addition, we select 5 datasets from each domain (Lung from biology, ORL
from face, USPS from digits, COIL20 from object and RCV1 from text), and
report the experimental results for diﬀerent number of hidden nodes in dRVFL
and edRVFL in Fig. 5. The number of hidden nodes in each dataset is varied
from 10 to 100 with a step-size of 10. One can see from Fig. 5, setting N = 100
is appropriate for both dRVFL and edRVFL. Increasing the number of hidden
nodes generally increases the generalization of the network until some point after
which it becomes stable. Similarly, we also compare the training and testing
times of dRVFL and edRVFL in these 5 datasets in Fig. 6. As can be seen
from the ﬁgure, the training times of both dRVFL and edRVFL increase with
the increase in the number of hidden nodes while there is only a slight increase
in the testing time for both cases.
edRVFL(-O)
Statistical comparison of classiﬁers against each other based on Nemenyi test.
Groups of classiﬁers that are not signiﬁcantly diﬀerent (at α = 0.05) are connected.
Comparison between RVFL and dRVFL. From Table 2, it can be observed that
the deep learning framework proposed in this paper, dRVFL, is more accurate than its baseline (shallow) RVFL network by approx.
the biggest improvements are in Yale (9.67%), BA (7.91%), Gisette (6.09%),
COIL20 (6.24%), BASEHOCK (9.85%), RCV1 (6.18%) and TDT2 (14.59%)
datasets. We also compare dRVFL and RVFL with the same number of hidden
nodes in each dataset in Fig. 7. The dRVFL network is on average 3.61% more
accurate than shallow RVFL network with the same number of hidden nodes.
This accentuates the beneﬁts of representational learning in case of multi-layer
(deep) networks wherein each hidden layer extracts meaningful feature representation from its input.
Comparison of edRVFL with True Ensembles. Here, we compare the implicit
ensembles of dRVFL (edRVFL) with its true ensemble (TedRVFL) in terms of
performance and training complexities. The true ensemble method, TedRVFL
averages dRVFL methods trained independently as in . The training and
testing accuracies of edRVFL and TedRVFL is presented in Fig. 8. For edRVFL,
the number of models corresponds to the number of hidden layers L while for
TedRVFL, this corresponds to an ensemble of L dRVFL models with L hidden
As can be observed from the ﬁgure, the training accuracies of both
# of hidden neurons
Accuracy (%)
Training Accuracy(dRVFL)
Test Accuracy(dRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
(a) COIL20
# of hidden neurons
Accuracy (%)
Training Accuracy(dRVFL)
Test Accuracy(dRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
# of hidden neurons
Accuracy (%)
Training Accuracy(dRVFL)
Test Accuracy(dRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
# of hidden neurons
Accuracy (%)
Training Accuracy(dRVFL)
Test Accuracy(dRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
# of hidden neurons
Accuracy (%)
Training Accuracy(dRVFL)
Test Accuracy(dRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
Figure 5: Comparison of dRVFL and edRVFL in terms of accuracy (%) w.r.t diﬀerent number of hidden
# of hidden neurons
Training Time (dRVFL)
Testing Time (dRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
(a) COIL20
# of hidden neurons
Training Time (dRVFL)
Testing Time (dRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
# of hidden neurons
Training Time (dRVFL)
Testing Time (dRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
# of hidden neurons
Training Time (dRVFL)
Testing Time (dRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
# of hidden neurons
Training Time (dRVFL)
Testing Time (dRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
Figure 6: Training and testing times comparison of dRVFL and edRVFL w.r.t diﬀerent number of hidden
nodes. For the same number of hidden nodes, the training time of edRVFL is in average 3 times more than
that of dRVFL.
Accuracy (%)
Figure 7: Comparison of dRVFL and RVFL in terms of accuracy (%) with the same number
of hidden nodes.
edRVLF and TedRVFL increase with the increase in the number of models.
However, this isn’t the case for the test accuracies thus, requiring the best
parameter search (in this case L).
However, from Fig.
8, one can see that
with proper selection of L, the edRVFL can achieve either comparable or even
better test accuracies compared to TedRVFL. Similarly, we also compare the
training and testing times of edRVFL and TedRVFL in 5 datasets in Fig. 9. As
can be seen from the ﬁgure, the training times of both edRVFL and TedRVFL
increase with the increase in the number of models while there is only a slight
increase in the testing times for both cases. As the number of models increases,
the training time of TedRVFL increases sharply while that for edRVFL only
increases slightly. In a nutshell, edRVFL has comparable or better performance
than TedRVFL while requiring signiﬁcantly less training time.
Eﬀect of direct links. The signiﬁcance of direct links in case of randomized shallow neural networks has been extensively articulated in the literature . In this paper, we investigate the eﬀect of direct links in the case of randomization based deep neural networks, speciﬁcally in the dRVFL and edRVFL
methods introduced in this paper. The dRVFL(-O) and edRVFL(-O) are the
deep learning methods equivalent to dRVFL and edRVFL respectively without
direct links. The experimental results of dRVFL(-O) and edRVFL(-O) on the
real-world classiﬁcation datasets are presented in Table 2. The dRVFL(-O) dif-
# of models
Accuracy (%)
Training Accuracy(TedRVFL)
Test Accuracy(TedRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
(a) COIL20
# of models
Accuracy (%)
Training Accuracy(TedRVFL)
Test Accuracy(TedRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
# of models
Accuracy (%)
Training Accuracy(TedRVFL)
Test Accuracy(TedRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
# of models
Accuracy (%)
Training Accuracy(TedRVFL)
Test Accuracy(TedRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
# of models
Accuracy (%)
Training Accuracy(TedRVFL)
Test Accuracy(TedRVFL)
Training Accuracy(edRVFL)
Test Accuracy(edRVFL)
Figure 8: Comparison of edRVLF (implicit ensemble) and TedRVFL (true ensemble) in terms of accuracy
# of models
Training Time (TedRVFL)
Testing Time (TedRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
(a) COIL20
# of models
Training Time (TedRVFL)
Testing Time (TedRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
# of models
Training Time (TedRVFL)
Testing Time (TedRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
# of models
Training Time (TedRVFL)
Testing Time (TedRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
# of models
Training Time (TedRVFL)
Testing Time (TedRVFL)
Training Time (edRVFL)
Testing Time (edRVFL)
Figure 9: Training and testing times comparison of edRVLF (implicit ensemble) and TedRVFL (true ensemble).
fers from HELM in that HELM uses only the last layer features extracted
from the feature extractor part for ﬁnal classiﬁcation while dRVFL uses all the
hidden layer features. From the table, we can see that the diﬀerence in accuracies between dRVFL and dRVFL(-O) is approx. 10.35% while that between
edRVFL and edRVFL(-O) is approx. 2.09%. In dRVFL (refer to Fig. 2), in addition to the randomly generated hidden layer features, the input to the output
nodes contains original features via the direct links. This enables the network
to weigh higher the discriminative features while ignoring the redundant or lessimportant features. As in the case of randomized shallow neural networks, the
direct links act as regularization for the randomization. Without direct links,
the inputs to the output of dRVFL is simply the hidden layer features (generated randomly) resulting in a sub-optimal performance. To avoid such issue
(sub-optimal performance) in case of edRVFL, the input to each hidden layer is
a concatenation of original features via direct links and the hidden layer features
from the preceding layer (except in case of the ﬁrst hidden layer).
Parameter Sensitivity Analysis. In dRVFL and edRVFL, the number of hidden
layers L, the number of hidden nodes N and the regularization parameter C
need to be properly selected for each dataset. To further analyze the dRVFL
and edRVFL methods, we conduct the sensitivity study for the parameters L, H
and C in case of two datasets COIL20 and USPS. The parameters H and C are
common parameters for both shallow and deep RVFL based methods while only
parameter L is relevant for deep learning methods. We show the experimental
results in two diﬀerent settings: varying L while keeping H ﬁxed and varying
H while keeping L ﬁxed. Speciﬁcally, we employ a grid search strategy to vary
these parameters. The parameter L is varied from 2:10 with a step-size of 1,
the parameter H is varied from 10:100 with a step-size of 10. Similarly, the
C parameter is varied within 2x, {x = −6, −4, −2, 0, 2, 4, 6, 8, 10, 12}. As can
be seen from Figs. 10 and 11, diﬀerent combination of the parameters result
in diﬀerent performance. Therefore, it is necessary to determine the suitable
values of the parameters L, H and C for each dataset.
Accuracy (%)
(a) COIL20
Accuracy (%)
Accuracy (%)
(c) COIL20
Accuracy (%)
Figure 10: Performance variation of the proposed dRVFL (ﬁrst row) and edRVFL(second row)
methods in terms of accuracy (%) for ﬁxed H. Diﬀerent parameter combinations may result
in diﬀerent performance.
Accuracy (%)
(a) COIL20
Accuracy (%)
Accuracy (%)
(c) COIL20
Accuracy (%)
Figure 11: Performance variation of the proposed dRVFL (ﬁrst row) and edRVFL(second row)
methods in terms of accuracy (%) for ﬁxed L. Diﬀerent parameter combinations may result
in diﬀerent performance.
4.4.2. Comparison between SP-RVFL based methods
The deep learning frameworks dRVFL and edRVFL proposed in Sections
3.1 and 3.2 respectively are generic and any RVFL network can be used as
a base model with both dRVFL and edRVFL. Here, we use SP-RVFL , a
recently proposed state-of-the-art RVFL network to create dRVFL and edRVFL
networks. Speciﬁcally, we term the deep architecture using SP-RVFL as deep
sparse pre-trained RVFL (dSP-RVFL) and its ensemble as edSP-RVFL. The SP-
RVFL is described in Section 2.3. We run the experiments on all the datasets
presented in Table 1 and report the experimental results of dSP-RVFL and
edSP-RVFL in Table 3. From the table, one can see that our deep learning
frameworks dSP-RVFL and edSP-RVFL have better performance compared to
SP-RVFL. Also, edSP-RVFL has either same or better performance than dSP-
Table 3: Comparison between SP-RVFL based methods in terms of accuracy (%)
SP-RVFL 
97.64±3.04
98.27±2.79
98.27±2.79
97.05±4.19
97.05±4.19
86.87±9.88
87.17±7.38
87.17±7.38
65.57±5.66
68.52±2.65
73.08±2.67
98.13±1.45
98.21±0.41
98.21±0.41
91.26±0.82
98.75±1.39
98.96±0.67
99.72±0.36
89.76±1.24
93.38±1.26
94.26±0.86
97.79±0.98
97.79±0.98
93.38±0.61
93.29±0.86
96.17±0.63
96.17±0.63
93.52±2.01
94.29±1.95
Avg. Friedman Rank
The results for SP-RVFL are directly copied from for all the datasets
except for the RCV1 dataset where we were unable to replicate the reported
result (94.84±0.64) within the given level of variability. Thus, SP-RVFL’s
performance in RCV1 is based on our implementation.
We also perform a statistical comparison of the algorithms using the Fried-
Table 4: Average Friedman rank
based on classiﬁcation accuracy
of each method
dRVFL(-O)†
edRVFL(-O)†
edSP-RVFL†
introduced in this paper.
Lower rank reﬂects better
performance.
man test as in Section 4.4.1. Based on simple calculations we obtain, χ2
20.54 and FF = 71.23. With 3 classiﬁers and 13 datasets, FF is distributed
according to the F-distribution with 3 −1 = 2 and (3 −1)(13 −1) = 24 degrees
of freedom. The critical value for F(2,24) for α = 0.05 is 3.4, so we reject the
null-hypothesis.
Based on the Nemenyi test, the critical diﬀerence is CD =
(m(m + 1))/(6M) = 2.344 ∗
3 ∗4/(6 ∗13) ≃0.92. From Table 3, one can
see that both dSP-RVFL and edSP-RVFL are statistically signiﬁcantly better
than their baseline method, SP-RVFL.
4.4.3. Overall Comparison
We present an overall comparison of the algorithms using the Friedman
rank in Table 4. From the table, one can see that the deep learning frameworks
introduced in this paper have the best ranks compared to other algorithms.
Speciﬁcally, the ensemble deep learning frameworks (edSP-RVFL and edRVFL)
obtain the top ranks followed by the single deep learning frameworks (dSP-
RVFL and dRVFL).
5. Conclusion
In this paper, we ﬁrst proposed a deep learning model (dRVFL) based on
random vector functional link network. As in a standard RVFL network, the
parameters of the hidden layers were randomly generated and kept ﬁxed with
the output weights computed analytically using a closed form solution. The
dRVFL network while extracting rich feature representations through several
hidden layers also acts as a weighting network thereby, providing a weight to
features from all the hidden layers including the original features obtained via
direct links. We then proposed an ensemble dRVFL, edRVFL, which combines
ensemble learning with deep learning. Instead of training several models independently as in traditional ensembles, edRVFL can be obtained by training a
deep network only once. The training cost of edRVFL is slightly greater than
that of a single dRVFL network but signiﬁcantly lower than that of the traditional ensembles. Both dRVFL and edRVFL are generic and any RVFL variant
can be used with them.
To demonstrate this generic nature, we developed
sparse-pretrained RVFL (SP-RVFL) based deep RVFL networks (dSP-RVFL
and edSP-RVFL). The SP-RVFL uses an sparse-autoencoder to learn the hidden layer parameters of RVFL as opposed to randomly generating them as in
standard RVFL. Extensive experiments on several classiﬁcation datasets showed
that the our deep learning RVFL networks achieve better generalization compared to pertinent randomized neural networks. As our future work, we will
consider other applications (datasets) related to but not limited to regression,
time-series forecasting and other learning tasks such as semi-supervised learning,
and incremental learning.
References