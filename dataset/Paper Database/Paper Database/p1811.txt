TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LSTM: A Search Space Odyssey
Klaus Greff, Rupesh K. Srivastava, Jan Koutn´ık, Bas R. Steunebrink, J¨urgen Schmidhuber
Abstract—Several variants of the Long Short-Term Memory
(LSTM) architecture for recurrent neural networks have been
proposed since its inception in 1995. In recent years, these
networks have become the state-of-the-art models for a variety
of machine learning problems. This has led to a renewed interest
in understanding the role and utility of various computational
components of typical LSTM variants. In this paper, we present
the ﬁrst large-scale analysis of eight LSTM variants on three
representative tasks: speech recognition, handwriting recognition,
and polyphonic music modeling. The hyperparameters of all
LSTM variants for each task were optimized separately using
random search, and their importance was assessed using the
powerful fANOVA framework. In total, we summarize the results
of 5400 experimental runs (≈15 years of CPU time), which
makes our study the largest of its kind on LSTM networks.
Our results show that none of the variants can improve upon
the standard LSTM architecture signiﬁcantly, and demonstrate
the forget gate and the output activation function to be its
most critical components. We further observe that the studied
hyperparameters are virtually independent and derive guidelines
for their efﬁcient adjustment.
Index Terms—Recurrent neural networks, Long Short-Term
Memory, LSTM, sequence learning, random search, fANOVA.
I. INTRODUCTION
Recurrent neural networks with Long Short-Term Memory
(which we will concisely refer to as LSTMs) have emerged as
an effective and scalable model for several learning problems
related to sequential data. Earlier methods for attacking these
problems have either been tailored towards a speciﬁc problem
or did not scale to long time dependencies. LSTMs on the
other hand are both general and effective at capturing longterm temporal dependencies. They do not suffer from the
optimization hurdles that plague simple recurrent networks
(SRNs) and have been used to advance the state-ofthe-art for many difﬁcult problems. This includes handwriting
recognition and generation , language modeling 
and translation , acoustic modeling of speech , speech
c⃝2016 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.
Manuscript received May 15, 2015; revised March 17, 2016; accepted June 9,
2016. Date of publication July 8, 2016; date of current version June 20, 2016.
DOI: 10.1109/TNNLS.2016.2582924
This research was supported by the Swiss National Science Foundation grants
“Theory and Practice of Reinforcement Learning 2” (#138219) and “Advanced
Reinforcement Learning” (#156682), and by EU projects “NASCENCE”
(FP7-ICT-317662), “NeuralDynamics” (FP7-ICT-270247) and WAY (FP7-
ICT-288551).
K. Greff, R. K. Srivastava, J. Kout´ık, B. R. Steunebrink and J. Schmidhuber
are with the Istituto Dalle Molle di studi sull’Intelligenza Artiﬁciale (IDSIA),
the Scuola universitaria professionale della Svizzera italiana (SUPSI), and the
Universit`a della Svizzera italiana (USI).
Author e-mails addresses: {klaus, rupesh, hkou, bas, juergen}@idsia.ch
synthesis , protein secondary structure prediction ,
analysis of audio , and video data among others.
The central idea behind the LSTM architecture is a memory
cell which can maintain its state over time, and non-linear
gating units which regulate the information ﬂow into and out of
the cell. Most modern studies incorporate many improvements
that have been made to the LSTM architecture since its
original formulation . However, LSTMs are now
applied to many learning problems which differ signiﬁcantly
in scale and nature from the problems that these improvements
were initially tested on. A systematic study of the utility of
various computational components which comprise LSTMs
(see Figure 1) was missing. This paper ﬁlls that gap and
systematically addresses the open question of improving the
LSTM architecture.
We evaluate the most popular LSTM architecture (vanilla
LSTM; Section II) and eight different variants thereof on
three benchmark problems: acoustic modeling, handwriting
recognition, and polyphonic music modeling. Each variant
differs from the vanilla LSTM by a single change. This
allows us to isolate the effect of each of these changes
on the performance of the architecture. Random search is used to ﬁnd the best-performing hyperparameters for
each variant on each problem, enabling a reliable comparison
of the performance of different variants. We also provide
insights gained about hyperparameters and their interaction
using fANOVA .
II. VANILLA LSTM
The LSTM setup most commonly used in literature was
originally described by Graves and Schmidhuber . We refer
to it as vanilla LSTM and use it as a reference for comparison
of all the variants. The vanilla LSTM incorporates changes
by Gers et al. and Gers and Schmidhuber into the
original LSTM and uses full gradient training. Section III
provides descriptions of these major LSTM changes.
A schematic of the vanilla LSTM block can be seen in
Figure 1. It features three gates (input, forget, output), block
input, a single cell (the Constant Error Carousel), an output
activation function, and peephole connections1. The output of
the block is recurrently connected back to the block input and
all of the gates.
1Some studies omit peephole connections, described in Section III-B.
 
TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
unweighted connection
weighted connection
connection with time-lag
mutliplication
sum over all inputs
branching point
gate activation function
(always sigmoid)
forget gate
input gate
block input
output gate
LSTM block
input activation function
(usually tanh)
output activation function
(usually tanh)
block output
Figure 1. Detailed schematic of the Simple Recurrent Network (SRN) unit (left) and a Long Short-Term Memory block (right) as used in the hidden layers of
a recurrent neural network.
A. Forward Pass
Let xt be the input vector at time t, N be the number of
LSTM blocks and M the number of inputs. Then we get the
following weights for an LSTM layer:
• Input weights: Wz, Wi, Wf, Wo ∈RN×M
• Recurrent weights: Rz, Ri, Rf, Ro ∈RN×N
• Peephole weights: pi, pf, po ∈RN
• Bias weights: bz, bi, bf, bo ∈RN
Then the vector formulas for a vanilla LSTM layer forward
pass can be written as:
¯zt = Wzxt + Rzyt−1 + bz
zt = g(¯zt)
block input
¯it = Wixt + Riyt−1 + pi ⊙ct−1 + bi
it = σ(¯it)
input gate
¯f t = Wfxt + Rfyt−1 + pf ⊙ct−1 + bf
f t = σ(¯f t)
forget gate
ct = zt ⊙it + ct−1 ⊙f t
¯ot = Woxt + Royt−1 + po ⊙ct + bo
ot = σ(¯ot)
output gate
yt = h(ct) ⊙ot
block output
Where σ, g and h are point-wise non-linear activation functions.
The logistic sigmoid (σ(x) =
1+e−x ) is used as gate activation
function and the hyperbolic tangent (g(x) = h(x) = tanh(x))
is usually used as the block input and output activation function.
Point-wise multiplication of two vectors is denoted by ⊙.
B. Backpropagation Through Time
The deltas inside the LSTM block are then calculated as:
δyt = ∆t + RT
z δzt+1 + RT
i δit+1 + RT
f δf t+1 + RT
δ¯ot = δyt ⊙h(ct) ⊙σ′(¯ot)
δct = δyt ⊙ot ⊙h′(ct) + po ⊙δ¯ot + pi ⊙δ¯it+1
+ pf ⊙δ¯f t+1 + δct+1 ⊙f t+1
δ¯f t = δct ⊙ct−1 ⊙σ′(¯f t)
δ¯it = δct ⊙zt ⊙σ′(¯it)
δ¯zt = δct ⊙it ⊙g′(¯zt)
Here ∆t is the vector of deltas passed down from the layer
above. If E is the loss function it formally corresponds to ∂E
but not including the recurrent dependencies. The deltas for
the inputs are only needed if there is a layer below that needs
training, and can be computed as follows:
z δ¯zt + WT
i δ¯it + WT
f δ¯f t + WT
Finally, the gradients for the weights are calculated as
follows, where ⋆can be any of {¯z,¯i,¯f, ¯o}, and ⟨⋆1, ⋆2⟩denotes
the outer product of two vectors:
ct ⊙δ¯it+1
⟨δ⋆t+1, yt⟩
ct ⊙δ¯f t+1
TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
III. HISTORY OF LSTM
The initial version of the LSTM block included
(possibly multiple) cells, input and output gates, but no forget
gate and no peephole connections. The output gate, unit
biases, or input activation function were omitted for certain
experiments. Training was done using a mixture of Real Time
Recurrent Learning (RTRL) and Backpropagation
Through Time (BPTT) . Only the gradient of the cell
was propagated back through time, and the gradient for the
other recurrent connections was truncated. Thus, that study
did not use the exact gradient for training. Another feature of
that version was the use of full gate recurrence, which means
that all the gates received recurrent inputs from all gates at the
previous time-step in addition to the recurrent inputs from the
block outputs. This feature did not appear in any of the later
A. Forget Gate
The ﬁrst paper to suggest a modiﬁcation of the LSTM
architecture introduced the forget gate , enabling the LSTM
to reset its own state. This allowed learning of continual tasks
such as embedded Reber grammar.
B. Peephole Connections
Gers and Schmidhuber argued that in order to learn
precise timings, the cell needs to control the gates. So far
this was only possible through an open output gate. Peephole
connections (connections from the cell to the gates, blue
in Figure 1) were added to the architecture in order to
make precise timings easier to learn. Additionally, the output
activation function was omitted, as there was no evidence that
it was essential for solving the problems that LSTM had been
tested on so far.
C. Full Gradient
The ﬁnal modiﬁcation towards the vanilla LSTM was
done by Graves and Schmidhuber . This study presented
the full backpropagation through time (BPTT) training for
LSTM networks with the architecture described in Section II,
and presented results on the TIMIT benchmark. Using
full BPTT had the added advantage that LSTM gradients
could be checked using ﬁnite differences, making practical
implementations more reliable.
D. Other Variants
Since its introduction the vanilla LSTM has been the most
commonly used architecture, but other variants have been
suggested too. Before the introduction of full BPTT training,
Gers et al. utilized a training method based on Extended
Kalman Filtering which enabled the LSTM to be trained on
some pathological cases at the cost of high computational
complexity. Schmidhuber et al. proposed using a hybrid
evolution-based method instead of BPTT for training but
retained the vanilla LSTM architecture.
Bayer et al. evolved different LSTM block architectures
that maximize ﬁtness on context-sensitive grammars. A larger
study of this kind was later done by Jozefowicz et al. . Sak
et al. introduced a linear projection layer that projects the
output of the LSTM layer down before recurrent and forward
connections in order to reduce the amount of parameters for
LSTM networks with many blocks. By introducing a trainable
scaling parameter for the slope of the gate activation functions,
Doetsch et al. were able to improve the performance of
LSTM on an ofﬂine handwriting recognition dataset. In what
they call Dynamic Cortex Memory, Otte et al. improved
convergence speed of LSTM by adding recurrent connections
between the gates of a single block (but not between the
Cho et al. proposed a simpliﬁed variant of the LSTM
architecture called Gated Recurrent Unit (GRU). They used
neither peephole connections nor output activation functions,
and coupled the input and the forget gate into an update gate.
Finally, their output gate (called reset gate) only gates the
recurrent connections to the block input (Wz). Chung et al.
 performed an initial comparison between GRU and Vanilla
LSTM and reported mixed results.
IV. EVALUATION SETUP
The focus of our study is to empirically compare different
LSTM variants, and not to achieve state-of-the-art results.
Therefore, our experiments are designed to keep the setup
simple and the comparisons fair. The vanilla LSTM is used as
a baseline and evaluated together with eight of its variants. Each
variant adds, removes, or modiﬁes the baseline in exactly one
aspect, which allows to isolate their effect. They are evaluated
on three different datasets from different domains to account
for cross-domain variations.
For fair comparison, the setup needs to be similar for
each variant. Different variants might require different settings
of hyperparameters to give good performance, and we are
interested in the best performance that can be achieved
with each variant. For this reason we chose to tune the
hyperparameters like learning rate or amount of input noise
individually for each variant. Since hyperparameter space is
large and impossible to traverse completely, random search was
used in order to obtain good-performing hyperparameters 
for every combination of variant and dataset. Random search
was also chosen for the added beneﬁt of providing enough data
for analyzing the general effect of various hyperparameters on
the performance of each LSTM variant (Section V-B).
A. Datasets
Each dataset is split into three parts: a training set, a
validation set used for early stopping and for optimizing the
hyperparameters, and a test set for the ﬁnal evaluation.
TIMIT: The TIMIT Speech corpus is large enough
to be a reasonable acoustic modeling benchmark for speech
recognition, yet it is small enough to keep a large study such
as ours manageable. Our experiments focus on the frame-wise
classiﬁcation task for this dataset, where the objective is to
TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
Ben Zoma said: "The days of 1thy
life means in the day-time; all the days
of 1thy life means even at night-time ."
(Berochoth .) And the Rabbis thought
it important that when we read the
Figure 2. (a) Example board (a08-551z, training set) from the IAM-OnDB
dataset and (b) its transcription into character label sequences.
classify each audio-frame as one of 61 phones.2 From the
raw audio we extract 12 Mel Frequency Cepstrum Coefﬁcients
(MFCCs) + energy over 25ms hamming-windows with
stride of 10ms and a pre-emphasis coefﬁcient of 0.97. This
preprocessing is standard in speech recognition and was chosen
in order to stay comparable with earlier LSTM-based results
(e.g. ). The 13 coefﬁcients along with their ﬁrst and
second derivatives comprise the 39 inputs to the network and
were normalized to have zero mean and unit variance.
The performance is measured as classiﬁcation error percentage. The training, testing, and validation sets are split in
line with Halberstadt into 3696, 400, and 192 sequences,
having 304 frames on average.
We restrict our study to the core test set, which is an
established subset of the full TIMIT corpus, and use the
splits into training, testing, and validation sets as detailed
by Halberstadt . In short, that means we only use the core
test set and drop the SA samples3 from the training set. The
validation set is built from some of the discarded samples from
the full test set.
IAM Online: The IAM Online Handwriting Database 4
consists of English sentences as time series of pen movements
that have to be mapped to characters. The IAM-OnDB dataset
splits into one training set, two validation sets, and one test set,
having 775, 192, 216, and 544 boards each. Each board, see
Figure 2(a), contains multiple hand-written lines, which in turn
consist of several strokes. We use one line per sequence, and
2Note that in linguistics a phone represents a distinct speech sound
independent of the language. In contrast, a phoneme refers to a sound that
distinguishes two words in a given language . These terms are often
confused in the machine learning literature.
3The dialect sentences (the SA samples) were meant to expose the dialectal
variants of the speakers and were read by all 630 speakers. We follow 
and remove them because they bias the distribution of phones.
4The IAM-OnDB was obtained from 
iam-on-line-handwriting-database
joined the two validation sets together, so the ﬁnal training,
validation, and testing sets contain 5 355, 2 956 and 3 859
sequences respectively.
Each handwriting line is accompanied with a target character
sequence, see Figure 2(b), assembled from the following
81 ASCII characters:
abcdefghijklmnopqrstuvwxyz
ABCDEFGHIJKLMNOPQRSTUVWXYZ
0123456789 !"#&\’()*+,-./[]:;?
The board labeled as a08-551z (in the training set) contains
a sequence of eleven percent (%) characters that does not have
an image in the strokes, and the percent character does not
occur in any other board. That board was removed from the
experiments.
We subsampled each sequence to half its length, which
speeds up the training and does not harm performance. Each
frame of the sequence is a 4-dimensional vector containing ∆x,
∆y (the change in pen position), t (time since the beginning of
the stroke), and a fourth dimension that contains value of one
at the time of the pen lifting (a transition to the next stroke)
and zeroes at all other time steps. Possible starts and ends of
characters within each stroke are not explicitly marked. No
additional preprocessing (like base-line straightening, cursive
correction, etc.) was used.
The networks were trained using the Connectionist Temporal
Classiﬁcation (CTC) error function by Graves et al. with
82 outputs (81 characters plus the special empty label). We
measure performance in terms of the Character Error Rate
(CER) after decoding using best-path decoding .
JSB Chorales: JSB Chorales is a collection of 382 fourpart harmonized chorales by J. S. Bach , consisting of
202 chorales in major keys and 180 chorals in minor keys.
We used the preprocessed piano-rolls provided by Boulanger-
Lewandowski et al. .5 These piano-rolls were generated
by transposing each MIDI sequence in C major or C minor
and sampling frames every quarter note. The networks where
trained to do next-step prediction by minimizing the negative
log-likelihood. The complete dataset consists of 229, 76, and
77 sequences (training, validation, and test sets respectively)
with an average length of 61.
B. Network Architectures & Training
A network with a single LSTM hidden layer and a sigmoid
output layer was used for the JSB Chorales task. Bidirectional
LSTM was used for TIMIT and IAM Online tasks,
consisting of two hidden layers, one processing the input
forwards and the other one backwards in time, both connected
to a single softmax output layer. As loss function we employed
Cross-Entropy Error for TIMIT and JSB Chorales, while for
the IAM Online task the Connectionist Temporal Classiﬁcation
(CTC) loss by Graves et al. was used. The initial weights
for all networks were drawn from a normal distribution with
standard deviation of 0.1. Training was done using Stochastic
Gradient Descent with Nesterov-style momentum with
5Available at at the
time of writing.
TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
updates after each sequence. The learning rate was rescaled by
a factor of (1 −momentum). Gradients were computed using
full BPTT for LSTMs . Training stopped after 150 epochs
or once there was no improvement on the validation set for
more than ﬁfteen epochs.
C. LSTM Variants
The vanilla LSTM from Section II is referred as Vanilla (V).
For activation functions we follow the standard and use the
logistic sigmoid for σ, and the hyperbolic tangent for both g
and h. The derived eight variants of the V architecture are
the following. We only report differences to the forward pass
formulas presented in Section II-A:
NIG: No Input Gate: it = 1
NFG: No Forget Gate: f t = 1
NOG: No Output Gate: ot = 1
NIAF: No Input Activation Function: g(x) = x
NOAF: No Output Activation Function: h(x) = x
CIFG: Coupled Input and Forget Gate: f t = 1 −it
NP: No Peepholes:
¯it = Wixt + Riyt−1 + bi
¯f t = Wfxt + Rfyt−1 + bf
¯ot = Woxt + Royt−1 + bo
FGR: Full Gate Recurrence:
¯it = Wixt + Riyt−1 + pi ⊙ct−1 + bi
+ Riiit−1 + Rfif t−1 + Roiot−1
¯f t = Wfxt + Rfyt−1 + pf ⊙ct−1 + bf
+ Rifit−1 + Rfff t−1 + Rofot−1
¯ot = Woxt + Royt−1 + po ⊙ct−1 + bo
+ Rioit−1 + Rfof t−1 + Rooot−1
The ﬁrst six variants are self-explanatory. The CIFG variant
uses only one gate for gating both the input and the cell
recurrent self-connection – a modiﬁcation of LSTM referred
to as Gated Recurrent Units (GRU) . This is equivalent to
setting ft = 1 −it instead of learning the forget gate weights
independently. The FGR variant adds recurrent connections
between all the gates as in the original formulation of the
LSTM . It adds nine additional recurrent weight matrices,
thus signiﬁcantly increasing the number of parameters.
D. Hyperparameter Search
While there are other methods to efﬁciently search for good
hyperparameters (cf. ), random search has several
advantages for our setting: it is easy to implement, trivial
to parallelize, and covers the search space more uniformly,
thereby improving the follow-up analysis of hyperparameter
importance.
We performed 27 random searches (one for each combination
of the nine variants and three datasets). Each random search
encompasses 200 trials for a total of 5400 trials of randomly
sampling the following hyperparameters:
• number of LSTM blocks per hidden layer: log-uniform
samples from ;
• learning rate: log-uniform samples from [10−6, 10−2];
• momentum: 1 −log-uniform samples from [0.01, 1.0];
• standard deviation of Gaussian input noise: uniform samples
from .
In the case of the TIMIT dataset, two additional (boolean)
hyperparameters were considered (not tuned for the other two
datasets). The ﬁrst one was the choice between traditional
momentum and Nesterov-style momentum . Our analysis
showed that this had no measurable effect on performance so
the latter was arbitrarily chosen for all further experiments.
The second one was whether to clip the gradients to the range
[−1, 1]. This turned out to hurt overall performance,6 therefore
the gradients were never clipped in the case of the other two
Note that, unlike an earlier small-scale study , the number
of parameters was not kept ﬁxed for all variants. Since different
variants can utilize their parameters differently, ﬁxing this
number can bias comparisons.
V. RESULTS & DISCUSSION
Each of the 5400 experiments was run on one of 128 AMD
Opteron CPUs at 2.5 GHz and took 24.3 h on average to
complete. This sums up to a total single-CPU computation
time of just below 15 years.
For TIMIT the test set performance of the best trial were
29.6% classiﬁcation error (CIFG) which is close to the best
reported result of 26.9% . Our best result of -8.38 loglikelihood (NIAF) on the JSB Chorales dataset on the other
hand is well below the -5.56 from Boulanger-Lewandowski
et al. . Best LSTM result is 26.9% For the IAM Online
dataset our best result was a Character Error Rate of 9.26%
(NP) on the test set. The best previously published result is
11.5% CER by Graves et al. using a different and much
more extensive preprocessing.7 Note though, that the goal of
this study is not to provide state-of-the-art results, but to do a
fair comparison of different LSTM variants. So these numbers
are only meant as a rough orientation for the reader.
A. Comparison of the Variants
A summary of the random search results is shown in Figure 3.
Welch’s t-test at a signiﬁcance level of p = 0.05 was used8
to determine whether the mean test set performance of each
variant was signiﬁcantly different from that of the baseline. The
box for a variant is highlighted in blue if its mean performance
differs signiﬁcantly from the mean performance of the vanilla
The results in the top half of Figure 3 represent the
distribution of all 200 test set performances over the whole
search space. Any conclusions drawn from them are therefore
6Although this may very well be the result of the range having been chosen
too tightly.
7Note that these numbers differ from the best test set performances that can
be found in Figure 3. This is the case because here we only report the single
best performing trial as determined on the validation set. In Figure 3, on the
other hand, we show the test set performance of the 20 best trials for each
8We applied the Bonferroni adjustment to correct for performing eight
different tests (one for each variant).
TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
NOG NIAF NIG NFG NOAF
classiﬁcation error in %
NOG NIAF NIG NFG NOAF
character error rate
IAM Online
NOG NIAF NIG NFG NOAF
negative log-likelihood
JSB Chorales
number of parameters ∗105
number of parameters ∗105
number of parameters ∗105
NOG NIAF NIG NFG NOAF
classiﬁcation error in %
NOG NIAF NIG NFG NOAF
character error rate
IAM Online
NOG NIAF NIG NFG NOAF
negative log-likelihood
JSB Chorales
number of parameters ∗105
number of parameters ∗105
number of parameters ∗105
Figure 3. Test set performance for all 200 trials (top) and for the best 10% (bottom) trials (according to the validation set) for each dataset and variant. Boxes
show the range between the 25th and the 75th percentile of the data, while the whiskers indicate the whole range. The red dot represents the mean and the red
line the median of the data. The boxes of variants that differ signiﬁcantly from the vanilla LSTM are shown in blue with thick lines. The grey histogram in the
background presents the average number of parameters for the top 10% performers of every variant.
speciﬁc to our choice of search ranges. We have tried to chose
reasonable ranges for the hyperparameters that include the best
settings for each variant and are still small enough to allow
for an effective search. The means and variances tend to be
rather similar for the different variants and datasets, but even
here some signiﬁcant differences can be found.
In order to draw some more interesting conclusions we
restrict our further analysis to the top 10% performing trials
for each combination of dataset and variant (see bottom half
of Figure 3). This way our ﬁndings will be less dependent on
the chosen search space and will be representative for the case
of “reasonable hyperparameter tuning efforts.”9
The ﬁrst important observation based on Figure 3 is that
removing the output activation function (NOAF) or the forget
gate (NFG) signiﬁcantly hurt performance on all three datasets.
Apart from the CEC, the ability to forget old information
and the squashing of the cell state appear to be critical for
the LSTM architecture. Indeed, without the output activation
function, the block output can in principle grow unbounded.
Coupling the input and the forget gate avoids this problem and
might render the use of an output non-linearity less important,
which could explain why GRU performs well without it.
9How much effort is “reasonable” will still depend on the search space. If
the ranges are chosen much larger, the search will take much longer to ﬁnd
good hyperparameters.
Input and forget gate coupling (CIFG) did not signiﬁcantly
change mean performance on any of the datasets, although
the best performance improved slightly on music modeling.
Similarly, removing peephole connections (NP) also did not
lead to signiﬁcant changes, but the best performance improved
slightly for handwriting recognition. Both of these variants
simplify LSTMs and reduce the computational complexity, so
it might be worthwhile to incorporate these changes into the
architecture.
Adding full gate recurrence (FGR) did not signiﬁcantly
change performance on TIMIT or IAM Online, but led to
worse results on the JSB Chorales dataset. Given that this
variant greatly increases the number of parameters, we generally
advise against using it. Note that this feature was present in
the original proposal of LSTM , but has been absent
in all following studies.
Removing the input gate (NIG), the output gate (NOG), and
the input activation function (NIAF) led to a signiﬁcant reduction in performance on speech and handwriting recognition.
However, there was no signiﬁcant effect on music modeling
performance. A small (but statistically insigniﬁcant) average
performance improvement was observed for the NIG and NIAF
architectures on music modeling. We hypothesize that these
behaviors will generalize to similar problems such as language
modeling. For supervised learning on continuous real-valued
TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
data (such as speech and handwriting recognition), the input
gate, output gate, and input activation function are all crucial
for obtaining good performance.
B. Impact of Hyperparameters
The fANOVA framework for assessing hyperparameter
importance by Hutter et al. is based on the observation
that marginalizing over dimensions can be done efﬁciently
in regression trees. This allows predicting the marginal error
for one hyperparameter while averaging over all the others.
Traditionally this would require a full hyperparameter grid
search, whereas here the hyperparameter space can be sampled
at random.
Average performance for any slice of the hyperparameter
space is obtained by ﬁrst training a regression tree and then
summing over its predictions along the corresponding subset
of dimensions. To be precise, a random regression forest
of 100 trees is trained and their prediction performance is
averaged. This improves the generalization and allows for an
estimation of uncertainty of those predictions. The obtained
marginals can then be used to decompose the variance into
additive components using the functional ANalysis Of VAriance
(fANOVA) method which provides an insight into the
overall importance of hyperparameters and their interactions.
Learning rate: Learning rate is the most important hyperparameter, therefore it is very important to understand how to
set it correctly in order to achieve good performance. Figure 4
shows (in blue) how setting the learning rate value affects the
predicted average performance on the test set. It is important
to note that this is an average over all other hyperparameters
and over all the trees in the regression forest. The shaded area
around the curve indicates the standard deviation over tree
predictions (not over other hyperparameters), thus quantifying
the reliability of the average. The same is shown in green with
the predicted average training time.
The plots in Figure 4 show that the optimal value for the
learning rate is dependent on the dataset. For each dataset,
there is a large basin (up to two orders of magnitude) of good
learning rates inside of which the performance does not vary
much. A related but unsurprising observation is that there is a
sweet-spot for the learning rate at the high end of the basin.10
In this region, the performance is good and the training time
is small. So while searching for a good learning rate for the
LSTM, it is sufﬁcient to do a coarse search by starting with a
high value (e.g. 1.0) and dividing it by ten until performance
stops increasing.
Figure 5 also shows that the fraction of variance caused
by the learning rate is much bigger than the fraction due
to interaction between learning rate and hidden layer size
(some part of the “higher order” piece, for more see below
at Interaction of Hyperparameters). This suggests that the
learning rate can be quickly tuned on a small network and
then used to train a large one.
10Note that it is unfortunately outside the investigated range for IAM Online
and JSB Chorales. This means that ideally we should have chosen the range
of learning rates to include higher values as well.
Hidden Layer Size: Not surprisingly the hidden layer size
is an important hyperparameter affecting the LSTM network
performance. As expected, larger networks perform better, but
with diminishing returns. It can also be seen in Figure 4 (middle,
green) that the required training time increases with the network
size. Note that the scale here is wall-time and thus factors in
both the increased computation time for each epoch as well as
the convergence speed.
Input Noise: Additive Gaussian noise on the inputs, a
traditional regularizer for neural networks, has been used for
LSTM as well. However, we ﬁnd that not only does it almost
always hurt performance, it also slightly increases training
times. The only exception is TIMIT, where a small dip in error
for the range of [0.2, 0.5] is observed.
Momentum: One unexpected result of this study is that
momentum affects neither performance nor training time in
any signiﬁcant way. This follows from the observation that for
none of the datasets, momentum accounted for more than 1%
of the variance of test set performance. It should be noted that
for TIMIT the interaction between learning rate and momentum
accounts for 2.5% of the total variance, but as with learning
rate × hidden size (cf. Interaction of Hyperparameters below)
it does not reveal any interpretable structure. This may be
the result of our choice to scale learning rates dependent on
momentum (Section IV-B). These observations suggest that
momentum does not offer substantial beneﬁts when training
LSTMs with online stochastic gradient descent.
Analysis of Variance: Figure 5 shows what fraction of the
test set performance variance can be attributed to different
hyperparameters. It is obvious that the learning rate is by
far the most important hyperparameter, always accounting for
more than two thirds of the variance. The next most important
hyperparameter is the hidden layer size, followed by the input
noise, leaving the momentum with less than one percent of the
variance. Higher order interactions play an important role in
the case of TIMIT, but are much less important for the other
two data sets.
Interaction of Hyperparameters: Some hyperparameters
interact with each other resulting in different performance
from what could be expected by looking at them individually.
As shown in Figure 5 all these interactions together explain
between 5% and 20% of the variance in test set performance.
Understanding these interactions might allow us to speed up
the search for good combinations of hyperparameters. To
that end we visualize the interaction between all pairs of
hyperparameters in Figure 6. Each heat map in the left part
shows marginal performance for different values of the respective two hyperparameters. This is the average performance
predicted by the decision forest when marginalizing over all
other hyperparameters. So each one is the 2D version of the
performance plots from Figure 4 in the paper.
The right side employs the idea of ANOVA to better
illustrate the interaction between the hyperparameters. This
means that variance of performance that can be explained
by varying a single hyperparameter has been removed. In
case two hyperparameters do not interact at all (are perfectly
independent), that residual would thus be all zero (grey).
TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
Classification Error
Character Error Rate
IAM Online
learning rate
Negative Log Likelihood
100 120 140 160 180 200
hidden size
JSB Chorales
input noise standard deviation
total time in h
total time in h
total time in h
Figure 4. Predicted marginal error (blue) and marginal time for different values of the learning rate, hidden size, and the input noise (columns) for the test set
of all three datasets (rows). The shaded area indicates the standard deviation between the tree-predicted marginals and thus the reliability of the predicted mean
performance. Note that each plot is for the vanilla LSTM but curves for all variants that are not signiﬁcantly worse look very similar.
Pie charts showing which fraction of variance of the test set
performance can be attributed to each of the hyperparameters. The percentage
of variance that is due to interactions between multiple parameters is indicated
as “higher order.”
For example, looking at the pair hidden size and learning
rate on the left side for the TIMIT dataset, we can see that
performance varies strongly along the x-axis (learning rate),
ﬁrst decreasing and then increasing again. This is what we
would expect knowing the valley-shape of the learning rate
from Figure 4. Along the y-axis (hidden size) performance
seems to decrease slightly from top to bottom. Again this is
roughly what we would expect from the hidden size plot in
On the right side of Figure 6 we can see for the same pair
of hyperparameters how their interaction differs from the case
of them being completely independent. This heat map exhibits
less structure, and it may in fact be the case that we would
need more samples to properly analyze the interplay between
them. However, given our observations so far this might not
be worth the effort. In any case, it is clear from the plot on the
left that varying the hidden size does not change the region of
optimal learning rate.
One clear interaction pattern can be observed in the IAM Online and JSB datasets between learning rate and input noise.
Here it can be seen that for high learning rates (⪆10−4)
lower input noise (⪅.5) is better like also observed in the
marginals from Figure 4. But this trend reverses for lower
learning rates, where higher values of input noise are beneﬁcial.
Though interesting this is not of any practical relevance because
performance is generally bad in that region of low learning
rates. Apart from this, however, it is difﬁcult to discern any
regularities in the analyzed hyperparameter interactions. We
conclude that there is little practical value in attending to the
interplay between hyperparameters. So for practical purposes
hyperparameters can be treated as approximately independent
and thus optimized separately.
VI. CONCLUSION
This paper reports the results of a large scale study on
variants of the LSTM architecture. We conclude that the
most commonly used LSTM architecture (vanilla LSTM)
performs reasonably well on various datasets. None of the eight
investigated modiﬁcations signiﬁcantly improves performance.
However, certain modiﬁcations such as coupling the input and
TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
hidden size
learning rate
input noise std
hidden size
hidden size
learning rate
input noise std
hidden size
hidden size
learning rate
input noise std
hidden size
hidden size
learning rate
input noise std
hidden size
JSB Chorales
hidden size
learning rate
input noise std
hidden size
JSB Chorales
hidden size
learning rate
input noise std
hidden size
Figure 6. Total marginal predicted performance for all pairs of hyperparameters (left) and the variation only due to their interaction (right). The plot is divided
vertically into three subplots, one for every dataset (TIMIT, IAM Online, and JSB Chorales). The subplots itself are divided horizontally into two parts, each
containing a lower triangular matrix of heat maps. The rows and columns of these matrices represent the different hyperparameters (learning rate, momentum,
hidden size, and input noise) and there is one heat map for every combination. The color encodes the performance as measured by the Classiﬁcation Error for
TIMIT, Character Error Rate for IAM Online and Negative Log-Likelihood for the JSB Chorales Dataset. For all datasets low (blue) is better than high (red).
TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
forget gates (CIFG) or removing peephole connections (NP)
simpliﬁed LSTMs in our experiments without signiﬁcantly
decreasing performance. These two variants are also attractive
because they reduce the number of parameters and the
computational cost of the LSTM.
The forget gate and the output activation function are the
most critical components of the LSTM block. Removing any
of them signiﬁcantly impairs performance. We hypothesize
that the output activation function is needed to prevent the
unbounded cell state to propagate through the network and
destabilize learning. This would explain why the LSTM variant
GRU can perform reasonably well without it: its cell state is
bounded because of the coupling of input and forget gate.
As expected, the learning rate is the most crucial hyperparameter, followed by the network size. Surprisingly though, the
use of momentum was found to be unimportant in our setting
of online gradient descent. Gaussian noise on the inputs was
found to be moderately helpful for TIMIT, but harmful for the
other datasets.
The analysis of hyperparameter interactions revealed no
apparent structure. Furthermore, even the highest measured
interaction (between learning rate and network size) is quite
small. This implies that for practical purposes the hyperparameters can be treated as approximately independent. In particular,
the learning rate can be tuned ﬁrst using a fairly small network,
thus saving a lot of experimentation time.
Neural networks can be tricky to use for many practitioners
compared to other methods whose properties are already well
understood. This has remained a hurdle for newcomers to the
ﬁeld since a lot of practical choices are based on the intuitions
of experts, as well as experiences gained over time. With this
study, we have attempted to back some of these intuitions with
experimental results. We have also presented new insights, both
on architecture selection and hyperparameter tuning for LSTM
networks which have emerged as the method of choice for
solving complex sequence learning problems. In future work,
we plan to explore more complex modiﬁcations of the LSTM
architecture.